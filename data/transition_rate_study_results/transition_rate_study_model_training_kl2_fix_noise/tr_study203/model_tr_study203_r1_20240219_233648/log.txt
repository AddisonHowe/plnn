Args:
Namespace(name='model_tr_study203', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1', training_data='data/transition_rate_studies/tr_study203/tr_study203_training/r1', validation_data='data/transition_rate_studies/tr_study203/tr_study203_validation/r1', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3169565536

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/10] avg loss: 11.235186953779511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.235186953779511 | validation: 11.125478664051421]
	TIME [epoch: 78.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.799536225598192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.799536225598192 | validation: 8.84973552094808]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.288409274487204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.288409274487204 | validation: 7.8134573638148215]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.562476728507103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.562476728507103 | validation: 9.042041606359927]
	TIME [epoch: 8.55 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.232969873096442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.232969873096442 | validation: 6.961620007032127]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.728466037581478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.728466037581478 | validation: 5.918210642845938]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.203872633501929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.203872633501929 | validation: 5.762368420118392]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.244669741122435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.244669741122435 | validation: 6.31608855360102]
	TIME [epoch: 8.51 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.909359387505853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.909359387505853 | validation: 5.887160181406502]
	TIME [epoch: 8.49 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.650246672781688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.650246672781688 | validation: 6.107418791255256]
	TIME [epoch: 8.49 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.620460342335819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.620460342335819 | validation: 6.171055377444057]
	TIME [epoch: 8.52 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.484445174630017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.484445174630017 | validation: 6.789724516211017]
	TIME [epoch: 8.5 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.77952478425361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.77952478425361 | validation: 5.128973483761886]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.107959588135345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.107959588135345 | validation: 6.135866104604004]
	TIME [epoch: 8.49 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.09195590231538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.09195590231538 | validation: 6.107869447710014]
	TIME [epoch: 8.51 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.941810324794934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.941810324794934 | validation: 5.136852469356711]
	TIME [epoch: 8.49 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.727531348585822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.727531348585822 | validation: 4.9707008939066615]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9990585644596535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9990585644596535 | validation: 4.981922133322902]
	TIME [epoch: 8.49 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.312388615034367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.312388615034367 | validation: 4.978455395952338]
	TIME [epoch: 8.52 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5601551942052994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5601551942052994 | validation: 4.381236127839877]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.477670355573923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.477670355573923 | validation: 4.867538072552413]
	TIME [epoch: 8.49 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.368287460213896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.368287460213896 | validation: 4.665269090446241]
	TIME [epoch: 8.49 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.393943085991718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.393943085991718 | validation: 4.901338281564828]
	TIME [epoch: 8.52 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.304372645095649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.304372645095649 | validation: 4.533711571407066]
	TIME [epoch: 8.49 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2399467792176724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2399467792176724 | validation: 5.114885650969125]
	TIME [epoch: 8.49 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.272225589116217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.272225589116217 | validation: 5.114725589435017]
	TIME [epoch: 8.49 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1501000983369125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1501000983369125 | validation: 4.375342119804633]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0607890285463517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0607890285463517 | validation: 4.610401692010038]
	TIME [epoch: 8.5 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.063669857040902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.063669857040902 | validation: 4.372810876613016]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.093994312622006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.093994312622006 | validation: 4.247745359702602]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.025631871514854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.025631871514854 | validation: 4.281200170873255]
	TIME [epoch: 8.51 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0533464950058047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0533464950058047 | validation: 4.1798646120382745]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.990011618969838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.990011618969838 | validation: 4.416366757737263]
	TIME [epoch: 8.5 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2565797490832815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2565797490832815 | validation: 4.031838961908916]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.224701226132513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.224701226132513 | validation: 4.560469181312662]
	TIME [epoch: 8.49 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9708339237044545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9708339237044545 | validation: 4.310245866344051]
	TIME [epoch: 8.49 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.967883485871569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.967883485871569 | validation: 4.527525895433856]
	TIME [epoch: 8.49 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.023232845595219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.023232845595219 | validation: 4.069730220343754]
	TIME [epoch: 8.52 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9130658128034526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9130658128034526 | validation: 3.8856470210517937]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8744601182342353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8744601182342353 | validation: 4.572620049799195]
	TIME [epoch: 8.49 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9361527407739945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9361527407739945 | validation: 4.445659209049077]
	TIME [epoch: 8.49 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9059151434685475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9059151434685475 | validation: 4.482517864475539]
	TIME [epoch: 8.51 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8747305157113643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8747305157113643 | validation: 4.285808930047846]
	TIME [epoch: 8.5 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.848949157018958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.848949157018958 | validation: 4.450775486978887]
	TIME [epoch: 8.49 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9126937511947797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9126937511947797 | validation: 4.239592036466211]
	TIME [epoch: 8.49 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.163959097065739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.163959097065739 | validation: 3.936327586640118]
	TIME [epoch: 8.52 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1112781493875006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1112781493875006 | validation: 3.9966349742390124]
	TIME [epoch: 8.5 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7774374541455296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7774374541455296 | validation: 5.466697144845325]
	TIME [epoch: 8.49 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.048262345548836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.048262345548836 | validation: 3.7111818450868266]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7873569335171147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7873569335171147 | validation: 4.0267321211431115]
	TIME [epoch: 8.52 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7078840956871826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7078840956871826 | validation: 4.023739665619993]
	TIME [epoch: 8.49 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8277624102720123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8277624102720123 | validation: 3.623713271912121]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8579140458128083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8579140458128083 | validation: 4.020447622564314]
	TIME [epoch: 8.5 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.675570550045873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.675570550045873 | validation: 3.982306892617949]
	TIME [epoch: 8.5 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7323623710994167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7323623710994167 | validation: 4.2319066836076855]
	TIME [epoch: 8.49 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6381018933831277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6381018933831277 | validation: 3.884954139857827]
	TIME [epoch: 8.48 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6234596493520135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6234596493520135 | validation: 4.0724935611553965]
	TIME [epoch: 8.5 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.432205216090466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.432205216090466 | validation: 3.9389919463454266]
	TIME [epoch: 8.5 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4106574905662748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4106574905662748 | validation: 3.661172640144375]
	TIME [epoch: 8.49 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4921355311500983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4921355311500983 | validation: 4.424824818533977]
	TIME [epoch: 8.48 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.347622309690133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.347622309690133 | validation: 4.834685570029619]
	TIME [epoch: 8.5 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.915856033055005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.915856033055005 | validation: 3.6303767724510543]
	TIME [epoch: 8.49 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.396780600662231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.396780600662231 | validation: 3.902349681173048]
	TIME [epoch: 8.48 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.433941986348419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.433941986348419 | validation: 3.7780494194276946]
	TIME [epoch: 8.48 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4648913045838916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4648913045838916 | validation: 3.562256083262774]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.436736016417715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.436736016417715 | validation: 3.8961989311687626]
	TIME [epoch: 8.5 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9306692421328373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9306692421328373 | validation: 5.4480322887699115]
	TIME [epoch: 8.48 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0849816379750155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0849816379750155 | validation: 4.548585255925872]
	TIME [epoch: 8.49 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5992090036434714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5992090036434714 | validation: 4.167738896582293]
	TIME [epoch: 8.51 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3589610208040717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3589610208040717 | validation: 4.263669168832953]
	TIME [epoch: 8.49 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3904363206964296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3904363206964296 | validation: 3.665690568771372]
	TIME [epoch: 8.49 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4302483050405046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4302483050405046 | validation: 3.9077737373199777]
	TIME [epoch: 8.5 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.868652085510169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.868652085510169 | validation: 4.373129762403667]
	TIME [epoch: 8.51 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.472955963620977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.472955963620977 | validation: 4.658263598768605]
	TIME [epoch: 8.49 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5117124128420714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5117124128420714 | validation: 3.9534328673608163]
	TIME [epoch: 8.48 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.37939401851325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.37939401851325 | validation: 3.564407823045464]
	TIME [epoch: 8.48 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4038401016220234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4038401016220234 | validation: 3.9131601613582347]
	TIME [epoch: 8.51 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3767824863720923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3767824863720923 | validation: 3.874533360392914]
	TIME [epoch: 8.48 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4405793367477924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4405793367477924 | validation: 3.6757384033586575]
	TIME [epoch: 8.48 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.408314877387953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.408314877387953 | validation: 3.945416684976636]
	TIME [epoch: 8.48 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.472871274667015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.472871274667015 | validation: 3.392811112676175]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.449988128257334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.449988128257334 | validation: 4.03525847542468]
	TIME [epoch: 8.49 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3915238018477516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3915238018477516 | validation: 4.302368742409666]
	TIME [epoch: 8.48 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3725974015274347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3725974015274347 | validation: 3.9321263695828783]
	TIME [epoch: 8.49 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3761597152155454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3761597152155454 | validation: 3.9498258361038054]
	TIME [epoch: 8.52 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5086187627620116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5086187627620116 | validation: 3.6109027906887796]
	TIME [epoch: 8.49 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.334370056900785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.334370056900785 | validation: 3.5895511041645647]
	TIME [epoch: 8.49 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3418503083791173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3418503083791173 | validation: 3.545484401619505]
	TIME [epoch: 8.49 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2936380886574472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2936380886574472 | validation: 3.5754310219465397]
	TIME [epoch: 8.51 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2473520373941356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2473520373941356 | validation: 4.220594984854795]
	TIME [epoch: 8.49 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.391203086026664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.391203086026664 | validation: 4.066337023632567]
	TIME [epoch: 8.49 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.402476741118723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.402476741118723 | validation: 3.5391672070111095]
	TIME [epoch: 8.51 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.272666016194444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.272666016194444 | validation: 3.586505455661664]
	TIME [epoch: 8.51 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2252908376097267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2252908376097267 | validation: 3.7423908991907697]
	TIME [epoch: 8.49 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.327360425365444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.327360425365444 | validation: 3.605354120895723]
	TIME [epoch: 8.49 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2484522821508524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2484522821508524 | validation: 3.7738568084061774]
	TIME [epoch: 8.51 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.270295075179462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.270295075179462 | validation: 4.156656633064827]
	TIME [epoch: 8.5 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2384493678244994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2384493678244994 | validation: 3.7675241669083337]
	TIME [epoch: 8.5 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.234094563186552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.234094563186552 | validation: 3.7581680129007955]
	TIME [epoch: 8.49 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3620970226976614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3620970226976614 | validation: 4.103031213483183]
	TIME [epoch: 8.52 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.479331082893371		[learning rate: 0.0099673]
	Learning Rate: 0.00996733
	LOSS [training: 2.479331082893371 | validation: 6.132837373591483]
	TIME [epoch: 8.51 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6283771950974804		[learning rate: 0.0099312]
	Learning Rate: 0.00993116
	LOSS [training: 3.6283771950974804 | validation: 4.396307487030047]
	TIME [epoch: 8.48 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.369372065999012		[learning rate: 0.0098951]
	Learning Rate: 0.00989512
	LOSS [training: 2.369372065999012 | validation: 3.423448872845143]
	TIME [epoch: 8.48 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.158695523608672		[learning rate: 0.0098592]
	Learning Rate: 0.00985921
	LOSS [training: 2.158695523608672 | validation: 3.872007991156199]
	TIME [epoch: 8.5 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2048806124536773		[learning rate: 0.0098234]
	Learning Rate: 0.00982343
	LOSS [training: 2.2048806124536773 | validation: 3.8866119981179637]
	TIME [epoch: 8.49 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1828302874722985		[learning rate: 0.0097878]
	Learning Rate: 0.00978778
	LOSS [training: 2.1828302874722985 | validation: 3.4074858555939325]
	TIME [epoch: 8.49 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.425972934622985		[learning rate: 0.0097523]
	Learning Rate: 0.00975226
	LOSS [training: 2.425972934622985 | validation: 3.3300677844925417]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1953352634868493		[learning rate: 0.0097169]
	Learning Rate: 0.00971687
	LOSS [training: 2.1953352634868493 | validation: 3.555708934603329]
	TIME [epoch: 8.52 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.130695802247571		[learning rate: 0.0096816]
	Learning Rate: 0.0096816
	LOSS [training: 2.130695802247571 | validation: 3.288105266107329]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1990028270023982		[learning rate: 0.0096465]
	Learning Rate: 0.00964647
	LOSS [training: 2.1990028270023982 | validation: 3.596163435469464]
	TIME [epoch: 8.51 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.121031159238798		[learning rate: 0.0096115]
	Learning Rate: 0.00961146
	LOSS [training: 2.121031159238798 | validation: 3.363404305595232]
	TIME [epoch: 8.51 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.087962989056593		[learning rate: 0.0095766]
	Learning Rate: 0.00957658
	LOSS [training: 2.087962989056593 | validation: 3.5172997250343627]
	TIME [epoch: 8.53 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.109971661131044		[learning rate: 0.0095418]
	Learning Rate: 0.00954183
	LOSS [training: 2.109971661131044 | validation: 3.8086668839155964]
	TIME [epoch: 8.51 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.143482584401151		[learning rate: 0.0095072]
	Learning Rate: 0.0095072
	LOSS [training: 2.143482584401151 | validation: 3.1134777754430383]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5939163651362245		[learning rate: 0.0094727]
	Learning Rate: 0.0094727
	LOSS [training: 2.5939163651362245 | validation: 6.037342816544081]
	TIME [epoch: 8.51 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0710730334342866		[learning rate: 0.0094383]
	Learning Rate: 0.00943832
	LOSS [training: 3.0710730334342866 | validation: 5.129032186252894]
	TIME [epoch: 8.53 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7084854840746755		[learning rate: 0.0094041]
	Learning Rate: 0.00940407
	LOSS [training: 2.7084854840746755 | validation: 4.8142062255211835]
	TIME [epoch: 8.5 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8236141218081534		[learning rate: 0.0093699]
	Learning Rate: 0.00936994
	LOSS [training: 2.8236141218081534 | validation: 4.4456428617207795]
	TIME [epoch: 8.5 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4237772351669475		[learning rate: 0.0093359]
	Learning Rate: 0.00933594
	LOSS [training: 2.4237772351669475 | validation: 5.032471736618018]
	TIME [epoch: 8.5 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5555512621035623		[learning rate: 0.0093021]
	Learning Rate: 0.00930206
	LOSS [training: 2.5555512621035623 | validation: 3.0974991355576575]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4209141678983173		[learning rate: 0.0092683]
	Learning Rate: 0.0092683
	LOSS [training: 2.4209141678983173 | validation: 3.241639080562326]
	TIME [epoch: 8.51 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0028155951951705		[learning rate: 0.0092347]
	Learning Rate: 0.00923466
	LOSS [training: 2.0028155951951705 | validation: 3.6047698309556457]
	TIME [epoch: 8.5 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.077340136693274		[learning rate: 0.0092011]
	Learning Rate: 0.00920115
	LOSS [training: 2.077340136693274 | validation: 4.861915560772631]
	TIME [epoch: 8.51 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4499929986608047		[learning rate: 0.0091678]
	Learning Rate: 0.00916776
	LOSS [training: 2.4499929986608047 | validation: 3.9042400070377203]
	TIME [epoch: 8.52 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.008802248318104		[learning rate: 0.0091345]
	Learning Rate: 0.00913449
	LOSS [training: 2.008802248318104 | validation: 3.549544130711893]
	TIME [epoch: 8.5 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9041853780434486		[learning rate: 0.0091013]
	Learning Rate: 0.00910134
	LOSS [training: 1.9041853780434486 | validation: 4.0077765091285045]
	TIME [epoch: 8.49 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9479417826320113		[learning rate: 0.0090683]
	Learning Rate: 0.00906831
	LOSS [training: 1.9479417826320113 | validation: 3.452390208896097]
	TIME [epoch: 8.51 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9869497052810865		[learning rate: 0.0090354]
	Learning Rate: 0.0090354
	LOSS [training: 1.9869497052810865 | validation: 3.8091190221842934]
	TIME [epoch: 8.51 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.411527856553302		[learning rate: 0.0090026]
	Learning Rate: 0.00900261
	LOSS [training: 2.411527856553302 | validation: 3.632521503822007]
	TIME [epoch: 8.5 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2168287303187952		[learning rate: 0.0089699]
	Learning Rate: 0.00896994
	LOSS [training: 2.2168287303187952 | validation: 3.749694756213765]
	TIME [epoch: 8.5 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.107629924210614		[learning rate: 0.0089374]
	Learning Rate: 0.00893739
	LOSS [training: 2.107629924210614 | validation: 5.97454096835985]
	TIME [epoch: 8.52 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.392214196559743		[learning rate: 0.008905]
	Learning Rate: 0.00890495
	LOSS [training: 2.392214196559743 | validation: 3.632382715496255]
	TIME [epoch: 8.51 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8986644089016729		[learning rate: 0.0088726]
	Learning Rate: 0.00887263
	LOSS [training: 1.8986644089016729 | validation: 3.32726504263724]
	TIME [epoch: 8.5 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8747042304352095		[learning rate: 0.0088404]
	Learning Rate: 0.00884044
	LOSS [training: 1.8747042304352095 | validation: 3.3371214567969774]
	TIME [epoch: 8.5 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8208779489274245		[learning rate: 0.0088084]
	Learning Rate: 0.00880835
	LOSS [training: 1.8208779489274245 | validation: 3.095587012090478]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3847294774191914		[learning rate: 0.0087764]
	Learning Rate: 0.00877639
	LOSS [training: 2.3847294774191914 | validation: 3.307432172634986]
	TIME [epoch: 8.52 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9552933552628722		[learning rate: 0.0087445]
	Learning Rate: 0.00874454
	LOSS [training: 1.9552933552628722 | validation: 3.637354260949894]
	TIME [epoch: 8.5 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.899498448095694		[learning rate: 0.0087128]
	Learning Rate: 0.0087128
	LOSS [training: 1.899498448095694 | validation: 3.1656141417038555]
	TIME [epoch: 8.51 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9335922812292914		[learning rate: 0.0086812]
	Learning Rate: 0.00868118
	LOSS [training: 1.9335922812292914 | validation: 3.2814641619515887]
	TIME [epoch: 8.53 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8757850213991887		[learning rate: 0.0086497]
	Learning Rate: 0.00864968
	LOSS [training: 1.8757850213991887 | validation: 3.288027169401505]
	TIME [epoch: 8.51 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.904621849995707		[learning rate: 0.0086183]
	Learning Rate: 0.00861829
	LOSS [training: 1.904621849995707 | validation: 3.1566046874712117]
	TIME [epoch: 8.5 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8126879396965574		[learning rate: 0.008587]
	Learning Rate: 0.00858701
	LOSS [training: 1.8126879396965574 | validation: 3.1089657925684167]
	TIME [epoch: 8.5 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7842657542506433		[learning rate: 0.0085559]
	Learning Rate: 0.00855585
	LOSS [training: 1.7842657542506433 | validation: 3.2269237884839996]
	TIME [epoch: 8.52 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7971150586576772		[learning rate: 0.0085248]
	Learning Rate: 0.0085248
	LOSS [training: 1.7971150586576772 | validation: 3.4149732547424847]
	TIME [epoch: 8.5 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.756849704998815		[learning rate: 0.0084939]
	Learning Rate: 0.00849386
	LOSS [training: 1.756849704998815 | validation: 3.900904846810567]
	TIME [epoch: 8.5 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3470807133523772		[learning rate: 0.008463]
	Learning Rate: 0.00846304
	LOSS [training: 2.3470807133523772 | validation: 4.036112832143204]
	TIME [epoch: 8.5 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3878755935232876		[learning rate: 0.0084323]
	Learning Rate: 0.00843233
	LOSS [training: 2.3878755935232876 | validation: 4.5362140470611365]
	TIME [epoch: 8.52 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5180525335407147		[learning rate: 0.0084017]
	Learning Rate: 0.00840172
	LOSS [training: 2.5180525335407147 | validation: 4.2709052768025195]
	TIME [epoch: 8.5 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.430238490746103		[learning rate: 0.0083712]
	Learning Rate: 0.00837123
	LOSS [training: 2.430238490746103 | validation: 4.264155096973564]
	TIME [epoch: 8.49 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9508686925111285		[learning rate: 0.0083409]
	Learning Rate: 0.00834085
	LOSS [training: 1.9508686925111285 | validation: 3.188785821132376]
	TIME [epoch: 8.5 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.873802824355997		[learning rate: 0.0083106]
	Learning Rate: 0.00831059
	LOSS [training: 1.873802824355997 | validation: 4.204553731886627]
	TIME [epoch: 8.52 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1828015354758383		[learning rate: 0.0082804]
	Learning Rate: 0.00828043
	LOSS [training: 2.1828015354758383 | validation: 4.530424528082425]
	TIME [epoch: 8.5 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1551076923315895		[learning rate: 0.0082504]
	Learning Rate: 0.00825037
	LOSS [training: 2.1551076923315895 | validation: 2.9965184294770957]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.019687023352758		[learning rate: 0.0082204]
	Learning Rate: 0.00822043
	LOSS [training: 2.019687023352758 | validation: 4.3401739823273795]
	TIME [epoch: 8.5 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2155151739617005		[learning rate: 0.0081906]
	Learning Rate: 0.0081906
	LOSS [training: 2.2155151739617005 | validation: 3.7813379760515717]
	TIME [epoch: 8.51 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.085660887708035		[learning rate: 0.0081609]
	Learning Rate: 0.00816088
	LOSS [training: 2.085660887708035 | validation: 5.064283566838363]
	TIME [epoch: 8.49 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.158174957508235		[learning rate: 0.0081313]
	Learning Rate: 0.00813126
	LOSS [training: 2.158174957508235 | validation: 3.3102464974212995]
	TIME [epoch: 8.49 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7960256969344144		[learning rate: 0.0081018]
	Learning Rate: 0.00810175
	LOSS [training: 1.7960256969344144 | validation: 4.281172338291711]
	TIME [epoch: 8.5 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.963642807062881		[learning rate: 0.0080724]
	Learning Rate: 0.00807235
	LOSS [training: 1.963642807062881 | validation: 3.5588672191504473]
	TIME [epoch: 8.51 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8867709993077793		[learning rate: 0.0080431]
	Learning Rate: 0.00804305
	LOSS [training: 1.8867709993077793 | validation: 4.806686341900301]
	TIME [epoch: 8.5 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.210770381648763		[learning rate: 0.0080139]
	Learning Rate: 0.00801387
	LOSS [training: 2.210770381648763 | validation: 2.9686954639341567]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8529792215204122		[learning rate: 0.0079848]
	Learning Rate: 0.00798478
	LOSS [training: 1.8529792215204122 | validation: 3.089826495274016]
	TIME [epoch: 8.52 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.114674262149951		[learning rate: 0.0079558]
	Learning Rate: 0.00795581
	LOSS [training: 2.114674262149951 | validation: 3.1543964599455196]
	TIME [epoch: 8.5 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9301351512289746		[learning rate: 0.0079269]
	Learning Rate: 0.00792693
	LOSS [training: 1.9301351512289746 | validation: 3.3197989878426757]
	TIME [epoch: 8.5 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7489399870431792		[learning rate: 0.0078982]
	Learning Rate: 0.00789817
	LOSS [training: 1.7489399870431792 | validation: 3.258079377993134]
	TIME [epoch: 8.5 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6558678980269188		[learning rate: 0.0078695]
	Learning Rate: 0.0078695
	LOSS [training: 1.6558678980269188 | validation: 3.477997999635674]
	TIME [epoch: 8.51 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6525153911197257		[learning rate: 0.0078409]
	Learning Rate: 0.00784094
	LOSS [training: 1.6525153911197257 | validation: 3.3994393801163207]
	TIME [epoch: 8.51 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2554947632039664		[learning rate: 0.0078125]
	Learning Rate: 0.00781249
	LOSS [training: 2.2554947632039664 | validation: 3.780992415863715]
	TIME [epoch: 8.5 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.142663054250632		[learning rate: 0.0077841]
	Learning Rate: 0.00778414
	LOSS [training: 2.142663054250632 | validation: 3.8557475951328843]
	TIME [epoch: 8.5 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8275726426190395		[learning rate: 0.0077559]
	Learning Rate: 0.00775589
	LOSS [training: 1.8275726426190395 | validation: 3.031614579563733]
	TIME [epoch: 8.51 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0272314178854542		[learning rate: 0.0077277]
	Learning Rate: 0.00772774
	LOSS [training: 2.0272314178854542 | validation: 3.0034122671826284]
	TIME [epoch: 8.5 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.871059355846553		[learning rate: 0.0076997]
	Learning Rate: 0.0076997
	LOSS [training: 1.871059355846553 | validation: 2.937288089648497]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.906332061784995		[learning rate: 0.0076718]
	Learning Rate: 0.00767176
	LOSS [training: 1.906332061784995 | validation: 3.017391944188388]
	TIME [epoch: 8.49 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9439703278673122		[learning rate: 0.0076439]
	Learning Rate: 0.00764391
	LOSS [training: 1.9439703278673122 | validation: 3.0224960092371194]
	TIME [epoch: 8.51 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.14546673654666		[learning rate: 0.0076162]
	Learning Rate: 0.00761617
	LOSS [training: 2.14546673654666 | validation: 4.091021722932183]
	TIME [epoch: 8.49 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9812416109194946		[learning rate: 0.0075885]
	Learning Rate: 0.00758853
	LOSS [training: 1.9812416109194946 | validation: 3.0576524385739923]
	TIME [epoch: 8.49 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7399514405589638		[learning rate: 0.007561]
	Learning Rate: 0.00756099
	LOSS [training: 1.7399514405589638 | validation: 2.9805576944625405]
	TIME [epoch: 8.48 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7715553415866077		[learning rate: 0.0075336]
	Learning Rate: 0.00753356
	LOSS [training: 1.7715553415866077 | validation: 3.0471653205162976]
	TIME [epoch: 8.51 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8869714164090503		[learning rate: 0.0075062]
	Learning Rate: 0.00750622
	LOSS [training: 1.8869714164090503 | validation: 3.135982421549701]
	TIME [epoch: 8.49 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5686871069153085		[learning rate: 0.007479]
	Learning Rate: 0.00747897
	LOSS [training: 1.5686871069153085 | validation: 3.3027803916007414]
	TIME [epoch: 8.5 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6328493005132105		[learning rate: 0.0074518]
	Learning Rate: 0.00745183
	LOSS [training: 1.6328493005132105 | validation: 2.908762430858282]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5961362214185109		[learning rate: 0.0074248]
	Learning Rate: 0.00742479
	LOSS [training: 1.5961362214185109 | validation: 3.109166341396875]
	TIME [epoch: 8.52 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.025760657210392		[learning rate: 0.0073978]
	Learning Rate: 0.00739784
	LOSS [training: 2.025760657210392 | validation: 3.1267583488443136]
	TIME [epoch: 8.49 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8792111073609745		[learning rate: 0.007371]
	Learning Rate: 0.007371
	LOSS [training: 1.8792111073609745 | validation: 2.9275738727112954]
	TIME [epoch: 8.48 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6085885837140304		[learning rate: 0.0073442]
	Learning Rate: 0.00734425
	LOSS [training: 1.6085885837140304 | validation: 2.88358994855043]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5993257572633013		[learning rate: 0.0073176]
	Learning Rate: 0.0073176
	LOSS [training: 1.5993257572633013 | validation: 2.929805919881553]
	TIME [epoch: 8.51 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0099739249761486		[learning rate: 0.007291]
	Learning Rate: 0.00729104
	LOSS [training: 2.0099739249761486 | validation: 3.3176454656534244]
	TIME [epoch: 8.48 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7804824003732365		[learning rate: 0.0072646]
	Learning Rate: 0.00726458
	LOSS [training: 1.7804824003732365 | validation: 3.2719814792201825]
	TIME [epoch: 8.48 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7116117933629678		[learning rate: 0.0072382]
	Learning Rate: 0.00723822
	LOSS [training: 1.7116117933629678 | validation: 3.127909752035974]
	TIME [epoch: 8.48 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7122473020915856		[learning rate: 0.0072119]
	Learning Rate: 0.00721195
	LOSS [training: 1.7122473020915856 | validation: 3.385510035131669]
	TIME [epoch: 8.5 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7534596326840823		[learning rate: 0.0071858]
	Learning Rate: 0.00718578
	LOSS [training: 1.7534596326840823 | validation: 2.975361865505924]
	TIME [epoch: 8.48 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.977074628882076		[learning rate: 0.0071597]
	Learning Rate: 0.0071597
	LOSS [training: 1.977074628882076 | validation: 3.0718613214779165]
	TIME [epoch: 8.48 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9338820973439692		[learning rate: 0.0071337]
	Learning Rate: 0.00713371
	LOSS [training: 1.9338820973439692 | validation: 2.931755338183277]
	TIME [epoch: 8.49 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.616735493526822		[learning rate: 0.0071078]
	Learning Rate: 0.00710783
	LOSS [training: 1.616735493526822 | validation: 2.915679825546553]
	TIME [epoch: 8.5 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7527874586162446		[learning rate: 0.007082]
	Learning Rate: 0.00708203
	LOSS [training: 1.7527874586162446 | validation: 2.8516393528762967]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5367058515441738		[learning rate: 0.0070563]
	Learning Rate: 0.00705633
	LOSS [training: 1.5367058515441738 | validation: 2.999826735419531]
	TIME [epoch: 8.48 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.497441343392268		[learning rate: 0.0070307]
	Learning Rate: 0.00703072
	LOSS [training: 1.497441343392268 | validation: 2.9942520337752283]
	TIME [epoch: 8.49 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5018346861952756		[learning rate: 0.0070052]
	Learning Rate: 0.00700521
	LOSS [training: 1.5018346861952756 | validation: 2.9501462468152178]
	TIME [epoch: 8.49 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7249299612979105		[learning rate: 0.0069798]
	Learning Rate: 0.00697979
	LOSS [training: 1.7249299612979105 | validation: 2.9958061929693196]
	TIME [epoch: 8.47 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5336837448479979		[learning rate: 0.0069545]
	Learning Rate: 0.00695446
	LOSS [training: 1.5336837448479979 | validation: 3.1168191396448517]
	TIME [epoch: 8.48 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4607711218295507		[learning rate: 0.0069292]
	Learning Rate: 0.00692922
	LOSS [training: 1.4607711218295507 | validation: 2.9056882446803645]
	TIME [epoch: 8.49 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.579104283666945		[learning rate: 0.0069041]
	Learning Rate: 0.00690407
	LOSS [training: 1.579104283666945 | validation: 2.934110120602063]
	TIME [epoch: 8.49 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7408206461604194		[learning rate: 0.006879]
	Learning Rate: 0.00687902
	LOSS [training: 1.7408206461604194 | validation: 3.086647298281357]
	TIME [epoch: 8.48 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5134606725997977		[learning rate: 0.0068541]
	Learning Rate: 0.00685405
	LOSS [training: 1.5134606725997977 | validation: 3.138217017647839]
	TIME [epoch: 8.48 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8735017262025706		[learning rate: 0.0068292]
	Learning Rate: 0.00682918
	LOSS [training: 1.8735017262025706 | validation: 2.968322848730539]
	TIME [epoch: 8.5 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5314296562120935		[learning rate: 0.0068044]
	Learning Rate: 0.00680439
	LOSS [training: 1.5314296562120935 | validation: 3.0279624923835464]
	TIME [epoch: 8.48 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4700101371811396		[learning rate: 0.0067797]
	Learning Rate: 0.0067797
	LOSS [training: 1.4700101371811396 | validation: 3.7712068180729426]
	TIME [epoch: 8.48 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6622279944748688		[learning rate: 0.0067551]
	Learning Rate: 0.0067551
	LOSS [training: 1.6622279944748688 | validation: 2.870200968907861]
	TIME [epoch: 8.48 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7680862982531564		[learning rate: 0.0067306]
	Learning Rate: 0.00673058
	LOSS [training: 1.7680862982531564 | validation: 3.1465878656530544]
	TIME [epoch: 8.5 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5672774513223582		[learning rate: 0.0067062]
	Learning Rate: 0.00670616
	LOSS [training: 1.5672774513223582 | validation: 2.965002646098725]
	TIME [epoch: 8.48 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7443710049457237		[learning rate: 0.0066818]
	Learning Rate: 0.00668182
	LOSS [training: 1.7443710049457237 | validation: 2.9735989720198974]
	TIME [epoch: 8.48 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6806601876768876		[learning rate: 0.0066576]
	Learning Rate: 0.00665757
	LOSS [training: 1.6806601876768876 | validation: 2.828845061480414]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6434469936413987		[learning rate: 0.0066334]
	Learning Rate: 0.00663341
	LOSS [training: 1.6434469936413987 | validation: 3.191591285171111]
	TIME [epoch: 8.51 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.576007895032356		[learning rate: 0.0066093]
	Learning Rate: 0.00660934
	LOSS [training: 1.576007895032356 | validation: 3.047358611262755]
	TIME [epoch: 8.49 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6917057000454492		[learning rate: 0.0065854]
	Learning Rate: 0.00658535
	LOSS [training: 1.6917057000454492 | validation: 3.7980042239681393]
	TIME [epoch: 8.48 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8941037367746003		[learning rate: 0.0065615]
	Learning Rate: 0.00656145
	LOSS [training: 1.8941037367746003 | validation: 3.5803523371095665]
	TIME [epoch: 8.48 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7784134946155912		[learning rate: 0.0065376]
	Learning Rate: 0.00653764
	LOSS [training: 1.7784134946155912 | validation: 3.599608744588618]
	TIME [epoch: 8.52 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.615462188619249		[learning rate: 0.0065139]
	Learning Rate: 0.00651392
	LOSS [training: 1.615462188619249 | validation: 4.076118515394324]
	TIME [epoch: 8.48 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.787519741714417		[learning rate: 0.0064903]
	Learning Rate: 0.00649028
	LOSS [training: 1.787519741714417 | validation: 2.9554453706221437]
	TIME [epoch: 8.48 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5110007933116738		[learning rate: 0.0064667]
	Learning Rate: 0.00646672
	LOSS [training: 1.5110007933116738 | validation: 3.175669139852852]
	TIME [epoch: 8.48 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5915539074891747		[learning rate: 0.0064433]
	Learning Rate: 0.00644325
	LOSS [training: 1.5915539074891747 | validation: 3.2058841252507992]
	TIME [epoch: 8.51 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5110770343246795		[learning rate: 0.0064199]
	Learning Rate: 0.00641987
	LOSS [training: 1.5110770343246795 | validation: 3.1988183374144854]
	TIME [epoch: 8.49 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.544717902173481		[learning rate: 0.0063966]
	Learning Rate: 0.00639657
	LOSS [training: 1.544717902173481 | validation: 2.995316761272583]
	TIME [epoch: 8.49 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3811255576546813		[learning rate: 0.0063734]
	Learning Rate: 0.00637336
	LOSS [training: 1.3811255576546813 | validation: 2.8269664101021768]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.610374865559529		[learning rate: 0.0063502]
	Learning Rate: 0.00635023
	LOSS [training: 1.610374865559529 | validation: 2.844497583511094]
	TIME [epoch: 8.53 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.381489290756256		[learning rate: 0.0063272]
	Learning Rate: 0.00632718
	LOSS [training: 1.381489290756256 | validation: 2.8229055586614074]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3539173890988645		[learning rate: 0.0063042]
	Learning Rate: 0.00630422
	LOSS [training: 1.3539173890988645 | validation: 3.760273221913904]
	TIME [epoch: 8.49 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5308454994680976		[learning rate: 0.0062813]
	Learning Rate: 0.00628134
	LOSS [training: 1.5308454994680976 | validation: 3.9484924917441386]
	TIME [epoch: 8.51 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8721712131354011		[learning rate: 0.0062585]
	Learning Rate: 0.00625855
	LOSS [training: 1.8721712131354011 | validation: 3.2433325028156212]
	TIME [epoch: 8.51 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.443901044687028		[learning rate: 0.0062358]
	Learning Rate: 0.00623584
	LOSS [training: 1.443901044687028 | validation: 2.9121358310315504]
	TIME [epoch: 8.5 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6236642392752354		[learning rate: 0.0062132]
	Learning Rate: 0.00621321
	LOSS [training: 1.6236642392752354 | validation: 3.4927491297221382]
	TIME [epoch: 8.5 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5427569852267635		[learning rate: 0.0061907]
	Learning Rate: 0.00619066
	LOSS [training: 1.5427569852267635 | validation: 3.405712650569968]
	TIME [epoch: 8.5 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5074089284010292		[learning rate: 0.0061682]
	Learning Rate: 0.00616819
	LOSS [training: 1.5074089284010292 | validation: 2.842956766634841]
	TIME [epoch: 8.51 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4334502872739354		[learning rate: 0.0061458]
	Learning Rate: 0.00614581
	LOSS [training: 1.4334502872739354 | validation: 2.8464235307953443]
	TIME [epoch: 8.49 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3618076085652935		[learning rate: 0.0061235]
	Learning Rate: 0.0061235
	LOSS [training: 1.3618076085652935 | validation: 3.122686757161115]
	TIME [epoch: 8.48 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5923599789261673		[learning rate: 0.0061013]
	Learning Rate: 0.00610128
	LOSS [training: 1.5923599789261673 | validation: 2.98948368052825]
	TIME [epoch: 8.5 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4580395341399766		[learning rate: 0.0060791]
	Learning Rate: 0.00607914
	LOSS [training: 1.4580395341399766 | validation: 2.8890201170862166]
	TIME [epoch: 8.5 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6675725154711114		[learning rate: 0.0060571]
	Learning Rate: 0.00605708
	LOSS [training: 1.6675725154711114 | validation: 3.042620795969997]
	TIME [epoch: 8.5 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5899614429468563		[learning rate: 0.0060351]
	Learning Rate: 0.0060351
	LOSS [training: 1.5899614429468563 | validation: 2.7665890987795443]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5920776463475674		[learning rate: 0.0060132]
	Learning Rate: 0.00601319
	LOSS [training: 1.5920776463475674 | validation: 2.8321294044955523]
	TIME [epoch: 8.52 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7482134820680906		[learning rate: 0.0059914]
	Learning Rate: 0.00599137
	LOSS [training: 1.7482134820680906 | validation: 2.8863886312757767]
	TIME [epoch: 8.5 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.555783251967988		[learning rate: 0.0059696]
	Learning Rate: 0.00596963
	LOSS [training: 1.555783251967988 | validation: 3.02874783950192]
	TIME [epoch: 8.5 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6755373456683196		[learning rate: 0.005948]
	Learning Rate: 0.00594797
	LOSS [training: 1.6755373456683196 | validation: 2.8444001446544736]
	TIME [epoch: 8.49 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6490996202606325		[learning rate: 0.0059264]
	Learning Rate: 0.00592638
	LOSS [training: 1.6490996202606325 | validation: 3.0015033804775975]
	TIME [epoch: 8.52 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5805159092208445		[learning rate: 0.0059049]
	Learning Rate: 0.00590487
	LOSS [training: 1.5805159092208445 | validation: 2.868075136954615]
	TIME [epoch: 8.5 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6476689906220254		[learning rate: 0.0058834]
	Learning Rate: 0.00588344
	LOSS [training: 1.6476689906220254 | validation: 2.7991657247707358]
	TIME [epoch: 8.5 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4096053759689084		[learning rate: 0.0058621]
	Learning Rate: 0.00586209
	LOSS [training: 1.4096053759689084 | validation: 2.8782345051884106]
	TIME [epoch: 8.5 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.325226673534298		[learning rate: 0.0058408]
	Learning Rate: 0.00584082
	LOSS [training: 1.325226673534298 | validation: 2.8589170490986]
	TIME [epoch: 8.52 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5163659726964942		[learning rate: 0.0058196]
	Learning Rate: 0.00581962
	LOSS [training: 1.5163659726964942 | validation: 3.1625027099766623]
	TIME [epoch: 8.5 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3957270726508786		[learning rate: 0.0057985]
	Learning Rate: 0.0057985
	LOSS [training: 1.3957270726508786 | validation: 2.8475303821352]
	TIME [epoch: 8.5 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3402091409144798		[learning rate: 0.0057775]
	Learning Rate: 0.00577746
	LOSS [training: 1.3402091409144798 | validation: 3.110753053667654]
	TIME [epoch: 8.5 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3433365672539432		[learning rate: 0.0057565]
	Learning Rate: 0.00575649
	LOSS [training: 1.3433365672539432 | validation: 3.643743092121344]
	TIME [epoch: 8.53 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4746969044708396		[learning rate: 0.0057356]
	Learning Rate: 0.0057356
	LOSS [training: 1.4746969044708396 | validation: 2.8590451120861404]
	TIME [epoch: 8.5 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3447139380584936		[learning rate: 0.0057148]
	Learning Rate: 0.00571479
	LOSS [training: 1.3447139380584936 | validation: 2.8170154940637655]
	TIME [epoch: 8.5 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3548757797004924		[learning rate: 0.005694]
	Learning Rate: 0.00569405
	LOSS [training: 1.3548757797004924 | validation: 3.1424135246194975]
	TIME [epoch: 8.5 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.384206289895634		[learning rate: 0.0056734]
	Learning Rate: 0.00567338
	LOSS [training: 1.384206289895634 | validation: 3.2173512763641456]
	TIME [epoch: 8.53 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4535547256890926		[learning rate: 0.0056528]
	Learning Rate: 0.00565279
	LOSS [training: 1.4535547256890926 | validation: 3.398468458047498]
	TIME [epoch: 8.5 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.405609793809682		[learning rate: 0.0056323]
	Learning Rate: 0.00563228
	LOSS [training: 1.405609793809682 | validation: 2.84838082173293]
	TIME [epoch: 8.5 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4725653505848353		[learning rate: 0.0056118]
	Learning Rate: 0.00561184
	LOSS [training: 1.4725653505848353 | validation: 3.4409131070476535]
	TIME [epoch: 8.5 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6493192189099077		[learning rate: 0.0055915]
	Learning Rate: 0.00559147
	LOSS [training: 1.6493192189099077 | validation: 2.966114647603781]
	TIME [epoch: 8.53 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5665095260541575		[learning rate: 0.0055712]
	Learning Rate: 0.00557118
	LOSS [training: 1.5665095260541575 | validation: 3.014179403443876]
	TIME [epoch: 8.5 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.542308501691065		[learning rate: 0.005551]
	Learning Rate: 0.00555096
	LOSS [training: 1.542308501691065 | validation: 2.9714123876848504]
	TIME [epoch: 8.5 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2822357498957808		[learning rate: 0.0055308]
	Learning Rate: 0.00553082
	LOSS [training: 1.2822357498957808 | validation: 2.9088815750210864]
	TIME [epoch: 8.51 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3923827993923998		[learning rate: 0.0055107]
	Learning Rate: 0.00551075
	LOSS [training: 1.3923827993923998 | validation: 2.9927002645275023]
	TIME [epoch: 8.52 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3770569548881384		[learning rate: 0.0054907]
	Learning Rate: 0.00549075
	LOSS [training: 1.3770569548881384 | validation: 2.8381150598435214]
	TIME [epoch: 8.5 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3155759727711303		[learning rate: 0.0054708]
	Learning Rate: 0.00547082
	LOSS [training: 1.3155759727711303 | validation: 2.950844837067063]
	TIME [epoch: 8.5 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3868555961022748		[learning rate: 0.005451]
	Learning Rate: 0.00545097
	LOSS [training: 1.3868555961022748 | validation: 2.857914261759661]
	TIME [epoch: 8.51 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3003027487056145		[learning rate: 0.0054312]
	Learning Rate: 0.00543119
	LOSS [training: 1.3003027487056145 | validation: 2.9936244712698183]
	TIME [epoch: 8.51 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3189244845761345		[learning rate: 0.0054115]
	Learning Rate: 0.00541148
	LOSS [training: 1.3189244845761345 | validation: 2.796861688389485]
	TIME [epoch: 8.5 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6753005535334953		[learning rate: 0.0053918]
	Learning Rate: 0.00539184
	LOSS [training: 1.6753005535334953 | validation: 2.7930838633328197]
	TIME [epoch: 8.5 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.343111107406238		[learning rate: 0.0053723]
	Learning Rate: 0.00537227
	LOSS [training: 1.343111107406238 | validation: 2.8724503954045826]
	TIME [epoch: 8.51 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3468509037803993		[learning rate: 0.0053528]
	Learning Rate: 0.00535277
	LOSS [training: 1.3468509037803993 | validation: 3.0342249270612243]
	TIME [epoch: 8.51 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2701514856617417		[learning rate: 0.0053333]
	Learning Rate: 0.00533335
	LOSS [training: 1.2701514856617417 | validation: 2.9769766240598194]
	TIME [epoch: 8.5 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3293184077022673		[learning rate: 0.005314]
	Learning Rate: 0.00531399
	LOSS [training: 1.3293184077022673 | validation: 2.7704961913388555]
	TIME [epoch: 8.5 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3531878468627077		[learning rate: 0.0052947]
	Learning Rate: 0.00529471
	LOSS [training: 1.3531878468627077 | validation: 2.790980620230675]
	TIME [epoch: 8.51 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4271335691323719		[learning rate: 0.0052755]
	Learning Rate: 0.00527549
	LOSS [training: 1.4271335691323719 | validation: 3.0118853619518635]
	TIME [epoch: 8.51 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3421811599544178		[learning rate: 0.0052563]
	Learning Rate: 0.00525635
	LOSS [training: 1.3421811599544178 | validation: 2.739245581685482]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4140557908699762		[learning rate: 0.0052373]
	Learning Rate: 0.00523727
	LOSS [training: 1.4140557908699762 | validation: 2.7430329781600635]
	TIME [epoch: 8.5 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.400941438001651		[learning rate: 0.0052183]
	Learning Rate: 0.00521827
	LOSS [training: 1.400941438001651 | validation: 3.038276303565163]
	TIME [epoch: 8.51 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4984294418090056		[learning rate: 0.0051993]
	Learning Rate: 0.00519933
	LOSS [training: 1.4984294418090056 | validation: 2.739581501786786]
	TIME [epoch: 8.5 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3820101514145928		[learning rate: 0.0051805]
	Learning Rate: 0.00518046
	LOSS [training: 1.3820101514145928 | validation: 2.777655785205733]
	TIME [epoch: 8.49 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2744168692344584		[learning rate: 0.0051617]
	Learning Rate: 0.00516166
	LOSS [training: 1.2744168692344584 | validation: 3.128779572289294]
	TIME [epoch: 8.49 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.320512569362325		[learning rate: 0.0051429]
	Learning Rate: 0.00514293
	LOSS [training: 1.320512569362325 | validation: 2.928083180656237]
	TIME [epoch: 8.51 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.269857692350533		[learning rate: 0.0051243]
	Learning Rate: 0.00512426
	LOSS [training: 1.269857692350533 | validation: 2.8565579654461235]
	TIME [epoch: 8.5 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.458529063440127		[learning rate: 0.0051057]
	Learning Rate: 0.00510567
	LOSS [training: 1.458529063440127 | validation: 2.8940923685291233]
	TIME [epoch: 8.49 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4045852142252806		[learning rate: 0.0050871]
	Learning Rate: 0.00508714
	LOSS [training: 1.4045852142252806 | validation: 2.7872070871621597]
	TIME [epoch: 8.49 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.327748843395034		[learning rate: 0.0050687]
	Learning Rate: 0.00506868
	LOSS [training: 1.327748843395034 | validation: 2.9416724346448686]
	TIME [epoch: 8.51 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4497122908455415		[learning rate: 0.0050503]
	Learning Rate: 0.00505028
	LOSS [training: 1.4497122908455415 | validation: 2.7433015791024595]
	TIME [epoch: 8.5 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.258011329426552		[learning rate: 0.005032]
	Learning Rate: 0.00503196
	LOSS [training: 1.258011329426552 | validation: 2.8154394409935763]
	TIME [epoch: 8.49 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2762195299652528		[learning rate: 0.0050137]
	Learning Rate: 0.00501369
	LOSS [training: 1.2762195299652528 | validation: 2.8345469697202006]
	TIME [epoch: 8.49 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4720054399897387		[learning rate: 0.0049955]
	Learning Rate: 0.0049955
	LOSS [training: 1.4720054399897387 | validation: 3.4495513094597987]
	TIME [epoch: 8.52 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.455369613509435		[learning rate: 0.0049774]
	Learning Rate: 0.00497737
	LOSS [training: 1.455369613509435 | validation: 3.0939996801794436]
	TIME [epoch: 8.49 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3426568702878359		[learning rate: 0.0049593]
	Learning Rate: 0.00495931
	LOSS [training: 1.3426568702878359 | validation: 2.907851828520652]
	TIME [epoch: 8.49 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3424062995623627		[learning rate: 0.0049413]
	Learning Rate: 0.00494131
	LOSS [training: 1.3424062995623627 | validation: 2.794832065859085]
	TIME [epoch: 8.5 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2724812132134986		[learning rate: 0.0049234]
	Learning Rate: 0.00492338
	LOSS [training: 1.2724812132134986 | validation: 3.3090849573259558]
	TIME [epoch: 8.52 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3578435583316413		[learning rate: 0.0049055]
	Learning Rate: 0.00490551
	LOSS [training: 1.3578435583316413 | validation: 2.9226011229282634]
	TIME [epoch: 8.49 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3359428129061386		[learning rate: 0.0048877]
	Learning Rate: 0.00488771
	LOSS [training: 1.3359428129061386 | validation: 2.7439048854680914]
	TIME [epoch: 8.49 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.283285491258398		[learning rate: 0.00487]
	Learning Rate: 0.00486997
	LOSS [training: 1.283285491258398 | validation: 2.874477054843929]
	TIME [epoch: 8.49 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.300049268868941		[learning rate: 0.0048523]
	Learning Rate: 0.0048523
	LOSS [training: 1.300049268868941 | validation: 2.7907387886742088]
	TIME [epoch: 8.51 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2721374884787677		[learning rate: 0.0048347]
	Learning Rate: 0.00483469
	LOSS [training: 1.2721374884787677 | validation: 3.1907342353212154]
	TIME [epoch: 8.49 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4134150985166882		[learning rate: 0.0048171]
	Learning Rate: 0.00481714
	LOSS [training: 1.4134150985166882 | validation: 2.8413418483289865]
	TIME [epoch: 8.49 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.246913844836799		[learning rate: 0.0047997]
	Learning Rate: 0.00479966
	LOSS [training: 1.246913844836799 | validation: 2.754237122549248]
	TIME [epoch: 8.5 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3691742393491948		[learning rate: 0.0047822]
	Learning Rate: 0.00478224
	LOSS [training: 1.3691742393491948 | validation: 2.893162632556531]
	TIME [epoch: 8.5 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3080793112447093		[learning rate: 0.0047649]
	Learning Rate: 0.00476489
	LOSS [training: 1.3080793112447093 | validation: 2.7498333691307]
	TIME [epoch: 8.49 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4198932548654533		[learning rate: 0.0047476]
	Learning Rate: 0.00474759
	LOSS [training: 1.4198932548654533 | validation: 2.7365889854272893]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4485365654419973		[learning rate: 0.0047304]
	Learning Rate: 0.00473037
	LOSS [training: 1.4485365654419973 | validation: 2.7957954136327285]
	TIME [epoch: 8.5 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3880028912874491		[learning rate: 0.0047132]
	Learning Rate: 0.0047132
	LOSS [training: 1.3880028912874491 | validation: 2.771561085947885]
	TIME [epoch: 8.5 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2685775741621517		[learning rate: 0.0046961]
	Learning Rate: 0.00469609
	LOSS [training: 1.2685775741621517 | validation: 2.730699933223791]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3359467138956922		[learning rate: 0.0046791]
	Learning Rate: 0.00467905
	LOSS [training: 1.3359467138956922 | validation: 3.3132500609754048]
	TIME [epoch: 8.49 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.474979338427342		[learning rate: 0.0046621]
	Learning Rate: 0.00466207
	LOSS [training: 1.474979338427342 | validation: 2.754585372149723]
	TIME [epoch: 8.5 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3372327405648492		[learning rate: 0.0046452]
	Learning Rate: 0.00464515
	LOSS [training: 1.3372327405648492 | validation: 2.9161000885898223]
	TIME [epoch: 8.5 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4057759769385616		[learning rate: 0.0046283]
	Learning Rate: 0.0046283
	LOSS [training: 1.4057759769385616 | validation: 3.12569289869303]
	TIME [epoch: 8.49 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3783255429677845		[learning rate: 0.0046115]
	Learning Rate: 0.0046115
	LOSS [training: 1.3783255429677845 | validation: 2.931429476868634]
	TIME [epoch: 8.49 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.433592631399597		[learning rate: 0.0045948]
	Learning Rate: 0.00459476
	LOSS [training: 1.433592631399597 | validation: 2.9700075876762346]
	TIME [epoch: 8.51 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3772882346476842		[learning rate: 0.0045781]
	Learning Rate: 0.00457809
	LOSS [training: 1.3772882346476842 | validation: 2.8193396137794156]
	TIME [epoch: 8.49 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4155841681318933		[learning rate: 0.0045615]
	Learning Rate: 0.00456148
	LOSS [training: 1.4155841681318933 | validation: 2.810792773932192]
	TIME [epoch: 8.48 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.317189520899597		[learning rate: 0.0045449]
	Learning Rate: 0.00454492
	LOSS [training: 1.317189520899597 | validation: 2.892740234827964]
	TIME [epoch: 8.49 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.301799142529974		[learning rate: 0.0045284]
	Learning Rate: 0.00452843
	LOSS [training: 1.301799142529974 | validation: 2.8420451798529847]
	TIME [epoch: 8.51 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3302333941780708		[learning rate: 0.004512]
	Learning Rate: 0.00451199
	LOSS [training: 1.3302333941780708 | validation: 2.834093230725381]
	TIME [epoch: 8.5 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2368850612334674		[learning rate: 0.0044956]
	Learning Rate: 0.00449562
	LOSS [training: 1.2368850612334674 | validation: 2.7670407513349486]
	TIME [epoch: 8.48 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2782520791950631		[learning rate: 0.0044793]
	Learning Rate: 0.0044793
	LOSS [training: 1.2782520791950631 | validation: 2.9222129578466935]
	TIME [epoch: 8.49 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3065914773826024		[learning rate: 0.004463]
	Learning Rate: 0.00446305
	LOSS [training: 1.3065914773826024 | validation: 2.7641238723669077]
	TIME [epoch: 8.51 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3499735503864099		[learning rate: 0.0044469]
	Learning Rate: 0.00444685
	LOSS [training: 1.3499735503864099 | validation: 2.790754162588263]
	TIME [epoch: 8.5 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3028798155842911		[learning rate: 0.0044307]
	Learning Rate: 0.00443071
	LOSS [training: 1.3028798155842911 | validation: 3.5577590975084106]
	TIME [epoch: 8.49 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3997582813229414		[learning rate: 0.0044146]
	Learning Rate: 0.00441463
	LOSS [training: 1.3997582813229414 | validation: 2.8617473425131035]
	TIME [epoch: 8.49 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2972155306611215		[learning rate: 0.0043986]
	Learning Rate: 0.00439861
	LOSS [training: 1.2972155306611215 | validation: 2.773841428040919]
	TIME [epoch: 8.51 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2322399876893682		[learning rate: 0.0043827]
	Learning Rate: 0.00438265
	LOSS [training: 1.2322399876893682 | validation: 2.7202196638407807]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2828437003548296		[learning rate: 0.0043667]
	Learning Rate: 0.00436675
	LOSS [training: 1.2828437003548296 | validation: 2.7813718568241845]
	TIME [epoch: 8.49 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1698306121843889		[learning rate: 0.0043509]
	Learning Rate: 0.0043509
	LOSS [training: 1.1698306121843889 | validation: 2.9636584961180468]
	TIME [epoch: 8.51 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3252735114250265		[learning rate: 0.0043351]
	Learning Rate: 0.00433511
	LOSS [training: 1.3252735114250265 | validation: 3.0233440825427436]
	TIME [epoch: 8.51 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6202398214586882		[learning rate: 0.0043194]
	Learning Rate: 0.00431938
	LOSS [training: 1.6202398214586882 | validation: 2.7516994293373425]
	TIME [epoch: 8.48 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3026177301447166		[learning rate: 0.0043037]
	Learning Rate: 0.0043037
	LOSS [training: 1.3026177301447166 | validation: 2.8224376452802757]
	TIME [epoch: 8.48 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.411224179442664		[learning rate: 0.0042881]
	Learning Rate: 0.00428808
	LOSS [training: 1.411224179442664 | validation: 3.0453428585866344]
	TIME [epoch: 8.48 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2972772651324909		[learning rate: 0.0042725]
	Learning Rate: 0.00427252
	LOSS [training: 1.2972772651324909 | validation: 2.954849349652611]
	TIME [epoch: 8.51 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.274024582635261		[learning rate: 0.004257]
	Learning Rate: 0.00425702
	LOSS [training: 1.274024582635261 | validation: 2.797236288697203]
	TIME [epoch: 8.48 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2276883489637416		[learning rate: 0.0042416]
	Learning Rate: 0.00424157
	LOSS [training: 1.2276883489637416 | validation: 2.800752253022402]
	TIME [epoch: 8.48 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2289568641807316		[learning rate: 0.0042262]
	Learning Rate: 0.00422617
	LOSS [training: 1.2289568641807316 | validation: 2.7697727551130953]
	TIME [epoch: 8.49 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.347093679123033		[learning rate: 0.0042108]
	Learning Rate: 0.00421084
	LOSS [training: 1.347093679123033 | validation: 2.820804179988584]
	TIME [epoch: 8.5 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2192097851878119		[learning rate: 0.0041956]
	Learning Rate: 0.00419556
	LOSS [training: 1.2192097851878119 | validation: 2.9639574545535834]
	TIME [epoch: 8.48 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2802228333924648		[learning rate: 0.0041803]
	Learning Rate: 0.00418033
	LOSS [training: 1.2802228333924648 | validation: 2.805627053225266]
	TIME [epoch: 8.48 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3860312153781886		[learning rate: 0.0041652]
	Learning Rate: 0.00416516
	LOSS [training: 1.3860312153781886 | validation: 3.7150681151758453]
	TIME [epoch: 8.48 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5360510710842488		[learning rate: 0.00415]
	Learning Rate: 0.00415004
	LOSS [training: 1.5360510710842488 | validation: 2.7784093338744396]
	TIME [epoch: 8.5 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2542973493111922		[learning rate: 0.004135]
	Learning Rate: 0.00413498
	LOSS [training: 1.2542973493111922 | validation: 2.961115110543677]
	TIME [epoch: 8.48 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2573783875043607		[learning rate: 0.00412]
	Learning Rate: 0.00411998
	LOSS [training: 1.2573783875043607 | validation: 2.737044428517388]
	TIME [epoch: 8.48 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3389202064827144		[learning rate: 0.004105]
	Learning Rate: 0.00410502
	LOSS [training: 1.3389202064827144 | validation: 3.515900411440464]
	TIME [epoch: 8.5 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.34660450844729		[learning rate: 0.0040901]
	Learning Rate: 0.00409013
	LOSS [training: 1.34660450844729 | validation: 2.713899689131889]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.274203565252281		[learning rate: 0.0040753]
	Learning Rate: 0.00407528
	LOSS [training: 1.274203565252281 | validation: 2.7190751466295824]
	TIME [epoch: 8.49 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.232383240965378		[learning rate: 0.0040605]
	Learning Rate: 0.00406049
	LOSS [training: 1.232383240965378 | validation: 2.7115178364247137]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.282956872734949		[learning rate: 0.0040458]
	Learning Rate: 0.00404576
	LOSS [training: 1.282956872734949 | validation: 2.898146088662488]
	TIME [epoch: 8.51 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3443394086172136		[learning rate: 0.0040311]
	Learning Rate: 0.00403108
	LOSS [training: 1.3443394086172136 | validation: 2.727297705116113]
	TIME [epoch: 8.48 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.284713215621771		[learning rate: 0.0040164]
	Learning Rate: 0.00401645
	LOSS [training: 1.284713215621771 | validation: 2.7400383418514322]
	TIME [epoch: 8.48 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1674931372314792		[learning rate: 0.0040019]
	Learning Rate: 0.00400187
	LOSS [training: 1.1674931372314792 | validation: 2.963939168789716]
	TIME [epoch: 8.48 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2313034140369963		[learning rate: 0.0039873]
	Learning Rate: 0.00398735
	LOSS [training: 1.2313034140369963 | validation: 2.8251098362039224]
	TIME [epoch: 8.5 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3082916177417214		[learning rate: 0.0039729]
	Learning Rate: 0.00397288
	LOSS [training: 1.3082916177417214 | validation: 2.8352824082197747]
	TIME [epoch: 8.49 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.242362911241324		[learning rate: 0.0039585]
	Learning Rate: 0.00395846
	LOSS [training: 1.242362911241324 | validation: 2.704739172332917]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2635257184997102		[learning rate: 0.0039441]
	Learning Rate: 0.00394409
	LOSS [training: 1.2635257184997102 | validation: 2.6819400701105125]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2186938784858126		[learning rate: 0.0039298]
	Learning Rate: 0.00392978
	LOSS [training: 1.2186938784858126 | validation: 2.7288252658034926]
	TIME [epoch: 8.5 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2233470024976711		[learning rate: 0.0039155]
	Learning Rate: 0.00391552
	LOSS [training: 1.2233470024976711 | validation: 2.7404927506032273]
	TIME [epoch: 8.48 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2252413795201553		[learning rate: 0.0039013]
	Learning Rate: 0.00390131
	LOSS [training: 1.2252413795201553 | validation: 2.8527466896620783]
	TIME [epoch: 8.47 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2751498013144142		[learning rate: 0.0038872]
	Learning Rate: 0.00388715
	LOSS [training: 1.2751498013144142 | validation: 2.8315453973283633]
	TIME [epoch: 8.48 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3574137773205972		[learning rate: 0.003873]
	Learning Rate: 0.00387305
	LOSS [training: 1.3574137773205972 | validation: 2.8044774311066254]
	TIME [epoch: 8.5 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.303761782791124		[learning rate: 0.003859]
	Learning Rate: 0.00385899
	LOSS [training: 1.303761782791124 | validation: 2.793118712742711]
	TIME [epoch: 8.48 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2724484245950123		[learning rate: 0.003845]
	Learning Rate: 0.00384499
	LOSS [training: 1.2724484245950123 | validation: 2.8644317298965802]
	TIME [epoch: 8.47 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2350200522349488		[learning rate: 0.003831]
	Learning Rate: 0.00383103
	LOSS [training: 1.2350200522349488 | validation: 2.7088911535763325]
	TIME [epoch: 8.48 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1623618570517267		[learning rate: 0.0038171]
	Learning Rate: 0.00381713
	LOSS [training: 1.1623618570517267 | validation: 2.7838609546302813]
	TIME [epoch: 8.5 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.210292706182357		[learning rate: 0.0038033]
	Learning Rate: 0.00380328
	LOSS [training: 1.210292706182357 | validation: 2.961459508830866]
	TIME [epoch: 8.48 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2434109062933696		[learning rate: 0.0037895]
	Learning Rate: 0.00378947
	LOSS [training: 1.2434109062933696 | validation: 3.2568559185774397]
	TIME [epoch: 8.47 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2708447743932365		[learning rate: 0.0037757]
	Learning Rate: 0.00377572
	LOSS [training: 1.2708447743932365 | validation: 2.72096425256872]
	TIME [epoch: 8.48 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2656624137458719		[learning rate: 0.003762]
	Learning Rate: 0.00376202
	LOSS [training: 1.2656624137458719 | validation: 2.7097737111982236]
	TIME [epoch: 8.5 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1982552782540266		[learning rate: 0.0037484]
	Learning Rate: 0.00374837
	LOSS [training: 1.1982552782540266 | validation: 2.7329870128093745]
	TIME [epoch: 8.48 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1762595877925106		[learning rate: 0.0037348]
	Learning Rate: 0.00373476
	LOSS [training: 1.1762595877925106 | validation: 2.7106986908798656]
	TIME [epoch: 8.47 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1704764536946168		[learning rate: 0.0037212]
	Learning Rate: 0.00372121
	LOSS [training: 1.1704764536946168 | validation: 2.738716129317]
	TIME [epoch: 8.49 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2458744480563488		[learning rate: 0.0037077]
	Learning Rate: 0.00370771
	LOSS [training: 1.2458744480563488 | validation: 2.7076380111497307]
	TIME [epoch: 8.5 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2757807142579376		[learning rate: 0.0036943]
	Learning Rate: 0.00369425
	LOSS [training: 1.2757807142579376 | validation: 2.8185429057405944]
	TIME [epoch: 8.48 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3081580915736857		[learning rate: 0.0036808]
	Learning Rate: 0.00368084
	LOSS [training: 1.3081580915736857 | validation: 2.789025563439072]
	TIME [epoch: 8.47 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2453910179122034		[learning rate: 0.0036675]
	Learning Rate: 0.00366749
	LOSS [training: 1.2453910179122034 | validation: 2.8569923242243185]
	TIME [epoch: 8.48 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1897804335798248		[learning rate: 0.0036542]
	Learning Rate: 0.00365418
	LOSS [training: 1.1897804335798248 | validation: 2.6804739032758214]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_377.pth
	Model improved!!!
EPOCH 378/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2031602633978784		[learning rate: 0.0036409]
	Learning Rate: 0.00364092
	LOSS [training: 1.2031602633978784 | validation: 2.6671610517878457]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2027298384268392		[learning rate: 0.0036277]
	Learning Rate: 0.0036277
	LOSS [training: 1.2027298384268392 | validation: 2.8124378812959803]
	TIME [epoch: 8.48 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3806379397574087		[learning rate: 0.0036145]
	Learning Rate: 0.00361454
	LOSS [training: 1.3806379397574087 | validation: 2.715301019618664]
	TIME [epoch: 8.49 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1284142798799224		[learning rate: 0.0036014]
	Learning Rate: 0.00360142
	LOSS [training: 1.1284142798799224 | validation: 2.9532833867702664]
	TIME [epoch: 8.5 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2193388323529395		[learning rate: 0.0035883]
	Learning Rate: 0.00358835
	LOSS [training: 1.2193388323529395 | validation: 3.0433898873675336]
	TIME [epoch: 8.48 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3078716631603267		[learning rate: 0.0035753]
	Learning Rate: 0.00357533
	LOSS [training: 1.3078716631603267 | validation: 2.7152214484996526]
	TIME [epoch: 8.48 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2163132597334136		[learning rate: 0.0035624]
	Learning Rate: 0.00356235
	LOSS [training: 1.2163132597334136 | validation: 2.908001981647552]
	TIME [epoch: 8.5 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.21119821378746		[learning rate: 0.0035494]
	Learning Rate: 0.00354942
	LOSS [training: 1.21119821378746 | validation: 2.915016613293081]
	TIME [epoch: 8.48 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.240715771906069		[learning rate: 0.0035365]
	Learning Rate: 0.00353654
	LOSS [training: 1.240715771906069 | validation: 2.7579456415738077]
	TIME [epoch: 8.47 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2955129242178047		[learning rate: 0.0035237]
	Learning Rate: 0.00352371
	LOSS [training: 1.2955129242178047 | validation: 2.6790543503032]
	TIME [epoch: 8.47 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2332581364714457		[learning rate: 0.0035109]
	Learning Rate: 0.00351092
	LOSS [training: 1.2332581364714457 | validation: 2.994355757153122]
	TIME [epoch: 8.5 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3006214332108426		[learning rate: 0.0034982]
	Learning Rate: 0.00349818
	LOSS [training: 1.3006214332108426 | validation: 2.8682865755620535]
	TIME [epoch: 8.48 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1439902887525712		[learning rate: 0.0034855]
	Learning Rate: 0.00348548
	LOSS [training: 1.1439902887525712 | validation: 2.6647065934934333]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1749089721121433		[learning rate: 0.0034728]
	Learning Rate: 0.00347284
	LOSS [training: 1.1749089721121433 | validation: 2.6741014037414668]
	TIME [epoch: 8.48 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.234085168228894		[learning rate: 0.0034602]
	Learning Rate: 0.00346023
	LOSS [training: 1.234085168228894 | validation: 2.683071127747731]
	TIME [epoch: 8.52 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.16244546420867		[learning rate: 0.0034477]
	Learning Rate: 0.00344767
	LOSS [training: 1.16244546420867 | validation: 2.928027112411063]
	TIME [epoch: 8.48 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2143751614661753		[learning rate: 0.0034352]
	Learning Rate: 0.00343516
	LOSS [training: 1.2143751614661753 | validation: 2.8572727438564067]
	TIME [epoch: 8.48 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.223107711169811		[learning rate: 0.0034227]
	Learning Rate: 0.0034227
	LOSS [training: 1.223107711169811 | validation: 2.8658259776217276]
	TIME [epoch: 8.47 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.224172912941068		[learning rate: 0.0034103]
	Learning Rate: 0.00341028
	LOSS [training: 1.224172912941068 | validation: 2.8461907531112383]
	TIME [epoch: 8.5 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.151585862999445		[learning rate: 0.0033979]
	Learning Rate: 0.0033979
	LOSS [training: 1.151585862999445 | validation: 2.6993615525853385]
	TIME [epoch: 8.48 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2066098890570076		[learning rate: 0.0033856]
	Learning Rate: 0.00338557
	LOSS [training: 1.2066098890570076 | validation: 3.601929851296343]
	TIME [epoch: 8.48 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2624143250548059		[learning rate: 0.0033733]
	Learning Rate: 0.00337328
	LOSS [training: 1.2624143250548059 | validation: 2.8377061407578754]
	TIME [epoch: 8.48 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1988519168787033		[learning rate: 0.003361]
	Learning Rate: 0.00336104
	LOSS [training: 1.1988519168787033 | validation: 2.89975471189709]
	TIME [epoch: 8.5 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2765448957680328		[learning rate: 0.0033488]
	Learning Rate: 0.00334884
	LOSS [training: 1.2765448957680328 | validation: 2.757014542133667]
	TIME [epoch: 8.48 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1915345917938733		[learning rate: 0.0033367]
	Learning Rate: 0.00333669
	LOSS [training: 1.1915345917938733 | validation: 3.0676598005162043]
	TIME [epoch: 8.48 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.292948441669304		[learning rate: 0.0033246]
	Learning Rate: 0.00332458
	LOSS [training: 1.292948441669304 | validation: 2.744514988908862]
	TIME [epoch: 8.48 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.116894390051457		[learning rate: 0.0033125]
	Learning Rate: 0.00331252
	LOSS [training: 1.116894390051457 | validation: 2.807410584983037]
	TIME [epoch: 8.5 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.166941709659787		[learning rate: 0.0033005]
	Learning Rate: 0.00330049
	LOSS [training: 1.166941709659787 | validation: 2.67495326050758]
	TIME [epoch: 8.48 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2426666219201303		[learning rate: 0.0032885]
	Learning Rate: 0.00328852
	LOSS [training: 1.2426666219201303 | validation: 2.7142373966479143]
	TIME [epoch: 8.48 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2385959864129803		[learning rate: 0.0032766]
	Learning Rate: 0.00327658
	LOSS [training: 1.2385959864129803 | validation: 2.6795923246065634]
	TIME [epoch: 8.48 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1861087238244945		[learning rate: 0.0032647]
	Learning Rate: 0.00326469
	LOSS [training: 1.1861087238244945 | validation: 3.0088936586171284]
	TIME [epoch: 8.51 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1779481647795054		[learning rate: 0.0032528]
	Learning Rate: 0.00325284
	LOSS [training: 1.1779481647795054 | validation: 2.747098881724204]
	TIME [epoch: 8.48 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1662568523864192		[learning rate: 0.003241]
	Learning Rate: 0.00324104
	LOSS [training: 1.1662568523864192 | validation: 2.713562451837469]
	TIME [epoch: 8.48 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1636674593727399		[learning rate: 0.0032293]
	Learning Rate: 0.00322928
	LOSS [training: 1.1636674593727399 | validation: 2.952738786111639]
	TIME [epoch: 8.49 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.23567581171898		[learning rate: 0.0032176]
	Learning Rate: 0.00321756
	LOSS [training: 1.23567581171898 | validation: 2.9996712233569687]
	TIME [epoch: 8.5 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2187081211540138		[learning rate: 0.0032059]
	Learning Rate: 0.00320588
	LOSS [training: 1.2187081211540138 | validation: 2.7316666428498078]
	TIME [epoch: 8.48 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1907805588287588		[learning rate: 0.0031942]
	Learning Rate: 0.00319425
	LOSS [training: 1.1907805588287588 | validation: 2.8972863121534544]
	TIME [epoch: 8.48 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1439098199177358		[learning rate: 0.0031827]
	Learning Rate: 0.00318265
	LOSS [training: 1.1439098199177358 | validation: 2.866389471847984]
	TIME [epoch: 8.49 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2018496233377784		[learning rate: 0.0031711]
	Learning Rate: 0.0031711
	LOSS [training: 1.2018496233377784 | validation: 3.4222579145794514]
	TIME [epoch: 8.48 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.266004151058437		[learning rate: 0.0031596]
	Learning Rate: 0.0031596
	LOSS [training: 1.266004151058437 | validation: 2.808161932807873]
	TIME [epoch: 8.47 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.293385774531771		[learning rate: 0.0031481]
	Learning Rate: 0.00314813
	LOSS [training: 1.293385774531771 | validation: 2.8562607178110735]
	TIME [epoch: 8.47 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1990156020883387		[learning rate: 0.0031367]
	Learning Rate: 0.00313671
	LOSS [training: 1.1990156020883387 | validation: 2.660482985025955]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_419.pth
	Model improved!!!
EPOCH 420/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1098265552315536		[learning rate: 0.0031253]
	Learning Rate: 0.00312532
	LOSS [training: 1.1098265552315536 | validation: 2.756398068003098]
	TIME [epoch: 8.5 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1459209612941124		[learning rate: 0.003114]
	Learning Rate: 0.00311398
	LOSS [training: 1.1459209612941124 | validation: 2.6908939960593]
	TIME [epoch: 8.48 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.134506044501915		[learning rate: 0.0031027]
	Learning Rate: 0.00310268
	LOSS [training: 1.134506044501915 | validation: 2.6578627971366044]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1805275886895092		[learning rate: 0.0030914]
	Learning Rate: 0.00309142
	LOSS [training: 1.1805275886895092 | validation: 2.7586695134017107]
	TIME [epoch: 8.52 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1653727045051683		[learning rate: 0.0030802]
	Learning Rate: 0.0030802
	LOSS [training: 1.1653727045051683 | validation: 2.666399887209887]
	TIME [epoch: 8.5 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1853856657750628		[learning rate: 0.003069]
	Learning Rate: 0.00306902
	LOSS [training: 1.1853856657750628 | validation: 2.8050587106337534]
	TIME [epoch: 8.49 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1702193608873546		[learning rate: 0.0030579]
	Learning Rate: 0.00305788
	LOSS [training: 1.1702193608873546 | validation: 2.8138569976784282]
	TIME [epoch: 8.49 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.148205612270959		[learning rate: 0.0030468]
	Learning Rate: 0.00304679
	LOSS [training: 1.148205612270959 | validation: 2.7226166081292766]
	TIME [epoch: 8.51 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2015404033861083		[learning rate: 0.0030357]
	Learning Rate: 0.00303573
	LOSS [training: 1.2015404033861083 | validation: 2.9664766550140196]
	TIME [epoch: 8.49 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2334261762751748		[learning rate: 0.0030247]
	Learning Rate: 0.00302471
	LOSS [training: 1.2334261762751748 | validation: 2.7684845730718335]
	TIME [epoch: 8.49 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.121045797583921		[learning rate: 0.0030137]
	Learning Rate: 0.00301374
	LOSS [training: 1.121045797583921 | validation: 2.7686905095108996]
	TIME [epoch: 8.49 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.165882640862165		[learning rate: 0.0030028]
	Learning Rate: 0.0030028
	LOSS [training: 1.165882640862165 | validation: 3.362583122540298]
	TIME [epoch: 8.51 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2392179351968975		[learning rate: 0.0029919]
	Learning Rate: 0.0029919
	LOSS [training: 1.2392179351968975 | validation: 2.688357622814525]
	TIME [epoch: 8.49 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1293994954589368		[learning rate: 0.002981]
	Learning Rate: 0.00298104
	LOSS [training: 1.1293994954589368 | validation: 2.7771705985521327]
	TIME [epoch: 8.49 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.176324393502418		[learning rate: 0.0029702]
	Learning Rate: 0.00297023
	LOSS [training: 1.176324393502418 | validation: 2.7383397142728434]
	TIME [epoch: 8.49 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1835200105531858		[learning rate: 0.0029594]
	Learning Rate: 0.00295945
	LOSS [training: 1.1835200105531858 | validation: 2.7226186635230865]
	TIME [epoch: 8.52 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1046546590502797		[learning rate: 0.0029487]
	Learning Rate: 0.00294871
	LOSS [training: 1.1046546590502797 | validation: 2.704291911131922]
	TIME [epoch: 8.49 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1494613220804755		[learning rate: 0.002938]
	Learning Rate: 0.00293801
	LOSS [training: 1.1494613220804755 | validation: 2.707012365810927]
	TIME [epoch: 8.49 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.170242793547884		[learning rate: 0.0029273]
	Learning Rate: 0.00292734
	LOSS [training: 1.170242793547884 | validation: 2.732039317808691]
	TIME [epoch: 8.49 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1832961391301553		[learning rate: 0.0029167]
	Learning Rate: 0.00291672
	LOSS [training: 1.1832961391301553 | validation: 2.6723310831680998]
	TIME [epoch: 8.51 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1524148072919846		[learning rate: 0.0029061]
	Learning Rate: 0.00290614
	LOSS [training: 1.1524148072919846 | validation: 2.6994723530380975]
	TIME [epoch: 8.49 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1368203424703551		[learning rate: 0.0028956]
	Learning Rate: 0.00289559
	LOSS [training: 1.1368203424703551 | validation: 2.662929433252552]
	TIME [epoch: 8.49 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.174907612296617		[learning rate: 0.0028851]
	Learning Rate: 0.00288508
	LOSS [training: 1.174907612296617 | validation: 2.6864624159646793]
	TIME [epoch: 8.49 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.157784739712536		[learning rate: 0.0028746]
	Learning Rate: 0.00287461
	LOSS [training: 1.157784739712536 | validation: 2.813283743534529]
	TIME [epoch: 8.51 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.125680616802296		[learning rate: 0.0028642]
	Learning Rate: 0.00286418
	LOSS [training: 1.125680616802296 | validation: 2.747045218593396]
	TIME [epoch: 8.49 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1369492194279578		[learning rate: 0.0028538]
	Learning Rate: 0.00285378
	LOSS [training: 1.1369492194279578 | validation: 2.706929646591325]
	TIME [epoch: 8.49 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1615795676381833		[learning rate: 0.0028434]
	Learning Rate: 0.00284343
	LOSS [training: 1.1615795676381833 | validation: 2.685247247964628]
	TIME [epoch: 8.49 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.108256873909069		[learning rate: 0.0028331]
	Learning Rate: 0.00283311
	LOSS [training: 1.108256873909069 | validation: 2.6483711690959253]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_447.pth
	Model improved!!!
EPOCH 448/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1494887058994732		[learning rate: 0.0028228]
	Learning Rate: 0.00282283
	LOSS [training: 1.1494887058994732 | validation: 2.767958861405229]
	TIME [epoch: 8.49 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.13792230010093		[learning rate: 0.0028126]
	Learning Rate: 0.00281258
	LOSS [training: 1.13792230010093 | validation: 2.6609591818681926]
	TIME [epoch: 8.48 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1523068074742209		[learning rate: 0.0028024]
	Learning Rate: 0.00280238
	LOSS [training: 1.1523068074742209 | validation: 2.6802278446267986]
	TIME [epoch: 8.51 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.228849949104743		[learning rate: 0.0027922]
	Learning Rate: 0.00279221
	LOSS [training: 1.228849949104743 | validation: 2.6865959707628324]
	TIME [epoch: 8.5 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1234864006650063		[learning rate: 0.0027821]
	Learning Rate: 0.00278207
	LOSS [training: 1.1234864006650063 | validation: 2.7783296031344547]
	TIME [epoch: 8.49 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1554233913087575		[learning rate: 0.002772]
	Learning Rate: 0.00277198
	LOSS [training: 1.1554233913087575 | validation: 2.8047841090900807]
	TIME [epoch: 8.48 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1417798870393394		[learning rate: 0.0027619]
	Learning Rate: 0.00276192
	LOSS [training: 1.1417798870393394 | validation: 2.6831323968817515]
	TIME [epoch: 8.5 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.148854035255906		[learning rate: 0.0027519]
	Learning Rate: 0.00275189
	LOSS [training: 1.148854035255906 | validation: 2.7377231127891686]
	TIME [epoch: 8.5 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.132378829177845		[learning rate: 0.0027419]
	Learning Rate: 0.00274191
	LOSS [training: 1.132378829177845 | validation: 2.7078617378694214]
	TIME [epoch: 8.49 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1748078607806727		[learning rate: 0.002732]
	Learning Rate: 0.00273196
	LOSS [training: 1.1748078607806727 | validation: 2.8850241866091095]
	TIME [epoch: 8.49 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2056130537577474		[learning rate: 0.002722]
	Learning Rate: 0.00272204
	LOSS [training: 1.2056130537577474 | validation: 2.689236455161975]
	TIME [epoch: 8.5 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1519340722365616		[learning rate: 0.0027122]
	Learning Rate: 0.00271216
	LOSS [training: 1.1519340722365616 | validation: 3.0987484001974748]
	TIME [epoch: 8.49 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2733378445768273		[learning rate: 0.0027023]
	Learning Rate: 0.00270232
	LOSS [training: 1.2733378445768273 | validation: 2.86626801310506]
	TIME [epoch: 8.49 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.118535674006021		[learning rate: 0.0026925]
	Learning Rate: 0.00269251
	LOSS [training: 1.118535674006021 | validation: 2.8034646039674844]
	TIME [epoch: 8.49 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.138996008314337		[learning rate: 0.0026827]
	Learning Rate: 0.00268274
	LOSS [training: 1.138996008314337 | validation: 2.8232275718611897]
	TIME [epoch: 8.51 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.157706638276274		[learning rate: 0.002673]
	Learning Rate: 0.00267301
	LOSS [training: 1.157706638276274 | validation: 2.751298046506534]
	TIME [epoch: 8.49 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1222915136017169		[learning rate: 0.0026633]
	Learning Rate: 0.00266331
	LOSS [training: 1.1222915136017169 | validation: 3.022524201066208]
	TIME [epoch: 8.49 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1664951111474255		[learning rate: 0.0026536]
	Learning Rate: 0.00265364
	LOSS [training: 1.1664951111474255 | validation: 2.6953331132011655]
	TIME [epoch: 8.49 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.188361982972145		[learning rate: 0.002644]
	Learning Rate: 0.00264401
	LOSS [training: 1.188361982972145 | validation: 2.823302737910539]
	TIME [epoch: 8.51 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.189005946808377		[learning rate: 0.0026344]
	Learning Rate: 0.00263441
	LOSS [training: 1.189005946808377 | validation: 2.6581616807775514]
	TIME [epoch: 8.49 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0782805148774919		[learning rate: 0.0026249]
	Learning Rate: 0.00262485
	LOSS [training: 1.0782805148774919 | validation: 2.653518648788832]
	TIME [epoch: 8.49 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1732434750816385		[learning rate: 0.0026153]
	Learning Rate: 0.00261533
	LOSS [training: 1.1732434750816385 | validation: 2.675831812754862]
	TIME [epoch: 8.49 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1291776507626232		[learning rate: 0.0026058]
	Learning Rate: 0.00260584
	LOSS [training: 1.1291776507626232 | validation: 2.6545428501323785]
	TIME [epoch: 8.51 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1556934915900396		[learning rate: 0.0025964]
	Learning Rate: 0.00259638
	LOSS [training: 1.1556934915900396 | validation: 2.675235070574955]
	TIME [epoch: 8.49 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.147816900977605		[learning rate: 0.002587]
	Learning Rate: 0.00258696
	LOSS [training: 1.147816900977605 | validation: 2.7090997047773984]
	TIME [epoch: 8.48 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1102734218170915		[learning rate: 0.0025776]
	Learning Rate: 0.00257757
	LOSS [training: 1.1102734218170915 | validation: 2.666952394489057]
	TIME [epoch: 8.49 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.102176112594484		[learning rate: 0.0025682]
	Learning Rate: 0.00256822
	LOSS [training: 1.102176112594484 | validation: 2.639393925535455]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_474.pth
	Model improved!!!
EPOCH 475/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1768369879050815		[learning rate: 0.0025589]
	Learning Rate: 0.0025589
	LOSS [training: 1.1768369879050815 | validation: 2.713230641480081]
	TIME [epoch: 8.48 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.080906513796831		[learning rate: 0.0025496]
	Learning Rate: 0.00254961
	LOSS [training: 1.080906513796831 | validation: 2.720615157789221]
	TIME [epoch: 8.48 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1136138109087068		[learning rate: 0.0025404]
	Learning Rate: 0.00254036
	LOSS [training: 1.1136138109087068 | validation: 2.671866829271884]
	TIME [epoch: 8.48 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0807254293726172		[learning rate: 0.0025311]
	Learning Rate: 0.00253114
	LOSS [training: 1.0807254293726172 | validation: 2.6736751109806334]
	TIME [epoch: 8.5 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1102951227578064		[learning rate: 0.002522]
	Learning Rate: 0.00252195
	LOSS [training: 1.1102951227578064 | validation: 2.85365256790148]
	TIME [epoch: 8.48 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.105528691175137		[learning rate: 0.0025128]
	Learning Rate: 0.0025128
	LOSS [training: 1.105528691175137 | validation: 2.6806638629243413]
	TIME [epoch: 8.48 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2213135131527904		[learning rate: 0.0025037]
	Learning Rate: 0.00250368
	LOSS [training: 1.2213135131527904 | validation: 2.6511765683776463]
	TIME [epoch: 8.48 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2372146972828388		[learning rate: 0.0024946]
	Learning Rate: 0.00249459
	LOSS [training: 1.2372146972828388 | validation: 2.6318307306924416]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_482.pth
	Model improved!!!
EPOCH 483/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2222928533770943		[learning rate: 0.0024855]
	Learning Rate: 0.00248554
	LOSS [training: 1.2222928533770943 | validation: 3.191819084865709]
	TIME [epoch: 8.49 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2069749427159275		[learning rate: 0.0024765]
	Learning Rate: 0.00247652
	LOSS [training: 1.2069749427159275 | validation: 2.6717536302705964]
	TIME [epoch: 8.49 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.088833897636383		[learning rate: 0.0024675]
	Learning Rate: 0.00246753
	LOSS [training: 1.088833897636383 | validation: 2.723144522968516]
	TIME [epoch: 8.49 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1495945718744411		[learning rate: 0.0024586]
	Learning Rate: 0.00245858
	LOSS [training: 1.1495945718744411 | validation: 2.690265765923963]
	TIME [epoch: 8.5 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.077153814609304		[learning rate: 0.0024497]
	Learning Rate: 0.00244966
	LOSS [training: 1.077153814609304 | validation: 2.7466263483845523]
	TIME [epoch: 8.49 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1368386875574088		[learning rate: 0.0024408]
	Learning Rate: 0.00244077
	LOSS [training: 1.1368386875574088 | validation: 2.691185244390597]
	TIME [epoch: 8.48 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0893023851618011		[learning rate: 0.0024319]
	Learning Rate: 0.00243191
	LOSS [training: 1.0893023851618011 | validation: 2.7739111498670694]
	TIME [epoch: 8.5 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1230375565483193		[learning rate: 0.0024231]
	Learning Rate: 0.00242308
	LOSS [training: 1.1230375565483193 | validation: 2.706221795736913]
	TIME [epoch: 8.5 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1275322969245327		[learning rate: 0.0024143]
	Learning Rate: 0.00241429
	LOSS [training: 1.1275322969245327 | validation: 2.707413431671413]
	TIME [epoch: 8.49 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1200102103905258		[learning rate: 0.0024055]
	Learning Rate: 0.00240553
	LOSS [training: 1.1200102103905258 | validation: 2.923834791086546]
	TIME [epoch: 8.49 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0878965495796398		[learning rate: 0.0023968]
	Learning Rate: 0.0023968
	LOSS [training: 1.0878965495796398 | validation: 2.841923771483458]
	TIME [epoch: 8.5 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1175607923975		[learning rate: 0.0023881]
	Learning Rate: 0.0023881
	LOSS [training: 1.1175607923975 | validation: 2.969229574639148]
	TIME [epoch: 8.49 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1399927319173018		[learning rate: 0.0023794]
	Learning Rate: 0.00237943
	LOSS [training: 1.1399927319173018 | validation: 2.6747092664778354]
	TIME [epoch: 8.48 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0925956898291502		[learning rate: 0.0023708]
	Learning Rate: 0.0023708
	LOSS [training: 1.0925956898291502 | validation: 2.6426335734306834]
	TIME [epoch: 8.48 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1369402536855486		[learning rate: 0.0023622]
	Learning Rate: 0.0023622
	LOSS [training: 1.1369402536855486 | validation: 2.616200585068514]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.174119123239681		[learning rate: 0.0023536]
	Learning Rate: 0.00235362
	LOSS [training: 1.174119123239681 | validation: 2.774131702426031]
	TIME [epoch: 8.49 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.108068589221519		[learning rate: 0.0023451]
	Learning Rate: 0.00234508
	LOSS [training: 1.108068589221519 | validation: 2.6574219384773925]
	TIME [epoch: 8.48 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1288842164639064		[learning rate: 0.0023366]
	Learning Rate: 0.00233657
	LOSS [training: 1.1288842164639064 | validation: 2.7184886182022794]
	TIME [epoch: 8.48 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.081259189861952		[learning rate: 0.0023281]
	Learning Rate: 0.00232809
	LOSS [training: 1.081259189861952 | validation: 2.76726614771791]
	TIME [epoch: 8.51 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.174777870799125		[learning rate: 0.0023196]
	Learning Rate: 0.00231964
	LOSS [training: 1.174777870799125 | validation: 2.656903053025779]
	TIME [epoch: 8.49 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1099620959868797		[learning rate: 0.0023112]
	Learning Rate: 0.00231122
	LOSS [training: 1.1099620959868797 | validation: 2.7198276349067583]
	TIME [epoch: 8.48 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.099966774723856		[learning rate: 0.0023028]
	Learning Rate: 0.00230284
	LOSS [training: 1.099966774723856 | validation: 2.7390848088435327]
	TIME [epoch: 8.49 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0653731642541668		[learning rate: 0.0022945]
	Learning Rate: 0.00229448
	LOSS [training: 1.0653731642541668 | validation: 2.7140300269639948]
	TIME [epoch: 8.51 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.179530179344118		[learning rate: 0.0022862]
	Learning Rate: 0.00228615
	LOSS [training: 1.179530179344118 | validation: 2.7509096665136763]
	TIME [epoch: 8.48 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0815884435815788		[learning rate: 0.0022779]
	Learning Rate: 0.00227786
	LOSS [training: 1.0815884435815788 | validation: 2.6997585517508105]
	TIME [epoch: 8.48 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1021331366203428		[learning rate: 0.0022696]
	Learning Rate: 0.00226959
	LOSS [training: 1.1021331366203428 | validation: 2.6484116931640638]
	TIME [epoch: 8.48 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0992419756153504		[learning rate: 0.0022614]
	Learning Rate: 0.00226135
	LOSS [training: 1.0992419756153504 | validation: 2.7886187350606764]
	TIME [epoch: 8.51 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0767321275071344		[learning rate: 0.0022531]
	Learning Rate: 0.00225315
	LOSS [training: 1.0767321275071344 | validation: 2.960419857473663]
	TIME [epoch: 8.48 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1077432253841342		[learning rate: 0.002245]
	Learning Rate: 0.00224497
	LOSS [training: 1.1077432253841342 | validation: 2.7302453171045027]
	TIME [epoch: 8.49 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1304242549131003		[learning rate: 0.0022368]
	Learning Rate: 0.00223682
	LOSS [training: 1.1304242549131003 | validation: 2.682005575120585]
	TIME [epoch: 8.48 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1335225426969053		[learning rate: 0.0022287]
	Learning Rate: 0.00222871
	LOSS [training: 1.1335225426969053 | validation: 2.640859339494636]
	TIME [epoch: 8.51 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1083114835987087		[learning rate: 0.0022206]
	Learning Rate: 0.00222062
	LOSS [training: 1.1083114835987087 | validation: 2.7989972709771473]
	TIME [epoch: 8.48 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.199211714048978		[learning rate: 0.0022126]
	Learning Rate: 0.00221256
	LOSS [training: 1.199211714048978 | validation: 2.7545639035556677]
	TIME [epoch: 8.49 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.121424585718462		[learning rate: 0.0022045]
	Learning Rate: 0.00220453
	LOSS [training: 1.121424585718462 | validation: 2.812322090478726]
	TIME [epoch: 8.49 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.097730897436977		[learning rate: 0.0021965]
	Learning Rate: 0.00219653
	LOSS [training: 1.097730897436977 | validation: 2.6861881930430416]
	TIME [epoch: 8.51 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1397563837496318		[learning rate: 0.0021886]
	Learning Rate: 0.00218856
	LOSS [training: 1.1397563837496318 | validation: 2.6668314774044912]
	TIME [epoch: 8.48 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1142459170395336		[learning rate: 0.0021806]
	Learning Rate: 0.00218061
	LOSS [training: 1.1142459170395336 | validation: 2.690218742336978]
	TIME [epoch: 8.49 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0906977859214515		[learning rate: 0.0021727]
	Learning Rate: 0.0021727
	LOSS [training: 1.0906977859214515 | validation: 2.69260429380107]
	TIME [epoch: 8.49 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0630481211213187		[learning rate: 0.0021648]
	Learning Rate: 0.00216482
	LOSS [training: 1.0630481211213187 | validation: 2.7545494651872082]
	TIME [epoch: 8.51 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1667810787621284		[learning rate: 0.002157]
	Learning Rate: 0.00215696
	LOSS [training: 1.1667810787621284 | validation: 2.7498680910596067]
	TIME [epoch: 8.49 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1090513469122372		[learning rate: 0.0021491]
	Learning Rate: 0.00214913
	LOSS [training: 1.1090513469122372 | validation: 2.768130931973064]
	TIME [epoch: 8.48 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1082901731104857		[learning rate: 0.0021413]
	Learning Rate: 0.00214133
	LOSS [training: 1.1082901731104857 | validation: 2.6494317181014417]
	TIME [epoch: 8.49 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0466457898961354		[learning rate: 0.0021336]
	Learning Rate: 0.00213356
	LOSS [training: 1.0466457898961354 | validation: 2.6705337034202334]
	TIME [epoch: 8.5 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0898789637490023		[learning rate: 0.0021258]
	Learning Rate: 0.00212582
	LOSS [training: 1.0898789637490023 | validation: 2.6296022233064873]
	TIME [epoch: 8.49 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1127787969855603		[learning rate: 0.0021181]
	Learning Rate: 0.0021181
	LOSS [training: 1.1127787969855603 | validation: 2.766197264500666]
	TIME [epoch: 8.49 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0594482718810327		[learning rate: 0.0021104]
	Learning Rate: 0.00211042
	LOSS [training: 1.0594482718810327 | validation: 2.7058541996675767]
	TIME [epoch: 8.5 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1030123415439885		[learning rate: 0.0021028]
	Learning Rate: 0.00210276
	LOSS [training: 1.1030123415439885 | validation: 2.7373937816944034]
	TIME [epoch: 8.5 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0799267601363394		[learning rate: 0.0020951]
	Learning Rate: 0.00209513
	LOSS [training: 1.0799267601363394 | validation: 2.6344394850655304]
	TIME [epoch: 8.48 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0728293765590853		[learning rate: 0.0020875]
	Learning Rate: 0.00208752
	LOSS [training: 1.0728293765590853 | validation: 2.6764568507580186]
	TIME [epoch: 8.48 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1052088726282467		[learning rate: 0.0020799]
	Learning Rate: 0.00207995
	LOSS [training: 1.1052088726282467 | validation: 2.70832097683145]
	TIME [epoch: 8.5 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0754518655522545		[learning rate: 0.0020724]
	Learning Rate: 0.0020724
	LOSS [training: 1.0754518655522545 | validation: 2.635251329227558]
	TIME [epoch: 8.49 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.08037044601522		[learning rate: 0.0020649]
	Learning Rate: 0.00206488
	LOSS [training: 1.08037044601522 | validation: 2.7102956139093113]
	TIME [epoch: 8.47 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0804244819092221		[learning rate: 0.0020574]
	Learning Rate: 0.00205739
	LOSS [training: 1.0804244819092221 | validation: 2.841900114641528]
	TIME [epoch: 8.49 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1221980084320407		[learning rate: 0.0020499]
	Learning Rate: 0.00204992
	LOSS [training: 1.1221980084320407 | validation: 2.766749946765757]
	TIME [epoch: 8.5 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0763532459224163		[learning rate: 0.0020425]
	Learning Rate: 0.00204248
	LOSS [training: 1.0763532459224163 | validation: 2.761730101075588]
	TIME [epoch: 8.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0703769384585842		[learning rate: 0.0020351]
	Learning Rate: 0.00203507
	LOSS [training: 1.0703769384585842 | validation: 2.698725765493111]
	TIME [epoch: 8.49 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1082551832576397		[learning rate: 0.0020277]
	Learning Rate: 0.00202768
	LOSS [training: 1.1082551832576397 | validation: 2.675797260113535]
	TIME [epoch: 8.5 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1148491748483205		[learning rate: 0.0020203]
	Learning Rate: 0.00202032
	LOSS [training: 1.1148491748483205 | validation: 2.714429132641139]
	TIME [epoch: 8.51 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1187043472529257		[learning rate: 0.002013]
	Learning Rate: 0.00201299
	LOSS [training: 1.1187043472529257 | validation: 2.644373695389348]
	TIME [epoch: 8.49 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.118984829501119		[learning rate: 0.0020057]
	Learning Rate: 0.00200569
	LOSS [training: 1.118984829501119 | validation: 2.696654333488974]
	TIME [epoch: 8.49 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0855023027139907		[learning rate: 0.0019984]
	Learning Rate: 0.00199841
	LOSS [training: 1.0855023027139907 | validation: 2.6799364588870396]
	TIME [epoch: 8.48 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.110293486611143		[learning rate: 0.0019912]
	Learning Rate: 0.00199116
	LOSS [training: 1.110293486611143 | validation: 2.647029303118647]
	TIME [epoch: 8.51 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1019543245830234		[learning rate: 0.0019839]
	Learning Rate: 0.00198393
	LOSS [training: 1.1019543245830234 | validation: 2.674091779482783]
	TIME [epoch: 8.5 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.09154966781899		[learning rate: 0.0019767]
	Learning Rate: 0.00197673
	LOSS [training: 1.09154966781899 | validation: 2.710382221211379]
	TIME [epoch: 8.48 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.108861113518424		[learning rate: 0.0019696]
	Learning Rate: 0.00196956
	LOSS [training: 1.108861113518424 | validation: 2.740809349824219]
	TIME [epoch: 8.48 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0920043533742374		[learning rate: 0.0019624]
	Learning Rate: 0.00196241
	LOSS [training: 1.0920043533742374 | validation: 2.7044660607406255]
	TIME [epoch: 8.52 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0955820184211793		[learning rate: 0.0019553]
	Learning Rate: 0.00195529
	LOSS [training: 1.0955820184211793 | validation: 2.7098143888617923]
	TIME [epoch: 8.5 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0725782436583196		[learning rate: 0.0019482]
	Learning Rate: 0.00194819
	LOSS [training: 1.0725782436583196 | validation: 2.689484069469562]
	TIME [epoch: 8.48 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0544250664340968		[learning rate: 0.0019411]
	Learning Rate: 0.00194112
	LOSS [training: 1.0544250664340968 | validation: 2.6344604069297834]
	TIME [epoch: 8.49 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0862068814497676		[learning rate: 0.0019341]
	Learning Rate: 0.00193408
	LOSS [training: 1.0862068814497676 | validation: 2.9383818474354633]
	TIME [epoch: 8.51 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1068569396337817		[learning rate: 0.0019271]
	Learning Rate: 0.00192706
	LOSS [training: 1.1068569396337817 | validation: 2.682758372193752]
	TIME [epoch: 8.5 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.097802665762924		[learning rate: 0.0019201]
	Learning Rate: 0.00192006
	LOSS [training: 1.097802665762924 | validation: 2.7246889156924152]
	TIME [epoch: 8.49 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0579169222165006		[learning rate: 0.0019131]
	Learning Rate: 0.0019131
	LOSS [training: 1.0579169222165006 | validation: 2.6649511670411137]
	TIME [epoch: 8.49 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1246715552425584		[learning rate: 0.0019062]
	Learning Rate: 0.00190615
	LOSS [training: 1.1246715552425584 | validation: 2.743520398319919]
	TIME [epoch: 8.52 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.115834349414523		[learning rate: 0.0018992]
	Learning Rate: 0.00189924
	LOSS [training: 1.115834349414523 | validation: 2.7417843650383498]
	TIME [epoch: 8.5 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1173035533853644		[learning rate: 0.0018923]
	Learning Rate: 0.00189234
	LOSS [training: 1.1173035533853644 | validation: 2.640419746910399]
	TIME [epoch: 8.49 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.071750933904439		[learning rate: 0.0018855]
	Learning Rate: 0.00188548
	LOSS [training: 1.071750933904439 | validation: 2.6480743183901945]
	TIME [epoch: 8.48 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0683791253435306		[learning rate: 0.0018786]
	Learning Rate: 0.00187863
	LOSS [training: 1.0683791253435306 | validation: 2.641332124887662]
	TIME [epoch: 8.51 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0687065972936978		[learning rate: 0.0018718]
	Learning Rate: 0.00187182
	LOSS [training: 1.0687065972936978 | validation: 2.765920633641026]
	TIME [epoch: 8.5 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0970777854549443		[learning rate: 0.001865]
	Learning Rate: 0.00186502
	LOSS [training: 1.0970777854549443 | validation: 2.753326146103924]
	TIME [epoch: 8.49 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0658102603488429		[learning rate: 0.0018583]
	Learning Rate: 0.00185825
	LOSS [training: 1.0658102603488429 | validation: 2.6496329126234137]
	TIME [epoch: 8.49 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0741830663586724		[learning rate: 0.0018515]
	Learning Rate: 0.00185151
	LOSS [training: 1.0741830663586724 | validation: 2.65248386835649]
	TIME [epoch: 8.51 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0925855328823373		[learning rate: 0.0018448]
	Learning Rate: 0.00184479
	LOSS [training: 1.0925855328823373 | validation: 2.6806642273629904]
	TIME [epoch: 8.49 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0822225084531796		[learning rate: 0.0018381]
	Learning Rate: 0.0018381
	LOSS [training: 1.0822225084531796 | validation: 3.0265992083578475]
	TIME [epoch: 8.49 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1188924826105737		[learning rate: 0.0018314]
	Learning Rate: 0.00183143
	LOSS [training: 1.1188924826105737 | validation: 2.7295428503741896]
	TIME [epoch: 8.49 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0880954271547483		[learning rate: 0.0018248]
	Learning Rate: 0.00182478
	LOSS [training: 1.0880954271547483 | validation: 2.862047545712331]
	TIME [epoch: 8.5 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1380138859166722		[learning rate: 0.0018182]
	Learning Rate: 0.00181816
	LOSS [training: 1.1380138859166722 | validation: 2.6776073943702277]
	TIME [epoch: 8.48 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0735933894760732		[learning rate: 0.0018116]
	Learning Rate: 0.00181156
	LOSS [training: 1.0735933894760732 | validation: 2.6897114371679334]
	TIME [epoch: 8.47 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1234884531850562		[learning rate: 0.001805]
	Learning Rate: 0.00180499
	LOSS [training: 1.1234884531850562 | validation: 2.7601282689870477]
	TIME [epoch: 8.5 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0539777949846219		[learning rate: 0.0017984]
	Learning Rate: 0.00179844
	LOSS [training: 1.0539777949846219 | validation: 2.6831715983675055]
	TIME [epoch: 8.5 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1015979947951116		[learning rate: 0.0017919]
	Learning Rate: 0.00179191
	LOSS [training: 1.1015979947951116 | validation: 2.681361648182762]
	TIME [epoch: 8.48 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0545823266663505		[learning rate: 0.0017854]
	Learning Rate: 0.00178541
	LOSS [training: 1.0545823266663505 | validation: 2.667994840533008]
	TIME [epoch: 8.48 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0662330128007331		[learning rate: 0.0017789]
	Learning Rate: 0.00177893
	LOSS [training: 1.0662330128007331 | validation: 2.694269739886563]
	TIME [epoch: 8.49 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1084626485084124		[learning rate: 0.0017725]
	Learning Rate: 0.00177247
	LOSS [training: 1.1084626485084124 | validation: 2.785547341095061]
	TIME [epoch: 8.49 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0682590706342239		[learning rate: 0.001766]
	Learning Rate: 0.00176604
	LOSS [training: 1.0682590706342239 | validation: 2.6737067722395027]
	TIME [epoch: 8.48 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0762840409953465		[learning rate: 0.0017596]
	Learning Rate: 0.00175963
	LOSS [training: 1.0762840409953465 | validation: 2.645943829489304]
	TIME [epoch: 8.47 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0688922214078573		[learning rate: 0.0017532]
	Learning Rate: 0.00175324
	LOSS [training: 1.0688922214078573 | validation: 2.63724133566824]
	TIME [epoch: 8.5 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0559368111824523		[learning rate: 0.0017469]
	Learning Rate: 0.00174688
	LOSS [training: 1.0559368111824523 | validation: 2.648480643512472]
	TIME [epoch: 8.48 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0787290345573377		[learning rate: 0.0017405]
	Learning Rate: 0.00174054
	LOSS [training: 1.0787290345573377 | validation: 2.6301768567192845]
	TIME [epoch: 8.48 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0737750622890552		[learning rate: 0.0017342]
	Learning Rate: 0.00173422
	LOSS [training: 1.0737750622890552 | validation: 2.6546741097425457]
	TIME [epoch: 8.48 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1417175291495363		[learning rate: 0.0017279]
	Learning Rate: 0.00172793
	LOSS [training: 1.1417175291495363 | validation: 2.7918458034227833]
	TIME [epoch: 8.51 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.085953156118141		[learning rate: 0.0017217]
	Learning Rate: 0.00172166
	LOSS [training: 1.085953156118141 | validation: 2.6598258000781807]
	TIME [epoch: 8.48 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0836328538204028		[learning rate: 0.0017154]
	Learning Rate: 0.00171541
	LOSS [training: 1.0836328538204028 | validation: 2.670742129681155]
	TIME [epoch: 8.48 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0524654175796249		[learning rate: 0.0017092]
	Learning Rate: 0.00170919
	LOSS [training: 1.0524654175796249 | validation: 2.635609217982454]
	TIME [epoch: 8.48 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0858637472513566		[learning rate: 0.001703]
	Learning Rate: 0.00170298
	LOSS [training: 1.0858637472513566 | validation: 2.6306376049537876]
	TIME [epoch: 8.51 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0454081842070893		[learning rate: 0.0016968]
	Learning Rate: 0.0016968
	LOSS [training: 1.0454081842070893 | validation: 2.6352068772038657]
	TIME [epoch: 8.49 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0813083880140208		[learning rate: 0.0016906]
	Learning Rate: 0.00169065
	LOSS [training: 1.0813083880140208 | validation: 2.6417494017324765]
	TIME [epoch: 8.48 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1382084361041769		[learning rate: 0.0016845]
	Learning Rate: 0.00168451
	LOSS [training: 1.1382084361041769 | validation: 2.7371023511222745]
	TIME [epoch: 8.48 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1028700855885147		[learning rate: 0.0016784]
	Learning Rate: 0.0016784
	LOSS [training: 1.1028700855885147 | validation: 2.6336820010983466]
	TIME [epoch: 8.5 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0494054880101786		[learning rate: 0.0016723]
	Learning Rate: 0.00167231
	LOSS [training: 1.0494054880101786 | validation: 2.7867290015388795]
	TIME [epoch: 8.48 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0784370841353526		[learning rate: 0.0016662]
	Learning Rate: 0.00166624
	LOSS [training: 1.0784370841353526 | validation: 2.7210703028736667]
	TIME [epoch: 8.48 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0636248648148636		[learning rate: 0.0016602]
	Learning Rate: 0.00166019
	LOSS [training: 1.0636248648148636 | validation: 2.702375332860518]
	TIME [epoch: 8.49 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0629811896931052		[learning rate: 0.0016542]
	Learning Rate: 0.00165417
	LOSS [training: 1.0629811896931052 | validation: 2.690522198421249]
	TIME [epoch: 8.5 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1443270584506224		[learning rate: 0.0016482]
	Learning Rate: 0.00164816
	LOSS [training: 1.1443270584506224 | validation: 2.7465669639519223]
	TIME [epoch: 8.48 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0489177020900509		[learning rate: 0.0016422]
	Learning Rate: 0.00164218
	LOSS [training: 1.0489177020900509 | validation: 2.783690804785105]
	TIME [epoch: 8.48 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0845946214525086		[learning rate: 0.0016362]
	Learning Rate: 0.00163622
	LOSS [training: 1.0845946214525086 | validation: 2.8306229229933564]
	TIME [epoch: 8.48 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0570196960920302		[learning rate: 0.0016303]
	Learning Rate: 0.00163028
	LOSS [training: 1.0570196960920302 | validation: 2.691249286204785]
	TIME [epoch: 8.5 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0806915449452539		[learning rate: 0.0016244]
	Learning Rate: 0.00162437
	LOSS [training: 1.0806915449452539 | validation: 2.669611456727781]
	TIME [epoch: 8.48 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.059328061193486		[learning rate: 0.0016185]
	Learning Rate: 0.00161847
	LOSS [training: 1.059328061193486 | validation: 2.7444179147644183]
	TIME [epoch: 8.48 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1067071284668795		[learning rate: 0.0016126]
	Learning Rate: 0.0016126
	LOSS [training: 1.1067071284668795 | validation: 2.749273431846593]
	TIME [epoch: 8.47 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0503873889010304		[learning rate: 0.0016067]
	Learning Rate: 0.00160675
	LOSS [training: 1.0503873889010304 | validation: 2.6513379507534083]
	TIME [epoch: 8.5 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0723264052142538		[learning rate: 0.0016009]
	Learning Rate: 0.00160092
	LOSS [training: 1.0723264052142538 | validation: 2.7002907940765226]
	TIME [epoch: 8.48 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0930783770510861		[learning rate: 0.0015951]
	Learning Rate: 0.00159511
	LOSS [training: 1.0930783770510861 | validation: 2.7532167953968654]
	TIME [epoch: 8.48 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0601895546914442		[learning rate: 0.0015893]
	Learning Rate: 0.00158932
	LOSS [training: 1.0601895546914442 | validation: 2.6692661405467586]
	TIME [epoch: 8.49 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0549637949022197		[learning rate: 0.0015835]
	Learning Rate: 0.00158355
	LOSS [training: 1.0549637949022197 | validation: 2.6570079184920887]
	TIME [epoch: 8.49 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0653795944287212		[learning rate: 0.0015778]
	Learning Rate: 0.0015778
	LOSS [training: 1.0653795944287212 | validation: 2.7082184764340393]
	TIME [epoch: 8.48 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0618357002926964		[learning rate: 0.0015721]
	Learning Rate: 0.00157208
	LOSS [training: 1.0618357002926964 | validation: 2.6460463904794653]
	TIME [epoch: 8.48 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.060874418655958		[learning rate: 0.0015664]
	Learning Rate: 0.00156637
	LOSS [training: 1.060874418655958 | validation: 2.6560018738250375]
	TIME [epoch: 8.49 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0448676549669467		[learning rate: 0.0015607]
	Learning Rate: 0.00156069
	LOSS [training: 1.0448676549669467 | validation: 2.6905366826356625]
	TIME [epoch: 8.5 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0484089742164542		[learning rate: 0.001555]
	Learning Rate: 0.00155502
	LOSS [training: 1.0484089742164542 | validation: 2.6658420207246687]
	TIME [epoch: 8.48 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.059967584468598		[learning rate: 0.0015494]
	Learning Rate: 0.00154938
	LOSS [training: 1.059967584468598 | validation: 2.6514871582545494]
	TIME [epoch: 8.48 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.075492647588877		[learning rate: 0.0015438]
	Learning Rate: 0.00154376
	LOSS [training: 1.075492647588877 | validation: 2.6440513173905957]
	TIME [epoch: 8.49 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0649311080004356		[learning rate: 0.0015382]
	Learning Rate: 0.00153815
	LOSS [training: 1.0649311080004356 | validation: 2.6647057966408876]
	TIME [epoch: 8.5 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0690211651500436		[learning rate: 0.0015326]
	Learning Rate: 0.00153257
	LOSS [training: 1.0690211651500436 | validation: 2.7011052435360163]
	TIME [epoch: 8.48 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0453206577662766		[learning rate: 0.001527]
	Learning Rate: 0.00152701
	LOSS [training: 1.0453206577662766 | validation: 2.6708767138135046]
	TIME [epoch: 8.48 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0930766697842853		[learning rate: 0.0015215]
	Learning Rate: 0.00152147
	LOSS [training: 1.0930766697842853 | validation: 2.738034150057664]
	TIME [epoch: 8.5 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0603086244655455		[learning rate: 0.0015159]
	Learning Rate: 0.00151595
	LOSS [training: 1.0603086244655455 | validation: 2.639593397393717]
	TIME [epoch: 8.49 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.080402065645315		[learning rate: 0.0015104]
	Learning Rate: 0.00151045
	LOSS [training: 1.080402065645315 | validation: 2.660695256309313]
	TIME [epoch: 8.48 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0450613246152423		[learning rate: 0.001505]
	Learning Rate: 0.00150496
	LOSS [training: 1.0450613246152423 | validation: 2.6587931752107834]
	TIME [epoch: 8.48 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0818811499918815		[learning rate: 0.0014995]
	Learning Rate: 0.0014995
	LOSS [training: 1.0818811499918815 | validation: 2.6318255916846724]
	TIME [epoch: 8.5 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0522706654977996		[learning rate: 0.0014941]
	Learning Rate: 0.00149406
	LOSS [training: 1.0522706654977996 | validation: 2.6857173356455872]
	TIME [epoch: 8.49 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.04806748826396		[learning rate: 0.0014886]
	Learning Rate: 0.00148864
	LOSS [training: 1.04806748826396 | validation: 2.8253848281282723]
	TIME [epoch: 8.48 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0735264382388743		[learning rate: 0.0014832]
	Learning Rate: 0.00148324
	LOSS [training: 1.0735264382388743 | validation: 2.7645153234852797]
	TIME [epoch: 8.48 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0618596545930772		[learning rate: 0.0014779]
	Learning Rate: 0.00147785
	LOSS [training: 1.0618596545930772 | validation: 2.6431060195205016]
	TIME [epoch: 8.5 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0389592679910142		[learning rate: 0.0014725]
	Learning Rate: 0.00147249
	LOSS [training: 1.0389592679910142 | validation: 2.659460781605152]
	TIME [epoch: 8.48 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0581811408321837		[learning rate: 0.0014671]
	Learning Rate: 0.00146715
	LOSS [training: 1.0581811408321837 | validation: 2.6238989578177696]
	TIME [epoch: 8.47 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.028308031119886		[learning rate: 0.0014618]
	Learning Rate: 0.00146182
	LOSS [training: 1.028308031119886 | validation: 2.6354404139708834]
	TIME [epoch: 8.48 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.043036236048768		[learning rate: 0.0014565]
	Learning Rate: 0.00145652
	LOSS [training: 1.043036236048768 | validation: 2.623307394492446]
	TIME [epoch: 8.5 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0609289260086399		[learning rate: 0.0014512]
	Learning Rate: 0.00145123
	LOSS [training: 1.0609289260086399 | validation: 2.6364639620065513]
	TIME [epoch: 8.48 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.059002521176788		[learning rate: 0.001446]
	Learning Rate: 0.00144597
	LOSS [training: 1.059002521176788 | validation: 2.6970378818552083]
	TIME [epoch: 8.47 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0349443193669083		[learning rate: 0.0014407]
	Learning Rate: 0.00144072
	LOSS [training: 1.0349443193669083 | validation: 2.706033938572961]
	TIME [epoch: 8.48 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.080615257676349		[learning rate: 0.0014355]
	Learning Rate: 0.00143549
	LOSS [training: 1.080615257676349 | validation: 2.6663518325950677]
	TIME [epoch: 8.5 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0575573156183404		[learning rate: 0.0014303]
	Learning Rate: 0.00143028
	LOSS [training: 1.0575573156183404 | validation: 2.6142382973993747]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_635.pth
	Model improved!!!
EPOCH 636/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.029323397997868		[learning rate: 0.0014251]
	Learning Rate: 0.00142509
	LOSS [training: 1.029323397997868 | validation: 2.669121146894745]
	TIME [epoch: 8.48 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.037216325194249		[learning rate: 0.0014199]
	Learning Rate: 0.00141992
	LOSS [training: 1.037216325194249 | validation: 2.803557814339765]
	TIME [epoch: 8.48 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0705560161351673		[learning rate: 0.0014148]
	Learning Rate: 0.00141476
	LOSS [training: 1.0705560161351673 | validation: 2.7166684207878697]
	TIME [epoch: 8.51 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0377276850574788		[learning rate: 0.0014096]
	Learning Rate: 0.00140963
	LOSS [training: 1.0377276850574788 | validation: 2.6288982326486154]
	TIME [epoch: 8.48 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0637208048747175		[learning rate: 0.0014045]
	Learning Rate: 0.00140451
	LOSS [training: 1.0637208048747175 | validation: 2.633952717679943]
	TIME [epoch: 8.48 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0274736865000365		[learning rate: 0.0013994]
	Learning Rate: 0.00139942
	LOSS [training: 1.0274736865000365 | validation: 2.6323404133909505]
	TIME [epoch: 8.47 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0395828971630867		[learning rate: 0.0013943]
	Learning Rate: 0.00139434
	LOSS [training: 1.0395828971630867 | validation: 2.6980192008781176]
	TIME [epoch: 8.5 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0646188894550632		[learning rate: 0.0013893]
	Learning Rate: 0.00138928
	LOSS [training: 1.0646188894550632 | validation: 2.6418857599484205]
	TIME [epoch: 8.48 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.06303513909073		[learning rate: 0.0013842]
	Learning Rate: 0.00138424
	LOSS [training: 1.06303513909073 | validation: 2.8105458893744792]
	TIME [epoch: 8.48 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1179318612646685		[learning rate: 0.0013792]
	Learning Rate: 0.00137921
	LOSS [training: 1.1179318612646685 | validation: 2.6272538407664845]
	TIME [epoch: 8.49 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0588858060784847		[learning rate: 0.0013742]
	Learning Rate: 0.00137421
	LOSS [training: 1.0588858060784847 | validation: 2.635637332613305]
	TIME [epoch: 8.55 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0415818754578903		[learning rate: 0.0013692]
	Learning Rate: 0.00136922
	LOSS [training: 1.0415818754578903 | validation: 2.6337465401251974]
	TIME [epoch: 8.47 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0873891227682315		[learning rate: 0.0013643]
	Learning Rate: 0.00136425
	LOSS [training: 1.0873891227682315 | validation: 2.6525829128615857]
	TIME [epoch: 8.48 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0488974203389094		[learning rate: 0.0013593]
	Learning Rate: 0.0013593
	LOSS [training: 1.0488974203389094 | validation: 2.624431173416834]
	TIME [epoch: 8.48 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0361601215230807		[learning rate: 0.0013544]
	Learning Rate: 0.00135437
	LOSS [training: 1.0361601215230807 | validation: 2.6975098431091444]
	TIME [epoch: 8.49 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0393300708669648		[learning rate: 0.0013495]
	Learning Rate: 0.00134945
	LOSS [training: 1.0393300708669648 | validation: 2.7944519767357567]
	TIME [epoch: 8.47 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0672397470967616		[learning rate: 0.0013446]
	Learning Rate: 0.00134456
	LOSS [training: 1.0672397470967616 | validation: 2.604133829423301]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_652.pth
	Model improved!!!
EPOCH 653/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0366850960982907		[learning rate: 0.0013397]
	Learning Rate: 0.00133968
	LOSS [training: 1.0366850960982907 | validation: 2.6467634471310815]
	TIME [epoch: 8.48 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0516301125210958		[learning rate: 0.0013348]
	Learning Rate: 0.00133481
	LOSS [training: 1.0516301125210958 | validation: 2.6950822925242117]
	TIME [epoch: 8.47 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1152301525836426		[learning rate: 0.00133]
	Learning Rate: 0.00132997
	LOSS [training: 1.1152301525836426 | validation: 2.731423153672546]
	TIME [epoch: 8.47 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0329652312147612		[learning rate: 0.0013251]
	Learning Rate: 0.00132514
	LOSS [training: 1.0329652312147612 | validation: 2.6809938174810775]
	TIME [epoch: 8.46 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0567180461417596		[learning rate: 0.0013203]
	Learning Rate: 0.00132034
	LOSS [training: 1.0567180461417596 | validation: 2.6182086027131124]
	TIME [epoch: 8.48 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0439416433480047		[learning rate: 0.0013155]
	Learning Rate: 0.00131554
	LOSS [training: 1.0439416433480047 | validation: 2.6232892929152802]
	TIME [epoch: 8.47 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0257115442640612		[learning rate: 0.0013108]
	Learning Rate: 0.00131077
	LOSS [training: 1.0257115442640612 | validation: 2.659732803424751]
	TIME [epoch: 8.47 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0571526392314323		[learning rate: 0.001306]
	Learning Rate: 0.00130601
	LOSS [training: 1.0571526392314323 | validation: 2.6852076066149597]
	TIME [epoch: 8.46 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0492981874463314		[learning rate: 0.0013013]
	Learning Rate: 0.00130127
	LOSS [training: 1.0492981874463314 | validation: 2.7812895216643962]
	TIME [epoch: 8.48 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0590766911251526		[learning rate: 0.0012966]
	Learning Rate: 0.00129655
	LOSS [training: 1.0590766911251526 | validation: 2.6288095868030257]
	TIME [epoch: 8.48 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0396466009295364		[learning rate: 0.0012918]
	Learning Rate: 0.00129185
	LOSS [training: 1.0396466009295364 | validation: 2.6302321300631273]
	TIME [epoch: 8.47 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0361479759985657		[learning rate: 0.0012872]
	Learning Rate: 0.00128716
	LOSS [training: 1.0361479759985657 | validation: 2.6670277449882223]
	TIME [epoch: 8.47 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0313725468062604		[learning rate: 0.0012825]
	Learning Rate: 0.00128249
	LOSS [training: 1.0313725468062604 | validation: 2.7955850792736143]
	TIME [epoch: 8.49 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1018586356757474		[learning rate: 0.0012778]
	Learning Rate: 0.00127783
	LOSS [training: 1.1018586356757474 | validation: 2.673561712428564]
	TIME [epoch: 8.47 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0303875910615654		[learning rate: 0.0012732]
	Learning Rate: 0.00127319
	LOSS [training: 1.0303875910615654 | validation: 2.697215138658494]
	TIME [epoch: 8.47 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0382491968071166		[learning rate: 0.0012686]
	Learning Rate: 0.00126857
	LOSS [training: 1.0382491968071166 | validation: 2.6944765432226045]
	TIME [epoch: 8.47 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.034891758567651		[learning rate: 0.001264]
	Learning Rate: 0.00126397
	LOSS [training: 1.034891758567651 | validation: 2.708921010870583]
	TIME [epoch: 8.49 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0443871700790477		[learning rate: 0.0012594]
	Learning Rate: 0.00125938
	LOSS [training: 1.0443871700790477 | validation: 2.6203806523454514]
	TIME [epoch: 8.47 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0373424019843844		[learning rate: 0.0012548]
	Learning Rate: 0.00125481
	LOSS [training: 1.0373424019843844 | validation: 2.6890677313101476]
	TIME [epoch: 8.47 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0422483242826002		[learning rate: 0.0012503]
	Learning Rate: 0.00125026
	LOSS [training: 1.0422483242826002 | validation: 2.657337092232634]
	TIME [epoch: 8.46 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0338393565284558		[learning rate: 0.0012457]
	Learning Rate: 0.00124572
	LOSS [training: 1.0338393565284558 | validation: 2.6229462819603775]
	TIME [epoch: 8.49 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.033282572384489		[learning rate: 0.0012412]
	Learning Rate: 0.0012412
	LOSS [training: 1.033282572384489 | validation: 2.7427103388258125]
	TIME [epoch: 8.47 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0876351212837112		[learning rate: 0.0012367]
	Learning Rate: 0.0012367
	LOSS [training: 1.0876351212837112 | validation: 2.747201346123049]
	TIME [epoch: 8.47 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0772820801200917		[learning rate: 0.0012322]
	Learning Rate: 0.00123221
	LOSS [training: 1.0772820801200917 | validation: 2.6183777499134666]
	TIME [epoch: 8.47 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.075172533989274		[learning rate: 0.0012277]
	Learning Rate: 0.00122774
	LOSS [training: 1.075172533989274 | validation: 2.6148968788048306]
	TIME [epoch: 8.49 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0672962260520993		[learning rate: 0.0012233]
	Learning Rate: 0.00122328
	LOSS [training: 1.0672962260520993 | validation: 2.6775415258993758]
	TIME [epoch: 8.46 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0692658092051577		[learning rate: 0.0012188]
	Learning Rate: 0.00121884
	LOSS [training: 1.0692658092051577 | validation: 2.63227763684724]
	TIME [epoch: 8.47 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0182725306211684		[learning rate: 0.0012144]
	Learning Rate: 0.00121442
	LOSS [training: 1.0182725306211684 | validation: 2.613906715720372]
	TIME [epoch: 8.47 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0540737487482246		[learning rate: 0.00121]
	Learning Rate: 0.00121001
	LOSS [training: 1.0540737487482246 | validation: 2.6128163585336672]
	TIME [epoch: 8.49 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0628538569923358		[learning rate: 0.0012056]
	Learning Rate: 0.00120562
	LOSS [training: 1.0628538569923358 | validation: 2.656920013829463]
	TIME [epoch: 8.47 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.077863158366059		[learning rate: 0.0012012]
	Learning Rate: 0.00120125
	LOSS [training: 1.077863158366059 | validation: 2.652076899831873]
	TIME [epoch: 8.46 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0715680781028354		[learning rate: 0.0011969]
	Learning Rate: 0.00119689
	LOSS [training: 1.0715680781028354 | validation: 2.6314540188158517]
	TIME [epoch: 8.47 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0669712615462168		[learning rate: 0.0011925]
	Learning Rate: 0.00119254
	LOSS [training: 1.0669712615462168 | validation: 2.7664712450488658]
	TIME [epoch: 8.51 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0407513741568957		[learning rate: 0.0011882]
	Learning Rate: 0.00118821
	LOSS [training: 1.0407513741568957 | validation: 2.6486105745303243]
	TIME [epoch: 8.47 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.03239848536018		[learning rate: 0.0011839]
	Learning Rate: 0.0011839
	LOSS [training: 1.03239848536018 | validation: 2.6692201296447076]
	TIME [epoch: 8.48 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0495059248721628		[learning rate: 0.0011796]
	Learning Rate: 0.00117961
	LOSS [training: 1.0495059248721628 | validation: 2.6447588682440504]
	TIME [epoch: 8.47 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0480435538046846		[learning rate: 0.0011753]
	Learning Rate: 0.00117532
	LOSS [training: 1.0480435538046846 | validation: 2.654566898154124]
	TIME [epoch: 8.49 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.036760454219015		[learning rate: 0.0011711]
	Learning Rate: 0.00117106
	LOSS [training: 1.036760454219015 | validation: 2.616044604585933]
	TIME [epoch: 8.47 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0737380942107408		[learning rate: 0.0011668]
	Learning Rate: 0.00116681
	LOSS [training: 1.0737380942107408 | validation: 2.630787606516617]
	TIME [epoch: 8.47 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.043068773407764		[learning rate: 0.0011626]
	Learning Rate: 0.00116258
	LOSS [training: 1.043068773407764 | validation: 2.624594303884665]
	TIME [epoch: 8.47 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0410929537607088		[learning rate: 0.0011584]
	Learning Rate: 0.00115836
	LOSS [training: 1.0410929537607088 | validation: 2.6861719519100093]
	TIME [epoch: 8.48 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.025853970065857		[learning rate: 0.0011542]
	Learning Rate: 0.00115415
	LOSS [training: 1.025853970065857 | validation: 2.700842551880001]
	TIME [epoch: 8.47 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0674128241544474		[learning rate: 0.00115]
	Learning Rate: 0.00114996
	LOSS [training: 1.0674128241544474 | validation: 2.6151183393501345]
	TIME [epoch: 8.48 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0453349105813485		[learning rate: 0.0011458]
	Learning Rate: 0.00114579
	LOSS [training: 1.0453349105813485 | validation: 2.6776699918825067]
	TIME [epoch: 8.48 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.025440405029043		[learning rate: 0.0011416]
	Learning Rate: 0.00114163
	LOSS [training: 1.025440405029043 | validation: 2.6253767747856545]
	TIME [epoch: 8.48 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0255990897569034		[learning rate: 0.0011375]
	Learning Rate: 0.00113749
	LOSS [training: 1.0255990897569034 | validation: 2.628630445403932]
	TIME [epoch: 8.47 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0247511115600547		[learning rate: 0.0011334]
	Learning Rate: 0.00113336
	LOSS [training: 1.0247511115600547 | validation: 2.762249058163667]
	TIME [epoch: 8.47 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0263114960822022		[learning rate: 0.0011292]
	Learning Rate: 0.00112925
	LOSS [training: 1.0263114960822022 | validation: 2.691663217209666]
	TIME [epoch: 8.48 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0281217659190334		[learning rate: 0.0011252]
	Learning Rate: 0.00112515
	LOSS [training: 1.0281217659190334 | validation: 2.6043661703690586]
	TIME [epoch: 8.48 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0375835051425901		[learning rate: 0.0011211]
	Learning Rate: 0.00112107
	LOSS [training: 1.0375835051425901 | validation: 2.6207961956603816]
	TIME [epoch: 8.47 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.018598408599052		[learning rate: 0.001117]
	Learning Rate: 0.001117
	LOSS [training: 1.018598408599052 | validation: 2.6165977326080436]
	TIME [epoch: 8.47 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0581891513433241		[learning rate: 0.0011129]
	Learning Rate: 0.00111295
	LOSS [training: 1.0581891513433241 | validation: 2.7561392949854837]
	TIME [epoch: 8.5 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0315637181230133		[learning rate: 0.0011089]
	Learning Rate: 0.00110891
	LOSS [training: 1.0315637181230133 | validation: 2.683733986247069]
	TIME [epoch: 8.48 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0334016218783986		[learning rate: 0.0011049]
	Learning Rate: 0.00110488
	LOSS [training: 1.0334016218783986 | validation: 2.617054070800246]
	TIME [epoch: 8.47 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.027254908620492		[learning rate: 0.0011009]
	Learning Rate: 0.00110087
	LOSS [training: 1.027254908620492 | validation: 2.6114598427201043]
	TIME [epoch: 8.47 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.040717788369626		[learning rate: 0.0010969]
	Learning Rate: 0.00109688
	LOSS [training: 1.040717788369626 | validation: 2.615980770085541]
	TIME [epoch: 8.49 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0232361552242568		[learning rate: 0.0010929]
	Learning Rate: 0.0010929
	LOSS [training: 1.0232361552242568 | validation: 2.610349663249318]
	TIME [epoch: 8.47 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0109223611330764		[learning rate: 0.0010889]
	Learning Rate: 0.00108893
	LOSS [training: 1.0109223611330764 | validation: 2.607142162171174]
	TIME [epoch: 8.47 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0226470220916732		[learning rate: 0.001085]
	Learning Rate: 0.00108498
	LOSS [training: 1.0226470220916732 | validation: 2.6594792174706274]
	TIME [epoch: 8.47 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0213995865414447		[learning rate: 0.001081]
	Learning Rate: 0.00108104
	LOSS [training: 1.0213995865414447 | validation: 2.8640245658514503]
	TIME [epoch: 8.49 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0542590987775413		[learning rate: 0.0010771]
	Learning Rate: 0.00107712
	LOSS [training: 1.0542590987775413 | validation: 2.6051916814992384]
	TIME [epoch: 8.47 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0114069562061285		[learning rate: 0.0010732]
	Learning Rate: 0.00107321
	LOSS [training: 1.0114069562061285 | validation: 2.6847639971660993]
	TIME [epoch: 8.47 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.021904723026927		[learning rate: 0.0010693]
	Learning Rate: 0.00106931
	LOSS [training: 1.021904723026927 | validation: 2.6496246784918602]
	TIME [epoch: 8.48 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0351357293386694		[learning rate: 0.0010654]
	Learning Rate: 0.00106543
	LOSS [training: 1.0351357293386694 | validation: 2.646044828136541]
	TIME [epoch: 8.5 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0340606671268529		[learning rate: 0.0010616]
	Learning Rate: 0.00106157
	LOSS [training: 1.0340606671268529 | validation: 2.655237419780409]
	TIME [epoch: 8.48 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.03022593559362		[learning rate: 0.0010577]
	Learning Rate: 0.00105771
	LOSS [training: 1.03022593559362 | validation: 2.608656209323587]
	TIME [epoch: 8.48 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0234520590873442		[learning rate: 0.0010539]
	Learning Rate: 0.00105388
	LOSS [training: 1.0234520590873442 | validation: 2.605527140087486]
	TIME [epoch: 8.48 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0201422511238074		[learning rate: 0.0010501]
	Learning Rate: 0.00105005
	LOSS [training: 1.0201422511238074 | validation: 2.6096075282701046]
	TIME [epoch: 8.5 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.014396905060617		[learning rate: 0.0010462]
	Learning Rate: 0.00104624
	LOSS [training: 1.014396905060617 | validation: 2.598849118971186]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_721.pth
	Model improved!!!
EPOCH 722/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0124172720930533		[learning rate: 0.0010424]
	Learning Rate: 0.00104244
	LOSS [training: 1.0124172720930533 | validation: 2.635935160632117]
	TIME [epoch: 8.48 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.012159133803678		[learning rate: 0.0010387]
	Learning Rate: 0.00103866
	LOSS [training: 1.012159133803678 | validation: 2.61302195754985]
	TIME [epoch: 8.48 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.051965163095874		[learning rate: 0.0010349]
	Learning Rate: 0.00103489
	LOSS [training: 1.051965163095874 | validation: 2.614019554425904]
	TIME [epoch: 8.5 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0305958902670624		[learning rate: 0.0010311]
	Learning Rate: 0.00103114
	LOSS [training: 1.0305958902670624 | validation: 2.6884720786096294]
	TIME [epoch: 8.47 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0266088885364049		[learning rate: 0.0010274]
	Learning Rate: 0.00102739
	LOSS [training: 1.0266088885364049 | validation: 2.7566516435442594]
	TIME [epoch: 8.47 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.059803903979486		[learning rate: 0.0010237]
	Learning Rate: 0.00102367
	LOSS [training: 1.059803903979486 | validation: 2.6105194896799198]
	TIME [epoch: 8.47 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.041701916611465		[learning rate: 0.00102]
	Learning Rate: 0.00101995
	LOSS [training: 1.041701916611465 | validation: 2.6237433449199656]
	TIME [epoch: 8.5 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0301027868797887		[learning rate: 0.0010162]
	Learning Rate: 0.00101625
	LOSS [training: 1.0301027868797887 | validation: 2.6288662595554646]
	TIME [epoch: 8.47 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.038471376226446		[learning rate: 0.0010126]
	Learning Rate: 0.00101256
	LOSS [training: 1.038471376226446 | validation: 2.6911650368709026]
	TIME [epoch: 8.47 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.025523798860863		[learning rate: 0.0010089]
	Learning Rate: 0.00100889
	LOSS [training: 1.025523798860863 | validation: 2.6159800090373633]
	TIME [epoch: 8.46 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0365927746211832		[learning rate: 0.0010052]
	Learning Rate: 0.00100522
	LOSS [training: 1.0365927746211832 | validation: 2.825593762372307]
	TIME [epoch: 8.49 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0414549969915328		[learning rate: 0.0010016]
	Learning Rate: 0.00100158
	LOSS [training: 1.0414549969915328 | validation: 2.6842758489731953]
	TIME [epoch: 8.47 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0251294997656855		[learning rate: 0.00099794]
	Learning Rate: 0.000997942
	LOSS [training: 1.0251294997656855 | validation: 2.6360961863435732]
	TIME [epoch: 8.48 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0310963293126292		[learning rate: 0.00099432]
	Learning Rate: 0.00099432
	LOSS [training: 1.0310963293126292 | validation: 2.632088091329714]
	TIME [epoch: 8.48 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0369571634269295		[learning rate: 0.00099071]
	Learning Rate: 0.000990712
	LOSS [training: 1.0369571634269295 | validation: 2.6113685793295542]
	TIME [epoch: 8.48 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0438452814523138		[learning rate: 0.00098712]
	Learning Rate: 0.000987116
	LOSS [training: 1.0438452814523138 | validation: 2.6670854360295664]
	TIME [epoch: 8.47 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0278153748805425		[learning rate: 0.00098353]
	Learning Rate: 0.000983534
	LOSS [training: 1.0278153748805425 | validation: 2.648312553719112]
	TIME [epoch: 8.47 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.009887078695615		[learning rate: 0.00097996]
	Learning Rate: 0.000979965
	LOSS [training: 1.009887078695615 | validation: 2.614090734565583]
	TIME [epoch: 8.48 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0101705734951796		[learning rate: 0.00097641]
	Learning Rate: 0.000976409
	LOSS [training: 1.0101705734951796 | validation: 2.627250940795676]
	TIME [epoch: 8.48 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0270360636162092		[learning rate: 0.00097287]
	Learning Rate: 0.000972865
	LOSS [training: 1.0270360636162092 | validation: 2.5941014159648326]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_741.pth
	Model improved!!!
EPOCH 742/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0189746959199746		[learning rate: 0.00096933]
	Learning Rate: 0.000969335
	LOSS [training: 1.0189746959199746 | validation: 2.60911100281046]
	TIME [epoch: 8.47 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0552322207416325		[learning rate: 0.00096582]
	Learning Rate: 0.000965817
	LOSS [training: 1.0552322207416325 | validation: 2.615843203122292]
	TIME [epoch: 8.49 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.028389618203642		[learning rate: 0.00096231]
	Learning Rate: 0.000962312
	LOSS [training: 1.028389618203642 | validation: 2.6296870609321417]
	TIME [epoch: 8.49 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0160825429019649		[learning rate: 0.00095882]
	Learning Rate: 0.00095882
	LOSS [training: 1.0160825429019649 | validation: 2.618245177396883]
	TIME [epoch: 8.48 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0008461096249355		[learning rate: 0.00095534]
	Learning Rate: 0.00095534
	LOSS [training: 1.0008461096249355 | validation: 2.674131498522234]
	TIME [epoch: 8.48 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0199654705242982		[learning rate: 0.00095187]
	Learning Rate: 0.000951873
	LOSS [training: 1.0199654705242982 | validation: 2.6737340100550853]
	TIME [epoch: 8.5 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.014646157840851		[learning rate: 0.00094842]
	Learning Rate: 0.000948419
	LOSS [training: 1.014646157840851 | validation: 2.627542868398535]
	TIME [epoch: 8.48 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.017084719660013		[learning rate: 0.00094498]
	Learning Rate: 0.000944977
	LOSS [training: 1.017084719660013 | validation: 2.618192993951929]
	TIME [epoch: 8.49 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0211297223564528		[learning rate: 0.00094155]
	Learning Rate: 0.000941547
	LOSS [training: 1.0211297223564528 | validation: 2.658193921507294]
	TIME [epoch: 8.48 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0742389333189855		[learning rate: 0.00093813]
	Learning Rate: 0.00093813
	LOSS [training: 1.0742389333189855 | validation: 2.628199039614951]
	TIME [epoch: 8.5 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0205767746107162		[learning rate: 0.00093473]
	Learning Rate: 0.000934726
	LOSS [training: 1.0205767746107162 | validation: 2.621102337547944]
	TIME [epoch: 8.49 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0055571870394446		[learning rate: 0.00093133]
	Learning Rate: 0.000931334
	LOSS [training: 1.0055571870394446 | validation: 2.6225341071137196]
	TIME [epoch: 8.47 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0044975838367765		[learning rate: 0.00092795]
	Learning Rate: 0.000927954
	LOSS [training: 1.0044975838367765 | validation: 2.6601191031818807]
	TIME [epoch: 8.47 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0205765469431862		[learning rate: 0.00092459]
	Learning Rate: 0.000924586
	LOSS [training: 1.0205765469431862 | validation: 2.6345238765426817]
	TIME [epoch: 8.49 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.023246208778476		[learning rate: 0.00092123]
	Learning Rate: 0.000921231
	LOSS [training: 1.023246208778476 | validation: 2.665729337561647]
	TIME [epoch: 8.48 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0146677091849932		[learning rate: 0.00091789]
	Learning Rate: 0.000917888
	LOSS [training: 1.0146677091849932 | validation: 2.7613616967425414]
	TIME [epoch: 8.48 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.019509119012291		[learning rate: 0.00091456]
	Learning Rate: 0.000914556
	LOSS [training: 1.019509119012291 | validation: 2.6260040563953293]
	TIME [epoch: 8.47 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0226193906318355		[learning rate: 0.00091124]
	Learning Rate: 0.000911237
	LOSS [training: 1.0226193906318355 | validation: 2.695778868251603]
	TIME [epoch: 8.49 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0172672812453698		[learning rate: 0.00090793]
	Learning Rate: 0.000907931
	LOSS [training: 1.0172672812453698 | validation: 2.6140492455122573]
	TIME [epoch: 8.47 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.004203247210428		[learning rate: 0.00090464]
	Learning Rate: 0.000904636
	LOSS [training: 1.004203247210428 | validation: 2.622835321494572]
	TIME [epoch: 8.48 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.005153288288828		[learning rate: 0.00090135]
	Learning Rate: 0.000901353
	LOSS [training: 1.005153288288828 | validation: 2.6054230141005923]
	TIME [epoch: 8.46 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0085191486059886		[learning rate: 0.00089808]
	Learning Rate: 0.000898082
	LOSS [training: 1.0085191486059886 | validation: 2.675225484323151]
	TIME [epoch: 8.49 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0171619635418083		[learning rate: 0.00089482]
	Learning Rate: 0.000894822
	LOSS [training: 1.0171619635418083 | validation: 2.607707219036932]
	TIME [epoch: 8.48 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0311336964319349		[learning rate: 0.00089157]
	Learning Rate: 0.000891575
	LOSS [training: 1.0311336964319349 | validation: 2.617796509259902]
	TIME [epoch: 8.47 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0082027253349364		[learning rate: 0.00088834]
	Learning Rate: 0.000888339
	LOSS [training: 1.0082027253349364 | validation: 2.597305584406514]
	TIME [epoch: 8.47 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.004399528707436		[learning rate: 0.00088512]
	Learning Rate: 0.000885116
	LOSS [training: 1.004399528707436 | validation: 2.6667325708993483]
	TIME [epoch: 8.49 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0079441553288029		[learning rate: 0.0008819]
	Learning Rate: 0.000881903
	LOSS [training: 1.0079441553288029 | validation: 2.6279076307844127]
	TIME [epoch: 8.48 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0218009863852102		[learning rate: 0.0008787]
	Learning Rate: 0.000878703
	LOSS [training: 1.0218009863852102 | validation: 2.6504307464954033]
	TIME [epoch: 8.46 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0092628531327545		[learning rate: 0.00087551]
	Learning Rate: 0.000875514
	LOSS [training: 1.0092628531327545 | validation: 2.6051187399474083]
	TIME [epoch: 8.47 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0174317522159844		[learning rate: 0.00087234]
	Learning Rate: 0.000872337
	LOSS [training: 1.0174317522159844 | validation: 2.6553600571051375]
	TIME [epoch: 8.48 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0458125568151289		[learning rate: 0.00086917]
	Learning Rate: 0.000869171
	LOSS [training: 1.0458125568151289 | validation: 2.6394421352807997]
	TIME [epoch: 8.46 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0314843667138884		[learning rate: 0.00086602]
	Learning Rate: 0.000866017
	LOSS [training: 1.0314843667138884 | validation: 2.6121804640279165]
	TIME [epoch: 8.47 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0079825445185087		[learning rate: 0.00086287]
	Learning Rate: 0.000862874
	LOSS [training: 1.0079825445185087 | validation: 2.6413808913857295]
	TIME [epoch: 8.46 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0089700372914883		[learning rate: 0.00085974]
	Learning Rate: 0.000859743
	LOSS [training: 1.0089700372914883 | validation: 2.6172259916657516]
	TIME [epoch: 8.49 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0193159419760973		[learning rate: 0.00085662]
	Learning Rate: 0.000856623
	LOSS [training: 1.0193159419760973 | validation: 2.6928584332346497]
	TIME [epoch: 8.47 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0169121504924212		[learning rate: 0.00085351]
	Learning Rate: 0.000853514
	LOSS [training: 1.0169121504924212 | validation: 2.6138505333827884]
	TIME [epoch: 8.47 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0059317013250826		[learning rate: 0.00085042]
	Learning Rate: 0.000850416
	LOSS [training: 1.0059317013250826 | validation: 2.6569861957128538]
	TIME [epoch: 8.48 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.024420580814049		[learning rate: 0.00084733]
	Learning Rate: 0.00084733
	LOSS [training: 1.024420580814049 | validation: 2.6094932374540476]
	TIME [epoch: 8.49 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.004854960728099		[learning rate: 0.00084426]
	Learning Rate: 0.000844255
	LOSS [training: 1.004854960728099 | validation: 2.6150281240931923]
	TIME [epoch: 8.47 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.00440516573095		[learning rate: 0.00084119]
	Learning Rate: 0.000841191
	LOSS [training: 1.00440516573095 | validation: 2.621209006952736]
	TIME [epoch: 8.47 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0472964519906858		[learning rate: 0.00083814]
	Learning Rate: 0.000838139
	LOSS [training: 1.0472964519906858 | validation: 2.6429709966222426]
	TIME [epoch: 8.48 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0084466269708652		[learning rate: 0.0008351]
	Learning Rate: 0.000835097
	LOSS [training: 1.0084466269708652 | validation: 2.7947412527204616]
	TIME [epoch: 8.48 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0174111564751134		[learning rate: 0.00083207]
	Learning Rate: 0.000832066
	LOSS [training: 1.0174111564751134 | validation: 2.6015543045698593]
	TIME [epoch: 8.47 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0309845573804512		[learning rate: 0.00082905]
	Learning Rate: 0.000829046
	LOSS [training: 1.0309845573804512 | validation: 2.6000546365800106]
	TIME [epoch: 8.47 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0222872877941274		[learning rate: 0.00082604]
	Learning Rate: 0.000826038
	LOSS [training: 1.0222872877941274 | validation: 2.6308231489423295]
	TIME [epoch: 8.48 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0144978751941278		[learning rate: 0.00082304]
	Learning Rate: 0.00082304
	LOSS [training: 1.0144978751941278 | validation: 2.602196808517538]
	TIME [epoch: 8.48 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.015930261303916		[learning rate: 0.00082005]
	Learning Rate: 0.000820053
	LOSS [training: 1.015930261303916 | validation: 2.6268546390956424]
	TIME [epoch: 8.47 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.002714351410321		[learning rate: 0.00081708]
	Learning Rate: 0.000817077
	LOSS [training: 1.002714351410321 | validation: 2.6666956128541632]
	TIME [epoch: 8.47 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0374334955366948		[learning rate: 0.00081411]
	Learning Rate: 0.000814112
	LOSS [training: 1.0374334955366948 | validation: 2.6856602893389128]
	TIME [epoch: 8.49 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.010321086582296		[learning rate: 0.00081116]
	Learning Rate: 0.000811158
	LOSS [training: 1.010321086582296 | validation: 2.635324491578162]
	TIME [epoch: 8.48 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.996286628270561		[learning rate: 0.00080821]
	Learning Rate: 0.000808214
	LOSS [training: 0.996286628270561 | validation: 2.601427609381038]
	TIME [epoch: 8.48 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0268010204580589		[learning rate: 0.00080528]
	Learning Rate: 0.000805281
	LOSS [training: 1.0268010204580589 | validation: 2.605246246512407]
	TIME [epoch: 8.48 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0101469652958637		[learning rate: 0.00080236]
	Learning Rate: 0.000802358
	LOSS [training: 1.0101469652958637 | validation: 2.6575311942501143]
	TIME [epoch: 8.5 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.005313010826564		[learning rate: 0.00079945]
	Learning Rate: 0.000799447
	LOSS [training: 1.005313010826564 | validation: 2.603392688986263]
	TIME [epoch: 8.49 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0022556653946026		[learning rate: 0.00079655]
	Learning Rate: 0.000796545
	LOSS [training: 1.0022556653946026 | validation: 2.617979557708102]
	TIME [epoch: 8.48 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0290444612301282		[learning rate: 0.00079365]
	Learning Rate: 0.000793655
	LOSS [training: 1.0290444612301282 | validation: 2.5971958399058708]
	TIME [epoch: 8.48 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.004720215457708		[learning rate: 0.00079077]
	Learning Rate: 0.000790775
	LOSS [training: 1.004720215457708 | validation: 2.5964867993503935]
	TIME [epoch: 8.5 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.002591735864673		[learning rate: 0.0007879]
	Learning Rate: 0.000787905
	LOSS [training: 1.002591735864673 | validation: 2.615655763092982]
	TIME [epoch: 8.48 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0058698489306233		[learning rate: 0.00078505]
	Learning Rate: 0.000785045
	LOSS [training: 1.0058698489306233 | validation: 2.608541571196036]
	TIME [epoch: 8.47 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9985547287797285		[learning rate: 0.0007822]
	Learning Rate: 0.000782196
	LOSS [training: 0.9985547287797285 | validation: 2.600556201732642]
	TIME [epoch: 8.47 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9948197041341361		[learning rate: 0.00077936]
	Learning Rate: 0.000779358
	LOSS [training: 0.9948197041341361 | validation: 2.61730644495007]
	TIME [epoch: 8.5 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9999780568759062		[learning rate: 0.00077653]
	Learning Rate: 0.000776529
	LOSS [training: 0.9999780568759062 | validation: 2.5942658485982077]
	TIME [epoch: 8.48 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9940192132213352		[learning rate: 0.00077371]
	Learning Rate: 0.000773711
	LOSS [training: 0.9940192132213352 | validation: 2.6155527864191574]
	TIME [epoch: 8.48 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0137322789295804		[learning rate: 0.0007709]
	Learning Rate: 0.000770903
	LOSS [training: 1.0137322789295804 | validation: 2.611445145215481]
	TIME [epoch: 8.48 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9921726253972659		[learning rate: 0.00076811]
	Learning Rate: 0.000768106
	LOSS [training: 0.9921726253972659 | validation: 2.6287613683421713]
	TIME [epoch: 8.5 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.003227750881775		[learning rate: 0.00076532]
	Learning Rate: 0.000765318
	LOSS [training: 1.003227750881775 | validation: 2.615515279824811]
	TIME [epoch: 8.48 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0586218177235662		[learning rate: 0.00076254]
	Learning Rate: 0.000762541
	LOSS [training: 1.0586218177235662 | validation: 2.6054406693921726]
	TIME [epoch: 8.48 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9874913821132505		[learning rate: 0.00075977]
	Learning Rate: 0.000759774
	LOSS [training: 0.9874913821132505 | validation: 2.6127245436942594]
	TIME [epoch: 8.48 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0037294880652463		[learning rate: 0.00075702]
	Learning Rate: 0.000757016
	LOSS [training: 1.0037294880652463 | validation: 2.678658467767878]
	TIME [epoch: 8.51 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0032119954164824		[learning rate: 0.00075427]
	Learning Rate: 0.000754269
	LOSS [training: 1.0032119954164824 | validation: 2.596896998538146]
	TIME [epoch: 8.48 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9935342656067274		[learning rate: 0.00075153]
	Learning Rate: 0.000751532
	LOSS [training: 0.9935342656067274 | validation: 2.6384533374209544]
	TIME [epoch: 8.48 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0122633234976592		[learning rate: 0.0007488]
	Learning Rate: 0.000748804
	LOSS [training: 1.0122633234976592 | validation: 2.6039460478242398]
	TIME [epoch: 8.48 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0142871400813465		[learning rate: 0.00074609]
	Learning Rate: 0.000746087
	LOSS [training: 1.0142871400813465 | validation: 2.629304147020569]
	TIME [epoch: 8.5 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0111667074308668		[learning rate: 0.00074338]
	Learning Rate: 0.000743379
	LOSS [training: 1.0111667074308668 | validation: 2.6515395641449393]
	TIME [epoch: 8.48 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0075857954385212		[learning rate: 0.00074068]
	Learning Rate: 0.000740682
	LOSS [training: 1.0075857954385212 | validation: 2.5913957883338132]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_816.pth
	Model improved!!!
EPOCH 817/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9962415112862109		[learning rate: 0.00073799]
	Learning Rate: 0.000737994
	LOSS [training: 0.9962415112862109 | validation: 2.608185835098243]
	TIME [epoch: 8.49 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0011192669210138		[learning rate: 0.00073532]
	Learning Rate: 0.000735315
	LOSS [training: 1.0011192669210138 | validation: 2.6441128722856937]
	TIME [epoch: 8.49 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0134953489226826		[learning rate: 0.00073265]
	Learning Rate: 0.000732647
	LOSS [training: 1.0134953489226826 | validation: 2.595165990703877]
	TIME [epoch: 8.48 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0040306135409534		[learning rate: 0.00072999]
	Learning Rate: 0.000729988
	LOSS [training: 1.0040306135409534 | validation: 2.6274675137198726]
	TIME [epoch: 8.48 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9926292432455168		[learning rate: 0.00072734]
	Learning Rate: 0.000727339
	LOSS [training: 0.9926292432455168 | validation: 2.6128042288676587]
	TIME [epoch: 8.48 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0028414940685695		[learning rate: 0.0007247]
	Learning Rate: 0.000724699
	LOSS [training: 1.0028414940685695 | validation: 2.654362343096281]
	TIME [epoch: 8.49 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0103593443059762		[learning rate: 0.00072207]
	Learning Rate: 0.000722069
	LOSS [training: 1.0103593443059762 | validation: 2.605193213184101]
	TIME [epoch: 8.48 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9969210365715606		[learning rate: 0.00071945]
	Learning Rate: 0.000719449
	LOSS [training: 0.9969210365715606 | validation: 2.6095436433051313]
	TIME [epoch: 8.47 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0011149612200831		[learning rate: 0.00071684]
	Learning Rate: 0.000716838
	LOSS [training: 1.0011149612200831 | validation: 2.6127074400219152]
	TIME [epoch: 8.49 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.00590830704621		[learning rate: 0.00071424]
	Learning Rate: 0.000714237
	LOSS [training: 1.00590830704621 | validation: 2.649716992128398]
	TIME [epoch: 8.48 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0036195845009774		[learning rate: 0.00071164]
	Learning Rate: 0.000711645
	LOSS [training: 1.0036195845009774 | validation: 2.6993139692904897]
	TIME [epoch: 8.47 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0072662052711245		[learning rate: 0.00070906]
	Learning Rate: 0.000709062
	LOSS [training: 1.0072662052711245 | validation: 2.5963584941580393]
	TIME [epoch: 8.47 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0246454378147263		[learning rate: 0.00070649]
	Learning Rate: 0.000706489
	LOSS [training: 1.0246454378147263 | validation: 2.6059491557513543]
	TIME [epoch: 8.49 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9950262232329792		[learning rate: 0.00070392]
	Learning Rate: 0.000703925
	LOSS [training: 0.9950262232329792 | validation: 2.646535073558039]
	TIME [epoch: 8.48 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.021960467580229		[learning rate: 0.00070137]
	Learning Rate: 0.00070137
	LOSS [training: 1.021960467580229 | validation: 2.7087459354570234]
	TIME [epoch: 8.47 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0185633214127832		[learning rate: 0.00069883]
	Learning Rate: 0.000698825
	LOSS [training: 1.0185633214127832 | validation: 2.6895530046338436]
	TIME [epoch: 8.48 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0042511877468745		[learning rate: 0.00069629]
	Learning Rate: 0.000696289
	LOSS [training: 1.0042511877468745 | validation: 2.6114372909170935]
	TIME [epoch: 8.5 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.00594012339655		[learning rate: 0.00069376]
	Learning Rate: 0.000693762
	LOSS [training: 1.00594012339655 | validation: 2.638572569320356]
	TIME [epoch: 8.48 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0012973131225693		[learning rate: 0.00069124]
	Learning Rate: 0.000691244
	LOSS [training: 1.0012973131225693 | validation: 2.7244809180591147]
	TIME [epoch: 8.47 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.049759121975701		[learning rate: 0.00068874]
	Learning Rate: 0.000688736
	LOSS [training: 1.049759121975701 | validation: 2.6082596994515894]
	TIME [epoch: 8.48 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9922744913649069		[learning rate: 0.00068624]
	Learning Rate: 0.000686236
	LOSS [training: 0.9922744913649069 | validation: 2.6692537165416774]
	TIME [epoch: 8.49 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0102134073026072		[learning rate: 0.00068375]
	Learning Rate: 0.000683746
	LOSS [training: 1.0102134073026072 | validation: 2.5935253546644574]
	TIME [epoch: 8.47 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9932957959461112		[learning rate: 0.00068126]
	Learning Rate: 0.000681265
	LOSS [training: 0.9932957959461112 | validation: 2.6460241572140033]
	TIME [epoch: 8.47 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9960005534358274		[learning rate: 0.00067879]
	Learning Rate: 0.000678792
	LOSS [training: 0.9960005534358274 | validation: 2.6181814586227734]
	TIME [epoch: 8.46 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0051939900324172		[learning rate: 0.00067633]
	Learning Rate: 0.000676329
	LOSS [training: 1.0051939900324172 | validation: 2.5908068673724243]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_841.pth
	Model improved!!!
EPOCH 842/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9934018550018424		[learning rate: 0.00067387]
	Learning Rate: 0.000673874
	LOSS [training: 0.9934018550018424 | validation: 2.6525554998138077]
	TIME [epoch: 8.49 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0066051567134475		[learning rate: 0.00067143]
	Learning Rate: 0.000671429
	LOSS [training: 1.0066051567134475 | validation: 2.6775007267196242]
	TIME [epoch: 8.47 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0035828423161008		[learning rate: 0.00066899]
	Learning Rate: 0.000668992
	LOSS [training: 1.0035828423161008 | validation: 2.608606102508359]
	TIME [epoch: 8.47 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0059635829170679		[learning rate: 0.00066656]
	Learning Rate: 0.000666564
	LOSS [training: 1.0059635829170679 | validation: 2.6197871870510583]
	TIME [epoch: 8.5 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9895752971158958		[learning rate: 0.00066415]
	Learning Rate: 0.000664145
	LOSS [training: 0.9895752971158958 | validation: 2.5933972903848255]
	TIME [epoch: 8.47 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9903090640701455		[learning rate: 0.00066174]
	Learning Rate: 0.000661735
	LOSS [training: 0.9903090640701455 | validation: 2.6003012551777562]
	TIME [epoch: 8.48 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0119957016075178		[learning rate: 0.00065933]
	Learning Rate: 0.000659334
	LOSS [training: 1.0119957016075178 | validation: 2.6426297641973555]
	TIME [epoch: 8.48 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0035553695654043		[learning rate: 0.00065694]
	Learning Rate: 0.000656941
	LOSS [training: 1.0035553695654043 | validation: 2.7126419693087835]
	TIME [epoch: 8.5 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.013381744446503		[learning rate: 0.00065456]
	Learning Rate: 0.000654557
	LOSS [training: 1.013381744446503 | validation: 2.608498137984098]
	TIME [epoch: 8.48 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.007199341079903		[learning rate: 0.00065218]
	Learning Rate: 0.000652182
	LOSS [training: 1.007199341079903 | validation: 2.628310787504552]
	TIME [epoch: 8.48 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.007180761186295		[learning rate: 0.00064981]
	Learning Rate: 0.000649815
	LOSS [training: 1.007180761186295 | validation: 2.6131869945681654]
	TIME [epoch: 8.47 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9854922647645795		[learning rate: 0.00064746]
	Learning Rate: 0.000647456
	LOSS [training: 0.9854922647645795 | validation: 2.615890965053824]
	TIME [epoch: 8.49 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0068895547194163		[learning rate: 0.00064511]
	Learning Rate: 0.000645107
	LOSS [training: 1.0068895547194163 | validation: 2.5956200805791543]
	TIME [epoch: 8.48 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9897858839551097		[learning rate: 0.00064277]
	Learning Rate: 0.000642766
	LOSS [training: 0.9897858839551097 | validation: 2.6705734683048297]
	TIME [epoch: 8.48 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0039940558266927		[learning rate: 0.00064043]
	Learning Rate: 0.000640433
	LOSS [training: 1.0039940558266927 | validation: 2.621830527259901]
	TIME [epoch: 8.48 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.982021311039912		[learning rate: 0.00063811]
	Learning Rate: 0.000638109
	LOSS [training: 0.982021311039912 | validation: 2.6062650647653167]
	TIME [epoch: 8.5 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0013671918192497		[learning rate: 0.00063579]
	Learning Rate: 0.000635793
	LOSS [training: 1.0013671918192497 | validation: 2.5926392671489897]
	TIME [epoch: 8.49 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9998782385328286		[learning rate: 0.00063349]
	Learning Rate: 0.000633486
	LOSS [training: 0.9998782385328286 | validation: 2.6405610881168613]
	TIME [epoch: 8.47 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0141475801494353		[learning rate: 0.00063119]
	Learning Rate: 0.000631187
	LOSS [training: 1.0141475801494353 | validation: 2.605058140934052]
	TIME [epoch: 8.47 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0071105351797636		[learning rate: 0.0006289]
	Learning Rate: 0.000628896
	LOSS [training: 1.0071105351797636 | validation: 2.6614469102723133]
	TIME [epoch: 8.48 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0020269206566297		[learning rate: 0.00062661]
	Learning Rate: 0.000626614
	LOSS [training: 1.0020269206566297 | validation: 2.679978109457042]
	TIME [epoch: 8.47 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0022161816220572		[learning rate: 0.00062434]
	Learning Rate: 0.00062434
	LOSS [training: 1.0022161816220572 | validation: 2.5923883580007594]
	TIME [epoch: 8.46 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9903713621325819		[learning rate: 0.00062207]
	Learning Rate: 0.000622074
	LOSS [training: 0.9903713621325819 | validation: 2.5970076810619784]
	TIME [epoch: 8.49 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9906079495954405		[learning rate: 0.00061982]
	Learning Rate: 0.000619816
	LOSS [training: 0.9906079495954405 | validation: 2.657013926456937]
	TIME [epoch: 8.47 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9954848458834238		[learning rate: 0.00061757]
	Learning Rate: 0.000617567
	LOSS [training: 0.9954848458834238 | validation: 2.590862338018504]
	TIME [epoch: 8.46 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0040205537299458		[learning rate: 0.00061533]
	Learning Rate: 0.000615326
	LOSS [training: 1.0040205537299458 | validation: 2.6141072483306713]
	TIME [epoch: 8.47 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0207054547412224		[learning rate: 0.00061309]
	Learning Rate: 0.000613093
	LOSS [training: 1.0207054547412224 | validation: 2.607427001535659]
	TIME [epoch: 8.49 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0073571298926984		[learning rate: 0.00061087]
	Learning Rate: 0.000610868
	LOSS [training: 1.0073571298926984 | validation: 2.6456442252734838]
	TIME [epoch: 8.48 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9990824395563129		[learning rate: 0.00060865]
	Learning Rate: 0.000608651
	LOSS [training: 0.9990824395563129 | validation: 2.6374002696568652]
	TIME [epoch: 8.47 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.009636263877378		[learning rate: 0.00060644]
	Learning Rate: 0.000606442
	LOSS [training: 1.009636263877378 | validation: 2.640926052973601]
	TIME [epoch: 8.48 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9999475847780767		[learning rate: 0.00060424]
	Learning Rate: 0.000604242
	LOSS [training: 0.9999475847780767 | validation: 2.6531092440624797]
	TIME [epoch: 8.49 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0206055489519508		[learning rate: 0.00060205]
	Learning Rate: 0.000602049
	LOSS [training: 1.0206055489519508 | validation: 2.653560216798671]
	TIME [epoch: 8.48 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0181804065529185		[learning rate: 0.00059986]
	Learning Rate: 0.000599864
	LOSS [training: 1.0181804065529185 | validation: 2.63815078607284]
	TIME [epoch: 8.48 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9887579458466018		[learning rate: 0.00059769]
	Learning Rate: 0.000597687
	LOSS [training: 0.9887579458466018 | validation: 2.5993113257557874]
	TIME [epoch: 8.46 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9976517733216769		[learning rate: 0.00059552]
	Learning Rate: 0.000595518
	LOSS [training: 0.9976517733216769 | validation: 2.626950077950418]
	TIME [epoch: 8.49 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.998540814287647		[learning rate: 0.00059336]
	Learning Rate: 0.000593357
	LOSS [training: 0.998540814287647 | validation: 2.691185773675278]
	TIME [epoch: 8.47 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9954109524279671		[learning rate: 0.0005912]
	Learning Rate: 0.000591203
	LOSS [training: 0.9954109524279671 | validation: 2.6034982630871566]
	TIME [epoch: 8.46 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0149324530037283		[learning rate: 0.00058906]
	Learning Rate: 0.000589058
	LOSS [training: 1.0149324530037283 | validation: 2.701765493410562]
	TIME [epoch: 8.47 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9943522951631278		[learning rate: 0.00058692]
	Learning Rate: 0.00058692
	LOSS [training: 0.9943522951631278 | validation: 2.601156391138607]
	TIME [epoch: 8.49 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9887762650939843		[learning rate: 0.00058479]
	Learning Rate: 0.00058479
	LOSS [training: 0.9887762650939843 | validation: 2.6159036075963615]
	TIME [epoch: 8.47 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9986163511835114		[learning rate: 0.00058267]
	Learning Rate: 0.000582668
	LOSS [training: 0.9986163511835114 | validation: 2.6069450926200193]
	TIME [epoch: 8.47 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9830280313756644		[learning rate: 0.00058055]
	Learning Rate: 0.000580553
	LOSS [training: 0.9830280313756644 | validation: 2.614315345449912]
	TIME [epoch: 8.47 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0029697038659122		[learning rate: 0.00057845]
	Learning Rate: 0.000578446
	LOSS [training: 1.0029697038659122 | validation: 2.597992015420347]
	TIME [epoch: 8.49 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0105944964185758		[learning rate: 0.00057635]
	Learning Rate: 0.000576347
	LOSS [training: 1.0105944964185758 | validation: 2.68253519639214]
	TIME [epoch: 8.48 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.001708736888989		[learning rate: 0.00057426]
	Learning Rate: 0.000574256
	LOSS [training: 1.001708736888989 | validation: 2.6448475936062588]
	TIME [epoch: 8.47 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9939261942848553		[learning rate: 0.00057217]
	Learning Rate: 0.000572172
	LOSS [training: 0.9939261942848553 | validation: 2.59417824859644]
	TIME [epoch: 8.48 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.030559369339185		[learning rate: 0.0005701]
	Learning Rate: 0.000570095
	LOSS [training: 1.030559369339185 | validation: 2.6226844630107635]
	TIME [epoch: 8.5 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9932982429846267		[learning rate: 0.00056803]
	Learning Rate: 0.000568026
	LOSS [training: 0.9932982429846267 | validation: 2.6020673022551604]
	TIME [epoch: 8.47 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9876999540379167		[learning rate: 0.00056596]
	Learning Rate: 0.000565965
	LOSS [training: 0.9876999540379167 | validation: 2.6033969962817647]
	TIME [epoch: 8.47 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0065918527354616		[learning rate: 0.00056391]
	Learning Rate: 0.000563911
	LOSS [training: 1.0065918527354616 | validation: 2.646053376272167]
	TIME [epoch: 8.47 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9860887510165259		[learning rate: 0.00056186]
	Learning Rate: 0.000561864
	LOSS [training: 0.9860887510165259 | validation: 2.6065681091555875]
	TIME [epoch: 8.49 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.984197812318183		[learning rate: 0.00055983]
	Learning Rate: 0.000559825
	LOSS [training: 0.984197812318183 | validation: 2.604855842086029]
	TIME [epoch: 8.46 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9928816794004544		[learning rate: 0.00055779]
	Learning Rate: 0.000557794
	LOSS [training: 0.9928816794004544 | validation: 2.601154611208905]
	TIME [epoch: 8.46 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9908193746038128		[learning rate: 0.00055577]
	Learning Rate: 0.00055577
	LOSS [training: 0.9908193746038128 | validation: 2.5970654919479044]
	TIME [epoch: 8.47 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9882942068094469		[learning rate: 0.00055375]
	Learning Rate: 0.000553753
	LOSS [training: 0.9882942068094469 | validation: 2.5864267736046633]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_896.pth
	Model improved!!!
EPOCH 897/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9850629252848503		[learning rate: 0.00055174]
	Learning Rate: 0.000551743
	LOSS [training: 0.9850629252848503 | validation: 2.6491863141526464]
	TIME [epoch: 8.48 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9897734340550061		[learning rate: 0.00054974]
	Learning Rate: 0.000549741
	LOSS [training: 0.9897734340550061 | validation: 2.5982971802381543]
	TIME [epoch: 8.46 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9899875949842969		[learning rate: 0.00054775]
	Learning Rate: 0.000547746
	LOSS [training: 0.9899875949842969 | validation: 2.602833021879026]
	TIME [epoch: 8.46 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0065844906776689		[learning rate: 0.00054576]
	Learning Rate: 0.000545758
	LOSS [training: 1.0065844906776689 | validation: 2.6101672322575573]
	TIME [epoch: 8.49 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.001289667074238		[learning rate: 0.00054378]
	Learning Rate: 0.000543777
	LOSS [training: 1.001289667074238 | validation: 2.635738613234717]
	TIME [epoch: 8.46 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.991943405288778		[learning rate: 0.0005418]
	Learning Rate: 0.000541804
	LOSS [training: 0.991943405288778 | validation: 2.603423390623073]
	TIME [epoch: 8.47 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9856441944254307		[learning rate: 0.00053984]
	Learning Rate: 0.000539838
	LOSS [training: 0.9856441944254307 | validation: 2.620114680486981]
	TIME [epoch: 8.47 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.008913111170276		[learning rate: 0.00053788]
	Learning Rate: 0.000537879
	LOSS [training: 1.008913111170276 | validation: 2.599405293640698]
	TIME [epoch: 8.49 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9981973468961158		[learning rate: 0.00053593]
	Learning Rate: 0.000535926
	LOSS [training: 0.9981973468961158 | validation: 2.703485585177149]
	TIME [epoch: 8.47 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0276362184118004		[learning rate: 0.00053398]
	Learning Rate: 0.000533982
	LOSS [training: 1.0276362184118004 | validation: 2.6153490748127184]
	TIME [epoch: 8.48 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0074169831077084		[learning rate: 0.00053204]
	Learning Rate: 0.000532044
	LOSS [training: 1.0074169831077084 | validation: 2.637708282431417]
	TIME [epoch: 8.48 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0343258443361572		[learning rate: 0.00053011]
	Learning Rate: 0.000530113
	LOSS [training: 1.0343258443361572 | validation: 2.5966237979242197]
	TIME [epoch: 8.47 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.983761466272601		[learning rate: 0.00052819]
	Learning Rate: 0.000528189
	LOSS [training: 0.983761466272601 | validation: 2.6004751355836637]
	TIME [epoch: 8.47 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0031116647407023		[learning rate: 0.00052627]
	Learning Rate: 0.000526272
	LOSS [training: 1.0031116647407023 | validation: 2.603539301686026]
	TIME [epoch: 8.46 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9789704139918809		[learning rate: 0.00052436]
	Learning Rate: 0.000524362
	LOSS [training: 0.9789704139918809 | validation: 2.5964374587357884]
	TIME [epoch: 8.49 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9947477752624139		[learning rate: 0.00052246]
	Learning Rate: 0.00052246
	LOSS [training: 0.9947477752624139 | validation: 2.5969476993513156]
	TIME [epoch: 8.48 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9955432100355504		[learning rate: 0.00052056]
	Learning Rate: 0.000520563
	LOSS [training: 0.9955432100355504 | validation: 2.620522654630317]
	TIME [epoch: 8.46 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9924581387199327		[learning rate: 0.00051867]
	Learning Rate: 0.000518674
	LOSS [training: 0.9924581387199327 | validation: 2.5909474642052395]
	TIME [epoch: 8.47 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9891855646194845		[learning rate: 0.00051679]
	Learning Rate: 0.000516792
	LOSS [training: 0.9891855646194845 | validation: 2.6123098690129014]
	TIME [epoch: 8.48 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9903437066371128		[learning rate: 0.00051492]
	Learning Rate: 0.000514917
	LOSS [training: 0.9903437066371128 | validation: 2.643422956781988]
	TIME [epoch: 8.47 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9896515878029566		[learning rate: 0.00051305]
	Learning Rate: 0.000513048
	LOSS [training: 0.9896515878029566 | validation: 2.607371489234627]
	TIME [epoch: 8.47 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0097789751042803		[learning rate: 0.00051119]
	Learning Rate: 0.000511186
	LOSS [training: 1.0097789751042803 | validation: 2.609134142369551]
	TIME [epoch: 8.47 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9944010800794333		[learning rate: 0.00050933]
	Learning Rate: 0.000509331
	LOSS [training: 0.9944010800794333 | validation: 2.591297218769905]
	TIME [epoch: 8.49 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9938667072826014		[learning rate: 0.00050748]
	Learning Rate: 0.000507483
	LOSS [training: 0.9938667072826014 | validation: 2.6001293228529265]
	TIME [epoch: 8.47 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9851813289463165		[learning rate: 0.00050564]
	Learning Rate: 0.000505641
	LOSS [training: 0.9851813289463165 | validation: 2.5888750235605125]
	TIME [epoch: 8.47 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9903070889476806		[learning rate: 0.00050381]
	Learning Rate: 0.000503806
	LOSS [training: 0.9903070889476806 | validation: 2.6480627927350286]
	TIME [epoch: 8.46 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9855352780068719		[learning rate: 0.00050198]
	Learning Rate: 0.000501977
	LOSS [training: 0.9855352780068719 | validation: 2.582640690457443]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_923.pth
	Model improved!!!
EPOCH 924/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9978248686268655		[learning rate: 0.00050016]
	Learning Rate: 0.000500156
	LOSS [training: 0.9978248686268655 | validation: 2.6015762400307896]
	TIME [epoch: 8.51 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9898054325897346		[learning rate: 0.00049834]
	Learning Rate: 0.000498341
	LOSS [training: 0.9898054325897346 | validation: 2.6160461406849924]
	TIME [epoch: 8.48 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9849650531790637		[learning rate: 0.00049653]
	Learning Rate: 0.000496532
	LOSS [training: 0.9849650531790637 | validation: 2.600573899821897]
	TIME [epoch: 8.48 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9836823291261171		[learning rate: 0.00049473]
	Learning Rate: 0.00049473
	LOSS [training: 0.9836823291261171 | validation: 2.62758270032034]
	TIME [epoch: 8.5 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9801702503718636		[learning rate: 0.00049293]
	Learning Rate: 0.000492935
	LOSS [training: 0.9801702503718636 | validation: 2.6127890178700346]
	TIME [epoch: 8.48 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0050238736012367		[learning rate: 0.00049115]
	Learning Rate: 0.000491146
	LOSS [training: 1.0050238736012367 | validation: 2.592945403900395]
	TIME [epoch: 8.48 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9856922752740334		[learning rate: 0.00048936]
	Learning Rate: 0.000489364
	LOSS [training: 0.9856922752740334 | validation: 2.60849081638401]
	TIME [epoch: 8.48 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0102686778484125		[learning rate: 0.00048759]
	Learning Rate: 0.000487588
	LOSS [training: 1.0102686778484125 | validation: 2.6288763502950525]
	TIME [epoch: 8.5 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9881677515482282		[learning rate: 0.00048582]
	Learning Rate: 0.000485818
	LOSS [training: 0.9881677515482282 | validation: 2.598757100718061]
	TIME [epoch: 8.48 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9825948890660751		[learning rate: 0.00048406]
	Learning Rate: 0.000484055
	LOSS [training: 0.9825948890660751 | validation: 2.6111593089043788]
	TIME [epoch: 8.48 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9892693643651137		[learning rate: 0.0004823]
	Learning Rate: 0.000482298
	LOSS [training: 0.9892693643651137 | validation: 2.632883429853975]
	TIME [epoch: 8.48 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9821931281283371		[learning rate: 0.00048055]
	Learning Rate: 0.000480548
	LOSS [training: 0.9821931281283371 | validation: 2.6609099467687214]
	TIME [epoch: 8.5 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.018475700411924		[learning rate: 0.0004788]
	Learning Rate: 0.000478804
	LOSS [training: 1.018475700411924 | validation: 2.5956582309431897]
	TIME [epoch: 8.48 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9864415213260159		[learning rate: 0.00047707]
	Learning Rate: 0.000477067
	LOSS [training: 0.9864415213260159 | validation: 2.615592696800562]
	TIME [epoch: 8.48 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9928907348755581		[learning rate: 0.00047534]
	Learning Rate: 0.000475335
	LOSS [training: 0.9928907348755581 | validation: 2.6062131835849924]
	TIME [epoch: 8.48 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9873232843958843		[learning rate: 0.00047361]
	Learning Rate: 0.00047361
	LOSS [training: 0.9873232843958843 | validation: 2.603270656683848]
	TIME [epoch: 8.51 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9894311094872785		[learning rate: 0.00047189]
	Learning Rate: 0.000471891
	LOSS [training: 0.9894311094872785 | validation: 2.5972100181277877]
	TIME [epoch: 8.48 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9807679605481866		[learning rate: 0.00047018]
	Learning Rate: 0.000470179
	LOSS [training: 0.9807679605481866 | validation: 2.622991033341925]
	TIME [epoch: 8.48 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9899340436353864		[learning rate: 0.00046847]
	Learning Rate: 0.000468473
	LOSS [training: 0.9899340436353864 | validation: 2.5956728367720174]
	TIME [epoch: 8.49 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9865507287183247		[learning rate: 0.00046677]
	Learning Rate: 0.000466773
	LOSS [training: 0.9865507287183247 | validation: 2.6008114479970033]
	TIME [epoch: 8.5 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9885913687606237		[learning rate: 0.00046508]
	Learning Rate: 0.000465079
	LOSS [training: 0.9885913687606237 | validation: 2.6027542634598695]
	TIME [epoch: 8.48 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.984120704008361		[learning rate: 0.00046339]
	Learning Rate: 0.000463391
	LOSS [training: 0.984120704008361 | validation: 2.5959402360726007]
	TIME [epoch: 8.49 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9881931821520074		[learning rate: 0.00046171]
	Learning Rate: 0.000461709
	LOSS [training: 0.9881931821520074 | validation: 2.6133589722767803]
	TIME [epoch: 8.5 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9822108847822074		[learning rate: 0.00046003]
	Learning Rate: 0.000460034
	LOSS [training: 0.9822108847822074 | validation: 2.5942405363822667]
	TIME [epoch: 8.49 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9941083671040403		[learning rate: 0.00045836]
	Learning Rate: 0.000458364
	LOSS [training: 0.9941083671040403 | validation: 2.635075440822933]
	TIME [epoch: 8.48 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0010204498262318		[learning rate: 0.0004567]
	Learning Rate: 0.000456701
	LOSS [training: 1.0010204498262318 | validation: 2.576360714133914]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_949.pth
	Model improved!!!
EPOCH 950/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9830565827963715		[learning rate: 0.00045504]
	Learning Rate: 0.000455043
	LOSS [training: 0.9830565827963715 | validation: 2.610324720429571]
	TIME [epoch: 8.5 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9902275238666407		[learning rate: 0.00045339]
	Learning Rate: 0.000453392
	LOSS [training: 0.9902275238666407 | validation: 2.6248960366405396]
	TIME [epoch: 8.49 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9870576431810655		[learning rate: 0.00045175]
	Learning Rate: 0.000451746
	LOSS [training: 0.9870576431810655 | validation: 2.599981439199916]
	TIME [epoch: 8.48 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9853480216253037		[learning rate: 0.00045011]
	Learning Rate: 0.000450107
	LOSS [training: 0.9853480216253037 | validation: 2.6051107639094586]
	TIME [epoch: 8.48 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9806991638597692		[learning rate: 0.00044847]
	Learning Rate: 0.000448474
	LOSS [training: 0.9806991638597692 | validation: 2.6040882682165782]
	TIME [epoch: 8.5 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.007287386764688		[learning rate: 0.00044685]
	Learning Rate: 0.000446846
	LOSS [training: 1.007287386764688 | validation: 2.6359455564772256]
	TIME [epoch: 8.48 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9878053888085769		[learning rate: 0.00044522]
	Learning Rate: 0.000445224
	LOSS [training: 0.9878053888085769 | validation: 2.5955117007736255]
	TIME [epoch: 8.48 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.003585567991459		[learning rate: 0.00044361]
	Learning Rate: 0.000443609
	LOSS [training: 1.003585567991459 | validation: 2.630051477374759]
	TIME [epoch: 8.48 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9874508714932893		[learning rate: 0.000442]
	Learning Rate: 0.000441999
	LOSS [training: 0.9874508714932893 | validation: 2.6108544703752004]
	TIME [epoch: 8.5 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9847483023323239		[learning rate: 0.00044039]
	Learning Rate: 0.000440395
	LOSS [training: 0.9847483023323239 | validation: 2.6017615351073244]
	TIME [epoch: 8.48 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9875888390165114		[learning rate: 0.0004388]
	Learning Rate: 0.000438797
	LOSS [training: 0.9875888390165114 | validation: 2.594787022310121]
	TIME [epoch: 8.48 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9968664717223288		[learning rate: 0.0004372]
	Learning Rate: 0.000437204
	LOSS [training: 0.9968664717223288 | validation: 2.6262565529996973]
	TIME [epoch: 8.47 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9792768209675493		[learning rate: 0.00043562]
	Learning Rate: 0.000435617
	LOSS [training: 0.9792768209675493 | validation: 2.602208830367628]
	TIME [epoch: 8.49 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9948097566135077		[learning rate: 0.00043404]
	Learning Rate: 0.000434037
	LOSS [training: 0.9948097566135077 | validation: 2.6029274610383504]
	TIME [epoch: 8.48 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9983064063234199		[learning rate: 0.00043246]
	Learning Rate: 0.000432461
	LOSS [training: 0.9983064063234199 | validation: 2.6101888494046417]
	TIME [epoch: 8.47 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.001707383690818		[learning rate: 0.00043089]
	Learning Rate: 0.000430892
	LOSS [training: 1.001707383690818 | validation: 2.5985642591907294]
	TIME [epoch: 8.47 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9890109841678694		[learning rate: 0.00042933]
	Learning Rate: 0.000429328
	LOSS [training: 0.9890109841678694 | validation: 2.605695758253647]
	TIME [epoch: 8.5 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9955398505641003		[learning rate: 0.00042777]
	Learning Rate: 0.00042777
	LOSS [training: 0.9955398505641003 | validation: 2.6049050488324137]
	TIME [epoch: 8.48 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9826850381708588		[learning rate: 0.00042622]
	Learning Rate: 0.000426218
	LOSS [training: 0.9826850381708588 | validation: 2.601724707637153]
	TIME [epoch: 8.48 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9939049128893428		[learning rate: 0.00042467]
	Learning Rate: 0.000424671
	LOSS [training: 0.9939049128893428 | validation: 2.617121749366302]
	TIME [epoch: 8.48 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.989906200578312		[learning rate: 0.00042313]
	Learning Rate: 0.00042313
	LOSS [training: 0.989906200578312 | validation: 2.636253146190885]
	TIME [epoch: 8.49 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9901390122187038		[learning rate: 0.00042159]
	Learning Rate: 0.000421594
	LOSS [training: 0.9901390122187038 | validation: 2.6519703497706075]
	TIME [epoch: 8.48 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9945115343908129		[learning rate: 0.00042006]
	Learning Rate: 0.000420064
	LOSS [training: 0.9945115343908129 | validation: 2.6372874905112225]
	TIME [epoch: 8.48 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9910841404045673		[learning rate: 0.00041854]
	Learning Rate: 0.00041854
	LOSS [training: 0.9910841404045673 | validation: 2.5993297383839415]
	TIME [epoch: 8.48 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.97769600917668		[learning rate: 0.00041702]
	Learning Rate: 0.000417021
	LOSS [training: 0.97769600917668 | validation: 2.5795907041570256]
	TIME [epoch: 8.5 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9974997862934292		[learning rate: 0.00041551]
	Learning Rate: 0.000415508
	LOSS [training: 0.9974997862934292 | validation: 2.6453722294543773]
	TIME [epoch: 8.48 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9848768941478869		[learning rate: 0.000414]
	Learning Rate: 0.000414
	LOSS [training: 0.9848768941478869 | validation: 2.591643293590322]
	TIME [epoch: 8.47 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9931476089382871		[learning rate: 0.0004125]
	Learning Rate: 0.000412497
	LOSS [training: 0.9931476089382871 | validation: 2.610147697739524]
	TIME [epoch: 8.47 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9776486436863892		[learning rate: 0.000411]
	Learning Rate: 0.000411
	LOSS [training: 0.9776486436863892 | validation: 2.598150407689976]
	TIME [epoch: 8.5 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9810888615033118		[learning rate: 0.00040951]
	Learning Rate: 0.000409509
	LOSS [training: 0.9810888615033118 | validation: 2.6290788118967097]
	TIME [epoch: 8.47 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9794620926568494		[learning rate: 0.00040802]
	Learning Rate: 0.000408023
	LOSS [training: 0.9794620926568494 | validation: 2.597134938070872]
	TIME [epoch: 8.47 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9947467990715895		[learning rate: 0.00040654]
	Learning Rate: 0.000406542
	LOSS [training: 0.9947467990715895 | validation: 2.6047407304329075]
	TIME [epoch: 8.48 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9840020985702249		[learning rate: 0.00040507]
	Learning Rate: 0.000405067
	LOSS [training: 0.9840020985702249 | validation: 2.618449390089744]
	TIME [epoch: 8.49 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9834366647417433		[learning rate: 0.0004036]
	Learning Rate: 0.000403597
	LOSS [training: 0.9834366647417433 | validation: 2.6130293061864345]
	TIME [epoch: 8.47 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9825727234986715		[learning rate: 0.00040213]
	Learning Rate: 0.000402132
	LOSS [training: 0.9825727234986715 | validation: 2.6064189851888786]
	TIME [epoch: 8.48 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9797404712382558		[learning rate: 0.00040067]
	Learning Rate: 0.000400672
	LOSS [training: 0.9797404712382558 | validation: 2.620123854457918]
	TIME [epoch: 8.48 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9805064432643797		[learning rate: 0.00039922]
	Learning Rate: 0.000399218
	LOSS [training: 0.9805064432643797 | validation: 2.597469786886689]
	TIME [epoch: 8.49 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9851587336940598		[learning rate: 0.00039777]
	Learning Rate: 0.00039777
	LOSS [training: 0.9851587336940598 | validation: 2.6008212232121237]
	TIME [epoch: 8.48 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9899306728750294		[learning rate: 0.00039633]
	Learning Rate: 0.000396326
	LOSS [training: 0.9899306728750294 | validation: 2.6316331212770603]
	TIME [epoch: 8.47 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9917422075429119		[learning rate: 0.00039489]
	Learning Rate: 0.000394888
	LOSS [training: 0.9917422075429119 | validation: 2.630230595929367]
	TIME [epoch: 8.49 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9886911750573493		[learning rate: 0.00039345]
	Learning Rate: 0.000393455
	LOSS [training: 0.9886911750573493 | validation: 2.589757310994406]
	TIME [epoch: 8.48 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9824996580380045		[learning rate: 0.00039203]
	Learning Rate: 0.000392027
	LOSS [training: 0.9824996580380045 | validation: 2.6078646550318547]
	TIME [epoch: 8.48 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9929460187320946		[learning rate: 0.0003906]
	Learning Rate: 0.000390604
	LOSS [training: 0.9929460187320946 | validation: 2.620273920344408]
	TIME [epoch: 8.47 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9958952723999813		[learning rate: 0.00038919]
	Learning Rate: 0.000389187
	LOSS [training: 0.9958952723999813 | validation: 2.5831136654511804]
	TIME [epoch: 8.49 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9855277014388655		[learning rate: 0.00038777]
	Learning Rate: 0.000387774
	LOSS [training: 0.9855277014388655 | validation: 2.5986915117927616]
	TIME [epoch: 8.48 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9752568022814156		[learning rate: 0.00038637]
	Learning Rate: 0.000386367
	LOSS [training: 0.9752568022814156 | validation: 2.604460066912963]
	TIME [epoch: 8.47 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.995185043940535		[learning rate: 0.00038496]
	Learning Rate: 0.000384965
	LOSS [training: 0.995185043940535 | validation: 2.5947283647910715]
	TIME [epoch: 8.47 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9835923978727928		[learning rate: 0.00038357]
	Learning Rate: 0.000383568
	LOSS [training: 0.9835923978727928 | validation: 2.6061078337598116]
	TIME [epoch: 8.49 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9884476997736125		[learning rate: 0.00038218]
	Learning Rate: 0.000382176
	LOSS [training: 0.9884476997736125 | validation: 2.594848976955748]
	TIME [epoch: 8.48 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.976336106394978		[learning rate: 0.00038079]
	Learning Rate: 0.000380789
	LOSS [training: 0.976336106394978 | validation: 2.61837170388079]
	TIME [epoch: 8.47 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.980829919206014		[learning rate: 0.00037941]
	Learning Rate: 0.000379407
	LOSS [training: 0.980829919206014 | validation: 2.595348122836831]
	TIME [epoch: 8.48 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9899972446070592		[learning rate: 0.00037803]
	Learning Rate: 0.00037803
	LOSS [training: 0.9899972446070592 | validation: 2.5973612333691056]
	TIME [epoch: 8.49 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9742897947992774		[learning rate: 0.00037666]
	Learning Rate: 0.000376658
	LOSS [training: 0.9742897947992774 | validation: 2.595989511096333]
	TIME [epoch: 8.47 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.97815084889936		[learning rate: 0.00037529]
	Learning Rate: 0.000375291
	LOSS [training: 0.97815084889936 | validation: 2.612338071247808]
	TIME [epoch: 8.47 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9812506588645474		[learning rate: 0.00037393]
	Learning Rate: 0.000373929
	LOSS [training: 0.9812506588645474 | validation: 2.618887115222018]
	TIME [epoch: 8.47 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9907257992491882		[learning rate: 0.00037257]
	Learning Rate: 0.000372572
	LOSS [training: 0.9907257992491882 | validation: 2.594130240306345]
	TIME [epoch: 8.49 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9788031030340644		[learning rate: 0.00037122]
	Learning Rate: 0.00037122
	LOSS [training: 0.9788031030340644 | validation: 2.592213112189581]
	TIME [epoch: 8.47 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9858423564693926		[learning rate: 0.00036987]
	Learning Rate: 0.000369873
	LOSS [training: 0.9858423564693926 | validation: 2.6166227223537395]
	TIME [epoch: 8.47 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9793596516138067		[learning rate: 0.00036853]
	Learning Rate: 0.000368531
	LOSS [training: 0.9793596516138067 | validation: 2.5945165389454488]
	TIME [epoch: 8.48 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9870569357490261		[learning rate: 0.00036719]
	Learning Rate: 0.000367193
	LOSS [training: 0.9870569357490261 | validation: 2.6008291658132157]
	TIME [epoch: 8.49 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.978895907605048		[learning rate: 0.00036586]
	Learning Rate: 0.000365861
	LOSS [training: 0.978895907605048 | validation: 2.623828123043059]
	TIME [epoch: 8.49 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.980844324788875		[learning rate: 0.00036453]
	Learning Rate: 0.000364533
	LOSS [training: 0.980844324788875 | validation: 2.5936673918171667]
	TIME [epoch: 8.47 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9769932462563986		[learning rate: 0.00036321]
	Learning Rate: 0.00036321
	LOSS [training: 0.9769932462563986 | validation: 2.6099787797882614]
	TIME [epoch: 8.48 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9810868456097296		[learning rate: 0.00036189]
	Learning Rate: 0.000361892
	LOSS [training: 0.9810868456097296 | validation: 2.613713320501725]
	TIME [epoch: 8.5 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9700072987651002		[learning rate: 0.00036058]
	Learning Rate: 0.000360579
	LOSS [training: 0.9700072987651002 | validation: 2.6133428972235637]
	TIME [epoch: 8.48 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9787700881768385		[learning rate: 0.00035927]
	Learning Rate: 0.00035927
	LOSS [training: 0.9787700881768385 | validation: 2.6630797105038213]
	TIME [epoch: 8.48 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9892298875479735		[learning rate: 0.00035797]
	Learning Rate: 0.000357966
	LOSS [training: 0.9892298875479735 | validation: 2.5878966185784167]
	TIME [epoch: 8.48 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9809805836588869		[learning rate: 0.00035667]
	Learning Rate: 0.000356667
	LOSS [training: 0.9809805836588869 | validation: 2.6026762218886237]
	TIME [epoch: 8.5 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9821405478719019		[learning rate: 0.00035537]
	Learning Rate: 0.000355373
	LOSS [training: 0.9821405478719019 | validation: 2.593651520823082]
	TIME [epoch: 8.48 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9832685836708894		[learning rate: 0.00035408]
	Learning Rate: 0.000354083
	LOSS [training: 0.9832685836708894 | validation: 2.6001975198694076]
	TIME [epoch: 8.47 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9756508547643552		[learning rate: 0.0003528]
	Learning Rate: 0.000352798
	LOSS [training: 0.9756508547643552 | validation: 2.6383394202379264]
	TIME [epoch: 8.47 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.976804468727188		[learning rate: 0.00035152]
	Learning Rate: 0.000351518
	LOSS [training: 0.976804468727188 | validation: 2.5936778791336423]
	TIME [epoch: 8.5 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.987004447822072		[learning rate: 0.00035024]
	Learning Rate: 0.000350242
	LOSS [training: 0.987004447822072 | validation: 2.5925053068780928]
	TIME [epoch: 8.47 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9774006024810887		[learning rate: 0.00034897]
	Learning Rate: 0.000348971
	LOSS [training: 0.9774006024810887 | validation: 2.5975049905625145]
	TIME [epoch: 8.47 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9782079397161281		[learning rate: 0.0003477]
	Learning Rate: 0.000347705
	LOSS [training: 0.9782079397161281 | validation: 2.592458835869992]
	TIME [epoch: 8.48 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.980153770266665		[learning rate: 0.00034644]
	Learning Rate: 0.000346443
	LOSS [training: 0.980153770266665 | validation: 2.674798545540375]
	TIME [epoch: 8.49 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9863559334742862		[learning rate: 0.00034519]
	Learning Rate: 0.000345186
	LOSS [training: 0.9863559334742862 | validation: 2.619896921378436]
	TIME [epoch: 8.48 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9715340674040167		[learning rate: 0.00034393]
	Learning Rate: 0.000343933
	LOSS [training: 0.9715340674040167 | validation: 2.5945854519569647]
	TIME [epoch: 8.47 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9897378782895692		[learning rate: 0.00034268]
	Learning Rate: 0.000342685
	LOSS [training: 0.9897378782895692 | validation: 2.5928789300182364]
	TIME [epoch: 8.48 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9919316071712647		[learning rate: 0.00034144]
	Learning Rate: 0.000341441
	LOSS [training: 0.9919316071712647 | validation: 2.599954473984969]
	TIME [epoch: 8.48 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9746994592585002		[learning rate: 0.0003402]
	Learning Rate: 0.000340202
	LOSS [training: 0.9746994592585002 | validation: 2.606068807431412]
	TIME [epoch: 8.47 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9749484096720572		[learning rate: 0.00033897]
	Learning Rate: 0.000338967
	LOSS [training: 0.9749484096720572 | validation: 2.5826194443854753]
	TIME [epoch: 8.47 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9739476637801966		[learning rate: 0.00033774]
	Learning Rate: 0.000337737
	LOSS [training: 0.9739476637801966 | validation: 2.609294268217808]
	TIME [epoch: 8.49 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9766023450048648		[learning rate: 0.00033651]
	Learning Rate: 0.000336512
	LOSS [training: 0.9766023450048648 | validation: 2.5904823893224216]
	TIME [epoch: 8.48 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9748736000519156		[learning rate: 0.00033529]
	Learning Rate: 0.00033529
	LOSS [training: 0.9748736000519156 | validation: 2.583060054826106]
	TIME [epoch: 8.47 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9835091543347824		[learning rate: 0.00033407]
	Learning Rate: 0.000334074
	LOSS [training: 0.9835091543347824 | validation: 2.608680040610181]
	TIME [epoch: 8.47 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9774699690383738		[learning rate: 0.00033286]
	Learning Rate: 0.000332861
	LOSS [training: 0.9774699690383738 | validation: 2.5984141674114425]
	TIME [epoch: 8.49 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9801643335806233		[learning rate: 0.00033165]
	Learning Rate: 0.000331653
	LOSS [training: 0.9801643335806233 | validation: 2.594902053483161]
	TIME [epoch: 8.48 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9823485491512847		[learning rate: 0.00033045]
	Learning Rate: 0.00033045
	LOSS [training: 0.9823485491512847 | validation: 2.596756646304308]
	TIME [epoch: 8.48 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9783555502684225		[learning rate: 0.00032925]
	Learning Rate: 0.00032925
	LOSS [training: 0.9783555502684225 | validation: 2.61181206765561]
	TIME [epoch: 8.47 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.981323726773401		[learning rate: 0.00032806]
	Learning Rate: 0.000328056
	LOSS [training: 0.981323726773401 | validation: 2.637751425645124]
	TIME [epoch: 8.49 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9846338072402607		[learning rate: 0.00032686]
	Learning Rate: 0.000326865
	LOSS [training: 0.9846338072402607 | validation: 2.5832375714062534]
	TIME [epoch: 8.48 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9753646946849649		[learning rate: 0.00032568]
	Learning Rate: 0.000325679
	LOSS [training: 0.9753646946849649 | validation: 2.595594954206645]
	TIME [epoch: 8.47 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9778626170444256		[learning rate: 0.0003245]
	Learning Rate: 0.000324497
	LOSS [training: 0.9778626170444256 | validation: 2.5968583019780622]
	TIME [epoch: 8.47 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.987643683350863		[learning rate: 0.00032332]
	Learning Rate: 0.000323319
	LOSS [training: 0.987643683350863 | validation: 2.613729426432424]
	TIME [epoch: 8.49 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9758216998492957		[learning rate: 0.00032215]
	Learning Rate: 0.000322146
	LOSS [training: 0.9758216998492957 | validation: 2.610319645076256]
	TIME [epoch: 8.48 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9927348821271526		[learning rate: 0.00032098]
	Learning Rate: 0.000320977
	LOSS [training: 0.9927348821271526 | validation: 2.6127143063474927]
	TIME [epoch: 8.47 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9716948282784585		[learning rate: 0.00031981]
	Learning Rate: 0.000319812
	LOSS [training: 0.9716948282784585 | validation: 2.5891182187687614]
	TIME [epoch: 8.47 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9810879643617108		[learning rate: 0.00031865]
	Learning Rate: 0.000318651
	LOSS [training: 0.9810879643617108 | validation: 2.572741448843794]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_1048.pth
	Model improved!!!
EPOCH 1049/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9840112847443979		[learning rate: 0.0003175]
	Learning Rate: 0.000317495
	LOSS [training: 0.9840112847443979 | validation: 2.5771606291402045]
	TIME [epoch: 8.47 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9830176466691963		[learning rate: 0.00031634]
	Learning Rate: 0.000316343
	LOSS [training: 0.9830176466691963 | validation: 2.6029797801562022]
	TIME [epoch: 8.46 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.989547977547287		[learning rate: 0.00031519]
	Learning Rate: 0.000315195
	LOSS [training: 0.989547977547287 | validation: 2.6131567165760434]
	TIME [epoch: 8.46 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9734852751652016		[learning rate: 0.00031405]
	Learning Rate: 0.000314051
	LOSS [training: 0.9734852751652016 | validation: 2.6098358571965594]
	TIME [epoch: 8.48 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9847080968944903		[learning rate: 0.00031291]
	Learning Rate: 0.000312911
	LOSS [training: 0.9847080968944903 | validation: 2.5905795802448237]
	TIME [epoch: 8.47 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9844389727641222		[learning rate: 0.00031178]
	Learning Rate: 0.000311776
	LOSS [training: 0.9844389727641222 | validation: 2.608485505870283]
	TIME [epoch: 8.46 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9706032630859474		[learning rate: 0.00031064]
	Learning Rate: 0.000310644
	LOSS [training: 0.9706032630859474 | validation: 2.6003764152960147]
	TIME [epoch: 8.46 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9819478348409383		[learning rate: 0.00030952]
	Learning Rate: 0.000309517
	LOSS [training: 0.9819478348409383 | validation: 2.5801202457517007]
	TIME [epoch: 8.49 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.971767271683847		[learning rate: 0.00030839]
	Learning Rate: 0.000308394
	LOSS [training: 0.971767271683847 | validation: 2.603863915253026]
	TIME [epoch: 8.46 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9651600864405377		[learning rate: 0.00030727]
	Learning Rate: 0.000307274
	LOSS [training: 0.9651600864405377 | validation: 2.583297295915423]
	TIME [epoch: 8.46 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9802021663872491		[learning rate: 0.00030616]
	Learning Rate: 0.000306159
	LOSS [training: 0.9802021663872491 | validation: 2.5853532634594982]
	TIME [epoch: 8.46 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9716359669331082		[learning rate: 0.00030505]
	Learning Rate: 0.000305048
	LOSS [training: 0.9716359669331082 | validation: 2.5866705217954253]
	TIME [epoch: 8.49 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9696398421231661		[learning rate: 0.00030394]
	Learning Rate: 0.000303941
	LOSS [training: 0.9696398421231661 | validation: 2.600340287996705]
	TIME [epoch: 8.47 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9745033668039442		[learning rate: 0.00030284]
	Learning Rate: 0.000302838
	LOSS [training: 0.9745033668039442 | validation: 2.5857250461003005]
	TIME [epoch: 8.47 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9689593258437019		[learning rate: 0.00030174]
	Learning Rate: 0.000301739
	LOSS [training: 0.9689593258437019 | validation: 2.5835650819105385]
	TIME [epoch: 8.47 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9800246187420992		[learning rate: 0.00030064]
	Learning Rate: 0.000300644
	LOSS [training: 0.9800246187420992 | validation: 2.592235325066171]
	TIME [epoch: 8.49 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9756955306371398		[learning rate: 0.00029955]
	Learning Rate: 0.000299553
	LOSS [training: 0.9756955306371398 | validation: 2.5938309422094963]
	TIME [epoch: 8.47 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9855562483120691		[learning rate: 0.00029847]
	Learning Rate: 0.000298466
	LOSS [training: 0.9855562483120691 | validation: 2.5967933810494612]
	TIME [epoch: 8.47 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9722633379853459		[learning rate: 0.00029738]
	Learning Rate: 0.000297383
	LOSS [training: 0.9722633379853459 | validation: 2.5805153823039606]
	TIME [epoch: 8.47 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9834195113834181		[learning rate: 0.0002963]
	Learning Rate: 0.000296304
	LOSS [training: 0.9834195113834181 | validation: 2.583869286286871]
	TIME [epoch: 8.49 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9753485240525503		[learning rate: 0.00029523]
	Learning Rate: 0.000295228
	LOSS [training: 0.9753485240525503 | validation: 2.5950833791084547]
	TIME [epoch: 8.46 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0029596327609516		[learning rate: 0.00029416]
	Learning Rate: 0.000294157
	LOSS [training: 1.0029596327609516 | validation: 2.618964243651683]
	TIME [epoch: 8.47 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9779236120958611		[learning rate: 0.00029309]
	Learning Rate: 0.000293089
	LOSS [training: 0.9779236120958611 | validation: 2.581053943756389]
	TIME [epoch: 8.48 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9775021014123768		[learning rate: 0.00029203]
	Learning Rate: 0.000292026
	LOSS [training: 0.9775021014123768 | validation: 2.608578432574989]
	TIME [epoch: 8.47 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9703326592353791		[learning rate: 0.00029097]
	Learning Rate: 0.000290966
	LOSS [training: 0.9703326592353791 | validation: 2.6025316886970167]
	TIME [epoch: 8.47 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9702364438355977		[learning rate: 0.00028991]
	Learning Rate: 0.00028991
	LOSS [training: 0.9702364438355977 | validation: 2.5935825056817015]
	TIME [epoch: 8.46 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9763886984600374		[learning rate: 0.00028886]
	Learning Rate: 0.000288858
	LOSS [training: 0.9763886984600374 | validation: 2.5791151905837353]
	TIME [epoch: 8.48 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9729382145530854		[learning rate: 0.00028781]
	Learning Rate: 0.00028781
	LOSS [training: 0.9729382145530854 | validation: 2.590886851571444]
	TIME [epoch: 8.47 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9736966802846568		[learning rate: 0.00028677]
	Learning Rate: 0.000286765
	LOSS [training: 0.9736966802846568 | validation: 2.5875009872596504]
	TIME [epoch: 8.47 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9781228443587274		[learning rate: 0.00028572]
	Learning Rate: 0.000285724
	LOSS [training: 0.9781228443587274 | validation: 2.60457850608643]
	TIME [epoch: 8.46 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9807502177468633		[learning rate: 0.00028469]
	Learning Rate: 0.000284688
	LOSS [training: 0.9807502177468633 | validation: 2.5972164295345275]
	TIME [epoch: 8.48 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9732960470757556		[learning rate: 0.00028365]
	Learning Rate: 0.000283654
	LOSS [training: 0.9732960470757556 | validation: 2.5924304363282684]
	TIME [epoch: 8.47 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9730463014397361		[learning rate: 0.00028263]
	Learning Rate: 0.000282625
	LOSS [training: 0.9730463014397361 | validation: 2.594030735235985]
	TIME [epoch: 8.46 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9844999759975976		[learning rate: 0.0002816]
	Learning Rate: 0.000281599
	LOSS [training: 0.9844999759975976 | validation: 2.5828889538738875]
	TIME [epoch: 8.46 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.975597692138107		[learning rate: 0.00028058]
	Learning Rate: 0.000280577
	LOSS [training: 0.975597692138107 | validation: 2.57515685355084]
	TIME [epoch: 8.48 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.973253928083356		[learning rate: 0.00027956]
	Learning Rate: 0.000279559
	LOSS [training: 0.973253928083356 | validation: 2.5814606648738083]
	TIME [epoch: 8.46 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9750229677885219		[learning rate: 0.00027854]
	Learning Rate: 0.000278545
	LOSS [training: 0.9750229677885219 | validation: 2.5804924910371305]
	TIME [epoch: 8.46 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9775091435420942		[learning rate: 0.00027753]
	Learning Rate: 0.000277534
	LOSS [training: 0.9775091435420942 | validation: 2.5809900326661217]
	TIME [epoch: 8.46 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9891161439787448		[learning rate: 0.00027653]
	Learning Rate: 0.000276527
	LOSS [training: 0.9891161439787448 | validation: 2.596571463699192]
	TIME [epoch: 8.48 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9817039796933258		[learning rate: 0.00027552]
	Learning Rate: 0.000275523
	LOSS [training: 0.9817039796933258 | validation: 2.6062069694650103]
	TIME [epoch: 8.46 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9658962640167212		[learning rate: 0.00027452]
	Learning Rate: 0.000274523
	LOSS [training: 0.9658962640167212 | validation: 2.5858587377297053]
	TIME [epoch: 8.46 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9726217411015614		[learning rate: 0.00027353]
	Learning Rate: 0.000273527
	LOSS [training: 0.9726217411015614 | validation: 2.5611391115213618]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r1_20240219_233648/states/model_tr_study203_1090.pth
	Model improved!!!
EPOCH 1091/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9748044876061372		[learning rate: 0.00027253]
	Learning Rate: 0.000272534
	LOSS [training: 0.9748044876061372 | validation: 2.6195080300322684]
	TIME [epoch: 8.48 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9851677882251361		[learning rate: 0.00027155]
	Learning Rate: 0.000271545
	LOSS [training: 0.9851677882251361 | validation: 2.6085864235815355]
	TIME [epoch: 8.46 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9699655360964314		[learning rate: 0.00027056]
	Learning Rate: 0.00027056
	LOSS [training: 0.9699655360964314 | validation: 2.5912879380355456]
	TIME [epoch: 8.46 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9684223994861505		[learning rate: 0.00026958]
	Learning Rate: 0.000269578
	LOSS [training: 0.9684223994861505 | validation: 2.5909700410010075]
	TIME [epoch: 8.45 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9771324499304269		[learning rate: 0.0002686]
	Learning Rate: 0.0002686
	LOSS [training: 0.9771324499304269 | validation: 2.580002538746127]
	TIME [epoch: 8.49 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9643912866276144		[learning rate: 0.00026762]
	Learning Rate: 0.000267625
	LOSS [training: 0.9643912866276144 | validation: 2.5874444619529005]
	TIME [epoch: 8.46 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9676275937988276		[learning rate: 0.00026665]
	Learning Rate: 0.000266654
	LOSS [training: 0.9676275937988276 | validation: 2.5801286556847756]
	TIME [epoch: 8.47 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9654474982223485		[learning rate: 0.00026569]
	Learning Rate: 0.000265686
	LOSS [training: 0.9654474982223485 | validation: 2.582320671521131]
	TIME [epoch: 8.47 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9862780850284706		[learning rate: 0.00026472]
	Learning Rate: 0.000264722
	LOSS [training: 0.9862780850284706 | validation: 2.5769514896838657]
	TIME [epoch: 8.48 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9754940825866447		[learning rate: 0.00026376]
	Learning Rate: 0.000263761
	LOSS [training: 0.9754940825866447 | validation: 2.591091073936832]
	TIME [epoch: 8.46 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9760541248817693		[learning rate: 0.0002628]
	Learning Rate: 0.000262804
	LOSS [training: 0.9760541248817693 | validation: 2.59556789291858]
	TIME [epoch: 8.47 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9691871539368819		[learning rate: 0.00026185]
	Learning Rate: 0.00026185
	LOSS [training: 0.9691871539368819 | validation: 2.5980890218119037]
	TIME [epoch: 8.46 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9747341247078796		[learning rate: 0.0002609]
	Learning Rate: 0.0002609
	LOSS [training: 0.9747341247078796 | validation: 2.5799274670461583]
	TIME [epoch: 8.48 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9717045932082062		[learning rate: 0.00025995]
	Learning Rate: 0.000259953
	LOSS [training: 0.9717045932082062 | validation: 2.593776341257003]
	TIME [epoch: 8.46 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9709883887823265		[learning rate: 0.00025901]
	Learning Rate: 0.00025901
	LOSS [training: 0.9709883887823265 | validation: 2.58602549181727]
	TIME [epoch: 8.46 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9742900349337213		[learning rate: 0.00025807]
	Learning Rate: 0.00025807
	LOSS [training: 0.9742900349337213 | validation: 2.592486243565457]
	TIME [epoch: 8.46 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9754666846972354		[learning rate: 0.00025713]
	Learning Rate: 0.000257133
	LOSS [training: 0.9754666846972354 | validation: 2.581631486713548]
	TIME [epoch: 8.48 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9764442871481253		[learning rate: 0.0002562]
	Learning Rate: 0.0002562
	LOSS [training: 0.9764442871481253 | validation: 2.601046590379664]
	TIME [epoch: 8.46 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9809103625606207		[learning rate: 0.00025527]
	Learning Rate: 0.00025527
	LOSS [training: 0.9809103625606207 | validation: 2.576752415641741]
	TIME [epoch: 8.46 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9677169119527083		[learning rate: 0.00025434]
	Learning Rate: 0.000254344
	LOSS [training: 0.9677169119527083 | validation: 2.5930889449891215]
	TIME [epoch: 8.47 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9751799146242165		[learning rate: 0.00025342]
	Learning Rate: 0.000253421
	LOSS [training: 0.9751799146242165 | validation: 2.5877481134145364]
	TIME [epoch: 8.48 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9679933893351539		[learning rate: 0.0002525]
	Learning Rate: 0.000252501
	LOSS [training: 0.9679933893351539 | validation: 2.590006295197105]
	TIME [epoch: 8.46 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.98230050419205		[learning rate: 0.00025158]
	Learning Rate: 0.000251585
	LOSS [training: 0.98230050419205 | validation: 2.5854319016237715]
	TIME [epoch: 8.47 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9690611238281098		[learning rate: 0.00025067]
	Learning Rate: 0.000250672
	LOSS [training: 0.9690611238281098 | validation: 2.607587775776009]
	TIME [epoch: 8.48 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9779080197973931		[learning rate: 0.00024976]
	Learning Rate: 0.000249762
	LOSS [training: 0.9779080197973931 | validation: 2.5992841449415587]
	TIME [epoch: 8.48 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9717380935598389		[learning rate: 0.00024886]
	Learning Rate: 0.000248856
	LOSS [training: 0.9717380935598389 | validation: 2.6148662469338655]
	TIME [epoch: 8.47 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9688039411216636		[learning rate: 0.00024795]
	Learning Rate: 0.000247952
	LOSS [training: 0.9688039411216636 | validation: 2.574163729039112]
	TIME [epoch: 8.46 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9834527887301405		[learning rate: 0.00024705]
	Learning Rate: 0.000247053
	LOSS [training: 0.9834527887301405 | validation: 2.59581481142769]
	TIME [epoch: 8.49 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9778229746771878		[learning rate: 0.00024616]
	Learning Rate: 0.000246156
	LOSS [training: 0.9778229746771878 | validation: 2.631289864434827]
	TIME [epoch: 8.48 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9913310621727438		[learning rate: 0.00024526]
	Learning Rate: 0.000245263
	LOSS [training: 0.9913310621727438 | validation: 2.5795288879382516]
	TIME [epoch: 8.47 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9684282267278086		[learning rate: 0.00024437]
	Learning Rate: 0.000244373
	LOSS [training: 0.9684282267278086 | validation: 2.5822739982549296]
	TIME [epoch: 8.46 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.968798183925041		[learning rate: 0.00024349]
	Learning Rate: 0.000243486
	LOSS [training: 0.968798183925041 | validation: 2.5797404282267986]
	TIME [epoch: 8.48 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9673051841043636		[learning rate: 0.0002426]
	Learning Rate: 0.000242602
	LOSS [training: 0.9673051841043636 | validation: 2.6194369356744938]
	TIME [epoch: 8.48 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9789116162389859		[learning rate: 0.00024172]
	Learning Rate: 0.000241722
	LOSS [training: 0.9789116162389859 | validation: 2.5852974436077254]
	TIME [epoch: 8.47 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9729326260979327		[learning rate: 0.00024084]
	Learning Rate: 0.000240845
	LOSS [training: 0.9729326260979327 | validation: 2.582592505284854]
	TIME [epoch: 8.47 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9707849665021895		[learning rate: 0.00023997]
	Learning Rate: 0.000239971
	LOSS [training: 0.9707849665021895 | validation: 2.5915731829377493]
	TIME [epoch: 8.5 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9733665238067857		[learning rate: 0.0002391]
	Learning Rate: 0.0002391
	LOSS [training: 0.9733665238067857 | validation: 2.591309159854459]
	TIME [epoch: 8.48 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9819161794790287		[learning rate: 0.00023823]
	Learning Rate: 0.000238232
	LOSS [training: 0.9819161794790287 | validation: 2.6207425513584255]
	TIME [epoch: 8.47 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9730648707935267		[learning rate: 0.00023737]
	Learning Rate: 0.000237367
	LOSS [training: 0.9730648707935267 | validation: 2.5907489685232292]
	TIME [epoch: 8.47 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9708493613490422		[learning rate: 0.00023651]
	Learning Rate: 0.000236506
	LOSS [training: 0.9708493613490422 | validation: 2.6100279303458884]
	TIME [epoch: 8.49 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9723951119780676		[learning rate: 0.00023565]
	Learning Rate: 0.000235648
	LOSS [training: 0.9723951119780676 | validation: 2.6124419092652005]
	TIME [epoch: 8.47 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9758932701042037		[learning rate: 0.00023479]
	Learning Rate: 0.000234793
	LOSS [training: 0.9758932701042037 | validation: 2.5862818189139065]
	TIME [epoch: 8.48 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9655305150623258		[learning rate: 0.00023394]
	Learning Rate: 0.00023394
	LOSS [training: 0.9655305150623258 | validation: 2.5952237864758825]
	TIME [epoch: 8.47 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9700581148859946		[learning rate: 0.00023309]
	Learning Rate: 0.000233091
	LOSS [training: 0.9700581148859946 | validation: 2.583716530914685]
	TIME [epoch: 8.49 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9683753156516375		[learning rate: 0.00023225]
	Learning Rate: 0.000232246
	LOSS [training: 0.9683753156516375 | validation: 2.593236424097374]
	TIME [epoch: 8.48 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9764124529703644		[learning rate: 0.0002314]
	Learning Rate: 0.000231403
	LOSS [training: 0.9764124529703644 | validation: 2.60335421359504]
	TIME [epoch: 8.47 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.978050127623017		[learning rate: 0.00023056]
	Learning Rate: 0.000230563
	LOSS [training: 0.978050127623017 | validation: 2.588685939189829]
	TIME [epoch: 8.48 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9603646331331213		[learning rate: 0.00022973]
	Learning Rate: 0.000229726
	LOSS [training: 0.9603646331331213 | validation: 2.596330705855729]
	TIME [epoch: 8.5 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9852443739543162		[learning rate: 0.00022889]
	Learning Rate: 0.000228893
	LOSS [training: 0.9852443739543162 | validation: 2.5930259949047483]
	TIME [epoch: 8.47 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9693781099596519		[learning rate: 0.00022806]
	Learning Rate: 0.000228062
	LOSS [training: 0.9693781099596519 | validation: 2.5839796262123844]
	TIME [epoch: 8.47 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9639259493448332		[learning rate: 0.00022723]
	Learning Rate: 0.000227234
	LOSS [training: 0.9639259493448332 | validation: 2.5875732790033648]
	TIME [epoch: 8.47 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9721286997294584		[learning rate: 0.00022641]
	Learning Rate: 0.00022641
	LOSS [training: 0.9721286997294584 | validation: 2.5725844003527225]
	TIME [epoch: 8.5 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9667894646939352		[learning rate: 0.00022559]
	Learning Rate: 0.000225588
	LOSS [training: 0.9667894646939352 | validation: 2.5917548160070165]
	TIME [epoch: 8.48 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9743769948248511		[learning rate: 0.00022477]
	Learning Rate: 0.000224769
	LOSS [training: 0.9743769948248511 | validation: 2.578530879756066]
	TIME [epoch: 8.47 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9867869847521493		[learning rate: 0.00022395]
	Learning Rate: 0.000223954
	LOSS [training: 0.9867869847521493 | validation: 2.5764267354577077]
	TIME [epoch: 8.48 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.961095436314997		[learning rate: 0.00022314]
	Learning Rate: 0.000223141
	LOSS [training: 0.961095436314997 | validation: 2.587283346387945]
	TIME [epoch: 8.5 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9730726211107757		[learning rate: 0.00022233]
	Learning Rate: 0.000222331
	LOSS [training: 0.9730726211107757 | validation: 2.580983734014354]
	TIME [epoch: 8.48 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9681291219176427		[learning rate: 0.00022152]
	Learning Rate: 0.000221524
	LOSS [training: 0.9681291219176427 | validation: 2.586179027977596]
	TIME [epoch: 8.48 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9720352607053335		[learning rate: 0.00022072]
	Learning Rate: 0.00022072
	LOSS [training: 0.9720352607053335 | validation: 2.57302911612962]
	TIME [epoch: 8.47 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9663313018727102		[learning rate: 0.00021992]
	Learning Rate: 0.000219919
	LOSS [training: 0.9663313018727102 | validation: 2.579821287683558]
	TIME [epoch: 8.5 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9748134472686774		[learning rate: 0.00021912]
	Learning Rate: 0.000219121
	LOSS [training: 0.9748134472686774 | validation: 2.58680913397031]
	TIME [epoch: 8.48 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9710526511214175		[learning rate: 0.00021833]
	Learning Rate: 0.000218326
	LOSS [training: 0.9710526511214175 | validation: 2.5811679494706388]
	TIME [epoch: 8.47 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9668172182141623		[learning rate: 0.00021753]
	Learning Rate: 0.000217534
	LOSS [training: 0.9668172182141623 | validation: 2.5851147015174765]
	TIME [epoch: 8.48 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9711386792361034		[learning rate: 0.00021674]
	Learning Rate: 0.000216744
	LOSS [training: 0.9711386792361034 | validation: 2.5958621154896813]
	TIME [epoch: 8.5 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9716112103248744		[learning rate: 0.00021596]
	Learning Rate: 0.000215958
	LOSS [training: 0.9716112103248744 | validation: 2.6027302325600328]
	TIME [epoch: 8.48 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.966911942189645		[learning rate: 0.00021517]
	Learning Rate: 0.000215174
	LOSS [training: 0.966911942189645 | validation: 2.615006318887879]
	TIME [epoch: 8.47 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9744766539012382		[learning rate: 0.00021439]
	Learning Rate: 0.000214393
	LOSS [training: 0.9744766539012382 | validation: 2.5881917125919243]
	TIME [epoch: 8.5 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9715385251747832		[learning rate: 0.00021361]
	Learning Rate: 0.000213615
	LOSS [training: 0.9715385251747832 | validation: 2.581596300795724]
	TIME [epoch: 8.49 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9713241855883579		[learning rate: 0.00021284]
	Learning Rate: 0.00021284
	LOSS [training: 0.9713241855883579 | validation: 2.5874804753892673]
	TIME [epoch: 8.47 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9695787162653635		[learning rate: 0.00021207]
	Learning Rate: 0.000212067
	LOSS [training: 0.9695787162653635 | validation: 2.5913752898925253]
	TIME [epoch: 8.47 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9704900549962241		[learning rate: 0.0002113]
	Learning Rate: 0.000211298
	LOSS [training: 0.9704900549962241 | validation: 2.6010422843912697]
	TIME [epoch: 8.48 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9725274483840364		[learning rate: 0.00021053]
	Learning Rate: 0.000210531
	LOSS [training: 0.9725274483840364 | validation: 2.593363015539706]
	TIME [epoch: 8.48 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9745892951883878		[learning rate: 0.00020977]
	Learning Rate: 0.000209767
	LOSS [training: 0.9745892951883878 | validation: 2.602999524462461]
	TIME [epoch: 8.48 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9707297381687752		[learning rate: 0.00020901]
	Learning Rate: 0.000209006
	LOSS [training: 0.9707297381687752 | validation: 2.5942729411550873]
	TIME [epoch: 8.47 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9711530403855786		[learning rate: 0.00020825]
	Learning Rate: 0.000208247
	LOSS [training: 0.9711530403855786 | validation: 2.585211551266606]
	TIME [epoch: 8.5 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9643203727539091		[learning rate: 0.00020749]
	Learning Rate: 0.000207491
	LOSS [training: 0.9643203727539091 | validation: 2.6032780478941167]
	TIME [epoch: 8.48 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9697594042148946		[learning rate: 0.00020674]
	Learning Rate: 0.000206738
	LOSS [training: 0.9697594042148946 | validation: 2.593565535379849]
	TIME [epoch: 8.47 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9686085855513623		[learning rate: 0.00020599]
	Learning Rate: 0.000205988
	LOSS [training: 0.9686085855513623 | validation: 2.622635363054573]
	TIME [epoch: 8.48 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.978825393421712		[learning rate: 0.00020524]
	Learning Rate: 0.000205241
	LOSS [training: 0.978825393421712 | validation: 2.5792750089645367]
	TIME [epoch: 8.49 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9823011279392958		[learning rate: 0.0002045]
	Learning Rate: 0.000204496
	LOSS [training: 0.9823011279392958 | validation: 2.582985407938262]
	TIME [epoch: 8.48 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9818069832231477		[learning rate: 0.00020375]
	Learning Rate: 0.000203754
	LOSS [training: 0.9818069832231477 | validation: 2.607752375222085]
	TIME [epoch: 8.47 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9671885373362406		[learning rate: 0.00020301]
	Learning Rate: 0.000203014
	LOSS [training: 0.9671885373362406 | validation: 2.575238079330907]
	TIME [epoch: 8.47 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.964936482742635		[learning rate: 0.00020228]
	Learning Rate: 0.000202277
	LOSS [training: 0.964936482742635 | validation: 2.5838736723199327]
	TIME [epoch: 8.49 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9838720956745958		[learning rate: 0.00020154]
	Learning Rate: 0.000201543
	LOSS [training: 0.9838720956745958 | validation: 2.569151622275888]
	TIME [epoch: 8.47 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9707783491935474		[learning rate: 0.00020081]
	Learning Rate: 0.000200812
	LOSS [training: 0.9707783491935474 | validation: 2.578782350186847]
	TIME [epoch: 8.47 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9692692362745511		[learning rate: 0.00020008]
	Learning Rate: 0.000200083
	LOSS [training: 0.9692692362745511 | validation: 2.6197420065323227]
	TIME [epoch: 8.47 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9707923337187754		[learning rate: 0.00019936]
	Learning Rate: 0.000199357
	LOSS [training: 0.9707923337187754 | validation: 2.589905231277731]
	TIME [epoch: 8.49 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.966610500840541		[learning rate: 0.00019863]
	Learning Rate: 0.000198634
	LOSS [training: 0.966610500840541 | validation: 2.591815618968002]
	TIME [epoch: 8.47 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9569135492735997		[learning rate: 0.00019791]
	Learning Rate: 0.000197913
	LOSS [training: 0.9569135492735997 | validation: 2.581198492583717]
	TIME [epoch: 8.47 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9660738583301548		[learning rate: 0.00019719]
	Learning Rate: 0.000197194
	LOSS [training: 0.9660738583301548 | validation: 2.613262550095154]
	TIME [epoch: 8.48 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9648185105941381		[learning rate: 0.00019648]
	Learning Rate: 0.000196479
	LOSS [training: 0.9648185105941381 | validation: 2.5832914374103]
	TIME [epoch: 8.49 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9621419018217093		[learning rate: 0.00019577]
	Learning Rate: 0.000195766
	LOSS [training: 0.9621419018217093 | validation: 2.5928591830421936]
	TIME [epoch: 8.48 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9839384857083051		[learning rate: 0.00019506]
	Learning Rate: 0.000195055
	LOSS [training: 0.9839384857083051 | validation: 2.5850374370680034]
	TIME [epoch: 8.46 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9697391499702374		[learning rate: 0.00019435]
	Learning Rate: 0.000194347
	LOSS [training: 0.9697391499702374 | validation: 2.5921879832284995]
	TIME [epoch: 8.48 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9635808901672351		[learning rate: 0.00019364]
	Learning Rate: 0.000193642
	LOSS [training: 0.9635808901672351 | validation: 2.5736669006524746]
	TIME [epoch: 8.49 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9707342368473346		[learning rate: 0.00019294]
	Learning Rate: 0.000192939
	LOSS [training: 0.9707342368473346 | validation: 2.6050803053507363]
	TIME [epoch: 8.47 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9797713786896741		[learning rate: 0.00019224]
	Learning Rate: 0.000192239
	LOSS [training: 0.9797713786896741 | validation: 2.59974673056373]
	TIME [epoch: 8.46 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.972006435647063		[learning rate: 0.00019154]
	Learning Rate: 0.000191542
	LOSS [training: 0.972006435647063 | validation: 2.5911923334489644]
	TIME [epoch: 8.47 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9695038275069934		[learning rate: 0.00019085]
	Learning Rate: 0.000190846
	LOSS [training: 0.9695038275069934 | validation: 2.602975043211483]
	TIME [epoch: 8.49 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9689505672973043		[learning rate: 0.00019015]
	Learning Rate: 0.000190154
	LOSS [training: 0.9689505672973043 | validation: 2.602089087042162]
	TIME [epoch: 8.47 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9701312100663019		[learning rate: 0.00018946]
	Learning Rate: 0.000189464
	LOSS [training: 0.9701312100663019 | validation: 2.584952180415156]
	TIME [epoch: 8.47 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9666584716335433		[learning rate: 0.00018878]
	Learning Rate: 0.000188776
	LOSS [training: 0.9666584716335433 | validation: 2.5818854667135467]
	TIME [epoch: 8.47 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9725907651575272		[learning rate: 0.00018809]
	Learning Rate: 0.000188091
	LOSS [training: 0.9725907651575272 | validation: 2.599781701855866]
	TIME [epoch: 8.49 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9761119215330046		[learning rate: 0.00018741]
	Learning Rate: 0.000187409
	LOSS [training: 0.9761119215330046 | validation: 2.591804809945698]
	TIME [epoch: 8.47 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9751387872456206		[learning rate: 0.00018673]
	Learning Rate: 0.000186729
	LOSS [training: 0.9751387872456206 | validation: 2.6076944905696795]
	TIME [epoch: 8.47 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.963619735056245		[learning rate: 0.00018605]
	Learning Rate: 0.000186051
	LOSS [training: 0.963619735056245 | validation: 2.5838929406100544]
	TIME [epoch: 8.48 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9729280212747596		[learning rate: 0.00018538]
	Learning Rate: 0.000185376
	LOSS [training: 0.9729280212747596 | validation: 2.5758552575935507]
	TIME [epoch: 8.49 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.957888401826929		[learning rate: 0.0001847]
	Learning Rate: 0.000184703
	LOSS [training: 0.957888401826929 | validation: 2.5809499331168526]
	TIME [epoch: 8.47 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9650135625153402		[learning rate: 0.00018403]
	Learning Rate: 0.000184033
	LOSS [training: 0.9650135625153402 | validation: 2.5863620493893884]
	TIME [epoch: 8.46 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9650487251371521		[learning rate: 0.00018336]
	Learning Rate: 0.000183365
	LOSS [training: 0.9650487251371521 | validation: 2.5862306033870572]
	TIME [epoch: 8.48 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.966101702314164		[learning rate: 0.0001827]
	Learning Rate: 0.000182699
	LOSS [training: 0.966101702314164 | validation: 2.5847803484585254]
	TIME [epoch: 8.48 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9712932695308172		[learning rate: 0.00018204]
	Learning Rate: 0.000182036
	LOSS [training: 0.9712932695308172 | validation: 2.589530851928652]
	TIME [epoch: 8.47 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9652763335964426		[learning rate: 0.00018138]
	Learning Rate: 0.000181376
	LOSS [training: 0.9652763335964426 | validation: 2.600835595626568]
	TIME [epoch: 8.47 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.966998212374172		[learning rate: 0.00018072]
	Learning Rate: 0.000180717
	LOSS [training: 0.966998212374172 | validation: 2.611975677082632]
	TIME [epoch: 8.48 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9625808033461031		[learning rate: 0.00018006]
	Learning Rate: 0.000180062
	LOSS [training: 0.9625808033461031 | validation: 2.589022755860199]
	TIME [epoch: 8.48 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9662709256263774		[learning rate: 0.00017941]
	Learning Rate: 0.000179408
	LOSS [training: 0.9662709256263774 | validation: 2.603507174837295]
	TIME [epoch: 8.47 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9722001170182331		[learning rate: 0.00017876]
	Learning Rate: 0.000178757
	LOSS [training: 0.9722001170182331 | validation: 2.5960086355037797]
	TIME [epoch: 8.47 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9717293270222654		[learning rate: 0.00017811]
	Learning Rate: 0.000178108
	LOSS [training: 0.9717293270222654 | validation: 2.6005894646713323]
	TIME [epoch: 8.48 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9706895957759991		[learning rate: 0.00017746]
	Learning Rate: 0.000177462
	LOSS [training: 0.9706895957759991 | validation: 2.586973556107618]
	TIME [epoch: 8.48 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9673032476641067		[learning rate: 0.00017682]
	Learning Rate: 0.000176818
	LOSS [training: 0.9673032476641067 | validation: 2.582562836683871]
	TIME [epoch: 8.47 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9701786416588292		[learning rate: 0.00017618]
	Learning Rate: 0.000176176
	LOSS [training: 0.9701786416588292 | validation: 2.5747230029462695]
	TIME [epoch: 8.47 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9650722186838279		[learning rate: 0.00017554]
	Learning Rate: 0.000175537
	LOSS [training: 0.9650722186838279 | validation: 2.602022118274941]
	TIME [epoch: 8.49 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9749597202035589		[learning rate: 0.0001749]
	Learning Rate: 0.0001749
	LOSS [training: 0.9749597202035589 | validation: 2.59754600515671]
	TIME [epoch: 8.46 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9692545143158584		[learning rate: 0.00017427]
	Learning Rate: 0.000174265
	LOSS [training: 0.9692545143158584 | validation: 2.5913728499621502]
	TIME [epoch: 8.47 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9679167594072794		[learning rate: 0.00017363]
	Learning Rate: 0.000173633
	LOSS [training: 0.9679167594072794 | validation: 2.5742429352272183]
	TIME [epoch: 8.47 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9638339090940832		[learning rate: 0.000173]
	Learning Rate: 0.000173003
	LOSS [training: 0.9638339090940832 | validation: 2.584608528679192]
	TIME [epoch: 8.49 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9741055498376452		[learning rate: 0.00017237]
	Learning Rate: 0.000172375
	LOSS [training: 0.9741055498376452 | validation: 2.5892042499938674]
	TIME [epoch: 8.47 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9665304478859259		[learning rate: 0.00017175]
	Learning Rate: 0.000171749
	LOSS [training: 0.9665304478859259 | validation: 2.591362916986896]
	TIME [epoch: 8.47 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9595997374987484		[learning rate: 0.00017113]
	Learning Rate: 0.000171126
	LOSS [training: 0.9595997374987484 | validation: 2.578075160567252]
	TIME [epoch: 8.47 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.964123519172204		[learning rate: 0.0001705]
	Learning Rate: 0.000170505
	LOSS [training: 0.964123519172204 | validation: 2.5881472427686214]
	TIME [epoch: 8.48 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9627142345680284		[learning rate: 0.00016989]
	Learning Rate: 0.000169886
	LOSS [training: 0.9627142345680284 | validation: 2.583873142912343]
	TIME [epoch: 8.47 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9641008184112845		[learning rate: 0.00016927]
	Learning Rate: 0.00016927
	LOSS [training: 0.9641008184112845 | validation: 2.57165958173954]
	TIME [epoch: 8.46 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.969186340109452		[learning rate: 0.00016866]
	Learning Rate: 0.000168655
	LOSS [training: 0.969186340109452 | validation: 2.602371951290774]
	TIME [epoch: 8.47 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9684304116936676		[learning rate: 0.00016804]
	Learning Rate: 0.000168043
	LOSS [training: 0.9684304116936676 | validation: 2.5880168725761807]
	TIME [epoch: 8.49 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9650456148951501		[learning rate: 0.00016743]
	Learning Rate: 0.000167433
	LOSS [training: 0.9650456148951501 | validation: 2.5961381755755086]
	TIME [epoch: 8.46 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9695672046691218		[learning rate: 0.00016683]
	Learning Rate: 0.000166826
	LOSS [training: 0.9695672046691218 | validation: 2.594581023968029]
	TIME [epoch: 8.48 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9788330514538265		[learning rate: 0.00016622]
	Learning Rate: 0.00016622
	LOSS [training: 0.9788330514538265 | validation: 2.58366206851297]
	TIME [epoch: 8.48 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9725787927587298		[learning rate: 0.00016562]
	Learning Rate: 0.000165617
	LOSS [training: 0.9725787927587298 | validation: 2.5932408833963256]
	TIME [epoch: 8.49 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9651834388819965		[learning rate: 0.00016502]
	Learning Rate: 0.000165016
	LOSS [training: 0.9651834388819965 | validation: 2.5761637836454536]
	TIME [epoch: 8.47 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9689414947814685		[learning rate: 0.00016442]
	Learning Rate: 0.000164417
	LOSS [training: 0.9689414947814685 | validation: 2.5930327633040533]
	TIME [epoch: 8.46 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9680364733468076		[learning rate: 0.00016382]
	Learning Rate: 0.000163821
	LOSS [training: 0.9680364733468076 | validation: 2.5685253941728545]
	TIME [epoch: 8.47 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9662041546563016		[learning rate: 0.00016323]
	Learning Rate: 0.000163226
	LOSS [training: 0.9662041546563016 | validation: 2.595652498813785]
	TIME [epoch: 8.49 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9608332495197877		[learning rate: 0.00016263]
	Learning Rate: 0.000162634
	LOSS [training: 0.9608332495197877 | validation: 2.596118103435224]
	TIME [epoch: 8.46 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9650837029746612		[learning rate: 0.00016204]
	Learning Rate: 0.000162043
	LOSS [training: 0.9650837029746612 | validation: 2.5930441118618113]
	TIME [epoch: 8.46 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9701061902142534		[learning rate: 0.00016146]
	Learning Rate: 0.000161455
	LOSS [training: 0.9701061902142534 | validation: 2.588098170544917]
	TIME [epoch: 8.46 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9678887250500157		[learning rate: 0.00016087]
	Learning Rate: 0.000160869
	LOSS [training: 0.9678887250500157 | validation: 2.589991223717123]
	TIME [epoch: 8.48 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9626846706179226		[learning rate: 0.00016029]
	Learning Rate: 0.000160286
	LOSS [training: 0.9626846706179226 | validation: 2.5869998177807134]
	TIME [epoch: 8.47 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9700039045645477		[learning rate: 0.0001597]
	Learning Rate: 0.000159704
	LOSS [training: 0.9700039045645477 | validation: 2.59357068213937]
	TIME [epoch: 8.47 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9646619871098722		[learning rate: 0.00015912]
	Learning Rate: 0.000159124
	LOSS [training: 0.9646619871098722 | validation: 2.5885009665876737]
	TIME [epoch: 8.47 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9704809965996193		[learning rate: 0.00015855]
	Learning Rate: 0.000158547
	LOSS [training: 0.9704809965996193 | validation: 2.585670564083626]
	TIME [epoch: 8.48 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9613779289167537		[learning rate: 0.00015797]
	Learning Rate: 0.000157972
	LOSS [training: 0.9613779289167537 | validation: 2.6053408398106432]
	TIME [epoch: 8.46 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9623853247431697		[learning rate: 0.0001574]
	Learning Rate: 0.000157398
	LOSS [training: 0.9623853247431697 | validation: 2.572215466821636]
	TIME [epoch: 8.47 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9700100066487906		[learning rate: 0.00015683]
	Learning Rate: 0.000156827
	LOSS [training: 0.9700100066487906 | validation: 2.572732434915244]
	TIME [epoch: 8.48 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9645042705469885		[learning rate: 0.00015626]
	Learning Rate: 0.000156258
	LOSS [training: 0.9645042705469885 | validation: 2.58835854029095]
	TIME [epoch: 8.47 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9681881501563545		[learning rate: 0.00015569]
	Learning Rate: 0.000155691
	LOSS [training: 0.9681881501563545 | validation: 2.572048290227901]
	TIME [epoch: 8.46 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9684249227465866		[learning rate: 0.00015513]
	Learning Rate: 0.000155126
	LOSS [training: 0.9684249227465866 | validation: 2.6466958114396912]
	TIME [epoch: 8.46 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9706377283272218		[learning rate: 0.00015456]
	Learning Rate: 0.000154563
	LOSS [training: 0.9706377283272218 | validation: 2.575387418122221]
	TIME [epoch: 8.48 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9699814506067806		[learning rate: 0.000154]
	Learning Rate: 0.000154002
	LOSS [training: 0.9699814506067806 | validation: 2.6033398372418324]
	TIME [epoch: 8.48 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9698939981201338		[learning rate: 0.00015344]
	Learning Rate: 0.000153443
	LOSS [training: 0.9698939981201338 | validation: 2.5816333920264563]
	TIME [epoch: 8.47 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.990348206364137		[learning rate: 0.00015289]
	Learning Rate: 0.000152886
	LOSS [training: 0.990348206364137 | validation: 2.6060811605129435]
	TIME [epoch: 8.46 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9610394538750299		[learning rate: 0.00015233]
	Learning Rate: 0.000152331
	LOSS [training: 0.9610394538750299 | validation: 2.5783095296383536]
	TIME [epoch: 8.48 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9789701974740112		[learning rate: 0.00015178]
	Learning Rate: 0.000151779
	LOSS [training: 0.9789701974740112 | validation: 2.574073359630481]
	TIME [epoch: 8.47 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.965355833453373		[learning rate: 0.00015123]
	Learning Rate: 0.000151228
	LOSS [training: 0.965355833453373 | validation: 2.58378757894523]
	TIME [epoch: 8.47 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9660160149889938		[learning rate: 0.00015068]
	Learning Rate: 0.000150679
	LOSS [training: 0.9660160149889938 | validation: 2.58713744266436]
	TIME [epoch: 8.46 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9664258202858701		[learning rate: 0.00015013]
	Learning Rate: 0.000150132
	LOSS [training: 0.9664258202858701 | validation: 2.6154975374182268]
	TIME [epoch: 8.48 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9671381107106786		[learning rate: 0.00014959]
	Learning Rate: 0.000149587
	LOSS [training: 0.9671381107106786 | validation: 2.5876037435912456]
	TIME [epoch: 8.46 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9612507270708722		[learning rate: 0.00014904]
	Learning Rate: 0.000149044
	LOSS [training: 0.9612507270708722 | validation: 2.5823285508271616]
	TIME [epoch: 8.47 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9682319362886209		[learning rate: 0.0001485]
	Learning Rate: 0.000148504
	LOSS [training: 0.9682319362886209 | validation: 2.6042426747372973]
	TIME [epoch: 8.47 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9658380941300788		[learning rate: 0.00014796]
	Learning Rate: 0.000147965
	LOSS [training: 0.9658380941300788 | validation: 2.5941079644933445]
	TIME [epoch: 8.48 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9712089390592519		[learning rate: 0.00014743]
	Learning Rate: 0.000147428
	LOSS [training: 0.9712089390592519 | validation: 2.5823997527218077]
	TIME [epoch: 8.46 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9656876821167366		[learning rate: 0.00014689]
	Learning Rate: 0.000146893
	LOSS [training: 0.9656876821167366 | validation: 2.593071074649362]
	TIME [epoch: 8.46 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9640333096603915		[learning rate: 0.00014636]
	Learning Rate: 0.00014636
	LOSS [training: 0.9640333096603915 | validation: 2.5785066705392046]
	TIME [epoch: 8.46 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9634209611839571		[learning rate: 0.00014583]
	Learning Rate: 0.000145828
	LOSS [training: 0.9634209611839571 | validation: 2.5755850216030542]
	TIME [epoch: 8.49 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9575256397217634		[learning rate: 0.0001453]
	Learning Rate: 0.000145299
	LOSS [training: 0.9575256397217634 | validation: 2.589446511788366]
	TIME [epoch: 8.47 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9614942863888626		[learning rate: 0.00014477]
	Learning Rate: 0.000144772
	LOSS [training: 0.9614942863888626 | validation: 2.5862202233357383]
	TIME [epoch: 8.47 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.964555574975166		[learning rate: 0.00014425]
	Learning Rate: 0.000144247
	LOSS [training: 0.964555574975166 | validation: 2.585684514986676]
	TIME [epoch: 8.47 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9636347845785863		[learning rate: 0.00014372]
	Learning Rate: 0.000143723
	LOSS [training: 0.9636347845785863 | validation: 2.589192881758186]
	TIME [epoch: 8.49 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9632869365418355		[learning rate: 0.0001432]
	Learning Rate: 0.000143201
	LOSS [training: 0.9632869365418355 | validation: 2.5878786432451495]
	TIME [epoch: 8.47 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9695875011515225		[learning rate: 0.00014268]
	Learning Rate: 0.000142682
	LOSS [training: 0.9695875011515225 | validation: 2.5764051904835013]
	TIME [epoch: 8.47 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9615494326659462		[learning rate: 0.00014216]
	Learning Rate: 0.000142164
	LOSS [training: 0.9615494326659462 | validation: 2.589557632841937]
	TIME [epoch: 8.47 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9611029133054332		[learning rate: 0.00014165]
	Learning Rate: 0.000141648
	LOSS [training: 0.9611029133054332 | validation: 2.583728704897444]
	TIME [epoch: 8.49 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9617181252964249		[learning rate: 0.00014113]
	Learning Rate: 0.000141134
	LOSS [training: 0.9617181252964249 | validation: 2.5784697147288003]
	TIME [epoch: 8.47 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9686481444522095		[learning rate: 0.00014062]
	Learning Rate: 0.000140622
	LOSS [training: 0.9686481444522095 | validation: 2.577942890155987]
	TIME [epoch: 8.46 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9664094540372362		[learning rate: 0.00014011]
	Learning Rate: 0.000140112
	LOSS [training: 0.9664094540372362 | validation: 2.5870170198513094]
	TIME [epoch: 8.47 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.967290126520993		[learning rate: 0.0001396]
	Learning Rate: 0.000139603
	LOSS [training: 0.967290126520993 | validation: 2.5845904268144997]
	TIME [epoch: 8.49 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9642386291498799		[learning rate: 0.0001391]
	Learning Rate: 0.000139096
	LOSS [training: 0.9642386291498799 | validation: 2.5707038831238167]
	TIME [epoch: 8.47 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9703121537155829		[learning rate: 0.00013859]
	Learning Rate: 0.000138592
	LOSS [training: 0.9703121537155829 | validation: 2.572279065116646]
	TIME [epoch: 8.47 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9701840011898366		[learning rate: 0.00013809]
	Learning Rate: 0.000138089
	LOSS [training: 0.9701840011898366 | validation: 2.628252069907484]
	TIME [epoch: 8.48 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9692590984205278		[learning rate: 0.00013759]
	Learning Rate: 0.000137587
	LOSS [training: 0.9692590984205278 | validation: 2.601757253733955]
	TIME [epoch: 8.48 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9634788541205754		[learning rate: 0.00013709]
	Learning Rate: 0.000137088
	LOSS [training: 0.9634788541205754 | validation: 2.5700972783231038]
	TIME [epoch: 8.47 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9664001368763758		[learning rate: 0.00013659]
	Learning Rate: 0.000136591
	LOSS [training: 0.9664001368763758 | validation: 2.583650385184504]
	TIME [epoch: 8.47 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.960969435151106		[learning rate: 0.00013609]
	Learning Rate: 0.000136095
	LOSS [training: 0.960969435151106 | validation: 2.5841348270062032]
	TIME [epoch: 8.47 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9631552050398009		[learning rate: 0.0001356]
	Learning Rate: 0.000135601
	LOSS [training: 0.9631552050398009 | validation: 2.582446273149977]
	TIME [epoch: 8.49 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9598989273655196		[learning rate: 0.00013511]
	Learning Rate: 0.000135109
	LOSS [training: 0.9598989273655196 | validation: 2.5764621873517752]
	TIME [epoch: 8.46 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9656167704621395		[learning rate: 0.00013462]
	Learning Rate: 0.000134619
	LOSS [training: 0.9656167704621395 | validation: 2.5762210608637552]
	TIME [epoch: 8.47 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9613149178784697		[learning rate: 0.00013413]
	Learning Rate: 0.00013413
	LOSS [training: 0.9613149178784697 | validation: 2.584393332399835]
	TIME [epoch: 8.47 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9684257291553363		[learning rate: 0.00013364]
	Learning Rate: 0.000133643
	LOSS [training: 0.9684257291553363 | validation: 2.5877396584494203]
	TIME [epoch: 8.47 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9666343533278164		[learning rate: 0.00013316]
	Learning Rate: 0.000133158
	LOSS [training: 0.9666343533278164 | validation: 2.5985005071261433]
	TIME [epoch: 8.47 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9661416557621519		[learning rate: 0.00013268]
	Learning Rate: 0.000132675
	LOSS [training: 0.9661416557621519 | validation: 2.5905363423236425]
	TIME [epoch: 8.47 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9641357837593155		[learning rate: 0.00013219]
	Learning Rate: 0.000132194
	LOSS [training: 0.9641357837593155 | validation: 2.600964056045453]
	TIME [epoch: 8.49 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9633551568964493		[learning rate: 0.00013171]
	Learning Rate: 0.000131714
	LOSS [training: 0.9633551568964493 | validation: 2.60739362987873]
	TIME [epoch: 8.49 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9636709205660626		[learning rate: 0.00013124]
	Learning Rate: 0.000131236
	LOSS [training: 0.9636709205660626 | validation: 2.580942033928769]
	TIME [epoch: 8.47 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9635458937834557		[learning rate: 0.00013076]
	Learning Rate: 0.00013076
	LOSS [training: 0.9635458937834557 | validation: 2.5814639810937217]
	TIME [epoch: 8.46 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9605996200577606		[learning rate: 0.00013029]
	Learning Rate: 0.000130285
	LOSS [training: 0.9605996200577606 | validation: 2.5882293330418475]
	TIME [epoch: 8.48 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9637795189351153		[learning rate: 0.00012981]
	Learning Rate: 0.000129812
	LOSS [training: 0.9637795189351153 | validation: 2.588870754774436]
	TIME [epoch: 8.47 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9708281484525344		[learning rate: 0.00012934]
	Learning Rate: 0.000129341
	LOSS [training: 0.9708281484525344 | validation: 2.5853560219434923]
	TIME [epoch: 8.47 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9644526623704479		[learning rate: 0.00012887]
	Learning Rate: 0.000128872
	LOSS [training: 0.9644526623704479 | validation: 2.5885438355758104]
	TIME [epoch: 8.47 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9609071253876147		[learning rate: 0.0001284]
	Learning Rate: 0.000128404
	LOSS [training: 0.9609071253876147 | validation: 2.5669155624113666]
	TIME [epoch: 8.48 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9654927265290661		[learning rate: 0.00012794]
	Learning Rate: 0.000127938
	LOSS [training: 0.9654927265290661 | validation: 2.5763957196728065]
	TIME [epoch: 8.47 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9629656766448058		[learning rate: 0.00012747]
	Learning Rate: 0.000127474
	LOSS [training: 0.9629656766448058 | validation: 2.57312977177098]
	TIME [epoch: 8.47 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9714446041457631		[learning rate: 0.00012701]
	Learning Rate: 0.000127011
	LOSS [training: 0.9714446041457631 | validation: 2.5925818436508425]
	TIME [epoch: 8.47 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9765543437975437		[learning rate: 0.00012655]
	Learning Rate: 0.00012655
	LOSS [training: 0.9765543437975437 | validation: 2.585819024007227]
	TIME [epoch: 8.48 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9622282899152204		[learning rate: 0.00012609]
	Learning Rate: 0.000126091
	LOSS [training: 0.9622282899152204 | validation: 2.597310520225894]
	TIME [epoch: 8.47 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9711868973463462		[learning rate: 0.00012563]
	Learning Rate: 0.000125633
	LOSS [training: 0.9711868973463462 | validation: 2.5921024470644056]
	TIME [epoch: 8.47 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9616397288869157		[learning rate: 0.00012518]
	Learning Rate: 0.000125178
	LOSS [training: 0.9616397288869157 | validation: 2.58151170418585]
	TIME [epoch: 8.47 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.961189375905773		[learning rate: 0.00012472]
	Learning Rate: 0.000124723
	LOSS [training: 0.961189375905773 | validation: 2.5785436960754007]
	TIME [epoch: 8.49 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9618504566426994		[learning rate: 0.00012427]
	Learning Rate: 0.000124271
	LOSS [training: 0.9618504566426994 | validation: 2.574353814874908]
	TIME [epoch: 8.47 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9642032990373945		[learning rate: 0.00012382]
	Learning Rate: 0.00012382
	LOSS [training: 0.9642032990373945 | validation: 2.583532280549176]
	TIME [epoch: 8.47 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9635809282219965		[learning rate: 0.00012337]
	Learning Rate: 0.00012337
	LOSS [training: 0.9635809282219965 | validation: 2.5766024164681633]
	TIME [epoch: 8.47 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9638986216580925		[learning rate: 0.00012292]
	Learning Rate: 0.000122923
	LOSS [training: 0.9638986216580925 | validation: 2.5930805077613726]
	TIME [epoch: 8.48 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9586698808849313		[learning rate: 0.00012248]
	Learning Rate: 0.000122476
	LOSS [training: 0.9586698808849313 | validation: 2.5959561781810327]
	TIME [epoch: 8.47 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9751907252488285		[learning rate: 0.00012203]
	Learning Rate: 0.000122032
	LOSS [training: 0.9751907252488285 | validation: 2.5978566412485025]
	TIME [epoch: 8.47 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9632582501133932		[learning rate: 0.00012159]
	Learning Rate: 0.000121589
	LOSS [training: 0.9632582501133932 | validation: 2.573807560884593]
	TIME [epoch: 8.47 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9609161667241244		[learning rate: 0.00012115]
	Learning Rate: 0.000121148
	LOSS [training: 0.9609161667241244 | validation: 2.584444464847386]
	TIME [epoch: 8.49 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9583590321581212		[learning rate: 0.00012071]
	Learning Rate: 0.000120708
	LOSS [training: 0.9583590321581212 | validation: 2.5946804077608485]
	TIME [epoch: 8.47 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9675700284082248		[learning rate: 0.00012027]
	Learning Rate: 0.00012027
	LOSS [training: 0.9675700284082248 | validation: 2.595038881661391]
	TIME [epoch: 8.48 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9617446476868066		[learning rate: 0.00011983]
	Learning Rate: 0.000119834
	LOSS [training: 0.9617446476868066 | validation: 2.583457365144973]
	TIME [epoch: 8.48 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9661181110537058		[learning rate: 0.0001194]
	Learning Rate: 0.000119399
	LOSS [training: 0.9661181110537058 | validation: 2.601386840823904]
	TIME [epoch: 8.49 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9632919992561169		[learning rate: 0.00011897]
	Learning Rate: 0.000118966
	LOSS [training: 0.9632919992561169 | validation: 2.573936273488927]
	TIME [epoch: 8.47 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9591464591405112		[learning rate: 0.00011853]
	Learning Rate: 0.000118534
	LOSS [training: 0.9591464591405112 | validation: 2.5843710185991178]
	TIME [epoch: 8.47 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9646092266577814		[learning rate: 0.0001181]
	Learning Rate: 0.000118104
	LOSS [training: 0.9646092266577814 | validation: 2.579621831370845]
	TIME [epoch: 8.47 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9672434574381086		[learning rate: 0.00011767]
	Learning Rate: 0.000117675
	LOSS [training: 0.9672434574381086 | validation: 2.583924875686594]
	TIME [epoch: 8.49 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9671274272250108		[learning rate: 0.00011725]
	Learning Rate: 0.000117248
	LOSS [training: 0.9671274272250108 | validation: 2.5831105745568204]
	TIME [epoch: 8.47 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9591381539872476		[learning rate: 0.00011682]
	Learning Rate: 0.000116822
	LOSS [training: 0.9591381539872476 | validation: 2.5684395435017455]
	TIME [epoch: 8.46 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9556528035305494		[learning rate: 0.0001164]
	Learning Rate: 0.000116398
	LOSS [training: 0.9556528035305494 | validation: 2.575016975691426]
	TIME [epoch: 8.48 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9619557960949798		[learning rate: 0.00011598]
	Learning Rate: 0.000115976
	LOSS [training: 0.9619557960949798 | validation: 2.576567560209145]
	TIME [epoch: 8.49 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9603595979229068		[learning rate: 0.00011556]
	Learning Rate: 0.000115555
	LOSS [training: 0.9603595979229068 | validation: 2.5660219136931457]
	TIME [epoch: 8.47 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9659901351142327		[learning rate: 0.00011514]
	Learning Rate: 0.000115136
	LOSS [training: 0.9659901351142327 | validation: 2.598678850754281]
	TIME [epoch: 8.46 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9603526033264895		[learning rate: 0.00011472]
	Learning Rate: 0.000114718
	LOSS [training: 0.9603526033264895 | validation: 2.572260901001373]
	TIME [epoch: 8.48 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9598315450031574		[learning rate: 0.0001143]
	Learning Rate: 0.000114302
	LOSS [training: 0.9598315450031574 | validation: 2.5707893676037625]
	TIME [epoch: 8.48 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9598046933422462		[learning rate: 0.00011389]
	Learning Rate: 0.000113887
	LOSS [training: 0.9598046933422462 | validation: 2.5657085926761773]
	TIME [epoch: 8.46 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9577846005211199		[learning rate: 0.00011347]
	Learning Rate: 0.000113474
	LOSS [training: 0.9577846005211199 | validation: 2.581579079504393]
	TIME [epoch: 8.47 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9599101783616767		[learning rate: 0.00011306]
	Learning Rate: 0.000113062
	LOSS [training: 0.9599101783616767 | validation: 2.586672988337958]
	TIME [epoch: 8.48 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9628449997413252		[learning rate: 0.00011265]
	Learning Rate: 0.000112651
	LOSS [training: 0.9628449997413252 | validation: 2.583390856297637]
	TIME [epoch: 8.48 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9629681130956216		[learning rate: 0.00011224]
	Learning Rate: 0.000112243
	LOSS [training: 0.9629681130956216 | validation: 2.5829709806205425]
	TIME [epoch: 8.47 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9655364837501462		[learning rate: 0.00011184]
	Learning Rate: 0.000111835
	LOSS [training: 0.9655364837501462 | validation: 2.5848262365888948]
	TIME [epoch: 8.47 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9672175173157944		[learning rate: 0.00011143]
	Learning Rate: 0.000111429
	LOSS [training: 0.9672175173157944 | validation: 2.5881597827245395]
	TIME [epoch: 8.48 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.960789511527825		[learning rate: 0.00011103]
	Learning Rate: 0.000111025
	LOSS [training: 0.960789511527825 | validation: 2.580069813896373]
	TIME [epoch: 8.48 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9585811852724062		[learning rate: 0.00011062]
	Learning Rate: 0.000110622
	LOSS [training: 0.9585811852724062 | validation: 2.5801321549877985]
	TIME [epoch: 8.48 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9612894981326591		[learning rate: 0.00011022]
	Learning Rate: 0.000110221
	LOSS [training: 0.9612894981326591 | validation: 2.5969515875616516]
	TIME [epoch: 8.46 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9622912683302415		[learning rate: 0.00010982]
	Learning Rate: 0.000109821
	LOSS [training: 0.9622912683302415 | validation: 2.5939575096731797]
	TIME [epoch: 8.48 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9643986486756566		[learning rate: 0.00010942]
	Learning Rate: 0.000109422
	LOSS [training: 0.9643986486756566 | validation: 2.5762209366108006]
	TIME [epoch: 8.47 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9625426338759494		[learning rate: 0.00010903]
	Learning Rate: 0.000109025
	LOSS [training: 0.9625426338759494 | validation: 2.6076831245410514]
	TIME [epoch: 8.47 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9565987686428674		[learning rate: 0.00010863]
	Learning Rate: 0.000108629
	LOSS [training: 0.9565987686428674 | validation: 2.5791439245631897]
	TIME [epoch: 8.47 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9636586145907617		[learning rate: 0.00010824]
	Learning Rate: 0.000108235
	LOSS [training: 0.9636586145907617 | validation: 2.5867614159831795]
	TIME [epoch: 8.49 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9576175310295467		[learning rate: 0.00010784]
	Learning Rate: 0.000107842
	LOSS [training: 0.9576175310295467 | validation: 2.5687751419500957]
	TIME [epoch: 8.47 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9666396944308543		[learning rate: 0.00010745]
	Learning Rate: 0.000107451
	LOSS [training: 0.9666396944308543 | validation: 2.5698824819859034]
	TIME [epoch: 8.46 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9606072379603997		[learning rate: 0.00010706]
	Learning Rate: 0.000107061
	LOSS [training: 0.9606072379603997 | validation: 2.5786934268630377]
	TIME [epoch: 8.46 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9583720190250361		[learning rate: 0.00010667]
	Learning Rate: 0.000106673
	LOSS [training: 0.9583720190250361 | validation: 2.5830971871006803]
	TIME [epoch: 8.48 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9564436908936009		[learning rate: 0.00010629]
	Learning Rate: 0.000106285
	LOSS [training: 0.9564436908936009 | validation: 2.5787068626699092]
	TIME [epoch: 8.46 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9613536499833824		[learning rate: 0.0001059]
	Learning Rate: 0.0001059
	LOSS [training: 0.9613536499833824 | validation: 2.5682465400197683]
	TIME [epoch: 8.48 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9616384181629914		[learning rate: 0.00010552]
	Learning Rate: 0.000105515
	LOSS [training: 0.9616384181629914 | validation: 2.5800623196966774]
	TIME [epoch: 8.46 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.959806187977829		[learning rate: 0.00010513]
	Learning Rate: 0.000105132
	LOSS [training: 0.959806187977829 | validation: 2.579008501510042]
	TIME [epoch: 8.49 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.963516525896367		[learning rate: 0.00010475]
	Learning Rate: 0.000104751
	LOSS [training: 0.963516525896367 | validation: 2.582635096847812]
	TIME [epoch: 8.47 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9613557460408921		[learning rate: 0.00010437]
	Learning Rate: 0.000104371
	LOSS [training: 0.9613557460408921 | validation: 2.590636728751051]
	TIME [epoch: 8.47 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9609443072730988		[learning rate: 0.00010399]
	Learning Rate: 0.000103992
	LOSS [training: 0.9609443072730988 | validation: 2.5780021183716504]
	TIME [epoch: 8.47 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.963162250245109		[learning rate: 0.00010361]
	Learning Rate: 0.000103615
	LOSS [training: 0.963162250245109 | validation: 2.5821618346412296]
	TIME [epoch: 8.49 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.96452942739841		[learning rate: 0.00010324]
	Learning Rate: 0.000103239
	LOSS [training: 0.96452942739841 | validation: 2.5943280191242266]
	TIME [epoch: 8.47 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9629203346080933		[learning rate: 0.00010286]
	Learning Rate: 0.000102864
	LOSS [training: 0.9629203346080933 | validation: 2.5925632286142637]
	TIME [epoch: 8.48 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9628629718169165		[learning rate: 0.00010249]
	Learning Rate: 0.000102491
	LOSS [training: 0.9628629718169165 | validation: 2.5774607099336873]
	TIME [epoch: 8.46 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.959639071335641		[learning rate: 0.00010212]
	Learning Rate: 0.000102119
	LOSS [training: 0.959639071335641 | validation: 2.5878980176261748]
	TIME [epoch: 8.5 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9586986154458664		[learning rate: 0.00010175]
	Learning Rate: 0.000101748
	LOSS [training: 0.9586986154458664 | validation: 2.5847341890845525]
	TIME [epoch: 8.47 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9552674747652764		[learning rate: 0.00010138]
	Learning Rate: 0.000101379
	LOSS [training: 0.9552674747652764 | validation: 2.5889642176564744]
	TIME [epoch: 8.47 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9569943811855625		[learning rate: 0.00010101]
	Learning Rate: 0.000101011
	LOSS [training: 0.9569943811855625 | validation: 2.5829438928192]
	TIME [epoch: 8.46 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9623605613100837		[learning rate: 0.00010064]
	Learning Rate: 0.000100644
	LOSS [training: 0.9623605613100837 | validation: 2.5769110543740257]
	TIME [epoch: 8.5 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9624529925168626		[learning rate: 0.00010028]
	Learning Rate: 0.000100279
	LOSS [training: 0.9624529925168626 | validation: 2.58711150774822]
	TIME [epoch: 8.47 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9591885850761731		[learning rate: 9.9915e-05]
	Learning Rate: 9.99152e-05
	LOSS [training: 0.9591885850761731 | validation: 2.5902410981757154]
	TIME [epoch: 8.47 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9619820461878017		[learning rate: 9.9553e-05]
	Learning Rate: 9.95526e-05
	LOSS [training: 0.9619820461878017 | validation: 2.5944289313538254]
	TIME [epoch: 8.47 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9686448744554074		[learning rate: 9.9191e-05]
	Learning Rate: 9.91913e-05
	LOSS [training: 0.9686448744554074 | validation: 2.586836376616335]
	TIME [epoch: 8.49 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9590181980438753		[learning rate: 9.8831e-05]
	Learning Rate: 9.88314e-05
	LOSS [training: 0.9590181980438753 | validation: 2.576137356388957]
	TIME [epoch: 8.47 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9606400869566057		[learning rate: 9.8473e-05]
	Learning Rate: 9.84727e-05
	LOSS [training: 0.9606400869566057 | validation: 2.588425775171207]
	TIME [epoch: 8.47 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9587488825514715		[learning rate: 9.8115e-05]
	Learning Rate: 9.81153e-05
	LOSS [training: 0.9587488825514715 | validation: 2.570894608077549]
	TIME [epoch: 8.48 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9674947221263155		[learning rate: 9.7759e-05]
	Learning Rate: 9.77592e-05
	LOSS [training: 0.9674947221263155 | validation: 2.583206341200695]
	TIME [epoch: 8.48 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9615296368786508		[learning rate: 9.7404e-05]
	Learning Rate: 9.74045e-05
	LOSS [training: 0.9615296368786508 | validation: 2.575215684956357]
	TIME [epoch: 8.47 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9569706505824811		[learning rate: 9.7051e-05]
	Learning Rate: 9.7051e-05
	LOSS [training: 0.9569706505824811 | validation: 2.5899886430359373]
	TIME [epoch: 8.47 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9604674411796192		[learning rate: 9.6699e-05]
	Learning Rate: 9.66988e-05
	LOSS [training: 0.9604674411796192 | validation: 2.607038322244339]
	TIME [epoch: 8.47 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9604504879202448		[learning rate: 9.6348e-05]
	Learning Rate: 9.63479e-05
	LOSS [training: 0.9604504879202448 | validation: 2.583720486932505]
	TIME [epoch: 8.48 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9640073407080741		[learning rate: 9.5998e-05]
	Learning Rate: 9.59982e-05
	LOSS [training: 0.9640073407080741 | validation: 2.581816176772952]
	TIME [epoch: 8.47 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9613904389033934		[learning rate: 9.565e-05]
	Learning Rate: 9.56499e-05
	LOSS [training: 0.9613904389033934 | validation: 2.577789464587966]
	TIME [epoch: 8.47 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9656571809443865		[learning rate: 9.5303e-05]
	Learning Rate: 9.53027e-05
	LOSS [training: 0.9656571809443865 | validation: 2.5772331523226004]
	TIME [epoch: 8.47 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9632121682040046		[learning rate: 9.4957e-05]
	Learning Rate: 9.49569e-05
	LOSS [training: 0.9632121682040046 | validation: 2.5737860491813778]
	TIME [epoch: 8.47 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9612188694456492		[learning rate: 9.4612e-05]
	Learning Rate: 9.46122e-05
	LOSS [training: 0.9612188694456492 | validation: 2.5763008615054446]
	TIME [epoch: 8.46 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9553921447374265		[learning rate: 9.4269e-05]
	Learning Rate: 9.42689e-05
	LOSS [training: 0.9553921447374265 | validation: 2.5853417564793015]
	TIME [epoch: 8.47 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9567480493740032		[learning rate: 9.3927e-05]
	Learning Rate: 9.39268e-05
	LOSS [training: 0.9567480493740032 | validation: 2.5865186323926554]
	TIME [epoch: 8.49 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9548731915424356		[learning rate: 9.3586e-05]
	Learning Rate: 9.35859e-05
	LOSS [training: 0.9548731915424356 | validation: 2.578466892537806]
	TIME [epoch: 8.46 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9540407403387198		[learning rate: 9.3246e-05]
	Learning Rate: 9.32463e-05
	LOSS [training: 0.9540407403387198 | validation: 2.57783338759567]
	TIME [epoch: 8.46 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9595901660200642		[learning rate: 9.2908e-05]
	Learning Rate: 9.29079e-05
	LOSS [training: 0.9595901660200642 | validation: 2.5733518443084566]
	TIME [epoch: 8.47 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9632722421167893		[learning rate: 9.2571e-05]
	Learning Rate: 9.25707e-05
	LOSS [training: 0.9632722421167893 | validation: 2.580008410322005]
	TIME [epoch: 8.49 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9602691639775447		[learning rate: 9.2235e-05]
	Learning Rate: 9.22348e-05
	LOSS [training: 0.9602691639775447 | validation: 2.5846669475916997]
	TIME [epoch: 8.47 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.957112671350699		[learning rate: 9.19e-05]
	Learning Rate: 9.19001e-05
	LOSS [training: 0.957112671350699 | validation: 2.5864098418623955]
	TIME [epoch: 8.47 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9598902200925259		[learning rate: 9.1567e-05]
	Learning Rate: 9.15665e-05
	LOSS [training: 0.9598902200925259 | validation: 2.5898755456188454]
	TIME [epoch: 8.46 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9615884053626147		[learning rate: 9.1234e-05]
	Learning Rate: 9.12343e-05
	LOSS [training: 0.9615884053626147 | validation: 2.572362148091632]
	TIME [epoch: 8.49 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9586330523821429		[learning rate: 9.0903e-05]
	Learning Rate: 9.09031e-05
	LOSS [training: 0.9586330523821429 | validation: 2.5915947207434216]
	TIME [epoch: 8.48 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9615532741726123		[learning rate: 9.0573e-05]
	Learning Rate: 9.05733e-05
	LOSS [training: 0.9615532741726123 | validation: 2.5783452331826258]
	TIME [epoch: 8.47 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9615949663493302		[learning rate: 9.0245e-05]
	Learning Rate: 9.02446e-05
	LOSS [training: 0.9615949663493302 | validation: 2.577249050320916]
	TIME [epoch: 8.48 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9624362668563282		[learning rate: 8.9917e-05]
	Learning Rate: 8.99171e-05
	LOSS [training: 0.9624362668563282 | validation: 2.57631668481653]
	TIME [epoch: 8.5 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9583725772122366		[learning rate: 8.9591e-05]
	Learning Rate: 8.95908e-05
	LOSS [training: 0.9583725772122366 | validation: 2.5809798064871514]
	TIME [epoch: 8.47 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.968622373437672		[learning rate: 8.9266e-05]
	Learning Rate: 8.92656e-05
	LOSS [training: 0.968622373437672 | validation: 2.574861716695766]
	TIME [epoch: 8.47 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9537666314996581		[learning rate: 8.8942e-05]
	Learning Rate: 8.89417e-05
	LOSS [training: 0.9537666314996581 | validation: 2.574166566574]
	TIME [epoch: 8.48 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9651978225158852		[learning rate: 8.8619e-05]
	Learning Rate: 8.86189e-05
	LOSS [training: 0.9651978225158852 | validation: 2.5825644569335626]
	TIME [epoch: 8.49 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.957919244616661		[learning rate: 8.8297e-05]
	Learning Rate: 8.82973e-05
	LOSS [training: 0.957919244616661 | validation: 2.582699296203498]
	TIME [epoch: 8.47 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9579507931713825		[learning rate: 8.7977e-05]
	Learning Rate: 8.79769e-05
	LOSS [training: 0.9579507931713825 | validation: 2.586784826066669]
	TIME [epoch: 8.47 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9550282331712202		[learning rate: 8.7658e-05]
	Learning Rate: 8.76576e-05
	LOSS [training: 0.9550282331712202 | validation: 2.591852335496369]
	TIME [epoch: 8.47 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9583465461023289		[learning rate: 8.7339e-05]
	Learning Rate: 8.73395e-05
	LOSS [training: 0.9583465461023289 | validation: 2.5952078742323277]
	TIME [epoch: 8.5 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9580299325166772		[learning rate: 8.7023e-05]
	Learning Rate: 8.70225e-05
	LOSS [training: 0.9580299325166772 | validation: 2.5739526676558655]
	TIME [epoch: 8.47 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9598712584820277		[learning rate: 8.6707e-05]
	Learning Rate: 8.67067e-05
	LOSS [training: 0.9598712584820277 | validation: 2.58577696255073]
	TIME [epoch: 8.47 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9648892714260333		[learning rate: 8.6392e-05]
	Learning Rate: 8.63921e-05
	LOSS [training: 0.9648892714260333 | validation: 2.582338630293431]
	TIME [epoch: 8.47 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9581150764880814		[learning rate: 8.6079e-05]
	Learning Rate: 8.60785e-05
	LOSS [training: 0.9581150764880814 | validation: 2.5936604932115044]
	TIME [epoch: 8.5 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9612945225565348		[learning rate: 8.5766e-05]
	Learning Rate: 8.57661e-05
	LOSS [training: 0.9612945225565348 | validation: 2.5861210975072533]
	TIME [epoch: 8.48 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9653220216713378		[learning rate: 8.5455e-05]
	Learning Rate: 8.54549e-05
	LOSS [training: 0.9653220216713378 | validation: 2.587306475320207]
	TIME [epoch: 8.47 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9537943151777064		[learning rate: 8.5145e-05]
	Learning Rate: 8.51448e-05
	LOSS [training: 0.9537943151777064 | validation: 2.584918059698288]
	TIME [epoch: 8.47 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9593784337024482		[learning rate: 8.4836e-05]
	Learning Rate: 8.48358e-05
	LOSS [training: 0.9593784337024482 | validation: 2.586062126240353]
	TIME [epoch: 8.48 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9606143525935092		[learning rate: 8.4528e-05]
	Learning Rate: 8.45279e-05
	LOSS [training: 0.9606143525935092 | validation: 2.585307501803549]
	TIME [epoch: 8.47 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.963257426819222		[learning rate: 8.4221e-05]
	Learning Rate: 8.42211e-05
	LOSS [training: 0.963257426819222 | validation: 2.589609054318838]
	TIME [epoch: 8.47 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9605122246569084		[learning rate: 8.3916e-05]
	Learning Rate: 8.39155e-05
	LOSS [training: 0.9605122246569084 | validation: 2.5967844683097585]
	TIME [epoch: 8.48 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.957021045359933		[learning rate: 8.3611e-05]
	Learning Rate: 8.3611e-05
	LOSS [training: 0.957021045359933 | validation: 2.5830124299457746]
	TIME [epoch: 8.47 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9615316883740148		[learning rate: 8.3308e-05]
	Learning Rate: 8.33075e-05
	LOSS [training: 0.9615316883740148 | validation: 2.5748525766486523]
	TIME [epoch: 8.46 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9554132921852704		[learning rate: 8.3005e-05]
	Learning Rate: 8.30052e-05
	LOSS [training: 0.9554132921852704 | validation: 2.5855348739579815]
	TIME [epoch: 8.46 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9619860293300114		[learning rate: 8.2704e-05]
	Learning Rate: 8.2704e-05
	LOSS [training: 0.9619860293300114 | validation: 2.6009994574071404]
	TIME [epoch: 8.49 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9752480941561888		[learning rate: 8.2404e-05]
	Learning Rate: 8.24038e-05
	LOSS [training: 0.9752480941561888 | validation: 2.6115047586714866]
	TIME [epoch: 8.47 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9637992537855495		[learning rate: 8.2105e-05]
	Learning Rate: 8.21048e-05
	LOSS [training: 0.9637992537855495 | validation: 2.5863548358152078]
	TIME [epoch: 8.47 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9605938776835912		[learning rate: 8.1807e-05]
	Learning Rate: 8.18068e-05
	LOSS [training: 0.9605938776835912 | validation: 2.5846295983953937]
	TIME [epoch: 8.47 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9605758323488326		[learning rate: 8.151e-05]
	Learning Rate: 8.15099e-05
	LOSS [training: 0.9605758323488326 | validation: 2.5768293448439965]
	TIME [epoch: 8.48 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9604065420284791		[learning rate: 8.1214e-05]
	Learning Rate: 8.12142e-05
	LOSS [training: 0.9604065420284791 | validation: 2.5746898441084056]
	TIME [epoch: 8.48 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9586688122917574		[learning rate: 8.0919e-05]
	Learning Rate: 8.09194e-05
	LOSS [training: 0.9586688122917574 | validation: 2.5946321818627194]
	TIME [epoch: 8.47 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9597519493656439		[learning rate: 8.0626e-05]
	Learning Rate: 8.06257e-05
	LOSS [training: 0.9597519493656439 | validation: 2.5824889820561197]
	TIME [epoch: 8.47 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9595523784448501		[learning rate: 8.0333e-05]
	Learning Rate: 8.03331e-05
	LOSS [training: 0.9595523784448501 | validation: 2.5802699611789706]
	TIME [epoch: 8.49 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9601332718485656		[learning rate: 8.0042e-05]
	Learning Rate: 8.00416e-05
	LOSS [training: 0.9601332718485656 | validation: 2.5911500361218414]
	TIME [epoch: 8.47 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9610121699276784		[learning rate: 7.9751e-05]
	Learning Rate: 7.97511e-05
	LOSS [training: 0.9610121699276784 | validation: 2.571600116114398]
	TIME [epoch: 8.47 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9600116819152701		[learning rate: 7.9462e-05]
	Learning Rate: 7.94617e-05
	LOSS [training: 0.9600116819152701 | validation: 2.5728580835210884]
	TIME [epoch: 8.47 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9648205784434858		[learning rate: 7.9173e-05]
	Learning Rate: 7.91733e-05
	LOSS [training: 0.9648205784434858 | validation: 2.5780160962740046]
	TIME [epoch: 8.49 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9567813962662288		[learning rate: 7.8886e-05]
	Learning Rate: 7.8886e-05
	LOSS [training: 0.9567813962662288 | validation: 2.586315542187351]
	TIME [epoch: 8.48 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9612859202219466		[learning rate: 7.86e-05]
	Learning Rate: 7.85997e-05
	LOSS [training: 0.9612859202219466 | validation: 2.578380035521392]
	TIME [epoch: 8.47 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9552305350638116		[learning rate: 7.8315e-05]
	Learning Rate: 7.83145e-05
	LOSS [training: 0.9552305350638116 | validation: 2.571346803549038]
	TIME [epoch: 8.47 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9581558723125234		[learning rate: 7.803e-05]
	Learning Rate: 7.80303e-05
	LOSS [training: 0.9581558723125234 | validation: 2.5772982050320445]
	TIME [epoch: 8.5 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9621757346946639		[learning rate: 7.7747e-05]
	Learning Rate: 7.77471e-05
	LOSS [training: 0.9621757346946639 | validation: 2.5780042854390652]
	TIME [epoch: 8.47 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9631732399241715		[learning rate: 7.7465e-05]
	Learning Rate: 7.7465e-05
	LOSS [training: 0.9631732399241715 | validation: 2.569128668679539]
	TIME [epoch: 8.47 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9688007340503987		[learning rate: 7.7184e-05]
	Learning Rate: 7.71838e-05
	LOSS [training: 0.9688007340503987 | validation: 2.5765067226123364]
	TIME [epoch: 8.47 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.959357613194759		[learning rate: 7.6904e-05]
	Learning Rate: 7.69037e-05
	LOSS [training: 0.959357613194759 | validation: 2.584711684379362]
	TIME [epoch: 8.48 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9622082407262778		[learning rate: 7.6625e-05]
	Learning Rate: 7.66246e-05
	LOSS [training: 0.9622082407262778 | validation: 2.5914307949327076]
	TIME [epoch: 8.48 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9594477224994036		[learning rate: 7.6347e-05]
	Learning Rate: 7.63466e-05
	LOSS [training: 0.9594477224994036 | validation: 2.591699251097659]
	TIME [epoch: 8.47 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9609227317393951		[learning rate: 7.607e-05]
	Learning Rate: 7.60695e-05
	LOSS [training: 0.9609227317393951 | validation: 2.5941746274592448]
	TIME [epoch: 8.47 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9592972976843985		[learning rate: 7.5793e-05]
	Learning Rate: 7.57935e-05
	LOSS [training: 0.9592972976843985 | validation: 2.5744725425724377]
	TIME [epoch: 8.5 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9605797429544959		[learning rate: 7.5518e-05]
	Learning Rate: 7.55184e-05
	LOSS [training: 0.9605797429544959 | validation: 2.577645899982379]
	TIME [epoch: 8.47 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9603482369716907		[learning rate: 7.5244e-05]
	Learning Rate: 7.52443e-05
	LOSS [training: 0.9603482369716907 | validation: 2.576518356183783]
	TIME [epoch: 8.47 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.954966302185863		[learning rate: 7.4971e-05]
	Learning Rate: 7.49712e-05
	LOSS [training: 0.954966302185863 | validation: 2.591790407176976]
	TIME [epoch: 8.47 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.960450569380642		[learning rate: 7.4699e-05]
	Learning Rate: 7.46992e-05
	LOSS [training: 0.960450569380642 | validation: 2.5845099908230016]
	TIME [epoch: 8.49 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9675873034076554		[learning rate: 7.4428e-05]
	Learning Rate: 7.44281e-05
	LOSS [training: 0.9675873034076554 | validation: 2.57636846385478]
	TIME [epoch: 8.47 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9609583404716625		[learning rate: 7.4158e-05]
	Learning Rate: 7.4158e-05
	LOSS [training: 0.9609583404716625 | validation: 2.5893361167681634]
	TIME [epoch: 8.47 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.961266324906368		[learning rate: 7.3889e-05]
	Learning Rate: 7.38889e-05
	LOSS [training: 0.961266324906368 | validation: 2.6052082545658757]
	TIME [epoch: 8.47 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9645617495487521		[learning rate: 7.3621e-05]
	Learning Rate: 7.36207e-05
	LOSS [training: 0.9645617495487521 | validation: 2.582914667738117]
	TIME [epoch: 8.49 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9581349386512248		[learning rate: 7.3354e-05]
	Learning Rate: 7.33536e-05
	LOSS [training: 0.9581349386512248 | validation: 2.5751121981680973]
	TIME [epoch: 8.47 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9580585277993882		[learning rate: 7.3087e-05]
	Learning Rate: 7.30873e-05
	LOSS [training: 0.9580585277993882 | validation: 2.577404374112007]
	TIME [epoch: 8.47 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9603377391320121		[learning rate: 7.2822e-05]
	Learning Rate: 7.28221e-05
	LOSS [training: 0.9603377391320121 | validation: 2.575623132164745]
	TIME [epoch: 8.48 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9607956802704478		[learning rate: 7.2558e-05]
	Learning Rate: 7.25578e-05
	LOSS [training: 0.9607956802704478 | validation: 2.5964260370322902]
	TIME [epoch: 8.49 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.964372356868411		[learning rate: 7.2295e-05]
	Learning Rate: 7.22945e-05
	LOSS [training: 0.964372356868411 | validation: 2.581081802369188]
	TIME [epoch: 8.47 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9617425236303354		[learning rate: 7.2032e-05]
	Learning Rate: 7.20321e-05
	LOSS [training: 0.9617425236303354 | validation: 2.5799380642579886]
	TIME [epoch: 8.47 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9550581533507557		[learning rate: 7.1771e-05]
	Learning Rate: 7.17707e-05
	LOSS [training: 0.9550581533507557 | validation: 2.589232847809587]
	TIME [epoch: 8.48 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9613000613479358		[learning rate: 7.151e-05]
	Learning Rate: 7.15103e-05
	LOSS [training: 0.9613000613479358 | validation: 2.6081305347813943]
	TIME [epoch: 8.48 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9622219827289975		[learning rate: 7.1251e-05]
	Learning Rate: 7.12508e-05
	LOSS [training: 0.9622219827289975 | validation: 2.588064437773187]
	TIME [epoch: 8.48 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.96138607934584		[learning rate: 7.0992e-05]
	Learning Rate: 7.09922e-05
	LOSS [training: 0.96138607934584 | validation: 2.578631889350052]
	TIME [epoch: 8.47 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9598594740825315		[learning rate: 7.0735e-05]
	Learning Rate: 7.07345e-05
	LOSS [training: 0.9598594740825315 | validation: 2.587472456712542]
	TIME [epoch: 8.48 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9587045573984296		[learning rate: 7.0478e-05]
	Learning Rate: 7.04778e-05
	LOSS [training: 0.9587045573984296 | validation: 2.589268924032881]
	TIME [epoch: 8.49 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9567314607733666		[learning rate: 7.0222e-05]
	Learning Rate: 7.02221e-05
	LOSS [training: 0.9567314607733666 | validation: 2.5872863762395903]
	TIME [epoch: 8.47 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9631961236994506		[learning rate: 6.9967e-05]
	Learning Rate: 6.99672e-05
	LOSS [training: 0.9631961236994506 | validation: 2.592599039722158]
	TIME [epoch: 8.48 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9603670594572378		[learning rate: 6.9713e-05]
	Learning Rate: 6.97133e-05
	LOSS [training: 0.9603670594572378 | validation: 2.579092267116544]
	TIME [epoch: 8.48 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9557992189414609		[learning rate: 6.946e-05]
	Learning Rate: 6.94603e-05
	LOSS [training: 0.9557992189414609 | validation: 2.581126167621023]
	TIME [epoch: 8.48 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9609913647209695		[learning rate: 6.9208e-05]
	Learning Rate: 6.92083e-05
	LOSS [training: 0.9609913647209695 | validation: 2.584042511186136]
	TIME [epoch: 8.46 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9534403319253837		[learning rate: 6.8957e-05]
	Learning Rate: 6.89571e-05
	LOSS [training: 0.9534403319253837 | validation: 2.575758477892083]
	TIME [epoch: 8.47 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.952945865832634		[learning rate: 6.8707e-05]
	Learning Rate: 6.87068e-05
	LOSS [training: 0.952945865832634 | validation: 2.574146462832832]
	TIME [epoch: 8.48 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9553619893909803		[learning rate: 6.8457e-05]
	Learning Rate: 6.84575e-05
	LOSS [training: 0.9553619893909803 | validation: 2.5682122128292857]
	TIME [epoch: 8.47 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9591390230542522		[learning rate: 6.8209e-05]
	Learning Rate: 6.82091e-05
	LOSS [training: 0.9591390230542522 | validation: 2.5760423811910065]
	TIME [epoch: 8.46 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9662196578954667		[learning rate: 6.7962e-05]
	Learning Rate: 6.79615e-05
	LOSS [training: 0.9662196578954667 | validation: 2.5780593737183466]
	TIME [epoch: 8.46 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9601995969363244		[learning rate: 6.7715e-05]
	Learning Rate: 6.77149e-05
	LOSS [training: 0.9601995969363244 | validation: 2.591071882486186]
	TIME [epoch: 8.48 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9551185167401677		[learning rate: 6.7469e-05]
	Learning Rate: 6.74692e-05
	LOSS [training: 0.9551185167401677 | validation: 2.575679379966508]
	TIME [epoch: 8.47 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9576451922738924		[learning rate: 6.7224e-05]
	Learning Rate: 6.72243e-05
	LOSS [training: 0.9576451922738924 | validation: 2.588678269017949]
	TIME [epoch: 8.46 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9583531517386215		[learning rate: 6.698e-05]
	Learning Rate: 6.69804e-05
	LOSS [training: 0.9583531517386215 | validation: 2.5820191103329733]
	TIME [epoch: 8.47 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9587802980333159		[learning rate: 6.6737e-05]
	Learning Rate: 6.67373e-05
	LOSS [training: 0.9587802980333159 | validation: 2.5874415451070814]
	TIME [epoch: 8.48 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9601745869855431		[learning rate: 6.6495e-05]
	Learning Rate: 6.64951e-05
	LOSS [training: 0.9601745869855431 | validation: 2.5719741968885312]
	TIME [epoch: 8.47 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9647341181689664		[learning rate: 6.6254e-05]
	Learning Rate: 6.62538e-05
	LOSS [training: 0.9647341181689664 | validation: 2.5857074700553717]
	TIME [epoch: 8.47 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9599113843329274		[learning rate: 6.6013e-05]
	Learning Rate: 6.60133e-05
	LOSS [training: 0.9599113843329274 | validation: 2.580287858246413]
	TIME [epoch: 8.47 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9599346057418071		[learning rate: 6.5774e-05]
	Learning Rate: 6.57738e-05
	LOSS [training: 0.9599346057418071 | validation: 2.5726106593524096]
	TIME [epoch: 8.49 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9581074736454684		[learning rate: 6.5535e-05]
	Learning Rate: 6.55351e-05
	LOSS [training: 0.9581074736454684 | validation: 2.578564586543298]
	TIME [epoch: 8.46 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9569423387065019		[learning rate: 6.5297e-05]
	Learning Rate: 6.52972e-05
	LOSS [training: 0.9569423387065019 | validation: 2.5928096633879925]
	TIME [epoch: 8.46 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9545337837867638		[learning rate: 6.506e-05]
	Learning Rate: 6.50603e-05
	LOSS [training: 0.9545337837867638 | validation: 2.572853892795899]
	TIME [epoch: 8.47 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9561856117234953		[learning rate: 6.4824e-05]
	Learning Rate: 6.48242e-05
	LOSS [training: 0.9561856117234953 | validation: 2.5811760170890388]
	TIME [epoch: 8.49 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9547244364199091		[learning rate: 6.4589e-05]
	Learning Rate: 6.45889e-05
	LOSS [training: 0.9547244364199091 | validation: 2.578033698985008]
	TIME [epoch: 8.47 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9594284056663097		[learning rate: 6.4354e-05]
	Learning Rate: 6.43545e-05
	LOSS [training: 0.9594284056663097 | validation: 2.5780216802989973]
	TIME [epoch: 8.46 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9605898258299224		[learning rate: 6.4121e-05]
	Learning Rate: 6.4121e-05
	LOSS [training: 0.9605898258299224 | validation: 2.5680141751800143]
	TIME [epoch: 8.47 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9627155097149341		[learning rate: 6.3888e-05]
	Learning Rate: 6.38883e-05
	LOSS [training: 0.9627155097149341 | validation: 2.5735208955392093]
	TIME [epoch: 8.48 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.958207077320161		[learning rate: 6.3656e-05]
	Learning Rate: 6.36564e-05
	LOSS [training: 0.958207077320161 | validation: 2.588708281323831]
	TIME [epoch: 8.46 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.955353692754346		[learning rate: 6.3425e-05]
	Learning Rate: 6.34254e-05
	LOSS [training: 0.955353692754346 | validation: 2.5779362236869354]
	TIME [epoch: 8.46 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9571543502464186		[learning rate: 6.3195e-05]
	Learning Rate: 6.31952e-05
	LOSS [training: 0.9571543502464186 | validation: 2.5923780672869783]
	TIME [epoch: 8.47 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9599389855002907		[learning rate: 6.2966e-05]
	Learning Rate: 6.29659e-05
	LOSS [training: 0.9599389855002907 | validation: 2.5767795675623337]
	TIME [epoch: 8.48 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9578594956416138		[learning rate: 6.2737e-05]
	Learning Rate: 6.27374e-05
	LOSS [training: 0.9578594956416138 | validation: 2.5865408871868465]
	TIME [epoch: 8.46 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.962795392007776		[learning rate: 6.251e-05]
	Learning Rate: 6.25097e-05
	LOSS [training: 0.962795392007776 | validation: 2.5781403211438665]
	TIME [epoch: 8.47 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.963246354376835		[learning rate: 6.2283e-05]
	Learning Rate: 6.22828e-05
	LOSS [training: 0.963246354376835 | validation: 2.5717728501433297]
	TIME [epoch: 8.47 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9562281275222493		[learning rate: 6.2057e-05]
	Learning Rate: 6.20568e-05
	LOSS [training: 0.9562281275222493 | validation: 2.580760265889628]
	TIME [epoch: 8.48 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9560843380727733		[learning rate: 6.1832e-05]
	Learning Rate: 6.18316e-05
	LOSS [training: 0.9560843380727733 | validation: 2.57914696636351]
	TIME [epoch: 8.47 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9595492718559626		[learning rate: 6.1607e-05]
	Learning Rate: 6.16072e-05
	LOSS [training: 0.9595492718559626 | validation: 2.5666529736466956]
	TIME [epoch: 8.47 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9571184062145468		[learning rate: 6.1384e-05]
	Learning Rate: 6.13836e-05
	LOSS [training: 0.9571184062145468 | validation: 2.5723970833857095]
	TIME [epoch: 8.48 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9607170938988651		[learning rate: 6.1161e-05]
	Learning Rate: 6.11609e-05
	LOSS [training: 0.9607170938988651 | validation: 2.577459061599679]
	TIME [epoch: 8.48 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9563997950202072		[learning rate: 6.0939e-05]
	Learning Rate: 6.09389e-05
	LOSS [training: 0.9563997950202072 | validation: 2.5911471321759914]
	TIME [epoch: 8.45 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9617605386655977		[learning rate: 6.0718e-05]
	Learning Rate: 6.07178e-05
	LOSS [training: 0.9617605386655977 | validation: 2.59112380877291]
	TIME [epoch: 8.46 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9624309652264673		[learning rate: 6.0497e-05]
	Learning Rate: 6.04974e-05
	LOSS [training: 0.9624309652264673 | validation: 2.580037956096348]
	TIME [epoch: 8.48 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9601582730201734		[learning rate: 6.0278e-05]
	Learning Rate: 6.02779e-05
	LOSS [training: 0.9601582730201734 | validation: 2.5784509244460656]
	TIME [epoch: 8.49 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9637567361682094		[learning rate: 6.0059e-05]
	Learning Rate: 6.00591e-05
	LOSS [training: 0.9637567361682094 | validation: 2.5812476728705462]
	TIME [epoch: 8.48 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9563482472119338		[learning rate: 5.9841e-05]
	Learning Rate: 5.98412e-05
	LOSS [training: 0.9563482472119338 | validation: 2.577640063035621]
	TIME [epoch: 8.48 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9555770408446833		[learning rate: 5.9624e-05]
	Learning Rate: 5.9624e-05
	LOSS [training: 0.9555770408446833 | validation: 2.5797168633613015]
	TIME [epoch: 8.49 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9618471590494264		[learning rate: 5.9408e-05]
	Learning Rate: 5.94076e-05
	LOSS [training: 0.9618471590494264 | validation: 2.593614681720847]
	TIME [epoch: 8.48 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.960188527265933		[learning rate: 5.9192e-05]
	Learning Rate: 5.9192e-05
	LOSS [training: 0.960188527265933 | validation: 2.582717923478735]
	TIME [epoch: 8.47 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9583384576137416		[learning rate: 5.8977e-05]
	Learning Rate: 5.89772e-05
	LOSS [training: 0.9583384576137416 | validation: 2.589384740073033]
	TIME [epoch: 8.48 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9569615509925109		[learning rate: 5.8763e-05]
	Learning Rate: 5.87632e-05
	LOSS [training: 0.9569615509925109 | validation: 2.5934311279113325]
	TIME [epoch: 8.5 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9599185178380848		[learning rate: 5.855e-05]
	Learning Rate: 5.85499e-05
	LOSS [training: 0.9599185178380848 | validation: 2.575764137495286]
	TIME [epoch: 8.48 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9583748103648579		[learning rate: 5.8337e-05]
	Learning Rate: 5.83374e-05
	LOSS [training: 0.9583748103648579 | validation: 2.577783968195297]
	TIME [epoch: 8.47 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9573366700189816		[learning rate: 5.8126e-05]
	Learning Rate: 5.81257e-05
	LOSS [training: 0.9573366700189816 | validation: 2.583375911611931]
	TIME [epoch: 8.48 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9602066899523216		[learning rate: 5.7915e-05]
	Learning Rate: 5.79148e-05
	LOSS [training: 0.9602066899523216 | validation: 2.5782584132146336]
	TIME [epoch: 8.49 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.961753097631658		[learning rate: 5.7705e-05]
	Learning Rate: 5.77046e-05
	LOSS [training: 0.961753097631658 | validation: 2.5719550678192826]
	TIME [epoch: 8.48 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9569794465782586		[learning rate: 5.7495e-05]
	Learning Rate: 5.74952e-05
	LOSS [training: 0.9569794465782586 | validation: 2.5747695637644243]
	TIME [epoch: 8.47 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9522741033027973		[learning rate: 5.7287e-05]
	Learning Rate: 5.72866e-05
	LOSS [training: 0.9522741033027973 | validation: 2.58142601766556]
	TIME [epoch: 8.48 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9623045138615061		[learning rate: 5.7079e-05]
	Learning Rate: 5.70787e-05
	LOSS [training: 0.9623045138615061 | validation: 2.5721403418847473]
	TIME [epoch: 8.49 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9565852328538125		[learning rate: 5.6872e-05]
	Learning Rate: 5.68715e-05
	LOSS [training: 0.9565852328538125 | validation: 2.579855178819121]
	TIME [epoch: 8.47 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9592020453765284		[learning rate: 5.6665e-05]
	Learning Rate: 5.66651e-05
	LOSS [training: 0.9592020453765284 | validation: 2.594592918663837]
	TIME [epoch: 8.48 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.959357379745595		[learning rate: 5.6459e-05]
	Learning Rate: 5.64595e-05
	LOSS [training: 0.959357379745595 | validation: 2.5649100773251043]
	TIME [epoch: 8.47 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9613054374090211		[learning rate: 5.6255e-05]
	Learning Rate: 5.62546e-05
	LOSS [training: 0.9613054374090211 | validation: 2.581608971252301]
	TIME [epoch: 8.49 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9571529926611875		[learning rate: 5.605e-05]
	Learning Rate: 5.60504e-05
	LOSS [training: 0.9571529926611875 | validation: 2.579610335317171]
	TIME [epoch: 8.47 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9541655647213378		[learning rate: 5.5847e-05]
	Learning Rate: 5.5847e-05
	LOSS [training: 0.9541655647213378 | validation: 2.5821004443737587]
	TIME [epoch: 8.48 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9569617127616535		[learning rate: 5.5644e-05]
	Learning Rate: 5.56444e-05
	LOSS [training: 0.9569617127616535 | validation: 2.5707718350946656]
	TIME [epoch: 8.47 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9552604211062536		[learning rate: 5.5442e-05]
	Learning Rate: 5.54424e-05
	LOSS [training: 0.9552604211062536 | validation: 2.576682937099377]
	TIME [epoch: 8.5 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9563308407881154		[learning rate: 5.5241e-05]
	Learning Rate: 5.52412e-05
	LOSS [training: 0.9563308407881154 | validation: 2.5894401443523973]
	TIME [epoch: 8.47 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9565680239929264		[learning rate: 5.5041e-05]
	Learning Rate: 5.50407e-05
	LOSS [training: 0.9565680239929264 | validation: 2.58186219300459]
	TIME [epoch: 8.47 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9564177599319313		[learning rate: 5.4841e-05]
	Learning Rate: 5.4841e-05
	LOSS [training: 0.9564177599319313 | validation: 2.5793303552631817]
	TIME [epoch: 8.47 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9602870104605333		[learning rate: 5.4642e-05]
	Learning Rate: 5.4642e-05
	LOSS [training: 0.9602870104605333 | validation: 2.5886994804294914]
	TIME [epoch: 8.48 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9632738446084336		[learning rate: 5.4444e-05]
	Learning Rate: 5.44437e-05
	LOSS [training: 0.9632738446084336 | validation: 2.58961666242509]
	TIME [epoch: 8.47 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9601197369295711		[learning rate: 5.4246e-05]
	Learning Rate: 5.42461e-05
	LOSS [training: 0.9601197369295711 | validation: 2.5758144911560468]
	TIME [epoch: 8.46 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.959227105241677		[learning rate: 5.4049e-05]
	Learning Rate: 5.40492e-05
	LOSS [training: 0.959227105241677 | validation: 2.576231839473927]
	TIME [epoch: 8.48 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9618382591663244		[learning rate: 5.3853e-05]
	Learning Rate: 5.38531e-05
	LOSS [training: 0.9618382591663244 | validation: 2.5721893720592632]
	TIME [epoch: 8.48 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9617800442238436		[learning rate: 5.3658e-05]
	Learning Rate: 5.36577e-05
	LOSS [training: 0.9617800442238436 | validation: 2.568005345837252]
	TIME [epoch: 8.47 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9569173109824556		[learning rate: 5.3463e-05]
	Learning Rate: 5.34629e-05
	LOSS [training: 0.9569173109824556 | validation: 2.5678506066839177]
	TIME [epoch: 8.47 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9574870123189474		[learning rate: 5.3269e-05]
	Learning Rate: 5.32689e-05
	LOSS [training: 0.9574870123189474 | validation: 2.5682384519713297]
	TIME [epoch: 8.48 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9596639374366915		[learning rate: 5.3076e-05]
	Learning Rate: 5.30756e-05
	LOSS [training: 0.9596639374366915 | validation: 2.5789060840327935]
	TIME [epoch: 8.48 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9586431233781088		[learning rate: 5.2883e-05]
	Learning Rate: 5.2883e-05
	LOSS [training: 0.9586431233781088 | validation: 2.585118466911683]
	TIME [epoch: 8.47 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9639608520001423		[learning rate: 5.2691e-05]
	Learning Rate: 5.26911e-05
	LOSS [training: 0.9639608520001423 | validation: 2.596639243792431]
	TIME [epoch: 8.47 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9573414641742926		[learning rate: 5.25e-05]
	Learning Rate: 5.24998e-05
	LOSS [training: 0.9573414641742926 | validation: 2.5812851519670597]
	TIME [epoch: 8.48 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9566905103188408		[learning rate: 5.2309e-05]
	Learning Rate: 5.23093e-05
	LOSS [training: 0.9566905103188408 | validation: 2.564293791309327]
	TIME [epoch: 8.47 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9523344369049529		[learning rate: 5.2119e-05]
	Learning Rate: 5.21195e-05
	LOSS [training: 0.9523344369049529 | validation: 2.5822669356206926]
	TIME [epoch: 8.47 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9567704125448163		[learning rate: 5.193e-05]
	Learning Rate: 5.19303e-05
	LOSS [training: 0.9567704125448163 | validation: 2.5716026899231923]
	TIME [epoch: 8.47 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9587151801122614		[learning rate: 5.1742e-05]
	Learning Rate: 5.17419e-05
	LOSS [training: 0.9587151801122614 | validation: 2.5709264094584245]
	TIME [epoch: 8.49 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9601592989262482		[learning rate: 5.1554e-05]
	Learning Rate: 5.15541e-05
	LOSS [training: 0.9601592989262482 | validation: 2.5861995330986396]
	TIME [epoch: 8.47 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9602753099201274		[learning rate: 5.1367e-05]
	Learning Rate: 5.1367e-05
	LOSS [training: 0.9602753099201274 | validation: 2.5863122899135567]
	TIME [epoch: 8.47 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9630874202939695		[learning rate: 5.1181e-05]
	Learning Rate: 5.11806e-05
	LOSS [training: 0.9630874202939695 | validation: 2.573278107611776]
	TIME [epoch: 8.47 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9542040826102157		[learning rate: 5.0995e-05]
	Learning Rate: 5.09949e-05
	LOSS [training: 0.9542040826102157 | validation: 2.5833102225230817]
	TIME [epoch: 8.49 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9572287638143886		[learning rate: 5.081e-05]
	Learning Rate: 5.08098e-05
	LOSS [training: 0.9572287638143886 | validation: 2.5627823267489016]
	TIME [epoch: 8.47 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.956326756474265		[learning rate: 5.0625e-05]
	Learning Rate: 5.06254e-05
	LOSS [training: 0.956326756474265 | validation: 2.5786927324199977]
	TIME [epoch: 8.46 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9652342993105029		[learning rate: 5.0442e-05]
	Learning Rate: 5.04417e-05
	LOSS [training: 0.9652342993105029 | validation: 2.566944709781212]
	TIME [epoch: 8.46 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9614195005755244		[learning rate: 5.0259e-05]
	Learning Rate: 5.02586e-05
	LOSS [training: 0.9614195005755244 | validation: 2.572089409365798]
	TIME [epoch: 8.48 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9621372985618951		[learning rate: 5.0076e-05]
	Learning Rate: 5.00762e-05
	LOSS [training: 0.9621372985618951 | validation: 2.5785182352708356]
	TIME [epoch: 8.47 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9634057365992315		[learning rate: 4.9894e-05]
	Learning Rate: 4.98945e-05
	LOSS [training: 0.9634057365992315 | validation: 2.5890750306615478]
	TIME [epoch: 8.46 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9578153382166761		[learning rate: 4.9713e-05]
	Learning Rate: 4.97134e-05
	LOSS [training: 0.9578153382166761 | validation: 2.584603314125621]
	TIME [epoch: 8.47 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9556507290221049		[learning rate: 4.9533e-05]
	Learning Rate: 4.9533e-05
	LOSS [training: 0.9556507290221049 | validation: 2.5921269641270723]
	TIME [epoch: 8.49 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9637604833001401		[learning rate: 4.9353e-05]
	Learning Rate: 4.93533e-05
	LOSS [training: 0.9637604833001401 | validation: 2.5911990907221294]
	TIME [epoch: 8.46 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9566460417838017		[learning rate: 4.9174e-05]
	Learning Rate: 4.91742e-05
	LOSS [training: 0.9566460417838017 | validation: 2.592088748040406]
	TIME [epoch: 8.47 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9592021407874045		[learning rate: 4.8996e-05]
	Learning Rate: 4.89957e-05
	LOSS [training: 0.9592021407874045 | validation: 2.5745625274428803]
	TIME [epoch: 8.47 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9570395076080604		[learning rate: 4.8818e-05]
	Learning Rate: 4.88179e-05
	LOSS [training: 0.9570395076080604 | validation: 2.578712242481364]
	TIME [epoch: 8.49 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9625651438581881		[learning rate: 4.8641e-05]
	Learning Rate: 4.86407e-05
	LOSS [training: 0.9625651438581881 | validation: 2.5807137647686194]
	TIME [epoch: 8.47 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9592133302025013		[learning rate: 4.8464e-05]
	Learning Rate: 4.84642e-05
	LOSS [training: 0.9592133302025013 | validation: 2.5723747887219295]
	TIME [epoch: 8.47 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.956280319285705		[learning rate: 4.8288e-05]
	Learning Rate: 4.82883e-05
	LOSS [training: 0.956280319285705 | validation: 2.5771785366829922]
	TIME [epoch: 8.47 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9585116302611543		[learning rate: 4.8113e-05]
	Learning Rate: 4.81131e-05
	LOSS [training: 0.9585116302611543 | validation: 2.5811651868606473]
	TIME [epoch: 8.49 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9603610007442864		[learning rate: 4.7938e-05]
	Learning Rate: 4.79385e-05
	LOSS [training: 0.9603610007442864 | validation: 2.577782982593282]
	TIME [epoch: 8.47 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9542276549044159		[learning rate: 4.7765e-05]
	Learning Rate: 4.77645e-05
	LOSS [training: 0.9542276549044159 | validation: 2.570488549095355]
	TIME [epoch: 8.46 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9589014713622923		[learning rate: 4.7591e-05]
	Learning Rate: 4.75912e-05
	LOSS [training: 0.9589014713622923 | validation: 2.5731422608615535]
	TIME [epoch: 8.47 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9579472953878098		[learning rate: 4.7418e-05]
	Learning Rate: 4.74185e-05
	LOSS [training: 0.9579472953878098 | validation: 2.5756442936180193]
	TIME [epoch: 8.48 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9585170666374939		[learning rate: 4.7246e-05]
	Learning Rate: 4.72464e-05
	LOSS [training: 0.9585170666374939 | validation: 2.5860971042676155]
	TIME [epoch: 8.47 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9588866196391244		[learning rate: 4.7075e-05]
	Learning Rate: 4.70749e-05
	LOSS [training: 0.9588866196391244 | validation: 2.58835549002618]
	TIME [epoch: 8.47 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9599734702324534		[learning rate: 4.6904e-05]
	Learning Rate: 4.69041e-05
	LOSS [training: 0.9599734702324534 | validation: 2.588117925479117]
	TIME [epoch: 8.46 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9576517779029008		[learning rate: 4.6734e-05]
	Learning Rate: 4.67339e-05
	LOSS [training: 0.9576517779029008 | validation: 2.594476425326215]
	TIME [epoch: 8.49 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9535096138340791		[learning rate: 4.6564e-05]
	Learning Rate: 4.65643e-05
	LOSS [training: 0.9535096138340791 | validation: 2.5712794145266815]
	TIME [epoch: 8.46 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9575496804107673		[learning rate: 4.6395e-05]
	Learning Rate: 4.63953e-05
	LOSS [training: 0.9575496804107673 | validation: 2.59257260556578]
	TIME [epoch: 8.46 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9569609571400097		[learning rate: 4.6227e-05]
	Learning Rate: 4.62269e-05
	LOSS [training: 0.9569609571400097 | validation: 2.5870909811382115]
	TIME [epoch: 8.46 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9567812765871533		[learning rate: 4.6059e-05]
	Learning Rate: 4.60591e-05
	LOSS [training: 0.9567812765871533 | validation: 2.5867408745175107]
	TIME [epoch: 8.48 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9564044843642272		[learning rate: 4.5892e-05]
	Learning Rate: 4.5892e-05
	LOSS [training: 0.9564044843642272 | validation: 2.581638631640727]
	TIME [epoch: 8.46 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9559737535784493		[learning rate: 4.5725e-05]
	Learning Rate: 4.57254e-05
	LOSS [training: 0.9559737535784493 | validation: 2.5837354893351736]
	TIME [epoch: 8.46 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9572863126242153		[learning rate: 4.556e-05]
	Learning Rate: 4.55595e-05
	LOSS [training: 0.9572863126242153 | validation: 2.5860928702212913]
	TIME [epoch: 8.48 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9575584844816625		[learning rate: 4.5394e-05]
	Learning Rate: 4.53942e-05
	LOSS [training: 0.9575584844816625 | validation: 2.591203611846184]
	TIME [epoch: 8.48 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9572598769544752		[learning rate: 4.5229e-05]
	Learning Rate: 4.52294e-05
	LOSS [training: 0.9572598769544752 | validation: 2.596249384201811]
	TIME [epoch: 8.47 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9588304075071825		[learning rate: 4.5065e-05]
	Learning Rate: 4.50653e-05
	LOSS [training: 0.9588304075071825 | validation: 2.5921404932794]
	TIME [epoch: 8.47 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9525686240653795		[learning rate: 4.4902e-05]
	Learning Rate: 4.49017e-05
	LOSS [training: 0.9525686240653795 | validation: 2.587773679141889]
	TIME [epoch: 8.48 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9553725358252201		[learning rate: 4.4739e-05]
	Learning Rate: 4.47388e-05
	LOSS [training: 0.9553725358252201 | validation: 2.575321760111428]
	TIME [epoch: 8.47 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9623589832848248		[learning rate: 4.4576e-05]
	Learning Rate: 4.45764e-05
	LOSS [training: 0.9623589832848248 | validation: 2.5785938837270703]
	TIME [epoch: 8.48 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9597700552481825		[learning rate: 4.4415e-05]
	Learning Rate: 4.44147e-05
	LOSS [training: 0.9597700552481825 | validation: 2.5783339723163516]
	TIME [epoch: 8.47 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9584471926665626		[learning rate: 4.4253e-05]
	Learning Rate: 4.42535e-05
	LOSS [training: 0.9584471926665626 | validation: 2.574142248573751]
	TIME [epoch: 8.49 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.954830659326867		[learning rate: 4.4093e-05]
	Learning Rate: 4.40929e-05
	LOSS [training: 0.954830659326867 | validation: 2.5869607241991988]
	TIME [epoch: 8.48 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9608490056460779		[learning rate: 4.3933e-05]
	Learning Rate: 4.39329e-05
	LOSS [training: 0.9608490056460779 | validation: 2.583728960321424]
	TIME [epoch: 8.47 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9553169392422601		[learning rate: 4.3773e-05]
	Learning Rate: 4.37734e-05
	LOSS [training: 0.9553169392422601 | validation: 2.583650731205989]
	TIME [epoch: 8.47 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9608262633455873		[learning rate: 4.3615e-05]
	Learning Rate: 4.36146e-05
	LOSS [training: 0.9608262633455873 | validation: 2.576811968712795]
	TIME [epoch: 8.49 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9565476930850121		[learning rate: 4.3456e-05]
	Learning Rate: 4.34563e-05
	LOSS [training: 0.9565476930850121 | validation: 2.59182895521785]
	TIME [epoch: 8.47 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9534324695067087		[learning rate: 4.3299e-05]
	Learning Rate: 4.32986e-05
	LOSS [training: 0.9534324695067087 | validation: 2.577044443550092]
	TIME [epoch: 8.47 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9588478884625704		[learning rate: 4.3141e-05]
	Learning Rate: 4.31415e-05
	LOSS [training: 0.9588478884625704 | validation: 2.593894690865938]
	TIME [epoch: 8.47 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9592663611909599		[learning rate: 4.2985e-05]
	Learning Rate: 4.29849e-05
	LOSS [training: 0.9592663611909599 | validation: 2.5870556607637334]
	TIME [epoch: 8.49 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9585914420402692		[learning rate: 4.2829e-05]
	Learning Rate: 4.28289e-05
	LOSS [training: 0.9585914420402692 | validation: 2.5739309587409975]
	TIME [epoch: 8.47 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9562049549162477		[learning rate: 4.2673e-05]
	Learning Rate: 4.26735e-05
	LOSS [training: 0.9562049549162477 | validation: 2.5855704332718745]
	TIME [epoch: 8.48 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9594678142006607		[learning rate: 4.2519e-05]
	Learning Rate: 4.25186e-05
	LOSS [training: 0.9594678142006607 | validation: 2.598229731478107]
	TIME [epoch: 8.47 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9695719266769116		[learning rate: 4.2364e-05]
	Learning Rate: 4.23643e-05
	LOSS [training: 0.9695719266769116 | validation: 2.592313808200797]
	TIME [epoch: 8.49 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9622271497761317		[learning rate: 4.2211e-05]
	Learning Rate: 4.22106e-05
	LOSS [training: 0.9622271497761317 | validation: 2.5721936456731407]
	TIME [epoch: 8.46 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9555196552215028		[learning rate: 4.2057e-05]
	Learning Rate: 4.20574e-05
	LOSS [training: 0.9555196552215028 | validation: 2.5755438676597486]
	TIME [epoch: 8.47 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9594577357983287		[learning rate: 4.1905e-05]
	Learning Rate: 4.19047e-05
	LOSS [training: 0.9594577357983287 | validation: 2.577985026055881]
	TIME [epoch: 8.46 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9562067088717345		[learning rate: 4.1753e-05]
	Learning Rate: 4.17527e-05
	LOSS [training: 0.9562067088717345 | validation: 2.582693251572321]
	TIME [epoch: 8.5 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9560006263211218		[learning rate: 4.1601e-05]
	Learning Rate: 4.16012e-05
	LOSS [training: 0.9560006263211218 | validation: 2.570786915143711]
	TIME [epoch: 8.48 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9594717856952686		[learning rate: 4.145e-05]
	Learning Rate: 4.14502e-05
	LOSS [training: 0.9594717856952686 | validation: 2.57303703035685]
	TIME [epoch: 8.48 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9531228317229397		[learning rate: 4.13e-05]
	Learning Rate: 4.12998e-05
	LOSS [training: 0.9531228317229397 | validation: 2.5803590314691633]
	TIME [epoch: 8.47 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9591016191284553		[learning rate: 4.115e-05]
	Learning Rate: 4.11499e-05
	LOSS [training: 0.9591016191284553 | validation: 2.5745515220524293]
	TIME [epoch: 8.49 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9526678904048751		[learning rate: 4.1001e-05]
	Learning Rate: 4.10005e-05
	LOSS [training: 0.9526678904048751 | validation: 2.57855751080116]
	TIME [epoch: 8.47 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9605298929967402		[learning rate: 4.0852e-05]
	Learning Rate: 4.08517e-05
	LOSS [training: 0.9605298929967402 | validation: 2.5752280268638477]
	TIME [epoch: 8.47 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9561611310715584		[learning rate: 4.0703e-05]
	Learning Rate: 4.07035e-05
	LOSS [training: 0.9561611310715584 | validation: 2.573155669456578]
	TIME [epoch: 8.47 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9547775588652018		[learning rate: 4.0556e-05]
	Learning Rate: 4.05558e-05
	LOSS [training: 0.9547775588652018 | validation: 2.5746339263838696]
	TIME [epoch: 8.5 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9538345381273556		[learning rate: 4.0409e-05]
	Learning Rate: 4.04086e-05
	LOSS [training: 0.9538345381273556 | validation: 2.592431816808981]
	TIME [epoch: 8.47 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9535550031765199		[learning rate: 4.0262e-05]
	Learning Rate: 4.0262e-05
	LOSS [training: 0.9535550031765199 | validation: 2.570914500287624]
	TIME [epoch: 8.46 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9576464099592379		[learning rate: 4.0116e-05]
	Learning Rate: 4.01158e-05
	LOSS [training: 0.9576464099592379 | validation: 2.5728964677094206]
	TIME [epoch: 8.47 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9574025083785273		[learning rate: 3.997e-05]
	Learning Rate: 3.99703e-05
	LOSS [training: 0.9574025083785273 | validation: 2.573821203773332]
	TIME [epoch: 8.49 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9562670514114779		[learning rate: 3.9825e-05]
	Learning Rate: 3.98252e-05
	LOSS [training: 0.9562670514114779 | validation: 2.5838205616290923]
	TIME [epoch: 8.47 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9561929536071256		[learning rate: 3.9681e-05]
	Learning Rate: 3.96807e-05
	LOSS [training: 0.9561929536071256 | validation: 2.583400054327339]
	TIME [epoch: 8.47 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9560460962142916		[learning rate: 3.9537e-05]
	Learning Rate: 3.95367e-05
	LOSS [training: 0.9560460962142916 | validation: 2.5861126303788713]
	TIME [epoch: 8.48 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9583108303734663		[learning rate: 3.9393e-05]
	Learning Rate: 3.93932e-05
	LOSS [training: 0.9583108303734663 | validation: 2.5814845558185855]
	TIME [epoch: 8.48 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9601574260605075		[learning rate: 3.925e-05]
	Learning Rate: 3.92502e-05
	LOSS [training: 0.9601574260605075 | validation: 2.588378892182424]
	TIME [epoch: 8.47 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9583702031458297		[learning rate: 3.9108e-05]
	Learning Rate: 3.91078e-05
	LOSS [training: 0.9583702031458297 | validation: 2.5780095396828275]
	TIME [epoch: 8.47 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9586315063615579		[learning rate: 3.8966e-05]
	Learning Rate: 3.89659e-05
	LOSS [training: 0.9586315063615579 | validation: 2.5703989772045883]
	TIME [epoch: 8.48 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.961828993801887		[learning rate: 3.8824e-05]
	Learning Rate: 3.88245e-05
	LOSS [training: 0.961828993801887 | validation: 2.590874727964697]
	TIME [epoch: 8.48 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9546596009912607		[learning rate: 3.8684e-05]
	Learning Rate: 3.86836e-05
	LOSS [training: 0.9546596009912607 | validation: 2.577228558460254]
	TIME [epoch: 8.47 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9542606092347355		[learning rate: 3.8543e-05]
	Learning Rate: 3.85432e-05
	LOSS [training: 0.9542606092347355 | validation: 2.5767687022429473]
	TIME [epoch: 8.47 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9534800867108914		[learning rate: 3.8403e-05]
	Learning Rate: 3.84033e-05
	LOSS [training: 0.9534800867108914 | validation: 2.5780220171336725]
	TIME [epoch: 8.49 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9590190324690437		[learning rate: 3.8264e-05]
	Learning Rate: 3.82639e-05
	LOSS [training: 0.9590190324690437 | validation: 2.601455509616701]
	TIME [epoch: 8.48 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9634836326340661		[learning rate: 3.8125e-05]
	Learning Rate: 3.81251e-05
	LOSS [training: 0.9634836326340661 | validation: 2.6032620529055683]
	TIME [epoch: 8.46 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9642965601236412		[learning rate: 3.7987e-05]
	Learning Rate: 3.79867e-05
	LOSS [training: 0.9642965601236412 | validation: 2.6065087183157907]
	TIME [epoch: 8.46 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9595901207603369		[learning rate: 3.7849e-05]
	Learning Rate: 3.78489e-05
	LOSS [training: 0.9595901207603369 | validation: 2.580521476176899]
	TIME [epoch: 8.48 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9526519309826688		[learning rate: 3.7711e-05]
	Learning Rate: 3.77115e-05
	LOSS [training: 0.9526519309826688 | validation: 2.5888268199970543]
	TIME [epoch: 8.48 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.957038413031681		[learning rate: 3.7575e-05]
	Learning Rate: 3.75746e-05
	LOSS [training: 0.957038413031681 | validation: 2.58261615048554]
	TIME [epoch: 8.46 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9559850044815411		[learning rate: 3.7438e-05]
	Learning Rate: 3.74383e-05
	LOSS [training: 0.9559850044815411 | validation: 2.5710516732854893]
	TIME [epoch: 8.47 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9616784339510707		[learning rate: 3.7302e-05]
	Learning Rate: 3.73024e-05
	LOSS [training: 0.9616784339510707 | validation: 2.579024100182025]
	TIME [epoch: 8.49 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9609007752399268		[learning rate: 3.7167e-05]
	Learning Rate: 3.7167e-05
	LOSS [training: 0.9609007752399268 | validation: 2.5717698474553146]
	TIME [epoch: 8.47 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9576758195539015		[learning rate: 3.7032e-05]
	Learning Rate: 3.70322e-05
	LOSS [training: 0.9576758195539015 | validation: 2.579872209014657]
	TIME [epoch: 8.47 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9551402571344649		[learning rate: 3.6898e-05]
	Learning Rate: 3.68978e-05
	LOSS [training: 0.9551402571344649 | validation: 2.576467431775094]
	TIME [epoch: 8.47 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9586486291618168		[learning rate: 3.6764e-05]
	Learning Rate: 3.67639e-05
	LOSS [training: 0.9586486291618168 | validation: 2.571879562071015]
	TIME [epoch: 8.48 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9609477078980113		[learning rate: 3.663e-05]
	Learning Rate: 3.66304e-05
	LOSS [training: 0.9609477078980113 | validation: 2.58110445969082]
	TIME [epoch: 8.48 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9555599760893889		[learning rate: 3.6498e-05]
	Learning Rate: 3.64975e-05
	LOSS [training: 0.9555599760893889 | validation: 2.57659360164648]
	TIME [epoch: 8.47 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9577942518374302		[learning rate: 3.6365e-05]
	Learning Rate: 3.63651e-05
	LOSS [training: 0.9577942518374302 | validation: 2.5730222405799563]
	TIME [epoch: 8.47 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9547053148861281		[learning rate: 3.6233e-05]
	Learning Rate: 3.62331e-05
	LOSS [training: 0.9547053148861281 | validation: 2.5806265825696517]
	TIME [epoch: 8.5 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9589754827927202		[learning rate: 3.6102e-05]
	Learning Rate: 3.61016e-05
	LOSS [training: 0.9589754827927202 | validation: 2.5768255861712133]
	TIME [epoch: 8.47 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9596178469458787		[learning rate: 3.5971e-05]
	Learning Rate: 3.59706e-05
	LOSS [training: 0.9596178469458787 | validation: 2.5723043867386157]
	TIME [epoch: 8.47 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9613898360537705		[learning rate: 3.584e-05]
	Learning Rate: 3.584e-05
	LOSS [training: 0.9613898360537705 | validation: 2.580327646783508]
	TIME [epoch: 8.46 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9575878090987816		[learning rate: 3.571e-05]
	Learning Rate: 3.571e-05
	LOSS [training: 0.9575878090987816 | validation: 2.567319455817897]
	TIME [epoch: 8.49 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9584572973578946		[learning rate: 3.558e-05]
	Learning Rate: 3.55804e-05
	LOSS [training: 0.9584572973578946 | validation: 2.573294537309968]
	TIME [epoch: 8.47 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9561222675181943		[learning rate: 3.5451e-05]
	Learning Rate: 3.54513e-05
	LOSS [training: 0.9561222675181943 | validation: 2.5908774798511427]
	TIME [epoch: 8.47 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9565245646582998		[learning rate: 3.5323e-05]
	Learning Rate: 3.53226e-05
	LOSS [training: 0.9565245646582998 | validation: 2.576264030999727]
	TIME [epoch: 8.47 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9565829862649455		[learning rate: 3.5194e-05]
	Learning Rate: 3.51944e-05
	LOSS [training: 0.9565829862649455 | validation: 2.58364333805985]
	TIME [epoch: 8.49 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9579216284322788		[learning rate: 3.5067e-05]
	Learning Rate: 3.50667e-05
	LOSS [training: 0.9579216284322788 | validation: 2.582180925046817]
	TIME [epoch: 8.47 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9555048975940872		[learning rate: 3.4939e-05]
	Learning Rate: 3.49394e-05
	LOSS [training: 0.9555048975940872 | validation: 2.588082578132455]
	TIME [epoch: 8.47 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9606060384826278		[learning rate: 3.4813e-05]
	Learning Rate: 3.48126e-05
	LOSS [training: 0.9606060384826278 | validation: 2.5804099012641606]
	TIME [epoch: 8.47 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9558312899043455		[learning rate: 3.4686e-05]
	Learning Rate: 3.46863e-05
	LOSS [training: 0.9558312899043455 | validation: 2.5808271002706804]
	TIME [epoch: 8.49 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.956519325430145		[learning rate: 3.456e-05]
	Learning Rate: 3.45604e-05
	LOSS [training: 0.956519325430145 | validation: 2.5728236409489047]
	TIME [epoch: 8.46 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9547622627720033		[learning rate: 3.4435e-05]
	Learning Rate: 3.4435e-05
	LOSS [training: 0.9547622627720033 | validation: 2.570323500847317]
	TIME [epoch: 8.46 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9586699881438371		[learning rate: 3.431e-05]
	Learning Rate: 3.431e-05
	LOSS [training: 0.9586699881438371 | validation: 2.57611957133066]
	TIME [epoch: 8.47 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9600013578976275		[learning rate: 3.4186e-05]
	Learning Rate: 3.41855e-05
	LOSS [training: 0.9600013578976275 | validation: 2.5747817761984386]
	TIME [epoch: 8.47 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.956134445550959		[learning rate: 3.4061e-05]
	Learning Rate: 3.40615e-05
	LOSS [training: 0.956134445550959 | validation: 2.5800322238478643]
	TIME [epoch: 8.47 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.958040725810316		[learning rate: 3.3938e-05]
	Learning Rate: 3.39378e-05
	LOSS [training: 0.958040725810316 | validation: 2.577299312361509]
	TIME [epoch: 8.47 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.953114374101147		[learning rate: 3.3815e-05]
	Learning Rate: 3.38147e-05
	LOSS [training: 0.953114374101147 | validation: 2.5878462561813524]
	TIME [epoch: 8.48 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9568658038106619		[learning rate: 3.3692e-05]
	Learning Rate: 3.3692e-05
	LOSS [training: 0.9568658038106619 | validation: 2.5826297122681745]
	TIME [epoch: 8.49 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9583376900023243		[learning rate: 3.357e-05]
	Learning Rate: 3.35697e-05
	LOSS [training: 0.9583376900023243 | validation: 2.5761897416777844]
	TIME [epoch: 8.47 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9562504136237621		[learning rate: 3.3448e-05]
	Learning Rate: 3.34479e-05
	LOSS [training: 0.9562504136237621 | validation: 2.5833037532964447]
	TIME [epoch: 8.46 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9554724544192554		[learning rate: 3.3326e-05]
	Learning Rate: 3.33265e-05
	LOSS [training: 0.9554724544192554 | validation: 2.571392294613771]
	TIME [epoch: 8.48 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9580759629279496		[learning rate: 3.3206e-05]
	Learning Rate: 3.32055e-05
	LOSS [training: 0.9580759629279496 | validation: 2.5816592754928434]
	TIME [epoch: 8.47 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9556195810353945		[learning rate: 3.3085e-05]
	Learning Rate: 3.3085e-05
	LOSS [training: 0.9556195810353945 | validation: 2.5817834657590524]
	TIME [epoch: 8.47 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9515333924767846		[learning rate: 3.2965e-05]
	Learning Rate: 3.2965e-05
	LOSS [training: 0.9515333924767846 | validation: 2.5768537093473136]
	TIME [epoch: 8.46 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9587500403243983		[learning rate: 3.2845e-05]
	Learning Rate: 3.28453e-05
	LOSS [training: 0.9587500403243983 | validation: 2.577893828984278]
	TIME [epoch: 8.48 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9598648973699964		[learning rate: 3.2726e-05]
	Learning Rate: 3.27261e-05
	LOSS [training: 0.9598648973699964 | validation: 2.5816043007199134]
	TIME [epoch: 8.48 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9570330890686323		[learning rate: 3.2607e-05]
	Learning Rate: 3.26074e-05
	LOSS [training: 0.9570330890686323 | validation: 2.5794533677056717]
	TIME [epoch: 8.47 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9574662637452324		[learning rate: 3.2489e-05]
	Learning Rate: 3.2489e-05
	LOSS [training: 0.9574662637452324 | validation: 2.5864724160557255]
	TIME [epoch: 8.47 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9553530933947416		[learning rate: 3.2371e-05]
	Learning Rate: 3.23711e-05
	LOSS [training: 0.9553530933947416 | validation: 2.5784990355963364]
	TIME [epoch: 8.48 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.955406227137005		[learning rate: 3.2254e-05]
	Learning Rate: 3.22537e-05
	LOSS [training: 0.955406227137005 | validation: 2.580435808533373]
	TIME [epoch: 8.47 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9578352235515519		[learning rate: 3.2137e-05]
	Learning Rate: 3.21366e-05
	LOSS [training: 0.9578352235515519 | validation: 2.5884372847544213]
	TIME [epoch: 8.46 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9552093371924592		[learning rate: 3.202e-05]
	Learning Rate: 3.202e-05
	LOSS [training: 0.9552093371924592 | validation: 2.5772900344292466]
	TIME [epoch: 8.47 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9518301870331548		[learning rate: 3.1904e-05]
	Learning Rate: 3.19038e-05
	LOSS [training: 0.9518301870331548 | validation: 2.5727732991975367]
	TIME [epoch: 8.49 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9582213650419338		[learning rate: 3.1788e-05]
	Learning Rate: 3.1788e-05
	LOSS [training: 0.9582213650419338 | validation: 2.582811224794944]
	TIME [epoch: 8.47 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.960094426647		[learning rate: 3.1673e-05]
	Learning Rate: 3.16726e-05
	LOSS [training: 0.960094426647 | validation: 2.572089371986197]
	TIME [epoch: 8.47 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9541596105264117		[learning rate: 3.1558e-05]
	Learning Rate: 3.15577e-05
	LOSS [training: 0.9541596105264117 | validation: 2.5778275686014274]
	TIME [epoch: 8.47 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9534277046816821		[learning rate: 3.1443e-05]
	Learning Rate: 3.14432e-05
	LOSS [training: 0.9534277046816821 | validation: 2.583755235315646]
	TIME [epoch: 8.49 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9558571334433316		[learning rate: 3.1329e-05]
	Learning Rate: 3.13291e-05
	LOSS [training: 0.9558571334433316 | validation: 2.5800402787302152]
	TIME [epoch: 8.47 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9570721832226112		[learning rate: 3.1215e-05]
	Learning Rate: 3.12154e-05
	LOSS [training: 0.9570721832226112 | validation: 2.5763098835597305]
	TIME [epoch: 8.48 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.957063233133753		[learning rate: 3.1102e-05]
	Learning Rate: 3.11021e-05
	LOSS [training: 0.957063233133753 | validation: 2.570672938158056]
	TIME [epoch: 8.47 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9547014887547283		[learning rate: 3.0989e-05]
	Learning Rate: 3.09892e-05
	LOSS [training: 0.9547014887547283 | validation: 2.583094365382526]
	TIME [epoch: 8.49 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9557310005293465		[learning rate: 3.0877e-05]
	Learning Rate: 3.08768e-05
	LOSS [training: 0.9557310005293465 | validation: 2.570582852346225]
	TIME [epoch: 8.46 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9634106120526008		[learning rate: 3.0765e-05]
	Learning Rate: 3.07647e-05
	LOSS [training: 0.9634106120526008 | validation: 2.5684903869244535]
	TIME [epoch: 8.47 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.958177153286694		[learning rate: 3.0653e-05]
	Learning Rate: 3.0653e-05
	LOSS [training: 0.958177153286694 | validation: 2.586672631396646]
	TIME [epoch: 8.47 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.959927553697451		[learning rate: 3.0542e-05]
	Learning Rate: 3.05418e-05
	LOSS [training: 0.959927553697451 | validation: 2.5828520125051857]
	TIME [epoch: 8.48 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9602796612368893		[learning rate: 3.0431e-05]
	Learning Rate: 3.0431e-05
	LOSS [training: 0.9602796612368893 | validation: 2.574173447070281]
	TIME [epoch: 8.47 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9520835287410592		[learning rate: 3.0321e-05]
	Learning Rate: 3.03205e-05
	LOSS [training: 0.9520835287410592 | validation: 2.5739405302317384]
	TIME [epoch: 8.46 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9576028106259505		[learning rate: 3.0211e-05]
	Learning Rate: 3.02105e-05
	LOSS [training: 0.9576028106259505 | validation: 2.583559455122768]
	TIME [epoch: 8.46 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9565892494377193		[learning rate: 3.0101e-05]
	Learning Rate: 3.01009e-05
	LOSS [training: 0.9565892494377193 | validation: 2.5847625867102093]
	TIME [epoch: 8.49 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9578169105122667		[learning rate: 2.9992e-05]
	Learning Rate: 2.99916e-05
	LOSS [training: 0.9578169105122667 | validation: 2.586207230264749]
	TIME [epoch: 8.47 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9560891811941252		[learning rate: 2.9883e-05]
	Learning Rate: 2.98828e-05
	LOSS [training: 0.9560891811941252 | validation: 2.5807969245934577]
	TIME [epoch: 8.47 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9594765764904073		[learning rate: 2.9774e-05]
	Learning Rate: 2.97743e-05
	LOSS [training: 0.9594765764904073 | validation: 2.573945455574135]
	TIME [epoch: 8.46 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.96116587335601		[learning rate: 2.9666e-05]
	Learning Rate: 2.96663e-05
	LOSS [training: 0.96116587335601 | validation: 2.5671871707521854]
	TIME [epoch: 8.49 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9572037396245523		[learning rate: 2.9559e-05]
	Learning Rate: 2.95586e-05
	LOSS [training: 0.9572037396245523 | validation: 2.579878879441823]
	TIME [epoch: 8.46 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9564189229658651		[learning rate: 2.9451e-05]
	Learning Rate: 2.94514e-05
	LOSS [training: 0.9564189229658651 | validation: 2.5780450055391206]
	TIME [epoch: 8.46 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9569077487462614		[learning rate: 2.9344e-05]
	Learning Rate: 2.93445e-05
	LOSS [training: 0.9569077487462614 | validation: 2.5748606790684088]
	TIME [epoch: 8.47 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9592485360200043		[learning rate: 2.9238e-05]
	Learning Rate: 2.9238e-05
	LOSS [training: 0.9592485360200043 | validation: 2.57389091065917]
	TIME [epoch: 8.48 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9585864973160806		[learning rate: 2.9132e-05]
	Learning Rate: 2.91319e-05
	LOSS [training: 0.9585864973160806 | validation: 2.5784167067176473]
	TIME [epoch: 8.46 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9573379522184935		[learning rate: 2.9026e-05]
	Learning Rate: 2.90262e-05
	LOSS [training: 0.9573379522184935 | validation: 2.575726301451507]
	TIME [epoch: 8.47 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9518926842580375		[learning rate: 2.8921e-05]
	Learning Rate: 2.89208e-05
	LOSS [training: 0.9518926842580375 | validation: 2.5846944132265466]
	TIME [epoch: 8.48 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9599484781481207		[learning rate: 2.8816e-05]
	Learning Rate: 2.88159e-05
	LOSS [training: 0.9599484781481207 | validation: 2.5887897679937617]
	TIME [epoch: 8.47 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9562655929539152		[learning rate: 2.8711e-05]
	Learning Rate: 2.87113e-05
	LOSS [training: 0.9562655929539152 | validation: 2.5906531452756423]
	TIME [epoch: 8.47 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9548555576707468		[learning rate: 2.8607e-05]
	Learning Rate: 2.86071e-05
	LOSS [training: 0.9548555576707468 | validation: 2.581055265039354]
	TIME [epoch: 8.46 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9583978261079158		[learning rate: 2.8503e-05]
	Learning Rate: 2.85033e-05
	LOSS [training: 0.9583978261079158 | validation: 2.569728606607769]
	TIME [epoch: 8.48 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.960277066932249		[learning rate: 2.84e-05]
	Learning Rate: 2.83998e-05
	LOSS [training: 0.960277066932249 | validation: 2.569139656984402]
	TIME [epoch: 8.47 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.961183700782166		[learning rate: 2.8297e-05]
	Learning Rate: 2.82968e-05
	LOSS [training: 0.961183700782166 | validation: 2.574242545414158]
	TIME [epoch: 8.46 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9620279762978592		[learning rate: 2.8194e-05]
	Learning Rate: 2.81941e-05
	LOSS [training: 0.9620279762978592 | validation: 2.567461898254046]
	TIME [epoch: 8.46 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9573238822416167		[learning rate: 2.8092e-05]
	Learning Rate: 2.80918e-05
	LOSS [training: 0.9573238822416167 | validation: 2.5710887905476074]
	TIME [epoch: 8.48 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9604801282793367		[learning rate: 2.799e-05]
	Learning Rate: 2.79898e-05
	LOSS [training: 0.9604801282793367 | validation: 2.581728743949164]
	TIME [epoch: 8.48 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9567254757288527		[learning rate: 2.7888e-05]
	Learning Rate: 2.78882e-05
	LOSS [training: 0.9567254757288527 | validation: 2.580685136875285]
	TIME [epoch: 8.47 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9566305546881837		[learning rate: 2.7787e-05]
	Learning Rate: 2.7787e-05
	LOSS [training: 0.9566305546881837 | validation: 2.576038816942276]
	TIME [epoch: 8.47 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9584214519982341		[learning rate: 2.7686e-05]
	Learning Rate: 2.76862e-05
	LOSS [training: 0.9584214519982341 | validation: 2.5783849486266055]
	TIME [epoch: 8.49 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9569243203358324		[learning rate: 2.7586e-05]
	Learning Rate: 2.75857e-05
	LOSS [training: 0.9569243203358324 | validation: 2.5773937894696455]
	TIME [epoch: 8.48 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9542288109196724		[learning rate: 2.7486e-05]
	Learning Rate: 2.74856e-05
	LOSS [training: 0.9542288109196724 | validation: 2.5723559007645997]
	TIME [epoch: 8.47 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9566596175674457		[learning rate: 2.7386e-05]
	Learning Rate: 2.73859e-05
	LOSS [training: 0.9566596175674457 | validation: 2.572661096565608]
	TIME [epoch: 8.46 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9604096621982118		[learning rate: 2.7286e-05]
	Learning Rate: 2.72865e-05
	LOSS [training: 0.9604096621982118 | validation: 2.578097947684083]
	TIME [epoch: 8.49 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9585280720278911		[learning rate: 2.7187e-05]
	Learning Rate: 2.71875e-05
	LOSS [training: 0.9585280720278911 | validation: 2.5869993493753816]
	TIME [epoch: 8.48 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9571277131954921		[learning rate: 2.7089e-05]
	Learning Rate: 2.70888e-05
	LOSS [training: 0.9571277131954921 | validation: 2.5675539420384075]
	TIME [epoch: 8.47 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9555612120477239		[learning rate: 2.699e-05]
	Learning Rate: 2.69905e-05
	LOSS [training: 0.9555612120477239 | validation: 2.5810009095399806]
	TIME [epoch: 8.47 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9530405520366733		[learning rate: 2.6893e-05]
	Learning Rate: 2.68925e-05
	LOSS [training: 0.9530405520366733 | validation: 2.5615228089574242]
	TIME [epoch: 8.48 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9572388428758118		[learning rate: 2.6795e-05]
	Learning Rate: 2.67949e-05
	LOSS [training: 0.9572388428758118 | validation: 2.571292675922125]
	TIME [epoch: 8.47 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9580447968943686		[learning rate: 2.6698e-05]
	Learning Rate: 2.66977e-05
	LOSS [training: 0.9580447968943686 | validation: 2.5842220368083253]
	TIME [epoch: 8.47 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.957098469717172		[learning rate: 2.6601e-05]
	Learning Rate: 2.66008e-05
	LOSS [training: 0.957098469717172 | validation: 2.5795616329965596]
	TIME [epoch: 8.47 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9594205629615832		[learning rate: 2.6504e-05]
	Learning Rate: 2.65043e-05
	LOSS [training: 0.9594205629615832 | validation: 2.5842241270807995]
	TIME [epoch: 8.49 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9542720116624567		[learning rate: 2.6408e-05]
	Learning Rate: 2.64081e-05
	LOSS [training: 0.9542720116624567 | validation: 2.5780092943042296]
	TIME [epoch: 8.47 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9517959618251958		[learning rate: 2.6312e-05]
	Learning Rate: 2.63123e-05
	LOSS [training: 0.9517959618251958 | validation: 2.580891813157246]
	TIME [epoch: 8.47 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9510092642483569		[learning rate: 2.6217e-05]
	Learning Rate: 2.62168e-05
	LOSS [training: 0.9510092642483569 | validation: 2.5739779371295417]
	TIME [epoch: 8.46 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.95728393983319		[learning rate: 2.6122e-05]
	Learning Rate: 2.61216e-05
	LOSS [training: 0.95728393983319 | validation: 2.5786074623804187]
	TIME [epoch: 8.48 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.955882039490225		[learning rate: 2.6027e-05]
	Learning Rate: 2.60268e-05
	LOSS [training: 0.955882039490225 | validation: 2.581734010150802]
	TIME [epoch: 8.47 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9579226452524331		[learning rate: 2.5932e-05]
	Learning Rate: 2.59324e-05
	LOSS [training: 0.9579226452524331 | validation: 2.572831432912036]
	TIME [epoch: 8.46 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9555280771745933		[learning rate: 2.5838e-05]
	Learning Rate: 2.58383e-05
	LOSS [training: 0.9555280771745933 | validation: 2.570980748960131]
	TIME [epoch: 8.47 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9600623684270442		[learning rate: 2.5744e-05]
	Learning Rate: 2.57445e-05
	LOSS [training: 0.9600623684270442 | validation: 2.593737350030149]
	TIME [epoch: 8.48 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9606142864524072		[learning rate: 2.5651e-05]
	Learning Rate: 2.56511e-05
	LOSS [training: 0.9606142864524072 | validation: 2.59316580809001]
	TIME [epoch: 8.46 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.959411028065938		[learning rate: 2.5558e-05]
	Learning Rate: 2.5558e-05
	LOSS [training: 0.959411028065938 | validation: 2.5922110014658903]
	TIME [epoch: 8.46 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9573834426738943		[learning rate: 2.5465e-05]
	Learning Rate: 2.54652e-05
	LOSS [training: 0.9573834426738943 | validation: 2.5740078083595224]
	TIME [epoch: 8.47 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9552351173058463		[learning rate: 2.5373e-05]
	Learning Rate: 2.53728e-05
	LOSS [training: 0.9552351173058463 | validation: 2.57373640082747]
	TIME [epoch: 8.49 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9539576776631014		[learning rate: 2.5281e-05]
	Learning Rate: 2.52807e-05
	LOSS [training: 0.9539576776631014 | validation: 2.594418876765306]
	TIME [epoch: 8.47 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9528624224843079		[learning rate: 2.5189e-05]
	Learning Rate: 2.5189e-05
	LOSS [training: 0.9528624224843079 | validation: 2.5760538013921526]
	TIME [epoch: 8.46 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9563698561847378		[learning rate: 2.5098e-05]
	Learning Rate: 2.50976e-05
	LOSS [training: 0.9563698561847378 | validation: 2.57981069531043]
	TIME [epoch: 8.47 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9605091323250804		[learning rate: 2.5006e-05]
	Learning Rate: 2.50065e-05
	LOSS [training: 0.9605091323250804 | validation: 2.569098160258165]
	TIME [epoch: 8.48 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9573866003389842		[learning rate: 2.4916e-05]
	Learning Rate: 2.49157e-05
	LOSS [training: 0.9573866003389842 | validation: 2.5627110560850213]
	TIME [epoch: 8.47 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9569855755866336		[learning rate: 2.4825e-05]
	Learning Rate: 2.48253e-05
	LOSS [training: 0.9569855755866336 | validation: 2.5769878370743062]
	TIME [epoch: 8.46 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.953969280127604		[learning rate: 2.4735e-05]
	Learning Rate: 2.47352e-05
	LOSS [training: 0.953969280127604 | validation: 2.568397582964569]
	TIME [epoch: 8.47 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9597268358054862		[learning rate: 2.4645e-05]
	Learning Rate: 2.46455e-05
	LOSS [training: 0.9597268358054862 | validation: 2.5822984164827516]
	TIME [epoch: 8.47 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9507934762881753		[learning rate: 2.4556e-05]
	Learning Rate: 2.4556e-05
	LOSS [training: 0.9507934762881753 | validation: 2.5730920529351695]
	TIME [epoch: 8.46 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9543219562958953		[learning rate: 2.4467e-05]
	Learning Rate: 2.44669e-05
	LOSS [training: 0.9543219562958953 | validation: 2.5700287945798754]
	TIME [epoch: 8.46 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9602383251612299		[learning rate: 2.4378e-05]
	Learning Rate: 2.43781e-05
	LOSS [training: 0.9602383251612299 | validation: 2.5726868209817026]
	TIME [epoch: 8.47 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9571909417084342		[learning rate: 2.429e-05]
	Learning Rate: 2.42896e-05
	LOSS [training: 0.9571909417084342 | validation: 2.5732643469332963]
	TIME [epoch: 8.47 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9596528467508927		[learning rate: 2.4201e-05]
	Learning Rate: 2.42015e-05
	LOSS [training: 0.9596528467508927 | validation: 2.5846321673053962]
	TIME [epoch: 8.46 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9623853204249727		[learning rate: 2.4114e-05]
	Learning Rate: 2.41137e-05
	LOSS [training: 0.9623853204249727 | validation: 2.575468279615051]
	TIME [epoch: 8.46 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9550048535825075		[learning rate: 2.4026e-05]
	Learning Rate: 2.40262e-05
	LOSS [training: 0.9550048535825075 | validation: 2.579376555909206]
	TIME [epoch: 8.47 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9565078593023537		[learning rate: 2.3939e-05]
	Learning Rate: 2.3939e-05
	LOSS [training: 0.9565078593023537 | validation: 2.5656506734547406]
	TIME [epoch: 8.47 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9580476339220227		[learning rate: 2.3852e-05]
	Learning Rate: 2.38521e-05
	LOSS [training: 0.9580476339220227 | validation: 2.5810389052989438]
	TIME [epoch: 8.46 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9567966493111733		[learning rate: 2.3766e-05]
	Learning Rate: 2.37655e-05
	LOSS [training: 0.9567966493111733 | validation: 2.5757222253288523]
	TIME [epoch: 8.45 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9548787019259668		[learning rate: 2.3679e-05]
	Learning Rate: 2.36793e-05
	LOSS [training: 0.9548787019259668 | validation: 2.5834598947512597]
	TIME [epoch: 8.49 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9624101429254548		[learning rate: 2.3593e-05]
	Learning Rate: 2.35933e-05
	LOSS [training: 0.9624101429254548 | validation: 2.5717928734433295]
	TIME [epoch: 8.47 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.952143800621721		[learning rate: 2.3508e-05]
	Learning Rate: 2.35077e-05
	LOSS [training: 0.952143800621721 | validation: 2.580147328846917]
	TIME [epoch: 8.47 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.950160408043725		[learning rate: 2.3422e-05]
	Learning Rate: 2.34224e-05
	LOSS [training: 0.950160408043725 | validation: 2.577590395791544]
	TIME [epoch: 8.46 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9575309479738635		[learning rate: 2.3337e-05]
	Learning Rate: 2.33374e-05
	LOSS [training: 0.9575309479738635 | validation: 2.5774561623433367]
	TIME [epoch: 8.48 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9602135748599985		[learning rate: 2.3253e-05]
	Learning Rate: 2.32527e-05
	LOSS [training: 0.9602135748599985 | validation: 2.57519581545042]
	TIME [epoch: 8.47 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9535545356146672		[learning rate: 2.3168e-05]
	Learning Rate: 2.31683e-05
	LOSS [training: 0.9535545356146672 | validation: 2.575049338036611]
	TIME [epoch: 8.46 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9577888811744252		[learning rate: 2.3084e-05]
	Learning Rate: 2.30843e-05
	LOSS [training: 0.9577888811744252 | validation: 2.573814649755373]
	TIME [epoch: 8.46 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.956950956192343		[learning rate: 2.3e-05]
	Learning Rate: 2.30005e-05
	LOSS [training: 0.956950956192343 | validation: 2.5727024841885333]
	TIME [epoch: 8.47 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9595559802999745		[learning rate: 2.2917e-05]
	Learning Rate: 2.2917e-05
	LOSS [training: 0.9595559802999745 | validation: 2.5831127456356078]
	TIME [epoch: 8.47 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9538637656410438		[learning rate: 2.2834e-05]
	Learning Rate: 2.28338e-05
	LOSS [training: 0.9538637656410438 | validation: 2.5770546606284492]
	TIME [epoch: 8.46 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.955063424093322		[learning rate: 2.2751e-05]
	Learning Rate: 2.2751e-05
	LOSS [training: 0.955063424093322 | validation: 2.5816203251799]
	TIME [epoch: 8.46 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9523091177672495		[learning rate: 2.2668e-05]
	Learning Rate: 2.26684e-05
	LOSS [training: 0.9523091177672495 | validation: 2.562063136598633]
	TIME [epoch: 8.48 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.958007859186378		[learning rate: 2.2586e-05]
	Learning Rate: 2.25862e-05
	LOSS [training: 0.958007859186378 | validation: 2.574057346794771]
	TIME [epoch: 8.47 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9581507562072714		[learning rate: 2.2504e-05]
	Learning Rate: 2.25042e-05
	LOSS [training: 0.9581507562072714 | validation: 2.579608861130972]
	TIME [epoch: 8.47 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9588943558464464		[learning rate: 2.2423e-05]
	Learning Rate: 2.24225e-05
	LOSS [training: 0.9588943558464464 | validation: 2.5847953244479833]
	TIME [epoch: 8.47 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.954030390823522		[learning rate: 2.2341e-05]
	Learning Rate: 2.23411e-05
	LOSS [training: 0.954030390823522 | validation: 2.5686562539742033]
	TIME [epoch: 8.5 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9588289083250116		[learning rate: 2.226e-05]
	Learning Rate: 2.22601e-05
	LOSS [training: 0.9588289083250116 | validation: 2.581364123844403]
	TIME [epoch: 8.47 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9587031794986569		[learning rate: 2.2179e-05]
	Learning Rate: 2.21793e-05
	LOSS [training: 0.9587031794986569 | validation: 2.581542499242586]
	TIME [epoch: 8.46 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9560811038616354		[learning rate: 2.2099e-05]
	Learning Rate: 2.20988e-05
	LOSS [training: 0.9560811038616354 | validation: 2.5770964709656106]
	TIME [epoch: 8.46 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9582676735226461		[learning rate: 2.2019e-05]
	Learning Rate: 2.20186e-05
	LOSS [training: 0.9582676735226461 | validation: 2.577815723888047]
	TIME [epoch: 8.48 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9542412369764228		[learning rate: 2.1939e-05]
	Learning Rate: 2.19387e-05
	LOSS [training: 0.9542412369764228 | validation: 2.5747624822373782]
	TIME [epoch: 8.45 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9525063519186429		[learning rate: 2.1859e-05]
	Learning Rate: 2.18591e-05
	LOSS [training: 0.9525063519186429 | validation: 2.5811209810542817]
	TIME [epoch: 8.46 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9584322635860113		[learning rate: 2.178e-05]
	Learning Rate: 2.17797e-05
	LOSS [training: 0.9584322635860113 | validation: 2.575504285743402]
	TIME [epoch: 8.46 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9591890190661039		[learning rate: 2.1701e-05]
	Learning Rate: 2.17007e-05
	LOSS [training: 0.9591890190661039 | validation: 2.5800698779029534]
	TIME [epoch: 8.49 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9541111644061757		[learning rate: 2.1622e-05]
	Learning Rate: 2.16219e-05
	LOSS [training: 0.9541111644061757 | validation: 2.5845500226632847]
	TIME [epoch: 8.47 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9591355287723434		[learning rate: 2.1543e-05]
	Learning Rate: 2.15435e-05
	LOSS [training: 0.9591355287723434 | validation: 2.5900083687892983]
	TIME [epoch: 8.45 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9552440282994412		[learning rate: 2.1465e-05]
	Learning Rate: 2.14653e-05
	LOSS [training: 0.9552440282994412 | validation: 2.5804633688405745]
	TIME [epoch: 8.47 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9538222570938084		[learning rate: 2.1387e-05]
	Learning Rate: 2.13874e-05
	LOSS [training: 0.9538222570938084 | validation: 2.585564107759125]
	TIME [epoch: 8.49 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9563772097292882		[learning rate: 2.131e-05]
	Learning Rate: 2.13098e-05
	LOSS [training: 0.9563772097292882 | validation: 2.5803985694483997]
	TIME [epoch: 8.47 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9533404928702025		[learning rate: 2.1232e-05]
	Learning Rate: 2.12325e-05
	LOSS [training: 0.9533404928702025 | validation: 2.5685619013954835]
	TIME [epoch: 8.45 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9542564210289914		[learning rate: 2.1155e-05]
	Learning Rate: 2.11554e-05
	LOSS [training: 0.9542564210289914 | validation: 2.5700698300360156]
	TIME [epoch: 8.48 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9567825662253894		[learning rate: 2.1079e-05]
	Learning Rate: 2.10786e-05
	LOSS [training: 0.9567825662253894 | validation: 2.57567783353442]
	TIME [epoch: 8.47 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9560955529743019		[learning rate: 2.1002e-05]
	Learning Rate: 2.10021e-05
	LOSS [training: 0.9560955529743019 | validation: 2.572147946428407]
	TIME [epoch: 8.47 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9575927604452337		[learning rate: 2.0926e-05]
	Learning Rate: 2.09259e-05
	LOSS [training: 0.9575927604452337 | validation: 2.578436346906651]
	TIME [epoch: 8.46 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9559971304142		[learning rate: 2.085e-05]
	Learning Rate: 2.085e-05
	LOSS [training: 0.9559971304142 | validation: 2.5729956614631377]
	TIME [epoch: 8.48 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9583085204893166		[learning rate: 2.0774e-05]
	Learning Rate: 2.07743e-05
	LOSS [training: 0.9583085204893166 | validation: 2.5717484238656256]
	TIME [epoch: 8.48 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9551515073424334		[learning rate: 2.0699e-05]
	Learning Rate: 2.06989e-05
	LOSS [training: 0.9551515073424334 | validation: 2.5745067765579432]
	TIME [epoch: 8.46 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9547539269029472		[learning rate: 2.0624e-05]
	Learning Rate: 2.06238e-05
	LOSS [training: 0.9547539269029472 | validation: 2.5816133093015825]
	TIME [epoch: 8.47 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9565194679331273		[learning rate: 2.0549e-05]
	Learning Rate: 2.05489e-05
	LOSS [training: 0.9565194679331273 | validation: 2.567573455443317]
	TIME [epoch: 8.5 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9585938925922687		[learning rate: 2.0474e-05]
	Learning Rate: 2.04744e-05
	LOSS [training: 0.9585938925922687 | validation: 2.5786183973503047]
	TIME [epoch: 8.48 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9502780993476222		[learning rate: 2.04e-05]
	Learning Rate: 2.04001e-05
	LOSS [training: 0.9502780993476222 | validation: 2.5812916115511615]
	TIME [epoch: 8.48 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9539205233040342		[learning rate: 2.0326e-05]
	Learning Rate: 2.0326e-05
	LOSS [training: 0.9539205233040342 | validation: 2.5767193308891403]
	TIME [epoch: 8.47 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9584636777173958		[learning rate: 2.0252e-05]
	Learning Rate: 2.02523e-05
	LOSS [training: 0.9584636777173958 | validation: 2.5827705890782298]
	TIME [epoch: 8.48 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9554324738472003		[learning rate: 2.0179e-05]
	Learning Rate: 2.01788e-05
	LOSS [training: 0.9554324738472003 | validation: 2.5863456895113606]
	TIME [epoch: 8.46 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9557024507959515		[learning rate: 2.0106e-05]
	Learning Rate: 2.01055e-05
	LOSS [training: 0.9557024507959515 | validation: 2.564531574751761]
	TIME [epoch: 8.47 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.955867601857903		[learning rate: 2.0033e-05]
	Learning Rate: 2.00326e-05
	LOSS [training: 0.955867601857903 | validation: 2.5728447693019687]
	TIME [epoch: 8.47 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9554811531822217		[learning rate: 1.996e-05]
	Learning Rate: 1.99599e-05
	LOSS [training: 0.9554811531822217 | validation: 2.5674936929732475]
	TIME [epoch: 8.49 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9592160500620288		[learning rate: 1.9887e-05]
	Learning Rate: 1.98874e-05
	LOSS [training: 0.9592160500620288 | validation: 2.5693600975832673]
	TIME [epoch: 8.46 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9576121008407222		[learning rate: 1.9815e-05]
	Learning Rate: 1.98153e-05
	LOSS [training: 0.9576121008407222 | validation: 2.5707069323733633]
	TIME [epoch: 8.47 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9554358910658939		[learning rate: 1.9743e-05]
	Learning Rate: 1.97434e-05
	LOSS [training: 0.9554358910658939 | validation: 2.582561697651426]
	TIME [epoch: 8.46 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9562710681626753		[learning rate: 1.9672e-05]
	Learning Rate: 1.96717e-05
	LOSS [training: 0.9562710681626753 | validation: 2.5811632362371446]
	TIME [epoch: 8.49 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.957228917770953		[learning rate: 1.96e-05]
	Learning Rate: 1.96003e-05
	LOSS [training: 0.957228917770953 | validation: 2.57503434706064]
	TIME [epoch: 8.47 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9526428844551376		[learning rate: 1.9529e-05]
	Learning Rate: 1.95292e-05
	LOSS [training: 0.9526428844551376 | validation: 2.582454656532585]
	TIME [epoch: 8.47 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.952361103469592		[learning rate: 1.9458e-05]
	Learning Rate: 1.94583e-05
	LOSS [training: 0.952361103469592 | validation: 2.573502796698009]
	TIME [epoch: 8.47 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9573828936831571		[learning rate: 1.9388e-05]
	Learning Rate: 1.93877e-05
	LOSS [training: 0.9573828936831571 | validation: 2.5941501397532734]
	TIME [epoch: 8.49 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9560473387252454		[learning rate: 1.9317e-05]
	Learning Rate: 1.93173e-05
	LOSS [training: 0.9560473387252454 | validation: 2.5959686769959154]
	TIME [epoch: 8.47 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9560566881576567		[learning rate: 1.9247e-05]
	Learning Rate: 1.92472e-05
	LOSS [training: 0.9560566881576567 | validation: 2.580534708855524]
	TIME [epoch: 8.47 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9563866762046672		[learning rate: 1.9177e-05]
	Learning Rate: 1.91774e-05
	LOSS [training: 0.9563866762046672 | validation: 2.576730624014612]
	TIME [epoch: 8.48 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9552047263198663		[learning rate: 1.9108e-05]
	Learning Rate: 1.91078e-05
	LOSS [training: 0.9552047263198663 | validation: 2.577521529325736]
	TIME [epoch: 8.48 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9528220416228652		[learning rate: 1.9038e-05]
	Learning Rate: 1.90384e-05
	LOSS [training: 0.9528220416228652 | validation: 2.576601435858719]
	TIME [epoch: 8.48 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.957407317695768		[learning rate: 1.8969e-05]
	Learning Rate: 1.89694e-05
	LOSS [training: 0.957407317695768 | validation: 2.5831578556274897]
	TIME [epoch: 8.47 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9529912364397564		[learning rate: 1.8901e-05]
	Learning Rate: 1.89005e-05
	LOSS [training: 0.9529912364397564 | validation: 2.5793568798396413]
	TIME [epoch: 8.47 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9509375509748403		[learning rate: 1.8832e-05]
	Learning Rate: 1.88319e-05
	LOSS [training: 0.9509375509748403 | validation: 2.5737362930909944]
	TIME [epoch: 8.48 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9546328909453384		[learning rate: 1.8764e-05]
	Learning Rate: 1.87636e-05
	LOSS [training: 0.9546328909453384 | validation: 2.585625128733462]
	TIME [epoch: 8.47 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9572277614665801		[learning rate: 1.8695e-05]
	Learning Rate: 1.86955e-05
	LOSS [training: 0.9572277614665801 | validation: 2.581878972044603]
	TIME [epoch: 8.46 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.950570165963828		[learning rate: 1.8628e-05]
	Learning Rate: 1.86276e-05
	LOSS [training: 0.950570165963828 | validation: 2.5732967864310115]
	TIME [epoch: 8.46 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9522105009753373		[learning rate: 1.856e-05]
	Learning Rate: 1.856e-05
	LOSS [training: 0.9522105009753373 | validation: 2.5796679676317433]
	TIME [epoch: 8.48 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9552918216529122		[learning rate: 1.8493e-05]
	Learning Rate: 1.84927e-05
	LOSS [training: 0.9552918216529122 | validation: 2.578660624939732]
	TIME [epoch: 8.47 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9593648354678633		[learning rate: 1.8426e-05]
	Learning Rate: 1.84256e-05
	LOSS [training: 0.9593648354678633 | validation: 2.5792001670155877]
	TIME [epoch: 8.46 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.952670903997413		[learning rate: 1.8359e-05]
	Learning Rate: 1.83587e-05
	LOSS [training: 0.952670903997413 | validation: 2.580582620111377]
	TIME [epoch: 8.48 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9538602943494601		[learning rate: 1.8292e-05]
	Learning Rate: 1.82921e-05
	LOSS [training: 0.9538602943494601 | validation: 2.5864661258237946]
	TIME [epoch: 8.47 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9575704794848239		[learning rate: 1.8226e-05]
	Learning Rate: 1.82257e-05
	LOSS [training: 0.9575704794848239 | validation: 2.5841868321441437]
	TIME [epoch: 8.48 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9552672074670658		[learning rate: 1.816e-05]
	Learning Rate: 1.81596e-05
	LOSS [training: 0.9552672074670658 | validation: 2.588896947973519]
	TIME [epoch: 8.46 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9581446431436345		[learning rate: 1.8094e-05]
	Learning Rate: 1.80937e-05
	LOSS [training: 0.9581446431436345 | validation: 2.5812770046849645]
	TIME [epoch: 8.48 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9553118172121449		[learning rate: 1.8028e-05]
	Learning Rate: 1.8028e-05
	LOSS [training: 0.9553118172121449 | validation: 2.5702791305347388]
	TIME [epoch: 8.47 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9575206989878584		[learning rate: 1.7963e-05]
	Learning Rate: 1.79626e-05
	LOSS [training: 0.9575206989878584 | validation: 2.574477245931171]
	TIME [epoch: 8.46 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9549773507375736		[learning rate: 1.7897e-05]
	Learning Rate: 1.78974e-05
	LOSS [training: 0.9549773507375736 | validation: 2.574447082222989]
	TIME [epoch: 8.46 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9520734691762737		[learning rate: 1.7832e-05]
	Learning Rate: 1.78324e-05
	LOSS [training: 0.9520734691762737 | validation: 2.578462674886464]
	TIME [epoch: 8.47 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9566886638936035		[learning rate: 1.7768e-05]
	Learning Rate: 1.77677e-05
	LOSS [training: 0.9566886638936035 | validation: 2.580244949593057]
	TIME [epoch: 8.47 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9563833509867706		[learning rate: 1.7703e-05]
	Learning Rate: 1.77032e-05
	LOSS [training: 0.9563833509867706 | validation: 2.5797563947443294]
	TIME [epoch: 8.46 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9510071853099917		[learning rate: 1.7639e-05]
	Learning Rate: 1.7639e-05
	LOSS [training: 0.9510071853099917 | validation: 2.5749873701338597]
	TIME [epoch: 8.46 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9564246132348048		[learning rate: 1.7575e-05]
	Learning Rate: 1.7575e-05
	LOSS [training: 0.9564246132348048 | validation: 2.578812576865332]
	TIME [epoch: 8.49 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9558725234686405		[learning rate: 1.7511e-05]
	Learning Rate: 1.75112e-05
	LOSS [training: 0.9558725234686405 | validation: 2.5672971092705112]
	TIME [epoch: 8.47 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.952469195311369		[learning rate: 1.7448e-05]
	Learning Rate: 1.74476e-05
	LOSS [training: 0.952469195311369 | validation: 2.576853471772848]
	TIME [epoch: 8.47 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9592220352874689		[learning rate: 1.7384e-05]
	Learning Rate: 1.73843e-05
	LOSS [training: 0.9592220352874689 | validation: 2.5697402561918583]
	TIME [epoch: 8.47 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9599057301181639		[learning rate: 1.7321e-05]
	Learning Rate: 1.73212e-05
	LOSS [training: 0.9599057301181639 | validation: 2.579314721103262]
	TIME [epoch: 8.49 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9584562772668074		[learning rate: 1.7258e-05]
	Learning Rate: 1.72584e-05
	LOSS [training: 0.9584562772668074 | validation: 2.5708259079839557]
	TIME [epoch: 8.47 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9574874979708449		[learning rate: 1.7196e-05]
	Learning Rate: 1.71957e-05
	LOSS [training: 0.9574874979708449 | validation: 2.5777097423487616]
	TIME [epoch: 8.46 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9552544648952702		[learning rate: 1.7133e-05]
	Learning Rate: 1.71333e-05
	LOSS [training: 0.9552544648952702 | validation: 2.5779880345045845]
	TIME [epoch: 8.47 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9569639544338834		[learning rate: 1.7071e-05]
	Learning Rate: 1.70712e-05
	LOSS [training: 0.9569639544338834 | validation: 2.586535194734505]
	TIME [epoch: 8.48 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9562493574977516		[learning rate: 1.7009e-05]
	Learning Rate: 1.70092e-05
	LOSS [training: 0.9562493574977516 | validation: 2.5763406962484403]
	TIME [epoch: 8.47 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9544826720795685		[learning rate: 1.6947e-05]
	Learning Rate: 1.69475e-05
	LOSS [training: 0.9544826720795685 | validation: 2.5766833088589984]
	TIME [epoch: 8.47 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9538439568668249		[learning rate: 1.6886e-05]
	Learning Rate: 1.6886e-05
	LOSS [training: 0.9538439568668249 | validation: 2.5842373764144635]
	TIME [epoch: 8.46 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9529960505745823		[learning rate: 1.6825e-05]
	Learning Rate: 1.68247e-05
	LOSS [training: 0.9529960505745823 | validation: 2.568196275252684]
	TIME [epoch: 8.48 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.959937549416734		[learning rate: 1.6764e-05]
	Learning Rate: 1.67636e-05
	LOSS [training: 0.959937549416734 | validation: 2.574900675973076]
	TIME [epoch: 8.47 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9559893177278571		[learning rate: 1.6703e-05]
	Learning Rate: 1.67028e-05
	LOSS [training: 0.9559893177278571 | validation: 2.5634030928125116]
	TIME [epoch: 8.47 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9556621026358348		[learning rate: 1.6642e-05]
	Learning Rate: 1.66422e-05
	LOSS [training: 0.9556621026358348 | validation: 2.575809059812767]
	TIME [epoch: 8.46 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9532077222938513		[learning rate: 1.6582e-05]
	Learning Rate: 1.65818e-05
	LOSS [training: 0.9532077222938513 | validation: 2.5679813285782735]
	TIME [epoch: 8.49 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9559893318212209		[learning rate: 1.6522e-05]
	Learning Rate: 1.65216e-05
	LOSS [training: 0.9559893318212209 | validation: 2.583378377426993]
	TIME [epoch: 8.47 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9525685736606064		[learning rate: 1.6462e-05]
	Learning Rate: 1.64617e-05
	LOSS [training: 0.9525685736606064 | validation: 2.581378507274687]
	TIME [epoch: 8.46 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9548911694527111		[learning rate: 1.6402e-05]
	Learning Rate: 1.64019e-05
	LOSS [training: 0.9548911694527111 | validation: 2.5801124028021665]
	TIME [epoch: 8.47 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9581350378960138		[learning rate: 1.6342e-05]
	Learning Rate: 1.63424e-05
	LOSS [training: 0.9581350378960138 | validation: 2.584014761727205]
	TIME [epoch: 8.48 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9569846739723893		[learning rate: 1.6283e-05]
	Learning Rate: 1.62831e-05
	LOSS [training: 0.9569846739723893 | validation: 2.5786008333618406]
	TIME [epoch: 8.46 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9571599308162261		[learning rate: 1.6224e-05]
	Learning Rate: 1.6224e-05
	LOSS [training: 0.9571599308162261 | validation: 2.5727100693684433]
	TIME [epoch: 8.46 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9550191550447318		[learning rate: 1.6165e-05]
	Learning Rate: 1.61651e-05
	LOSS [training: 0.9550191550447318 | validation: 2.573133509582655]
	TIME [epoch: 8.47 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9526687124891131		[learning rate: 1.6106e-05]
	Learning Rate: 1.61065e-05
	LOSS [training: 0.9526687124891131 | validation: 2.5791364038045455]
	TIME [epoch: 8.48 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.954989427914194		[learning rate: 1.6048e-05]
	Learning Rate: 1.6048e-05
	LOSS [training: 0.954989427914194 | validation: 2.579538492137665]
	TIME [epoch: 8.47 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9536902346384138		[learning rate: 1.599e-05]
	Learning Rate: 1.59898e-05
	LOSS [training: 0.9536902346384138 | validation: 2.580848903759147]
	TIME [epoch: 8.47 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9513244101508288		[learning rate: 1.5932e-05]
	Learning Rate: 1.59317e-05
	LOSS [training: 0.9513244101508288 | validation: 2.5795907854939113]
	TIME [epoch: 8.47 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9542899835629669		[learning rate: 1.5874e-05]
	Learning Rate: 1.58739e-05
	LOSS [training: 0.9542899835629669 | validation: 2.568313635367016]
	TIME [epoch: 8.49 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9576067822988641		[learning rate: 1.5816e-05]
	Learning Rate: 1.58163e-05
	LOSS [training: 0.9576067822988641 | validation: 2.56925168041705]
	TIME [epoch: 8.46 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9584240453622133		[learning rate: 1.5759e-05]
	Learning Rate: 1.57589e-05
	LOSS [training: 0.9584240453622133 | validation: 2.5867067865666655]
	TIME [epoch: 8.47 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9523908627472231		[learning rate: 1.5702e-05]
	Learning Rate: 1.57017e-05
	LOSS [training: 0.9523908627472231 | validation: 2.5798017891932785]
	TIME [epoch: 8.48 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9564334415734832		[learning rate: 1.5645e-05]
	Learning Rate: 1.56447e-05
	LOSS [training: 0.9564334415734832 | validation: 2.583447648410694]
	TIME [epoch: 8.47 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9528692710882668		[learning rate: 1.5588e-05]
	Learning Rate: 1.5588e-05
	LOSS [training: 0.9528692710882668 | validation: 2.5918961590432796]
	TIME [epoch: 8.47 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9540496417898294		[learning rate: 1.5531e-05]
	Learning Rate: 1.55314e-05
	LOSS [training: 0.9540496417898294 | validation: 2.577076591973036]
	TIME [epoch: 8.46 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9565906708615743		[learning rate: 1.5475e-05]
	Learning Rate: 1.5475e-05
	LOSS [training: 0.9565906708615743 | validation: 2.573952773080088]
	TIME [epoch: 8.48 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9558220860237929		[learning rate: 1.5419e-05]
	Learning Rate: 1.54189e-05
	LOSS [training: 0.9558220860237929 | validation: 2.5808562829688593]
	TIME [epoch: 8.47 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9560808630250766		[learning rate: 1.5363e-05]
	Learning Rate: 1.53629e-05
	LOSS [training: 0.9560808630250766 | validation: 2.5826001945953188]
	TIME [epoch: 8.46 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9549184203802332		[learning rate: 1.5307e-05]
	Learning Rate: 1.53072e-05
	LOSS [training: 0.9549184203802332 | validation: 2.5869692839871212]
	TIME [epoch: 8.46 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9561419917971661		[learning rate: 1.5252e-05]
	Learning Rate: 1.52516e-05
	LOSS [training: 0.9561419917971661 | validation: 2.580417169945527]
	TIME [epoch: 8.48 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.95284637009768		[learning rate: 1.5196e-05]
	Learning Rate: 1.51963e-05
	LOSS [training: 0.95284637009768 | validation: 2.5626086376594386]
	TIME [epoch: 8.47 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.958523821317139		[learning rate: 1.5141e-05]
	Learning Rate: 1.51411e-05
	LOSS [training: 0.958523821317139 | validation: 2.5707471387460554]
	TIME [epoch: 8.47 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9571500041414132		[learning rate: 1.5086e-05]
	Learning Rate: 1.50862e-05
	LOSS [training: 0.9571500041414132 | validation: 2.584627867403151]
	TIME [epoch: 8.46 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9591744821037294		[learning rate: 1.5031e-05]
	Learning Rate: 1.50314e-05
	LOSS [training: 0.9591744821037294 | validation: 2.570282913711646]
	TIME [epoch: 8.49 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9522181306556909		[learning rate: 1.4977e-05]
	Learning Rate: 1.49769e-05
	LOSS [training: 0.9522181306556909 | validation: 2.5677179109985078]
	TIME [epoch: 8.47 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9552563144867179		[learning rate: 1.4923e-05]
	Learning Rate: 1.49225e-05
	LOSS [training: 0.9552563144867179 | validation: 2.57748224198069]
	TIME [epoch: 8.47 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9547214527752927		[learning rate: 1.4868e-05]
	Learning Rate: 1.48684e-05
	LOSS [training: 0.9547214527752927 | validation: 2.5633458666313227]
	TIME [epoch: 8.47 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9575767958926891		[learning rate: 1.4814e-05]
	Learning Rate: 1.48144e-05
	LOSS [training: 0.9575767958926891 | validation: 2.580481111441555]
	TIME [epoch: 8.48 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9535242655928513		[learning rate: 1.4761e-05]
	Learning Rate: 1.47606e-05
	LOSS [training: 0.9535242655928513 | validation: 2.580538112272464]
	TIME [epoch: 8.47 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9552808797659973		[learning rate: 1.4707e-05]
	Learning Rate: 1.47071e-05
	LOSS [training: 0.9552808797659973 | validation: 2.568300750001303]
	TIME [epoch: 8.47 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9574459929830196		[learning rate: 1.4654e-05]
	Learning Rate: 1.46537e-05
	LOSS [training: 0.9574459929830196 | validation: 2.5823437145582364]
	TIME [epoch: 8.47 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9555159682880641		[learning rate: 1.4601e-05]
	Learning Rate: 1.46005e-05
	LOSS [training: 0.9555159682880641 | validation: 2.574206003406972]
	TIME [epoch: 8.48 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9577863651026947		[learning rate: 1.4548e-05]
	Learning Rate: 1.45475e-05
	LOSS [training: 0.9577863651026947 | validation: 2.5719823840397957]
	TIME [epoch: 8.46 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9554977820943021		[learning rate: 1.4495e-05]
	Learning Rate: 1.44947e-05
	LOSS [training: 0.9554977820943021 | validation: 2.5850474400607935]
	TIME [epoch: 8.47 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9561098808597549		[learning rate: 1.4442e-05]
	Learning Rate: 1.44421e-05
	LOSS [training: 0.9561098808597549 | validation: 2.5819558757023158]
	TIME [epoch: 8.46 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9560735389648956		[learning rate: 1.439e-05]
	Learning Rate: 1.43897e-05
	LOSS [training: 0.9560735389648956 | validation: 2.5808509090725917]
	TIME [epoch: 8.49 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9553068098097348		[learning rate: 1.4338e-05]
	Learning Rate: 1.43375e-05
	LOSS [training: 0.9553068098097348 | validation: 2.579371797773918]
	TIME [epoch: 8.47 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9537843195977288		[learning rate: 1.4285e-05]
	Learning Rate: 1.42855e-05
	LOSS [training: 0.9537843195977288 | validation: 2.584535597275331]
	TIME [epoch: 8.46 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9580552460952332		[learning rate: 1.4234e-05]
	Learning Rate: 1.42336e-05
	LOSS [training: 0.9580552460952332 | validation: 2.5793900086351123]
	TIME [epoch: 8.47 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9560766380629119		[learning rate: 1.4182e-05]
	Learning Rate: 1.4182e-05
	LOSS [training: 0.9560766380629119 | validation: 2.5800327704402446]
	TIME [epoch: 8.49 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9578561506093795		[learning rate: 1.4131e-05]
	Learning Rate: 1.41305e-05
	LOSS [training: 0.9578561506093795 | validation: 2.584936044233368]
	TIME [epoch: 8.47 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9538523113899078		[learning rate: 1.4079e-05]
	Learning Rate: 1.40792e-05
	LOSS [training: 0.9538523113899078 | validation: 2.5757969903730156]
	TIME [epoch: 8.47 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9539357811747486		[learning rate: 1.4028e-05]
	Learning Rate: 1.40281e-05
	LOSS [training: 0.9539357811747486 | validation: 2.570469878243575]
	TIME [epoch: 8.47 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9571706255714225		[learning rate: 1.3977e-05]
	Learning Rate: 1.39772e-05
	LOSS [training: 0.9571706255714225 | validation: 2.5765523026808346]
	TIME [epoch: 8.49 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9561364102419139		[learning rate: 1.3927e-05]
	Learning Rate: 1.39265e-05
	LOSS [training: 0.9561364102419139 | validation: 2.571764242283563]
	TIME [epoch: 8.47 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9580307525267735		[learning rate: 1.3876e-05]
	Learning Rate: 1.3876e-05
	LOSS [training: 0.9580307525267735 | validation: 2.56658793539322]
	TIME [epoch: 8.45 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9535958325716194		[learning rate: 1.3826e-05]
	Learning Rate: 1.38256e-05
	LOSS [training: 0.9535958325716194 | validation: 2.579465483614631]
	TIME [epoch: 8.47 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9555760674676345		[learning rate: 1.3775e-05]
	Learning Rate: 1.37754e-05
	LOSS [training: 0.9555760674676345 | validation: 2.577514330352254]
	TIME [epoch: 8.48 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9558495951090368		[learning rate: 1.3725e-05]
	Learning Rate: 1.37254e-05
	LOSS [training: 0.9558495951090368 | validation: 2.5826810256771218]
	TIME [epoch: 8.47 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9546235876938889		[learning rate: 1.3676e-05]
	Learning Rate: 1.36756e-05
	LOSS [training: 0.9546235876938889 | validation: 2.580002319769699]
	TIME [epoch: 8.47 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9537707245903688		[learning rate: 1.3626e-05]
	Learning Rate: 1.3626e-05
	LOSS [training: 0.9537707245903688 | validation: 2.5742935856322577]
	TIME [epoch: 8.46 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9579505637626637		[learning rate: 1.3577e-05]
	Learning Rate: 1.35766e-05
	LOSS [training: 0.9579505637626637 | validation: 2.5778575670562898]
	TIME [epoch: 8.47 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9552015609443985		[learning rate: 1.3527e-05]
	Learning Rate: 1.35273e-05
	LOSS [training: 0.9552015609443985 | validation: 2.584710756420893]
	TIME [epoch: 8.45 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9529893424007863		[learning rate: 1.3478e-05]
	Learning Rate: 1.34782e-05
	LOSS [training: 0.9529893424007863 | validation: 2.578858607388784]
	TIME [epoch: 8.45 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9556554046063217		[learning rate: 1.3429e-05]
	Learning Rate: 1.34293e-05
	LOSS [training: 0.9556554046063217 | validation: 2.5750780203080565]
	TIME [epoch: 8.47 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9538993794420593		[learning rate: 1.3381e-05]
	Learning Rate: 1.33805e-05
	LOSS [training: 0.9538993794420593 | validation: 2.5816837414043916]
	TIME [epoch: 8.47 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9521845929549381		[learning rate: 1.3332e-05]
	Learning Rate: 1.3332e-05
	LOSS [training: 0.9521845929549381 | validation: 2.5754493458696412]
	TIME [epoch: 8.46 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9529066592858634		[learning rate: 1.3284e-05]
	Learning Rate: 1.32836e-05
	LOSS [training: 0.9529066592858634 | validation: 2.569600215009898]
	TIME [epoch: 8.46 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9572297480567507		[learning rate: 1.3235e-05]
	Learning Rate: 1.32354e-05
	LOSS [training: 0.9572297480567507 | validation: 2.585455683551414]
	TIME [epoch: 8.47 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9559417369819465		[learning rate: 1.3187e-05]
	Learning Rate: 1.31874e-05
	LOSS [training: 0.9559417369819465 | validation: 2.5837623551662388]
	TIME [epoch: 8.47 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9530614093551601		[learning rate: 1.314e-05]
	Learning Rate: 1.31395e-05
	LOSS [training: 0.9530614093551601 | validation: 2.5889371879957435]
	TIME [epoch: 8.46 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9570013324803173		[learning rate: 1.3092e-05]
	Learning Rate: 1.30918e-05
	LOSS [training: 0.9570013324803173 | validation: 2.596017438989137]
	TIME [epoch: 8.46 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9530496161428677		[learning rate: 1.3044e-05]
	Learning Rate: 1.30443e-05
	LOSS [training: 0.9530496161428677 | validation: 2.5788944220425503]
	TIME [epoch: 8.48 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9552881064677907		[learning rate: 1.2997e-05]
	Learning Rate: 1.2997e-05
	LOSS [training: 0.9552881064677907 | validation: 2.5761781961414627]
	TIME [epoch: 8.47 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9564988546425861		[learning rate: 1.295e-05]
	Learning Rate: 1.29498e-05
	LOSS [training: 0.9564988546425861 | validation: 2.584750990254636]
	TIME [epoch: 8.46 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9535743661075771		[learning rate: 1.2903e-05]
	Learning Rate: 1.29028e-05
	LOSS [training: 0.9535743661075771 | validation: 2.572564161376563]
	TIME [epoch: 8.46 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9611052494376514		[learning rate: 1.2856e-05]
	Learning Rate: 1.2856e-05
	LOSS [training: 0.9611052494376514 | validation: 2.5858389805142643]
	TIME [epoch: 8.48 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9540468963358121		[learning rate: 1.2809e-05]
	Learning Rate: 1.28093e-05
	LOSS [training: 0.9540468963358121 | validation: 2.582980084040497]
	TIME [epoch: 8.46 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9577239000269998		[learning rate: 1.2763e-05]
	Learning Rate: 1.27628e-05
	LOSS [training: 0.9577239000269998 | validation: 2.5776322398423694]
	TIME [epoch: 8.46 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9527722900756546		[learning rate: 1.2717e-05]
	Learning Rate: 1.27165e-05
	LOSS [training: 0.9527722900756546 | validation: 2.5821101388501533]
	TIME [epoch: 8.45 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9577076444114775		[learning rate: 1.267e-05]
	Learning Rate: 1.26704e-05
	LOSS [training: 0.9577076444114775 | validation: 2.5712993651934535]
	TIME [epoch: 8.47 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9511234401757764		[learning rate: 1.2624e-05]
	Learning Rate: 1.26244e-05
	LOSS [training: 0.9511234401757764 | validation: 2.5833683792277697]
	TIME [epoch: 8.46 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9556370084668636		[learning rate: 1.2579e-05]
	Learning Rate: 1.25786e-05
	LOSS [training: 0.9556370084668636 | validation: 2.564665000898951]
	TIME [epoch: 8.46 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.95233783223914		[learning rate: 1.2533e-05]
	Learning Rate: 1.25329e-05
	LOSS [training: 0.95233783223914 | validation: 2.567786135831269]
	TIME [epoch: 8.45 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9567076555378623		[learning rate: 1.2487e-05]
	Learning Rate: 1.24874e-05
	LOSS [training: 0.9567076555378623 | validation: 2.5725688319313087]
	TIME [epoch: 8.48 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9561949468052943		[learning rate: 1.2442e-05]
	Learning Rate: 1.24421e-05
	LOSS [training: 0.9561949468052943 | validation: 2.5714412491118273]
	TIME [epoch: 8.47 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9575092468851171		[learning rate: 1.2397e-05]
	Learning Rate: 1.2397e-05
	LOSS [training: 0.9575092468851171 | validation: 2.5796471582290317]
	TIME [epoch: 8.45 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9562716771203414		[learning rate: 1.2352e-05]
	Learning Rate: 1.2352e-05
	LOSS [training: 0.9562716771203414 | validation: 2.573958028445165]
	TIME [epoch: 8.45 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9498518776093663		[learning rate: 1.2307e-05]
	Learning Rate: 1.23072e-05
	LOSS [training: 0.9498518776093663 | validation: 2.5792449977700347]
	TIME [epoch: 8.47 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9576455966190259		[learning rate: 1.2263e-05]
	Learning Rate: 1.22625e-05
	LOSS [training: 0.9576455966190259 | validation: 2.574884217056539]
	TIME [epoch: 8.46 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9535986993269064		[learning rate: 1.2218e-05]
	Learning Rate: 1.2218e-05
	LOSS [training: 0.9535986993269064 | validation: 2.5770487792126247]
	TIME [epoch: 8.45 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9568458390425405		[learning rate: 1.2174e-05]
	Learning Rate: 1.21737e-05
	LOSS [training: 0.9568458390425405 | validation: 2.5721027056100505]
	TIME [epoch: 8.46 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9539460899485535		[learning rate: 1.2129e-05]
	Learning Rate: 1.21295e-05
	LOSS [training: 0.9539460899485535 | validation: 2.575163622790063]
	TIME [epoch: 8.48 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9547483249369355		[learning rate: 1.2085e-05]
	Learning Rate: 1.20855e-05
	LOSS [training: 0.9547483249369355 | validation: 2.574683275446769]
	TIME [epoch: 8.46 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.954364472672961		[learning rate: 1.2042e-05]
	Learning Rate: 1.20416e-05
	LOSS [training: 0.954364472672961 | validation: 2.5807343927935205]
	TIME [epoch: 8.46 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9525157563518876		[learning rate: 1.1998e-05]
	Learning Rate: 1.19979e-05
	LOSS [training: 0.9525157563518876 | validation: 2.5849480051825164]
	TIME [epoch: 8.45 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9545547372628477		[learning rate: 1.1954e-05]
	Learning Rate: 1.19544e-05
	LOSS [training: 0.9545547372628477 | validation: 2.5833597969585784]
	TIME [epoch: 8.47 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9556969034700963		[learning rate: 1.1911e-05]
	Learning Rate: 1.1911e-05
	LOSS [training: 0.9556969034700963 | validation: 2.5849772441171046]
	TIME [epoch: 8.46 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9566173513058012		[learning rate: 1.1868e-05]
	Learning Rate: 1.18678e-05
	LOSS [training: 0.9566173513058012 | validation: 2.5699342436013692]
	TIME [epoch: 8.45 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9574348802504884		[learning rate: 1.1825e-05]
	Learning Rate: 1.18247e-05
	LOSS [training: 0.9574348802504884 | validation: 2.5848782440786486]
	TIME [epoch: 8.46 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.957138953971806		[learning rate: 1.1782e-05]
	Learning Rate: 1.17818e-05
	LOSS [training: 0.957138953971806 | validation: 2.574531991247167]
	TIME [epoch: 8.47 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9537603448755482		[learning rate: 1.1739e-05]
	Learning Rate: 1.1739e-05
	LOSS [training: 0.9537603448755482 | validation: 2.57615178962512]
	TIME [epoch: 8.46 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9561885362854156		[learning rate: 1.1696e-05]
	Learning Rate: 1.16964e-05
	LOSS [training: 0.9561885362854156 | validation: 2.563268563228914]
	TIME [epoch: 8.46 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9548688697381198		[learning rate: 1.1654e-05]
	Learning Rate: 1.1654e-05
	LOSS [training: 0.9548688697381198 | validation: 2.588354095572983]
	TIME [epoch: 8.47 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9564600046126293		[learning rate: 1.1612e-05]
	Learning Rate: 1.16117e-05
	LOSS [training: 0.9564600046126293 | validation: 2.5844626084424993]
	TIME [epoch: 8.47 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9560728914556957		[learning rate: 1.157e-05]
	Learning Rate: 1.15695e-05
	LOSS [training: 0.9560728914556957 | validation: 2.576405077441228]
	TIME [epoch: 8.46 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9565227572340532		[learning rate: 1.1528e-05]
	Learning Rate: 1.15275e-05
	LOSS [training: 0.9565227572340532 | validation: 2.579239400535118]
	TIME [epoch: 8.45 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9586632209516559		[learning rate: 1.1486e-05]
	Learning Rate: 1.14857e-05
	LOSS [training: 0.9586632209516559 | validation: 2.5771996548086427]
	TIME [epoch: 8.48 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9544869215627466		[learning rate: 1.1444e-05]
	Learning Rate: 1.1444e-05
	LOSS [training: 0.9544869215627466 | validation: 2.5806628685403443]
	TIME [epoch: 8.47 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9571313981863806		[learning rate: 1.1402e-05]
	Learning Rate: 1.14025e-05
	LOSS [training: 0.9571313981863806 | validation: 2.580266096837968]
	TIME [epoch: 8.46 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9547429267538428		[learning rate: 1.1361e-05]
	Learning Rate: 1.13611e-05
	LOSS [training: 0.9547429267538428 | validation: 2.576215649256172]
	TIME [epoch: 8.46 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9518829072800449		[learning rate: 1.132e-05]
	Learning Rate: 1.13199e-05
	LOSS [training: 0.9518829072800449 | validation: 2.580456673285942]
	TIME [epoch: 8.47 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9549649774724175		[learning rate: 1.1279e-05]
	Learning Rate: 1.12788e-05
	LOSS [training: 0.9549649774724175 | validation: 2.5729614067408773]
	TIME [epoch: 8.47 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9579841749371351		[learning rate: 1.1238e-05]
	Learning Rate: 1.12379e-05
	LOSS [training: 0.9579841749371351 | validation: 2.5769431428540823]
	TIME [epoch: 8.45 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9528904620317993		[learning rate: 1.1197e-05]
	Learning Rate: 1.11971e-05
	LOSS [training: 0.9528904620317993 | validation: 2.5762101159667106]
	TIME [epoch: 8.45 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9597919842750468		[learning rate: 1.1156e-05]
	Learning Rate: 1.11565e-05
	LOSS [training: 0.9597919842750468 | validation: 2.575406710365223]
	TIME [epoch: 8.47 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9522207420804945		[learning rate: 1.1116e-05]
	Learning Rate: 1.1116e-05
	LOSS [training: 0.9522207420804945 | validation: 2.576033446687601]
	TIME [epoch: 8.46 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9528370573099518		[learning rate: 1.1076e-05]
	Learning Rate: 1.10756e-05
	LOSS [training: 0.9528370573099518 | validation: 2.5806330865147844]
	TIME [epoch: 8.45 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9582508601420893		[learning rate: 1.1035e-05]
	Learning Rate: 1.10354e-05
	LOSS [training: 0.9582508601420893 | validation: 2.5759215243538476]
	TIME [epoch: 8.45 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.950778068736077		[learning rate: 1.0995e-05]
	Learning Rate: 1.09954e-05
	LOSS [training: 0.950778068736077 | validation: 2.568853405305375]
	TIME [epoch: 8.47 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9553301361093481		[learning rate: 1.0955e-05]
	Learning Rate: 1.09555e-05
	LOSS [training: 0.9553301361093481 | validation: 2.5787868231627304]
	TIME [epoch: 8.45 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9540805013694327		[learning rate: 1.0916e-05]
	Learning Rate: 1.09157e-05
	LOSS [training: 0.9540805013694327 | validation: 2.5739859609227946]
	TIME [epoch: 8.45 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9569660066217182		[learning rate: 1.0876e-05]
	Learning Rate: 1.08761e-05
	LOSS [training: 0.9569660066217182 | validation: 2.583607594687953]
	TIME [epoch: 8.45 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9519610779227508		[learning rate: 1.0837e-05]
	Learning Rate: 1.08366e-05
	LOSS [training: 0.9519610779227508 | validation: 2.5760749988982736]
	TIME [epoch: 8.48 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9533968613605432		[learning rate: 1.0797e-05]
	Learning Rate: 1.07973e-05
	LOSS [training: 0.9533968613605432 | validation: 2.57184007438693]
	TIME [epoch: 8.45 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9536048437678175		[learning rate: 1.0758e-05]
	Learning Rate: 1.07581e-05
	LOSS [training: 0.9536048437678175 | validation: 2.5706414218413123]
	TIME [epoch: 8.45 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9540926422136152		[learning rate: 1.0719e-05]
	Learning Rate: 1.07191e-05
	LOSS [training: 0.9540926422136152 | validation: 2.582284187862696]
	TIME [epoch: 8.45 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9541246803362394		[learning rate: 1.068e-05]
	Learning Rate: 1.06802e-05
	LOSS [training: 0.9541246803362394 | validation: 2.5782930103920387]
	TIME [epoch: 8.48 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9539928902552525		[learning rate: 1.0641e-05]
	Learning Rate: 1.06414e-05
	LOSS [training: 0.9539928902552525 | validation: 2.563371968160003]
	TIME [epoch: 8.46 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9551016914653969		[learning rate: 1.0603e-05]
	Learning Rate: 1.06028e-05
	LOSS [training: 0.9551016914653969 | validation: 2.5715291226487436]
	TIME [epoch: 8.45 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.95489915090517		[learning rate: 1.0564e-05]
	Learning Rate: 1.05643e-05
	LOSS [training: 0.95489915090517 | validation: 2.5729011189540065]
	TIME [epoch: 8.45 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9523967061428298		[learning rate: 1.0526e-05]
	Learning Rate: 1.0526e-05
	LOSS [training: 0.9523967061428298 | validation: 2.5814803482920214]
	TIME [epoch: 8.48 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9511172607179127		[learning rate: 1.0488e-05]
	Learning Rate: 1.04878e-05
	LOSS [training: 0.9511172607179127 | validation: 2.575606325887793]
	TIME [epoch: 8.45 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9551548043368939		[learning rate: 1.045e-05]
	Learning Rate: 1.04497e-05
	LOSS [training: 0.9551548043368939 | validation: 2.5751899504765685]
	TIME [epoch: 8.45 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9515462005738066		[learning rate: 1.0412e-05]
	Learning Rate: 1.04118e-05
	LOSS [training: 0.9515462005738066 | validation: 2.578032298240773]
	TIME [epoch: 8.45 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9552772326562595		[learning rate: 1.0374e-05]
	Learning Rate: 1.0374e-05
	LOSS [training: 0.9552772326562595 | validation: 2.5798068457666434]
	TIME [epoch: 8.48 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9545936405248913		[learning rate: 1.0336e-05]
	Learning Rate: 1.03364e-05
	LOSS [training: 0.9545936405248913 | validation: 2.5792370118485364]
	TIME [epoch: 8.44 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9534927657334167		[learning rate: 1.0299e-05]
	Learning Rate: 1.02989e-05
	LOSS [training: 0.9534927657334167 | validation: 2.5710479947790685]
	TIME [epoch: 8.46 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9562827152148652		[learning rate: 1.0261e-05]
	Learning Rate: 1.02615e-05
	LOSS [training: 0.9562827152148652 | validation: 2.5649266614340007]
	TIME [epoch: 8.44 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9515397063262079		[learning rate: 1.0224e-05]
	Learning Rate: 1.02243e-05
	LOSS [training: 0.9515397063262079 | validation: 2.5751401745023426]
	TIME [epoch: 8.48 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.953713920001861		[learning rate: 1.0187e-05]
	Learning Rate: 1.01872e-05
	LOSS [training: 0.953713920001861 | validation: 2.5731644099521107]
	TIME [epoch: 8.45 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9538687829600736		[learning rate: 1.015e-05]
	Learning Rate: 1.01502e-05
	LOSS [training: 0.9538687829600736 | validation: 2.5694451729705854]
	TIME [epoch: 8.46 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9519735030669837		[learning rate: 1.0113e-05]
	Learning Rate: 1.01133e-05
	LOSS [training: 0.9519735030669837 | validation: 2.5788608627343015]
	TIME [epoch: 8.44 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9566047604295521		[learning rate: 1.0077e-05]
	Learning Rate: 1.00766e-05
	LOSS [training: 0.9566047604295521 | validation: 2.5695338842352924]
	TIME [epoch: 8.46 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9556749077324038		[learning rate: 1.004e-05]
	Learning Rate: 1.00401e-05
	LOSS [training: 0.9556749077324038 | validation: 2.5701340396012164]
	TIME [epoch: 8.44 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9507575701148312		[learning rate: 1.0004e-05]
	Learning Rate: 1.00036e-05
	LOSS [training: 0.9507575701148312 | validation: 2.580797026249814]
	TIME [epoch: 8.45 sec]
Finished training in 17117.012 seconds.
