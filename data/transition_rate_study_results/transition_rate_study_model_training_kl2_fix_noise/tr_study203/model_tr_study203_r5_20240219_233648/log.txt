Args:
Namespace(name='model_tr_study203', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5', training_data='data/transition_rate_studies/tr_study203/tr_study203_training/r5', validation_data='data/transition_rate_studies/tr_study203/tr_study203_validation/r5', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2956557803

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/10] avg loss: 12.816770992845097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.816770992845097 | validation: 13.243871445447219]
	TIME [epoch: 79.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/10] avg loss: 11.596698678932245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.596698678932245 | validation: 9.397137598826856]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.493724233849054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.493724233849054 | validation: 7.129820670568916]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.017410366558023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.017410366558023 | validation: 10.723387751020962]
	TIME [epoch: 8.55 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.370125602729052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.370125602729052 | validation: 6.3158636237353685]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.460497484600566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.460497484600566 | validation: 5.915792005149383]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.727927599878416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.727927599878416 | validation: 4.476745575643658]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.25200835002404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.25200835002404 | validation: 5.044729866739681]
	TIME [epoch: 8.55 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.743706090534978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.743706090534978 | validation: 4.677244229500937]
	TIME [epoch: 8.53 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.495274350357898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.495274350357898 | validation: 6.054788550304327]
	TIME [epoch: 8.52 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.573399016425002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.573399016425002 | validation: 6.488745721044165]
	TIME [epoch: 8.53 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.547402102215793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.547402102215793 | validation: 6.00235796226877]
	TIME [epoch: 8.55 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.568494555450877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.568494555450877 | validation: 7.248104720385459]
	TIME [epoch: 8.52 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.756755433267595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.756755433267595 | validation: 5.457938054444039]
	TIME [epoch: 8.53 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.500246789359637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.500246789359637 | validation: 3.9688112301775336]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.254988834264979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.254988834264979 | validation: 4.73906165814509]
	TIME [epoch: 8.56 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.299513556810354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.299513556810354 | validation: 4.490502242591246]
	TIME [epoch: 8.52 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.225775830921112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.225775830921112 | validation: 4.123194676451519]
	TIME [epoch: 8.52 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.159979880688074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.159979880688074 | validation: 4.347157276965397]
	TIME [epoch: 8.54 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.149541951768692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.149541951768692 | validation: 3.973053625841439]
	TIME [epoch: 8.52 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.120766887407531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.120766887407531 | validation: 3.9865093128441167]
	TIME [epoch: 8.52 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.957817171986159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.957817171986159 | validation: 4.274986704034401]
	TIME [epoch: 8.53 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.083911047891883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.083911047891883 | validation: 5.295015261663309]
	TIME [epoch: 8.55 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.22094623928793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.22094623928793 | validation: 3.6924147148631103]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.064270362552534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.064270362552534 | validation: 4.067056291956963]
	TIME [epoch: 8.52 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.98199579605317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.98199579605317 | validation: 3.897647704366763]
	TIME [epoch: 8.54 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.973179267311407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.973179267311407 | validation: 4.345453337075753]
	TIME [epoch: 8.53 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.921481912507326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.921481912507326 | validation: 3.6888158748648547]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.8912651422753886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8912651422753886 | validation: 3.56825640829083]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.940285562679258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.940285562679258 | validation: 3.7481655584767903]
	TIME [epoch: 8.56 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.762845897174623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.762845897174623 | validation: 3.5087793381826593]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.924652474429909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.924652474429909 | validation: 4.801995366686364]
	TIME [epoch: 8.52 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.959015471912836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.959015471912836 | validation: 6.8639459022514]
	TIME [epoch: 8.55 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.626409360868044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.626409360868044 | validation: 3.8199709830099717]
	TIME [epoch: 8.53 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.829947396930182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.829947396930182 | validation: 3.0119964263137375]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.7132453295521675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7132453295521675 | validation: 3.2686421760624986]
	TIME [epoch: 8.53 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.886480222653029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.886480222653029 | validation: 3.135524227515003]
	TIME [epoch: 8.54 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.792959993424014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.792959993424014 | validation: 2.8897978351505778]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.911794700959058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.911794700959058 | validation: 2.5896964400210747]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.64909756683627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.64909756683627 | validation: 3.6304677342663547]
	TIME [epoch: 8.54 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.736257497615141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.736257497615141 | validation: 2.8036607546390724]
	TIME [epoch: 8.52 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.717018632142632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.717018632142632 | validation: 2.491851086872315]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.588810895766473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.588810895766473 | validation: 2.7474891530542536]
	TIME [epoch: 8.54 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.501032319934299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.501032319934299 | validation: 3.482647378153232]
	TIME [epoch: 8.53 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.669025056730313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.669025056730313 | validation: 3.3997344319122096]
	TIME [epoch: 8.52 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.6282643661728216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6282643661728216 | validation: 2.74878653730765]
	TIME [epoch: 8.52 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.5597328695531365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5597328695531365 | validation: 2.9112855692316817]
	TIME [epoch: 8.54 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.527667748600227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.527667748600227 | validation: 3.109454140883014]
	TIME [epoch: 8.52 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.480853859904538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.480853859904538 | validation: 2.8641231652403913]
	TIME [epoch: 8.52 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.538469955567144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.538469955567144 | validation: 4.1176529989160855]
	TIME [epoch: 8.52 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.655373870433392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.655373870433392 | validation: 2.596149781839898]
	TIME [epoch: 8.54 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.522065006525347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.522065006525347 | validation: 2.611050310338288]
	TIME [epoch: 8.52 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.4382360861631325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4382360861631325 | validation: 2.9083907000351923]
	TIME [epoch: 8.52 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.456823821606543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.456823821606543 | validation: 2.7050757889062074]
	TIME [epoch: 8.55 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.420212313465608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.420212313465608 | validation: 2.5945411531542097]
	TIME [epoch: 8.52 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.503545315013968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.503545315013968 | validation: 2.6911323746246683]
	TIME [epoch: 8.52 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.409843720936419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.409843720936419 | validation: 2.893857588974563]
	TIME [epoch: 8.52 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.49965445517804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.49965445517804 | validation: 2.5725301601643538]
	TIME [epoch: 8.54 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.408255614017646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.408255614017646 | validation: 3.2928984051319308]
	TIME [epoch: 8.52 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.36170833222558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.36170833222558 | validation: 3.9353628804885528]
	TIME [epoch: 8.52 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.394806691445669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.394806691445669 | validation: 3.213840957858868]
	TIME [epoch: 8.54 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.3388436991156745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3388436991156745 | validation: 3.4634442757564257]
	TIME [epoch: 8.52 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.327653253045594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.327653253045594 | validation: 2.86673045025917]
	TIME [epoch: 8.52 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.307722028168558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.307722028168558 | validation: 2.974470000970908]
	TIME [epoch: 8.52 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.389421105428009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.389421105428009 | validation: 2.853043108434023]
	TIME [epoch: 8.55 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.318843417483308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.318843417483308 | validation: 5.1141768208348655]
	TIME [epoch: 8.52 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.713004706669642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.713004706669642 | validation: 2.219489601859116]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.321177405319969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.321177405319969 | validation: 2.395384377520258]
	TIME [epoch: 8.54 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.367478738573682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.367478738573682 | validation: 2.7338368442081604]
	TIME [epoch: 8.52 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.28027361340237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.28027361340237 | validation: 2.8528671518860667]
	TIME [epoch: 8.52 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.218552776818875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.218552776818875 | validation: 3.1807074189686704]
	TIME [epoch: 8.52 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.326570715693764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.326570715693764 | validation: 3.012651877776451]
	TIME [epoch: 8.54 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.3075995739947865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3075995739947865 | validation: 2.6370877245346422]
	TIME [epoch: 8.52 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.276468849353603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.276468849353603 | validation: 3.7271629174462375]
	TIME [epoch: 8.52 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.289240461120075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.289240461120075 | validation: 4.297405661488626]
	TIME [epoch: 8.54 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.288652110650999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.288652110650999 | validation: 2.8961665736543885]
	TIME [epoch: 8.52 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.173832462995051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.173832462995051 | validation: 2.9056739732820045]
	TIME [epoch: 8.52 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.145057502531273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.145057502531273 | validation: 3.5839607249499696]
	TIME [epoch: 8.51 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.164680261418154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.164680261418154 | validation: 3.002701953856775]
	TIME [epoch: 8.54 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.311384744367311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.311384744367311 | validation: 3.3457789208127307]
	TIME [epoch: 8.52 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.240277110953374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.240277110953374 | validation: 1.969311078224742]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1793205146380235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1793205146380235 | validation: 3.4941836058590283]
	TIME [epoch: 8.53 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.187837190489588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.187837190489588 | validation: 3.0146825848992798]
	TIME [epoch: 8.52 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.092868379321591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.092868379321591 | validation: 2.6312872831468423]
	TIME [epoch: 8.52 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1405221205244755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1405221205244755 | validation: 4.1267505782584095]
	TIME [epoch: 8.51 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.208260899684918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.208260899684918 | validation: 3.0095177415931005]
	TIME [epoch: 8.54 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.133337792960581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.133337792960581 | validation: 2.6257784414956404]
	TIME [epoch: 8.51 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1056774446301025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1056774446301025 | validation: 3.4055912407309346]
	TIME [epoch: 8.51 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.435565448780481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.435565448780481 | validation: 1.943543840062634]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.658283907182146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.658283907182146 | validation: 2.151303533904837]
	TIME [epoch: 8.53 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.054221907675979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.054221907675979 | validation: 2.8554134696417366]
	TIME [epoch: 8.52 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.054374868366635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.054374868366635 | validation: 2.8136160529953314]
	TIME [epoch: 8.51 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.187521098451686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.187521098451686 | validation: 3.9097032829779]
	TIME [epoch: 8.54 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.411130165004299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.411130165004299 | validation: 2.787515574525596]
	TIME [epoch: 8.51 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.175573832599477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.175573832599477 | validation: 2.688873417682363]
	TIME [epoch: 8.51 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.266014797470773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.266014797470773 | validation: 3.8739316348594626]
	TIME [epoch: 8.52 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.4498401387337125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4498401387337125 | validation: 2.913333192307086]
	TIME [epoch: 8.52 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.072127567333772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.072127567333772 | validation: 6.494484882229402]
	TIME [epoch: 8.51 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.431074751760842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.431074751760842 | validation: 4.045327109462367]
	TIME [epoch: 8.51 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.728491943972408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.728491943972408 | validation: 5.274564564223557]
	TIME [epoch: 8.54 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.784131197285456		[learning rate: 0.0099673]
	Learning Rate: 0.00996733
	LOSS [training: 4.784131197285456 | validation: 6.28925186455444]
	TIME [epoch: 8.52 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.30722568934347		[learning rate: 0.0099312]
	Learning Rate: 0.00993116
	LOSS [training: 5.30722568934347 | validation: 4.874110532465604]
	TIME [epoch: 8.5 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.054246532711016		[learning rate: 0.0098951]
	Learning Rate: 0.00989512
	LOSS [training: 5.054246532711016 | validation: 5.06900490373764]
	TIME [epoch: 8.52 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.917917866228427		[learning rate: 0.0098592]
	Learning Rate: 0.00985921
	LOSS [training: 4.917917866228427 | validation: 4.432035858374128]
	TIME [epoch: 8.52 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.76238282519522		[learning rate: 0.0098234]
	Learning Rate: 0.00982343
	LOSS [training: 4.76238282519522 | validation: 4.202502059069691]
	TIME [epoch: 8.51 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.428991179105338		[learning rate: 0.0097878]
	Learning Rate: 0.00978778
	LOSS [training: 4.428991179105338 | validation: 4.202057177619034]
	TIME [epoch: 8.51 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.465888315846969		[learning rate: 0.0097523]
	Learning Rate: 0.00975226
	LOSS [training: 4.465888315846969 | validation: 1.8424639288014266]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.979062609012503		[learning rate: 0.0097169]
	Learning Rate: 0.00971687
	LOSS [training: 3.979062609012503 | validation: 1.8925451491768162]
	TIME [epoch: 8.52 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9138471970806363		[learning rate: 0.0096816]
	Learning Rate: 0.0096816
	LOSS [training: 3.9138471970806363 | validation: 2.400584204782087]
	TIME [epoch: 8.51 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.918869225718079		[learning rate: 0.0096465]
	Learning Rate: 0.00964647
	LOSS [training: 3.918869225718079 | validation: 2.103152754749893]
	TIME [epoch: 8.53 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.023872300661271		[learning rate: 0.0096115]
	Learning Rate: 0.00961146
	LOSS [training: 4.023872300661271 | validation: 1.7979273022593967]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.034845972378522		[learning rate: 0.0095766]
	Learning Rate: 0.00957658
	LOSS [training: 4.034845972378522 | validation: 2.003565652359817]
	TIME [epoch: 8.52 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2288761264501176		[learning rate: 0.0095418]
	Learning Rate: 0.00954183
	LOSS [training: 3.2288761264501176 | validation: 2.5461487519928756]
	TIME [epoch: 8.51 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1770966705268746		[learning rate: 0.0095072]
	Learning Rate: 0.0095072
	LOSS [training: 3.1770966705268746 | validation: 1.5865576231890217]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.962505828063389		[learning rate: 0.0094727]
	Learning Rate: 0.0094727
	LOSS [training: 2.962505828063389 | validation: 2.4263518541546136]
	TIME [epoch: 8.53 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.964359418004151		[learning rate: 0.0094383]
	Learning Rate: 0.00943832
	LOSS [training: 2.964359418004151 | validation: 2.0183577889381343]
	TIME [epoch: 8.53 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9714729831797615		[learning rate: 0.0094041]
	Learning Rate: 0.00940407
	LOSS [training: 2.9714729831797615 | validation: 1.896555275399321]
	TIME [epoch: 8.54 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7739084645528673		[learning rate: 0.0093699]
	Learning Rate: 0.00936994
	LOSS [training: 2.7739084645528673 | validation: 1.5698162938456368]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7118812400591876		[learning rate: 0.0093359]
	Learning Rate: 0.00933594
	LOSS [training: 2.7118812400591876 | validation: 1.4664616830152388]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8359923075698568		[learning rate: 0.0093021]
	Learning Rate: 0.00930206
	LOSS [training: 2.8359923075698568 | validation: 3.455776713713104]
	TIME [epoch: 8.53 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.82306748494567		[learning rate: 0.0092683]
	Learning Rate: 0.0092683
	LOSS [training: 2.82306748494567 | validation: 1.703110343051993]
	TIME [epoch: 8.55 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.695999414639455		[learning rate: 0.0092347]
	Learning Rate: 0.00923466
	LOSS [training: 2.695999414639455 | validation: 2.761548146498645]
	TIME [epoch: 8.52 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.510612154999889		[learning rate: 0.0092011]
	Learning Rate: 0.00920115
	LOSS [training: 3.510612154999889 | validation: 4.608686164665288]
	TIME [epoch: 8.52 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.015458293940763		[learning rate: 0.0091678]
	Learning Rate: 0.00916776
	LOSS [training: 3.015458293940763 | validation: 1.6457574962344172]
	TIME [epoch: 8.54 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.632465026342566		[learning rate: 0.0091345]
	Learning Rate: 0.00913449
	LOSS [training: 2.632465026342566 | validation: 1.6331897379220346]
	TIME [epoch: 8.53 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7735625882822568		[learning rate: 0.0091013]
	Learning Rate: 0.00910134
	LOSS [training: 2.7735625882822568 | validation: 1.9646539587397625]
	TIME [epoch: 8.52 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4277123343901303		[learning rate: 0.0090683]
	Learning Rate: 0.00906831
	LOSS [training: 2.4277123343901303 | validation: 2.1191148906825927]
	TIME [epoch: 8.52 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5419964883738437		[learning rate: 0.0090354]
	Learning Rate: 0.0090354
	LOSS [training: 2.5419964883738437 | validation: 1.8343655596654953]
	TIME [epoch: 8.54 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.594273103688516		[learning rate: 0.0090026]
	Learning Rate: 0.00900261
	LOSS [training: 2.594273103688516 | validation: 2.480838657439692]
	TIME [epoch: 8.52 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4984788623140544		[learning rate: 0.0089699]
	Learning Rate: 0.00896994
	LOSS [training: 2.4984788623140544 | validation: 1.5582118442167368]
	TIME [epoch: 8.52 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.776463961014894		[learning rate: 0.0089374]
	Learning Rate: 0.00893739
	LOSS [training: 2.776463961014894 | validation: 2.1363242247869914]
	TIME [epoch: 8.54 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3413070322070113		[learning rate: 0.008905]
	Learning Rate: 0.00890495
	LOSS [training: 2.3413070322070113 | validation: 1.4148366109126331]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.547669718636411		[learning rate: 0.0088726]
	Learning Rate: 0.00887263
	LOSS [training: 2.547669718636411 | validation: 1.4916316648319463]
	TIME [epoch: 8.52 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.874274665055521		[learning rate: 0.0088404]
	Learning Rate: 0.00884044
	LOSS [training: 2.874274665055521 | validation: 1.2291052333013572]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7736283677515354		[learning rate: 0.0088084]
	Learning Rate: 0.00880835
	LOSS [training: 2.7736283677515354 | validation: 2.0975588176371485]
	TIME [epoch: 8.55 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.525073491449174		[learning rate: 0.0087764]
	Learning Rate: 0.00877639
	LOSS [training: 2.525073491449174 | validation: 1.5007514897225593]
	TIME [epoch: 8.52 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3026236418531805		[learning rate: 0.0087445]
	Learning Rate: 0.00874454
	LOSS [training: 2.3026236418531805 | validation: 1.71949300627274]
	TIME [epoch: 8.52 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2551822205094485		[learning rate: 0.0087128]
	Learning Rate: 0.0087128
	LOSS [training: 2.2551822205094485 | validation: 1.4258098266346624]
	TIME [epoch: 8.53 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.301123856980191		[learning rate: 0.0086812]
	Learning Rate: 0.00868118
	LOSS [training: 2.301123856980191 | validation: 1.734745390648936]
	TIME [epoch: 8.53 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2989815834973046		[learning rate: 0.0086497]
	Learning Rate: 0.00864968
	LOSS [training: 2.2989815834973046 | validation: 2.6487352988256934]
	TIME [epoch: 8.52 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.289152826491919		[learning rate: 0.0086183]
	Learning Rate: 0.00861829
	LOSS [training: 2.289152826491919 | validation: 1.6346074877838697]
	TIME [epoch: 8.52 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1703764769792704		[learning rate: 0.008587]
	Learning Rate: 0.00858701
	LOSS [training: 2.1703764769792704 | validation: 1.3327987319967909]
	TIME [epoch: 8.55 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.826548795603076		[learning rate: 0.0085559]
	Learning Rate: 0.00855585
	LOSS [training: 2.826548795603076 | validation: 1.5937322995189935]
	TIME [epoch: 8.52 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7020739879672955		[learning rate: 0.0085248]
	Learning Rate: 0.0085248
	LOSS [training: 2.7020739879672955 | validation: 1.8369578307738208]
	TIME [epoch: 8.52 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.396014359391284		[learning rate: 0.0084939]
	Learning Rate: 0.00849386
	LOSS [training: 2.396014359391284 | validation: 1.6879700949396084]
	TIME [epoch: 8.54 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2969441661263983		[learning rate: 0.008463]
	Learning Rate: 0.00846304
	LOSS [training: 2.2969441661263983 | validation: 1.5723282288575477]
	TIME [epoch: 8.54 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.631937512322017		[learning rate: 0.0084323]
	Learning Rate: 0.00843233
	LOSS [training: 2.631937512322017 | validation: 1.875601825815663]
	TIME [epoch: 8.53 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4488548004943516		[learning rate: 0.0084017]
	Learning Rate: 0.00840172
	LOSS [training: 2.4488548004943516 | validation: 1.2302170208614942]
	TIME [epoch: 8.52 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7482556883731335		[learning rate: 0.0083712]
	Learning Rate: 0.00837123
	LOSS [training: 2.7482556883731335 | validation: 2.589036522814682]
	TIME [epoch: 8.54 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5117286448724565		[learning rate: 0.0083409]
	Learning Rate: 0.00834085
	LOSS [training: 2.5117286448724565 | validation: 1.5770821813946383]
	TIME [epoch: 8.53 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2392748752306884		[learning rate: 0.0083106]
	Learning Rate: 0.00831059
	LOSS [training: 2.2392748752306884 | validation: 1.6310961478096555]
	TIME [epoch: 8.52 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.252932089344402		[learning rate: 0.0082804]
	Learning Rate: 0.00828043
	LOSS [training: 2.252932089344402 | validation: 1.4098568993785836]
	TIME [epoch: 8.54 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.257095823829398		[learning rate: 0.0082504]
	Learning Rate: 0.00825037
	LOSS [training: 2.257095823829398 | validation: 1.6692135186106922]
	TIME [epoch: 8.53 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7370147238930325		[learning rate: 0.0082204]
	Learning Rate: 0.00822043
	LOSS [training: 2.7370147238930325 | validation: 2.2470976168128116]
	TIME [epoch: 8.52 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.357800724212212		[learning rate: 0.0081906]
	Learning Rate: 0.0081906
	LOSS [training: 2.357800724212212 | validation: 1.492837155538644]
	TIME [epoch: 8.52 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.416189832866448		[learning rate: 0.0081609]
	Learning Rate: 0.00816088
	LOSS [training: 2.416189832866448 | validation: 1.3398860966942054]
	TIME [epoch: 8.55 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.774690592652612		[learning rate: 0.0081313]
	Learning Rate: 0.00813126
	LOSS [training: 2.774690592652612 | validation: 1.2812441945518294]
	TIME [epoch: 8.52 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2189309219799958		[learning rate: 0.0081018]
	Learning Rate: 0.00810175
	LOSS [training: 2.2189309219799958 | validation: 1.626190390157247]
	TIME [epoch: 8.52 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3235797444983435		[learning rate: 0.0080724]
	Learning Rate: 0.00807235
	LOSS [training: 2.3235797444983435 | validation: 1.2878800979506444]
	TIME [epoch: 8.52 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2830230951771293		[learning rate: 0.0080431]
	Learning Rate: 0.00804305
	LOSS [training: 2.2830230951771293 | validation: 1.3754738555258306]
	TIME [epoch: 8.54 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5336943386354625		[learning rate: 0.0080139]
	Learning Rate: 0.00801387
	LOSS [training: 3.5336943386354625 | validation: 1.5718480056143767]
	TIME [epoch: 8.53 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.314246589434214		[learning rate: 0.0079848]
	Learning Rate: 0.00798478
	LOSS [training: 2.314246589434214 | validation: 1.609250754610354]
	TIME [epoch: 8.52 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1358290073843875		[learning rate: 0.0079558]
	Learning Rate: 0.00795581
	LOSS [training: 2.1358290073843875 | validation: 1.0132783519607296]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4842866645184905		[learning rate: 0.0079269]
	Learning Rate: 0.00792693
	LOSS [training: 2.4842866645184905 | validation: 2.2744142037122868]
	TIME [epoch: 8.52 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1704915241195506		[learning rate: 0.0078982]
	Learning Rate: 0.00789817
	LOSS [training: 2.1704915241195506 | validation: 2.775246947921133]
	TIME [epoch: 8.52 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7570087461938657		[learning rate: 0.0078695]
	Learning Rate: 0.0078695
	LOSS [training: 2.7570087461938657 | validation: 1.4548505898332746]
	TIME [epoch: 8.52 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2181381213126015		[learning rate: 0.0078409]
	Learning Rate: 0.00784094
	LOSS [training: 2.2181381213126015 | validation: 1.6990043278715112]
	TIME [epoch: 8.53 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5019188911636654		[learning rate: 0.0078125]
	Learning Rate: 0.00781249
	LOSS [training: 2.5019188911636654 | validation: 1.3543284547578578]
	TIME [epoch: 8.52 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.622523206191507		[learning rate: 0.0077841]
	Learning Rate: 0.00778414
	LOSS [training: 2.622523206191507 | validation: 1.3948978237568521]
	TIME [epoch: 8.51 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.899963931116406		[learning rate: 0.0077559]
	Learning Rate: 0.00775589
	LOSS [training: 2.899963931116406 | validation: 1.8409363595586785]
	TIME [epoch: 8.54 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7018280351564976		[learning rate: 0.0077277]
	Learning Rate: 0.00772774
	LOSS [training: 2.7018280351564976 | validation: 1.2514439213155395]
	TIME [epoch: 8.52 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.922462990627307		[learning rate: 0.0076997]
	Learning Rate: 0.0076997
	LOSS [training: 2.922462990627307 | validation: 1.7545043334653847]
	TIME [epoch: 8.51 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.469535838747249		[learning rate: 0.0076718]
	Learning Rate: 0.00767176
	LOSS [training: 2.469535838747249 | validation: 1.2519454922647115]
	TIME [epoch: 8.52 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.238883569993601		[learning rate: 0.0076439]
	Learning Rate: 0.00764391
	LOSS [training: 2.238883569993601 | validation: 2.3456860301264424]
	TIME [epoch: 8.54 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.197516797998346		[learning rate: 0.0076162]
	Learning Rate: 0.00761617
	LOSS [training: 2.197516797998346 | validation: 1.7781077793336917]
	TIME [epoch: 8.51 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8901619454259393		[learning rate: 0.0075885]
	Learning Rate: 0.00758853
	LOSS [training: 1.8901619454259393 | validation: 1.57887103848772]
	TIME [epoch: 8.51 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.082860284104702		[learning rate: 0.007561]
	Learning Rate: 0.00756099
	LOSS [training: 2.082860284104702 | validation: 1.6083426228840476]
	TIME [epoch: 8.53 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1643605542281064		[learning rate: 0.0075336]
	Learning Rate: 0.00753356
	LOSS [training: 2.1643605542281064 | validation: 1.966356779464942]
	TIME [epoch: 8.52 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.066230314284439		[learning rate: 0.0075062]
	Learning Rate: 0.00750622
	LOSS [training: 2.066230314284439 | validation: 1.4653229793489537]
	TIME [epoch: 8.5 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9829447571631378		[learning rate: 0.007479]
	Learning Rate: 0.00747897
	LOSS [training: 1.9829447571631378 | validation: 1.2953019010733267]
	TIME [epoch: 8.51 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0480659870244198		[learning rate: 0.0074518]
	Learning Rate: 0.00745183
	LOSS [training: 2.0480659870244198 | validation: 1.6957991029975046]
	TIME [epoch: 8.53 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.220738648436373		[learning rate: 0.0074248]
	Learning Rate: 0.00742479
	LOSS [training: 2.220738648436373 | validation: 1.1940574447351022]
	TIME [epoch: 8.52 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.185821744461199		[learning rate: 0.0073978]
	Learning Rate: 0.00739784
	LOSS [training: 2.185821744461199 | validation: 1.3438890388956446]
	TIME [epoch: 8.51 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0332669239251695		[learning rate: 0.007371]
	Learning Rate: 0.007371
	LOSS [training: 2.0332669239251695 | validation: 1.9089131023975319]
	TIME [epoch: 8.54 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9074099896604164		[learning rate: 0.0073442]
	Learning Rate: 0.00734425
	LOSS [training: 1.9074099896604164 | validation: 1.7426853525497306]
	TIME [epoch: 8.52 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3348933039837236		[learning rate: 0.0073176]
	Learning Rate: 0.0073176
	LOSS [training: 2.3348933039837236 | validation: 1.4121035353783653]
	TIME [epoch: 8.51 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.468903609317452		[learning rate: 0.007291]
	Learning Rate: 0.00729104
	LOSS [training: 2.468903609317452 | validation: 1.659717123918261]
	TIME [epoch: 8.51 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1563842032789324		[learning rate: 0.0072646]
	Learning Rate: 0.00726458
	LOSS [training: 2.1563842032789324 | validation: 1.155256255496482]
	TIME [epoch: 8.54 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8772288953145058		[learning rate: 0.0072382]
	Learning Rate: 0.00723822
	LOSS [training: 1.8772288953145058 | validation: 1.2295479744076339]
	TIME [epoch: 8.51 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8364655036249993		[learning rate: 0.0072119]
	Learning Rate: 0.00721195
	LOSS [training: 1.8364655036249993 | validation: 0.9714871283215797]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.938696358803416		[learning rate: 0.0071858]
	Learning Rate: 0.00718578
	LOSS [training: 1.938696358803416 | validation: 2.7260455660318286]
	TIME [epoch: 8.54 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4896252805066155		[learning rate: 0.0071597]
	Learning Rate: 0.0071597
	LOSS [training: 2.4896252805066155 | validation: 3.4444106887612995]
	TIME [epoch: 8.52 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.3827699791056838		[learning rate: 0.0071337]
	Learning Rate: 0.00713371
	LOSS [training: 2.3827699791056838 | validation: 1.092960109203974]
	TIME [epoch: 8.52 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0419123428754986		[learning rate: 0.0071078]
	Learning Rate: 0.00710783
	LOSS [training: 2.0419123428754986 | validation: 1.3947255198450017]
	TIME [epoch: 8.52 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9890277566974768		[learning rate: 0.007082]
	Learning Rate: 0.00708203
	LOSS [training: 1.9890277566974768 | validation: 1.4398336789733106]
	TIME [epoch: 8.54 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1642526165389637		[learning rate: 0.0070563]
	Learning Rate: 0.00705633
	LOSS [training: 2.1642526165389637 | validation: 1.5290562969891344]
	TIME [epoch: 8.51 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.219934428183123		[learning rate: 0.0070307]
	Learning Rate: 0.00703072
	LOSS [training: 2.219934428183123 | validation: 1.5236134914180754]
	TIME [epoch: 8.51 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.825713897774122		[learning rate: 0.0070052]
	Learning Rate: 0.00700521
	LOSS [training: 1.825713897774122 | validation: 1.1975951321071177]
	TIME [epoch: 8.53 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.887220360714776		[learning rate: 0.0069798]
	Learning Rate: 0.00697979
	LOSS [training: 1.887220360714776 | validation: 1.8758369231014262]
	TIME [epoch: 8.52 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0086426976535696		[learning rate: 0.0069545]
	Learning Rate: 0.00695446
	LOSS [training: 2.0086426976535696 | validation: 1.3082892656753724]
	TIME [epoch: 8.51 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7040392532370816		[learning rate: 0.0069292]
	Learning Rate: 0.00692922
	LOSS [training: 1.7040392532370816 | validation: 1.401441129651925]
	TIME [epoch: 8.52 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.953022328303249		[learning rate: 0.0069041]
	Learning Rate: 0.00690407
	LOSS [training: 1.953022328303249 | validation: 1.1449666395457196]
	TIME [epoch: 8.54 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.787063331184323		[learning rate: 0.006879]
	Learning Rate: 0.00687902
	LOSS [training: 1.787063331184323 | validation: 1.2506751796645466]
	TIME [epoch: 8.52 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7429374492622685		[learning rate: 0.0068541]
	Learning Rate: 0.00685405
	LOSS [training: 1.7429374492622685 | validation: 1.258858189874821]
	TIME [epoch: 8.51 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8159085588467097		[learning rate: 0.0068292]
	Learning Rate: 0.00682918
	LOSS [training: 1.8159085588467097 | validation: 1.2862290725633672]
	TIME [epoch: 8.53 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7049386555156452		[learning rate: 0.0068044]
	Learning Rate: 0.00680439
	LOSS [training: 1.7049386555156452 | validation: 1.1534066394384126]
	TIME [epoch: 8.53 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7984655112761458		[learning rate: 0.0067797]
	Learning Rate: 0.0067797
	LOSS [training: 1.7984655112761458 | validation: 1.5414371026716056]
	TIME [epoch: 8.51 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.760187912339981		[learning rate: 0.0067551]
	Learning Rate: 0.0067551
	LOSS [training: 1.760187912339981 | validation: 1.1975038556585793]
	TIME [epoch: 8.51 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9287502973432935		[learning rate: 0.0067306]
	Learning Rate: 0.00673058
	LOSS [training: 1.9287502973432935 | validation: 1.0868931444668113]
	TIME [epoch: 8.54 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1155241379900835		[learning rate: 0.0067062]
	Learning Rate: 0.00670616
	LOSS [training: 2.1155241379900835 | validation: 1.3403471617970049]
	TIME [epoch: 8.52 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8222504419384016		[learning rate: 0.0066818]
	Learning Rate: 0.00668182
	LOSS [training: 1.8222504419384016 | validation: 1.2188393533787008]
	TIME [epoch: 8.51 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9493271877649012		[learning rate: 0.0066576]
	Learning Rate: 0.00665757
	LOSS [training: 1.9493271877649012 | validation: 1.147012300313842]
	TIME [epoch: 8.53 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7315133261951092		[learning rate: 0.0066334]
	Learning Rate: 0.00663341
	LOSS [training: 1.7315133261951092 | validation: 2.078857184461041]
	TIME [epoch: 8.52 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1308111651324397		[learning rate: 0.0066093]
	Learning Rate: 0.00660934
	LOSS [training: 2.1308111651324397 | validation: 1.6760446040450394]
	TIME [epoch: 8.51 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.077356591657357		[learning rate: 0.0065854]
	Learning Rate: 0.00658535
	LOSS [training: 2.077356591657357 | validation: 2.2097122967407614]
	TIME [epoch: 8.52 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1280704351990605		[learning rate: 0.0065615]
	Learning Rate: 0.00656145
	LOSS [training: 2.1280704351990605 | validation: 2.794651990561423]
	TIME [epoch: 8.53 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9262399433235238		[learning rate: 0.0065376]
	Learning Rate: 0.00653764
	LOSS [training: 1.9262399433235238 | validation: 0.8888617355657525]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_217.pth
	Model improved!!!
EPOCH 218/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6892374400842773		[learning rate: 0.0065139]
	Learning Rate: 0.00651392
	LOSS [training: 1.6892374400842773 | validation: 1.577055396022006]
	TIME [epoch: 8.51 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7702807650156533		[learning rate: 0.0064903]
	Learning Rate: 0.00649028
	LOSS [training: 1.7702807650156533 | validation: 0.9871817543306531]
	TIME [epoch: 8.53 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8599549050463557		[learning rate: 0.0064667]
	Learning Rate: 0.00646672
	LOSS [training: 1.8599549050463557 | validation: 1.1485692582137819]
	TIME [epoch: 8.52 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8316790910267997		[learning rate: 0.0064433]
	Learning Rate: 0.00644325
	LOSS [training: 1.8316790910267997 | validation: 0.9523513293001986]
	TIME [epoch: 8.51 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0519022257713866		[learning rate: 0.0064199]
	Learning Rate: 0.00641987
	LOSS [training: 2.0519022257713866 | validation: 2.060310034057535]
	TIME [epoch: 8.5 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9896070013966483		[learning rate: 0.0063966]
	Learning Rate: 0.00639657
	LOSS [training: 1.9896070013966483 | validation: 1.270913526175261]
	TIME [epoch: 8.53 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.630589403545759		[learning rate: 0.0063734]
	Learning Rate: 0.00637336
	LOSS [training: 1.630589403545759 | validation: 0.9949024169764513]
	TIME [epoch: 8.5 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8862599842698742		[learning rate: 0.0063502]
	Learning Rate: 0.00635023
	LOSS [training: 1.8862599842698742 | validation: 1.0847034391972699]
	TIME [epoch: 8.5 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5173292828029101		[learning rate: 0.0063272]
	Learning Rate: 0.00632718
	LOSS [training: 1.5173292828029101 | validation: 1.2636865307192884]
	TIME [epoch: 8.51 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7373998781367317		[learning rate: 0.0063042]
	Learning Rate: 0.00630422
	LOSS [training: 1.7373998781367317 | validation: 1.1827301493443163]
	TIME [epoch: 8.53 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9451809100564597		[learning rate: 0.0062813]
	Learning Rate: 0.00628134
	LOSS [training: 1.9451809100564597 | validation: 1.2239733492254372]
	TIME [epoch: 8.52 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5341935628441894		[learning rate: 0.0062585]
	Learning Rate: 0.00625855
	LOSS [training: 1.5341935628441894 | validation: 0.9985340890799864]
	TIME [epoch: 8.52 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8580936943103858		[learning rate: 0.0062358]
	Learning Rate: 0.00623584
	LOSS [training: 1.8580936943103858 | validation: 1.1436785940272893]
	TIME [epoch: 8.54 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6548270967119827		[learning rate: 0.0062132]
	Learning Rate: 0.00621321
	LOSS [training: 1.6548270967119827 | validation: 1.3351528629766818]
	TIME [epoch: 8.52 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.793647082802977		[learning rate: 0.0061907]
	Learning Rate: 0.00619066
	LOSS [training: 1.793647082802977 | validation: 0.9286443494153503]
	TIME [epoch: 8.51 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6021983581320112		[learning rate: 0.0061682]
	Learning Rate: 0.00616819
	LOSS [training: 1.6021983581320112 | validation: 1.672723259181935]
	TIME [epoch: 8.51 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7427717005870995		[learning rate: 0.0061458]
	Learning Rate: 0.00614581
	LOSS [training: 1.7427717005870995 | validation: 1.5200460790099126]
	TIME [epoch: 8.53 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9354797331577234		[learning rate: 0.0061235]
	Learning Rate: 0.0061235
	LOSS [training: 1.9354797331577234 | validation: 1.1560224849687022]
	TIME [epoch: 8.51 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8337983802129998		[learning rate: 0.0061013]
	Learning Rate: 0.00610128
	LOSS [training: 1.8337983802129998 | validation: 0.9574293603274269]
	TIME [epoch: 8.51 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6281807406932642		[learning rate: 0.0060791]
	Learning Rate: 0.00607914
	LOSS [training: 1.6281807406932642 | validation: 1.7212194393138995]
	TIME [epoch: 8.53 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8381557102357093		[learning rate: 0.0060571]
	Learning Rate: 0.00605708
	LOSS [training: 1.8381557102357093 | validation: 1.4016397734610706]
	TIME [epoch: 8.52 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6989389563183734		[learning rate: 0.0060351]
	Learning Rate: 0.0060351
	LOSS [training: 1.6989389563183734 | validation: 1.4019337789700823]
	TIME [epoch: 8.51 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9153419786792594		[learning rate: 0.0060132]
	Learning Rate: 0.00601319
	LOSS [training: 1.9153419786792594 | validation: 1.0225568987169678]
	TIME [epoch: 8.51 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.813453027406633		[learning rate: 0.0059914]
	Learning Rate: 0.00599137
	LOSS [training: 1.813453027406633 | validation: 0.9465027334683204]
	TIME [epoch: 8.53 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.708683634159836		[learning rate: 0.0059696]
	Learning Rate: 0.00596963
	LOSS [training: 1.708683634159836 | validation: 1.2548977703317798]
	TIME [epoch: 8.51 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.727408373655805		[learning rate: 0.005948]
	Learning Rate: 0.00594797
	LOSS [training: 1.727408373655805 | validation: 0.9920461413632677]
	TIME [epoch: 8.51 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8766710103605522		[learning rate: 0.0059264]
	Learning Rate: 0.00592638
	LOSS [training: 1.8766710103605522 | validation: 0.9731312511906356]
	TIME [epoch: 8.53 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7556110218029701		[learning rate: 0.0059049]
	Learning Rate: 0.00590487
	LOSS [training: 1.7556110218029701 | validation: 1.1510666198661132]
	TIME [epoch: 8.52 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5037910375092611		[learning rate: 0.0058834]
	Learning Rate: 0.00588344
	LOSS [training: 1.5037910375092611 | validation: 0.9828996832449688]
	TIME [epoch: 8.51 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.483286311924235		[learning rate: 0.0058621]
	Learning Rate: 0.00586209
	LOSS [training: 1.483286311924235 | validation: 1.3628404201836215]
	TIME [epoch: 8.51 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5297366998275213		[learning rate: 0.0058408]
	Learning Rate: 0.00584082
	LOSS [training: 1.5297366998275213 | validation: 0.974721391491579]
	TIME [epoch: 8.53 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6119453449746655		[learning rate: 0.0058196]
	Learning Rate: 0.00581962
	LOSS [training: 1.6119453449746655 | validation: 1.0636288853528835]
	TIME [epoch: 8.51 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.816627049192456		[learning rate: 0.0057985]
	Learning Rate: 0.0057985
	LOSS [training: 1.816627049192456 | validation: 1.67139358367276]
	TIME [epoch: 8.51 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8917670083605571		[learning rate: 0.0057775]
	Learning Rate: 0.00577746
	LOSS [training: 1.8917670083605571 | validation: 1.8889653294124522]
	TIME [epoch: 8.52 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7119811916720362		[learning rate: 0.0057565]
	Learning Rate: 0.00575649
	LOSS [training: 1.7119811916720362 | validation: 1.0545858799280285]
	TIME [epoch: 8.52 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6776581734121954		[learning rate: 0.0057356]
	Learning Rate: 0.0057356
	LOSS [training: 1.6776581734121954 | validation: 0.9157704055929764]
	TIME [epoch: 8.5 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4828811831272		[learning rate: 0.0057148]
	Learning Rate: 0.00571479
	LOSS [training: 1.4828811831272 | validation: 0.9609495763729916]
	TIME [epoch: 8.51 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8303713736644596		[learning rate: 0.005694]
	Learning Rate: 0.00569405
	LOSS [training: 1.8303713736644596 | validation: 1.109674304293526]
	TIME [epoch: 8.53 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6794023485151777		[learning rate: 0.0056734]
	Learning Rate: 0.00567338
	LOSS [training: 1.6794023485151777 | validation: 1.463979762238242]
	TIME [epoch: 8.51 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5744043298319585		[learning rate: 0.0056528]
	Learning Rate: 0.00565279
	LOSS [training: 1.5744043298319585 | validation: 0.8752641578365976]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.598594634416308		[learning rate: 0.0056323]
	Learning Rate: 0.00563228
	LOSS [training: 1.598594634416308 | validation: 0.9149951774755353]
	TIME [epoch: 8.53 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5995120634698707		[learning rate: 0.0056118]
	Learning Rate: 0.00561184
	LOSS [training: 1.5995120634698707 | validation: 1.4954847605522281]
	TIME [epoch: 8.51 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7567182768299987		[learning rate: 0.0055915]
	Learning Rate: 0.00559147
	LOSS [training: 1.7567182768299987 | validation: 1.9836188599438969]
	TIME [epoch: 8.5 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7704745006807827		[learning rate: 0.0055712]
	Learning Rate: 0.00557118
	LOSS [training: 1.7704745006807827 | validation: 1.1157802781152069]
	TIME [epoch: 8.5 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5547832641200958		[learning rate: 0.005551]
	Learning Rate: 0.00555096
	LOSS [training: 1.5547832641200958 | validation: 1.069564627226535]
	TIME [epoch: 8.53 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5664023905855369		[learning rate: 0.0055308]
	Learning Rate: 0.00553082
	LOSS [training: 1.5664023905855369 | validation: 1.311833952782665]
	TIME [epoch: 8.51 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5100763963093182		[learning rate: 0.0055107]
	Learning Rate: 0.00551075
	LOSS [training: 1.5100763963093182 | validation: 1.0744591805687447]
	TIME [epoch: 8.51 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.572387984140933		[learning rate: 0.0054907]
	Learning Rate: 0.00549075
	LOSS [training: 1.572387984140933 | validation: 0.9102733655209391]
	TIME [epoch: 8.52 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6702153032753664		[learning rate: 0.0054708]
	Learning Rate: 0.00547082
	LOSS [training: 1.6702153032753664 | validation: 1.0126830298493452]
	TIME [epoch: 8.52 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6668192342329538		[learning rate: 0.005451]
	Learning Rate: 0.00545097
	LOSS [training: 1.6668192342329538 | validation: 1.1785304777345513]
	TIME [epoch: 8.51 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.487434955119819		[learning rate: 0.0054312]
	Learning Rate: 0.00543119
	LOSS [training: 1.487434955119819 | validation: 1.3050347116908079]
	TIME [epoch: 8.51 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.483057278826855		[learning rate: 0.0054115]
	Learning Rate: 0.00541148
	LOSS [training: 1.483057278826855 | validation: 1.0596487152190777]
	TIME [epoch: 8.53 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7897182511627547		[learning rate: 0.0053918]
	Learning Rate: 0.00539184
	LOSS [training: 1.7897182511627547 | validation: 1.1797187969571572]
	TIME [epoch: 8.51 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9111392126073556		[learning rate: 0.0053723]
	Learning Rate: 0.00537227
	LOSS [training: 1.9111392126073556 | validation: 0.9297151788974323]
	TIME [epoch: 8.52 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7039546870990894		[learning rate: 0.0053528]
	Learning Rate: 0.00535277
	LOSS [training: 1.7039546870990894 | validation: 1.0496853467247313]
	TIME [epoch: 8.52 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4480437935580108		[learning rate: 0.0053333]
	Learning Rate: 0.00533335
	LOSS [training: 1.4480437935580108 | validation: 1.1795738591242368]
	TIME [epoch: 8.53 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7340023103270439		[learning rate: 0.005314]
	Learning Rate: 0.00531399
	LOSS [training: 1.7340023103270439 | validation: 0.990656081215071]
	TIME [epoch: 8.51 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6001934461266685		[learning rate: 0.0052947]
	Learning Rate: 0.00529471
	LOSS [training: 1.6001934461266685 | validation: 1.068059205923046]
	TIME [epoch: 8.51 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6942231616107661		[learning rate: 0.0052755]
	Learning Rate: 0.00527549
	LOSS [training: 1.6942231616107661 | validation: 1.3380276103438138]
	TIME [epoch: 8.53 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.411364601815777		[learning rate: 0.0052563]
	Learning Rate: 0.00525635
	LOSS [training: 1.411364601815777 | validation: 1.0017621090970918]
	TIME [epoch: 8.51 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6053550491672581		[learning rate: 0.0052373]
	Learning Rate: 0.00523727
	LOSS [training: 1.6053550491672581 | validation: 0.9553599540493336]
	TIME [epoch: 8.5 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6175119681696208		[learning rate: 0.0052183]
	Learning Rate: 0.00521827
	LOSS [training: 1.6175119681696208 | validation: 1.3896541225292138]
	TIME [epoch: 8.51 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6413476704924477		[learning rate: 0.0051993]
	Learning Rate: 0.00519933
	LOSS [training: 1.6413476704924477 | validation: 0.9422379361377042]
	TIME [epoch: 8.53 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4578579060390973		[learning rate: 0.0051805]
	Learning Rate: 0.00518046
	LOSS [training: 1.4578579060390973 | validation: 0.8282876178258893]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_281.pth
	Model improved!!!
EPOCH 282/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5863351888469244		[learning rate: 0.0051617]
	Learning Rate: 0.00516166
	LOSS [training: 1.5863351888469244 | validation: 0.9572332393627805]
	TIME [epoch: 8.5 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4187034269216194		[learning rate: 0.0051429]
	Learning Rate: 0.00514293
	LOSS [training: 1.4187034269216194 | validation: 1.0299428743616266]
	TIME [epoch: 8.53 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6399037935339478		[learning rate: 0.0051243]
	Learning Rate: 0.00512426
	LOSS [training: 1.6399037935339478 | validation: 1.0449577781343264]
	TIME [epoch: 8.51 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5468106350980302		[learning rate: 0.0051057]
	Learning Rate: 0.00510567
	LOSS [training: 1.5468106350980302 | validation: 0.9429549680275108]
	TIME [epoch: 8.5 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4667350037081097		[learning rate: 0.0050871]
	Learning Rate: 0.00508714
	LOSS [training: 1.4667350037081097 | validation: 1.3618747228263692]
	TIME [epoch: 8.5 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5428582497519077		[learning rate: 0.0050687]
	Learning Rate: 0.00506868
	LOSS [training: 1.5428582497519077 | validation: 1.0835758761688945]
	TIME [epoch: 8.52 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.676121695407596		[learning rate: 0.0050503]
	Learning Rate: 0.00505028
	LOSS [training: 1.676121695407596 | validation: 1.1689340362305218]
	TIME [epoch: 8.51 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.567432289125929		[learning rate: 0.005032]
	Learning Rate: 0.00503196
	LOSS [training: 1.567432289125929 | validation: 1.666611103195884]
	TIME [epoch: 8.5 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.716230943319887		[learning rate: 0.0050137]
	Learning Rate: 0.00501369
	LOSS [training: 1.716230943319887 | validation: 1.6613849706814299]
	TIME [epoch: 8.52 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.545686725357569		[learning rate: 0.0049955]
	Learning Rate: 0.0049955
	LOSS [training: 1.545686725357569 | validation: 0.9443984667668751]
	TIME [epoch: 8.51 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6057126945918114		[learning rate: 0.0049774]
	Learning Rate: 0.00497737
	LOSS [training: 1.6057126945918114 | validation: 0.9114697715390304]
	TIME [epoch: 8.51 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3592931091655134		[learning rate: 0.0049593]
	Learning Rate: 0.00495931
	LOSS [training: 1.3592931091655134 | validation: 0.9940549007304185]
	TIME [epoch: 8.5 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6084570223230046		[learning rate: 0.0049413]
	Learning Rate: 0.00494131
	LOSS [training: 1.6084570223230046 | validation: 0.8199690348526265]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4542688011620037		[learning rate: 0.0049234]
	Learning Rate: 0.00492338
	LOSS [training: 1.4542688011620037 | validation: 1.4981560577321291]
	TIME [epoch: 8.51 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5861314215411713		[learning rate: 0.0049055]
	Learning Rate: 0.00490551
	LOSS [training: 1.5861314215411713 | validation: 1.0713894541140079]
	TIME [epoch: 8.5 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.631817255816752		[learning rate: 0.0048877]
	Learning Rate: 0.00488771
	LOSS [training: 1.631817255816752 | validation: 1.061679653962635]
	TIME [epoch: 8.52 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4435018756467275		[learning rate: 0.00487]
	Learning Rate: 0.00486997
	LOSS [training: 1.4435018756467275 | validation: 0.9064369406397162]
	TIME [epoch: 8.5 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6717063359587718		[learning rate: 0.0048523]
	Learning Rate: 0.0048523
	LOSS [training: 1.6717063359587718 | validation: 1.3266995966984445]
	TIME [epoch: 8.51 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4550073492518456		[learning rate: 0.0048347]
	Learning Rate: 0.00483469
	LOSS [training: 1.4550073492518456 | validation: 1.0879204315095858]
	TIME [epoch: 8.51 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.520280732704525		[learning rate: 0.0048171]
	Learning Rate: 0.00481714
	LOSS [training: 1.520280732704525 | validation: 0.9320926614456346]
	TIME [epoch: 8.53 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.389038538456412		[learning rate: 0.0047997]
	Learning Rate: 0.00479966
	LOSS [training: 1.389038538456412 | validation: 1.2472473897046334]
	TIME [epoch: 8.51 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4549114565435128		[learning rate: 0.0047822]
	Learning Rate: 0.00478224
	LOSS [training: 1.4549114565435128 | validation: 1.1729758725802455]
	TIME [epoch: 8.5 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3576465082868951		[learning rate: 0.0047649]
	Learning Rate: 0.00476489
	LOSS [training: 1.3576465082868951 | validation: 0.8733547787600271]
	TIME [epoch: 8.52 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4063496429579398		[learning rate: 0.0047476]
	Learning Rate: 0.00474759
	LOSS [training: 1.4063496429579398 | validation: 1.272667891687677]
	TIME [epoch: 8.5 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5205743135427527		[learning rate: 0.0047304]
	Learning Rate: 0.00473037
	LOSS [training: 1.5205743135427527 | validation: 1.2144429074970187]
	TIME [epoch: 8.5 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4001302476418869		[learning rate: 0.0047132]
	Learning Rate: 0.0047132
	LOSS [training: 1.4001302476418869 | validation: 1.1273225418392223]
	TIME [epoch: 8.5 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5993804946278485		[learning rate: 0.0046961]
	Learning Rate: 0.00469609
	LOSS [training: 1.5993804946278485 | validation: 1.5930694996411752]
	TIME [epoch: 8.53 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.599781463636726		[learning rate: 0.0046791]
	Learning Rate: 0.00467905
	LOSS [training: 1.599781463636726 | validation: 0.9315998015692312]
	TIME [epoch: 8.5 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5480970906968614		[learning rate: 0.0046621]
	Learning Rate: 0.00466207
	LOSS [training: 1.5480970906968614 | validation: 1.121417293269045]
	TIME [epoch: 8.5 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4240710861376777		[learning rate: 0.0046452]
	Learning Rate: 0.00464515
	LOSS [training: 1.4240710861376777 | validation: 1.4446434005898567]
	TIME [epoch: 8.51 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4178649640420358		[learning rate: 0.0046283]
	Learning Rate: 0.0046283
	LOSS [training: 1.4178649640420358 | validation: 0.909302758180961]
	TIME [epoch: 8.53 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3468469202047582		[learning rate: 0.0046115]
	Learning Rate: 0.0046115
	LOSS [training: 1.3468469202047582 | validation: 0.8826834809719848]
	TIME [epoch: 8.5 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.472790111328963		[learning rate: 0.0045948]
	Learning Rate: 0.00459476
	LOSS [training: 1.472790111328963 | validation: 1.0448395151836438]
	TIME [epoch: 8.51 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4535659518676822		[learning rate: 0.0045781]
	Learning Rate: 0.00457809
	LOSS [training: 1.4535659518676822 | validation: 1.2076051555955511]
	TIME [epoch: 8.53 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5463317628891111		[learning rate: 0.0045615]
	Learning Rate: 0.00456148
	LOSS [training: 1.5463317628891111 | validation: 1.4904277035996287]
	TIME [epoch: 8.5 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5663904480613389		[learning rate: 0.0045449]
	Learning Rate: 0.00454492
	LOSS [training: 1.5663904480613389 | validation: 1.3280041973029373]
	TIME [epoch: 8.5 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5126066062302956		[learning rate: 0.0045284]
	Learning Rate: 0.00452843
	LOSS [training: 1.5126066062302956 | validation: 1.1819460484272521]
	TIME [epoch: 8.51 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4641789334985984		[learning rate: 0.004512]
	Learning Rate: 0.00451199
	LOSS [training: 1.4641789334985984 | validation: 0.9276282552228944]
	TIME [epoch: 8.51 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3952396966680378		[learning rate: 0.0044956]
	Learning Rate: 0.00449562
	LOSS [training: 1.3952396966680378 | validation: 0.9086244041813134]
	TIME [epoch: 8.49 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3800069115436213		[learning rate: 0.0044793]
	Learning Rate: 0.0044793
	LOSS [training: 1.3800069115436213 | validation: 0.8641712720823931]
	TIME [epoch: 8.49 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5250566811702428		[learning rate: 0.004463]
	Learning Rate: 0.00446305
	LOSS [training: 1.5250566811702428 | validation: 1.049856235304788]
	TIME [epoch: 8.53 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6183337528262711		[learning rate: 0.0044469]
	Learning Rate: 0.00444685
	LOSS [training: 1.6183337528262711 | validation: 0.9796639318018387]
	TIME [epoch: 8.5 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3725190798901206		[learning rate: 0.0044307]
	Learning Rate: 0.00443071
	LOSS [training: 1.3725190798901206 | validation: 1.1954670567518788]
	TIME [epoch: 8.5 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5149439783700358		[learning rate: 0.0044146]
	Learning Rate: 0.00441463
	LOSS [training: 1.5149439783700358 | validation: 1.2728035253641785]
	TIME [epoch: 8.51 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.402407823886455		[learning rate: 0.0043986]
	Learning Rate: 0.00439861
	LOSS [training: 1.402407823886455 | validation: 1.028609689629982]
	TIME [epoch: 8.52 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4446940276812537		[learning rate: 0.0043827]
	Learning Rate: 0.00438265
	LOSS [training: 1.4446940276812537 | validation: 1.4430195003921398]
	TIME [epoch: 8.5 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.590504936400725		[learning rate: 0.0043667]
	Learning Rate: 0.00436675
	LOSS [training: 1.590504936400725 | validation: 1.0402987930685035]
	TIME [epoch: 8.49 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3428187117390165		[learning rate: 0.0043509]
	Learning Rate: 0.0043509
	LOSS [training: 1.3428187117390165 | validation: 1.0211854904412383]
	TIME [epoch: 8.52 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.396960928448294		[learning rate: 0.0043351]
	Learning Rate: 0.00433511
	LOSS [training: 1.396960928448294 | validation: 0.9723782535332224]
	TIME [epoch: 8.5 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3223354877608269		[learning rate: 0.0043194]
	Learning Rate: 0.00431938
	LOSS [training: 1.3223354877608269 | validation: 1.0368532399708916]
	TIME [epoch: 8.5 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5101094057051665		[learning rate: 0.0043037]
	Learning Rate: 0.0043037
	LOSS [training: 1.5101094057051665 | validation: 0.9966532817463118]
	TIME [epoch: 8.5 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.465386285647031		[learning rate: 0.0042881]
	Learning Rate: 0.00428808
	LOSS [training: 1.465386285647031 | validation: 0.8352895965993183]
	TIME [epoch: 8.51 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4843519453273664		[learning rate: 0.0042725]
	Learning Rate: 0.00427252
	LOSS [training: 1.4843519453273664 | validation: 1.4729253908638273]
	TIME [epoch: 8.49 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4709396250583056		[learning rate: 0.004257]
	Learning Rate: 0.00425702
	LOSS [training: 1.4709396250583056 | validation: 0.9206102880887941]
	TIME [epoch: 8.5 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5242722355955007		[learning rate: 0.0042416]
	Learning Rate: 0.00424157
	LOSS [training: 1.5242722355955007 | validation: 0.9551207072726037]
	TIME [epoch: 8.51 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4667038132015229		[learning rate: 0.0042262]
	Learning Rate: 0.00422617
	LOSS [training: 1.4667038132015229 | validation: 1.0093055625595033]
	TIME [epoch: 8.5 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4875380165880976		[learning rate: 0.0042108]
	Learning Rate: 0.00421084
	LOSS [training: 1.4875380165880976 | validation: 0.8643039903844674]
	TIME [epoch: 8.5 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3657301324704811		[learning rate: 0.0041956]
	Learning Rate: 0.00419556
	LOSS [training: 1.3657301324704811 | validation: 1.2787210720244393]
	TIME [epoch: 8.5 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.61235555331741		[learning rate: 0.0041803]
	Learning Rate: 0.00418033
	LOSS [training: 1.61235555331741 | validation: 0.9218270807531019]
	TIME [epoch: 8.52 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4382260998688634		[learning rate: 0.0041652]
	Learning Rate: 0.00416516
	LOSS [training: 1.4382260998688634 | validation: 0.8527700251627508]
	TIME [epoch: 8.49 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3504302430165738		[learning rate: 0.00415]
	Learning Rate: 0.00415004
	LOSS [training: 1.3504302430165738 | validation: 1.178539945655979]
	TIME [epoch: 8.48 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3517463271301458		[learning rate: 0.004135]
	Learning Rate: 0.00413498
	LOSS [training: 1.3517463271301458 | validation: 1.1189950158476116]
	TIME [epoch: 8.51 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3297846663333213		[learning rate: 0.00412]
	Learning Rate: 0.00411998
	LOSS [training: 1.3297846663333213 | validation: 0.8865982193963124]
	TIME [epoch: 8.5 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3819659832666353		[learning rate: 0.004105]
	Learning Rate: 0.00410502
	LOSS [training: 1.3819659832666353 | validation: 1.022987712249021]
	TIME [epoch: 8.5 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4099299804773735		[learning rate: 0.0040901]
	Learning Rate: 0.00409013
	LOSS [training: 1.4099299804773735 | validation: 1.0314723941151198]
	TIME [epoch: 8.5 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5077432846101861		[learning rate: 0.0040753]
	Learning Rate: 0.00407528
	LOSS [training: 1.5077432846101861 | validation: 1.1255954337897447]
	TIME [epoch: 8.52 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3759266371861387		[learning rate: 0.0040605]
	Learning Rate: 0.00406049
	LOSS [training: 1.3759266371861387 | validation: 0.9604083950301803]
	TIME [epoch: 8.49 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3491624336098726		[learning rate: 0.0040458]
	Learning Rate: 0.00404576
	LOSS [training: 1.3491624336098726 | validation: 0.9262091491922922]
	TIME [epoch: 8.5 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4003293105908905		[learning rate: 0.0040311]
	Learning Rate: 0.00403108
	LOSS [training: 1.4003293105908905 | validation: 0.88187646377596]
	TIME [epoch: 8.51 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.364693578625746		[learning rate: 0.0040164]
	Learning Rate: 0.00401645
	LOSS [training: 1.364693578625746 | validation: 0.9322623984954699]
	TIME [epoch: 8.51 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5207344989457319		[learning rate: 0.0040019]
	Learning Rate: 0.00400187
	LOSS [training: 1.5207344989457319 | validation: 0.9045414886361961]
	TIME [epoch: 8.5 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.439851831240303		[learning rate: 0.0039873]
	Learning Rate: 0.00398735
	LOSS [training: 1.439851831240303 | validation: 0.8918315667978483]
	TIME [epoch: 8.49 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3066628301526353		[learning rate: 0.0039729]
	Learning Rate: 0.00397288
	LOSS [training: 1.3066628301526353 | validation: 0.8426733536090429]
	TIME [epoch: 8.52 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3237393726847144		[learning rate: 0.0039585]
	Learning Rate: 0.00395846
	LOSS [training: 1.3237393726847144 | validation: 1.0268024526505357]
	TIME [epoch: 8.5 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4508518205916419		[learning rate: 0.0039441]
	Learning Rate: 0.00394409
	LOSS [training: 1.4508518205916419 | validation: 1.3988305746234637]
	TIME [epoch: 8.5 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7536188214150936		[learning rate: 0.0039298]
	Learning Rate: 0.00392978
	LOSS [training: 1.7536188214150936 | validation: 0.9412961284870945]
	TIME [epoch: 8.52 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3760803538875437		[learning rate: 0.0039155]
	Learning Rate: 0.00391552
	LOSS [training: 1.3760803538875437 | validation: 1.145921308335399]
	TIME [epoch: 8.51 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5144058141980818		[learning rate: 0.0039013]
	Learning Rate: 0.00390131
	LOSS [training: 1.5144058141980818 | validation: 1.179677788074123]
	TIME [epoch: 8.5 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3833877054198453		[learning rate: 0.0038872]
	Learning Rate: 0.00388715
	LOSS [training: 1.3833877054198453 | validation: 1.3077345469962012]
	TIME [epoch: 8.5 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4170168249400894		[learning rate: 0.003873]
	Learning Rate: 0.00387305
	LOSS [training: 1.4170168249400894 | validation: 0.8855885167722123]
	TIME [epoch: 8.53 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4752546491332732		[learning rate: 0.003859]
	Learning Rate: 0.00385899
	LOSS [training: 1.4752546491332732 | validation: 0.8420260617277853]
	TIME [epoch: 8.51 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3710607533455539		[learning rate: 0.003845]
	Learning Rate: 0.00384499
	LOSS [training: 1.3710607533455539 | validation: 0.8354068384529862]
	TIME [epoch: 8.51 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3570493813313125		[learning rate: 0.003831]
	Learning Rate: 0.00383103
	LOSS [training: 1.3570493813313125 | validation: 1.05051066329299]
	TIME [epoch: 8.52 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4478454009860005		[learning rate: 0.0038171]
	Learning Rate: 0.00381713
	LOSS [training: 1.4478454009860005 | validation: 1.3534804483246539]
	TIME [epoch: 8.51 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5017695195838512		[learning rate: 0.0038033]
	Learning Rate: 0.00380328
	LOSS [training: 1.5017695195838512 | validation: 0.9484974032107556]
	TIME [epoch: 8.5 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3787041761265875		[learning rate: 0.0037895]
	Learning Rate: 0.00378947
	LOSS [training: 1.3787041761265875 | validation: 1.6057748409669408]
	TIME [epoch: 8.5 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6104680504186994		[learning rate: 0.0037757]
	Learning Rate: 0.00377572
	LOSS [training: 1.6104680504186994 | validation: 1.1726136824065148]
	TIME [epoch: 8.52 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3160356546358758		[learning rate: 0.003762]
	Learning Rate: 0.00376202
	LOSS [training: 1.3160356546358758 | validation: 0.8974183040261692]
	TIME [epoch: 8.5 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3798548381874713		[learning rate: 0.0037484]
	Learning Rate: 0.00374837
	LOSS [training: 1.3798548381874713 | validation: 1.0326315085303355]
	TIME [epoch: 8.5 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3657031496244514		[learning rate: 0.0037348]
	Learning Rate: 0.00373476
	LOSS [training: 1.3657031496244514 | validation: 1.0217838707050622]
	TIME [epoch: 8.5 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2699858071962606		[learning rate: 0.0037212]
	Learning Rate: 0.00372121
	LOSS [training: 1.2699858071962606 | validation: 0.8774276461101482]
	TIME [epoch: 8.52 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2936483128031682		[learning rate: 0.0037077]
	Learning Rate: 0.00370771
	LOSS [training: 1.2936483128031682 | validation: 0.9636968665195773]
	TIME [epoch: 8.5 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3654115382612855		[learning rate: 0.0036943]
	Learning Rate: 0.00369425
	LOSS [training: 1.3654115382612855 | validation: 0.8854917857698886]
	TIME [epoch: 8.5 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.444353082992939		[learning rate: 0.0036808]
	Learning Rate: 0.00368084
	LOSS [training: 1.444353082992939 | validation: 1.1148930997719753]
	TIME [epoch: 8.52 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.398691352362969		[learning rate: 0.0036675]
	Learning Rate: 0.00366749
	LOSS [training: 1.398691352362969 | validation: 1.0614602107314624]
	TIME [epoch: 8.5 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.410260132061117		[learning rate: 0.0036542]
	Learning Rate: 0.00365418
	LOSS [training: 1.410260132061117 | validation: 2.8369981243277627]
	TIME [epoch: 8.5 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6968009832965922		[learning rate: 0.0036409]
	Learning Rate: 0.00364092
	LOSS [training: 1.6968009832965922 | validation: 0.944065742997616]
	TIME [epoch: 8.5 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.341388096085527		[learning rate: 0.0036277]
	Learning Rate: 0.0036277
	LOSS [training: 1.341388096085527 | validation: 1.2544230631682884]
	TIME [epoch: 8.53 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4846404784230396		[learning rate: 0.0036145]
	Learning Rate: 0.00361454
	LOSS [training: 1.4846404784230396 | validation: 1.1712312618649667]
	TIME [epoch: 8.5 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5282845940767613		[learning rate: 0.0036014]
	Learning Rate: 0.00360142
	LOSS [training: 1.5282845940767613 | validation: 1.392381500196187]
	TIME [epoch: 8.51 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.415776028942369		[learning rate: 0.0035883]
	Learning Rate: 0.00358835
	LOSS [training: 1.415776028942369 | validation: 1.0063922507708472]
	TIME [epoch: 8.53 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3239635238322147		[learning rate: 0.0035753]
	Learning Rate: 0.00357533
	LOSS [training: 1.3239635238322147 | validation: 0.8426300461543533]
	TIME [epoch: 8.51 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.392813342295176		[learning rate: 0.0035624]
	Learning Rate: 0.00356235
	LOSS [training: 1.392813342295176 | validation: 0.8661879620347692]
	TIME [epoch: 8.51 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3224312617360672		[learning rate: 0.0035494]
	Learning Rate: 0.00354942
	LOSS [training: 1.3224312617360672 | validation: 1.153261053252539]
	TIME [epoch: 8.5 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4681100995510374		[learning rate: 0.0035365]
	Learning Rate: 0.00353654
	LOSS [training: 1.4681100995510374 | validation: 0.9051211643629795]
	TIME [epoch: 8.53 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3617776992681274		[learning rate: 0.0035237]
	Learning Rate: 0.00352371
	LOSS [training: 1.3617776992681274 | validation: 1.1561482860459544]
	TIME [epoch: 8.51 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2980830427906445		[learning rate: 0.0035109]
	Learning Rate: 0.00351092
	LOSS [training: 1.2980830427906445 | validation: 0.8963292423798034]
	TIME [epoch: 8.51 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.334561920370082		[learning rate: 0.0034982]
	Learning Rate: 0.00349818
	LOSS [training: 1.334561920370082 | validation: 0.9909107938437913]
	TIME [epoch: 8.52 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3601508823536255		[learning rate: 0.0034855]
	Learning Rate: 0.00348548
	LOSS [training: 1.3601508823536255 | validation: 1.2965404478337326]
	TIME [epoch: 8.51 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3968493851205661		[learning rate: 0.0034728]
	Learning Rate: 0.00347284
	LOSS [training: 1.3968493851205661 | validation: 0.8456761243486306]
	TIME [epoch: 8.51 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3319376224930124		[learning rate: 0.0034602]
	Learning Rate: 0.00346023
	LOSS [training: 1.3319376224930124 | validation: 0.8587919833855702]
	TIME [epoch: 8.5 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2586594598939613		[learning rate: 0.0034477]
	Learning Rate: 0.00344767
	LOSS [training: 1.2586594598939613 | validation: 0.938110272684098]
	TIME [epoch: 8.53 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4767956030826779		[learning rate: 0.0034352]
	Learning Rate: 0.00343516
	LOSS [training: 1.4767956030826779 | validation: 0.9570384021358304]
	TIME [epoch: 8.51 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.347621355835821		[learning rate: 0.0034227]
	Learning Rate: 0.0034227
	LOSS [training: 1.347621355835821 | validation: 0.9429979883717114]
	TIME [epoch: 8.51 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3133897479117065		[learning rate: 0.0034103]
	Learning Rate: 0.00341028
	LOSS [training: 1.3133897479117065 | validation: 0.9758173666464809]
	TIME [epoch: 8.53 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.363804896614053		[learning rate: 0.0033979]
	Learning Rate: 0.0033979
	LOSS [training: 1.363804896614053 | validation: 0.950188609323565]
	TIME [epoch: 8.52 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3398972743456972		[learning rate: 0.0033856]
	Learning Rate: 0.00338557
	LOSS [training: 1.3398972743456972 | validation: 0.8662696544598649]
	TIME [epoch: 8.5 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.368117516221909		[learning rate: 0.0033733]
	Learning Rate: 0.00337328
	LOSS [training: 1.368117516221909 | validation: 1.0681167404556218]
	TIME [epoch: 8.5 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5164474048796204		[learning rate: 0.003361]
	Learning Rate: 0.00336104
	LOSS [training: 1.5164474048796204 | validation: 0.9081392178205437]
	TIME [epoch: 8.53 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3291432341845701		[learning rate: 0.0033488]
	Learning Rate: 0.00334884
	LOSS [training: 1.3291432341845701 | validation: 1.0797588007342704]
	TIME [epoch: 8.51 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7921721882420534		[learning rate: 0.0033367]
	Learning Rate: 0.00333669
	LOSS [training: 1.7921721882420534 | validation: 0.8519779642323726]
	TIME [epoch: 8.51 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3260946451700208		[learning rate: 0.0033246]
	Learning Rate: 0.00332458
	LOSS [training: 1.3260946451700208 | validation: 1.0270912348717047]
	TIME [epoch: 8.53 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4654932776033576		[learning rate: 0.0033125]
	Learning Rate: 0.00331252
	LOSS [training: 1.4654932776033576 | validation: 1.869940013209589]
	TIME [epoch: 8.52 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3449886278649341		[learning rate: 0.0033005]
	Learning Rate: 0.00330049
	LOSS [training: 1.3449886278649341 | validation: 0.826061347093521]
	TIME [epoch: 8.5 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.385149037170955		[learning rate: 0.0032885]
	Learning Rate: 0.00328852
	LOSS [training: 1.385149037170955 | validation: 1.0880786377234961]
	TIME [epoch: 8.52 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2714784688155765		[learning rate: 0.0032766]
	Learning Rate: 0.00327658
	LOSS [training: 1.2714784688155765 | validation: 0.9980613218219992]
	TIME [epoch: 8.54 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.344101709296009		[learning rate: 0.0032647]
	Learning Rate: 0.00326469
	LOSS [training: 1.344101709296009 | validation: 0.9510494602437282]
	TIME [epoch: 8.52 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3036202936166439		[learning rate: 0.0032528]
	Learning Rate: 0.00325284
	LOSS [training: 1.3036202936166439 | validation: 0.8567407990114246]
	TIME [epoch: 8.52 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3515488311909571		[learning rate: 0.003241]
	Learning Rate: 0.00324104
	LOSS [training: 1.3515488311909571 | validation: 0.9018640326252011]
	TIME [epoch: 8.52 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2414240722826186		[learning rate: 0.0032293]
	Learning Rate: 0.00322928
	LOSS [training: 1.2414240722826186 | validation: 0.8314770584168]
	TIME [epoch: 8.52 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3350004312501758		[learning rate: 0.0032176]
	Learning Rate: 0.00321756
	LOSS [training: 1.3350004312501758 | validation: 0.9904038060228687]
	TIME [epoch: 8.51 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.366876563200036		[learning rate: 0.0032059]
	Learning Rate: 0.00320588
	LOSS [training: 1.366876563200036 | validation: 0.9627154841872047]
	TIME [epoch: 8.5 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3839680671982921		[learning rate: 0.0031942]
	Learning Rate: 0.00319425
	LOSS [training: 1.3839680671982921 | validation: 1.0743788317657645]
	TIME [epoch: 8.53 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2201344494095567		[learning rate: 0.0031827]
	Learning Rate: 0.00318265
	LOSS [training: 1.2201344494095567 | validation: 0.8759277838436679]
	TIME [epoch: 8.52 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2558091392653123		[learning rate: 0.0031711]
	Learning Rate: 0.0031711
	LOSS [training: 1.2558091392653123 | validation: 0.8718788166972293]
	TIME [epoch: 8.51 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2985867203050996		[learning rate: 0.0031596]
	Learning Rate: 0.0031596
	LOSS [training: 1.2985867203050996 | validation: 1.0444341228230116]
	TIME [epoch: 8.51 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.282250455916892		[learning rate: 0.0031481]
	Learning Rate: 0.00314813
	LOSS [training: 1.282250455916892 | validation: 0.8574189829993122]
	TIME [epoch: 8.52 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3264454428839052		[learning rate: 0.0031367]
	Learning Rate: 0.00313671
	LOSS [training: 1.3264454428839052 | validation: 0.9724612181347105]
	TIME [epoch: 8.51 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2871260111728018		[learning rate: 0.0031253]
	Learning Rate: 0.00312532
	LOSS [training: 1.2871260111728018 | validation: 1.3845576451718182]
	TIME [epoch: 8.51 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7660462654178275		[learning rate: 0.003114]
	Learning Rate: 0.00311398
	LOSS [training: 1.7660462654178275 | validation: 1.0019595233642677]
	TIME [epoch: 8.53 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3654598017759398		[learning rate: 0.0031027]
	Learning Rate: 0.00310268
	LOSS [training: 1.3654598017759398 | validation: 1.1938001376947696]
	TIME [epoch: 8.52 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2905171301663327		[learning rate: 0.0030914]
	Learning Rate: 0.00309142
	LOSS [training: 1.2905171301663327 | validation: 0.8225329319843451]
	TIME [epoch: 8.51 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.288461733651801		[learning rate: 0.0030802]
	Learning Rate: 0.0030802
	LOSS [training: 1.288461733651801 | validation: 0.8497819250142276]
	TIME [epoch: 8.52 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2706556093363317		[learning rate: 0.003069]
	Learning Rate: 0.00306902
	LOSS [training: 1.2706556093363317 | validation: 1.1825578636083975]
	TIME [epoch: 8.53 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4067250041546617		[learning rate: 0.0030579]
	Learning Rate: 0.00305788
	LOSS [training: 1.4067250041546617 | validation: 0.9941064463699845]
	TIME [epoch: 8.52 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.335171642769046		[learning rate: 0.0030468]
	Learning Rate: 0.00304679
	LOSS [training: 1.335171642769046 | validation: 0.968360415443297]
	TIME [epoch: 8.5 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2823179808707403		[learning rate: 0.0030357]
	Learning Rate: 0.00303573
	LOSS [training: 1.2823179808707403 | validation: 0.8378095414071209]
	TIME [epoch: 8.54 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2482613625307741		[learning rate: 0.0030247]
	Learning Rate: 0.00302471
	LOSS [training: 1.2482613625307741 | validation: 1.4058809983623908]
	TIME [epoch: 8.52 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2807082476263623		[learning rate: 0.0030137]
	Learning Rate: 0.00301374
	LOSS [training: 1.2807082476263623 | validation: 0.8586387849120778]
	TIME [epoch: 8.5 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3643224932639917		[learning rate: 0.0030028]
	Learning Rate: 0.0030028
	LOSS [training: 1.3643224932639917 | validation: 0.8289874406463278]
	TIME [epoch: 8.51 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3842862277357844		[learning rate: 0.0029919]
	Learning Rate: 0.0029919
	LOSS [training: 1.3842862277357844 | validation: 0.8449160107252003]
	TIME [epoch: 8.54 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8780948634491548		[learning rate: 0.002981]
	Learning Rate: 0.00298104
	LOSS [training: 1.8780948634491548 | validation: 1.3389971134850716]
	TIME [epoch: 8.52 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2707850829106049		[learning rate: 0.0029702]
	Learning Rate: 0.00297023
	LOSS [training: 1.2707850829106049 | validation: 1.0266057832990139]
	TIME [epoch: 8.51 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2614162549424497		[learning rate: 0.0029594]
	Learning Rate: 0.00295945
	LOSS [training: 1.2614162549424497 | validation: 0.9649254007425034]
	TIME [epoch: 8.53 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3442304549587407		[learning rate: 0.0029487]
	Learning Rate: 0.00294871
	LOSS [training: 1.3442304549587407 | validation: 0.9282073422977705]
	TIME [epoch: 8.51 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2908583124900974		[learning rate: 0.002938]
	Learning Rate: 0.00293801
	LOSS [training: 1.2908583124900974 | validation: 1.107224199751898]
	TIME [epoch: 8.52 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3698524038892466		[learning rate: 0.0029273]
	Learning Rate: 0.00292734
	LOSS [training: 1.3698524038892466 | validation: 0.9309100599433624]
	TIME [epoch: 8.5 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2328811890132776		[learning rate: 0.0029167]
	Learning Rate: 0.00291672
	LOSS [training: 1.2328811890132776 | validation: 0.8410684064388062]
	TIME [epoch: 8.53 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2431184983324997		[learning rate: 0.0029061]
	Learning Rate: 0.00290614
	LOSS [training: 1.2431184983324997 | validation: 0.8806223671799578]
	TIME [epoch: 8.51 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3953192031049515		[learning rate: 0.0028956]
	Learning Rate: 0.00289559
	LOSS [training: 1.3953192031049515 | validation: 1.0095939974114274]
	TIME [epoch: 8.51 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2698876622146722		[learning rate: 0.0028851]
	Learning Rate: 0.00288508
	LOSS [training: 1.2698876622146722 | validation: 0.8401672988216455]
	TIME [epoch: 8.52 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.272150796614657		[learning rate: 0.0028746]
	Learning Rate: 0.00287461
	LOSS [training: 1.272150796614657 | validation: 0.8488470845520037]
	TIME [epoch: 8.52 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.25673413555939		[learning rate: 0.0028642]
	Learning Rate: 0.00286418
	LOSS [training: 1.25673413555939 | validation: 1.0010529416117147]
	TIME [epoch: 8.51 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2309518949353628		[learning rate: 0.0028538]
	Learning Rate: 0.00285378
	LOSS [training: 1.2309518949353628 | validation: 0.8295695974926425]
	TIME [epoch: 8.52 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.183675553542124		[learning rate: 0.0028434]
	Learning Rate: 0.00284343
	LOSS [training: 1.183675553542124 | validation: 1.345896354792626]
	TIME [epoch: 8.53 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2917718161640324		[learning rate: 0.0028331]
	Learning Rate: 0.00283311
	LOSS [training: 1.2917718161640324 | validation: 1.0738967098055978]
	TIME [epoch: 8.51 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2718122409150228		[learning rate: 0.0028228]
	Learning Rate: 0.00282283
	LOSS [training: 1.2718122409150228 | validation: 1.099625567235868]
	TIME [epoch: 8.51 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.35318465692004		[learning rate: 0.0028126]
	Learning Rate: 0.00281258
	LOSS [training: 1.35318465692004 | validation: 0.8733114142905496]
	TIME [epoch: 8.53 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3114859154766283		[learning rate: 0.0028024]
	Learning Rate: 0.00280238
	LOSS [training: 1.3114859154766283 | validation: 0.8237485201188697]
	TIME [epoch: 8.52 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2334644292308279		[learning rate: 0.0027922]
	Learning Rate: 0.00279221
	LOSS [training: 1.2334644292308279 | validation: 0.9854554146893175]
	TIME [epoch: 8.51 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.342596280363494		[learning rate: 0.0027821]
	Learning Rate: 0.00278207
	LOSS [training: 1.342596280363494 | validation: 0.932659725498979]
	TIME [epoch: 8.51 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2223919499783953		[learning rate: 0.002772]
	Learning Rate: 0.00277198
	LOSS [training: 1.2223919499783953 | validation: 0.9303714718673957]
	TIME [epoch: 8.52 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2841013317873877		[learning rate: 0.0027619]
	Learning Rate: 0.00276192
	LOSS [training: 1.2841013317873877 | validation: 0.8691250417127995]
	TIME [epoch: 8.51 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2128693281812932		[learning rate: 0.0027519]
	Learning Rate: 0.00275189
	LOSS [training: 1.2128693281812932 | validation: 0.8756142254433452]
	TIME [epoch: 8.5 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2432048598849126		[learning rate: 0.0027419]
	Learning Rate: 0.00274191
	LOSS [training: 1.2432048598849126 | validation: 1.6235170305347513]
	TIME [epoch: 8.52 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3033725493328547		[learning rate: 0.002732]
	Learning Rate: 0.00273196
	LOSS [training: 1.3033725493328547 | validation: 1.351416007547164]
	TIME [epoch: 8.52 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2453905800056504		[learning rate: 0.002722]
	Learning Rate: 0.00272204
	LOSS [training: 1.2453905800056504 | validation: 0.8279251253178987]
	TIME [epoch: 8.49 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2657906969189403		[learning rate: 0.0027122]
	Learning Rate: 0.00271216
	LOSS [training: 1.2657906969189403 | validation: 0.9402993687365815]
	TIME [epoch: 8.5 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2296625865727795		[learning rate: 0.0027023]
	Learning Rate: 0.00270232
	LOSS [training: 1.2296625865727795 | validation: 0.8666332760300746]
	TIME [epoch: 8.53 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.268140589321883		[learning rate: 0.0026925]
	Learning Rate: 0.00269251
	LOSS [training: 1.268140589321883 | validation: 1.051072230271645]
	TIME [epoch: 8.52 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2338313019160068		[learning rate: 0.0026827]
	Learning Rate: 0.00268274
	LOSS [training: 1.2338313019160068 | validation: 0.8927905875390613]
	TIME [epoch: 8.51 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3035360762760473		[learning rate: 0.002673]
	Learning Rate: 0.00267301
	LOSS [training: 1.3035360762760473 | validation: 0.8519503732604508]
	TIME [epoch: 8.52 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1866389818880225		[learning rate: 0.0026633]
	Learning Rate: 0.00266331
	LOSS [training: 1.1866389818880225 | validation: 0.873422542796725]
	TIME [epoch: 8.53 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2287003631990632		[learning rate: 0.0026536]
	Learning Rate: 0.00265364
	LOSS [training: 1.2287003631990632 | validation: 1.0909574459753868]
	TIME [epoch: 8.51 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2905401808243362		[learning rate: 0.002644]
	Learning Rate: 0.00264401
	LOSS [training: 1.2905401808243362 | validation: 1.0276491632770788]
	TIME [epoch: 8.49 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.237607811114495		[learning rate: 0.0026344]
	Learning Rate: 0.00263441
	LOSS [training: 1.237607811114495 | validation: 0.9268542335914202]
	TIME [epoch: 8.52 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2559308702733973		[learning rate: 0.0026249]
	Learning Rate: 0.00262485
	LOSS [training: 1.2559308702733973 | validation: 1.327615811881719]
	TIME [epoch: 8.52 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2876417376458886		[learning rate: 0.0026153]
	Learning Rate: 0.00261533
	LOSS [training: 1.2876417376458886 | validation: 0.9372517169555564]
	TIME [epoch: 8.51 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.435986505375697		[learning rate: 0.0026058]
	Learning Rate: 0.00260584
	LOSS [training: 1.435986505375697 | validation: 1.1254709231621096]
	TIME [epoch: 8.51 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.272645665402266		[learning rate: 0.0025964]
	Learning Rate: 0.00259638
	LOSS [training: 1.272645665402266 | validation: 0.984107021376365]
	TIME [epoch: 8.53 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2451629298799727		[learning rate: 0.002587]
	Learning Rate: 0.00258696
	LOSS [training: 1.2451629298799727 | validation: 0.8741064194822585]
	TIME [epoch: 8.5 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.29451388060807		[learning rate: 0.0025776]
	Learning Rate: 0.00257757
	LOSS [training: 1.29451388060807 | validation: 0.8303444628034777]
	TIME [epoch: 8.51 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2468106281362998		[learning rate: 0.0025682]
	Learning Rate: 0.00256822
	LOSS [training: 1.2468106281362998 | validation: 0.9153244787322911]
	TIME [epoch: 8.53 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2106872101862767		[learning rate: 0.0025589]
	Learning Rate: 0.0025589
	LOSS [training: 1.2106872101862767 | validation: 0.8780103639450356]
	TIME [epoch: 8.51 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2012869512429727		[learning rate: 0.0025496]
	Learning Rate: 0.00254961
	LOSS [training: 1.2012869512429727 | validation: 0.8856364262257369]
	TIME [epoch: 8.51 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.197091088552052		[learning rate: 0.0025404]
	Learning Rate: 0.00254036
	LOSS [training: 1.197091088552052 | validation: 0.853312971989373]
	TIME [epoch: 8.51 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2159205107234339		[learning rate: 0.0025311]
	Learning Rate: 0.00253114
	LOSS [training: 1.2159205107234339 | validation: 0.8379620480184924]
	TIME [epoch: 8.52 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2266977007421271		[learning rate: 0.002522]
	Learning Rate: 0.00252195
	LOSS [training: 1.2266977007421271 | validation: 0.8216033399769628]
	TIME [epoch: 8.51 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1873869439484395		[learning rate: 0.0025128]
	Learning Rate: 0.0025128
	LOSS [training: 1.1873869439484395 | validation: 0.9050179964566161]
	TIME [epoch: 8.5 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.261080085217207		[learning rate: 0.0025037]
	Learning Rate: 0.00250368
	LOSS [training: 1.261080085217207 | validation: 1.0964414051909812]
	TIME [epoch: 8.53 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2151105126656134		[learning rate: 0.0024946]
	Learning Rate: 0.00249459
	LOSS [training: 1.2151105126656134 | validation: 0.8791064280190193]
	TIME [epoch: 8.51 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3047244326638148		[learning rate: 0.0024855]
	Learning Rate: 0.00248554
	LOSS [training: 1.3047244326638148 | validation: 0.9727990115564139]
	TIME [epoch: 8.51 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.254248299248738		[learning rate: 0.0024765]
	Learning Rate: 0.00247652
	LOSS [training: 1.254248299248738 | validation: 0.8785581284330065]
	TIME [epoch: 8.5 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2783762697384806		[learning rate: 0.0024675]
	Learning Rate: 0.00246753
	LOSS [training: 1.2783762697384806 | validation: 0.8993106252864128]
	TIME [epoch: 8.52 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2707756076601573		[learning rate: 0.0024586]
	Learning Rate: 0.00245858
	LOSS [training: 1.2707756076601573 | validation: 0.9068068340653797]
	TIME [epoch: 8.51 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.231566540661194		[learning rate: 0.0024497]
	Learning Rate: 0.00244966
	LOSS [training: 1.231566540661194 | validation: 0.8121970655413364]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_487.pth
	Model improved!!!
EPOCH 488/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2025107256134082		[learning rate: 0.0024408]
	Learning Rate: 0.00244077
	LOSS [training: 1.2025107256134082 | validation: 0.8154783005947457]
	TIME [epoch: 8.53 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1791217586567637		[learning rate: 0.0024319]
	Learning Rate: 0.00243191
	LOSS [training: 1.1791217586567637 | validation: 1.1210584467739892]
	TIME [epoch: 8.51 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2805041718132522		[learning rate: 0.0024231]
	Learning Rate: 0.00242308
	LOSS [training: 1.2805041718132522 | validation: 0.951874938052421]
	TIME [epoch: 8.5 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.269882203220284		[learning rate: 0.0024143]
	Learning Rate: 0.00241429
	LOSS [training: 1.269882203220284 | validation: 0.8260975253653271]
	TIME [epoch: 8.51 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1568997384474278		[learning rate: 0.0024055]
	Learning Rate: 0.00240553
	LOSS [training: 1.1568997384474278 | validation: 0.8474022310373074]
	TIME [epoch: 8.53 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1925271175453322		[learning rate: 0.0023968]
	Learning Rate: 0.0023968
	LOSS [training: 1.1925271175453322 | validation: 0.9504910517960472]
	TIME [epoch: 8.5 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2465273846703		[learning rate: 0.0023881]
	Learning Rate: 0.0023881
	LOSS [training: 1.2465273846703 | validation: 1.2956069699088544]
	TIME [epoch: 8.5 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3588966981750805		[learning rate: 0.0023794]
	Learning Rate: 0.00237943
	LOSS [training: 1.3588966981750805 | validation: 0.9323811889550977]
	TIME [epoch: 8.51 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2744192547864919		[learning rate: 0.0023708]
	Learning Rate: 0.0023708
	LOSS [training: 1.2744192547864919 | validation: 0.8647364842327905]
	TIME [epoch: 8.52 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2169752847291548		[learning rate: 0.0023622]
	Learning Rate: 0.0023622
	LOSS [training: 1.2169752847291548 | validation: 0.9448629370126103]
	TIME [epoch: 8.51 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2095154493489366		[learning rate: 0.0023536]
	Learning Rate: 0.00235362
	LOSS [training: 1.2095154493489366 | validation: 0.8173494076855432]
	TIME [epoch: 8.5 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2233985947920072		[learning rate: 0.0023451]
	Learning Rate: 0.00234508
	LOSS [training: 1.2233985947920072 | validation: 0.8268070308759025]
	TIME [epoch: 8.53 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2646116760213368		[learning rate: 0.0023366]
	Learning Rate: 0.00233657
	LOSS [training: 1.2646116760213368 | validation: 0.8737527604178696]
	TIME [epoch: 8.5 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2158521269782707		[learning rate: 0.0023281]
	Learning Rate: 0.00232809
	LOSS [training: 1.2158521269782707 | validation: 0.8866275240635714]
	TIME [epoch: 8.5 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2038806482411655		[learning rate: 0.0023196]
	Learning Rate: 0.00231964
	LOSS [training: 1.2038806482411655 | validation: 0.9867700066097342]
	TIME [epoch: 8.52 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.226239107329814		[learning rate: 0.0023112]
	Learning Rate: 0.00231122
	LOSS [training: 1.226239107329814 | validation: 0.9022851281568662]
	TIME [epoch: 8.52 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2759747394061123		[learning rate: 0.0023028]
	Learning Rate: 0.00230284
	LOSS [training: 1.2759747394061123 | validation: 1.1268365904000524]
	TIME [epoch: 8.5 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2876544443414053		[learning rate: 0.0022945]
	Learning Rate: 0.00229448
	LOSS [training: 1.2876544443414053 | validation: 0.88889607900067]
	TIME [epoch: 8.51 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2709567855766588		[learning rate: 0.0022862]
	Learning Rate: 0.00228615
	LOSS [training: 1.2709567855766588 | validation: 0.8536545329294252]
	TIME [epoch: 8.53 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1874583254093964		[learning rate: 0.0022779]
	Learning Rate: 0.00227786
	LOSS [training: 1.1874583254093964 | validation: 0.8320303731301503]
	TIME [epoch: 8.52 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2133545810912054		[learning rate: 0.0022696]
	Learning Rate: 0.00226959
	LOSS [training: 1.2133545810912054 | validation: 0.9435173073243902]
	TIME [epoch: 8.5 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2605414645083701		[learning rate: 0.0022614]
	Learning Rate: 0.00226135
	LOSS [training: 1.2605414645083701 | validation: 0.9712628987214416]
	TIME [epoch: 8.53 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2049319657737416		[learning rate: 0.0022531]
	Learning Rate: 0.00225315
	LOSS [training: 1.2049319657737416 | validation: 0.7996565270514514]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.289933649506237		[learning rate: 0.002245]
	Learning Rate: 0.00224497
	LOSS [training: 1.289933649506237 | validation: 0.896281418120728]
	TIME [epoch: 8.53 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3004338438514185		[learning rate: 0.0022368]
	Learning Rate: 0.00223682
	LOSS [training: 1.3004338438514185 | validation: 1.0708425104517258]
	TIME [epoch: 8.53 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2106997501102206		[learning rate: 0.0022287]
	Learning Rate: 0.00222871
	LOSS [training: 1.2106997501102206 | validation: 0.8392574666657876]
	TIME [epoch: 8.56 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1800349930147542		[learning rate: 0.0022206]
	Learning Rate: 0.00222062
	LOSS [training: 1.1800349930147542 | validation: 0.8532163052632081]
	TIME [epoch: 8.53 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.239273396454078		[learning rate: 0.0022126]
	Learning Rate: 0.00221256
	LOSS [training: 1.239273396454078 | validation: 0.8782126851927493]
	TIME [epoch: 8.53 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1695315805232176		[learning rate: 0.0022045]
	Learning Rate: 0.00220453
	LOSS [training: 1.1695315805232176 | validation: 0.8556668322610527]
	TIME [epoch: 8.54 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2220217373638234		[learning rate: 0.0021965]
	Learning Rate: 0.00219653
	LOSS [training: 1.2220217373638234 | validation: 0.8974177105227745]
	TIME [epoch: 8.54 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2165870940405983		[learning rate: 0.0021886]
	Learning Rate: 0.00218856
	LOSS [training: 1.2165870940405983 | validation: 0.829667941100234]
	TIME [epoch: 8.53 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.211463041323018		[learning rate: 0.0021806]
	Learning Rate: 0.00218061
	LOSS [training: 1.211463041323018 | validation: 0.8796175366752732]
	TIME [epoch: 8.53 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2057105360151434		[learning rate: 0.0021727]
	Learning Rate: 0.0021727
	LOSS [training: 1.2057105360151434 | validation: 1.1560069389095138]
	TIME [epoch: 8.55 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.238358025478213		[learning rate: 0.0021648]
	Learning Rate: 0.00216482
	LOSS [training: 1.238358025478213 | validation: 0.8846127435229729]
	TIME [epoch: 8.53 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.19111686570167		[learning rate: 0.002157]
	Learning Rate: 0.00215696
	LOSS [training: 1.19111686570167 | validation: 0.7945825011310407]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_522.pth
	Model improved!!!
EPOCH 523/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.267375947766515		[learning rate: 0.0021491]
	Learning Rate: 0.00214913
	LOSS [training: 1.267375947766515 | validation: 0.813740766017757]
	TIME [epoch: 8.55 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1926307822534745		[learning rate: 0.0021413]
	Learning Rate: 0.00214133
	LOSS [training: 1.1926307822534745 | validation: 0.8143876951659392]
	TIME [epoch: 8.54 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2244636271813836		[learning rate: 0.0021336]
	Learning Rate: 0.00213356
	LOSS [training: 1.2244636271813836 | validation: 1.0723653053907114]
	TIME [epoch: 8.53 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2149622559130504		[learning rate: 0.0021258]
	Learning Rate: 0.00212582
	LOSS [training: 1.2149622559130504 | validation: 0.9107051655917859]
	TIME [epoch: 8.53 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2291970876442464		[learning rate: 0.0021181]
	Learning Rate: 0.0021181
	LOSS [training: 1.2291970876442464 | validation: 0.8246754319170156]
	TIME [epoch: 8.55 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2169670439355014		[learning rate: 0.0021104]
	Learning Rate: 0.00211042
	LOSS [training: 1.2169670439355014 | validation: 0.9017758604599808]
	TIME [epoch: 8.54 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2325127535804656		[learning rate: 0.0021028]
	Learning Rate: 0.00210276
	LOSS [training: 1.2325127535804656 | validation: 0.9529905634896358]
	TIME [epoch: 8.53 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1981046849987655		[learning rate: 0.0020951]
	Learning Rate: 0.00209513
	LOSS [training: 1.1981046849987655 | validation: 0.8250126418401222]
	TIME [epoch: 8.55 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.189042249952754		[learning rate: 0.0020875]
	Learning Rate: 0.00208752
	LOSS [training: 1.189042249952754 | validation: 0.8401200843364117]
	TIME [epoch: 8.54 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2368504666098228		[learning rate: 0.0020799]
	Learning Rate: 0.00207995
	LOSS [training: 1.2368504666098228 | validation: 0.8232033217828741]
	TIME [epoch: 8.53 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.165430333702337		[learning rate: 0.0020724]
	Learning Rate: 0.0020724
	LOSS [training: 1.165430333702337 | validation: 1.0578662769667078]
	TIME [epoch: 8.53 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.200883004414603		[learning rate: 0.0020649]
	Learning Rate: 0.00206488
	LOSS [training: 1.200883004414603 | validation: 0.847214004506132]
	TIME [epoch: 8.56 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.178593434084793		[learning rate: 0.0020574]
	Learning Rate: 0.00205739
	LOSS [training: 1.178593434084793 | validation: 0.8030938046899776]
	TIME [epoch: 8.53 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.192210401815791		[learning rate: 0.0020499]
	Learning Rate: 0.00204992
	LOSS [training: 1.192210401815791 | validation: 0.8864825777853926]
	TIME [epoch: 8.53 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1704717315189443		[learning rate: 0.0020425]
	Learning Rate: 0.00204248
	LOSS [training: 1.1704717315189443 | validation: 0.8770553078062024]
	TIME [epoch: 8.54 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1621959854585504		[learning rate: 0.0020351]
	Learning Rate: 0.00203507
	LOSS [training: 1.1621959854585504 | validation: 0.9380636989264214]
	TIME [epoch: 8.55 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1538081490827148		[learning rate: 0.0020277]
	Learning Rate: 0.00202768
	LOSS [training: 1.1538081490827148 | validation: 0.8564798003636727]
	TIME [epoch: 8.53 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2327386351885083		[learning rate: 0.0020203]
	Learning Rate: 0.00202032
	LOSS [training: 1.2327386351885083 | validation: 1.3831435507558973]
	TIME [epoch: 8.53 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3195025262165097		[learning rate: 0.002013]
	Learning Rate: 0.00201299
	LOSS [training: 1.3195025262165097 | validation: 0.9872586509264294]
	TIME [epoch: 8.56 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4722579360224244		[learning rate: 0.0020057]
	Learning Rate: 0.00200569
	LOSS [training: 1.4722579360224244 | validation: 0.9098969354584395]
	TIME [epoch: 8.54 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1763902164728681		[learning rate: 0.0019984]
	Learning Rate: 0.00199841
	LOSS [training: 1.1763902164728681 | validation: 0.955719500171666]
	TIME [epoch: 8.54 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.142760489902416		[learning rate: 0.0019912]
	Learning Rate: 0.00199116
	LOSS [training: 1.142760489902416 | validation: 0.810476134594352]
	TIME [epoch: 8.54 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.19245401697547		[learning rate: 0.0019839]
	Learning Rate: 0.00198393
	LOSS [training: 1.19245401697547 | validation: 0.8152717155359364]
	TIME [epoch: 8.55 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1751201435245417		[learning rate: 0.0019767]
	Learning Rate: 0.00197673
	LOSS [training: 1.1751201435245417 | validation: 0.8051327360001629]
	TIME [epoch: 8.54 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.160303146159095		[learning rate: 0.0019696]
	Learning Rate: 0.00196956
	LOSS [training: 1.160303146159095 | validation: 0.8334066860061856]
	TIME [epoch: 8.53 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2186409026610139		[learning rate: 0.0019624]
	Learning Rate: 0.00196241
	LOSS [training: 1.2186409026610139 | validation: 0.7988582788456635]
	TIME [epoch: 8.56 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.17520802592845		[learning rate: 0.0019553]
	Learning Rate: 0.00195529
	LOSS [training: 1.17520802592845 | validation: 0.7812362719000646]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_549.pth
	Model improved!!!
EPOCH 550/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1461438025802573		[learning rate: 0.0019482]
	Learning Rate: 0.00194819
	LOSS [training: 1.1461438025802573 | validation: 0.8604486316680523]
	TIME [epoch: 8.53 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.182608624440002		[learning rate: 0.0019411]
	Learning Rate: 0.00194112
	LOSS [training: 1.182608624440002 | validation: 0.8654093094344998]
	TIME [epoch: 8.53 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2049006523630703		[learning rate: 0.0019341]
	Learning Rate: 0.00193408
	LOSS [training: 1.2049006523630703 | validation: 0.9397034584725887]
	TIME [epoch: 8.55 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2092640502252243		[learning rate: 0.0019271]
	Learning Rate: 0.00192706
	LOSS [training: 1.2092640502252243 | validation: 0.8107453610047348]
	TIME [epoch: 8.53 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1516149127828896		[learning rate: 0.0019201]
	Learning Rate: 0.00192006
	LOSS [training: 1.1516149127828896 | validation: 0.8691099023950635]
	TIME [epoch: 8.53 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2920869435044284		[learning rate: 0.0019131]
	Learning Rate: 0.0019131
	LOSS [training: 1.2920869435044284 | validation: 0.9587259535428192]
	TIME [epoch: 8.55 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2175173378552695		[learning rate: 0.0019062]
	Learning Rate: 0.00190615
	LOSS [training: 1.2175173378552695 | validation: 0.8288980925910856]
	TIME [epoch: 8.54 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.147428984770536		[learning rate: 0.0018992]
	Learning Rate: 0.00189924
	LOSS [training: 1.147428984770536 | validation: 1.052031220482128]
	TIME [epoch: 8.52 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5262300544137863		[learning rate: 0.0018923]
	Learning Rate: 0.00189234
	LOSS [training: 1.5262300544137863 | validation: 2.0191412259351633]
	TIME [epoch: 8.53 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3742512641396336		[learning rate: 0.0018855]
	Learning Rate: 0.00188548
	LOSS [training: 1.3742512641396336 | validation: 0.8532032141794613]
	TIME [epoch: 8.55 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.254033675246913		[learning rate: 0.0018786]
	Learning Rate: 0.00187863
	LOSS [training: 1.254033675246913 | validation: 0.8151487845122347]
	TIME [epoch: 8.52 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.145561099718194		[learning rate: 0.0018718]
	Learning Rate: 0.00187182
	LOSS [training: 1.145561099718194 | validation: 0.8387574271297199]
	TIME [epoch: 8.52 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1068043402469974		[learning rate: 0.001865]
	Learning Rate: 0.00186502
	LOSS [training: 1.1068043402469974 | validation: 0.8270507374779393]
	TIME [epoch: 8.54 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3287262603384484		[learning rate: 0.0018583]
	Learning Rate: 0.00185825
	LOSS [training: 1.3287262603384484 | validation: 0.8618734730626658]
	TIME [epoch: 8.53 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2026479950434967		[learning rate: 0.0018515]
	Learning Rate: 0.00185151
	LOSS [training: 1.2026479950434967 | validation: 0.8248701125827013]
	TIME [epoch: 8.52 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1751075538453903		[learning rate: 0.0018448]
	Learning Rate: 0.00184479
	LOSS [training: 1.1751075538453903 | validation: 0.9691684178106991]
	TIME [epoch: 8.52 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2343922374104088		[learning rate: 0.0018381]
	Learning Rate: 0.0018381
	LOSS [training: 1.2343922374104088 | validation: 0.971490860715183]
	TIME [epoch: 8.55 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1444250453688558		[learning rate: 0.0018314]
	Learning Rate: 0.00183143
	LOSS [training: 1.1444250453688558 | validation: 0.7913962328467256]
	TIME [epoch: 8.53 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1650672399061968		[learning rate: 0.0018248]
	Learning Rate: 0.00182478
	LOSS [training: 1.1650672399061968 | validation: 0.8307623758654411]
	TIME [epoch: 8.54 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.154185852436434		[learning rate: 0.0018182]
	Learning Rate: 0.00181816
	LOSS [training: 1.154185852436434 | validation: 0.905765755809807]
	TIME [epoch: 8.56 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1563651869585843		[learning rate: 0.0018116]
	Learning Rate: 0.00181156
	LOSS [training: 1.1563651869585843 | validation: 0.954238023740193]
	TIME [epoch: 8.54 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.231414344455065		[learning rate: 0.001805]
	Learning Rate: 0.00180499
	LOSS [training: 1.231414344455065 | validation: 0.8538246026110123]
	TIME [epoch: 8.52 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1572232007510415		[learning rate: 0.0017984]
	Learning Rate: 0.00179844
	LOSS [training: 1.1572232007510415 | validation: 0.8473558644242071]
	TIME [epoch: 8.53 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1555319840814882		[learning rate: 0.0017919]
	Learning Rate: 0.00179191
	LOSS [training: 1.1555319840814882 | validation: 0.8086340971624172]
	TIME [epoch: 8.55 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1572608248732104		[learning rate: 0.0017854]
	Learning Rate: 0.00178541
	LOSS [training: 1.1572608248732104 | validation: 0.8784912425808341]
	TIME [epoch: 8.52 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.459043323312263		[learning rate: 0.0017789]
	Learning Rate: 0.00177893
	LOSS [training: 1.459043323312263 | validation: 0.8545603147181686]
	TIME [epoch: 8.52 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2799222249752158		[learning rate: 0.0017725]
	Learning Rate: 0.00177247
	LOSS [training: 1.2799222249752158 | validation: 1.2274657263802466]
	TIME [epoch: 8.55 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.236648972723555		[learning rate: 0.001766]
	Learning Rate: 0.00176604
	LOSS [training: 1.236648972723555 | validation: 0.9589709290313988]
	TIME [epoch: 8.53 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2455427722849564		[learning rate: 0.0017596]
	Learning Rate: 0.00175963
	LOSS [training: 1.2455427722849564 | validation: 0.9226389460308343]
	TIME [epoch: 8.52 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1666277909745735		[learning rate: 0.0017532]
	Learning Rate: 0.00175324
	LOSS [training: 1.1666277909745735 | validation: 1.2365706290460756]
	TIME [epoch: 8.52 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5299454013686191		[learning rate: 0.0017469]
	Learning Rate: 0.00174688
	LOSS [training: 1.5299454013686191 | validation: 1.1767824090693564]
	TIME [epoch: 8.54 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2056611612570896		[learning rate: 0.0017405]
	Learning Rate: 0.00174054
	LOSS [training: 1.2056611612570896 | validation: 0.836409961204531]
	TIME [epoch: 8.52 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1307126027327032		[learning rate: 0.0017342]
	Learning Rate: 0.00173422
	LOSS [training: 1.1307126027327032 | validation: 0.9243254400279762]
	TIME [epoch: 8.52 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2672497982228064		[learning rate: 0.0017279]
	Learning Rate: 0.00172793
	LOSS [training: 1.2672497982228064 | validation: 0.9459443555440391]
	TIME [epoch: 8.53 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1991087098737025		[learning rate: 0.0017217]
	Learning Rate: 0.00172166
	LOSS [training: 1.1991087098737025 | validation: 0.8250084428658034]
	TIME [epoch: 8.53 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1535668649836899		[learning rate: 0.0017154]
	Learning Rate: 0.00171541
	LOSS [training: 1.1535668649836899 | validation: 0.9821806442425102]
	TIME [epoch: 8.52 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.156883983108801		[learning rate: 0.0017092]
	Learning Rate: 0.00170919
	LOSS [training: 1.156883983108801 | validation: 1.0892374099228883]
	TIME [epoch: 8.52 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1802918187568305		[learning rate: 0.001703]
	Learning Rate: 0.00170298
	LOSS [training: 1.1802918187568305 | validation: 0.8219533741697901]
	TIME [epoch: 8.54 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1467287712274454		[learning rate: 0.0016968]
	Learning Rate: 0.0016968
	LOSS [training: 1.1467287712274454 | validation: 0.8629571715911701]
	TIME [epoch: 8.52 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1756813971249653		[learning rate: 0.0016906]
	Learning Rate: 0.00169065
	LOSS [training: 1.1756813971249653 | validation: 0.8451120614211123]
	TIME [epoch: 8.52 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1853210842861914		[learning rate: 0.0016845]
	Learning Rate: 0.00168451
	LOSS [training: 1.1853210842861914 | validation: 0.8317745770265781]
	TIME [epoch: 8.53 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1415079210428116		[learning rate: 0.0016784]
	Learning Rate: 0.0016784
	LOSS [training: 1.1415079210428116 | validation: 0.8587242717971033]
	TIME [epoch: 8.52 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1199340371543898		[learning rate: 0.0016723]
	Learning Rate: 0.00167231
	LOSS [training: 1.1199340371543898 | validation: 0.9673788169246855]
	TIME [epoch: 8.52 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.261229926760205		[learning rate: 0.0016662]
	Learning Rate: 0.00166624
	LOSS [training: 1.261229926760205 | validation: 1.4098063491645918]
	TIME [epoch: 8.52 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2699875783706722		[learning rate: 0.0016602]
	Learning Rate: 0.00166019
	LOSS [training: 1.2699875783706722 | validation: 1.1514761126830388]
	TIME [epoch: 8.54 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1465363189141151		[learning rate: 0.0016542]
	Learning Rate: 0.00165417
	LOSS [training: 1.1465363189141151 | validation: 0.8516672124005454]
	TIME [epoch: 8.52 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1226645777886237		[learning rate: 0.0016482]
	Learning Rate: 0.00164816
	LOSS [training: 1.1226645777886237 | validation: 1.0346942622112443]
	TIME [epoch: 8.51 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.307646053017283		[learning rate: 0.0016422]
	Learning Rate: 0.00164218
	LOSS [training: 1.307646053017283 | validation: 0.8555161767806352]
	TIME [epoch: 8.53 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1670820158055522		[learning rate: 0.0016362]
	Learning Rate: 0.00163622
	LOSS [training: 1.1670820158055522 | validation: 1.814851125056026]
	TIME [epoch: 8.54 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.288916077285346		[learning rate: 0.0016303]
	Learning Rate: 0.00163028
	LOSS [training: 1.288916077285346 | validation: 0.8178694157221198]
	TIME [epoch: 8.51 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1240066782316913		[learning rate: 0.0016244]
	Learning Rate: 0.00162437
	LOSS [training: 1.1240066782316913 | validation: 0.9065011702467936]
	TIME [epoch: 8.52 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1696037117301616		[learning rate: 0.0016185]
	Learning Rate: 0.00161847
	LOSS [training: 1.1696037117301616 | validation: 0.9703484698181987]
	TIME [epoch: 8.54 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1733932986721434		[learning rate: 0.0016126]
	Learning Rate: 0.0016126
	LOSS [training: 1.1733932986721434 | validation: 0.8455796267104514]
	TIME [epoch: 8.52 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1311788027117164		[learning rate: 0.0016067]
	Learning Rate: 0.00160675
	LOSS [training: 1.1311788027117164 | validation: 0.879170717058484]
	TIME [epoch: 8.52 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1426151485304012		[learning rate: 0.0016009]
	Learning Rate: 0.00160092
	LOSS [training: 1.1426151485304012 | validation: 0.8571719840019284]
	TIME [epoch: 8.53 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1666454842460912		[learning rate: 0.0015951]
	Learning Rate: 0.00159511
	LOSS [training: 1.1666454842460912 | validation: 0.8948267324689408]
	TIME [epoch: 8.54 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1111904631375915		[learning rate: 0.0015893]
	Learning Rate: 0.00158932
	LOSS [training: 1.1111904631375915 | validation: 0.8016467941154306]
	TIME [epoch: 8.52 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1450885987507375		[learning rate: 0.0015835]
	Learning Rate: 0.00158355
	LOSS [training: 1.1450885987507375 | validation: 0.7905106520397942]
	TIME [epoch: 8.52 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1487338465779517		[learning rate: 0.0015778]
	Learning Rate: 0.0015778
	LOSS [training: 1.1487338465779517 | validation: 0.9708681864600558]
	TIME [epoch: 8.55 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1534163111056936		[learning rate: 0.0015721]
	Learning Rate: 0.00157208
	LOSS [training: 1.1534163111056936 | validation: 0.872777216624822]
	TIME [epoch: 8.52 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1615774718678722		[learning rate: 0.0015664]
	Learning Rate: 0.00156637
	LOSS [training: 1.1615774718678722 | validation: 0.8055844972794117]
	TIME [epoch: 8.52 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1753527201311251		[learning rate: 0.0015607]
	Learning Rate: 0.00156069
	LOSS [training: 1.1753527201311251 | validation: 0.918342664851368]
	TIME [epoch: 8.53 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.16273842554006		[learning rate: 0.001555]
	Learning Rate: 0.00155502
	LOSS [training: 1.16273842554006 | validation: 0.9777224400741192]
	TIME [epoch: 8.54 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1468557634065808		[learning rate: 0.0015494]
	Learning Rate: 0.00154938
	LOSS [training: 1.1468557634065808 | validation: 0.8513853263090214]
	TIME [epoch: 8.52 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1599888369906128		[learning rate: 0.0015438]
	Learning Rate: 0.00154376
	LOSS [training: 1.1599888369906128 | validation: 0.9162381943531792]
	TIME [epoch: 8.52 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1346470905760957		[learning rate: 0.0015382]
	Learning Rate: 0.00153815
	LOSS [training: 1.1346470905760957 | validation: 0.9694962204864778]
	TIME [epoch: 8.54 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2005878093921014		[learning rate: 0.0015326]
	Learning Rate: 0.00153257
	LOSS [training: 1.2005878093921014 | validation: 0.9248258905122962]
	TIME [epoch: 8.52 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.142263294830746		[learning rate: 0.001527]
	Learning Rate: 0.00152701
	LOSS [training: 1.142263294830746 | validation: 0.7967866590296657]
	TIME [epoch: 8.52 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1983192886737095		[learning rate: 0.0015215]
	Learning Rate: 0.00152147
	LOSS [training: 1.1983192886737095 | validation: 1.6471594131296416]
	TIME [epoch: 8.52 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2787621273802265		[learning rate: 0.0015159]
	Learning Rate: 0.00151595
	LOSS [training: 1.2787621273802265 | validation: 1.0099529555022095]
	TIME [epoch: 8.54 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1343114960355019		[learning rate: 0.0015104]
	Learning Rate: 0.00151045
	LOSS [training: 1.1343114960355019 | validation: 0.7981209624166358]
	TIME [epoch: 8.52 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2053495847275368		[learning rate: 0.001505]
	Learning Rate: 0.00150496
	LOSS [training: 1.2053495847275368 | validation: 0.9052098314895769]
	TIME [epoch: 8.52 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1153606514675423		[learning rate: 0.0014995]
	Learning Rate: 0.0014995
	LOSS [training: 1.1153606514675423 | validation: 0.8690149428392577]
	TIME [epoch: 8.54 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1307074495299712		[learning rate: 0.0014941]
	Learning Rate: 0.00149406
	LOSS [training: 1.1307074495299712 | validation: 0.8279250314013566]
	TIME [epoch: 8.52 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.148851415572818		[learning rate: 0.0014886]
	Learning Rate: 0.00148864
	LOSS [training: 1.148851415572818 | validation: 0.981887103298055]
	TIME [epoch: 8.51 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1573957880451031		[learning rate: 0.0014832]
	Learning Rate: 0.00148324
	LOSS [training: 1.1573957880451031 | validation: 0.8817641450410768]
	TIME [epoch: 8.52 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0984688274843615		[learning rate: 0.0014779]
	Learning Rate: 0.00147785
	LOSS [training: 1.0984688274843615 | validation: 1.2308086953090838]
	TIME [epoch: 8.55 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2929977635242238		[learning rate: 0.0014725]
	Learning Rate: 0.00147249
	LOSS [training: 1.2929977635242238 | validation: 0.9268641703388277]
	TIME [epoch: 8.52 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0957408575140246		[learning rate: 0.0014671]
	Learning Rate: 0.00146715
	LOSS [training: 1.0957408575140246 | validation: 0.9023070102459205]
	TIME [epoch: 8.52 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1313047590802823		[learning rate: 0.0014618]
	Learning Rate: 0.00146182
	LOSS [training: 1.1313047590802823 | validation: 0.8036204752360292]
	TIME [epoch: 8.54 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1773686383735866		[learning rate: 0.0014565]
	Learning Rate: 0.00145652
	LOSS [training: 1.1773686383735866 | validation: 0.8133289630871168]
	TIME [epoch: 8.52 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1440785417806638		[learning rate: 0.0014512]
	Learning Rate: 0.00145123
	LOSS [training: 1.1440785417806638 | validation: 0.8930640324336602]
	TIME [epoch: 8.52 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0856012894595353		[learning rate: 0.001446]
	Learning Rate: 0.00144597
	LOSS [training: 1.0856012894595353 | validation: 0.8105218212859132]
	TIME [epoch: 8.52 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4115238020511693		[learning rate: 0.0014407]
	Learning Rate: 0.00144072
	LOSS [training: 1.4115238020511693 | validation: 0.9918099677934797]
	TIME [epoch: 8.54 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2182470157519385		[learning rate: 0.0014355]
	Learning Rate: 0.00143549
	LOSS [training: 1.2182470157519385 | validation: 1.0444365886590867]
	TIME [epoch: 8.52 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1730302210914123		[learning rate: 0.0014303]
	Learning Rate: 0.00143028
	LOSS [training: 1.1730302210914123 | validation: 0.8511662955846196]
	TIME [epoch: 8.51 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1215390591722196		[learning rate: 0.0014251]
	Learning Rate: 0.00142509
	LOSS [training: 1.1215390591722196 | validation: 0.8553013747704661]
	TIME [epoch: 8.53 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.260619928032948		[learning rate: 0.0014199]
	Learning Rate: 0.00141992
	LOSS [training: 1.260619928032948 | validation: 0.8399178372645321]
	TIME [epoch: 8.53 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1123939994700616		[learning rate: 0.0014148]
	Learning Rate: 0.00141476
	LOSS [training: 1.1123939994700616 | validation: 0.903784112926816]
	TIME [epoch: 8.51 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.198245068990317		[learning rate: 0.0014096]
	Learning Rate: 0.00140963
	LOSS [training: 1.198245068990317 | validation: 0.8452669489694604]
	TIME [epoch: 8.53 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1616313415648938		[learning rate: 0.0014045]
	Learning Rate: 0.00140451
	LOSS [training: 1.1616313415648938 | validation: 1.0267228219503526]
	TIME [epoch: 8.54 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1369787863233793		[learning rate: 0.0013994]
	Learning Rate: 0.00139942
	LOSS [training: 1.1369787863233793 | validation: 0.809983068811771]
	TIME [epoch: 8.52 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1478971561685027		[learning rate: 0.0013943]
	Learning Rate: 0.00139434
	LOSS [training: 1.1478971561685027 | validation: 0.8497715722522954]
	TIME [epoch: 8.51 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1327359135798343		[learning rate: 0.0013893]
	Learning Rate: 0.00138928
	LOSS [training: 1.1327359135798343 | validation: 0.9263483714064196]
	TIME [epoch: 8.53 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1415418309137435		[learning rate: 0.0013842]
	Learning Rate: 0.00138424
	LOSS [training: 1.1415418309137435 | validation: 0.81992619784062]
	TIME [epoch: 8.53 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.089039666908314		[learning rate: 0.0013792]
	Learning Rate: 0.00137921
	LOSS [training: 1.089039666908314 | validation: 0.8722423936192569]
	TIME [epoch: 8.52 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0844437353487157		[learning rate: 0.0013742]
	Learning Rate: 0.00137421
	LOSS [training: 1.0844437353487157 | validation: 0.7906231993069559]
	TIME [epoch: 8.52 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1091553344004337		[learning rate: 0.0013692]
	Learning Rate: 0.00136922
	LOSS [training: 1.1091553344004337 | validation: 0.838218688560871]
	TIME [epoch: 8.55 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1632697048456284		[learning rate: 0.0013643]
	Learning Rate: 0.00136425
	LOSS [training: 1.1632697048456284 | validation: 0.8515951986118447]
	TIME [epoch: 8.52 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.241073326765577		[learning rate: 0.0013593]
	Learning Rate: 0.0013593
	LOSS [training: 1.241073326765577 | validation: 0.8594107846867476]
	TIME [epoch: 8.52 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.210966406134569		[learning rate: 0.0013544]
	Learning Rate: 0.00135437
	LOSS [training: 1.210966406134569 | validation: 0.9398482185535881]
	TIME [epoch: 8.53 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.117856973326751		[learning rate: 0.0013495]
	Learning Rate: 0.00134945
	LOSS [training: 1.117856973326751 | validation: 0.762274258103548]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_651.pth
	Model improved!!!
EPOCH 652/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.156737293783161		[learning rate: 0.0013446]
	Learning Rate: 0.00134456
	LOSS [training: 1.156737293783161 | validation: 0.9505609933825213]
	TIME [epoch: 8.53 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1357345091019453		[learning rate: 0.0013397]
	Learning Rate: 0.00133968
	LOSS [training: 1.1357345091019453 | validation: 0.8065753300271359]
	TIME [epoch: 8.53 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1692340849157639		[learning rate: 0.0013348]
	Learning Rate: 0.00133481
	LOSS [training: 1.1692340849157639 | validation: 0.838808604597452]
	TIME [epoch: 8.55 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1118571447905172		[learning rate: 0.00133]
	Learning Rate: 0.00132997
	LOSS [training: 1.1118571447905172 | validation: 0.878900087504921]
	TIME [epoch: 8.52 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1159630102099518		[learning rate: 0.0013251]
	Learning Rate: 0.00132514
	LOSS [training: 1.1159630102099518 | validation: 0.8131897575597486]
	TIME [epoch: 8.52 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1108885288782564		[learning rate: 0.0013203]
	Learning Rate: 0.00132034
	LOSS [training: 1.1108885288782564 | validation: 0.8703609715513032]
	TIME [epoch: 8.54 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1231775726784794		[learning rate: 0.0013155]
	Learning Rate: 0.00131554
	LOSS [training: 1.1231775726784794 | validation: 1.1083401317998733]
	TIME [epoch: 8.53 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1942593233894623		[learning rate: 0.0013108]
	Learning Rate: 0.00131077
	LOSS [training: 1.1942593233894623 | validation: 0.8285301419742057]
	TIME [epoch: 8.52 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1086769556666238		[learning rate: 0.001306]
	Learning Rate: 0.00130601
	LOSS [training: 1.1086769556666238 | validation: 0.8329671687644449]
	TIME [epoch: 8.52 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.081602405885838		[learning rate: 0.0013013]
	Learning Rate: 0.00130127
	LOSS [training: 1.081602405885838 | validation: 0.8356051389552366]
	TIME [epoch: 8.54 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1448911611568662		[learning rate: 0.0012966]
	Learning Rate: 0.00129655
	LOSS [training: 1.1448911611568662 | validation: 0.8208327265693072]
	TIME [epoch: 8.52 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.151649324427717		[learning rate: 0.0012918]
	Learning Rate: 0.00129185
	LOSS [training: 1.151649324427717 | validation: 0.8077115453219548]
	TIME [epoch: 8.52 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1349140421884836		[learning rate: 0.0012872]
	Learning Rate: 0.00128716
	LOSS [training: 1.1349140421884836 | validation: 0.826157827879147]
	TIME [epoch: 8.53 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0930175719756867		[learning rate: 0.0012825]
	Learning Rate: 0.00128249
	LOSS [training: 1.0930175719756867 | validation: 0.9466331194829656]
	TIME [epoch: 8.53 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0788723343613693		[learning rate: 0.0012778]
	Learning Rate: 0.00127783
	LOSS [training: 1.0788723343613693 | validation: 0.8136659290919526]
	TIME [epoch: 8.51 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.131635650456978		[learning rate: 0.0012732]
	Learning Rate: 0.00127319
	LOSS [training: 1.131635650456978 | validation: 1.176601684692657]
	TIME [epoch: 8.52 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2228412609222439		[learning rate: 0.0012686]
	Learning Rate: 0.00126857
	LOSS [training: 1.2228412609222439 | validation: 0.8125809798927734]
	TIME [epoch: 8.54 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0634490642158523		[learning rate: 0.001264]
	Learning Rate: 0.00126397
	LOSS [training: 1.0634490642158523 | validation: 0.8226664199796648]
	TIME [epoch: 8.52 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0767628044164514		[learning rate: 0.0012594]
	Learning Rate: 0.00125938
	LOSS [training: 1.0767628044164514 | validation: 0.9143248972439616]
	TIME [epoch: 8.52 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2288569499821151		[learning rate: 0.0012548]
	Learning Rate: 0.00125481
	LOSS [training: 1.2288569499821151 | validation: 0.8811881021016061]
	TIME [epoch: 8.53 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.25201941639788		[learning rate: 0.0012503]
	Learning Rate: 0.00125026
	LOSS [training: 1.25201941639788 | validation: 0.7919724234722]
	TIME [epoch: 8.54 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2799096532205079		[learning rate: 0.0012457]
	Learning Rate: 0.00124572
	LOSS [training: 1.2799096532205079 | validation: 1.3176223652705694]
	TIME [epoch: 8.51 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2275740466982334		[learning rate: 0.0012412]
	Learning Rate: 0.0012412
	LOSS [training: 1.2275740466982334 | validation: 0.7799745904762012]
	TIME [epoch: 8.52 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0951803166807923		[learning rate: 0.0012367]
	Learning Rate: 0.0012367
	LOSS [training: 1.0951803166807923 | validation: 0.8132395879219035]
	TIME [epoch: 8.54 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1175389175856139		[learning rate: 0.0012322]
	Learning Rate: 0.00123221
	LOSS [training: 1.1175389175856139 | validation: 0.7872497156665381]
	TIME [epoch: 8.52 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1145365740916549		[learning rate: 0.0012277]
	Learning Rate: 0.00122774
	LOSS [training: 1.1145365740916549 | validation: 0.7806898456078449]
	TIME [epoch: 8.51 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0968680365039651		[learning rate: 0.0012233]
	Learning Rate: 0.00122328
	LOSS [training: 1.0968680365039651 | validation: 0.8613244715289946]
	TIME [epoch: 8.52 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1336501434801394		[learning rate: 0.0012188]
	Learning Rate: 0.00121884
	LOSS [training: 1.1336501434801394 | validation: 0.8162446162714179]
	TIME [epoch: 8.53 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0998964591076736		[learning rate: 0.0012144]
	Learning Rate: 0.00121442
	LOSS [training: 1.0998964591076736 | validation: 0.9496257336475624]
	TIME [epoch: 8.51 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1163735268314066		[learning rate: 0.00121]
	Learning Rate: 0.00121001
	LOSS [training: 1.1163735268314066 | validation: 0.809399704517038]
	TIME [epoch: 8.51 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1046182650833805		[learning rate: 0.0012056]
	Learning Rate: 0.00120562
	LOSS [training: 1.1046182650833805 | validation: 0.8427795140735075]
	TIME [epoch: 8.54 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.113132504898929		[learning rate: 0.0012012]
	Learning Rate: 0.00120125
	LOSS [training: 1.113132504898929 | validation: 0.8049123500313276]
	TIME [epoch: 8.52 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1002003423263234		[learning rate: 0.0011969]
	Learning Rate: 0.00119689
	LOSS [training: 1.1002003423263234 | validation: 0.8878673440851396]
	TIME [epoch: 8.52 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2269758075795962		[learning rate: 0.0011925]
	Learning Rate: 0.00119254
	LOSS [training: 1.2269758075795962 | validation: 0.8415286207585488]
	TIME [epoch: 8.52 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0722793427339192		[learning rate: 0.0011882]
	Learning Rate: 0.00118821
	LOSS [training: 1.0722793427339192 | validation: 0.9521545621802986]
	TIME [epoch: 8.54 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1185789605412226		[learning rate: 0.0011839]
	Learning Rate: 0.0011839
	LOSS [training: 1.1185789605412226 | validation: 0.847888862402199]
	TIME [epoch: 8.52 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1757576613470504		[learning rate: 0.0011796]
	Learning Rate: 0.00117961
	LOSS [training: 1.1757576613470504 | validation: 1.0003624072609638]
	TIME [epoch: 8.51 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.10473697867539		[learning rate: 0.0011753]
	Learning Rate: 0.00117532
	LOSS [training: 1.10473697867539 | validation: 0.7893454281563033]
	TIME [epoch: 8.54 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1274454747887181		[learning rate: 0.0011711]
	Learning Rate: 0.00117106
	LOSS [training: 1.1274454747887181 | validation: 0.9234263428746385]
	TIME [epoch: 8.52 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1363120846392705		[learning rate: 0.0011668]
	Learning Rate: 0.00116681
	LOSS [training: 1.1363120846392705 | validation: 0.8823247521677988]
	TIME [epoch: 8.51 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.144107859593996		[learning rate: 0.0011626]
	Learning Rate: 0.00116258
	LOSS [training: 1.144107859593996 | validation: 0.8040888793842422]
	TIME [epoch: 8.51 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.146303609116508		[learning rate: 0.0011584]
	Learning Rate: 0.00115836
	LOSS [training: 1.146303609116508 | validation: 0.788756151274463]
	TIME [epoch: 8.53 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1243191240323485		[learning rate: 0.0011542]
	Learning Rate: 0.00115415
	LOSS [training: 1.1243191240323485 | validation: 0.7994449390379212]
	TIME [epoch: 8.52 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1031724540359371		[learning rate: 0.00115]
	Learning Rate: 0.00114996
	LOSS [training: 1.1031724540359371 | validation: 0.8117681434986369]
	TIME [epoch: 8.51 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0662157386814566		[learning rate: 0.0011458]
	Learning Rate: 0.00114579
	LOSS [training: 1.0662157386814566 | validation: 0.8177453600776832]
	TIME [epoch: 8.52 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.081605640344716		[learning rate: 0.0011416]
	Learning Rate: 0.00114163
	LOSS [training: 1.081605640344716 | validation: 0.8266181089531045]
	TIME [epoch: 8.52 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1119886895521043		[learning rate: 0.0011375]
	Learning Rate: 0.00113749
	LOSS [training: 1.1119886895521043 | validation: 0.8230869828457329]
	TIME [epoch: 8.51 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0941919593936786		[learning rate: 0.0011334]
	Learning Rate: 0.00113336
	LOSS [training: 1.0941919593936786 | validation: 0.8482580739489296]
	TIME [epoch: 8.51 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1302998070286683		[learning rate: 0.0011292]
	Learning Rate: 0.00112925
	LOSS [training: 1.1302998070286683 | validation: 0.783699705412236]
	TIME [epoch: 8.53 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0725394885692234		[learning rate: 0.0011252]
	Learning Rate: 0.00112515
	LOSS [training: 1.0725394885692234 | validation: 0.8298152499335987]
	TIME [epoch: 8.51 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0909352158756893		[learning rate: 0.0011211]
	Learning Rate: 0.00112107
	LOSS [training: 1.0909352158756893 | validation: 0.8067761290348606]
	TIME [epoch: 8.51 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1191914111708285		[learning rate: 0.001117]
	Learning Rate: 0.001117
	LOSS [training: 1.1191914111708285 | validation: 0.777087320464857]
	TIME [epoch: 8.52 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0676437005126385		[learning rate: 0.0011129]
	Learning Rate: 0.00111295
	LOSS [training: 1.0676437005126385 | validation: 0.7729719774534094]
	TIME [epoch: 8.52 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1078555242283965		[learning rate: 0.0011089]
	Learning Rate: 0.00110891
	LOSS [training: 1.1078555242283965 | validation: 0.8259570689930942]
	TIME [epoch: 8.52 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0780342145778394		[learning rate: 0.0011049]
	Learning Rate: 0.00110488
	LOSS [training: 1.0780342145778394 | validation: 0.8480290555330835]
	TIME [epoch: 8.51 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4537232859917852		[learning rate: 0.0011009]
	Learning Rate: 0.00110087
	LOSS [training: 1.4537232859917852 | validation: 0.846683183636613]
	TIME [epoch: 8.54 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1667247149430395		[learning rate: 0.0010969]
	Learning Rate: 0.00109688
	LOSS [training: 1.1667247149430395 | validation: 0.8365398959409519]
	TIME [epoch: 8.51 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1715264634148936		[learning rate: 0.0010929]
	Learning Rate: 0.0010929
	LOSS [training: 1.1715264634148936 | validation: 0.7658135237563976]
	TIME [epoch: 8.51 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0905998582473133		[learning rate: 0.0010889]
	Learning Rate: 0.00108893
	LOSS [training: 1.0905998582473133 | validation: 0.8060130590726083]
	TIME [epoch: 8.52 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0953423769845332		[learning rate: 0.001085]
	Learning Rate: 0.00108498
	LOSS [training: 1.0953423769845332 | validation: 0.7935774731133344]
	TIME [epoch: 8.52 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1401768818655953		[learning rate: 0.001081]
	Learning Rate: 0.00108104
	LOSS [training: 1.1401768818655953 | validation: 0.8133831634812114]
	TIME [epoch: 8.51 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1830294154459122		[learning rate: 0.0010771]
	Learning Rate: 0.00107712
	LOSS [training: 1.1830294154459122 | validation: 0.8354501500097153]
	TIME [epoch: 8.52 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.077243638202448		[learning rate: 0.0010732]
	Learning Rate: 0.00107321
	LOSS [training: 1.077243638202448 | validation: 0.811320241145083]
	TIME [epoch: 8.54 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0656784684567897		[learning rate: 0.0010693]
	Learning Rate: 0.00106931
	LOSS [training: 1.0656784684567897 | validation: 0.8534445987402824]
	TIME [epoch: 8.52 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0592819299139782		[learning rate: 0.0010654]
	Learning Rate: 0.00106543
	LOSS [training: 1.0592819299139782 | validation: 0.8407952148668297]
	TIME [epoch: 8.52 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1074863821175525		[learning rate: 0.0010616]
	Learning Rate: 0.00106157
	LOSS [training: 1.1074863821175525 | validation: 0.8076253509783374]
	TIME [epoch: 8.53 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.182719531971034		[learning rate: 0.0010577]
	Learning Rate: 0.00105771
	LOSS [training: 1.182719531971034 | validation: 0.8456755688780149]
	TIME [epoch: 8.53 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1563510949150693		[learning rate: 0.0010539]
	Learning Rate: 0.00105388
	LOSS [training: 1.1563510949150693 | validation: 0.8304763359070108]
	TIME [epoch: 8.52 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0817182876062126		[learning rate: 0.0010501]
	Learning Rate: 0.00105005
	LOSS [training: 1.0817182876062126 | validation: 0.7750261720828138]
	TIME [epoch: 8.52 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0646695340163994		[learning rate: 0.0010462]
	Learning Rate: 0.00104624
	LOSS [training: 1.0646695340163994 | validation: 0.8977711812737581]
	TIME [epoch: 8.54 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0709678049906055		[learning rate: 0.0010424]
	Learning Rate: 0.00104244
	LOSS [training: 1.0709678049906055 | validation: 0.8663411486352991]
	TIME [epoch: 8.52 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0484320579974358		[learning rate: 0.0010387]
	Learning Rate: 0.00103866
	LOSS [training: 1.0484320579974358 | validation: 0.8404377232855883]
	TIME [epoch: 8.52 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0981080630892315		[learning rate: 0.0010349]
	Learning Rate: 0.00103489
	LOSS [training: 1.0981080630892315 | validation: 0.8158446649239306]
	TIME [epoch: 8.52 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0549768496578333		[learning rate: 0.0010311]
	Learning Rate: 0.00103114
	LOSS [training: 1.0549768496578333 | validation: 0.7955182570980899]
	TIME [epoch: 8.54 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0698865779443478		[learning rate: 0.0010274]
	Learning Rate: 0.00102739
	LOSS [training: 1.0698865779443478 | validation: 0.9189713647636517]
	TIME [epoch: 8.52 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1206594244919956		[learning rate: 0.0010237]
	Learning Rate: 0.00102367
	LOSS [training: 1.1206594244919956 | validation: 0.8081164924175297]
	TIME [epoch: 8.51 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.062674461119231		[learning rate: 0.00102]
	Learning Rate: 0.00101995
	LOSS [training: 1.062674461119231 | validation: 0.7904634133363678]
	TIME [epoch: 8.54 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0341092942792536		[learning rate: 0.0010162]
	Learning Rate: 0.00101625
	LOSS [training: 1.0341092942792536 | validation: 0.8123855856351334]
	TIME [epoch: 8.52 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1236279118058274		[learning rate: 0.0010126]
	Learning Rate: 0.00101256
	LOSS [training: 1.1236279118058274 | validation: 0.923070600585287]
	TIME [epoch: 8.52 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1334480524758692		[learning rate: 0.0010089]
	Learning Rate: 0.00100889
	LOSS [training: 1.1334480524758692 | validation: 0.7902833912357544]
	TIME [epoch: 8.52 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1378419746408155		[learning rate: 0.0010052]
	Learning Rate: 0.00100522
	LOSS [training: 1.1378419746408155 | validation: 0.9739548381880108]
	TIME [epoch: 8.53 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1292081408593084		[learning rate: 0.0010016]
	Learning Rate: 0.00100158
	LOSS [training: 1.1292081408593084 | validation: 1.0556853427212085]
	TIME [epoch: 8.51 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0989278010484285		[learning rate: 0.00099794]
	Learning Rate: 0.000997942
	LOSS [training: 1.0989278010484285 | validation: 0.7674833797385789]
	TIME [epoch: 8.51 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0588293143306955		[learning rate: 0.00099432]
	Learning Rate: 0.00099432
	LOSS [training: 1.0588293143306955 | validation: 0.798638022605891]
	TIME [epoch: 8.54 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0231774884703317		[learning rate: 0.00099071]
	Learning Rate: 0.000990712
	LOSS [training: 1.0231774884703317 | validation: 0.8136404101294789]
	TIME [epoch: 8.52 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1109217345581204		[learning rate: 0.00098712]
	Learning Rate: 0.000987116
	LOSS [training: 1.1109217345581204 | validation: 0.8355139309595031]
	TIME [epoch: 8.52 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.208060380589376		[learning rate: 0.00098353]
	Learning Rate: 0.000983534
	LOSS [training: 1.208060380589376 | validation: 0.8651832589152249]
	TIME [epoch: 8.52 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.049481951454402		[learning rate: 0.00097996]
	Learning Rate: 0.000979965
	LOSS [training: 1.049481951454402 | validation: 0.8348109019502555]
	TIME [epoch: 8.54 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0514750417604601		[learning rate: 0.00097641]
	Learning Rate: 0.000976409
	LOSS [training: 1.0514750417604601 | validation: 0.8005393117664182]
	TIME [epoch: 8.52 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.064387832688806		[learning rate: 0.00097287]
	Learning Rate: 0.000972865
	LOSS [training: 1.064387832688806 | validation: 0.832018014479037]
	TIME [epoch: 8.52 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0741496705763045		[learning rate: 0.00096933]
	Learning Rate: 0.000969335
	LOSS [training: 1.0741496705763045 | validation: 0.8045139729203125]
	TIME [epoch: 8.53 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.071396158749197		[learning rate: 0.00096582]
	Learning Rate: 0.000965817
	LOSS [training: 1.071396158749197 | validation: 0.8115175939640333]
	TIME [epoch: 8.52 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.068068015453544		[learning rate: 0.00096231]
	Learning Rate: 0.000962312
	LOSS [training: 1.068068015453544 | validation: 1.0805923587232233]
	TIME [epoch: 8.51 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1053854611192908		[learning rate: 0.00095882]
	Learning Rate: 0.00095882
	LOSS [training: 1.1053854611192908 | validation: 0.8059603193030669]
	TIME [epoch: 8.52 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0543001339877436		[learning rate: 0.00095534]
	Learning Rate: 0.00095534
	LOSS [training: 1.0543001339877436 | validation: 0.7941165963612321]
	TIME [epoch: 8.54 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.062566185657536		[learning rate: 0.00095187]
	Learning Rate: 0.000951873
	LOSS [training: 1.062566185657536 | validation: 0.7953001421168944]
	TIME [epoch: 8.51 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0497364102800764		[learning rate: 0.00094842]
	Learning Rate: 0.000948419
	LOSS [training: 1.0497364102800764 | validation: 0.7824416286201016]
	TIME [epoch: 8.51 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.112733630228266		[learning rate: 0.00094498]
	Learning Rate: 0.000944977
	LOSS [training: 1.112733630228266 | validation: 0.8445174088645814]
	TIME [epoch: 8.58 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.051065949657412		[learning rate: 0.00094155]
	Learning Rate: 0.000941547
	LOSS [training: 1.051065949657412 | validation: 0.7780684218888215]
	TIME [epoch: 8.52 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.087915271242303		[learning rate: 0.00093813]
	Learning Rate: 0.00093813
	LOSS [training: 1.087915271242303 | validation: 0.7782042778611713]
	TIME [epoch: 8.52 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0558523117023548		[learning rate: 0.00093473]
	Learning Rate: 0.000934726
	LOSS [training: 1.0558523117023548 | validation: 0.7980544437023585]
	TIME [epoch: 8.52 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0608682506938414		[learning rate: 0.00093133]
	Learning Rate: 0.000931334
	LOSS [training: 1.0608682506938414 | validation: 1.1788587275563716]
	TIME [epoch: 8.53 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1114462637508837		[learning rate: 0.00092795]
	Learning Rate: 0.000927954
	LOSS [training: 1.1114462637508837 | validation: 0.8209632784712277]
	TIME [epoch: 8.51 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0753903905776003		[learning rate: 0.00092459]
	Learning Rate: 0.000924586
	LOSS [training: 1.0753903905776003 | validation: 0.9347262021692232]
	TIME [epoch: 8.52 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2917154819661394		[learning rate: 0.00092123]
	Learning Rate: 0.000921231
	LOSS [training: 1.2917154819661394 | validation: 0.8717611315468248]
	TIME [epoch: 8.53 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1441311518800865		[learning rate: 0.00091789]
	Learning Rate: 0.000917888
	LOSS [training: 1.1441311518800865 | validation: 0.7740137926086454]
	TIME [epoch: 8.52 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0651205985828438		[learning rate: 0.00091456]
	Learning Rate: 0.000914556
	LOSS [training: 1.0651205985828438 | validation: 0.7797122940699253]
	TIME [epoch: 8.51 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.135162867059353		[learning rate: 0.00091124]
	Learning Rate: 0.000911237
	LOSS [training: 1.135162867059353 | validation: 0.9094746664720603]
	TIME [epoch: 8.51 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0918004961452632		[learning rate: 0.00090793]
	Learning Rate: 0.000907931
	LOSS [training: 1.0918004961452632 | validation: 0.78526541433288]
	TIME [epoch: 8.54 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1209313120874462		[learning rate: 0.00090464]
	Learning Rate: 0.000904636
	LOSS [training: 1.1209313120874462 | validation: 0.8086782486614346]
	TIME [epoch: 8.5 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0645250144062417		[learning rate: 0.00090135]
	Learning Rate: 0.000901353
	LOSS [training: 1.0645250144062417 | validation: 0.7739297703726705]
	TIME [epoch: 8.51 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0506815647756775		[learning rate: 0.00089808]
	Learning Rate: 0.000898082
	LOSS [training: 1.0506815647756775 | validation: 0.8829297185487871]
	TIME [epoch: 8.52 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0557196344912527		[learning rate: 0.00089482]
	Learning Rate: 0.000894822
	LOSS [training: 1.0557196344912527 | validation: 0.8263261348388035]
	TIME [epoch: 8.52 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0673771725583714		[learning rate: 0.00089157]
	Learning Rate: 0.000891575
	LOSS [training: 1.0673771725583714 | validation: 0.7696106477610174]
	TIME [epoch: 8.51 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0960067447244675		[learning rate: 0.00088834]
	Learning Rate: 0.000888339
	LOSS [training: 1.0960067447244675 | validation: 0.8071409865098235]
	TIME [epoch: 8.51 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.042638068867535		[learning rate: 0.00088512]
	Learning Rate: 0.000885116
	LOSS [training: 1.042638068867535 | validation: 0.824814366911355]
	TIME [epoch: 8.54 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0814095872542369		[learning rate: 0.0008819]
	Learning Rate: 0.000881903
	LOSS [training: 1.0814095872542369 | validation: 0.7898066454256029]
	TIME [epoch: 8.51 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0954095817436094		[learning rate: 0.0008787]
	Learning Rate: 0.000878703
	LOSS [training: 1.0954095817436094 | validation: 0.7993462232519027]
	TIME [epoch: 8.51 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0491097729900813		[learning rate: 0.00087551]
	Learning Rate: 0.000875514
	LOSS [training: 1.0491097729900813 | validation: 0.8948022072240245]
	TIME [epoch: 8.53 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1130869871222113		[learning rate: 0.00087234]
	Learning Rate: 0.000872337
	LOSS [training: 1.1130869871222113 | validation: 0.802700723107969]
	TIME [epoch: 8.52 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0739187502565757		[learning rate: 0.00086917]
	Learning Rate: 0.000869171
	LOSS [training: 1.0739187502565757 | validation: 0.8069124782804202]
	TIME [epoch: 8.51 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0695707124393448		[learning rate: 0.00086602]
	Learning Rate: 0.000866017
	LOSS [training: 1.0695707124393448 | validation: 0.802812598669548]
	TIME [epoch: 8.51 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0657376010430748		[learning rate: 0.00086287]
	Learning Rate: 0.000862874
	LOSS [training: 1.0657376010430748 | validation: 0.7887028279786411]
	TIME [epoch: 8.53 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0420596021902615		[learning rate: 0.00085974]
	Learning Rate: 0.000859743
	LOSS [training: 1.0420596021902615 | validation: 0.7813633605327222]
	TIME [epoch: 8.52 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0610798283412648		[learning rate: 0.00085662]
	Learning Rate: 0.000856623
	LOSS [training: 1.0610798283412648 | validation: 0.7937277277009975]
	TIME [epoch: 8.52 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0750293346436461		[learning rate: 0.00085351]
	Learning Rate: 0.000853514
	LOSS [training: 1.0750293346436461 | validation: 0.8286968109748385]
	TIME [epoch: 8.51 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.113501238917889		[learning rate: 0.00085042]
	Learning Rate: 0.000850416
	LOSS [training: 1.113501238917889 | validation: 0.8228555815146369]
	TIME [epoch: 8.52 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0687176327235375		[learning rate: 0.00084733]
	Learning Rate: 0.00084733
	LOSS [training: 1.0687176327235375 | validation: 0.8090997984157087]
	TIME [epoch: 8.5 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0555826169921378		[learning rate: 0.00084426]
	Learning Rate: 0.000844255
	LOSS [training: 1.0555826169921378 | validation: 0.8016932602912921]
	TIME [epoch: 8.5 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0909942501551284		[learning rate: 0.00084119]
	Learning Rate: 0.000841191
	LOSS [training: 1.0909942501551284 | validation: 0.7812222982478281]
	TIME [epoch: 8.52 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0663141786815231		[learning rate: 0.00083814]
	Learning Rate: 0.000838139
	LOSS [training: 1.0663141786815231 | validation: 0.7757234675740714]
	TIME [epoch: 8.52 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0721079344494584		[learning rate: 0.0008351]
	Learning Rate: 0.000835097
	LOSS [training: 1.0721079344494584 | validation: 0.809597212559096]
	TIME [epoch: 8.51 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0796811064630707		[learning rate: 0.00083207]
	Learning Rate: 0.000832066
	LOSS [training: 1.0796811064630707 | validation: 0.8199760345218368]
	TIME [epoch: 8.51 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2170795422774177		[learning rate: 0.00082905]
	Learning Rate: 0.000829046
	LOSS [training: 1.2170795422774177 | validation: 0.8509007674468123]
	TIME [epoch: 8.51 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0787607295970114		[learning rate: 0.00082604]
	Learning Rate: 0.000826038
	LOSS [training: 1.0787607295970114 | validation: 0.8130246411261576]
	TIME [epoch: 8.52 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0452976064348716		[learning rate: 0.00082304]
	Learning Rate: 0.00082304
	LOSS [training: 1.0452976064348716 | validation: 0.8379083488888553]
	TIME [epoch: 8.51 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0610577357510453		[learning rate: 0.00082005]
	Learning Rate: 0.000820053
	LOSS [training: 1.0610577357510453 | validation: 0.8070147051801394]
	TIME [epoch: 8.53 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0777042093972589		[learning rate: 0.00081708]
	Learning Rate: 0.000817077
	LOSS [training: 1.0777042093972589 | validation: 0.8743770332687745]
	TIME [epoch: 8.51 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0731232010032954		[learning rate: 0.00081411]
	Learning Rate: 0.000814112
	LOSS [training: 1.0731232010032954 | validation: 0.7921473296253154]
	TIME [epoch: 8.51 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0698316669837662		[learning rate: 0.00081116]
	Learning Rate: 0.000811158
	LOSS [training: 1.0698316669837662 | validation: 0.794737126592112]
	TIME [epoch: 8.5 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0507766405997914		[learning rate: 0.00080821]
	Learning Rate: 0.000808214
	LOSS [training: 1.0507766405997914 | validation: 0.8280480178534791]
	TIME [epoch: 8.53 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.06743126381007		[learning rate: 0.00080528]
	Learning Rate: 0.000805281
	LOSS [training: 1.06743126381007 | validation: 0.7936028695860303]
	TIME [epoch: 8.52 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0206413775497367		[learning rate: 0.00080236]
	Learning Rate: 0.000802358
	LOSS [training: 1.0206413775497367 | validation: 0.8470793146340592]
	TIME [epoch: 8.51 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0623950681137617		[learning rate: 0.00079945]
	Learning Rate: 0.000799447
	LOSS [training: 1.0623950681137617 | validation: 0.7937963327035319]
	TIME [epoch: 8.54 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0682126807376993		[learning rate: 0.00079655]
	Learning Rate: 0.000796545
	LOSS [training: 1.0682126807376993 | validation: 0.8311664189846008]
	TIME [epoch: 8.51 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.128783516069283		[learning rate: 0.00079365]
	Learning Rate: 0.000793655
	LOSS [training: 1.128783516069283 | validation: 0.792477348070319]
	TIME [epoch: 8.5 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0340972639039783		[learning rate: 0.00079077]
	Learning Rate: 0.000790775
	LOSS [training: 1.0340972639039783 | validation: 1.1954818909621936]
	TIME [epoch: 8.52 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.108190606530059		[learning rate: 0.0007879]
	Learning Rate: 0.000787905
	LOSS [training: 1.108190606530059 | validation: 0.7962035993758412]
	TIME [epoch: 8.53 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0102666641708415		[learning rate: 0.00078505]
	Learning Rate: 0.000785045
	LOSS [training: 1.0102666641708415 | validation: 0.8130475466290707]
	TIME [epoch: 8.51 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0629939189204731		[learning rate: 0.0007822]
	Learning Rate: 0.000782196
	LOSS [training: 1.0629939189204731 | validation: 0.7755040012704113]
	TIME [epoch: 8.5 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0365783247289655		[learning rate: 0.00077936]
	Learning Rate: 0.000779358
	LOSS [training: 1.0365783247289655 | validation: 0.8058927262655092]
	TIME [epoch: 8.51 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1466697906527585		[learning rate: 0.00077653]
	Learning Rate: 0.000776529
	LOSS [training: 1.1466697906527585 | validation: 1.6368576592147832]
	TIME [epoch: 8.51 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2995884545664738		[learning rate: 0.00077371]
	Learning Rate: 0.000773711
	LOSS [training: 1.2995884545664738 | validation: 0.7840922468661483]
	TIME [epoch: 8.51 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0964518725376546		[learning rate: 0.0007709]
	Learning Rate: 0.000770903
	LOSS [training: 1.0964518725376546 | validation: 0.7749352521126874]
	TIME [epoch: 8.52 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0604163665065476		[learning rate: 0.00076811]
	Learning Rate: 0.000768106
	LOSS [training: 1.0604163665065476 | validation: 0.7818008959312882]
	TIME [epoch: 8.53 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.07824486481499		[learning rate: 0.00076532]
	Learning Rate: 0.000765318
	LOSS [training: 1.07824486481499 | validation: 0.8138511690621182]
	TIME [epoch: 8.51 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.015246150838618		[learning rate: 0.00076254]
	Learning Rate: 0.000762541
	LOSS [training: 1.015246150838618 | validation: 0.7804389731444406]
	TIME [epoch: 8.5 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1182515668961133		[learning rate: 0.00075977]
	Learning Rate: 0.000759774
	LOSS [training: 1.1182515668961133 | validation: 0.8035018221739454]
	TIME [epoch: 8.52 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0334264143231195		[learning rate: 0.00075702]
	Learning Rate: 0.000757016
	LOSS [training: 1.0334264143231195 | validation: 0.8565274847804397]
	TIME [epoch: 8.52 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1043806170581383		[learning rate: 0.00075427]
	Learning Rate: 0.000754269
	LOSS [training: 1.1043806170581383 | validation: 0.8013885292527853]
	TIME [epoch: 8.51 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0691958988113526		[learning rate: 0.00075153]
	Learning Rate: 0.000751532
	LOSS [training: 1.0691958988113526 | validation: 0.7981091317704803]
	TIME [epoch: 8.51 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0441200386782588		[learning rate: 0.0007488]
	Learning Rate: 0.000748804
	LOSS [training: 1.0441200386782588 | validation: 1.005324480216283]
	TIME [epoch: 8.53 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.068831510281735		[learning rate: 0.00074609]
	Learning Rate: 0.000746087
	LOSS [training: 1.068831510281735 | validation: 0.9062533360574144]
	TIME [epoch: 8.51 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1027245032922308		[learning rate: 0.00074338]
	Learning Rate: 0.000743379
	LOSS [training: 1.1027245032922308 | validation: 0.8165668118273928]
	TIME [epoch: 8.51 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1108295557905488		[learning rate: 0.00074068]
	Learning Rate: 0.000740682
	LOSS [training: 1.1108295557905488 | validation: 0.7704343298188648]
	TIME [epoch: 8.52 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0278324327790795		[learning rate: 0.00073799]
	Learning Rate: 0.000737994
	LOSS [training: 1.0278324327790795 | validation: 0.7800733359266016]
	TIME [epoch: 8.51 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0704176792144502		[learning rate: 0.00073532]
	Learning Rate: 0.000735315
	LOSS [training: 1.0704176792144502 | validation: 0.8642080646341247]
	TIME [epoch: 8.5 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0509437084228368		[learning rate: 0.00073265]
	Learning Rate: 0.000732647
	LOSS [training: 1.0509437084228368 | validation: 0.9703094543787363]
	TIME [epoch: 8.51 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2571225375529635		[learning rate: 0.00072999]
	Learning Rate: 0.000729988
	LOSS [training: 1.2571225375529635 | validation: 0.8332702883339562]
	TIME [epoch: 8.52 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0526859340923698		[learning rate: 0.00072734]
	Learning Rate: 0.000727339
	LOSS [training: 1.0526859340923698 | validation: 0.7847319046546717]
	TIME [epoch: 8.51 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.093007851871107		[learning rate: 0.0007247]
	Learning Rate: 0.000724699
	LOSS [training: 1.093007851871107 | validation: 0.8547825232302072]
	TIME [epoch: 8.5 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1115596600643325		[learning rate: 0.00072207]
	Learning Rate: 0.000722069
	LOSS [training: 1.1115596600643325 | validation: 0.8263116540324118]
	TIME [epoch: 8.52 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.049212215219924		[learning rate: 0.00071945]
	Learning Rate: 0.000719449
	LOSS [training: 1.049212215219924 | validation: 0.8030966892425748]
	TIME [epoch: 8.51 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0583623492467817		[learning rate: 0.00071684]
	Learning Rate: 0.000716838
	LOSS [training: 1.0583623492467817 | validation: 0.8446171679372041]
	TIME [epoch: 8.5 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0480171423254483		[learning rate: 0.00071424]
	Learning Rate: 0.000714237
	LOSS [training: 1.0480171423254483 | validation: 0.7976277504327469]
	TIME [epoch: 8.5 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0411030045988734		[learning rate: 0.00071164]
	Learning Rate: 0.000711645
	LOSS [training: 1.0411030045988734 | validation: 0.855544975864139]
	TIME [epoch: 8.53 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.08445367443644		[learning rate: 0.00070906]
	Learning Rate: 0.000709062
	LOSS [training: 1.08445367443644 | validation: 0.8286707762576309]
	TIME [epoch: 8.51 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0896407049847803		[learning rate: 0.00070649]
	Learning Rate: 0.000706489
	LOSS [training: 1.0896407049847803 | validation: 0.772776416322807]
	TIME [epoch: 8.5 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1329680305918195		[learning rate: 0.00070392]
	Learning Rate: 0.000703925
	LOSS [training: 1.1329680305918195 | validation: 0.7973235460967439]
	TIME [epoch: 8.51 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0469391061397828		[learning rate: 0.00070137]
	Learning Rate: 0.00070137
	LOSS [training: 1.0469391061397828 | validation: 0.7850243037543665]
	TIME [epoch: 8.52 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0459706124737203		[learning rate: 0.00069883]
	Learning Rate: 0.000698825
	LOSS [training: 1.0459706124737203 | validation: 0.8178552254248921]
	TIME [epoch: 8.51 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0285287491854453		[learning rate: 0.00069629]
	Learning Rate: 0.000696289
	LOSS [training: 1.0285287491854453 | validation: 0.8041839521441385]
	TIME [epoch: 8.51 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.103330656495044		[learning rate: 0.00069376]
	Learning Rate: 0.000693762
	LOSS [training: 1.103330656495044 | validation: 1.076732126221268]
	TIME [epoch: 8.53 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.403448769363825		[learning rate: 0.00069124]
	Learning Rate: 0.000691244
	LOSS [training: 1.403448769363825 | validation: 1.0831845168414322]
	TIME [epoch: 8.51 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0821939968365386		[learning rate: 0.00068874]
	Learning Rate: 0.000688736
	LOSS [training: 1.0821939968365386 | validation: 0.8017498165831034]
	TIME [epoch: 8.5 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0396230358132161		[learning rate: 0.00068624]
	Learning Rate: 0.000686236
	LOSS [training: 1.0396230358132161 | validation: 0.7711127777563792]
	TIME [epoch: 8.51 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.037527590397527		[learning rate: 0.00068375]
	Learning Rate: 0.000683746
	LOSS [training: 1.037527590397527 | validation: 0.8036295824037293]
	TIME [epoch: 8.53 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1298868006362026		[learning rate: 0.00068126]
	Learning Rate: 0.000681265
	LOSS [training: 1.1298868006362026 | validation: 0.9164579892292127]
	TIME [epoch: 8.5 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1118939610275569		[learning rate: 0.00067879]
	Learning Rate: 0.000678792
	LOSS [training: 1.1118939610275569 | validation: 0.7805616560122909]
	TIME [epoch: 8.5 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0471293017746295		[learning rate: 0.00067633]
	Learning Rate: 0.000676329
	LOSS [training: 1.0471293017746295 | validation: 0.7625987007821783]
	TIME [epoch: 8.53 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0719938853320976		[learning rate: 0.00067387]
	Learning Rate: 0.000673874
	LOSS [training: 1.0719938853320976 | validation: 0.8371840142965422]
	TIME [epoch: 8.5 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0980347547467015		[learning rate: 0.00067143]
	Learning Rate: 0.000671429
	LOSS [training: 1.0980347547467015 | validation: 0.9249585831318843]
	TIME [epoch: 8.5 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.085989285431877		[learning rate: 0.00066899]
	Learning Rate: 0.000668992
	LOSS [training: 1.085989285431877 | validation: 0.793243560599463]
	TIME [epoch: 8.5 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1678949065436446		[learning rate: 0.00066656]
	Learning Rate: 0.000666564
	LOSS [training: 1.1678949065436446 | validation: 0.899947761657786]
	TIME [epoch: 8.53 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0987004065450885		[learning rate: 0.00066415]
	Learning Rate: 0.000664145
	LOSS [training: 1.0987004065450885 | validation: 0.8763622125330962]
	TIME [epoch: 8.5 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0974770234416735		[learning rate: 0.00066174]
	Learning Rate: 0.000661735
	LOSS [training: 1.0974770234416735 | validation: 0.7933635612785221]
	TIME [epoch: 8.51 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.05223527321775		[learning rate: 0.00065933]
	Learning Rate: 0.000659334
	LOSS [training: 1.05223527321775 | validation: 0.8129277668048115]
	TIME [epoch: 8.52 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.038097941447708		[learning rate: 0.00065694]
	Learning Rate: 0.000656941
	LOSS [training: 1.038097941447708 | validation: 0.7958003788000588]
	TIME [epoch: 8.51 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0398529464545505		[learning rate: 0.00065456]
	Learning Rate: 0.000654557
	LOSS [training: 1.0398529464545505 | validation: 0.8417441509827097]
	TIME [epoch: 8.5 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0789037981603675		[learning rate: 0.00065218]
	Learning Rate: 0.000652182
	LOSS [training: 1.0789037981603675 | validation: 0.8360555907669349]
	TIME [epoch: 8.5 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0668217681727277		[learning rate: 0.00064981]
	Learning Rate: 0.000649815
	LOSS [training: 1.0668217681727277 | validation: 0.7974892095459551]
	TIME [epoch: 8.52 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.038547780126959		[learning rate: 0.00064746]
	Learning Rate: 0.000647456
	LOSS [training: 1.038547780126959 | validation: 0.8032895304435026]
	TIME [epoch: 8.51 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0420174477577089		[learning rate: 0.00064511]
	Learning Rate: 0.000645107
	LOSS [training: 1.0420174477577089 | validation: 0.8392096833127738]
	TIME [epoch: 8.5 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0336013213045259		[learning rate: 0.00064277]
	Learning Rate: 0.000642766
	LOSS [training: 1.0336013213045259 | validation: 0.7847866007208297]
	TIME [epoch: 8.52 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0584182856879885		[learning rate: 0.00064043]
	Learning Rate: 0.000640433
	LOSS [training: 1.0584182856879885 | validation: 0.8041313035725428]
	TIME [epoch: 8.51 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0425912590226827		[learning rate: 0.00063811]
	Learning Rate: 0.000638109
	LOSS [training: 1.0425912590226827 | validation: 0.8422663572164744]
	TIME [epoch: 8.51 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.051280785285979		[learning rate: 0.00063579]
	Learning Rate: 0.000635793
	LOSS [training: 1.051280785285979 | validation: 0.787900604129899]
	TIME [epoch: 8.5 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1077687303308372		[learning rate: 0.00063349]
	Learning Rate: 0.000633486
	LOSS [training: 1.1077687303308372 | validation: 0.8210231004870732]
	TIME [epoch: 8.53 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0773421151108817		[learning rate: 0.00063119]
	Learning Rate: 0.000631187
	LOSS [training: 1.0773421151108817 | validation: 0.9617375006974398]
	TIME [epoch: 8.5 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.095364895333267		[learning rate: 0.0006289]
	Learning Rate: 0.000628896
	LOSS [training: 1.095364895333267 | validation: 0.7905283659667256]
	TIME [epoch: 8.51 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0705090442348082		[learning rate: 0.00062661]
	Learning Rate: 0.000626614
	LOSS [training: 1.0705090442348082 | validation: 0.7588789097030163]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_862.pth
	Model improved!!!
EPOCH 863/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0362878180552617		[learning rate: 0.00062434]
	Learning Rate: 0.00062434
	LOSS [training: 1.0362878180552617 | validation: 0.793888072744326]
	TIME [epoch: 8.52 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0398838262682548		[learning rate: 0.00062207]
	Learning Rate: 0.000622074
	LOSS [training: 1.0398838262682548 | validation: 0.8495100571349476]
	TIME [epoch: 8.5 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0683554953805765		[learning rate: 0.00061982]
	Learning Rate: 0.000619816
	LOSS [training: 1.0683554953805765 | validation: 0.7786234832907063]
	TIME [epoch: 8.5 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0304510738747013		[learning rate: 0.00061757]
	Learning Rate: 0.000617567
	LOSS [training: 1.0304510738747013 | validation: 0.7911506612768379]
	TIME [epoch: 8.52 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0403737705086997		[learning rate: 0.00061533]
	Learning Rate: 0.000615326
	LOSS [training: 1.0403737705086997 | validation: 0.7921490531451714]
	TIME [epoch: 8.51 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0270299868295694		[learning rate: 0.00061309]
	Learning Rate: 0.000613093
	LOSS [training: 1.0270299868295694 | validation: 0.7799749940794963]
	TIME [epoch: 8.5 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0646167727840607		[learning rate: 0.00061087]
	Learning Rate: 0.000610868
	LOSS [training: 1.0646167727840607 | validation: 0.777594957917809]
	TIME [epoch: 8.52 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.043807849349188		[learning rate: 0.00060865]
	Learning Rate: 0.000608651
	LOSS [training: 1.043807849349188 | validation: 0.855988467004572]
	TIME [epoch: 8.51 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0360582647168688		[learning rate: 0.00060644]
	Learning Rate: 0.000606442
	LOSS [training: 1.0360582647168688 | validation: 0.9132136176314242]
	TIME [epoch: 8.51 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0430795625315528		[learning rate: 0.00060424]
	Learning Rate: 0.000604242
	LOSS [training: 1.0430795625315528 | validation: 0.7784578277467714]
	TIME [epoch: 8.51 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0219150997357405		[learning rate: 0.00060205]
	Learning Rate: 0.000602049
	LOSS [training: 1.0219150997357405 | validation: 0.829525542095287]
	TIME [epoch: 8.53 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0409933400580214		[learning rate: 0.00059986]
	Learning Rate: 0.000599864
	LOSS [training: 1.0409933400580214 | validation: 1.1029770407170232]
	TIME [epoch: 8.5 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0759636019988066		[learning rate: 0.00059769]
	Learning Rate: 0.000597687
	LOSS [training: 1.0759636019988066 | validation: 0.829191736267407]
	TIME [epoch: 8.5 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0493898479380763		[learning rate: 0.00059552]
	Learning Rate: 0.000595518
	LOSS [training: 1.0493898479380763 | validation: 0.8255831865315094]
	TIME [epoch: 8.52 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0480699451693374		[learning rate: 0.00059336]
	Learning Rate: 0.000593357
	LOSS [training: 1.0480699451693374 | validation: 0.7848997837644667]
	TIME [epoch: 8.52 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0705744208426426		[learning rate: 0.0005912]
	Learning Rate: 0.000591203
	LOSS [training: 1.0705744208426426 | validation: 0.8801086418124746]
	TIME [epoch: 8.5 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.079708500836448		[learning rate: 0.00058906]
	Learning Rate: 0.000589058
	LOSS [training: 1.079708500836448 | validation: 0.7805347541567762]
	TIME [epoch: 8.51 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0438357466336226		[learning rate: 0.00058692]
	Learning Rate: 0.00058692
	LOSS [training: 1.0438357466336226 | validation: 1.0356269915696437]
	TIME [epoch: 8.52 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2297633783075543		[learning rate: 0.00058479]
	Learning Rate: 0.00058479
	LOSS [training: 1.2297633783075543 | validation: 0.8555926618247589]
	TIME [epoch: 8.51 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.041844354940555		[learning rate: 0.00058267]
	Learning Rate: 0.000582668
	LOSS [training: 1.041844354940555 | validation: 0.8186992888902792]
	TIME [epoch: 8.5 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0327066980327508		[learning rate: 0.00058055]
	Learning Rate: 0.000580553
	LOSS [training: 1.0327066980327508 | validation: 0.9291497740204979]
	TIME [epoch: 8.51 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0480315582114095		[learning rate: 0.00057845]
	Learning Rate: 0.000578446
	LOSS [training: 1.0480315582114095 | validation: 0.8015894571570605]
	TIME [epoch: 8.52 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0349908070721305		[learning rate: 0.00057635]
	Learning Rate: 0.000576347
	LOSS [training: 1.0349908070721305 | validation: 0.8046866235837636]
	TIME [epoch: 8.51 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0031907308311023		[learning rate: 0.00057426]
	Learning Rate: 0.000574256
	LOSS [training: 1.0031907308311023 | validation: 0.8065434772357954]
	TIME [epoch: 8.51 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0293706620915215		[learning rate: 0.00057217]
	Learning Rate: 0.000572172
	LOSS [training: 1.0293706620915215 | validation: 0.8628798374029807]
	TIME [epoch: 8.53 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.08925758472343		[learning rate: 0.0005701]
	Learning Rate: 0.000570095
	LOSS [training: 1.08925758472343 | validation: 0.9445955078118916]
	TIME [epoch: 8.51 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0749623683864011		[learning rate: 0.00056803]
	Learning Rate: 0.000568026
	LOSS [training: 1.0749623683864011 | validation: 0.7818952911290761]
	TIME [epoch: 8.5 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0109283801595645		[learning rate: 0.00056596]
	Learning Rate: 0.000565965
	LOSS [training: 1.0109283801595645 | validation: 0.8296987069656576]
	TIME [epoch: 8.51 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1072952636768545		[learning rate: 0.00056391]
	Learning Rate: 0.000563911
	LOSS [training: 1.1072952636768545 | validation: 0.7793930782285925]
	TIME [epoch: 8.52 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2404580726729821		[learning rate: 0.00056186]
	Learning Rate: 0.000561864
	LOSS [training: 1.2404580726729821 | validation: 0.9595842211500277]
	TIME [epoch: 8.51 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0825709583195535		[learning rate: 0.00055983]
	Learning Rate: 0.000559825
	LOSS [training: 1.0825709583195535 | validation: 0.8543549383160205]
	TIME [epoch: 8.5 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.115052270643828		[learning rate: 0.00055779]
	Learning Rate: 0.000557794
	LOSS [training: 1.115052270643828 | validation: 0.8787769475767018]
	TIME [epoch: 8.52 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0546031542250867		[learning rate: 0.00055577]
	Learning Rate: 0.00055577
	LOSS [training: 1.0546031542250867 | validation: 0.8192412950884627]
	TIME [epoch: 8.51 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0859506489465978		[learning rate: 0.00055375]
	Learning Rate: 0.000553753
	LOSS [training: 1.0859506489465978 | validation: 0.7910130910994213]
	TIME [epoch: 8.51 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0515906534391783		[learning rate: 0.00055174]
	Learning Rate: 0.000551743
	LOSS [training: 1.0515906534391783 | validation: 0.7628983468375132]
	TIME [epoch: 8.51 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9986950191134343		[learning rate: 0.00054974]
	Learning Rate: 0.000549741
	LOSS [training: 0.9986950191134343 | validation: 0.8054731487994344]
	TIME [epoch: 8.53 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0528004678353242		[learning rate: 0.00054775]
	Learning Rate: 0.000547746
	LOSS [training: 1.0528004678353242 | validation: 0.8171817665252008]
	TIME [epoch: 8.51 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0346781096905797		[learning rate: 0.00054576]
	Learning Rate: 0.000545758
	LOSS [training: 1.0346781096905797 | validation: 0.7978430267561656]
	TIME [epoch: 8.5 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.025384889068361		[learning rate: 0.00054378]
	Learning Rate: 0.000543777
	LOSS [training: 1.025384889068361 | validation: 0.7802763665483258]
	TIME [epoch: 8.52 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0458973757532977		[learning rate: 0.0005418]
	Learning Rate: 0.000541804
	LOSS [training: 1.0458973757532977 | validation: 0.7716961705994668]
	TIME [epoch: 8.51 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0122982823734246		[learning rate: 0.00053984]
	Learning Rate: 0.000539838
	LOSS [training: 1.0122982823734246 | validation: 0.8749816069366285]
	TIME [epoch: 8.49 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0216525235160625		[learning rate: 0.00053788]
	Learning Rate: 0.000537879
	LOSS [training: 1.0216525235160625 | validation: 0.8473356521015325]
	TIME [epoch: 8.5 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.065719467770331		[learning rate: 0.00053593]
	Learning Rate: 0.000535926
	LOSS [training: 1.065719467770331 | validation: 0.7724939676382315]
	TIME [epoch: 8.53 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0363978196545902		[learning rate: 0.00053398]
	Learning Rate: 0.000533982
	LOSS [training: 1.0363978196545902 | validation: 0.7874033342983585]
	TIME [epoch: 8.5 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0178784549305002		[learning rate: 0.00053204]
	Learning Rate: 0.000532044
	LOSS [training: 1.0178784549305002 | validation: 0.8336209000373199]
	TIME [epoch: 8.51 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0657038546432438		[learning rate: 0.00053011]
	Learning Rate: 0.000530113
	LOSS [training: 1.0657038546432438 | validation: 0.871745703151418]
	TIME [epoch: 8.52 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0696487604436338		[learning rate: 0.00052819]
	Learning Rate: 0.000528189
	LOSS [training: 1.0696487604436338 | validation: 0.7646803903341549]
	TIME [epoch: 8.52 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0624903925946192		[learning rate: 0.00052627]
	Learning Rate: 0.000526272
	LOSS [training: 1.0624903925946192 | validation: 0.7824820454982745]
	TIME [epoch: 8.51 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0436353626299035		[learning rate: 0.00052436]
	Learning Rate: 0.000524362
	LOSS [training: 1.0436353626299035 | validation: 0.8017358301742691]
	TIME [epoch: 8.51 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0848188783425061		[learning rate: 0.00052246]
	Learning Rate: 0.00052246
	LOSS [training: 1.0848188783425061 | validation: 0.796038282130425]
	TIME [epoch: 8.53 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0512135455157163		[learning rate: 0.00052056]
	Learning Rate: 0.000520563
	LOSS [training: 1.0512135455157163 | validation: 0.7759848049223959]
	TIME [epoch: 8.51 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0479189030158138		[learning rate: 0.00051867]
	Learning Rate: 0.000518674
	LOSS [training: 1.0479189030158138 | validation: 0.8075895175961902]
	TIME [epoch: 8.51 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0779277114888453		[learning rate: 0.00051679]
	Learning Rate: 0.000516792
	LOSS [training: 1.0779277114888453 | validation: 0.7739129357940254]
	TIME [epoch: 8.52 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0275945758717406		[learning rate: 0.00051492]
	Learning Rate: 0.000514917
	LOSS [training: 1.0275945758717406 | validation: 0.7752044488772762]
	TIME [epoch: 8.52 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0615492132868192		[learning rate: 0.00051305]
	Learning Rate: 0.000513048
	LOSS [training: 1.0615492132868192 | validation: 0.7957702817080635]
	TIME [epoch: 8.5 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0504076302950787		[learning rate: 0.00051119]
	Learning Rate: 0.000511186
	LOSS [training: 1.0504076302950787 | validation: 0.7758427317593763]
	TIME [epoch: 8.51 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0354862758663157		[learning rate: 0.00050933]
	Learning Rate: 0.000509331
	LOSS [training: 1.0354862758663157 | validation: 0.7961790902910445]
	TIME [epoch: 8.53 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0794972519017438		[learning rate: 0.00050748]
	Learning Rate: 0.000507483
	LOSS [training: 1.0794972519017438 | validation: 0.7990030586538264]
	TIME [epoch: 8.51 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0685865049978096		[learning rate: 0.00050564]
	Learning Rate: 0.000505641
	LOSS [training: 1.0685865049978096 | validation: 0.8108346057653303]
	TIME [epoch: 8.51 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.094157684023195		[learning rate: 0.00050381]
	Learning Rate: 0.000503806
	LOSS [training: 1.094157684023195 | validation: 0.7984296764000641]
	TIME [epoch: 8.52 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0534926200996027		[learning rate: 0.00050198]
	Learning Rate: 0.000501977
	LOSS [training: 1.0534926200996027 | validation: 0.7729051445305352]
	TIME [epoch: 8.51 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0976916770380611		[learning rate: 0.00050016]
	Learning Rate: 0.000500156
	LOSS [training: 1.0976916770380611 | validation: 0.7825164945168441]
	TIME [epoch: 8.5 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.058632307785662		[learning rate: 0.00049834]
	Learning Rate: 0.000498341
	LOSS [training: 1.058632307785662 | validation: 0.8206476145796046]
	TIME [epoch: 8.5 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.086831663408483		[learning rate: 0.00049653]
	Learning Rate: 0.000496532
	LOSS [training: 1.086831663408483 | validation: 0.8742355842881141]
	TIME [epoch: 8.52 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0572478436968384		[learning rate: 0.00049473]
	Learning Rate: 0.00049473
	LOSS [training: 1.0572478436968384 | validation: 0.7692964412926921]
	TIME [epoch: 8.5 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0544322042961145		[learning rate: 0.00049293]
	Learning Rate: 0.000492935
	LOSS [training: 1.0544322042961145 | validation: 0.7946587290169655]
	TIME [epoch: 8.49 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0343495678078445		[learning rate: 0.00049115]
	Learning Rate: 0.000491146
	LOSS [training: 1.0343495678078445 | validation: 0.8032407565835418]
	TIME [epoch: 8.51 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0410988544187003		[learning rate: 0.00048936]
	Learning Rate: 0.000489364
	LOSS [training: 1.0410988544187003 | validation: 0.8226131469662312]
	TIME [epoch: 8.51 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0385070536699121		[learning rate: 0.00048759]
	Learning Rate: 0.000487588
	LOSS [training: 1.0385070536699121 | validation: 0.7993897954873044]
	TIME [epoch: 8.5 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0596723607630802		[learning rate: 0.00048582]
	Learning Rate: 0.000485818
	LOSS [training: 1.0596723607630802 | validation: 0.790359557915456]
	TIME [epoch: 8.49 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0271636875458658		[learning rate: 0.00048406]
	Learning Rate: 0.000484055
	LOSS [training: 1.0271636875458658 | validation: 0.8399217357912767]
	TIME [epoch: 8.52 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0098362703007517		[learning rate: 0.0004823]
	Learning Rate: 0.000482298
	LOSS [training: 1.0098362703007517 | validation: 0.8456954840033499]
	TIME [epoch: 8.5 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0186683620740822		[learning rate: 0.00048055]
	Learning Rate: 0.000480548
	LOSS [training: 1.0186683620740822 | validation: 0.7849434507422086]
	TIME [epoch: 8.5 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0420647735597577		[learning rate: 0.0004788]
	Learning Rate: 0.000478804
	LOSS [training: 1.0420647735597577 | validation: 0.8033366425399747]
	TIME [epoch: 8.5 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0459501513896494		[learning rate: 0.00047707]
	Learning Rate: 0.000477067
	LOSS [training: 1.0459501513896494 | validation: 0.7968028048623638]
	TIME [epoch: 8.52 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0471023196193374		[learning rate: 0.00047534]
	Learning Rate: 0.000475335
	LOSS [training: 1.0471023196193374 | validation: 0.8031238733536017]
	TIME [epoch: 8.51 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0494139164027425		[learning rate: 0.00047361]
	Learning Rate: 0.00047361
	LOSS [training: 1.0494139164027425 | validation: 0.9175338419302228]
	TIME [epoch: 8.51 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0543152695910991		[learning rate: 0.00047189]
	Learning Rate: 0.000471891
	LOSS [training: 1.0543152695910991 | validation: 0.7739237098924596]
	TIME [epoch: 8.53 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0473623243035177		[learning rate: 0.00047018]
	Learning Rate: 0.000470179
	LOSS [training: 1.0473623243035177 | validation: 0.8154780650861481]
	TIME [epoch: 8.5 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0381996131377047		[learning rate: 0.00046847]
	Learning Rate: 0.000468473
	LOSS [training: 1.0381996131377047 | validation: 0.7850475786108315]
	TIME [epoch: 8.5 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0655639501116665		[learning rate: 0.00046677]
	Learning Rate: 0.000466773
	LOSS [training: 1.0655639501116665 | validation: 0.7928414462852773]
	TIME [epoch: 8.5 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0476320960092138		[learning rate: 0.00046508]
	Learning Rate: 0.000465079
	LOSS [training: 1.0476320960092138 | validation: 0.789627392682996]
	TIME [epoch: 8.52 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0584688931388484		[learning rate: 0.00046339]
	Learning Rate: 0.000463391
	LOSS [training: 1.0584688931388484 | validation: 0.7822498110765457]
	TIME [epoch: 8.5 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.03178013742284		[learning rate: 0.00046171]
	Learning Rate: 0.000461709
	LOSS [training: 1.03178013742284 | validation: 0.796426674527657]
	TIME [epoch: 8.51 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0468417509540136		[learning rate: 0.00046003]
	Learning Rate: 0.000460034
	LOSS [training: 1.0468417509540136 | validation: 0.7941573023773395]
	TIME [epoch: 8.5 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0197475196719785		[learning rate: 0.00045836]
	Learning Rate: 0.000458364
	LOSS [training: 1.0197475196719785 | validation: 0.7815441435785077]
	TIME [epoch: 8.5 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0189647516419071		[learning rate: 0.0004567]
	Learning Rate: 0.000456701
	LOSS [training: 1.0189647516419071 | validation: 0.7681970984319507]
	TIME [epoch: 8.49 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0448933280825012		[learning rate: 0.00045504]
	Learning Rate: 0.000455043
	LOSS [training: 1.0448933280825012 | validation: 0.7714697303387821]
	TIME [epoch: 8.49 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0501687740009185		[learning rate: 0.00045339]
	Learning Rate: 0.000453392
	LOSS [training: 1.0501687740009185 | validation: 0.7746124827359857]
	TIME [epoch: 8.52 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0291749197455746		[learning rate: 0.00045175]
	Learning Rate: 0.000451746
	LOSS [training: 1.0291749197455746 | validation: 0.8687027948712319]
	TIME [epoch: 8.49 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0673552675231366		[learning rate: 0.00045011]
	Learning Rate: 0.000450107
	LOSS [training: 1.0673552675231366 | validation: 0.8007455978876822]
	TIME [epoch: 8.49 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.039530029324597		[learning rate: 0.00044847]
	Learning Rate: 0.000448474
	LOSS [training: 1.039530029324597 | validation: 0.7920440977897617]
	TIME [epoch: 8.51 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0576083667124907		[learning rate: 0.00044685]
	Learning Rate: 0.000446846
	LOSS [training: 1.0576083667124907 | validation: 0.7944106010874945]
	TIME [epoch: 8.5 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0431005826900615		[learning rate: 0.00044522]
	Learning Rate: 0.000445224
	LOSS [training: 1.0431005826900615 | validation: 0.8160013077261852]
	TIME [epoch: 8.49 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0553605108702306		[learning rate: 0.00044361]
	Learning Rate: 0.000443609
	LOSS [training: 1.0553605108702306 | validation: 0.7850796392913617]
	TIME [epoch: 8.5 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0682987916379503		[learning rate: 0.000442]
	Learning Rate: 0.000441999
	LOSS [training: 1.0682987916379503 | validation: 0.7649981482103955]
	TIME [epoch: 8.52 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.062474679299752		[learning rate: 0.00044039]
	Learning Rate: 0.000440395
	LOSS [training: 1.062474679299752 | validation: 0.8569080551135891]
	TIME [epoch: 8.49 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0573423825058084		[learning rate: 0.0004388]
	Learning Rate: 0.000438797
	LOSS [training: 1.0573423825058084 | validation: 0.7774490411593385]
	TIME [epoch: 8.49 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0387894546692134		[learning rate: 0.0004372]
	Learning Rate: 0.000437204
	LOSS [training: 1.0387894546692134 | validation: 0.7796520742783648]
	TIME [epoch: 8.51 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0775501334710553		[learning rate: 0.00043562]
	Learning Rate: 0.000435617
	LOSS [training: 1.0775501334710553 | validation: 0.7798182734541397]
	TIME [epoch: 8.5 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0591062773331688		[learning rate: 0.00043404]
	Learning Rate: 0.000434037
	LOSS [training: 1.0591062773331688 | validation: 0.7817413499705648]
	TIME [epoch: 8.49 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0627369077783044		[learning rate: 0.00043246]
	Learning Rate: 0.000432461
	LOSS [training: 1.0627369077783044 | validation: 0.7750673235584069]
	TIME [epoch: 8.48 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0669288007658744		[learning rate: 0.00043089]
	Learning Rate: 0.000430892
	LOSS [training: 1.0669288007658744 | validation: 0.8081733786465419]
	TIME [epoch: 8.51 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.078061186920921		[learning rate: 0.00042933]
	Learning Rate: 0.000429328
	LOSS [training: 1.078061186920921 | validation: 0.7806460214928271]
	TIME [epoch: 8.49 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0286644785275132		[learning rate: 0.00042777]
	Learning Rate: 0.00042777
	LOSS [training: 1.0286644785275132 | validation: 0.8507600397456658]
	TIME [epoch: 8.5 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0624418281976293		[learning rate: 0.00042622]
	Learning Rate: 0.000426218
	LOSS [training: 1.0624418281976293 | validation: 0.8258854382044636]
	TIME [epoch: 8.52 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0999129964410328		[learning rate: 0.00042467]
	Learning Rate: 0.000424671
	LOSS [training: 1.0999129964410328 | validation: 0.7977764192556467]
	TIME [epoch: 8.51 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0716906389776502		[learning rate: 0.00042313]
	Learning Rate: 0.00042313
	LOSS [training: 1.0716906389776502 | validation: 0.7959358627604628]
	TIME [epoch: 8.5 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0670994492291206		[learning rate: 0.00042159]
	Learning Rate: 0.000421594
	LOSS [training: 1.0670994492291206 | validation: 0.7746159975496234]
	TIME [epoch: 8.49 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.058745691208473		[learning rate: 0.00042006]
	Learning Rate: 0.000420064
	LOSS [training: 1.058745691208473 | validation: 0.7707948482462471]
	TIME [epoch: 8.51 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0774477967273663		[learning rate: 0.00041854]
	Learning Rate: 0.00041854
	LOSS [training: 1.0774477967273663 | validation: 0.791497773416731]
	TIME [epoch: 8.5 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.036613324746211		[learning rate: 0.00041702]
	Learning Rate: 0.000417021
	LOSS [training: 1.036613324746211 | validation: 0.8004728173472224]
	TIME [epoch: 8.5 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0562057749671219		[learning rate: 0.00041551]
	Learning Rate: 0.000415508
	LOSS [training: 1.0562057749671219 | validation: 0.7725908558213517]
	TIME [epoch: 8.51 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0495726058027057		[learning rate: 0.000414]
	Learning Rate: 0.000414
	LOSS [training: 1.0495726058027057 | validation: 0.7786330834771191]
	TIME [epoch: 8.49 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.034382733158382		[learning rate: 0.0004125]
	Learning Rate: 0.000412497
	LOSS [training: 1.034382733158382 | validation: 0.8885546084523547]
	TIME [epoch: 8.48 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0958734233188039		[learning rate: 0.000411]
	Learning Rate: 0.000411
	LOSS [training: 1.0958734233188039 | validation: 0.7785427766604712]
	TIME [epoch: 8.48 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0492916887660086		[learning rate: 0.00040951]
	Learning Rate: 0.000409509
	LOSS [training: 1.0492916887660086 | validation: 0.7677535383894015]
	TIME [epoch: 8.52 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.028217285242648		[learning rate: 0.00040802]
	Learning Rate: 0.000408023
	LOSS [training: 1.028217285242648 | validation: 0.780375300345429]
	TIME [epoch: 8.5 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.051796602365616		[learning rate: 0.00040654]
	Learning Rate: 0.000406542
	LOSS [training: 1.051796602365616 | validation: 0.811517671524942]
	TIME [epoch: 8.5 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0835336739918389		[learning rate: 0.00040507]
	Learning Rate: 0.000405067
	LOSS [training: 1.0835336739918389 | validation: 0.822835060373291]
	TIME [epoch: 8.5 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0486832590942807		[learning rate: 0.0004036]
	Learning Rate: 0.000403597
	LOSS [training: 1.0486832590942807 | validation: 0.8181340992367586]
	TIME [epoch: 8.51 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.06428736764884		[learning rate: 0.00040213]
	Learning Rate: 0.000402132
	LOSS [training: 1.06428736764884 | validation: 0.7992350486348265]
	TIME [epoch: 8.5 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0438125499647177		[learning rate: 0.00040067]
	Learning Rate: 0.000400672
	LOSS [training: 1.0438125499647177 | validation: 0.7748255962194402]
	TIME [epoch: 8.5 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0511504604812811		[learning rate: 0.00039922]
	Learning Rate: 0.000399218
	LOSS [training: 1.0511504604812811 | validation: 0.774814596566813]
	TIME [epoch: 8.53 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0375511366693755		[learning rate: 0.00039777]
	Learning Rate: 0.00039777
	LOSS [training: 1.0375511366693755 | validation: 1.0305554813688695]
	TIME [epoch: 8.51 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1160418729939605		[learning rate: 0.00039633]
	Learning Rate: 0.000396326
	LOSS [training: 1.1160418729939605 | validation: 0.7951834125422975]
	TIME [epoch: 8.5 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0534356539321252		[learning rate: 0.00039489]
	Learning Rate: 0.000394888
	LOSS [training: 1.0534356539321252 | validation: 0.800872189131842]
	TIME [epoch: 8.5 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0429456900889806		[learning rate: 0.00039345]
	Learning Rate: 0.000393455
	LOSS [training: 1.0429456900889806 | validation: 0.829864829164916]
	TIME [epoch: 8.51 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0668422316374309		[learning rate: 0.00039203]
	Learning Rate: 0.000392027
	LOSS [training: 1.0668422316374309 | validation: 0.7776625566270898]
	TIME [epoch: 8.49 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0413645239704468		[learning rate: 0.0003906]
	Learning Rate: 0.000390604
	LOSS [training: 1.0413645239704468 | validation: 0.7773466796646631]
	TIME [epoch: 8.5 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0735429076324698		[learning rate: 0.00038919]
	Learning Rate: 0.000389187
	LOSS [training: 1.0735429076324698 | validation: 0.796924109848829]
	TIME [epoch: 8.51 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0760770437160836		[learning rate: 0.00038777]
	Learning Rate: 0.000387774
	LOSS [training: 1.0760770437160836 | validation: 1.0067191328753036]
	TIME [epoch: 8.49 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0787529554321347		[learning rate: 0.00038637]
	Learning Rate: 0.000386367
	LOSS [training: 1.0787529554321347 | validation: 0.84529376100434]
	TIME [epoch: 8.49 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.04801483857571		[learning rate: 0.00038496]
	Learning Rate: 0.000384965
	LOSS [training: 1.04801483857571 | validation: 0.7699636413549331]
	TIME [epoch: 8.49 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.03751881398007		[learning rate: 0.00038357]
	Learning Rate: 0.000383568
	LOSS [training: 1.03751881398007 | validation: 0.7783491128852287]
	TIME [epoch: 8.52 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0322825899895467		[learning rate: 0.00038218]
	Learning Rate: 0.000382176
	LOSS [training: 1.0322825899895467 | validation: 0.8035544437304025]
	TIME [epoch: 8.49 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0871088766429773		[learning rate: 0.00038079]
	Learning Rate: 0.000380789
	LOSS [training: 1.0871088766429773 | validation: 0.7883758621955239]
	TIME [epoch: 8.5 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0811750946052088		[learning rate: 0.00037941]
	Learning Rate: 0.000379407
	LOSS [training: 1.0811750946052088 | validation: 0.7867799795536187]
	TIME [epoch: 8.52 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0530979651286492		[learning rate: 0.00037803]
	Learning Rate: 0.00037803
	LOSS [training: 1.0530979651286492 | validation: 0.7871676456079517]
	TIME [epoch: 8.5 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0600596451415116		[learning rate: 0.00037666]
	Learning Rate: 0.000376658
	LOSS [training: 1.0600596451415116 | validation: 0.7837667679319928]
	TIME [epoch: 8.49 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0606815455380423		[learning rate: 0.00037529]
	Learning Rate: 0.000375291
	LOSS [training: 1.0606815455380423 | validation: 0.8148437527122772]
	TIME [epoch: 8.5 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0640350095632403		[learning rate: 0.00037393]
	Learning Rate: 0.000373929
	LOSS [training: 1.0640350095632403 | validation: 0.7995120189059106]
	TIME [epoch: 8.52 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0378346951051882		[learning rate: 0.00037257]
	Learning Rate: 0.000372572
	LOSS [training: 1.0378346951051882 | validation: 0.76759907227577]
	TIME [epoch: 8.5 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0345206784967043		[learning rate: 0.00037122]
	Learning Rate: 0.00037122
	LOSS [training: 1.0345206784967043 | validation: 0.7986657889812326]
	TIME [epoch: 8.5 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0400999705788516		[learning rate: 0.00036987]
	Learning Rate: 0.000369873
	LOSS [training: 1.0400999705788516 | validation: 0.7749922429510526]
	TIME [epoch: 8.51 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0428280622437853		[learning rate: 0.00036853]
	Learning Rate: 0.000368531
	LOSS [training: 1.0428280622437853 | validation: 0.7754830962202358]
	TIME [epoch: 8.51 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0278283084122692		[learning rate: 0.00036719]
	Learning Rate: 0.000367193
	LOSS [training: 1.0278283084122692 | validation: 0.7994868226861077]
	TIME [epoch: 8.5 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.052117429454333		[learning rate: 0.00036586]
	Learning Rate: 0.000365861
	LOSS [training: 1.052117429454333 | validation: 0.7792156309592471]
	TIME [epoch: 8.51 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0284146533621812		[learning rate: 0.00036453]
	Learning Rate: 0.000364533
	LOSS [training: 1.0284146533621812 | validation: 0.7946520390912489]
	TIME [epoch: 8.52 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.056761221639498		[learning rate: 0.00036321]
	Learning Rate: 0.00036321
	LOSS [training: 1.056761221639498 | validation: 0.7743071063275817]
	TIME [epoch: 8.5 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0476952189611701		[learning rate: 0.00036189]
	Learning Rate: 0.000361892
	LOSS [training: 1.0476952189611701 | validation: 0.7837490913800033]
	TIME [epoch: 8.5 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0416036564029267		[learning rate: 0.00036058]
	Learning Rate: 0.000360579
	LOSS [training: 1.0416036564029267 | validation: 0.780492782307405]
	TIME [epoch: 8.51 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.02246062811856		[learning rate: 0.00035927]
	Learning Rate: 0.00035927
	LOSS [training: 1.02246062811856 | validation: 0.7661649126113907]
	TIME [epoch: 8.51 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0512035116595821		[learning rate: 0.00035797]
	Learning Rate: 0.000357966
	LOSS [training: 1.0512035116595821 | validation: 0.7830222921232172]
	TIME [epoch: 8.5 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0266845035283958		[learning rate: 0.00035667]
	Learning Rate: 0.000356667
	LOSS [training: 1.0266845035283958 | validation: 0.7763015737253075]
	TIME [epoch: 8.5 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0110362205298564		[learning rate: 0.00035537]
	Learning Rate: 0.000355373
	LOSS [training: 1.0110362205298564 | validation: 0.7982308087507324]
	TIME [epoch: 8.52 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0364147857418637		[learning rate: 0.00035408]
	Learning Rate: 0.000354083
	LOSS [training: 1.0364147857418637 | validation: 0.779138526925109]
	TIME [epoch: 8.5 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0269689861588305		[learning rate: 0.0003528]
	Learning Rate: 0.000352798
	LOSS [training: 1.0269689861588305 | validation: 0.7766294895621152]
	TIME [epoch: 8.5 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0266564805516543		[learning rate: 0.00035152]
	Learning Rate: 0.000351518
	LOSS [training: 1.0266564805516543 | validation: 0.7819799084250373]
	TIME [epoch: 8.52 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0558191750780306		[learning rate: 0.00035024]
	Learning Rate: 0.000350242
	LOSS [training: 1.0558191750780306 | validation: 0.7746649413739908]
	TIME [epoch: 8.51 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.049660862708108		[learning rate: 0.00034897]
	Learning Rate: 0.000348971
	LOSS [training: 1.049660862708108 | validation: 0.7815425116905583]
	TIME [epoch: 8.49 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0317500723223296		[learning rate: 0.0003477]
	Learning Rate: 0.000347705
	LOSS [training: 1.0317500723223296 | validation: 0.7628913307196594]
	TIME [epoch: 8.5 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0462672199480345		[learning rate: 0.00034644]
	Learning Rate: 0.000346443
	LOSS [training: 1.0462672199480345 | validation: 0.7658592567219241]
	TIME [epoch: 8.52 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0270259574985712		[learning rate: 0.00034519]
	Learning Rate: 0.000345186
	LOSS [training: 1.0270259574985712 | validation: 0.7908752089107043]
	TIME [epoch: 8.5 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0285297568513054		[learning rate: 0.00034393]
	Learning Rate: 0.000343933
	LOSS [training: 1.0285297568513054 | validation: 0.7923154404328858]
	TIME [epoch: 8.5 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0231110244021522		[learning rate: 0.00034268]
	Learning Rate: 0.000342685
	LOSS [training: 1.0231110244021522 | validation: 0.7907982556123313]
	TIME [epoch: 8.5 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0124087165220357		[learning rate: 0.00034144]
	Learning Rate: 0.000341441
	LOSS [training: 1.0124087165220357 | validation: 0.7615464648343027]
	TIME [epoch: 8.52 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0304922530105558		[learning rate: 0.0003402]
	Learning Rate: 0.000340202
	LOSS [training: 1.0304922530105558 | validation: 0.7687395620869932]
	TIME [epoch: 8.49 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9950192442757805		[learning rate: 0.00033897]
	Learning Rate: 0.000338967
	LOSS [training: 0.9950192442757805 | validation: 0.8076450560602064]
	TIME [epoch: 8.5 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9926112862546909		[learning rate: 0.00033774]
	Learning Rate: 0.000337737
	LOSS [training: 0.9926112862546909 | validation: 0.7749834799660015]
	TIME [epoch: 8.52 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0012396427095374		[learning rate: 0.00033651]
	Learning Rate: 0.000336512
	LOSS [training: 1.0012396427095374 | validation: 0.7761811310395714]
	TIME [epoch: 8.49 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0078951397496345		[learning rate: 0.00033529]
	Learning Rate: 0.00033529
	LOSS [training: 1.0078951397496345 | validation: 0.7879996394012416]
	TIME [epoch: 8.5 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0072245307727665		[learning rate: 0.00033407]
	Learning Rate: 0.000334074
	LOSS [training: 1.0072245307727665 | validation: 0.7792793765808047]
	TIME [epoch: 8.49 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9943921696628444		[learning rate: 0.00033286]
	Learning Rate: 0.000332861
	LOSS [training: 0.9943921696628444 | validation: 0.7782781805980379]
	TIME [epoch: 8.53 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0000490115916576		[learning rate: 0.00033165]
	Learning Rate: 0.000331653
	LOSS [training: 1.0000490115916576 | validation: 0.8032890246607314]
	TIME [epoch: 8.49 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0240840666697364		[learning rate: 0.00033045]
	Learning Rate: 0.00033045
	LOSS [training: 1.0240840666697364 | validation: 0.7810840422741306]
	TIME [epoch: 8.51 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0278348907642556		[learning rate: 0.00032925]
	Learning Rate: 0.00032925
	LOSS [training: 1.0278348907642556 | validation: 0.7850334881790014]
	TIME [epoch: 8.51 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9995647094240437		[learning rate: 0.00032806]
	Learning Rate: 0.000328056
	LOSS [training: 0.9995647094240437 | validation: 0.7704346882295166]
	TIME [epoch: 8.5 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0326538837513122		[learning rate: 0.00032686]
	Learning Rate: 0.000326865
	LOSS [training: 1.0326538837513122 | validation: 0.8011046991984485]
	TIME [epoch: 8.49 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9989575958469622		[learning rate: 0.00032568]
	Learning Rate: 0.000325679
	LOSS [training: 0.9989575958469622 | validation: 0.7766687529247177]
	TIME [epoch: 8.5 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9983466837903958		[learning rate: 0.0003245]
	Learning Rate: 0.000324497
	LOSS [training: 0.9983466837903958 | validation: 0.7769464681434292]
	TIME [epoch: 8.52 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9814995065840806		[learning rate: 0.00032332]
	Learning Rate: 0.000323319
	LOSS [training: 0.9814995065840806 | validation: 0.7730103574353595]
	TIME [epoch: 8.49 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9913296680405246		[learning rate: 0.00032215]
	Learning Rate: 0.000322146
	LOSS [training: 0.9913296680405246 | validation: 0.8173655498177504]
	TIME [epoch: 8.5 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0488055579122015		[learning rate: 0.00032098]
	Learning Rate: 0.000320977
	LOSS [training: 1.0488055579122015 | validation: 0.7808271484240961]
	TIME [epoch: 8.52 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0298793725684439		[learning rate: 0.00031981]
	Learning Rate: 0.000319812
	LOSS [training: 1.0298793725684439 | validation: 0.7678429124032121]
	TIME [epoch: 8.5 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0062016281373853		[learning rate: 0.00031865]
	Learning Rate: 0.000318651
	LOSS [training: 1.0062016281373853 | validation: 0.7684777405701562]
	TIME [epoch: 8.5 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.05754113143596		[learning rate: 0.0003175]
	Learning Rate: 0.000317495
	LOSS [training: 1.05754113143596 | validation: 0.7818608847818975]
	TIME [epoch: 8.49 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.020404305982582		[learning rate: 0.00031634]
	Learning Rate: 0.000316343
	LOSS [training: 1.020404305982582 | validation: 0.7686282054362668]
	TIME [epoch: 8.52 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.006157952104305		[learning rate: 0.00031519]
	Learning Rate: 0.000315195
	LOSS [training: 1.006157952104305 | validation: 0.7741320186360827]
	TIME [epoch: 8.49 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9915645896334823		[learning rate: 0.00031405]
	Learning Rate: 0.000314051
	LOSS [training: 0.9915645896334823 | validation: 0.7738573345662048]
	TIME [epoch: 8.49 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.990764469128178		[learning rate: 0.00031291]
	Learning Rate: 0.000312911
	LOSS [training: 0.990764469128178 | validation: 0.7851336135070652]
	TIME [epoch: 8.51 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0287199529332582		[learning rate: 0.00031178]
	Learning Rate: 0.000311776
	LOSS [training: 1.0287199529332582 | validation: 0.7952064926029414]
	TIME [epoch: 8.5 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0193564370314696		[learning rate: 0.00031064]
	Learning Rate: 0.000310644
	LOSS [training: 1.0193564370314696 | validation: 0.7767569441109641]
	TIME [epoch: 8.5 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9876315888834307		[learning rate: 0.00030952]
	Learning Rate: 0.000309517
	LOSS [training: 0.9876315888834307 | validation: 0.7759948985750602]
	TIME [epoch: 8.5 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0117425908649982		[learning rate: 0.00030839]
	Learning Rate: 0.000308394
	LOSS [training: 1.0117425908649982 | validation: 0.7653306413062956]
	TIME [epoch: 8.52 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0060079951244576		[learning rate: 0.00030727]
	Learning Rate: 0.000307274
	LOSS [training: 1.0060079951244576 | validation: 0.7838630403152421]
	TIME [epoch: 8.5 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.011179493263261		[learning rate: 0.00030616]
	Learning Rate: 0.000306159
	LOSS [training: 1.011179493263261 | validation: 0.7666956609167374]
	TIME [epoch: 8.5 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.027108226691769		[learning rate: 0.00030505]
	Learning Rate: 0.000305048
	LOSS [training: 1.027108226691769 | validation: 0.7803591080538947]
	TIME [epoch: 8.52 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0158698909667494		[learning rate: 0.00030394]
	Learning Rate: 0.000303941
	LOSS [training: 1.0158698909667494 | validation: 0.7906374642942882]
	TIME [epoch: 8.5 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0089153424326742		[learning rate: 0.00030284]
	Learning Rate: 0.000302838
	LOSS [training: 1.0089153424326742 | validation: 0.7687970750001472]
	TIME [epoch: 8.5 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0316941376128834		[learning rate: 0.00030174]
	Learning Rate: 0.000301739
	LOSS [training: 1.0316941376128834 | validation: 0.772451228278487]
	TIME [epoch: 8.49 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0395779939965788		[learning rate: 0.00030064]
	Learning Rate: 0.000300644
	LOSS [training: 1.0395779939965788 | validation: 0.79312615312823]
	TIME [epoch: 8.53 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0206043982104902		[learning rate: 0.00029955]
	Learning Rate: 0.000299553
	LOSS [training: 1.0206043982104902 | validation: 0.7794354310901802]
	TIME [epoch: 8.51 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0345060469640577		[learning rate: 0.00029847]
	Learning Rate: 0.000298466
	LOSS [training: 1.0345060469640577 | validation: 0.8224886648332679]
	TIME [epoch: 8.5 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0410821101287957		[learning rate: 0.00029738]
	Learning Rate: 0.000297383
	LOSS [training: 1.0410821101287957 | validation: 0.7629385674123612]
	TIME [epoch: 8.52 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0561490180525093		[learning rate: 0.0002963]
	Learning Rate: 0.000296304
	LOSS [training: 1.0561490180525093 | validation: 0.7731294419126613]
	TIME [epoch: 8.51 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0841652475689696		[learning rate: 0.00029523]
	Learning Rate: 0.000295228
	LOSS [training: 1.0841652475689696 | validation: 0.7849900919169998]
	TIME [epoch: 8.51 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0343887491825545		[learning rate: 0.00029416]
	Learning Rate: 0.000294157
	LOSS [training: 1.0343887491825545 | validation: 0.7832028172738016]
	TIME [epoch: 8.49 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0269859453620716		[learning rate: 0.00029309]
	Learning Rate: 0.000293089
	LOSS [training: 1.0269859453620716 | validation: 0.769938938891734]
	TIME [epoch: 8.52 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0312479076491685		[learning rate: 0.00029203]
	Learning Rate: 0.000292026
	LOSS [training: 1.0312479076491685 | validation: 0.8053746445136739]
	TIME [epoch: 8.5 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0217575538062724		[learning rate: 0.00029097]
	Learning Rate: 0.000290966
	LOSS [training: 1.0217575538062724 | validation: 0.7640699531031577]
	TIME [epoch: 8.5 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0572745877751801		[learning rate: 0.00028991]
	Learning Rate: 0.00028991
	LOSS [training: 1.0572745877751801 | validation: 0.7791686166941941]
	TIME [epoch: 8.49 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0335567264083807		[learning rate: 0.00028886]
	Learning Rate: 0.000288858
	LOSS [training: 1.0335567264083807 | validation: 0.7831021074855145]
	TIME [epoch: 8.51 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0304694861226171		[learning rate: 0.00028781]
	Learning Rate: 0.00028781
	LOSS [training: 1.0304694861226171 | validation: 0.8133718772692613]
	TIME [epoch: 8.5 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0236976013958845		[learning rate: 0.00028677]
	Learning Rate: 0.000286765
	LOSS [training: 1.0236976013958845 | validation: 0.7914195707457388]
	TIME [epoch: 8.5 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.031971314687373		[learning rate: 0.00028572]
	Learning Rate: 0.000285724
	LOSS [training: 1.031971314687373 | validation: 0.7674439229905649]
	TIME [epoch: 8.52 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.006592698205753		[learning rate: 0.00028469]
	Learning Rate: 0.000284688
	LOSS [training: 1.006592698205753 | validation: 0.8210275726095736]
	TIME [epoch: 8.5 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0203848545999978		[learning rate: 0.00028365]
	Learning Rate: 0.000283654
	LOSS [training: 1.0203848545999978 | validation: 0.7767010439498659]
	TIME [epoch: 8.5 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0000419847827389		[learning rate: 0.00028263]
	Learning Rate: 0.000282625
	LOSS [training: 1.0000419847827389 | validation: 0.8079862874112564]
	TIME [epoch: 8.5 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0206871646356641		[learning rate: 0.0002816]
	Learning Rate: 0.000281599
	LOSS [training: 1.0206871646356641 | validation: 0.8156159133624011]
	TIME [epoch: 8.51 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0221425093837027		[learning rate: 0.00028058]
	Learning Rate: 0.000280577
	LOSS [training: 1.0221425093837027 | validation: 0.756105931244096]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1083.pth
	Model improved!!!
EPOCH 1084/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9839098219931962		[learning rate: 0.00027956]
	Learning Rate: 0.000279559
	LOSS [training: 0.9839098219931962 | validation: 0.7536161716883845]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1084.pth
	Model improved!!!
EPOCH 1085/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0041231588778259		[learning rate: 0.00027854]
	Learning Rate: 0.000278545
	LOSS [training: 1.0041231588778259 | validation: 0.758861458069521]
	TIME [epoch: 8.52 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.004564803190994		[learning rate: 0.00027753]
	Learning Rate: 0.000277534
	LOSS [training: 1.004564803190994 | validation: 0.7763604595231306]
	TIME [epoch: 8.5 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0316373291414966		[learning rate: 0.00027653]
	Learning Rate: 0.000276527
	LOSS [training: 1.0316373291414966 | validation: 0.8162581903555155]
	TIME [epoch: 8.5 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0192534776524866		[learning rate: 0.00027552]
	Learning Rate: 0.000275523
	LOSS [training: 1.0192534776524866 | validation: 0.7874723473781762]
	TIME [epoch: 8.51 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0159387944734395		[learning rate: 0.00027452]
	Learning Rate: 0.000274523
	LOSS [training: 1.0159387944734395 | validation: 0.7939394357923153]
	TIME [epoch: 8.51 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.013549523148654		[learning rate: 0.00027353]
	Learning Rate: 0.000273527
	LOSS [training: 1.013549523148654 | validation: 0.7735755689159545]
	TIME [epoch: 8.5 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.006072939557828		[learning rate: 0.00027253]
	Learning Rate: 0.000272534
	LOSS [training: 1.006072939557828 | validation: 0.8068497357022304]
	TIME [epoch: 8.49 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0573513343862166		[learning rate: 0.00027155]
	Learning Rate: 0.000271545
	LOSS [training: 1.0573513343862166 | validation: 0.7849406788582312]
	TIME [epoch: 8.52 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.034031814164709		[learning rate: 0.00027056]
	Learning Rate: 0.00027056
	LOSS [training: 1.034031814164709 | validation: 0.9510051012840235]
	TIME [epoch: 8.5 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0675597151792282		[learning rate: 0.00026958]
	Learning Rate: 0.000269578
	LOSS [training: 1.0675597151792282 | validation: 0.7642879119141569]
	TIME [epoch: 8.5 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9935979979832146		[learning rate: 0.0002686]
	Learning Rate: 0.0002686
	LOSS [training: 0.9935979979832146 | validation: 0.7843597028054156]
	TIME [epoch: 8.5 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9943322022726105		[learning rate: 0.00026762]
	Learning Rate: 0.000267625
	LOSS [training: 0.9943322022726105 | validation: 0.8048454306209426]
	TIME [epoch: 8.51 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.007779326730152		[learning rate: 0.00026665]
	Learning Rate: 0.000266654
	LOSS [training: 1.007779326730152 | validation: 0.7741453082135719]
	TIME [epoch: 8.5 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0053931155575875		[learning rate: 0.00026569]
	Learning Rate: 0.000265686
	LOSS [training: 1.0053931155575875 | validation: 0.7733461652169686]
	TIME [epoch: 8.49 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9942891195806739		[learning rate: 0.00026472]
	Learning Rate: 0.000264722
	LOSS [training: 0.9942891195806739 | validation: 0.7888373862287923]
	TIME [epoch: 8.52 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.985439931852018		[learning rate: 0.00026376]
	Learning Rate: 0.000263761
	LOSS [training: 0.985439931852018 | validation: 0.7658475750987813]
	TIME [epoch: 8.5 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.990349191035979		[learning rate: 0.0002628]
	Learning Rate: 0.000262804
	LOSS [training: 0.990349191035979 | validation: 0.7644266118685172]
	TIME [epoch: 8.5 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9986162454966984		[learning rate: 0.00026185]
	Learning Rate: 0.00026185
	LOSS [training: 0.9986162454966984 | validation: 0.7730667364104413]
	TIME [epoch: 8.5 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9934006631832911		[learning rate: 0.0002609]
	Learning Rate: 0.0002609
	LOSS [training: 0.9934006631832911 | validation: 0.7801184108887081]
	TIME [epoch: 8.52 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9967741443356115		[learning rate: 0.00025995]
	Learning Rate: 0.000259953
	LOSS [training: 0.9967741443356115 | validation: 0.7549935925448168]
	TIME [epoch: 8.5 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0249575691665125		[learning rate: 0.00025901]
	Learning Rate: 0.00025901
	LOSS [training: 1.0249575691665125 | validation: 0.7718406949114925]
	TIME [epoch: 8.49 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0097480095580968		[learning rate: 0.00025807]
	Learning Rate: 0.00025807
	LOSS [training: 1.0097480095580968 | validation: 0.7733122757607146]
	TIME [epoch: 8.52 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0040947236305455		[learning rate: 0.00025713]
	Learning Rate: 0.000257133
	LOSS [training: 1.0040947236305455 | validation: 0.7726306158352707]
	TIME [epoch: 8.5 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0102024609807265		[learning rate: 0.0002562]
	Learning Rate: 0.0002562
	LOSS [training: 1.0102024609807265 | validation: 0.7681554233207868]
	TIME [epoch: 8.49 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0019100259483475		[learning rate: 0.00025527]
	Learning Rate: 0.00025527
	LOSS [training: 1.0019100259483475 | validation: 0.7816134409430944]
	TIME [epoch: 8.5 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0151694641326692		[learning rate: 0.00025434]
	Learning Rate: 0.000254344
	LOSS [training: 1.0151694641326692 | validation: 0.7893303075046043]
	TIME [epoch: 8.52 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0374932864353843		[learning rate: 0.00025342]
	Learning Rate: 0.000253421
	LOSS [training: 1.0374932864353843 | validation: 0.7812221502989961]
	TIME [epoch: 8.5 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9875717086151206		[learning rate: 0.0002525]
	Learning Rate: 0.000252501
	LOSS [training: 0.9875717086151206 | validation: 0.7742340068251475]
	TIME [epoch: 8.5 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9831596784459256		[learning rate: 0.00025158]
	Learning Rate: 0.000251585
	LOSS [training: 0.9831596784459256 | validation: 0.7720498363305535]
	TIME [epoch: 8.52 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.043655864478589		[learning rate: 0.00025067]
	Learning Rate: 0.000250672
	LOSS [training: 1.043655864478589 | validation: 0.7789557974135802]
	TIME [epoch: 8.5 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0355119997401292		[learning rate: 0.00024976]
	Learning Rate: 0.000249762
	LOSS [training: 1.0355119997401292 | validation: 0.7848896851975472]
	TIME [epoch: 8.5 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0352402147222013		[learning rate: 0.00024886]
	Learning Rate: 0.000248856
	LOSS [training: 1.0352402147222013 | validation: 0.7692015226884942]
	TIME [epoch: 8.49 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0216882131716618		[learning rate: 0.00024795]
	Learning Rate: 0.000247952
	LOSS [training: 1.0216882131716618 | validation: 0.7821358119181068]
	TIME [epoch: 8.52 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9999862190417804		[learning rate: 0.00024705]
	Learning Rate: 0.000247053
	LOSS [training: 0.9999862190417804 | validation: 0.777785785092123]
	TIME [epoch: 8.49 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9969830441214054		[learning rate: 0.00024616]
	Learning Rate: 0.000246156
	LOSS [training: 0.9969830441214054 | validation: 0.7843311205264578]
	TIME [epoch: 8.49 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0266480957216144		[learning rate: 0.00024526]
	Learning Rate: 0.000245263
	LOSS [training: 1.0266480957216144 | validation: 0.8032288459461718]
	TIME [epoch: 8.51 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0586102911178232		[learning rate: 0.00024437]
	Learning Rate: 0.000244373
	LOSS [training: 1.0586102911178232 | validation: 0.7641004916250468]
	TIME [epoch: 8.5 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0190394322399638		[learning rate: 0.00024349]
	Learning Rate: 0.000243486
	LOSS [training: 1.0190394322399638 | validation: 0.8171325589723413]
	TIME [epoch: 8.49 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0726672541544335		[learning rate: 0.0002426]
	Learning Rate: 0.000242602
	LOSS [training: 1.0726672541544335 | validation: 0.7739585799485893]
	TIME [epoch: 8.5 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0043347752675975		[learning rate: 0.00024172]
	Learning Rate: 0.000241722
	LOSS [training: 1.0043347752675975 | validation: 0.7654581154888975]
	TIME [epoch: 8.52 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9649148513403375		[learning rate: 0.00024084]
	Learning Rate: 0.000240845
	LOSS [training: 0.9649148513403375 | validation: 0.7546957455504707]
	TIME [epoch: 8.5 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9726371973129784		[learning rate: 0.00023997]
	Learning Rate: 0.000239971
	LOSS [training: 0.9726371973129784 | validation: 0.764845956117273]
	TIME [epoch: 8.49 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9654169561877103		[learning rate: 0.0002391]
	Learning Rate: 0.0002391
	LOSS [training: 0.9654169561877103 | validation: 0.7803933933090021]
	TIME [epoch: 8.51 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.03889687024335		[learning rate: 0.00023823]
	Learning Rate: 0.000238232
	LOSS [training: 1.03889687024335 | validation: 0.7774834074483458]
	TIME [epoch: 8.5 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0024136941365616		[learning rate: 0.00023737]
	Learning Rate: 0.000237367
	LOSS [training: 1.0024136941365616 | validation: 0.7649913119743357]
	TIME [epoch: 8.49 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9914799726745784		[learning rate: 0.00023651]
	Learning Rate: 0.000236506
	LOSS [training: 0.9914799726745784 | validation: 0.8226111366128251]
	TIME [epoch: 8.49 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0363240670997178		[learning rate: 0.00023565]
	Learning Rate: 0.000235648
	LOSS [training: 1.0363240670997178 | validation: 0.7650018436731659]
	TIME [epoch: 8.51 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0249833913832518		[learning rate: 0.00023479]
	Learning Rate: 0.000234793
	LOSS [training: 1.0249833913832518 | validation: 0.816361100125703]
	TIME [epoch: 8.5 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9959369922650133		[learning rate: 0.00023394]
	Learning Rate: 0.00023394
	LOSS [training: 0.9959369922650133 | validation: 0.7898013812269988]
	TIME [epoch: 8.49 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9841869647661838		[learning rate: 0.00023309]
	Learning Rate: 0.000233091
	LOSS [training: 0.9841869647661838 | validation: 0.7651673780716645]
	TIME [epoch: 8.5 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.983066382180597		[learning rate: 0.00023225]
	Learning Rate: 0.000232246
	LOSS [training: 0.983066382180597 | validation: 0.833446073816535]
	TIME [epoch: 8.51 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9895550975341632		[learning rate: 0.0002314]
	Learning Rate: 0.000231403
	LOSS [training: 0.9895550975341632 | validation: 0.7507606885599591]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1136.pth
	Model improved!!!
EPOCH 1137/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9793371941330122		[learning rate: 0.00023056]
	Learning Rate: 0.000230563
	LOSS [training: 0.9793371941330122 | validation: 0.7583353109041159]
	TIME [epoch: 8.5 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9663958036035349		[learning rate: 0.00022973]
	Learning Rate: 0.000229726
	LOSS [training: 0.9663958036035349 | validation: 0.7569205357954655]
	TIME [epoch: 8.52 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9820314722938723		[learning rate: 0.00022889]
	Learning Rate: 0.000228893
	LOSS [training: 0.9820314722938723 | validation: 0.7935185457741687]
	TIME [epoch: 8.5 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9953721810427533		[learning rate: 0.00022806]
	Learning Rate: 0.000228062
	LOSS [training: 0.9953721810427533 | validation: 0.7632986159031825]
	TIME [epoch: 8.49 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9703543521733383		[learning rate: 0.00022723]
	Learning Rate: 0.000227234
	LOSS [training: 0.9703543521733383 | validation: 0.7599099745556803]
	TIME [epoch: 8.5 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.973593171853901		[learning rate: 0.00022641]
	Learning Rate: 0.00022641
	LOSS [training: 0.973593171853901 | validation: 0.8101794185376048]
	TIME [epoch: 8.51 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9779059951518564		[learning rate: 0.00022559]
	Learning Rate: 0.000225588
	LOSS [training: 0.9779059951518564 | validation: 0.7816283277939543]
	TIME [epoch: 8.5 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9755215492538323		[learning rate: 0.00022477]
	Learning Rate: 0.000224769
	LOSS [training: 0.9755215492538323 | validation: 0.7567020541193413]
	TIME [epoch: 8.49 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9889988349745524		[learning rate: 0.00022395]
	Learning Rate: 0.000223954
	LOSS [training: 0.9889988349745524 | validation: 0.7756175529197384]
	TIME [epoch: 8.51 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9672435374422073		[learning rate: 0.00022314]
	Learning Rate: 0.000223141
	LOSS [training: 0.9672435374422073 | validation: 0.7603279413326768]
	TIME [epoch: 8.49 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9654819441617362		[learning rate: 0.00022233]
	Learning Rate: 0.000222331
	LOSS [training: 0.9654819441617362 | validation: 0.8087031370235289]
	TIME [epoch: 8.49 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.981043248810202		[learning rate: 0.00022152]
	Learning Rate: 0.000221524
	LOSS [training: 0.981043248810202 | validation: 0.7659872755409441]
	TIME [epoch: 8.5 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9689305792649773		[learning rate: 0.00022072]
	Learning Rate: 0.00022072
	LOSS [training: 0.9689305792649773 | validation: 0.7668076218871985]
	TIME [epoch: 8.51 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0332059716286188		[learning rate: 0.00021992]
	Learning Rate: 0.000219919
	LOSS [training: 1.0332059716286188 | validation: 0.7628680458209631]
	TIME [epoch: 8.49 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0678734460370785		[learning rate: 0.00021912]
	Learning Rate: 0.000219121
	LOSS [training: 1.0678734460370785 | validation: 0.8223195052464001]
	TIME [epoch: 8.49 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9923344452941858		[learning rate: 0.00021833]
	Learning Rate: 0.000218326
	LOSS [training: 0.9923344452941858 | validation: 0.7873277385979542]
	TIME [epoch: 8.51 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9699466824497447		[learning rate: 0.00021753]
	Learning Rate: 0.000217534
	LOSS [training: 0.9699466824497447 | validation: 0.7691809994802629]
	TIME [epoch: 8.5 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9705842508343515		[learning rate: 0.00021674]
	Learning Rate: 0.000216744
	LOSS [training: 0.9705842508343515 | validation: 0.7697251551544875]
	TIME [epoch: 8.49 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9635857982586893		[learning rate: 0.00021596]
	Learning Rate: 0.000215958
	LOSS [training: 0.9635857982586893 | validation: 0.7576305869990421]
	TIME [epoch: 8.49 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9763052115689661		[learning rate: 0.00021517]
	Learning Rate: 0.000215174
	LOSS [training: 0.9763052115689661 | validation: 0.7740976674170092]
	TIME [epoch: 8.52 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.975701008364919		[learning rate: 0.00021439]
	Learning Rate: 0.000214393
	LOSS [training: 0.975701008364919 | validation: 0.7811528794838967]
	TIME [epoch: 8.49 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9762321228767217		[learning rate: 0.00021361]
	Learning Rate: 0.000213615
	LOSS [training: 0.9762321228767217 | validation: 0.7618728722271909]
	TIME [epoch: 8.49 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9673683786716454		[learning rate: 0.00021284]
	Learning Rate: 0.00021284
	LOSS [training: 0.9673683786716454 | validation: 0.766086848423216]
	TIME [epoch: 8.51 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9999882813061042		[learning rate: 0.00021207]
	Learning Rate: 0.000212067
	LOSS [training: 0.9999882813061042 | validation: 0.779037136025633]
	TIME [epoch: 8.5 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9902499701008514		[learning rate: 0.0002113]
	Learning Rate: 0.000211298
	LOSS [training: 0.9902499701008514 | validation: 0.8715043666367995]
	TIME [epoch: 8.49 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0084895226947872		[learning rate: 0.00021053]
	Learning Rate: 0.000210531
	LOSS [training: 1.0084895226947872 | validation: 0.7869037478562149]
	TIME [epoch: 8.49 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9673879571708184		[learning rate: 0.00020977]
	Learning Rate: 0.000209767
	LOSS [training: 0.9673879571708184 | validation: 0.7607149048013275]
	TIME [epoch: 8.51 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9719676921338506		[learning rate: 0.00020901]
	Learning Rate: 0.000209006
	LOSS [training: 0.9719676921338506 | validation: 0.7819120200777017]
	TIME [epoch: 8.49 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9820542627381454		[learning rate: 0.00020825]
	Learning Rate: 0.000208247
	LOSS [training: 0.9820542627381454 | validation: 0.7702376322653379]
	TIME [epoch: 8.5 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9986090142502346		[learning rate: 0.00020749]
	Learning Rate: 0.000207491
	LOSS [training: 0.9986090142502346 | validation: 0.7519025560710577]
	TIME [epoch: 8.51 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9854251240815142		[learning rate: 0.00020674]
	Learning Rate: 0.000206738
	LOSS [training: 0.9854251240815142 | validation: 0.7787033906300651]
	TIME [epoch: 8.5 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9684362645477471		[learning rate: 0.00020599]
	Learning Rate: 0.000205988
	LOSS [training: 0.9684362645477471 | validation: 0.7775201111796665]
	TIME [epoch: 8.49 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9662725538956884		[learning rate: 0.00020524]
	Learning Rate: 0.000205241
	LOSS [training: 0.9662725538956884 | validation: 0.7948665117086913]
	TIME [epoch: 8.49 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9801791649208618		[learning rate: 0.0002045]
	Learning Rate: 0.000204496
	LOSS [training: 0.9801791649208618 | validation: 0.7468935300068485]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1170.pth
	Model improved!!!
EPOCH 1171/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9766587868304087		[learning rate: 0.00020375]
	Learning Rate: 0.000203754
	LOSS [training: 0.9766587868304087 | validation: 0.7669962493129356]
	TIME [epoch: 8.49 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9616312654390959		[learning rate: 0.00020301]
	Learning Rate: 0.000203014
	LOSS [training: 0.9616312654390959 | validation: 0.7593876162658523]
	TIME [epoch: 8.49 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9749435963472208		[learning rate: 0.00020228]
	Learning Rate: 0.000202277
	LOSS [training: 0.9749435963472208 | validation: 0.7675048130268097]
	TIME [epoch: 8.5 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9834905838069323		[learning rate: 0.00020154]
	Learning Rate: 0.000201543
	LOSS [training: 0.9834905838069323 | validation: 0.7911158221712745]
	TIME [epoch: 8.5 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.998929559251619		[learning rate: 0.00020081]
	Learning Rate: 0.000200812
	LOSS [training: 0.998929559251619 | validation: 0.7624735136626812]
	TIME [epoch: 8.49 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9754324488530945		[learning rate: 0.00020008]
	Learning Rate: 0.000200083
	LOSS [training: 0.9754324488530945 | validation: 0.8070039746363866]
	TIME [epoch: 8.49 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0048913732827516		[learning rate: 0.00019936]
	Learning Rate: 0.000199357
	LOSS [training: 1.0048913732827516 | validation: 0.7712313434883835]
	TIME [epoch: 8.51 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9722327210705055		[learning rate: 0.00019863]
	Learning Rate: 0.000198634
	LOSS [training: 0.9722327210705055 | validation: 0.7545496427105636]
	TIME [epoch: 8.49 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9714160404570494		[learning rate: 0.00019791]
	Learning Rate: 0.000197913
	LOSS [training: 0.9714160404570494 | validation: 0.7645068881019611]
	TIME [epoch: 8.49 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9898774413587826		[learning rate: 0.00019719]
	Learning Rate: 0.000197194
	LOSS [training: 0.9898774413587826 | validation: 0.7588388021087795]
	TIME [epoch: 8.5 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9643781702274834		[learning rate: 0.00019648]
	Learning Rate: 0.000196479
	LOSS [training: 0.9643781702274834 | validation: 0.7721681008324643]
	TIME [epoch: 8.5 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9775794806900684		[learning rate: 0.00019577]
	Learning Rate: 0.000195766
	LOSS [training: 0.9775794806900684 | validation: 0.7806381490925671]
	TIME [epoch: 8.49 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0189125488917063		[learning rate: 0.00019506]
	Learning Rate: 0.000195055
	LOSS [training: 1.0189125488917063 | validation: 0.7714071992687164]
	TIME [epoch: 8.49 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9820553208256777		[learning rate: 0.00019435]
	Learning Rate: 0.000194347
	LOSS [training: 0.9820553208256777 | validation: 0.7666531625895805]
	TIME [epoch: 8.51 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9801297038325352		[learning rate: 0.00019364]
	Learning Rate: 0.000193642
	LOSS [training: 0.9801297038325352 | validation: 0.7602427995060737]
	TIME [epoch: 8.49 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9814680690468442		[learning rate: 0.00019294]
	Learning Rate: 0.000192939
	LOSS [training: 0.9814680690468442 | validation: 0.7470383728166712]
	TIME [epoch: 8.49 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9707086597122657		[learning rate: 0.00019224]
	Learning Rate: 0.000192239
	LOSS [training: 0.9707086597122657 | validation: 0.7614405755818663]
	TIME [epoch: 8.49 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9619265434173553		[learning rate: 0.00019154]
	Learning Rate: 0.000191542
	LOSS [training: 0.9619265434173553 | validation: 0.7665582772648343]
	TIME [epoch: 8.51 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.963640961831028		[learning rate: 0.00019085]
	Learning Rate: 0.000190846
	LOSS [training: 0.963640961831028 | validation: 0.7517404498743978]
	TIME [epoch: 8.49 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9682266442006396		[learning rate: 0.00019015]
	Learning Rate: 0.000190154
	LOSS [training: 0.9682266442006396 | validation: 0.7598189419735011]
	TIME [epoch: 8.49 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9630438079514372		[learning rate: 0.00018946]
	Learning Rate: 0.000189464
	LOSS [training: 0.9630438079514372 | validation: 0.7761721515987469]
	TIME [epoch: 8.51 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.958570745348189		[learning rate: 0.00018878]
	Learning Rate: 0.000188776
	LOSS [training: 0.958570745348189 | validation: 0.8012467383857974]
	TIME [epoch: 8.5 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9731696224014804		[learning rate: 0.00018809]
	Learning Rate: 0.000188091
	LOSS [training: 0.9731696224014804 | validation: 0.7702182571087598]
	TIME [epoch: 8.49 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9775557112249633		[learning rate: 0.00018741]
	Learning Rate: 0.000187409
	LOSS [training: 0.9775557112249633 | validation: 0.7490524989427252]
	TIME [epoch: 8.49 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9506028875668916		[learning rate: 0.00018673]
	Learning Rate: 0.000186729
	LOSS [training: 0.9506028875668916 | validation: 0.7883783470836238]
	TIME [epoch: 8.51 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9522504511911863		[learning rate: 0.00018605]
	Learning Rate: 0.000186051
	LOSS [training: 0.9522504511911863 | validation: 0.7774245271029886]
	TIME [epoch: 8.49 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9516109851047176		[learning rate: 0.00018538]
	Learning Rate: 0.000185376
	LOSS [training: 0.9516109851047176 | validation: 0.7660914736229931]
	TIME [epoch: 8.49 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9557608963753397		[learning rate: 0.0001847]
	Learning Rate: 0.000184703
	LOSS [training: 0.9557608963753397 | validation: 0.7627300859921289]
	TIME [epoch: 8.51 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9550765190258688		[learning rate: 0.00018403]
	Learning Rate: 0.000184033
	LOSS [training: 0.9550765190258688 | validation: 0.760306002840073]
	TIME [epoch: 8.49 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9934404728834723		[learning rate: 0.00018336]
	Learning Rate: 0.000183365
	LOSS [training: 0.9934404728834723 | validation: 0.8271254227737344]
	TIME [epoch: 8.48 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9841249600964754		[learning rate: 0.0001827]
	Learning Rate: 0.000182699
	LOSS [training: 0.9841249600964754 | validation: 0.7723536055785181]
	TIME [epoch: 8.49 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9752125686645851		[learning rate: 0.00018204]
	Learning Rate: 0.000182036
	LOSS [training: 0.9752125686645851 | validation: 0.7568619418500778]
	TIME [epoch: 8.51 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9896291276363275		[learning rate: 0.00018138]
	Learning Rate: 0.000181376
	LOSS [training: 0.9896291276363275 | validation: 0.7705590990246143]
	TIME [epoch: 8.5 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9811531826207439		[learning rate: 0.00018072]
	Learning Rate: 0.000180717
	LOSS [training: 0.9811531826207439 | validation: 0.760551592898317]
	TIME [epoch: 8.49 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9769695053109617		[learning rate: 0.00018006]
	Learning Rate: 0.000180062
	LOSS [training: 0.9769695053109617 | validation: 0.8068794808854193]
	TIME [epoch: 8.51 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9940792603652241		[learning rate: 0.00017941]
	Learning Rate: 0.000179408
	LOSS [training: 0.9940792603652241 | validation: 0.7608963151913174]
	TIME [epoch: 8.49 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9699643599636142		[learning rate: 0.00017876]
	Learning Rate: 0.000178757
	LOSS [training: 0.9699643599636142 | validation: 0.760055670225825]
	TIME [epoch: 8.49 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9983848510709292		[learning rate: 0.00017811]
	Learning Rate: 0.000178108
	LOSS [training: 0.9983848510709292 | validation: 0.7620598934589767]
	TIME [epoch: 8.49 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9705531635310605		[learning rate: 0.00017746]
	Learning Rate: 0.000177462
	LOSS [training: 0.9705531635310605 | validation: 0.7537366260532605]
	TIME [epoch: 8.51 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9820204119171464		[learning rate: 0.00017682]
	Learning Rate: 0.000176818
	LOSS [training: 0.9820204119171464 | validation: 0.7943341717433644]
	TIME [epoch: 8.49 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9753261985658778		[learning rate: 0.00017618]
	Learning Rate: 0.000176176
	LOSS [training: 0.9753261985658778 | validation: 0.7715784645223595]
	TIME [epoch: 8.49 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9572811472741469		[learning rate: 0.00017554]
	Learning Rate: 0.000175537
	LOSS [training: 0.9572811472741469 | validation: 0.7623892670847016]
	TIME [epoch: 8.51 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9555103672268421		[learning rate: 0.0001749]
	Learning Rate: 0.0001749
	LOSS [training: 0.9555103672268421 | validation: 0.7626610730388276]
	TIME [epoch: 8.5 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9613591164127688		[learning rate: 0.00017427]
	Learning Rate: 0.000174265
	LOSS [training: 0.9613591164127688 | validation: 0.7740051158123767]
	TIME [epoch: 8.5 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9677522778827574		[learning rate: 0.00017363]
	Learning Rate: 0.000173633
	LOSS [training: 0.9677522778827574 | validation: 0.7771571466021893]
	TIME [epoch: 8.49 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0091061111856716		[learning rate: 0.000173]
	Learning Rate: 0.000173003
	LOSS [training: 1.0091061111856716 | validation: 0.7645623098248715]
	TIME [epoch: 8.51 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.017148331127134		[learning rate: 0.00017237]
	Learning Rate: 0.000172375
	LOSS [training: 1.017148331127134 | validation: 0.75862484121679]
	TIME [epoch: 8.48 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9910716074589153		[learning rate: 0.00017175]
	Learning Rate: 0.000171749
	LOSS [training: 0.9910716074589153 | validation: 0.7680958231779137]
	TIME [epoch: 8.49 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9724274501341569		[learning rate: 0.00017113]
	Learning Rate: 0.000171126
	LOSS [training: 0.9724274501341569 | validation: 0.7808963035186629]
	TIME [epoch: 8.5 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0075381141157653		[learning rate: 0.0001705]
	Learning Rate: 0.000170505
	LOSS [training: 1.0075381141157653 | validation: 0.7669435391732689]
	TIME [epoch: 8.5 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9706557099728567		[learning rate: 0.00016989]
	Learning Rate: 0.000169886
	LOSS [training: 0.9706557099728567 | validation: 0.7613651271156103]
	TIME [epoch: 8.49 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9675732747092892		[learning rate: 0.00016927]
	Learning Rate: 0.00016927
	LOSS [training: 0.9675732747092892 | validation: 0.7599396403509836]
	TIME [epoch: 8.49 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9743507747166369		[learning rate: 0.00016866]
	Learning Rate: 0.000168655
	LOSS [training: 0.9743507747166369 | validation: 0.7608026630244873]
	TIME [epoch: 8.51 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9647662796974436		[learning rate: 0.00016804]
	Learning Rate: 0.000168043
	LOSS [training: 0.9647662796974436 | validation: 0.7579216659593546]
	TIME [epoch: 8.49 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.962676404979985		[learning rate: 0.00016743]
	Learning Rate: 0.000167433
	LOSS [training: 0.962676404979985 | validation: 0.754894592171099]
	TIME [epoch: 8.49 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9823911147492979		[learning rate: 0.00016683]
	Learning Rate: 0.000166826
	LOSS [training: 0.9823911147492979 | validation: 0.76156237183787]
	TIME [epoch: 8.51 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9593833678376997		[learning rate: 0.00016622]
	Learning Rate: 0.00016622
	LOSS [training: 0.9593833678376997 | validation: 0.7693189325901657]
	TIME [epoch: 8.5 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9591167305129294		[learning rate: 0.00016562]
	Learning Rate: 0.000165617
	LOSS [training: 0.9591167305129294 | validation: 0.7534218303558108]
	TIME [epoch: 8.49 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9562601335363		[learning rate: 0.00016502]
	Learning Rate: 0.000165016
	LOSS [training: 0.9562601335363 | validation: 0.7570231719677089]
	TIME [epoch: 8.49 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9603744952170203		[learning rate: 0.00016442]
	Learning Rate: 0.000164417
	LOSS [training: 0.9603744952170203 | validation: 0.7517450187163612]
	TIME [epoch: 8.51 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9765740479315687		[learning rate: 0.00016382]
	Learning Rate: 0.000163821
	LOSS [training: 0.9765740479315687 | validation: 0.7929602276274406]
	TIME [epoch: 8.49 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9993364735926666		[learning rate: 0.00016323]
	Learning Rate: 0.000163226
	LOSS [training: 0.9993364735926666 | validation: 0.7523510419279912]
	TIME [epoch: 8.49 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.978489281834906		[learning rate: 0.00016263]
	Learning Rate: 0.000162634
	LOSS [training: 0.978489281834906 | validation: 0.777899574916337]
	TIME [epoch: 8.49 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9700056086476543		[learning rate: 0.00016204]
	Learning Rate: 0.000162043
	LOSS [training: 0.9700056086476543 | validation: 0.7638568652096277]
	TIME [epoch: 8.51 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9624098047519899		[learning rate: 0.00016146]
	Learning Rate: 0.000161455
	LOSS [training: 0.9624098047519899 | validation: 0.7619086326830449]
	TIME [epoch: 8.49 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9801904461329544		[learning rate: 0.00016087]
	Learning Rate: 0.000160869
	LOSS [training: 0.9801904461329544 | validation: 0.7882442651662835]
	TIME [epoch: 8.49 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9752693333909299		[learning rate: 0.00016029]
	Learning Rate: 0.000160286
	LOSS [training: 0.9752693333909299 | validation: 0.745882716683755]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1237.pth
	Model improved!!!
EPOCH 1238/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9658308631191794		[learning rate: 0.0001597]
	Learning Rate: 0.000159704
	LOSS [training: 0.9658308631191794 | validation: 0.7580897303698271]
	TIME [epoch: 8.5 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9863737795848782		[learning rate: 0.00015912]
	Learning Rate: 0.000159124
	LOSS [training: 0.9863737795848782 | validation: 0.7638600487279363]
	TIME [epoch: 8.49 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9634862561022232		[learning rate: 0.00015855]
	Learning Rate: 0.000158547
	LOSS [training: 0.9634862561022232 | validation: 0.7555860434665822]
	TIME [epoch: 8.49 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9555681257037545		[learning rate: 0.00015797]
	Learning Rate: 0.000157972
	LOSS [training: 0.9555681257037545 | validation: 0.7712608120841608]
	TIME [epoch: 8.51 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9697370007198088		[learning rate: 0.0001574]
	Learning Rate: 0.000157398
	LOSS [training: 0.9697370007198088 | validation: 0.7730361002366417]
	TIME [epoch: 8.49 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9657313283266064		[learning rate: 0.00015683]
	Learning Rate: 0.000156827
	LOSS [training: 0.9657313283266064 | validation: 0.7535298277826428]
	TIME [epoch: 8.5 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9536375409281623		[learning rate: 0.00015626]
	Learning Rate: 0.000156258
	LOSS [training: 0.9536375409281623 | validation: 0.7805181698374617]
	TIME [epoch: 8.51 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0983357749565772		[learning rate: 0.00015569]
	Learning Rate: 0.000155691
	LOSS [training: 1.0983357749565772 | validation: 0.7652112484712765]
	TIME [epoch: 8.49 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9811711401218064		[learning rate: 0.00015513]
	Learning Rate: 0.000155126
	LOSS [training: 0.9811711401218064 | validation: 0.7611217354174189]
	TIME [epoch: 8.49 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9548074440722921		[learning rate: 0.00015456]
	Learning Rate: 0.000154563
	LOSS [training: 0.9548074440722921 | validation: 0.7602790184288689]
	TIME [epoch: 8.49 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.94733649295954		[learning rate: 0.000154]
	Learning Rate: 0.000154002
	LOSS [training: 0.94733649295954 | validation: 0.7568359850650399]
	TIME [epoch: 8.5 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9564486647871767		[learning rate: 0.00015344]
	Learning Rate: 0.000153443
	LOSS [training: 0.9564486647871767 | validation: 0.7532784301820923]
	TIME [epoch: 8.49 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9493144841415777		[learning rate: 0.00015289]
	Learning Rate: 0.000152886
	LOSS [training: 0.9493144841415777 | validation: 0.743785239716593]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1250.pth
	Model improved!!!
EPOCH 1251/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9495827897002492		[learning rate: 0.00015233]
	Learning Rate: 0.000152331
	LOSS [training: 0.9495827897002492 | validation: 0.7483495856590013]
	TIME [epoch: 8.52 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9545243947983385		[learning rate: 0.00015178]
	Learning Rate: 0.000151779
	LOSS [training: 0.9545243947983385 | validation: 0.7610827840807599]
	TIME [epoch: 8.49 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9486377231483267		[learning rate: 0.00015123]
	Learning Rate: 0.000151228
	LOSS [training: 0.9486377231483267 | validation: 0.7461352212430763]
	TIME [epoch: 8.49 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9528467165421395		[learning rate: 0.00015068]
	Learning Rate: 0.000150679
	LOSS [training: 0.9528467165421395 | validation: 0.7626766177801979]
	TIME [epoch: 8.49 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9543189065637032		[learning rate: 0.00015013]
	Learning Rate: 0.000150132
	LOSS [training: 0.9543189065637032 | validation: 0.7753628342215618]
	TIME [epoch: 8.51 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9488146520449575		[learning rate: 0.00014959]
	Learning Rate: 0.000149587
	LOSS [training: 0.9488146520449575 | validation: 0.7970965250913045]
	TIME [epoch: 8.49 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9513656967032169		[learning rate: 0.00014904]
	Learning Rate: 0.000149044
	LOSS [training: 0.9513656967032169 | validation: 0.8233933689808184]
	TIME [epoch: 8.49 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9643312486257951		[learning rate: 0.0001485]
	Learning Rate: 0.000148504
	LOSS [training: 0.9643312486257951 | validation: 0.7992865303976956]
	TIME [epoch: 8.51 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9518645826487513		[learning rate: 0.00014796]
	Learning Rate: 0.000147965
	LOSS [training: 0.9518645826487513 | validation: 0.7601988978759346]
	TIME [epoch: 8.49 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.944768098685312		[learning rate: 0.00014743]
	Learning Rate: 0.000147428
	LOSS [training: 0.944768098685312 | validation: 0.7531997879988267]
	TIME [epoch: 8.49 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.948345282669453		[learning rate: 0.00014689]
	Learning Rate: 0.000146893
	LOSS [training: 0.948345282669453 | validation: 0.7483880659322649]
	TIME [epoch: 8.49 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9567346115666094		[learning rate: 0.00014636]
	Learning Rate: 0.00014636
	LOSS [training: 0.9567346115666094 | validation: 0.7586427038023508]
	TIME [epoch: 8.51 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9513790276818476		[learning rate: 0.00014583]
	Learning Rate: 0.000145828
	LOSS [training: 0.9513790276818476 | validation: 0.7460703948389583]
	TIME [epoch: 8.49 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9561706324529846		[learning rate: 0.0001453]
	Learning Rate: 0.000145299
	LOSS [training: 0.9561706324529846 | validation: 0.7507970418686865]
	TIME [epoch: 8.49 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9663495574765066		[learning rate: 0.00014477]
	Learning Rate: 0.000144772
	LOSS [training: 0.9663495574765066 | validation: 0.7707750985364816]
	TIME [epoch: 8.51 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9490131970864084		[learning rate: 0.00014425]
	Learning Rate: 0.000144247
	LOSS [training: 0.9490131970864084 | validation: 0.7503995217418575]
	TIME [epoch: 8.49 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9595705562589958		[learning rate: 0.00014372]
	Learning Rate: 0.000143723
	LOSS [training: 0.9595705562589958 | validation: 0.7557869364150088]
	TIME [epoch: 8.49 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.950084810414906		[learning rate: 0.0001432]
	Learning Rate: 0.000143201
	LOSS [training: 0.950084810414906 | validation: 0.7723604775414595]
	TIME [epoch: 8.49 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9441301124848527		[learning rate: 0.00014268]
	Learning Rate: 0.000142682
	LOSS [training: 0.9441301124848527 | validation: 0.7683056183587155]
	TIME [epoch: 8.51 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9345892490143756		[learning rate: 0.00014216]
	Learning Rate: 0.000142164
	LOSS [training: 0.9345892490143756 | validation: 0.7694702724679118]
	TIME [epoch: 8.49 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9857432193267274		[learning rate: 0.00014165]
	Learning Rate: 0.000141648
	LOSS [training: 0.9857432193267274 | validation: 0.7871056457765637]
	TIME [epoch: 8.5 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9603136902929904		[learning rate: 0.00014113]
	Learning Rate: 0.000141134
	LOSS [training: 0.9603136902929904 | validation: 0.7591005207762657]
	TIME [epoch: 8.5 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.948873224581124		[learning rate: 0.00014062]
	Learning Rate: 0.000140622
	LOSS [training: 0.948873224581124 | validation: 0.7591697257376812]
	TIME [epoch: 8.5 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9376145323015674		[learning rate: 0.00014011]
	Learning Rate: 0.000140112
	LOSS [training: 0.9376145323015674 | validation: 0.7656586934999052]
	TIME [epoch: 8.48 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9723839078050137		[learning rate: 0.0001396]
	Learning Rate: 0.000139603
	LOSS [training: 0.9723839078050137 | validation: 0.7746511784466809]
	TIME [epoch: 8.49 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9690344964894173		[learning rate: 0.0001391]
	Learning Rate: 0.000139096
	LOSS [training: 0.9690344964894173 | validation: 0.767122796695195]
	TIME [epoch: 8.51 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9457330034164814		[learning rate: 0.00013859]
	Learning Rate: 0.000138592
	LOSS [training: 0.9457330034164814 | validation: 0.7592394603638616]
	TIME [epoch: 8.49 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9339287104394005		[learning rate: 0.00013809]
	Learning Rate: 0.000138089
	LOSS [training: 0.9339287104394005 | validation: 0.7523210881643149]
	TIME [epoch: 8.49 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9391707010592031		[learning rate: 0.00013759]
	Learning Rate: 0.000137587
	LOSS [training: 0.9391707010592031 | validation: 0.7589887828909777]
	TIME [epoch: 8.5 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9365326203502216		[learning rate: 0.00013709]
	Learning Rate: 0.000137088
	LOSS [training: 0.9365326203502216 | validation: 0.7556160261055196]
	TIME [epoch: 8.49 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9606285477207692		[learning rate: 0.00013659]
	Learning Rate: 0.000136591
	LOSS [training: 0.9606285477207692 | validation: 0.8120375765567589]
	TIME [epoch: 8.49 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9622967577149387		[learning rate: 0.00013609]
	Learning Rate: 0.000136095
	LOSS [training: 0.9622967577149387 | validation: 0.7593846486176803]
	TIME [epoch: 8.49 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9449343097856617		[learning rate: 0.0001356]
	Learning Rate: 0.000135601
	LOSS [training: 0.9449343097856617 | validation: 0.7506507516732617]
	TIME [epoch: 8.51 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9574440559744476		[learning rate: 0.00013511]
	Learning Rate: 0.000135109
	LOSS [training: 0.9574440559744476 | validation: 0.7568634994743823]
	TIME [epoch: 8.49 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9558548156421771		[learning rate: 0.00013462]
	Learning Rate: 0.000134619
	LOSS [training: 0.9558548156421771 | validation: 0.7527148846294143]
	TIME [epoch: 8.49 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9391538120113936		[learning rate: 0.00013413]
	Learning Rate: 0.00013413
	LOSS [training: 0.9391538120113936 | validation: 0.7660285550517875]
	TIME [epoch: 8.5 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9393266697441683		[learning rate: 0.00013364]
	Learning Rate: 0.000133643
	LOSS [training: 0.9393266697441683 | validation: 0.7530092416148809]
	TIME [epoch: 8.5 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9510667335101456		[learning rate: 0.00013316]
	Learning Rate: 0.000133158
	LOSS [training: 0.9510667335101456 | validation: 0.7760464815729882]
	TIME [epoch: 8.49 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9502160339034857		[learning rate: 0.00013268]
	Learning Rate: 0.000132675
	LOSS [training: 0.9502160339034857 | validation: 0.7463709104055928]
	TIME [epoch: 8.49 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9457522667489165		[learning rate: 0.00013219]
	Learning Rate: 0.000132194
	LOSS [training: 0.9457522667489165 | validation: 0.770210992090447]
	TIME [epoch: 8.51 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.938037336071384		[learning rate: 0.00013171]
	Learning Rate: 0.000131714
	LOSS [training: 0.938037336071384 | validation: 0.7570223414395751]
	TIME [epoch: 8.5 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9399780283889567		[learning rate: 0.00013124]
	Learning Rate: 0.000131236
	LOSS [training: 0.9399780283889567 | validation: 0.743677692310399]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1292.pth
	Model improved!!!
EPOCH 1293/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9569152238484584		[learning rate: 0.00013076]
	Learning Rate: 0.00013076
	LOSS [training: 0.9569152238484584 | validation: 0.7966517787210696]
	TIME [epoch: 8.53 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9877221513870481		[learning rate: 0.00013029]
	Learning Rate: 0.000130285
	LOSS [training: 0.9877221513870481 | validation: 0.7615134186312993]
	TIME [epoch: 8.52 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9539964685808627		[learning rate: 0.00012981]
	Learning Rate: 0.000129812
	LOSS [training: 0.9539964685808627 | validation: 0.749912888303049]
	TIME [epoch: 8.51 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9479483223732037		[learning rate: 0.00012934]
	Learning Rate: 0.000129341
	LOSS [training: 0.9479483223732037 | validation: 0.7420933691247513]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1296.pth
	Model improved!!!
EPOCH 1297/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.941148284079959		[learning rate: 0.00012887]
	Learning Rate: 0.000128872
	LOSS [training: 0.941148284079959 | validation: 0.7646142992562028]
	TIME [epoch: 8.53 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.942330671069586		[learning rate: 0.0001284]
	Learning Rate: 0.000128404
	LOSS [training: 0.942330671069586 | validation: 0.7577483842638046]
	TIME [epoch: 8.51 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9493178982149161		[learning rate: 0.00012794]
	Learning Rate: 0.000127938
	LOSS [training: 0.9493178982149161 | validation: 0.754111109939899]
	TIME [epoch: 8.51 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.939597406222552		[learning rate: 0.00012747]
	Learning Rate: 0.000127474
	LOSS [training: 0.939597406222552 | validation: 0.743071151218026]
	TIME [epoch: 8.52 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9507914402998413		[learning rate: 0.00012701]
	Learning Rate: 0.000127011
	LOSS [training: 0.9507914402998413 | validation: 0.7729762192978712]
	TIME [epoch: 8.52 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9663714547111255		[learning rate: 0.00012655]
	Learning Rate: 0.00012655
	LOSS [training: 0.9663714547111255 | validation: 0.7526804777351528]
	TIME [epoch: 8.5 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9571531739400809		[learning rate: 0.00012609]
	Learning Rate: 0.000126091
	LOSS [training: 0.9571531739400809 | validation: 0.755626987893009]
	TIME [epoch: 8.51 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9366855774141033		[learning rate: 0.00012563]
	Learning Rate: 0.000125633
	LOSS [training: 0.9366855774141033 | validation: 0.7429913109025701]
	TIME [epoch: 8.53 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9363691347157088		[learning rate: 0.00012518]
	Learning Rate: 0.000125178
	LOSS [training: 0.9363691347157088 | validation: 0.7473237910552293]
	TIME [epoch: 8.51 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9513620729492578		[learning rate: 0.00012472]
	Learning Rate: 0.000124723
	LOSS [training: 0.9513620729492578 | validation: 0.7617749797078289]
	TIME [epoch: 8.51 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9403075159064371		[learning rate: 0.00012427]
	Learning Rate: 0.000124271
	LOSS [training: 0.9403075159064371 | validation: 0.7466226769290081]
	TIME [epoch: 8.51 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9394614468605751		[learning rate: 0.00012382]
	Learning Rate: 0.00012382
	LOSS [training: 0.9394614468605751 | validation: 0.7596866248989576]
	TIME [epoch: 8.53 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9530368185053488		[learning rate: 0.00012337]
	Learning Rate: 0.00012337
	LOSS [training: 0.9530368185053488 | validation: 0.7469469734989119]
	TIME [epoch: 8.51 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9402893271159687		[learning rate: 0.00012292]
	Learning Rate: 0.000122923
	LOSS [training: 0.9402893271159687 | validation: 0.7764897442638344]
	TIME [epoch: 8.51 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9322877052375185		[learning rate: 0.00012248]
	Learning Rate: 0.000122476
	LOSS [training: 0.9322877052375185 | validation: 0.7574904876883856]
	TIME [epoch: 8.53 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.958162690153976		[learning rate: 0.00012203]
	Learning Rate: 0.000122032
	LOSS [training: 0.958162690153976 | validation: 0.7575507745892653]
	TIME [epoch: 8.51 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9331886405641102		[learning rate: 0.00012159]
	Learning Rate: 0.000121589
	LOSS [training: 0.9331886405641102 | validation: 0.757274989527112]
	TIME [epoch: 8.51 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9362486806107471		[learning rate: 0.00012115]
	Learning Rate: 0.000121148
	LOSS [training: 0.9362486806107471 | validation: 0.7691808914743493]
	TIME [epoch: 8.51 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9329858598847096		[learning rate: 0.00012071]
	Learning Rate: 0.000120708
	LOSS [training: 0.9329858598847096 | validation: 0.7513573392145756]
	TIME [epoch: 8.53 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9292848198673095		[learning rate: 0.00012027]
	Learning Rate: 0.00012027
	LOSS [training: 0.9292848198673095 | validation: 0.7464728398553607]
	TIME [epoch: 8.51 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9415916393980481		[learning rate: 0.00011983]
	Learning Rate: 0.000119834
	LOSS [training: 0.9415916393980481 | validation: 0.7499338031738585]
	TIME [epoch: 8.51 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9382077256669614		[learning rate: 0.0001194]
	Learning Rate: 0.000119399
	LOSS [training: 0.9382077256669614 | validation: 0.7524575563957905]
	TIME [epoch: 8.53 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9459873608128524		[learning rate: 0.00011897]
	Learning Rate: 0.000118966
	LOSS [training: 0.9459873608128524 | validation: 0.7496014014947162]
	TIME [epoch: 8.51 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9427266499488279		[learning rate: 0.00011853]
	Learning Rate: 0.000118534
	LOSS [training: 0.9427266499488279 | validation: 0.7463222565823315]
	TIME [epoch: 8.51 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9303108108195115		[learning rate: 0.0001181]
	Learning Rate: 0.000118104
	LOSS [training: 0.9303108108195115 | validation: 0.7530830749643389]
	TIME [epoch: 8.51 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9333481458191132		[learning rate: 0.00011767]
	Learning Rate: 0.000117675
	LOSS [training: 0.9333481458191132 | validation: 0.7587326923655328]
	TIME [epoch: 8.53 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9433697736507203		[learning rate: 0.00011725]
	Learning Rate: 0.000117248
	LOSS [training: 0.9433697736507203 | validation: 0.7477826280785597]
	TIME [epoch: 8.51 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9453683612570382		[learning rate: 0.00011682]
	Learning Rate: 0.000116822
	LOSS [training: 0.9453683612570382 | validation: 0.7565536676678318]
	TIME [epoch: 8.51 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.957205292637564		[learning rate: 0.0001164]
	Learning Rate: 0.000116398
	LOSS [training: 0.957205292637564 | validation: 0.7636880037871159]
	TIME [epoch: 8.53 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9395486586618128		[learning rate: 0.00011598]
	Learning Rate: 0.000115976
	LOSS [training: 0.9395486586618128 | validation: 0.7741279509022736]
	TIME [epoch: 8.51 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9492771953506909		[learning rate: 0.00011556]
	Learning Rate: 0.000115555
	LOSS [training: 0.9492771953506909 | validation: 0.7404770146354847]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1327.pth
	Model improved!!!
EPOCH 1328/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9337970712248345		[learning rate: 0.00011514]
	Learning Rate: 0.000115136
	LOSS [training: 0.9337970712248345 | validation: 0.7622039380419334]
	TIME [epoch: 8.51 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9374227884314681		[learning rate: 0.00011472]
	Learning Rate: 0.000114718
	LOSS [training: 0.9374227884314681 | validation: 0.7439483491918281]
	TIME [epoch: 8.52 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9457690427400604		[learning rate: 0.0001143]
	Learning Rate: 0.000114302
	LOSS [training: 0.9457690427400604 | validation: 0.7973584668540732]
	TIME [epoch: 8.51 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9826310761222073		[learning rate: 0.00011389]
	Learning Rate: 0.000113887
	LOSS [training: 0.9826310761222073 | validation: 0.7425382921422385]
	TIME [epoch: 8.51 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9346222600903499		[learning rate: 0.00011347]
	Learning Rate: 0.000113474
	LOSS [training: 0.9346222600903499 | validation: 0.7550281323487605]
	TIME [epoch: 8.53 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9469893664152254		[learning rate: 0.00011306]
	Learning Rate: 0.000113062
	LOSS [training: 0.9469893664152254 | validation: 0.7592313742535086]
	TIME [epoch: 8.51 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.951046841224492		[learning rate: 0.00011265]
	Learning Rate: 0.000112651
	LOSS [training: 0.951046841224492 | validation: 0.7543473168228525]
	TIME [epoch: 8.51 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9362937666164098		[learning rate: 0.00011224]
	Learning Rate: 0.000112243
	LOSS [training: 0.9362937666164098 | validation: 0.7556181764749268]
	TIME [epoch: 8.51 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9476512612714725		[learning rate: 0.00011184]
	Learning Rate: 0.000111835
	LOSS [training: 0.9476512612714725 | validation: 0.7446076487994452]
	TIME [epoch: 8.53 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9310345528305239		[learning rate: 0.00011143]
	Learning Rate: 0.000111429
	LOSS [training: 0.9310345528305239 | validation: 0.7537870431403367]
	TIME [epoch: 8.51 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9398696091800913		[learning rate: 0.00011103]
	Learning Rate: 0.000111025
	LOSS [training: 0.9398696091800913 | validation: 0.7455128470990389]
	TIME [epoch: 8.51 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9389219087541436		[learning rate: 0.00011062]
	Learning Rate: 0.000110622
	LOSS [training: 0.9389219087541436 | validation: 0.7455585382913524]
	TIME [epoch: 8.53 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9334505814910129		[learning rate: 0.00011022]
	Learning Rate: 0.000110221
	LOSS [training: 0.9334505814910129 | validation: 0.7536492931102413]
	TIME [epoch: 8.51 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9350631980898516		[learning rate: 0.00010982]
	Learning Rate: 0.000109821
	LOSS [training: 0.9350631980898516 | validation: 0.7548525791647778]
	TIME [epoch: 8.5 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9451532389583438		[learning rate: 0.00010942]
	Learning Rate: 0.000109422
	LOSS [training: 0.9451532389583438 | validation: 0.76991040288761]
	TIME [epoch: 8.51 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9368745247967398		[learning rate: 0.00010903]
	Learning Rate: 0.000109025
	LOSS [training: 0.9368745247967398 | validation: 0.7543195143099389]
	TIME [epoch: 8.53 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9350339568481731		[learning rate: 0.00010863]
	Learning Rate: 0.000108629
	LOSS [training: 0.9350339568481731 | validation: 0.7677782737678249]
	TIME [epoch: 8.5 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9344622207910197		[learning rate: 0.00010824]
	Learning Rate: 0.000108235
	LOSS [training: 0.9344622207910197 | validation: 0.7592364653217714]
	TIME [epoch: 8.51 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9303925692185195		[learning rate: 0.00010784]
	Learning Rate: 0.000107842
	LOSS [training: 0.9303925692185195 | validation: 0.7501998190485073]
	TIME [epoch: 8.52 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9395615176787917		[learning rate: 0.00010745]
	Learning Rate: 0.000107451
	LOSS [training: 0.9395615176787917 | validation: 0.7636101369841074]
	TIME [epoch: 8.54 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9393349244474194		[learning rate: 0.00010706]
	Learning Rate: 0.000107061
	LOSS [training: 0.9393349244474194 | validation: 0.7707833450609286]
	TIME [epoch: 8.51 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9516094886041685		[learning rate: 0.00010667]
	Learning Rate: 0.000106673
	LOSS [training: 0.9516094886041685 | validation: 0.7730606348366592]
	TIME [epoch: 8.51 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9394517019605478		[learning rate: 0.00010629]
	Learning Rate: 0.000106285
	LOSS [training: 0.9394517019605478 | validation: 0.7549922213880458]
	TIME [epoch: 8.53 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9413407657674908		[learning rate: 0.0001059]
	Learning Rate: 0.0001059
	LOSS [training: 0.9413407657674908 | validation: 0.7527417750940958]
	TIME [epoch: 8.51 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9339052818709744		[learning rate: 0.00010552]
	Learning Rate: 0.000105515
	LOSS [training: 0.9339052818709744 | validation: 0.745596655525155]
	TIME [epoch: 8.5 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9377818187428998		[learning rate: 0.00010513]
	Learning Rate: 0.000105132
	LOSS [training: 0.9377818187428998 | validation: 0.7435174034144958]
	TIME [epoch: 8.52 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9289114313992883		[learning rate: 0.00010475]
	Learning Rate: 0.000104751
	LOSS [training: 0.9289114313992883 | validation: 0.7495591410956111]
	TIME [epoch: 8.51 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9245493538497825		[learning rate: 0.00010437]
	Learning Rate: 0.000104371
	LOSS [training: 0.9245493538497825 | validation: 0.7584676030520078]
	TIME [epoch: 8.5 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9389126747839974		[learning rate: 0.00010399]
	Learning Rate: 0.000103992
	LOSS [training: 0.9389126747839974 | validation: 0.7628160722694708]
	TIME [epoch: 8.51 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9432920608010583		[learning rate: 0.00010361]
	Learning Rate: 0.000103615
	LOSS [training: 0.9432920608010583 | validation: 0.7451232499833846]
	TIME [epoch: 8.53 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9356341476557087		[learning rate: 0.00010324]
	Learning Rate: 0.000103239
	LOSS [training: 0.9356341476557087 | validation: 0.7433977862525996]
	TIME [epoch: 8.51 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9254129801842484		[learning rate: 0.00010286]
	Learning Rate: 0.000102864
	LOSS [training: 0.9254129801842484 | validation: 0.7717697459300543]
	TIME [epoch: 8.51 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9397133323744873		[learning rate: 0.00010249]
	Learning Rate: 0.000102491
	LOSS [training: 0.9397133323744873 | validation: 0.7674213912641177]
	TIME [epoch: 8.52 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9429375955659125		[learning rate: 0.00010212]
	Learning Rate: 0.000102119
	LOSS [training: 0.9429375955659125 | validation: 0.75833022202768]
	TIME [epoch: 8.51 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9432021016314186		[learning rate: 0.00010175]
	Learning Rate: 0.000101748
	LOSS [training: 0.9432021016314186 | validation: 0.7508750134902713]
	TIME [epoch: 8.51 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.93431381179973		[learning rate: 0.00010138]
	Learning Rate: 0.000101379
	LOSS [training: 0.93431381179973 | validation: 0.7480928743275543]
	TIME [epoch: 8.51 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9316762214385695		[learning rate: 0.00010101]
	Learning Rate: 0.000101011
	LOSS [training: 0.9316762214385695 | validation: 0.7486942785519038]
	TIME [epoch: 8.53 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9489542977322077		[learning rate: 0.00010064]
	Learning Rate: 0.000100644
	LOSS [training: 0.9489542977322077 | validation: 0.8194740388966999]
	TIME [epoch: 8.51 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9779908339223571		[learning rate: 0.00010028]
	Learning Rate: 0.000100279
	LOSS [training: 0.9779908339223571 | validation: 0.7757740405905615]
	TIME [epoch: 8.51 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9999891917878253		[learning rate: 9.9915e-05]
	Learning Rate: 9.99152e-05
	LOSS [training: 0.9999891917878253 | validation: 0.7546175396672585]
	TIME [epoch: 8.52 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9371767479204876		[learning rate: 9.9553e-05]
	Learning Rate: 9.95526e-05
	LOSS [training: 0.9371767479204876 | validation: 0.7444023835089428]
	TIME [epoch: 8.52 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9360108729952117		[learning rate: 9.9191e-05]
	Learning Rate: 9.91913e-05
	LOSS [training: 0.9360108729952117 | validation: 0.7629202933257269]
	TIME [epoch: 8.5 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9326537573964803		[learning rate: 9.8831e-05]
	Learning Rate: 9.88314e-05
	LOSS [training: 0.9326537573964803 | validation: 0.746556691594929]
	TIME [epoch: 8.5 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9294260285636968		[learning rate: 9.8473e-05]
	Learning Rate: 9.84727e-05
	LOSS [training: 0.9294260285636968 | validation: 0.7414269152111694]
	TIME [epoch: 8.52 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9351180662877028		[learning rate: 9.8115e-05]
	Learning Rate: 9.81153e-05
	LOSS [training: 0.9351180662877028 | validation: 0.7527955123835751]
	TIME [epoch: 8.51 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9456118548933304		[learning rate: 9.7759e-05]
	Learning Rate: 9.77592e-05
	LOSS [training: 0.9456118548933304 | validation: 0.7516353843515706]
	TIME [epoch: 8.5 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9500726487265082		[learning rate: 9.7404e-05]
	Learning Rate: 9.74045e-05
	LOSS [training: 0.9500726487265082 | validation: 0.7735603691747617]
	TIME [epoch: 8.51 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9464709226739861		[learning rate: 9.7051e-05]
	Learning Rate: 9.7051e-05
	LOSS [training: 0.9464709226739861 | validation: 0.7535537113818687]
	TIME [epoch: 8.52 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9347206510644025		[learning rate: 9.6699e-05]
	Learning Rate: 9.66988e-05
	LOSS [training: 0.9347206510644025 | validation: 0.7593039569087723]
	TIME [epoch: 8.51 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9307237639456332		[learning rate: 9.6348e-05]
	Learning Rate: 9.63479e-05
	LOSS [training: 0.9307237639456332 | validation: 0.75127871088946]
	TIME [epoch: 8.5 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.929889924759971		[learning rate: 9.5998e-05]
	Learning Rate: 9.59982e-05
	LOSS [training: 0.929889924759971 | validation: 0.7484321936226691]
	TIME [epoch: 8.53 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9393989979853707		[learning rate: 9.565e-05]
	Learning Rate: 9.56499e-05
	LOSS [training: 0.9393989979853707 | validation: 0.7585643188872087]
	TIME [epoch: 8.51 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.936729135562031		[learning rate: 9.5303e-05]
	Learning Rate: 9.53027e-05
	LOSS [training: 0.936729135562031 | validation: 0.7417735501652026]
	TIME [epoch: 8.51 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9247012063273541		[learning rate: 9.4957e-05]
	Learning Rate: 9.49569e-05
	LOSS [training: 0.9247012063273541 | validation: 0.755557645173302]
	TIME [epoch: 8.51 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9359783135041655		[learning rate: 9.4612e-05]
	Learning Rate: 9.46122e-05
	LOSS [training: 0.9359783135041655 | validation: 0.7593400726711984]
	TIME [epoch: 8.52 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9637419295968049		[learning rate: 9.4269e-05]
	Learning Rate: 9.42689e-05
	LOSS [training: 0.9637419295968049 | validation: 0.7713180986532313]
	TIME [epoch: 8.5 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9357887823002302		[learning rate: 9.3927e-05]
	Learning Rate: 9.39268e-05
	LOSS [training: 0.9357887823002302 | validation: 0.7500874836659406]
	TIME [epoch: 8.51 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9269989392258342		[learning rate: 9.3586e-05]
	Learning Rate: 9.35859e-05
	LOSS [training: 0.9269989392258342 | validation: 0.7594280080568597]
	TIME [epoch: 8.53 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9334300915418045		[learning rate: 9.3246e-05]
	Learning Rate: 9.32463e-05
	LOSS [training: 0.9334300915418045 | validation: 0.7509002350521118]
	TIME [epoch: 8.51 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9340014112331956		[learning rate: 9.2908e-05]
	Learning Rate: 9.29079e-05
	LOSS [training: 0.9340014112331956 | validation: 0.751972054473349]
	TIME [epoch: 8.5 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9308923680104593		[learning rate: 9.2571e-05]
	Learning Rate: 9.25707e-05
	LOSS [training: 0.9308923680104593 | validation: 0.755500205981382]
	TIME [epoch: 8.5 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9319583631651096		[learning rate: 9.2235e-05]
	Learning Rate: 9.22348e-05
	LOSS [training: 0.9319583631651096 | validation: 0.7608346968820154]
	TIME [epoch: 8.53 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9346555456674223		[learning rate: 9.19e-05]
	Learning Rate: 9.19001e-05
	LOSS [training: 0.9346555456674223 | validation: 0.7417733864038831]
	TIME [epoch: 8.51 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9198713239814686		[learning rate: 9.1567e-05]
	Learning Rate: 9.15665e-05
	LOSS [training: 0.9198713239814686 | validation: 0.7513448080819655]
	TIME [epoch: 8.5 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9246709581692499		[learning rate: 9.1234e-05]
	Learning Rate: 9.12343e-05
	LOSS [training: 0.9246709581692499 | validation: 0.7444845583310136]
	TIME [epoch: 8.52 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9299758005285976		[learning rate: 9.0903e-05]
	Learning Rate: 9.09031e-05
	LOSS [training: 0.9299758005285976 | validation: 0.7519218253269074]
	TIME [epoch: 8.5 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9274316321238814		[learning rate: 9.0573e-05]
	Learning Rate: 9.05733e-05
	LOSS [training: 0.9274316321238814 | validation: 0.756030152013228]
	TIME [epoch: 8.51 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9443359541449017		[learning rate: 9.0245e-05]
	Learning Rate: 9.02446e-05
	LOSS [training: 0.9443359541449017 | validation: 0.7403429802565807]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1395.pth
	Model improved!!!
EPOCH 1396/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.926697715217553		[learning rate: 8.9917e-05]
	Learning Rate: 8.99171e-05
	LOSS [training: 0.926697715217553 | validation: 0.7442976074917649]
	TIME [epoch: 8.53 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9292330560548268		[learning rate: 8.9591e-05]
	Learning Rate: 8.95908e-05
	LOSS [training: 0.9292330560548268 | validation: 0.7398801516655817]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1397.pth
	Model improved!!!
EPOCH 1398/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9363911571225427		[learning rate: 8.9266e-05]
	Learning Rate: 8.92656e-05
	LOSS [training: 0.9363911571225427 | validation: 0.7543506375760176]
	TIME [epoch: 8.51 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9287811950797848		[learning rate: 8.8942e-05]
	Learning Rate: 8.89417e-05
	LOSS [training: 0.9287811950797848 | validation: 0.749233875352391]
	TIME [epoch: 8.52 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9361202728346004		[learning rate: 8.8619e-05]
	Learning Rate: 8.86189e-05
	LOSS [training: 0.9361202728346004 | validation: 0.7504951219297885]
	TIME [epoch: 8.5 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9201772921614859		[learning rate: 8.8297e-05]
	Learning Rate: 8.82973e-05
	LOSS [training: 0.9201772921614859 | validation: 0.7435526928681396]
	TIME [epoch: 8.49 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9254282792109054		[learning rate: 8.7977e-05]
	Learning Rate: 8.79769e-05
	LOSS [training: 0.9254282792109054 | validation: 0.757780958246633]
	TIME [epoch: 8.49 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9258200968470061		[learning rate: 8.7658e-05]
	Learning Rate: 8.76576e-05
	LOSS [training: 0.9258200968470061 | validation: 0.7442201901505527]
	TIME [epoch: 8.52 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9291221941480947		[learning rate: 8.7339e-05]
	Learning Rate: 8.73395e-05
	LOSS [training: 0.9291221941480947 | validation: 0.7582146845459188]
	TIME [epoch: 8.49 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9303336959292668		[learning rate: 8.7023e-05]
	Learning Rate: 8.70225e-05
	LOSS [training: 0.9303336959292668 | validation: 0.7669554314436374]
	TIME [epoch: 8.5 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.925799167407968		[learning rate: 8.6707e-05]
	Learning Rate: 8.67067e-05
	LOSS [training: 0.925799167407968 | validation: 0.7515566395353245]
	TIME [epoch: 8.52 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9430398050387634		[learning rate: 8.6392e-05]
	Learning Rate: 8.63921e-05
	LOSS [training: 0.9430398050387634 | validation: 0.7467063102739288]
	TIME [epoch: 8.5 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.92479580706326		[learning rate: 8.6079e-05]
	Learning Rate: 8.60785e-05
	LOSS [training: 0.92479580706326 | validation: 0.748761488788027]
	TIME [epoch: 8.49 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.936762781234744		[learning rate: 8.5766e-05]
	Learning Rate: 8.57661e-05
	LOSS [training: 0.936762781234744 | validation: 0.7695113667324547]
	TIME [epoch: 8.49 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9649680906543227		[learning rate: 8.5455e-05]
	Learning Rate: 8.54549e-05
	LOSS [training: 0.9649680906543227 | validation: 0.7627601745192343]
	TIME [epoch: 8.51 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0043086491436721		[learning rate: 8.5145e-05]
	Learning Rate: 8.51448e-05
	LOSS [training: 1.0043086491436721 | validation: 0.7619321597980985]
	TIME [epoch: 8.5 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9497181548462128		[learning rate: 8.4836e-05]
	Learning Rate: 8.48358e-05
	LOSS [training: 0.9497181548462128 | validation: 0.7632906050512149]
	TIME [epoch: 8.49 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9307244935704924		[learning rate: 8.4528e-05]
	Learning Rate: 8.45279e-05
	LOSS [training: 0.9307244935704924 | validation: 0.7535356245731004]
	TIME [epoch: 8.51 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9329554143835814		[learning rate: 8.4221e-05]
	Learning Rate: 8.42211e-05
	LOSS [training: 0.9329554143835814 | validation: 0.7593156785997501]
	TIME [epoch: 8.5 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9258463640421579		[learning rate: 8.3916e-05]
	Learning Rate: 8.39155e-05
	LOSS [training: 0.9258463640421579 | validation: 0.7540767772474308]
	TIME [epoch: 8.49 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9235132742659452		[learning rate: 8.3611e-05]
	Learning Rate: 8.3611e-05
	LOSS [training: 0.9235132742659452 | validation: 0.7451554149047801]
	TIME [epoch: 8.5 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9251977555311406		[learning rate: 8.3308e-05]
	Learning Rate: 8.33075e-05
	LOSS [training: 0.9251977555311406 | validation: 0.7577846574704827]
	TIME [epoch: 8.52 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9296706245808609		[learning rate: 8.3005e-05]
	Learning Rate: 8.30052e-05
	LOSS [training: 0.9296706245808609 | validation: 0.7616321884559124]
	TIME [epoch: 8.51 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9322765990123385		[learning rate: 8.2704e-05]
	Learning Rate: 8.2704e-05
	LOSS [training: 0.9322765990123385 | validation: 0.7611037883902358]
	TIME [epoch: 8.5 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9358846715923329		[learning rate: 8.2404e-05]
	Learning Rate: 8.24038e-05
	LOSS [training: 0.9358846715923329 | validation: 0.754717235996301]
	TIME [epoch: 8.51 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9240349972756311		[learning rate: 8.2105e-05]
	Learning Rate: 8.21048e-05
	LOSS [training: 0.9240349972756311 | validation: 0.7538498942915965]
	TIME [epoch: 8.51 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9311991253171341		[learning rate: 8.1807e-05]
	Learning Rate: 8.18068e-05
	LOSS [training: 0.9311991253171341 | validation: 0.7557515057695557]
	TIME [epoch: 8.5 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9300790793363012		[learning rate: 8.151e-05]
	Learning Rate: 8.15099e-05
	LOSS [training: 0.9300790793363012 | validation: 0.7672667669034647]
	TIME [epoch: 8.49 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9285725203722361		[learning rate: 8.1214e-05]
	Learning Rate: 8.12142e-05
	LOSS [training: 0.9285725203722361 | validation: 0.7687104711748726]
	TIME [epoch: 8.52 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9227978453328853		[learning rate: 8.0919e-05]
	Learning Rate: 8.09194e-05
	LOSS [training: 0.9227978453328853 | validation: 0.7501282110539026]
	TIME [epoch: 8.5 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.920955615853531		[learning rate: 8.0626e-05]
	Learning Rate: 8.06257e-05
	LOSS [training: 0.920955615853531 | validation: 0.7497995946866475]
	TIME [epoch: 8.5 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9241330891524069		[learning rate: 8.0333e-05]
	Learning Rate: 8.03331e-05
	LOSS [training: 0.9241330891524069 | validation: 0.7451729946181861]
	TIME [epoch: 8.52 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9325844257006073		[learning rate: 8.0042e-05]
	Learning Rate: 8.00416e-05
	LOSS [training: 0.9325844257006073 | validation: 0.7573556601399494]
	TIME [epoch: 8.51 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9452692750666742		[learning rate: 7.9751e-05]
	Learning Rate: 7.97511e-05
	LOSS [training: 0.9452692750666742 | validation: 0.7513508302458598]
	TIME [epoch: 8.5 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9287077204281757		[learning rate: 7.9462e-05]
	Learning Rate: 7.94617e-05
	LOSS [training: 0.9287077204281757 | validation: 0.7368561564047453]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1430.pth
	Model improved!!!
EPOCH 1431/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9299830343833317		[learning rate: 7.9173e-05]
	Learning Rate: 7.91733e-05
	LOSS [training: 0.9299830343833317 | validation: 0.7581293125795215]
	TIME [epoch: 8.52 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.932417423595911		[learning rate: 7.8886e-05]
	Learning Rate: 7.8886e-05
	LOSS [training: 0.932417423595911 | validation: 0.7501259333482639]
	TIME [epoch: 8.49 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9292781469803447		[learning rate: 7.86e-05]
	Learning Rate: 7.85997e-05
	LOSS [training: 0.9292781469803447 | validation: 0.7618430758634497]
	TIME [epoch: 8.49 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9296831086327575		[learning rate: 7.8315e-05]
	Learning Rate: 7.83145e-05
	LOSS [training: 0.9296831086327575 | validation: 0.7631234669060973]
	TIME [epoch: 8.51 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9293553509509584		[learning rate: 7.803e-05]
	Learning Rate: 7.80303e-05
	LOSS [training: 0.9293553509509584 | validation: 0.758333590570724]
	TIME [epoch: 8.5 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9255560608202767		[learning rate: 7.7747e-05]
	Learning Rate: 7.77471e-05
	LOSS [training: 0.9255560608202767 | validation: 0.7555627298343038]
	TIME [epoch: 8.49 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9192436293485665		[learning rate: 7.7465e-05]
	Learning Rate: 7.7465e-05
	LOSS [training: 0.9192436293485665 | validation: 0.7544433177644172]
	TIME [epoch: 8.49 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9305778473095107		[learning rate: 7.7184e-05]
	Learning Rate: 7.71838e-05
	LOSS [training: 0.9305778473095107 | validation: 0.7644879974412429]
	TIME [epoch: 8.51 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9323983734429593		[learning rate: 7.6904e-05]
	Learning Rate: 7.69037e-05
	LOSS [training: 0.9323983734429593 | validation: 0.7652148511826924]
	TIME [epoch: 8.49 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9265964430651475		[learning rate: 7.6625e-05]
	Learning Rate: 7.66246e-05
	LOSS [training: 0.9265964430651475 | validation: 0.7627429048788481]
	TIME [epoch: 8.49 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9244186694675868		[learning rate: 7.6347e-05]
	Learning Rate: 7.63466e-05
	LOSS [training: 0.9244186694675868 | validation: 0.7596963799952585]
	TIME [epoch: 8.5 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9170916562411333		[learning rate: 7.607e-05]
	Learning Rate: 7.60695e-05
	LOSS [training: 0.9170916562411333 | validation: 0.7500660805996227]
	TIME [epoch: 8.49 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9213968064860321		[learning rate: 7.5793e-05]
	Learning Rate: 7.57935e-05
	LOSS [training: 0.9213968064860321 | validation: 0.756203343057942]
	TIME [epoch: 8.49 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9202890371994178		[learning rate: 7.5518e-05]
	Learning Rate: 7.55184e-05
	LOSS [training: 0.9202890371994178 | validation: 0.749316436532962]
	TIME [epoch: 8.49 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9243041809449911		[learning rate: 7.5244e-05]
	Learning Rate: 7.52443e-05
	LOSS [training: 0.9243041809449911 | validation: 0.7486616104208293]
	TIME [epoch: 8.5 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9178902515659273		[learning rate: 7.4971e-05]
	Learning Rate: 7.49712e-05
	LOSS [training: 0.9178902515659273 | validation: 0.7553555717357295]
	TIME [epoch: 8.49 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9174607167025112		[learning rate: 7.4699e-05]
	Learning Rate: 7.46992e-05
	LOSS [training: 0.9174607167025112 | validation: 0.7385983679383099]
	TIME [epoch: 8.49 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9169731274362046		[learning rate: 7.4428e-05]
	Learning Rate: 7.44281e-05
	LOSS [training: 0.9169731274362046 | validation: 0.7390412767318175]
	TIME [epoch: 8.5 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9294106296771284		[learning rate: 7.4158e-05]
	Learning Rate: 7.4158e-05
	LOSS [training: 0.9294106296771284 | validation: 0.7848146867369348]
	TIME [epoch: 8.5 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9304145736230499		[learning rate: 7.3889e-05]
	Learning Rate: 7.38889e-05
	LOSS [training: 0.9304145736230499 | validation: 0.7530075794646206]
	TIME [epoch: 8.49 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9235036821495008		[learning rate: 7.3621e-05]
	Learning Rate: 7.36207e-05
	LOSS [training: 0.9235036821495008 | validation: 0.760770126722263]
	TIME [epoch: 8.48 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9220627887517816		[learning rate: 7.3354e-05]
	Learning Rate: 7.33536e-05
	LOSS [training: 0.9220627887517816 | validation: 0.7532634997910722]
	TIME [epoch: 8.52 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.92223346800288		[learning rate: 7.3087e-05]
	Learning Rate: 7.30873e-05
	LOSS [training: 0.92223346800288 | validation: 0.7744660551217522]
	TIME [epoch: 8.5 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9461540112784765		[learning rate: 7.2822e-05]
	Learning Rate: 7.28221e-05
	LOSS [training: 0.9461540112784765 | validation: 0.7907012516338793]
	TIME [epoch: 8.49 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9444623569898845		[learning rate: 7.2558e-05]
	Learning Rate: 7.25578e-05
	LOSS [training: 0.9444623569898845 | validation: 0.7646878815262071]
	TIME [epoch: 8.5 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9236491143392571		[learning rate: 7.2295e-05]
	Learning Rate: 7.22945e-05
	LOSS [training: 0.9236491143392571 | validation: 0.7622314876686617]
	TIME [epoch: 8.51 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9272726017654142		[learning rate: 7.2032e-05]
	Learning Rate: 7.20321e-05
	LOSS [training: 0.9272726017654142 | validation: 0.7572899870288861]
	TIME [epoch: 8.49 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9236586663917035		[learning rate: 7.1771e-05]
	Learning Rate: 7.17707e-05
	LOSS [training: 0.9236586663917035 | validation: 0.7749048309870057]
	TIME [epoch: 8.49 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9228791980237491		[learning rate: 7.151e-05]
	Learning Rate: 7.15103e-05
	LOSS [training: 0.9228791980237491 | validation: 0.7485036923519082]
	TIME [epoch: 8.51 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.920745762194245		[learning rate: 7.1251e-05]
	Learning Rate: 7.12508e-05
	LOSS [training: 0.920745762194245 | validation: 0.7618018403838154]
	TIME [epoch: 8.5 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9261436759256629		[learning rate: 7.0992e-05]
	Learning Rate: 7.09922e-05
	LOSS [training: 0.9261436759256629 | validation: 0.7426223756660838]
	TIME [epoch: 8.48 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9198262399137814		[learning rate: 7.0735e-05]
	Learning Rate: 7.07345e-05
	LOSS [training: 0.9198262399137814 | validation: 0.7626945610565213]
	TIME [epoch: 8.49 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9228821059456079		[learning rate: 7.0478e-05]
	Learning Rate: 7.04778e-05
	LOSS [training: 0.9228821059456079 | validation: 0.7527359996082672]
	TIME [epoch: 8.51 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9229094455937862		[learning rate: 7.0222e-05]
	Learning Rate: 7.02221e-05
	LOSS [training: 0.9229094455937862 | validation: 0.753216095727174]
	TIME [epoch: 8.5 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.926268777361608		[learning rate: 6.9967e-05]
	Learning Rate: 6.99672e-05
	LOSS [training: 0.926268777361608 | validation: 0.7540866411476568]
	TIME [epoch: 8.49 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.91342329235713		[learning rate: 6.9713e-05]
	Learning Rate: 6.97133e-05
	LOSS [training: 0.91342329235713 | validation: 0.7562481692306983]
	TIME [epoch: 8.51 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9127851022423001		[learning rate: 6.946e-05]
	Learning Rate: 6.94603e-05
	LOSS [training: 0.9127851022423001 | validation: 0.7555583486391422]
	TIME [epoch: 8.51 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9214717279713449		[learning rate: 6.9208e-05]
	Learning Rate: 6.92083e-05
	LOSS [training: 0.9214717279713449 | validation: 0.7633232740976745]
	TIME [epoch: 8.5 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9295474475965388		[learning rate: 6.8957e-05]
	Learning Rate: 6.89571e-05
	LOSS [training: 0.9295474475965388 | validation: 0.7627765123281364]
	TIME [epoch: 8.5 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9175829013809957		[learning rate: 6.8707e-05]
	Learning Rate: 6.87068e-05
	LOSS [training: 0.9175829013809957 | validation: 0.7561742704066103]
	TIME [epoch: 8.51 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9337924175822601		[learning rate: 6.8457e-05]
	Learning Rate: 6.84575e-05
	LOSS [training: 0.9337924175822601 | validation: 0.7740521135464954]
	TIME [epoch: 8.49 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.947253374207348		[learning rate: 6.8209e-05]
	Learning Rate: 6.82091e-05
	LOSS [training: 0.947253374207348 | validation: 0.7683441350259164]
	TIME [epoch: 8.48 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9316207427356116		[learning rate: 6.7962e-05]
	Learning Rate: 6.79615e-05
	LOSS [training: 0.9316207427356116 | validation: 0.7539468195363298]
	TIME [epoch: 8.51 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9166691791809486		[learning rate: 6.7715e-05]
	Learning Rate: 6.77149e-05
	LOSS [training: 0.9166691791809486 | validation: 0.7684701959676117]
	TIME [epoch: 8.5 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9430919245529722		[learning rate: 6.7469e-05]
	Learning Rate: 6.74692e-05
	LOSS [training: 0.9430919245529722 | validation: 0.7740275672994277]
	TIME [epoch: 8.49 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.925821809277711		[learning rate: 6.7224e-05]
	Learning Rate: 6.72243e-05
	LOSS [training: 0.925821809277711 | validation: 0.7578432308969035]
	TIME [epoch: 8.5 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9362249447375044		[learning rate: 6.698e-05]
	Learning Rate: 6.69804e-05
	LOSS [training: 0.9362249447375044 | validation: 0.770408525641785]
	TIME [epoch: 8.51 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9363616363893346		[learning rate: 6.6737e-05]
	Learning Rate: 6.67373e-05
	LOSS [training: 0.9363616363893346 | validation: 0.7444815055723538]
	TIME [epoch: 8.49 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.918872923062913		[learning rate: 6.6495e-05]
	Learning Rate: 6.64951e-05
	LOSS [training: 0.918872923062913 | validation: 0.7629463445208708]
	TIME [epoch: 8.5 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9168958492778831		[learning rate: 6.6254e-05]
	Learning Rate: 6.62538e-05
	LOSS [training: 0.9168958492778831 | validation: 0.7495972337688549]
	TIME [epoch: 8.51 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9130280979192712		[learning rate: 6.6013e-05]
	Learning Rate: 6.60133e-05
	LOSS [training: 0.9130280979192712 | validation: 0.7555458570291511]
	TIME [epoch: 8.5 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9161413159665015		[learning rate: 6.5774e-05]
	Learning Rate: 6.57738e-05
	LOSS [training: 0.9161413159665015 | validation: 0.7491646757367212]
	TIME [epoch: 8.49 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9202026804484602		[learning rate: 6.5535e-05]
	Learning Rate: 6.55351e-05
	LOSS [training: 0.9202026804484602 | validation: 0.7520930064844149]
	TIME [epoch: 8.5 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.913524959526173		[learning rate: 6.5297e-05]
	Learning Rate: 6.52972e-05
	LOSS [training: 0.913524959526173 | validation: 0.7560436831518436]
	TIME [epoch: 8.51 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9162861818365696		[learning rate: 6.506e-05]
	Learning Rate: 6.50603e-05
	LOSS [training: 0.9162861818365696 | validation: 0.7710602094919573]
	TIME [epoch: 8.5 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9166274419785315		[learning rate: 6.4824e-05]
	Learning Rate: 6.48242e-05
	LOSS [training: 0.9166274419785315 | validation: 0.7574905230245748]
	TIME [epoch: 8.48 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9100947826132082		[learning rate: 6.4589e-05]
	Learning Rate: 6.45889e-05
	LOSS [training: 0.9100947826132082 | validation: 0.7420104288303786]
	TIME [epoch: 8.5 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9158009485782672		[learning rate: 6.4354e-05]
	Learning Rate: 6.43545e-05
	LOSS [training: 0.9158009485782672 | validation: 0.7560852295009818]
	TIME [epoch: 8.5 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9104129543522402		[learning rate: 6.4121e-05]
	Learning Rate: 6.4121e-05
	LOSS [training: 0.9104129543522402 | validation: 0.743322152286745]
	TIME [epoch: 8.49 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9115037763410887		[learning rate: 6.3888e-05]
	Learning Rate: 6.38883e-05
	LOSS [training: 0.9115037763410887 | validation: 0.758183673099821]
	TIME [epoch: 8.49 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9129299225002532		[learning rate: 6.3656e-05]
	Learning Rate: 6.36564e-05
	LOSS [training: 0.9129299225002532 | validation: 0.7684976510460517]
	TIME [epoch: 8.52 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9200699770044803		[learning rate: 6.3425e-05]
	Learning Rate: 6.34254e-05
	LOSS [training: 0.9200699770044803 | validation: 0.7589107000115989]
	TIME [epoch: 8.49 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9102954147227127		[learning rate: 6.3195e-05]
	Learning Rate: 6.31952e-05
	LOSS [training: 0.9102954147227127 | validation: 0.7450300097857772]
	TIME [epoch: 8.49 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9188812425216277		[learning rate: 6.2966e-05]
	Learning Rate: 6.29659e-05
	LOSS [training: 0.9188812425216277 | validation: 0.775347469855471]
	TIME [epoch: 8.51 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9349295847871893		[learning rate: 6.2737e-05]
	Learning Rate: 6.27374e-05
	LOSS [training: 0.9349295847871893 | validation: 0.7650874068960867]
	TIME [epoch: 8.51 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9113646608380221		[learning rate: 6.251e-05]
	Learning Rate: 6.25097e-05
	LOSS [training: 0.9113646608380221 | validation: 0.7595657584256176]
	TIME [epoch: 8.5 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9065170541271048		[learning rate: 6.2283e-05]
	Learning Rate: 6.22828e-05
	LOSS [training: 0.9065170541271048 | validation: 0.7754146536923423]
	TIME [epoch: 8.49 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9199526709319354		[learning rate: 6.2057e-05]
	Learning Rate: 6.20568e-05
	LOSS [training: 0.9199526709319354 | validation: 0.7782542064400105]
	TIME [epoch: 8.51 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9083696748560349		[learning rate: 6.1832e-05]
	Learning Rate: 6.18316e-05
	LOSS [training: 0.9083696748560349 | validation: 0.7624342345859414]
	TIME [epoch: 8.49 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.909203245758374		[learning rate: 6.1607e-05]
	Learning Rate: 6.16072e-05
	LOSS [training: 0.909203245758374 | validation: 0.7574793140406776]
	TIME [epoch: 8.49 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.908585162095261		[learning rate: 6.1384e-05]
	Learning Rate: 6.13836e-05
	LOSS [training: 0.908585162095261 | validation: 0.748788282236622]
	TIME [epoch: 8.5 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9106756627164266		[learning rate: 6.1161e-05]
	Learning Rate: 6.11609e-05
	LOSS [training: 0.9106756627164266 | validation: 0.7647301989353371]
	TIME [epoch: 8.52 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9058689639039865		[learning rate: 6.0939e-05]
	Learning Rate: 6.09389e-05
	LOSS [training: 0.9058689639039865 | validation: 0.759578091705485]
	TIME [epoch: 8.48 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9088554150891724		[learning rate: 6.0718e-05]
	Learning Rate: 6.07178e-05
	LOSS [training: 0.9088554150891724 | validation: 0.7716952436045545]
	TIME [epoch: 8.49 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9109507716757633		[learning rate: 6.0497e-05]
	Learning Rate: 6.04974e-05
	LOSS [training: 0.9109507716757633 | validation: 0.7753545187726356]
	TIME [epoch: 8.52 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9043871977000377		[learning rate: 6.0278e-05]
	Learning Rate: 6.02779e-05
	LOSS [training: 0.9043871977000377 | validation: 0.7694514528087942]
	TIME [epoch: 8.51 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9088044437982281		[learning rate: 6.0059e-05]
	Learning Rate: 6.00591e-05
	LOSS [training: 0.9088044437982281 | validation: 0.7639354583663498]
	TIME [epoch: 8.5 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9079264387952748		[learning rate: 5.9841e-05]
	Learning Rate: 5.98412e-05
	LOSS [training: 0.9079264387952748 | validation: 0.7717658010329453]
	TIME [epoch: 8.51 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9077031651763878		[learning rate: 5.9624e-05]
	Learning Rate: 5.9624e-05
	LOSS [training: 0.9077031651763878 | validation: 0.7734009816001234]
	TIME [epoch: 8.52 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9141080960770005		[learning rate: 5.9408e-05]
	Learning Rate: 5.94076e-05
	LOSS [training: 0.9141080960770005 | validation: 0.7818688084257506]
	TIME [epoch: 8.5 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9083857504221623		[learning rate: 5.9192e-05]
	Learning Rate: 5.9192e-05
	LOSS [training: 0.9083857504221623 | validation: 0.7629821743027895]
	TIME [epoch: 8.5 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9010959453075426		[learning rate: 5.8977e-05]
	Learning Rate: 5.89772e-05
	LOSS [training: 0.9010959453075426 | validation: 0.7682898753752659]
	TIME [epoch: 8.52 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8987932028245817		[learning rate: 5.8763e-05]
	Learning Rate: 5.87632e-05
	LOSS [training: 0.8987932028245817 | validation: 0.7683379603548806]
	TIME [epoch: 8.51 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9068996171500776		[learning rate: 5.855e-05]
	Learning Rate: 5.85499e-05
	LOSS [training: 0.9068996171500776 | validation: 0.7792154108053596]
	TIME [epoch: 8.49 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9029442990444752		[learning rate: 5.8337e-05]
	Learning Rate: 5.83374e-05
	LOSS [training: 0.9029442990444752 | validation: 0.7753320479158904]
	TIME [epoch: 8.5 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8976125548832314		[learning rate: 5.8126e-05]
	Learning Rate: 5.81257e-05
	LOSS [training: 0.8976125548832314 | validation: 0.7609949296104579]
	TIME [epoch: 8.53 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8967677813507132		[learning rate: 5.7915e-05]
	Learning Rate: 5.79148e-05
	LOSS [training: 0.8967677813507132 | validation: 0.7572797328077054]
	TIME [epoch: 8.51 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9137288719469827		[learning rate: 5.7705e-05]
	Learning Rate: 5.77046e-05
	LOSS [training: 0.9137288719469827 | validation: 0.7651994378549833]
	TIME [epoch: 8.49 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8988927197901619		[learning rate: 5.7495e-05]
	Learning Rate: 5.74952e-05
	LOSS [training: 0.8988927197901619 | validation: 0.7571041151144046]
	TIME [epoch: 8.51 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8996214119031587		[learning rate: 5.7287e-05]
	Learning Rate: 5.72866e-05
	LOSS [training: 0.8996214119031587 | validation: 0.7667494096299917]
	TIME [epoch: 8.5 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8957981788359227		[learning rate: 5.7079e-05]
	Learning Rate: 5.70787e-05
	LOSS [training: 0.8957981788359227 | validation: 0.7635955073357776]
	TIME [epoch: 8.5 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8982189288075292		[learning rate: 5.6872e-05]
	Learning Rate: 5.68715e-05
	LOSS [training: 0.8982189288075292 | validation: 0.7628841493204568]
	TIME [epoch: 8.51 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8958506979689004		[learning rate: 5.6665e-05]
	Learning Rate: 5.66651e-05
	LOSS [training: 0.8958506979689004 | validation: 0.7615522830951487]
	TIME [epoch: 8.52 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8974637195876193		[learning rate: 5.6459e-05]
	Learning Rate: 5.64595e-05
	LOSS [training: 0.8974637195876193 | validation: 0.7551636245857393]
	TIME [epoch: 8.5 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9019155227572909		[learning rate: 5.6255e-05]
	Learning Rate: 5.62546e-05
	LOSS [training: 0.9019155227572909 | validation: 0.7643657708084753]
	TIME [epoch: 8.5 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9163998111818245		[learning rate: 5.605e-05]
	Learning Rate: 5.60504e-05
	LOSS [training: 0.9163998111818245 | validation: 0.7909500954020086]
	TIME [epoch: 8.51 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9190342379274169		[learning rate: 5.5847e-05]
	Learning Rate: 5.5847e-05
	LOSS [training: 0.9190342379274169 | validation: 0.7606584895295215]
	TIME [epoch: 8.51 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9053849730538885		[learning rate: 5.5644e-05]
	Learning Rate: 5.56444e-05
	LOSS [training: 0.9053849730538885 | validation: 0.7696805844446906]
	TIME [epoch: 8.5 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8964588086019443		[learning rate: 5.5442e-05]
	Learning Rate: 5.54424e-05
	LOSS [training: 0.8964588086019443 | validation: 0.7627221867352831]
	TIME [epoch: 8.5 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8929756652557677		[learning rate: 5.5241e-05]
	Learning Rate: 5.52412e-05
	LOSS [training: 0.8929756652557677 | validation: 0.7599209723005637]
	TIME [epoch: 8.52 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8914765314452447		[learning rate: 5.5041e-05]
	Learning Rate: 5.50407e-05
	LOSS [training: 0.8914765314452447 | validation: 0.7570882076164787]
	TIME [epoch: 8.5 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8991413704789831		[learning rate: 5.4841e-05]
	Learning Rate: 5.4841e-05
	LOSS [training: 0.8991413704789831 | validation: 0.7764292011337923]
	TIME [epoch: 8.5 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.898238561433169		[learning rate: 5.4642e-05]
	Learning Rate: 5.4642e-05
	LOSS [training: 0.898238561433169 | validation: 0.7600092667242659]
	TIME [epoch: 8.52 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8927180649423108		[learning rate: 5.4444e-05]
	Learning Rate: 5.44437e-05
	LOSS [training: 0.8927180649423108 | validation: 0.7531608353185435]
	TIME [epoch: 8.5 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9007248829609376		[learning rate: 5.4246e-05]
	Learning Rate: 5.42461e-05
	LOSS [training: 0.9007248829609376 | validation: 0.767075611514694]
	TIME [epoch: 8.49 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9027475860959836		[learning rate: 5.4049e-05]
	Learning Rate: 5.40492e-05
	LOSS [training: 0.9027475860959836 | validation: 0.7633971278331788]
	TIME [epoch: 8.5 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8944765712141247		[learning rate: 5.3853e-05]
	Learning Rate: 5.38531e-05
	LOSS [training: 0.8944765712141247 | validation: 0.7668673633831659]
	TIME [epoch: 8.52 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9003678814418589		[learning rate: 5.3658e-05]
	Learning Rate: 5.36577e-05
	LOSS [training: 0.9003678814418589 | validation: 0.7595060181518686]
	TIME [epoch: 8.5 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8979990335462397		[learning rate: 5.3463e-05]
	Learning Rate: 5.34629e-05
	LOSS [training: 0.8979990335462397 | validation: 0.7619927557015345]
	TIME [epoch: 8.49 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.896976104526788		[learning rate: 5.3269e-05]
	Learning Rate: 5.32689e-05
	LOSS [training: 0.896976104526788 | validation: 0.7682051832170197]
	TIME [epoch: 8.51 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8957235163027649		[learning rate: 5.3076e-05]
	Learning Rate: 5.30756e-05
	LOSS [training: 0.8957235163027649 | validation: 0.7600108580992577]
	TIME [epoch: 8.51 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8918833190796962		[learning rate: 5.2883e-05]
	Learning Rate: 5.2883e-05
	LOSS [training: 0.8918833190796962 | validation: 0.7657803167667907]
	TIME [epoch: 8.5 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8925953405262852		[learning rate: 5.2691e-05]
	Learning Rate: 5.26911e-05
	LOSS [training: 0.8925953405262852 | validation: 0.7662921228847142]
	TIME [epoch: 8.49 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8947545805724653		[learning rate: 5.25e-05]
	Learning Rate: 5.24998e-05
	LOSS [training: 0.8947545805724653 | validation: 0.7550818946074449]
	TIME [epoch: 8.51 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8887179006586795		[learning rate: 5.2309e-05]
	Learning Rate: 5.23093e-05
	LOSS [training: 0.8887179006586795 | validation: 0.7583588345955992]
	TIME [epoch: 8.5 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8861547750416363		[learning rate: 5.2119e-05]
	Learning Rate: 5.21195e-05
	LOSS [training: 0.8861547750416363 | validation: 0.7708477965573483]
	TIME [epoch: 8.5 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8865092527455338		[learning rate: 5.193e-05]
	Learning Rate: 5.19303e-05
	LOSS [training: 0.8865092527455338 | validation: 0.7679734064977595]
	TIME [epoch: 8.49 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.885767117276683		[learning rate: 5.1742e-05]
	Learning Rate: 5.17419e-05
	LOSS [training: 0.885767117276683 | validation: 0.7645371595036087]
	TIME [epoch: 8.51 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8856649241358905		[learning rate: 5.1554e-05]
	Learning Rate: 5.15541e-05
	LOSS [training: 0.8856649241358905 | validation: 0.747551604558059]
	TIME [epoch: 8.48 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8874708273268139		[learning rate: 5.1367e-05]
	Learning Rate: 5.1367e-05
	LOSS [training: 0.8874708273268139 | validation: 0.7594305934042955]
	TIME [epoch: 8.49 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.893626346656782		[learning rate: 5.1181e-05]
	Learning Rate: 5.11806e-05
	LOSS [training: 0.893626346656782 | validation: 0.7538776763549595]
	TIME [epoch: 8.51 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8886437840651705		[learning rate: 5.0995e-05]
	Learning Rate: 5.09949e-05
	LOSS [training: 0.8886437840651705 | validation: 0.7676938927186601]
	TIME [epoch: 8.49 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.887630673672682		[learning rate: 5.081e-05]
	Learning Rate: 5.08098e-05
	LOSS [training: 0.887630673672682 | validation: 0.7598446938867122]
	TIME [epoch: 8.49 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8776851086126095		[learning rate: 5.0625e-05]
	Learning Rate: 5.06254e-05
	LOSS [training: 0.8776851086126095 | validation: 0.7490844111998669]
	TIME [epoch: 8.5 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8901941839524445		[learning rate: 5.0442e-05]
	Learning Rate: 5.04417e-05
	LOSS [training: 0.8901941839524445 | validation: 0.7666494400741464]
	TIME [epoch: 8.53 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8858356010958545		[learning rate: 5.0259e-05]
	Learning Rate: 5.02586e-05
	LOSS [training: 0.8858356010958545 | validation: 0.7603834852471474]
	TIME [epoch: 8.49 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8836413600566756		[learning rate: 5.0076e-05]
	Learning Rate: 5.00762e-05
	LOSS [training: 0.8836413600566756 | validation: 0.7588517372399184]
	TIME [epoch: 8.49 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8833793660853242		[learning rate: 4.9894e-05]
	Learning Rate: 4.98945e-05
	LOSS [training: 0.8833793660853242 | validation: 0.7524938158649318]
	TIME [epoch: 8.52 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8798012572848588		[learning rate: 4.9713e-05]
	Learning Rate: 4.97134e-05
	LOSS [training: 0.8798012572848588 | validation: 0.771854100645964]
	TIME [epoch: 8.5 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8859136512546171		[learning rate: 4.9533e-05]
	Learning Rate: 4.9533e-05
	LOSS [training: 0.8859136512546171 | validation: 0.7573157446365996]
	TIME [epoch: 8.49 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8800423805809888		[learning rate: 4.9353e-05]
	Learning Rate: 4.93533e-05
	LOSS [training: 0.8800423805809888 | validation: 0.7436664467932057]
	TIME [epoch: 8.5 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8743176080806748		[learning rate: 4.9174e-05]
	Learning Rate: 4.91742e-05
	LOSS [training: 0.8743176080806748 | validation: 0.7506589071628158]
	TIME [epoch: 8.52 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8780217834982367		[learning rate: 4.8996e-05]
	Learning Rate: 4.89957e-05
	LOSS [training: 0.8780217834982367 | validation: 0.764824014190066]
	TIME [epoch: 8.51 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8743248175529107		[learning rate: 4.8818e-05]
	Learning Rate: 4.88179e-05
	LOSS [training: 0.8743248175529107 | validation: 0.7585580645653878]
	TIME [epoch: 8.5 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8757177840569957		[learning rate: 4.8641e-05]
	Learning Rate: 4.86407e-05
	LOSS [training: 0.8757177840569957 | validation: 0.7524282265040627]
	TIME [epoch: 8.52 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8727546545199628		[learning rate: 4.8464e-05]
	Learning Rate: 4.84642e-05
	LOSS [training: 0.8727546545199628 | validation: 0.7674436042985783]
	TIME [epoch: 8.51 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8795294727670729		[learning rate: 4.8288e-05]
	Learning Rate: 4.82883e-05
	LOSS [training: 0.8795294727670729 | validation: 0.7629955391787934]
	TIME [epoch: 8.5 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8770238846779647		[learning rate: 4.8113e-05]
	Learning Rate: 4.81131e-05
	LOSS [training: 0.8770238846779647 | validation: 0.7513712485845274]
	TIME [epoch: 8.5 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8704141407603991		[learning rate: 4.7938e-05]
	Learning Rate: 4.79385e-05
	LOSS [training: 0.8704141407603991 | validation: 0.7615242829351357]
	TIME [epoch: 8.52 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8680396438889695		[learning rate: 4.7765e-05]
	Learning Rate: 4.77645e-05
	LOSS [training: 0.8680396438889695 | validation: 0.7547452994090481]
	TIME [epoch: 8.51 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8689055342270956		[learning rate: 4.7591e-05]
	Learning Rate: 4.75912e-05
	LOSS [training: 0.8689055342270956 | validation: 0.757445828941613]
	TIME [epoch: 8.51 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8649979793341769		[learning rate: 4.7418e-05]
	Learning Rate: 4.74185e-05
	LOSS [training: 0.8649979793341769 | validation: 0.7584953601472769]
	TIME [epoch: 8.52 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8695193976128657		[learning rate: 4.7246e-05]
	Learning Rate: 4.72464e-05
	LOSS [training: 0.8695193976128657 | validation: 0.7537608320559199]
	TIME [epoch: 8.51 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8668253655182419		[learning rate: 4.7075e-05]
	Learning Rate: 4.70749e-05
	LOSS [training: 0.8668253655182419 | validation: 0.7611376135728929]
	TIME [epoch: 8.49 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8749854554230081		[learning rate: 4.6904e-05]
	Learning Rate: 4.69041e-05
	LOSS [training: 0.8749854554230081 | validation: 0.7528862920236044]
	TIME [epoch: 8.5 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8640441437689921		[learning rate: 4.6734e-05]
	Learning Rate: 4.67339e-05
	LOSS [training: 0.8640441437689921 | validation: 0.758160671527319]
	TIME [epoch: 8.52 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.864623088309194		[learning rate: 4.6564e-05]
	Learning Rate: 4.65643e-05
	LOSS [training: 0.864623088309194 | validation: 0.7523717801034999]
	TIME [epoch: 8.49 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8699609761224922		[learning rate: 4.6395e-05]
	Learning Rate: 4.63953e-05
	LOSS [training: 0.8699609761224922 | validation: 0.75324992336886]
	TIME [epoch: 8.5 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8733432855359397		[learning rate: 4.6227e-05]
	Learning Rate: 4.62269e-05
	LOSS [training: 0.8733432855359397 | validation: 0.7557961248481592]
	TIME [epoch: 8.51 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8721613574524497		[learning rate: 4.6059e-05]
	Learning Rate: 4.60591e-05
	LOSS [training: 0.8721613574524497 | validation: 0.7578949971417763]
	TIME [epoch: 8.51 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8645403501101075		[learning rate: 4.5892e-05]
	Learning Rate: 4.5892e-05
	LOSS [training: 0.8645403501101075 | validation: 0.7569946761244299]
	TIME [epoch: 8.5 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8756046863710993		[learning rate: 4.5725e-05]
	Learning Rate: 4.57254e-05
	LOSS [training: 0.8756046863710993 | validation: 0.7605058340130968]
	TIME [epoch: 8.5 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8642412403131935		[learning rate: 4.556e-05]
	Learning Rate: 4.55595e-05
	LOSS [training: 0.8642412403131935 | validation: 0.762665235952196]
	TIME [epoch: 8.53 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8820475322970097		[learning rate: 4.5394e-05]
	Learning Rate: 4.53942e-05
	LOSS [training: 0.8820475322970097 | validation: 0.7730674109891069]
	TIME [epoch: 8.49 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.869778009731084		[learning rate: 4.5229e-05]
	Learning Rate: 4.52294e-05
	LOSS [training: 0.869778009731084 | validation: 0.7637907565893851]
	TIME [epoch: 8.49 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8750241463229417		[learning rate: 4.5065e-05]
	Learning Rate: 4.50653e-05
	LOSS [training: 0.8750241463229417 | validation: 0.7743233003076105]
	TIME [epoch: 8.5 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8747397637959164		[learning rate: 4.4902e-05]
	Learning Rate: 4.49017e-05
	LOSS [training: 0.8747397637959164 | validation: 0.7651000401799817]
	TIME [epoch: 8.5 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8648932198901637		[learning rate: 4.4739e-05]
	Learning Rate: 4.47388e-05
	LOSS [training: 0.8648932198901637 | validation: 0.7494647300695159]
	TIME [epoch: 8.49 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8616732406134122		[learning rate: 4.4576e-05]
	Learning Rate: 4.45764e-05
	LOSS [training: 0.8616732406134122 | validation: 0.7617556313935445]
	TIME [epoch: 8.49 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8645779148153909		[learning rate: 4.4415e-05]
	Learning Rate: 4.44147e-05
	LOSS [training: 0.8645779148153909 | validation: 0.7640131368111814]
	TIME [epoch: 8.51 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8764187304111271		[learning rate: 4.4253e-05]
	Learning Rate: 4.42535e-05
	LOSS [training: 0.8764187304111271 | validation: 0.7656338586582747]
	TIME [epoch: 8.49 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8591939003421543		[learning rate: 4.4093e-05]
	Learning Rate: 4.40929e-05
	LOSS [training: 0.8591939003421543 | validation: 0.7520355751474729]
	TIME [epoch: 8.49 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8635110509114806		[learning rate: 4.3933e-05]
	Learning Rate: 4.39329e-05
	LOSS [training: 0.8635110509114806 | validation: 0.7481269251308768]
	TIME [epoch: 8.51 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8547782216187061		[learning rate: 4.3773e-05]
	Learning Rate: 4.37734e-05
	LOSS [training: 0.8547782216187061 | validation: 0.7483634331928519]
	TIME [epoch: 8.51 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8608509778299235		[learning rate: 4.3615e-05]
	Learning Rate: 4.36146e-05
	LOSS [training: 0.8608509778299235 | validation: 0.7534717599272441]
	TIME [epoch: 8.5 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8523732196288416		[learning rate: 4.3456e-05]
	Learning Rate: 4.34563e-05
	LOSS [training: 0.8523732196288416 | validation: 0.7568819930606078]
	TIME [epoch: 8.5 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8615667902053457		[learning rate: 4.3299e-05]
	Learning Rate: 4.32986e-05
	LOSS [training: 0.8615667902053457 | validation: 0.7511455675616001]
	TIME [epoch: 8.52 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8578032778235007		[learning rate: 4.3141e-05]
	Learning Rate: 4.31415e-05
	LOSS [training: 0.8578032778235007 | validation: 0.7701913339794717]
	TIME [epoch: 8.5 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8609687143277274		[learning rate: 4.2985e-05]
	Learning Rate: 4.29849e-05
	LOSS [training: 0.8609687143277274 | validation: 0.751503566814506]
	TIME [epoch: 8.49 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8559135124425966		[learning rate: 4.2829e-05]
	Learning Rate: 4.28289e-05
	LOSS [training: 0.8559135124425966 | validation: 0.752172417742916]
	TIME [epoch: 8.51 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.855964859418893		[learning rate: 4.2673e-05]
	Learning Rate: 4.26735e-05
	LOSS [training: 0.855964859418893 | validation: 0.750129206065481]
	TIME [epoch: 8.52 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8676561031377238		[learning rate: 4.2519e-05]
	Learning Rate: 4.25186e-05
	LOSS [training: 0.8676561031377238 | validation: 0.7871793828415334]
	TIME [epoch: 8.5 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8750427800236767		[learning rate: 4.2364e-05]
	Learning Rate: 4.23643e-05
	LOSS [training: 0.8750427800236767 | validation: 0.7572728538035509]
	TIME [epoch: 8.5 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.855339283670361		[learning rate: 4.2211e-05]
	Learning Rate: 4.22106e-05
	LOSS [training: 0.855339283670361 | validation: 0.7495877481054606]
	TIME [epoch: 8.52 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8515324903098047		[learning rate: 4.2057e-05]
	Learning Rate: 4.20574e-05
	LOSS [training: 0.8515324903098047 | validation: 0.7457974180916572]
	TIME [epoch: 8.5 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8525885365948545		[learning rate: 4.1905e-05]
	Learning Rate: 4.19047e-05
	LOSS [training: 0.8525885365948545 | validation: 0.7564273032210358]
	TIME [epoch: 8.5 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8535765741728192		[learning rate: 4.1753e-05]
	Learning Rate: 4.17527e-05
	LOSS [training: 0.8535765741728192 | validation: 0.7538764061919744]
	TIME [epoch: 8.49 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8621486695926043		[learning rate: 4.1601e-05]
	Learning Rate: 4.16012e-05
	LOSS [training: 0.8621486695926043 | validation: 0.7494007376415384]
	TIME [epoch: 8.51 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8590361272998092		[learning rate: 4.145e-05]
	Learning Rate: 4.14502e-05
	LOSS [training: 0.8590361272998092 | validation: 0.7465413331124178]
	TIME [epoch: 8.51 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8498475140099613		[learning rate: 4.13e-05]
	Learning Rate: 4.12998e-05
	LOSS [training: 0.8498475140099613 | validation: 0.7559087110612346]
	TIME [epoch: 8.49 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8528394834052382		[learning rate: 4.115e-05]
	Learning Rate: 4.11499e-05
	LOSS [training: 0.8528394834052382 | validation: 0.7549790604096338]
	TIME [epoch: 8.52 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8580142504132746		[learning rate: 4.1001e-05]
	Learning Rate: 4.10005e-05
	LOSS [training: 0.8580142504132746 | validation: 0.7552568719838588]
	TIME [epoch: 8.5 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8540271061857384		[learning rate: 4.0852e-05]
	Learning Rate: 4.08517e-05
	LOSS [training: 0.8540271061857384 | validation: 0.7594185324283991]
	TIME [epoch: 8.5 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.849628631478357		[learning rate: 4.0703e-05]
	Learning Rate: 4.07035e-05
	LOSS [training: 0.849628631478357 | validation: 0.7554369172201015]
	TIME [epoch: 8.5 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.861114177567511		[learning rate: 4.0556e-05]
	Learning Rate: 4.05558e-05
	LOSS [training: 0.861114177567511 | validation: 0.7505854272812923]
	TIME [epoch: 8.52 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8679677721733665		[learning rate: 4.0409e-05]
	Learning Rate: 4.04086e-05
	LOSS [training: 0.8679677721733665 | validation: 0.7550112362977395]
	TIME [epoch: 8.49 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8691470698599097		[learning rate: 4.0262e-05]
	Learning Rate: 4.0262e-05
	LOSS [training: 0.8691470698599097 | validation: 0.7679045566930637]
	TIME [epoch: 8.51 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8759110268303708		[learning rate: 4.0116e-05]
	Learning Rate: 4.01158e-05
	LOSS [training: 0.8759110268303708 | validation: 0.7722911749230174]
	TIME [epoch: 8.51 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8730171274825377		[learning rate: 3.997e-05]
	Learning Rate: 3.99703e-05
	LOSS [training: 0.8730171274825377 | validation: 0.7511383963612819]
	TIME [epoch: 8.5 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8512135087337283		[learning rate: 3.9825e-05]
	Learning Rate: 3.98252e-05
	LOSS [training: 0.8512135087337283 | validation: 0.7619876215641185]
	TIME [epoch: 8.49 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8604372582359009		[learning rate: 3.9681e-05]
	Learning Rate: 3.96807e-05
	LOSS [training: 0.8604372582359009 | validation: 0.7528564871872863]
	TIME [epoch: 8.5 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8541991237801282		[learning rate: 3.9537e-05]
	Learning Rate: 3.95367e-05
	LOSS [training: 0.8541991237801282 | validation: 0.7618183326875744]
	TIME [epoch: 8.53 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8577997317355164		[learning rate: 3.9393e-05]
	Learning Rate: 3.93932e-05
	LOSS [training: 0.8577997317355164 | validation: 0.7630097149661825]
	TIME [epoch: 8.5 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8493032357121288		[learning rate: 3.925e-05]
	Learning Rate: 3.92502e-05
	LOSS [training: 0.8493032357121288 | validation: 0.7642166919884831]
	TIME [epoch: 8.51 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8498394141312783		[learning rate: 3.9108e-05]
	Learning Rate: 3.91078e-05
	LOSS [training: 0.8498394141312783 | validation: 0.7554853393170229]
	TIME [epoch: 8.52 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8527397027140801		[learning rate: 3.8966e-05]
	Learning Rate: 3.89659e-05
	LOSS [training: 0.8527397027140801 | validation: 0.7560954446508492]
	TIME [epoch: 8.51 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8545794064683212		[learning rate: 3.8824e-05]
	Learning Rate: 3.88245e-05
	LOSS [training: 0.8545794064683212 | validation: 0.7467066662421304]
	TIME [epoch: 8.5 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8462170163190281		[learning rate: 3.8684e-05]
	Learning Rate: 3.86836e-05
	LOSS [training: 0.8462170163190281 | validation: 0.7461376649819469]
	TIME [epoch: 8.51 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8497414835642403		[learning rate: 3.8543e-05]
	Learning Rate: 3.85432e-05
	LOSS [training: 0.8497414835642403 | validation: 0.7544090769499789]
	TIME [epoch: 8.52 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8423877453577815		[learning rate: 3.8403e-05]
	Learning Rate: 3.84033e-05
	LOSS [training: 0.8423877453577815 | validation: 0.7355984761889003]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1630.pth
	Model improved!!!
EPOCH 1631/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8406141379097726		[learning rate: 3.8264e-05]
	Learning Rate: 3.82639e-05
	LOSS [training: 0.8406141379097726 | validation: 0.7463500823039022]
	TIME [epoch: 8.5 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8479285820887993		[learning rate: 3.8125e-05]
	Learning Rate: 3.81251e-05
	LOSS [training: 0.8479285820887993 | validation: 0.7453035806357405]
	TIME [epoch: 8.53 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8432329869939114		[learning rate: 3.7987e-05]
	Learning Rate: 3.79867e-05
	LOSS [training: 0.8432329869939114 | validation: 0.7438575997176171]
	TIME [epoch: 8.5 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8439089333716279		[learning rate: 3.7849e-05]
	Learning Rate: 3.78489e-05
	LOSS [training: 0.8439089333716279 | validation: 0.7341792504805048]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1634.pth
	Model improved!!!
EPOCH 1635/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8523683878236697		[learning rate: 3.7711e-05]
	Learning Rate: 3.77115e-05
	LOSS [training: 0.8523683878236697 | validation: 0.7493101333367177]
	TIME [epoch: 8.5 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8504768583674153		[learning rate: 3.7575e-05]
	Learning Rate: 3.75746e-05
	LOSS [training: 0.8504768583674153 | validation: 0.7614100101900853]
	TIME [epoch: 8.53 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8453952333981517		[learning rate: 3.7438e-05]
	Learning Rate: 3.74383e-05
	LOSS [training: 0.8453952333981517 | validation: 0.7557091816939919]
	TIME [epoch: 8.5 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8618883199426589		[learning rate: 3.7302e-05]
	Learning Rate: 3.73024e-05
	LOSS [training: 0.8618883199426589 | validation: 0.769089808939442]
	TIME [epoch: 8.5 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8591036516660606		[learning rate: 3.7167e-05]
	Learning Rate: 3.7167e-05
	LOSS [training: 0.8591036516660606 | validation: 0.751160049843187]
	TIME [epoch: 8.52 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8477176680987455		[learning rate: 3.7032e-05]
	Learning Rate: 3.70322e-05
	LOSS [training: 0.8477176680987455 | validation: 0.7433378703918255]
	TIME [epoch: 8.5 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8403451446999615		[learning rate: 3.6898e-05]
	Learning Rate: 3.68978e-05
	LOSS [training: 0.8403451446999615 | validation: 0.7449616207431693]
	TIME [epoch: 8.5 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8456149343074681		[learning rate: 3.6764e-05]
	Learning Rate: 3.67639e-05
	LOSS [training: 0.8456149343074681 | validation: 0.7435435475963723]
	TIME [epoch: 8.5 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8428366627230852		[learning rate: 3.663e-05]
	Learning Rate: 3.66304e-05
	LOSS [training: 0.8428366627230852 | validation: 0.7468885348246598]
	TIME [epoch: 8.51 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8434083583821549		[learning rate: 3.6498e-05]
	Learning Rate: 3.64975e-05
	LOSS [training: 0.8434083583821549 | validation: 0.7517578189776113]
	TIME [epoch: 8.5 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8447035601960551		[learning rate: 3.6365e-05]
	Learning Rate: 3.63651e-05
	LOSS [training: 0.8447035601960551 | validation: 0.7552222145504114]
	TIME [epoch: 8.51 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8418267410304825		[learning rate: 3.6233e-05]
	Learning Rate: 3.62331e-05
	LOSS [training: 0.8418267410304825 | validation: 0.738266936849923]
	TIME [epoch: 8.51 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8388074644078978		[learning rate: 3.6102e-05]
	Learning Rate: 3.61016e-05
	LOSS [training: 0.8388074644078978 | validation: 0.7491023691873606]
	TIME [epoch: 8.5 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.871628843650684		[learning rate: 3.5971e-05]
	Learning Rate: 3.59706e-05
	LOSS [training: 0.871628843650684 | validation: 0.7792135236368665]
	TIME [epoch: 8.5 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8627154161516056		[learning rate: 3.584e-05]
	Learning Rate: 3.584e-05
	LOSS [training: 0.8627154161516056 | validation: 0.7345634175720616]
	TIME [epoch: 8.5 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8397685688433327		[learning rate: 3.571e-05]
	Learning Rate: 3.571e-05
	LOSS [training: 0.8397685688433327 | validation: 0.7454834833601073]
	TIME [epoch: 8.52 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.842341054865295		[learning rate: 3.558e-05]
	Learning Rate: 3.55804e-05
	LOSS [training: 0.842341054865295 | validation: 0.7442884289755133]
	TIME [epoch: 8.49 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8513144275584287		[learning rate: 3.5451e-05]
	Learning Rate: 3.54513e-05
	LOSS [training: 0.8513144275584287 | validation: 0.7454566582997993]
	TIME [epoch: 8.49 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8414926791154558		[learning rate: 3.5323e-05]
	Learning Rate: 3.53226e-05
	LOSS [training: 0.8414926791154558 | validation: 0.7522113278928605]
	TIME [epoch: 8.52 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.840720764100029		[learning rate: 3.5194e-05]
	Learning Rate: 3.51944e-05
	LOSS [training: 0.840720764100029 | validation: 0.7359585254615112]
	TIME [epoch: 8.5 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8429087611808608		[learning rate: 3.5067e-05]
	Learning Rate: 3.50667e-05
	LOSS [training: 0.8429087611808608 | validation: 0.7437522554231952]
	TIME [epoch: 8.5 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8349725669918184		[learning rate: 3.4939e-05]
	Learning Rate: 3.49394e-05
	LOSS [training: 0.8349725669918184 | validation: 0.7327709860869175]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1656.pth
	Model improved!!!
EPOCH 1657/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8370591243443817		[learning rate: 3.4813e-05]
	Learning Rate: 3.48126e-05
	LOSS [training: 0.8370591243443817 | validation: 0.7370440929699145]
	TIME [epoch: 8.53 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8430774977076778		[learning rate: 3.4686e-05]
	Learning Rate: 3.46863e-05
	LOSS [training: 0.8430774977076778 | validation: 0.7438838309208954]
	TIME [epoch: 8.5 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8397573391509228		[learning rate: 3.456e-05]
	Learning Rate: 3.45604e-05
	LOSS [training: 0.8397573391509228 | validation: 0.7457954323533489]
	TIME [epoch: 8.5 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.855405734080627		[learning rate: 3.4435e-05]
	Learning Rate: 3.4435e-05
	LOSS [training: 0.855405734080627 | validation: 0.7564181615544596]
	TIME [epoch: 8.51 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8412701424047416		[learning rate: 3.431e-05]
	Learning Rate: 3.431e-05
	LOSS [training: 0.8412701424047416 | validation: 0.74567410926918]
	TIME [epoch: 8.5 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8391976149324432		[learning rate: 3.4186e-05]
	Learning Rate: 3.41855e-05
	LOSS [training: 0.8391976149324432 | validation: 0.7309946773094149]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1662.pth
	Model improved!!!
EPOCH 1663/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8391831088979368		[learning rate: 3.4061e-05]
	Learning Rate: 3.40615e-05
	LOSS [training: 0.8391831088979368 | validation: 0.7458751564017466]
	TIME [epoch: 8.51 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8399546979398348		[learning rate: 3.3938e-05]
	Learning Rate: 3.39378e-05
	LOSS [training: 0.8399546979398348 | validation: 0.7425872560754058]
	TIME [epoch: 8.53 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8348823696263368		[learning rate: 3.3815e-05]
	Learning Rate: 3.38147e-05
	LOSS [training: 0.8348823696263368 | validation: 0.7462606126189443]
	TIME [epoch: 8.5 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.830364694726655		[learning rate: 3.3692e-05]
	Learning Rate: 3.3692e-05
	LOSS [training: 0.830364694726655 | validation: 0.7334388322579417]
	TIME [epoch: 8.49 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8361051644688795		[learning rate: 3.357e-05]
	Learning Rate: 3.35697e-05
	LOSS [training: 0.8361051644688795 | validation: 0.7392307388192457]
	TIME [epoch: 8.52 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8359394401461893		[learning rate: 3.3448e-05]
	Learning Rate: 3.34479e-05
	LOSS [training: 0.8359394401461893 | validation: 0.74447582134707]
	TIME [epoch: 8.52 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.842287541751164		[learning rate: 3.3326e-05]
	Learning Rate: 3.33265e-05
	LOSS [training: 0.842287541751164 | validation: 0.7358644439500074]
	TIME [epoch: 8.5 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8354528210583488		[learning rate: 3.3206e-05]
	Learning Rate: 3.32055e-05
	LOSS [training: 0.8354528210583488 | validation: 0.7408653491024318]
	TIME [epoch: 8.5 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8337991141070498		[learning rate: 3.3085e-05]
	Learning Rate: 3.3085e-05
	LOSS [training: 0.8337991141070498 | validation: 0.7476219811994352]
	TIME [epoch: 8.53 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8370390534275012		[learning rate: 3.2965e-05]
	Learning Rate: 3.2965e-05
	LOSS [training: 0.8370390534275012 | validation: 0.7380020478424063]
	TIME [epoch: 8.51 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8374464650388754		[learning rate: 3.2845e-05]
	Learning Rate: 3.28453e-05
	LOSS [training: 0.8374464650388754 | validation: 0.7389362136499721]
	TIME [epoch: 8.5 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8355913235195388		[learning rate: 3.2726e-05]
	Learning Rate: 3.27261e-05
	LOSS [training: 0.8355913235195388 | validation: 0.7505644939273296]
	TIME [epoch: 8.53 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8390493190742001		[learning rate: 3.2607e-05]
	Learning Rate: 3.26074e-05
	LOSS [training: 0.8390493190742001 | validation: 0.7414340797268673]
	TIME [epoch: 8.51 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8305994524648712		[learning rate: 3.2489e-05]
	Learning Rate: 3.2489e-05
	LOSS [training: 0.8305994524648712 | validation: 0.7407067898871224]
	TIME [epoch: 8.5 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8344411738601988		[learning rate: 3.2371e-05]
	Learning Rate: 3.23711e-05
	LOSS [training: 0.8344411738601988 | validation: 0.7553805092430866]
	TIME [epoch: 8.51 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8288937245707168		[learning rate: 3.2254e-05]
	Learning Rate: 3.22537e-05
	LOSS [training: 0.8288937245707168 | validation: 0.7334845741667593]
	TIME [epoch: 8.52 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8332185087357404		[learning rate: 3.2137e-05]
	Learning Rate: 3.21366e-05
	LOSS [training: 0.8332185087357404 | validation: 0.7294639946744559]
	TIME [epoch: 8.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1679.pth
	Model improved!!!
EPOCH 1680/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8332260356480063		[learning rate: 3.202e-05]
	Learning Rate: 3.202e-05
	LOSS [training: 0.8332260356480063 | validation: 0.7372622593702234]
	TIME [epoch: 8.5 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8364908231912918		[learning rate: 3.1904e-05]
	Learning Rate: 3.19038e-05
	LOSS [training: 0.8364908231912918 | validation: 0.7446862285388919]
	TIME [epoch: 8.52 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8308885063912385		[learning rate: 3.1788e-05]
	Learning Rate: 3.1788e-05
	LOSS [training: 0.8308885063912385 | validation: 0.7269849895902557]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1682.pth
	Model improved!!!
EPOCH 1683/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8399887287240304		[learning rate: 3.1673e-05]
	Learning Rate: 3.16726e-05
	LOSS [training: 0.8399887287240304 | validation: 0.7359609542696868]
	TIME [epoch: 8.5 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8309514706069342		[learning rate: 3.1558e-05]
	Learning Rate: 3.15577e-05
	LOSS [training: 0.8309514706069342 | validation: 0.7346381158007012]
	TIME [epoch: 8.5 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8269784652257632		[learning rate: 3.1443e-05]
	Learning Rate: 3.14432e-05
	LOSS [training: 0.8269784652257632 | validation: 0.7203771252807887]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1685.pth
	Model improved!!!
EPOCH 1686/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8328591754715982		[learning rate: 3.1329e-05]
	Learning Rate: 3.13291e-05
	LOSS [training: 0.8328591754715982 | validation: 0.7342363190499777]
	TIME [epoch: 8.51 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8312490095814493		[learning rate: 3.1215e-05]
	Learning Rate: 3.12154e-05
	LOSS [training: 0.8312490095814493 | validation: 0.7297819054981034]
	TIME [epoch: 8.52 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8295045380490423		[learning rate: 3.1102e-05]
	Learning Rate: 3.11021e-05
	LOSS [training: 0.8295045380490423 | validation: 0.7273884599543506]
	TIME [epoch: 8.53 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8317294814296264		[learning rate: 3.0989e-05]
	Learning Rate: 3.09892e-05
	LOSS [training: 0.8317294814296264 | validation: 0.7272657645881749]
	TIME [epoch: 8.52 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8327584222497479		[learning rate: 3.0877e-05]
	Learning Rate: 3.08768e-05
	LOSS [training: 0.8327584222497479 | validation: 0.7300743062009105]
	TIME [epoch: 8.51 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8330545501698662		[learning rate: 3.0765e-05]
	Learning Rate: 3.07647e-05
	LOSS [training: 0.8330545501698662 | validation: 0.7362167777138965]
	TIME [epoch: 8.52 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.832755718062578		[learning rate: 3.0653e-05]
	Learning Rate: 3.0653e-05
	LOSS [training: 0.832755718062578 | validation: 0.7213209683811371]
	TIME [epoch: 8.54 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8262953688530186		[learning rate: 3.0542e-05]
	Learning Rate: 3.05418e-05
	LOSS [training: 0.8262953688530186 | validation: 0.7135079484795743]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1693.pth
	Model improved!!!
EPOCH 1694/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8279789254163928		[learning rate: 3.0431e-05]
	Learning Rate: 3.0431e-05
	LOSS [training: 0.8279789254163928 | validation: 0.7176831807516276]
	TIME [epoch: 8.51 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8303863070096277		[learning rate: 3.0321e-05]
	Learning Rate: 3.03205e-05
	LOSS [training: 0.8303863070096277 | validation: 0.7278423733137424]
	TIME [epoch: 8.53 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8289754110151517		[learning rate: 3.0211e-05]
	Learning Rate: 3.02105e-05
	LOSS [training: 0.8289754110151517 | validation: 0.7094965493092819]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1696.pth
	Model improved!!!
EPOCH 1697/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8285844430576654		[learning rate: 3.0101e-05]
	Learning Rate: 3.01009e-05
	LOSS [training: 0.8285844430576654 | validation: 0.7206837564388986]
	TIME [epoch: 8.51 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8304689685907276		[learning rate: 2.9992e-05]
	Learning Rate: 2.99916e-05
	LOSS [training: 0.8304689685907276 | validation: 0.7361255599192482]
	TIME [epoch: 8.51 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8310330337843279		[learning rate: 2.9883e-05]
	Learning Rate: 2.98828e-05
	LOSS [training: 0.8310330337843279 | validation: 0.7292926278196561]
	TIME [epoch: 8.54 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8242030548950954		[learning rate: 2.9774e-05]
	Learning Rate: 2.97743e-05
	LOSS [training: 0.8242030548950954 | validation: 0.7195692968741938]
	TIME [epoch: 8.52 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8267927394911607		[learning rate: 2.9666e-05]
	Learning Rate: 2.96663e-05
	LOSS [training: 0.8267927394911607 | validation: 0.7288553941622726]
	TIME [epoch: 8.52 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8325498060718932		[learning rate: 2.9559e-05]
	Learning Rate: 2.95586e-05
	LOSS [training: 0.8325498060718932 | validation: 0.7102902767008218]
	TIME [epoch: 8.53 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8235390332925568		[learning rate: 2.9451e-05]
	Learning Rate: 2.94514e-05
	LOSS [training: 0.8235390332925568 | validation: 0.7241372530493336]
	TIME [epoch: 8.52 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8243840458035139		[learning rate: 2.9344e-05]
	Learning Rate: 2.93445e-05
	LOSS [training: 0.8243840458035139 | validation: 0.7172338421660525]
	TIME [epoch: 8.52 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.828283788492843		[learning rate: 2.9238e-05]
	Learning Rate: 2.9238e-05
	LOSS [training: 0.828283788492843 | validation: 0.7216670356018648]
	TIME [epoch: 8.52 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8268320472476359		[learning rate: 2.9132e-05]
	Learning Rate: 2.91319e-05
	LOSS [training: 0.8268320472476359 | validation: 0.7221863448623209]
	TIME [epoch: 8.54 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.821181375688352		[learning rate: 2.9026e-05]
	Learning Rate: 2.90262e-05
	LOSS [training: 0.821181375688352 | validation: 0.7127532358308657]
	TIME [epoch: 8.51 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8259361824307423		[learning rate: 2.8921e-05]
	Learning Rate: 2.89208e-05
	LOSS [training: 0.8259361824307423 | validation: 0.7233397875176814]
	TIME [epoch: 8.51 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8229648938690989		[learning rate: 2.8816e-05]
	Learning Rate: 2.88159e-05
	LOSS [training: 0.8229648938690989 | validation: 0.7221903881960798]
	TIME [epoch: 8.52 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8243197035947363		[learning rate: 2.8711e-05]
	Learning Rate: 2.87113e-05
	LOSS [training: 0.8243197035947363 | validation: 0.7179270379557969]
	TIME [epoch: 8.52 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8186262448553503		[learning rate: 2.8607e-05]
	Learning Rate: 2.86071e-05
	LOSS [training: 0.8186262448553503 | validation: 0.7213548238186539]
	TIME [epoch: 8.52 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.819864413776679		[learning rate: 2.8503e-05]
	Learning Rate: 2.85033e-05
	LOSS [training: 0.819864413776679 | validation: 0.7155432459904001]
	TIME [epoch: 8.51 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8251676515318745		[learning rate: 2.84e-05]
	Learning Rate: 2.83998e-05
	LOSS [training: 0.8251676515318745 | validation: 0.7138216720549084]
	TIME [epoch: 8.53 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8233425719876217		[learning rate: 2.8297e-05]
	Learning Rate: 2.82968e-05
	LOSS [training: 0.8233425719876217 | validation: 0.7242134921856792]
	TIME [epoch: 8.51 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8274831686316364		[learning rate: 2.8194e-05]
	Learning Rate: 2.81941e-05
	LOSS [training: 0.8274831686316364 | validation: 0.7301097355426047]
	TIME [epoch: 8.51 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8249157524839233		[learning rate: 2.8092e-05]
	Learning Rate: 2.80918e-05
	LOSS [training: 0.8249157524839233 | validation: 0.7094638025634551]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1716.pth
	Model improved!!!
EPOCH 1717/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8232531962808558		[learning rate: 2.799e-05]
	Learning Rate: 2.79898e-05
	LOSS [training: 0.8232531962808558 | validation: 0.7061733050155167]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1717.pth
	Model improved!!!
EPOCH 1718/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8240367521162482		[learning rate: 2.7888e-05]
	Learning Rate: 2.78882e-05
	LOSS [training: 0.8240367521162482 | validation: 0.7190857983601812]
	TIME [epoch: 8.51 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8253127782605574		[learning rate: 2.7787e-05]
	Learning Rate: 2.7787e-05
	LOSS [training: 0.8253127782605574 | validation: 0.7123876665408739]
	TIME [epoch: 8.51 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8230941469503386		[learning rate: 2.7686e-05]
	Learning Rate: 2.76862e-05
	LOSS [training: 0.8230941469503386 | validation: 0.7253707047908882]
	TIME [epoch: 8.53 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8237337279537462		[learning rate: 2.7586e-05]
	Learning Rate: 2.75857e-05
	LOSS [training: 0.8237337279537462 | validation: 0.7045975858960247]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1721.pth
	Model improved!!!
EPOCH 1722/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8217565628386809		[learning rate: 2.7486e-05]
	Learning Rate: 2.74856e-05
	LOSS [training: 0.8217565628386809 | validation: 0.7111715904510093]
	TIME [epoch: 8.51 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8229117925672662		[learning rate: 2.7386e-05]
	Learning Rate: 2.73859e-05
	LOSS [training: 0.8229117925672662 | validation: 0.7028349304273258]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1723.pth
	Model improved!!!
EPOCH 1724/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8149024055753266		[learning rate: 2.7286e-05]
	Learning Rate: 2.72865e-05
	LOSS [training: 0.8149024055753266 | validation: 0.7045379828863373]
	TIME [epoch: 8.51 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8189558964203325		[learning rate: 2.7187e-05]
	Learning Rate: 2.71875e-05
	LOSS [training: 0.8189558964203325 | validation: 0.7142910743655859]
	TIME [epoch: 8.51 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8243944067961332		[learning rate: 2.7089e-05]
	Learning Rate: 2.70888e-05
	LOSS [training: 0.8243944067961332 | validation: 0.7182183735801217]
	TIME [epoch: 8.51 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8222124643009325		[learning rate: 2.699e-05]
	Learning Rate: 2.69905e-05
	LOSS [training: 0.8222124643009325 | validation: 0.7123599878202898]
	TIME [epoch: 8.53 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8208381092746183		[learning rate: 2.6893e-05]
	Learning Rate: 2.68925e-05
	LOSS [training: 0.8208381092746183 | validation: 0.7100125203586345]
	TIME [epoch: 8.5 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8142578728670273		[learning rate: 2.6795e-05]
	Learning Rate: 2.67949e-05
	LOSS [training: 0.8142578728670273 | validation: 0.7032013013204295]
	TIME [epoch: 8.5 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8167984843922241		[learning rate: 2.6698e-05]
	Learning Rate: 2.66977e-05
	LOSS [training: 0.8167984843922241 | validation: 0.7112936941992127]
	TIME [epoch: 8.52 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8133406064899626		[learning rate: 2.6601e-05]
	Learning Rate: 2.66008e-05
	LOSS [training: 0.8133406064899626 | validation: 0.7064073803977678]
	TIME [epoch: 8.51 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8247889141561247		[learning rate: 2.6504e-05]
	Learning Rate: 2.65043e-05
	LOSS [training: 0.8247889141561247 | validation: 0.7346003353781296]
	TIME [epoch: 8.5 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8323334099325785		[learning rate: 2.6408e-05]
	Learning Rate: 2.64081e-05
	LOSS [training: 0.8323334099325785 | validation: 0.6994385840823494]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1733.pth
	Model improved!!!
EPOCH 1734/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8197982710969848		[learning rate: 2.6312e-05]
	Learning Rate: 2.63123e-05
	LOSS [training: 0.8197982710969848 | validation: 0.7047534067205766]
	TIME [epoch: 8.53 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8172010059737229		[learning rate: 2.6217e-05]
	Learning Rate: 2.62168e-05
	LOSS [training: 0.8172010059737229 | validation: 0.7052683899360989]
	TIME [epoch: 8.51 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8130101630703639		[learning rate: 2.6122e-05]
	Learning Rate: 2.61216e-05
	LOSS [training: 0.8130101630703639 | validation: 0.7076037286630193]
	TIME [epoch: 8.51 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8159384892980951		[learning rate: 2.6027e-05]
	Learning Rate: 2.60268e-05
	LOSS [training: 0.8159384892980951 | validation: 0.7071190915914117]
	TIME [epoch: 8.52 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8188608524895425		[learning rate: 2.5932e-05]
	Learning Rate: 2.59324e-05
	LOSS [training: 0.8188608524895425 | validation: 0.7078082270603102]
	TIME [epoch: 8.51 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8156593612613845		[learning rate: 2.5838e-05]
	Learning Rate: 2.58383e-05
	LOSS [training: 0.8156593612613845 | validation: 0.6993975467648444]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1739.pth
	Model improved!!!
EPOCH 1740/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8129927929879953		[learning rate: 2.5744e-05]
	Learning Rate: 2.57445e-05
	LOSS [training: 0.8129927929879953 | validation: 0.7066106493818325]
	TIME [epoch: 8.51 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8153978101625452		[learning rate: 2.5651e-05]
	Learning Rate: 2.56511e-05
	LOSS [training: 0.8153978101625452 | validation: 0.7038053613755186]
	TIME [epoch: 8.52 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8118461860512115		[learning rate: 2.5558e-05]
	Learning Rate: 2.5558e-05
	LOSS [training: 0.8118461860512115 | validation: 0.7064767778667058]
	TIME [epoch: 8.5 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8148942152122917		[learning rate: 2.5465e-05]
	Learning Rate: 2.54652e-05
	LOSS [training: 0.8148942152122917 | validation: 0.7060847664231282]
	TIME [epoch: 8.51 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.809360340169756		[learning rate: 2.5373e-05]
	Learning Rate: 2.53728e-05
	LOSS [training: 0.809360340169756 | validation: 0.7147240993649163]
	TIME [epoch: 8.52 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8135122175954084		[learning rate: 2.5281e-05]
	Learning Rate: 2.52807e-05
	LOSS [training: 0.8135122175954084 | validation: 0.7066704108081564]
	TIME [epoch: 8.51 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8103658029853511		[learning rate: 2.5189e-05]
	Learning Rate: 2.5189e-05
	LOSS [training: 0.8103658029853511 | validation: 0.7028551125707829]
	TIME [epoch: 8.5 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8075594415493228		[learning rate: 2.5098e-05]
	Learning Rate: 2.50976e-05
	LOSS [training: 0.8075594415493228 | validation: 0.7052473517318133]
	TIME [epoch: 8.51 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.806317226521417		[learning rate: 2.5006e-05]
	Learning Rate: 2.50065e-05
	LOSS [training: 0.806317226521417 | validation: 0.7021786489421056]
	TIME [epoch: 8.51 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8067316021924789		[learning rate: 2.4916e-05]
	Learning Rate: 2.49157e-05
	LOSS [training: 0.8067316021924789 | validation: 0.7041423303027747]
	TIME [epoch: 8.5 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8080397816283262		[learning rate: 2.4825e-05]
	Learning Rate: 2.48253e-05
	LOSS [training: 0.8080397816283262 | validation: 0.7127072948185109]
	TIME [epoch: 8.5 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.805845799532438		[learning rate: 2.4735e-05]
	Learning Rate: 2.47352e-05
	LOSS [training: 0.805845799532438 | validation: 0.7029239359641881]
	TIME [epoch: 8.53 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8072271483650117		[learning rate: 2.4645e-05]
	Learning Rate: 2.46455e-05
	LOSS [training: 0.8072271483650117 | validation: 0.7045533197254809]
	TIME [epoch: 8.51 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8113530827987763		[learning rate: 2.4556e-05]
	Learning Rate: 2.4556e-05
	LOSS [training: 0.8113530827987763 | validation: 0.7098716615903953]
	TIME [epoch: 8.51 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8027116884996728		[learning rate: 2.4467e-05]
	Learning Rate: 2.44669e-05
	LOSS [training: 0.8027116884996728 | validation: 0.6981663380514725]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1754.pth
	Model improved!!!
EPOCH 1755/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8091858099356358		[learning rate: 2.4378e-05]
	Learning Rate: 2.43781e-05
	LOSS [training: 0.8091858099356358 | validation: 0.7152664130591917]
	TIME [epoch: 8.52 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8086635913680661		[learning rate: 2.429e-05]
	Learning Rate: 2.42896e-05
	LOSS [training: 0.8086635913680661 | validation: 0.7017009996174755]
	TIME [epoch: 8.5 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8065488188567429		[learning rate: 2.4201e-05]
	Learning Rate: 2.42015e-05
	LOSS [training: 0.8065488188567429 | validation: 0.7011411280070354]
	TIME [epoch: 8.5 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8191315002433743		[learning rate: 2.4114e-05]
	Learning Rate: 2.41137e-05
	LOSS [training: 0.8191315002433743 | validation: 0.7133918148311931]
	TIME [epoch: 8.51 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.819339700955477		[learning rate: 2.4026e-05]
	Learning Rate: 2.40262e-05
	LOSS [training: 0.819339700955477 | validation: 0.7035482637966224]
	TIME [epoch: 8.5 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8153328187644219		[learning rate: 2.3939e-05]
	Learning Rate: 2.3939e-05
	LOSS [training: 0.8153328187644219 | validation: 0.7192550588336711]
	TIME [epoch: 8.5 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8279517825453814		[learning rate: 2.3852e-05]
	Learning Rate: 2.38521e-05
	LOSS [training: 0.8279517825453814 | validation: 0.7115826359716915]
	TIME [epoch: 8.5 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8165484406366617		[learning rate: 2.3766e-05]
	Learning Rate: 2.37655e-05
	LOSS [training: 0.8165484406366617 | validation: 0.7176139855890092]
	TIME [epoch: 8.53 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8101931595714079		[learning rate: 2.3679e-05]
	Learning Rate: 2.36793e-05
	LOSS [training: 0.8101931595714079 | validation: 0.6999388332600234]
	TIME [epoch: 8.5 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.806663978472962		[learning rate: 2.3593e-05]
	Learning Rate: 2.35933e-05
	LOSS [training: 0.806663978472962 | validation: 0.7040555782154497]
	TIME [epoch: 8.5 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8024003907483392		[learning rate: 2.3508e-05]
	Learning Rate: 2.35077e-05
	LOSS [training: 0.8024003907483392 | validation: 0.7024755829092946]
	TIME [epoch: 8.51 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7985509772951588		[learning rate: 2.3422e-05]
	Learning Rate: 2.34224e-05
	LOSS [training: 0.7985509772951588 | validation: 0.6906543372990311]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1766.pth
	Model improved!!!
EPOCH 1767/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8005727988687305		[learning rate: 2.3337e-05]
	Learning Rate: 2.33374e-05
	LOSS [training: 0.8005727988687305 | validation: 0.7084044029439845]
	TIME [epoch: 8.5 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8014640847982019		[learning rate: 2.3253e-05]
	Learning Rate: 2.32527e-05
	LOSS [training: 0.8014640847982019 | validation: 0.6993583146371178]
	TIME [epoch: 8.5 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7999872714582545		[learning rate: 2.3168e-05]
	Learning Rate: 2.31683e-05
	LOSS [training: 0.7999872714582545 | validation: 0.6954720518144507]
	TIME [epoch: 8.52 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8032043398749273		[learning rate: 2.3084e-05]
	Learning Rate: 2.30843e-05
	LOSS [training: 0.8032043398749273 | validation: 0.7114788376455428]
	TIME [epoch: 8.5 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7984891479299274		[learning rate: 2.3e-05]
	Learning Rate: 2.30005e-05
	LOSS [training: 0.7984891479299274 | validation: 0.7049315348351455]
	TIME [epoch: 8.5 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8021221575847001		[learning rate: 2.2917e-05]
	Learning Rate: 2.2917e-05
	LOSS [training: 0.8021221575847001 | validation: 0.70067540303277]
	TIME [epoch: 8.51 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8013772598260402		[learning rate: 2.2834e-05]
	Learning Rate: 2.28338e-05
	LOSS [training: 0.8013772598260402 | validation: 0.7017297575324278]
	TIME [epoch: 8.5 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.799216976967385		[learning rate: 2.2751e-05]
	Learning Rate: 2.2751e-05
	LOSS [training: 0.799216976967385 | validation: 0.697168265165168]
	TIME [epoch: 8.48 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7992694818619761		[learning rate: 2.2668e-05]
	Learning Rate: 2.26684e-05
	LOSS [training: 0.7992694818619761 | validation: 0.6932878166172449]
	TIME [epoch: 8.49 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7955155274314885		[learning rate: 2.2586e-05]
	Learning Rate: 2.25862e-05
	LOSS [training: 0.7955155274314885 | validation: 0.6883748011119472]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1776.pth
	Model improved!!!
EPOCH 1777/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.79536712832778		[learning rate: 2.2504e-05]
	Learning Rate: 2.25042e-05
	LOSS [training: 0.79536712832778 | validation: 0.6964943062424309]
	TIME [epoch: 8.51 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7915770468102815		[learning rate: 2.2423e-05]
	Learning Rate: 2.24225e-05
	LOSS [training: 0.7915770468102815 | validation: 0.6947398965911136]
	TIME [epoch: 8.51 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.793361256277699		[learning rate: 2.2341e-05]
	Learning Rate: 2.23411e-05
	LOSS [training: 0.793361256277699 | validation: 0.697127565465594]
	TIME [epoch: 8.52 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7986919051927777		[learning rate: 2.226e-05]
	Learning Rate: 2.22601e-05
	LOSS [training: 0.7986919051927777 | validation: 0.7038732422625176]
	TIME [epoch: 8.52 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.795113912420402		[learning rate: 2.2179e-05]
	Learning Rate: 2.21793e-05
	LOSS [training: 0.795113912420402 | validation: 0.7023775094009267]
	TIME [epoch: 8.5 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7939181504225483		[learning rate: 2.2099e-05]
	Learning Rate: 2.20988e-05
	LOSS [training: 0.7939181504225483 | validation: 0.6945491994328]
	TIME [epoch: 8.51 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7938735199684935		[learning rate: 2.2019e-05]
	Learning Rate: 2.20186e-05
	LOSS [training: 0.7938735199684935 | validation: 0.686657286370294]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1783.pth
	Model improved!!!
EPOCH 1784/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.793507205830611		[learning rate: 2.1939e-05]
	Learning Rate: 2.19387e-05
	LOSS [training: 0.793507205830611 | validation: 0.6858035026919387]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1784.pth
	Model improved!!!
EPOCH 1785/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.793350634606989		[learning rate: 2.1859e-05]
	Learning Rate: 2.18591e-05
	LOSS [training: 0.793350634606989 | validation: 0.7003181445324136]
	TIME [epoch: 8.51 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7947759469368831		[learning rate: 2.178e-05]
	Learning Rate: 2.17797e-05
	LOSS [training: 0.7947759469368831 | validation: 0.6808035449761863]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1786.pth
	Model improved!!!
EPOCH 1787/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7906583903583786		[learning rate: 2.1701e-05]
	Learning Rate: 2.17007e-05
	LOSS [training: 0.7906583903583786 | validation: 0.7090909069438376]
	TIME [epoch: 8.52 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7956145840409057		[learning rate: 2.1622e-05]
	Learning Rate: 2.16219e-05
	LOSS [training: 0.7956145840409057 | validation: 0.6856095799748875]
	TIME [epoch: 8.51 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7934490832095443		[learning rate: 2.1543e-05]
	Learning Rate: 2.15435e-05
	LOSS [training: 0.7934490832095443 | validation: 0.700466749499345]
	TIME [epoch: 8.51 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7871705545964505		[learning rate: 2.1465e-05]
	Learning Rate: 2.14653e-05
	LOSS [training: 0.7871705545964505 | validation: 0.6718601813052006]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1790.pth
	Model improved!!!
EPOCH 1791/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7856730064569748		[learning rate: 2.1387e-05]
	Learning Rate: 2.13874e-05
	LOSS [training: 0.7856730064569748 | validation: 0.6811113426197763]
	TIME [epoch: 8.51 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7929473024097492		[learning rate: 2.131e-05]
	Learning Rate: 2.13098e-05
	LOSS [training: 0.7929473024097492 | validation: 0.6875940407116966]
	TIME [epoch: 8.5 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7883664639799477		[learning rate: 2.1232e-05]
	Learning Rate: 2.12325e-05
	LOSS [training: 0.7883664639799477 | validation: 0.7003927880533676]
	TIME [epoch: 8.53 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.789919747144028		[learning rate: 2.1155e-05]
	Learning Rate: 2.11554e-05
	LOSS [training: 0.789919747144028 | validation: 0.6802680748768238]
	TIME [epoch: 8.51 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7864578397540727		[learning rate: 2.1079e-05]
	Learning Rate: 2.10786e-05
	LOSS [training: 0.7864578397540727 | validation: 0.6756491725625747]
	TIME [epoch: 8.51 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7866308870417963		[learning rate: 2.1002e-05]
	Learning Rate: 2.10021e-05
	LOSS [training: 0.7866308870417963 | validation: 0.6661919667638574]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1796.pth
	Model improved!!!
EPOCH 1797/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7852955203211743		[learning rate: 2.0926e-05]
	Learning Rate: 2.09259e-05
	LOSS [training: 0.7852955203211743 | validation: 0.6843437515187482]
	TIME [epoch: 8.53 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7891472125011519		[learning rate: 2.085e-05]
	Learning Rate: 2.085e-05
	LOSS [training: 0.7891472125011519 | validation: 0.6802535428012585]
	TIME [epoch: 8.51 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7852895189023377		[learning rate: 2.0774e-05]
	Learning Rate: 2.07743e-05
	LOSS [training: 0.7852895189023377 | validation: 0.6905396449011021]
	TIME [epoch: 8.51 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7820740443841083		[learning rate: 2.0699e-05]
	Learning Rate: 2.06989e-05
	LOSS [training: 0.7820740443841083 | validation: 0.6706663726512577]
	TIME [epoch: 8.52 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7816383477816401		[learning rate: 2.0624e-05]
	Learning Rate: 2.06238e-05
	LOSS [training: 0.7816383477816401 | validation: 0.6655374487546744]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1801.pth
	Model improved!!!
EPOCH 1802/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7832157951249049		[learning rate: 2.0549e-05]
	Learning Rate: 2.05489e-05
	LOSS [training: 0.7832157951249049 | validation: 0.666208922553173]
	TIME [epoch: 8.52 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7836991627580078		[learning rate: 2.0474e-05]
	Learning Rate: 2.04744e-05
	LOSS [training: 0.7836991627580078 | validation: 0.6523565130551159]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1803.pth
	Model improved!!!
EPOCH 1804/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7828700195284858		[learning rate: 2.04e-05]
	Learning Rate: 2.04001e-05
	LOSS [training: 0.7828700195284858 | validation: 0.6685686569652202]
	TIME [epoch: 8.53 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7821432611084336		[learning rate: 2.0326e-05]
	Learning Rate: 2.0326e-05
	LOSS [training: 0.7821432611084336 | validation: 0.6674952103802967]
	TIME [epoch: 8.5 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7779586498581269		[learning rate: 2.0252e-05]
	Learning Rate: 2.02523e-05
	LOSS [training: 0.7779586498581269 | validation: 0.6595111882917042]
	TIME [epoch: 8.5 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7855426772101957		[learning rate: 2.0179e-05]
	Learning Rate: 2.01788e-05
	LOSS [training: 0.7855426772101957 | validation: 0.6720804996333754]
	TIME [epoch: 8.52 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7917030910541211		[learning rate: 2.0106e-05]
	Learning Rate: 2.01055e-05
	LOSS [training: 0.7917030910541211 | validation: 0.668774364298606]
	TIME [epoch: 8.5 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7822833809289782		[learning rate: 2.0033e-05]
	Learning Rate: 2.00326e-05
	LOSS [training: 0.7822833809289782 | validation: 0.6741316344090055]
	TIME [epoch: 8.5 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7820173967346646		[learning rate: 1.996e-05]
	Learning Rate: 1.99599e-05
	LOSS [training: 0.7820173967346646 | validation: 0.6687426230934266]
	TIME [epoch: 8.5 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7754487486909073		[learning rate: 1.9887e-05]
	Learning Rate: 1.98874e-05
	LOSS [training: 0.7754487486909073 | validation: 0.6665659646622308]
	TIME [epoch: 8.52 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7794064895033618		[learning rate: 1.9815e-05]
	Learning Rate: 1.98153e-05
	LOSS [training: 0.7794064895033618 | validation: 0.665761932083246]
	TIME [epoch: 8.49 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7828502618854243		[learning rate: 1.9743e-05]
	Learning Rate: 1.97434e-05
	LOSS [training: 0.7828502618854243 | validation: 0.6689854754100639]
	TIME [epoch: 8.5 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7828552958694063		[learning rate: 1.9672e-05]
	Learning Rate: 1.96717e-05
	LOSS [training: 0.7828552958694063 | validation: 0.6624086169794606]
	TIME [epoch: 8.52 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7762557858674561		[learning rate: 1.96e-05]
	Learning Rate: 1.96003e-05
	LOSS [training: 0.7762557858674561 | validation: 0.6646021032553078]
	TIME [epoch: 8.5 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7851894190156783		[learning rate: 1.9529e-05]
	Learning Rate: 1.95292e-05
	LOSS [training: 0.7851894190156783 | validation: 0.6567767557993665]
	TIME [epoch: 8.49 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7765913699760059		[learning rate: 1.9458e-05]
	Learning Rate: 1.94583e-05
	LOSS [training: 0.7765913699760059 | validation: 0.6624424361501995]
	TIME [epoch: 8.5 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7816928346370968		[learning rate: 1.9388e-05]
	Learning Rate: 1.93877e-05
	LOSS [training: 0.7816928346370968 | validation: 0.6669462111800444]
	TIME [epoch: 8.52 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7773896875913145		[learning rate: 1.9317e-05]
	Learning Rate: 1.93173e-05
	LOSS [training: 0.7773896875913145 | validation: 0.6663241674998359]
	TIME [epoch: 8.5 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7799175105217825		[learning rate: 1.9247e-05]
	Learning Rate: 1.92472e-05
	LOSS [training: 0.7799175105217825 | validation: 0.6552224929225209]
	TIME [epoch: 8.49 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7714032412856342		[learning rate: 1.9177e-05]
	Learning Rate: 1.91774e-05
	LOSS [training: 0.7714032412856342 | validation: 0.6621211005114243]
	TIME [epoch: 8.52 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7770915972690543		[learning rate: 1.9108e-05]
	Learning Rate: 1.91078e-05
	LOSS [training: 0.7770915972690543 | validation: 0.6665520485979951]
	TIME [epoch: 8.5 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7772913846104142		[learning rate: 1.9038e-05]
	Learning Rate: 1.90384e-05
	LOSS [training: 0.7772913846104142 | validation: 0.6504043670396843]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1823.pth
	Model improved!!!
EPOCH 1824/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7725563123948593		[learning rate: 1.8969e-05]
	Learning Rate: 1.89694e-05
	LOSS [training: 0.7725563123948593 | validation: 0.6556636359596545]
	TIME [epoch: 8.5 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7718083545073557		[learning rate: 1.8901e-05]
	Learning Rate: 1.89005e-05
	LOSS [training: 0.7718083545073557 | validation: 0.6619064732976014]
	TIME [epoch: 8.52 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.770788732261758		[learning rate: 1.8832e-05]
	Learning Rate: 1.88319e-05
	LOSS [training: 0.770788732261758 | validation: 0.6560887867624111]
	TIME [epoch: 8.5 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7667266941391249		[learning rate: 1.8764e-05]
	Learning Rate: 1.87636e-05
	LOSS [training: 0.7667266941391249 | validation: 0.6589378704539919]
	TIME [epoch: 8.5 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7699431784235443		[learning rate: 1.8695e-05]
	Learning Rate: 1.86955e-05
	LOSS [training: 0.7699431784235443 | validation: 0.6521105350539717]
	TIME [epoch: 8.52 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7620602826200091		[learning rate: 1.8628e-05]
	Learning Rate: 1.86276e-05
	LOSS [training: 0.7620602826200091 | validation: 0.6451387529166926]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1829.pth
	Model improved!!!
EPOCH 1830/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7639692445416602		[learning rate: 1.856e-05]
	Learning Rate: 1.856e-05
	LOSS [training: 0.7639692445416602 | validation: 0.6496039529090121]
	TIME [epoch: 8.5 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.76668019873028		[learning rate: 1.8493e-05]
	Learning Rate: 1.84927e-05
	LOSS [training: 0.76668019873028 | validation: 0.662867431444934]
	TIME [epoch: 8.5 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7641050115740627		[learning rate: 1.8426e-05]
	Learning Rate: 1.84256e-05
	LOSS [training: 0.7641050115740627 | validation: 0.6607667259720716]
	TIME [epoch: 8.52 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7665882143179414		[learning rate: 1.8359e-05]
	Learning Rate: 1.83587e-05
	LOSS [training: 0.7665882143179414 | validation: 0.6617064760135573]
	TIME [epoch: 8.5 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7645183214289428		[learning rate: 1.8292e-05]
	Learning Rate: 1.82921e-05
	LOSS [training: 0.7645183214289428 | validation: 0.6532134334775102]
	TIME [epoch: 8.5 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7593999711947371		[learning rate: 1.8226e-05]
	Learning Rate: 1.82257e-05
	LOSS [training: 0.7593999711947371 | validation: 0.6574455893058615]
	TIME [epoch: 8.52 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7654708817607584		[learning rate: 1.816e-05]
	Learning Rate: 1.81596e-05
	LOSS [training: 0.7654708817607584 | validation: 0.656298976327172]
	TIME [epoch: 8.5 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7598905467900389		[learning rate: 1.8094e-05]
	Learning Rate: 1.80937e-05
	LOSS [training: 0.7598905467900389 | validation: 0.6580783027272354]
	TIME [epoch: 8.5 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7606803213304205		[learning rate: 1.8028e-05]
	Learning Rate: 1.8028e-05
	LOSS [training: 0.7606803213304205 | validation: 0.6533091006318348]
	TIME [epoch: 8.5 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7621672469480756		[learning rate: 1.7963e-05]
	Learning Rate: 1.79626e-05
	LOSS [training: 0.7621672469480756 | validation: 0.662373269445525]
	TIME [epoch: 8.52 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7606906003737424		[learning rate: 1.7897e-05]
	Learning Rate: 1.78974e-05
	LOSS [training: 0.7606906003737424 | validation: 0.6570065122800781]
	TIME [epoch: 8.5 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7586760596357177		[learning rate: 1.7832e-05]
	Learning Rate: 1.78324e-05
	LOSS [training: 0.7586760596357177 | validation: 0.6513243422928493]
	TIME [epoch: 8.5 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.761946113957444		[learning rate: 1.7768e-05]
	Learning Rate: 1.77677e-05
	LOSS [training: 0.761946113957444 | validation: 0.6673959757739587]
	TIME [epoch: 8.51 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7623400768052104		[learning rate: 1.7703e-05]
	Learning Rate: 1.77032e-05
	LOSS [training: 0.7623400768052104 | validation: 0.6402416951700429]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1843.pth
	Model improved!!!
EPOCH 1844/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7592317375821661		[learning rate: 1.7639e-05]
	Learning Rate: 1.7639e-05
	LOSS [training: 0.7592317375821661 | validation: 0.6676142273411105]
	TIME [epoch: 8.5 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7523413969429213		[learning rate: 1.7575e-05]
	Learning Rate: 1.7575e-05
	LOSS [training: 0.7523413969429213 | validation: 0.6501619640554108]
	TIME [epoch: 8.5 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7550078997538661		[learning rate: 1.7511e-05]
	Learning Rate: 1.75112e-05
	LOSS [training: 0.7550078997538661 | validation: 0.6463031558206122]
	TIME [epoch: 8.53 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.762202395877335		[learning rate: 1.7448e-05]
	Learning Rate: 1.74476e-05
	LOSS [training: 0.762202395877335 | validation: 0.6505402678167593]
	TIME [epoch: 8.51 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7631195896350217		[learning rate: 1.7384e-05]
	Learning Rate: 1.73843e-05
	LOSS [training: 0.7631195896350217 | validation: 0.6647392063105017]
	TIME [epoch: 8.5 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7646177786359785		[learning rate: 1.7321e-05]
	Learning Rate: 1.73212e-05
	LOSS [training: 0.7646177786359785 | validation: 0.6536305354281089]
	TIME [epoch: 8.51 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7551729290068103		[learning rate: 1.7258e-05]
	Learning Rate: 1.72584e-05
	LOSS [training: 0.7551729290068103 | validation: 0.6433728940622702]
	TIME [epoch: 8.51 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.758204791820744		[learning rate: 1.7196e-05]
	Learning Rate: 1.71957e-05
	LOSS [training: 0.758204791820744 | validation: 0.6548934212781715]
	TIME [epoch: 8.49 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7577283735985074		[learning rate: 1.7133e-05]
	Learning Rate: 1.71333e-05
	LOSS [training: 0.7577283735985074 | validation: 0.6475231934097456]
	TIME [epoch: 8.51 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7524297817640305		[learning rate: 1.7071e-05]
	Learning Rate: 1.70712e-05
	LOSS [training: 0.7524297817640305 | validation: 0.6481465238066837]
	TIME [epoch: 8.52 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7516387244239127		[learning rate: 1.7009e-05]
	Learning Rate: 1.70092e-05
	LOSS [training: 0.7516387244239127 | validation: 0.626073875944416]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1854.pth
	Model improved!!!
EPOCH 1855/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7559348790525491		[learning rate: 1.6947e-05]
	Learning Rate: 1.69475e-05
	LOSS [training: 0.7559348790525491 | validation: 0.6617316256945005]
	TIME [epoch: 8.51 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7613109545513569		[learning rate: 1.6886e-05]
	Learning Rate: 1.6886e-05
	LOSS [training: 0.7613109545513569 | validation: 0.6339392758910486]
	TIME [epoch: 8.52 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7648042415131784		[learning rate: 1.6825e-05]
	Learning Rate: 1.68247e-05
	LOSS [training: 0.7648042415131784 | validation: 0.6621515655840168]
	TIME [epoch: 8.51 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7630590222028254		[learning rate: 1.6764e-05]
	Learning Rate: 1.67636e-05
	LOSS [training: 0.7630590222028254 | validation: 0.6423820024259389]
	TIME [epoch: 8.51 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7674803532149189		[learning rate: 1.6703e-05]
	Learning Rate: 1.67028e-05
	LOSS [training: 0.7674803532149189 | validation: 0.6539636506750447]
	TIME [epoch: 8.5 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.761618590282534		[learning rate: 1.6642e-05]
	Learning Rate: 1.66422e-05
	LOSS [training: 0.761618590282534 | validation: 0.6474946911929337]
	TIME [epoch: 8.53 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7518658831736043		[learning rate: 1.6582e-05]
	Learning Rate: 1.65818e-05
	LOSS [training: 0.7518658831736043 | validation: 0.6442683802247849]
	TIME [epoch: 8.51 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7548559436245174		[learning rate: 1.6522e-05]
	Learning Rate: 1.65216e-05
	LOSS [training: 0.7548559436245174 | validation: 0.6462413398395888]
	TIME [epoch: 8.5 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.750088709289927		[learning rate: 1.6462e-05]
	Learning Rate: 1.64617e-05
	LOSS [training: 0.750088709289927 | validation: 0.6187288988615107]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1863.pth
	Model improved!!!
EPOCH 1864/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7461770319537766		[learning rate: 1.6402e-05]
	Learning Rate: 1.64019e-05
	LOSS [training: 0.7461770319537766 | validation: 0.6216527376976628]
	TIME [epoch: 8.51 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7527648345145191		[learning rate: 1.6342e-05]
	Learning Rate: 1.63424e-05
	LOSS [training: 0.7527648345145191 | validation: 0.6321434257533629]
	TIME [epoch: 8.5 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7497458158471032		[learning rate: 1.6283e-05]
	Learning Rate: 1.62831e-05
	LOSS [training: 0.7497458158471032 | validation: 0.6385873092048175]
	TIME [epoch: 8.5 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7512100119354073		[learning rate: 1.6224e-05]
	Learning Rate: 1.6224e-05
	LOSS [training: 0.7512100119354073 | validation: 0.6321044904047718]
	TIME [epoch: 8.52 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7510896571541633		[learning rate: 1.6165e-05]
	Learning Rate: 1.61651e-05
	LOSS [training: 0.7510896571541633 | validation: 0.6359604869646361]
	TIME [epoch: 8.49 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7490066286860256		[learning rate: 1.6106e-05]
	Learning Rate: 1.61065e-05
	LOSS [training: 0.7490066286860256 | validation: 0.6173741591610176]
	TIME [epoch: 8.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1869.pth
	Model improved!!!
EPOCH 1870/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.740009972019126		[learning rate: 1.6048e-05]
	Learning Rate: 1.6048e-05
	LOSS [training: 0.740009972019126 | validation: 0.6260311912081926]
	TIME [epoch: 8.53 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7426889578036653		[learning rate: 1.599e-05]
	Learning Rate: 1.59898e-05
	LOSS [training: 0.7426889578036653 | validation: 0.6353220388291709]
	TIME [epoch: 8.53 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7381229066972668		[learning rate: 1.5932e-05]
	Learning Rate: 1.59317e-05
	LOSS [training: 0.7381229066972668 | validation: 0.6131589931137106]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1872.pth
	Model improved!!!
EPOCH 1873/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7364475076633242		[learning rate: 1.5874e-05]
	Learning Rate: 1.58739e-05
	LOSS [training: 0.7364475076633242 | validation: 0.6385245734062784]
	TIME [epoch: 8.52 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7357728745332335		[learning rate: 1.5816e-05]
	Learning Rate: 1.58163e-05
	LOSS [training: 0.7357728745332335 | validation: 0.6449497282308211]
	TIME [epoch: 8.54 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7364616248240845		[learning rate: 1.5759e-05]
	Learning Rate: 1.57589e-05
	LOSS [training: 0.7364616248240845 | validation: 0.6157404067573303]
	TIME [epoch: 8.52 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7345964373382511		[learning rate: 1.5702e-05]
	Learning Rate: 1.57017e-05
	LOSS [training: 0.7345964373382511 | validation: 0.6201875996302331]
	TIME [epoch: 8.52 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7375296127300015		[learning rate: 1.5645e-05]
	Learning Rate: 1.56447e-05
	LOSS [training: 0.7375296127300015 | validation: 0.6288115893038069]
	TIME [epoch: 8.54 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7472574546130506		[learning rate: 1.5588e-05]
	Learning Rate: 1.5588e-05
	LOSS [training: 0.7472574546130506 | validation: 0.6409275982048437]
	TIME [epoch: 8.52 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7376810540324157		[learning rate: 1.5531e-05]
	Learning Rate: 1.55314e-05
	LOSS [training: 0.7376810540324157 | validation: 0.6206849953246779]
	TIME [epoch: 8.52 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7302331516836544		[learning rate: 1.5475e-05]
	Learning Rate: 1.5475e-05
	LOSS [training: 0.7302331516836544 | validation: 0.6286349136710427]
	TIME [epoch: 8.52 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7284086063585922		[learning rate: 1.5419e-05]
	Learning Rate: 1.54189e-05
	LOSS [training: 0.7284086063585922 | validation: 0.621393031993582]
	TIME [epoch: 8.54 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7292235969321881		[learning rate: 1.5363e-05]
	Learning Rate: 1.53629e-05
	LOSS [training: 0.7292235969321881 | validation: 0.6192347320135666]
	TIME [epoch: 8.52 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7269935397026466		[learning rate: 1.5307e-05]
	Learning Rate: 1.53072e-05
	LOSS [training: 0.7269935397026466 | validation: 0.624223957665647]
	TIME [epoch: 8.52 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7301815236644378		[learning rate: 1.5252e-05]
	Learning Rate: 1.52516e-05
	LOSS [training: 0.7301815236644378 | validation: 0.618373349886888]
	TIME [epoch: 8.53 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7295165839531421		[learning rate: 1.5196e-05]
	Learning Rate: 1.51963e-05
	LOSS [training: 0.7295165839531421 | validation: 0.6146160701862179]
	TIME [epoch: 8.53 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7259273269984403		[learning rate: 1.5141e-05]
	Learning Rate: 1.51411e-05
	LOSS [training: 0.7259273269984403 | validation: 0.6194564422619914]
	TIME [epoch: 8.51 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7205472679894276		[learning rate: 1.5086e-05]
	Learning Rate: 1.50862e-05
	LOSS [training: 0.7205472679894276 | validation: 0.6190795152635726]
	TIME [epoch: 8.52 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7250027218759754		[learning rate: 1.5031e-05]
	Learning Rate: 1.50314e-05
	LOSS [training: 0.7250027218759754 | validation: 0.6273749688782769]
	TIME [epoch: 8.54 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7185373845648124		[learning rate: 1.4977e-05]
	Learning Rate: 1.49769e-05
	LOSS [training: 0.7185373845648124 | validation: 0.6344392445408058]
	TIME [epoch: 8.52 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7185404004969248		[learning rate: 1.4923e-05]
	Learning Rate: 1.49225e-05
	LOSS [training: 0.7185404004969248 | validation: 0.6232105805681882]
	TIME [epoch: 8.51 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7215479286289462		[learning rate: 1.4868e-05]
	Learning Rate: 1.48684e-05
	LOSS [training: 0.7215479286289462 | validation: 0.6120888046660996]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1891.pth
	Model improved!!!
EPOCH 1892/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7144180390190887		[learning rate: 1.4814e-05]
	Learning Rate: 1.48144e-05
	LOSS [training: 0.7144180390190887 | validation: 0.6128856073632781]
	TIME [epoch: 8.52 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7129114939156727		[learning rate: 1.4761e-05]
	Learning Rate: 1.47606e-05
	LOSS [training: 0.7129114939156727 | validation: 0.6009183178746038]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1893.pth
	Model improved!!!
EPOCH 1894/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7059324349643792		[learning rate: 1.4707e-05]
	Learning Rate: 1.47071e-05
	LOSS [training: 0.7059324349643792 | validation: 0.6135542936380852]
	TIME [epoch: 8.52 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7063844792810167		[learning rate: 1.4654e-05]
	Learning Rate: 1.46537e-05
	LOSS [training: 0.7063844792810167 | validation: 0.6116143886589526]
	TIME [epoch: 8.55 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7044396892237368		[learning rate: 1.4601e-05]
	Learning Rate: 1.46005e-05
	LOSS [training: 0.7044396892237368 | validation: 0.6215748983520917]
	TIME [epoch: 8.51 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7010012831958724		[learning rate: 1.4548e-05]
	Learning Rate: 1.45475e-05
	LOSS [training: 0.7010012831958724 | validation: 0.6236182927711821]
	TIME [epoch: 8.52 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6936745352419493		[learning rate: 1.4495e-05]
	Learning Rate: 1.44947e-05
	LOSS [training: 0.6936745352419493 | validation: 0.6131235544903897]
	TIME [epoch: 8.54 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6880898143857348		[learning rate: 1.4442e-05]
	Learning Rate: 1.44421e-05
	LOSS [training: 0.6880898143857348 | validation: 0.6204219152436687]
	TIME [epoch: 8.52 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6915224553175614		[learning rate: 1.439e-05]
	Learning Rate: 1.43897e-05
	LOSS [training: 0.6915224553175614 | validation: 0.6247238443453521]
	TIME [epoch: 8.51 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6841390852973047		[learning rate: 1.4338e-05]
	Learning Rate: 1.43375e-05
	LOSS [training: 0.6841390852973047 | validation: 0.6142155617647331]
	TIME [epoch: 8.52 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6792006779688855		[learning rate: 1.4285e-05]
	Learning Rate: 1.42855e-05
	LOSS [training: 0.6792006779688855 | validation: 0.6048603887798891]
	TIME [epoch: 8.54 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6746181159704879		[learning rate: 1.4234e-05]
	Learning Rate: 1.42336e-05
	LOSS [training: 0.6746181159704879 | validation: 0.6263745269036955]
	TIME [epoch: 8.52 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6640394875761227		[learning rate: 1.4182e-05]
	Learning Rate: 1.4182e-05
	LOSS [training: 0.6640394875761227 | validation: 0.605400293905514]
	TIME [epoch: 8.52 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6572782536112666		[learning rate: 1.4131e-05]
	Learning Rate: 1.41305e-05
	LOSS [training: 0.6572782536112666 | validation: 0.618561747676037]
	TIME [epoch: 8.53 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.651419894100749		[learning rate: 1.4079e-05]
	Learning Rate: 1.40792e-05
	LOSS [training: 0.651419894100749 | validation: 0.627031508298179]
	TIME [epoch: 8.52 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6363309754200106		[learning rate: 1.4028e-05]
	Learning Rate: 1.40281e-05
	LOSS [training: 0.6363309754200106 | validation: 0.625125171661418]
	TIME [epoch: 8.51 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6378920781602853		[learning rate: 1.3977e-05]
	Learning Rate: 1.39772e-05
	LOSS [training: 0.6378920781602853 | validation: 0.6209701538963388]
	TIME [epoch: 8.51 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6296918518115782		[learning rate: 1.3927e-05]
	Learning Rate: 1.39265e-05
	LOSS [training: 0.6296918518115782 | validation: 0.6299472219060911]
	TIME [epoch: 8.54 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6134030490723072		[learning rate: 1.3876e-05]
	Learning Rate: 1.3876e-05
	LOSS [training: 0.6134030490723072 | validation: 0.6381690421675023]
	TIME [epoch: 8.52 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.610492639664218		[learning rate: 1.3826e-05]
	Learning Rate: 1.38256e-05
	LOSS [training: 0.610492639664218 | validation: 0.6238082621422806]
	TIME [epoch: 8.52 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6048939605607224		[learning rate: 1.3775e-05]
	Learning Rate: 1.37754e-05
	LOSS [training: 0.6048939605607224 | validation: 0.6157802864424194]
	TIME [epoch: 8.53 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6028497669416076		[learning rate: 1.3725e-05]
	Learning Rate: 1.37254e-05
	LOSS [training: 0.6028497669416076 | validation: 0.6259039922417935]
	TIME [epoch: 8.53 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5982798252862864		[learning rate: 1.3676e-05]
	Learning Rate: 1.36756e-05
	LOSS [training: 0.5982798252862864 | validation: 0.6238113782567332]
	TIME [epoch: 8.51 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5904554803665637		[learning rate: 1.3626e-05]
	Learning Rate: 1.3626e-05
	LOSS [training: 0.5904554803665637 | validation: 0.6167297565871703]
	TIME [epoch: 8.51 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5845319867172524		[learning rate: 1.3577e-05]
	Learning Rate: 1.35766e-05
	LOSS [training: 0.5845319867172524 | validation: 0.6056597920017956]
	TIME [epoch: 8.54 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5788506749482173		[learning rate: 1.3527e-05]
	Learning Rate: 1.35273e-05
	LOSS [training: 0.5788506749482173 | validation: 0.6170850648529311]
	TIME [epoch: 8.52 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5776717192069759		[learning rate: 1.3478e-05]
	Learning Rate: 1.34782e-05
	LOSS [training: 0.5776717192069759 | validation: 0.6148837127792637]
	TIME [epoch: 8.52 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5768845176073223		[learning rate: 1.3429e-05]
	Learning Rate: 1.34293e-05
	LOSS [training: 0.5768845176073223 | validation: 0.615267441711233]
	TIME [epoch: 8.53 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5740898412733019		[learning rate: 1.3381e-05]
	Learning Rate: 1.33805e-05
	LOSS [training: 0.5740898412733019 | validation: 0.6049995808825156]
	TIME [epoch: 8.52 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5713388538570264		[learning rate: 1.3332e-05]
	Learning Rate: 1.3332e-05
	LOSS [training: 0.5713388538570264 | validation: 0.6176866512814442]
	TIME [epoch: 8.51 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5724193734525497		[learning rate: 1.3284e-05]
	Learning Rate: 1.32836e-05
	LOSS [training: 0.5724193734525497 | validation: 0.604057151997555]
	TIME [epoch: 8.52 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5673613750733656		[learning rate: 1.3235e-05]
	Learning Rate: 1.32354e-05
	LOSS [training: 0.5673613750733656 | validation: 0.6066305328152809]
	TIME [epoch: 8.54 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5683558298131524		[learning rate: 1.3187e-05]
	Learning Rate: 1.31874e-05
	LOSS [training: 0.5683558298131524 | validation: 0.6141464367741397]
	TIME [epoch: 8.51 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5661421347716349		[learning rate: 1.314e-05]
	Learning Rate: 1.31395e-05
	LOSS [training: 0.5661421347716349 | validation: 0.6189497766714313]
	TIME [epoch: 8.51 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5599158011281972		[learning rate: 1.3092e-05]
	Learning Rate: 1.30918e-05
	LOSS [training: 0.5599158011281972 | validation: 0.6154186458355748]
	TIME [epoch: 8.53 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5585195236048964		[learning rate: 1.3044e-05]
	Learning Rate: 1.30443e-05
	LOSS [training: 0.5585195236048964 | validation: 0.5868509874546061]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1927.pth
	Model improved!!!
EPOCH 1928/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5534091825774452		[learning rate: 1.2997e-05]
	Learning Rate: 1.2997e-05
	LOSS [training: 0.5534091825774452 | validation: 0.6105478687035811]
	TIME [epoch: 8.5 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5491595081952649		[learning rate: 1.295e-05]
	Learning Rate: 1.29498e-05
	LOSS [training: 0.5491595081952649 | validation: 0.6225652791287779]
	TIME [epoch: 8.5 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5542450136889971		[learning rate: 1.2903e-05]
	Learning Rate: 1.29028e-05
	LOSS [training: 0.5542450136889971 | validation: 0.5939748772720894]
	TIME [epoch: 8.52 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5539287747730712		[learning rate: 1.2856e-05]
	Learning Rate: 1.2856e-05
	LOSS [training: 0.5539287747730712 | validation: 0.5978235715533756]
	TIME [epoch: 8.5 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5468798569345504		[learning rate: 1.2809e-05]
	Learning Rate: 1.28093e-05
	LOSS [training: 0.5468798569345504 | validation: 0.5977636029946093]
	TIME [epoch: 8.5 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5439917276599715		[learning rate: 1.2763e-05]
	Learning Rate: 1.27628e-05
	LOSS [training: 0.5439917276599715 | validation: 0.5866188548066337]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1933.pth
	Model improved!!!
EPOCH 1934/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5492258841009305		[learning rate: 1.2717e-05]
	Learning Rate: 1.27165e-05
	LOSS [training: 0.5492258841009305 | validation: 0.5982864606454728]
	TIME [epoch: 8.51 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5444223829187507		[learning rate: 1.267e-05]
	Learning Rate: 1.26704e-05
	LOSS [training: 0.5444223829187507 | validation: 0.6034053264140111]
	TIME [epoch: 8.5 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5419484573600697		[learning rate: 1.2624e-05]
	Learning Rate: 1.26244e-05
	LOSS [training: 0.5419484573600697 | validation: 0.6062180210202116]
	TIME [epoch: 8.5 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5415601125974258		[learning rate: 1.2579e-05]
	Learning Rate: 1.25786e-05
	LOSS [training: 0.5415601125974258 | validation: 0.5975171497149587]
	TIME [epoch: 8.52 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5465816633369884		[learning rate: 1.2533e-05]
	Learning Rate: 1.25329e-05
	LOSS [training: 0.5465816633369884 | validation: 0.6095624230583625]
	TIME [epoch: 8.5 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5438140157697102		[learning rate: 1.2487e-05]
	Learning Rate: 1.24874e-05
	LOSS [training: 0.5438140157697102 | validation: 0.5978577000357449]
	TIME [epoch: 8.5 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5430846939032679		[learning rate: 1.2442e-05]
	Learning Rate: 1.24421e-05
	LOSS [training: 0.5430846939032679 | validation: 0.6028443136265085]
	TIME [epoch: 8.51 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5416925696944835		[learning rate: 1.2397e-05]
	Learning Rate: 1.2397e-05
	LOSS [training: 0.5416925696944835 | validation: 0.5989697449836267]
	TIME [epoch: 8.51 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5398588758182128		[learning rate: 1.2352e-05]
	Learning Rate: 1.2352e-05
	LOSS [training: 0.5398588758182128 | validation: 0.6029326032737532]
	TIME [epoch: 8.5 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5401635095978029		[learning rate: 1.2307e-05]
	Learning Rate: 1.23072e-05
	LOSS [training: 0.5401635095978029 | validation: 0.6038226914731698]
	TIME [epoch: 8.49 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5397869246308318		[learning rate: 1.2263e-05]
	Learning Rate: 1.22625e-05
	LOSS [training: 0.5397869246308318 | validation: 0.5943009037777631]
	TIME [epoch: 8.51 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5354860196992732		[learning rate: 1.2218e-05]
	Learning Rate: 1.2218e-05
	LOSS [training: 0.5354860196992732 | validation: 0.6095979504992277]
	TIME [epoch: 8.5 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5353670556818678		[learning rate: 1.2174e-05]
	Learning Rate: 1.21737e-05
	LOSS [training: 0.5353670556818678 | validation: 0.6027165059697555]
	TIME [epoch: 8.49 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5379767654418406		[learning rate: 1.2129e-05]
	Learning Rate: 1.21295e-05
	LOSS [training: 0.5379767654418406 | validation: 0.609164690089534]
	TIME [epoch: 8.5 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5373718829131111		[learning rate: 1.2085e-05]
	Learning Rate: 1.20855e-05
	LOSS [training: 0.5373718829131111 | validation: 0.6111237858925426]
	TIME [epoch: 8.51 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5328302615899451		[learning rate: 1.2042e-05]
	Learning Rate: 1.20416e-05
	LOSS [training: 0.5328302615899451 | validation: 0.5993234362720343]
	TIME [epoch: 8.5 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5383860296061226		[learning rate: 1.1998e-05]
	Learning Rate: 1.19979e-05
	LOSS [training: 0.5383860296061226 | validation: 0.6029181131196867]
	TIME [epoch: 8.49 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5405163135480875		[learning rate: 1.1954e-05]
	Learning Rate: 1.19544e-05
	LOSS [training: 0.5405163135480875 | validation: 0.6096616573331174]
	TIME [epoch: 8.52 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5426866872808179		[learning rate: 1.1911e-05]
	Learning Rate: 1.1911e-05
	LOSS [training: 0.5426866872808179 | validation: 0.6112996594270286]
	TIME [epoch: 8.51 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5416174070317971		[learning rate: 1.1868e-05]
	Learning Rate: 1.18678e-05
	LOSS [training: 0.5416174070317971 | validation: 0.5977775687787381]
	TIME [epoch: 8.49 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5297413147548273		[learning rate: 1.1825e-05]
	Learning Rate: 1.18247e-05
	LOSS [training: 0.5297413147548273 | validation: 0.6052704134945297]
	TIME [epoch: 8.49 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5426105609559384		[learning rate: 1.1782e-05]
	Learning Rate: 1.17818e-05
	LOSS [training: 0.5426105609559384 | validation: 0.6206281312577522]
	TIME [epoch: 8.5 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5475638843822639		[learning rate: 1.1739e-05]
	Learning Rate: 1.1739e-05
	LOSS [training: 0.5475638843822639 | validation: 0.6098861261664842]
	TIME [epoch: 8.49 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5413233311909009		[learning rate: 1.1696e-05]
	Learning Rate: 1.16964e-05
	LOSS [training: 0.5413233311909009 | validation: 0.6004316571630643]
	TIME [epoch: 8.49 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5285688427559438		[learning rate: 1.1654e-05]
	Learning Rate: 1.1654e-05
	LOSS [training: 0.5285688427559438 | validation: 0.6078243550241714]
	TIME [epoch: 8.5 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5299764860810037		[learning rate: 1.1612e-05]
	Learning Rate: 1.16117e-05
	LOSS [training: 0.5299764860810037 | validation: 0.5987019665420634]
	TIME [epoch: 8.49 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5309213358290118		[learning rate: 1.157e-05]
	Learning Rate: 1.15695e-05
	LOSS [training: 0.5309213358290118 | validation: 0.6029697049482128]
	TIME [epoch: 8.49 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.532438611495744		[learning rate: 1.1528e-05]
	Learning Rate: 1.15275e-05
	LOSS [training: 0.532438611495744 | validation: 0.6256552545945696]
	TIME [epoch: 8.48 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5337950618610777		[learning rate: 1.1486e-05]
	Learning Rate: 1.14857e-05
	LOSS [training: 0.5337950618610777 | validation: 0.5931293119568535]
	TIME [epoch: 8.5 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5279048507303136		[learning rate: 1.1444e-05]
	Learning Rate: 1.1444e-05
	LOSS [training: 0.5279048507303136 | validation: 0.5955141046916411]
	TIME [epoch: 8.48 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5216929739879601		[learning rate: 1.1402e-05]
	Learning Rate: 1.14025e-05
	LOSS [training: 0.5216929739879601 | validation: 0.5894790331867175]
	TIME [epoch: 8.49 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5277792783417294		[learning rate: 1.1361e-05]
	Learning Rate: 1.13611e-05
	LOSS [training: 0.5277792783417294 | validation: 0.5973927761583427]
	TIME [epoch: 8.51 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5243980189396706		[learning rate: 1.132e-05]
	Learning Rate: 1.13199e-05
	LOSS [training: 0.5243980189396706 | validation: 0.6081518913232367]
	TIME [epoch: 8.49 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5265118566295338		[learning rate: 1.1279e-05]
	Learning Rate: 1.12788e-05
	LOSS [training: 0.5265118566295338 | validation: 0.6115133062886248]
	TIME [epoch: 8.48 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.528571529619652		[learning rate: 1.1238e-05]
	Learning Rate: 1.12379e-05
	LOSS [training: 0.528571529619652 | validation: 0.609428094922653]
	TIME [epoch: 8.48 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5242096091672115		[learning rate: 1.1197e-05]
	Learning Rate: 1.11971e-05
	LOSS [training: 0.5242096091672115 | validation: 0.5969664663260423]
	TIME [epoch: 8.51 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5259113301764373		[learning rate: 1.1156e-05]
	Learning Rate: 1.11565e-05
	LOSS [training: 0.5259113301764373 | validation: 0.5998083709239765]
	TIME [epoch: 8.48 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5238864478946352		[learning rate: 1.1116e-05]
	Learning Rate: 1.1116e-05
	LOSS [training: 0.5238864478946352 | validation: 0.6169372763073216]
	TIME [epoch: 8.48 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5218080613938767		[learning rate: 1.1076e-05]
	Learning Rate: 1.10756e-05
	LOSS [training: 0.5218080613938767 | validation: 0.5946480227283438]
	TIME [epoch: 8.5 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.528395241697378		[learning rate: 1.1035e-05]
	Learning Rate: 1.10354e-05
	LOSS [training: 0.528395241697378 | validation: 0.6033071983366455]
	TIME [epoch: 8.48 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5281874763803114		[learning rate: 1.0995e-05]
	Learning Rate: 1.09954e-05
	LOSS [training: 0.5281874763803114 | validation: 0.6020094974324552]
	TIME [epoch: 8.49 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.52978786734985		[learning rate: 1.0955e-05]
	Learning Rate: 1.09555e-05
	LOSS [training: 0.52978786734985 | validation: 0.598708917702699]
	TIME [epoch: 8.49 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5277596026379949		[learning rate: 1.0916e-05]
	Learning Rate: 1.09157e-05
	LOSS [training: 0.5277596026379949 | validation: 0.5979425394432765]
	TIME [epoch: 8.51 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5225647515869956		[learning rate: 1.0876e-05]
	Learning Rate: 1.08761e-05
	LOSS [training: 0.5225647515869956 | validation: 0.598053435494526]
	TIME [epoch: 8.49 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5252785368529225		[learning rate: 1.0837e-05]
	Learning Rate: 1.08366e-05
	LOSS [training: 0.5252785368529225 | validation: 0.6028723195943275]
	TIME [epoch: 8.48 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5236495857942819		[learning rate: 1.0797e-05]
	Learning Rate: 1.07973e-05
	LOSS [training: 0.5236495857942819 | validation: 0.5894057678511456]
	TIME [epoch: 8.5 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5173611221311696		[learning rate: 1.0758e-05]
	Learning Rate: 1.07581e-05
	LOSS [training: 0.5173611221311696 | validation: 0.610799018857214]
	TIME [epoch: 8.5 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5233697808888498		[learning rate: 1.0719e-05]
	Learning Rate: 1.07191e-05
	LOSS [training: 0.5233697808888498 | validation: 0.5934840802291548]
	TIME [epoch: 8.49 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5207555242817798		[learning rate: 1.068e-05]
	Learning Rate: 1.06802e-05
	LOSS [training: 0.5207555242817798 | validation: 0.5890713346535255]
	TIME [epoch: 8.48 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5234865022628782		[learning rate: 1.0641e-05]
	Learning Rate: 1.06414e-05
	LOSS [training: 0.5234865022628782 | validation: 0.5939660141854685]
	TIME [epoch: 8.51 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5282350267183306		[learning rate: 1.0603e-05]
	Learning Rate: 1.06028e-05
	LOSS [training: 0.5282350267183306 | validation: 0.6102721545435326]
	TIME [epoch: 8.49 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5219847740180278		[learning rate: 1.0564e-05]
	Learning Rate: 1.05643e-05
	LOSS [training: 0.5219847740180278 | validation: 0.5832114352880953]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r5_20240219_233648/states/model_tr_study203_1985.pth
	Model improved!!!
EPOCH 1986/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5206961129825239		[learning rate: 1.0526e-05]
	Learning Rate: 1.0526e-05
	LOSS [training: 0.5206961129825239 | validation: 0.5903686687908647]
	TIME [epoch: 8.5 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5249665537251254		[learning rate: 1.0488e-05]
	Learning Rate: 1.04878e-05
	LOSS [training: 0.5249665537251254 | validation: 0.6098839061923538]
	TIME [epoch: 8.5 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5244144030062279		[learning rate: 1.045e-05]
	Learning Rate: 1.04497e-05
	LOSS [training: 0.5244144030062279 | validation: 0.6037742360571996]
	TIME [epoch: 8.48 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5267701500718578		[learning rate: 1.0412e-05]
	Learning Rate: 1.04118e-05
	LOSS [training: 0.5267701500718578 | validation: 0.6057959351201663]
	TIME [epoch: 8.49 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5211784263334208		[learning rate: 1.0374e-05]
	Learning Rate: 1.0374e-05
	LOSS [training: 0.5211784263334208 | validation: 0.6002193850757617]
	TIME [epoch: 8.51 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5260709507537511		[learning rate: 1.0336e-05]
	Learning Rate: 1.03364e-05
	LOSS [training: 0.5260709507537511 | validation: 0.6039112888169254]
	TIME [epoch: 8.49 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5346768941072716		[learning rate: 1.0299e-05]
	Learning Rate: 1.02989e-05
	LOSS [training: 0.5346768941072716 | validation: 0.6044207040813319]
	TIME [epoch: 8.49 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5338564673353968		[learning rate: 1.0261e-05]
	Learning Rate: 1.02615e-05
	LOSS [training: 0.5338564673353968 | validation: 0.609653304412862]
	TIME [epoch: 8.5 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5295708705722841		[learning rate: 1.0224e-05]
	Learning Rate: 1.02243e-05
	LOSS [training: 0.5295708705722841 | validation: 0.6119817831968909]
	TIME [epoch: 8.5 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.528401627989731		[learning rate: 1.0187e-05]
	Learning Rate: 1.01872e-05
	LOSS [training: 0.528401627989731 | validation: 0.6127262904137598]
	TIME [epoch: 8.49 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5230512448666532		[learning rate: 1.015e-05]
	Learning Rate: 1.01502e-05
	LOSS [training: 0.5230512448666532 | validation: 0.596112305934948]
	TIME [epoch: 8.48 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5282663647325652		[learning rate: 1.0113e-05]
	Learning Rate: 1.01133e-05
	LOSS [training: 0.5282663647325652 | validation: 0.6032678178966874]
	TIME [epoch: 8.51 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5243888839496158		[learning rate: 1.0077e-05]
	Learning Rate: 1.00766e-05
	LOSS [training: 0.5243888839496158 | validation: 0.5994854784565488]
	TIME [epoch: 8.5 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5157239251325982		[learning rate: 1.004e-05]
	Learning Rate: 1.00401e-05
	LOSS [training: 0.5157239251325982 | validation: 0.5971140453715537]
	TIME [epoch: 8.48 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5190172083699489		[learning rate: 1.0004e-05]
	Learning Rate: 1.00036e-05
	LOSS [training: 0.5190172083699489 | validation: 0.5947219447807874]
	TIME [epoch: 8.49 sec]
Finished training in 17249.287 seconds.
