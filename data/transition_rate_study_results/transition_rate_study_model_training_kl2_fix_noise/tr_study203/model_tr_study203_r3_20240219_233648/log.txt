Args:
Namespace(name='model_tr_study203', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3', training_data='data/transition_rate_studies/tr_study203/tr_study203_training/r3', validation_data='data/transition_rate_studies/tr_study203/tr_study203_validation/r3', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3403219301

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/10] avg loss: 12.389311969002563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.389311969002563 | validation: 11.952644522263881]
	TIME [epoch: 79.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/10] avg loss: 10.990534605032057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.990534605032057 | validation: 12.16899049063537]
	TIME [epoch: 8.35 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 10/10] avg loss: 10.467107910716226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.467107910716226 | validation: 10.871662844260799]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.773221310326939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.773221310326939 | validation: 6.771805913671834]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.136572123084585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.136572123084585 | validation: 6.909569880192741]
	TIME [epoch: 8.34 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.554394880801295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.554394880801295 | validation: 6.403127603164256]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.504878115997779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.504878115997779 | validation: 7.171902922400983]
	TIME [epoch: 8.32 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.037989062412171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.037989062412171 | validation: 4.780480107476091]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.465312134111357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.465312134111357 | validation: 6.016255293349436]
	TIME [epoch: 8.33 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.028562502094717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.028562502094717 | validation: 5.170803840488199]
	TIME [epoch: 8.31 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.7034669601350325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7034669601350325 | validation: 5.276966610893435]
	TIME [epoch: 8.3 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.614841297925982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.614841297925982 | validation: 5.077096825523289]
	TIME [epoch: 8.3 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.614725352795127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.614725352795127 | validation: 4.020668768368935]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.52668874207926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.52668874207926 | validation: 4.0656194910087375]
	TIME [epoch: 8.33 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.4693188778994974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4693188778994974 | validation: 4.275606117171809]
	TIME [epoch: 8.3 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.4529664612483675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4529664612483675 | validation: 4.403640207718855]
	TIME [epoch: 8.29 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.367090373818977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.367090373818977 | validation: 4.31107510845332]
	TIME [epoch: 8.29 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.311654984282639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.311654984282639 | validation: 4.340035094172688]
	TIME [epoch: 8.32 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.444582154006804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.444582154006804 | validation: 4.62116932219419]
	TIME [epoch: 8.3 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.401522439952613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.401522439952613 | validation: 5.326260338163098]
	TIME [epoch: 8.3 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.455657935522492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.455657935522492 | validation: 4.542313538269678]
	TIME [epoch: 8.29 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.382752274200803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.382752274200803 | validation: 5.383912997317841]
	TIME [epoch: 8.31 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.27206695275711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.27206695275711 | validation: 3.613015698185037]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.489625981492886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.489625981492886 | validation: 4.574513586715998]
	TIME [epoch: 8.29 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.858608372605639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.858608372605639 | validation: 4.343913962971156]
	TIME [epoch: 8.29 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.396414527790339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.396414527790339 | validation: 4.055168025649184]
	TIME [epoch: 8.29 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.702913361678998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.702913361678998 | validation: 4.422284195250173]
	TIME [epoch: 8.32 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.209529836658395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.209529836658395 | validation: 3.878092780546961]
	TIME [epoch: 8.3 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.183224494740524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.183224494740524 | validation: 5.052457873804615]
	TIME [epoch: 8.3 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.204605773580196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.204605773580196 | validation: 4.171939287643139]
	TIME [epoch: 8.3 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.186120746806152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.186120746806152 | validation: 3.657591495458843]
	TIME [epoch: 8.32 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.123971187520676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.123971187520676 | validation: 4.296093684051715]
	TIME [epoch: 8.29 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.182259031469365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.182259031469365 | validation: 3.616502680792353]
	TIME [epoch: 8.29 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1303662736204085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1303662736204085 | validation: 4.01053109769004]
	TIME [epoch: 8.29 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.06774976170527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.06774976170527 | validation: 4.336849355790964]
	TIME [epoch: 8.31 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.084357294881089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.084357294881089 | validation: 4.305307421801954]
	TIME [epoch: 8.31 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.128783197961211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.128783197961211 | validation: 3.790449002705439]
	TIME [epoch: 8.29 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.161324338686056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.161324338686056 | validation: 3.4898724651670054]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.046724974513357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.046724974513357 | validation: 3.67389437757322]
	TIME [epoch: 8.29 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.024945622345092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.024945622345092 | validation: 4.007477624403545]
	TIME [epoch: 8.33 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.188932665557617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.188932665557617 | validation: 3.9715712832994177]
	TIME [epoch: 8.29 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.074223833853372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.074223833853372 | validation: 3.7427144646650587]
	TIME [epoch: 8.29 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.116218601658437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.116218601658437 | validation: 3.451964194634521]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.670441542658845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.670441542658845 | validation: 3.3389621379334002]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.701735319803822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.701735319803822 | validation: 3.8815843365772755]
	TIME [epoch: 8.3 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1052913450923025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1052913450923025 | validation: 4.479641061187843]
	TIME [epoch: 8.3 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.144182094116124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.144182094116124 | validation: 4.207326120312667]
	TIME [epoch: 8.3 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.085722580260642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.085722580260642 | validation: 4.033216350793825]
	TIME [epoch: 8.32 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.0397269421181825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0397269421181825 | validation: 3.651008307062172]
	TIME [epoch: 8.3 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.8728135460593505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8728135460593505 | validation: 3.9635232004570966]
	TIME [epoch: 8.3 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.357532264597016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.357532264597016 | validation: 4.3460260629076135]
	TIME [epoch: 8.3 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.121677255392839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.121677255392839 | validation: 4.327395875511074]
	TIME [epoch: 8.31 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.7212685546780095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7212685546780095 | validation: 4.098843807041755]
	TIME [epoch: 8.32 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.640371433170203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.640371433170203 | validation: 3.6138204501655826]
	TIME [epoch: 8.3 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.399156802019751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.399156802019751 | validation: 3.468680698490514]
	TIME [epoch: 8.3 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.530026369284131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.530026369284131 | validation: 4.646294907557732]
	TIME [epoch: 8.3 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.897132380763223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.897132380763223 | validation: 3.964982651807384]
	TIME [epoch: 8.33 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9585560977724894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9585560977724894 | validation: 3.660471810919371]
	TIME [epoch: 8.3 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.129345630788746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.129345630788746 | validation: 3.5435369121601408]
	TIME [epoch: 8.3 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.838930923174442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.838930923174442 | validation: 3.5055220414742236]
	TIME [epoch: 8.3 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7579021341687002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7579021341687002 | validation: 3.9727385352062123]
	TIME [epoch: 8.31 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9309169540747364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9309169540747364 | validation: 3.5916851762415036]
	TIME [epoch: 8.31 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.888743185466854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.888743185466854 | validation: 3.5289780925122445]
	TIME [epoch: 8.3 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7732910932912667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7732910932912667 | validation: 3.5450659570497782]
	TIME [epoch: 8.3 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.000893848910168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.000893848910168 | validation: 3.784559420208226]
	TIME [epoch: 8.3 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.94658389815665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.94658389815665 | validation: 3.6782618596902736]
	TIME [epoch: 8.33 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9229171863542738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9229171863542738 | validation: 4.0755696266564225]
	TIME [epoch: 8.3 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.716465835219003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.716465835219003 | validation: 5.967360625787331]
	TIME [epoch: 8.3 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.87418126426571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.87418126426571 | validation: 5.688023868988321]
	TIME [epoch: 8.3 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.3929235654748915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3929235654748915 | validation: 5.13457412561283]
	TIME [epoch: 8.33 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.551716463864105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.551716463864105 | validation: 3.551035677080062]
	TIME [epoch: 8.3 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8269072524458885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8269072524458885 | validation: 3.8715295174043343]
	TIME [epoch: 8.3 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7701711060388554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7701711060388554 | validation: 3.8664215118778813]
	TIME [epoch: 8.3 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.95960511512902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.95960511512902 | validation: 4.534406543776887]
	TIME [epoch: 8.31 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9221514937263238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9221514937263238 | validation: 4.572721338957015]
	TIME [epoch: 8.31 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8615796056195557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8615796056195557 | validation: 3.913578340175156]
	TIME [epoch: 8.3 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7618632743944573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7618632743944573 | validation: 6.351587487030322]
	TIME [epoch: 8.29 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.323181534463215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.323181534463215 | validation: 5.855559884562208]
	TIME [epoch: 8.3 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.706684888463157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.706684888463157 | validation: 4.155052626348172]
	TIME [epoch: 8.33 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.835578460285089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.835578460285089 | validation: 7.6039885147332456]
	TIME [epoch: 8.3 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.294560178394768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.294560178394768 | validation: 6.310973409770195]
	TIME [epoch: 8.3 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.0137871671440735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.0137871671440735 | validation: 5.928766255586247]
	TIME [epoch: 8.3 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.942708748603275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.942708748603275 | validation: 6.058021784543785]
	TIME [epoch: 8.33 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.770053159414383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.770053159414383 | validation: 4.550552073895166]
	TIME [epoch: 8.3 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.151652456945348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.151652456945348 | validation: 5.357069016019562]
	TIME [epoch: 8.3 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.19467807966757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.19467807966757 | validation: 4.252209407071005]
	TIME [epoch: 8.29 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.104370086930343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.104370086930343 | validation: 4.352716481037827]
	TIME [epoch: 8.32 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8284026112629634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8284026112629634 | validation: 3.6063523091134364]
	TIME [epoch: 8.31 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7520092031697425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7520092031697425 | validation: 3.881558290704457]
	TIME [epoch: 8.3 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.221147221428715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.221147221428715 | validation: 3.447030861516717]
	TIME [epoch: 8.3 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.85665185293471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.85665185293471 | validation: 3.526405104468046]
	TIME [epoch: 8.3 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.703936224002794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.703936224002794 | validation: 3.6288639164661003]
	TIME [epoch: 8.33 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8626006904064467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8626006904064467 | validation: 3.4180705675303162]
	TIME [epoch: 8.3 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.735189428802542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.735189428802542 | validation: 3.818915272151343]
	TIME [epoch: 8.3 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6542774504998414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6542774504998414 | validation: 3.973354626135298]
	TIME [epoch: 8.3 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7937362501813063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7937362501813063 | validation: 3.4457990520765147]
	TIME [epoch: 8.32 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.938028509420717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.938028509420717 | validation: 3.861279444975324]
	TIME [epoch: 8.3 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7437311171773198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7437311171773198 | validation: 3.7090440452812556]
	TIME [epoch: 8.3 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7410216537263294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7410216537263294 | validation: 3.6020902287467687]
	TIME [epoch: 8.3 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.710347329681869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.710347329681869 | validation: 3.3851681094760706]
	TIME [epoch: 8.3 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.695941359305813		[learning rate: 0.0099673]
	Learning Rate: 0.00996733
	LOSS [training: 3.695941359305813 | validation: 3.6091785892814876]
	TIME [epoch: 8.32 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6144152141327046		[learning rate: 0.0099312]
	Learning Rate: 0.00993116
	LOSS [training: 3.6144152141327046 | validation: 3.4852417856482045]
	TIME [epoch: 8.29 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.851216186186716		[learning rate: 0.0098951]
	Learning Rate: 0.00989512
	LOSS [training: 3.851216186186716 | validation: 3.4821877769234906]
	TIME [epoch: 8.29 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.674471006992339		[learning rate: 0.0098592]
	Learning Rate: 0.00985921
	LOSS [training: 3.674471006992339 | validation: 3.5982179471677576]
	TIME [epoch: 8.3 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.646940329014895		[learning rate: 0.0098234]
	Learning Rate: 0.00982343
	LOSS [training: 3.646940329014895 | validation: 3.7720969465569407]
	TIME [epoch: 8.33 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.665481323028221		[learning rate: 0.0097878]
	Learning Rate: 0.00978778
	LOSS [training: 3.665481323028221 | validation: 3.576769421021689]
	TIME [epoch: 8.31 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.74019633500793		[learning rate: 0.0097523]
	Learning Rate: 0.00975226
	LOSS [training: 3.74019633500793 | validation: 3.62380871885218]
	TIME [epoch: 8.29 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6339471001200208		[learning rate: 0.0097169]
	Learning Rate: 0.00971687
	LOSS [training: 3.6339471001200208 | validation: 3.5310211609728013]
	TIME [epoch: 8.3 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5646622178081606		[learning rate: 0.0096816]
	Learning Rate: 0.0096816
	LOSS [training: 3.5646622178081606 | validation: 4.06282294741388]
	TIME [epoch: 8.32 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7258182206197206		[learning rate: 0.0096465]
	Learning Rate: 0.00964647
	LOSS [training: 3.7258182206197206 | validation: 3.9668118017135225]
	TIME [epoch: 8.3 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.210873020963195		[learning rate: 0.0096115]
	Learning Rate: 0.00961146
	LOSS [training: 4.210873020963195 | validation: 3.9439193801533317]
	TIME [epoch: 8.3 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.670607069344103		[learning rate: 0.0095766]
	Learning Rate: 0.00957658
	LOSS [training: 4.670607069344103 | validation: 3.66297658720102]
	TIME [epoch: 8.3 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.313425753416297		[learning rate: 0.0095418]
	Learning Rate: 0.00954183
	LOSS [training: 4.313425753416297 | validation: 3.2335267222693287]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6121694468279246		[learning rate: 0.0095072]
	Learning Rate: 0.0095072
	LOSS [training: 3.6121694468279246 | validation: 3.1571014546134286]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.582305177691796		[learning rate: 0.0094727]
	Learning Rate: 0.0094727
	LOSS [training: 3.582305177691796 | validation: 3.6351122249751473]
	TIME [epoch: 8.31 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5917296660880935		[learning rate: 0.0094383]
	Learning Rate: 0.00943832
	LOSS [training: 3.5917296660880935 | validation: 4.1864671571395]
	TIME [epoch: 8.3 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6500214867704566		[learning rate: 0.0094041]
	Learning Rate: 0.00940407
	LOSS [training: 3.6500214867704566 | validation: 3.299733417443383]
	TIME [epoch: 8.31 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.2128881799963525		[learning rate: 0.0093699]
	Learning Rate: 0.00936994
	LOSS [training: 4.2128881799963525 | validation: 5.0385776113598695]
	TIME [epoch: 8.33 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.020771314968886		[learning rate: 0.0093359]
	Learning Rate: 0.00933594
	LOSS [training: 4.020771314968886 | validation: 3.8504568653070255]
	TIME [epoch: 8.3 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.564715927869534		[learning rate: 0.0093021]
	Learning Rate: 0.00930206
	LOSS [training: 3.564715927869534 | validation: 3.516899436012524]
	TIME [epoch: 8.3 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5756581736125215		[learning rate: 0.0092683]
	Learning Rate: 0.0092683
	LOSS [training: 3.5756581736125215 | validation: 5.38236946795787]
	TIME [epoch: 8.31 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.925316217689344		[learning rate: 0.0092347]
	Learning Rate: 0.00923466
	LOSS [training: 3.925316217689344 | validation: 3.5112401185019646]
	TIME [epoch: 8.32 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6025612243639236		[learning rate: 0.0092011]
	Learning Rate: 0.00920115
	LOSS [training: 3.6025612243639236 | validation: 3.1452935858009567]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.554497093992674		[learning rate: 0.0091678]
	Learning Rate: 0.00916776
	LOSS [training: 3.554497093992674 | validation: 3.642502014014787]
	TIME [epoch: 8.31 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.540409211789561		[learning rate: 0.0091345]
	Learning Rate: 0.00913449
	LOSS [training: 3.540409211789561 | validation: 3.356419727741539]
	TIME [epoch: 8.3 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6306883904645013		[learning rate: 0.0091013]
	Learning Rate: 0.00910134
	LOSS [training: 3.6306883904645013 | validation: 3.17722367086646]
	TIME [epoch: 8.32 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7582022519836036		[learning rate: 0.0090683]
	Learning Rate: 0.00906831
	LOSS [training: 3.7582022519836036 | validation: 6.536075913353988]
	TIME [epoch: 8.32 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.529856438984278		[learning rate: 0.0090354]
	Learning Rate: 0.0090354
	LOSS [training: 4.529856438984278 | validation: 5.149092519315934]
	TIME [epoch: 8.28 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.22389907676852		[learning rate: 0.0090026]
	Learning Rate: 0.00900261
	LOSS [training: 4.22389907676852 | validation: 5.017554117262708]
	TIME [epoch: 8.3 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.851445495297086		[learning rate: 0.0089699]
	Learning Rate: 0.00896994
	LOSS [training: 3.851445495297086 | validation: 3.3373692572342226]
	TIME [epoch: 8.31 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.564918947166799		[learning rate: 0.0089374]
	Learning Rate: 0.00893739
	LOSS [training: 3.564918947166799 | validation: 4.597423417028343]
	TIME [epoch: 8.33 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.097507039368028		[learning rate: 0.008905]
	Learning Rate: 0.00890495
	LOSS [training: 4.097507039368028 | validation: 3.5560774933732553]
	TIME [epoch: 8.3 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5574081409896		[learning rate: 0.0088726]
	Learning Rate: 0.00887263
	LOSS [training: 3.5574081409896 | validation: 3.302059081000462]
	TIME [epoch: 8.3 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4711777466384293		[learning rate: 0.0088404]
	Learning Rate: 0.00884044
	LOSS [training: 3.4711777466384293 | validation: 3.9621358384582726]
	TIME [epoch: 8.3 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5670070243493806		[learning rate: 0.0088084]
	Learning Rate: 0.00880835
	LOSS [training: 3.5670070243493806 | validation: 6.009623156323689]
	TIME [epoch: 8.31 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.103941024480553		[learning rate: 0.0087764]
	Learning Rate: 0.00877639
	LOSS [training: 4.103941024480553 | validation: 4.652358077255029]
	TIME [epoch: 8.3 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.79730710541721		[learning rate: 0.0087445]
	Learning Rate: 0.00874454
	LOSS [training: 3.79730710541721 | validation: 3.69297005943067]
	TIME [epoch: 8.31 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6777593175515255		[learning rate: 0.0087128]
	Learning Rate: 0.0087128
	LOSS [training: 3.6777593175515255 | validation: 2.9954947484992664]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.050216025863029		[learning rate: 0.0086812]
	Learning Rate: 0.00868118
	LOSS [training: 4.050216025863029 | validation: 3.9625897963680448]
	TIME [epoch: 8.32 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.291692294771465		[learning rate: 0.0086497]
	Learning Rate: 0.00864968
	LOSS [training: 4.291692294771465 | validation: 3.3307410037865433]
	TIME [epoch: 8.33 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.147049845687748		[learning rate: 0.0086183]
	Learning Rate: 0.00861829
	LOSS [training: 4.147049845687748 | validation: 3.3104655008106922]
	TIME [epoch: 8.31 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.095155771123531		[learning rate: 0.008587]
	Learning Rate: 0.00858701
	LOSS [training: 4.095155771123531 | validation: 3.27861665814281]
	TIME [epoch: 8.31 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.092738774613417		[learning rate: 0.0085559]
	Learning Rate: 0.00855585
	LOSS [training: 4.092738774613417 | validation: 3.2467874902924443]
	TIME [epoch: 8.31 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4157982246950374		[learning rate: 0.0085248]
	Learning Rate: 0.0085248
	LOSS [training: 3.4157982246950374 | validation: 3.725698995527053]
	TIME [epoch: 8.34 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.569353782545247		[learning rate: 0.0084939]
	Learning Rate: 0.00849386
	LOSS [training: 3.569353782545247 | validation: 6.2078986503619]
	TIME [epoch: 8.31 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.265813781357442		[learning rate: 0.008463]
	Learning Rate: 0.00846304
	LOSS [training: 4.265813781357442 | validation: 4.829772239567356]
	TIME [epoch: 8.31 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.0423255110192695		[learning rate: 0.0084323]
	Learning Rate: 0.00843233
	LOSS [training: 4.0423255110192695 | validation: 4.47552695719591]
	TIME [epoch: 8.31 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8076364440900696		[learning rate: 0.0084017]
	Learning Rate: 0.00840172
	LOSS [training: 3.8076364440900696 | validation: 4.228062529610142]
	TIME [epoch: 8.33 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.525407560163103		[learning rate: 0.0083712]
	Learning Rate: 0.00837123
	LOSS [training: 3.525407560163103 | validation: 3.251946393101875]
	TIME [epoch: 8.32 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.293992867951123		[learning rate: 0.0083409]
	Learning Rate: 0.00834085
	LOSS [training: 3.293992867951123 | validation: 5.700892319913991]
	TIME [epoch: 8.31 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.234528694460569		[learning rate: 0.0083106]
	Learning Rate: 0.00831059
	LOSS [training: 4.234528694460569 | validation: 3.641901300870871]
	TIME [epoch: 8.31 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.571359278636872		[learning rate: 0.0082804]
	Learning Rate: 0.00828043
	LOSS [training: 3.571359278636872 | validation: 3.7872054380976463]
	TIME [epoch: 8.32 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.905386067208061		[learning rate: 0.0082504]
	Learning Rate: 0.00825037
	LOSS [training: 3.905386067208061 | validation: 3.351046158151199]
	TIME [epoch: 8.33 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.882274054553742		[learning rate: 0.0082204]
	Learning Rate: 0.00822043
	LOSS [training: 3.882274054553742 | validation: 2.9384389676925933]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9526023626741003		[learning rate: 0.0081906]
	Learning Rate: 0.0081906
	LOSS [training: 3.9526023626741003 | validation: 3.10608163933433]
	TIME [epoch: 8.31 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3602463069371		[learning rate: 0.0081609]
	Learning Rate: 0.00816088
	LOSS [training: 3.3602463069371 | validation: 3.1828016774950303]
	TIME [epoch: 8.3 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3050465236256614		[learning rate: 0.0081313]
	Learning Rate: 0.00813126
	LOSS [training: 3.3050465236256614 | validation: 3.3836605884002604]
	TIME [epoch: 8.33 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3100329332334626		[learning rate: 0.0081018]
	Learning Rate: 0.00810175
	LOSS [training: 3.3100329332334626 | validation: 3.0908365642345395]
	TIME [epoch: 8.3 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3772776602990446		[learning rate: 0.0080724]
	Learning Rate: 0.00807235
	LOSS [training: 3.3772776602990446 | validation: 4.048112111409944]
	TIME [epoch: 8.31 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.435648271447157		[learning rate: 0.0080431]
	Learning Rate: 0.00804305
	LOSS [training: 3.435648271447157 | validation: 3.3770764298544114]
	TIME [epoch: 8.3 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4785721284435995		[learning rate: 0.0080139]
	Learning Rate: 0.00801387
	LOSS [training: 3.4785721284435995 | validation: 3.796028811920732]
	TIME [epoch: 8.32 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5842659711191316		[learning rate: 0.0079848]
	Learning Rate: 0.00798478
	LOSS [training: 3.5842659711191316 | validation: 3.544302398224052]
	TIME [epoch: 8.31 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.328603413278278		[learning rate: 0.0079558]
	Learning Rate: 0.00795581
	LOSS [training: 3.328603413278278 | validation: 3.185758576226366]
	TIME [epoch: 8.3 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6286187697170975		[learning rate: 0.0079269]
	Learning Rate: 0.00792693
	LOSS [training: 3.6286187697170975 | validation: 3.008940744995183]
	TIME [epoch: 8.3 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2302527286256923		[learning rate: 0.0078982]
	Learning Rate: 0.00789817
	LOSS [training: 3.2302527286256923 | validation: 3.847850093096124]
	TIME [epoch: 8.31 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.935875996580483		[learning rate: 0.0078695]
	Learning Rate: 0.0078695
	LOSS [training: 3.935875996580483 | validation: 4.649353835246373]
	TIME [epoch: 8.32 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8862424861824367		[learning rate: 0.0078409]
	Learning Rate: 0.00784094
	LOSS [training: 3.8862424861824367 | validation: 3.079817638179545]
	TIME [epoch: 8.3 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3190410873270566		[learning rate: 0.0078125]
	Learning Rate: 0.00781249
	LOSS [training: 3.3190410873270566 | validation: 3.5448311194710174]
	TIME [epoch: 8.3 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4991647123373917		[learning rate: 0.0077841]
	Learning Rate: 0.00778414
	LOSS [training: 3.4991647123373917 | validation: 3.814542816975068]
	TIME [epoch: 8.31 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3364388103251366		[learning rate: 0.0077559]
	Learning Rate: 0.00775589
	LOSS [training: 3.3364388103251366 | validation: 3.0510660212050427]
	TIME [epoch: 8.33 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2439772942463803		[learning rate: 0.0077277]
	Learning Rate: 0.00772774
	LOSS [training: 3.2439772942463803 | validation: 3.359374151456059]
	TIME [epoch: 8.31 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2742496757303927		[learning rate: 0.0076997]
	Learning Rate: 0.0076997
	LOSS [training: 3.2742496757303927 | validation: 3.170675780596479]
	TIME [epoch: 8.3 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.431298809691242		[learning rate: 0.0076718]
	Learning Rate: 0.00767176
	LOSS [training: 3.431298809691242 | validation: 3.352280738871629]
	TIME [epoch: 8.3 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3325739083236883		[learning rate: 0.0076439]
	Learning Rate: 0.00764391
	LOSS [training: 3.3325739083236883 | validation: 3.048936409922029]
	TIME [epoch: 8.32 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.290036696059507		[learning rate: 0.0076162]
	Learning Rate: 0.00761617
	LOSS [training: 3.290036696059507 | validation: 3.06271919039131]
	TIME [epoch: 8.31 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4083803984132777		[learning rate: 0.0075885]
	Learning Rate: 0.00758853
	LOSS [training: 3.4083803984132777 | validation: 3.007755394691994]
	TIME [epoch: 8.31 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.436563613109852		[learning rate: 0.007561]
	Learning Rate: 0.00756099
	LOSS [training: 3.436563613109852 | validation: 3.0604135301580087]
	TIME [epoch: 8.3 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.388662703405243		[learning rate: 0.0075336]
	Learning Rate: 0.00753356
	LOSS [training: 3.388662703405243 | validation: 3.115727808337633]
	TIME [epoch: 8.3 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.225807332011141		[learning rate: 0.0075062]
	Learning Rate: 0.00750622
	LOSS [training: 3.225807332011141 | validation: 2.958572536920128]
	TIME [epoch: 8.33 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2383593787553453		[learning rate: 0.007479]
	Learning Rate: 0.00747897
	LOSS [training: 3.2383593787553453 | validation: 3.1479858299675114]
	TIME [epoch: 8.3 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.225729570088299		[learning rate: 0.0074518]
	Learning Rate: 0.00745183
	LOSS [training: 3.225729570088299 | validation: 2.9988035359638827]
	TIME [epoch: 8.3 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2415127287862795		[learning rate: 0.0074248]
	Learning Rate: 0.00742479
	LOSS [training: 3.2415127287862795 | validation: 3.1703950463623904]
	TIME [epoch: 8.3 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3111568254096833		[learning rate: 0.0073978]
	Learning Rate: 0.00739784
	LOSS [training: 3.3111568254096833 | validation: 3.017179180914746]
	TIME [epoch: 8.32 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2145406064235464		[learning rate: 0.007371]
	Learning Rate: 0.007371
	LOSS [training: 3.2145406064235464 | validation: 3.0106168825359347]
	TIME [epoch: 8.3 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.222622861191418		[learning rate: 0.0073442]
	Learning Rate: 0.00734425
	LOSS [training: 3.222622861191418 | validation: 3.0652339456729614]
	TIME [epoch: 8.3 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.286570737815258		[learning rate: 0.0073176]
	Learning Rate: 0.0073176
	LOSS [training: 3.286570737815258 | validation: 2.8564616266810687]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.392524834737782		[learning rate: 0.007291]
	Learning Rate: 0.00729104
	LOSS [training: 3.392524834737782 | validation: 3.459254956300768]
	TIME [epoch: 8.32 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2139093292909364		[learning rate: 0.0072646]
	Learning Rate: 0.00726458
	LOSS [training: 3.2139093292909364 | validation: 3.827639302212427]
	TIME [epoch: 8.31 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3382476222035704		[learning rate: 0.0072382]
	Learning Rate: 0.00723822
	LOSS [training: 3.3382476222035704 | validation: 3.028948581078329]
	TIME [epoch: 8.31 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3398636544524294		[learning rate: 0.0072119]
	Learning Rate: 0.00721195
	LOSS [training: 3.3398636544524294 | validation: 3.1360712017930426]
	TIME [epoch: 8.3 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.387634942357508		[learning rate: 0.0071858]
	Learning Rate: 0.00718578
	LOSS [training: 3.387634942357508 | validation: 3.0145159970359856]
	TIME [epoch: 8.3 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.263403675483628		[learning rate: 0.0071597]
	Learning Rate: 0.0071597
	LOSS [training: 3.263403675483628 | validation: 2.9964870711930613]
	TIME [epoch: 8.33 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.273268436748277		[learning rate: 0.0071337]
	Learning Rate: 0.00713371
	LOSS [training: 3.273268436748277 | validation: 3.442850294509964]
	TIME [epoch: 8.3 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1674136960002386		[learning rate: 0.0071078]
	Learning Rate: 0.00710783
	LOSS [training: 3.1674136960002386 | validation: 2.855950451913953]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.192672548729349		[learning rate: 0.007082]
	Learning Rate: 0.00708203
	LOSS [training: 3.192672548729349 | validation: 3.1400929303767757]
	TIME [epoch: 8.3 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.332087425961339		[learning rate: 0.0070563]
	Learning Rate: 0.00705633
	LOSS [training: 3.332087425961339 | validation: 2.9939875386843173]
	TIME [epoch: 8.34 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.193914670267996		[learning rate: 0.0070307]
	Learning Rate: 0.00703072
	LOSS [training: 3.193914670267996 | validation: 3.3836704184575117]
	TIME [epoch: 8.3 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.107223753404919		[learning rate: 0.0070052]
	Learning Rate: 0.00700521
	LOSS [training: 3.107223753404919 | validation: 3.1174747783186785]
	TIME [epoch: 8.3 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.192903342318215		[learning rate: 0.0069798]
	Learning Rate: 0.00697979
	LOSS [training: 3.192903342318215 | validation: 3.0370056168277832]
	TIME [epoch: 8.3 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4655297828187814		[learning rate: 0.0069545]
	Learning Rate: 0.00695446
	LOSS [training: 3.4655297828187814 | validation: 2.914415333713854]
	TIME [epoch: 8.32 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.201477803515405		[learning rate: 0.0069292]
	Learning Rate: 0.00692922
	LOSS [training: 3.201477803515405 | validation: 2.8156034124473224]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1337030968777593		[learning rate: 0.0069041]
	Learning Rate: 0.00690407
	LOSS [training: 3.1337030968777593 | validation: 2.8945939156342453]
	TIME [epoch: 8.3 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2362519765066793		[learning rate: 0.006879]
	Learning Rate: 0.00687902
	LOSS [training: 3.2362519765066793 | validation: 2.892570242868725]
	TIME [epoch: 8.29 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.035188821607151		[learning rate: 0.0068541]
	Learning Rate: 0.00685405
	LOSS [training: 3.035188821607151 | validation: 3.1614459292290604]
	TIME [epoch: 8.31 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1657872121862622		[learning rate: 0.0068292]
	Learning Rate: 0.00682918
	LOSS [training: 3.1657872121862622 | validation: 2.856243745866625]
	TIME [epoch: 8.32 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3395938846973463		[learning rate: 0.0068044]
	Learning Rate: 0.00680439
	LOSS [training: 3.3395938846973463 | validation: 2.9627078124784223]
	TIME [epoch: 8.3 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1968433234998566		[learning rate: 0.0067797]
	Learning Rate: 0.0067797
	LOSS [training: 3.1968433234998566 | validation: 2.9345647331486213]
	TIME [epoch: 8.3 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.297182132842619		[learning rate: 0.0067551]
	Learning Rate: 0.0067551
	LOSS [training: 3.297182132842619 | validation: 3.620817794163735]
	TIME [epoch: 8.3 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1904655448286996		[learning rate: 0.0067306]
	Learning Rate: 0.00673058
	LOSS [training: 3.1904655448286996 | validation: 2.917368086670435]
	TIME [epoch: 8.32 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2557149008489668		[learning rate: 0.0067062]
	Learning Rate: 0.00670616
	LOSS [training: 3.2557149008489668 | validation: 2.9507809137670318]
	TIME [epoch: 8.3 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3751716317928375		[learning rate: 0.0066818]
	Learning Rate: 0.00668182
	LOSS [training: 3.3751716317928375 | validation: 3.642012663588024]
	TIME [epoch: 8.3 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3524889843064054		[learning rate: 0.0066576]
	Learning Rate: 0.00665757
	LOSS [training: 3.3524889843064054 | validation: 4.14579213957068]
	TIME [epoch: 8.29 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2780052911124793		[learning rate: 0.0066334]
	Learning Rate: 0.00663341
	LOSS [training: 3.2780052911124793 | validation: 3.0110191325093947]
	TIME [epoch: 8.32 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1223483786747166		[learning rate: 0.0066093]
	Learning Rate: 0.00660934
	LOSS [training: 3.1223483786747166 | validation: 2.8780322116041273]
	TIME [epoch: 8.31 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0489094914301775		[learning rate: 0.0065854]
	Learning Rate: 0.00658535
	LOSS [training: 3.0489094914301775 | validation: 3.064145453133481]
	TIME [epoch: 8.3 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1384323309668747		[learning rate: 0.0065615]
	Learning Rate: 0.00656145
	LOSS [training: 3.1384323309668747 | validation: 3.1139352519052275]
	TIME [epoch: 8.3 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0648647787005685		[learning rate: 0.0065376]
	Learning Rate: 0.00653764
	LOSS [training: 3.0648647787005685 | validation: 3.6692210627115074]
	TIME [epoch: 8.34 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.178241212414227		[learning rate: 0.0065139]
	Learning Rate: 0.00651392
	LOSS [training: 3.178241212414227 | validation: 3.003332467503143]
	TIME [epoch: 8.33 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1343500454196263		[learning rate: 0.0064903]
	Learning Rate: 0.00649028
	LOSS [training: 3.1343500454196263 | validation: 2.8345390284321725]
	TIME [epoch: 8.3 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1096069664507033		[learning rate: 0.0064667]
	Learning Rate: 0.00646672
	LOSS [training: 3.1096069664507033 | validation: 2.9806723358776903]
	TIME [epoch: 8.3 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.168107656004495		[learning rate: 0.0064433]
	Learning Rate: 0.00644325
	LOSS [training: 3.168107656004495 | validation: 2.870079302326877]
	TIME [epoch: 8.3 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0817396196849867		[learning rate: 0.0064199]
	Learning Rate: 0.00641987
	LOSS [training: 3.0817396196849867 | validation: 2.981173021113648]
	TIME [epoch: 8.32 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1347478794064743		[learning rate: 0.0063966]
	Learning Rate: 0.00639657
	LOSS [training: 3.1347478794064743 | validation: 3.2046435507665842]
	TIME [epoch: 8.3 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0509048184196		[learning rate: 0.0063734]
	Learning Rate: 0.00637336
	LOSS [training: 3.0509048184196 | validation: 3.0040669420721926]
	TIME [epoch: 8.3 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2047376075687177		[learning rate: 0.0063502]
	Learning Rate: 0.00635023
	LOSS [training: 3.2047376075687177 | validation: 3.2312966365298115]
	TIME [epoch: 8.3 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.044312522994116		[learning rate: 0.0063272]
	Learning Rate: 0.00632718
	LOSS [training: 3.044312522994116 | validation: 2.825337775854584]
	TIME [epoch: 8.31 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0198989387909903		[learning rate: 0.0063042]
	Learning Rate: 0.00630422
	LOSS [training: 3.0198989387909903 | validation: 3.1634670603661075]
	TIME [epoch: 8.31 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.008352741181172		[learning rate: 0.0062813]
	Learning Rate: 0.00628134
	LOSS [training: 3.008352741181172 | validation: 2.996180036496454]
	TIME [epoch: 8.3 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.057164007338815		[learning rate: 0.0062585]
	Learning Rate: 0.00625855
	LOSS [training: 3.057164007338815 | validation: 3.541609592332077]
	TIME [epoch: 8.3 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.176218959473661		[learning rate: 0.0062358]
	Learning Rate: 0.00623584
	LOSS [training: 3.176218959473661 | validation: 2.8488290820708775]
	TIME [epoch: 8.3 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.118351418700926		[learning rate: 0.0062132]
	Learning Rate: 0.00621321
	LOSS [training: 3.118351418700926 | validation: 2.936501703680073]
	TIME [epoch: 8.33 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.059657816754408		[learning rate: 0.0061907]
	Learning Rate: 0.00619066
	LOSS [training: 3.059657816754408 | validation: 2.8630070605368605]
	TIME [epoch: 8.3 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1363631226052866		[learning rate: 0.0061682]
	Learning Rate: 0.00616819
	LOSS [training: 3.1363631226052866 | validation: 2.8888342049051134]
	TIME [epoch: 8.31 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0925342604543613		[learning rate: 0.0061458]
	Learning Rate: 0.00614581
	LOSS [training: 3.0925342604543613 | validation: 3.000983447500904]
	TIME [epoch: 8.3 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0406197284572434		[learning rate: 0.0061235]
	Learning Rate: 0.0061235
	LOSS [training: 3.0406197284572434 | validation: 2.7950660796329827]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.962487228918615		[learning rate: 0.0061013]
	Learning Rate: 0.00610128
	LOSS [training: 2.962487228918615 | validation: 2.8356016264781903]
	TIME [epoch: 8.32 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9717542228958984		[learning rate: 0.0060791]
	Learning Rate: 0.00607914
	LOSS [training: 2.9717542228958984 | validation: 3.100352518299882]
	TIME [epoch: 8.31 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2795947920118964		[learning rate: 0.0060571]
	Learning Rate: 0.00605708
	LOSS [training: 3.2795947920118964 | validation: 3.353955926524326]
	TIME [epoch: 8.31 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2190540516482713		[learning rate: 0.0060351]
	Learning Rate: 0.0060351
	LOSS [training: 3.2190540516482713 | validation: 3.3877142324205574]
	TIME [epoch: 8.33 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.084141270143556		[learning rate: 0.0060132]
	Learning Rate: 0.00601319
	LOSS [training: 3.084141270143556 | validation: 2.8395473287680337]
	TIME [epoch: 8.32 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9688846783276843		[learning rate: 0.0059914]
	Learning Rate: 0.00599137
	LOSS [training: 2.9688846783276843 | validation: 2.9756842174680633]
	TIME [epoch: 8.31 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.050268047879187		[learning rate: 0.0059696]
	Learning Rate: 0.00596963
	LOSS [training: 3.050268047879187 | validation: 3.1435982313639395]
	TIME [epoch: 8.3 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7150271555022285		[learning rate: 0.005948]
	Learning Rate: 0.00594797
	LOSS [training: 3.7150271555022285 | validation: 2.8293130418007912]
	TIME [epoch: 8.3 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.104741517776565		[learning rate: 0.0059264]
	Learning Rate: 0.00592638
	LOSS [training: 3.104741517776565 | validation: 3.0089188268143943]
	TIME [epoch: 8.32 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0061645424553283		[learning rate: 0.0059049]
	Learning Rate: 0.00590487
	LOSS [training: 3.0061645424553283 | validation: 2.80964410459932]
	TIME [epoch: 8.3 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0402559187867597		[learning rate: 0.0058834]
	Learning Rate: 0.00588344
	LOSS [training: 3.0402559187867597 | validation: 3.2986135175259252]
	TIME [epoch: 8.29 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0104300718954566		[learning rate: 0.0058621]
	Learning Rate: 0.00586209
	LOSS [training: 3.0104300718954566 | validation: 2.825527724516515]
	TIME [epoch: 8.3 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.02539144651476		[learning rate: 0.0058408]
	Learning Rate: 0.00584082
	LOSS [training: 3.02539144651476 | validation: 3.066310007532369]
	TIME [epoch: 8.32 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9830090078421216		[learning rate: 0.0058196]
	Learning Rate: 0.00581962
	LOSS [training: 2.9830090078421216 | validation: 2.9705259735669527]
	TIME [epoch: 8.3 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1277944938235707		[learning rate: 0.0057985]
	Learning Rate: 0.0057985
	LOSS [training: 3.1277944938235707 | validation: 3.003416877385085]
	TIME [epoch: 8.29 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.99933325226293		[learning rate: 0.0057775]
	Learning Rate: 0.00577746
	LOSS [training: 2.99933325226293 | validation: 3.135296176336751]
	TIME [epoch: 8.3 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.991281143283252		[learning rate: 0.0057565]
	Learning Rate: 0.00575649
	LOSS [training: 2.991281143283252 | validation: 2.951318848759044]
	TIME [epoch: 8.31 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9803207596912933		[learning rate: 0.0057356]
	Learning Rate: 0.0057356
	LOSS [training: 2.9803207596912933 | validation: 3.197902843706041]
	TIME [epoch: 8.31 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.928827018151385		[learning rate: 0.0057148]
	Learning Rate: 0.00571479
	LOSS [training: 2.928827018151385 | validation: 3.0939188763544863]
	TIME [epoch: 8.3 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9342747560787275		[learning rate: 0.005694]
	Learning Rate: 0.00569405
	LOSS [training: 2.9342747560787275 | validation: 2.749562020941715]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.012937530765618		[learning rate: 0.0056734]
	Learning Rate: 0.00567338
	LOSS [training: 3.012937530765618 | validation: 2.9052671342627017]
	TIME [epoch: 8.3 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.892686471956717		[learning rate: 0.0056528]
	Learning Rate: 0.00565279
	LOSS [training: 2.892686471956717 | validation: 2.793333212248243]
	TIME [epoch: 8.32 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.198931010571938		[learning rate: 0.0056323]
	Learning Rate: 0.00563228
	LOSS [training: 3.198931010571938 | validation: 2.7709186071441345]
	TIME [epoch: 8.29 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.105582177177125		[learning rate: 0.0056118]
	Learning Rate: 0.00561184
	LOSS [training: 3.105582177177125 | validation: 2.753307854943892]
	TIME [epoch: 8.3 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.968274970658998		[learning rate: 0.0055915]
	Learning Rate: 0.00559147
	LOSS [training: 2.968274970658998 | validation: 2.9510983321537614]
	TIME [epoch: 8.3 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.919710461496365		[learning rate: 0.0055712]
	Learning Rate: 0.00557118
	LOSS [training: 2.919710461496365 | validation: 3.1628729127359905]
	TIME [epoch: 8.32 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9956127047404246		[learning rate: 0.005551]
	Learning Rate: 0.00555096
	LOSS [training: 2.9956127047404246 | validation: 2.983633711522919]
	TIME [epoch: 8.3 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.977158297621172		[learning rate: 0.0055308]
	Learning Rate: 0.00553082
	LOSS [training: 2.977158297621172 | validation: 2.742178671368201]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.879689641110807		[learning rate: 0.0055107]
	Learning Rate: 0.00551075
	LOSS [training: 2.879689641110807 | validation: 3.3548603379569397]
	TIME [epoch: 8.31 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.939857846280019		[learning rate: 0.0054907]
	Learning Rate: 0.00549075
	LOSS [training: 2.939857846280019 | validation: 3.10234581944314]
	TIME [epoch: 8.33 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1102563657555886		[learning rate: 0.0054708]
	Learning Rate: 0.00547082
	LOSS [training: 3.1102563657555886 | validation: 2.8183899486739223]
	TIME [epoch: 8.32 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9897278048778966		[learning rate: 0.005451]
	Learning Rate: 0.00545097
	LOSS [training: 2.9897278048778966 | validation: 2.730852113189853]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.969011357962457		[learning rate: 0.0054312]
	Learning Rate: 0.00543119
	LOSS [training: 2.969011357962457 | validation: 2.8057558417911963]
	TIME [epoch: 8.32 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9559845203432316		[learning rate: 0.0054115]
	Learning Rate: 0.00541148
	LOSS [training: 2.9559845203432316 | validation: 2.76630359522602]
	TIME [epoch: 8.31 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.997530564310058		[learning rate: 0.0053918]
	Learning Rate: 0.00539184
	LOSS [training: 2.997530564310058 | validation: 2.7969143836959667]
	TIME [epoch: 8.34 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.877915832897161		[learning rate: 0.0053723]
	Learning Rate: 0.00537227
	LOSS [training: 2.877915832897161 | validation: 2.698147496446853]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.836535316004346		[learning rate: 0.0053528]
	Learning Rate: 0.00535277
	LOSS [training: 2.836535316004346 | validation: 2.732100140902409]
	TIME [epoch: 8.31 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9024862863749705		[learning rate: 0.0053333]
	Learning Rate: 0.00533335
	LOSS [training: 2.9024862863749705 | validation: 3.543098862178765]
	TIME [epoch: 8.31 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9425688868433157		[learning rate: 0.005314]
	Learning Rate: 0.00531399
	LOSS [training: 2.9425688868433157 | validation: 2.7299883566888052]
	TIME [epoch: 8.33 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9109351229926608		[learning rate: 0.0052947]
	Learning Rate: 0.00529471
	LOSS [training: 2.9109351229926608 | validation: 3.0670290872581014]
	TIME [epoch: 8.31 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8652731691235767		[learning rate: 0.0052755]
	Learning Rate: 0.00527549
	LOSS [training: 2.8652731691235767 | validation: 2.7585331003282825]
	TIME [epoch: 8.31 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.052755722855598		[learning rate: 0.0052563]
	Learning Rate: 0.00525635
	LOSS [training: 3.052755722855598 | validation: 3.0754462462859724]
	TIME [epoch: 8.31 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0197697972895674		[learning rate: 0.0052373]
	Learning Rate: 0.00523727
	LOSS [training: 3.0197697972895674 | validation: 2.9352186761390993]
	TIME [epoch: 8.32 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.895025568470124		[learning rate: 0.0052183]
	Learning Rate: 0.00521827
	LOSS [training: 2.895025568470124 | validation: 2.738130980659737]
	TIME [epoch: 8.32 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.942457853200446		[learning rate: 0.0051993]
	Learning Rate: 0.00519933
	LOSS [training: 2.942457853200446 | validation: 2.8512465142437264]
	TIME [epoch: 8.3 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8400879524507756		[learning rate: 0.0051805]
	Learning Rate: 0.00518046
	LOSS [training: 2.8400879524507756 | validation: 2.690638676574652]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_281.pth
	Model improved!!!
EPOCH 282/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9165091413857676		[learning rate: 0.0051617]
	Learning Rate: 0.00516166
	LOSS [training: 2.9165091413857676 | validation: 2.724542601710977]
	TIME [epoch: 8.31 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8594027414850287		[learning rate: 0.0051429]
	Learning Rate: 0.00514293
	LOSS [training: 2.8594027414850287 | validation: 2.836139396868561]
	TIME [epoch: 8.33 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.913504297518822		[learning rate: 0.0051243]
	Learning Rate: 0.00512426
	LOSS [training: 2.913504297518822 | validation: 2.870384538691045]
	TIME [epoch: 8.31 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9595053539186598		[learning rate: 0.0051057]
	Learning Rate: 0.00510567
	LOSS [training: 2.9595053539186598 | validation: 2.867318770685147]
	TIME [epoch: 8.31 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9153478005847226		[learning rate: 0.0050871]
	Learning Rate: 0.00508714
	LOSS [training: 2.9153478005847226 | validation: 2.7056792339657862]
	TIME [epoch: 8.31 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8880590184283292		[learning rate: 0.0050687]
	Learning Rate: 0.00506868
	LOSS [training: 2.8880590184283292 | validation: 2.9594538541865747]
	TIME [epoch: 8.33 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9098300202180964		[learning rate: 0.0050503]
	Learning Rate: 0.00505028
	LOSS [training: 2.9098300202180964 | validation: 2.726791525687613]
	TIME [epoch: 8.31 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8480372126497504		[learning rate: 0.005032]
	Learning Rate: 0.00503196
	LOSS [training: 2.8480372126497504 | validation: 3.0515769583434373]
	TIME [epoch: 8.31 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.998164830429842		[learning rate: 0.0050137]
	Learning Rate: 0.00501369
	LOSS [training: 2.998164830429842 | validation: 2.7095194721993696]
	TIME [epoch: 8.31 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8542646552736857		[learning rate: 0.0049955]
	Learning Rate: 0.0049955
	LOSS [training: 2.8542646552736857 | validation: 3.052830337237464]
	TIME [epoch: 8.32 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8939809409627277		[learning rate: 0.0049774]
	Learning Rate: 0.00497737
	LOSS [training: 2.8939809409627277 | validation: 2.9096026661582255]
	TIME [epoch: 8.32 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8968772194532537		[learning rate: 0.0049593]
	Learning Rate: 0.00495931
	LOSS [training: 2.8968772194532537 | validation: 3.205882741931756]
	TIME [epoch: 8.31 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9304618977139727		[learning rate: 0.0049413]
	Learning Rate: 0.00494131
	LOSS [training: 2.9304618977139727 | validation: 2.8822418264438836]
	TIME [epoch: 8.31 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.89323928049238		[learning rate: 0.0049234]
	Learning Rate: 0.00492338
	LOSS [training: 2.89323928049238 | validation: 3.12963801806834]
	TIME [epoch: 8.3 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.936562948249069		[learning rate: 0.0049055]
	Learning Rate: 0.00490551
	LOSS [training: 2.936562948249069 | validation: 2.911311229175384]
	TIME [epoch: 8.34 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9324924136732546		[learning rate: 0.0048877]
	Learning Rate: 0.00488771
	LOSS [training: 2.9324924136732546 | validation: 2.700264837908314]
	TIME [epoch: 8.31 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.958884931441671		[learning rate: 0.00487]
	Learning Rate: 0.00486997
	LOSS [training: 2.958884931441671 | validation: 2.700379779681298]
	TIME [epoch: 8.3 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.879535797267738		[learning rate: 0.0048523]
	Learning Rate: 0.0048523
	LOSS [training: 2.879535797267738 | validation: 2.981737430154798]
	TIME [epoch: 8.31 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.942714994883089		[learning rate: 0.0048347]
	Learning Rate: 0.00483469
	LOSS [training: 2.942714994883089 | validation: 2.785040573308131]
	TIME [epoch: 8.33 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8175733674008523		[learning rate: 0.0048171]
	Learning Rate: 0.00481714
	LOSS [training: 2.8175733674008523 | validation: 2.9582695937091876]
	TIME [epoch: 8.31 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.978930029237226		[learning rate: 0.0047997]
	Learning Rate: 0.00479966
	LOSS [training: 2.978930029237226 | validation: 3.489222206931912]
	TIME [epoch: 8.31 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.012828925610838		[learning rate: 0.0047822]
	Learning Rate: 0.00478224
	LOSS [training: 3.012828925610838 | validation: 3.334263751239413]
	TIME [epoch: 8.31 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9756746887874406		[learning rate: 0.0047649]
	Learning Rate: 0.00476489
	LOSS [training: 2.9756746887874406 | validation: 2.7965688684387464]
	TIME [epoch: 8.32 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.804948069831867		[learning rate: 0.0047476]
	Learning Rate: 0.00474759
	LOSS [training: 2.804948069831867 | validation: 2.7083087763131712]
	TIME [epoch: 8.32 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8600456530153875		[learning rate: 0.0047304]
	Learning Rate: 0.00473037
	LOSS [training: 2.8600456530153875 | validation: 3.132746151997035]
	TIME [epoch: 8.31 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.946051681828147		[learning rate: 0.0047132]
	Learning Rate: 0.0047132
	LOSS [training: 2.946051681828147 | validation: 2.822418511221928]
	TIME [epoch: 8.3 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8464956916672945		[learning rate: 0.0046961]
	Learning Rate: 0.00469609
	LOSS [training: 2.8464956916672945 | validation: 2.822555861758626]
	TIME [epoch: 8.3 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.933171624367241		[learning rate: 0.0046791]
	Learning Rate: 0.00467905
	LOSS [training: 2.933171624367241 | validation: 2.7350820379889433]
	TIME [epoch: 8.33 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.887506721680657		[learning rate: 0.0046621]
	Learning Rate: 0.00466207
	LOSS [training: 2.887506721680657 | validation: 3.0852526881305984]
	TIME [epoch: 8.3 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.826363737698646		[learning rate: 0.0046452]
	Learning Rate: 0.00464515
	LOSS [training: 2.826363737698646 | validation: 2.997767717000608]
	TIME [epoch: 8.3 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8609125314041273		[learning rate: 0.0046283]
	Learning Rate: 0.0046283
	LOSS [training: 2.8609125314041273 | validation: 2.8172038286554404]
	TIME [epoch: 8.3 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.857904553277865		[learning rate: 0.0046115]
	Learning Rate: 0.0046115
	LOSS [training: 2.857904553277865 | validation: 2.9356341102583636]
	TIME [epoch: 8.33 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.903870294141894		[learning rate: 0.0045948]
	Learning Rate: 0.00459476
	LOSS [training: 2.903870294141894 | validation: 2.7580860450899154]
	TIME [epoch: 8.3 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8440949428910907		[learning rate: 0.0045781]
	Learning Rate: 0.00457809
	LOSS [training: 2.8440949428910907 | validation: 2.6987779936793816]
	TIME [epoch: 8.31 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9302934456662584		[learning rate: 0.0045615]
	Learning Rate: 0.00456148
	LOSS [training: 2.9302934456662584 | validation: 2.772648416375895]
	TIME [epoch: 8.31 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.986016608994335		[learning rate: 0.0045449]
	Learning Rate: 0.00454492
	LOSS [training: 2.986016608994335 | validation: 2.7029415732637636]
	TIME [epoch: 8.32 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.909537543401251		[learning rate: 0.0045284]
	Learning Rate: 0.00452843
	LOSS [training: 2.909537543401251 | validation: 2.659664361220586]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8360137865484085		[learning rate: 0.004512]
	Learning Rate: 0.00451199
	LOSS [training: 2.8360137865484085 | validation: 2.6601344851342055]
	TIME [epoch: 8.3 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7918117321471625		[learning rate: 0.0044956]
	Learning Rate: 0.00449562
	LOSS [training: 2.7918117321471625 | validation: 2.6724222391825174]
	TIME [epoch: 8.3 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7875068057888686		[learning rate: 0.0044793]
	Learning Rate: 0.0044793
	LOSS [training: 2.7875068057888686 | validation: 2.7728051314536093]
	TIME [epoch: 8.3 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8994986788868067		[learning rate: 0.004463]
	Learning Rate: 0.00446305
	LOSS [training: 2.8994986788868067 | validation: 2.805806761122243]
	TIME [epoch: 8.33 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.818083694356713		[learning rate: 0.0044469]
	Learning Rate: 0.00444685
	LOSS [training: 2.818083694356713 | validation: 3.085033081538919]
	TIME [epoch: 8.3 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.984351811170042		[learning rate: 0.0044307]
	Learning Rate: 0.00443071
	LOSS [training: 2.984351811170042 | validation: 3.0753496899015698]
	TIME [epoch: 8.3 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.87968763401224		[learning rate: 0.0044146]
	Learning Rate: 0.00441463
	LOSS [training: 2.87968763401224 | validation: 2.75532577041601]
	TIME [epoch: 8.3 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8294645641033895		[learning rate: 0.0043986]
	Learning Rate: 0.00439861
	LOSS [training: 2.8294645641033895 | validation: 2.6312333364424374]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_326.pth
	Model improved!!!
EPOCH 327/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.813217879668903		[learning rate: 0.0043827]
	Learning Rate: 0.00438265
	LOSS [training: 2.813217879668903 | validation: 2.6595926262957796]
	TIME [epoch: 8.3 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8627672762110437		[learning rate: 0.0043667]
	Learning Rate: 0.00436675
	LOSS [training: 2.8627672762110437 | validation: 2.728824202271954]
	TIME [epoch: 8.31 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.921175067138589		[learning rate: 0.0043509]
	Learning Rate: 0.0043509
	LOSS [training: 2.921175067138589 | validation: 2.632967550955186]
	TIME [epoch: 8.3 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7829871923545375		[learning rate: 0.0043351]
	Learning Rate: 0.00433511
	LOSS [training: 2.7829871923545375 | validation: 3.0685138596109747]
	TIME [epoch: 8.32 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8158185676736784		[learning rate: 0.0043194]
	Learning Rate: 0.00431938
	LOSS [training: 2.8158185676736784 | validation: 2.9696200950287808]
	TIME [epoch: 8.32 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8788691374631847		[learning rate: 0.0043037]
	Learning Rate: 0.0043037
	LOSS [training: 2.8788691374631847 | validation: 2.667461741585431]
	TIME [epoch: 8.3 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7919491310366755		[learning rate: 0.0042881]
	Learning Rate: 0.00428808
	LOSS [training: 2.7919491310366755 | validation: 2.869072641058285]
	TIME [epoch: 8.3 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8979251741691554		[learning rate: 0.0042725]
	Learning Rate: 0.00427252
	LOSS [training: 2.8979251741691554 | validation: 2.7896617567087723]
	TIME [epoch: 8.3 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9066130205462253		[learning rate: 0.004257]
	Learning Rate: 0.00425702
	LOSS [training: 2.9066130205462253 | validation: 2.787766070954242]
	TIME [epoch: 8.33 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.834365383925942		[learning rate: 0.0042416]
	Learning Rate: 0.00424157
	LOSS [training: 2.834365383925942 | validation: 2.7339775093439402]
	TIME [epoch: 8.3 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.864151093705975		[learning rate: 0.0042262]
	Learning Rate: 0.00422617
	LOSS [training: 2.864151093705975 | validation: 2.9379792018257724]
	TIME [epoch: 8.3 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.886946609997101		[learning rate: 0.0042108]
	Learning Rate: 0.00421084
	LOSS [training: 2.886946609997101 | validation: 2.6793269848711008]
	TIME [epoch: 8.3 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8250983418332405		[learning rate: 0.0041956]
	Learning Rate: 0.00419556
	LOSS [training: 2.8250983418332405 | validation: 2.683363534504176]
	TIME [epoch: 8.33 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8437143584666575		[learning rate: 0.0041803]
	Learning Rate: 0.00418033
	LOSS [training: 2.8437143584666575 | validation: 2.8172063214225638]
	TIME [epoch: 8.31 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.806863840582887		[learning rate: 0.0041652]
	Learning Rate: 0.00416516
	LOSS [training: 2.806863840582887 | validation: 2.768974933487706]
	TIME [epoch: 8.3 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7862254559068917		[learning rate: 0.00415]
	Learning Rate: 0.00415004
	LOSS [training: 2.7862254559068917 | validation: 2.7437036148043523]
	TIME [epoch: 8.3 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8590081300940287		[learning rate: 0.004135]
	Learning Rate: 0.00413498
	LOSS [training: 2.8590081300940287 | validation: 2.6930225611018166]
	TIME [epoch: 8.31 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8733069649270715		[learning rate: 0.00412]
	Learning Rate: 0.00411998
	LOSS [training: 2.8733069649270715 | validation: 2.746566451379751]
	TIME [epoch: 8.31 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.780269497070514		[learning rate: 0.004105]
	Learning Rate: 0.00410502
	LOSS [training: 2.780269497070514 | validation: 2.7875951097339735]
	TIME [epoch: 8.3 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8362193775806555		[learning rate: 0.0040901]
	Learning Rate: 0.00409013
	LOSS [training: 2.8362193775806555 | validation: 3.031428722337585]
	TIME [epoch: 8.3 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8354291666033196		[learning rate: 0.0040753]
	Learning Rate: 0.00407528
	LOSS [training: 2.8354291666033196 | validation: 3.2426610457838416]
	TIME [epoch: 8.3 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.955410994424148		[learning rate: 0.0040605]
	Learning Rate: 0.00406049
	LOSS [training: 2.955410994424148 | validation: 2.860944982038117]
	TIME [epoch: 8.33 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8032066220216945		[learning rate: 0.0040458]
	Learning Rate: 0.00404576
	LOSS [training: 2.8032066220216945 | validation: 2.768402605715276]
	TIME [epoch: 8.3 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.826264045932167		[learning rate: 0.0040311]
	Learning Rate: 0.00403108
	LOSS [training: 2.826264045932167 | validation: 2.756396442781269]
	TIME [epoch: 8.3 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.809199534857532		[learning rate: 0.0040164]
	Learning Rate: 0.00401645
	LOSS [training: 2.809199534857532 | validation: 3.119831208251977]
	TIME [epoch: 8.3 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.812229954049169		[learning rate: 0.0040019]
	Learning Rate: 0.00400187
	LOSS [training: 2.812229954049169 | validation: 2.7923752908748645]
	TIME [epoch: 8.33 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8821285705514876		[learning rate: 0.0039873]
	Learning Rate: 0.00398735
	LOSS [training: 2.8821285705514876 | validation: 2.732044590502062]
	TIME [epoch: 8.3 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.807375412859332		[learning rate: 0.0039729]
	Learning Rate: 0.00397288
	LOSS [training: 2.807375412859332 | validation: 2.682767849746745]
	TIME [epoch: 8.3 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7385712084947302		[learning rate: 0.0039585]
	Learning Rate: 0.00395846
	LOSS [training: 2.7385712084947302 | validation: 2.7289832544912507]
	TIME [epoch: 8.3 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.809875208779368		[learning rate: 0.0039441]
	Learning Rate: 0.00394409
	LOSS [training: 2.809875208779368 | validation: 2.661469941814061]
	TIME [epoch: 8.31 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8271153061760055		[learning rate: 0.0039298]
	Learning Rate: 0.00392978
	LOSS [training: 2.8271153061760055 | validation: 2.685893623041835]
	TIME [epoch: 8.32 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.918228236556473		[learning rate: 0.0039155]
	Learning Rate: 0.00391552
	LOSS [training: 2.918228236556473 | validation: 2.6886534583820674]
	TIME [epoch: 8.3 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7771294625921272		[learning rate: 0.0039013]
	Learning Rate: 0.00390131
	LOSS [training: 2.7771294625921272 | validation: 2.658368055850079]
	TIME [epoch: 8.3 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9501118309652314		[learning rate: 0.0038872]
	Learning Rate: 0.00388715
	LOSS [training: 2.9501118309652314 | validation: 2.6702481112507304]
	TIME [epoch: 8.3 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8429704860676197		[learning rate: 0.003873]
	Learning Rate: 0.00387305
	LOSS [training: 2.8429704860676197 | validation: 3.009798907187615]
	TIME [epoch: 8.33 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.862509471989074		[learning rate: 0.003859]
	Learning Rate: 0.00385899
	LOSS [training: 2.862509471989074 | validation: 3.0693008555207033]
	TIME [epoch: 8.3 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.834609947998516		[learning rate: 0.003845]
	Learning Rate: 0.00384499
	LOSS [training: 2.834609947998516 | validation: 2.6871147022410753]
	TIME [epoch: 8.3 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.878164393390776		[learning rate: 0.003831]
	Learning Rate: 0.00383103
	LOSS [training: 2.878164393390776 | validation: 2.6816449133942433]
	TIME [epoch: 8.3 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8306317373487486		[learning rate: 0.0038171]
	Learning Rate: 0.00381713
	LOSS [training: 2.8306317373487486 | validation: 2.652884500005996]
	TIME [epoch: 8.32 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.825007414805038		[learning rate: 0.0038033]
	Learning Rate: 0.00380328
	LOSS [training: 2.825007414805038 | validation: 2.730608233279167]
	TIME [epoch: 8.3 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9563681556716377		[learning rate: 0.0037895]
	Learning Rate: 0.00378947
	LOSS [training: 2.9563681556716377 | validation: 2.664744220561837]
	TIME [epoch: 8.3 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.769051201001861		[learning rate: 0.0037757]
	Learning Rate: 0.00377572
	LOSS [training: 2.769051201001861 | validation: 2.707032495390886]
	TIME [epoch: 8.3 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.777784456457097		[learning rate: 0.003762]
	Learning Rate: 0.00376202
	LOSS [training: 2.777784456457097 | validation: 2.8524911101758]
	TIME [epoch: 8.31 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.800075776906053		[learning rate: 0.0037484]
	Learning Rate: 0.00374837
	LOSS [training: 2.800075776906053 | validation: 2.6465044110169726]
	TIME [epoch: 8.32 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8931626467436096		[learning rate: 0.0037348]
	Learning Rate: 0.00373476
	LOSS [training: 2.8931626467436096 | validation: 2.941709073720392]
	TIME [epoch: 8.3 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8659638454164105		[learning rate: 0.0037212]
	Learning Rate: 0.00372121
	LOSS [training: 2.8659638454164105 | validation: 2.8034609631886767]
	TIME [epoch: 8.3 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7781967493238535		[learning rate: 0.0037077]
	Learning Rate: 0.00370771
	LOSS [training: 2.7781967493238535 | validation: 3.3245609406449472]
	TIME [epoch: 8.3 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8707910886694514		[learning rate: 0.0036943]
	Learning Rate: 0.00369425
	LOSS [training: 2.8707910886694514 | validation: 2.6293948859184155]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7989795438802796		[learning rate: 0.0036808]
	Learning Rate: 0.00368084
	LOSS [training: 2.7989795438802796 | validation: 3.283895652911058]
	TIME [epoch: 8.31 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.870102841268136		[learning rate: 0.0036675]
	Learning Rate: 0.00366749
	LOSS [training: 2.870102841268136 | validation: 2.9010493803815223]
	TIME [epoch: 8.31 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8003026340406487		[learning rate: 0.0036542]
	Learning Rate: 0.00365418
	LOSS [training: 2.8003026340406487 | validation: 2.930847792730236]
	TIME [epoch: 8.3 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7911600911788943		[learning rate: 0.0036409]
	Learning Rate: 0.00364092
	LOSS [training: 2.7911600911788943 | validation: 2.701054308288603]
	TIME [epoch: 8.32 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7964561858312322		[learning rate: 0.0036277]
	Learning Rate: 0.0036277
	LOSS [training: 2.7964561858312322 | validation: 2.687032584774455]
	TIME [epoch: 8.3 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7682492679673296		[learning rate: 0.0036145]
	Learning Rate: 0.00361454
	LOSS [training: 2.7682492679673296 | validation: 2.845477597869375]
	TIME [epoch: 8.3 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8952189092569767		[learning rate: 0.0036014]
	Learning Rate: 0.00360142
	LOSS [training: 2.8952189092569767 | validation: 2.752951697680712]
	TIME [epoch: 8.3 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8338659563178896		[learning rate: 0.0035883]
	Learning Rate: 0.00358835
	LOSS [training: 2.8338659563178896 | validation: 2.7765992925587035]
	TIME [epoch: 8.31 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.772899557052353		[learning rate: 0.0035753]
	Learning Rate: 0.00357533
	LOSS [training: 2.772899557052353 | validation: 2.6665001489665308]
	TIME [epoch: 8.32 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8756621301319756		[learning rate: 0.0035624]
	Learning Rate: 0.00356235
	LOSS [training: 2.8756621301319756 | validation: 2.6683586621438486]
	TIME [epoch: 8.3 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7687967995394986		[learning rate: 0.0035494]
	Learning Rate: 0.00354942
	LOSS [training: 2.7687967995394986 | validation: 2.6467461025570005]
	TIME [epoch: 8.3 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8256183006280873		[learning rate: 0.0035365]
	Learning Rate: 0.00353654
	LOSS [training: 2.8256183006280873 | validation: 2.7676588926824732]
	TIME [epoch: 8.3 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8225424924732647		[learning rate: 0.0035237]
	Learning Rate: 0.00352371
	LOSS [training: 2.8225424924732647 | validation: 2.624132055669966]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_387.pth
	Model improved!!!
EPOCH 388/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8117037467236394		[learning rate: 0.0035109]
	Learning Rate: 0.00351092
	LOSS [training: 2.8117037467236394 | validation: 2.7746278790356875]
	TIME [epoch: 8.31 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8587845881965674		[learning rate: 0.0034982]
	Learning Rate: 0.00349818
	LOSS [training: 2.8587845881965674 | validation: 2.8179876972815703]
	TIME [epoch: 8.3 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8333705576824473		[learning rate: 0.0034855]
	Learning Rate: 0.00348548
	LOSS [training: 2.8333705576824473 | validation: 2.621581439386306]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.819143239007868		[learning rate: 0.0034728]
	Learning Rate: 0.00347284
	LOSS [training: 2.819143239007868 | validation: 3.0131135802217948]
	TIME [epoch: 8.33 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8047282776024205		[learning rate: 0.0034602]
	Learning Rate: 0.00346023
	LOSS [training: 2.8047282776024205 | validation: 2.9958716869830955]
	TIME [epoch: 8.3 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8156834412541785		[learning rate: 0.0034477]
	Learning Rate: 0.00344767
	LOSS [training: 2.8156834412541785 | validation: 2.7392940847223484]
	TIME [epoch: 8.3 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7550715354138644		[learning rate: 0.0034352]
	Learning Rate: 0.00343516
	LOSS [training: 2.7550715354138644 | validation: 2.6586811462338957]
	TIME [epoch: 8.29 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7419234055262978		[learning rate: 0.0034227]
	Learning Rate: 0.0034227
	LOSS [training: 2.7419234055262978 | validation: 2.6589691294785616]
	TIME [epoch: 8.3 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.737574077873508		[learning rate: 0.0034103]
	Learning Rate: 0.00341028
	LOSS [training: 2.737574077873508 | validation: 2.646562970883301]
	TIME [epoch: 8.31 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7612034006170694		[learning rate: 0.0033979]
	Learning Rate: 0.0033979
	LOSS [training: 2.7612034006170694 | validation: 2.6074490255183127]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8734089319098284		[learning rate: 0.0033856]
	Learning Rate: 0.00338557
	LOSS [training: 2.8734089319098284 | validation: 2.685816745301401]
	TIME [epoch: 8.3 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.756929366959731		[learning rate: 0.0033733]
	Learning Rate: 0.00337328
	LOSS [training: 2.756929366959731 | validation: 2.6868933141083993]
	TIME [epoch: 8.3 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.761500987265704		[learning rate: 0.003361]
	Learning Rate: 0.00336104
	LOSS [training: 2.761500987265704 | validation: 2.8935295280925963]
	TIME [epoch: 8.33 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8018648145936957		[learning rate: 0.0033488]
	Learning Rate: 0.00334884
	LOSS [training: 2.8018648145936957 | validation: 2.6504550302362517]
	TIME [epoch: 8.3 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8287252476214872		[learning rate: 0.0033367]
	Learning Rate: 0.00333669
	LOSS [training: 2.8287252476214872 | validation: 2.796690080310328]
	TIME [epoch: 8.3 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7732245446149846		[learning rate: 0.0033246]
	Learning Rate: 0.00332458
	LOSS [training: 2.7732245446149846 | validation: 2.8732658221539777]
	TIME [epoch: 8.3 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8277548722501007		[learning rate: 0.0033125]
	Learning Rate: 0.00331252
	LOSS [training: 2.8277548722501007 | validation: 2.86898460827021]
	TIME [epoch: 8.32 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7443967180232884		[learning rate: 0.0033005]
	Learning Rate: 0.00330049
	LOSS [training: 2.7443967180232884 | validation: 2.7050962819298046]
	TIME [epoch: 8.3 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7764764109424296		[learning rate: 0.0032885]
	Learning Rate: 0.00328852
	LOSS [training: 2.7764764109424296 | validation: 2.679689185687038]
	TIME [epoch: 8.3 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7555043122989		[learning rate: 0.0032766]
	Learning Rate: 0.00327658
	LOSS [training: 2.7555043122989 | validation: 2.7608877254251416]
	TIME [epoch: 8.3 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.741423331013949		[learning rate: 0.0032647]
	Learning Rate: 0.00326469
	LOSS [training: 2.741423331013949 | validation: 2.6672674220687447]
	TIME [epoch: 8.3 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7514557589190014		[learning rate: 0.0032528]
	Learning Rate: 0.00325284
	LOSS [training: 2.7514557589190014 | validation: 2.770589048299928]
	TIME [epoch: 8.32 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.739265616288159		[learning rate: 0.003241]
	Learning Rate: 0.00324104
	LOSS [training: 2.739265616288159 | validation: 2.7824940811920125]
	TIME [epoch: 8.3 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.780727469796044		[learning rate: 0.0032293]
	Learning Rate: 0.00322928
	LOSS [training: 2.780727469796044 | validation: 2.7356322555699877]
	TIME [epoch: 8.3 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7979847387498227		[learning rate: 0.0032176]
	Learning Rate: 0.00321756
	LOSS [training: 2.7979847387498227 | validation: 2.747270712206272]
	TIME [epoch: 8.3 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7713477757370826		[learning rate: 0.0032059]
	Learning Rate: 0.00320588
	LOSS [training: 2.7713477757370826 | validation: 2.6741547285650933]
	TIME [epoch: 8.32 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7275324088410327		[learning rate: 0.0031942]
	Learning Rate: 0.00319425
	LOSS [training: 2.7275324088410327 | validation: 2.6664329750474876]
	TIME [epoch: 8.3 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8062134766188245		[learning rate: 0.0031827]
	Learning Rate: 0.00318265
	LOSS [training: 2.8062134766188245 | validation: 2.695881610045891]
	TIME [epoch: 8.3 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.739908312521604		[learning rate: 0.0031711]
	Learning Rate: 0.0031711
	LOSS [training: 2.739908312521604 | validation: 2.845600884885995]
	TIME [epoch: 8.3 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8830783902454		[learning rate: 0.0031596]
	Learning Rate: 0.0031596
	LOSS [training: 2.8830783902454 | validation: 2.7381919258385476]
	TIME [epoch: 8.31 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.755037581625701		[learning rate: 0.0031481]
	Learning Rate: 0.00314813
	LOSS [training: 2.755037581625701 | validation: 2.6934208189169464]
	TIME [epoch: 8.31 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.774551220486515		[learning rate: 0.0031367]
	Learning Rate: 0.00313671
	LOSS [training: 2.774551220486515 | validation: 2.6626842632819123]
	TIME [epoch: 8.3 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.742376600122668		[learning rate: 0.0031253]
	Learning Rate: 0.00312532
	LOSS [training: 2.742376600122668 | validation: 2.667166537799697]
	TIME [epoch: 8.3 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.762645995090837		[learning rate: 0.003114]
	Learning Rate: 0.00311398
	LOSS [training: 2.762645995090837 | validation: 2.635734100949965]
	TIME [epoch: 8.3 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.738905402387042		[learning rate: 0.0031027]
	Learning Rate: 0.00310268
	LOSS [training: 2.738905402387042 | validation: 3.2508541861708036]
	TIME [epoch: 8.32 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8758790992012013		[learning rate: 0.0030914]
	Learning Rate: 0.00309142
	LOSS [training: 2.8758790992012013 | validation: 2.671804320471796]
	TIME [epoch: 8.3 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7584523004916437		[learning rate: 0.0030802]
	Learning Rate: 0.0030802
	LOSS [training: 2.7584523004916437 | validation: 2.8074414725568717]
	TIME [epoch: 8.3 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.762010430064137		[learning rate: 0.003069]
	Learning Rate: 0.00306902
	LOSS [training: 2.762010430064137 | validation: 2.6362557180555366]
	TIME [epoch: 8.3 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7518025702641298		[learning rate: 0.0030579]
	Learning Rate: 0.00305788
	LOSS [training: 2.7518025702641298 | validation: 2.6619555841282656]
	TIME [epoch: 8.32 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8401718487705394		[learning rate: 0.0030468]
	Learning Rate: 0.00304679
	LOSS [training: 2.8401718487705394 | validation: 2.6550460555270505]
	TIME [epoch: 8.3 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.747024280554075		[learning rate: 0.0030357]
	Learning Rate: 0.00303573
	LOSS [training: 2.747024280554075 | validation: 2.645353331519938]
	TIME [epoch: 8.3 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.782861043268875		[learning rate: 0.0030247]
	Learning Rate: 0.00302471
	LOSS [training: 2.782861043268875 | validation: 2.593345432338242]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.81769803423927		[learning rate: 0.0030137]
	Learning Rate: 0.00301374
	LOSS [training: 2.81769803423927 | validation: 3.0247627672789967]
	TIME [epoch: 8.33 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.819867973782503		[learning rate: 0.0030028]
	Learning Rate: 0.0030028
	LOSS [training: 2.819867973782503 | validation: 2.619571044184017]
	TIME [epoch: 8.31 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7887032214941567		[learning rate: 0.0029919]
	Learning Rate: 0.0029919
	LOSS [training: 2.7887032214941567 | validation: 2.646635980559253]
	TIME [epoch: 8.3 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7202859557580297		[learning rate: 0.002981]
	Learning Rate: 0.00298104
	LOSS [training: 2.7202859557580297 | validation: 2.6216018921666544]
	TIME [epoch: 8.3 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.801955967654427		[learning rate: 0.0029702]
	Learning Rate: 0.00297023
	LOSS [training: 2.801955967654427 | validation: 2.846954582195009]
	TIME [epoch: 8.3 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8701024118144263		[learning rate: 0.0029594]
	Learning Rate: 0.00295945
	LOSS [training: 2.8701024118144263 | validation: 2.6362685447750396]
	TIME [epoch: 8.32 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7654833551247564		[learning rate: 0.0029487]
	Learning Rate: 0.00294871
	LOSS [training: 2.7654833551247564 | validation: 3.2122057013476355]
	TIME [epoch: 8.3 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8420063112599134		[learning rate: 0.002938]
	Learning Rate: 0.00293801
	LOSS [training: 2.8420063112599134 | validation: 2.607453163402749]
	TIME [epoch: 8.3 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.772723629398933		[learning rate: 0.0029273]
	Learning Rate: 0.00292734
	LOSS [training: 2.772723629398933 | validation: 2.760366339674758]
	TIME [epoch: 8.29 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.791357336929711		[learning rate: 0.0029167]
	Learning Rate: 0.00291672
	LOSS [training: 2.791357336929711 | validation: 2.6233274552928734]
	TIME [epoch: 8.32 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.729189009865457		[learning rate: 0.0029061]
	Learning Rate: 0.00290614
	LOSS [training: 2.729189009865457 | validation: 2.6272700207674795]
	TIME [epoch: 8.3 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.787579127805821		[learning rate: 0.0028956]
	Learning Rate: 0.00289559
	LOSS [training: 2.787579127805821 | validation: 2.8708564630514672]
	TIME [epoch: 8.3 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7651562406427384		[learning rate: 0.0028851]
	Learning Rate: 0.00288508
	LOSS [training: 2.7651562406427384 | validation: 2.676504826890251]
	TIME [epoch: 8.3 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7184489481610212		[learning rate: 0.0028746]
	Learning Rate: 0.00287461
	LOSS [training: 2.7184489481610212 | validation: 2.80572963995081]
	TIME [epoch: 8.31 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7694716751913586		[learning rate: 0.0028642]
	Learning Rate: 0.00286418
	LOSS [training: 2.7694716751913586 | validation: 2.622509148016122]
	TIME [epoch: 8.31 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.779039308335439		[learning rate: 0.0028538]
	Learning Rate: 0.00285378
	LOSS [training: 2.779039308335439 | validation: 2.7928391597685938]
	TIME [epoch: 8.3 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7295545643625894		[learning rate: 0.0028434]
	Learning Rate: 0.00284343
	LOSS [training: 2.7295545643625894 | validation: 2.7647329032176584]
	TIME [epoch: 8.3 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7689582619790474		[learning rate: 0.0028331]
	Learning Rate: 0.00283311
	LOSS [training: 2.7689582619790474 | validation: 2.6282740478080973]
	TIME [epoch: 8.3 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7546075905417746		[learning rate: 0.0028228]
	Learning Rate: 0.00282283
	LOSS [training: 2.7546075905417746 | validation: 2.679496310508694]
	TIME [epoch: 8.32 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7361204166400137		[learning rate: 0.0028126]
	Learning Rate: 0.00281258
	LOSS [training: 2.7361204166400137 | validation: 2.6456406258456187]
	TIME [epoch: 8.3 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7253215559313118		[learning rate: 0.0028024]
	Learning Rate: 0.00280238
	LOSS [training: 2.7253215559313118 | validation: 2.6645401277145218]
	TIME [epoch: 8.3 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7142264980766653		[learning rate: 0.0027922]
	Learning Rate: 0.00279221
	LOSS [training: 2.7142264980766653 | validation: 2.7155369474283226]
	TIME [epoch: 8.3 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.726890537458285		[learning rate: 0.0027821]
	Learning Rate: 0.00278207
	LOSS [training: 2.726890537458285 | validation: 2.620102187408821]
	TIME [epoch: 8.32 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.853586836017969		[learning rate: 0.002772]
	Learning Rate: 0.00277198
	LOSS [training: 2.853586836017969 | validation: 2.9184592740895843]
	TIME [epoch: 8.3 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9060291769664266		[learning rate: 0.0027619]
	Learning Rate: 0.00276192
	LOSS [training: 2.9060291769664266 | validation: 2.6426579941372426]
	TIME [epoch: 8.3 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.719455216697102		[learning rate: 0.0027519]
	Learning Rate: 0.00275189
	LOSS [training: 2.719455216697102 | validation: 2.6304381633033]
	TIME [epoch: 8.3 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7733129607359563		[learning rate: 0.0027419]
	Learning Rate: 0.00274191
	LOSS [training: 2.7733129607359563 | validation: 2.7380474460473008]
	TIME [epoch: 8.32 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7962351111985555		[learning rate: 0.002732]
	Learning Rate: 0.00273196
	LOSS [training: 2.7962351111985555 | validation: 2.6689722331101553]
	TIME [epoch: 8.32 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7832357967128383		[learning rate: 0.002722]
	Learning Rate: 0.00272204
	LOSS [training: 2.7832357967128383 | validation: 2.6573445355211014]
	TIME [epoch: 8.3 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.703812501734717		[learning rate: 0.0027122]
	Learning Rate: 0.00271216
	LOSS [training: 2.703812501734717 | validation: 3.2450086381027763]
	TIME [epoch: 8.31 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.84638885581068		[learning rate: 0.0027023]
	Learning Rate: 0.00270232
	LOSS [training: 2.84638885581068 | validation: 2.6354355942059002]
	TIME [epoch: 8.3 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7613933783040325		[learning rate: 0.0026925]
	Learning Rate: 0.00269251
	LOSS [training: 2.7613933783040325 | validation: 2.635875307312371]
	TIME [epoch: 8.33 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7134761640677896		[learning rate: 0.0026827]
	Learning Rate: 0.00268274
	LOSS [training: 2.7134761640677896 | validation: 2.6868264394303996]
	TIME [epoch: 8.3 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8083835447622363		[learning rate: 0.002673]
	Learning Rate: 0.00267301
	LOSS [training: 2.8083835447622363 | validation: 2.62340615898313]
	TIME [epoch: 8.3 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7140705868065185		[learning rate: 0.0026633]
	Learning Rate: 0.00266331
	LOSS [training: 2.7140705868065185 | validation: 2.707019117121346]
	TIME [epoch: 8.3 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7330587300908458		[learning rate: 0.0026536]
	Learning Rate: 0.00265364
	LOSS [training: 2.7330587300908458 | validation: 2.778885462473179]
	TIME [epoch: 8.33 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7781814706036805		[learning rate: 0.002644]
	Learning Rate: 0.00264401
	LOSS [training: 2.7781814706036805 | validation: 2.6756340990032097]
	TIME [epoch: 8.31 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7334884560324744		[learning rate: 0.0026344]
	Learning Rate: 0.00263441
	LOSS [training: 2.7334884560324744 | validation: 2.614011011021882]
	TIME [epoch: 8.3 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.794148346181159		[learning rate: 0.0026249]
	Learning Rate: 0.00262485
	LOSS [training: 2.794148346181159 | validation: 2.6403906778714643]
	TIME [epoch: 8.3 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.718954044126657		[learning rate: 0.0026153]
	Learning Rate: 0.00261533
	LOSS [training: 2.718954044126657 | validation: 2.672283527110512]
	TIME [epoch: 8.32 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7348830340460495		[learning rate: 0.0026058]
	Learning Rate: 0.00260584
	LOSS [training: 2.7348830340460495 | validation: 2.836694535506357]
	TIME [epoch: 8.31 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.738629631826645		[learning rate: 0.0025964]
	Learning Rate: 0.00259638
	LOSS [training: 2.738629631826645 | validation: 2.638779651854242]
	TIME [epoch: 8.31 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.742131963998253		[learning rate: 0.002587]
	Learning Rate: 0.00258696
	LOSS [training: 2.742131963998253 | validation: 2.6224278433186736]
	TIME [epoch: 8.31 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7492969739390665		[learning rate: 0.0025776]
	Learning Rate: 0.00257757
	LOSS [training: 2.7492969739390665 | validation: 2.5894015508222714]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_473.pth
	Model improved!!!
EPOCH 474/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.751580114518684		[learning rate: 0.0025682]
	Learning Rate: 0.00256822
	LOSS [training: 2.751580114518684 | validation: 2.6318859094685947]
	TIME [epoch: 8.33 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7962460094585575		[learning rate: 0.0025589]
	Learning Rate: 0.0025589
	LOSS [training: 2.7962460094585575 | validation: 2.616249381845229]
	TIME [epoch: 8.3 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.713407957196588		[learning rate: 0.0025496]
	Learning Rate: 0.00254961
	LOSS [training: 2.713407957196588 | validation: 2.6744776245188877]
	TIME [epoch: 8.31 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7395443042804604		[learning rate: 0.0025404]
	Learning Rate: 0.00254036
	LOSS [training: 2.7395443042804604 | validation: 2.897019102083195]
	TIME [epoch: 8.3 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.846924540488312		[learning rate: 0.0025311]
	Learning Rate: 0.00253114
	LOSS [training: 2.846924540488312 | validation: 2.608298197580019]
	TIME [epoch: 8.33 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.722340013347101		[learning rate: 0.002522]
	Learning Rate: 0.00252195
	LOSS [training: 2.722340013347101 | validation: 2.607236785855971]
	TIME [epoch: 8.31 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.791405828119642		[learning rate: 0.0025128]
	Learning Rate: 0.0025128
	LOSS [training: 2.791405828119642 | validation: 2.9444327159955126]
	TIME [epoch: 8.31 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7292693428637724		[learning rate: 0.0025037]
	Learning Rate: 0.00250368
	LOSS [training: 2.7292693428637724 | validation: 2.6125051917198974]
	TIME [epoch: 8.3 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7259700366480706		[learning rate: 0.0024946]
	Learning Rate: 0.00249459
	LOSS [training: 2.7259700366480706 | validation: 2.65470204399128]
	TIME [epoch: 8.32 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7751816841554904		[learning rate: 0.0024855]
	Learning Rate: 0.00248554
	LOSS [training: 2.7751816841554904 | validation: 2.7036291866765536]
	TIME [epoch: 8.31 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.734486137946669		[learning rate: 0.0024765]
	Learning Rate: 0.00247652
	LOSS [training: 2.734486137946669 | validation: 2.8227196758048825]
	TIME [epoch: 8.3 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7687105478383605		[learning rate: 0.0024675]
	Learning Rate: 0.00246753
	LOSS [training: 2.7687105478383605 | validation: 2.6347276780165627]
	TIME [epoch: 8.31 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.728485934836322		[learning rate: 0.0024586]
	Learning Rate: 0.00245858
	LOSS [training: 2.728485934836322 | validation: 2.706746339091218]
	TIME [epoch: 8.31 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7220432253279165		[learning rate: 0.0024497]
	Learning Rate: 0.00244966
	LOSS [training: 2.7220432253279165 | validation: 2.6419065368590084]
	TIME [epoch: 8.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.751062289342248		[learning rate: 0.0024408]
	Learning Rate: 0.00244077
	LOSS [training: 2.751062289342248 | validation: 2.777276571999174]
	TIME [epoch: 8.31 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7599638408799434		[learning rate: 0.0024319]
	Learning Rate: 0.00243191
	LOSS [training: 2.7599638408799434 | validation: 2.674661470738025]
	TIME [epoch: 8.31 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7437526272374324		[learning rate: 0.0024231]
	Learning Rate: 0.00242308
	LOSS [training: 2.7437526272374324 | validation: 2.6402182840290758]
	TIME [epoch: 8.31 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.774089399189094		[learning rate: 0.0024143]
	Learning Rate: 0.00241429
	LOSS [training: 2.774089399189094 | validation: 2.814759617040186]
	TIME [epoch: 8.33 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7603339702302057		[learning rate: 0.0024055]
	Learning Rate: 0.00240553
	LOSS [training: 2.7603339702302057 | validation: 2.6216234424852214]
	TIME [epoch: 8.3 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.713285683236264		[learning rate: 0.0023968]
	Learning Rate: 0.0023968
	LOSS [training: 2.713285683236264 | validation: 2.646018478741699]
	TIME [epoch: 8.31 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7757845944096893		[learning rate: 0.0023881]
	Learning Rate: 0.0023881
	LOSS [training: 2.7757845944096893 | validation: 2.6611325407128916]
	TIME [epoch: 8.3 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7462611025055956		[learning rate: 0.0023794]
	Learning Rate: 0.00237943
	LOSS [training: 2.7462611025055956 | validation: 2.6069949347126746]
	TIME [epoch: 8.3 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7522783464186444		[learning rate: 0.0023708]
	Learning Rate: 0.0023708
	LOSS [training: 2.7522783464186444 | validation: 2.746028509895636]
	TIME [epoch: 8.32 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7226860621401996		[learning rate: 0.0023622]
	Learning Rate: 0.0023622
	LOSS [training: 2.7226860621401996 | validation: 2.6543876683416903]
	TIME [epoch: 8.3 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7236695814712166		[learning rate: 0.0023536]
	Learning Rate: 0.00235362
	LOSS [training: 2.7236695814712166 | validation: 2.6175675501642566]
	TIME [epoch: 8.31 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.836336916060448		[learning rate: 0.0023451]
	Learning Rate: 0.00234508
	LOSS [training: 2.836336916060448 | validation: 2.6351048775572345]
	TIME [epoch: 8.31 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7438632593451477		[learning rate: 0.0023366]
	Learning Rate: 0.00233657
	LOSS [training: 2.7438632593451477 | validation: 2.6378157693763837]
	TIME [epoch: 8.33 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7595447229301806		[learning rate: 0.0023281]
	Learning Rate: 0.00232809
	LOSS [training: 2.7595447229301806 | validation: 2.6488955221967743]
	TIME [epoch: 8.3 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7834398693594964		[learning rate: 0.0023196]
	Learning Rate: 0.00231964
	LOSS [training: 2.7834398693594964 | validation: 2.662222718347339]
	TIME [epoch: 8.31 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.734173891499486		[learning rate: 0.0023112]
	Learning Rate: 0.00231122
	LOSS [training: 2.734173891499486 | validation: 2.7146477755380527]
	TIME [epoch: 8.3 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6960352322186347		[learning rate: 0.0023028]
	Learning Rate: 0.00230284
	LOSS [training: 2.6960352322186347 | validation: 2.7669038214360784]
	TIME [epoch: 8.32 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7394816269325877		[learning rate: 0.0022945]
	Learning Rate: 0.00229448
	LOSS [training: 2.7394816269325877 | validation: 2.642228887682979]
	TIME [epoch: 8.31 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7137092015128004		[learning rate: 0.0022862]
	Learning Rate: 0.00228615
	LOSS [training: 2.7137092015128004 | validation: 2.624969149057863]
	TIME [epoch: 8.3 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7063323612473353		[learning rate: 0.0022779]
	Learning Rate: 0.00227786
	LOSS [training: 2.7063323612473353 | validation: 2.6918487827504824]
	TIME [epoch: 8.3 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7360541240175555		[learning rate: 0.0022696]
	Learning Rate: 0.00226959
	LOSS [training: 2.7360541240175555 | validation: 2.735970120087006]
	TIME [epoch: 8.31 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7452478410668015		[learning rate: 0.0022614]
	Learning Rate: 0.00226135
	LOSS [training: 2.7452478410668015 | validation: 2.6509497988122286]
	TIME [epoch: 8.32 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.714760698501579		[learning rate: 0.0022531]
	Learning Rate: 0.00225315
	LOSS [training: 2.714760698501579 | validation: 2.6662743509493634]
	TIME [epoch: 8.3 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.801477606618744		[learning rate: 0.002245]
	Learning Rate: 0.00224497
	LOSS [training: 2.801477606618744 | validation: 2.8522683060610214]
	TIME [epoch: 8.3 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8049416606529296		[learning rate: 0.0022368]
	Learning Rate: 0.00223682
	LOSS [training: 2.8049416606529296 | validation: 2.803758985621606]
	TIME [epoch: 8.3 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7069199414844034		[learning rate: 0.0022287]
	Learning Rate: 0.00222871
	LOSS [training: 2.7069199414844034 | validation: 2.6519416987483915]
	TIME [epoch: 8.33 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.706441134990138		[learning rate: 0.0022206]
	Learning Rate: 0.00222062
	LOSS [training: 2.706441134990138 | validation: 2.6564499396689225]
	TIME [epoch: 8.3 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7182081985547875		[learning rate: 0.0022126]
	Learning Rate: 0.00221256
	LOSS [training: 2.7182081985547875 | validation: 2.841642002366976]
	TIME [epoch: 8.3 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.730654232492532		[learning rate: 0.0022045]
	Learning Rate: 0.00220453
	LOSS [training: 2.730654232492532 | validation: 2.660178238396507]
	TIME [epoch: 8.3 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.767110682809946		[learning rate: 0.0021965]
	Learning Rate: 0.00219653
	LOSS [training: 2.767110682809946 | validation: 2.6126190097198467]
	TIME [epoch: 8.32 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.745038060473965		[learning rate: 0.0021886]
	Learning Rate: 0.00218856
	LOSS [training: 2.745038060473965 | validation: 2.656495233291804]
	TIME [epoch: 8.31 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.727068009554378		[learning rate: 0.0021806]
	Learning Rate: 0.00218061
	LOSS [training: 2.727068009554378 | validation: 2.643007704184846]
	TIME [epoch: 8.31 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6969613714509815		[learning rate: 0.0021727]
	Learning Rate: 0.0021727
	LOSS [training: 2.6969613714509815 | validation: 2.5997497947087504]
	TIME [epoch: 8.3 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.782238517205705		[learning rate: 0.0021648]
	Learning Rate: 0.00216482
	LOSS [training: 2.782238517205705 | validation: 2.6913367993459545]
	TIME [epoch: 8.3 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.726183246204398		[learning rate: 0.002157]
	Learning Rate: 0.00215696
	LOSS [training: 2.726183246204398 | validation: 2.6461465302958684]
	TIME [epoch: 8.33 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6883227506519423		[learning rate: 0.0021491]
	Learning Rate: 0.00214913
	LOSS [training: 2.6883227506519423 | validation: 2.6991313972038276]
	TIME [epoch: 8.31 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.751471240354829		[learning rate: 0.0021413]
	Learning Rate: 0.00214133
	LOSS [training: 2.751471240354829 | validation: 2.661730801900409]
	TIME [epoch: 8.3 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.684715427392364		[learning rate: 0.0021336]
	Learning Rate: 0.00213356
	LOSS [training: 2.684715427392364 | validation: 2.651864649742276]
	TIME [epoch: 8.3 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.725785587328043		[learning rate: 0.0021258]
	Learning Rate: 0.00212582
	LOSS [training: 2.725785587328043 | validation: 2.6974443767258984]
	TIME [epoch: 8.33 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7080493246506756		[learning rate: 0.0021181]
	Learning Rate: 0.0021181
	LOSS [training: 2.7080493246506756 | validation: 2.682545911637462]
	TIME [epoch: 8.31 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.733208465763843		[learning rate: 0.0021104]
	Learning Rate: 0.00211042
	LOSS [training: 2.733208465763843 | validation: 2.721505866443298]
	TIME [epoch: 8.3 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.731178203132006		[learning rate: 0.0021028]
	Learning Rate: 0.00210276
	LOSS [training: 2.731178203132006 | validation: 2.6944534572345127]
	TIME [epoch: 8.3 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.693606579670698		[learning rate: 0.0020951]
	Learning Rate: 0.00209513
	LOSS [training: 2.693606579670698 | validation: 2.658765130062096]
	TIME [epoch: 8.32 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.69502138470039		[learning rate: 0.0020875]
	Learning Rate: 0.00208752
	LOSS [training: 2.69502138470039 | validation: 2.6468183396605296]
	TIME [epoch: 8.31 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.745990812045411		[learning rate: 0.0020799]
	Learning Rate: 0.00207995
	LOSS [training: 2.745990812045411 | validation: 2.803760927444833]
	TIME [epoch: 8.31 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7219774004896466		[learning rate: 0.0020724]
	Learning Rate: 0.0020724
	LOSS [training: 2.7219774004896466 | validation: 2.620853295107767]
	TIME [epoch: 8.31 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.754759090194272		[learning rate: 0.0020649]
	Learning Rate: 0.00206488
	LOSS [training: 2.754759090194272 | validation: 2.699621622129201]
	TIME [epoch: 8.3 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.717309728495194		[learning rate: 0.0020574]
	Learning Rate: 0.00205739
	LOSS [training: 2.717309728495194 | validation: 2.6139600068524844]
	TIME [epoch: 8.33 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7283336398384996		[learning rate: 0.0020499]
	Learning Rate: 0.00204992
	LOSS [training: 2.7283336398384996 | validation: 2.7076773433253103]
	TIME [epoch: 8.3 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7169565541687946		[learning rate: 0.0020425]
	Learning Rate: 0.00204248
	LOSS [training: 2.7169565541687946 | validation: 2.6408048844382987]
	TIME [epoch: 8.31 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.70182462691385		[learning rate: 0.0020351]
	Learning Rate: 0.00203507
	LOSS [training: 2.70182462691385 | validation: 2.6608878509491882]
	TIME [epoch: 8.3 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.742000560513892		[learning rate: 0.0020277]
	Learning Rate: 0.00202768
	LOSS [training: 2.742000560513892 | validation: 2.5922478315574393]
	TIME [epoch: 8.33 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7529870567156154		[learning rate: 0.0020203]
	Learning Rate: 0.00202032
	LOSS [training: 2.7529870567156154 | validation: 2.7419996593139664]
	TIME [epoch: 8.3 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.71969427967983		[learning rate: 0.002013]
	Learning Rate: 0.00201299
	LOSS [training: 2.71969427967983 | validation: 2.6456099667345248]
	TIME [epoch: 8.3 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.701190205772913		[learning rate: 0.0020057]
	Learning Rate: 0.00200569
	LOSS [training: 2.701190205772913 | validation: 2.6042047679296205]
	TIME [epoch: 8.3 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7362464311357817		[learning rate: 0.0019984]
	Learning Rate: 0.00199841
	LOSS [training: 2.7362464311357817 | validation: 2.7279976947295848]
	TIME [epoch: 8.32 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.709679923836396		[learning rate: 0.0019912]
	Learning Rate: 0.00199116
	LOSS [training: 2.709679923836396 | validation: 2.675374734301819]
	TIME [epoch: 8.32 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.72048780624301		[learning rate: 0.0019839]
	Learning Rate: 0.00198393
	LOSS [training: 2.72048780624301 | validation: 2.5941779280021553]
	TIME [epoch: 8.3 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.712233399984389		[learning rate: 0.0019767]
	Learning Rate: 0.00197673
	LOSS [training: 2.712233399984389 | validation: 2.644515162868058]
	TIME [epoch: 8.3 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6822714401967422		[learning rate: 0.0019696]
	Learning Rate: 0.00196956
	LOSS [training: 2.6822714401967422 | validation: 2.722700147643536]
	TIME [epoch: 8.3 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7007735129827326		[learning rate: 0.0019624]
	Learning Rate: 0.00196241
	LOSS [training: 2.7007735129827326 | validation: 2.5991080686554486]
	TIME [epoch: 8.33 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6877215717019767		[learning rate: 0.0019553]
	Learning Rate: 0.00195529
	LOSS [training: 2.6877215717019767 | validation: 2.6187504825429193]
	TIME [epoch: 8.37 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7180113731317825		[learning rate: 0.0019482]
	Learning Rate: 0.00194819
	LOSS [training: 2.7180113731317825 | validation: 2.6524631674229395]
	TIME [epoch: 8.3 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7084722987825662		[learning rate: 0.0019411]
	Learning Rate: 0.00194112
	LOSS [training: 2.7084722987825662 | validation: 2.5926545322786705]
	TIME [epoch: 8.3 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6865397866465592		[learning rate: 0.0019341]
	Learning Rate: 0.00193408
	LOSS [training: 2.6865397866465592 | validation: 2.62380680738573]
	TIME [epoch: 8.33 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.670598290851876		[learning rate: 0.0019271]
	Learning Rate: 0.00192706
	LOSS [training: 2.670598290851876 | validation: 2.8981741366027416]
	TIME [epoch: 8.3 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.726400889789881		[learning rate: 0.0019201]
	Learning Rate: 0.00192006
	LOSS [training: 2.726400889789881 | validation: 2.5897965654248556]
	TIME [epoch: 8.3 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7071714468235806		[learning rate: 0.0019131]
	Learning Rate: 0.0019131
	LOSS [training: 2.7071714468235806 | validation: 2.610652769953404]
	TIME [epoch: 8.3 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.68532457720845		[learning rate: 0.0019062]
	Learning Rate: 0.00190615
	LOSS [training: 2.68532457720845 | validation: 2.670539773927297]
	TIME [epoch: 8.32 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7250448088628296		[learning rate: 0.0018992]
	Learning Rate: 0.00189924
	LOSS [training: 2.7250448088628296 | validation: 2.8701365911614367]
	TIME [epoch: 8.31 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.734417177800644		[learning rate: 0.0018923]
	Learning Rate: 0.00189234
	LOSS [training: 2.734417177800644 | validation: 2.663288925512248]
	TIME [epoch: 8.31 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6831184495527554		[learning rate: 0.0018855]
	Learning Rate: 0.00188548
	LOSS [training: 2.6831184495527554 | validation: 2.7626568446888786]
	TIME [epoch: 8.3 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7619957132284463		[learning rate: 0.0018786]
	Learning Rate: 0.00187863
	LOSS [training: 2.7619957132284463 | validation: 2.5951135434975123]
	TIME [epoch: 8.31 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6855776023063522		[learning rate: 0.0018718]
	Learning Rate: 0.00187182
	LOSS [training: 2.6855776023063522 | validation: 2.8233096403441675]
	TIME [epoch: 8.33 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7174144728016905		[learning rate: 0.001865]
	Learning Rate: 0.00186502
	LOSS [training: 2.7174144728016905 | validation: 2.6112737502917853]
	TIME [epoch: 8.3 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.711132080108355		[learning rate: 0.0018583]
	Learning Rate: 0.00185825
	LOSS [training: 2.711132080108355 | validation: 2.617715123100731]
	TIME [epoch: 8.3 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.696854527392816		[learning rate: 0.0018515]
	Learning Rate: 0.00185151
	LOSS [training: 2.696854527392816 | validation: 2.616871390914034]
	TIME [epoch: 8.3 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6830054074076237		[learning rate: 0.0018448]
	Learning Rate: 0.00184479
	LOSS [training: 2.6830054074076237 | validation: 2.752081389293563]
	TIME [epoch: 8.33 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.729996579319737		[learning rate: 0.0018381]
	Learning Rate: 0.0018381
	LOSS [training: 2.729996579319737 | validation: 2.5792993621862452]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_566.pth
	Model improved!!!
EPOCH 567/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7555564080443644		[learning rate: 0.0018314]
	Learning Rate: 0.00183143
	LOSS [training: 2.7555564080443644 | validation: 2.6236136105939667]
	TIME [epoch: 8.31 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7467433640775547		[learning rate: 0.0018248]
	Learning Rate: 0.00182478
	LOSS [training: 2.7467433640775547 | validation: 2.9874673285818623]
	TIME [epoch: 8.3 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.796320369980134		[learning rate: 0.0018182]
	Learning Rate: 0.00181816
	LOSS [training: 2.796320369980134 | validation: 2.592268450239734]
	TIME [epoch: 8.31 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6870622439194123		[learning rate: 0.0018116]
	Learning Rate: 0.00181156
	LOSS [training: 2.6870622439194123 | validation: 2.6058765821185217]
	TIME [epoch: 8.32 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.690342955527478		[learning rate: 0.001805]
	Learning Rate: 0.00180499
	LOSS [training: 2.690342955527478 | validation: 2.670253330898904]
	TIME [epoch: 8.3 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.720872617112371		[learning rate: 0.0017984]
	Learning Rate: 0.00179844
	LOSS [training: 2.720872617112371 | validation: 2.7392992944564973]
	TIME [epoch: 8.3 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6951083406322764		[learning rate: 0.0017919]
	Learning Rate: 0.00179191
	LOSS [training: 2.6951083406322764 | validation: 2.685154915205471]
	TIME [epoch: 8.3 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7069818491419735		[learning rate: 0.0017854]
	Learning Rate: 0.00178541
	LOSS [training: 2.7069818491419735 | validation: 2.6579369184400052]
	TIME [epoch: 8.33 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7217405556106766		[learning rate: 0.0017789]
	Learning Rate: 0.00177893
	LOSS [training: 2.7217405556106766 | validation: 2.625974669611553]
	TIME [epoch: 8.3 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6707835039744423		[learning rate: 0.0017725]
	Learning Rate: 0.00177247
	LOSS [training: 2.6707835039744423 | validation: 2.87197362206925]
	TIME [epoch: 8.3 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.729129296656134		[learning rate: 0.001766]
	Learning Rate: 0.00176604
	LOSS [training: 2.729129296656134 | validation: 2.6268642331896364]
	TIME [epoch: 8.3 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.732737968263873		[learning rate: 0.0017596]
	Learning Rate: 0.00175963
	LOSS [training: 2.732737968263873 | validation: 2.6836038462797767]
	TIME [epoch: 8.32 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.692675927583657		[learning rate: 0.0017532]
	Learning Rate: 0.00175324
	LOSS [training: 2.692675927583657 | validation: 2.6559562093183122]
	TIME [epoch: 8.3 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7026645806877982		[learning rate: 0.0017469]
	Learning Rate: 0.00174688
	LOSS [training: 2.7026645806877982 | validation: 2.726192982442223]
	TIME [epoch: 8.3 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.742784393588429		[learning rate: 0.0017405]
	Learning Rate: 0.00174054
	LOSS [training: 2.742784393588429 | validation: 2.64809888563836]
	TIME [epoch: 8.3 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6771678365841		[learning rate: 0.0017342]
	Learning Rate: 0.00173422
	LOSS [training: 2.6771678365841 | validation: 2.6239545066815135]
	TIME [epoch: 8.31 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6980977409408453		[learning rate: 0.0017279]
	Learning Rate: 0.00172793
	LOSS [training: 2.6980977409408453 | validation: 2.5953248800909092]
	TIME [epoch: 8.32 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.686141329413706		[learning rate: 0.0017217]
	Learning Rate: 0.00172166
	LOSS [training: 2.686141329413706 | validation: 2.598166960404294]
	TIME [epoch: 8.3 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6802608530427365		[learning rate: 0.0017154]
	Learning Rate: 0.00171541
	LOSS [training: 2.6802608530427365 | validation: 2.6391470985411374]
	TIME [epoch: 8.3 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6994388701829033		[learning rate: 0.0017092]
	Learning Rate: 0.00170919
	LOSS [training: 2.6994388701829033 | validation: 2.625125251548066]
	TIME [epoch: 8.3 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.690941731218129		[learning rate: 0.001703]
	Learning Rate: 0.00170298
	LOSS [training: 2.690941731218129 | validation: 2.721218728607715]
	TIME [epoch: 8.32 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.691649215836698		[learning rate: 0.0016968]
	Learning Rate: 0.0016968
	LOSS [training: 2.691649215836698 | validation: 2.6063231618546903]
	TIME [epoch: 8.3 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6782305219368507		[learning rate: 0.0016906]
	Learning Rate: 0.00169065
	LOSS [training: 2.6782305219368507 | validation: 2.7612914766316816]
	TIME [epoch: 8.3 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.702418457641166		[learning rate: 0.0016845]
	Learning Rate: 0.00168451
	LOSS [training: 2.702418457641166 | validation: 2.599737291691265]
	TIME [epoch: 8.3 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7177108386438578		[learning rate: 0.0016784]
	Learning Rate: 0.0016784
	LOSS [training: 2.7177108386438578 | validation: 2.5988146260156935]
	TIME [epoch: 8.31 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7384101969626853		[learning rate: 0.0016723]
	Learning Rate: 0.00167231
	LOSS [training: 2.7384101969626853 | validation: 2.5936442076521447]
	TIME [epoch: 8.31 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.691579402054269		[learning rate: 0.0016662]
	Learning Rate: 0.00166624
	LOSS [training: 2.691579402054269 | validation: 2.6123614719818447]
	TIME [epoch: 8.3 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6886276521360024		[learning rate: 0.0016602]
	Learning Rate: 0.00166019
	LOSS [training: 2.6886276521360024 | validation: 2.828068075668951]
	TIME [epoch: 8.3 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7167134302205627		[learning rate: 0.0016542]
	Learning Rate: 0.00165417
	LOSS [training: 2.7167134302205627 | validation: 2.645904768628652]
	TIME [epoch: 8.31 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6717406270476927		[learning rate: 0.0016482]
	Learning Rate: 0.00164816
	LOSS [training: 2.6717406270476927 | validation: 2.6080009443450782]
	TIME [epoch: 8.32 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.682415830472483		[learning rate: 0.0016422]
	Learning Rate: 0.00164218
	LOSS [training: 2.682415830472483 | validation: 2.6058294644197693]
	TIME [epoch: 8.3 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.687623353796508		[learning rate: 0.0016362]
	Learning Rate: 0.00163622
	LOSS [training: 2.687623353796508 | validation: 2.6457088467497463]
	TIME [epoch: 8.3 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6946447795261532		[learning rate: 0.0016303]
	Learning Rate: 0.00163028
	LOSS [training: 2.6946447795261532 | validation: 2.6643832934090423]
	TIME [epoch: 8.3 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7852828100235345		[learning rate: 0.0016244]
	Learning Rate: 0.00162437
	LOSS [training: 2.7852828100235345 | validation: 2.704360454579078]
	TIME [epoch: 8.32 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.789025318353162		[learning rate: 0.0016185]
	Learning Rate: 0.00161847
	LOSS [training: 2.789025318353162 | validation: 2.792553282436022]
	TIME [epoch: 8.31 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.711985864731895		[learning rate: 0.0016126]
	Learning Rate: 0.0016126
	LOSS [training: 2.711985864731895 | validation: 2.6381950256141335]
	TIME [epoch: 8.3 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6865899515478726		[learning rate: 0.0016067]
	Learning Rate: 0.00160675
	LOSS [training: 2.6865899515478726 | validation: 2.621533188726159]
	TIME [epoch: 8.3 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6914068135036695		[learning rate: 0.0016009]
	Learning Rate: 0.00160092
	LOSS [training: 2.6914068135036695 | validation: 2.6074784831675535]
	TIME [epoch: 8.31 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.668009675944019		[learning rate: 0.0015951]
	Learning Rate: 0.00159511
	LOSS [training: 2.668009675944019 | validation: 2.6927200717107365]
	TIME [epoch: 8.31 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.67076446591659		[learning rate: 0.0015893]
	Learning Rate: 0.00158932
	LOSS [training: 2.67076446591659 | validation: 2.641765440235311]
	TIME [epoch: 8.3 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6709934841540055		[learning rate: 0.0015835]
	Learning Rate: 0.00158355
	LOSS [training: 2.6709934841540055 | validation: 2.634875592966768]
	TIME [epoch: 8.3 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6782885136587398		[learning rate: 0.0015778]
	Learning Rate: 0.0015778
	LOSS [training: 2.6782885136587398 | validation: 2.6074542437549058]
	TIME [epoch: 8.3 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6691270750630824		[learning rate: 0.0015721]
	Learning Rate: 0.00157208
	LOSS [training: 2.6691270750630824 | validation: 2.6820292581720206]
	TIME [epoch: 8.33 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8221774036405844		[learning rate: 0.0015664]
	Learning Rate: 0.00156637
	LOSS [training: 2.8221774036405844 | validation: 2.768320457443445]
	TIME [epoch: 8.3 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.709213140443684		[learning rate: 0.0015607]
	Learning Rate: 0.00156069
	LOSS [training: 2.709213140443684 | validation: 2.7013921064800894]
	TIME [epoch: 8.3 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.681783032645515		[learning rate: 0.001555]
	Learning Rate: 0.00155502
	LOSS [training: 2.681783032645515 | validation: 2.6058061062896876]
	TIME [epoch: 8.3 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.683532465706788		[learning rate: 0.0015494]
	Learning Rate: 0.00154938
	LOSS [training: 2.683532465706788 | validation: 2.5779013588753745]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_613.pth
	Model improved!!!
EPOCH 614/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.702386075828503		[learning rate: 0.0015438]
	Learning Rate: 0.00154376
	LOSS [training: 2.702386075828503 | validation: 2.8473465485414073]
	TIME [epoch: 8.32 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7284060413977906		[learning rate: 0.0015382]
	Learning Rate: 0.00153815
	LOSS [training: 2.7284060413977906 | validation: 2.598332190627235]
	TIME [epoch: 8.31 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.696315644228101		[learning rate: 0.0015326]
	Learning Rate: 0.00153257
	LOSS [training: 2.696315644228101 | validation: 2.6827816064080494]
	TIME [epoch: 8.31 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.683712695010244		[learning rate: 0.001527]
	Learning Rate: 0.00152701
	LOSS [training: 2.683712695010244 | validation: 2.6277977772378875]
	TIME [epoch: 8.32 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6871060055990474		[learning rate: 0.0015215]
	Learning Rate: 0.00152147
	LOSS [training: 2.6871060055990474 | validation: 2.6528926585096935]
	TIME [epoch: 8.32 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.672660900695644		[learning rate: 0.0015159]
	Learning Rate: 0.00151595
	LOSS [training: 2.672660900695644 | validation: 2.6173868793828943]
	TIME [epoch: 8.31 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.689829727712439		[learning rate: 0.0015104]
	Learning Rate: 0.00151045
	LOSS [training: 2.689829727712439 | validation: 2.641575821432456]
	TIME [epoch: 8.31 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6977789821086904		[learning rate: 0.001505]
	Learning Rate: 0.00150496
	LOSS [training: 2.6977789821086904 | validation: 2.619402314221259]
	TIME [epoch: 8.31 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.663396658455032		[learning rate: 0.0014995]
	Learning Rate: 0.0014995
	LOSS [training: 2.663396658455032 | validation: 2.6356609615360713]
	TIME [epoch: 8.33 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7000111584635698		[learning rate: 0.0014941]
	Learning Rate: 0.00149406
	LOSS [training: 2.7000111584635698 | validation: 2.5874009769965065]
	TIME [epoch: 8.31 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.678509889693306		[learning rate: 0.0014886]
	Learning Rate: 0.00148864
	LOSS [training: 2.678509889693306 | validation: 2.6021985754958883]
	TIME [epoch: 8.31 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6754855412051226		[learning rate: 0.0014832]
	Learning Rate: 0.00148324
	LOSS [training: 2.6754855412051226 | validation: 2.6244620041829445]
	TIME [epoch: 8.31 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.707426093046041		[learning rate: 0.0014779]
	Learning Rate: 0.00147785
	LOSS [training: 2.707426093046041 | validation: 2.604257286541193]
	TIME [epoch: 8.34 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.682043872178782		[learning rate: 0.0014725]
	Learning Rate: 0.00147249
	LOSS [training: 2.682043872178782 | validation: 2.651767748838027]
	TIME [epoch: 8.31 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7619168800788607		[learning rate: 0.0014671]
	Learning Rate: 0.00146715
	LOSS [training: 2.7619168800788607 | validation: 2.6542963001286313]
	TIME [epoch: 8.31 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6899105308588576		[learning rate: 0.0014618]
	Learning Rate: 0.00146182
	LOSS [training: 2.6899105308588576 | validation: 2.584340101908178]
	TIME [epoch: 8.31 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.681792805946948		[learning rate: 0.0014565]
	Learning Rate: 0.00145652
	LOSS [training: 2.681792805946948 | validation: 2.8117726927623066]
	TIME [epoch: 8.33 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.70673690126801		[learning rate: 0.0014512]
	Learning Rate: 0.00145123
	LOSS [training: 2.70673690126801 | validation: 2.689576799791479]
	TIME [epoch: 8.32 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.660099768437085		[learning rate: 0.001446]
	Learning Rate: 0.00144597
	LOSS [training: 2.660099768437085 | validation: 2.644898569841661]
	TIME [epoch: 8.31 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.690975087464996		[learning rate: 0.0014407]
	Learning Rate: 0.00144072
	LOSS [training: 2.690975087464996 | validation: 2.645576667643626]
	TIME [epoch: 8.31 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6814811836188914		[learning rate: 0.0014355]
	Learning Rate: 0.00143549
	LOSS [training: 2.6814811836188914 | validation: 2.5982474625455683]
	TIME [epoch: 8.31 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7052385203156475		[learning rate: 0.0014303]
	Learning Rate: 0.00143028
	LOSS [training: 2.7052385203156475 | validation: 2.701287711708299]
	TIME [epoch: 8.34 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6939066993044505		[learning rate: 0.0014251]
	Learning Rate: 0.00142509
	LOSS [training: 2.6939066993044505 | validation: 2.6059127078608144]
	TIME [epoch: 8.31 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.670529581518149		[learning rate: 0.0014199]
	Learning Rate: 0.00141992
	LOSS [training: 2.670529581518149 | validation: 2.6072830595297605]
	TIME [epoch: 8.31 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6705110687430107		[learning rate: 0.0014148]
	Learning Rate: 0.00141476
	LOSS [training: 2.6705110687430107 | validation: 2.6534090537284873]
	TIME [epoch: 8.31 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.692129339120865		[learning rate: 0.0014096]
	Learning Rate: 0.00140963
	LOSS [training: 2.692129339120865 | validation: 2.606548586440301]
	TIME [epoch: 8.33 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6822159477881646		[learning rate: 0.0014045]
	Learning Rate: 0.00140451
	LOSS [training: 2.6822159477881646 | validation: 2.6094312842057175]
	TIME [epoch: 8.31 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6773562060686547		[learning rate: 0.0013994]
	Learning Rate: 0.00139942
	LOSS [training: 2.6773562060686547 | validation: 2.626136796681383]
	TIME [epoch: 8.31 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.685004674305786		[learning rate: 0.0013943]
	Learning Rate: 0.00139434
	LOSS [training: 2.685004674305786 | validation: 2.5966999398218107]
	TIME [epoch: 8.31 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6836281843624348		[learning rate: 0.0013893]
	Learning Rate: 0.00138928
	LOSS [training: 2.6836281843624348 | validation: 2.5843188280333047]
	TIME [epoch: 8.33 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.679179951755648		[learning rate: 0.0013842]
	Learning Rate: 0.00138424
	LOSS [training: 2.679179951755648 | validation: 2.5847435716607565]
	TIME [epoch: 8.32 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6760603674466528		[learning rate: 0.0013792]
	Learning Rate: 0.00137921
	LOSS [training: 2.6760603674466528 | validation: 2.6191525369407342]
	TIME [epoch: 8.31 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.669425095282638		[learning rate: 0.0013742]
	Learning Rate: 0.00137421
	LOSS [training: 2.669425095282638 | validation: 2.580392824618081]
	TIME [epoch: 8.31 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6759078593059886		[learning rate: 0.0013692]
	Learning Rate: 0.00136922
	LOSS [training: 2.6759078593059886 | validation: 2.6210538078407337]
	TIME [epoch: 8.31 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.673285391956241		[learning rate: 0.0013643]
	Learning Rate: 0.00136425
	LOSS [training: 2.673285391956241 | validation: 2.617752300770069]
	TIME [epoch: 8.34 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.68568128186694		[learning rate: 0.0013593]
	Learning Rate: 0.0013593
	LOSS [training: 2.68568128186694 | validation: 2.6491262482647495]
	TIME [epoch: 8.31 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.689956946498983		[learning rate: 0.0013544]
	Learning Rate: 0.00135437
	LOSS [training: 2.689956946498983 | validation: 2.7683294602822364]
	TIME [epoch: 8.31 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.710970811250781		[learning rate: 0.0013495]
	Learning Rate: 0.00134945
	LOSS [training: 2.710970811250781 | validation: 2.6023099172104236]
	TIME [epoch: 8.31 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.667999583493029		[learning rate: 0.0013446]
	Learning Rate: 0.00134456
	LOSS [training: 2.667999583493029 | validation: 2.6172778297061474]
	TIME [epoch: 8.34 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.66534750458614		[learning rate: 0.0013397]
	Learning Rate: 0.00133968
	LOSS [training: 2.66534750458614 | validation: 2.5991647244773426]
	TIME [epoch: 8.31 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.690064188537879		[learning rate: 0.0013348]
	Learning Rate: 0.00133481
	LOSS [training: 2.690064188537879 | validation: 2.60376105537895]
	TIME [epoch: 8.31 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.649559650064454		[learning rate: 0.00133]
	Learning Rate: 0.00132997
	LOSS [training: 2.649559650064454 | validation: 2.5977760007833313]
	TIME [epoch: 8.31 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.66022182012765		[learning rate: 0.0013251]
	Learning Rate: 0.00132514
	LOSS [training: 2.66022182012765 | validation: 2.6817619081513944]
	TIME [epoch: 8.33 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.683376118998678		[learning rate: 0.0013203]
	Learning Rate: 0.00132034
	LOSS [training: 2.683376118998678 | validation: 2.610185943879697]
	TIME [epoch: 8.32 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.677883494080232		[learning rate: 0.0013155]
	Learning Rate: 0.00131554
	LOSS [training: 2.677883494080232 | validation: 2.6191958994807774]
	TIME [epoch: 8.31 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6586675185915327		[learning rate: 0.0013108]
	Learning Rate: 0.00131077
	LOSS [training: 2.6586675185915327 | validation: 2.5890520638037477]
	TIME [epoch: 8.31 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6843874633054874		[learning rate: 0.001306]
	Learning Rate: 0.00130601
	LOSS [training: 2.6843874633054874 | validation: 2.6733109234191517]
	TIME [epoch: 8.31 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.678998035673079		[learning rate: 0.0013013]
	Learning Rate: 0.00130127
	LOSS [training: 2.678998035673079 | validation: 2.6263515351121294]
	TIME [epoch: 8.34 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6920094115679563		[learning rate: 0.0012966]
	Learning Rate: 0.00129655
	LOSS [training: 2.6920094115679563 | validation: 2.6221069313032377]
	TIME [epoch: 8.31 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.669298296781348		[learning rate: 0.0012918]
	Learning Rate: 0.00129185
	LOSS [training: 2.669298296781348 | validation: 2.5963666587205827]
	TIME [epoch: 8.31 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.652188804197574		[learning rate: 0.0012872]
	Learning Rate: 0.00128716
	LOSS [training: 2.652188804197574 | validation: 2.612431180103457]
	TIME [epoch: 8.31 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.680057628943781		[learning rate: 0.0012825]
	Learning Rate: 0.00128249
	LOSS [training: 2.680057628943781 | validation: 2.6137083796051352]
	TIME [epoch: 8.33 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6491804070408858		[learning rate: 0.0012778]
	Learning Rate: 0.00127783
	LOSS [training: 2.6491804070408858 | validation: 2.608673489136624]
	TIME [epoch: 8.31 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.680577923728369		[learning rate: 0.0012732]
	Learning Rate: 0.00127319
	LOSS [training: 2.680577923728369 | validation: 2.585759350206756]
	TIME [epoch: 8.31 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6608935734453762		[learning rate: 0.0012686]
	Learning Rate: 0.00126857
	LOSS [training: 2.6608935734453762 | validation: 2.5966545164480186]
	TIME [epoch: 8.3 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.654257141239184		[learning rate: 0.001264]
	Learning Rate: 0.00126397
	LOSS [training: 2.654257141239184 | validation: 2.597735767348346]
	TIME [epoch: 8.32 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6639656721533536		[learning rate: 0.0012594]
	Learning Rate: 0.00125938
	LOSS [training: 2.6639656721533536 | validation: 2.646121008596425]
	TIME [epoch: 8.32 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6845610080375364		[learning rate: 0.0012548]
	Learning Rate: 0.00125481
	LOSS [training: 2.6845610080375364 | validation: 2.5853801088944843]
	TIME [epoch: 8.31 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.647192863540794		[learning rate: 0.0012503]
	Learning Rate: 0.00125026
	LOSS [training: 2.647192863540794 | validation: 2.635346535983353]
	TIME [epoch: 8.31 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6635168845760977		[learning rate: 0.0012457]
	Learning Rate: 0.00124572
	LOSS [training: 2.6635168845760977 | validation: 2.649623829882432]
	TIME [epoch: 8.31 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6721308843979146		[learning rate: 0.0012412]
	Learning Rate: 0.0012412
	LOSS [training: 2.6721308843979146 | validation: 2.594369651844021]
	TIME [epoch: 8.33 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7021840892210984		[learning rate: 0.0012367]
	Learning Rate: 0.0012367
	LOSS [training: 2.7021840892210984 | validation: 2.715135415017951]
	TIME [epoch: 8.3 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7054219277355473		[learning rate: 0.0012322]
	Learning Rate: 0.00123221
	LOSS [training: 2.7054219277355473 | validation: 2.669149805456866]
	TIME [epoch: 8.3 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.672829522727282		[learning rate: 0.0012277]
	Learning Rate: 0.00122774
	LOSS [training: 2.672829522727282 | validation: 2.5852267955868067]
	TIME [epoch: 8.3 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6780620983466195		[learning rate: 0.0012233]
	Learning Rate: 0.00122328
	LOSS [training: 2.6780620983466195 | validation: 2.697538172207616]
	TIME [epoch: 8.33 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.678383670491424		[learning rate: 0.0012188]
	Learning Rate: 0.00121884
	LOSS [training: 2.678383670491424 | validation: 2.597116281311165]
	TIME [epoch: 8.31 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.686182186404305		[learning rate: 0.0012144]
	Learning Rate: 0.00121442
	LOSS [training: 2.686182186404305 | validation: 2.6835594417663793]
	TIME [epoch: 8.31 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.676751653782069		[learning rate: 0.00121]
	Learning Rate: 0.00121001
	LOSS [training: 2.676751653782069 | validation: 2.6080898453320356]
	TIME [epoch: 8.31 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6657288891559476		[learning rate: 0.0012056]
	Learning Rate: 0.00120562
	LOSS [training: 2.6657288891559476 | validation: 2.579764510559927]
	TIME [epoch: 8.31 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6604470630078376		[learning rate: 0.0012012]
	Learning Rate: 0.00120125
	LOSS [training: 2.6604470630078376 | validation: 2.607530483919237]
	TIME [epoch: 8.32 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6535483649353844		[learning rate: 0.0011969]
	Learning Rate: 0.00119689
	LOSS [training: 2.6535483649353844 | validation: 2.6032748307818565]
	TIME [epoch: 8.3 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6721556138612548		[learning rate: 0.0011925]
	Learning Rate: 0.00119254
	LOSS [training: 2.6721556138612548 | validation: 2.603083614870488]
	TIME [epoch: 8.3 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.66739198070731		[learning rate: 0.0011882]
	Learning Rate: 0.00118821
	LOSS [training: 2.66739198070731 | validation: 2.602193053959505]
	TIME [epoch: 8.3 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.676437812973449		[learning rate: 0.0011839]
	Learning Rate: 0.0011839
	LOSS [training: 2.676437812973449 | validation: 2.577026436993112]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_687.pth
	Model improved!!!
EPOCH 688/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6627345772884605		[learning rate: 0.0011796]
	Learning Rate: 0.00117961
	LOSS [training: 2.6627345772884605 | validation: 2.7106547028293386]
	TIME [epoch: 8.31 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.67113482075146		[learning rate: 0.0011753]
	Learning Rate: 0.00117532
	LOSS [training: 2.67113482075146 | validation: 2.606863258405082]
	TIME [epoch: 8.31 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.653847128232687		[learning rate: 0.0011711]
	Learning Rate: 0.00117106
	LOSS [training: 2.653847128232687 | validation: 2.6154995287212808]
	TIME [epoch: 8.31 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.639627762278126		[learning rate: 0.0011668]
	Learning Rate: 0.00116681
	LOSS [training: 2.639627762278126 | validation: 2.578981261137459]
	TIME [epoch: 8.33 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.645053057324971		[learning rate: 0.0011626]
	Learning Rate: 0.00116258
	LOSS [training: 2.645053057324971 | validation: 2.594390568147567]
	TIME [epoch: 8.31 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6719809183921157		[learning rate: 0.0011584]
	Learning Rate: 0.00115836
	LOSS [training: 2.6719809183921157 | validation: 2.630794833925024]
	TIME [epoch: 8.31 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6628716598271054		[learning rate: 0.0011542]
	Learning Rate: 0.00115415
	LOSS [training: 2.6628716598271054 | validation: 2.6282534479826625]
	TIME [epoch: 8.31 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6826681792274494		[learning rate: 0.00115]
	Learning Rate: 0.00114996
	LOSS [training: 2.6826681792274494 | validation: 2.6340674532739152]
	TIME [epoch: 8.32 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.675398157856712		[learning rate: 0.0011458]
	Learning Rate: 0.00114579
	LOSS [training: 2.675398157856712 | validation: 2.587444070579763]
	TIME [epoch: 8.32 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.665773900127767		[learning rate: 0.0011416]
	Learning Rate: 0.00114163
	LOSS [training: 2.665773900127767 | validation: 2.5909406848771797]
	TIME [epoch: 8.31 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.683451005944299		[learning rate: 0.0011375]
	Learning Rate: 0.00113749
	LOSS [training: 2.683451005944299 | validation: 2.6068678613106044]
	TIME [epoch: 8.31 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6501811424254327		[learning rate: 0.0011334]
	Learning Rate: 0.00113336
	LOSS [training: 2.6501811424254327 | validation: 2.583372655334161]
	TIME [epoch: 8.3 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.661935116057252		[learning rate: 0.0011292]
	Learning Rate: 0.00112925
	LOSS [training: 2.661935116057252 | validation: 2.5984634856182973]
	TIME [epoch: 8.33 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.649517623993434		[learning rate: 0.0011252]
	Learning Rate: 0.00112515
	LOSS [training: 2.649517623993434 | validation: 2.6129830698149066]
	TIME [epoch: 8.3 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.65142357352265		[learning rate: 0.0011211]
	Learning Rate: 0.00112107
	LOSS [training: 2.65142357352265 | validation: 2.7196233120233146]
	TIME [epoch: 8.31 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.673623063366157		[learning rate: 0.001117]
	Learning Rate: 0.001117
	LOSS [training: 2.673623063366157 | validation: 2.618220384302684]
	TIME [epoch: 8.31 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6472527508956136		[learning rate: 0.0011129]
	Learning Rate: 0.00111295
	LOSS [training: 2.6472527508956136 | validation: 2.748673186674554]
	TIME [epoch: 8.33 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6757304752665534		[learning rate: 0.0011089]
	Learning Rate: 0.00110891
	LOSS [training: 2.6757304752665534 | validation: 2.627112896176927]
	TIME [epoch: 8.31 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6597256567394845		[learning rate: 0.0011049]
	Learning Rate: 0.00110488
	LOSS [training: 2.6597256567394845 | validation: 2.582852396780889]
	TIME [epoch: 8.31 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6663563527472727		[learning rate: 0.0011009]
	Learning Rate: 0.00110087
	LOSS [training: 2.6663563527472727 | validation: 2.6220419850799996]
	TIME [epoch: 8.31 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.647111214319007		[learning rate: 0.0010969]
	Learning Rate: 0.00109688
	LOSS [training: 2.647111214319007 | validation: 2.5981262532935077]
	TIME [epoch: 8.31 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.711065975383025		[learning rate: 0.0010929]
	Learning Rate: 0.0010929
	LOSS [training: 2.711065975383025 | validation: 2.620181447208241]
	TIME [epoch: 8.33 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7295183350453938		[learning rate: 0.0010889]
	Learning Rate: 0.00108893
	LOSS [training: 2.7295183350453938 | validation: 2.600466268666258]
	TIME [epoch: 8.31 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6497376552037966		[learning rate: 0.001085]
	Learning Rate: 0.00108498
	LOSS [training: 2.6497376552037966 | validation: 2.5954242209784666]
	TIME [epoch: 8.3 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.656941390674093		[learning rate: 0.001081]
	Learning Rate: 0.00108104
	LOSS [training: 2.656941390674093 | validation: 2.618512721399364]
	TIME [epoch: 8.31 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6745130619636086		[learning rate: 0.0010771]
	Learning Rate: 0.00107712
	LOSS [training: 2.6745130619636086 | validation: 2.5902568559156585]
	TIME [epoch: 8.33 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6595192730629487		[learning rate: 0.0010732]
	Learning Rate: 0.00107321
	LOSS [training: 2.6595192730629487 | validation: 2.584940565069263]
	TIME [epoch: 8.31 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.660331946180931		[learning rate: 0.0010693]
	Learning Rate: 0.00106931
	LOSS [training: 2.660331946180931 | validation: 2.621291330052704]
	TIME [epoch: 8.31 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.648028548996698		[learning rate: 0.0010654]
	Learning Rate: 0.00106543
	LOSS [training: 2.648028548996698 | validation: 2.617312691376621]
	TIME [epoch: 8.31 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.64444933836397		[learning rate: 0.0010616]
	Learning Rate: 0.00106157
	LOSS [training: 2.64444933836397 | validation: 2.607613722339783]
	TIME [epoch: 8.33 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6419096534929047		[learning rate: 0.0010577]
	Learning Rate: 0.00105771
	LOSS [training: 2.6419096534929047 | validation: 2.61366065925813]
	TIME [epoch: 8.31 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6721995679582577		[learning rate: 0.0010539]
	Learning Rate: 0.00105388
	LOSS [training: 2.6721995679582577 | validation: 2.663783400357757]
	TIME [epoch: 8.3 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6631591361450875		[learning rate: 0.0010501]
	Learning Rate: 0.00105005
	LOSS [training: 2.6631591361450875 | validation: 2.570909648069562]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_720.pth
	Model improved!!!
EPOCH 721/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.683661070924069		[learning rate: 0.0010462]
	Learning Rate: 0.00104624
	LOSS [training: 2.683661070924069 | validation: 2.5844748388274024]
	TIME [epoch: 8.31 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.652563045654919		[learning rate: 0.0010424]
	Learning Rate: 0.00104244
	LOSS [training: 2.652563045654919 | validation: 2.582389237035164]
	TIME [epoch: 8.32 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6769860507694014		[learning rate: 0.0010387]
	Learning Rate: 0.00103866
	LOSS [training: 2.6769860507694014 | validation: 2.5705065833854572]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_723.pth
	Model improved!!!
EPOCH 724/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6713450728589407		[learning rate: 0.0010349]
	Learning Rate: 0.00103489
	LOSS [training: 2.6713450728589407 | validation: 2.616102017592637]
	TIME [epoch: 8.31 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.649481078131814		[learning rate: 0.0010311]
	Learning Rate: 0.00103114
	LOSS [training: 2.649481078131814 | validation: 2.5947978876029403]
	TIME [epoch: 8.3 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6813852167935073		[learning rate: 0.0010274]
	Learning Rate: 0.00102739
	LOSS [training: 2.6813852167935073 | validation: 2.58936348052959]
	TIME [epoch: 8.33 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6727304228798077		[learning rate: 0.0010237]
	Learning Rate: 0.00102367
	LOSS [training: 2.6727304228798077 | validation: 2.5759549097920145]
	TIME [epoch: 8.3 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.65652802509154		[learning rate: 0.00102]
	Learning Rate: 0.00101995
	LOSS [training: 2.65652802509154 | validation: 2.6099176180466017]
	TIME [epoch: 8.3 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.664272164029967		[learning rate: 0.0010162]
	Learning Rate: 0.00101625
	LOSS [training: 2.664272164029967 | validation: 2.6071051574055835]
	TIME [epoch: 8.3 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.656379579724314		[learning rate: 0.0010126]
	Learning Rate: 0.00101256
	LOSS [training: 2.656379579724314 | validation: 2.5780438433355455]
	TIME [epoch: 8.33 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6598707204946326		[learning rate: 0.0010089]
	Learning Rate: 0.00100889
	LOSS [training: 2.6598707204946326 | validation: 2.6093907796644062]
	TIME [epoch: 8.3 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.678910486542878		[learning rate: 0.0010052]
	Learning Rate: 0.00100522
	LOSS [training: 2.678910486542878 | validation: 2.594168660760977]
	TIME [epoch: 8.3 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.655404300471637		[learning rate: 0.0010016]
	Learning Rate: 0.00100158
	LOSS [training: 2.655404300471637 | validation: 2.5832190496686303]
	TIME [epoch: 8.3 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6466829875908466		[learning rate: 0.00099794]
	Learning Rate: 0.000997942
	LOSS [training: 2.6466829875908466 | validation: 2.6239575813882468]
	TIME [epoch: 8.31 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6564987026142717		[learning rate: 0.00099432]
	Learning Rate: 0.00099432
	LOSS [training: 2.6564987026142717 | validation: 2.6278680090974826]
	TIME [epoch: 8.32 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.654772976093087		[learning rate: 0.00099071]
	Learning Rate: 0.000990712
	LOSS [training: 2.654772976093087 | validation: 2.63976234319814]
	TIME [epoch: 8.3 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.663622673189752		[learning rate: 0.00098712]
	Learning Rate: 0.000987116
	LOSS [training: 2.663622673189752 | validation: 2.672439568104881]
	TIME [epoch: 8.3 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6531699351691476		[learning rate: 0.00098353]
	Learning Rate: 0.000983534
	LOSS [training: 2.6531699351691476 | validation: 2.5856602703055214]
	TIME [epoch: 8.3 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6582527933401416		[learning rate: 0.00097996]
	Learning Rate: 0.000979965
	LOSS [training: 2.6582527933401416 | validation: 2.591481978904861]
	TIME [epoch: 8.33 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6544478132192393		[learning rate: 0.00097641]
	Learning Rate: 0.000976409
	LOSS [training: 2.6544478132192393 | validation: 2.6053543115393363]
	TIME [epoch: 8.3 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6600121302342865		[learning rate: 0.00097287]
	Learning Rate: 0.000972865
	LOSS [training: 2.6600121302342865 | validation: 2.592957028577269]
	TIME [epoch: 8.3 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6692672330012908		[learning rate: 0.00096933]
	Learning Rate: 0.000969335
	LOSS [training: 2.6692672330012908 | validation: 2.58326576466107]
	TIME [epoch: 8.3 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.653014467254408		[learning rate: 0.00096582]
	Learning Rate: 0.000965817
	LOSS [training: 2.653014467254408 | validation: 2.6124243240163603]
	TIME [epoch: 8.33 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6585119185104586		[learning rate: 0.00096231]
	Learning Rate: 0.000962312
	LOSS [training: 2.6585119185104586 | validation: 2.589224806235445]
	TIME [epoch: 8.3 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.660975275295894		[learning rate: 0.00095882]
	Learning Rate: 0.00095882
	LOSS [training: 2.660975275295894 | validation: 2.6387313927122564]
	TIME [epoch: 8.3 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.655959914380501		[learning rate: 0.00095534]
	Learning Rate: 0.00095534
	LOSS [training: 2.655959914380501 | validation: 2.602627570232139]
	TIME [epoch: 8.3 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.648411774062759		[learning rate: 0.00095187]
	Learning Rate: 0.000951873
	LOSS [training: 2.648411774062759 | validation: 2.5972796661685136]
	TIME [epoch: 8.31 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.660288374507855		[learning rate: 0.00094842]
	Learning Rate: 0.000948419
	LOSS [training: 2.660288374507855 | validation: 2.65694997131871]
	TIME [epoch: 8.33 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.651414955301985		[learning rate: 0.00094498]
	Learning Rate: 0.000944977
	LOSS [training: 2.651414955301985 | validation: 2.5939465099909462]
	TIME [epoch: 8.3 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6590571363863456		[learning rate: 0.00094155]
	Learning Rate: 0.000941547
	LOSS [training: 2.6590571363863456 | validation: 2.5955435368734348]
	TIME [epoch: 8.3 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.651321067109072		[learning rate: 0.00093813]
	Learning Rate: 0.00093813
	LOSS [training: 2.651321067109072 | validation: 2.625146567491691]
	TIME [epoch: 8.3 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6671640918438064		[learning rate: 0.00093473]
	Learning Rate: 0.000934726
	LOSS [training: 2.6671640918438064 | validation: 2.609756278349619]
	TIME [epoch: 8.32 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6429773028633248		[learning rate: 0.00093133]
	Learning Rate: 0.000931334
	LOSS [training: 2.6429773028633248 | validation: 2.731422788592857]
	TIME [epoch: 8.3 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.688768631152434		[learning rate: 0.00092795]
	Learning Rate: 0.000927954
	LOSS [training: 2.688768631152434 | validation: 2.580074126785746]
	TIME [epoch: 8.3 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6526182964436993		[learning rate: 0.00092459]
	Learning Rate: 0.000924586
	LOSS [training: 2.6526182964436993 | validation: 2.5770086559185]
	TIME [epoch: 8.3 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6707860015174005		[learning rate: 0.00092123]
	Learning Rate: 0.000921231
	LOSS [training: 2.6707860015174005 | validation: 2.5861239806492757]
	TIME [epoch: 8.32 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.663704423478244		[learning rate: 0.00091789]
	Learning Rate: 0.000917888
	LOSS [training: 2.663704423478244 | validation: 2.6116331905139174]
	TIME [epoch: 8.31 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6489122622489822		[learning rate: 0.00091456]
	Learning Rate: 0.000914556
	LOSS [training: 2.6489122622489822 | validation: 2.603268992127531]
	TIME [epoch: 8.3 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6536634804216077		[learning rate: 0.00091124]
	Learning Rate: 0.000911237
	LOSS [training: 2.6536634804216077 | validation: 2.60289492846058]
	TIME [epoch: 8.3 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.658758555893558		[learning rate: 0.00090793]
	Learning Rate: 0.000907931
	LOSS [training: 2.658758555893558 | validation: 2.599539013968435]
	TIME [epoch: 8.3 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6716577983550143		[learning rate: 0.00090464]
	Learning Rate: 0.000904636
	LOSS [training: 2.6716577983550143 | validation: 2.6247842537755126]
	TIME [epoch: 8.32 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6599213916376714		[learning rate: 0.00090135]
	Learning Rate: 0.000901353
	LOSS [training: 2.6599213916376714 | validation: 2.574489330382426]
	TIME [epoch: 8.3 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.638693340673221		[learning rate: 0.00089808]
	Learning Rate: 0.000898082
	LOSS [training: 2.638693340673221 | validation: 2.5926107535728713]
	TIME [epoch: 8.3 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6323601781407353		[learning rate: 0.00089482]
	Learning Rate: 0.000894822
	LOSS [training: 2.6323601781407353 | validation: 2.6450033335371503]
	TIME [epoch: 8.3 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7070007811183165		[learning rate: 0.00089157]
	Learning Rate: 0.000891575
	LOSS [training: 2.7070007811183165 | validation: 2.692385696464934]
	TIME [epoch: 8.32 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6755138185204026		[learning rate: 0.00088834]
	Learning Rate: 0.000888339
	LOSS [training: 2.6755138185204026 | validation: 2.6473258454697843]
	TIME [epoch: 8.3 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.655674422357162		[learning rate: 0.00088512]
	Learning Rate: 0.000885116
	LOSS [training: 2.655674422357162 | validation: 2.587171317154903]
	TIME [epoch: 8.3 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6482552935211716		[learning rate: 0.0008819]
	Learning Rate: 0.000881903
	LOSS [training: 2.6482552935211716 | validation: 2.5924937756486752]
	TIME [epoch: 8.3 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6610022672918388		[learning rate: 0.0008787]
	Learning Rate: 0.000878703
	LOSS [training: 2.6610022672918388 | validation: 2.578393865081121]
	TIME [epoch: 8.31 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6395827329775003		[learning rate: 0.00087551]
	Learning Rate: 0.000875514
	LOSS [training: 2.6395827329775003 | validation: 2.610838260247204]
	TIME [epoch: 8.31 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6568965098497648		[learning rate: 0.00087234]
	Learning Rate: 0.000872337
	LOSS [training: 2.6568965098497648 | validation: 2.6127052606568637]
	TIME [epoch: 8.29 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6365923201039765		[learning rate: 0.00086917]
	Learning Rate: 0.000869171
	LOSS [training: 2.6365923201039765 | validation: 2.5904900711623435]
	TIME [epoch: 8.3 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.675974101222678		[learning rate: 0.00086602]
	Learning Rate: 0.000866017
	LOSS [training: 2.675974101222678 | validation: 2.571357250184343]
	TIME [epoch: 8.3 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6397287789951753		[learning rate: 0.00086287]
	Learning Rate: 0.000862874
	LOSS [training: 2.6397287789951753 | validation: 2.595827526300271]
	TIME [epoch: 8.33 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6521051799564135		[learning rate: 0.00085974]
	Learning Rate: 0.000859743
	LOSS [training: 2.6521051799564135 | validation: 2.5739070545584273]
	TIME [epoch: 8.29 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.636128815816458		[learning rate: 0.00085662]
	Learning Rate: 0.000856623
	LOSS [training: 2.636128815816458 | validation: 2.5858876700523776]
	TIME [epoch: 8.3 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.644439580072379		[learning rate: 0.00085351]
	Learning Rate: 0.000853514
	LOSS [training: 2.644439580072379 | validation: 2.5944128495490606]
	TIME [epoch: 8.3 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6422775057097363		[learning rate: 0.00085042]
	Learning Rate: 0.000850416
	LOSS [training: 2.6422775057097363 | validation: 2.5813464002837496]
	TIME [epoch: 8.33 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.639597173051988		[learning rate: 0.00084733]
	Learning Rate: 0.00084733
	LOSS [training: 2.639597173051988 | validation: 2.5916711073735454]
	TIME [epoch: 8.3 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6520919651611052		[learning rate: 0.00084426]
	Learning Rate: 0.000844255
	LOSS [training: 2.6520919651611052 | validation: 2.5761717245901856]
	TIME [epoch: 8.3 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6349583843429882		[learning rate: 0.00084119]
	Learning Rate: 0.000841191
	LOSS [training: 2.6349583843429882 | validation: 2.5999652894263825]
	TIME [epoch: 8.3 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6532336614021608		[learning rate: 0.00083814]
	Learning Rate: 0.000838139
	LOSS [training: 2.6532336614021608 | validation: 2.613129790811443]
	TIME [epoch: 8.32 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6607025300384675		[learning rate: 0.0008351]
	Learning Rate: 0.000835097
	LOSS [training: 2.6607025300384675 | validation: 2.6621585483450696]
	TIME [epoch: 8.31 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.652495461412107		[learning rate: 0.00083207]
	Learning Rate: 0.000832066
	LOSS [training: 2.652495461412107 | validation: 2.5979369540298554]
	TIME [epoch: 8.3 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6464957665381417		[learning rate: 0.00082905]
	Learning Rate: 0.000829046
	LOSS [training: 2.6464957665381417 | validation: 2.624571222941339]
	TIME [epoch: 8.3 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6443601052174737		[learning rate: 0.00082604]
	Learning Rate: 0.000826038
	LOSS [training: 2.6443601052174737 | validation: 2.6132987949851074]
	TIME [epoch: 8.3 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6578414917099726		[learning rate: 0.00082304]
	Learning Rate: 0.00082304
	LOSS [training: 2.6578414917099726 | validation: 2.5838095408572963]
	TIME [epoch: 8.33 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6486335586483465		[learning rate: 0.00082005]
	Learning Rate: 0.000820053
	LOSS [training: 2.6486335586483465 | validation: 2.580464645389463]
	TIME [epoch: 8.3 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6348576247860267		[learning rate: 0.00081708]
	Learning Rate: 0.000817077
	LOSS [training: 2.6348576247860267 | validation: 2.593914499604932]
	TIME [epoch: 8.3 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6402258119031634		[learning rate: 0.00081411]
	Learning Rate: 0.000814112
	LOSS [training: 2.6402258119031634 | validation: 2.5922059218099585]
	TIME [epoch: 8.3 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6482047951230268		[learning rate: 0.00081116]
	Learning Rate: 0.000811158
	LOSS [training: 2.6482047951230268 | validation: 2.5744681321885876]
	TIME [epoch: 8.32 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6534781997682817		[learning rate: 0.00080821]
	Learning Rate: 0.000808214
	LOSS [training: 2.6534781997682817 | validation: 2.8409149756424927]
	TIME [epoch: 8.3 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.704160416035115		[learning rate: 0.00080528]
	Learning Rate: 0.000805281
	LOSS [training: 2.704160416035115 | validation: 2.5972290879339948]
	TIME [epoch: 8.3 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6455503963763105		[learning rate: 0.00080236]
	Learning Rate: 0.000802358
	LOSS [training: 2.6455503963763105 | validation: 2.6230298505645857]
	TIME [epoch: 8.3 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6683838534436815		[learning rate: 0.00079945]
	Learning Rate: 0.000799447
	LOSS [training: 2.6683838534436815 | validation: 2.5912534717194564]
	TIME [epoch: 8.32 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6386494939797354		[learning rate: 0.00079655]
	Learning Rate: 0.000796545
	LOSS [training: 2.6386494939797354 | validation: 2.5717922126857253]
	TIME [epoch: 8.31 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6308474866327693		[learning rate: 0.00079365]
	Learning Rate: 0.000793655
	LOSS [training: 2.6308474866327693 | validation: 2.5807027087754797]
	TIME [epoch: 8.3 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6575891404249314		[learning rate: 0.00079077]
	Learning Rate: 0.000790775
	LOSS [training: 2.6575891404249314 | validation: 2.5798747253387893]
	TIME [epoch: 8.3 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6448950982546062		[learning rate: 0.0007879]
	Learning Rate: 0.000787905
	LOSS [training: 2.6448950982546062 | validation: 2.617382555657115]
	TIME [epoch: 8.3 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6306471853315414		[learning rate: 0.00078505]
	Learning Rate: 0.000785045
	LOSS [training: 2.6306471853315414 | validation: 2.5649454782612775]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_800.pth
	Model improved!!!
EPOCH 801/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6431946708387315		[learning rate: 0.0007822]
	Learning Rate: 0.000782196
	LOSS [training: 2.6431946708387315 | validation: 2.6211974034859904]
	TIME [epoch: 8.3 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6478318516969144		[learning rate: 0.00077936]
	Learning Rate: 0.000779358
	LOSS [training: 2.6478318516969144 | validation: 2.575894877488253]
	TIME [epoch: 8.3 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.634505622097016		[learning rate: 0.00077653]
	Learning Rate: 0.000776529
	LOSS [training: 2.634505622097016 | validation: 2.5760924760770907]
	TIME [epoch: 8.3 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6315070746699676		[learning rate: 0.00077371]
	Learning Rate: 0.000773711
	LOSS [training: 2.6315070746699676 | validation: 2.6003981108047727]
	TIME [epoch: 8.32 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.64665552129218		[learning rate: 0.0007709]
	Learning Rate: 0.000770903
	LOSS [training: 2.64665552129218 | validation: 2.590927644727208]
	TIME [epoch: 8.3 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.641854421815043		[learning rate: 0.00076811]
	Learning Rate: 0.000768106
	LOSS [training: 2.641854421815043 | validation: 2.6730114641164255]
	TIME [epoch: 8.3 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.659215734533508		[learning rate: 0.00076532]
	Learning Rate: 0.000765318
	LOSS [training: 2.659215734533508 | validation: 2.615702049639779]
	TIME [epoch: 8.3 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6387964351017374		[learning rate: 0.00076254]
	Learning Rate: 0.000762541
	LOSS [training: 2.6387964351017374 | validation: 2.5879386490822887]
	TIME [epoch: 8.31 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.648578898213841		[learning rate: 0.00075977]
	Learning Rate: 0.000759774
	LOSS [training: 2.648578898213841 | validation: 2.578224578919194]
	TIME [epoch: 8.31 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6324305505084125		[learning rate: 0.00075702]
	Learning Rate: 0.000757016
	LOSS [training: 2.6324305505084125 | validation: 2.6534414467296212]
	TIME [epoch: 8.3 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.659594856958912		[learning rate: 0.00075427]
	Learning Rate: 0.000754269
	LOSS [training: 2.659594856958912 | validation: 2.677856144884111]
	TIME [epoch: 8.3 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.657701900667049		[learning rate: 0.00075153]
	Learning Rate: 0.000751532
	LOSS [training: 2.657701900667049 | validation: 2.710109886903317]
	TIME [epoch: 8.3 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.655966257650248		[learning rate: 0.0007488]
	Learning Rate: 0.000748804
	LOSS [training: 2.655966257650248 | validation: 2.56000921856294]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_813.pth
	Model improved!!!
EPOCH 814/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.641047797426295		[learning rate: 0.00074609]
	Learning Rate: 0.000746087
	LOSS [training: 2.641047797426295 | validation: 2.5785985923095005]
	TIME [epoch: 8.3 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6380322912566267		[learning rate: 0.00074338]
	Learning Rate: 0.000743379
	LOSS [training: 2.6380322912566267 | validation: 2.5805713056132884]
	TIME [epoch: 8.3 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.629966886751712		[learning rate: 0.00074068]
	Learning Rate: 0.000740682
	LOSS [training: 2.629966886751712 | validation: 2.5960371045244717]
	TIME [epoch: 8.3 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.638176816455485		[learning rate: 0.00073799]
	Learning Rate: 0.000737994
	LOSS [training: 2.638176816455485 | validation: 2.572132844352499]
	TIME [epoch: 8.32 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6500657586883554		[learning rate: 0.00073532]
	Learning Rate: 0.000735315
	LOSS [training: 2.6500657586883554 | validation: 2.5674076900613967]
	TIME [epoch: 8.3 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.638035399449037		[learning rate: 0.00073265]
	Learning Rate: 0.000732647
	LOSS [training: 2.638035399449037 | validation: 2.6073309992325573]
	TIME [epoch: 8.3 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.657473783372535		[learning rate: 0.00072999]
	Learning Rate: 0.000729988
	LOSS [training: 2.657473783372535 | validation: 2.5880444544614116]
	TIME [epoch: 8.3 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6372808375363617		[learning rate: 0.00072734]
	Learning Rate: 0.000727339
	LOSS [training: 2.6372808375363617 | validation: 2.5659094076818914]
	TIME [epoch: 8.31 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.636263191117227		[learning rate: 0.0007247]
	Learning Rate: 0.000724699
	LOSS [training: 2.636263191117227 | validation: 2.6791216747566002]
	TIME [epoch: 8.31 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.663382917210262		[learning rate: 0.00072207]
	Learning Rate: 0.000722069
	LOSS [training: 2.663382917210262 | validation: 2.5751241903975295]
	TIME [epoch: 8.3 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6446023648977524		[learning rate: 0.00071945]
	Learning Rate: 0.000719449
	LOSS [training: 2.6446023648977524 | validation: 2.6167621020395933]
	TIME [epoch: 8.3 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.641685222630549		[learning rate: 0.00071684]
	Learning Rate: 0.000716838
	LOSS [training: 2.641685222630549 | validation: 2.566768482560107]
	TIME [epoch: 8.3 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.634712949721523		[learning rate: 0.00071424]
	Learning Rate: 0.000714237
	LOSS [training: 2.634712949721523 | validation: 2.614129557462408]
	TIME [epoch: 8.32 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.643666245747883		[learning rate: 0.00071164]
	Learning Rate: 0.000711645
	LOSS [training: 2.643666245747883 | validation: 2.702899857111354]
	TIME [epoch: 8.3 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6526148610019002		[learning rate: 0.00070906]
	Learning Rate: 0.000709062
	LOSS [training: 2.6526148610019002 | validation: 2.60220127973817]
	TIME [epoch: 8.3 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.648921925479553		[learning rate: 0.00070649]
	Learning Rate: 0.000706489
	LOSS [training: 2.648921925479553 | validation: 2.6006172151815177]
	TIME [epoch: 8.3 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.629611179163372		[learning rate: 0.00070392]
	Learning Rate: 0.000703925
	LOSS [training: 2.629611179163372 | validation: 2.5738239584627225]
	TIME [epoch: 8.32 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.642560433777385		[learning rate: 0.00070137]
	Learning Rate: 0.00070137
	LOSS [training: 2.642560433777385 | validation: 2.5962175695656105]
	TIME [epoch: 8.3 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6429732282922975		[learning rate: 0.00069883]
	Learning Rate: 0.000698825
	LOSS [training: 2.6429732282922975 | validation: 2.5573411599260343]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_832.pth
	Model improved!!!
EPOCH 833/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6366170855642173		[learning rate: 0.00069629]
	Learning Rate: 0.000696289
	LOSS [training: 2.6366170855642173 | validation: 2.5839029525265884]
	TIME [epoch: 8.3 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6405192371841175		[learning rate: 0.00069376]
	Learning Rate: 0.000693762
	LOSS [training: 2.6405192371841175 | validation: 2.578718053431198]
	TIME [epoch: 8.31 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.634056831600554		[learning rate: 0.00069124]
	Learning Rate: 0.000691244
	LOSS [training: 2.634056831600554 | validation: 2.5596344152171855]
	TIME [epoch: 8.31 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.645859515923578		[learning rate: 0.00068874]
	Learning Rate: 0.000688736
	LOSS [training: 2.645859515923578 | validation: 2.6289698539796156]
	TIME [epoch: 8.29 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6284647002940416		[learning rate: 0.00068624]
	Learning Rate: 0.000686236
	LOSS [training: 2.6284647002940416 | validation: 2.5595367591450744]
	TIME [epoch: 8.29 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.635708537917046		[learning rate: 0.00068375]
	Learning Rate: 0.000683746
	LOSS [training: 2.635708537917046 | validation: 2.643163025906955]
	TIME [epoch: 8.3 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.658539417346052		[learning rate: 0.00068126]
	Learning Rate: 0.000681265
	LOSS [training: 2.658539417346052 | validation: 2.5708426790321073]
	TIME [epoch: 8.32 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.631259983553458		[learning rate: 0.00067879]
	Learning Rate: 0.000678792
	LOSS [training: 2.631259983553458 | validation: 2.6036642429053947]
	TIME [epoch: 8.3 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6332884969256165		[learning rate: 0.00067633]
	Learning Rate: 0.000676329
	LOSS [training: 2.6332884969256165 | validation: 2.5759570249580186]
	TIME [epoch: 8.3 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6485226436982114		[learning rate: 0.00067387]
	Learning Rate: 0.000673874
	LOSS [training: 2.6485226436982114 | validation: 2.582706748407011]
	TIME [epoch: 8.29 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6486810603317226		[learning rate: 0.00067143]
	Learning Rate: 0.000671429
	LOSS [training: 2.6486810603317226 | validation: 2.5613455043279267]
	TIME [epoch: 8.32 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6323965630172763		[learning rate: 0.00066899]
	Learning Rate: 0.000668992
	LOSS [training: 2.6323965630172763 | validation: 2.569939735049779]
	TIME [epoch: 8.3 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.637553962771413		[learning rate: 0.00066656]
	Learning Rate: 0.000666564
	LOSS [training: 2.637553962771413 | validation: 2.619865132659342]
	TIME [epoch: 8.29 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.661726949306198		[learning rate: 0.00066415]
	Learning Rate: 0.000664145
	LOSS [training: 2.661726949306198 | validation: 2.5632598113124554]
	TIME [epoch: 8.29 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6275087643140678		[learning rate: 0.00066174]
	Learning Rate: 0.000661735
	LOSS [training: 2.6275087643140678 | validation: 2.570881813081421]
	TIME [epoch: 8.31 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.638922670540141		[learning rate: 0.00065933]
	Learning Rate: 0.000659334
	LOSS [training: 2.638922670540141 | validation: 2.582208723478457]
	TIME [epoch: 8.32 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.635403138512232		[learning rate: 0.00065694]
	Learning Rate: 0.000656941
	LOSS [training: 2.635403138512232 | validation: 2.5790956533637663]
	TIME [epoch: 8.3 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6375028059527272		[learning rate: 0.00065456]
	Learning Rate: 0.000654557
	LOSS [training: 2.6375028059527272 | validation: 2.6021007372275737]
	TIME [epoch: 8.3 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6487496573687026		[learning rate: 0.00065218]
	Learning Rate: 0.000652182
	LOSS [training: 2.6487496573687026 | validation: 2.598422782814075]
	TIME [epoch: 8.3 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.631245031500362		[learning rate: 0.00064981]
	Learning Rate: 0.000649815
	LOSS [training: 2.631245031500362 | validation: 2.5675611585518388]
	TIME [epoch: 8.32 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.638434050210487		[learning rate: 0.00064746]
	Learning Rate: 0.000647456
	LOSS [training: 2.638434050210487 | validation: 2.5676018355405787]
	TIME [epoch: 8.3 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6474926998624815		[learning rate: 0.00064511]
	Learning Rate: 0.000645107
	LOSS [training: 2.6474926998624815 | validation: 2.5748867301122518]
	TIME [epoch: 8.3 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6329890378953036		[learning rate: 0.00064277]
	Learning Rate: 0.000642766
	LOSS [training: 2.6329890378953036 | validation: 2.592167217614069]
	TIME [epoch: 8.3 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6349137822651807		[learning rate: 0.00064043]
	Learning Rate: 0.000640433
	LOSS [training: 2.6349137822651807 | validation: 2.617133327150822]
	TIME [epoch: 8.32 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.642556282430427		[learning rate: 0.00063811]
	Learning Rate: 0.000638109
	LOSS [training: 2.642556282430427 | validation: 2.592320190171872]
	TIME [epoch: 8.3 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.641039321970777		[learning rate: 0.00063579]
	Learning Rate: 0.000635793
	LOSS [training: 2.641039321970777 | validation: 2.570142103620277]
	TIME [epoch: 8.3 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6408528781450187		[learning rate: 0.00063349]
	Learning Rate: 0.000633486
	LOSS [training: 2.6408528781450187 | validation: 2.593676655917196]
	TIME [epoch: 8.3 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6317066001065124		[learning rate: 0.00063119]
	Learning Rate: 0.000631187
	LOSS [training: 2.6317066001065124 | validation: 2.5642619271013825]
	TIME [epoch: 8.3 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6278205697144297		[learning rate: 0.0006289]
	Learning Rate: 0.000628896
	LOSS [training: 2.6278205697144297 | validation: 2.587350169214721]
	TIME [epoch: 8.31 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.633179261798957		[learning rate: 0.00062661]
	Learning Rate: 0.000626614
	LOSS [training: 2.633179261798957 | validation: 2.5779803278668787]
	TIME [epoch: 8.3 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6463462152138844		[learning rate: 0.00062434]
	Learning Rate: 0.00062434
	LOSS [training: 2.6463462152138844 | validation: 2.569923677867167]
	TIME [epoch: 8.3 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.623803253824921		[learning rate: 0.00062207]
	Learning Rate: 0.000622074
	LOSS [training: 2.623803253824921 | validation: 2.597116153867823]
	TIME [epoch: 8.3 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.643885882227015		[learning rate: 0.00061982]
	Learning Rate: 0.000619816
	LOSS [training: 2.643885882227015 | validation: 2.6002767341799884]
	TIME [epoch: 8.32 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6465127507037782		[learning rate: 0.00061757]
	Learning Rate: 0.000617567
	LOSS [training: 2.6465127507037782 | validation: 2.574279979497663]
	TIME [epoch: 8.3 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.633267412173617		[learning rate: 0.00061533]
	Learning Rate: 0.000615326
	LOSS [training: 2.633267412173617 | validation: 2.5670155216154695]
	TIME [epoch: 8.3 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6381762517238183		[learning rate: 0.00061309]
	Learning Rate: 0.000613093
	LOSS [training: 2.6381762517238183 | validation: 2.573682180192151]
	TIME [epoch: 8.29 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.630349031541793		[learning rate: 0.00061087]
	Learning Rate: 0.000610868
	LOSS [training: 2.630349031541793 | validation: 2.563974837300023]
	TIME [epoch: 8.31 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6305147643112092		[learning rate: 0.00060865]
	Learning Rate: 0.000608651
	LOSS [training: 2.6305147643112092 | validation: 2.5605489811520004]
	TIME [epoch: 8.3 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.627756237582674		[learning rate: 0.00060644]
	Learning Rate: 0.000606442
	LOSS [training: 2.627756237582674 | validation: 2.570912274421952]
	TIME [epoch: 8.3 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.634360944485484		[learning rate: 0.00060424]
	Learning Rate: 0.000604242
	LOSS [training: 2.634360944485484 | validation: 2.586623296683454]
	TIME [epoch: 8.29 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6325809920801433		[learning rate: 0.00060205]
	Learning Rate: 0.000602049
	LOSS [training: 2.6325809920801433 | validation: 2.570248322602562]
	TIME [epoch: 8.29 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.621613954785855		[learning rate: 0.00059986]
	Learning Rate: 0.000599864
	LOSS [training: 2.621613954785855 | validation: 2.5692313721835283]
	TIME [epoch: 8.32 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6308310941851216		[learning rate: 0.00059769]
	Learning Rate: 0.000597687
	LOSS [training: 2.6308310941851216 | validation: 2.5921362107640222]
	TIME [epoch: 8.29 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6435175918637728		[learning rate: 0.00059552]
	Learning Rate: 0.000595518
	LOSS [training: 2.6435175918637728 | validation: 2.572164084471965]
	TIME [epoch: 8.29 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6322898916155455		[learning rate: 0.00059336]
	Learning Rate: 0.000593357
	LOSS [training: 2.6322898916155455 | validation: 2.6308085211187997]
	TIME [epoch: 8.29 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.630926657464753		[learning rate: 0.0005912]
	Learning Rate: 0.000591203
	LOSS [training: 2.630926657464753 | validation: 2.5853812741410565]
	TIME [epoch: 8.32 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6497426278085876		[learning rate: 0.00058906]
	Learning Rate: 0.000589058
	LOSS [training: 2.6497426278085876 | validation: 2.586649194993999]
	TIME [epoch: 8.3 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.628712425305564		[learning rate: 0.00058692]
	Learning Rate: 0.00058692
	LOSS [training: 2.628712425305564 | validation: 2.583603304567428]
	TIME [epoch: 8.29 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6309098564924307		[learning rate: 0.00058479]
	Learning Rate: 0.00058479
	LOSS [training: 2.6309098564924307 | validation: 2.593011496664153]
	TIME [epoch: 8.29 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6342075588761147		[learning rate: 0.00058267]
	Learning Rate: 0.000582668
	LOSS [training: 2.6342075588761147 | validation: 2.585133842767321]
	TIME [epoch: 8.31 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.630239611475433		[learning rate: 0.00058055]
	Learning Rate: 0.000580553
	LOSS [training: 2.630239611475433 | validation: 2.584866470953455]
	TIME [epoch: 8.3 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.618360427899116		[learning rate: 0.00057845]
	Learning Rate: 0.000578446
	LOSS [training: 2.618360427899116 | validation: 2.565145504474774]
	TIME [epoch: 8.3 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6370742695875897		[learning rate: 0.00057635]
	Learning Rate: 0.000576347
	LOSS [training: 2.6370742695875897 | validation: 2.593186508067846]
	TIME [epoch: 8.3 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.635249494848522		[learning rate: 0.00057426]
	Learning Rate: 0.000574256
	LOSS [training: 2.635249494848522 | validation: 2.5858517143475197]
	TIME [epoch: 8.3 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.645628270398804		[learning rate: 0.00057217]
	Learning Rate: 0.000572172
	LOSS [training: 2.645628270398804 | validation: 2.5926576302650393]
	TIME [epoch: 8.32 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.634931893931096		[learning rate: 0.0005701]
	Learning Rate: 0.000570095
	LOSS [training: 2.634931893931096 | validation: 2.61544747072794]
	TIME [epoch: 8.29 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.682234041962566		[learning rate: 0.00056803]
	Learning Rate: 0.000568026
	LOSS [training: 2.682234041962566 | validation: 2.5777986375973247]
	TIME [epoch: 8.3 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.634061717445914		[learning rate: 0.00056596]
	Learning Rate: 0.000565965
	LOSS [training: 2.634061717445914 | validation: 2.5671238147759032]
	TIME [epoch: 8.29 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6293475747876207		[learning rate: 0.00056391]
	Learning Rate: 0.000563911
	LOSS [training: 2.6293475747876207 | validation: 2.566690067401379]
	TIME [epoch: 8.32 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6568215980064354		[learning rate: 0.00056186]
	Learning Rate: 0.000561864
	LOSS [training: 2.6568215980064354 | validation: 2.582704472201165]
	TIME [epoch: 8.3 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6376275588349056		[learning rate: 0.00055983]
	Learning Rate: 0.000559825
	LOSS [training: 2.6376275588349056 | validation: 2.5698753263461827]
	TIME [epoch: 8.3 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6263082263418887		[learning rate: 0.00055779]
	Learning Rate: 0.000557794
	LOSS [training: 2.6263082263418887 | validation: 2.5806729483572495]
	TIME [epoch: 8.29 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6374834124165085		[learning rate: 0.00055577]
	Learning Rate: 0.00055577
	LOSS [training: 2.6374834124165085 | validation: 2.5751004756937554]
	TIME [epoch: 8.31 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.630507391102021		[learning rate: 0.00055375]
	Learning Rate: 0.000553753
	LOSS [training: 2.630507391102021 | validation: 2.5673635724067374]
	TIME [epoch: 8.3 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.637905788876735		[learning rate: 0.00055174]
	Learning Rate: 0.000551743
	LOSS [training: 2.637905788876735 | validation: 2.613140786412185]
	TIME [epoch: 8.29 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.638383190538895		[learning rate: 0.00054974]
	Learning Rate: 0.000549741
	LOSS [training: 2.638383190538895 | validation: 2.6089769969598278]
	TIME [epoch: 8.29 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.629880218353424		[learning rate: 0.00054775]
	Learning Rate: 0.000547746
	LOSS [training: 2.629880218353424 | validation: 2.5746918276344877]
	TIME [epoch: 8.29 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.627329553744283		[learning rate: 0.00054576]
	Learning Rate: 0.000545758
	LOSS [training: 2.627329553744283 | validation: 2.580386972955905]
	TIME [epoch: 8.32 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.641244619854038		[learning rate: 0.00054378]
	Learning Rate: 0.000543777
	LOSS [training: 2.641244619854038 | validation: 2.569455113517317]
	TIME [epoch: 8.29 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.633145504484406		[learning rate: 0.0005418]
	Learning Rate: 0.000541804
	LOSS [training: 2.633145504484406 | validation: 2.570164437649315]
	TIME [epoch: 8.29 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6264436704535212		[learning rate: 0.00053984]
	Learning Rate: 0.000539838
	LOSS [training: 2.6264436704535212 | validation: 2.584317630426245]
	TIME [epoch: 8.29 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6208268077404364		[learning rate: 0.00053788]
	Learning Rate: 0.000537879
	LOSS [training: 2.6208268077404364 | validation: 2.576657557967434]
	TIME [epoch: 8.32 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6325524392269424		[learning rate: 0.00053593]
	Learning Rate: 0.000535926
	LOSS [training: 2.6325524392269424 | validation: 2.56666248530248]
	TIME [epoch: 8.3 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6397001200076193		[learning rate: 0.00053398]
	Learning Rate: 0.000533982
	LOSS [training: 2.6397001200076193 | validation: 2.555005633025223]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_906.pth
	Model improved!!!
EPOCH 907/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6268775952437546		[learning rate: 0.00053204]
	Learning Rate: 0.000532044
	LOSS [training: 2.6268775952437546 | validation: 2.5627272134565064]
	TIME [epoch: 8.3 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6226172987391303		[learning rate: 0.00053011]
	Learning Rate: 0.000530113
	LOSS [training: 2.6226172987391303 | validation: 2.573690907779415]
	TIME [epoch: 8.31 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.623220520392058		[learning rate: 0.00052819]
	Learning Rate: 0.000528189
	LOSS [training: 2.623220520392058 | validation: 2.588298917491652]
	TIME [epoch: 8.31 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.63301844045611		[learning rate: 0.00052627]
	Learning Rate: 0.000526272
	LOSS [training: 2.63301844045611 | validation: 2.5867421382884395]
	TIME [epoch: 8.29 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.631892138394336		[learning rate: 0.00052436]
	Learning Rate: 0.000524362
	LOSS [training: 2.631892138394336 | validation: 2.5767336706083634]
	TIME [epoch: 8.29 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.629813398987262		[learning rate: 0.00052246]
	Learning Rate: 0.00052246
	LOSS [training: 2.629813398987262 | validation: 2.5667962861585547]
	TIME [epoch: 8.3 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6242651262666734		[learning rate: 0.00052056]
	Learning Rate: 0.000520563
	LOSS [training: 2.6242651262666734 | validation: 2.5838061641400336]
	TIME [epoch: 8.32 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6293129127364105		[learning rate: 0.00051867]
	Learning Rate: 0.000518674
	LOSS [training: 2.6293129127364105 | validation: 2.5719112998813944]
	TIME [epoch: 8.29 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.624758818608542		[learning rate: 0.00051679]
	Learning Rate: 0.000516792
	LOSS [training: 2.624758818608542 | validation: 2.569328110825376]
	TIME [epoch: 8.29 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.626396914202681		[learning rate: 0.00051492]
	Learning Rate: 0.000514917
	LOSS [training: 2.626396914202681 | validation: 2.588847982032015]
	TIME [epoch: 8.3 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.632940625955711		[learning rate: 0.00051305]
	Learning Rate: 0.000513048
	LOSS [training: 2.632940625955711 | validation: 2.5796739008108105]
	TIME [epoch: 8.32 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.627957443303744		[learning rate: 0.00051119]
	Learning Rate: 0.000511186
	LOSS [training: 2.627957443303744 | validation: 2.57639330790978]
	TIME [epoch: 8.3 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6334954216087048		[learning rate: 0.00050933]
	Learning Rate: 0.000509331
	LOSS [training: 2.6334954216087048 | validation: 2.6052989760477954]
	TIME [epoch: 8.3 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.644448907207529		[learning rate: 0.00050748]
	Learning Rate: 0.000507483
	LOSS [training: 2.644448907207529 | validation: 2.594044199733193]
	TIME [epoch: 8.29 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.629902717366252		[learning rate: 0.00050564]
	Learning Rate: 0.000505641
	LOSS [training: 2.629902717366252 | validation: 2.5742757525078996]
	TIME [epoch: 8.31 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6367180072706975		[learning rate: 0.00050381]
	Learning Rate: 0.000503806
	LOSS [training: 2.6367180072706975 | validation: 2.604038553937877]
	TIME [epoch: 8.3 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6317500329772976		[learning rate: 0.00050198]
	Learning Rate: 0.000501977
	LOSS [training: 2.6317500329772976 | validation: 2.58168277295061]
	TIME [epoch: 8.33 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.634256032317851		[learning rate: 0.00050016]
	Learning Rate: 0.000500156
	LOSS [training: 2.634256032317851 | validation: 2.5717468354879287]
	TIME [epoch: 8.29 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6444675366076877		[learning rate: 0.00049834]
	Learning Rate: 0.000498341
	LOSS [training: 2.6444675366076877 | validation: 2.59818221674106]
	TIME [epoch: 8.3 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6425677326590042		[learning rate: 0.00049653]
	Learning Rate: 0.000496532
	LOSS [training: 2.6425677326590042 | validation: 2.588844177115341]
	TIME [epoch: 8.32 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.629970940296226		[learning rate: 0.00049473]
	Learning Rate: 0.00049473
	LOSS [training: 2.629970940296226 | validation: 2.6026460204303055]
	TIME [epoch: 8.3 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6245358525230706		[learning rate: 0.00049293]
	Learning Rate: 0.000492935
	LOSS [training: 2.6245358525230706 | validation: 2.6210185376335997]
	TIME [epoch: 8.29 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6236445655649314		[learning rate: 0.00049115]
	Learning Rate: 0.000491146
	LOSS [training: 2.6236445655649314 | validation: 2.567694610504211]
	TIME [epoch: 8.29 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6267123025392216		[learning rate: 0.00048936]
	Learning Rate: 0.000489364
	LOSS [training: 2.6267123025392216 | validation: 2.575672291041981]
	TIME [epoch: 8.31 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6248654490807932		[learning rate: 0.00048759]
	Learning Rate: 0.000487588
	LOSS [training: 2.6248654490807932 | validation: 2.5647475450883284]
	TIME [epoch: 8.29 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6284954976020787		[learning rate: 0.00048582]
	Learning Rate: 0.000485818
	LOSS [training: 2.6284954976020787 | validation: 2.5770584312742906]
	TIME [epoch: 8.29 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6258610013199624		[learning rate: 0.00048406]
	Learning Rate: 0.000484055
	LOSS [training: 2.6258610013199624 | validation: 2.565632198803712]
	TIME [epoch: 8.29 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6208913376731386		[learning rate: 0.0004823]
	Learning Rate: 0.000482298
	LOSS [training: 2.6208913376731386 | validation: 2.5869496727008423]
	TIME [epoch: 8.3 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.630424473676523		[learning rate: 0.00048055]
	Learning Rate: 0.000480548
	LOSS [training: 2.630424473676523 | validation: 2.5689388533816495]
	TIME [epoch: 8.31 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.622028510208592		[learning rate: 0.0004788]
	Learning Rate: 0.000478804
	LOSS [training: 2.622028510208592 | validation: 2.6131558136113417]
	TIME [epoch: 8.29 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6409475619261307		[learning rate: 0.00047707]
	Learning Rate: 0.000477067
	LOSS [training: 2.6409475619261307 | validation: 2.581764501033317]
	TIME [epoch: 8.3 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6345005462272324		[learning rate: 0.00047534]
	Learning Rate: 0.000475335
	LOSS [training: 2.6345005462272324 | validation: 2.58125581921332]
	TIME [epoch: 8.3 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6213601301258764		[learning rate: 0.00047361]
	Learning Rate: 0.00047361
	LOSS [training: 2.6213601301258764 | validation: 2.5720994459306987]
	TIME [epoch: 8.32 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.630053573786798		[learning rate: 0.00047189]
	Learning Rate: 0.000471891
	LOSS [training: 2.630053573786798 | validation: 2.5684931324671405]
	TIME [epoch: 8.29 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6263097225437133		[learning rate: 0.00047018]
	Learning Rate: 0.000470179
	LOSS [training: 2.6263097225437133 | validation: 2.595675851410266]
	TIME [epoch: 8.29 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.626559535051155		[learning rate: 0.00046847]
	Learning Rate: 0.000468473
	LOSS [training: 2.626559535051155 | validation: 2.5678588213772313]
	TIME [epoch: 8.29 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.628580198064814		[learning rate: 0.00046677]
	Learning Rate: 0.000466773
	LOSS [training: 2.628580198064814 | validation: 2.57296518832422]
	TIME [epoch: 8.32 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6326862384180347		[learning rate: 0.00046508]
	Learning Rate: 0.000465079
	LOSS [training: 2.6326862384180347 | validation: 2.5830188445158626]
	TIME [epoch: 8.3 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6230122097394535		[learning rate: 0.00046339]
	Learning Rate: 0.000463391
	LOSS [training: 2.6230122097394535 | validation: 2.5635029544035612]
	TIME [epoch: 8.3 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6317845727270672		[learning rate: 0.00046171]
	Learning Rate: 0.000461709
	LOSS [training: 2.6317845727270672 | validation: 2.5943760361813646]
	TIME [epoch: 8.3 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6205774673089044		[learning rate: 0.00046003]
	Learning Rate: 0.000460034
	LOSS [training: 2.6205774673089044 | validation: 2.587427784492574]
	TIME [epoch: 8.3 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6244356231630386		[learning rate: 0.00045836]
	Learning Rate: 0.000458364
	LOSS [training: 2.6244356231630386 | validation: 2.5760402389906942]
	TIME [epoch: 8.32 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.621644833616385		[learning rate: 0.0004567]
	Learning Rate: 0.000456701
	LOSS [training: 2.621644833616385 | validation: 2.5589365644326643]
	TIME [epoch: 8.3 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6195942964930325		[learning rate: 0.00045504]
	Learning Rate: 0.000455043
	LOSS [training: 2.6195942964930325 | validation: 2.5591622392300484]
	TIME [epoch: 8.29 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6303289738504994		[learning rate: 0.00045339]
	Learning Rate: 0.000453392
	LOSS [training: 2.6303289738504994 | validation: 2.5635605407000206]
	TIME [epoch: 8.3 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.626579213721299		[learning rate: 0.00045175]
	Learning Rate: 0.000451746
	LOSS [training: 2.626579213721299 | validation: 2.576268540036165]
	TIME [epoch: 8.32 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.643383632534616		[learning rate: 0.00045011]
	Learning Rate: 0.000450107
	LOSS [training: 2.643383632534616 | validation: 2.572891588248116]
	TIME [epoch: 8.3 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.629328752789676		[learning rate: 0.00044847]
	Learning Rate: 0.000448474
	LOSS [training: 2.629328752789676 | validation: 2.608850931700858]
	TIME [epoch: 8.3 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.627863880899449		[learning rate: 0.00044685]
	Learning Rate: 0.000446846
	LOSS [training: 2.627863880899449 | validation: 2.574534506222191]
	TIME [epoch: 8.29 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.625953871249149		[learning rate: 0.00044522]
	Learning Rate: 0.000445224
	LOSS [training: 2.625953871249149 | validation: 2.5771506567339477]
	TIME [epoch: 8.31 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6356955540235356		[learning rate: 0.00044361]
	Learning Rate: 0.000443609
	LOSS [training: 2.6356955540235356 | validation: 2.586963866797187]
	TIME [epoch: 8.3 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.639329025908915		[learning rate: 0.000442]
	Learning Rate: 0.000441999
	LOSS [training: 2.639329025908915 | validation: 2.5691950681721494]
	TIME [epoch: 8.29 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6316461377903337		[learning rate: 0.00044039]
	Learning Rate: 0.000440395
	LOSS [training: 2.6316461377903337 | validation: 2.557930923169183]
	TIME [epoch: 8.3 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.623838518398503		[learning rate: 0.0004388]
	Learning Rate: 0.000438797
	LOSS [training: 2.623838518398503 | validation: 2.5689488065040895]
	TIME [epoch: 8.3 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6252491774134588		[learning rate: 0.0004372]
	Learning Rate: 0.000437204
	LOSS [training: 2.6252491774134588 | validation: 2.58100411240957]
	TIME [epoch: 8.32 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.62681240498049		[learning rate: 0.00043562]
	Learning Rate: 0.000435617
	LOSS [training: 2.62681240498049 | validation: 2.6057740472841893]
	TIME [epoch: 8.29 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6278661480446415		[learning rate: 0.00043404]
	Learning Rate: 0.000434037
	LOSS [training: 2.6278661480446415 | validation: 2.571386055344728]
	TIME [epoch: 8.3 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6141633965557456		[learning rate: 0.00043246]
	Learning Rate: 0.000432461
	LOSS [training: 2.6141633965557456 | validation: 2.556954949023522]
	TIME [epoch: 8.3 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6308504844715186		[learning rate: 0.00043089]
	Learning Rate: 0.000430892
	LOSS [training: 2.6308504844715186 | validation: 2.597112280268262]
	TIME [epoch: 8.32 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.624262435974501		[learning rate: 0.00042933]
	Learning Rate: 0.000429328
	LOSS [training: 2.624262435974501 | validation: 2.570873893301421]
	TIME [epoch: 8.3 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6211340122750926		[learning rate: 0.00042777]
	Learning Rate: 0.00042777
	LOSS [training: 2.6211340122750926 | validation: 2.571112721321299]
	TIME [epoch: 8.29 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.621959754438728		[learning rate: 0.00042622]
	Learning Rate: 0.000426218
	LOSS [training: 2.621959754438728 | validation: 2.575316607325279]
	TIME [epoch: 8.3 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6267932114564596		[learning rate: 0.00042467]
	Learning Rate: 0.000424671
	LOSS [training: 2.6267932114564596 | validation: 2.587291328000143]
	TIME [epoch: 8.31 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6322196963714086		[learning rate: 0.00042313]
	Learning Rate: 0.00042313
	LOSS [training: 2.6322196963714086 | validation: 2.5804263091039883]
	TIME [epoch: 8.31 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.624575846157758		[learning rate: 0.00042159]
	Learning Rate: 0.000421594
	LOSS [training: 2.624575846157758 | validation: 2.572363249242798]
	TIME [epoch: 8.3 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6231059026954955		[learning rate: 0.00042006]
	Learning Rate: 0.000420064
	LOSS [training: 2.6231059026954955 | validation: 2.567290902668716]
	TIME [epoch: 8.3 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6202582641963184		[learning rate: 0.00041854]
	Learning Rate: 0.00041854
	LOSS [training: 2.6202582641963184 | validation: 2.57169677720701]
	TIME [epoch: 8.3 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.620618545324162		[learning rate: 0.00041702]
	Learning Rate: 0.000417021
	LOSS [training: 2.620618545324162 | validation: 2.5722425842672045]
	TIME [epoch: 8.32 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.619541749283396		[learning rate: 0.00041551]
	Learning Rate: 0.000415508
	LOSS [training: 2.619541749283396 | validation: 2.5713507440960344]
	TIME [epoch: 8.3 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6310777801906617		[learning rate: 0.000414]
	Learning Rate: 0.000414
	LOSS [training: 2.6310777801906617 | validation: 2.574232046558407]
	TIME [epoch: 8.3 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.627043663412062		[learning rate: 0.0004125]
	Learning Rate: 0.000412497
	LOSS [training: 2.627043663412062 | validation: 2.5796739096808476]
	TIME [epoch: 8.29 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6221101932050095		[learning rate: 0.000411]
	Learning Rate: 0.000411
	LOSS [training: 2.6221101932050095 | validation: 2.572683133152224]
	TIME [epoch: 8.32 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6399201899758302		[learning rate: 0.00040951]
	Learning Rate: 0.000409509
	LOSS [training: 2.6399201899758302 | validation: 2.5814930942322656]
	TIME [epoch: 8.3 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.625420935929206		[learning rate: 0.00040802]
	Learning Rate: 0.000408023
	LOSS [training: 2.625420935929206 | validation: 2.5811350820577608]
	TIME [epoch: 8.29 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6260340878310817		[learning rate: 0.00040654]
	Learning Rate: 0.000406542
	LOSS [training: 2.6260340878310817 | validation: 2.5777216595326387]
	TIME [epoch: 8.3 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6210498552929704		[learning rate: 0.00040507]
	Learning Rate: 0.000405067
	LOSS [training: 2.6210498552929704 | validation: 2.581483308474584]
	TIME [epoch: 8.31 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6156870212557495		[learning rate: 0.0004036]
	Learning Rate: 0.000403597
	LOSS [training: 2.6156870212557495 | validation: 2.5697539696805896]
	TIME [epoch: 8.3 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.626324115772948		[learning rate: 0.00040213]
	Learning Rate: 0.000402132
	LOSS [training: 2.626324115772948 | validation: 2.5907779371515107]
	TIME [epoch: 8.29 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.619742155852977		[learning rate: 0.00040067]
	Learning Rate: 0.000400672
	LOSS [training: 2.619742155852977 | validation: 2.5522030477277147]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_985.pth
	Model improved!!!
EPOCH 986/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6193356301306645		[learning rate: 0.00039922]
	Learning Rate: 0.000399218
	LOSS [training: 2.6193356301306645 | validation: 2.568394042893013]
	TIME [epoch: 8.3 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6292265123313756		[learning rate: 0.00039777]
	Learning Rate: 0.00039777
	LOSS [training: 2.6292265123313756 | validation: 2.602015907125067]
	TIME [epoch: 8.32 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6231557047023526		[learning rate: 0.00039633]
	Learning Rate: 0.000396326
	LOSS [training: 2.6231557047023526 | validation: 2.5773636371966857]
	TIME [epoch: 8.29 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6173052780093125		[learning rate: 0.00039489]
	Learning Rate: 0.000394888
	LOSS [training: 2.6173052780093125 | validation: 2.559817922578805]
	TIME [epoch: 8.3 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6169792490289714		[learning rate: 0.00039345]
	Learning Rate: 0.000393455
	LOSS [training: 2.6169792490289714 | validation: 2.5718339399368553]
	TIME [epoch: 8.3 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6326633840697307		[learning rate: 0.00039203]
	Learning Rate: 0.000392027
	LOSS [training: 2.6326633840697307 | validation: 2.564290367994419]
	TIME [epoch: 8.32 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6171286623264884		[learning rate: 0.0003906]
	Learning Rate: 0.000390604
	LOSS [training: 2.6171286623264884 | validation: 2.570009753587536]
	TIME [epoch: 8.3 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.621640526588627		[learning rate: 0.00038919]
	Learning Rate: 0.000389187
	LOSS [training: 2.621640526588627 | validation: 2.5762767428121087]
	TIME [epoch: 8.3 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.618265868435017		[learning rate: 0.00038777]
	Learning Rate: 0.000387774
	LOSS [training: 2.618265868435017 | validation: 2.575347115544573]
	TIME [epoch: 8.3 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6216102505994106		[learning rate: 0.00038637]
	Learning Rate: 0.000386367
	LOSS [training: 2.6216102505994106 | validation: 2.565780334700305]
	TIME [epoch: 8.31 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.608529523561333		[learning rate: 0.00038496]
	Learning Rate: 0.000384965
	LOSS [training: 2.608529523561333 | validation: 2.556757146269858]
	TIME [epoch: 8.3 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6292783584068538		[learning rate: 0.00038357]
	Learning Rate: 0.000383568
	LOSS [training: 2.6292783584068538 | validation: 2.56838505143735]
	TIME [epoch: 8.29 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6225649306001175		[learning rate: 0.00038218]
	Learning Rate: 0.000382176
	LOSS [training: 2.6225649306001175 | validation: 2.556764572753613]
	TIME [epoch: 8.29 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.62865011967813		[learning rate: 0.00038079]
	Learning Rate: 0.000380789
	LOSS [training: 2.62865011967813 | validation: 2.5866611312928476]
	TIME [epoch: 8.29 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6283641501963215		[learning rate: 0.00037941]
	Learning Rate: 0.000379407
	LOSS [training: 2.6283641501963215 | validation: 2.602712767016926]
	TIME [epoch: 8.32 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6644436170972607		[learning rate: 0.00037803]
	Learning Rate: 0.00037803
	LOSS [training: 2.6644436170972607 | validation: 2.5864764473797295]
	TIME [epoch: 8.29 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.615326249274156		[learning rate: 0.00037666]
	Learning Rate: 0.000376658
	LOSS [training: 2.615326249274156 | validation: 2.567876224293652]
	TIME [epoch: 8.3 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.626176534474566		[learning rate: 0.00037529]
	Learning Rate: 0.000375291
	LOSS [training: 2.626176534474566 | validation: 2.5797124596708123]
	TIME [epoch: 8.29 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6246372527706128		[learning rate: 0.00037393]
	Learning Rate: 0.000373929
	LOSS [training: 2.6246372527706128 | validation: 2.5567991257878093]
	TIME [epoch: 8.31 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6163963104711874		[learning rate: 0.00037257]
	Learning Rate: 0.000372572
	LOSS [training: 2.6163963104711874 | validation: 2.556477673108348]
	TIME [epoch: 8.29 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6251930002698356		[learning rate: 0.00037122]
	Learning Rate: 0.00037122
	LOSS [training: 2.6251930002698356 | validation: 2.603164435287127]
	TIME [epoch: 8.29 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.631564068204118		[learning rate: 0.00036987]
	Learning Rate: 0.000369873
	LOSS [training: 2.631564068204118 | validation: 2.578016779757486]
	TIME [epoch: 8.29 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.626344201031899		[learning rate: 0.00036853]
	Learning Rate: 0.000368531
	LOSS [training: 2.626344201031899 | validation: 2.570740556898212]
	TIME [epoch: 8.31 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6215997551302883		[learning rate: 0.00036719]
	Learning Rate: 0.000367193
	LOSS [training: 2.6215997551302883 | validation: 2.56785835034629]
	TIME [epoch: 8.3 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6196702185620553		[learning rate: 0.00036586]
	Learning Rate: 0.000365861
	LOSS [training: 2.6196702185620553 | validation: 2.5767082361513918]
	TIME [epoch: 8.29 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.632186115591943		[learning rate: 0.00036453]
	Learning Rate: 0.000364533
	LOSS [training: 2.632186115591943 | validation: 2.5762147797109476]
	TIME [epoch: 8.29 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6245152675538472		[learning rate: 0.00036321]
	Learning Rate: 0.00036321
	LOSS [training: 2.6245152675538472 | validation: 2.569580128136632]
	TIME [epoch: 8.29 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6190310474778804		[learning rate: 0.00036189]
	Learning Rate: 0.000361892
	LOSS [training: 2.6190310474778804 | validation: 2.586710389029573]
	TIME [epoch: 8.32 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6260052742296094		[learning rate: 0.00036058]
	Learning Rate: 0.000360579
	LOSS [training: 2.6260052742296094 | validation: 2.6143395687775843]
	TIME [epoch: 8.29 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.627194406217906		[learning rate: 0.00035927]
	Learning Rate: 0.00035927
	LOSS [training: 2.627194406217906 | validation: 2.571544117317421]
	TIME [epoch: 8.29 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6135994891972834		[learning rate: 0.00035797]
	Learning Rate: 0.000357966
	LOSS [training: 2.6135994891972834 | validation: 2.56345012475587]
	TIME [epoch: 8.29 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.622760276336236		[learning rate: 0.00035667]
	Learning Rate: 0.000356667
	LOSS [training: 2.622760276336236 | validation: 2.560394587767903]
	TIME [epoch: 8.32 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.615136582815071		[learning rate: 0.00035537]
	Learning Rate: 0.000355373
	LOSS [training: 2.615136582815071 | validation: 2.564045894651625]
	TIME [epoch: 8.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6153837512296265		[learning rate: 0.00035408]
	Learning Rate: 0.000354083
	LOSS [training: 2.6153837512296265 | validation: 2.553636926843926]
	TIME [epoch: 8.29 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.62430395057799		[learning rate: 0.0003528]
	Learning Rate: 0.000352798
	LOSS [training: 2.62430395057799 | validation: 2.5808963136786356]
	TIME [epoch: 8.29 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.625758091495683		[learning rate: 0.00035152]
	Learning Rate: 0.000351518
	LOSS [training: 2.625758091495683 | validation: 2.56766125552313]
	TIME [epoch: 8.3 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.635344714826921		[learning rate: 0.00035024]
	Learning Rate: 0.000350242
	LOSS [training: 2.635344714826921 | validation: 2.5505078956582667]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_1022.pth
	Model improved!!!
EPOCH 1023/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6211903547976996		[learning rate: 0.00034897]
	Learning Rate: 0.000348971
	LOSS [training: 2.6211903547976996 | validation: 2.5608065097369366]
	TIME [epoch: 8.3 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6167848686555883		[learning rate: 0.0003477]
	Learning Rate: 0.000347705
	LOSS [training: 2.6167848686555883 | validation: 2.6156377370701]
	TIME [epoch: 8.29 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6372303886986117		[learning rate: 0.00034644]
	Learning Rate: 0.000346443
	LOSS [training: 2.6372303886986117 | validation: 2.5647414925706]
	TIME [epoch: 8.3 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.616359191356035		[learning rate: 0.00034519]
	Learning Rate: 0.000345186
	LOSS [training: 2.616359191356035 | validation: 2.5674950361846234]
	TIME [epoch: 8.32 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6247619590416447		[learning rate: 0.00034393]
	Learning Rate: 0.000343933
	LOSS [training: 2.6247619590416447 | validation: 2.5776260629985703]
	TIME [epoch: 8.3 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6217538716500512		[learning rate: 0.00034268]
	Learning Rate: 0.000342685
	LOSS [training: 2.6217538716500512 | validation: 2.561050363534156]
	TIME [epoch: 8.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6142518663724497		[learning rate: 0.00034144]
	Learning Rate: 0.000341441
	LOSS [training: 2.6142518663724497 | validation: 2.567430733996047]
	TIME [epoch: 8.3 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.618675820285483		[learning rate: 0.0003402]
	Learning Rate: 0.000340202
	LOSS [training: 2.618675820285483 | validation: 2.5899048385198]
	TIME [epoch: 8.31 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.629175178569277		[learning rate: 0.00033897]
	Learning Rate: 0.000338967
	LOSS [training: 2.629175178569277 | validation: 2.5743399018485302]
	TIME [epoch: 8.3 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.629667899629104		[learning rate: 0.00033774]
	Learning Rate: 0.000337737
	LOSS [training: 2.629667899629104 | validation: 2.5757941898947805]
	TIME [epoch: 8.29 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.614844734382785		[learning rate: 0.00033651]
	Learning Rate: 0.000336512
	LOSS [training: 2.614844734382785 | validation: 2.5797509429576952]
	TIME [epoch: 8.3 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.622543353462456		[learning rate: 0.00033529]
	Learning Rate: 0.00033529
	LOSS [training: 2.622543353462456 | validation: 2.5669945997500823]
	TIME [epoch: 8.31 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6137946631556206		[learning rate: 0.00033407]
	Learning Rate: 0.000334074
	LOSS [training: 2.6137946631556206 | validation: 2.5674510412231664]
	TIME [epoch: 8.31 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6200027604931244		[learning rate: 0.00033286]
	Learning Rate: 0.000332861
	LOSS [training: 2.6200027604931244 | validation: 2.569964893317975]
	TIME [epoch: 8.3 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.628594700202578		[learning rate: 0.00033165]
	Learning Rate: 0.000331653
	LOSS [training: 2.628594700202578 | validation: 2.58046818070322]
	TIME [epoch: 8.29 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6192685703502483		[learning rate: 0.00033045]
	Learning Rate: 0.00033045
	LOSS [training: 2.6192685703502483 | validation: 2.5876419341101413]
	TIME [epoch: 8.29 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6300491853015573		[learning rate: 0.00032925]
	Learning Rate: 0.00032925
	LOSS [training: 2.6300491853015573 | validation: 2.5743113504755923]
	TIME [epoch: 8.32 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6202497255416524		[learning rate: 0.00032806]
	Learning Rate: 0.000328056
	LOSS [training: 2.6202497255416524 | validation: 2.5607570665352104]
	TIME [epoch: 8.3 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6232823349620307		[learning rate: 0.00032686]
	Learning Rate: 0.000326865
	LOSS [training: 2.6232823349620307 | validation: 2.5515697410670466]
	TIME [epoch: 8.29 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6180119512116353		[learning rate: 0.00032568]
	Learning Rate: 0.000325679
	LOSS [training: 2.6180119512116353 | validation: 2.562415122951466]
	TIME [epoch: 8.3 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6124445876707245		[learning rate: 0.0003245]
	Learning Rate: 0.000324497
	LOSS [training: 2.6124445876707245 | validation: 2.552314654709609]
	TIME [epoch: 8.31 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.612731062055016		[learning rate: 0.00032332]
	Learning Rate: 0.000323319
	LOSS [training: 2.612731062055016 | validation: 2.56294012440699]
	TIME [epoch: 8.3 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6161743834876754		[learning rate: 0.00032215]
	Learning Rate: 0.000322146
	LOSS [training: 2.6161743834876754 | validation: 2.5800393310374705]
	TIME [epoch: 8.3 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.620433999655249		[learning rate: 0.00032098]
	Learning Rate: 0.000320977
	LOSS [training: 2.620433999655249 | validation: 2.5616347242162156]
	TIME [epoch: 8.29 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6172042804739952		[learning rate: 0.00031981]
	Learning Rate: 0.000319812
	LOSS [training: 2.6172042804739952 | validation: 2.5762886926443223]
	TIME [epoch: 8.3 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6325591189583584		[learning rate: 0.00031865]
	Learning Rate: 0.000318651
	LOSS [training: 2.6325591189583584 | validation: 2.567885991705186]
	TIME [epoch: 8.32 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.618436539255601		[learning rate: 0.0003175]
	Learning Rate: 0.000317495
	LOSS [training: 2.618436539255601 | validation: 2.564857590631952]
	TIME [epoch: 8.3 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6225571820338076		[learning rate: 0.00031634]
	Learning Rate: 0.000316343
	LOSS [training: 2.6225571820338076 | validation: 2.5605150789305524]
	TIME [epoch: 8.3 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6315725119323634		[learning rate: 0.00031519]
	Learning Rate: 0.000315195
	LOSS [training: 2.6315725119323634 | validation: 2.5699315394278526]
	TIME [epoch: 8.3 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.614045706705912		[learning rate: 0.00031405]
	Learning Rate: 0.000314051
	LOSS [training: 2.614045706705912 | validation: 2.561585568073215]
	TIME [epoch: 8.32 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6202253096204737		[learning rate: 0.00031291]
	Learning Rate: 0.000312911
	LOSS [training: 2.6202253096204737 | validation: 2.552417840944429]
	TIME [epoch: 8.3 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6224528892070547		[learning rate: 0.00031178]
	Learning Rate: 0.000311776
	LOSS [training: 2.6224528892070547 | validation: 2.547708274381192]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_1054.pth
	Model improved!!!
EPOCH 1055/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.614403683347754		[learning rate: 0.00031064]
	Learning Rate: 0.000310644
	LOSS [training: 2.614403683347754 | validation: 2.5563456702880973]
	TIME [epoch: 8.29 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.612444926770892		[learning rate: 0.00030952]
	Learning Rate: 0.000309517
	LOSS [training: 2.612444926770892 | validation: 2.554049579698759]
	TIME [epoch: 8.31 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.621329984224378		[learning rate: 0.00030839]
	Learning Rate: 0.000308394
	LOSS [training: 2.621329984224378 | validation: 2.5668773541340357]
	TIME [epoch: 8.3 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6171622317103087		[learning rate: 0.00030727]
	Learning Rate: 0.000307274
	LOSS [training: 2.6171622317103087 | validation: 2.5646872024133707]
	TIME [epoch: 8.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6100787721720775		[learning rate: 0.00030616]
	Learning Rate: 0.000306159
	LOSS [training: 2.6100787721720775 | validation: 2.5526748598004643]
	TIME [epoch: 8.29 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.613664579258447		[learning rate: 0.00030505]
	Learning Rate: 0.000305048
	LOSS [training: 2.613664579258447 | validation: 2.5612531438083037]
	TIME [epoch: 8.3 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6188224437937953		[learning rate: 0.00030394]
	Learning Rate: 0.000303941
	LOSS [training: 2.6188224437937953 | validation: 2.5659630213319877]
	TIME [epoch: 8.31 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6167268045634695		[learning rate: 0.00030284]
	Learning Rate: 0.000302838
	LOSS [training: 2.6167268045634695 | validation: 2.560053260074066]
	TIME [epoch: 8.29 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.618664719933107		[learning rate: 0.00030174]
	Learning Rate: 0.000301739
	LOSS [training: 2.618664719933107 | validation: 2.5708336191944556]
	TIME [epoch: 8.3 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.615512349712218		[learning rate: 0.00030064]
	Learning Rate: 0.000300644
	LOSS [training: 2.615512349712218 | validation: 2.549057234436976]
	TIME [epoch: 8.29 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.616572708489219		[learning rate: 0.00029955]
	Learning Rate: 0.000299553
	LOSS [training: 2.616572708489219 | validation: 2.573824472132258]
	TIME [epoch: 8.31 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.61904901791984		[learning rate: 0.00029847]
	Learning Rate: 0.000298466
	LOSS [training: 2.61904901791984 | validation: 2.5579422850240707]
	TIME [epoch: 8.29 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6115563018319206		[learning rate: 0.00029738]
	Learning Rate: 0.000297383
	LOSS [training: 2.6115563018319206 | validation: 2.5636384329608077]
	TIME [epoch: 8.29 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6128275895810824		[learning rate: 0.0002963]
	Learning Rate: 0.000296304
	LOSS [training: 2.6128275895810824 | validation: 2.5707184207269]
	TIME [epoch: 8.29 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.636602847444935		[learning rate: 0.00029523]
	Learning Rate: 0.000295228
	LOSS [training: 2.636602847444935 | validation: 2.552159224380752]
	TIME [epoch: 8.31 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6131486389078056		[learning rate: 0.00029416]
	Learning Rate: 0.000294157
	LOSS [training: 2.6131486389078056 | validation: 2.569262501796014]
	TIME [epoch: 8.3 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.637980257288505		[learning rate: 0.00029309]
	Learning Rate: 0.000293089
	LOSS [training: 2.637980257288505 | validation: 2.562928287898176]
	TIME [epoch: 8.29 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6127021145683904		[learning rate: 0.00029203]
	Learning Rate: 0.000292026
	LOSS [training: 2.6127021145683904 | validation: 2.5585340329966506]
	TIME [epoch: 8.29 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.614335047686168		[learning rate: 0.00029097]
	Learning Rate: 0.000290966
	LOSS [training: 2.614335047686168 | validation: 2.56416927345553]
	TIME [epoch: 8.29 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.621542383747797		[learning rate: 0.00028991]
	Learning Rate: 0.00028991
	LOSS [training: 2.621542383747797 | validation: 2.5735874225639597]
	TIME [epoch: 8.31 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6126410300024494		[learning rate: 0.00028886]
	Learning Rate: 0.000288858
	LOSS [training: 2.6126410300024494 | validation: 2.5692804302962626]
	TIME [epoch: 8.29 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6108631500986315		[learning rate: 0.00028781]
	Learning Rate: 0.00028781
	LOSS [training: 2.6108631500986315 | validation: 2.557374336685674]
	TIME [epoch: 8.29 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.614014979181099		[learning rate: 0.00028677]
	Learning Rate: 0.000286765
	LOSS [training: 2.614014979181099 | validation: 2.576168280887437]
	TIME [epoch: 8.28 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.620044638238677		[learning rate: 0.00028572]
	Learning Rate: 0.000285724
	LOSS [training: 2.620044638238677 | validation: 2.599830906653753]
	TIME [epoch: 8.31 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.620880512861578		[learning rate: 0.00028469]
	Learning Rate: 0.000284688
	LOSS [training: 2.620880512861578 | validation: 2.5663361684876858]
	TIME [epoch: 8.29 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6108113232355885		[learning rate: 0.00028365]
	Learning Rate: 0.000283654
	LOSS [training: 2.6108113232355885 | validation: 2.5659863121614093]
	TIME [epoch: 8.29 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.62074869592706		[learning rate: 0.00028263]
	Learning Rate: 0.000282625
	LOSS [training: 2.62074869592706 | validation: 2.5712939259996563]
	TIME [epoch: 8.29 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.615671986356098		[learning rate: 0.0002816]
	Learning Rate: 0.000281599
	LOSS [training: 2.615671986356098 | validation: 2.550344343030665]
	TIME [epoch: 8.3 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606295316093779		[learning rate: 0.00028058]
	Learning Rate: 0.000280577
	LOSS [training: 2.606295316093779 | validation: 2.5800303969146103]
	TIME [epoch: 8.3 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6175574152535743		[learning rate: 0.00027956]
	Learning Rate: 0.000279559
	LOSS [training: 2.6175574152535743 | validation: 2.556539388016867]
	TIME [epoch: 8.29 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6095629583588877		[learning rate: 0.00027854]
	Learning Rate: 0.000278545
	LOSS [training: 2.6095629583588877 | validation: 2.5540606313093193]
	TIME [epoch: 8.29 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6344407294334973		[learning rate: 0.00027753]
	Learning Rate: 0.000277534
	LOSS [training: 2.6344407294334973 | validation: 2.585039417501554]
	TIME [epoch: 8.29 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6191844087207947		[learning rate: 0.00027653]
	Learning Rate: 0.000276527
	LOSS [training: 2.6191844087207947 | validation: 2.5603226475671796]
	TIME [epoch: 8.31 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6120825982253004		[learning rate: 0.00027552]
	Learning Rate: 0.000275523
	LOSS [training: 2.6120825982253004 | validation: 2.5752344352710033]
	TIME [epoch: 8.29 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6215251655310086		[learning rate: 0.00027452]
	Learning Rate: 0.000274523
	LOSS [training: 2.6215251655310086 | validation: 2.5541384438231756]
	TIME [epoch: 8.29 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6232729169386357		[learning rate: 0.00027353]
	Learning Rate: 0.000273527
	LOSS [training: 2.6232729169386357 | validation: 2.5988028522570668]
	TIME [epoch: 8.29 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.628915166218334		[learning rate: 0.00027253]
	Learning Rate: 0.000272534
	LOSS [training: 2.628915166218334 | validation: 2.5635048307336383]
	TIME [epoch: 8.32 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.611248283828245		[learning rate: 0.00027155]
	Learning Rate: 0.000271545
	LOSS [training: 2.611248283828245 | validation: 2.560127366304657]
	TIME [epoch: 8.3 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6123518403019115		[learning rate: 0.00027056]
	Learning Rate: 0.00027056
	LOSS [training: 2.6123518403019115 | validation: 2.5580478436415097]
	TIME [epoch: 8.29 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.618006045203259		[learning rate: 0.00026958]
	Learning Rate: 0.000269578
	LOSS [training: 2.618006045203259 | validation: 2.5622374706228204]
	TIME [epoch: 8.3 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6161281471202957		[learning rate: 0.0002686]
	Learning Rate: 0.0002686
	LOSS [training: 2.6161281471202957 | validation: 2.566495598713947]
	TIME [epoch: 8.31 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6141296476224958		[learning rate: 0.00026762]
	Learning Rate: 0.000267625
	LOSS [training: 2.6141296476224958 | validation: 2.5454605986799317]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_1096.pth
	Model improved!!!
EPOCH 1097/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6114007104860315		[learning rate: 0.00026665]
	Learning Rate: 0.000266654
	LOSS [training: 2.6114007104860315 | validation: 2.572772100847361]
	TIME [epoch: 8.29 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6207318479691546		[learning rate: 0.00026569]
	Learning Rate: 0.000265686
	LOSS [training: 2.6207318479691546 | validation: 2.5506792837670123]
	TIME [epoch: 8.29 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6124050931029257		[learning rate: 0.00026472]
	Learning Rate: 0.000264722
	LOSS [training: 2.6124050931029257 | validation: 2.562009813105931]
	TIME [epoch: 8.29 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6117738606970193		[learning rate: 0.00026376]
	Learning Rate: 0.000263761
	LOSS [training: 2.6117738606970193 | validation: 2.5612293137712423]
	TIME [epoch: 8.31 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.616530241678353		[learning rate: 0.0002628]
	Learning Rate: 0.000262804
	LOSS [training: 2.616530241678353 | validation: 2.5617433876900826]
	TIME [epoch: 8.29 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6049410892680642		[learning rate: 0.00026185]
	Learning Rate: 0.00026185
	LOSS [training: 2.6049410892680642 | validation: 2.570156907614301]
	TIME [epoch: 8.28 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.612350140700042		[learning rate: 0.0002609]
	Learning Rate: 0.0002609
	LOSS [training: 2.612350140700042 | validation: 2.588539052978059]
	TIME [epoch: 8.28 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6218809646445056		[learning rate: 0.00025995]
	Learning Rate: 0.000259953
	LOSS [training: 2.6218809646445056 | validation: 2.5604570548305206]
	TIME [epoch: 8.31 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6125905967124106		[learning rate: 0.00025901]
	Learning Rate: 0.00025901
	LOSS [training: 2.6125905967124106 | validation: 2.557759322929872]
	TIME [epoch: 8.29 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6142013034576985		[learning rate: 0.00025807]
	Learning Rate: 0.00025807
	LOSS [training: 2.6142013034576985 | validation: 2.567804495102808]
	TIME [epoch: 8.29 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.615159104802522		[learning rate: 0.00025713]
	Learning Rate: 0.000257133
	LOSS [training: 2.615159104802522 | validation: 2.555363755378859]
	TIME [epoch: 8.29 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.612040678509293		[learning rate: 0.0002562]
	Learning Rate: 0.0002562
	LOSS [training: 2.612040678509293 | validation: 2.5654092893351943]
	TIME [epoch: 8.3 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6168744451595876		[learning rate: 0.00025527]
	Learning Rate: 0.00025527
	LOSS [training: 2.6168744451595876 | validation: 2.5501377288914053]
	TIME [epoch: 8.3 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6177295280314428		[learning rate: 0.00025434]
	Learning Rate: 0.000254344
	LOSS [training: 2.6177295280314428 | validation: 2.564346799372801]
	TIME [epoch: 8.28 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6078615822854894		[learning rate: 0.00025342]
	Learning Rate: 0.000253421
	LOSS [training: 2.6078615822854894 | validation: 2.574152082258524]
	TIME [epoch: 8.29 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.616745599430868		[learning rate: 0.0002525]
	Learning Rate: 0.000252501
	LOSS [training: 2.616745599430868 | validation: 2.5609814660054777]
	TIME [epoch: 8.29 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.610369974069081		[learning rate: 0.00025158]
	Learning Rate: 0.000251585
	LOSS [training: 2.610369974069081 | validation: 2.5683290144279916]
	TIME [epoch: 8.32 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.624850007607583		[learning rate: 0.00025067]
	Learning Rate: 0.000250672
	LOSS [training: 2.624850007607583 | validation: 2.5936560733397322]
	TIME [epoch: 8.29 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.623147951342236		[learning rate: 0.00024976]
	Learning Rate: 0.000249762
	LOSS [training: 2.623147951342236 | validation: 2.5770407778698896]
	TIME [epoch: 8.29 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6205826359161306		[learning rate: 0.00024886]
	Learning Rate: 0.000248856
	LOSS [training: 2.6205826359161306 | validation: 2.560636844831989]
	TIME [epoch: 8.29 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.615140549299472		[learning rate: 0.00024795]
	Learning Rate: 0.000247952
	LOSS [training: 2.615140549299472 | validation: 2.5524141860299236]
	TIME [epoch: 8.32 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.608725365753945		[learning rate: 0.00024705]
	Learning Rate: 0.000247053
	LOSS [training: 2.608725365753945 | validation: 2.5697761592065618]
	TIME [epoch: 8.29 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6123311088448142		[learning rate: 0.00024616]
	Learning Rate: 0.000246156
	LOSS [training: 2.6123311088448142 | validation: 2.5605280129016075]
	TIME [epoch: 8.29 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.615809516191881		[learning rate: 0.00024526]
	Learning Rate: 0.000245263
	LOSS [training: 2.615809516191881 | validation: 2.5659595744803565]
	TIME [epoch: 8.29 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6241991012438177		[learning rate: 0.00024437]
	Learning Rate: 0.000244373
	LOSS [training: 2.6241991012438177 | validation: 2.5541665935458644]
	TIME [epoch: 8.31 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6169841514088974		[learning rate: 0.00024349]
	Learning Rate: 0.000243486
	LOSS [training: 2.6169841514088974 | validation: 2.5595697329778657]
	TIME [epoch: 8.29 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6129254991005224		[learning rate: 0.0002426]
	Learning Rate: 0.000242602
	LOSS [training: 2.6129254991005224 | validation: 2.551152123122727]
	TIME [epoch: 8.29 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6100509275485213		[learning rate: 0.00024172]
	Learning Rate: 0.000241722
	LOSS [training: 2.6100509275485213 | validation: 2.5731539848596894]
	TIME [epoch: 8.28 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60989870253036		[learning rate: 0.00024084]
	Learning Rate: 0.000240845
	LOSS [training: 2.60989870253036 | validation: 2.557487525381065]
	TIME [epoch: 8.28 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6113454536135725		[learning rate: 0.00023997]
	Learning Rate: 0.000239971
	LOSS [training: 2.6113454536135725 | validation: 2.5740293991658865]
	TIME [epoch: 8.3 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.616208938629034		[learning rate: 0.0002391]
	Learning Rate: 0.0002391
	LOSS [training: 2.616208938629034 | validation: 2.5580937232737213]
	TIME [epoch: 8.29 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6188312511534377		[learning rate: 0.00023823]
	Learning Rate: 0.000238232
	LOSS [training: 2.6188312511534377 | validation: 2.5539626652263143]
	TIME [epoch: 8.29 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6087539512215567		[learning rate: 0.00023737]
	Learning Rate: 0.000237367
	LOSS [training: 2.6087539512215567 | validation: 2.5601609737118483]
	TIME [epoch: 8.28 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6182885091842643		[learning rate: 0.00023651]
	Learning Rate: 0.000236506
	LOSS [training: 2.6182885091842643 | validation: 2.5700477632978833]
	TIME [epoch: 8.31 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.612624898985743		[learning rate: 0.00023565]
	Learning Rate: 0.000235648
	LOSS [training: 2.612624898985743 | validation: 2.5918463222544004]
	TIME [epoch: 8.29 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6159267779752264		[learning rate: 0.00023479]
	Learning Rate: 0.000234793
	LOSS [training: 2.6159267779752264 | validation: 2.573643147336331]
	TIME [epoch: 8.29 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6194079253714264		[learning rate: 0.00023394]
	Learning Rate: 0.00023394
	LOSS [training: 2.6194079253714264 | validation: 2.560505243419229]
	TIME [epoch: 8.29 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6129476264711853		[learning rate: 0.00023309]
	Learning Rate: 0.000233091
	LOSS [training: 2.6129476264711853 | validation: 2.591048658422215]
	TIME [epoch: 8.29 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.63444875345658		[learning rate: 0.00023225]
	Learning Rate: 0.000232246
	LOSS [training: 2.63444875345658 | validation: 2.5714611162627627]
	TIME [epoch: 8.31 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6065230578583947		[learning rate: 0.0002314]
	Learning Rate: 0.000231403
	LOSS [training: 2.6065230578583947 | validation: 2.557174540751554]
	TIME [epoch: 8.29 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.614660424703189		[learning rate: 0.00023056]
	Learning Rate: 0.000230563
	LOSS [training: 2.614660424703189 | validation: 2.5509892667876697]
	TIME [epoch: 8.29 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.61066878574177		[learning rate: 0.00022973]
	Learning Rate: 0.000229726
	LOSS [training: 2.61066878574177 | validation: 2.5676354925803855]
	TIME [epoch: 8.29 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6133247810238216		[learning rate: 0.00022889]
	Learning Rate: 0.000228893
	LOSS [training: 2.6133247810238216 | validation: 2.5506148419582804]
	TIME [epoch: 8.31 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.609184071853616		[learning rate: 0.00022806]
	Learning Rate: 0.000228062
	LOSS [training: 2.609184071853616 | validation: 2.559184252962665]
	TIME [epoch: 8.28 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6094118695286648		[learning rate: 0.00022723]
	Learning Rate: 0.000227234
	LOSS [training: 2.6094118695286648 | validation: 2.554263189770239]
	TIME [epoch: 8.29 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.608085236523821		[learning rate: 0.00022641]
	Learning Rate: 0.00022641
	LOSS [training: 2.608085236523821 | validation: 2.5628758267884826]
	TIME [epoch: 8.29 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.609707649521677		[learning rate: 0.00022559]
	Learning Rate: 0.000225588
	LOSS [training: 2.609707649521677 | validation: 2.5693344080592184]
	TIME [epoch: 8.31 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6130449880839057		[learning rate: 0.00022477]
	Learning Rate: 0.000224769
	LOSS [training: 2.6130449880839057 | validation: 2.5663746006367374]
	TIME [epoch: 8.28 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6061711768620244		[learning rate: 0.00022395]
	Learning Rate: 0.000223954
	LOSS [training: 2.6061711768620244 | validation: 2.55396758567273]
	TIME [epoch: 8.29 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6109297300622605		[learning rate: 0.00022314]
	Learning Rate: 0.000223141
	LOSS [training: 2.6109297300622605 | validation: 2.5562918453566024]
	TIME [epoch: 8.29 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6170564251099155		[learning rate: 0.00022233]
	Learning Rate: 0.000222331
	LOSS [training: 2.6170564251099155 | validation: 2.5707348328596566]
	TIME [epoch: 8.3 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6275566059583535		[learning rate: 0.00022152]
	Learning Rate: 0.000221524
	LOSS [training: 2.6275566059583535 | validation: 2.570254789276099]
	TIME [epoch: 8.31 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.607657266261442		[learning rate: 0.00022072]
	Learning Rate: 0.00022072
	LOSS [training: 2.607657266261442 | validation: 2.5680924209549425]
	TIME [epoch: 8.29 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6209715074712614		[learning rate: 0.00021992]
	Learning Rate: 0.000219919
	LOSS [training: 2.6209715074712614 | validation: 2.559621590365496]
	TIME [epoch: 8.29 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6155945825324323		[learning rate: 0.00021912]
	Learning Rate: 0.000219121
	LOSS [training: 2.6155945825324323 | validation: 2.601985949317982]
	TIME [epoch: 8.29 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6188703335439483		[learning rate: 0.00021833]
	Learning Rate: 0.000218326
	LOSS [training: 2.6188703335439483 | validation: 2.5674249985121635]
	TIME [epoch: 8.31 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.613418919699728		[learning rate: 0.00021753]
	Learning Rate: 0.000217534
	LOSS [training: 2.613418919699728 | validation: 2.5669195496284765]
	TIME [epoch: 8.28 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6106792283459246		[learning rate: 0.00021674]
	Learning Rate: 0.000216744
	LOSS [training: 2.6106792283459246 | validation: 2.549976785793249]
	TIME [epoch: 8.29 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6096410270379367		[learning rate: 0.00021596]
	Learning Rate: 0.000215958
	LOSS [training: 2.6096410270379367 | validation: 2.5977163605053004]
	TIME [epoch: 8.28 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.628963934315375		[learning rate: 0.00021517]
	Learning Rate: 0.000215174
	LOSS [training: 2.628963934315375 | validation: 2.562495660401507]
	TIME [epoch: 8.31 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6135681907726003		[learning rate: 0.00021439]
	Learning Rate: 0.000214393
	LOSS [training: 2.6135681907726003 | validation: 2.566745495445999]
	TIME [epoch: 8.3 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.619488540013233		[learning rate: 0.00021361]
	Learning Rate: 0.000213615
	LOSS [training: 2.619488540013233 | validation: 2.5633157818875945]
	TIME [epoch: 8.29 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6119362254459246		[learning rate: 0.00021284]
	Learning Rate: 0.00021284
	LOSS [training: 2.6119362254459246 | validation: 2.5730733942940716]
	TIME [epoch: 8.28 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.623382043227591		[learning rate: 0.00021207]
	Learning Rate: 0.000212067
	LOSS [training: 2.623382043227591 | validation: 2.544006733211087]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_1160.pth
	Model improved!!!
EPOCH 1161/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.615613843559273		[learning rate: 0.0002113]
	Learning Rate: 0.000211298
	LOSS [training: 2.615613843559273 | validation: 2.5498321872148564]
	TIME [epoch: 8.32 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6070843179782788		[learning rate: 0.00021053]
	Learning Rate: 0.000210531
	LOSS [training: 2.6070843179782788 | validation: 2.559712193077339]
	TIME [epoch: 8.29 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6122708724457393		[learning rate: 0.00020977]
	Learning Rate: 0.000209767
	LOSS [training: 2.6122708724457393 | validation: 2.550482363077186]
	TIME [epoch: 8.3 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.613336696434738		[learning rate: 0.00020901]
	Learning Rate: 0.000209006
	LOSS [training: 2.613336696434738 | validation: 2.5593865204482515]
	TIME [epoch: 8.3 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605302599588978		[learning rate: 0.00020825]
	Learning Rate: 0.000208247
	LOSS [training: 2.605302599588978 | validation: 2.5516265541170666]
	TIME [epoch: 8.33 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6115333791244058		[learning rate: 0.00020749]
	Learning Rate: 0.000207491
	LOSS [training: 2.6115333791244058 | validation: 2.5729435740625606]
	TIME [epoch: 8.3 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6178723787754388		[learning rate: 0.00020674]
	Learning Rate: 0.000206738
	LOSS [training: 2.6178723787754388 | validation: 2.5869534006454185]
	TIME [epoch: 8.3 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.614182805604261		[learning rate: 0.00020599]
	Learning Rate: 0.000205988
	LOSS [training: 2.614182805604261 | validation: 2.55921292903013]
	TIME [epoch: 8.3 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605710272636693		[learning rate: 0.00020524]
	Learning Rate: 0.000205241
	LOSS [training: 2.605710272636693 | validation: 2.5555534233847492]
	TIME [epoch: 8.32 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6181289033831727		[learning rate: 0.0002045]
	Learning Rate: 0.000204496
	LOSS [training: 2.6181289033831727 | validation: 2.558808558910748]
	TIME [epoch: 8.3 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.612445291491082		[learning rate: 0.00020375]
	Learning Rate: 0.000203754
	LOSS [training: 2.612445291491082 | validation: 2.568360102964683]
	TIME [epoch: 8.3 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606661131115542		[learning rate: 0.00020301]
	Learning Rate: 0.000203014
	LOSS [training: 2.606661131115542 | validation: 2.5657107141317246]
	TIME [epoch: 8.3 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6131373005136904		[learning rate: 0.00020228]
	Learning Rate: 0.000202277
	LOSS [training: 2.6131373005136904 | validation: 2.5486880597442925]
	TIME [epoch: 8.31 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6062481482157422		[learning rate: 0.00020154]
	Learning Rate: 0.000201543
	LOSS [training: 2.6062481482157422 | validation: 2.5508153346085316]
	TIME [epoch: 8.32 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.610489645984076		[learning rate: 0.00020081]
	Learning Rate: 0.000200812
	LOSS [training: 2.610489645984076 | validation: 2.5521163069682538]
	TIME [epoch: 8.3 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.61329525017672		[learning rate: 0.00020008]
	Learning Rate: 0.000200083
	LOSS [training: 2.61329525017672 | validation: 2.5476463294519593]
	TIME [epoch: 8.3 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.609692932180794		[learning rate: 0.00019936]
	Learning Rate: 0.000199357
	LOSS [training: 2.609692932180794 | validation: 2.57060846172176]
	TIME [epoch: 8.3 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.613067130555108		[learning rate: 0.00019863]
	Learning Rate: 0.000198634
	LOSS [training: 2.613067130555108 | validation: 2.5650795743675556]
	TIME [epoch: 8.32 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606183589993888		[learning rate: 0.00019791]
	Learning Rate: 0.000197913
	LOSS [training: 2.606183589993888 | validation: 2.5672140807656767]
	TIME [epoch: 8.3 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.611310902162386		[learning rate: 0.00019719]
	Learning Rate: 0.000197194
	LOSS [training: 2.611310902162386 | validation: 2.5882918717142633]
	TIME [epoch: 8.3 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6184471584011892		[learning rate: 0.00019648]
	Learning Rate: 0.000196479
	LOSS [training: 2.6184471584011892 | validation: 2.569033312200684]
	TIME [epoch: 8.3 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6080320776758987		[learning rate: 0.00019577]
	Learning Rate: 0.000195766
	LOSS [training: 2.6080320776758987 | validation: 2.558461027431341]
	TIME [epoch: 8.31 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606791720630472		[learning rate: 0.00019506]
	Learning Rate: 0.000195055
	LOSS [training: 2.606791720630472 | validation: 2.573392426973763]
	TIME [epoch: 8.31 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6113912072748375		[learning rate: 0.00019435]
	Learning Rate: 0.000194347
	LOSS [training: 2.6113912072748375 | validation: 2.5570616790462735]
	TIME [epoch: 8.3 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6083117510202767		[learning rate: 0.00019364]
	Learning Rate: 0.000193642
	LOSS [training: 2.6083117510202767 | validation: 2.5611415956933845]
	TIME [epoch: 8.3 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.615729246971972		[learning rate: 0.00019294]
	Learning Rate: 0.000192939
	LOSS [training: 2.615729246971972 | validation: 2.5638516282795534]
	TIME [epoch: 8.3 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.617281390518543		[learning rate: 0.00019224]
	Learning Rate: 0.000192239
	LOSS [training: 2.617281390518543 | validation: 2.5508194341813413]
	TIME [epoch: 8.33 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6074284438931428		[learning rate: 0.00019154]
	Learning Rate: 0.000191542
	LOSS [training: 2.6074284438931428 | validation: 2.5526428826499483]
	TIME [epoch: 8.3 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.611044020786522		[learning rate: 0.00019085]
	Learning Rate: 0.000190846
	LOSS [training: 2.611044020786522 | validation: 2.5512209504714614]
	TIME [epoch: 8.3 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.608887493752336		[learning rate: 0.00019015]
	Learning Rate: 0.000190154
	LOSS [training: 2.608887493752336 | validation: 2.5584494257365]
	TIME [epoch: 8.29 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6052681226694956		[learning rate: 0.00018946]
	Learning Rate: 0.000189464
	LOSS [training: 2.6052681226694956 | validation: 2.5573514430949924]
	TIME [epoch: 8.33 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6048147197681004		[learning rate: 0.00018878]
	Learning Rate: 0.000188776
	LOSS [training: 2.6048147197681004 | validation: 2.5703112284213145]
	TIME [epoch: 8.3 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6147379918482687		[learning rate: 0.00018809]
	Learning Rate: 0.000188091
	LOSS [training: 2.6147379918482687 | validation: 2.5653475188285015]
	TIME [epoch: 8.31 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6181076366016915		[learning rate: 0.00018741]
	Learning Rate: 0.000187409
	LOSS [training: 2.6181076366016915 | validation: 2.56232679733466]
	TIME [epoch: 8.3 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606940288360222		[learning rate: 0.00018673]
	Learning Rate: 0.000186729
	LOSS [training: 2.606940288360222 | validation: 2.556772809594446]
	TIME [epoch: 8.32 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6109453848683932		[learning rate: 0.00018605]
	Learning Rate: 0.000186051
	LOSS [training: 2.6109453848683932 | validation: 2.5590000261190733]
	TIME [epoch: 8.31 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6110735315958005		[learning rate: 0.00018538]
	Learning Rate: 0.000185376
	LOSS [training: 2.6110735315958005 | validation: 2.5556279515824816]
	TIME [epoch: 8.31 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.620448768646864		[learning rate: 0.0001847]
	Learning Rate: 0.000184703
	LOSS [training: 2.620448768646864 | validation: 2.5503535326336477]
	TIME [epoch: 8.3 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605620832076471		[learning rate: 0.00018403]
	Learning Rate: 0.000184033
	LOSS [training: 2.605620832076471 | validation: 2.549936283602249]
	TIME [epoch: 8.3 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605708344170194		[learning rate: 0.00018336]
	Learning Rate: 0.000183365
	LOSS [training: 2.605708344170194 | validation: 2.5558408466086777]
	TIME [epoch: 8.33 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605839644906611		[learning rate: 0.0001827]
	Learning Rate: 0.000182699
	LOSS [training: 2.605839644906611 | validation: 2.557686254960509]
	TIME [epoch: 8.3 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.607309429317966		[learning rate: 0.00018204]
	Learning Rate: 0.000182036
	LOSS [training: 2.607309429317966 | validation: 2.5669554559976895]
	TIME [epoch: 8.3 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.613852809220634		[learning rate: 0.00018138]
	Learning Rate: 0.000181376
	LOSS [training: 2.613852809220634 | validation: 2.5620322581531227]
	TIME [epoch: 8.3 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6157096114186005		[learning rate: 0.00018072]
	Learning Rate: 0.000180717
	LOSS [training: 2.6157096114186005 | validation: 2.5634436866343173]
	TIME [epoch: 8.32 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6066732837395397		[learning rate: 0.00018006]
	Learning Rate: 0.000180062
	LOSS [training: 2.6066732837395397 | validation: 2.5593509317846213]
	TIME [epoch: 8.3 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6169157620884227		[learning rate: 0.00017941]
	Learning Rate: 0.000179408
	LOSS [training: 2.6169157620884227 | validation: 2.547181561556331]
	TIME [epoch: 8.3 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.610015121144026		[learning rate: 0.00017876]
	Learning Rate: 0.000178757
	LOSS [training: 2.610015121144026 | validation: 2.5592734165222213]
	TIME [epoch: 8.3 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.610042647474354		[learning rate: 0.00017811]
	Learning Rate: 0.000178108
	LOSS [training: 2.610042647474354 | validation: 2.5449297796544954]
	TIME [epoch: 8.32 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602689691294403		[learning rate: 0.00017746]
	Learning Rate: 0.000177462
	LOSS [training: 2.602689691294403 | validation: 2.566450126167737]
	TIME [epoch: 8.3 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6152360098488328		[learning rate: 0.00017682]
	Learning Rate: 0.000176818
	LOSS [training: 2.6152360098488328 | validation: 2.5564724516657327]
	TIME [epoch: 8.3 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.610142752399479		[learning rate: 0.00017618]
	Learning Rate: 0.000176176
	LOSS [training: 2.610142752399479 | validation: 2.5462401587206234]
	TIME [epoch: 8.29 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6165880115938234		[learning rate: 0.00017554]
	Learning Rate: 0.000175537
	LOSS [training: 2.6165880115938234 | validation: 2.563359244877176]
	TIME [epoch: 8.29 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.611310042572464		[learning rate: 0.0001749]
	Learning Rate: 0.0001749
	LOSS [training: 2.611310042572464 | validation: 2.5720613742475047]
	TIME [epoch: 8.32 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6098365565814063		[learning rate: 0.00017427]
	Learning Rate: 0.000174265
	LOSS [training: 2.6098365565814063 | validation: 2.561439450173855]
	TIME [epoch: 8.29 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6111967648981294		[learning rate: 0.00017363]
	Learning Rate: 0.000173633
	LOSS [training: 2.6111967648981294 | validation: 2.5547095317073207]
	TIME [epoch: 8.3 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.613247082849466		[learning rate: 0.000173]
	Learning Rate: 0.000173003
	LOSS [training: 2.613247082849466 | validation: 2.5532226387744963]
	TIME [epoch: 8.29 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6160432300379637		[learning rate: 0.00017237]
	Learning Rate: 0.000172375
	LOSS [training: 2.6160432300379637 | validation: 2.566767856969074]
	TIME [epoch: 8.32 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6079970583920047		[learning rate: 0.00017175]
	Learning Rate: 0.000171749
	LOSS [training: 2.6079970583920047 | validation: 2.5443987649069335]
	TIME [epoch: 8.3 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6071133445497		[learning rate: 0.00017113]
	Learning Rate: 0.000171126
	LOSS [training: 2.6071133445497 | validation: 2.5594302724232714]
	TIME [epoch: 8.29 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604520910783217		[learning rate: 0.0001705]
	Learning Rate: 0.000170505
	LOSS [training: 2.604520910783217 | validation: 2.552869574804763]
	TIME [epoch: 8.29 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6109242406907582		[learning rate: 0.00016989]
	Learning Rate: 0.000169886
	LOSS [training: 2.6109242406907582 | validation: 2.5670813504231846]
	TIME [epoch: 8.31 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6084139539117235		[learning rate: 0.00016927]
	Learning Rate: 0.00016927
	LOSS [training: 2.6084139539117235 | validation: 2.5591998151991637]
	TIME [epoch: 8.31 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6054841207226946		[learning rate: 0.00016866]
	Learning Rate: 0.000168655
	LOSS [training: 2.6054841207226946 | validation: 2.560734607016914]
	TIME [epoch: 8.3 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605973416851209		[learning rate: 0.00016804]
	Learning Rate: 0.000168043
	LOSS [training: 2.605973416851209 | validation: 2.551688315209004]
	TIME [epoch: 8.29 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604297878690876		[learning rate: 0.00016743]
	Learning Rate: 0.000167433
	LOSS [training: 2.604297878690876 | validation: 2.5523984531778847]
	TIME [epoch: 8.3 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604671436460725		[learning rate: 0.00016683]
	Learning Rate: 0.000166826
	LOSS [training: 2.604671436460725 | validation: 2.554410205731414]
	TIME [epoch: 8.32 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6080170538704035		[learning rate: 0.00016622]
	Learning Rate: 0.00016622
	LOSS [training: 2.6080170538704035 | validation: 2.560135687666433]
	TIME [epoch: 8.3 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6107100198379922		[learning rate: 0.00016562]
	Learning Rate: 0.000165617
	LOSS [training: 2.6107100198379922 | validation: 2.556252742505798]
	TIME [epoch: 8.3 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6147307853285664		[learning rate: 0.00016502]
	Learning Rate: 0.000165016
	LOSS [training: 2.6147307853285664 | validation: 2.556171754455346]
	TIME [epoch: 8.29 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605702586098035		[learning rate: 0.00016442]
	Learning Rate: 0.000164417
	LOSS [training: 2.605702586098035 | validation: 2.5530703855466506]
	TIME [epoch: 8.32 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6045674744948473		[learning rate: 0.00016382]
	Learning Rate: 0.000163821
	LOSS [training: 2.6045674744948473 | validation: 2.5531809125566385]
	TIME [epoch: 8.3 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6046859993575593		[learning rate: 0.00016323]
	Learning Rate: 0.000163226
	LOSS [training: 2.6046859993575593 | validation: 2.54798953067289]
	TIME [epoch: 8.29 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6119012089808282		[learning rate: 0.00016263]
	Learning Rate: 0.000162634
	LOSS [training: 2.6119012089808282 | validation: 2.5521555297854426]
	TIME [epoch: 8.29 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6052145474483153		[learning rate: 0.00016204]
	Learning Rate: 0.000162043
	LOSS [training: 2.6052145474483153 | validation: 2.5473199238442237]
	TIME [epoch: 8.31 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.607406205085579		[learning rate: 0.00016146]
	Learning Rate: 0.000161455
	LOSS [training: 2.607406205085579 | validation: 2.5533467212424936]
	TIME [epoch: 8.3 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6101174778228176		[learning rate: 0.00016087]
	Learning Rate: 0.000160869
	LOSS [training: 2.6101174778228176 | validation: 2.559770556201685]
	TIME [epoch: 8.29 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6067295377436284		[learning rate: 0.00016029]
	Learning Rate: 0.000160286
	LOSS [training: 2.6067295377436284 | validation: 2.5519756974527494]
	TIME [epoch: 8.29 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.611906063934946		[learning rate: 0.0001597]
	Learning Rate: 0.000159704
	LOSS [training: 2.611906063934946 | validation: 2.5619529113412227]
	TIME [epoch: 8.3 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604553626620576		[learning rate: 0.00015912]
	Learning Rate: 0.000159124
	LOSS [training: 2.604553626620576 | validation: 2.5468229358945913]
	TIME [epoch: 8.32 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.608994068810378		[learning rate: 0.00015855]
	Learning Rate: 0.000158547
	LOSS [training: 2.608994068810378 | validation: 2.5563153654355926]
	TIME [epoch: 8.29 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6096330508048973		[learning rate: 0.00015797]
	Learning Rate: 0.000157972
	LOSS [training: 2.6096330508048973 | validation: 2.5630762743119373]
	TIME [epoch: 8.29 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6118760535368315		[learning rate: 0.0001574]
	Learning Rate: 0.000157398
	LOSS [training: 2.6118760535368315 | validation: 2.558492956095379]
	TIME [epoch: 8.29 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.613411412586613		[learning rate: 0.00015683]
	Learning Rate: 0.000156827
	LOSS [training: 2.613411412586613 | validation: 2.5477019689512357]
	TIME [epoch: 8.32 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605415213574477		[learning rate: 0.00015626]
	Learning Rate: 0.000156258
	LOSS [training: 2.605415213574477 | validation: 2.546191949502391]
	TIME [epoch: 8.3 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599628238364692		[learning rate: 0.00015569]
	Learning Rate: 0.000155691
	LOSS [training: 2.599628238364692 | validation: 2.558086022211312]
	TIME [epoch: 8.29 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6076910198635757		[learning rate: 0.00015513]
	Learning Rate: 0.000155126
	LOSS [training: 2.6076910198635757 | validation: 2.552932447149371]
	TIME [epoch: 8.29 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603592327058915		[learning rate: 0.00015456]
	Learning Rate: 0.000154563
	LOSS [training: 2.603592327058915 | validation: 2.5484986939873875]
	TIME [epoch: 8.3 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6014675283082758		[learning rate: 0.000154]
	Learning Rate: 0.000154002
	LOSS [training: 2.6014675283082758 | validation: 2.5515312370690193]
	TIME [epoch: 8.31 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6019873660353405		[learning rate: 0.00015344]
	Learning Rate: 0.000153443
	LOSS [training: 2.6019873660353405 | validation: 2.5605614733578093]
	TIME [epoch: 8.29 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6024763548640566		[learning rate: 0.00015289]
	Learning Rate: 0.000152886
	LOSS [training: 2.6024763548640566 | validation: 2.5546141915878895]
	TIME [epoch: 8.3 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.614366901990029		[learning rate: 0.00015233]
	Learning Rate: 0.000152331
	LOSS [training: 2.614366901990029 | validation: 2.562762066865843]
	TIME [epoch: 8.29 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.613039592291704		[learning rate: 0.00015178]
	Learning Rate: 0.000151779
	LOSS [training: 2.613039592291704 | validation: 2.5601885183870614]
	TIME [epoch: 8.32 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6062395919249886		[learning rate: 0.00015123]
	Learning Rate: 0.000151228
	LOSS [training: 2.6062395919249886 | validation: 2.5535438078358803]
	TIME [epoch: 8.29 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6072857573038974		[learning rate: 0.00015068]
	Learning Rate: 0.000150679
	LOSS [training: 2.6072857573038974 | validation: 2.5522939983503266]
	TIME [epoch: 8.3 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6063141916474613		[learning rate: 0.00015013]
	Learning Rate: 0.000150132
	LOSS [training: 2.6063141916474613 | validation: 2.549247139498866]
	TIME [epoch: 8.29 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.615450330886167		[learning rate: 0.00014959]
	Learning Rate: 0.000149587
	LOSS [training: 2.615450330886167 | validation: 2.5617678952258087]
	TIME [epoch: 8.32 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6097947860024653		[learning rate: 0.00014904]
	Learning Rate: 0.000149044
	LOSS [training: 2.6097947860024653 | validation: 2.555298935938105]
	TIME [epoch: 8.3 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6073954832748085		[learning rate: 0.0001485]
	Learning Rate: 0.000148504
	LOSS [training: 2.6073954832748085 | validation: 2.553373276835139]
	TIME [epoch: 8.3 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6060093676461866		[learning rate: 0.00014796]
	Learning Rate: 0.000147965
	LOSS [training: 2.6060093676461866 | validation: 2.5662432424494437]
	TIME [epoch: 8.3 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6112634582321954		[learning rate: 0.00014743]
	Learning Rate: 0.000147428
	LOSS [training: 2.6112634582321954 | validation: 2.571618778013685]
	TIME [epoch: 8.3 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6194301611129847		[learning rate: 0.00014689]
	Learning Rate: 0.000146893
	LOSS [training: 2.6194301611129847 | validation: 2.5662507420197347]
	TIME [epoch: 8.31 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6038269845347535		[learning rate: 0.00014636]
	Learning Rate: 0.00014636
	LOSS [training: 2.6038269845347535 | validation: 2.559265305978111]
	TIME [epoch: 8.29 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.612690197953458		[learning rate: 0.00014583]
	Learning Rate: 0.000145828
	LOSS [training: 2.612690197953458 | validation: 2.567971775206685]
	TIME [epoch: 8.29 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6030553810956203		[learning rate: 0.0001453]
	Learning Rate: 0.000145299
	LOSS [training: 2.6030553810956203 | validation: 2.5678433281515787]
	TIME [epoch: 8.3 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.608241391039914		[learning rate: 0.00014477]
	Learning Rate: 0.000144772
	LOSS [training: 2.608241391039914 | validation: 2.5465665909771538]
	TIME [epoch: 8.32 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.607283628292673		[learning rate: 0.00014425]
	Learning Rate: 0.000144247
	LOSS [training: 2.607283628292673 | validation: 2.548616138376453]
	TIME [epoch: 8.3 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604051197292741		[learning rate: 0.00014372]
	Learning Rate: 0.000143723
	LOSS [training: 2.604051197292741 | validation: 2.5545895929700633]
	TIME [epoch: 8.3 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6049158526390643		[learning rate: 0.0001432]
	Learning Rate: 0.000143201
	LOSS [training: 2.6049158526390643 | validation: 2.5533804549356747]
	TIME [epoch: 8.3 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6079824943630583		[learning rate: 0.00014268]
	Learning Rate: 0.000142682
	LOSS [training: 2.6079824943630583 | validation: 2.5600369895272364]
	TIME [epoch: 8.31 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6124882686775637		[learning rate: 0.00014216]
	Learning Rate: 0.000142164
	LOSS [training: 2.6124882686775637 | validation: 2.5640953423803112]
	TIME [epoch: 8.3 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6056026563404644		[learning rate: 0.00014165]
	Learning Rate: 0.000141648
	LOSS [training: 2.6056026563404644 | validation: 2.5484703108298294]
	TIME [epoch: 8.29 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604970080067528		[learning rate: 0.00014113]
	Learning Rate: 0.000141134
	LOSS [training: 2.604970080067528 | validation: 2.5521413266330955]
	TIME [epoch: 8.29 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6090151700055495		[learning rate: 0.00014062]
	Learning Rate: 0.000140622
	LOSS [training: 2.6090151700055495 | validation: 2.553930823039922]
	TIME [epoch: 8.29 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6072368285620477		[learning rate: 0.00014011]
	Learning Rate: 0.000140112
	LOSS [training: 2.6072368285620477 | validation: 2.5535958246886223]
	TIME [epoch: 8.32 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6180780372301387		[learning rate: 0.0001396]
	Learning Rate: 0.000139603
	LOSS [training: 2.6180780372301387 | validation: 2.550389276060237]
	TIME [epoch: 8.29 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605389842221262		[learning rate: 0.0001391]
	Learning Rate: 0.000139096
	LOSS [training: 2.605389842221262 | validation: 2.553485003434977]
	TIME [epoch: 8.29 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.608176816530709		[learning rate: 0.00013859]
	Learning Rate: 0.000138592
	LOSS [training: 2.608176816530709 | validation: 2.562283066812676]
	TIME [epoch: 8.29 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606523349707394		[learning rate: 0.00013809]
	Learning Rate: 0.000138089
	LOSS [training: 2.606523349707394 | validation: 2.557937786063082]
	TIME [epoch: 8.31 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605796524572456		[learning rate: 0.00013759]
	Learning Rate: 0.000137587
	LOSS [training: 2.605796524572456 | validation: 2.5540770083206024]
	TIME [epoch: 8.29 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6040849333005487		[learning rate: 0.00013709]
	Learning Rate: 0.000137088
	LOSS [training: 2.6040849333005487 | validation: 2.5608985956387356]
	TIME [epoch: 8.29 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601705280481256		[learning rate: 0.00013659]
	Learning Rate: 0.000136591
	LOSS [training: 2.601705280481256 | validation: 2.5565605699662095]
	TIME [epoch: 8.29 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6078304952191487		[learning rate: 0.00013609]
	Learning Rate: 0.000136095
	LOSS [training: 2.6078304952191487 | validation: 2.556498414866016]
	TIME [epoch: 8.31 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.610913026480836		[learning rate: 0.0001356]
	Learning Rate: 0.000135601
	LOSS [training: 2.610913026480836 | validation: 2.563560350895375]
	TIME [epoch: 8.3 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6048170336975844		[learning rate: 0.00013511]
	Learning Rate: 0.000135109
	LOSS [training: 2.6048170336975844 | validation: 2.5491780124439902]
	TIME [epoch: 8.29 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60284420539167		[learning rate: 0.00013462]
	Learning Rate: 0.000134619
	LOSS [training: 2.60284420539167 | validation: 2.556750693375525]
	TIME [epoch: 8.29 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6081217104062544		[learning rate: 0.00013413]
	Learning Rate: 0.00013413
	LOSS [training: 2.6081217104062544 | validation: 2.5547413887089925]
	TIME [epoch: 8.29 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6038159564397025		[learning rate: 0.00013364]
	Learning Rate: 0.000133643
	LOSS [training: 2.6038159564397025 | validation: 2.5473047527967756]
	TIME [epoch: 8.31 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606344401269912		[learning rate: 0.00013316]
	Learning Rate: 0.000133158
	LOSS [training: 2.606344401269912 | validation: 2.557737383794122]
	TIME [epoch: 8.29 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6053919001048365		[learning rate: 0.00013268]
	Learning Rate: 0.000132675
	LOSS [training: 2.6053919001048365 | validation: 2.565033937454201]
	TIME [epoch: 8.29 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.609697485797808		[learning rate: 0.00013219]
	Learning Rate: 0.000132194
	LOSS [training: 2.609697485797808 | validation: 2.551206579839784]
	TIME [epoch: 8.29 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6019271973735014		[learning rate: 0.00013171]
	Learning Rate: 0.000131714
	LOSS [training: 2.6019271973735014 | validation: 2.5493268615710076]
	TIME [epoch: 8.31 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6096178512648436		[learning rate: 0.00013124]
	Learning Rate: 0.000131236
	LOSS [training: 2.6096178512648436 | validation: 2.5611344796288305]
	TIME [epoch: 8.29 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604501909525573		[learning rate: 0.00013076]
	Learning Rate: 0.00013076
	LOSS [training: 2.604501909525573 | validation: 2.558009193219206]
	TIME [epoch: 8.29 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600500442694625		[learning rate: 0.00013029]
	Learning Rate: 0.000130285
	LOSS [training: 2.600500442694625 | validation: 2.5533166732767842]
	TIME [epoch: 8.29 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6090060087123126		[learning rate: 0.00012981]
	Learning Rate: 0.000129812
	LOSS [training: 2.6090060087123126 | validation: 2.558766473593251]
	TIME [epoch: 8.3 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6075065641363593		[learning rate: 0.00012934]
	Learning Rate: 0.000129341
	LOSS [training: 2.6075065641363593 | validation: 2.5563662525881785]
	TIME [epoch: 8.3 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6085360991086284		[learning rate: 0.00012887]
	Learning Rate: 0.000128872
	LOSS [training: 2.6085360991086284 | validation: 2.559578426989445]
	TIME [epoch: 8.29 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606249758613333		[learning rate: 0.0001284]
	Learning Rate: 0.000128404
	LOSS [training: 2.606249758613333 | validation: 2.5526871294403795]
	TIME [epoch: 8.29 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.607117261955795		[learning rate: 0.00012794]
	Learning Rate: 0.000127938
	LOSS [training: 2.607117261955795 | validation: 2.563385366190733]
	TIME [epoch: 8.29 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6086098335485772		[learning rate: 0.00012747]
	Learning Rate: 0.000127474
	LOSS [training: 2.6086098335485772 | validation: 2.5623051690144756]
	TIME [epoch: 8.31 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.610076127608006		[learning rate: 0.00012701]
	Learning Rate: 0.000127011
	LOSS [training: 2.610076127608006 | validation: 2.5568477538451346]
	TIME [epoch: 8.28 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6091452426010635		[learning rate: 0.00012655]
	Learning Rate: 0.00012655
	LOSS [training: 2.6091452426010635 | validation: 2.5766234389133036]
	TIME [epoch: 8.29 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6113554049883803		[learning rate: 0.00012609]
	Learning Rate: 0.000126091
	LOSS [training: 2.6113554049883803 | validation: 2.55510414475618]
	TIME [epoch: 8.29 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6057341650638914		[learning rate: 0.00012563]
	Learning Rate: 0.000125633
	LOSS [training: 2.6057341650638914 | validation: 2.556469528817744]
	TIME [epoch: 8.31 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605385016632608		[learning rate: 0.00012518]
	Learning Rate: 0.000125178
	LOSS [training: 2.605385016632608 | validation: 2.5587132015821306]
	TIME [epoch: 8.29 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6063645731918217		[learning rate: 0.00012472]
	Learning Rate: 0.000124723
	LOSS [training: 2.6063645731918217 | validation: 2.546508828462044]
	TIME [epoch: 8.29 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6019116050639095		[learning rate: 0.00012427]
	Learning Rate: 0.000124271
	LOSS [training: 2.6019116050639095 | validation: 2.5605446959317995]
	TIME [epoch: 8.29 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6060211713430688		[learning rate: 0.00012382]
	Learning Rate: 0.00012382
	LOSS [training: 2.6060211713430688 | validation: 2.5531936412321263]
	TIME [epoch: 8.3 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605903079777837		[learning rate: 0.00012337]
	Learning Rate: 0.00012337
	LOSS [training: 2.605903079777837 | validation: 2.5523455166236935]
	TIME [epoch: 8.3 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602938655822367		[learning rate: 0.00012292]
	Learning Rate: 0.000122923
	LOSS [training: 2.602938655822367 | validation: 2.5502356758804265]
	TIME [epoch: 8.29 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6093946452618		[learning rate: 0.00012248]
	Learning Rate: 0.000122476
	LOSS [training: 2.6093946452618 | validation: 2.5585689705637704]
	TIME [epoch: 8.28 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606931787631833		[learning rate: 0.00012203]
	Learning Rate: 0.000122032
	LOSS [training: 2.606931787631833 | validation: 2.5632052920392883]
	TIME [epoch: 8.29 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598717220232893		[learning rate: 0.00012159]
	Learning Rate: 0.000121589
	LOSS [training: 2.598717220232893 | validation: 2.557529607954259]
	TIME [epoch: 8.31 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605572155653101		[learning rate: 0.00012115]
	Learning Rate: 0.000121148
	LOSS [training: 2.605572155653101 | validation: 2.5588230985161564]
	TIME [epoch: 8.28 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6065705345341303		[learning rate: 0.00012071]
	Learning Rate: 0.000120708
	LOSS [training: 2.6065705345341303 | validation: 2.5593315423325675]
	TIME [epoch: 8.29 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6061022653314345		[learning rate: 0.00012027]
	Learning Rate: 0.00012027
	LOSS [training: 2.6061022653314345 | validation: 2.5477646770570423]
	TIME [epoch: 8.28 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6007261466981673		[learning rate: 0.00011983]
	Learning Rate: 0.000119834
	LOSS [training: 2.6007261466981673 | validation: 2.5591137618989688]
	TIME [epoch: 8.31 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.608146057095938		[learning rate: 0.0001194]
	Learning Rate: 0.000119399
	LOSS [training: 2.608146057095938 | validation: 2.5513657683410176]
	TIME [epoch: 8.29 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6081899486223876		[learning rate: 0.00011897]
	Learning Rate: 0.000118966
	LOSS [training: 2.6081899486223876 | validation: 2.5428377592030746]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_1319.pth
	Model improved!!!
EPOCH 1320/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6040973500265965		[learning rate: 0.00011853]
	Learning Rate: 0.000118534
	LOSS [training: 2.6040973500265965 | validation: 2.543249362145436]
	TIME [epoch: 8.29 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6025187313510108		[learning rate: 0.0001181]
	Learning Rate: 0.000118104
	LOSS [training: 2.6025187313510108 | validation: 2.557445418262174]
	TIME [epoch: 8.3 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606071513799228		[learning rate: 0.00011767]
	Learning Rate: 0.000117675
	LOSS [training: 2.606071513799228 | validation: 2.554729675695146]
	TIME [epoch: 8.3 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601791650789731		[learning rate: 0.00011725]
	Learning Rate: 0.000117248
	LOSS [training: 2.601791650789731 | validation: 2.5541226779269826]
	TIME [epoch: 8.29 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6045889921612626		[learning rate: 0.00011682]
	Learning Rate: 0.000116822
	LOSS [training: 2.6045889921612626 | validation: 2.5540868079431407]
	TIME [epoch: 8.29 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6005888133459547		[learning rate: 0.0001164]
	Learning Rate: 0.000116398
	LOSS [training: 2.6005888133459547 | validation: 2.561597865535081]
	TIME [epoch: 8.29 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5988927725845636		[learning rate: 0.00011598]
	Learning Rate: 0.000115976
	LOSS [training: 2.5988927725845636 | validation: 2.5492598502823496]
	TIME [epoch: 8.32 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5996226760552377		[learning rate: 0.00011556]
	Learning Rate: 0.000115555
	LOSS [training: 2.5996226760552377 | validation: 2.554272982026549]
	TIME [epoch: 8.29 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6078486101436797		[learning rate: 0.00011514]
	Learning Rate: 0.000115136
	LOSS [training: 2.6078486101436797 | validation: 2.551897996862902]
	TIME [epoch: 8.29 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.609825325228493		[learning rate: 0.00011472]
	Learning Rate: 0.000114718
	LOSS [training: 2.609825325228493 | validation: 2.5534154338706143]
	TIME [epoch: 8.29 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6043905868192794		[learning rate: 0.0001143]
	Learning Rate: 0.000114302
	LOSS [training: 2.6043905868192794 | validation: 2.5557143898360435]
	TIME [epoch: 8.31 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6053518146956365		[learning rate: 0.00011389]
	Learning Rate: 0.000113887
	LOSS [training: 2.6053518146956365 | validation: 2.548531024115623]
	TIME [epoch: 8.29 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6080810175164046		[learning rate: 0.00011347]
	Learning Rate: 0.000113474
	LOSS [training: 2.6080810175164046 | validation: 2.553904086362676]
	TIME [epoch: 8.29 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6014832222023356		[learning rate: 0.00011306]
	Learning Rate: 0.000113062
	LOSS [training: 2.6014832222023356 | validation: 2.553531261252304]
	TIME [epoch: 8.29 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6018810417955534		[learning rate: 0.00011265]
	Learning Rate: 0.000112651
	LOSS [training: 2.6018810417955534 | validation: 2.5590261599908937]
	TIME [epoch: 8.3 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603588620330219		[learning rate: 0.00011224]
	Learning Rate: 0.000112243
	LOSS [training: 2.603588620330219 | validation: 2.5409001432788934]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_1335.pth
	Model improved!!!
EPOCH 1336/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6048761362809953		[learning rate: 0.00011184]
	Learning Rate: 0.000111835
	LOSS [training: 2.6048761362809953 | validation: 2.5534917198341986]
	TIME [epoch: 8.29 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604983185914236		[learning rate: 0.00011143]
	Learning Rate: 0.000111429
	LOSS [training: 2.604983185914236 | validation: 2.550698857535668]
	TIME [epoch: 8.29 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6047629574461353		[learning rate: 0.00011103]
	Learning Rate: 0.000111025
	LOSS [training: 2.6047629574461353 | validation: 2.561031027276419]
	TIME [epoch: 8.28 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6017474549493356		[learning rate: 0.00011062]
	Learning Rate: 0.000110622
	LOSS [training: 2.6017474549493356 | validation: 2.563708750481715]
	TIME [epoch: 8.31 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6088028771511587		[learning rate: 0.00011022]
	Learning Rate: 0.000110221
	LOSS [training: 2.6088028771511587 | validation: 2.5551337829491443]
	TIME [epoch: 8.29 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598466305104323		[learning rate: 0.00010982]
	Learning Rate: 0.000109821
	LOSS [training: 2.598466305104323 | validation: 2.5552271666230135]
	TIME [epoch: 8.29 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603182994664781		[learning rate: 0.00010942]
	Learning Rate: 0.000109422
	LOSS [training: 2.603182994664781 | validation: 2.5460468650854304]
	TIME [epoch: 8.29 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6048506949250982		[learning rate: 0.00010903]
	Learning Rate: 0.000109025
	LOSS [training: 2.6048506949250982 | validation: 2.568031222677953]
	TIME [epoch: 8.31 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.610031331416643		[learning rate: 0.00010863]
	Learning Rate: 0.000108629
	LOSS [training: 2.610031331416643 | validation: 2.5504842874453275]
	TIME [epoch: 8.29 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6067447582449303		[learning rate: 0.00010824]
	Learning Rate: 0.000108235
	LOSS [training: 2.6067447582449303 | validation: 2.543679405275997]
	TIME [epoch: 8.29 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60406977274748		[learning rate: 0.00010784]
	Learning Rate: 0.000107842
	LOSS [training: 2.60406977274748 | validation: 2.558847046075268]
	TIME [epoch: 8.29 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6032348733755244		[learning rate: 0.00010745]
	Learning Rate: 0.000107451
	LOSS [training: 2.6032348733755244 | validation: 2.5648683602085445]
	TIME [epoch: 8.29 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6062183471568696		[learning rate: 0.00010706]
	Learning Rate: 0.000107061
	LOSS [training: 2.6062183471568696 | validation: 2.557625779175086]
	TIME [epoch: 8.3 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5976415206084273		[learning rate: 0.00010667]
	Learning Rate: 0.000106673
	LOSS [training: 2.5976415206084273 | validation: 2.5536034336439215]
	TIME [epoch: 8.28 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6047149037906805		[learning rate: 0.00010629]
	Learning Rate: 0.000106285
	LOSS [training: 2.6047149037906805 | validation: 2.5528091871841365]
	TIME [epoch: 8.29 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604853678948822		[learning rate: 0.0001059]
	Learning Rate: 0.0001059
	LOSS [training: 2.604853678948822 | validation: 2.553306911036052]
	TIME [epoch: 8.29 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603343663992415		[learning rate: 0.00010552]
	Learning Rate: 0.000105515
	LOSS [training: 2.603343663992415 | validation: 2.5667876161704006]
	TIME [epoch: 8.31 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6076070772436752		[learning rate: 0.00010513]
	Learning Rate: 0.000105132
	LOSS [training: 2.6076070772436752 | validation: 2.5578801859422304]
	TIME [epoch: 8.28 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604891590494821		[learning rate: 0.00010475]
	Learning Rate: 0.000104751
	LOSS [training: 2.604891590494821 | validation: 2.5541916979831987]
	TIME [epoch: 8.29 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6043716282870664		[learning rate: 0.00010437]
	Learning Rate: 0.000104371
	LOSS [training: 2.6043716282870664 | validation: 2.5548530116316535]
	TIME [epoch: 8.28 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.610182007851182		[learning rate: 0.00010399]
	Learning Rate: 0.000103992
	LOSS [training: 2.610182007851182 | validation: 2.5515922557461503]
	TIME [epoch: 8.31 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602780838414975		[learning rate: 0.00010361]
	Learning Rate: 0.000103615
	LOSS [training: 2.602780838414975 | validation: 2.554922427342877]
	TIME [epoch: 8.29 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6035844461172504		[learning rate: 0.00010324]
	Learning Rate: 0.000103239
	LOSS [training: 2.6035844461172504 | validation: 2.546395873054891]
	TIME [epoch: 8.28 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6078492486620224		[learning rate: 0.00010286]
	Learning Rate: 0.000102864
	LOSS [training: 2.6078492486620224 | validation: 2.557371697018026]
	TIME [epoch: 8.29 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6038894157487085		[learning rate: 0.00010249]
	Learning Rate: 0.000102491
	LOSS [training: 2.6038894157487085 | validation: 2.559719407098993]
	TIME [epoch: 8.29 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602932009725337		[learning rate: 0.00010212]
	Learning Rate: 0.000102119
	LOSS [training: 2.602932009725337 | validation: 2.5504119390905746]
	TIME [epoch: 8.3 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600835535499147		[learning rate: 0.00010175]
	Learning Rate: 0.000101748
	LOSS [training: 2.600835535499147 | validation: 2.552544643639968]
	TIME [epoch: 8.29 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60704935741379		[learning rate: 0.00010138]
	Learning Rate: 0.000101379
	LOSS [training: 2.60704935741379 | validation: 2.5481837134158023]
	TIME [epoch: 8.28 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606447704903696		[learning rate: 0.00010101]
	Learning Rate: 0.000101011
	LOSS [training: 2.606447704903696 | validation: 2.562482215652903]
	TIME [epoch: 8.29 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606183950749402		[learning rate: 0.00010064]
	Learning Rate: 0.000100644
	LOSS [training: 2.606183950749402 | validation: 2.5558567386358213]
	TIME [epoch: 8.31 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6095383488868107		[learning rate: 0.00010028]
	Learning Rate: 0.000100279
	LOSS [training: 2.6095383488868107 | validation: 2.5573803798547683]
	TIME [epoch: 8.29 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6058087265776098		[learning rate: 9.9915e-05]
	Learning Rate: 9.99152e-05
	LOSS [training: 2.6058087265776098 | validation: 2.559752638179317]
	TIME [epoch: 8.29 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.612096117772393		[learning rate: 9.9553e-05]
	Learning Rate: 9.95526e-05
	LOSS [training: 2.612096117772393 | validation: 2.559054601876043]
	TIME [epoch: 8.28 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602551099217473		[learning rate: 9.9191e-05]
	Learning Rate: 9.91913e-05
	LOSS [training: 2.602551099217473 | validation: 2.558597019283015]
	TIME [epoch: 8.3 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6047353560154405		[learning rate: 9.8831e-05]
	Learning Rate: 9.88314e-05
	LOSS [training: 2.6047353560154405 | validation: 2.5583147320033897]
	TIME [epoch: 8.29 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602272920831166		[learning rate: 9.8473e-05]
	Learning Rate: 9.84727e-05
	LOSS [training: 2.602272920831166 | validation: 2.558362249703139]
	TIME [epoch: 8.29 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602061432685341		[learning rate: 9.8115e-05]
	Learning Rate: 9.81153e-05
	LOSS [training: 2.602061432685341 | validation: 2.556084250357392]
	TIME [epoch: 8.28 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604171474787908		[learning rate: 9.7759e-05]
	Learning Rate: 9.77592e-05
	LOSS [training: 2.604171474787908 | validation: 2.561832290386822]
	TIME [epoch: 8.29 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605137197142974		[learning rate: 9.7404e-05]
	Learning Rate: 9.74045e-05
	LOSS [training: 2.605137197142974 | validation: 2.5515398354354057]
	TIME [epoch: 8.31 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6034275183326256		[learning rate: 9.7051e-05]
	Learning Rate: 9.7051e-05
	LOSS [training: 2.6034275183326256 | validation: 2.5578339041511153]
	TIME [epoch: 8.29 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6036925645882034		[learning rate: 9.6699e-05]
	Learning Rate: 9.66988e-05
	LOSS [training: 2.6036925645882034 | validation: 2.5538741008081596]
	TIME [epoch: 8.28 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60178515779157		[learning rate: 9.6348e-05]
	Learning Rate: 9.63479e-05
	LOSS [training: 2.60178515779157 | validation: 2.5594283126606676]
	TIME [epoch: 8.29 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601564676313484		[learning rate: 9.5998e-05]
	Learning Rate: 9.59982e-05
	LOSS [training: 2.601564676313484 | validation: 2.5594002176793618]
	TIME [epoch: 8.31 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.607140339154304		[learning rate: 9.565e-05]
	Learning Rate: 9.56499e-05
	LOSS [training: 2.607140339154304 | validation: 2.5592231045737]
	TIME [epoch: 8.29 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6011130805196077		[learning rate: 9.5303e-05]
	Learning Rate: 9.53027e-05
	LOSS [training: 2.6011130805196077 | validation: 2.5514380366546225]
	TIME [epoch: 8.28 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.607264697027163		[learning rate: 9.4957e-05]
	Learning Rate: 9.49569e-05
	LOSS [training: 2.607264697027163 | validation: 2.564047541888847]
	TIME [epoch: 8.29 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6038701009274083		[learning rate: 9.4612e-05]
	Learning Rate: 9.46122e-05
	LOSS [training: 2.6038701009274083 | validation: 2.5529811938411013]
	TIME [epoch: 8.3 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605974536586553		[learning rate: 9.4269e-05]
	Learning Rate: 9.42689e-05
	LOSS [training: 2.605974536586553 | validation: 2.565894350077744]
	TIME [epoch: 8.29 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6052659612607365		[learning rate: 9.3927e-05]
	Learning Rate: 9.39268e-05
	LOSS [training: 2.6052659612607365 | validation: 2.5714192248013115]
	TIME [epoch: 8.28 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6119457138972395		[learning rate: 9.3586e-05]
	Learning Rate: 9.35859e-05
	LOSS [training: 2.6119457138972395 | validation: 2.5599359265230612]
	TIME [epoch: 8.28 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600515851190745		[learning rate: 9.3246e-05]
	Learning Rate: 9.32463e-05
	LOSS [training: 2.600515851190745 | validation: 2.557210189296345]
	TIME [epoch: 8.28 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60715355864457		[learning rate: 9.2908e-05]
	Learning Rate: 9.29079e-05
	LOSS [training: 2.60715355864457 | validation: 2.556340050302399]
	TIME [epoch: 8.31 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605931435406008		[learning rate: 9.2571e-05]
	Learning Rate: 9.25707e-05
	LOSS [training: 2.605931435406008 | validation: 2.5556802490291517]
	TIME [epoch: 8.29 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603147773184822		[learning rate: 9.2235e-05]
	Learning Rate: 9.22348e-05
	LOSS [training: 2.603147773184822 | validation: 2.560023661160124]
	TIME [epoch: 8.29 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6023385616745314		[learning rate: 9.19e-05]
	Learning Rate: 9.19001e-05
	LOSS [training: 2.6023385616745314 | validation: 2.5450599769511277]
	TIME [epoch: 8.28 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600656292600223		[learning rate: 9.1567e-05]
	Learning Rate: 9.15665e-05
	LOSS [training: 2.600656292600223 | validation: 2.5608067427239107]
	TIME [epoch: 8.31 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.607431369755698		[learning rate: 9.1234e-05]
	Learning Rate: 9.12343e-05
	LOSS [training: 2.607431369755698 | validation: 2.555782085560568]
	TIME [epoch: 8.29 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6038064706892703		[learning rate: 9.0903e-05]
	Learning Rate: 9.09031e-05
	LOSS [training: 2.6038064706892703 | validation: 2.5535476259614756]
	TIME [epoch: 8.28 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5960708318472685		[learning rate: 9.0573e-05]
	Learning Rate: 9.05733e-05
	LOSS [training: 2.5960708318472685 | validation: 2.561679607438581]
	TIME [epoch: 8.28 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602874604757999		[learning rate: 9.0245e-05]
	Learning Rate: 9.02446e-05
	LOSS [training: 2.602874604757999 | validation: 2.5548009528913593]
	TIME [epoch: 8.3 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605610519309785		[learning rate: 8.9917e-05]
	Learning Rate: 8.99171e-05
	LOSS [training: 2.605610519309785 | validation: 2.55144655568341]
	TIME [epoch: 8.3 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6035790312966807		[learning rate: 8.9591e-05]
	Learning Rate: 8.95908e-05
	LOSS [training: 2.6035790312966807 | validation: 2.546221064967488]
	TIME [epoch: 8.29 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.609121209094929		[learning rate: 8.9266e-05]
	Learning Rate: 8.92656e-05
	LOSS [training: 2.609121209094929 | validation: 2.5630588016523244]
	TIME [epoch: 8.29 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602602810344589		[learning rate: 8.8942e-05]
	Learning Rate: 8.89417e-05
	LOSS [training: 2.602602810344589 | validation: 2.557990948810312]
	TIME [epoch: 8.28 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600338072981864		[learning rate: 8.8619e-05]
	Learning Rate: 8.86189e-05
	LOSS [training: 2.600338072981864 | validation: 2.5486887183040645]
	TIME [epoch: 8.31 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605382035988219		[learning rate: 8.8297e-05]
	Learning Rate: 8.82973e-05
	LOSS [training: 2.605382035988219 | validation: 2.5638291857626863]
	TIME [epoch: 8.28 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6059621300187237		[learning rate: 8.7977e-05]
	Learning Rate: 8.79769e-05
	LOSS [training: 2.6059621300187237 | validation: 2.5593476799274244]
	TIME [epoch: 8.28 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6103313567382753		[learning rate: 8.7658e-05]
	Learning Rate: 8.76576e-05
	LOSS [training: 2.6103313567382753 | validation: 2.5459099743083016]
	TIME [epoch: 8.28 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606900848742485		[learning rate: 8.7339e-05]
	Learning Rate: 8.73395e-05
	LOSS [training: 2.606900848742485 | validation: 2.55788775412541]
	TIME [epoch: 8.31 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6051880048596665		[learning rate: 8.7023e-05]
	Learning Rate: 8.70225e-05
	LOSS [training: 2.6051880048596665 | validation: 2.557479655294324]
	TIME [epoch: 8.29 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.607403711358932		[learning rate: 8.6707e-05]
	Learning Rate: 8.67067e-05
	LOSS [training: 2.607403711358932 | validation: 2.5541576190006987]
	TIME [epoch: 8.28 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6046265167034486		[learning rate: 8.6392e-05]
	Learning Rate: 8.63921e-05
	LOSS [training: 2.6046265167034486 | validation: 2.5569888560016643]
	TIME [epoch: 8.29 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6051428935662706		[learning rate: 8.6079e-05]
	Learning Rate: 8.60785e-05
	LOSS [training: 2.6051428935662706 | validation: 2.5528026824690544]
	TIME [epoch: 8.3 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597118938174444		[learning rate: 8.5766e-05]
	Learning Rate: 8.57661e-05
	LOSS [training: 2.597118938174444 | validation: 2.5639153387886093]
	TIME [epoch: 8.3 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6006981524709682		[learning rate: 8.5455e-05]
	Learning Rate: 8.54549e-05
	LOSS [training: 2.6006981524709682 | validation: 2.5537709476592454]
	TIME [epoch: 8.28 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599241849393569		[learning rate: 8.5145e-05]
	Learning Rate: 8.51448e-05
	LOSS [training: 2.599241849393569 | validation: 2.5524319438343155]
	TIME [epoch: 8.29 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600824693252476		[learning rate: 8.4836e-05]
	Learning Rate: 8.48358e-05
	LOSS [training: 2.600824693252476 | validation: 2.5635211566081324]
	TIME [epoch: 8.28 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6021013563403175		[learning rate: 8.4528e-05]
	Learning Rate: 8.45279e-05
	LOSS [training: 2.6021013563403175 | validation: 2.5531680198234854]
	TIME [epoch: 8.31 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605781985295036		[learning rate: 8.4221e-05]
	Learning Rate: 8.42211e-05
	LOSS [training: 2.605781985295036 | validation: 2.5581624704131167]
	TIME [epoch: 8.29 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6030903862125028		[learning rate: 8.3916e-05]
	Learning Rate: 8.39155e-05
	LOSS [training: 2.6030903862125028 | validation: 2.564345109559367]
	TIME [epoch: 8.28 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5999258897575657		[learning rate: 8.3611e-05]
	Learning Rate: 8.3611e-05
	LOSS [training: 2.5999258897575657 | validation: 2.555015358834839]
	TIME [epoch: 8.29 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6031198904195265		[learning rate: 8.3308e-05]
	Learning Rate: 8.33075e-05
	LOSS [training: 2.6031198904195265 | validation: 2.5514490323726142]
	TIME [epoch: 8.3 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60308137693666		[learning rate: 8.3005e-05]
	Learning Rate: 8.30052e-05
	LOSS [training: 2.60308137693666 | validation: 2.561866136151779]
	TIME [epoch: 8.28 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.609027273473093		[learning rate: 8.2704e-05]
	Learning Rate: 8.2704e-05
	LOSS [training: 2.609027273473093 | validation: 2.561088575768416]
	TIME [epoch: 8.28 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601084390744391		[learning rate: 8.2404e-05]
	Learning Rate: 8.24038e-05
	LOSS [training: 2.601084390744391 | validation: 2.5497992931628986]
	TIME [epoch: 8.28 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601841768890547		[learning rate: 8.2105e-05]
	Learning Rate: 8.21048e-05
	LOSS [training: 2.601841768890547 | validation: 2.548674763182084]
	TIME [epoch: 8.29 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6035658221911877		[learning rate: 8.1807e-05]
	Learning Rate: 8.18068e-05
	LOSS [training: 2.6035658221911877 | validation: 2.5574908003549854]
	TIME [epoch: 8.3 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6027379053807804		[learning rate: 8.151e-05]
	Learning Rate: 8.15099e-05
	LOSS [training: 2.6027379053807804 | validation: 2.546359761412914]
	TIME [epoch: 8.28 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6036194011368434		[learning rate: 8.1214e-05]
	Learning Rate: 8.12142e-05
	LOSS [training: 2.6036194011368434 | validation: 2.5586508368606746]
	TIME [epoch: 8.28 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602368256701257		[learning rate: 8.0919e-05]
	Learning Rate: 8.09194e-05
	LOSS [training: 2.602368256701257 | validation: 2.5487138688304523]
	TIME [epoch: 8.29 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600933024605262		[learning rate: 8.0626e-05]
	Learning Rate: 8.06257e-05
	LOSS [training: 2.600933024605262 | validation: 2.547140433883957]
	TIME [epoch: 8.31 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602328144613012		[learning rate: 8.0333e-05]
	Learning Rate: 8.03331e-05
	LOSS [training: 2.602328144613012 | validation: 2.5457121413501738]
	TIME [epoch: 8.29 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604541051531537		[learning rate: 8.0042e-05]
	Learning Rate: 8.00416e-05
	LOSS [training: 2.604541051531537 | validation: 2.552227111652149]
	TIME [epoch: 8.28 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602888257684566		[learning rate: 7.9751e-05]
	Learning Rate: 7.97511e-05
	LOSS [training: 2.602888257684566 | validation: 2.554251350052737]
	TIME [epoch: 8.28 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600517582626274		[learning rate: 7.9462e-05]
	Learning Rate: 7.94617e-05
	LOSS [training: 2.600517582626274 | validation: 2.5518436612835433]
	TIME [epoch: 8.3 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603351745503784		[learning rate: 7.9173e-05]
	Learning Rate: 7.91733e-05
	LOSS [training: 2.603351745503784 | validation: 2.5520016850389373]
	TIME [epoch: 8.29 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606987484697466		[learning rate: 7.8886e-05]
	Learning Rate: 7.8886e-05
	LOSS [training: 2.606987484697466 | validation: 2.549790003315187]
	TIME [epoch: 8.29 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603214387717247		[learning rate: 7.86e-05]
	Learning Rate: 7.85997e-05
	LOSS [training: 2.603214387717247 | validation: 2.555034291953855]
	TIME [epoch: 8.29 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6034374895152537		[learning rate: 7.8315e-05]
	Learning Rate: 7.83145e-05
	LOSS [training: 2.6034374895152537 | validation: 2.5657853738680076]
	TIME [epoch: 8.3 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60195625087265		[learning rate: 7.803e-05]
	Learning Rate: 7.80303e-05
	LOSS [training: 2.60195625087265 | validation: 2.5508847230816567]
	TIME [epoch: 8.31 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600204905478488		[learning rate: 7.7747e-05]
	Learning Rate: 7.77471e-05
	LOSS [training: 2.600204905478488 | validation: 2.5392429423042397]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_1436.pth
	Model improved!!!
EPOCH 1437/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6084083059866914		[learning rate: 7.7465e-05]
	Learning Rate: 7.7465e-05
	LOSS [training: 2.6084083059866914 | validation: 2.563892944285622]
	TIME [epoch: 8.28 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6050148093687473		[learning rate: 7.7184e-05]
	Learning Rate: 7.71838e-05
	LOSS [training: 2.6050148093687473 | validation: 2.549787402992863]
	TIME [epoch: 8.29 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6002428262339388		[learning rate: 7.6904e-05]
	Learning Rate: 7.69037e-05
	LOSS [training: 2.6002428262339388 | validation: 2.5497631309116136]
	TIME [epoch: 8.3 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5984485584589754		[learning rate: 7.6625e-05]
	Learning Rate: 7.66246e-05
	LOSS [training: 2.5984485584589754 | validation: 2.5484863798630917]
	TIME [epoch: 8.28 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600422524975209		[learning rate: 7.6347e-05]
	Learning Rate: 7.63466e-05
	LOSS [training: 2.600422524975209 | validation: 2.5682928672533905]
	TIME [epoch: 8.28 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6028383120578065		[learning rate: 7.607e-05]
	Learning Rate: 7.60695e-05
	LOSS [training: 2.6028383120578065 | validation: 2.551633950280168]
	TIME [epoch: 8.28 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601215217871266		[learning rate: 7.5793e-05]
	Learning Rate: 7.57935e-05
	LOSS [training: 2.601215217871266 | validation: 2.5509416451713296]
	TIME [epoch: 8.3 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603295065925237		[learning rate: 7.5518e-05]
	Learning Rate: 7.55184e-05
	LOSS [training: 2.603295065925237 | validation: 2.5373681904440435]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_1444.pth
	Model improved!!!
EPOCH 1445/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604634066294542		[learning rate: 7.5244e-05]
	Learning Rate: 7.52443e-05
	LOSS [training: 2.604634066294542 | validation: 2.5544717661207796]
	TIME [epoch: 8.29 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6030875576482018		[learning rate: 7.4971e-05]
	Learning Rate: 7.49712e-05
	LOSS [training: 2.6030875576482018 | validation: 2.54453256254299]
	TIME [epoch: 8.28 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6023173188657713		[learning rate: 7.4699e-05]
	Learning Rate: 7.46992e-05
	LOSS [training: 2.6023173188657713 | validation: 2.562008313224315]
	TIME [epoch: 8.3 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602365353820434		[learning rate: 7.4428e-05]
	Learning Rate: 7.44281e-05
	LOSS [training: 2.602365353820434 | validation: 2.555345184165434]
	TIME [epoch: 8.31 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6068554909310553		[learning rate: 7.4158e-05]
	Learning Rate: 7.4158e-05
	LOSS [training: 2.6068554909310553 | validation: 2.5553741337085443]
	TIME [epoch: 8.28 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605786151811478		[learning rate: 7.3889e-05]
	Learning Rate: 7.38889e-05
	LOSS [training: 2.605786151811478 | validation: 2.554776257032632]
	TIME [epoch: 8.28 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6048386895723974		[learning rate: 7.3621e-05]
	Learning Rate: 7.36207e-05
	LOSS [training: 2.6048386895723974 | validation: 2.5534339945631985]
	TIME [epoch: 8.28 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6039238430918084		[learning rate: 7.3354e-05]
	Learning Rate: 7.33536e-05
	LOSS [training: 2.6039238430918084 | validation: 2.558539130322641]
	TIME [epoch: 8.31 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6007626439824074		[learning rate: 7.3087e-05]
	Learning Rate: 7.30873e-05
	LOSS [training: 2.6007626439824074 | validation: 2.5510685492693854]
	TIME [epoch: 8.28 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600132721102811		[learning rate: 7.2822e-05]
	Learning Rate: 7.28221e-05
	LOSS [training: 2.600132721102811 | validation: 2.5507940016110515]
	TIME [epoch: 8.29 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603480267567287		[learning rate: 7.2558e-05]
	Learning Rate: 7.25578e-05
	LOSS [training: 2.603480267567287 | validation: 2.554820593405501]
	TIME [epoch: 8.29 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6033445491864193		[learning rate: 7.2295e-05]
	Learning Rate: 7.22945e-05
	LOSS [training: 2.6033445491864193 | validation: 2.5566171159174576]
	TIME [epoch: 8.31 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6017771330614314		[learning rate: 7.2032e-05]
	Learning Rate: 7.20321e-05
	LOSS [training: 2.6017771330614314 | validation: 2.5520431044705987]
	TIME [epoch: 8.29 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60889619778717		[learning rate: 7.1771e-05]
	Learning Rate: 7.17707e-05
	LOSS [training: 2.60889619778717 | validation: 2.5638199436746527]
	TIME [epoch: 8.29 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5999172767851455		[learning rate: 7.151e-05]
	Learning Rate: 7.15103e-05
	LOSS [training: 2.5999172767851455 | validation: 2.5582861727693746]
	TIME [epoch: 8.28 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6007539871817005		[learning rate: 7.1251e-05]
	Learning Rate: 7.12508e-05
	LOSS [training: 2.6007539871817005 | validation: 2.552933541698999]
	TIME [epoch: 8.29 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602366632435349		[learning rate: 7.0992e-05]
	Learning Rate: 7.09922e-05
	LOSS [training: 2.602366632435349 | validation: 2.5544046187677205]
	TIME [epoch: 8.3 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5972605399242963		[learning rate: 7.0735e-05]
	Learning Rate: 7.07345e-05
	LOSS [training: 2.5972605399242963 | validation: 2.5486739027572494]
	TIME [epoch: 8.29 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6004903514096855		[learning rate: 7.0478e-05]
	Learning Rate: 7.04778e-05
	LOSS [training: 2.6004903514096855 | validation: 2.5493296629362288]
	TIME [epoch: 8.28 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599278402860782		[learning rate: 7.0222e-05]
	Learning Rate: 7.02221e-05
	LOSS [training: 2.599278402860782 | validation: 2.548735626270175]
	TIME [epoch: 8.28 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6047382403974924		[learning rate: 6.9967e-05]
	Learning Rate: 6.99672e-05
	LOSS [training: 2.6047382403974924 | validation: 2.5652327516623954]
	TIME [epoch: 8.31 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6078421937629725		[learning rate: 6.9713e-05]
	Learning Rate: 6.97133e-05
	LOSS [training: 2.6078421937629725 | validation: 2.5359656810580367]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_1466.pth
	Model improved!!!
EPOCH 1467/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6037138444312395		[learning rate: 6.946e-05]
	Learning Rate: 6.94603e-05
	LOSS [training: 2.6037138444312395 | validation: 2.5465724163366743]
	TIME [epoch: 8.29 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601278184595944		[learning rate: 6.9208e-05]
	Learning Rate: 6.92083e-05
	LOSS [training: 2.601278184595944 | validation: 2.5424215925890454]
	TIME [epoch: 8.28 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5989265487207485		[learning rate: 6.8957e-05]
	Learning Rate: 6.89571e-05
	LOSS [training: 2.5989265487207485 | validation: 2.55105998209081]
	TIME [epoch: 8.3 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600000030176181		[learning rate: 6.8707e-05]
	Learning Rate: 6.87068e-05
	LOSS [training: 2.600000030176181 | validation: 2.553957501503064]
	TIME [epoch: 8.28 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.607881192216957		[learning rate: 6.8457e-05]
	Learning Rate: 6.84575e-05
	LOSS [training: 2.607881192216957 | validation: 2.560877130644344]
	TIME [epoch: 8.28 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6075102439960496		[learning rate: 6.8209e-05]
	Learning Rate: 6.82091e-05
	LOSS [training: 2.6075102439960496 | validation: 2.546817573684323]
	TIME [epoch: 8.29 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604058031682338		[learning rate: 6.7962e-05]
	Learning Rate: 6.79615e-05
	LOSS [training: 2.604058031682338 | validation: 2.554077768479442]
	TIME [epoch: 8.29 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6033641980637956		[learning rate: 6.7715e-05]
	Learning Rate: 6.77149e-05
	LOSS [training: 2.6033641980637956 | validation: 2.561350812032325]
	TIME [epoch: 8.3 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6052582432586604		[learning rate: 6.7469e-05]
	Learning Rate: 6.74692e-05
	LOSS [training: 2.6052582432586604 | validation: 2.551048738962553]
	TIME [epoch: 8.28 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602227903526341		[learning rate: 6.7224e-05]
	Learning Rate: 6.72243e-05
	LOSS [training: 2.602227903526341 | validation: 2.5525687470215916]
	TIME [epoch: 8.28 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6001901099287075		[learning rate: 6.698e-05]
	Learning Rate: 6.69804e-05
	LOSS [training: 2.6001901099287075 | validation: 2.5553150298383014]
	TIME [epoch: 8.28 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603580969171815		[learning rate: 6.6737e-05]
	Learning Rate: 6.67373e-05
	LOSS [training: 2.603580969171815 | validation: 2.5508352519171726]
	TIME [epoch: 8.3 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604034884723588		[learning rate: 6.6495e-05]
	Learning Rate: 6.64951e-05
	LOSS [training: 2.604034884723588 | validation: 2.550962904296915]
	TIME [epoch: 8.29 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6024224288373548		[learning rate: 6.6254e-05]
	Learning Rate: 6.62538e-05
	LOSS [training: 2.6024224288373548 | validation: 2.5532331918383733]
	TIME [epoch: 8.29 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6032809677661226		[learning rate: 6.6013e-05]
	Learning Rate: 6.60133e-05
	LOSS [training: 2.6032809677661226 | validation: 2.5472233909158977]
	TIME [epoch: 8.28 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606266288975605		[learning rate: 6.5774e-05]
	Learning Rate: 6.57738e-05
	LOSS [training: 2.606266288975605 | validation: 2.5507728470285604]
	TIME [epoch: 8.29 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600564381838343		[learning rate: 6.5535e-05]
	Learning Rate: 6.55351e-05
	LOSS [training: 2.600564381838343 | validation: 2.5612723399082453]
	TIME [epoch: 8.29 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.596927592160813		[learning rate: 6.5297e-05]
	Learning Rate: 6.52972e-05
	LOSS [training: 2.596927592160813 | validation: 2.5499802664823585]
	TIME [epoch: 8.28 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5993004993176627		[learning rate: 6.506e-05]
	Learning Rate: 6.50603e-05
	LOSS [training: 2.5993004993176627 | validation: 2.5581058630499007]
	TIME [epoch: 8.28 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599337780708718		[learning rate: 6.4824e-05]
	Learning Rate: 6.48242e-05
	LOSS [training: 2.599337780708718 | validation: 2.5503415972134067]
	TIME [epoch: 8.28 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601670620214452		[learning rate: 6.4589e-05]
	Learning Rate: 6.45889e-05
	LOSS [training: 2.601670620214452 | validation: 2.5534544567967963]
	TIME [epoch: 8.31 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606690017708236		[learning rate: 6.4354e-05]
	Learning Rate: 6.43545e-05
	LOSS [training: 2.606690017708236 | validation: 2.5528408223096335]
	TIME [epoch: 8.29 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603273059440974		[learning rate: 6.4121e-05]
	Learning Rate: 6.4121e-05
	LOSS [training: 2.603273059440974 | validation: 2.551894578863622]
	TIME [epoch: 8.29 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6030863401145043		[learning rate: 6.3888e-05]
	Learning Rate: 6.38883e-05
	LOSS [training: 2.6030863401145043 | validation: 2.5439115823386826]
	TIME [epoch: 8.29 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6002248256204803		[learning rate: 6.3656e-05]
	Learning Rate: 6.36564e-05
	LOSS [training: 2.6002248256204803 | validation: 2.549829781540867]
	TIME [epoch: 8.31 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6053126316639483		[learning rate: 6.3425e-05]
	Learning Rate: 6.34254e-05
	LOSS [training: 2.6053126316639483 | validation: 2.550094285314084]
	TIME [epoch: 8.29 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6013348960384706		[learning rate: 6.3195e-05]
	Learning Rate: 6.31952e-05
	LOSS [training: 2.6013348960384706 | validation: 2.5467178209349632]
	TIME [epoch: 8.29 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5998402827371927		[learning rate: 6.2966e-05]
	Learning Rate: 6.29659e-05
	LOSS [training: 2.5998402827371927 | validation: 2.5467970605888044]
	TIME [epoch: 8.28 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599647180150716		[learning rate: 6.2737e-05]
	Learning Rate: 6.27374e-05
	LOSS [training: 2.599647180150716 | validation: 2.55096991208777]
	TIME [epoch: 8.3 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6001705305496383		[learning rate: 6.251e-05]
	Learning Rate: 6.25097e-05
	LOSS [training: 2.6001705305496383 | validation: 2.556636537971024]
	TIME [epoch: 8.29 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6046239170026535		[learning rate: 6.2283e-05]
	Learning Rate: 6.22828e-05
	LOSS [training: 2.6046239170026535 | validation: 2.551707152487162]
	TIME [epoch: 8.29 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6069851224046294		[learning rate: 6.2057e-05]
	Learning Rate: 6.20568e-05
	LOSS [training: 2.6069851224046294 | validation: 2.5675985517991577]
	TIME [epoch: 8.28 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6011813450188264		[learning rate: 6.1832e-05]
	Learning Rate: 6.18316e-05
	LOSS [training: 2.6011813450188264 | validation: 2.545890068594184]
	TIME [epoch: 8.29 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5984941390026592		[learning rate: 6.1607e-05]
	Learning Rate: 6.16072e-05
	LOSS [training: 2.5984941390026592 | validation: 2.5494453871471885]
	TIME [epoch: 8.32 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.596092274404948		[learning rate: 6.1384e-05]
	Learning Rate: 6.13836e-05
	LOSS [training: 2.596092274404948 | validation: 2.549657130279238]
	TIME [epoch: 8.29 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5991884574544017		[learning rate: 6.1161e-05]
	Learning Rate: 6.11609e-05
	LOSS [training: 2.5991884574544017 | validation: 2.551186453308564]
	TIME [epoch: 8.28 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6041753359108473		[learning rate: 6.0939e-05]
	Learning Rate: 6.09389e-05
	LOSS [training: 2.6041753359108473 | validation: 2.5573140172421196]
	TIME [epoch: 8.28 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606198604753762		[learning rate: 6.0718e-05]
	Learning Rate: 6.07178e-05
	LOSS [training: 2.606198604753762 | validation: 2.549088185271029]
	TIME [epoch: 8.31 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5999381553572602		[learning rate: 6.0497e-05]
	Learning Rate: 6.04974e-05
	LOSS [training: 2.5999381553572602 | validation: 2.556484454778344]
	TIME [epoch: 8.29 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602256616298677		[learning rate: 6.0278e-05]
	Learning Rate: 6.02779e-05
	LOSS [training: 2.602256616298677 | validation: 2.554016038997726]
	TIME [epoch: 8.29 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601093969929532		[learning rate: 6.0059e-05]
	Learning Rate: 6.00591e-05
	LOSS [training: 2.601093969929532 | validation: 2.539462996579971]
	TIME [epoch: 8.28 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603528090646668		[learning rate: 5.9841e-05]
	Learning Rate: 5.98412e-05
	LOSS [training: 2.603528090646668 | validation: 2.542338247142708]
	TIME [epoch: 8.3 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600948256134613		[learning rate: 5.9624e-05]
	Learning Rate: 5.9624e-05
	LOSS [training: 2.600948256134613 | validation: 2.546783841277663]
	TIME [epoch: 8.3 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600960783080408		[learning rate: 5.9408e-05]
	Learning Rate: 5.94076e-05
	LOSS [training: 2.600960783080408 | validation: 2.5566549315325515]
	TIME [epoch: 8.29 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604182482785318		[learning rate: 5.9192e-05]
	Learning Rate: 5.9192e-05
	LOSS [training: 2.604182482785318 | validation: 2.5470637421547573]
	TIME [epoch: 8.29 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603646439545145		[learning rate: 5.8977e-05]
	Learning Rate: 5.89772e-05
	LOSS [training: 2.603646439545145 | validation: 2.553287111310529]
	TIME [epoch: 8.29 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6070768642198527		[learning rate: 5.8763e-05]
	Learning Rate: 5.87632e-05
	LOSS [training: 2.6070768642198527 | validation: 2.5528306710391977]
	TIME [epoch: 8.31 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6047663611682674		[learning rate: 5.855e-05]
	Learning Rate: 5.85499e-05
	LOSS [training: 2.6047663611682674 | validation: 2.546854650354555]
	TIME [epoch: 8.28 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6031550430070043		[learning rate: 5.8337e-05]
	Learning Rate: 5.83374e-05
	LOSS [training: 2.6031550430070043 | validation: 2.5463988765080843]
	TIME [epoch: 8.29 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599181921557729		[learning rate: 5.8126e-05]
	Learning Rate: 5.81257e-05
	LOSS [training: 2.599181921557729 | validation: 2.5514701230973733]
	TIME [epoch: 8.28 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601307069361762		[learning rate: 5.7915e-05]
	Learning Rate: 5.79148e-05
	LOSS [training: 2.601307069361762 | validation: 2.5462266340071915]
	TIME [epoch: 8.31 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6045262179991817		[learning rate: 5.7705e-05]
	Learning Rate: 5.77046e-05
	LOSS [training: 2.6045262179991817 | validation: 2.551557487029111]
	TIME [epoch: 8.29 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6023536722745626		[learning rate: 5.7495e-05]
	Learning Rate: 5.74952e-05
	LOSS [training: 2.6023536722745626 | validation: 2.5610896396899534]
	TIME [epoch: 8.28 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6003261842566006		[learning rate: 5.7287e-05]
	Learning Rate: 5.72866e-05
	LOSS [training: 2.6003261842566006 | validation: 2.5448701497320574]
	TIME [epoch: 8.28 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5983582094624014		[learning rate: 5.7079e-05]
	Learning Rate: 5.70787e-05
	LOSS [training: 2.5983582094624014 | validation: 2.559426456491153]
	TIME [epoch: 8.3 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6019238383161594		[learning rate: 5.6872e-05]
	Learning Rate: 5.68715e-05
	LOSS [training: 2.6019238383161594 | validation: 2.562329239559572]
	TIME [epoch: 8.29 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605382441891307		[learning rate: 5.6665e-05]
	Learning Rate: 5.66651e-05
	LOSS [training: 2.605382441891307 | validation: 2.5495418907054432]
	TIME [epoch: 8.28 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6001933134311916		[learning rate: 5.6459e-05]
	Learning Rate: 5.64595e-05
	LOSS [training: 2.6001933134311916 | validation: 2.551955589973177]
	TIME [epoch: 8.29 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6034215361317825		[learning rate: 5.6255e-05]
	Learning Rate: 5.62546e-05
	LOSS [training: 2.6034215361317825 | validation: 2.5553107902377024]
	TIME [epoch: 8.28 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5974750149852524		[learning rate: 5.605e-05]
	Learning Rate: 5.60504e-05
	LOSS [training: 2.5974750149852524 | validation: 2.5475462437615723]
	TIME [epoch: 8.31 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5986792019686162		[learning rate: 5.5847e-05]
	Learning Rate: 5.5847e-05
	LOSS [training: 2.5986792019686162 | validation: 2.5456821059040617]
	TIME [epoch: 8.28 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601602650612871		[learning rate: 5.5644e-05]
	Learning Rate: 5.56444e-05
	LOSS [training: 2.601602650612871 | validation: 2.557499169371871]
	TIME [epoch: 8.28 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6044074316995536		[learning rate: 5.5442e-05]
	Learning Rate: 5.54424e-05
	LOSS [training: 2.6044074316995536 | validation: 2.5508503561418516]
	TIME [epoch: 8.28 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604927585522306		[learning rate: 5.5241e-05]
	Learning Rate: 5.52412e-05
	LOSS [training: 2.604927585522306 | validation: 2.548777786506305]
	TIME [epoch: 8.3 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6072842637715405		[learning rate: 5.5041e-05]
	Learning Rate: 5.50407e-05
	LOSS [training: 2.6072842637715405 | validation: 2.549723004210371]
	TIME [epoch: 8.29 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6004599056581474		[learning rate: 5.4841e-05]
	Learning Rate: 5.4841e-05
	LOSS [training: 2.6004599056581474 | validation: 2.5544183310755786]
	TIME [epoch: 8.29 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6005420115109374		[learning rate: 5.4642e-05]
	Learning Rate: 5.4642e-05
	LOSS [training: 2.6005420115109374 | validation: 2.5514223825236133]
	TIME [epoch: 8.28 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598308559921814		[learning rate: 5.4444e-05]
	Learning Rate: 5.44437e-05
	LOSS [training: 2.598308559921814 | validation: 2.5553541942468865]
	TIME [epoch: 8.29 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598522377961918		[learning rate: 5.4246e-05]
	Learning Rate: 5.42461e-05
	LOSS [training: 2.598522377961918 | validation: 2.5503127276654185]
	TIME [epoch: 8.31 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605135187531647		[learning rate: 5.4049e-05]
	Learning Rate: 5.40492e-05
	LOSS [training: 2.605135187531647 | validation: 2.562057256654663]
	TIME [epoch: 8.29 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6065434824168063		[learning rate: 5.3853e-05]
	Learning Rate: 5.38531e-05
	LOSS [training: 2.6065434824168063 | validation: 2.5517240639481376]
	TIME [epoch: 8.28 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6025742677414647		[learning rate: 5.3658e-05]
	Learning Rate: 5.36577e-05
	LOSS [training: 2.6025742677414647 | validation: 2.556378038377712]
	TIME [epoch: 8.28 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6018298718832846		[learning rate: 5.3463e-05]
	Learning Rate: 5.34629e-05
	LOSS [training: 2.6018298718832846 | validation: 2.554355245969985]
	TIME [epoch: 8.31 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6038339100221095		[learning rate: 5.3269e-05]
	Learning Rate: 5.32689e-05
	LOSS [training: 2.6038339100221095 | validation: 2.56831675492054]
	TIME [epoch: 8.29 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6035584658382107		[learning rate: 5.3076e-05]
	Learning Rate: 5.30756e-05
	LOSS [training: 2.6035584658382107 | validation: 2.5475134841757]
	TIME [epoch: 8.29 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5989465515642896		[learning rate: 5.2883e-05]
	Learning Rate: 5.2883e-05
	LOSS [training: 2.5989465515642896 | validation: 2.5595461559862844]
	TIME [epoch: 8.29 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599069640112058		[learning rate: 5.2691e-05]
	Learning Rate: 5.26911e-05
	LOSS [training: 2.599069640112058 | validation: 2.551104218808505]
	TIME [epoch: 8.3 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597972452909		[learning rate: 5.25e-05]
	Learning Rate: 5.24998e-05
	LOSS [training: 2.597972452909 | validation: 2.556439690801536]
	TIME [epoch: 8.29 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603459542244829		[learning rate: 5.2309e-05]
	Learning Rate: 5.23093e-05
	LOSS [training: 2.603459542244829 | validation: 2.560142861607119]
	TIME [epoch: 8.28 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5998233382599834		[learning rate: 5.2119e-05]
	Learning Rate: 5.21195e-05
	LOSS [training: 2.5998233382599834 | validation: 2.5622764939267157]
	TIME [epoch: 8.28 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603576134713097		[learning rate: 5.193e-05]
	Learning Rate: 5.19303e-05
	LOSS [training: 2.603576134713097 | validation: 2.532614767714944]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study203/model_tr_study203_r3_20240219_233648/states/model_tr_study203_1547.pth
	Model improved!!!
EPOCH 1548/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605452930396739		[learning rate: 5.1742e-05]
	Learning Rate: 5.17419e-05
	LOSS [training: 2.605452930396739 | validation: 2.5570777214741396]
	TIME [epoch: 8.31 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5993224357837046		[learning rate: 5.1554e-05]
	Learning Rate: 5.15541e-05
	LOSS [training: 2.5993224357837046 | validation: 2.5510585149639264]
	TIME [epoch: 8.3 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.59661757292887		[learning rate: 5.1367e-05]
	Learning Rate: 5.1367e-05
	LOSS [training: 2.59661757292887 | validation: 2.552286356740686]
	TIME [epoch: 8.29 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602643636711535		[learning rate: 5.1181e-05]
	Learning Rate: 5.11806e-05
	LOSS [training: 2.602643636711535 | validation: 2.552859229144409]
	TIME [epoch: 8.29 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5989408941225944		[learning rate: 5.0995e-05]
	Learning Rate: 5.09949e-05
	LOSS [training: 2.5989408941225944 | validation: 2.5504163444482306]
	TIME [epoch: 8.31 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6041751777420714		[learning rate: 5.081e-05]
	Learning Rate: 5.08098e-05
	LOSS [training: 2.6041751777420714 | validation: 2.5603801441019183]
	TIME [epoch: 8.29 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5995790901972997		[learning rate: 5.0625e-05]
	Learning Rate: 5.06254e-05
	LOSS [training: 2.5995790901972997 | validation: 2.541862102198432]
	TIME [epoch: 8.29 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5976535528771927		[learning rate: 5.0442e-05]
	Learning Rate: 5.04417e-05
	LOSS [training: 2.5976535528771927 | validation: 2.554149018180196]
	TIME [epoch: 8.29 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603517261572088		[learning rate: 5.0259e-05]
	Learning Rate: 5.02586e-05
	LOSS [training: 2.603517261572088 | validation: 2.550592163632329]
	TIME [epoch: 8.3 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6036326716672678		[learning rate: 5.0076e-05]
	Learning Rate: 5.00762e-05
	LOSS [training: 2.6036326716672678 | validation: 2.544741116965996]
	TIME [epoch: 8.29 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601201949682116		[learning rate: 4.9894e-05]
	Learning Rate: 4.98945e-05
	LOSS [training: 2.601201949682116 | validation: 2.5649693197854377]
	TIME [epoch: 8.29 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601758965902585		[learning rate: 4.9713e-05]
	Learning Rate: 4.97134e-05
	LOSS [training: 2.601758965902585 | validation: 2.5505452524347114]
	TIME [epoch: 8.29 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602075607463861		[learning rate: 4.9533e-05]
	Learning Rate: 4.9533e-05
	LOSS [training: 2.602075607463861 | validation: 2.5509873111040013]
	TIME [epoch: 8.31 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6047578070412243		[learning rate: 4.9353e-05]
	Learning Rate: 4.93533e-05
	LOSS [training: 2.6047578070412243 | validation: 2.5493015715136558]
	TIME [epoch: 8.31 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5994620001604867		[learning rate: 4.9174e-05]
	Learning Rate: 4.91742e-05
	LOSS [training: 2.5994620001604867 | validation: 2.549782442942626]
	TIME [epoch: 8.29 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5974127532983586		[learning rate: 4.8996e-05]
	Learning Rate: 4.89957e-05
	LOSS [training: 2.5974127532983586 | validation: 2.5479345233471618]
	TIME [epoch: 8.29 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603573681412702		[learning rate: 4.8818e-05]
	Learning Rate: 4.88179e-05
	LOSS [training: 2.603573681412702 | validation: 2.5557042756136505]
	TIME [epoch: 8.29 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5974141808429088		[learning rate: 4.8641e-05]
	Learning Rate: 4.86407e-05
	LOSS [training: 2.5974141808429088 | validation: 2.554830550518149]
	TIME [epoch: 8.32 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60417518180829		[learning rate: 4.8464e-05]
	Learning Rate: 4.84642e-05
	LOSS [training: 2.60417518180829 | validation: 2.5636787347394554]
	TIME [epoch: 8.29 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605256869297919		[learning rate: 4.8288e-05]
	Learning Rate: 4.82883e-05
	LOSS [training: 2.605256869297919 | validation: 2.55956262338785]
	TIME [epoch: 8.28 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6042773300216115		[learning rate: 4.8113e-05]
	Learning Rate: 4.81131e-05
	LOSS [training: 2.6042773300216115 | validation: 2.547101169611704]
	TIME [epoch: 8.28 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6030847405952504		[learning rate: 4.7938e-05]
	Learning Rate: 4.79385e-05
	LOSS [training: 2.6030847405952504 | validation: 2.5533681834722195]
	TIME [epoch: 8.31 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6005147494205354		[learning rate: 4.7765e-05]
	Learning Rate: 4.77645e-05
	LOSS [training: 2.6005147494205354 | validation: 2.5512473829090663]
	TIME [epoch: 8.28 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6017366742184604		[learning rate: 4.7591e-05]
	Learning Rate: 4.75912e-05
	LOSS [training: 2.6017366742184604 | validation: 2.5513111871953953]
	TIME [epoch: 8.28 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604323983450105		[learning rate: 4.7418e-05]
	Learning Rate: 4.74185e-05
	LOSS [training: 2.604323983450105 | validation: 2.56484276096287]
	TIME [epoch: 8.29 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6008913785897736		[learning rate: 4.7246e-05]
	Learning Rate: 4.72464e-05
	LOSS [training: 2.6008913785897736 | validation: 2.5521424260173236]
	TIME [epoch: 8.3 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598792306372666		[learning rate: 4.7075e-05]
	Learning Rate: 4.70749e-05
	LOSS [training: 2.598792306372666 | validation: 2.5496881633321076]
	TIME [epoch: 8.31 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598905330603096		[learning rate: 4.6904e-05]
	Learning Rate: 4.69041e-05
	LOSS [training: 2.598905330603096 | validation: 2.5571358951691563]
	TIME [epoch: 8.28 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5986959421088853		[learning rate: 4.6734e-05]
	Learning Rate: 4.67339e-05
	LOSS [training: 2.5986959421088853 | validation: 2.554561196317815]
	TIME [epoch: 8.29 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6003389072825085		[learning rate: 4.6564e-05]
	Learning Rate: 4.65643e-05
	LOSS [training: 2.6003389072825085 | validation: 2.5526820093126017]
	TIME [epoch: 8.29 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600378603061159		[learning rate: 4.6395e-05]
	Learning Rate: 4.63953e-05
	LOSS [training: 2.600378603061159 | validation: 2.546553554721356]
	TIME [epoch: 8.31 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604681319566715		[learning rate: 4.6227e-05]
	Learning Rate: 4.62269e-05
	LOSS [training: 2.604681319566715 | validation: 2.549220688012328]
	TIME [epoch: 8.29 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598575542773793		[learning rate: 4.6059e-05]
	Learning Rate: 4.60591e-05
	LOSS [training: 2.598575542773793 | validation: 2.5505286099916242]
	TIME [epoch: 8.3 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600674550363616		[learning rate: 4.5892e-05]
	Learning Rate: 4.5892e-05
	LOSS [training: 2.600674550363616 | validation: 2.5479756500065966]
	TIME [epoch: 8.29 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600453501130016		[learning rate: 4.5725e-05]
	Learning Rate: 4.57254e-05
	LOSS [training: 2.600453501130016 | validation: 2.5428104531174127]
	TIME [epoch: 8.3 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604363816750839		[learning rate: 4.556e-05]
	Learning Rate: 4.55595e-05
	LOSS [training: 2.604363816750839 | validation: 2.556536450460261]
	TIME [epoch: 8.3 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601670710679506		[learning rate: 4.5394e-05]
	Learning Rate: 4.53942e-05
	LOSS [training: 2.601670710679506 | validation: 2.552233281834682]
	TIME [epoch: 8.29 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604603551324958		[learning rate: 4.5229e-05]
	Learning Rate: 4.52294e-05
	LOSS [training: 2.604603551324958 | validation: 2.5528007800082415]
	TIME [epoch: 8.28 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6007995168180496		[learning rate: 4.5065e-05]
	Learning Rate: 4.50653e-05
	LOSS [training: 2.6007995168180496 | validation: 2.5601826166185147]
	TIME [epoch: 8.28 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605294551558285		[learning rate: 4.4902e-05]
	Learning Rate: 4.49017e-05
	LOSS [training: 2.605294551558285 | validation: 2.5457324952973885]
	TIME [epoch: 8.31 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6038220198765374		[learning rate: 4.4739e-05]
	Learning Rate: 4.47388e-05
	LOSS [training: 2.6038220198765374 | validation: 2.5561904367513346]
	TIME [epoch: 8.28 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5967886112300493		[learning rate: 4.4576e-05]
	Learning Rate: 4.45764e-05
	LOSS [training: 2.5967886112300493 | validation: 2.5541876596336737]
	TIME [epoch: 8.29 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6016367298777814		[learning rate: 4.4415e-05]
	Learning Rate: 4.44147e-05
	LOSS [training: 2.6016367298777814 | validation: 2.546658674255264]
	TIME [epoch: 8.28 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6031807083880887		[learning rate: 4.4253e-05]
	Learning Rate: 4.42535e-05
	LOSS [training: 2.6031807083880887 | validation: 2.542355158334181]
	TIME [epoch: 8.32 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606742259051216		[learning rate: 4.4093e-05]
	Learning Rate: 4.40929e-05
	LOSS [training: 2.606742259051216 | validation: 2.5513345986364095]
	TIME [epoch: 8.3 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6039586411044033		[learning rate: 4.3933e-05]
	Learning Rate: 4.39329e-05
	LOSS [training: 2.6039586411044033 | validation: 2.544973066158896]
	TIME [epoch: 8.3 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6052677903357297		[learning rate: 4.3773e-05]
	Learning Rate: 4.37734e-05
	LOSS [training: 2.6052677903357297 | validation: 2.547847055881902]
	TIME [epoch: 8.29 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60861925219928		[learning rate: 4.3615e-05]
	Learning Rate: 4.36146e-05
	LOSS [training: 2.60861925219928 | validation: 2.5487403991823103]
	TIME [epoch: 8.3 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6081196372830098		[learning rate: 4.3456e-05]
	Learning Rate: 4.34563e-05
	LOSS [training: 2.6081196372830098 | validation: 2.5559875202620814]
	TIME [epoch: 8.29 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600436662680208		[learning rate: 4.3299e-05]
	Learning Rate: 4.32986e-05
	LOSS [training: 2.600436662680208 | validation: 2.546210532275019]
	TIME [epoch: 8.28 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.595868371339702		[learning rate: 4.3141e-05]
	Learning Rate: 4.31415e-05
	LOSS [training: 2.595868371339702 | validation: 2.5675379746271787]
	TIME [epoch: 8.29 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600790073039154		[learning rate: 4.2985e-05]
	Learning Rate: 4.29849e-05
	LOSS [training: 2.600790073039154 | validation: 2.547942184918873]
	TIME [epoch: 8.29 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6002115878550205		[learning rate: 4.2829e-05]
	Learning Rate: 4.28289e-05
	LOSS [training: 2.6002115878550205 | validation: 2.552181176703452]
	TIME [epoch: 8.31 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6013580508660126		[learning rate: 4.2673e-05]
	Learning Rate: 4.26735e-05
	LOSS [training: 2.6013580508660126 | validation: 2.551640383276669]
	TIME [epoch: 8.29 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599504967311201		[learning rate: 4.2519e-05]
	Learning Rate: 4.25186e-05
	LOSS [training: 2.599504967311201 | validation: 2.5527707625639313]
	TIME [epoch: 8.29 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6028704403258365		[learning rate: 4.2364e-05]
	Learning Rate: 4.23643e-05
	LOSS [training: 2.6028704403258365 | validation: 2.548274932573069]
	TIME [epoch: 8.28 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5999358211964188		[learning rate: 4.2211e-05]
	Learning Rate: 4.22106e-05
	LOSS [training: 2.5999358211964188 | validation: 2.556768171207831]
	TIME [epoch: 8.31 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6000813474177265		[learning rate: 4.2057e-05]
	Learning Rate: 4.20574e-05
	LOSS [training: 2.6000813474177265 | validation: 2.552127890130582]
	TIME [epoch: 8.3 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5990685737142587		[learning rate: 4.1905e-05]
	Learning Rate: 4.19047e-05
	LOSS [training: 2.5990685737142587 | validation: 2.552123023773023]
	TIME [epoch: 8.29 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599435775897266		[learning rate: 4.1753e-05]
	Learning Rate: 4.17527e-05
	LOSS [training: 2.599435775897266 | validation: 2.5585632263319775]
	TIME [epoch: 8.29 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600136840254439		[learning rate: 4.1601e-05]
	Learning Rate: 4.16012e-05
	LOSS [training: 2.600136840254439 | validation: 2.5595265300151233]
	TIME [epoch: 8.29 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603861074738993		[learning rate: 4.145e-05]
	Learning Rate: 4.14502e-05
	LOSS [training: 2.603861074738993 | validation: 2.5536381429003487]
	TIME [epoch: 8.3 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6016314296781773		[learning rate: 4.13e-05]
	Learning Rate: 4.12998e-05
	LOSS [training: 2.6016314296781773 | validation: 2.5538787091630564]
	TIME [epoch: 8.28 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.607667184329062		[learning rate: 4.115e-05]
	Learning Rate: 4.11499e-05
	LOSS [training: 2.607667184329062 | validation: 2.55289916587951]
	TIME [epoch: 8.29 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6040049576809645		[learning rate: 4.1001e-05]
	Learning Rate: 4.10005e-05
	LOSS [training: 2.6040049576809645 | validation: 2.55572956901213]
	TIME [epoch: 8.29 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600492809063137		[learning rate: 4.0852e-05]
	Learning Rate: 4.08517e-05
	LOSS [training: 2.600492809063137 | validation: 2.5440669513635115]
	TIME [epoch: 8.31 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6020008511891595		[learning rate: 4.0703e-05]
	Learning Rate: 4.07035e-05
	LOSS [training: 2.6020008511891595 | validation: 2.549121882201453]
	TIME [epoch: 8.29 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600986836123814		[learning rate: 4.0556e-05]
	Learning Rate: 4.05558e-05
	LOSS [training: 2.600986836123814 | validation: 2.5583818299947865]
	TIME [epoch: 8.29 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603717101781362		[learning rate: 4.0409e-05]
	Learning Rate: 4.04086e-05
	LOSS [training: 2.603717101781362 | validation: 2.542563368490696]
	TIME [epoch: 8.29 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601009642397712		[learning rate: 4.0262e-05]
	Learning Rate: 4.0262e-05
	LOSS [training: 2.601009642397712 | validation: 2.544047651396127]
	TIME [epoch: 8.31 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6004066894412743		[learning rate: 4.0116e-05]
	Learning Rate: 4.01158e-05
	LOSS [training: 2.6004066894412743 | validation: 2.5465155598993867]
	TIME [epoch: 8.28 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599625999096397		[learning rate: 3.997e-05]
	Learning Rate: 3.99703e-05
	LOSS [training: 2.599625999096397 | validation: 2.545108709388325]
	TIME [epoch: 8.29 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6020750138237823		[learning rate: 3.9825e-05]
	Learning Rate: 3.98252e-05
	LOSS [training: 2.6020750138237823 | validation: 2.545265473501745]
	TIME [epoch: 8.29 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603596398037287		[learning rate: 3.9681e-05]
	Learning Rate: 3.96807e-05
	LOSS [training: 2.603596398037287 | validation: 2.553286486829202]
	TIME [epoch: 8.32 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606717650054713		[learning rate: 3.9537e-05]
	Learning Rate: 3.95367e-05
	LOSS [training: 2.606717650054713 | validation: 2.5566589375157616]
	TIME [epoch: 8.3 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606724629183349		[learning rate: 3.9393e-05]
	Learning Rate: 3.93932e-05
	LOSS [training: 2.606724629183349 | validation: 2.563577861654604]
	TIME [epoch: 8.28 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5984939683851307		[learning rate: 3.925e-05]
	Learning Rate: 3.92502e-05
	LOSS [training: 2.5984939683851307 | validation: 2.553981085683808]
	TIME [epoch: 8.3 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599935180407898		[learning rate: 3.9108e-05]
	Learning Rate: 3.91078e-05
	LOSS [training: 2.599935180407898 | validation: 2.572090414886415]
	TIME [epoch: 8.29 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6005611291169695		[learning rate: 3.8966e-05]
	Learning Rate: 3.89659e-05
	LOSS [training: 2.6005611291169695 | validation: 2.5533281318015586]
	TIME [epoch: 8.3 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6001389091006653		[learning rate: 3.8824e-05]
	Learning Rate: 3.88245e-05
	LOSS [training: 2.6001389091006653 | validation: 2.5506885358976303]
	TIME [epoch: 8.29 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6027424535032213		[learning rate: 3.8684e-05]
	Learning Rate: 3.86836e-05
	LOSS [training: 2.6027424535032213 | validation: 2.560293049048793]
	TIME [epoch: 8.28 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599593028349915		[learning rate: 3.8543e-05]
	Learning Rate: 3.85432e-05
	LOSS [training: 2.599593028349915 | validation: 2.556983939289605]
	TIME [epoch: 8.28 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5979690273325633		[learning rate: 3.8403e-05]
	Learning Rate: 3.84033e-05
	LOSS [training: 2.5979690273325633 | validation: 2.5490366756501794]
	TIME [epoch: 8.32 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599252148550735		[learning rate: 3.8264e-05]
	Learning Rate: 3.82639e-05
	LOSS [training: 2.599252148550735 | validation: 2.552071605680291]
	TIME [epoch: 8.3 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6013091600554006		[learning rate: 3.8125e-05]
	Learning Rate: 3.81251e-05
	LOSS [training: 2.6013091600554006 | validation: 2.5567374693385077]
	TIME [epoch: 8.29 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5996446056903473		[learning rate: 3.7987e-05]
	Learning Rate: 3.79867e-05
	LOSS [training: 2.5996446056903473 | validation: 2.556285606737658]
	TIME [epoch: 8.29 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602948268297049		[learning rate: 3.7849e-05]
	Learning Rate: 3.78489e-05
	LOSS [training: 2.602948268297049 | validation: 2.5482707264993785]
	TIME [epoch: 8.29 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6009140855424215		[learning rate: 3.7711e-05]
	Learning Rate: 3.77115e-05
	LOSS [training: 2.6009140855424215 | validation: 2.5588970177859536]
	TIME [epoch: 8.31 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6054491275684226		[learning rate: 3.7575e-05]
	Learning Rate: 3.75746e-05
	LOSS [training: 2.6054491275684226 | validation: 2.5488747682270154]
	TIME [epoch: 8.28 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6056260023764843		[learning rate: 3.7438e-05]
	Learning Rate: 3.74383e-05
	LOSS [training: 2.6056260023764843 | validation: 2.554009071538932]
	TIME [epoch: 8.28 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598208845241834		[learning rate: 3.7302e-05]
	Learning Rate: 3.73024e-05
	LOSS [training: 2.598208845241834 | validation: 2.554227376993059]
	TIME [epoch: 8.29 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601491430500991		[learning rate: 3.7167e-05]
	Learning Rate: 3.7167e-05
	LOSS [training: 2.601491430500991 | validation: 2.5471409136617824]
	TIME [epoch: 8.31 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6001349795580793		[learning rate: 3.7032e-05]
	Learning Rate: 3.70322e-05
	LOSS [training: 2.6001349795580793 | validation: 2.542790465969816]
	TIME [epoch: 8.29 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6023403225115276		[learning rate: 3.6898e-05]
	Learning Rate: 3.68978e-05
	LOSS [training: 2.6023403225115276 | validation: 2.5563375280532403]
	TIME [epoch: 8.28 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.596264409930368		[learning rate: 3.6764e-05]
	Learning Rate: 3.67639e-05
	LOSS [training: 2.596264409930368 | validation: 2.547836484074626]
	TIME [epoch: 8.28 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5993324400427746		[learning rate: 3.663e-05]
	Learning Rate: 3.66304e-05
	LOSS [training: 2.5993324400427746 | validation: 2.5503686791334674]
	TIME [epoch: 8.31 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5987970987604845		[learning rate: 3.6498e-05]
	Learning Rate: 3.64975e-05
	LOSS [training: 2.5987970987604845 | validation: 2.5558318701862577]
	TIME [epoch: 8.29 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6004621365198672		[learning rate: 3.6365e-05]
	Learning Rate: 3.63651e-05
	LOSS [training: 2.6004621365198672 | validation: 2.553972804825821]
	TIME [epoch: 8.29 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6006033747834816		[learning rate: 3.6233e-05]
	Learning Rate: 3.62331e-05
	LOSS [training: 2.6006033747834816 | validation: 2.5491264385493073]
	TIME [epoch: 8.29 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6047877491751943		[learning rate: 3.6102e-05]
	Learning Rate: 3.61016e-05
	LOSS [training: 2.6047877491751943 | validation: 2.5572281645658497]
	TIME [epoch: 8.29 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.606946871898075		[learning rate: 3.5971e-05]
	Learning Rate: 3.59706e-05
	LOSS [training: 2.606946871898075 | validation: 2.5691565951300555]
	TIME [epoch: 8.3 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5949361986612276		[learning rate: 3.584e-05]
	Learning Rate: 3.584e-05
	LOSS [training: 2.5949361986612276 | validation: 2.558264312418115]
	TIME [epoch: 8.29 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.596481382148478		[learning rate: 3.571e-05]
	Learning Rate: 3.571e-05
	LOSS [training: 2.596481382148478 | validation: 2.55587131249277]
	TIME [epoch: 8.29 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5999456755383603		[learning rate: 3.558e-05]
	Learning Rate: 3.55804e-05
	LOSS [training: 2.5999456755383603 | validation: 2.559379391517737]
	TIME [epoch: 8.29 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.605028835969541		[learning rate: 3.5451e-05]
	Learning Rate: 3.54513e-05
	LOSS [training: 2.605028835969541 | validation: 2.552797179547396]
	TIME [epoch: 8.31 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601605170974501		[learning rate: 3.5323e-05]
	Learning Rate: 3.53226e-05
	LOSS [training: 2.601605170974501 | validation: 2.5567378761853368]
	TIME [epoch: 8.29 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602885847161255		[learning rate: 3.5194e-05]
	Learning Rate: 3.51944e-05
	LOSS [training: 2.602885847161255 | validation: 2.5528736386149626]
	TIME [epoch: 8.28 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6008998101446617		[learning rate: 3.5067e-05]
	Learning Rate: 3.50667e-05
	LOSS [training: 2.6008998101446617 | validation: 2.551235285409844]
	TIME [epoch: 8.28 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5984105712572814		[learning rate: 3.4939e-05]
	Learning Rate: 3.49394e-05
	LOSS [training: 2.5984105712572814 | validation: 2.552639900318507]
	TIME [epoch: 8.3 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598594360133066		[learning rate: 3.4813e-05]
	Learning Rate: 3.48126e-05
	LOSS [training: 2.598594360133066 | validation: 2.5427070941303658]
	TIME [epoch: 8.29 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602639421731421		[learning rate: 3.4686e-05]
	Learning Rate: 3.46863e-05
	LOSS [training: 2.602639421731421 | validation: 2.54674038751866]
	TIME [epoch: 8.28 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.604869234405837		[learning rate: 3.456e-05]
	Learning Rate: 3.45604e-05
	LOSS [training: 2.604869234405837 | validation: 2.5456067960886664]
	TIME [epoch: 8.29 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603246206563805		[learning rate: 3.4435e-05]
	Learning Rate: 3.4435e-05
	LOSS [training: 2.603246206563805 | validation: 2.551184623741036]
	TIME [epoch: 8.29 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5979888969479807		[learning rate: 3.431e-05]
	Learning Rate: 3.431e-05
	LOSS [training: 2.5979888969479807 | validation: 2.558028547879161]
	TIME [epoch: 8.31 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5973843023432237		[learning rate: 3.4186e-05]
	Learning Rate: 3.41855e-05
	LOSS [training: 2.5973843023432237 | validation: 2.5490855139032402]
	TIME [epoch: 8.29 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601796326292506		[learning rate: 3.4061e-05]
	Learning Rate: 3.40615e-05
	LOSS [training: 2.601796326292506 | validation: 2.561805889119663]
	TIME [epoch: 8.29 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.596775679738978		[learning rate: 3.3938e-05]
	Learning Rate: 3.39378e-05
	LOSS [training: 2.596775679738978 | validation: 2.5486361362059404]
	TIME [epoch: 8.29 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6004662527088667		[learning rate: 3.3815e-05]
	Learning Rate: 3.38147e-05
	LOSS [training: 2.6004662527088667 | validation: 2.541963573439814]
	TIME [epoch: 8.31 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6006947795573017		[learning rate: 3.3692e-05]
	Learning Rate: 3.3692e-05
	LOSS [training: 2.6006947795573017 | validation: 2.547495839115856]
	TIME [epoch: 8.28 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5985778828147454		[learning rate: 3.357e-05]
	Learning Rate: 3.35697e-05
	LOSS [training: 2.5985778828147454 | validation: 2.549746765506778]
	TIME [epoch: 8.29 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599780447157045		[learning rate: 3.3448e-05]
	Learning Rate: 3.34479e-05
	LOSS [training: 2.599780447157045 | validation: 2.5514012203874517]
	TIME [epoch: 8.29 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602648295010237		[learning rate: 3.3326e-05]
	Learning Rate: 3.33265e-05
	LOSS [training: 2.602648295010237 | validation: 2.550872509989226]
	TIME [epoch: 8.31 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603435771819044		[learning rate: 3.3206e-05]
	Learning Rate: 3.32055e-05
	LOSS [training: 2.603435771819044 | validation: 2.551221089871686]
	TIME [epoch: 8.31 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5992987560613425		[learning rate: 3.3085e-05]
	Learning Rate: 3.3085e-05
	LOSS [training: 2.5992987560613425 | validation: 2.5585036260654794]
	TIME [epoch: 8.29 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5980301752914534		[learning rate: 3.2965e-05]
	Learning Rate: 3.2965e-05
	LOSS [training: 2.5980301752914534 | validation: 2.5592562106483463]
	TIME [epoch: 8.29 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6014913249262475		[learning rate: 3.2845e-05]
	Learning Rate: 3.28453e-05
	LOSS [training: 2.6014913249262475 | validation: 2.5459574392044493]
	TIME [epoch: 8.29 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600726692935154		[learning rate: 3.2726e-05]
	Learning Rate: 3.27261e-05
	LOSS [training: 2.600726692935154 | validation: 2.554213058430997]
	TIME [epoch: 8.32 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5995867635452248		[learning rate: 3.2607e-05]
	Learning Rate: 3.26074e-05
	LOSS [training: 2.5995867635452248 | validation: 2.549966935133205]
	TIME [epoch: 8.28 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5989634781005386		[learning rate: 3.2489e-05]
	Learning Rate: 3.2489e-05
	LOSS [training: 2.5989634781005386 | validation: 2.5499117332529417]
	TIME [epoch: 8.29 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599994247092851		[learning rate: 3.2371e-05]
	Learning Rate: 3.23711e-05
	LOSS [training: 2.599994247092851 | validation: 2.5405233113857486]
	TIME [epoch: 8.29 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599451046250408		[learning rate: 3.2254e-05]
	Learning Rate: 3.22537e-05
	LOSS [training: 2.599451046250408 | validation: 2.5438090001341767]
	TIME [epoch: 8.31 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5967984201544563		[learning rate: 3.2137e-05]
	Learning Rate: 3.21366e-05
	LOSS [training: 2.5967984201544563 | validation: 2.55572716954359]
	TIME [epoch: 8.29 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6006751152811343		[learning rate: 3.202e-05]
	Learning Rate: 3.202e-05
	LOSS [training: 2.6006751152811343 | validation: 2.559232879032641]
	TIME [epoch: 8.29 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600397749636776		[learning rate: 3.1904e-05]
	Learning Rate: 3.19038e-05
	LOSS [training: 2.600397749636776 | validation: 2.552923741136321]
	TIME [epoch: 8.29 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5989284064989424		[learning rate: 3.1788e-05]
	Learning Rate: 3.1788e-05
	LOSS [training: 2.5989284064989424 | validation: 2.5681556264564263]
	TIME [epoch: 8.3 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603459885938059		[learning rate: 3.1673e-05]
	Learning Rate: 3.16726e-05
	LOSS [training: 2.603459885938059 | validation: 2.5577615993089435]
	TIME [epoch: 8.29 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598728774079869		[learning rate: 3.1558e-05]
	Learning Rate: 3.15577e-05
	LOSS [training: 2.598728774079869 | validation: 2.5578692142805224]
	TIME [epoch: 8.29 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6050745497560994		[learning rate: 3.1443e-05]
	Learning Rate: 3.14432e-05
	LOSS [training: 2.6050745497560994 | validation: 2.5495350630885927]
	TIME [epoch: 8.28 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601383945000721		[learning rate: 3.1329e-05]
	Learning Rate: 3.13291e-05
	LOSS [training: 2.601383945000721 | validation: 2.5552916870608557]
	TIME [epoch: 8.28 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5992808279263935		[learning rate: 3.1215e-05]
	Learning Rate: 3.12154e-05
	LOSS [training: 2.5992808279263935 | validation: 2.546845774757389]
	TIME [epoch: 8.31 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599141716538818		[learning rate: 3.1102e-05]
	Learning Rate: 3.11021e-05
	LOSS [training: 2.599141716538818 | validation: 2.54937427794263]
	TIME [epoch: 8.29 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5962534365654633		[learning rate: 3.0989e-05]
	Learning Rate: 3.09892e-05
	LOSS [training: 2.5962534365654633 | validation: 2.556137447577213]
	TIME [epoch: 8.29 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5965124300207174		[learning rate: 3.0877e-05]
	Learning Rate: 3.08768e-05
	LOSS [training: 2.5965124300207174 | validation: 2.5546957121529554]
	TIME [epoch: 8.29 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603130849129129		[learning rate: 3.0765e-05]
	Learning Rate: 3.07647e-05
	LOSS [training: 2.603130849129129 | validation: 2.5530608997010433]
	TIME [epoch: 8.35 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6019127193127067		[learning rate: 3.0653e-05]
	Learning Rate: 3.0653e-05
	LOSS [training: 2.6019127193127067 | validation: 2.5482404816384587]
	TIME [epoch: 8.28 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601295223746105		[learning rate: 3.0542e-05]
	Learning Rate: 3.05418e-05
	LOSS [training: 2.601295223746105 | validation: 2.5515310068422434]
	TIME [epoch: 8.28 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601251414545407		[learning rate: 3.0431e-05]
	Learning Rate: 3.0431e-05
	LOSS [training: 2.601251414545407 | validation: 2.5519048119063337]
	TIME [epoch: 8.3 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602160260969326		[learning rate: 3.0321e-05]
	Learning Rate: 3.03205e-05
	LOSS [training: 2.602160260969326 | validation: 2.5535480236621866]
	TIME [epoch: 8.3 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5959160681018956		[learning rate: 3.0211e-05]
	Learning Rate: 3.02105e-05
	LOSS [training: 2.5959160681018956 | validation: 2.549066652269918]
	TIME [epoch: 8.3 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5999128382862966		[learning rate: 3.0101e-05]
	Learning Rate: 3.01009e-05
	LOSS [training: 2.5999128382862966 | validation: 2.5467232279147782]
	TIME [epoch: 8.28 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5995812620279124		[learning rate: 2.9992e-05]
	Learning Rate: 2.99916e-05
	LOSS [training: 2.5995812620279124 | validation: 2.553844715184024]
	TIME [epoch: 8.28 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5982693302625814		[learning rate: 2.9883e-05]
	Learning Rate: 2.98828e-05
	LOSS [training: 2.5982693302625814 | validation: 2.5543788935030287]
	TIME [epoch: 8.28 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5993524714427307		[learning rate: 2.9774e-05]
	Learning Rate: 2.97743e-05
	LOSS [training: 2.5993524714427307 | validation: 2.5452564769066495]
	TIME [epoch: 8.31 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601503989773449		[learning rate: 2.9666e-05]
	Learning Rate: 2.96663e-05
	LOSS [training: 2.601503989773449 | validation: 2.5456743824917227]
	TIME [epoch: 8.28 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602940946557843		[learning rate: 2.9559e-05]
	Learning Rate: 2.95586e-05
	LOSS [training: 2.602940946557843 | validation: 2.5453930044924746]
	TIME [epoch: 8.29 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6017056716079443		[learning rate: 2.9451e-05]
	Learning Rate: 2.94514e-05
	LOSS [training: 2.6017056716079443 | validation: 2.5481887414191613]
	TIME [epoch: 8.29 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598338219330628		[learning rate: 2.9344e-05]
	Learning Rate: 2.93445e-05
	LOSS [training: 2.598338219330628 | validation: 2.551625670277386]
	TIME [epoch: 8.3 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6026669728784673		[learning rate: 2.9238e-05]
	Learning Rate: 2.9238e-05
	LOSS [training: 2.6026669728784673 | validation: 2.5457415903781064]
	TIME [epoch: 8.28 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600543906217075		[learning rate: 2.9132e-05]
	Learning Rate: 2.91319e-05
	LOSS [training: 2.600543906217075 | validation: 2.542374241803781]
	TIME [epoch: 8.29 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6042313648929554		[learning rate: 2.9026e-05]
	Learning Rate: 2.90262e-05
	LOSS [training: 2.6042313648929554 | validation: 2.5543404136034082]
	TIME [epoch: 8.28 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6015783285094436		[learning rate: 2.8921e-05]
	Learning Rate: 2.89208e-05
	LOSS [training: 2.6015783285094436 | validation: 2.5603174146742704]
	TIME [epoch: 8.3 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6039723138705657		[learning rate: 2.8816e-05]
	Learning Rate: 2.88159e-05
	LOSS [training: 2.6039723138705657 | validation: 2.556549388474225]
	TIME [epoch: 8.3 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5997600782276584		[learning rate: 2.8711e-05]
	Learning Rate: 2.87113e-05
	LOSS [training: 2.5997600782276584 | validation: 2.546740780862959]
	TIME [epoch: 8.28 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600653864147791		[learning rate: 2.8607e-05]
	Learning Rate: 2.86071e-05
	LOSS [training: 2.600653864147791 | validation: 2.555396786694333]
	TIME [epoch: 8.29 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601523949467765		[learning rate: 2.8503e-05]
	Learning Rate: 2.85033e-05
	LOSS [training: 2.601523949467765 | validation: 2.5591871602039746]
	TIME [epoch: 8.29 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6061233901568217		[learning rate: 2.84e-05]
	Learning Rate: 2.83998e-05
	LOSS [training: 2.6061233901568217 | validation: 2.552405087939052]
	TIME [epoch: 8.31 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6042708056148056		[learning rate: 2.8297e-05]
	Learning Rate: 2.82968e-05
	LOSS [training: 2.6042708056148056 | validation: 2.558664411391222]
	TIME [epoch: 8.29 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5994028997489593		[learning rate: 2.8194e-05]
	Learning Rate: 2.81941e-05
	LOSS [training: 2.5994028997489593 | validation: 2.5496315448610822]
	TIME [epoch: 8.28 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6006769259953764		[learning rate: 2.8092e-05]
	Learning Rate: 2.80918e-05
	LOSS [training: 2.6006769259953764 | validation: 2.5490462363448008]
	TIME [epoch: 8.29 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6003868112990447		[learning rate: 2.799e-05]
	Learning Rate: 2.79898e-05
	LOSS [training: 2.6003868112990447 | validation: 2.5443055442337856]
	TIME [epoch: 8.31 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598303443681144		[learning rate: 2.7888e-05]
	Learning Rate: 2.78882e-05
	LOSS [training: 2.598303443681144 | validation: 2.552212143007926]
	TIME [epoch: 8.29 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.596724271392799		[learning rate: 2.7787e-05]
	Learning Rate: 2.7787e-05
	LOSS [training: 2.596724271392799 | validation: 2.547948630539352]
	TIME [epoch: 8.28 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601469600346269		[learning rate: 2.7686e-05]
	Learning Rate: 2.76862e-05
	LOSS [training: 2.601469600346269 | validation: 2.5492865111994325]
	TIME [epoch: 8.28 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6012648759629187		[learning rate: 2.7586e-05]
	Learning Rate: 2.75857e-05
	LOSS [training: 2.6012648759629187 | validation: 2.550529528089466]
	TIME [epoch: 8.28 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6007180510316306		[learning rate: 2.7486e-05]
	Learning Rate: 2.74856e-05
	LOSS [training: 2.6007180510316306 | validation: 2.5473129237093133]
	TIME [epoch: 8.29 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5982928686993843		[learning rate: 2.7386e-05]
	Learning Rate: 2.73859e-05
	LOSS [training: 2.5982928686993843 | validation: 2.5470495647075193]
	TIME [epoch: 8.3 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5969389016269435		[learning rate: 2.7286e-05]
	Learning Rate: 2.72865e-05
	LOSS [training: 2.5969389016269435 | validation: 2.5512962914473656]
	TIME [epoch: 8.29 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600888359726105		[learning rate: 2.7187e-05]
	Learning Rate: 2.71875e-05
	LOSS [training: 2.600888359726105 | validation: 2.5606842892475687]
	TIME [epoch: 8.29 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6008723760730255		[learning rate: 2.7089e-05]
	Learning Rate: 2.70888e-05
	LOSS [training: 2.6008723760730255 | validation: 2.5525261744997887]
	TIME [epoch: 8.31 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6027263445387225		[learning rate: 2.699e-05]
	Learning Rate: 2.69905e-05
	LOSS [training: 2.6027263445387225 | validation: 2.5589041995372286]
	TIME [epoch: 8.3 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5998225739388916		[learning rate: 2.6893e-05]
	Learning Rate: 2.68925e-05
	LOSS [training: 2.5998225739388916 | validation: 2.5593163605216085]
	TIME [epoch: 8.3 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6042262450041425		[learning rate: 2.6795e-05]
	Learning Rate: 2.67949e-05
	LOSS [training: 2.6042262450041425 | validation: 2.551933860358071]
	TIME [epoch: 8.3 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601927776874414		[learning rate: 2.6698e-05]
	Learning Rate: 2.66977e-05
	LOSS [training: 2.601927776874414 | validation: 2.559681820865176]
	TIME [epoch: 8.3 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599960236279855		[learning rate: 2.6601e-05]
	Learning Rate: 2.66008e-05
	LOSS [training: 2.599960236279855 | validation: 2.548058649689354]
	TIME [epoch: 8.3 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6003765869794018		[learning rate: 2.6504e-05]
	Learning Rate: 2.65043e-05
	LOSS [training: 2.6003765869794018 | validation: 2.550222779559485]
	TIME [epoch: 8.29 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603417137932942		[learning rate: 2.6408e-05]
	Learning Rate: 2.64081e-05
	LOSS [training: 2.603417137932942 | validation: 2.550906532551388]
	TIME [epoch: 8.28 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5984875211830802		[learning rate: 2.6312e-05]
	Learning Rate: 2.63123e-05
	LOSS [training: 2.5984875211830802 | validation: 2.5442439774246166]
	TIME [epoch: 8.29 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602325149156739		[learning rate: 2.6217e-05]
	Learning Rate: 2.62168e-05
	LOSS [training: 2.602325149156739 | validation: 2.552595970821133]
	TIME [epoch: 8.31 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6019598759905227		[learning rate: 2.6122e-05]
	Learning Rate: 2.61216e-05
	LOSS [training: 2.6019598759905227 | validation: 2.5621194097086253]
	TIME [epoch: 8.29 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5973927280402274		[learning rate: 2.6027e-05]
	Learning Rate: 2.60268e-05
	LOSS [training: 2.5973927280402274 | validation: 2.5494796367941537]
	TIME [epoch: 8.3 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602161683650796		[learning rate: 2.5932e-05]
	Learning Rate: 2.59324e-05
	LOSS [training: 2.602161683650796 | validation: 2.554280714289923]
	TIME [epoch: 8.29 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597137316584842		[learning rate: 2.5838e-05]
	Learning Rate: 2.58383e-05
	LOSS [training: 2.597137316584842 | validation: 2.5469101409004162]
	TIME [epoch: 8.31 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5988276778562236		[learning rate: 2.5744e-05]
	Learning Rate: 2.57445e-05
	LOSS [training: 2.5988276778562236 | validation: 2.5421311473362858]
	TIME [epoch: 8.28 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5973990778057616		[learning rate: 2.5651e-05]
	Learning Rate: 2.56511e-05
	LOSS [training: 2.5973990778057616 | validation: 2.548662453783294]
	TIME [epoch: 8.28 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5966568026773778		[learning rate: 2.5558e-05]
	Learning Rate: 2.5558e-05
	LOSS [training: 2.5966568026773778 | validation: 2.550940211826985]
	TIME [epoch: 8.28 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6003289236935982		[learning rate: 2.5465e-05]
	Learning Rate: 2.54652e-05
	LOSS [training: 2.6003289236935982 | validation: 2.5430357913664503]
	TIME [epoch: 8.3 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603162002644398		[learning rate: 2.5373e-05]
	Learning Rate: 2.53728e-05
	LOSS [training: 2.603162002644398 | validation: 2.558910735463721]
	TIME [epoch: 8.29 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6039198856295256		[learning rate: 2.5281e-05]
	Learning Rate: 2.52807e-05
	LOSS [training: 2.6039198856295256 | validation: 2.5523185271502973]
	TIME [epoch: 8.3 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6022206767062164		[learning rate: 2.5189e-05]
	Learning Rate: 2.5189e-05
	LOSS [training: 2.6022206767062164 | validation: 2.55366134345363]
	TIME [epoch: 8.29 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601409548775584		[learning rate: 2.5098e-05]
	Learning Rate: 2.50976e-05
	LOSS [training: 2.601409548775584 | validation: 2.5495083391872804]
	TIME [epoch: 8.3 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5993819911437024		[learning rate: 2.5006e-05]
	Learning Rate: 2.50065e-05
	LOSS [training: 2.5993819911437024 | validation: 2.5502697999727353]
	TIME [epoch: 8.31 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599861085940555		[learning rate: 2.4916e-05]
	Learning Rate: 2.49157e-05
	LOSS [training: 2.599861085940555 | validation: 2.555245954751294]
	TIME [epoch: 8.29 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60307696144094		[learning rate: 2.4825e-05]
	Learning Rate: 2.48253e-05
	LOSS [training: 2.60307696144094 | validation: 2.5440606391510974]
	TIME [epoch: 8.29 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5995504358724077		[learning rate: 2.4735e-05]
	Learning Rate: 2.47352e-05
	LOSS [training: 2.5995504358724077 | validation: 2.543187940751968]
	TIME [epoch: 8.29 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5977570408013313		[learning rate: 2.4645e-05]
	Learning Rate: 2.46455e-05
	LOSS [training: 2.5977570408013313 | validation: 2.54746538788406]
	TIME [epoch: 8.29 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6015255644427726		[learning rate: 2.4556e-05]
	Learning Rate: 2.4556e-05
	LOSS [training: 2.6015255644427726 | validation: 2.550551609291253]
	TIME [epoch: 8.3 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6012916163857933		[learning rate: 2.4467e-05]
	Learning Rate: 2.44669e-05
	LOSS [training: 2.6012916163857933 | validation: 2.5515608149387408]
	TIME [epoch: 8.29 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599342282236257		[learning rate: 2.4378e-05]
	Learning Rate: 2.43781e-05
	LOSS [training: 2.599342282236257 | validation: 2.547910328549559]
	TIME [epoch: 8.29 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5986417228145813		[learning rate: 2.429e-05]
	Learning Rate: 2.42896e-05
	LOSS [training: 2.5986417228145813 | validation: 2.545631121587662]
	TIME [epoch: 8.3 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599847751455918		[learning rate: 2.4201e-05]
	Learning Rate: 2.42015e-05
	LOSS [training: 2.599847751455918 | validation: 2.546694545594292]
	TIME [epoch: 8.3 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600970312364398		[learning rate: 2.4114e-05]
	Learning Rate: 2.41137e-05
	LOSS [training: 2.600970312364398 | validation: 2.546469232778752]
	TIME [epoch: 8.29 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597763209703163		[learning rate: 2.4026e-05]
	Learning Rate: 2.40262e-05
	LOSS [training: 2.597763209703163 | validation: 2.546511350765999]
	TIME [epoch: 8.29 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6013071637752487		[learning rate: 2.3939e-05]
	Learning Rate: 2.3939e-05
	LOSS [training: 2.6013071637752487 | validation: 2.5504001803958176]
	TIME [epoch: 8.29 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6025782276790155		[learning rate: 2.3852e-05]
	Learning Rate: 2.38521e-05
	LOSS [training: 2.6025782276790155 | validation: 2.554146410730098]
	TIME [epoch: 8.31 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6020120891615286		[learning rate: 2.3766e-05]
	Learning Rate: 2.37655e-05
	LOSS [training: 2.6020120891615286 | validation: 2.5478165660970795]
	TIME [epoch: 8.29 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6035398834373145		[learning rate: 2.3679e-05]
	Learning Rate: 2.36793e-05
	LOSS [training: 2.6035398834373145 | validation: 2.5412541081503615]
	TIME [epoch: 8.29 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6017961931732705		[learning rate: 2.3593e-05]
	Learning Rate: 2.35933e-05
	LOSS [training: 2.6017961931732705 | validation: 2.5497142613730777]
	TIME [epoch: 8.29 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6021886948998043		[learning rate: 2.3508e-05]
	Learning Rate: 2.35077e-05
	LOSS [training: 2.6021886948998043 | validation: 2.541592824379979]
	TIME [epoch: 8.31 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602949868835522		[learning rate: 2.3422e-05]
	Learning Rate: 2.34224e-05
	LOSS [training: 2.602949868835522 | validation: 2.5482910867287893]
	TIME [epoch: 8.3 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597112796745045		[learning rate: 2.3337e-05]
	Learning Rate: 2.33374e-05
	LOSS [training: 2.597112796745045 | validation: 2.5467676069731624]
	TIME [epoch: 8.29 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6001611687995028		[learning rate: 2.3253e-05]
	Learning Rate: 2.32527e-05
	LOSS [training: 2.6001611687995028 | validation: 2.552118450781234]
	TIME [epoch: 8.29 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5970002121324463		[learning rate: 2.3168e-05]
	Learning Rate: 2.31683e-05
	LOSS [training: 2.5970002121324463 | validation: 2.5522162329888167]
	TIME [epoch: 8.3 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597590735617456		[learning rate: 2.3084e-05]
	Learning Rate: 2.30843e-05
	LOSS [training: 2.597590735617456 | validation: 2.546188890564105]
	TIME [epoch: 8.3 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6005093852987082		[learning rate: 2.3e-05]
	Learning Rate: 2.30005e-05
	LOSS [training: 2.6005093852987082 | validation: 2.5533204187056695]
	TIME [epoch: 8.29 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5955117765967057		[learning rate: 2.2917e-05]
	Learning Rate: 2.2917e-05
	LOSS [training: 2.5955117765967057 | validation: 2.5513368667224663]
	TIME [epoch: 8.29 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5970139604404325		[learning rate: 2.2834e-05]
	Learning Rate: 2.28338e-05
	LOSS [training: 2.5970139604404325 | validation: 2.5590908775940373]
	TIME [epoch: 8.29 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598173905280606		[learning rate: 2.2751e-05]
	Learning Rate: 2.2751e-05
	LOSS [training: 2.598173905280606 | validation: 2.5551546951000867]
	TIME [epoch: 8.31 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5981711927257702		[learning rate: 2.2668e-05]
	Learning Rate: 2.26684e-05
	LOSS [training: 2.5981711927257702 | validation: 2.5494857935757693]
	TIME [epoch: 8.29 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597607337599602		[learning rate: 2.2586e-05]
	Learning Rate: 2.25862e-05
	LOSS [training: 2.597607337599602 | validation: 2.552411702395402]
	TIME [epoch: 8.3 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60170892086923		[learning rate: 2.2504e-05]
	Learning Rate: 2.25042e-05
	LOSS [training: 2.60170892086923 | validation: 2.5499124675060774]
	TIME [epoch: 8.29 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5981508402843185		[learning rate: 2.2423e-05]
	Learning Rate: 2.24225e-05
	LOSS [training: 2.5981508402843185 | validation: 2.555415408007561]
	TIME [epoch: 8.31 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5997318230470516		[learning rate: 2.2341e-05]
	Learning Rate: 2.23411e-05
	LOSS [training: 2.5997318230470516 | validation: 2.551897456381406]
	TIME [epoch: 8.29 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597769733311592		[learning rate: 2.226e-05]
	Learning Rate: 2.22601e-05
	LOSS [training: 2.597769733311592 | validation: 2.5509310434683012]
	TIME [epoch: 8.29 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599828761643993		[learning rate: 2.2179e-05]
	Learning Rate: 2.21793e-05
	LOSS [training: 2.599828761643993 | validation: 2.5466061974517538]
	TIME [epoch: 8.29 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5982870927927477		[learning rate: 2.2099e-05]
	Learning Rate: 2.20988e-05
	LOSS [training: 2.5982870927927477 | validation: 2.553684953602138]
	TIME [epoch: 8.3 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5962018466061956		[learning rate: 2.2019e-05]
	Learning Rate: 2.20186e-05
	LOSS [training: 2.5962018466061956 | validation: 2.543118750645924]
	TIME [epoch: 8.3 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599755294677384		[learning rate: 2.1939e-05]
	Learning Rate: 2.19387e-05
	LOSS [training: 2.599755294677384 | validation: 2.5419870082473066]
	TIME [epoch: 8.29 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5977937212589604		[learning rate: 2.1859e-05]
	Learning Rate: 2.18591e-05
	LOSS [training: 2.5977937212589604 | validation: 2.5476108504181325]
	TIME [epoch: 8.29 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5994526732022734		[learning rate: 2.178e-05]
	Learning Rate: 2.17797e-05
	LOSS [training: 2.5994526732022734 | validation: 2.5577510257483542]
	TIME [epoch: 8.29 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599738019145938		[learning rate: 2.1701e-05]
	Learning Rate: 2.17007e-05
	LOSS [training: 2.599738019145938 | validation: 2.5499754592501245]
	TIME [epoch: 8.32 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597605514586991		[learning rate: 2.1622e-05]
	Learning Rate: 2.16219e-05
	LOSS [training: 2.597605514586991 | validation: 2.5518352317201547]
	TIME [epoch: 8.29 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603951412122458		[learning rate: 2.1543e-05]
	Learning Rate: 2.15435e-05
	LOSS [training: 2.603951412122458 | validation: 2.5452016531163797]
	TIME [epoch: 8.29 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6017136075092635		[learning rate: 2.1465e-05]
	Learning Rate: 2.14653e-05
	LOSS [training: 2.6017136075092635 | validation: 2.5458807077197507]
	TIME [epoch: 8.29 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5983728832881026		[learning rate: 2.1387e-05]
	Learning Rate: 2.13874e-05
	LOSS [training: 2.5983728832881026 | validation: 2.5406990793255355]
	TIME [epoch: 8.31 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.596899309019744		[learning rate: 2.131e-05]
	Learning Rate: 2.13098e-05
	LOSS [training: 2.596899309019744 | validation: 2.5513539669836973]
	TIME [epoch: 8.29 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602595226854831		[learning rate: 2.1232e-05]
	Learning Rate: 2.12325e-05
	LOSS [training: 2.602595226854831 | validation: 2.5499859325214405]
	TIME [epoch: 8.29 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602858625826155		[learning rate: 2.1155e-05]
	Learning Rate: 2.11554e-05
	LOSS [training: 2.602858625826155 | validation: 2.5556404029979594]
	TIME [epoch: 8.29 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6004793162329567		[learning rate: 2.1079e-05]
	Learning Rate: 2.10786e-05
	LOSS [training: 2.6004793162329567 | validation: 2.5418929177949243]
	TIME [epoch: 8.31 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6033074436527395		[learning rate: 2.1002e-05]
	Learning Rate: 2.10021e-05
	LOSS [training: 2.6033074436527395 | validation: 2.556046373413902]
	TIME [epoch: 8.3 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6017891049532915		[learning rate: 2.0926e-05]
	Learning Rate: 2.09259e-05
	LOSS [training: 2.6017891049532915 | validation: 2.550885133184187]
	TIME [epoch: 8.3 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600426548858827		[learning rate: 2.085e-05]
	Learning Rate: 2.085e-05
	LOSS [training: 2.600426548858827 | validation: 2.54842926401249]
	TIME [epoch: 8.29 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5982337278437475		[learning rate: 2.0774e-05]
	Learning Rate: 2.07743e-05
	LOSS [training: 2.5982337278437475 | validation: 2.5500503601651285]
	TIME [epoch: 8.29 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598379833378652		[learning rate: 2.0699e-05]
	Learning Rate: 2.06989e-05
	LOSS [training: 2.598379833378652 | validation: 2.5418831563471747]
	TIME [epoch: 8.31 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6007693531163056		[learning rate: 2.0624e-05]
	Learning Rate: 2.06238e-05
	LOSS [training: 2.6007693531163056 | validation: 2.551800324362883]
	TIME [epoch: 8.29 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600468388783651		[learning rate: 2.0549e-05]
	Learning Rate: 2.05489e-05
	LOSS [training: 2.600468388783651 | validation: 2.5533490040488362]
	TIME [epoch: 8.29 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5982083609872606		[learning rate: 2.0474e-05]
	Learning Rate: 2.04744e-05
	LOSS [training: 2.5982083609872606 | validation: 2.5491553875426134]
	TIME [epoch: 8.29 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5965836124857224		[learning rate: 2.04e-05]
	Learning Rate: 2.04001e-05
	LOSS [training: 2.5965836124857224 | validation: 2.5527045942168076]
	TIME [epoch: 8.31 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6020579687582903		[learning rate: 2.0326e-05]
	Learning Rate: 2.0326e-05
	LOSS [training: 2.6020579687582903 | validation: 2.5581356167982685]
	TIME [epoch: 8.3 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.596912731476328		[learning rate: 2.0252e-05]
	Learning Rate: 2.02523e-05
	LOSS [training: 2.596912731476328 | validation: 2.5487353814347626]
	TIME [epoch: 8.29 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6018364226498107		[learning rate: 2.0179e-05]
	Learning Rate: 2.01788e-05
	LOSS [training: 2.6018364226498107 | validation: 2.5506454738562683]
	TIME [epoch: 8.29 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5991807274140735		[learning rate: 2.0106e-05]
	Learning Rate: 2.01055e-05
	LOSS [training: 2.5991807274140735 | validation: 2.5451821792589286]
	TIME [epoch: 8.3 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598443237914555		[learning rate: 2.0033e-05]
	Learning Rate: 2.00326e-05
	LOSS [training: 2.598443237914555 | validation: 2.547947897099684]
	TIME [epoch: 8.3 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602231483966625		[learning rate: 1.996e-05]
	Learning Rate: 1.99599e-05
	LOSS [training: 2.602231483966625 | validation: 2.546987817258149]
	TIME [epoch: 8.29 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598804216436657		[learning rate: 1.9887e-05]
	Learning Rate: 1.98874e-05
	LOSS [training: 2.598804216436657 | validation: 2.5530558582647966]
	TIME [epoch: 8.29 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601199044783335		[learning rate: 1.9815e-05]
	Learning Rate: 1.98153e-05
	LOSS [training: 2.601199044783335 | validation: 2.545946462481667]
	TIME [epoch: 8.3 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6001537892331004		[learning rate: 1.9743e-05]
	Learning Rate: 1.97434e-05
	LOSS [training: 2.6001537892331004 | validation: 2.552706381652087]
	TIME [epoch: 8.31 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597249146657028		[learning rate: 1.9672e-05]
	Learning Rate: 1.96717e-05
	LOSS [training: 2.597249146657028 | validation: 2.5483492433543065]
	TIME [epoch: 8.29 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5969462872162055		[learning rate: 1.96e-05]
	Learning Rate: 1.96003e-05
	LOSS [training: 2.5969462872162055 | validation: 2.546836998352056]
	TIME [epoch: 8.29 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5998893580526024		[learning rate: 1.9529e-05]
	Learning Rate: 1.95292e-05
	LOSS [training: 2.5998893580526024 | validation: 2.5463124899005214]
	TIME [epoch: 8.29 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6003021732598137		[learning rate: 1.9458e-05]
	Learning Rate: 1.94583e-05
	LOSS [training: 2.6003021732598137 | validation: 2.561364826162622]
	TIME [epoch: 8.31 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600613184091338		[learning rate: 1.9388e-05]
	Learning Rate: 1.93877e-05
	LOSS [training: 2.600613184091338 | validation: 2.557539928973528]
	TIME [epoch: 8.29 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602145377767015		[learning rate: 1.9317e-05]
	Learning Rate: 1.93173e-05
	LOSS [training: 2.602145377767015 | validation: 2.5529058532057984]
	TIME [epoch: 8.29 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5994535127382106		[learning rate: 1.9247e-05]
	Learning Rate: 1.92472e-05
	LOSS [training: 2.5994535127382106 | validation: 2.549741003654194]
	TIME [epoch: 8.29 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5988795115914574		[learning rate: 1.9177e-05]
	Learning Rate: 1.91774e-05
	LOSS [training: 2.5988795115914574 | validation: 2.5459303853001036]
	TIME [epoch: 8.3 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.594709887644852		[learning rate: 1.9108e-05]
	Learning Rate: 1.91078e-05
	LOSS [training: 2.594709887644852 | validation: 2.5438493852817516]
	TIME [epoch: 8.31 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597395493908633		[learning rate: 1.9038e-05]
	Learning Rate: 1.90384e-05
	LOSS [training: 2.597395493908633 | validation: 2.547758742538897]
	TIME [epoch: 8.29 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599713554793705		[learning rate: 1.8969e-05]
	Learning Rate: 1.89694e-05
	LOSS [training: 2.599713554793705 | validation: 2.5476697991802157]
	TIME [epoch: 8.29 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599616316164402		[learning rate: 1.8901e-05]
	Learning Rate: 1.89005e-05
	LOSS [training: 2.599616316164402 | validation: 2.544484144563408]
	TIME [epoch: 8.3 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.595854236439898		[learning rate: 1.8832e-05]
	Learning Rate: 1.88319e-05
	LOSS [training: 2.595854236439898 | validation: 2.544745191681042]
	TIME [epoch: 8.31 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602892655947753		[learning rate: 1.8764e-05]
	Learning Rate: 1.87636e-05
	LOSS [training: 2.602892655947753 | validation: 2.5520100681236935]
	TIME [epoch: 8.29 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6019548876546197		[learning rate: 1.8695e-05]
	Learning Rate: 1.86955e-05
	LOSS [training: 2.6019548876546197 | validation: 2.5490820216632537]
	TIME [epoch: 8.29 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600187428612604		[learning rate: 1.8628e-05]
	Learning Rate: 1.86276e-05
	LOSS [training: 2.600187428612604 | validation: 2.5539175631128415]
	TIME [epoch: 8.29 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599564995818233		[learning rate: 1.856e-05]
	Learning Rate: 1.856e-05
	LOSS [training: 2.599564995818233 | validation: 2.556493981223051]
	TIME [epoch: 8.31 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5988081118018007		[learning rate: 1.8493e-05]
	Learning Rate: 1.84927e-05
	LOSS [training: 2.5988081118018007 | validation: 2.5441912101531248]
	TIME [epoch: 8.3 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597042823776179		[learning rate: 1.8426e-05]
	Learning Rate: 1.84256e-05
	LOSS [training: 2.597042823776179 | validation: 2.5606043095084097]
	TIME [epoch: 8.29 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5970408702639025		[learning rate: 1.8359e-05]
	Learning Rate: 1.83587e-05
	LOSS [training: 2.5970408702639025 | validation: 2.552636056815965]
	TIME [epoch: 8.3 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5992389620165453		[learning rate: 1.8292e-05]
	Learning Rate: 1.82921e-05
	LOSS [training: 2.5992389620165453 | validation: 2.5461564346328798]
	TIME [epoch: 8.3 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601334071199441		[learning rate: 1.8226e-05]
	Learning Rate: 1.82257e-05
	LOSS [training: 2.601334071199441 | validation: 2.5512435364697543]
	TIME [epoch: 8.31 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598799031265439		[learning rate: 1.816e-05]
	Learning Rate: 1.81596e-05
	LOSS [training: 2.598799031265439 | validation: 2.5456795435802055]
	TIME [epoch: 8.29 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600025484490591		[learning rate: 1.8094e-05]
	Learning Rate: 1.80937e-05
	LOSS [training: 2.600025484490591 | validation: 2.549235001335531]
	TIME [epoch: 8.29 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5987513033570515		[learning rate: 1.8028e-05]
	Learning Rate: 1.8028e-05
	LOSS [training: 2.5987513033570515 | validation: 2.547811994966923]
	TIME [epoch: 8.29 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600407991605212		[learning rate: 1.7963e-05]
	Learning Rate: 1.79626e-05
	LOSS [training: 2.600407991605212 | validation: 2.5441434570011134]
	TIME [epoch: 8.31 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60008008370345		[learning rate: 1.7897e-05]
	Learning Rate: 1.78974e-05
	LOSS [training: 2.60008008370345 | validation: 2.5520682282785185]
	TIME [epoch: 8.29 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.59656461141569		[learning rate: 1.7832e-05]
	Learning Rate: 1.78324e-05
	LOSS [training: 2.59656461141569 | validation: 2.5542909301941688]
	TIME [epoch: 8.29 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5979670908762076		[learning rate: 1.7768e-05]
	Learning Rate: 1.77677e-05
	LOSS [training: 2.5979670908762076 | validation: 2.5439600426330413]
	TIME [epoch: 8.29 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5986933412331217		[learning rate: 1.7703e-05]
	Learning Rate: 1.77032e-05
	LOSS [training: 2.5986933412331217 | validation: 2.5449874633027743]
	TIME [epoch: 8.31 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6002695597102696		[learning rate: 1.7639e-05]
	Learning Rate: 1.7639e-05
	LOSS [training: 2.6002695597102696 | validation: 2.541388861500768]
	TIME [epoch: 8.3 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5989029003490507		[learning rate: 1.7575e-05]
	Learning Rate: 1.7575e-05
	LOSS [training: 2.5989029003490507 | validation: 2.5539896715595125]
	TIME [epoch: 8.29 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600047362612071		[learning rate: 1.7511e-05]
	Learning Rate: 1.75112e-05
	LOSS [training: 2.600047362612071 | validation: 2.5584365885751605]
	TIME [epoch: 8.29 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603834211171622		[learning rate: 1.7448e-05]
	Learning Rate: 1.74476e-05
	LOSS [training: 2.603834211171622 | validation: 2.554130700158609]
	TIME [epoch: 8.3 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5989986808447636		[learning rate: 1.7384e-05]
	Learning Rate: 1.73843e-05
	LOSS [training: 2.5989986808447636 | validation: 2.5484493608561283]
	TIME [epoch: 8.31 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5969973676826337		[learning rate: 1.7321e-05]
	Learning Rate: 1.73212e-05
	LOSS [training: 2.5969973676826337 | validation: 2.5484026370845863]
	TIME [epoch: 8.28 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598343312980839		[learning rate: 1.7258e-05]
	Learning Rate: 1.72584e-05
	LOSS [training: 2.598343312980839 | validation: 2.5464673108472358]
	TIME [epoch: 8.29 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6007544017607644		[learning rate: 1.7196e-05]
	Learning Rate: 1.71957e-05
	LOSS [training: 2.6007544017607644 | validation: 2.5486250280209464]
	TIME [epoch: 8.29 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5979776607594585		[learning rate: 1.7133e-05]
	Learning Rate: 1.71333e-05
	LOSS [training: 2.5979776607594585 | validation: 2.5467968687485447]
	TIME [epoch: 8.31 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.59823159445335		[learning rate: 1.7071e-05]
	Learning Rate: 1.70712e-05
	LOSS [training: 2.59823159445335 | validation: 2.548532617059278]
	TIME [epoch: 8.29 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6017393141227254		[learning rate: 1.7009e-05]
	Learning Rate: 1.70092e-05
	LOSS [training: 2.6017393141227254 | validation: 2.54624680923724]
	TIME [epoch: 8.28 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598985645154303		[learning rate: 1.6947e-05]
	Learning Rate: 1.69475e-05
	LOSS [training: 2.598985645154303 | validation: 2.557862037282279]
	TIME [epoch: 8.29 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5999795106452552		[learning rate: 1.6886e-05]
	Learning Rate: 1.6886e-05
	LOSS [training: 2.5999795106452552 | validation: 2.5521582485367933]
	TIME [epoch: 8.3 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.59746967704765		[learning rate: 1.6825e-05]
	Learning Rate: 1.68247e-05
	LOSS [training: 2.59746967704765 | validation: 2.5464468986970212]
	TIME [epoch: 8.3 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598804068343317		[learning rate: 1.6764e-05]
	Learning Rate: 1.67636e-05
	LOSS [training: 2.598804068343317 | validation: 2.547667013562932]
	TIME [epoch: 8.29 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5977042938962733		[learning rate: 1.6703e-05]
	Learning Rate: 1.67028e-05
	LOSS [training: 2.5977042938962733 | validation: 2.550971506546034]
	TIME [epoch: 8.29 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5963808813597136		[learning rate: 1.6642e-05]
	Learning Rate: 1.66422e-05
	LOSS [training: 2.5963808813597136 | validation: 2.550397375054763]
	TIME [epoch: 8.29 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597876490769578		[learning rate: 1.6582e-05]
	Learning Rate: 1.65818e-05
	LOSS [training: 2.597876490769578 | validation: 2.5556315951767807]
	TIME [epoch: 8.31 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6006945169044444		[learning rate: 1.6522e-05]
	Learning Rate: 1.65216e-05
	LOSS [training: 2.6006945169044444 | validation: 2.5420313291109764]
	TIME [epoch: 8.29 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5987553107360175		[learning rate: 1.6462e-05]
	Learning Rate: 1.64617e-05
	LOSS [training: 2.5987553107360175 | validation: 2.5474877268836593]
	TIME [epoch: 8.29 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5986219998173183		[learning rate: 1.6402e-05]
	Learning Rate: 1.64019e-05
	LOSS [training: 2.5986219998173183 | validation: 2.5483958770449897]
	TIME [epoch: 8.29 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5946088970517907		[learning rate: 1.6342e-05]
	Learning Rate: 1.63424e-05
	LOSS [training: 2.5946088970517907 | validation: 2.55255375181767]
	TIME [epoch: 8.31 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.59917509498045		[learning rate: 1.6283e-05]
	Learning Rate: 1.62831e-05
	LOSS [training: 2.59917509498045 | validation: 2.5537636030762627]
	TIME [epoch: 8.29 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598781594523191		[learning rate: 1.6224e-05]
	Learning Rate: 1.6224e-05
	LOSS [training: 2.598781594523191 | validation: 2.547258041833328]
	TIME [epoch: 8.29 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6001317189004975		[learning rate: 1.6165e-05]
	Learning Rate: 1.61651e-05
	LOSS [training: 2.6001317189004975 | validation: 2.5637007105191527]
	TIME [epoch: 8.29 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5985755459474325		[learning rate: 1.6106e-05]
	Learning Rate: 1.61065e-05
	LOSS [training: 2.5985755459474325 | validation: 2.5555929217176727]
	TIME [epoch: 8.3 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5972831999407555		[learning rate: 1.6048e-05]
	Learning Rate: 1.6048e-05
	LOSS [training: 2.5972831999407555 | validation: 2.549445416394836]
	TIME [epoch: 8.3 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6056919665840317		[learning rate: 1.599e-05]
	Learning Rate: 1.59898e-05
	LOSS [training: 2.6056919665840317 | validation: 2.5496182956980227]
	TIME [epoch: 8.29 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599663726576517		[learning rate: 1.5932e-05]
	Learning Rate: 1.59317e-05
	LOSS [training: 2.599663726576517 | validation: 2.5503813381634624]
	TIME [epoch: 8.29 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5967721363691183		[learning rate: 1.5874e-05]
	Learning Rate: 1.58739e-05
	LOSS [training: 2.5967721363691183 | validation: 2.549613854290746]
	TIME [epoch: 8.29 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5983522303056077		[learning rate: 1.5816e-05]
	Learning Rate: 1.58163e-05
	LOSS [training: 2.5983522303056077 | validation: 2.562113960771536]
	TIME [epoch: 8.31 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599895718084421		[learning rate: 1.5759e-05]
	Learning Rate: 1.57589e-05
	LOSS [training: 2.599895718084421 | validation: 2.551092268479837]
	TIME [epoch: 8.29 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60264524666833		[learning rate: 1.5702e-05]
	Learning Rate: 1.57017e-05
	LOSS [training: 2.60264524666833 | validation: 2.5573797108226355]
	TIME [epoch: 8.29 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6016864989807287		[learning rate: 1.5645e-05]
	Learning Rate: 1.56447e-05
	LOSS [training: 2.6016864989807287 | validation: 2.5531586586986212]
	TIME [epoch: 8.29 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600671106072226		[learning rate: 1.5588e-05]
	Learning Rate: 1.5588e-05
	LOSS [training: 2.600671106072226 | validation: 2.552670281220836]
	TIME [epoch: 8.31 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5999598539626003		[learning rate: 1.5531e-05]
	Learning Rate: 1.55314e-05
	LOSS [training: 2.5999598539626003 | validation: 2.5592909176201415]
	TIME [epoch: 8.29 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598619791214742		[learning rate: 1.5475e-05]
	Learning Rate: 1.5475e-05
	LOSS [training: 2.598619791214742 | validation: 2.5480633183059775]
	TIME [epoch: 8.28 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5979613490238953		[learning rate: 1.5419e-05]
	Learning Rate: 1.54189e-05
	LOSS [training: 2.5979613490238953 | validation: 2.553948141624202]
	TIME [epoch: 8.29 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6003825353994614		[learning rate: 1.5363e-05]
	Learning Rate: 1.53629e-05
	LOSS [training: 2.6003825353994614 | validation: 2.5417609489896336]
	TIME [epoch: 8.3 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5981193118950836		[learning rate: 1.5307e-05]
	Learning Rate: 1.53072e-05
	LOSS [training: 2.5981193118950836 | validation: 2.5490855030385955]
	TIME [epoch: 8.3 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601284816886977		[learning rate: 1.5252e-05]
	Learning Rate: 1.52516e-05
	LOSS [training: 2.601284816886977 | validation: 2.550524185097149]
	TIME [epoch: 8.29 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5985433981009614		[learning rate: 1.5196e-05]
	Learning Rate: 1.51963e-05
	LOSS [training: 2.5985433981009614 | validation: 2.5514928716261034]
	TIME [epoch: 8.28 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5993956668278235		[learning rate: 1.5141e-05]
	Learning Rate: 1.51411e-05
	LOSS [training: 2.5993956668278235 | validation: 2.5488078777059813]
	TIME [epoch: 8.29 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597013290419652		[learning rate: 1.5086e-05]
	Learning Rate: 1.50862e-05
	LOSS [training: 2.597013290419652 | validation: 2.546039060772344]
	TIME [epoch: 8.31 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.594456212424645		[learning rate: 1.5031e-05]
	Learning Rate: 1.50314e-05
	LOSS [training: 2.594456212424645 | validation: 2.554735837874863]
	TIME [epoch: 8.29 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6017034123039813		[learning rate: 1.4977e-05]
	Learning Rate: 1.49769e-05
	LOSS [training: 2.6017034123039813 | validation: 2.5542943186634606]
	TIME [epoch: 8.29 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5986110934926585		[learning rate: 1.4923e-05]
	Learning Rate: 1.49225e-05
	LOSS [training: 2.5986110934926585 | validation: 2.5453489309426818]
	TIME [epoch: 8.29 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6005153964022645		[learning rate: 1.4868e-05]
	Learning Rate: 1.48684e-05
	LOSS [training: 2.6005153964022645 | validation: 2.5486715205651773]
	TIME [epoch: 8.31 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5963315199472823		[learning rate: 1.4814e-05]
	Learning Rate: 1.48144e-05
	LOSS [training: 2.5963315199472823 | validation: 2.5570872225736876]
	TIME [epoch: 8.29 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598914377968583		[learning rate: 1.4761e-05]
	Learning Rate: 1.47606e-05
	LOSS [training: 2.598914377968583 | validation: 2.546137739321552]
	TIME [epoch: 8.29 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5979536147453235		[learning rate: 1.4707e-05]
	Learning Rate: 1.47071e-05
	LOSS [training: 2.5979536147453235 | validation: 2.550301207475176]
	TIME [epoch: 8.28 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5998719770007765		[learning rate: 1.4654e-05]
	Learning Rate: 1.46537e-05
	LOSS [training: 2.5998719770007765 | validation: 2.5531869543468577]
	TIME [epoch: 8.3 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602601591779676		[learning rate: 1.4601e-05]
	Learning Rate: 1.46005e-05
	LOSS [training: 2.602601591779676 | validation: 2.5473702935875897]
	TIME [epoch: 8.3 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5972112247796555		[learning rate: 1.4548e-05]
	Learning Rate: 1.45475e-05
	LOSS [training: 2.5972112247796555 | validation: 2.5481864674160803]
	TIME [epoch: 8.29 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5937685093791893		[learning rate: 1.4495e-05]
	Learning Rate: 1.44947e-05
	LOSS [training: 2.5937685093791893 | validation: 2.5512800554733603]
	TIME [epoch: 8.29 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5998198601087035		[learning rate: 1.4442e-05]
	Learning Rate: 1.44421e-05
	LOSS [training: 2.5998198601087035 | validation: 2.54548492673326]
	TIME [epoch: 8.29 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6009303468637857		[learning rate: 1.439e-05]
	Learning Rate: 1.43897e-05
	LOSS [training: 2.6009303468637857 | validation: 2.5529529272915967]
	TIME [epoch: 8.32 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600832983996909		[learning rate: 1.4338e-05]
	Learning Rate: 1.43375e-05
	LOSS [training: 2.600832983996909 | validation: 2.5501227080878914]
	TIME [epoch: 8.29 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5992811209652196		[learning rate: 1.4285e-05]
	Learning Rate: 1.42855e-05
	LOSS [training: 2.5992811209652196 | validation: 2.552552948537732]
	TIME [epoch: 8.28 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6015955892754095		[learning rate: 1.4234e-05]
	Learning Rate: 1.42336e-05
	LOSS [training: 2.6015955892754095 | validation: 2.545018764372074]
	TIME [epoch: 8.29 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5970111573325685		[learning rate: 1.4182e-05]
	Learning Rate: 1.4182e-05
	LOSS [training: 2.5970111573325685 | validation: 2.5463551386138326]
	TIME [epoch: 8.31 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5993527983647033		[learning rate: 1.4131e-05]
	Learning Rate: 1.41305e-05
	LOSS [training: 2.5993527983647033 | validation: 2.550998710435861]
	TIME [epoch: 8.29 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5995735801522937		[learning rate: 1.4079e-05]
	Learning Rate: 1.40792e-05
	LOSS [training: 2.5995735801522937 | validation: 2.5529402543758852]
	TIME [epoch: 8.29 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597079629636947		[learning rate: 1.4028e-05]
	Learning Rate: 1.40281e-05
	LOSS [training: 2.597079629636947 | validation: 2.549631740889919]
	TIME [epoch: 8.29 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5998999055409118		[learning rate: 1.3977e-05]
	Learning Rate: 1.39772e-05
	LOSS [training: 2.5998999055409118 | validation: 2.550255329992871]
	TIME [epoch: 8.3 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598507279228245		[learning rate: 1.3927e-05]
	Learning Rate: 1.39265e-05
	LOSS [training: 2.598507279228245 | validation: 2.5486505992999904]
	TIME [epoch: 8.3 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5987730891927354		[learning rate: 1.3876e-05]
	Learning Rate: 1.3876e-05
	LOSS [training: 2.5987730891927354 | validation: 2.5531586113820106]
	TIME [epoch: 8.29 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5991052495787366		[learning rate: 1.3826e-05]
	Learning Rate: 1.38256e-05
	LOSS [training: 2.5991052495787366 | validation: 2.540347606419811]
	TIME [epoch: 8.29 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598168847439892		[learning rate: 1.3775e-05]
	Learning Rate: 1.37754e-05
	LOSS [training: 2.598168847439892 | validation: 2.5505424029577775]
	TIME [epoch: 8.29 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600014893408526		[learning rate: 1.3725e-05]
	Learning Rate: 1.37254e-05
	LOSS [training: 2.600014893408526 | validation: 2.5484253458968977]
	TIME [epoch: 8.32 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6027983803378736		[learning rate: 1.3676e-05]
	Learning Rate: 1.36756e-05
	LOSS [training: 2.6027983803378736 | validation: 2.5445306649036437]
	TIME [epoch: 8.29 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600561542162313		[learning rate: 1.3626e-05]
	Learning Rate: 1.3626e-05
	LOSS [training: 2.600561542162313 | validation: 2.546245063271754]
	TIME [epoch: 8.29 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5997867012611957		[learning rate: 1.3577e-05]
	Learning Rate: 1.35766e-05
	LOSS [training: 2.5997867012611957 | validation: 2.548647311963476]
	TIME [epoch: 8.28 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5975323883057917		[learning rate: 1.3527e-05]
	Learning Rate: 1.35273e-05
	LOSS [training: 2.5975323883057917 | validation: 2.551507587078107]
	TIME [epoch: 8.3 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5977353655375897		[learning rate: 1.3478e-05]
	Learning Rate: 1.34782e-05
	LOSS [training: 2.5977353655375897 | validation: 2.5443477095095752]
	TIME [epoch: 8.29 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601864172514953		[learning rate: 1.3429e-05]
	Learning Rate: 1.34293e-05
	LOSS [training: 2.601864172514953 | validation: 2.5543177316696353]
	TIME [epoch: 8.29 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598888666423553		[learning rate: 1.3381e-05]
	Learning Rate: 1.33805e-05
	LOSS [training: 2.598888666423553 | validation: 2.5467329778574572]
	TIME [epoch: 8.29 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6012908607589975		[learning rate: 1.3332e-05]
	Learning Rate: 1.3332e-05
	LOSS [training: 2.6012908607589975 | validation: 2.5551218306281402]
	TIME [epoch: 8.3 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5978443295496656		[learning rate: 1.3284e-05]
	Learning Rate: 1.32836e-05
	LOSS [training: 2.5978443295496656 | validation: 2.5478678907604713]
	TIME [epoch: 8.31 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.602916934484738		[learning rate: 1.3235e-05]
	Learning Rate: 1.32354e-05
	LOSS [training: 2.602916934484738 | validation: 2.5535728642618643]
	TIME [epoch: 8.29 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599035252264506		[learning rate: 1.3187e-05]
	Learning Rate: 1.31874e-05
	LOSS [training: 2.599035252264506 | validation: 2.5511435774383844]
	TIME [epoch: 8.3 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599611577426214		[learning rate: 1.314e-05]
	Learning Rate: 1.31395e-05
	LOSS [training: 2.599611577426214 | validation: 2.5529618583251863]
	TIME [epoch: 8.29 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5975247280505345		[learning rate: 1.3092e-05]
	Learning Rate: 1.30918e-05
	LOSS [training: 2.5975247280505345 | validation: 2.5578484526477165]
	TIME [epoch: 8.33 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5966445901316915		[learning rate: 1.3044e-05]
	Learning Rate: 1.30443e-05
	LOSS [training: 2.5966445901316915 | validation: 2.5506778141694633]
	TIME [epoch: 8.3 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6003588845811985		[learning rate: 1.2997e-05]
	Learning Rate: 1.2997e-05
	LOSS [training: 2.6003588845811985 | validation: 2.5432012792446566]
	TIME [epoch: 8.29 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6003545957070635		[learning rate: 1.295e-05]
	Learning Rate: 1.29498e-05
	LOSS [training: 2.6003545957070635 | validation: 2.5547075152342806]
	TIME [epoch: 8.3 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5976104903759536		[learning rate: 1.2903e-05]
	Learning Rate: 1.29028e-05
	LOSS [training: 2.5976104903759536 | validation: 2.5558822797664007]
	TIME [epoch: 8.32 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601662266904254		[learning rate: 1.2856e-05]
	Learning Rate: 1.2856e-05
	LOSS [training: 2.601662266904254 | validation: 2.56161271084946]
	TIME [epoch: 8.3 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600668727964792		[learning rate: 1.2809e-05]
	Learning Rate: 1.28093e-05
	LOSS [training: 2.600668727964792 | validation: 2.549776036563153]
	TIME [epoch: 8.3 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5981979484042577		[learning rate: 1.2763e-05]
	Learning Rate: 1.27628e-05
	LOSS [training: 2.5981979484042577 | validation: 2.5461097615453103]
	TIME [epoch: 8.3 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5983363354555027		[learning rate: 1.2717e-05]
	Learning Rate: 1.27165e-05
	LOSS [training: 2.5983363354555027 | validation: 2.538239102770876]
	TIME [epoch: 8.3 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599224897520473		[learning rate: 1.267e-05]
	Learning Rate: 1.26704e-05
	LOSS [training: 2.599224897520473 | validation: 2.537760001577275]
	TIME [epoch: 8.32 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5986633652230053		[learning rate: 1.2624e-05]
	Learning Rate: 1.26244e-05
	LOSS [training: 2.5986633652230053 | validation: 2.548163489778518]
	TIME [epoch: 8.3 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5998201559368725		[learning rate: 1.2579e-05]
	Learning Rate: 1.25786e-05
	LOSS [training: 2.5998201559368725 | validation: 2.543941978444505]
	TIME [epoch: 8.3 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5989325102749623		[learning rate: 1.2533e-05]
	Learning Rate: 1.25329e-05
	LOSS [training: 2.5989325102749623 | validation: 2.552399009797388]
	TIME [epoch: 8.3 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5996334823227047		[learning rate: 1.2487e-05]
	Learning Rate: 1.24874e-05
	LOSS [training: 2.5996334823227047 | validation: 2.5607698212904215]
	TIME [epoch: 8.32 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600798157877884		[learning rate: 1.2442e-05]
	Learning Rate: 1.24421e-05
	LOSS [training: 2.600798157877884 | validation: 2.5517504716771997]
	TIME [epoch: 8.3 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6001501072311495		[learning rate: 1.2397e-05]
	Learning Rate: 1.2397e-05
	LOSS [training: 2.6001501072311495 | validation: 2.543773957694344]
	TIME [epoch: 8.3 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.593351315702211		[learning rate: 1.2352e-05]
	Learning Rate: 1.2352e-05
	LOSS [training: 2.593351315702211 | validation: 2.5568542923828668]
	TIME [epoch: 8.3 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5975448407284123		[learning rate: 1.2307e-05]
	Learning Rate: 1.23072e-05
	LOSS [training: 2.5975448407284123 | validation: 2.5456438953508824]
	TIME [epoch: 8.32 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601803032720581		[learning rate: 1.2263e-05]
	Learning Rate: 1.22625e-05
	LOSS [training: 2.601803032720581 | validation: 2.560657648954286]
	TIME [epoch: 8.3 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597520807746929		[learning rate: 1.2218e-05]
	Learning Rate: 1.2218e-05
	LOSS [training: 2.597520807746929 | validation: 2.538673967126831]
	TIME [epoch: 8.3 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6015480291899		[learning rate: 1.2174e-05]
	Learning Rate: 1.21737e-05
	LOSS [training: 2.6015480291899 | validation: 2.557884704731051]
	TIME [epoch: 8.3 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6022487493029884		[learning rate: 1.2129e-05]
	Learning Rate: 1.21295e-05
	LOSS [training: 2.6022487493029884 | validation: 2.546109537534546]
	TIME [epoch: 8.31 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5988062508734666		[learning rate: 1.2085e-05]
	Learning Rate: 1.20855e-05
	LOSS [training: 2.5988062508734666 | validation: 2.546939975705603]
	TIME [epoch: 8.32 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5993868330569283		[learning rate: 1.2042e-05]
	Learning Rate: 1.20416e-05
	LOSS [training: 2.5993868330569283 | validation: 2.5507388680205687]
	TIME [epoch: 8.3 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.603970253549735		[learning rate: 1.1998e-05]
	Learning Rate: 1.19979e-05
	LOSS [training: 2.603970253549735 | validation: 2.555151752174643]
	TIME [epoch: 8.29 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5983758062887445		[learning rate: 1.1954e-05]
	Learning Rate: 1.19544e-05
	LOSS [training: 2.5983758062887445 | validation: 2.5554321840109817]
	TIME [epoch: 8.29 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6011187292055675		[learning rate: 1.1911e-05]
	Learning Rate: 1.1911e-05
	LOSS [training: 2.6011187292055675 | validation: 2.555076750344137]
	TIME [epoch: 8.32 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5967443218249846		[learning rate: 1.1868e-05]
	Learning Rate: 1.18678e-05
	LOSS [training: 2.5967443218249846 | validation: 2.5529558058326556]
	TIME [epoch: 8.29 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.596225438586127		[learning rate: 1.1825e-05]
	Learning Rate: 1.18247e-05
	LOSS [training: 2.596225438586127 | validation: 2.5557831131363677]
	TIME [epoch: 8.29 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5958981760067883		[learning rate: 1.1782e-05]
	Learning Rate: 1.17818e-05
	LOSS [training: 2.5958981760067883 | validation: 2.5514617137235924]
	TIME [epoch: 8.29 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600813417546628		[learning rate: 1.1739e-05]
	Learning Rate: 1.1739e-05
	LOSS [training: 2.600813417546628 | validation: 2.559782120144957]
	TIME [epoch: 8.31 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5972287390897524		[learning rate: 1.1696e-05]
	Learning Rate: 1.16964e-05
	LOSS [training: 2.5972287390897524 | validation: 2.5512244315758954]
	TIME [epoch: 8.29 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.592783070155393		[learning rate: 1.1654e-05]
	Learning Rate: 1.1654e-05
	LOSS [training: 2.592783070155393 | validation: 2.5466815767638966]
	TIME [epoch: 8.3 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.60191310926431		[learning rate: 1.1612e-05]
	Learning Rate: 1.16117e-05
	LOSS [training: 2.60191310926431 | validation: 2.549033585185228]
	TIME [epoch: 8.29 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5992583266008387		[learning rate: 1.157e-05]
	Learning Rate: 1.15695e-05
	LOSS [training: 2.5992583266008387 | validation: 2.5454262908937]
	TIME [epoch: 8.29 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597741344116546		[learning rate: 1.1528e-05]
	Learning Rate: 1.15275e-05
	LOSS [training: 2.597741344116546 | validation: 2.5508121367760204]
	TIME [epoch: 8.31 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597583765083882		[learning rate: 1.1486e-05]
	Learning Rate: 1.14857e-05
	LOSS [training: 2.597583765083882 | validation: 2.5516939554659928]
	TIME [epoch: 8.29 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6012271198734647		[learning rate: 1.1444e-05]
	Learning Rate: 1.1444e-05
	LOSS [training: 2.6012271198734647 | validation: 2.5520912232696773]
	TIME [epoch: 8.29 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6007518067043		[learning rate: 1.1402e-05]
	Learning Rate: 1.14025e-05
	LOSS [training: 2.6007518067043 | validation: 2.5534260118253336]
	TIME [epoch: 8.29 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6019904713382083		[learning rate: 1.1361e-05]
	Learning Rate: 1.13611e-05
	LOSS [training: 2.6019904713382083 | validation: 2.5392241732437637]
	TIME [epoch: 8.31 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.594934818733428		[learning rate: 1.132e-05]
	Learning Rate: 1.13199e-05
	LOSS [training: 2.594934818733428 | validation: 2.5529646707982163]
	TIME [epoch: 8.29 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597219586566601		[learning rate: 1.1279e-05]
	Learning Rate: 1.12788e-05
	LOSS [training: 2.597219586566601 | validation: 2.546319395799772]
	TIME [epoch: 8.29 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599957912054024		[learning rate: 1.1238e-05]
	Learning Rate: 1.12379e-05
	LOSS [training: 2.599957912054024 | validation: 2.5452031218976385]
	TIME [epoch: 8.29 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5986854050685237		[learning rate: 1.1197e-05]
	Learning Rate: 1.11971e-05
	LOSS [training: 2.5986854050685237 | validation: 2.548651871953731]
	TIME [epoch: 8.31 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6012118968618685		[learning rate: 1.1156e-05]
	Learning Rate: 1.11565e-05
	LOSS [training: 2.6012118968618685 | validation: 2.5527958247504317]
	TIME [epoch: 8.31 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6023409358060494		[learning rate: 1.1116e-05]
	Learning Rate: 1.1116e-05
	LOSS [training: 2.6023409358060494 | validation: 2.54666418991519]
	TIME [epoch: 8.29 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6005910860765775		[learning rate: 1.1076e-05]
	Learning Rate: 1.10756e-05
	LOSS [training: 2.6005910860765775 | validation: 2.5530839946016193]
	TIME [epoch: 8.29 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5977463378390433		[learning rate: 1.1035e-05]
	Learning Rate: 1.10354e-05
	LOSS [training: 2.5977463378390433 | validation: 2.545364151321772]
	TIME [epoch: 8.29 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6007922801402836		[learning rate: 1.0995e-05]
	Learning Rate: 1.09954e-05
	LOSS [training: 2.6007922801402836 | validation: 2.5456983593912876]
	TIME [epoch: 8.32 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600675532458238		[learning rate: 1.0955e-05]
	Learning Rate: 1.09555e-05
	LOSS [training: 2.600675532458238 | validation: 2.5464041233069885]
	TIME [epoch: 8.29 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5979126915254596		[learning rate: 1.0916e-05]
	Learning Rate: 1.09157e-05
	LOSS [training: 2.5979126915254596 | validation: 2.5415230753849025]
	TIME [epoch: 8.29 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5959816137147955		[learning rate: 1.0876e-05]
	Learning Rate: 1.08761e-05
	LOSS [training: 2.5959816137147955 | validation: 2.556093892510476]
	TIME [epoch: 8.29 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5984223009763143		[learning rate: 1.0837e-05]
	Learning Rate: 1.08366e-05
	LOSS [training: 2.5984223009763143 | validation: 2.547911232045415]
	TIME [epoch: 8.31 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5935124203144633		[learning rate: 1.0797e-05]
	Learning Rate: 1.07973e-05
	LOSS [training: 2.5935124203144633 | validation: 2.5521666766765363]
	TIME [epoch: 8.29 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5977050278988556		[learning rate: 1.0758e-05]
	Learning Rate: 1.07581e-05
	LOSS [training: 2.5977050278988556 | validation: 2.545420311535881]
	TIME [epoch: 8.29 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.597487144886881		[learning rate: 1.0719e-05]
	Learning Rate: 1.07191e-05
	LOSS [training: 2.597487144886881 | validation: 2.5541054140795723]
	TIME [epoch: 8.29 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601719826385845		[learning rate: 1.068e-05]
	Learning Rate: 1.06802e-05
	LOSS [training: 2.601719826385845 | validation: 2.554943746885329]
	TIME [epoch: 8.3 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598510557652285		[learning rate: 1.0641e-05]
	Learning Rate: 1.06414e-05
	LOSS [training: 2.598510557652285 | validation: 2.558559922627609]
	TIME [epoch: 8.31 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.600507658534784		[learning rate: 1.0603e-05]
	Learning Rate: 1.06028e-05
	LOSS [training: 2.600507658534784 | validation: 2.5480675624459033]
	TIME [epoch: 8.29 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.599666500285775		[learning rate: 1.0564e-05]
	Learning Rate: 1.05643e-05
	LOSS [training: 2.599666500285775 | validation: 2.557706200982498]
	TIME [epoch: 8.29 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6000604248767787		[learning rate: 1.0526e-05]
	Learning Rate: 1.0526e-05
	LOSS [training: 2.6000604248767787 | validation: 2.5436691602871377]
	TIME [epoch: 8.29 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5997451247037824		[learning rate: 1.0488e-05]
	Learning Rate: 1.04878e-05
	LOSS [training: 2.5997451247037824 | validation: 2.553660359460703]
	TIME [epoch: 8.32 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6002674757103876		[learning rate: 1.045e-05]
	Learning Rate: 1.04497e-05
	LOSS [training: 2.6002674757103876 | validation: 2.55330249193179]
	TIME [epoch: 8.3 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.601076004003611		[learning rate: 1.0412e-05]
	Learning Rate: 1.04118e-05
	LOSS [training: 2.601076004003611 | validation: 2.5500748175499055]
	TIME [epoch: 8.3 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.598904273861409		[learning rate: 1.0374e-05]
	Learning Rate: 1.0374e-05
	LOSS [training: 2.598904273861409 | validation: 2.550065718864627]
	TIME [epoch: 8.29 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5985213344056235		[learning rate: 1.0336e-05]
	Learning Rate: 1.03364e-05
	LOSS [training: 2.5985213344056235 | validation: 2.553121368015778]
	TIME [epoch: 8.31 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.594122533342211		[learning rate: 1.0299e-05]
	Learning Rate: 1.02989e-05
	LOSS [training: 2.594122533342211 | validation: 2.5545436359117772]
	TIME [epoch: 8.3 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5975461397905915		[learning rate: 1.0261e-05]
	Learning Rate: 1.02615e-05
	LOSS [training: 2.5975461397905915 | validation: 2.5472356301721613]
	TIME [epoch: 8.29 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6001128735608927		[learning rate: 1.0224e-05]
	Learning Rate: 1.02243e-05
	LOSS [training: 2.6001128735608927 | validation: 2.5523961304526033]
	TIME [epoch: 8.29 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5973995914075303		[learning rate: 1.0187e-05]
	Learning Rate: 1.01872e-05
	LOSS [training: 2.5973995914075303 | validation: 2.5548665286968593]
	TIME [epoch: 8.31 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6012227491117215		[learning rate: 1.015e-05]
	Learning Rate: 1.01502e-05
	LOSS [training: 2.6012227491117215 | validation: 2.5458450735551046]
	TIME [epoch: 8.3 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.59721774307913		[learning rate: 1.0113e-05]
	Learning Rate: 1.01133e-05
	LOSS [training: 2.59721774307913 | validation: 2.5491436035781625]
	TIME [epoch: 8.29 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6000090781969405		[learning rate: 1.0077e-05]
	Learning Rate: 1.00766e-05
	LOSS [training: 2.6000090781969405 | validation: 2.552855090946264]
	TIME [epoch: 8.29 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6002934082905056		[learning rate: 1.004e-05]
	Learning Rate: 1.00401e-05
	LOSS [training: 2.6002934082905056 | validation: 2.551647589795448]
	TIME [epoch: 8.3 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5998012263206505		[learning rate: 1.0004e-05]
	Learning Rate: 1.00036e-05
	LOSS [training: 2.5998012263206505 | validation: 2.5459548173770434]
	TIME [epoch: 8.31 sec]
Finished training in 16750.503 seconds.
