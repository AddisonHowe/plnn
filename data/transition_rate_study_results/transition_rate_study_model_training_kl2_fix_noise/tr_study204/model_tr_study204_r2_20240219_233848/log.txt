Args:
Namespace(name='model_tr_study204', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2', training_data='data/transition_rate_studies/tr_study204/tr_study204_training/r2', validation_data='data/transition_rate_studies/tr_study204/tr_study204_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3261072979

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/10] avg loss: 11.090575596617605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.090575596617605 | validation: 11.32364483530608]
	TIME [epoch: 78.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/10] avg loss: 10.420631452846708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.420631452846708 | validation: 9.34272243058167]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.760596488057622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.760596488057622 | validation: 8.168127840341466]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.288555434082625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.288555434082625 | validation: 6.349048383105864]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.691850189587251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.691850189587251 | validation: 5.74999405305379]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.048845371879817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.048845371879817 | validation: 4.482849056884125]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.650990832105239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.650990832105239 | validation: 4.105066883699788]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.934225182401476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.934225182401476 | validation: 4.007629563826885]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.0682487924436845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.0682487924436845 | validation: 4.361830554008776]
	TIME [epoch: 8.32 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.745142866323884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.745142866323884 | validation: 3.905753485580071]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.965886191928611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.965886191928611 | validation: 3.6989220694345475]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.870481897331413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.870481897331413 | validation: 4.122261086229565]
	TIME [epoch: 8.3 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.954629446559361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.954629446559361 | validation: 3.571364174894815]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.652360389950852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.652360389950852 | validation: 3.8421804824767882]
	TIME [epoch: 8.31 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.504849422095636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.504849422095636 | validation: 4.124615622949229]
	TIME [epoch: 8.3 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.7485627928232335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7485627928232335 | validation: 3.93864098682719]
	TIME [epoch: 8.3 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.196695694939274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.196695694939274 | validation: 3.7211175964064225]
	TIME [epoch: 8.32 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.6256750554657255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6256750554657255 | validation: 3.730494569526249]
	TIME [epoch: 8.3 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.776521395654183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.776521395654183 | validation: 3.6647616261742035]
	TIME [epoch: 8.3 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.547528933130955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.547528933130955 | validation: 3.760817481481963]
	TIME [epoch: 8.3 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.742960500314822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.742960500314822 | validation: 3.9938885956440067]
	TIME [epoch: 8.31 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.747714231131427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.747714231131427 | validation: 3.659891583423944]
	TIME [epoch: 8.31 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.342245787347692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.342245787347692 | validation: 4.629204106713251]
	TIME [epoch: 8.3 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.64972665437029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.64972665437029 | validation: 3.4805700637930768]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.9352926123778325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9352926123778325 | validation: 4.363895548216019]
	TIME [epoch: 8.32 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.707515980401739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.707515980401739 | validation: 4.087855187539617]
	TIME [epoch: 8.3 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.552023227966883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.552023227966883 | validation: 3.7332537509883856]
	TIME [epoch: 8.3 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.607133171284039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.607133171284039 | validation: 3.4922499619061207]
	TIME [epoch: 8.29 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.5002630927936575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5002630927936575 | validation: 3.561318957788207]
	TIME [epoch: 8.3 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.363688337938143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.363688337938143 | validation: 4.545412263065159]
	TIME [epoch: 8.32 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.600081480198737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.600081480198737 | validation: 4.042279753966158]
	TIME [epoch: 8.3 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.583430981366958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.583430981366958 | validation: 4.621362537481884]
	TIME [epoch: 8.3 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.47079816465364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.47079816465364 | validation: 4.810170785728008]
	TIME [epoch: 8.3 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.468252807171606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.468252807171606 | validation: 3.5975214731695067]
	TIME [epoch: 8.33 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.2847937379703875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2847937379703875 | validation: 4.1667726420643305]
	TIME [epoch: 8.29 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.377780589661072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.377780589661072 | validation: 3.9864863995217616]
	TIME [epoch: 8.3 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.405057087847194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.405057087847194 | validation: 4.3105358408406795]
	TIME [epoch: 8.3 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.4625964784085514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4625964784085514 | validation: 5.160523340056487]
	TIME [epoch: 8.33 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.4673552066079925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4673552066079925 | validation: 3.662228181271902]
	TIME [epoch: 8.3 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1570921114897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1570921114897 | validation: 5.232959614336837]
	TIME [epoch: 8.3 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.554432241026091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.554432241026091 | validation: 3.8516592309774103]
	TIME [epoch: 8.3 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.165321916338488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.165321916338488 | validation: 3.3249604724950936]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.150021865672851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.150021865672851 | validation: 3.9673033254921313]
	TIME [epoch: 8.3 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.253096925960703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.253096925960703 | validation: 3.425184554504076]
	TIME [epoch: 8.31 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.125268288986886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.125268288986886 | validation: 3.3001007146169217]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.4342375310806155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4342375310806155 | validation: 3.3152686510495633]
	TIME [epoch: 8.31 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.3184327540021865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3184327540021865 | validation: 3.2534894842899558]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.489306632486861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.489306632486861 | validation: 3.1247529539364223]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.433597895269019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.433597895269019 | validation: 3.8978692559072865]
	TIME [epoch: 8.3 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.14992733550493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.14992733550493 | validation: 3.4432108416389973]
	TIME [epoch: 8.32 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.30001798684935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.30001798684935 | validation: 3.66466871323213]
	TIME [epoch: 8.29 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.614436896673814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.614436896673814 | validation: 3.3865012271310277]
	TIME [epoch: 8.3 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.625623347657934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.625623347657934 | validation: 3.024550593044305]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.118576650742183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.118576650742183 | validation: 3.3384714781426594]
	TIME [epoch: 8.32 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.541592340368789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.541592340368789 | validation: 3.643169507608105]
	TIME [epoch: 8.29 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.399619220429329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.399619220429329 | validation: 3.3334161640144035]
	TIME [epoch: 8.3 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.14092078349772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.14092078349772 | validation: 3.780970623393971]
	TIME [epoch: 8.29 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.510601816398177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.510601816398177 | validation: 3.5780131983775423]
	TIME [epoch: 8.31 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.2751268496362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2751268496362 | validation: 3.5555404360082603]
	TIME [epoch: 8.3 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.164683673302714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.164683673302714 | validation: 7.639483945956192]
	TIME [epoch: 8.29 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.858317603640962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.858317603640962 | validation: 2.986612925340461]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.381061548901923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.381061548901923 | validation: 3.2973095164858055]
	TIME [epoch: 8.32 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.113986671453272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.113986671453272 | validation: 3.807057832109408]
	TIME [epoch: 8.31 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.186875573921318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.186875573921318 | validation: 3.357977731606238]
	TIME [epoch: 8.29 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.061619939795549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.061619939795549 | validation: 3.125021284011517]
	TIME [epoch: 8.29 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.979379206459808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.979379206459808 | validation: 4.016065055282986]
	TIME [epoch: 8.29 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.234906016448159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.234906016448159 | validation: 4.480411981284856]
	TIME [epoch: 8.32 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1444832227521955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1444832227521955 | validation: 3.553779224354605]
	TIME [epoch: 8.29 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.099889200654057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.099889200654057 | validation: 2.9120983075647136]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.10555137780353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.10555137780353 | validation: 3.4667222246007054]
	TIME [epoch: 8.3 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.952707007260147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.952707007260147 | validation: 2.984683951260599]
	TIME [epoch: 8.32 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.920871367474831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.920871367474831 | validation: 3.022694586270357]
	TIME [epoch: 8.29 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.203882217523215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.203882217523215 | validation: 3.005111088988868]
	TIME [epoch: 8.3 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.5124045038774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5124045038774 | validation: 2.9831250274229397]
	TIME [epoch: 8.29 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.358702699005709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.358702699005709 | validation: 3.024892786636628]
	TIME [epoch: 8.31 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.441628140980049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.441628140980049 | validation: 3.145860585180511]
	TIME [epoch: 8.29 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.17219366837903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.17219366837903 | validation: 3.101105609127969]
	TIME [epoch: 8.29 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8760054587170423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8760054587170423 | validation: 3.31352652558727]
	TIME [epoch: 8.29 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.87815077607097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.87815077607097 | validation: 3.0534423074840467]
	TIME [epoch: 8.32 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.976522151566317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.976522151566317 | validation: 4.09258200031066]
	TIME [epoch: 8.3 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.033350672617521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.033350672617521 | validation: 2.8574145851910036]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.3890571990323215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3890571990323215 | validation: 3.1443022643372442]
	TIME [epoch: 8.3 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.035569706251559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.035569706251559 | validation: 3.207369852051017]
	TIME [epoch: 8.33 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.11339005569551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.11339005569551 | validation: 3.1323357178440148]
	TIME [epoch: 8.3 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.207653572704541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.207653572704541 | validation: 3.218683890262905]
	TIME [epoch: 8.29 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.354653312771881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.354653312771881 | validation: 2.9790734121550844]
	TIME [epoch: 8.3 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.277078912818955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.277078912818955 | validation: 3.003102910304765]
	TIME [epoch: 8.32 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8949617376894743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8949617376894743 | validation: 2.845506573922373]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9285082243201734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9285082243201734 | validation: 2.871683783886506]
	TIME [epoch: 8.3 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8681320376982784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8681320376982784 | validation: 2.884881458026025]
	TIME [epoch: 8.3 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8183925549808633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8183925549808633 | validation: 4.449134649763047]
	TIME [epoch: 8.33 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1053945267488015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1053945267488015 | validation: 2.9131439081531294]
	TIME [epoch: 8.31 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8391884689677673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8391884689677673 | validation: 3.013522946574309]
	TIME [epoch: 8.3 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8829051081286616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8829051081286616 | validation: 4.527955847336942]
	TIME [epoch: 8.3 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.988002663811579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.988002663811579 | validation: 3.4159268318831093]
	TIME [epoch: 8.31 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.69568697399304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.69568697399304 | validation: 2.9064680579249864]
	TIME [epoch: 8.32 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.024099619410943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.024099619410943 | validation: 2.820837583065855]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.032657847380517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.032657847380517 | validation: 2.9636788425055265]
	TIME [epoch: 8.3 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.288824610862767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.288824610862767 | validation: 2.9437773784612786]
	TIME [epoch: 8.31 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.768043039181088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.768043039181088 | validation: 3.1817164475974966]
	TIME [epoch: 8.32 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.825886653846222		[learning rate: 0.0099673]
	Learning Rate: 0.00996733
	LOSS [training: 3.825886653846222 | validation: 3.1793888492274567]
	TIME [epoch: 8.3 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7488629846178965		[learning rate: 0.0099312]
	Learning Rate: 0.00993116
	LOSS [training: 3.7488629846178965 | validation: 3.339963876462761]
	TIME [epoch: 8.3 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.834921174235201		[learning rate: 0.0098951]
	Learning Rate: 0.00989512
	LOSS [training: 3.834921174235201 | validation: 2.934173358336669]
	TIME [epoch: 8.3 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.735935440229459		[learning rate: 0.0098592]
	Learning Rate: 0.00985921
	LOSS [training: 3.735935440229459 | validation: 3.116084158829998]
	TIME [epoch: 8.33 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7004638733467097		[learning rate: 0.0098234]
	Learning Rate: 0.00982343
	LOSS [training: 3.7004638733467097 | validation: 3.008448248739762]
	TIME [epoch: 8.3 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.697452267206807		[learning rate: 0.0097878]
	Learning Rate: 0.00978778
	LOSS [training: 3.697452267206807 | validation: 2.894098553317416]
	TIME [epoch: 8.3 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7368637326649745		[learning rate: 0.0097523]
	Learning Rate: 0.00975226
	LOSS [training: 3.7368637326649745 | validation: 4.003588541717034]
	TIME [epoch: 8.29 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8119968800565465		[learning rate: 0.0097169]
	Learning Rate: 0.00971687
	LOSS [training: 3.8119968800565465 | validation: 2.918148016599938]
	TIME [epoch: 8.32 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.67094727777233		[learning rate: 0.0096816]
	Learning Rate: 0.0096816
	LOSS [training: 3.67094727777233 | validation: 3.210830475832808]
	TIME [epoch: 8.3 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6857771123675382		[learning rate: 0.0096465]
	Learning Rate: 0.00964647
	LOSS [training: 3.6857771123675382 | validation: 3.3012527482280962]
	TIME [epoch: 8.3 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8419888542943452		[learning rate: 0.0096115]
	Learning Rate: 0.00961146
	LOSS [training: 3.8419888542943452 | validation: 2.7592114123804325]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.118254665450521		[learning rate: 0.0095766]
	Learning Rate: 0.00957658
	LOSS [training: 4.118254665450521 | validation: 2.7680882521570944]
	TIME [epoch: 8.33 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8272440074265845		[learning rate: 0.0095418]
	Learning Rate: 0.00954183
	LOSS [training: 3.8272440074265845 | validation: 4.226873856085883]
	TIME [epoch: 8.3 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.038621915304031		[learning rate: 0.0095072]
	Learning Rate: 0.0095072
	LOSS [training: 4.038621915304031 | validation: 3.825507134102227]
	TIME [epoch: 8.3 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.034759125650906		[learning rate: 0.0094727]
	Learning Rate: 0.0094727
	LOSS [training: 4.034759125650906 | validation: 3.0204667850197127]
	TIME [epoch: 8.29 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9157468895910426		[learning rate: 0.0094383]
	Learning Rate: 0.00943832
	LOSS [training: 3.9157468895910426 | validation: 3.8217612662354465]
	TIME [epoch: 8.32 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.825083073471731		[learning rate: 0.0094041]
	Learning Rate: 0.00940407
	LOSS [training: 3.825083073471731 | validation: 2.7333311052035962]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.087865127921631		[learning rate: 0.0093699]
	Learning Rate: 0.00936994
	LOSS [training: 4.087865127921631 | validation: 3.1452033523672602]
	TIME [epoch: 8.3 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9070532883116087		[learning rate: 0.0093359]
	Learning Rate: 0.00933594
	LOSS [training: 3.9070532883116087 | validation: 3.115167154236821]
	TIME [epoch: 8.3 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.64264600350313		[learning rate: 0.0093021]
	Learning Rate: 0.00930206
	LOSS [training: 3.64264600350313 | validation: 2.83719487586668]
	TIME [epoch: 8.32 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.845998133502399		[learning rate: 0.0092683]
	Learning Rate: 0.0092683
	LOSS [training: 3.845998133502399 | validation: 2.7960611938045616]
	TIME [epoch: 8.3 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6199209536236117		[learning rate: 0.0092347]
	Learning Rate: 0.00923466
	LOSS [training: 3.6199209536236117 | validation: 3.166565153093204]
	TIME [epoch: 8.3 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9060125103286993		[learning rate: 0.0092011]
	Learning Rate: 0.00920115
	LOSS [training: 3.9060125103286993 | validation: 3.2751696307775116]
	TIME [epoch: 8.3 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.744793256852515		[learning rate: 0.0091678]
	Learning Rate: 0.00916776
	LOSS [training: 3.744793256852515 | validation: 2.809917063448645]
	TIME [epoch: 8.32 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.640785430972211		[learning rate: 0.0091345]
	Learning Rate: 0.00913449
	LOSS [training: 3.640785430972211 | validation: 3.6516769407747907]
	TIME [epoch: 8.31 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7362931756508724		[learning rate: 0.0091013]
	Learning Rate: 0.00910134
	LOSS [training: 3.7362931756508724 | validation: 2.914284476505265]
	TIME [epoch: 8.3 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.693547146828345		[learning rate: 0.0090683]
	Learning Rate: 0.00906831
	LOSS [training: 3.693547146828345 | validation: 3.0110903484731057]
	TIME [epoch: 8.29 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.641075145076053		[learning rate: 0.0090354]
	Learning Rate: 0.0090354
	LOSS [training: 3.641075145076053 | validation: 3.1400539883358314]
	TIME [epoch: 8.32 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.839975608812547		[learning rate: 0.0090026]
	Learning Rate: 0.00900261
	LOSS [training: 3.839975608812547 | validation: 2.8320786574138417]
	TIME [epoch: 8.31 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.931098459072833		[learning rate: 0.0089699]
	Learning Rate: 0.00896994
	LOSS [training: 3.931098459072833 | validation: 2.8790010074167416]
	TIME [epoch: 8.3 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7504075397896246		[learning rate: 0.0089374]
	Learning Rate: 0.00893739
	LOSS [training: 3.7504075397896246 | validation: 3.2476849563315517]
	TIME [epoch: 8.3 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8080586708967585		[learning rate: 0.008905]
	Learning Rate: 0.00890495
	LOSS [training: 3.8080586708967585 | validation: 3.199691571034617]
	TIME [epoch: 8.31 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.763814229278944		[learning rate: 0.0088726]
	Learning Rate: 0.00887263
	LOSS [training: 3.763814229278944 | validation: 2.826748325617107]
	TIME [epoch: 8.31 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.718237554076935		[learning rate: 0.0088404]
	Learning Rate: 0.00884044
	LOSS [training: 3.718237554076935 | validation: 2.736210989969301]
	TIME [epoch: 8.29 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.609890238792738		[learning rate: 0.0088084]
	Learning Rate: 0.00880835
	LOSS [training: 3.609890238792738 | validation: 3.4787781298700047]
	TIME [epoch: 8.3 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8254461507508752		[learning rate: 0.0087764]
	Learning Rate: 0.00877639
	LOSS [training: 3.8254461507508752 | validation: 2.746311554029712]
	TIME [epoch: 8.31 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7955603916558216		[learning rate: 0.0087445]
	Learning Rate: 0.00874454
	LOSS [training: 3.7955603916558216 | validation: 2.925401600993889]
	TIME [epoch: 8.31 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6191801453222197		[learning rate: 0.0087128]
	Learning Rate: 0.0087128
	LOSS [training: 3.6191801453222197 | validation: 2.670744235809025]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6228650995644514		[learning rate: 0.0086812]
	Learning Rate: 0.00868118
	LOSS [training: 3.6228650995644514 | validation: 3.23761606367538]
	TIME [epoch: 8.29 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8427387949843252		[learning rate: 0.0086497]
	Learning Rate: 0.00864968
	LOSS [training: 3.8427387949843252 | validation: 3.534923049073523]
	TIME [epoch: 8.31 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7657302619655355		[learning rate: 0.0086183]
	Learning Rate: 0.00861829
	LOSS [training: 3.7657302619655355 | validation: 3.718662143310322]
	TIME [epoch: 8.3 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.916131497459734		[learning rate: 0.008587]
	Learning Rate: 0.00858701
	LOSS [training: 3.916131497459734 | validation: 2.8041524670780356]
	TIME [epoch: 8.29 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.57057576641771		[learning rate: 0.0085559]
	Learning Rate: 0.00855585
	LOSS [training: 3.57057576641771 | validation: 2.8432911666131826]
	TIME [epoch: 8.29 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.80086819069279		[learning rate: 0.0085248]
	Learning Rate: 0.0085248
	LOSS [training: 3.80086819069279 | validation: 2.780911529689096]
	TIME [epoch: 8.29 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5876515825772772		[learning rate: 0.0084939]
	Learning Rate: 0.00849386
	LOSS [training: 3.5876515825772772 | validation: 2.6941346606656724]
	TIME [epoch: 8.32 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5881292378992207		[learning rate: 0.008463]
	Learning Rate: 0.00846304
	LOSS [training: 3.5881292378992207 | validation: 2.724028006341093]
	TIME [epoch: 8.29 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5630749547042497		[learning rate: 0.0084323]
	Learning Rate: 0.00843233
	LOSS [training: 3.5630749547042497 | validation: 3.9746005448867914]
	TIME [epoch: 8.29 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8303202704085324		[learning rate: 0.0084017]
	Learning Rate: 0.00840172
	LOSS [training: 3.8303202704085324 | validation: 3.0805524675371023]
	TIME [epoch: 8.29 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.637599770610753		[learning rate: 0.0083712]
	Learning Rate: 0.00837123
	LOSS [training: 3.637599770610753 | validation: 3.449631340461203]
	TIME [epoch: 8.32 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7386567771982193		[learning rate: 0.0083409]
	Learning Rate: 0.00834085
	LOSS [training: 3.7386567771982193 | validation: 2.6552447376466173]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.743609119231786		[learning rate: 0.0083106]
	Learning Rate: 0.00831059
	LOSS [training: 3.743609119231786 | validation: 3.010387607433202]
	TIME [epoch: 8.32 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7060370242152083		[learning rate: 0.0082804]
	Learning Rate: 0.00828043
	LOSS [training: 3.7060370242152083 | validation: 3.1043251521646553]
	TIME [epoch: 8.29 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.707900302660417		[learning rate: 0.0082504]
	Learning Rate: 0.00825037
	LOSS [training: 3.707900302660417 | validation: 2.918760634502138]
	TIME [epoch: 8.31 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.676530767131827		[learning rate: 0.0082204]
	Learning Rate: 0.00822043
	LOSS [training: 3.676530767131827 | validation: 2.7250103618003907]
	TIME [epoch: 8.29 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7512684294186776		[learning rate: 0.0081906]
	Learning Rate: 0.0081906
	LOSS [training: 3.7512684294186776 | validation: 3.737026545935873]
	TIME [epoch: 8.29 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.869956292262264		[learning rate: 0.0081609]
	Learning Rate: 0.00816088
	LOSS [training: 3.869956292262264 | validation: 3.649311588241629]
	TIME [epoch: 8.29 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7789731393149117		[learning rate: 0.0081313]
	Learning Rate: 0.00813126
	LOSS [training: 3.7789731393149117 | validation: 2.8468224321763573]
	TIME [epoch: 8.31 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.561118656152548		[learning rate: 0.0081018]
	Learning Rate: 0.00810175
	LOSS [training: 3.561118656152548 | validation: 2.828630240527758]
	TIME [epoch: 8.29 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6014207799113174		[learning rate: 0.0080724]
	Learning Rate: 0.00807235
	LOSS [training: 3.6014207799113174 | validation: 2.823590822813259]
	TIME [epoch: 8.28 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.632428162025146		[learning rate: 0.0080431]
	Learning Rate: 0.00804305
	LOSS [training: 3.632428162025146 | validation: 2.919213278641367]
	TIME [epoch: 8.29 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.809176275352301		[learning rate: 0.0080139]
	Learning Rate: 0.00801387
	LOSS [training: 3.809176275352301 | validation: 3.289375875428634]
	TIME [epoch: 8.31 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.721599004720489		[learning rate: 0.0079848]
	Learning Rate: 0.00798478
	LOSS [training: 3.721599004720489 | validation: 3.7234360826430204]
	TIME [epoch: 8.29 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.694422534533558		[learning rate: 0.0079558]
	Learning Rate: 0.00795581
	LOSS [training: 3.694422534533558 | validation: 2.694498752753233]
	TIME [epoch: 8.29 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.82334163021905		[learning rate: 0.0079269]
	Learning Rate: 0.00792693
	LOSS [training: 3.82334163021905 | validation: 2.6833187996126613]
	TIME [epoch: 8.29 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8654336418767983		[learning rate: 0.0078982]
	Learning Rate: 0.00789817
	LOSS [training: 3.8654336418767983 | validation: 2.70156179078055]
	TIME [epoch: 8.32 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.64326592346574		[learning rate: 0.0078695]
	Learning Rate: 0.0078695
	LOSS [training: 3.64326592346574 | validation: 3.058466513597349]
	TIME [epoch: 8.29 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.012638038551704		[learning rate: 0.0078409]
	Learning Rate: 0.00784094
	LOSS [training: 4.012638038551704 | validation: 3.708254673720036]
	TIME [epoch: 8.29 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8114188362272925		[learning rate: 0.0078125]
	Learning Rate: 0.00781249
	LOSS [training: 3.8114188362272925 | validation: 3.4511124898662713]
	TIME [epoch: 8.28 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.64723777748699		[learning rate: 0.0077841]
	Learning Rate: 0.00778414
	LOSS [training: 3.64723777748699 | validation: 3.057898395101513]
	TIME [epoch: 8.33 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6004497785579646		[learning rate: 0.0077559]
	Learning Rate: 0.00775589
	LOSS [training: 3.6004497785579646 | validation: 2.749684484326438]
	TIME [epoch: 8.3 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.592569868103543		[learning rate: 0.0077277]
	Learning Rate: 0.00772774
	LOSS [training: 3.592569868103543 | validation: 3.19730771185952]
	TIME [epoch: 8.3 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.660107103016903		[learning rate: 0.0076997]
	Learning Rate: 0.0076997
	LOSS [training: 3.660107103016903 | validation: 2.7506284296621724]
	TIME [epoch: 8.29 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6610376520619567		[learning rate: 0.0076718]
	Learning Rate: 0.00767176
	LOSS [training: 3.6610376520619567 | validation: 2.7258864744581373]
	TIME [epoch: 8.32 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.544722283053725		[learning rate: 0.0076439]
	Learning Rate: 0.00764391
	LOSS [training: 3.544722283053725 | validation: 2.886435360368822]
	TIME [epoch: 8.29 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5943600852284243		[learning rate: 0.0076162]
	Learning Rate: 0.00761617
	LOSS [training: 3.5943600852284243 | validation: 2.9002803961030654]
	TIME [epoch: 8.29 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6009963764798045		[learning rate: 0.0075885]
	Learning Rate: 0.00758853
	LOSS [training: 3.6009963764798045 | validation: 2.989088488859932]
	TIME [epoch: 8.29 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.804248254391812		[learning rate: 0.007561]
	Learning Rate: 0.00756099
	LOSS [training: 3.804248254391812 | validation: 2.789145177890693]
	TIME [epoch: 8.31 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.656714547309374		[learning rate: 0.0075336]
	Learning Rate: 0.00753356
	LOSS [training: 3.656714547309374 | validation: 2.7186703681327535]
	TIME [epoch: 8.3 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7288022990877714		[learning rate: 0.0075062]
	Learning Rate: 0.00750622
	LOSS [training: 3.7288022990877714 | validation: 2.670411925189505]
	TIME [epoch: 8.29 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6596451716836547		[learning rate: 0.007479]
	Learning Rate: 0.00747897
	LOSS [training: 3.6596451716836547 | validation: 2.796012536498948]
	TIME [epoch: 8.3 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6015177722902996		[learning rate: 0.0074518]
	Learning Rate: 0.00745183
	LOSS [training: 3.6015177722902996 | validation: 2.672223231084576]
	TIME [epoch: 8.32 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.59708753032596		[learning rate: 0.0074248]
	Learning Rate: 0.00742479
	LOSS [training: 3.59708753032596 | validation: 3.183860861466504]
	TIME [epoch: 8.3 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.558398935986747		[learning rate: 0.0073978]
	Learning Rate: 0.00739784
	LOSS [training: 3.558398935986747 | validation: 2.667619346312048]
	TIME [epoch: 8.3 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6622044183503		[learning rate: 0.007371]
	Learning Rate: 0.007371
	LOSS [training: 3.6622044183503 | validation: 2.6713243560489657]
	TIME [epoch: 8.29 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5331576616887297		[learning rate: 0.0073442]
	Learning Rate: 0.00734425
	LOSS [training: 3.5331576616887297 | validation: 2.852384769199142]
	TIME [epoch: 8.32 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6386941799814303		[learning rate: 0.0073176]
	Learning Rate: 0.0073176
	LOSS [training: 3.6386941799814303 | validation: 2.69737292848866]
	TIME [epoch: 8.29 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.537588212318888		[learning rate: 0.007291]
	Learning Rate: 0.00729104
	LOSS [training: 3.537588212318888 | validation: 3.0861974127084992]
	TIME [epoch: 8.29 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5823261700629843		[learning rate: 0.0072646]
	Learning Rate: 0.00726458
	LOSS [training: 3.5823261700629843 | validation: 2.6465014944157823]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.596730678392413		[learning rate: 0.0072382]
	Learning Rate: 0.00723822
	LOSS [training: 3.596730678392413 | validation: 2.7939783214946052]
	TIME [epoch: 8.32 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5684650953345503		[learning rate: 0.0072119]
	Learning Rate: 0.00721195
	LOSS [training: 3.5684650953345503 | validation: 2.721542366541353]
	TIME [epoch: 8.3 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.569192871697272		[learning rate: 0.0071858]
	Learning Rate: 0.00718578
	LOSS [training: 3.569192871697272 | validation: 3.1496273124940055]
	TIME [epoch: 8.3 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6737286189054545		[learning rate: 0.0071597]
	Learning Rate: 0.0071597
	LOSS [training: 3.6737286189054545 | validation: 3.1969290753658752]
	TIME [epoch: 8.3 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.562556596486952		[learning rate: 0.0071337]
	Learning Rate: 0.00713371
	LOSS [training: 3.562556596486952 | validation: 2.6784843622908197]
	TIME [epoch: 8.31 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6026254263343973		[learning rate: 0.0071078]
	Learning Rate: 0.00710783
	LOSS [training: 3.6026254263343973 | validation: 3.2923382394577376]
	TIME [epoch: 8.31 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6177538496869284		[learning rate: 0.007082]
	Learning Rate: 0.00708203
	LOSS [training: 3.6177538496869284 | validation: 2.663148897040969]
	TIME [epoch: 8.29 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5439320663548224		[learning rate: 0.0070563]
	Learning Rate: 0.00705633
	LOSS [training: 3.5439320663548224 | validation: 2.9291200486222575]
	TIME [epoch: 8.31 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5574600168442365		[learning rate: 0.0070307]
	Learning Rate: 0.00703072
	LOSS [training: 3.5574600168442365 | validation: 2.658984612470574]
	TIME [epoch: 8.31 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4939368081163913		[learning rate: 0.0070052]
	Learning Rate: 0.00700521
	LOSS [training: 3.4939368081163913 | validation: 2.7752032481792255]
	TIME [epoch: 8.31 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.50840891758439		[learning rate: 0.0069798]
	Learning Rate: 0.00697979
	LOSS [training: 3.50840891758439 | validation: 3.1346187243600987]
	TIME [epoch: 8.3 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5522085274601793		[learning rate: 0.0069545]
	Learning Rate: 0.00695446
	LOSS [training: 3.5522085274601793 | validation: 2.6849881339495623]
	TIME [epoch: 8.3 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.591934131031478		[learning rate: 0.0069292]
	Learning Rate: 0.00692922
	LOSS [training: 3.591934131031478 | validation: 2.8467702942237194]
	TIME [epoch: 8.32 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5714720918456253		[learning rate: 0.0069041]
	Learning Rate: 0.00690407
	LOSS [training: 3.5714720918456253 | validation: 2.8345477584744745]
	TIME [epoch: 8.31 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5331562194496526		[learning rate: 0.006879]
	Learning Rate: 0.00687902
	LOSS [training: 3.5331562194496526 | validation: 2.6700275712657136]
	TIME [epoch: 8.3 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.523237153863377		[learning rate: 0.0068541]
	Learning Rate: 0.00685405
	LOSS [training: 3.523237153863377 | validation: 2.6773730588270443]
	TIME [epoch: 8.3 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5021590335826938		[learning rate: 0.0068292]
	Learning Rate: 0.00682918
	LOSS [training: 3.5021590335826938 | validation: 2.7654364625180112]
	TIME [epoch: 8.31 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.498273336987688		[learning rate: 0.0068044]
	Learning Rate: 0.00680439
	LOSS [training: 3.498273336987688 | validation: 3.103153089645935]
	TIME [epoch: 8.31 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5272709771579103		[learning rate: 0.0067797]
	Learning Rate: 0.0067797
	LOSS [training: 3.5272709771579103 | validation: 3.7387507845875185]
	TIME [epoch: 8.3 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7120458558932428		[learning rate: 0.0067551]
	Learning Rate: 0.0067551
	LOSS [training: 3.7120458558932428 | validation: 2.760536873960622]
	TIME [epoch: 8.3 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5385339069541524		[learning rate: 0.0067306]
	Learning Rate: 0.00673058
	LOSS [training: 3.5385339069541524 | validation: 2.7581965959347565]
	TIME [epoch: 8.32 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.510997669203193		[learning rate: 0.0067062]
	Learning Rate: 0.00670616
	LOSS [training: 3.510997669203193 | validation: 2.703061637903228]
	TIME [epoch: 8.31 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.530095185358183		[learning rate: 0.0066818]
	Learning Rate: 0.00668182
	LOSS [training: 3.530095185358183 | validation: 2.641218918908695]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.488484410673658		[learning rate: 0.0066576]
	Learning Rate: 0.00665757
	LOSS [training: 3.488484410673658 | validation: 2.6536894210725825]
	TIME [epoch: 8.3 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.510597163520419		[learning rate: 0.0066334]
	Learning Rate: 0.00663341
	LOSS [training: 3.510597163520419 | validation: 2.6424833368294234]
	TIME [epoch: 8.31 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4825756359321125		[learning rate: 0.0066093]
	Learning Rate: 0.00660934
	LOSS [training: 3.4825756359321125 | validation: 3.1207344247227122]
	TIME [epoch: 8.31 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6186901598998547		[learning rate: 0.0065854]
	Learning Rate: 0.00658535
	LOSS [training: 3.6186901598998547 | validation: 2.8699545515133393]
	TIME [epoch: 8.3 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.544326542987561		[learning rate: 0.0065615]
	Learning Rate: 0.00656145
	LOSS [training: 3.544326542987561 | validation: 2.6250101166924575]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5227446018566306		[learning rate: 0.0065376]
	Learning Rate: 0.00653764
	LOSS [training: 3.5227446018566306 | validation: 2.753678379872799]
	TIME [epoch: 8.32 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5389881914007333		[learning rate: 0.0065139]
	Learning Rate: 0.00651392
	LOSS [training: 3.5389881914007333 | validation: 3.0570129819394554]
	TIME [epoch: 8.31 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5816810690678667		[learning rate: 0.0064903]
	Learning Rate: 0.00649028
	LOSS [training: 3.5816810690678667 | validation: 2.8191320221101757]
	TIME [epoch: 8.3 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6100992967410925		[learning rate: 0.0064667]
	Learning Rate: 0.00646672
	LOSS [training: 3.6100992967410925 | validation: 2.702004106621839]
	TIME [epoch: 8.29 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5625201664170376		[learning rate: 0.0064433]
	Learning Rate: 0.00644325
	LOSS [training: 3.5625201664170376 | validation: 2.6378041193412334]
	TIME [epoch: 8.3 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.487454427267211		[learning rate: 0.0064199]
	Learning Rate: 0.00641987
	LOSS [training: 3.487454427267211 | validation: 2.854822717288373]
	TIME [epoch: 8.31 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5587717193101427		[learning rate: 0.0063966]
	Learning Rate: 0.00639657
	LOSS [training: 3.5587717193101427 | validation: 2.7328357919309894]
	TIME [epoch: 8.3 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.583471156274304		[learning rate: 0.0063734]
	Learning Rate: 0.00637336
	LOSS [training: 3.583471156274304 | validation: 2.6723821759484343]
	TIME [epoch: 8.29 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4897312862396794		[learning rate: 0.0063502]
	Learning Rate: 0.00635023
	LOSS [training: 3.4897312862396794 | validation: 2.8091434432623306]
	TIME [epoch: 8.3 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.530515975994571		[learning rate: 0.0063272]
	Learning Rate: 0.00632718
	LOSS [training: 3.530515975994571 | validation: 2.7166740601980472]
	TIME [epoch: 8.32 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4930864242957447		[learning rate: 0.0063042]
	Learning Rate: 0.00630422
	LOSS [training: 3.4930864242957447 | validation: 2.6642786082798766]
	TIME [epoch: 8.29 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.502882119613461		[learning rate: 0.0062813]
	Learning Rate: 0.00628134
	LOSS [training: 3.502882119613461 | validation: 2.6692869796072163]
	TIME [epoch: 8.3 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4731271719085077		[learning rate: 0.0062585]
	Learning Rate: 0.00625855
	LOSS [training: 3.4731271719085077 | validation: 2.674816472641072]
	TIME [epoch: 8.29 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.497953310111291		[learning rate: 0.0062358]
	Learning Rate: 0.00623584
	LOSS [training: 3.497953310111291 | validation: 2.6369733402419184]
	TIME [epoch: 8.32 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4893843999359886		[learning rate: 0.0062132]
	Learning Rate: 0.00621321
	LOSS [training: 3.4893843999359886 | validation: 2.6978865583070015]
	TIME [epoch: 8.3 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.489799063697066		[learning rate: 0.0061907]
	Learning Rate: 0.00619066
	LOSS [training: 3.489799063697066 | validation: 2.9585890808313757]
	TIME [epoch: 8.29 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5920576064857697		[learning rate: 0.0061682]
	Learning Rate: 0.00616819
	LOSS [training: 3.5920576064857697 | validation: 2.6128291077954753]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5045286292433873		[learning rate: 0.0061458]
	Learning Rate: 0.00614581
	LOSS [training: 3.5045286292433873 | validation: 3.0108413954625237]
	TIME [epoch: 8.32 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5391494080748616		[learning rate: 0.0061235]
	Learning Rate: 0.0061235
	LOSS [training: 3.5391494080748616 | validation: 3.1714665978219885]
	TIME [epoch: 8.3 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5296341993027682		[learning rate: 0.0061013]
	Learning Rate: 0.00610128
	LOSS [training: 3.5296341993027682 | validation: 2.628206767169068]
	TIME [epoch: 8.3 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.520750439950865		[learning rate: 0.0060791]
	Learning Rate: 0.00607914
	LOSS [training: 3.520750439950865 | validation: 2.6047764604132704]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4306955212322685		[learning rate: 0.0060571]
	Learning Rate: 0.00605708
	LOSS [training: 3.4306955212322685 | validation: 2.908247322041972]
	TIME [epoch: 8.32 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4623750366426043		[learning rate: 0.0060351]
	Learning Rate: 0.0060351
	LOSS [training: 3.4623750366426043 | validation: 2.790138310564875]
	TIME [epoch: 8.31 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5516413462217384		[learning rate: 0.0060132]
	Learning Rate: 0.00601319
	LOSS [training: 3.5516413462217384 | validation: 2.8198162189470777]
	TIME [epoch: 8.3 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.476831617876838		[learning rate: 0.0059914]
	Learning Rate: 0.00599137
	LOSS [training: 3.476831617876838 | validation: 2.825806444561887]
	TIME [epoch: 8.29 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.48171521958634		[learning rate: 0.0059696]
	Learning Rate: 0.00596963
	LOSS [training: 3.48171521958634 | validation: 2.631969031445273]
	TIME [epoch: 8.3 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4507327778235135		[learning rate: 0.005948]
	Learning Rate: 0.00594797
	LOSS [training: 3.4507327778235135 | validation: 2.6473542709984725]
	TIME [epoch: 8.32 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.451936905972122		[learning rate: 0.0059264]
	Learning Rate: 0.00592638
	LOSS [training: 3.451936905972122 | validation: 2.6164348310002334]
	TIME [epoch: 8.29 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4717663011407494		[learning rate: 0.0059049]
	Learning Rate: 0.00590487
	LOSS [training: 3.4717663011407494 | validation: 2.6740939847275036]
	TIME [epoch: 8.3 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4743536506279424		[learning rate: 0.0058834]
	Learning Rate: 0.00588344
	LOSS [training: 3.4743536506279424 | validation: 2.747879687220035]
	TIME [epoch: 8.31 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4437292600772493		[learning rate: 0.0058621]
	Learning Rate: 0.00586209
	LOSS [training: 3.4437292600772493 | validation: 2.754534884930171]
	TIME [epoch: 8.32 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.510912789352459		[learning rate: 0.0058408]
	Learning Rate: 0.00584082
	LOSS [training: 3.510912789352459 | validation: 2.6450962831726903]
	TIME [epoch: 8.3 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.483522994111648		[learning rate: 0.0058196]
	Learning Rate: 0.00581962
	LOSS [training: 3.483522994111648 | validation: 2.64153412057367]
	TIME [epoch: 8.3 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.456051101707098		[learning rate: 0.0057985]
	Learning Rate: 0.0057985
	LOSS [training: 3.456051101707098 | validation: 2.681501501735804]
	TIME [epoch: 8.31 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4390134404052772		[learning rate: 0.0057775]
	Learning Rate: 0.00577746
	LOSS [training: 3.4390134404052772 | validation: 2.636596796529332]
	TIME [epoch: 8.31 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4816528001237708		[learning rate: 0.0057565]
	Learning Rate: 0.00575649
	LOSS [training: 3.4816528001237708 | validation: 3.1626357399324987]
	TIME [epoch: 8.3 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4951983664527346		[learning rate: 0.0057356]
	Learning Rate: 0.0057356
	LOSS [training: 3.4951983664527346 | validation: 2.598964965249566]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4299770084492422		[learning rate: 0.0057148]
	Learning Rate: 0.00571479
	LOSS [training: 3.4299770084492422 | validation: 2.757791326505952]
	TIME [epoch: 8.32 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4862740939697163		[learning rate: 0.005694]
	Learning Rate: 0.00569405
	LOSS [training: 3.4862740939697163 | validation: 2.593806900543256]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.467394866268461		[learning rate: 0.0056734]
	Learning Rate: 0.00567338
	LOSS [training: 3.467394866268461 | validation: 2.6170294455621947]
	TIME [epoch: 8.3 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.532523039784312		[learning rate: 0.0056528]
	Learning Rate: 0.00565279
	LOSS [training: 3.532523039784312 | validation: 2.751022332747346]
	TIME [epoch: 8.3 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4732350381046673		[learning rate: 0.0056323]
	Learning Rate: 0.00563228
	LOSS [training: 3.4732350381046673 | validation: 2.7502064353348175]
	TIME [epoch: 8.29 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4686570840351267		[learning rate: 0.0056118]
	Learning Rate: 0.00561184
	LOSS [training: 3.4686570840351267 | validation: 2.693222133604144]
	TIME [epoch: 8.32 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0075193141475163		[learning rate: 0.0055915]
	Learning Rate: 0.00559147
	LOSS [training: 3.0075193141475163 | validation: 2.1796188146884274]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.701899244420714		[learning rate: 0.0055712]
	Learning Rate: 0.00557118
	LOSS [training: 2.701899244420714 | validation: 2.409001317400268]
	TIME [epoch: 8.3 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6731114100361606		[learning rate: 0.005551]
	Learning Rate: 0.00555096
	LOSS [training: 2.6731114100361606 | validation: 2.356811259581871]
	TIME [epoch: 8.29 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.64363929243069		[learning rate: 0.0055308]
	Learning Rate: 0.00553082
	LOSS [training: 2.64363929243069 | validation: 2.1208277508355637]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.515744195048562		[learning rate: 0.0055107]
	Learning Rate: 0.00551075
	LOSS [training: 2.515744195048562 | validation: 1.2957327155914249]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8713315113650384		[learning rate: 0.0054907]
	Learning Rate: 0.00549075
	LOSS [training: 1.8713315113650384 | validation: 1.1740121113073083]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6294773071296895		[learning rate: 0.0054708]
	Learning Rate: 0.00547082
	LOSS [training: 1.6294773071296895 | validation: 0.9948073740884891]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.53827076172557		[learning rate: 0.005451]
	Learning Rate: 0.00545097
	LOSS [training: 1.53827076172557 | validation: 0.8081820162065527]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.307844433890818		[learning rate: 0.0054312]
	Learning Rate: 0.00543119
	LOSS [training: 1.307844433890818 | validation: 0.6192134310360248]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8266011294848397		[learning rate: 0.0054115]
	Learning Rate: 0.00541148
	LOSS [training: 0.8266011294848397 | validation: 0.5215074077524504]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7511082357727366		[learning rate: 0.0053918]
	Learning Rate: 0.00539184
	LOSS [training: 0.7511082357727366 | validation: 0.8974620618904314]
	TIME [epoch: 8.29 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7338531585488747		[learning rate: 0.0053723]
	Learning Rate: 0.00537227
	LOSS [training: 0.7338531585488747 | validation: 0.3235372684703586]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6308464771379718		[learning rate: 0.0053528]
	Learning Rate: 0.00535277
	LOSS [training: 0.6308464771379718 | validation: 0.3927264810109915]
	TIME [epoch: 8.29 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5963660556006986		[learning rate: 0.0053333]
	Learning Rate: 0.00533335
	LOSS [training: 0.5963660556006986 | validation: 0.48240010883170215]
	TIME [epoch: 8.29 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.631197716258338		[learning rate: 0.005314]
	Learning Rate: 0.00531399
	LOSS [training: 0.631197716258338 | validation: 0.5199099410860013]
	TIME [epoch: 8.28 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5505868307787016		[learning rate: 0.0052947]
	Learning Rate: 0.00529471
	LOSS [training: 0.5505868307787016 | validation: 0.4688449240311998]
	TIME [epoch: 8.28 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5795368001797343		[learning rate: 0.0052755]
	Learning Rate: 0.00527549
	LOSS [training: 0.5795368001797343 | validation: 0.5284640024183787]
	TIME [epoch: 8.3 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.576806919125308		[learning rate: 0.0052563]
	Learning Rate: 0.00525635
	LOSS [training: 0.576806919125308 | validation: 0.4139027678589664]
	TIME [epoch: 8.28 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7685867986906891		[learning rate: 0.0052373]
	Learning Rate: 0.00523727
	LOSS [training: 0.7685867986906891 | validation: 0.42734251496361747]
	TIME [epoch: 8.28 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5959483886653905		[learning rate: 0.0052183]
	Learning Rate: 0.00521827
	LOSS [training: 0.5959483886653905 | validation: 0.4012865517477743]
	TIME [epoch: 8.28 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49135524904712796		[learning rate: 0.0051993]
	Learning Rate: 0.00519933
	LOSS [training: 0.49135524904712796 | validation: 0.30230513463938075]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4928476706222254		[learning rate: 0.0051805]
	Learning Rate: 0.00518046
	LOSS [training: 0.4928476706222254 | validation: 0.3694495788751387]
	TIME [epoch: 8.29 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49549986877041635		[learning rate: 0.0051617]
	Learning Rate: 0.00516166
	LOSS [training: 0.49549986877041635 | validation: 0.3446988907361723]
	TIME [epoch: 8.29 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48668591326287575		[learning rate: 0.0051429]
	Learning Rate: 0.00514293
	LOSS [training: 0.48668591326287575 | validation: 0.46047314082548585]
	TIME [epoch: 8.29 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4697676413728192		[learning rate: 0.0051243]
	Learning Rate: 0.00512426
	LOSS [training: 0.4697676413728192 | validation: 0.31798364760581643]
	TIME [epoch: 8.3 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4596807025168954		[learning rate: 0.0051057]
	Learning Rate: 0.00510567
	LOSS [training: 0.4596807025168954 | validation: 0.31823387640686746]
	TIME [epoch: 8.28 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4873482805337314		[learning rate: 0.0050871]
	Learning Rate: 0.00508714
	LOSS [training: 0.4873482805337314 | validation: 0.36575677800334727]
	TIME [epoch: 8.28 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4839102046461127		[learning rate: 0.0050687]
	Learning Rate: 0.00506868
	LOSS [training: 0.4839102046461127 | validation: 0.7573922549729768]
	TIME [epoch: 8.28 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4713567362003987		[learning rate: 0.0050503]
	Learning Rate: 0.00505028
	LOSS [training: 0.4713567362003987 | validation: 0.5866747688831342]
	TIME [epoch: 8.3 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5066208839978612		[learning rate: 0.005032]
	Learning Rate: 0.00503196
	LOSS [training: 0.5066208839978612 | validation: 0.4000167126928626]
	TIME [epoch: 8.29 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43516751933742814		[learning rate: 0.0050137]
	Learning Rate: 0.00501369
	LOSS [training: 0.43516751933742814 | validation: 0.3557697849870969]
	TIME [epoch: 8.28 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47774573552726346		[learning rate: 0.0049955]
	Learning Rate: 0.0049955
	LOSS [training: 0.47774573552726346 | validation: 0.6757731218212473]
	TIME [epoch: 8.28 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.536530761337062		[learning rate: 0.0049774]
	Learning Rate: 0.00497737
	LOSS [training: 0.536530761337062 | validation: 0.30180176135166203]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4488544598061698		[learning rate: 0.0049593]
	Learning Rate: 0.00495931
	LOSS [training: 0.4488544598061698 | validation: 0.40612894804999383]
	TIME [epoch: 8.29 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43010271155258895		[learning rate: 0.0049413]
	Learning Rate: 0.00494131
	LOSS [training: 0.43010271155258895 | validation: 0.35861167316156284]
	TIME [epoch: 8.29 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47157759935304105		[learning rate: 0.0049234]
	Learning Rate: 0.00492338
	LOSS [training: 0.47157759935304105 | validation: 0.3131223807049084]
	TIME [epoch: 8.29 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49628082232127013		[learning rate: 0.0049055]
	Learning Rate: 0.00490551
	LOSS [training: 0.49628082232127013 | validation: 0.28848891497052787]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.470827176523966		[learning rate: 0.0048877]
	Learning Rate: 0.00488771
	LOSS [training: 0.470827176523966 | validation: 0.3705435050926425]
	TIME [epoch: 8.3 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4191311835147986		[learning rate: 0.00487]
	Learning Rate: 0.00486997
	LOSS [training: 0.4191311835147986 | validation: 0.4555370727084763]
	TIME [epoch: 8.29 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5275643572686608		[learning rate: 0.0048523]
	Learning Rate: 0.0048523
	LOSS [training: 0.5275643572686608 | validation: 0.3804788807039734]
	TIME [epoch: 8.29 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3963534296417779		[learning rate: 0.0048347]
	Learning Rate: 0.00483469
	LOSS [training: 0.3963534296417779 | validation: 0.28110754285581663]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4779646625737036		[learning rate: 0.0048171]
	Learning Rate: 0.00481714
	LOSS [training: 0.4779646625737036 | validation: 0.32965537776271625]
	TIME [epoch: 8.32 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43778783265788146		[learning rate: 0.0047997]
	Learning Rate: 0.00479966
	LOSS [training: 0.43778783265788146 | validation: 0.2435657918453763]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39970031396683725		[learning rate: 0.0047822]
	Learning Rate: 0.00478224
	LOSS [training: 0.39970031396683725 | validation: 0.25317928341039464]
	TIME [epoch: 8.3 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4527078638042107		[learning rate: 0.0047649]
	Learning Rate: 0.00476489
	LOSS [training: 0.4527078638042107 | validation: 0.216068294671807]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39892221194329974		[learning rate: 0.0047476]
	Learning Rate: 0.00474759
	LOSS [training: 0.39892221194329974 | validation: 0.43529979879322833]
	TIME [epoch: 8.33 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38063289677651785		[learning rate: 0.0047304]
	Learning Rate: 0.00473037
	LOSS [training: 0.38063289677651785 | validation: 0.3134971583430917]
	TIME [epoch: 8.29 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4203624697624271		[learning rate: 0.0047132]
	Learning Rate: 0.0047132
	LOSS [training: 0.4203624697624271 | validation: 0.3382574532674258]
	TIME [epoch: 8.3 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39316528961570507		[learning rate: 0.0046961]
	Learning Rate: 0.00469609
	LOSS [training: 0.39316528961570507 | validation: 0.24136199646506348]
	TIME [epoch: 8.29 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39956829666630805		[learning rate: 0.0046791]
	Learning Rate: 0.00467905
	LOSS [training: 0.39956829666630805 | validation: 0.6047333042560425]
	TIME [epoch: 8.32 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6266171488091128		[learning rate: 0.0046621]
	Learning Rate: 0.00466207
	LOSS [training: 0.6266171488091128 | validation: 0.2744629809078209]
	TIME [epoch: 8.3 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3481747833443099		[learning rate: 0.0046452]
	Learning Rate: 0.00464515
	LOSS [training: 0.3481747833443099 | validation: 0.22457620392963218]
	TIME [epoch: 8.3 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37707028321402775		[learning rate: 0.0046283]
	Learning Rate: 0.0046283
	LOSS [training: 0.37707028321402775 | validation: 0.27421545047069973]
	TIME [epoch: 8.3 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36017301369150745		[learning rate: 0.0046115]
	Learning Rate: 0.0046115
	LOSS [training: 0.36017301369150745 | validation: 0.42132153939583383]
	TIME [epoch: 8.32 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43950779654219935		[learning rate: 0.0045948]
	Learning Rate: 0.00459476
	LOSS [training: 0.43950779654219935 | validation: 0.46710030798402813]
	TIME [epoch: 8.3 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44331384736741963		[learning rate: 0.0045781]
	Learning Rate: 0.00457809
	LOSS [training: 0.44331384736741963 | validation: 0.2567738243547092]
	TIME [epoch: 8.3 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.399485206009273		[learning rate: 0.0045615]
	Learning Rate: 0.00456148
	LOSS [training: 0.399485206009273 | validation: 0.3628678124769599]
	TIME [epoch: 8.3 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29947795897545854		[learning rate: 0.0045449]
	Learning Rate: 0.00454492
	LOSS [training: 0.29947795897545854 | validation: 0.3094914938377653]
	TIME [epoch: 8.33 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33243850351521936		[learning rate: 0.0045284]
	Learning Rate: 0.00452843
	LOSS [training: 0.33243850351521936 | validation: 0.2646865400065115]
	TIME [epoch: 8.31 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3454608489951063		[learning rate: 0.004512]
	Learning Rate: 0.00451199
	LOSS [training: 0.3454608489951063 | validation: 0.2565343050070264]
	TIME [epoch: 8.31 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3421156797995009		[learning rate: 0.0044956]
	Learning Rate: 0.00449562
	LOSS [training: 0.3421156797995009 | validation: 0.5004997984514503]
	TIME [epoch: 8.3 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41577580246501933		[learning rate: 0.0044793]
	Learning Rate: 0.0044793
	LOSS [training: 0.41577580246501933 | validation: 0.23290289232015446]
	TIME [epoch: 8.33 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36151028950071185		[learning rate: 0.004463]
	Learning Rate: 0.00446305
	LOSS [training: 0.36151028950071185 | validation: 0.26402417033400394]
	TIME [epoch: 8.31 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3272992497811301		[learning rate: 0.0044469]
	Learning Rate: 0.00444685
	LOSS [training: 0.3272992497811301 | validation: 0.47777015007614776]
	TIME [epoch: 8.3 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36702313461980435		[learning rate: 0.0044307]
	Learning Rate: 0.00443071
	LOSS [training: 0.36702313461980435 | validation: 0.40176565626819816]
	TIME [epoch: 8.31 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3646373175077319		[learning rate: 0.0044146]
	Learning Rate: 0.00441463
	LOSS [training: 0.3646373175077319 | validation: 0.2861612497692699]
	TIME [epoch: 8.33 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3486598819621606		[learning rate: 0.0043986]
	Learning Rate: 0.00439861
	LOSS [training: 0.3486598819621606 | validation: 0.8075071315733455]
	TIME [epoch: 8.31 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4603447264841173		[learning rate: 0.0043827]
	Learning Rate: 0.00438265
	LOSS [training: 0.4603447264841173 | validation: 0.22818019150421787]
	TIME [epoch: 8.31 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3852090664842412		[learning rate: 0.0043667]
	Learning Rate: 0.00436675
	LOSS [training: 0.3852090664842412 | validation: 0.36515583537646323]
	TIME [epoch: 8.31 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3331288114748658		[learning rate: 0.0043509]
	Learning Rate: 0.0043509
	LOSS [training: 0.3331288114748658 | validation: 0.295684675046189]
	TIME [epoch: 8.33 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3683594831912105		[learning rate: 0.0043351]
	Learning Rate: 0.00433511
	LOSS [training: 0.3683594831912105 | validation: 0.3793538335052829]
	TIME [epoch: 8.31 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3800156829692559		[learning rate: 0.0043194]
	Learning Rate: 0.00431938
	LOSS [training: 0.3800156829692559 | validation: 0.1980893625061409]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_331.pth
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34977907912502026		[learning rate: 0.0043037]
	Learning Rate: 0.0043037
	LOSS [training: 0.34977907912502026 | validation: 0.2329120603857957]
	TIME [epoch: 8.3 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3838529654469993		[learning rate: 0.0042881]
	Learning Rate: 0.00428808
	LOSS [training: 0.3838529654469993 | validation: 0.18174546573303113]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38122009793694617		[learning rate: 0.0042725]
	Learning Rate: 0.00427252
	LOSS [training: 0.38122009793694617 | validation: 0.2536674279691938]
	TIME [epoch: 8.3 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3835830890051418		[learning rate: 0.004257]
	Learning Rate: 0.00425702
	LOSS [training: 0.3835830890051418 | validation: 0.28629629846507454]
	TIME [epoch: 8.3 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3752285310311806		[learning rate: 0.0042416]
	Learning Rate: 0.00424157
	LOSS [training: 0.3752285310311806 | validation: 0.23246794927633696]
	TIME [epoch: 8.3 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34329671384673954		[learning rate: 0.0042262]
	Learning Rate: 0.00422617
	LOSS [training: 0.34329671384673954 | validation: 0.3164618079415679]
	TIME [epoch: 8.32 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3351746860254522		[learning rate: 0.0042108]
	Learning Rate: 0.00421084
	LOSS [training: 0.3351746860254522 | validation: 0.24797222635951227]
	TIME [epoch: 8.3 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.296591190451793		[learning rate: 0.0041956]
	Learning Rate: 0.00419556
	LOSS [training: 0.296591190451793 | validation: 0.2697096575975691]
	TIME [epoch: 8.3 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38783362698185214		[learning rate: 0.0041803]
	Learning Rate: 0.00418033
	LOSS [training: 0.38783362698185214 | validation: 0.33192545482654945]
	TIME [epoch: 8.3 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.319132782209479		[learning rate: 0.0041652]
	Learning Rate: 0.00416516
	LOSS [training: 0.319132782209479 | validation: 0.15543385216374775]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3205739899450082		[learning rate: 0.00415]
	Learning Rate: 0.00415004
	LOSS [training: 0.3205739899450082 | validation: 0.3211413921170851]
	TIME [epoch: 8.31 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33307726936604115		[learning rate: 0.004135]
	Learning Rate: 0.00413498
	LOSS [training: 0.33307726936604115 | validation: 0.389206389391472]
	TIME [epoch: 8.29 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3181458687991209		[learning rate: 0.00412]
	Learning Rate: 0.00411998
	LOSS [training: 0.3181458687991209 | validation: 0.22932467600473477]
	TIME [epoch: 8.28 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3689867653921008		[learning rate: 0.004105]
	Learning Rate: 0.00410502
	LOSS [training: 0.3689867653921008 | validation: 0.34133034903412707]
	TIME [epoch: 8.29 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41747921038330366		[learning rate: 0.0040901]
	Learning Rate: 0.00409013
	LOSS [training: 0.41747921038330366 | validation: 0.3752309556030321]
	TIME [epoch: 8.31 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.357018225283696		[learning rate: 0.0040753]
	Learning Rate: 0.00407528
	LOSS [training: 0.357018225283696 | validation: 0.1892614386190758]
	TIME [epoch: 8.29 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28968827490027993		[learning rate: 0.0040605]
	Learning Rate: 0.00406049
	LOSS [training: 0.28968827490027993 | validation: 0.3693962761737701]
	TIME [epoch: 8.29 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31992650978750076		[learning rate: 0.0040458]
	Learning Rate: 0.00404576
	LOSS [training: 0.31992650978750076 | validation: 0.34889340874418084]
	TIME [epoch: 8.29 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31089378541012913		[learning rate: 0.0040311]
	Learning Rate: 0.00403108
	LOSS [training: 0.31089378541012913 | validation: 0.22905127214628568]
	TIME [epoch: 8.31 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40265361325582943		[learning rate: 0.0040164]
	Learning Rate: 0.00401645
	LOSS [training: 0.40265361325582943 | validation: 0.3551341642465271]
	TIME [epoch: 8.3 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3011253848668726		[learning rate: 0.0040019]
	Learning Rate: 0.00400187
	LOSS [training: 0.3011253848668726 | validation: 0.2045281485193661]
	TIME [epoch: 8.29 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30882683028543345		[learning rate: 0.0039873]
	Learning Rate: 0.00398735
	LOSS [training: 0.30882683028543345 | validation: 0.23376583934605522]
	TIME [epoch: 8.3 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3133889907837535		[learning rate: 0.0039729]
	Learning Rate: 0.00397288
	LOSS [training: 0.3133889907837535 | validation: 0.1754910285363966]
	TIME [epoch: 8.31 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3186754803784443		[learning rate: 0.0039585]
	Learning Rate: 0.00395846
	LOSS [training: 0.3186754803784443 | validation: 0.1898785115016841]
	TIME [epoch: 8.3 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32737161576705887		[learning rate: 0.0039441]
	Learning Rate: 0.00394409
	LOSS [training: 0.32737161576705887 | validation: 0.14106979520532273]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3498873697126434		[learning rate: 0.0039298]
	Learning Rate: 0.00392978
	LOSS [training: 0.3498873697126434 | validation: 0.386908300579474]
	TIME [epoch: 8.3 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3459381060376331		[learning rate: 0.0039155]
	Learning Rate: 0.00391552
	LOSS [training: 0.3459381060376331 | validation: 0.27853356397764034]
	TIME [epoch: 8.32 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3061621108783813		[learning rate: 0.0039013]
	Learning Rate: 0.00390131
	LOSS [training: 0.3061621108783813 | validation: 0.2897078051039235]
	TIME [epoch: 8.29 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3544545137757547		[learning rate: 0.0038872]
	Learning Rate: 0.00388715
	LOSS [training: 0.3544545137757547 | validation: 0.12935363549570586]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2833475580593269		[learning rate: 0.003873]
	Learning Rate: 0.00387305
	LOSS [training: 0.2833475580593269 | validation: 0.21245234855374245]
	TIME [epoch: 8.29 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24894537333655004		[learning rate: 0.003859]
	Learning Rate: 0.00385899
	LOSS [training: 0.24894537333655004 | validation: 0.16342051278801856]
	TIME [epoch: 8.31 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30836331444343745		[learning rate: 0.003845]
	Learning Rate: 0.00384499
	LOSS [training: 0.30836331444343745 | validation: 0.2389430550947485]
	TIME [epoch: 8.29 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2970443205653771		[learning rate: 0.003831]
	Learning Rate: 0.00383103
	LOSS [training: 0.2970443205653771 | validation: 0.18120464968241082]
	TIME [epoch: 8.29 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3434683084888836		[learning rate: 0.0038171]
	Learning Rate: 0.00381713
	LOSS [training: 0.3434683084888836 | validation: 0.19017722407242016]
	TIME [epoch: 8.29 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27725070051048667		[learning rate: 0.0038033]
	Learning Rate: 0.00380328
	LOSS [training: 0.27725070051048667 | validation: 0.1391989353302627]
	TIME [epoch: 8.3 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27809608543548436		[learning rate: 0.0037895]
	Learning Rate: 0.00378947
	LOSS [training: 0.27809608543548436 | validation: 0.24134351818934893]
	TIME [epoch: 8.31 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2766988171387477		[learning rate: 0.0037757]
	Learning Rate: 0.00377572
	LOSS [training: 0.2766988171387477 | validation: 0.15766274577446215]
	TIME [epoch: 8.29 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32788924603262926		[learning rate: 0.003762]
	Learning Rate: 0.00376202
	LOSS [training: 0.32788924603262926 | validation: 0.15131733680452725]
	TIME [epoch: 8.29 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.305058018937725		[learning rate: 0.0037484]
	Learning Rate: 0.00374837
	LOSS [training: 0.305058018937725 | validation: 0.8569542152471343]
	TIME [epoch: 8.29 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38565237477497755		[learning rate: 0.0037348]
	Learning Rate: 0.00373476
	LOSS [training: 0.38565237477497755 | validation: 0.42833495596175286]
	TIME [epoch: 8.31 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32773729987629413		[learning rate: 0.0037212]
	Learning Rate: 0.00372121
	LOSS [training: 0.32773729987629413 | validation: 0.16789679688887524]
	TIME [epoch: 8.29 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2619930626147071		[learning rate: 0.0037077]
	Learning Rate: 0.00370771
	LOSS [training: 0.2619930626147071 | validation: 0.2572770357229216]
	TIME [epoch: 8.29 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3093469616840364		[learning rate: 0.0036943]
	Learning Rate: 0.00369425
	LOSS [training: 0.3093469616840364 | validation: 0.14029021816500017]
	TIME [epoch: 8.3 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37417774449362673		[learning rate: 0.0036808]
	Learning Rate: 0.00368084
	LOSS [training: 0.37417774449362673 | validation: 0.248569201491221]
	TIME [epoch: 8.3 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25280885630657146		[learning rate: 0.0036675]
	Learning Rate: 0.00366749
	LOSS [training: 0.25280885630657146 | validation: 0.44208314742658195]
	TIME [epoch: 8.29 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29836747027202637		[learning rate: 0.0036542]
	Learning Rate: 0.00365418
	LOSS [training: 0.29836747027202637 | validation: 0.34940285573489127]
	TIME [epoch: 8.28 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35157125106487197		[learning rate: 0.0036409]
	Learning Rate: 0.00364092
	LOSS [training: 0.35157125106487197 | validation: 0.2383563904906631]
	TIME [epoch: 8.3 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24593766035473524		[learning rate: 0.0036277]
	Learning Rate: 0.0036277
	LOSS [training: 0.24593766035473524 | validation: 0.9397527620614503]
	TIME [epoch: 8.31 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37657593690920477		[learning rate: 0.0036145]
	Learning Rate: 0.00361454
	LOSS [training: 0.37657593690920477 | validation: 0.37703594107214833]
	TIME [epoch: 8.29 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32761427247540337		[learning rate: 0.0036014]
	Learning Rate: 0.00360142
	LOSS [training: 0.32761427247540337 | validation: 0.16182484478915665]
	TIME [epoch: 8.28 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2776563334266936		[learning rate: 0.0035883]
	Learning Rate: 0.00358835
	LOSS [training: 0.2776563334266936 | validation: 0.2856177163978162]
	TIME [epoch: 8.3 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32197799997369436		[learning rate: 0.0035753]
	Learning Rate: 0.00357533
	LOSS [training: 0.32197799997369436 | validation: 0.22611537914973276]
	TIME [epoch: 8.3 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3403976660967326		[learning rate: 0.0035624]
	Learning Rate: 0.00356235
	LOSS [training: 0.3403976660967326 | validation: 0.32510981299148073]
	TIME [epoch: 8.28 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39903149094599033		[learning rate: 0.0035494]
	Learning Rate: 0.00354942
	LOSS [training: 0.39903149094599033 | validation: 0.3112461691357383]
	TIME [epoch: 8.28 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26563696678077986		[learning rate: 0.0035365]
	Learning Rate: 0.00353654
	LOSS [training: 0.26563696678077986 | validation: 0.1639585354856276]
	TIME [epoch: 8.29 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2870908684673917		[learning rate: 0.0035237]
	Learning Rate: 0.00352371
	LOSS [training: 0.2870908684673917 | validation: 0.1672285284721682]
	TIME [epoch: 8.3 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28044253619162307		[learning rate: 0.0035109]
	Learning Rate: 0.00351092
	LOSS [training: 0.28044253619162307 | validation: 0.16752614080458655]
	TIME [epoch: 8.28 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29836424280782736		[learning rate: 0.0034982]
	Learning Rate: 0.00349818
	LOSS [training: 0.29836424280782736 | validation: 0.18658228081084685]
	TIME [epoch: 8.29 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25217787743357756		[learning rate: 0.0034855]
	Learning Rate: 0.00348548
	LOSS [training: 0.25217787743357756 | validation: 0.2008520333760348]
	TIME [epoch: 8.29 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2652218414071294		[learning rate: 0.0034728]
	Learning Rate: 0.00347284
	LOSS [training: 0.2652218414071294 | validation: 0.1807205220621722]
	TIME [epoch: 8.3 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36214101869677384		[learning rate: 0.0034602]
	Learning Rate: 0.00346023
	LOSS [training: 0.36214101869677384 | validation: 0.23955776710816806]
	TIME [epoch: 8.28 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28568058250195927		[learning rate: 0.0034477]
	Learning Rate: 0.00344767
	LOSS [training: 0.28568058250195927 | validation: 0.19811718703942643]
	TIME [epoch: 8.29 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2101978278976151		[learning rate: 0.0034352]
	Learning Rate: 0.00343516
	LOSS [training: 0.2101978278976151 | validation: 0.12673580050600503]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_394.pth
	Model improved!!!
EPOCH 395/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22914219685656265		[learning rate: 0.0034227]
	Learning Rate: 0.0034227
	LOSS [training: 0.22914219685656265 | validation: 0.3012308950694422]
	TIME [epoch: 8.31 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3170401502664212		[learning rate: 0.0034103]
	Learning Rate: 0.00341028
	LOSS [training: 0.3170401502664212 | validation: 0.27510259352790034]
	TIME [epoch: 8.3 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25503829443365944		[learning rate: 0.0033979]
	Learning Rate: 0.0033979
	LOSS [training: 0.25503829443365944 | validation: 0.22831315754080939]
	TIME [epoch: 8.29 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3277384152847355		[learning rate: 0.0033856]
	Learning Rate: 0.00338557
	LOSS [training: 0.3277384152847355 | validation: 0.13977679807393728]
	TIME [epoch: 8.29 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26862879826120073		[learning rate: 0.0033733]
	Learning Rate: 0.00337328
	LOSS [training: 0.26862879826120073 | validation: 0.19633603814585648]
	TIME [epoch: 8.31 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2828466199036093		[learning rate: 0.003361]
	Learning Rate: 0.00336104
	LOSS [training: 0.2828466199036093 | validation: 0.3279786533542213]
	TIME [epoch: 8.29 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2677086994519574		[learning rate: 0.0033488]
	Learning Rate: 0.00334884
	LOSS [training: 0.2677086994519574 | validation: 0.13754518102753238]
	TIME [epoch: 8.29 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24160564109762372		[learning rate: 0.0033367]
	Learning Rate: 0.00333669
	LOSS [training: 0.24160564109762372 | validation: 0.17750074792005988]
	TIME [epoch: 8.29 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2334241227674378		[learning rate: 0.0033246]
	Learning Rate: 0.00332458
	LOSS [training: 0.2334241227674378 | validation: 0.4133810254655401]
	TIME [epoch: 8.31 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29704088985869836		[learning rate: 0.0033125]
	Learning Rate: 0.00331252
	LOSS [training: 0.29704088985869836 | validation: 0.17364732164187613]
	TIME [epoch: 8.3 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25871715193568967		[learning rate: 0.0033005]
	Learning Rate: 0.00330049
	LOSS [training: 0.25871715193568967 | validation: 0.24672532697346838]
	TIME [epoch: 8.3 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2368522789103682		[learning rate: 0.0032885]
	Learning Rate: 0.00328852
	LOSS [training: 0.2368522789103682 | validation: 0.1821166983411328]
	TIME [epoch: 8.29 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25190377496897315		[learning rate: 0.0032766]
	Learning Rate: 0.00327658
	LOSS [training: 0.25190377496897315 | validation: 0.22285328099077917]
	TIME [epoch: 8.32 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2600411315260193		[learning rate: 0.0032647]
	Learning Rate: 0.00326469
	LOSS [training: 0.2600411315260193 | validation: 0.2149599145278543]
	TIME [epoch: 8.29 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24413102867598707		[learning rate: 0.0032528]
	Learning Rate: 0.00325284
	LOSS [training: 0.24413102867598707 | validation: 0.5324991106479138]
	TIME [epoch: 8.29 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2962384948452008		[learning rate: 0.003241]
	Learning Rate: 0.00324104
	LOSS [training: 0.2962384948452008 | validation: 0.19980890744564925]
	TIME [epoch: 8.29 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22263654491026946		[learning rate: 0.0032293]
	Learning Rate: 0.00322928
	LOSS [training: 0.22263654491026946 | validation: 0.15533507488453066]
	TIME [epoch: 8.32 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2776031938024932		[learning rate: 0.0032176]
	Learning Rate: 0.00321756
	LOSS [training: 0.2776031938024932 | validation: 0.23051535982112548]
	TIME [epoch: 8.3 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24809957514278258		[learning rate: 0.0032059]
	Learning Rate: 0.00320588
	LOSS [training: 0.24809957514278258 | validation: 0.33640389835792023]
	TIME [epoch: 8.29 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2766193121539604		[learning rate: 0.0031942]
	Learning Rate: 0.00319425
	LOSS [training: 0.2766193121539604 | validation: 0.19149773120639008]
	TIME [epoch: 8.3 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2567944395536238		[learning rate: 0.0031827]
	Learning Rate: 0.00318265
	LOSS [training: 0.2567944395536238 | validation: 0.15356765823245358]
	TIME [epoch: 8.33 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2671438480321614		[learning rate: 0.0031711]
	Learning Rate: 0.0031711
	LOSS [training: 0.2671438480321614 | validation: 0.15208723199805435]
	TIME [epoch: 8.3 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22201028989643343		[learning rate: 0.0031596]
	Learning Rate: 0.0031596
	LOSS [training: 0.22201028989643343 | validation: 0.13266774077777344]
	TIME [epoch: 8.3 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25872057626058825		[learning rate: 0.0031481]
	Learning Rate: 0.00314813
	LOSS [training: 0.25872057626058825 | validation: 0.15231653797060207]
	TIME [epoch: 8.29 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3191402015097719		[learning rate: 0.0031367]
	Learning Rate: 0.00313671
	LOSS [training: 0.3191402015097719 | validation: 0.19563119298981385]
	TIME [epoch: 8.32 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25703850007713697		[learning rate: 0.0031253]
	Learning Rate: 0.00312532
	LOSS [training: 0.25703850007713697 | validation: 0.14370475212614275]
	TIME [epoch: 8.29 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23165683624900124		[learning rate: 0.003114]
	Learning Rate: 0.00311398
	LOSS [training: 0.23165683624900124 | validation: 0.12230040688739616]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2116044160659445		[learning rate: 0.0031027]
	Learning Rate: 0.00310268
	LOSS [training: 0.2116044160659445 | validation: 0.1855511474741246]
	TIME [epoch: 8.3 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2701089479543768		[learning rate: 0.0030914]
	Learning Rate: 0.00309142
	LOSS [training: 0.2701089479543768 | validation: 0.17239998982536947]
	TIME [epoch: 8.31 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2043359306127221		[learning rate: 0.0030802]
	Learning Rate: 0.0030802
	LOSS [training: 0.2043359306127221 | validation: 0.13411401746622642]
	TIME [epoch: 8.29 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23695326570682945		[learning rate: 0.003069]
	Learning Rate: 0.00306902
	LOSS [training: 0.23695326570682945 | validation: 0.19133143346951909]
	TIME [epoch: 8.29 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22485241399186964		[learning rate: 0.0030579]
	Learning Rate: 0.00305788
	LOSS [training: 0.22485241399186964 | validation: 0.11323930063911411]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_426.pth
	Model improved!!!
EPOCH 427/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2188277078421948		[learning rate: 0.0030468]
	Learning Rate: 0.00304679
	LOSS [training: 0.2188277078421948 | validation: 0.17487502653836756]
	TIME [epoch: 8.32 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22144153328210786		[learning rate: 0.0030357]
	Learning Rate: 0.00303573
	LOSS [training: 0.22144153328210786 | validation: 0.23015862569245496]
	TIME [epoch: 8.29 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27873241706054186		[learning rate: 0.0030247]
	Learning Rate: 0.00302471
	LOSS [training: 0.27873241706054186 | validation: 0.2887091620566585]
	TIME [epoch: 8.29 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20671552353909534		[learning rate: 0.0030137]
	Learning Rate: 0.00301374
	LOSS [training: 0.20671552353909534 | validation: 0.11922903736471527]
	TIME [epoch: 8.29 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20845208813106866		[learning rate: 0.0030028]
	Learning Rate: 0.0030028
	LOSS [training: 0.20845208813106866 | validation: 0.24183859429681542]
	TIME [epoch: 8.31 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20712994781067406		[learning rate: 0.0029919]
	Learning Rate: 0.0029919
	LOSS [training: 0.20712994781067406 | validation: 0.2031625955280169]
	TIME [epoch: 8.3 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2427555551568119		[learning rate: 0.002981]
	Learning Rate: 0.00298104
	LOSS [training: 0.2427555551568119 | validation: 0.3128764150534552]
	TIME [epoch: 8.29 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25646755291356405		[learning rate: 0.0029702]
	Learning Rate: 0.00297023
	LOSS [training: 0.25646755291356405 | validation: 0.29142256929306826]
	TIME [epoch: 8.29 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22442142820830266		[learning rate: 0.0029594]
	Learning Rate: 0.00295945
	LOSS [training: 0.22442142820830266 | validation: 0.12453982656282245]
	TIME [epoch: 8.3 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22924293407481228		[learning rate: 0.0029487]
	Learning Rate: 0.00294871
	LOSS [training: 0.22924293407481228 | validation: 0.12651806371440777]
	TIME [epoch: 8.3 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23326472834178155		[learning rate: 0.002938]
	Learning Rate: 0.00293801
	LOSS [training: 0.23326472834178155 | validation: 0.1219345684043528]
	TIME [epoch: 8.29 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21447515257810287		[learning rate: 0.0029273]
	Learning Rate: 0.00292734
	LOSS [training: 0.21447515257810287 | validation: 0.20179717179801587]
	TIME [epoch: 8.29 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18680014748113505		[learning rate: 0.0029167]
	Learning Rate: 0.00291672
	LOSS [training: 0.18680014748113505 | validation: 0.16579361663374984]
	TIME [epoch: 8.31 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22691601107044948		[learning rate: 0.0029061]
	Learning Rate: 0.00290614
	LOSS [training: 0.22691601107044948 | validation: 0.1754672000254428]
	TIME [epoch: 8.31 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19185736913854007		[learning rate: 0.0028956]
	Learning Rate: 0.00289559
	LOSS [training: 0.19185736913854007 | validation: 0.23659072672793624]
	TIME [epoch: 8.3 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2719536240084251		[learning rate: 0.0028851]
	Learning Rate: 0.00288508
	LOSS [training: 0.2719536240084251 | validation: 0.2227022909336377]
	TIME [epoch: 8.3 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.227667539062227		[learning rate: 0.0028746]
	Learning Rate: 0.00287461
	LOSS [training: 0.227667539062227 | validation: 0.11779733089456418]
	TIME [epoch: 8.32 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2548972376596811		[learning rate: 0.0028642]
	Learning Rate: 0.00286418
	LOSS [training: 0.2548972376596811 | validation: 0.15964904763894205]
	TIME [epoch: 8.31 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21859512033441247		[learning rate: 0.0028538]
	Learning Rate: 0.00285378
	LOSS [training: 0.21859512033441247 | validation: 0.2605540189440963]
	TIME [epoch: 8.29 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20211851711241335		[learning rate: 0.0028434]
	Learning Rate: 0.00284343
	LOSS [training: 0.20211851711241335 | validation: 0.1944614999475655]
	TIME [epoch: 8.29 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22650178363990653		[learning rate: 0.0028331]
	Learning Rate: 0.00283311
	LOSS [training: 0.22650178363990653 | validation: 0.1274879467563985]
	TIME [epoch: 8.31 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2630919839186757		[learning rate: 0.0028228]
	Learning Rate: 0.00282283
	LOSS [training: 0.2630919839186757 | validation: 0.12037080971220385]
	TIME [epoch: 8.3 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26219321465486073		[learning rate: 0.0028126]
	Learning Rate: 0.00281258
	LOSS [training: 0.26219321465486073 | validation: 0.1110341101965546]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23380539358855185		[learning rate: 0.0028024]
	Learning Rate: 0.00280238
	LOSS [training: 0.23380539358855185 | validation: 0.18225312833801688]
	TIME [epoch: 8.3 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19732568651456645		[learning rate: 0.0027922]
	Learning Rate: 0.00279221
	LOSS [training: 0.19732568651456645 | validation: 0.1809320384220883]
	TIME [epoch: 8.3 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23814108712834967		[learning rate: 0.0027821]
	Learning Rate: 0.00278207
	LOSS [training: 0.23814108712834967 | validation: 0.2550830570700137]
	TIME [epoch: 8.3 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22217668594058665		[learning rate: 0.002772]
	Learning Rate: 0.00277198
	LOSS [training: 0.22217668594058665 | validation: 0.10026013085047977]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21455917835112093		[learning rate: 0.0027619]
	Learning Rate: 0.00276192
	LOSS [training: 0.21455917835112093 | validation: 0.12926451190376692]
	TIME [epoch: 8.3 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20204309582590754		[learning rate: 0.0027519]
	Learning Rate: 0.00275189
	LOSS [training: 0.20204309582590754 | validation: 0.14526307259244095]
	TIME [epoch: 8.31 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20565040854197297		[learning rate: 0.0027419]
	Learning Rate: 0.00274191
	LOSS [training: 0.20565040854197297 | validation: 0.2027422246502929]
	TIME [epoch: 8.32 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21679989529963478		[learning rate: 0.002732]
	Learning Rate: 0.00273196
	LOSS [training: 0.21679989529963478 | validation: 0.24409551917469924]
	TIME [epoch: 8.3 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25724149095309606		[learning rate: 0.002722]
	Learning Rate: 0.00272204
	LOSS [training: 0.25724149095309606 | validation: 0.113808267028841]
	TIME [epoch: 8.3 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22139268509404797		[learning rate: 0.0027122]
	Learning Rate: 0.00271216
	LOSS [training: 0.22139268509404797 | validation: 0.302209496187435]
	TIME [epoch: 8.3 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30756120193969316		[learning rate: 0.0027023]
	Learning Rate: 0.00270232
	LOSS [training: 0.30756120193969316 | validation: 0.1868744420167976]
	TIME [epoch: 8.32 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1973693964684004		[learning rate: 0.0026925]
	Learning Rate: 0.00269251
	LOSS [training: 0.1973693964684004 | validation: 0.20875868883056614]
	TIME [epoch: 8.3 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22110162680582074		[learning rate: 0.0026827]
	Learning Rate: 0.00268274
	LOSS [training: 0.22110162680582074 | validation: 0.13996599377204072]
	TIME [epoch: 8.29 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2014637419657898		[learning rate: 0.002673]
	Learning Rate: 0.00267301
	LOSS [training: 0.2014637419657898 | validation: 0.11832716028664257]
	TIME [epoch: 8.3 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18245218050761483		[learning rate: 0.0026633]
	Learning Rate: 0.00266331
	LOSS [training: 0.18245218050761483 | validation: 0.2511834191072939]
	TIME [epoch: 8.32 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1969917031070839		[learning rate: 0.0026536]
	Learning Rate: 0.00265364
	LOSS [training: 0.1969917031070839 | validation: 0.17089618506553195]
	TIME [epoch: 8.3 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19943222899825228		[learning rate: 0.002644]
	Learning Rate: 0.00264401
	LOSS [training: 0.19943222899825228 | validation: 0.15393600370671945]
	TIME [epoch: 8.3 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19981501024922088		[learning rate: 0.0026344]
	Learning Rate: 0.00263441
	LOSS [training: 0.19981501024922088 | validation: 0.09674616351114591]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17733268267102148		[learning rate: 0.0026249]
	Learning Rate: 0.00262485
	LOSS [training: 0.17733268267102148 | validation: 0.20909729958067477]
	TIME [epoch: 8.33 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20692309493876473		[learning rate: 0.0026153]
	Learning Rate: 0.00261533
	LOSS [training: 0.20692309493876473 | validation: 0.26155577068425145]
	TIME [epoch: 8.3 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18077049733089445		[learning rate: 0.0026058]
	Learning Rate: 0.00260584
	LOSS [training: 0.18077049733089445 | validation: 0.10503521024591424]
	TIME [epoch: 8.3 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1860016486189589		[learning rate: 0.0025964]
	Learning Rate: 0.00259638
	LOSS [training: 0.1860016486189589 | validation: 0.16446348640142416]
	TIME [epoch: 8.3 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.280213640736375		[learning rate: 0.002587]
	Learning Rate: 0.00258696
	LOSS [training: 0.280213640736375 | validation: 0.15706993626366778]
	TIME [epoch: 8.33 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1772160379056505		[learning rate: 0.0025776]
	Learning Rate: 0.00257757
	LOSS [training: 0.1772160379056505 | validation: 0.15906953264743706]
	TIME [epoch: 8.31 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20379275603857933		[learning rate: 0.0025682]
	Learning Rate: 0.00256822
	LOSS [training: 0.20379275603857933 | validation: 0.10189809300905239]
	TIME [epoch: 8.3 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18661761196119783		[learning rate: 0.0025589]
	Learning Rate: 0.0025589
	LOSS [training: 0.18661761196119783 | validation: 0.09992316893868028]
	TIME [epoch: 8.3 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2311453048844665		[learning rate: 0.0025496]
	Learning Rate: 0.00254961
	LOSS [training: 0.2311453048844665 | validation: 0.15766551327877215]
	TIME [epoch: 8.32 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19291147758120486		[learning rate: 0.0025404]
	Learning Rate: 0.00254036
	LOSS [training: 0.19291147758120486 | validation: 0.12312796946484858]
	TIME [epoch: 8.31 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18751843712874255		[learning rate: 0.0025311]
	Learning Rate: 0.00253114
	LOSS [training: 0.18751843712874255 | validation: 0.12149503271636361]
	TIME [epoch: 8.3 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20574407924776197		[learning rate: 0.002522]
	Learning Rate: 0.00252195
	LOSS [training: 0.20574407924776197 | validation: 0.12414318936011773]
	TIME [epoch: 8.3 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19851542394799804		[learning rate: 0.0025128]
	Learning Rate: 0.0025128
	LOSS [training: 0.19851542394799804 | validation: 0.14792381038508998]
	TIME [epoch: 8.33 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21988705223338148		[learning rate: 0.0025037]
	Learning Rate: 0.00250368
	LOSS [training: 0.21988705223338148 | validation: 0.11675010847173645]
	TIME [epoch: 8.31 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22370305460657774		[learning rate: 0.0024946]
	Learning Rate: 0.00249459
	LOSS [training: 0.22370305460657774 | validation: 0.13778472111925721]
	TIME [epoch: 8.3 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2547204258851549		[learning rate: 0.0024855]
	Learning Rate: 0.00248554
	LOSS [training: 0.2547204258851549 | validation: 0.1558920871287412]
	TIME [epoch: 8.3 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22959367088656019		[learning rate: 0.0024765]
	Learning Rate: 0.00247652
	LOSS [training: 0.22959367088656019 | validation: 0.1357616201554841]
	TIME [epoch: 8.33 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2127739737158289		[learning rate: 0.0024675]
	Learning Rate: 0.00246753
	LOSS [training: 0.2127739737158289 | validation: 0.1717378010990251]
	TIME [epoch: 8.31 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21304164103916207		[learning rate: 0.0024586]
	Learning Rate: 0.00245858
	LOSS [training: 0.21304164103916207 | validation: 0.1205951253958922]
	TIME [epoch: 8.3 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16645817893272402		[learning rate: 0.0024497]
	Learning Rate: 0.00244966
	LOSS [training: 0.16645817893272402 | validation: 0.14045706217835632]
	TIME [epoch: 8.3 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22370716436303617		[learning rate: 0.0024408]
	Learning Rate: 0.00244077
	LOSS [training: 0.22370716436303617 | validation: 0.12449302282528718]
	TIME [epoch: 8.33 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19465603467397957		[learning rate: 0.0024319]
	Learning Rate: 0.00243191
	LOSS [training: 0.19465603467397957 | validation: 0.12186712539094646]
	TIME [epoch: 8.31 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1830423176163844		[learning rate: 0.0024231]
	Learning Rate: 0.00242308
	LOSS [training: 0.1830423176163844 | validation: 0.14544825309775725]
	TIME [epoch: 8.3 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20262425566172274		[learning rate: 0.0024143]
	Learning Rate: 0.00241429
	LOSS [training: 0.20262425566172274 | validation: 0.141554413097545]
	TIME [epoch: 8.3 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1894501790955729		[learning rate: 0.0024055]
	Learning Rate: 0.00240553
	LOSS [training: 0.1894501790955729 | validation: 0.10531578415263662]
	TIME [epoch: 8.32 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17423468480767076		[learning rate: 0.0023968]
	Learning Rate: 0.0023968
	LOSS [training: 0.17423468480767076 | validation: 0.2025556457082862]
	TIME [epoch: 8.31 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18767834760643254		[learning rate: 0.0023881]
	Learning Rate: 0.0023881
	LOSS [training: 0.18767834760643254 | validation: 0.2503152365146036]
	TIME [epoch: 8.3 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20286989946823802		[learning rate: 0.0023794]
	Learning Rate: 0.00237943
	LOSS [training: 0.20286989946823802 | validation: 0.2303839804624357]
	TIME [epoch: 8.3 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23792406810197586		[learning rate: 0.0023708]
	Learning Rate: 0.0023708
	LOSS [training: 0.23792406810197586 | validation: 0.13989321945815736]
	TIME [epoch: 8.32 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20071892562095428		[learning rate: 0.0023622]
	Learning Rate: 0.0023622
	LOSS [training: 0.20071892562095428 | validation: 0.11177434018334391]
	TIME [epoch: 8.31 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16720027408316657		[learning rate: 0.0023536]
	Learning Rate: 0.00235362
	LOSS [training: 0.16720027408316657 | validation: 0.13246873414690152]
	TIME [epoch: 8.3 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18254221505759358		[learning rate: 0.0023451]
	Learning Rate: 0.00234508
	LOSS [training: 0.18254221505759358 | validation: 0.11883175502221544]
	TIME [epoch: 8.3 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17334503938734364		[learning rate: 0.0023366]
	Learning Rate: 0.00233657
	LOSS [training: 0.17334503938734364 | validation: 0.18727792148708028]
	TIME [epoch: 8.33 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20828255001226403		[learning rate: 0.0023281]
	Learning Rate: 0.00232809
	LOSS [training: 0.20828255001226403 | validation: 0.14825499330381425]
	TIME [epoch: 8.31 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20059961349428962		[learning rate: 0.0023196]
	Learning Rate: 0.00231964
	LOSS [training: 0.20059961349428962 | validation: 0.11657067389561934]
	TIME [epoch: 8.3 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18991003057763314		[learning rate: 0.0023112]
	Learning Rate: 0.00231122
	LOSS [training: 0.18991003057763314 | validation: 0.14708786974124605]
	TIME [epoch: 8.3 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1948950860913102		[learning rate: 0.0023028]
	Learning Rate: 0.00230284
	LOSS [training: 0.1948950860913102 | validation: 0.09105106582760589]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_504.pth
	Model improved!!!
EPOCH 505/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1650805339115121		[learning rate: 0.0022945]
	Learning Rate: 0.00229448
	LOSS [training: 0.1650805339115121 | validation: 0.2827622077538585]
	TIME [epoch: 8.3 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2115831264258122		[learning rate: 0.0022862]
	Learning Rate: 0.00228615
	LOSS [training: 0.2115831264258122 | validation: 0.15819152247413326]
	TIME [epoch: 8.3 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1941795476687855		[learning rate: 0.0022779]
	Learning Rate: 0.00227786
	LOSS [training: 0.1941795476687855 | validation: 0.1276873944147576]
	TIME [epoch: 8.29 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1453543854708278		[learning rate: 0.0022696]
	Learning Rate: 0.00226959
	LOSS [training: 0.1453543854708278 | validation: 0.32668149948619013]
	TIME [epoch: 8.31 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2086819887628891		[learning rate: 0.0022614]
	Learning Rate: 0.00226135
	LOSS [training: 0.2086819887628891 | validation: 0.1562198824452954]
	TIME [epoch: 8.31 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21739566562692914		[learning rate: 0.0022531]
	Learning Rate: 0.00225315
	LOSS [training: 0.21739566562692914 | validation: 0.1553370381018479]
	TIME [epoch: 8.3 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19217791508889048		[learning rate: 0.002245]
	Learning Rate: 0.00224497
	LOSS [training: 0.19217791508889048 | validation: 0.14652217819778918]
	TIME [epoch: 8.29 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1919732585608333		[learning rate: 0.0022368]
	Learning Rate: 0.00223682
	LOSS [training: 0.1919732585608333 | validation: 0.17054454729334226]
	TIME [epoch: 8.32 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20337211264193095		[learning rate: 0.0022287]
	Learning Rate: 0.00222871
	LOSS [training: 0.20337211264193095 | validation: 0.11648309655976619]
	TIME [epoch: 8.31 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1410871308817425		[learning rate: 0.0022206]
	Learning Rate: 0.00222062
	LOSS [training: 0.1410871308817425 | validation: 0.08960408828821037]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_514.pth
	Model improved!!!
EPOCH 515/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19894423759296626		[learning rate: 0.0022126]
	Learning Rate: 0.00221256
	LOSS [training: 0.19894423759296626 | validation: 0.11279080036909175]
	TIME [epoch: 8.3 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15366305967400246		[learning rate: 0.0022045]
	Learning Rate: 0.00220453
	LOSS [training: 0.15366305967400246 | validation: 0.1083465666579027]
	TIME [epoch: 8.31 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20101005408686903		[learning rate: 0.0021965]
	Learning Rate: 0.00219653
	LOSS [training: 0.20101005408686903 | validation: 0.11293701672726306]
	TIME [epoch: 8.3 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16766103976271288		[learning rate: 0.0021886]
	Learning Rate: 0.00218856
	LOSS [training: 0.16766103976271288 | validation: 0.12473688455217256]
	TIME [epoch: 8.29 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20487780770276834		[learning rate: 0.0021806]
	Learning Rate: 0.00218061
	LOSS [training: 0.20487780770276834 | validation: 0.10042796227385967]
	TIME [epoch: 8.29 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22592952284952078		[learning rate: 0.0021727]
	Learning Rate: 0.0021727
	LOSS [training: 0.22592952284952078 | validation: 0.11925901370144722]
	TIME [epoch: 8.29 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20741239234696135		[learning rate: 0.0021648]
	Learning Rate: 0.00216482
	LOSS [training: 0.20741239234696135 | validation: 0.19851337520730905]
	TIME [epoch: 8.32 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17675658673246575		[learning rate: 0.002157]
	Learning Rate: 0.00215696
	LOSS [training: 0.17675658673246575 | validation: 0.18027073124775753]
	TIME [epoch: 8.3 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17972737268987724		[learning rate: 0.0021491]
	Learning Rate: 0.00214913
	LOSS [training: 0.17972737268987724 | validation: 0.10701453780446846]
	TIME [epoch: 8.3 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1571794238722098		[learning rate: 0.0021413]
	Learning Rate: 0.00214133
	LOSS [training: 0.1571794238722098 | validation: 0.10226412658140202]
	TIME [epoch: 8.29 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19620181033386536		[learning rate: 0.0021336]
	Learning Rate: 0.00213356
	LOSS [training: 0.19620181033386536 | validation: 0.12014563239267538]
	TIME [epoch: 8.32 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15218928723735806		[learning rate: 0.0021258]
	Learning Rate: 0.00212582
	LOSS [training: 0.15218928723735806 | validation: 0.21757453267194804]
	TIME [epoch: 8.29 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24544254899277926		[learning rate: 0.0021181]
	Learning Rate: 0.0021181
	LOSS [training: 0.24544254899277926 | validation: 0.17290682926157877]
	TIME [epoch: 8.3 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17271333265924316		[learning rate: 0.0021104]
	Learning Rate: 0.00211042
	LOSS [training: 0.17271333265924316 | validation: 0.14723243601194463]
	TIME [epoch: 8.29 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2024944660427877		[learning rate: 0.0021028]
	Learning Rate: 0.00210276
	LOSS [training: 0.2024944660427877 | validation: 0.11928426383992297]
	TIME [epoch: 8.31 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16949084290300886		[learning rate: 0.0020951]
	Learning Rate: 0.00209513
	LOSS [training: 0.16949084290300886 | validation: 0.11013845892328608]
	TIME [epoch: 8.29 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17440374816531548		[learning rate: 0.0020875]
	Learning Rate: 0.00208752
	LOSS [training: 0.17440374816531548 | validation: 0.12637798147940724]
	TIME [epoch: 8.28 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18575112482038406		[learning rate: 0.0020799]
	Learning Rate: 0.00207995
	LOSS [training: 0.18575112482038406 | validation: 0.5440450569511521]
	TIME [epoch: 8.28 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2363148915042284		[learning rate: 0.0020724]
	Learning Rate: 0.0020724
	LOSS [training: 0.2363148915042284 | validation: 0.12422811825460348]
	TIME [epoch: 8.31 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1575088639131636		[learning rate: 0.0020649]
	Learning Rate: 0.00206488
	LOSS [training: 0.1575088639131636 | validation: 0.12536634172206998]
	TIME [epoch: 8.28 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19750485939221846		[learning rate: 0.0020574]
	Learning Rate: 0.00205739
	LOSS [training: 0.19750485939221846 | validation: 0.1778248059943112]
	TIME [epoch: 8.29 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18453770831400335		[learning rate: 0.0020499]
	Learning Rate: 0.00204992
	LOSS [training: 0.18453770831400335 | validation: 0.13532454336231853]
	TIME [epoch: 8.28 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21766214605793763		[learning rate: 0.0020425]
	Learning Rate: 0.00204248
	LOSS [training: 0.21766214605793763 | validation: 0.12423571203032932]
	TIME [epoch: 8.31 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1952729475563582		[learning rate: 0.0020351]
	Learning Rate: 0.00203507
	LOSS [training: 0.1952729475563582 | validation: 0.11642999716994537]
	TIME [epoch: 8.28 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16003663941265972		[learning rate: 0.0020277]
	Learning Rate: 0.00202768
	LOSS [training: 0.16003663941265972 | validation: 0.16344544157135085]
	TIME [epoch: 8.28 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21465762747430278		[learning rate: 0.0020203]
	Learning Rate: 0.00202032
	LOSS [training: 0.21465762747430278 | validation: 0.13132572869226436]
	TIME [epoch: 8.29 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16538822997424715		[learning rate: 0.002013]
	Learning Rate: 0.00201299
	LOSS [training: 0.16538822997424715 | validation: 0.1413271428004054]
	TIME [epoch: 8.31 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19788378107505547		[learning rate: 0.0020057]
	Learning Rate: 0.00200569
	LOSS [training: 0.19788378107505547 | validation: 0.08172725772951711]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_542.pth
	Model improved!!!
EPOCH 543/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1613781858970808		[learning rate: 0.0019984]
	Learning Rate: 0.00199841
	LOSS [training: 0.1613781858970808 | validation: 0.1489592951496903]
	TIME [epoch: 8.28 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1834287157942317		[learning rate: 0.0019912]
	Learning Rate: 0.00199116
	LOSS [training: 0.1834287157942317 | validation: 0.15975667418161427]
	TIME [epoch: 8.29 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1677232443316198		[learning rate: 0.0019839]
	Learning Rate: 0.00198393
	LOSS [training: 0.1677232443316198 | validation: 0.1291437197502026]
	TIME [epoch: 8.31 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18007459775230517		[learning rate: 0.0019767]
	Learning Rate: 0.00197673
	LOSS [training: 0.18007459775230517 | validation: 0.16571857817501723]
	TIME [epoch: 8.29 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16959285726578496		[learning rate: 0.0019696]
	Learning Rate: 0.00196956
	LOSS [training: 0.16959285726578496 | validation: 0.13892935311004231]
	TIME [epoch: 8.28 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16998161890133698		[learning rate: 0.0019624]
	Learning Rate: 0.00196241
	LOSS [training: 0.16998161890133698 | validation: 0.0863069607097385]
	TIME [epoch: 8.28 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16260513414559857		[learning rate: 0.0019553]
	Learning Rate: 0.00195529
	LOSS [training: 0.16260513414559857 | validation: 0.12255075457492724]
	TIME [epoch: 8.31 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20018447135927375		[learning rate: 0.0019482]
	Learning Rate: 0.00194819
	LOSS [training: 0.20018447135927375 | validation: 0.10019907236163181]
	TIME [epoch: 8.28 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1594305594276851		[learning rate: 0.0019411]
	Learning Rate: 0.00194112
	LOSS [training: 0.1594305594276851 | validation: 0.11529700005226001]
	TIME [epoch: 8.29 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16625474316140895		[learning rate: 0.0019341]
	Learning Rate: 0.00193408
	LOSS [training: 0.16625474316140895 | validation: 0.12016614254430874]
	TIME [epoch: 8.28 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18889200171146156		[learning rate: 0.0019271]
	Learning Rate: 0.00192706
	LOSS [training: 0.18889200171146156 | validation: 0.16004569359936965]
	TIME [epoch: 8.31 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17930558524403026		[learning rate: 0.0019201]
	Learning Rate: 0.00192006
	LOSS [training: 0.17930558524403026 | validation: 0.07052052794850328]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_554.pth
	Model improved!!!
EPOCH 555/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14590591739548228		[learning rate: 0.0019131]
	Learning Rate: 0.0019131
	LOSS [training: 0.14590591739548228 | validation: 0.09928219553127662]
	TIME [epoch: 8.28 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1547234298878794		[learning rate: 0.0019062]
	Learning Rate: 0.00190615
	LOSS [training: 0.1547234298878794 | validation: 0.0983082488394165]
	TIME [epoch: 8.28 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24334354027673263		[learning rate: 0.0018992]
	Learning Rate: 0.00189924
	LOSS [training: 0.24334354027673263 | validation: 0.08693515839119031]
	TIME [epoch: 8.3 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1661265360144053		[learning rate: 0.0018923]
	Learning Rate: 0.00189234
	LOSS [training: 0.1661265360144053 | validation: 0.10768322951113138]
	TIME [epoch: 8.28 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17835901021685768		[learning rate: 0.0018855]
	Learning Rate: 0.00188548
	LOSS [training: 0.17835901021685768 | validation: 0.16401816295968505]
	TIME [epoch: 8.28 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22073308692793897		[learning rate: 0.0018786]
	Learning Rate: 0.00187863
	LOSS [training: 0.22073308692793897 | validation: 0.1250028598592987]
	TIME [epoch: 8.28 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16181631679186934		[learning rate: 0.0018718]
	Learning Rate: 0.00187182
	LOSS [training: 0.16181631679186934 | validation: 0.09302205419736907]
	TIME [epoch: 8.3 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16236008345203207		[learning rate: 0.001865]
	Learning Rate: 0.00186502
	LOSS [training: 0.16236008345203207 | validation: 0.13321742852063972]
	TIME [epoch: 8.29 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1690623917559228		[learning rate: 0.0018583]
	Learning Rate: 0.00185825
	LOSS [training: 0.1690623917559228 | validation: 0.10500522967807319]
	TIME [epoch: 8.28 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1658714044934768		[learning rate: 0.0018515]
	Learning Rate: 0.00185151
	LOSS [training: 0.1658714044934768 | validation: 0.23129792036196528]
	TIME [epoch: 8.28 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17972596343365796		[learning rate: 0.0018448]
	Learning Rate: 0.00184479
	LOSS [training: 0.17972596343365796 | validation: 0.17780839627739145]
	TIME [epoch: 8.3 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1711467439699295		[learning rate: 0.0018381]
	Learning Rate: 0.0018381
	LOSS [training: 0.1711467439699295 | validation: 0.09226452268180682]
	TIME [epoch: 8.3 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18974841231539114		[learning rate: 0.0018314]
	Learning Rate: 0.00183143
	LOSS [training: 0.18974841231539114 | validation: 0.0864702265907169]
	TIME [epoch: 8.28 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14465173572811357		[learning rate: 0.0018248]
	Learning Rate: 0.00182478
	LOSS [training: 0.14465173572811357 | validation: 0.11503485448574652]
	TIME [epoch: 8.28 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14851640199776378		[learning rate: 0.0018182]
	Learning Rate: 0.00181816
	LOSS [training: 0.14851640199776378 | validation: 0.18230152630910768]
	TIME [epoch: 8.3 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16594261076888325		[learning rate: 0.0018116]
	Learning Rate: 0.00181156
	LOSS [training: 0.16594261076888325 | validation: 0.0945904690995002]
	TIME [epoch: 8.29 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19128227721920926		[learning rate: 0.001805]
	Learning Rate: 0.00180499
	LOSS [training: 0.19128227721920926 | validation: 0.1375980868467334]
	TIME [epoch: 8.28 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1711622433046645		[learning rate: 0.0017984]
	Learning Rate: 0.00179844
	LOSS [training: 0.1711622433046645 | validation: 0.07890949625922514]
	TIME [epoch: 8.28 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1540123759052534		[learning rate: 0.0017919]
	Learning Rate: 0.00179191
	LOSS [training: 0.1540123759052534 | validation: 0.1484897050165065]
	TIME [epoch: 8.3 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1627539725863293		[learning rate: 0.0017854]
	Learning Rate: 0.00178541
	LOSS [training: 0.1627539725863293 | validation: 0.09331284095544332]
	TIME [epoch: 8.29 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14179966605437927		[learning rate: 0.0017789]
	Learning Rate: 0.00177893
	LOSS [training: 0.14179966605437927 | validation: 0.2352152569741307]
	TIME [epoch: 8.28 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1933536104260448		[learning rate: 0.0017725]
	Learning Rate: 0.00177247
	LOSS [training: 0.1933536104260448 | validation: 0.10341536818526421]
	TIME [epoch: 8.29 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16309338758973077		[learning rate: 0.001766]
	Learning Rate: 0.00176604
	LOSS [training: 0.16309338758973077 | validation: 0.2047313707868889]
	TIME [epoch: 8.3 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15399211549476616		[learning rate: 0.0017596]
	Learning Rate: 0.00175963
	LOSS [training: 0.15399211549476616 | validation: 0.1660782236064003]
	TIME [epoch: 8.29 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1861512150195555		[learning rate: 0.0017532]
	Learning Rate: 0.00175324
	LOSS [training: 0.1861512150195555 | validation: 0.10736655779068768]
	TIME [epoch: 8.28 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14884842445965074		[learning rate: 0.0017469]
	Learning Rate: 0.00174688
	LOSS [training: 0.14884842445965074 | validation: 0.12059679205106985]
	TIME [epoch: 8.28 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16274708937791657		[learning rate: 0.0017405]
	Learning Rate: 0.00174054
	LOSS [training: 0.16274708937791657 | validation: 0.12356744468781912]
	TIME [epoch: 8.3 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15756405486514977		[learning rate: 0.0017342]
	Learning Rate: 0.00173422
	LOSS [training: 0.15756405486514977 | validation: 0.08241354343417867]
	TIME [epoch: 8.29 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13190301178745256		[learning rate: 0.0017279]
	Learning Rate: 0.00172793
	LOSS [training: 0.13190301178745256 | validation: 0.09262722918787289]
	TIME [epoch: 8.28 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1684767948445413		[learning rate: 0.0017217]
	Learning Rate: 0.00172166
	LOSS [training: 0.1684767948445413 | validation: 0.07020263667609306]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14357042978512102		[learning rate: 0.0017154]
	Learning Rate: 0.00171541
	LOSS [training: 0.14357042978512102 | validation: 0.11637614811428662]
	TIME [epoch: 8.31 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1870558060468898		[learning rate: 0.0017092]
	Learning Rate: 0.00170919
	LOSS [training: 0.1870558060468898 | validation: 0.0931208586183575]
	TIME [epoch: 8.29 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.212270833630676		[learning rate: 0.001703]
	Learning Rate: 0.00170298
	LOSS [training: 0.212270833630676 | validation: 0.10732322167985059]
	TIME [epoch: 8.29 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1502736748091885		[learning rate: 0.0016968]
	Learning Rate: 0.0016968
	LOSS [training: 0.1502736748091885 | validation: 0.1792381992134756]
	TIME [epoch: 8.29 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13283524721121445		[learning rate: 0.0016906]
	Learning Rate: 0.00169065
	LOSS [training: 0.13283524721121445 | validation: 0.07041025998264702]
	TIME [epoch: 8.29 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13830902666773343		[learning rate: 0.0016845]
	Learning Rate: 0.00168451
	LOSS [training: 0.13830902666773343 | validation: 0.08335383246685099]
	TIME [epoch: 8.3 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12823032056681247		[learning rate: 0.0016784]
	Learning Rate: 0.0016784
	LOSS [training: 0.12823032056681247 | validation: 0.09139473459489034]
	TIME [epoch: 8.29 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1501902405058994		[learning rate: 0.0016723]
	Learning Rate: 0.00167231
	LOSS [training: 0.1501902405058994 | validation: 0.11644050197675415]
	TIME [epoch: 8.29 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1880779804932174		[learning rate: 0.0016662]
	Learning Rate: 0.00166624
	LOSS [training: 0.1880779804932174 | validation: 0.11970751647218622]
	TIME [epoch: 8.29 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16476628900251264		[learning rate: 0.0016602]
	Learning Rate: 0.00166019
	LOSS [training: 0.16476628900251264 | validation: 0.13904728552832982]
	TIME [epoch: 8.31 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15307864323330994		[learning rate: 0.0016542]
	Learning Rate: 0.00165417
	LOSS [training: 0.15307864323330994 | validation: 0.11926620501685092]
	TIME [epoch: 8.28 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17177498588653978		[learning rate: 0.0016482]
	Learning Rate: 0.00164816
	LOSS [training: 0.17177498588653978 | validation: 0.15782526975634337]
	TIME [epoch: 8.28 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1345937030504658		[learning rate: 0.0016422]
	Learning Rate: 0.00164218
	LOSS [training: 0.1345937030504658 | validation: 0.08234793045579342]
	TIME [epoch: 8.29 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15916923706843344		[learning rate: 0.0016362]
	Learning Rate: 0.00163622
	LOSS [training: 0.15916923706843344 | validation: 0.1091033090942865]
	TIME [epoch: 8.31 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16589049374999995		[learning rate: 0.0016303]
	Learning Rate: 0.00163028
	LOSS [training: 0.16589049374999995 | validation: 0.0924404315175979]
	TIME [epoch: 8.28 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13964027206472904		[learning rate: 0.0016244]
	Learning Rate: 0.00162437
	LOSS [training: 0.13964027206472904 | validation: 0.11007549539893476]
	TIME [epoch: 8.29 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2029536941899211		[learning rate: 0.0016185]
	Learning Rate: 0.00161847
	LOSS [training: 0.2029536941899211 | validation: 0.08066409451060358]
	TIME [epoch: 8.29 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13798841431366976		[learning rate: 0.0016126]
	Learning Rate: 0.0016126
	LOSS [training: 0.13798841431366976 | validation: 0.17439401466387044]
	TIME [epoch: 8.3 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19317741042521908		[learning rate: 0.0016067]
	Learning Rate: 0.00160675
	LOSS [training: 0.19317741042521908 | validation: 0.1166798788836294]
	TIME [epoch: 8.29 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16685067359357778		[learning rate: 0.0016009]
	Learning Rate: 0.00160092
	LOSS [training: 0.16685067359357778 | validation: 0.09856608797288849]
	TIME [epoch: 8.28 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15662230452754128		[learning rate: 0.0015951]
	Learning Rate: 0.00159511
	LOSS [training: 0.15662230452754128 | validation: 0.2542561944824535]
	TIME [epoch: 8.29 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15850767638179392		[learning rate: 0.0015893]
	Learning Rate: 0.00158932
	LOSS [training: 0.15850767638179392 | validation: 0.10892255575623888]
	TIME [epoch: 8.3 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1866365213648616		[learning rate: 0.0015835]
	Learning Rate: 0.00158355
	LOSS [training: 0.1866365213648616 | validation: 0.07517202089399075]
	TIME [epoch: 8.28 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1389702697749438		[learning rate: 0.0015778]
	Learning Rate: 0.0015778
	LOSS [training: 0.1389702697749438 | validation: 0.07608598473181116]
	TIME [epoch: 8.28 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15308753730617047		[learning rate: 0.0015721]
	Learning Rate: 0.00157208
	LOSS [training: 0.15308753730617047 | validation: 0.11820571321560827]
	TIME [epoch: 8.29 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16744738466457076		[learning rate: 0.0015664]
	Learning Rate: 0.00156637
	LOSS [training: 0.16744738466457076 | validation: 0.08536126864701676]
	TIME [epoch: 8.3 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16514503892733456		[learning rate: 0.0015607]
	Learning Rate: 0.00156069
	LOSS [training: 0.16514503892733456 | validation: 0.2573569499173707]
	TIME [epoch: 8.28 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1626949070854943		[learning rate: 0.001555]
	Learning Rate: 0.00155502
	LOSS [training: 0.1626949070854943 | validation: 0.08768224560883205]
	TIME [epoch: 8.28 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13922010242539293		[learning rate: 0.0015494]
	Learning Rate: 0.00154938
	LOSS [training: 0.13922010242539293 | validation: 0.10997630190131746]
	TIME [epoch: 8.28 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14603818684160166		[learning rate: 0.0015438]
	Learning Rate: 0.00154376
	LOSS [training: 0.14603818684160166 | validation: 0.09608678459896311]
	TIME [epoch: 8.3 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1332085084181282		[learning rate: 0.0015382]
	Learning Rate: 0.00153815
	LOSS [training: 0.1332085084181282 | validation: 0.12035465692254133]
	TIME [epoch: 8.28 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1611848519108836		[learning rate: 0.0015326]
	Learning Rate: 0.00153257
	LOSS [training: 0.1611848519108836 | validation: 0.14041143841530168]
	TIME [epoch: 8.29 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15371581689122635		[learning rate: 0.001527]
	Learning Rate: 0.00152701
	LOSS [training: 0.15371581689122635 | validation: 0.11678107064622861]
	TIME [epoch: 8.29 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13911007393498775		[learning rate: 0.0015215]
	Learning Rate: 0.00152147
	LOSS [training: 0.13911007393498775 | validation: 0.07357317306235421]
	TIME [epoch: 8.3 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13679472773707996		[learning rate: 0.0015159]
	Learning Rate: 0.00151595
	LOSS [training: 0.13679472773707996 | validation: 0.11344628980745478]
	TIME [epoch: 8.28 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.149055789812354		[learning rate: 0.0015104]
	Learning Rate: 0.00151045
	LOSS [training: 0.149055789812354 | validation: 0.078427871255037]
	TIME [epoch: 8.29 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1312056551028811		[learning rate: 0.001505]
	Learning Rate: 0.00150496
	LOSS [training: 0.1312056551028811 | validation: 0.08509737864599348]
	TIME [epoch: 8.29 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16792772400870754		[learning rate: 0.0014995]
	Learning Rate: 0.0014995
	LOSS [training: 0.16792772400870754 | validation: 0.1186929498884539]
	TIME [epoch: 8.3 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17441357141604946		[learning rate: 0.0014941]
	Learning Rate: 0.00149406
	LOSS [training: 0.17441357141604946 | validation: 0.12055490627428408]
	TIME [epoch: 8.28 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1409154896712213		[learning rate: 0.0014886]
	Learning Rate: 0.00148864
	LOSS [training: 0.1409154896712213 | validation: 0.0601130180773828]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_624.pth
	Model improved!!!
EPOCH 625/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13943342668325043		[learning rate: 0.0014832]
	Learning Rate: 0.00148324
	LOSS [training: 0.13943342668325043 | validation: 0.2039199612400286]
	TIME [epoch: 8.29 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1916112814833156		[learning rate: 0.0014779]
	Learning Rate: 0.00147785
	LOSS [training: 0.1916112814833156 | validation: 0.11230484281532613]
	TIME [epoch: 8.3 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12589758658838654		[learning rate: 0.0014725]
	Learning Rate: 0.00147249
	LOSS [training: 0.12589758658838654 | validation: 0.10340219898264692]
	TIME [epoch: 8.29 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15046740347622806		[learning rate: 0.0014671]
	Learning Rate: 0.00146715
	LOSS [training: 0.15046740347622806 | validation: 0.15384643716587598]
	TIME [epoch: 8.28 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16140505040157668		[learning rate: 0.0014618]
	Learning Rate: 0.00146182
	LOSS [training: 0.16140505040157668 | validation: 0.1144790497748994]
	TIME [epoch: 8.28 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17039880671462862		[learning rate: 0.0014565]
	Learning Rate: 0.00145652
	LOSS [training: 0.17039880671462862 | validation: 0.23064015693508505]
	TIME [epoch: 8.31 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.162604426631105		[learning rate: 0.0014512]
	Learning Rate: 0.00145123
	LOSS [training: 0.162604426631105 | validation: 0.12127881711445626]
	TIME [epoch: 8.28 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.150454680543218		[learning rate: 0.001446]
	Learning Rate: 0.00144597
	LOSS [training: 0.150454680543218 | validation: 0.16705137988625918]
	TIME [epoch: 8.28 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18084257559231723		[learning rate: 0.0014407]
	Learning Rate: 0.00144072
	LOSS [training: 0.18084257559231723 | validation: 0.18202958725951496]
	TIME [epoch: 8.28 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17781149601603938		[learning rate: 0.0014355]
	Learning Rate: 0.00143549
	LOSS [training: 0.17781149601603938 | validation: 0.09325646336276094]
	TIME [epoch: 8.31 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15306361050704814		[learning rate: 0.0014303]
	Learning Rate: 0.00143028
	LOSS [training: 0.15306361050704814 | validation: 0.184704514680958]
	TIME [epoch: 8.28 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16173871499395578		[learning rate: 0.0014251]
	Learning Rate: 0.00142509
	LOSS [training: 0.16173871499395578 | validation: 0.08537443684311807]
	TIME [epoch: 8.28 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15533510841000164		[learning rate: 0.0014199]
	Learning Rate: 0.00141992
	LOSS [training: 0.15533510841000164 | validation: 0.12410875679251551]
	TIME [epoch: 8.28 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14857259237617337		[learning rate: 0.0014148]
	Learning Rate: 0.00141476
	LOSS [training: 0.14857259237617337 | validation: 0.13423736976527495]
	TIME [epoch: 8.3 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15518504871774316		[learning rate: 0.0014096]
	Learning Rate: 0.00140963
	LOSS [training: 0.15518504871774316 | validation: 0.09876218699088106]
	TIME [epoch: 8.29 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13843557045358826		[learning rate: 0.0014045]
	Learning Rate: 0.00140451
	LOSS [training: 0.13843557045358826 | validation: 0.08453585569258314]
	TIME [epoch: 8.28 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.134017263492966		[learning rate: 0.0013994]
	Learning Rate: 0.00139942
	LOSS [training: 0.134017263492966 | validation: 0.08790191260690328]
	TIME [epoch: 8.28 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15441776752635183		[learning rate: 0.0013943]
	Learning Rate: 0.00139434
	LOSS [training: 0.15441776752635183 | validation: 0.12878849211270182]
	TIME [epoch: 8.31 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17702819099731185		[learning rate: 0.0013893]
	Learning Rate: 0.00138928
	LOSS [training: 0.17702819099731185 | validation: 0.16628836967949617]
	TIME [epoch: 8.28 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1606022721937237		[learning rate: 0.0013842]
	Learning Rate: 0.00138424
	LOSS [training: 0.1606022721937237 | validation: 0.12331661660432593]
	TIME [epoch: 8.28 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1479180656875243		[learning rate: 0.0013792]
	Learning Rate: 0.00137921
	LOSS [training: 0.1479180656875243 | validation: 0.08530769828074261]
	TIME [epoch: 8.29 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13195092067870517		[learning rate: 0.0013742]
	Learning Rate: 0.00137421
	LOSS [training: 0.13195092067870517 | validation: 0.10425839007333836]
	TIME [epoch: 8.31 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15489256768259502		[learning rate: 0.0013692]
	Learning Rate: 0.00136922
	LOSS [training: 0.15489256768259502 | validation: 0.1402611069224761]
	TIME [epoch: 8.29 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14135048238668876		[learning rate: 0.0013643]
	Learning Rate: 0.00136425
	LOSS [training: 0.14135048238668876 | validation: 0.08487678121358017]
	TIME [epoch: 8.28 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14374777187547322		[learning rate: 0.0013593]
	Learning Rate: 0.0013593
	LOSS [training: 0.14374777187547322 | validation: 0.13341187348250233]
	TIME [epoch: 8.28 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14163909266505667		[learning rate: 0.0013544]
	Learning Rate: 0.00135437
	LOSS [training: 0.14163909266505667 | validation: 0.07894612532373908]
	TIME [epoch: 8.31 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13543276092222528		[learning rate: 0.0013495]
	Learning Rate: 0.00134945
	LOSS [training: 0.13543276092222528 | validation: 0.09376602926719697]
	TIME [epoch: 8.28 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12691977134930732		[learning rate: 0.0013446]
	Learning Rate: 0.00134456
	LOSS [training: 0.12691977134930732 | validation: 0.1018470194823944]
	TIME [epoch: 8.28 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13479122847400102		[learning rate: 0.0013397]
	Learning Rate: 0.00133968
	LOSS [training: 0.13479122847400102 | validation: 0.13005613412753325]
	TIME [epoch: 8.28 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13661836953361522		[learning rate: 0.0013348]
	Learning Rate: 0.00133481
	LOSS [training: 0.13661836953361522 | validation: 0.09969969260149675]
	TIME [epoch: 8.31 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15328090745489353		[learning rate: 0.00133]
	Learning Rate: 0.00132997
	LOSS [training: 0.15328090745489353 | validation: 0.09384078166115646]
	TIME [epoch: 8.28 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14265646072149638		[learning rate: 0.0013251]
	Learning Rate: 0.00132514
	LOSS [training: 0.14265646072149638 | validation: 0.09946779388049745]
	TIME [epoch: 8.28 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1455363678536801		[learning rate: 0.0013203]
	Learning Rate: 0.00132034
	LOSS [training: 0.1455363678536801 | validation: 0.09238773045544828]
	TIME [epoch: 8.28 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1418435405132736		[learning rate: 0.0013155]
	Learning Rate: 0.00131554
	LOSS [training: 0.1418435405132736 | validation: 0.0739223307746775]
	TIME [epoch: 8.3 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15176062329705803		[learning rate: 0.0013108]
	Learning Rate: 0.00131077
	LOSS [training: 0.15176062329705803 | validation: 0.12776453569776502]
	TIME [epoch: 8.28 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1350219683096408		[learning rate: 0.001306]
	Learning Rate: 0.00130601
	LOSS [training: 0.1350219683096408 | validation: 0.08000075869762305]
	TIME [epoch: 8.28 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14111852794871066		[learning rate: 0.0013013]
	Learning Rate: 0.00130127
	LOSS [training: 0.14111852794871066 | validation: 0.08589328461397089]
	TIME [epoch: 8.28 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12070039111316175		[learning rate: 0.0012966]
	Learning Rate: 0.00129655
	LOSS [training: 0.12070039111316175 | validation: 0.09037316068612253]
	TIME [epoch: 8.3 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11954089989167886		[learning rate: 0.0012918]
	Learning Rate: 0.00129185
	LOSS [training: 0.11954089989167886 | validation: 0.12124171799013408]
	TIME [epoch: 8.28 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13463774634897732		[learning rate: 0.0012872]
	Learning Rate: 0.00128716
	LOSS [training: 0.13463774634897732 | validation: 0.0947590450445113]
	TIME [epoch: 8.28 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1570922688939207		[learning rate: 0.0012825]
	Learning Rate: 0.00128249
	LOSS [training: 0.1570922688939207 | validation: 0.26457413721634787]
	TIME [epoch: 8.28 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13675313143201734		[learning rate: 0.0012778]
	Learning Rate: 0.00127783
	LOSS [training: 0.13675313143201734 | validation: 0.10709785169632988]
	TIME [epoch: 8.3 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12107277588126837		[learning rate: 0.0012732]
	Learning Rate: 0.00127319
	LOSS [training: 0.12107277588126837 | validation: 0.10304471284251243]
	TIME [epoch: 8.27 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1400189060949825		[learning rate: 0.0012686]
	Learning Rate: 0.00126857
	LOSS [training: 0.1400189060949825 | validation: 0.12869059154084092]
	TIME [epoch: 8.28 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14546427576515703		[learning rate: 0.001264]
	Learning Rate: 0.00126397
	LOSS [training: 0.14546427576515703 | validation: 0.11343531733278386]
	TIME [epoch: 8.28 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13829787160982182		[learning rate: 0.0012594]
	Learning Rate: 0.00125938
	LOSS [training: 0.13829787160982182 | validation: 0.07969737556209178]
	TIME [epoch: 8.31 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13759712113307757		[learning rate: 0.0012548]
	Learning Rate: 0.00125481
	LOSS [training: 0.13759712113307757 | validation: 0.09709438082391637]
	TIME [epoch: 8.28 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15090328542975684		[learning rate: 0.0012503]
	Learning Rate: 0.00125026
	LOSS [training: 0.15090328542975684 | validation: 0.07867976446417083]
	TIME [epoch: 8.28 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1672236385984351		[learning rate: 0.0012457]
	Learning Rate: 0.00124572
	LOSS [training: 0.1672236385984351 | validation: 0.22315208266428255]
	TIME [epoch: 8.28 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15051873922121758		[learning rate: 0.0012412]
	Learning Rate: 0.0012412
	LOSS [training: 0.15051873922121758 | validation: 0.06944916516204876]
	TIME [epoch: 8.31 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11967966489547496		[learning rate: 0.0012367]
	Learning Rate: 0.0012367
	LOSS [training: 0.11967966489547496 | validation: 0.06503194336386633]
	TIME [epoch: 8.28 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17408557685368733		[learning rate: 0.0012322]
	Learning Rate: 0.00123221
	LOSS [training: 0.17408557685368733 | validation: 0.10267852478794577]
	TIME [epoch: 8.28 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1301162035523804		[learning rate: 0.0012277]
	Learning Rate: 0.00122774
	LOSS [training: 0.1301162035523804 | validation: 0.0860287106288886]
	TIME [epoch: 8.28 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14485049010556963		[learning rate: 0.0012233]
	Learning Rate: 0.00122328
	LOSS [training: 0.14485049010556963 | validation: 0.0717297225670649]
	TIME [epoch: 8.31 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12022903651001333		[learning rate: 0.0012188]
	Learning Rate: 0.00121884
	LOSS [training: 0.12022903651001333 | validation: 0.08283354192560649]
	TIME [epoch: 8.29 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13342356784917536		[learning rate: 0.0012144]
	Learning Rate: 0.00121442
	LOSS [training: 0.13342356784917536 | validation: 0.08251919480142683]
	TIME [epoch: 8.28 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2743258801923484		[learning rate: 0.00121]
	Learning Rate: 0.00121001
	LOSS [training: 0.2743258801923484 | validation: 0.09279252105956681]
	TIME [epoch: 8.29 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12213443366599692		[learning rate: 0.0012056]
	Learning Rate: 0.00120562
	LOSS [training: 0.12213443366599692 | validation: 0.07704977207240085]
	TIME [epoch: 8.31 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12679662818322174		[learning rate: 0.0012012]
	Learning Rate: 0.00120125
	LOSS [training: 0.12679662818322174 | validation: 0.08439057865653299]
	TIME [epoch: 8.29 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12388169173903693		[learning rate: 0.0011969]
	Learning Rate: 0.00119689
	LOSS [training: 0.12388169173903693 | validation: 0.25487848898893306]
	TIME [epoch: 8.29 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15225652047561825		[learning rate: 0.0011925]
	Learning Rate: 0.00119254
	LOSS [training: 0.15225652047561825 | validation: 0.07387054371477622]
	TIME [epoch: 8.28 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1277827822653415		[learning rate: 0.0011882]
	Learning Rate: 0.00118821
	LOSS [training: 0.1277827822653415 | validation: 0.0736872674956752]
	TIME [epoch: 8.31 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15015904527126897		[learning rate: 0.0011839]
	Learning Rate: 0.0011839
	LOSS [training: 0.15015904527126897 | validation: 0.1609997991456221]
	TIME [epoch: 8.28 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1421537026713938		[learning rate: 0.0011796]
	Learning Rate: 0.00117961
	LOSS [training: 0.1421537026713938 | validation: 0.11717424800754007]
	TIME [epoch: 8.29 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1201131740270158		[learning rate: 0.0011753]
	Learning Rate: 0.00117532
	LOSS [training: 0.1201131740270158 | validation: 0.06484955491116637]
	TIME [epoch: 8.29 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11537354638300142		[learning rate: 0.0011711]
	Learning Rate: 0.00117106
	LOSS [training: 0.11537354638300142 | validation: 0.09427923888487155]
	TIME [epoch: 8.31 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11470189365425376		[learning rate: 0.0011668]
	Learning Rate: 0.00116681
	LOSS [training: 0.11470189365425376 | validation: 0.07884922165632444]
	TIME [epoch: 8.28 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12550076953421957		[learning rate: 0.0011626]
	Learning Rate: 0.00116258
	LOSS [training: 0.12550076953421957 | validation: 0.07691990709035161]
	TIME [epoch: 8.28 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1622638325635247		[learning rate: 0.0011584]
	Learning Rate: 0.00115836
	LOSS [training: 0.1622638325635247 | validation: 0.07459577308800919]
	TIME [epoch: 8.28 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11733622178861455		[learning rate: 0.0011542]
	Learning Rate: 0.00115415
	LOSS [training: 0.11733622178861455 | validation: 0.07228807328722099]
	TIME [epoch: 8.31 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13934751706540516		[learning rate: 0.00115]
	Learning Rate: 0.00114996
	LOSS [training: 0.13934751706540516 | validation: 0.06651477758002855]
	TIME [epoch: 8.29 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1240303903808293		[learning rate: 0.0011458]
	Learning Rate: 0.00114579
	LOSS [training: 0.1240303903808293 | validation: 0.18854497045532556]
	TIME [epoch: 8.29 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13695120465566663		[learning rate: 0.0011416]
	Learning Rate: 0.00114163
	LOSS [training: 0.13695120465566663 | validation: 0.08011198823111923]
	TIME [epoch: 8.29 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11124688881689733		[learning rate: 0.0011375]
	Learning Rate: 0.00113749
	LOSS [training: 0.11124688881689733 | validation: 0.06944136648834556]
	TIME [epoch: 8.31 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1227814120996455		[learning rate: 0.0011334]
	Learning Rate: 0.00113336
	LOSS [training: 0.1227814120996455 | validation: 0.09685796616759412]
	TIME [epoch: 8.28 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1238844196489203		[learning rate: 0.0011292]
	Learning Rate: 0.00112925
	LOSS [training: 0.1238844196489203 | validation: 0.10056150048515229]
	TIME [epoch: 8.28 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18011744945555125		[learning rate: 0.0011252]
	Learning Rate: 0.00112515
	LOSS [training: 0.18011744945555125 | validation: 0.08076958220028871]
	TIME [epoch: 8.29 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17247131546846067		[learning rate: 0.0011211]
	Learning Rate: 0.00112107
	LOSS [training: 0.17247131546846067 | validation: 0.06721452922965024]
	TIME [epoch: 8.31 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12533442335938982		[learning rate: 0.001117]
	Learning Rate: 0.001117
	LOSS [training: 0.12533442335938982 | validation: 0.08435813710644086]
	TIME [epoch: 8.29 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11548241759466453		[learning rate: 0.0011129]
	Learning Rate: 0.00111295
	LOSS [training: 0.11548241759466453 | validation: 0.0644431812462303]
	TIME [epoch: 8.29 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14341467590434917		[learning rate: 0.0011089]
	Learning Rate: 0.00110891
	LOSS [training: 0.14341467590434917 | validation: 0.05193347618907832]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_705.pth
	Model improved!!!
EPOCH 706/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11451743359747975		[learning rate: 0.0011049]
	Learning Rate: 0.00110488
	LOSS [training: 0.11451743359747975 | validation: 0.08761867351039254]
	TIME [epoch: 8.32 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11579129541515368		[learning rate: 0.0011009]
	Learning Rate: 0.00110087
	LOSS [training: 0.11579129541515368 | validation: 0.08938137659591938]
	TIME [epoch: 8.28 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12304981555366785		[learning rate: 0.0010969]
	Learning Rate: 0.00109688
	LOSS [training: 0.12304981555366785 | validation: 0.08371796406785686]
	TIME [epoch: 8.29 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12085787055770951		[learning rate: 0.0010929]
	Learning Rate: 0.0010929
	LOSS [training: 0.12085787055770951 | validation: 0.06614557191538166]
	TIME [epoch: 8.28 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13075712596593075		[learning rate: 0.0010889]
	Learning Rate: 0.00108893
	LOSS [training: 0.13075712596593075 | validation: 0.0732887497885887]
	TIME [epoch: 8.31 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1216713939190657		[learning rate: 0.001085]
	Learning Rate: 0.00108498
	LOSS [training: 0.1216713939190657 | validation: 0.07657098438188761]
	TIME [epoch: 8.29 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11346426087158072		[learning rate: 0.001081]
	Learning Rate: 0.00108104
	LOSS [training: 0.11346426087158072 | validation: 0.11381972422779468]
	TIME [epoch: 8.29 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12109828527358853		[learning rate: 0.0010771]
	Learning Rate: 0.00107712
	LOSS [training: 0.12109828527358853 | validation: 0.07452737442309212]
	TIME [epoch: 8.28 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11432947316392221		[learning rate: 0.0010732]
	Learning Rate: 0.00107321
	LOSS [training: 0.11432947316392221 | validation: 0.06467924745010181]
	TIME [epoch: 8.3 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13136613684885093		[learning rate: 0.0010693]
	Learning Rate: 0.00106931
	LOSS [training: 0.13136613684885093 | validation: 0.0759956043564384]
	TIME [epoch: 8.28 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1234517907369624		[learning rate: 0.0010654]
	Learning Rate: 0.00106543
	LOSS [training: 0.1234517907369624 | validation: 0.04484838529943619]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_716.pth
	Model improved!!!
EPOCH 717/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10745514828123814		[learning rate: 0.0010616]
	Learning Rate: 0.00106157
	LOSS [training: 0.10745514828123814 | validation: 0.04826644545848388]
	TIME [epoch: 8.29 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11410041415031454		[learning rate: 0.0010577]
	Learning Rate: 0.00105771
	LOSS [training: 0.11410041415031454 | validation: 0.055337386183682204]
	TIME [epoch: 8.31 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13027037195889182		[learning rate: 0.0010539]
	Learning Rate: 0.00105388
	LOSS [training: 0.13027037195889182 | validation: 0.07665510802045028]
	TIME [epoch: 8.28 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12051191352404367		[learning rate: 0.0010501]
	Learning Rate: 0.00105005
	LOSS [training: 0.12051191352404367 | validation: 0.07817132552463654]
	TIME [epoch: 8.29 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13185252414906928		[learning rate: 0.0010462]
	Learning Rate: 0.00104624
	LOSS [training: 0.13185252414906928 | validation: 0.09650762427640014]
	TIME [epoch: 8.29 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11165299866180237		[learning rate: 0.0010424]
	Learning Rate: 0.00104244
	LOSS [training: 0.11165299866180237 | validation: 0.07435462636015058]
	TIME [epoch: 8.3 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13007206358060192		[learning rate: 0.0010387]
	Learning Rate: 0.00103866
	LOSS [training: 0.13007206358060192 | validation: 0.06482751206883064]
	TIME [epoch: 8.3 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11691112949134012		[learning rate: 0.0010349]
	Learning Rate: 0.00103489
	LOSS [training: 0.11691112949134012 | validation: 0.08063429884939383]
	TIME [epoch: 8.29 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13309480450989453		[learning rate: 0.0010311]
	Learning Rate: 0.00103114
	LOSS [training: 0.13309480450989453 | validation: 0.10185213353860127]
	TIME [epoch: 8.29 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15493954823649725		[learning rate: 0.0010274]
	Learning Rate: 0.00102739
	LOSS [training: 0.15493954823649725 | validation: 0.09565469516485336]
	TIME [epoch: 8.31 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13772292519860635		[learning rate: 0.0010237]
	Learning Rate: 0.00102367
	LOSS [training: 0.13772292519860635 | validation: 0.09447767459538077]
	TIME [epoch: 8.3 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12987732640297384		[learning rate: 0.00102]
	Learning Rate: 0.00101995
	LOSS [training: 0.12987732640297384 | validation: 0.08079061150646603]
	TIME [epoch: 8.29 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12408580137201655		[learning rate: 0.0010162]
	Learning Rate: 0.00101625
	LOSS [training: 0.12408580137201655 | validation: 0.060530401673903375]
	TIME [epoch: 8.29 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11179837473900837		[learning rate: 0.0010126]
	Learning Rate: 0.00101256
	LOSS [training: 0.11179837473900837 | validation: 0.0819581437722688]
	TIME [epoch: 8.31 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11436838741902719		[learning rate: 0.0010089]
	Learning Rate: 0.00100889
	LOSS [training: 0.11436838741902719 | validation: 0.09005262961291366]
	TIME [epoch: 8.3 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12114358807723546		[learning rate: 0.0010052]
	Learning Rate: 0.00100522
	LOSS [training: 0.12114358807723546 | validation: 0.08482415213554681]
	TIME [epoch: 8.29 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12171693187506258		[learning rate: 0.0010016]
	Learning Rate: 0.00100158
	LOSS [training: 0.12171693187506258 | validation: 0.08063031379719995]
	TIME [epoch: 8.29 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12675709920437067		[learning rate: 0.00099794]
	Learning Rate: 0.000997942
	LOSS [training: 0.12675709920437067 | validation: 0.0775906494158166]
	TIME [epoch: 8.31 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11530297751506166		[learning rate: 0.00099432]
	Learning Rate: 0.00099432
	LOSS [training: 0.11530297751506166 | validation: 0.07106944364506723]
	TIME [epoch: 8.3 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11799113031684638		[learning rate: 0.00099071]
	Learning Rate: 0.000990712
	LOSS [training: 0.11799113031684638 | validation: 0.06818975687205132]
	TIME [epoch: 8.29 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14443753021279157		[learning rate: 0.00098712]
	Learning Rate: 0.000987116
	LOSS [training: 0.14443753021279157 | validation: 0.05519953042783398]
	TIME [epoch: 8.28 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12268634211533669		[learning rate: 0.00098353]
	Learning Rate: 0.000983534
	LOSS [training: 0.12268634211533669 | validation: 0.06410002915421548]
	TIME [epoch: 8.3 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1358549325618455		[learning rate: 0.00097996]
	Learning Rate: 0.000979965
	LOSS [training: 0.1358549325618455 | validation: 0.0733227676961678]
	TIME [epoch: 8.3 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11813964453246059		[learning rate: 0.00097641]
	Learning Rate: 0.000976409
	LOSS [training: 0.11813964453246059 | validation: 0.09464562735763275]
	TIME [epoch: 8.29 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1308412341294497		[learning rate: 0.00097287]
	Learning Rate: 0.000972865
	LOSS [training: 0.1308412341294497 | validation: 0.0640743057616727]
	TIME [epoch: 8.29 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.153564632895213		[learning rate: 0.00096933]
	Learning Rate: 0.000969335
	LOSS [training: 0.153564632895213 | validation: 0.0564714614292026]
	TIME [epoch: 8.31 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12129116272065152		[learning rate: 0.00096582]
	Learning Rate: 0.000965817
	LOSS [training: 0.12129116272065152 | validation: 0.08381374876962318]
	TIME [epoch: 8.29 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10450953897064634		[learning rate: 0.00096231]
	Learning Rate: 0.000962312
	LOSS [training: 0.10450953897064634 | validation: 0.07775037718980694]
	TIME [epoch: 8.29 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11931465703568434		[learning rate: 0.00095882]
	Learning Rate: 0.00095882
	LOSS [training: 0.11931465703568434 | validation: 0.06262740778109495]
	TIME [epoch: 8.29 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10700383510554708		[learning rate: 0.00095534]
	Learning Rate: 0.00095534
	LOSS [training: 0.10700383510554708 | validation: 0.10297836387464618]
	TIME [epoch: 8.3 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13301208950834723		[learning rate: 0.00095187]
	Learning Rate: 0.000951873
	LOSS [training: 0.13301208950834723 | validation: 0.0847971667773377]
	TIME [epoch: 8.3 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12066218538924052		[learning rate: 0.00094842]
	Learning Rate: 0.000948419
	LOSS [training: 0.12066218538924052 | validation: 0.08963296038751549]
	TIME [epoch: 8.29 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13012741291386398		[learning rate: 0.00094498]
	Learning Rate: 0.000944977
	LOSS [training: 0.13012741291386398 | validation: 0.06482661120072941]
	TIME [epoch: 8.29 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14443419779781186		[learning rate: 0.00094155]
	Learning Rate: 0.000941547
	LOSS [training: 0.14443419779781186 | validation: 0.11828935652021777]
	TIME [epoch: 8.31 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12248514715880976		[learning rate: 0.00093813]
	Learning Rate: 0.00093813
	LOSS [training: 0.12248514715880976 | validation: 0.06990261983906182]
	TIME [epoch: 8.31 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13491687136554206		[learning rate: 0.00093473]
	Learning Rate: 0.000934726
	LOSS [training: 0.13491687136554206 | validation: 0.08389170145611141]
	TIME [epoch: 8.29 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1235393402507406		[learning rate: 0.00093133]
	Learning Rate: 0.000931334
	LOSS [training: 0.1235393402507406 | validation: 0.06986230429604492]
	TIME [epoch: 8.29 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11960310613161888		[learning rate: 0.00092795]
	Learning Rate: 0.000927954
	LOSS [training: 0.11960310613161888 | validation: 0.0891694326765504]
	TIME [epoch: 8.31 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11397511927716877		[learning rate: 0.00092459]
	Learning Rate: 0.000924586
	LOSS [training: 0.11397511927716877 | validation: 0.10137392042264978]
	TIME [epoch: 8.3 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12173892871203684		[learning rate: 0.00092123]
	Learning Rate: 0.000921231
	LOSS [training: 0.12173892871203684 | validation: 0.1000572431636462]
	TIME [epoch: 8.29 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1439692899568547		[learning rate: 0.00091789]
	Learning Rate: 0.000917888
	LOSS [training: 0.1439692899568547 | validation: 0.11249427787573829]
	TIME [epoch: 8.29 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12074531031857813		[learning rate: 0.00091456]
	Learning Rate: 0.000914556
	LOSS [training: 0.12074531031857813 | validation: 0.07070167302969234]
	TIME [epoch: 8.31 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1107304464338534		[learning rate: 0.00091124]
	Learning Rate: 0.000911237
	LOSS [training: 0.1107304464338534 | validation: 0.05051830308286705]
	TIME [epoch: 8.3 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10771613333838		[learning rate: 0.00090793]
	Learning Rate: 0.000907931
	LOSS [training: 0.10771613333838 | validation: 0.07338267376503183]
	TIME [epoch: 8.29 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11827793335195477		[learning rate: 0.00090464]
	Learning Rate: 0.000904636
	LOSS [training: 0.11827793335195477 | validation: 0.06530073544590013]
	TIME [epoch: 8.29 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10670903302776771		[learning rate: 0.00090135]
	Learning Rate: 0.000901353
	LOSS [training: 0.10670903302776771 | validation: 0.09230485392490058]
	TIME [epoch: 8.31 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1516063763590821		[learning rate: 0.00089808]
	Learning Rate: 0.000898082
	LOSS [training: 0.1516063763590821 | validation: 0.06867238605825032]
	TIME [epoch: 8.3 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12491730297461845		[learning rate: 0.00089482]
	Learning Rate: 0.000894822
	LOSS [training: 0.12491730297461845 | validation: 0.1143110506145302]
	TIME [epoch: 8.29 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11142979677679805		[learning rate: 0.00089157]
	Learning Rate: 0.000891575
	LOSS [training: 0.11142979677679805 | validation: 0.07235164795196583]
	TIME [epoch: 8.29 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10672649515742798		[learning rate: 0.00088834]
	Learning Rate: 0.000888339
	LOSS [training: 0.10672649515742798 | validation: 0.0968163647143199]
	TIME [epoch: 8.3 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11920525848080807		[learning rate: 0.00088512]
	Learning Rate: 0.000885116
	LOSS [training: 0.11920525848080807 | validation: 0.05781013050570587]
	TIME [epoch: 8.31 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1035631803350497		[learning rate: 0.0008819]
	Learning Rate: 0.000881903
	LOSS [training: 0.1035631803350497 | validation: 0.06774193318332056]
	TIME [epoch: 8.29 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13202071096093368		[learning rate: 0.0008787]
	Learning Rate: 0.000878703
	LOSS [training: 0.13202071096093368 | validation: 0.09923674498701662]
	TIME [epoch: 8.3 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10603655515612709		[learning rate: 0.00087551]
	Learning Rate: 0.000875514
	LOSS [training: 0.10603655515612709 | validation: 0.0730809507746657]
	TIME [epoch: 8.3 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11037229819904044		[learning rate: 0.00087234]
	Learning Rate: 0.000872337
	LOSS [training: 0.11037229819904044 | validation: 0.0672224648066765]
	TIME [epoch: 8.3 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1095066758279782		[learning rate: 0.00086917]
	Learning Rate: 0.000869171
	LOSS [training: 0.1095066758279782 | validation: 0.05952624981304778]
	TIME [epoch: 8.28 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11763867057263963		[learning rate: 0.00086602]
	Learning Rate: 0.000866017
	LOSS [training: 0.11763867057263963 | validation: 0.09128409504176987]
	TIME [epoch: 8.29 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09455594117902359		[learning rate: 0.00086287]
	Learning Rate: 0.000862874
	LOSS [training: 0.09455594117902359 | validation: 0.14194981875206056]
	TIME [epoch: 8.3 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12642865784214147		[learning rate: 0.00085974]
	Learning Rate: 0.000859743
	LOSS [training: 0.12642865784214147 | validation: 0.08120901799691968]
	TIME [epoch: 8.3 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13125303474182798		[learning rate: 0.00085662]
	Learning Rate: 0.000856623
	LOSS [training: 0.13125303474182798 | validation: 0.11509483620867857]
	TIME [epoch: 8.29 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1137461845141031		[learning rate: 0.00085351]
	Learning Rate: 0.000853514
	LOSS [training: 0.1137461845141031 | validation: 0.06199331312714257]
	TIME [epoch: 8.28 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10866307459296194		[learning rate: 0.00085042]
	Learning Rate: 0.000850416
	LOSS [training: 0.10866307459296194 | validation: 0.06061321709300037]
	TIME [epoch: 8.3 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11859585239486645		[learning rate: 0.00084733]
	Learning Rate: 0.00084733
	LOSS [training: 0.11859585239486645 | validation: 0.07510675238668471]
	TIME [epoch: 8.3 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11511968152278204		[learning rate: 0.00084426]
	Learning Rate: 0.000844255
	LOSS [training: 0.11511968152278204 | validation: 0.08967738645753073]
	TIME [epoch: 8.29 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11283130860061345		[learning rate: 0.00084119]
	Learning Rate: 0.000841191
	LOSS [training: 0.11283130860061345 | validation: 0.071084687268325]
	TIME [epoch: 8.29 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11338880335514734		[learning rate: 0.00083814]
	Learning Rate: 0.000838139
	LOSS [training: 0.11338880335514734 | validation: 0.07182991798053764]
	TIME [epoch: 8.31 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1111054015999364		[learning rate: 0.0008351]
	Learning Rate: 0.000835097
	LOSS [training: 0.1111054015999364 | validation: 0.0569161721777043]
	TIME [epoch: 8.29 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15766942886720017		[learning rate: 0.00083207]
	Learning Rate: 0.000832066
	LOSS [training: 0.15766942886720017 | validation: 0.06667141854256088]
	TIME [epoch: 8.29 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10434636679789569		[learning rate: 0.00082905]
	Learning Rate: 0.000829046
	LOSS [training: 0.10434636679789569 | validation: 0.07029790287094159]
	TIME [epoch: 8.29 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11443661911918104		[learning rate: 0.00082604]
	Learning Rate: 0.000826038
	LOSS [training: 0.11443661911918104 | validation: 0.07288816915549051]
	TIME [epoch: 8.3 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11130801679833731		[learning rate: 0.00082304]
	Learning Rate: 0.00082304
	LOSS [training: 0.11130801679833731 | validation: 0.0645669656770378]
	TIME [epoch: 8.3 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10365175959424804		[learning rate: 0.00082005]
	Learning Rate: 0.000820053
	LOSS [training: 0.10365175959424804 | validation: 0.05386414534642804]
	TIME [epoch: 8.29 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12128599426460182		[learning rate: 0.00081708]
	Learning Rate: 0.000817077
	LOSS [training: 0.12128599426460182 | validation: 0.06065134315408488]
	TIME [epoch: 8.29 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10932412407295003		[learning rate: 0.00081411]
	Learning Rate: 0.000814112
	LOSS [training: 0.10932412407295003 | validation: 0.08068093082539521]
	TIME [epoch: 8.3 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10011807888693416		[learning rate: 0.00081116]
	Learning Rate: 0.000811158
	LOSS [training: 0.10011807888693416 | validation: 0.06085716907423167]
	TIME [epoch: 8.29 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10299073184726974		[learning rate: 0.00080821]
	Learning Rate: 0.000808214
	LOSS [training: 0.10299073184726974 | validation: 0.06686052506057982]
	TIME [epoch: 8.28 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11512948164792931		[learning rate: 0.00080528]
	Learning Rate: 0.000805281
	LOSS [training: 0.11512948164792931 | validation: 0.09381404308574243]
	TIME [epoch: 8.28 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1224318187701298		[learning rate: 0.00080236]
	Learning Rate: 0.000802358
	LOSS [training: 0.1224318187701298 | validation: 0.07378315662206378]
	TIME [epoch: 8.3 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09290318104459828		[learning rate: 0.00079945]
	Learning Rate: 0.000799447
	LOSS [training: 0.09290318104459828 | validation: 0.1433824201041516]
	TIME [epoch: 8.3 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12287842283000812		[learning rate: 0.00079655]
	Learning Rate: 0.000796545
	LOSS [training: 0.12287842283000812 | validation: 0.052074992869883166]
	TIME [epoch: 8.29 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10995529075512885		[learning rate: 0.00079365]
	Learning Rate: 0.000793655
	LOSS [training: 0.10995529075512885 | validation: 0.07112391463117584]
	TIME [epoch: 8.29 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10022826296811463		[learning rate: 0.00079077]
	Learning Rate: 0.000790775
	LOSS [training: 0.10022826296811463 | validation: 0.06089682686782288]
	TIME [epoch: 8.3 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09960186816801092		[learning rate: 0.0007879]
	Learning Rate: 0.000787905
	LOSS [training: 0.09960186816801092 | validation: 0.08324135019978979]
	TIME [epoch: 8.29 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10018271247157676		[learning rate: 0.00078505]
	Learning Rate: 0.000785045
	LOSS [training: 0.10018271247157676 | validation: 0.061631247553581577]
	TIME [epoch: 8.28 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10186788641120217		[learning rate: 0.0007822]
	Learning Rate: 0.000782196
	LOSS [training: 0.10186788641120217 | validation: 0.061973126683156084]
	TIME [epoch: 8.28 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11221011054315007		[learning rate: 0.00077936]
	Learning Rate: 0.000779358
	LOSS [training: 0.11221011054315007 | validation: 0.07599092291688547]
	TIME [epoch: 8.3 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09891416566817765		[learning rate: 0.00077653]
	Learning Rate: 0.000776529
	LOSS [training: 0.09891416566817765 | validation: 0.07407590322544766]
	TIME [epoch: 8.3 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1104536065850753		[learning rate: 0.00077371]
	Learning Rate: 0.000773711
	LOSS [training: 0.1104536065850753 | validation: 0.05953216180791365]
	TIME [epoch: 8.28 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10758288422490543		[learning rate: 0.0007709]
	Learning Rate: 0.000770903
	LOSS [training: 0.10758288422490543 | validation: 0.09394962777198404]
	TIME [epoch: 8.28 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11109986262729796		[learning rate: 0.00076811]
	Learning Rate: 0.000768106
	LOSS [training: 0.11109986262729796 | validation: 0.11022742280784203]
	TIME [epoch: 8.3 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11918163085697538		[learning rate: 0.00076532]
	Learning Rate: 0.000765318
	LOSS [training: 0.11918163085697538 | validation: 0.1545880944039585]
	TIME [epoch: 8.3 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11905693991455413		[learning rate: 0.00076254]
	Learning Rate: 0.000762541
	LOSS [training: 0.11905693991455413 | validation: 0.088553357206689]
	TIME [epoch: 8.28 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09609946872369812		[learning rate: 0.00075977]
	Learning Rate: 0.000759774
	LOSS [training: 0.09609946872369812 | validation: 0.037925654667529424]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_809.pth
	Model improved!!!
EPOCH 810/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10417796302777135		[learning rate: 0.00075702]
	Learning Rate: 0.000757016
	LOSS [training: 0.10417796302777135 | validation: 0.08026460180866708]
	TIME [epoch: 8.29 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10679124937776603		[learning rate: 0.00075427]
	Learning Rate: 0.000754269
	LOSS [training: 0.10679124937776603 | validation: 0.07071458074855957]
	TIME [epoch: 8.29 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10457138466184435		[learning rate: 0.00075153]
	Learning Rate: 0.000751532
	LOSS [training: 0.10457138466184435 | validation: 0.06469076006861957]
	TIME [epoch: 8.28 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10962462602092422		[learning rate: 0.0007488]
	Learning Rate: 0.000748804
	LOSS [training: 0.10962462602092422 | validation: 0.05365600612689384]
	TIME [epoch: 8.28 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10678763032251386		[learning rate: 0.00074609]
	Learning Rate: 0.000746087
	LOSS [training: 0.10678763032251386 | validation: 0.07346247400075145]
	TIME [epoch: 8.28 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10406735785579913		[learning rate: 0.00074338]
	Learning Rate: 0.000743379
	LOSS [training: 0.10406735785579913 | validation: 0.0650751967000908]
	TIME [epoch: 8.3 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10516951534038659		[learning rate: 0.00074068]
	Learning Rate: 0.000740682
	LOSS [training: 0.10516951534038659 | validation: 0.06524892364165459]
	TIME [epoch: 8.28 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1249072392579104		[learning rate: 0.00073799]
	Learning Rate: 0.000737994
	LOSS [training: 0.1249072392579104 | validation: 0.05584525571956154]
	TIME [epoch: 8.28 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10095835618183994		[learning rate: 0.00073532]
	Learning Rate: 0.000735315
	LOSS [training: 0.10095835618183994 | validation: 0.08342444294400769]
	TIME [epoch: 8.28 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09851801781247019		[learning rate: 0.00073265]
	Learning Rate: 0.000732647
	LOSS [training: 0.09851801781247019 | validation: 0.07571949721037813]
	TIME [epoch: 8.3 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11837947569621059		[learning rate: 0.00072999]
	Learning Rate: 0.000729988
	LOSS [training: 0.11837947569621059 | validation: 0.0641384155626787]
	TIME [epoch: 8.29 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10323591212809195		[learning rate: 0.00072734]
	Learning Rate: 0.000727339
	LOSS [training: 0.10323591212809195 | validation: 0.0650733549769673]
	TIME [epoch: 8.27 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10882130546376845		[learning rate: 0.0007247]
	Learning Rate: 0.000724699
	LOSS [training: 0.10882130546376845 | validation: 0.06561000540895169]
	TIME [epoch: 8.28 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09250897455815402		[learning rate: 0.00072207]
	Learning Rate: 0.000722069
	LOSS [training: 0.09250897455815402 | validation: 0.08773937773283991]
	TIME [epoch: 8.3 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13406567667016842		[learning rate: 0.00071945]
	Learning Rate: 0.000719449
	LOSS [training: 0.13406567667016842 | validation: 0.034868787210323826]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_824.pth
	Model improved!!!
EPOCH 825/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10304065266953495		[learning rate: 0.00071684]
	Learning Rate: 0.000716838
	LOSS [training: 0.10304065266953495 | validation: 0.07076834824953214]
	TIME [epoch: 8.3 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11499037413707838		[learning rate: 0.00071424]
	Learning Rate: 0.000714237
	LOSS [training: 0.11499037413707838 | validation: 0.09934341383813916]
	TIME [epoch: 8.3 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1122088628796056		[learning rate: 0.00071164]
	Learning Rate: 0.000711645
	LOSS [training: 0.1122088628796056 | validation: 0.05704309737612363]
	TIME [epoch: 8.31 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10185762875962663		[learning rate: 0.00070906]
	Learning Rate: 0.000709062
	LOSS [training: 0.10185762875962663 | validation: 0.13896573731353395]
	TIME [epoch: 8.29 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11857011981153662		[learning rate: 0.00070649]
	Learning Rate: 0.000706489
	LOSS [training: 0.11857011981153662 | validation: 0.048535360027508134]
	TIME [epoch: 8.29 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1070230660084589		[learning rate: 0.00070392]
	Learning Rate: 0.000703925
	LOSS [training: 0.1070230660084589 | validation: 0.058970784144555345]
	TIME [epoch: 8.29 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10042830690974355		[learning rate: 0.00070137]
	Learning Rate: 0.00070137
	LOSS [training: 0.10042830690974355 | validation: 0.06661235020289834]
	TIME [epoch: 8.32 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10935173994575649		[learning rate: 0.00069883]
	Learning Rate: 0.000698825
	LOSS [training: 0.10935173994575649 | validation: 0.04737147511891795]
	TIME [epoch: 8.29 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12347836638668312		[learning rate: 0.00069629]
	Learning Rate: 0.000696289
	LOSS [training: 0.12347836638668312 | validation: 0.13033537919303162]
	TIME [epoch: 8.29 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11416679302520541		[learning rate: 0.00069376]
	Learning Rate: 0.000693762
	LOSS [training: 0.11416679302520541 | validation: 0.06784059256847634]
	TIME [epoch: 8.29 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0995953811772212		[learning rate: 0.00069124]
	Learning Rate: 0.000691244
	LOSS [training: 0.0995953811772212 | validation: 0.06658768799577405]
	TIME [epoch: 8.32 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09676104590931225		[learning rate: 0.00068874]
	Learning Rate: 0.000688736
	LOSS [training: 0.09676104590931225 | validation: 0.07213190157563384]
	TIME [epoch: 8.29 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10549893518135915		[learning rate: 0.00068624]
	Learning Rate: 0.000686236
	LOSS [training: 0.10549893518135915 | validation: 0.05730820198170314]
	TIME [epoch: 8.29 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10814652156023064		[learning rate: 0.00068375]
	Learning Rate: 0.000683746
	LOSS [training: 0.10814652156023064 | validation: 0.08972571001320956]
	TIME [epoch: 8.29 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11224458017523613		[learning rate: 0.00068126]
	Learning Rate: 0.000681265
	LOSS [training: 0.11224458017523613 | validation: 0.06782352712976727]
	TIME [epoch: 8.33 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09812926453122298		[learning rate: 0.00067879]
	Learning Rate: 0.000678792
	LOSS [training: 0.09812926453122298 | validation: 0.05889756897047632]
	TIME [epoch: 8.29 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11444860287798331		[learning rate: 0.00067633]
	Learning Rate: 0.000676329
	LOSS [training: 0.11444860287798331 | validation: 0.06434329600347889]
	TIME [epoch: 8.3 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10881871442830113		[learning rate: 0.00067387]
	Learning Rate: 0.000673874
	LOSS [training: 0.10881871442830113 | validation: 0.06788381716036299]
	TIME [epoch: 8.29 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09907968343735922		[learning rate: 0.00067143]
	Learning Rate: 0.000671429
	LOSS [training: 0.09907968343735922 | validation: 0.07821719326725302]
	TIME [epoch: 8.32 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10009734301514797		[learning rate: 0.00066899]
	Learning Rate: 0.000668992
	LOSS [training: 0.10009734301514797 | validation: 0.06163941558868731]
	TIME [epoch: 8.29 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1065108153371143		[learning rate: 0.00066656]
	Learning Rate: 0.000666564
	LOSS [training: 0.1065108153371143 | validation: 0.05967821344830115]
	TIME [epoch: 8.3 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10027179499893946		[learning rate: 0.00066415]
	Learning Rate: 0.000664145
	LOSS [training: 0.10027179499893946 | validation: 0.0734910329060119]
	TIME [epoch: 8.3 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10293673258020478		[learning rate: 0.00066174]
	Learning Rate: 0.000661735
	LOSS [training: 0.10293673258020478 | validation: 0.06220991010515024]
	TIME [epoch: 8.32 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09842115343698044		[learning rate: 0.00065933]
	Learning Rate: 0.000659334
	LOSS [training: 0.09842115343698044 | validation: 0.07923110165806707]
	TIME [epoch: 8.29 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10344038013426862		[learning rate: 0.00065694]
	Learning Rate: 0.000656941
	LOSS [training: 0.10344038013426862 | validation: 0.07070317073110974]
	TIME [epoch: 8.29 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09598879977673316		[learning rate: 0.00065456]
	Learning Rate: 0.000654557
	LOSS [training: 0.09598879977673316 | validation: 0.05752298097929736]
	TIME [epoch: 8.29 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09939083927958994		[learning rate: 0.00065218]
	Learning Rate: 0.000652182
	LOSS [training: 0.09939083927958994 | validation: 0.058891551035002945]
	TIME [epoch: 8.31 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11439235111610888		[learning rate: 0.00064981]
	Learning Rate: 0.000649815
	LOSS [training: 0.11439235111610888 | validation: 0.048880535010147635]
	TIME [epoch: 8.29 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09221298629648358		[learning rate: 0.00064746]
	Learning Rate: 0.000647456
	LOSS [training: 0.09221298629648358 | validation: 0.06875365473133892]
	TIME [epoch: 8.29 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10266944026620994		[learning rate: 0.00064511]
	Learning Rate: 0.000645107
	LOSS [training: 0.10266944026620994 | validation: 0.05115020623884845]
	TIME [epoch: 8.29 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10587796109123374		[learning rate: 0.00064277]
	Learning Rate: 0.000642766
	LOSS [training: 0.10587796109123374 | validation: 0.05446763497146023]
	TIME [epoch: 8.32 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10200788029854566		[learning rate: 0.00064043]
	Learning Rate: 0.000640433
	LOSS [training: 0.10200788029854566 | validation: 0.07551954612184825]
	TIME [epoch: 8.29 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10166184384081145		[learning rate: 0.00063811]
	Learning Rate: 0.000638109
	LOSS [training: 0.10166184384081145 | validation: 0.049889402028585585]
	TIME [epoch: 8.29 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1448453185376692		[learning rate: 0.00063579]
	Learning Rate: 0.000635793
	LOSS [training: 0.1448453185376692 | validation: 0.15159977223499055]
	TIME [epoch: 8.29 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1234126896061694		[learning rate: 0.00063349]
	Learning Rate: 0.000633486
	LOSS [training: 0.1234126896061694 | validation: 0.07896459488611648]
	TIME [epoch: 8.32 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0886888853141361		[learning rate: 0.00063119]
	Learning Rate: 0.000631187
	LOSS [training: 0.0886888853141361 | validation: 0.0788562266441408]
	TIME [epoch: 8.28 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11190659805852672		[learning rate: 0.0006289]
	Learning Rate: 0.000628896
	LOSS [training: 0.11190659805852672 | validation: 0.05882750774049965]
	TIME [epoch: 8.29 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11838862345343286		[learning rate: 0.00062661]
	Learning Rate: 0.000626614
	LOSS [training: 0.11838862345343286 | validation: 0.07529950015994571]
	TIME [epoch: 8.29 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09496713470107342		[learning rate: 0.00062434]
	Learning Rate: 0.00062434
	LOSS [training: 0.09496713470107342 | validation: 0.0638060167044073]
	TIME [epoch: 8.32 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11232032890958181		[learning rate: 0.00062207]
	Learning Rate: 0.000622074
	LOSS [training: 0.11232032890958181 | validation: 0.05457404302743344]
	TIME [epoch: 8.29 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10030992755201715		[learning rate: 0.00061982]
	Learning Rate: 0.000619816
	LOSS [training: 0.10030992755201715 | validation: 0.07484921392566997]
	TIME [epoch: 8.29 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09919056129986195		[learning rate: 0.00061757]
	Learning Rate: 0.000617567
	LOSS [training: 0.09919056129986195 | validation: 0.055224841227875986]
	TIME [epoch: 8.29 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08817670922720917		[learning rate: 0.00061533]
	Learning Rate: 0.000615326
	LOSS [training: 0.08817670922720917 | validation: 0.05839101405355459]
	TIME [epoch: 8.31 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09827130630158132		[learning rate: 0.00061309]
	Learning Rate: 0.000613093
	LOSS [training: 0.09827130630158132 | validation: 0.05419980038717903]
	TIME [epoch: 8.29 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10617432770626097		[learning rate: 0.00061087]
	Learning Rate: 0.000610868
	LOSS [training: 0.10617432770626097 | validation: 0.08965967642179698]
	TIME [epoch: 8.28 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11227686202529352		[learning rate: 0.00060865]
	Learning Rate: 0.000608651
	LOSS [training: 0.11227686202529352 | validation: 0.06666834808496043]
	TIME [epoch: 8.29 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10145346090938592		[learning rate: 0.00060644]
	Learning Rate: 0.000606442
	LOSS [training: 0.10145346090938592 | validation: 0.08225279838336409]
	TIME [epoch: 8.32 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09459257049787302		[learning rate: 0.00060424]
	Learning Rate: 0.000604242
	LOSS [training: 0.09459257049787302 | validation: 0.10028028016272424]
	TIME [epoch: 8.29 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11277502001701234		[learning rate: 0.00060205]
	Learning Rate: 0.000602049
	LOSS [training: 0.11277502001701234 | validation: 0.05197143216226921]
	TIME [epoch: 8.29 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09760652352670846		[learning rate: 0.00059986]
	Learning Rate: 0.000599864
	LOSS [training: 0.09760652352670846 | validation: 0.07440082618757744]
	TIME [epoch: 8.29 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10044046536591797		[learning rate: 0.00059769]
	Learning Rate: 0.000597687
	LOSS [training: 0.10044046536591797 | validation: 0.06123806333734206]
	TIME [epoch: 8.32 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0889259068590637		[learning rate: 0.00059552]
	Learning Rate: 0.000595518
	LOSS [training: 0.0889259068590637 | validation: 0.04528927755911696]
	TIME [epoch: 8.29 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08656277782851354		[learning rate: 0.00059336]
	Learning Rate: 0.000593357
	LOSS [training: 0.08656277782851354 | validation: 0.07521808967829055]
	TIME [epoch: 8.29 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10761889430394381		[learning rate: 0.0005912]
	Learning Rate: 0.000591203
	LOSS [training: 0.10761889430394381 | validation: 0.19349865733504362]
	TIME [epoch: 8.29 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13302587533119653		[learning rate: 0.00058906]
	Learning Rate: 0.000589058
	LOSS [training: 0.13302587533119653 | validation: 0.13515448043770234]
	TIME [epoch: 8.32 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12761324629832368		[learning rate: 0.00058692]
	Learning Rate: 0.00058692
	LOSS [training: 0.12761324629832368 | validation: 0.0692674387179103]
	TIME [epoch: 8.29 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08906223333378384		[learning rate: 0.00058479]
	Learning Rate: 0.00058479
	LOSS [training: 0.08906223333378384 | validation: 0.05155403304000239]
	TIME [epoch: 8.29 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1081497855544152		[learning rate: 0.00058267]
	Learning Rate: 0.000582668
	LOSS [training: 0.1081497855544152 | validation: 0.10960537115023955]
	TIME [epoch: 8.29 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10269799892371334		[learning rate: 0.00058055]
	Learning Rate: 0.000580553
	LOSS [training: 0.10269799892371334 | validation: 0.154809478932959]
	TIME [epoch: 8.32 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13841090802520792		[learning rate: 0.00057845]
	Learning Rate: 0.000578446
	LOSS [training: 0.13841090802520792 | validation: 0.0743586701035426]
	TIME [epoch: 8.29 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10897929998834081		[learning rate: 0.00057635]
	Learning Rate: 0.000576347
	LOSS [training: 0.10897929998834081 | validation: 0.04772662017707134]
	TIME [epoch: 8.29 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10038813076598288		[learning rate: 0.00057426]
	Learning Rate: 0.000574256
	LOSS [training: 0.10038813076598288 | validation: 0.06561824069720701]
	TIME [epoch: 8.29 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09844898803773759		[learning rate: 0.00057217]
	Learning Rate: 0.000572172
	LOSS [training: 0.09844898803773759 | validation: 0.19558758386527192]
	TIME [epoch: 8.31 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10290911141402304		[learning rate: 0.0005701]
	Learning Rate: 0.000570095
	LOSS [training: 0.10290911141402304 | validation: 0.055145526925767296]
	TIME [epoch: 8.29 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10197606534416279		[learning rate: 0.00056803]
	Learning Rate: 0.000568026
	LOSS [training: 0.10197606534416279 | validation: 0.06230359839907039]
	TIME [epoch: 8.28 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09040610275660407		[learning rate: 0.00056596]
	Learning Rate: 0.000565965
	LOSS [training: 0.09040610275660407 | validation: 0.06616826610123626]
	TIME [epoch: 8.29 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08491540408296054		[learning rate: 0.00056391]
	Learning Rate: 0.000563911
	LOSS [training: 0.08491540408296054 | validation: 0.0858396116930147]
	TIME [epoch: 8.32 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11200085665041122		[learning rate: 0.00056186]
	Learning Rate: 0.000561864
	LOSS [training: 0.11200085665041122 | validation: 0.06491116058317459]
	TIME [epoch: 8.29 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12605883703131648		[learning rate: 0.00055983]
	Learning Rate: 0.000559825
	LOSS [training: 0.12605883703131648 | validation: 0.061146346366294166]
	TIME [epoch: 8.29 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1368448461573531		[learning rate: 0.00055779]
	Learning Rate: 0.000557794
	LOSS [training: 0.1368448461573531 | validation: 0.07201032025112791]
	TIME [epoch: 8.29 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08497922482671721		[learning rate: 0.00055577]
	Learning Rate: 0.00055577
	LOSS [training: 0.08497922482671721 | validation: 0.05795757930286319]
	TIME [epoch: 8.31 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08910510544287598		[learning rate: 0.00055375]
	Learning Rate: 0.000553753
	LOSS [training: 0.08910510544287598 | validation: 0.0787634848720359]
	TIME [epoch: 8.28 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10637569830253808		[learning rate: 0.00055174]
	Learning Rate: 0.000551743
	LOSS [training: 0.10637569830253808 | validation: 0.07227244366473501]
	TIME [epoch: 8.28 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1030809320398354		[learning rate: 0.00054974]
	Learning Rate: 0.000549741
	LOSS [training: 0.1030809320398354 | validation: 0.04252012103035739]
	TIME [epoch: 8.28 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1082658371075543		[learning rate: 0.00054775]
	Learning Rate: 0.000547746
	LOSS [training: 0.1082658371075543 | validation: 0.058726307285432135]
	TIME [epoch: 8.31 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11765360248844861		[learning rate: 0.00054576]
	Learning Rate: 0.000545758
	LOSS [training: 0.11765360248844861 | validation: 0.12904290154720396]
	TIME [epoch: 8.28 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12653813634959132		[learning rate: 0.00054378]
	Learning Rate: 0.000543777
	LOSS [training: 0.12653813634959132 | validation: 0.08913498644842534]
	TIME [epoch: 8.28 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10021524026663134		[learning rate: 0.0005418]
	Learning Rate: 0.000541804
	LOSS [training: 0.10021524026663134 | validation: 0.06816279188813426]
	TIME [epoch: 8.29 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09385306546893993		[learning rate: 0.00053984]
	Learning Rate: 0.000539838
	LOSS [training: 0.09385306546893993 | validation: 0.05398957240091526]
	TIME [epoch: 8.31 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09654636014945467		[learning rate: 0.00053788]
	Learning Rate: 0.000537879
	LOSS [training: 0.09654636014945467 | validation: 0.062406538334458896]
	TIME [epoch: 8.28 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0902730697124762		[learning rate: 0.00053593]
	Learning Rate: 0.000535926
	LOSS [training: 0.0902730697124762 | validation: 0.05864049427365474]
	TIME [epoch: 8.28 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10617383790314668		[learning rate: 0.00053398]
	Learning Rate: 0.000533982
	LOSS [training: 0.10617383790314668 | validation: 0.05483709635381201]
	TIME [epoch: 8.28 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09215701333449797		[learning rate: 0.00053204]
	Learning Rate: 0.000532044
	LOSS [training: 0.09215701333449797 | validation: 0.09362650135838513]
	TIME [epoch: 8.31 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08576431833473731		[learning rate: 0.00053011]
	Learning Rate: 0.000530113
	LOSS [training: 0.08576431833473731 | validation: 0.07312589265881482]
	TIME [epoch: 8.29 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08634865026034715		[learning rate: 0.00052819]
	Learning Rate: 0.000528189
	LOSS [training: 0.08634865026034715 | validation: 0.057886749561312734]
	TIME [epoch: 8.28 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09148687060495715		[learning rate: 0.00052627]
	Learning Rate: 0.000526272
	LOSS [training: 0.09148687060495715 | validation: 0.06612794493106897]
	TIME [epoch: 8.28 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11327463367665784		[learning rate: 0.00052436]
	Learning Rate: 0.000524362
	LOSS [training: 0.11327463367665784 | validation: 0.06243170130419173]
	TIME [epoch: 8.31 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09092490130698262		[learning rate: 0.00052246]
	Learning Rate: 0.00052246
	LOSS [training: 0.09092490130698262 | validation: 0.0711299655213512]
	TIME [epoch: 8.28 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09583201121872864		[learning rate: 0.00052056]
	Learning Rate: 0.000520563
	LOSS [training: 0.09583201121872864 | validation: 0.07619289605524462]
	TIME [epoch: 8.28 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09057337340567953		[learning rate: 0.00051867]
	Learning Rate: 0.000518674
	LOSS [training: 0.09057337340567953 | validation: 0.08937826543149971]
	TIME [epoch: 8.28 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10411857682100786		[learning rate: 0.00051679]
	Learning Rate: 0.000516792
	LOSS [training: 0.10411857682100786 | validation: 0.05169878205884407]
	TIME [epoch: 8.31 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0934413212815893		[learning rate: 0.00051492]
	Learning Rate: 0.000514917
	LOSS [training: 0.0934413212815893 | validation: 0.06048412326154476]
	TIME [epoch: 8.28 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09611418095486036		[learning rate: 0.00051305]
	Learning Rate: 0.000513048
	LOSS [training: 0.09611418095486036 | validation: 0.06937165613095445]
	TIME [epoch: 8.29 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08152926148323943		[learning rate: 0.00051119]
	Learning Rate: 0.000511186
	LOSS [training: 0.08152926148323943 | validation: 0.089965993426705]
	TIME [epoch: 8.29 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0877327507893647		[learning rate: 0.00050933]
	Learning Rate: 0.000509331
	LOSS [training: 0.0877327507893647 | validation: 0.09249599141943549]
	TIME [epoch: 8.31 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1161600050410607		[learning rate: 0.00050748]
	Learning Rate: 0.000507483
	LOSS [training: 0.1161600050410607 | validation: 0.06390664643398136]
	TIME [epoch: 8.29 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09568905744171333		[learning rate: 0.00050564]
	Learning Rate: 0.000505641
	LOSS [training: 0.09568905744171333 | validation: 0.0812463127317952]
	TIME [epoch: 8.29 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09265147592337189		[learning rate: 0.00050381]
	Learning Rate: 0.000503806
	LOSS [training: 0.09265147592337189 | validation: 0.0628071966240171]
	TIME [epoch: 8.29 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09315023999934333		[learning rate: 0.00050198]
	Learning Rate: 0.000501977
	LOSS [training: 0.09315023999934333 | validation: 0.05602958091431069]
	TIME [epoch: 8.31 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09830507037072371		[learning rate: 0.00050016]
	Learning Rate: 0.000500156
	LOSS [training: 0.09830507037072371 | validation: 0.08987096695699485]
	TIME [epoch: 8.28 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10288386754330821		[learning rate: 0.00049834]
	Learning Rate: 0.000498341
	LOSS [training: 0.10288386754330821 | validation: 0.05146538010117682]
	TIME [epoch: 8.29 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0991397607117924		[learning rate: 0.00049653]
	Learning Rate: 0.000496532
	LOSS [training: 0.0991397607117924 | validation: 0.05558675376280431]
	TIME [epoch: 8.29 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0943192969126734		[learning rate: 0.00049473]
	Learning Rate: 0.00049473
	LOSS [training: 0.0943192969126734 | validation: 0.04397950726039563]
	TIME [epoch: 8.31 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09403915824503853		[learning rate: 0.00049293]
	Learning Rate: 0.000492935
	LOSS [training: 0.09403915824503853 | validation: 0.0529896843428766]
	TIME [epoch: 8.29 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08831881045083191		[learning rate: 0.00049115]
	Learning Rate: 0.000491146
	LOSS [training: 0.08831881045083191 | validation: 0.05102067760618692]
	TIME [epoch: 8.28 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08930493673062952		[learning rate: 0.00048936]
	Learning Rate: 0.000489364
	LOSS [training: 0.08930493673062952 | validation: 0.06720660878260376]
	TIME [epoch: 8.28 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09775508791634176		[learning rate: 0.00048759]
	Learning Rate: 0.000487588
	LOSS [training: 0.09775508791634176 | validation: 0.05485251174678755]
	TIME [epoch: 8.3 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08337680425701745		[learning rate: 0.00048582]
	Learning Rate: 0.000485818
	LOSS [training: 0.08337680425701745 | validation: 0.06637040499298266]
	TIME [epoch: 8.28 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09416422684161345		[learning rate: 0.00048406]
	Learning Rate: 0.000484055
	LOSS [training: 0.09416422684161345 | validation: 0.05571418255814395]
	TIME [epoch: 8.28 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09079911724696504		[learning rate: 0.0004823]
	Learning Rate: 0.000482298
	LOSS [training: 0.09079911724696504 | validation: 0.0716094671757248]
	TIME [epoch: 8.28 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09893563092831596		[learning rate: 0.00048055]
	Learning Rate: 0.000480548
	LOSS [training: 0.09893563092831596 | validation: 0.04356308465888675]
	TIME [epoch: 8.3 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08824934610313599		[learning rate: 0.0004788]
	Learning Rate: 0.000478804
	LOSS [training: 0.08824934610313599 | validation: 0.04993747762037882]
	TIME [epoch: 8.28 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08678146985066196		[learning rate: 0.00047707]
	Learning Rate: 0.000477067
	LOSS [training: 0.08678146985066196 | validation: 0.05859366304305727]
	TIME [epoch: 8.28 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09461088552486413		[learning rate: 0.00047534]
	Learning Rate: 0.000475335
	LOSS [training: 0.09461088552486413 | validation: 0.09400138394428065]
	TIME [epoch: 8.28 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09889115851167393		[learning rate: 0.00047361]
	Learning Rate: 0.00047361
	LOSS [training: 0.09889115851167393 | validation: 0.05081731163320918]
	TIME [epoch: 8.3 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09890339766034113		[learning rate: 0.00047189]
	Learning Rate: 0.000471891
	LOSS [training: 0.09890339766034113 | validation: 0.06401395636291837]
	TIME [epoch: 8.28 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10145190749177664		[learning rate: 0.00047018]
	Learning Rate: 0.000470179
	LOSS [training: 0.10145190749177664 | validation: 0.07343020571674935]
	TIME [epoch: 8.27 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10827891667559093		[learning rate: 0.00046847]
	Learning Rate: 0.000468473
	LOSS [training: 0.10827891667559093 | validation: 0.06037524794935825]
	TIME [epoch: 8.28 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11255361534691477		[learning rate: 0.00046677]
	Learning Rate: 0.000466773
	LOSS [training: 0.11255361534691477 | validation: 0.045923229433315726]
	TIME [epoch: 8.3 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10072337523103725		[learning rate: 0.00046508]
	Learning Rate: 0.000465079
	LOSS [training: 0.10072337523103725 | validation: 0.05899148210406717]
	TIME [epoch: 8.28 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0996797493635687		[learning rate: 0.00046339]
	Learning Rate: 0.000463391
	LOSS [training: 0.0996797493635687 | validation: 0.05777791603504765]
	TIME [epoch: 8.28 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10487360394454448		[learning rate: 0.00046171]
	Learning Rate: 0.000461709
	LOSS [training: 0.10487360394454448 | validation: 0.05400670455289737]
	TIME [epoch: 8.28 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09531165655166275		[learning rate: 0.00046003]
	Learning Rate: 0.000460034
	LOSS [training: 0.09531165655166275 | validation: 0.0603409328949772]
	TIME [epoch: 8.3 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09002316299476208		[learning rate: 0.00045836]
	Learning Rate: 0.000458364
	LOSS [training: 0.09002316299476208 | validation: 0.06752636880714594]
	TIME [epoch: 8.28 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09141530999962375		[learning rate: 0.0004567]
	Learning Rate: 0.000456701
	LOSS [training: 0.09141530999962375 | validation: 0.059333997357630154]
	TIME [epoch: 8.28 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09872079340905368		[learning rate: 0.00045504]
	Learning Rate: 0.000455043
	LOSS [training: 0.09872079340905368 | validation: 0.09228542546089932]
	TIME [epoch: 8.27 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09952996524729181		[learning rate: 0.00045339]
	Learning Rate: 0.000453392
	LOSS [training: 0.09952996524729181 | validation: 0.0769569438312642]
	TIME [epoch: 8.3 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09132923503526671		[learning rate: 0.00045175]
	Learning Rate: 0.000451746
	LOSS [training: 0.09132923503526671 | validation: 0.05757826814967058]
	TIME [epoch: 8.28 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10584919344655522		[learning rate: 0.00045011]
	Learning Rate: 0.000450107
	LOSS [training: 0.10584919344655522 | validation: 0.06974115308643905]
	TIME [epoch: 8.27 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09632476646112323		[learning rate: 0.00044847]
	Learning Rate: 0.000448474
	LOSS [training: 0.09632476646112323 | validation: 0.04896603646540143]
	TIME [epoch: 8.27 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08367246201546016		[learning rate: 0.00044685]
	Learning Rate: 0.000446846
	LOSS [training: 0.08367246201546016 | validation: 0.052070969813158494]
	TIME [epoch: 8.3 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0975996110715169		[learning rate: 0.00044522]
	Learning Rate: 0.000445224
	LOSS [training: 0.0975996110715169 | validation: 0.05098504493628246]
	TIME [epoch: 8.27 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09811004170941338		[learning rate: 0.00044361]
	Learning Rate: 0.000443609
	LOSS [training: 0.09811004170941338 | validation: 0.06273147002960237]
	TIME [epoch: 8.27 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0971798878701807		[learning rate: 0.000442]
	Learning Rate: 0.000441999
	LOSS [training: 0.0971798878701807 | validation: 0.04958026132062422]
	TIME [epoch: 8.28 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10335038473571054		[learning rate: 0.00044039]
	Learning Rate: 0.000440395
	LOSS [training: 0.10335038473571054 | validation: 0.07244017277218584]
	TIME [epoch: 8.3 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08720600760324505		[learning rate: 0.0004388]
	Learning Rate: 0.000438797
	LOSS [training: 0.08720600760324505 | validation: 0.07467738976988916]
	TIME [epoch: 8.28 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0892533420950979		[learning rate: 0.0004372]
	Learning Rate: 0.000437204
	LOSS [training: 0.0892533420950979 | validation: 0.06390227205642077]
	TIME [epoch: 8.27 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09558536396679784		[learning rate: 0.00043562]
	Learning Rate: 0.000435617
	LOSS [training: 0.09558536396679784 | validation: 0.07333739463529136]
	TIME [epoch: 8.28 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10392785753107414		[learning rate: 0.00043404]
	Learning Rate: 0.000434037
	LOSS [training: 0.10392785753107414 | validation: 0.07410737536591361]
	TIME [epoch: 8.3 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09701976875882733		[learning rate: 0.00043246]
	Learning Rate: 0.000432461
	LOSS [training: 0.09701976875882733 | validation: 0.0625512939825721]
	TIME [epoch: 8.28 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10106408469818422		[learning rate: 0.00043089]
	Learning Rate: 0.000430892
	LOSS [training: 0.10106408469818422 | validation: 0.04829537636398838]
	TIME [epoch: 8.27 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08821355076837747		[learning rate: 0.00042933]
	Learning Rate: 0.000429328
	LOSS [training: 0.08821355076837747 | validation: 0.06498228449473079]
	TIME [epoch: 8.28 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09147613841974003		[learning rate: 0.00042777]
	Learning Rate: 0.00042777
	LOSS [training: 0.09147613841974003 | validation: 0.06639660833421554]
	TIME [epoch: 8.3 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0978182571291135		[learning rate: 0.00042622]
	Learning Rate: 0.000426218
	LOSS [training: 0.0978182571291135 | validation: 0.043907831283926155]
	TIME [epoch: 8.28 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08760435667977066		[learning rate: 0.00042467]
	Learning Rate: 0.000424671
	LOSS [training: 0.08760435667977066 | validation: 0.04374583323336256]
	TIME [epoch: 8.28 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09824809901162354		[learning rate: 0.00042313]
	Learning Rate: 0.00042313
	LOSS [training: 0.09824809901162354 | validation: 0.07028756342420309]
	TIME [epoch: 8.28 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09905659939187743		[learning rate: 0.00042159]
	Learning Rate: 0.000421594
	LOSS [training: 0.09905659939187743 | validation: 0.05703827283400552]
	TIME [epoch: 8.3 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10540194782674961		[learning rate: 0.00042006]
	Learning Rate: 0.000420064
	LOSS [training: 0.10540194782674961 | validation: 0.10953489661548985]
	TIME [epoch: 8.28 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10848637556399318		[learning rate: 0.00041854]
	Learning Rate: 0.00041854
	LOSS [training: 0.10848637556399318 | validation: 0.08546336328191315]
	TIME [epoch: 8.28 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0946534065542228		[learning rate: 0.00041702]
	Learning Rate: 0.000417021
	LOSS [training: 0.0946534065542228 | validation: 0.054578426333507145]
	TIME [epoch: 8.27 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10351261312174423		[learning rate: 0.00041551]
	Learning Rate: 0.000415508
	LOSS [training: 0.10351261312174423 | validation: 0.09267149648391917]
	TIME [epoch: 8.3 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09363847104459105		[learning rate: 0.000414]
	Learning Rate: 0.000414
	LOSS [training: 0.09363847104459105 | validation: 0.06800482914142109]
	TIME [epoch: 8.27 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09001512469488693		[learning rate: 0.0004125]
	Learning Rate: 0.000412497
	LOSS [training: 0.09001512469488693 | validation: 0.07097931781157464]
	TIME [epoch: 8.28 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10150785704099423		[learning rate: 0.000411]
	Learning Rate: 0.000411
	LOSS [training: 0.10150785704099423 | validation: 0.07860478758722311]
	TIME [epoch: 8.28 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0981996356024832		[learning rate: 0.00040951]
	Learning Rate: 0.000409509
	LOSS [training: 0.0981996356024832 | validation: 0.07877008999234583]
	TIME [epoch: 8.3 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10106079350712256		[learning rate: 0.00040802]
	Learning Rate: 0.000408023
	LOSS [training: 0.10106079350712256 | validation: 0.0704504484030335]
	TIME [epoch: 8.28 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08317097377846737		[learning rate: 0.00040654]
	Learning Rate: 0.000406542
	LOSS [training: 0.08317097377846737 | validation: 0.05150149295251709]
	TIME [epoch: 8.27 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08901418076743085		[learning rate: 0.00040507]
	Learning Rate: 0.000405067
	LOSS [training: 0.08901418076743085 | validation: 0.048570461982180256]
	TIME [epoch: 8.28 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09830996721837326		[learning rate: 0.0004036]
	Learning Rate: 0.000403597
	LOSS [training: 0.09830996721837326 | validation: 0.04529296745384251]
	TIME [epoch: 8.3 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0836344871937799		[learning rate: 0.00040213]
	Learning Rate: 0.000402132
	LOSS [training: 0.0836344871937799 | validation: 0.05351508202007567]
	TIME [epoch: 8.28 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08781054445427895		[learning rate: 0.00040067]
	Learning Rate: 0.000400672
	LOSS [training: 0.08781054445427895 | validation: 0.05031745173307159]
	TIME [epoch: 8.28 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09322221999878146		[learning rate: 0.00039922]
	Learning Rate: 0.000399218
	LOSS [training: 0.09322221999878146 | validation: 0.06814797164499777]
	TIME [epoch: 8.28 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08808025660364617		[learning rate: 0.00039777]
	Learning Rate: 0.00039777
	LOSS [training: 0.08808025660364617 | validation: 0.06025243078795952]
	TIME [epoch: 8.3 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08084447552094984		[learning rate: 0.00039633]
	Learning Rate: 0.000396326
	LOSS [training: 0.08084447552094984 | validation: 0.06266656306144293]
	TIME [epoch: 8.28 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09353812374813701		[learning rate: 0.00039489]
	Learning Rate: 0.000394888
	LOSS [training: 0.09353812374813701 | validation: 0.06645435488045412]
	TIME [epoch: 8.28 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0887368648994562		[learning rate: 0.00039345]
	Learning Rate: 0.000393455
	LOSS [training: 0.0887368648994562 | validation: 0.05313362861742183]
	TIME [epoch: 8.28 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0987796766852908		[learning rate: 0.00039203]
	Learning Rate: 0.000392027
	LOSS [training: 0.0987796766852908 | validation: 0.06882675664723434]
	TIME [epoch: 8.31 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09044687650623866		[learning rate: 0.0003906]
	Learning Rate: 0.000390604
	LOSS [training: 0.09044687650623866 | validation: 0.061156584308232176]
	TIME [epoch: 8.28 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08478777521543826		[learning rate: 0.00038919]
	Learning Rate: 0.000389187
	LOSS [training: 0.08478777521543826 | validation: 0.05933795402468044]
	TIME [epoch: 8.28 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08929392670114726		[learning rate: 0.00038777]
	Learning Rate: 0.000387774
	LOSS [training: 0.08929392670114726 | validation: 0.048484216457660115]
	TIME [epoch: 8.28 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08845613032543208		[learning rate: 0.00038637]
	Learning Rate: 0.000386367
	LOSS [training: 0.08845613032543208 | validation: 0.045297270647999044]
	TIME [epoch: 8.3 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0918854469090609		[learning rate: 0.00038496]
	Learning Rate: 0.000384965
	LOSS [training: 0.0918854469090609 | validation: 0.04214322305314918]
	TIME [epoch: 8.28 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08597595512569886		[learning rate: 0.00038357]
	Learning Rate: 0.000383568
	LOSS [training: 0.08597595512569886 | validation: 0.0626728518249793]
	TIME [epoch: 8.27 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08524636927654108		[learning rate: 0.00038218]
	Learning Rate: 0.000382176
	LOSS [training: 0.08524636927654108 | validation: 0.05659550392171658]
	TIME [epoch: 8.28 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1259262249846398		[learning rate: 0.00038079]
	Learning Rate: 0.000380789
	LOSS [training: 0.1259262249846398 | validation: 0.08022614555276736]
	TIME [epoch: 8.3 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09109846537907462		[learning rate: 0.00037941]
	Learning Rate: 0.000379407
	LOSS [training: 0.09109846537907462 | validation: 0.05851235659222151]
	TIME [epoch: 8.28 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09044607181907419		[learning rate: 0.00037803]
	Learning Rate: 0.00037803
	LOSS [training: 0.09044607181907419 | validation: 0.06301919354940098]
	TIME [epoch: 8.27 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08495009194965375		[learning rate: 0.00037666]
	Learning Rate: 0.000376658
	LOSS [training: 0.08495009194965375 | validation: 0.04799808318234414]
	TIME [epoch: 8.28 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09485762164553138		[learning rate: 0.00037529]
	Learning Rate: 0.000375291
	LOSS [training: 0.09485762164553138 | validation: 0.052654034285442256]
	TIME [epoch: 8.3 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08212031422060066		[learning rate: 0.00037393]
	Learning Rate: 0.000373929
	LOSS [training: 0.08212031422060066 | validation: 0.045096286258043586]
	TIME [epoch: 8.28 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09097381135973465		[learning rate: 0.00037257]
	Learning Rate: 0.000372572
	LOSS [training: 0.09097381135973465 | validation: 0.061348949280051135]
	TIME [epoch: 8.28 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0970007768913448		[learning rate: 0.00037122]
	Learning Rate: 0.00037122
	LOSS [training: 0.0970007768913448 | validation: 0.08138171218696036]
	TIME [epoch: 8.28 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09250732928168519		[learning rate: 0.00036987]
	Learning Rate: 0.000369873
	LOSS [training: 0.09250732928168519 | validation: 0.043588056321118805]
	TIME [epoch: 8.3 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08755708562726527		[learning rate: 0.00036853]
	Learning Rate: 0.000368531
	LOSS [training: 0.08755708562726527 | validation: 0.058523057565362864]
	TIME [epoch: 8.27 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08470203640218958		[learning rate: 0.00036719]
	Learning Rate: 0.000367193
	LOSS [training: 0.08470203640218958 | validation: 0.04495507563680672]
	TIME [epoch: 8.28 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09347840013716063		[learning rate: 0.00036586]
	Learning Rate: 0.000365861
	LOSS [training: 0.09347840013716063 | validation: 0.051478146830011035]
	TIME [epoch: 8.27 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08585170960653798		[learning rate: 0.00036453]
	Learning Rate: 0.000364533
	LOSS [training: 0.08585170960653798 | validation: 0.06530013542869134]
	TIME [epoch: 8.3 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09766362041998453		[learning rate: 0.00036321]
	Learning Rate: 0.00036321
	LOSS [training: 0.09766362041998453 | validation: 0.04848553038233347]
	TIME [epoch: 8.27 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09742497458626503		[learning rate: 0.00036189]
	Learning Rate: 0.000361892
	LOSS [training: 0.09742497458626503 | validation: 0.06031708790938099]
	TIME [epoch: 8.27 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0855548405972953		[learning rate: 0.00036058]
	Learning Rate: 0.000360579
	LOSS [training: 0.0855548405972953 | validation: 0.037261820755759775]
	TIME [epoch: 8.27 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09912165095233112		[learning rate: 0.00035927]
	Learning Rate: 0.00035927
	LOSS [training: 0.09912165095233112 | validation: 0.05616360112751246]
	TIME [epoch: 8.3 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08176429460167864		[learning rate: 0.00035797]
	Learning Rate: 0.000357966
	LOSS [training: 0.08176429460167864 | validation: 0.04399676937172303]
	TIME [epoch: 8.28 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08437079048478414		[learning rate: 0.00035667]
	Learning Rate: 0.000356667
	LOSS [training: 0.08437079048478414 | validation: 0.055079635682765055]
	TIME [epoch: 8.27 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10543360786131424		[learning rate: 0.00035537]
	Learning Rate: 0.000355373
	LOSS [training: 0.10543360786131424 | validation: 0.05476627057241869]
	TIME [epoch: 8.28 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09507140844233622		[learning rate: 0.00035408]
	Learning Rate: 0.000354083
	LOSS [training: 0.09507140844233622 | validation: 0.1327433054085671]
	TIME [epoch: 8.3 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10941685373300321		[learning rate: 0.0003528]
	Learning Rate: 0.000352798
	LOSS [training: 0.10941685373300321 | validation: 0.05464484088407406]
	TIME [epoch: 8.28 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08593150792600997		[learning rate: 0.00035152]
	Learning Rate: 0.000351518
	LOSS [training: 0.08593150792600997 | validation: 0.1115576937861714]
	TIME [epoch: 8.27 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09476435191304997		[learning rate: 0.00035024]
	Learning Rate: 0.000350242
	LOSS [training: 0.09476435191304997 | validation: 0.047273110607466154]
	TIME [epoch: 8.28 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08501608171770951		[learning rate: 0.00034897]
	Learning Rate: 0.000348971
	LOSS [training: 0.08501608171770951 | validation: 0.047727466843537555]
	TIME [epoch: 8.3 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08264018966320938		[learning rate: 0.0003477]
	Learning Rate: 0.000347705
	LOSS [training: 0.08264018966320938 | validation: 0.05614735951295958]
	TIME [epoch: 8.28 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08162759215435342		[learning rate: 0.00034644]
	Learning Rate: 0.000346443
	LOSS [training: 0.08162759215435342 | validation: 0.0654659208144971]
	TIME [epoch: 8.28 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08269882128754932		[learning rate: 0.00034519]
	Learning Rate: 0.000345186
	LOSS [training: 0.08269882128754932 | validation: 0.05800496792971477]
	TIME [epoch: 8.27 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08345368418988658		[learning rate: 0.00034393]
	Learning Rate: 0.000343933
	LOSS [training: 0.08345368418988658 | validation: 0.07550502685699251]
	TIME [epoch: 8.3 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.087090852893781		[learning rate: 0.00034268]
	Learning Rate: 0.000342685
	LOSS [training: 0.087090852893781 | validation: 0.042443870730001707]
	TIME [epoch: 8.28 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08071563774258471		[learning rate: 0.00034144]
	Learning Rate: 0.000341441
	LOSS [training: 0.08071563774258471 | validation: 0.04933587920592726]
	TIME [epoch: 8.28 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08453099103654574		[learning rate: 0.0003402]
	Learning Rate: 0.000340202
	LOSS [training: 0.08453099103654574 | validation: 0.04415700021370387]
	TIME [epoch: 8.27 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08765376985967041		[learning rate: 0.00033897]
	Learning Rate: 0.000338967
	LOSS [training: 0.08765376985967041 | validation: 0.05353979791607068]
	TIME [epoch: 8.3 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08621780910699582		[learning rate: 0.00033774]
	Learning Rate: 0.000337737
	LOSS [training: 0.08621780910699582 | validation: 0.05188963573890103]
	TIME [epoch: 8.27 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09182178139881483		[learning rate: 0.00033651]
	Learning Rate: 0.000336512
	LOSS [training: 0.09182178139881483 | validation: 0.05199569149655357]
	TIME [epoch: 8.27 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10266396585616921		[learning rate: 0.00033529]
	Learning Rate: 0.00033529
	LOSS [training: 0.10266396585616921 | validation: 0.0651934913317637]
	TIME [epoch: 8.27 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09084210002733265		[learning rate: 0.00033407]
	Learning Rate: 0.000334074
	LOSS [training: 0.09084210002733265 | validation: 0.05749635936547301]
	TIME [epoch: 8.3 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08688713866837453		[learning rate: 0.00033286]
	Learning Rate: 0.000332861
	LOSS [training: 0.08688713866837453 | validation: 0.051630051987703235]
	TIME [epoch: 8.27 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08241677776438441		[learning rate: 0.00033165]
	Learning Rate: 0.000331653
	LOSS [training: 0.08241677776438441 | validation: 0.050915654241959235]
	TIME [epoch: 8.27 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08154290467538537		[learning rate: 0.00033045]
	Learning Rate: 0.00033045
	LOSS [training: 0.08154290467538537 | validation: 0.03812545326970174]
	TIME [epoch: 8.27 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08971748239875958		[learning rate: 0.00032925]
	Learning Rate: 0.00032925
	LOSS [training: 0.08971748239875958 | validation: 0.06421017010859693]
	TIME [epoch: 8.29 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08407409241735704		[learning rate: 0.00032806]
	Learning Rate: 0.000328056
	LOSS [training: 0.08407409241735704 | validation: 0.09141725564501929]
	TIME [epoch: 8.27 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08928760193864797		[learning rate: 0.00032686]
	Learning Rate: 0.000326865
	LOSS [training: 0.08928760193864797 | validation: 0.06816958356348143]
	TIME [epoch: 8.28 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09332759601316949		[learning rate: 0.00032568]
	Learning Rate: 0.000325679
	LOSS [training: 0.09332759601316949 | validation: 0.048683620030345584]
	TIME [epoch: 8.27 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08985870404638112		[learning rate: 0.0003245]
	Learning Rate: 0.000324497
	LOSS [training: 0.08985870404638112 | validation: 0.043896344721207896]
	TIME [epoch: 8.3 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08819378859254581		[learning rate: 0.00032332]
	Learning Rate: 0.000323319
	LOSS [training: 0.08819378859254581 | validation: 0.06752699235039784]
	TIME [epoch: 8.27 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08219339412935513		[learning rate: 0.00032215]
	Learning Rate: 0.000322146
	LOSS [training: 0.08219339412935513 | validation: 0.04940511460406477]
	TIME [epoch: 8.28 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0816592081027824		[learning rate: 0.00032098]
	Learning Rate: 0.000320977
	LOSS [training: 0.0816592081027824 | validation: 0.054769918163184744]
	TIME [epoch: 8.27 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08282916791066239		[learning rate: 0.00031981]
	Learning Rate: 0.000319812
	LOSS [training: 0.08282916791066239 | validation: 0.05272586987346574]
	TIME [epoch: 8.3 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09174315492642297		[learning rate: 0.00031865]
	Learning Rate: 0.000318651
	LOSS [training: 0.09174315492642297 | validation: 0.06572804611840301]
	TIME [epoch: 8.28 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08329528498016349		[learning rate: 0.0003175]
	Learning Rate: 0.000317495
	LOSS [training: 0.08329528498016349 | validation: 0.04035887884564454]
	TIME [epoch: 8.28 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09178242232309983		[learning rate: 0.00031634]
	Learning Rate: 0.000316343
	LOSS [training: 0.09178242232309983 | validation: 0.04731560982659375]
	TIME [epoch: 8.27 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08455712165420184		[learning rate: 0.00031519]
	Learning Rate: 0.000315195
	LOSS [training: 0.08455712165420184 | validation: 0.059948514217300464]
	TIME [epoch: 8.3 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10253202519066287		[learning rate: 0.00031405]
	Learning Rate: 0.000314051
	LOSS [training: 0.10253202519066287 | validation: 0.040447840501253676]
	TIME [epoch: 8.28 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08252776254455245		[learning rate: 0.00031291]
	Learning Rate: 0.000312911
	LOSS [training: 0.08252776254455245 | validation: 0.06486632511936724]
	TIME [epoch: 8.28 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08087439151045364		[learning rate: 0.00031178]
	Learning Rate: 0.000311776
	LOSS [training: 0.08087439151045364 | validation: 0.04300503835130472]
	TIME [epoch: 8.28 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08038052645975499		[learning rate: 0.00031064]
	Learning Rate: 0.000310644
	LOSS [training: 0.08038052645975499 | validation: 0.0550130569621261]
	TIME [epoch: 8.3 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08074331570401069		[learning rate: 0.00030952]
	Learning Rate: 0.000309517
	LOSS [training: 0.08074331570401069 | validation: 0.05384135977939016]
	TIME [epoch: 8.28 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0986633300992447		[learning rate: 0.00030839]
	Learning Rate: 0.000308394
	LOSS [training: 0.0986633300992447 | validation: 0.05070895417855542]
	TIME [epoch: 8.27 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08725825478794773		[learning rate: 0.00030727]
	Learning Rate: 0.000307274
	LOSS [training: 0.08725825478794773 | validation: 0.07588806619604577]
	TIME [epoch: 8.27 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08257230845876258		[learning rate: 0.00030616]
	Learning Rate: 0.000306159
	LOSS [training: 0.08257230845876258 | validation: 0.05097613230424425]
	TIME [epoch: 8.3 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08844176784142989		[learning rate: 0.00030505]
	Learning Rate: 0.000305048
	LOSS [training: 0.08844176784142989 | validation: 0.06649591380168145]
	TIME [epoch: 8.27 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09143377122755969		[learning rate: 0.00030394]
	Learning Rate: 0.000303941
	LOSS [training: 0.09143377122755969 | validation: 0.04681773620766787]
	TIME [epoch: 8.27 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07915158992068005		[learning rate: 0.00030284]
	Learning Rate: 0.000302838
	LOSS [training: 0.07915158992068005 | validation: 0.06727826220009477]
	TIME [epoch: 8.27 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08253216988004326		[learning rate: 0.00030174]
	Learning Rate: 0.000301739
	LOSS [training: 0.08253216988004326 | validation: 0.04389625780521042]
	TIME [epoch: 8.29 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09740834736205886		[learning rate: 0.00030064]
	Learning Rate: 0.000300644
	LOSS [training: 0.09740834736205886 | validation: 0.043971313329561626]
	TIME [epoch: 8.28 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08244416432396086		[learning rate: 0.00029955]
	Learning Rate: 0.000299553
	LOSS [training: 0.08244416432396086 | validation: 0.0542924560527991]
	TIME [epoch: 8.28 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09868504845780615		[learning rate: 0.00029847]
	Learning Rate: 0.000298466
	LOSS [training: 0.09868504845780615 | validation: 0.04889075472146455]
	TIME [epoch: 8.27 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09033622899179092		[learning rate: 0.00029738]
	Learning Rate: 0.000297383
	LOSS [training: 0.09033622899179092 | validation: 0.04597775102196085]
	TIME [epoch: 8.3 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09486818739579994		[learning rate: 0.0002963]
	Learning Rate: 0.000296304
	LOSS [training: 0.09486818739579994 | validation: 0.04086877345089135]
	TIME [epoch: 8.28 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08192622262048245		[learning rate: 0.00029523]
	Learning Rate: 0.000295228
	LOSS [training: 0.08192622262048245 | validation: 0.03897863020324633]
	TIME [epoch: 8.27 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07434313819344723		[learning rate: 0.00029416]
	Learning Rate: 0.000294157
	LOSS [training: 0.07434313819344723 | validation: 0.05041193089541961]
	TIME [epoch: 8.27 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08418847004926175		[learning rate: 0.00029309]
	Learning Rate: 0.000293089
	LOSS [training: 0.08418847004926175 | validation: 0.03736930362540673]
	TIME [epoch: 8.3 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08217792040615682		[learning rate: 0.00029203]
	Learning Rate: 0.000292026
	LOSS [training: 0.08217792040615682 | validation: 0.052376761063970795]
	TIME [epoch: 8.28 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0843059975757214		[learning rate: 0.00029097]
	Learning Rate: 0.000290966
	LOSS [training: 0.0843059975757214 | validation: 0.03558611777244941]
	TIME [epoch: 8.27 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08109606723176466		[learning rate: 0.00028991]
	Learning Rate: 0.00028991
	LOSS [training: 0.08109606723176466 | validation: 0.0572873039328778]
	TIME [epoch: 8.28 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08269487334000361		[learning rate: 0.00028886]
	Learning Rate: 0.000288858
	LOSS [training: 0.08269487334000361 | validation: 0.04597440554736015]
	TIME [epoch: 8.3 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07691675853236489		[learning rate: 0.00028781]
	Learning Rate: 0.00028781
	LOSS [training: 0.07691675853236489 | validation: 0.04819224043234733]
	TIME [epoch: 8.27 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0822867399680428		[learning rate: 0.00028677]
	Learning Rate: 0.000286765
	LOSS [training: 0.0822867399680428 | validation: 0.045633716938922424]
	TIME [epoch: 8.27 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08215808531566257		[learning rate: 0.00028572]
	Learning Rate: 0.000285724
	LOSS [training: 0.08215808531566257 | validation: 0.03785754308745814]
	TIME [epoch: 8.27 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08604562561302079		[learning rate: 0.00028469]
	Learning Rate: 0.000284688
	LOSS [training: 0.08604562561302079 | validation: 0.04462133301949729]
	TIME [epoch: 8.3 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08009337219192542		[learning rate: 0.00028365]
	Learning Rate: 0.000283654
	LOSS [training: 0.08009337219192542 | validation: 0.049992882938857244]
	TIME [epoch: 8.27 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07996110374926434		[learning rate: 0.00028263]
	Learning Rate: 0.000282625
	LOSS [training: 0.07996110374926434 | validation: 0.0564093462941415]
	TIME [epoch: 8.28 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08566255862625967		[learning rate: 0.0002816]
	Learning Rate: 0.000281599
	LOSS [training: 0.08566255862625967 | validation: 0.05946962196721001]
	TIME [epoch: 8.27 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09322456633571075		[learning rate: 0.00028058]
	Learning Rate: 0.000280577
	LOSS [training: 0.09322456633571075 | validation: 0.044799433418446236]
	TIME [epoch: 8.3 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08239105451100204		[learning rate: 0.00027956]
	Learning Rate: 0.000279559
	LOSS [training: 0.08239105451100204 | validation: 0.04965618717616428]
	TIME [epoch: 8.28 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08446675521369154		[learning rate: 0.00027854]
	Learning Rate: 0.000278545
	LOSS [training: 0.08446675521369154 | validation: 0.06795998040075835]
	TIME [epoch: 8.27 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08293053400771311		[learning rate: 0.00027753]
	Learning Rate: 0.000277534
	LOSS [training: 0.08293053400771311 | validation: 0.06357419377644494]
	TIME [epoch: 8.27 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0848897722853387		[learning rate: 0.00027653]
	Learning Rate: 0.000276527
	LOSS [training: 0.0848897722853387 | validation: 0.059785179340180385]
	TIME [epoch: 8.31 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07525996308950658		[learning rate: 0.00027552]
	Learning Rate: 0.000275523
	LOSS [training: 0.07525996308950658 | validation: 0.04401318976966985]
	TIME [epoch: 8.27 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07729264504798018		[learning rate: 0.00027452]
	Learning Rate: 0.000274523
	LOSS [training: 0.07729264504798018 | validation: 0.04856996863528879]
	TIME [epoch: 8.27 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08438828462741362		[learning rate: 0.00027353]
	Learning Rate: 0.000273527
	LOSS [training: 0.08438828462741362 | validation: 0.06559313697669368]
	TIME [epoch: 8.27 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08146351851589875		[learning rate: 0.00027253]
	Learning Rate: 0.000272534
	LOSS [training: 0.08146351851589875 | validation: 0.056480670146013795]
	TIME [epoch: 8.29 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08367849201177953		[learning rate: 0.00027155]
	Learning Rate: 0.000271545
	LOSS [training: 0.08367849201177953 | validation: 0.039113196018500336]
	TIME [epoch: 8.27 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09238970922622318		[learning rate: 0.00027056]
	Learning Rate: 0.00027056
	LOSS [training: 0.09238970922622318 | validation: 0.05041438147036309]
	TIME [epoch: 8.27 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08633640622448062		[learning rate: 0.00026958]
	Learning Rate: 0.000269578
	LOSS [training: 0.08633640622448062 | validation: 0.03229663934215868]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_1094.pth
	Model improved!!!
EPOCH 1095/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08728571698147053		[learning rate: 0.0002686]
	Learning Rate: 0.0002686
	LOSS [training: 0.08728571698147053 | validation: 0.07492113873144078]
	TIME [epoch: 8.3 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08547079600190467		[learning rate: 0.00026762]
	Learning Rate: 0.000267625
	LOSS [training: 0.08547079600190467 | validation: 0.07232575809641734]
	TIME [epoch: 8.26 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09166785168289743		[learning rate: 0.00026665]
	Learning Rate: 0.000266654
	LOSS [training: 0.09166785168289743 | validation: 0.03976102938429975]
	TIME [epoch: 8.27 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10129655432775517		[learning rate: 0.00026569]
	Learning Rate: 0.000265686
	LOSS [training: 0.10129655432775517 | validation: 0.0701611848568743]
	TIME [epoch: 8.27 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07490027715342516		[learning rate: 0.00026472]
	Learning Rate: 0.000264722
	LOSS [training: 0.07490027715342516 | validation: 0.040604316763292486]
	TIME [epoch: 8.29 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08248544589274051		[learning rate: 0.00026376]
	Learning Rate: 0.000263761
	LOSS [training: 0.08248544589274051 | validation: 0.04482819203978253]
	TIME [epoch: 8.27 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08137737239700811		[learning rate: 0.0002628]
	Learning Rate: 0.000262804
	LOSS [training: 0.08137737239700811 | validation: 0.03364375136274762]
	TIME [epoch: 8.27 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07616609839361832		[learning rate: 0.00026185]
	Learning Rate: 0.00026185
	LOSS [training: 0.07616609839361832 | validation: 0.05096319858332491]
	TIME [epoch: 8.27 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08350422397634334		[learning rate: 0.0002609]
	Learning Rate: 0.0002609
	LOSS [training: 0.08350422397634334 | validation: 0.045140099928692196]
	TIME [epoch: 8.29 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07495278858876746		[learning rate: 0.00025995]
	Learning Rate: 0.000259953
	LOSS [training: 0.07495278858876746 | validation: 0.050450698839016234]
	TIME [epoch: 8.27 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08661665490734442		[learning rate: 0.00025901]
	Learning Rate: 0.00025901
	LOSS [training: 0.08661665490734442 | validation: 0.071979908158211]
	TIME [epoch: 8.27 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0837139524731431		[learning rate: 0.00025807]
	Learning Rate: 0.00025807
	LOSS [training: 0.0837139524731431 | validation: 0.05756079210686625]
	TIME [epoch: 8.27 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08912097375129538		[learning rate: 0.00025713]
	Learning Rate: 0.000257133
	LOSS [training: 0.08912097375129538 | validation: 0.05824457519035189]
	TIME [epoch: 8.29 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08563319128177374		[learning rate: 0.0002562]
	Learning Rate: 0.0002562
	LOSS [training: 0.08563319128177374 | validation: 0.05688446151139373]
	TIME [epoch: 8.27 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08626793296767868		[learning rate: 0.00025527]
	Learning Rate: 0.00025527
	LOSS [training: 0.08626793296767868 | validation: 0.06595527711257765]
	TIME [epoch: 8.27 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0816899663363821		[learning rate: 0.00025434]
	Learning Rate: 0.000254344
	LOSS [training: 0.0816899663363821 | validation: 0.11063339060480143]
	TIME [epoch: 8.27 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09751246005838562		[learning rate: 0.00025342]
	Learning Rate: 0.000253421
	LOSS [training: 0.09751246005838562 | validation: 0.05192962255982244]
	TIME [epoch: 8.29 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08345080843701363		[learning rate: 0.0002525]
	Learning Rate: 0.000252501
	LOSS [training: 0.08345080843701363 | validation: 0.042200361128845836]
	TIME [epoch: 8.27 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07903966084019998		[learning rate: 0.00025158]
	Learning Rate: 0.000251585
	LOSS [training: 0.07903966084019998 | validation: 0.04343942063618981]
	TIME [epoch: 8.26 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0792279333337959		[learning rate: 0.00025067]
	Learning Rate: 0.000250672
	LOSS [training: 0.0792279333337959 | validation: 0.04536874550620756]
	TIME [epoch: 8.27 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08349792931809966		[learning rate: 0.00024976]
	Learning Rate: 0.000249762
	LOSS [training: 0.08349792931809966 | validation: 0.031328421681982674]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_1115.pth
	Model improved!!!
EPOCH 1116/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07638106222726751		[learning rate: 0.00024886]
	Learning Rate: 0.000248856
	LOSS [training: 0.07638106222726751 | validation: 0.049800816342430124]
	TIME [epoch: 8.26 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0865984848816255		[learning rate: 0.00024795]
	Learning Rate: 0.000247952
	LOSS [training: 0.0865984848816255 | validation: 0.052462847690444875]
	TIME [epoch: 8.27 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07959003077169118		[learning rate: 0.00024705]
	Learning Rate: 0.000247053
	LOSS [training: 0.07959003077169118 | validation: 0.052206353876511964]
	TIME [epoch: 8.28 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08009216573493533		[learning rate: 0.00024616]
	Learning Rate: 0.000246156
	LOSS [training: 0.08009216573493533 | validation: 0.05396382031349943]
	TIME [epoch: 8.29 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09117878618026345		[learning rate: 0.00024526]
	Learning Rate: 0.000245263
	LOSS [training: 0.09117878618026345 | validation: 0.038791486116485904]
	TIME [epoch: 8.28 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08206053816762018		[learning rate: 0.00024437]
	Learning Rate: 0.000244373
	LOSS [training: 0.08206053816762018 | validation: 0.05292512487227877]
	TIME [epoch: 8.27 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07767009291534624		[learning rate: 0.00024349]
	Learning Rate: 0.000243486
	LOSS [training: 0.07767009291534624 | validation: 0.03777803194255179]
	TIME [epoch: 8.27 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07752477418841426		[learning rate: 0.0002426]
	Learning Rate: 0.000242602
	LOSS [training: 0.07752477418841426 | validation: 0.04417339453500718]
	TIME [epoch: 8.29 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07898516133976977		[learning rate: 0.00024172]
	Learning Rate: 0.000241722
	LOSS [training: 0.07898516133976977 | validation: 0.05945765342459548]
	TIME [epoch: 8.28 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09241707591617201		[learning rate: 0.00024084]
	Learning Rate: 0.000240845
	LOSS [training: 0.09241707591617201 | validation: 0.056432616989969056]
	TIME [epoch: 8.27 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0838017729496998		[learning rate: 0.00023997]
	Learning Rate: 0.000239971
	LOSS [training: 0.0838017729496998 | validation: 0.036815656461970936]
	TIME [epoch: 8.28 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07578669040643234		[learning rate: 0.0002391]
	Learning Rate: 0.0002391
	LOSS [training: 0.07578669040643234 | validation: 0.04477685294341084]
	TIME [epoch: 8.29 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07259603679056585		[learning rate: 0.00023823]
	Learning Rate: 0.000238232
	LOSS [training: 0.07259603679056585 | validation: 0.0500896245576221]
	TIME [epoch: 8.28 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07279464463737335		[learning rate: 0.00023737]
	Learning Rate: 0.000237367
	LOSS [training: 0.07279464463737335 | validation: 0.0561188182127793]
	TIME [epoch: 8.28 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07674298866469192		[learning rate: 0.00023651]
	Learning Rate: 0.000236506
	LOSS [training: 0.07674298866469192 | validation: 0.04303133310735497]
	TIME [epoch: 8.27 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09377185152007007		[learning rate: 0.00023565]
	Learning Rate: 0.000235648
	LOSS [training: 0.09377185152007007 | validation: 0.04261940039074326]
	TIME [epoch: 8.28 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08844947760597711		[learning rate: 0.00023479]
	Learning Rate: 0.000234793
	LOSS [training: 0.08844947760597711 | validation: 0.07631879294168671]
	TIME [epoch: 8.28 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08977680379484446		[learning rate: 0.00023394]
	Learning Rate: 0.00023394
	LOSS [training: 0.08977680379484446 | validation: 0.03963577855579762]
	TIME [epoch: 8.27 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0783288287312067		[learning rate: 0.00023309]
	Learning Rate: 0.000233091
	LOSS [training: 0.0783288287312067 | validation: 0.04666335331536747]
	TIME [epoch: 8.27 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08202601320648494		[learning rate: 0.00023225]
	Learning Rate: 0.000232246
	LOSS [training: 0.08202601320648494 | validation: 0.03657010957998446]
	TIME [epoch: 8.29 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08482503552343708		[learning rate: 0.0002314]
	Learning Rate: 0.000231403
	LOSS [training: 0.08482503552343708 | validation: 0.03996753721904225]
	TIME [epoch: 8.27 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0725576371531931		[learning rate: 0.00023056]
	Learning Rate: 0.000230563
	LOSS [training: 0.0725576371531931 | validation: 0.048394066115955064]
	TIME [epoch: 8.27 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08666151457024118		[learning rate: 0.00022973]
	Learning Rate: 0.000229726
	LOSS [training: 0.08666151457024118 | validation: 0.059470558522284916]
	TIME [epoch: 8.26 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08251142033900606		[learning rate: 0.00022889]
	Learning Rate: 0.000228893
	LOSS [training: 0.08251142033900606 | validation: 0.0413425263719547]
	TIME [epoch: 8.29 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0814709290035657		[learning rate: 0.00022806]
	Learning Rate: 0.000228062
	LOSS [training: 0.0814709290035657 | validation: 0.05993212372393792]
	TIME [epoch: 8.28 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0825600648775204		[learning rate: 0.00022723]
	Learning Rate: 0.000227234
	LOSS [training: 0.0825600648775204 | validation: 0.04377359805242744]
	TIME [epoch: 8.26 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08431932924738679		[learning rate: 0.00022641]
	Learning Rate: 0.00022641
	LOSS [training: 0.08431932924738679 | validation: 0.06986298830281196]
	TIME [epoch: 8.27 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0880786864172962		[learning rate: 0.00022559]
	Learning Rate: 0.000225588
	LOSS [training: 0.0880786864172962 | validation: 0.0426505352634011]
	TIME [epoch: 8.28 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08973167156369757		[learning rate: 0.00022477]
	Learning Rate: 0.000224769
	LOSS [training: 0.08973167156369757 | validation: 0.044512191374676616]
	TIME [epoch: 8.28 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08160214100004905		[learning rate: 0.00022395]
	Learning Rate: 0.000223954
	LOSS [training: 0.08160214100004905 | validation: 0.04931391711038116]
	TIME [epoch: 8.27 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08915731146918654		[learning rate: 0.00022314]
	Learning Rate: 0.000223141
	LOSS [training: 0.08915731146918654 | validation: 0.046630487946054504]
	TIME [epoch: 8.28 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08567617296810692		[learning rate: 0.00022233]
	Learning Rate: 0.000222331
	LOSS [training: 0.08567617296810692 | validation: 0.06395225091055975]
	TIME [epoch: 8.29 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08341215878742253		[learning rate: 0.00022152]
	Learning Rate: 0.000221524
	LOSS [training: 0.08341215878742253 | validation: 0.05385217786092699]
	TIME [epoch: 8.28 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0781849269516006		[learning rate: 0.00022072]
	Learning Rate: 0.00022072
	LOSS [training: 0.0781849269516006 | validation: 0.049615370948933056]
	TIME [epoch: 8.27 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09118654682980841		[learning rate: 0.00021992]
	Learning Rate: 0.000219919
	LOSS [training: 0.09118654682980841 | validation: 0.06039737026598685]
	TIME [epoch: 8.27 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07896335566768162		[learning rate: 0.00021912]
	Learning Rate: 0.000219121
	LOSS [training: 0.07896335566768162 | validation: 0.05049823817543445]
	TIME [epoch: 8.28 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08734151206555593		[learning rate: 0.00021833]
	Learning Rate: 0.000218326
	LOSS [training: 0.08734151206555593 | validation: 0.04668853792751562]
	TIME [epoch: 8.28 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07872069376705905		[learning rate: 0.00021753]
	Learning Rate: 0.000217534
	LOSS [training: 0.07872069376705905 | validation: 0.038961259778977055]
	TIME [epoch: 8.27 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07632489842546313		[learning rate: 0.00021674]
	Learning Rate: 0.000216744
	LOSS [training: 0.07632489842546313 | validation: 0.05236947573077623]
	TIME [epoch: 8.27 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08552682645744072		[learning rate: 0.00021596]
	Learning Rate: 0.000215958
	LOSS [training: 0.08552682645744072 | validation: 0.048969764471776416]
	TIME [epoch: 8.29 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08131874185734753		[learning rate: 0.00021517]
	Learning Rate: 0.000215174
	LOSS [training: 0.08131874185734753 | validation: 0.05065442222754217]
	TIME [epoch: 8.28 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08192528064811078		[learning rate: 0.00021439]
	Learning Rate: 0.000214393
	LOSS [training: 0.08192528064811078 | validation: 0.05615814624291129]
	TIME [epoch: 8.27 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07186583787808531		[learning rate: 0.00021361]
	Learning Rate: 0.000213615
	LOSS [training: 0.07186583787808531 | validation: 0.04721160061906571]
	TIME [epoch: 8.26 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09371093488079371		[learning rate: 0.00021284]
	Learning Rate: 0.00021284
	LOSS [training: 0.09371093488079371 | validation: 0.05627701581446841]
	TIME [epoch: 8.29 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09538377543880022		[learning rate: 0.00021207]
	Learning Rate: 0.000212067
	LOSS [training: 0.09538377543880022 | validation: 0.07326960269177787]
	TIME [epoch: 8.28 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09019009525233308		[learning rate: 0.0002113]
	Learning Rate: 0.000211298
	LOSS [training: 0.09019009525233308 | validation: 0.043878441858447234]
	TIME [epoch: 8.27 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09357582045671273		[learning rate: 0.00021053]
	Learning Rate: 0.000210531
	LOSS [training: 0.09357582045671273 | validation: 0.054081715216473744]
	TIME [epoch: 8.27 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08557605142347847		[learning rate: 0.00020977]
	Learning Rate: 0.000209767
	LOSS [training: 0.08557605142347847 | validation: 0.04508463745638969]
	TIME [epoch: 8.28 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.09033672705893668		[learning rate: 0.00020901]
	Learning Rate: 0.000209006
	LOSS [training: 0.09033672705893668 | validation: 0.05205369072551826]
	TIME [epoch: 8.28 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0823606557643454		[learning rate: 0.00020825]
	Learning Rate: 0.000208247
	LOSS [training: 0.0823606557643454 | validation: 0.04539932113033819]
	TIME [epoch: 8.28 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08541926578338685		[learning rate: 0.00020749]
	Learning Rate: 0.000207491
	LOSS [training: 0.08541926578338685 | validation: 0.05269526255553808]
	TIME [epoch: 8.27 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08098431779201767		[learning rate: 0.00020674]
	Learning Rate: 0.000206738
	LOSS [training: 0.08098431779201767 | validation: 0.045151661705541926]
	TIME [epoch: 8.28 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08141370320878875		[learning rate: 0.00020599]
	Learning Rate: 0.000205988
	LOSS [training: 0.08141370320878875 | validation: 0.042632055240971516]
	TIME [epoch: 8.28 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08458334726647253		[learning rate: 0.00020524]
	Learning Rate: 0.000205241
	LOSS [training: 0.08458334726647253 | validation: 0.0660568309424617]
	TIME [epoch: 8.28 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.080421823891719		[learning rate: 0.0002045]
	Learning Rate: 0.000204496
	LOSS [training: 0.080421823891719 | validation: 0.03807600601089636]
	TIME [epoch: 8.27 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.072132281819999		[learning rate: 0.00020375]
	Learning Rate: 0.000203754
	LOSS [training: 0.072132281819999 | validation: 0.04952106696380541]
	TIME [epoch: 8.29 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07215410378168396		[learning rate: 0.00020301]
	Learning Rate: 0.000203014
	LOSS [training: 0.07215410378168396 | validation: 0.049159229725948314]
	TIME [epoch: 8.28 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08069950752851898		[learning rate: 0.00020228]
	Learning Rate: 0.000202277
	LOSS [training: 0.08069950752851898 | validation: 0.04557672415755143]
	TIME [epoch: 8.28 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08344409493404414		[learning rate: 0.00020154]
	Learning Rate: 0.000201543
	LOSS [training: 0.08344409493404414 | validation: 0.062398369891082064]
	TIME [epoch: 8.28 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07414623939245639		[learning rate: 0.00020081]
	Learning Rate: 0.000200812
	LOSS [training: 0.07414623939245639 | validation: 0.053023058489301333]
	TIME [epoch: 8.29 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08501922041557004		[learning rate: 0.00020008]
	Learning Rate: 0.000200083
	LOSS [training: 0.08501922041557004 | validation: 0.04706118701785145]
	TIME [epoch: 8.28 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07200061764207127		[learning rate: 0.00019936]
	Learning Rate: 0.000199357
	LOSS [training: 0.07200061764207127 | validation: 0.05150049004478656]
	TIME [epoch: 8.27 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07386128944765767		[learning rate: 0.00019863]
	Learning Rate: 0.000198634
	LOSS [training: 0.07386128944765767 | validation: 0.04082520683746861]
	TIME [epoch: 8.27 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08113477981259473		[learning rate: 0.00019791]
	Learning Rate: 0.000197913
	LOSS [training: 0.08113477981259473 | validation: 0.04413941507591051]
	TIME [epoch: 8.29 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07850739924753376		[learning rate: 0.00019719]
	Learning Rate: 0.000197194
	LOSS [training: 0.07850739924753376 | validation: 0.05044703193865626]
	TIME [epoch: 8.28 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0835607899227187		[learning rate: 0.00019648]
	Learning Rate: 0.000196479
	LOSS [training: 0.0835607899227187 | validation: 0.051351819721205866]
	TIME [epoch: 8.27 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08540388134004244		[learning rate: 0.00019577]
	Learning Rate: 0.000195766
	LOSS [training: 0.08540388134004244 | validation: 0.06012506738224056]
	TIME [epoch: 8.27 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07845579962134899		[learning rate: 0.00019506]
	Learning Rate: 0.000195055
	LOSS [training: 0.07845579962134899 | validation: 0.03660383485086537]
	TIME [epoch: 8.28 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08277768182583302		[learning rate: 0.00019435]
	Learning Rate: 0.000194347
	LOSS [training: 0.08277768182583302 | validation: 0.04925543163696622]
	TIME [epoch: 8.29 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0877821344974905		[learning rate: 0.00019364]
	Learning Rate: 0.000193642
	LOSS [training: 0.0877821344974905 | validation: 0.04419241913504338]
	TIME [epoch: 8.27 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08111612770825025		[learning rate: 0.00019294]
	Learning Rate: 0.000192939
	LOSS [training: 0.08111612770825025 | validation: 0.04737439166638675]
	TIME [epoch: 8.27 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08134549827773226		[learning rate: 0.00019224]
	Learning Rate: 0.000192239
	LOSS [training: 0.08134549827773226 | validation: 0.06053865315496353]
	TIME [epoch: 8.28 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07620161876121745		[learning rate: 0.00019154]
	Learning Rate: 0.000191542
	LOSS [training: 0.07620161876121745 | validation: 0.04446257686216128]
	TIME [epoch: 8.29 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.075724488692238		[learning rate: 0.00019085]
	Learning Rate: 0.000190846
	LOSS [training: 0.075724488692238 | validation: 0.054387258543417444]
	TIME [epoch: 8.27 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07501678117416369		[learning rate: 0.00019015]
	Learning Rate: 0.000190154
	LOSS [training: 0.07501678117416369 | validation: 0.04853927009210349]
	TIME [epoch: 8.27 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07953283358463631		[learning rate: 0.00018946]
	Learning Rate: 0.000189464
	LOSS [training: 0.07953283358463631 | validation: 0.04048518451697604]
	TIME [epoch: 8.29 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08466127264723138		[learning rate: 0.00018878]
	Learning Rate: 0.000188776
	LOSS [training: 0.08466127264723138 | validation: 0.061429797093043984]
	TIME [epoch: 8.28 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07717517764071155		[learning rate: 0.00018809]
	Learning Rate: 0.000188091
	LOSS [training: 0.07717517764071155 | validation: 0.04458487314354713]
	TIME [epoch: 8.28 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07953453715047328		[learning rate: 0.00018741]
	Learning Rate: 0.000187409
	LOSS [training: 0.07953453715047328 | validation: 0.06269462975934298]
	TIME [epoch: 8.27 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07537011201962099		[learning rate: 0.00018673]
	Learning Rate: 0.000186729
	LOSS [training: 0.07537011201962099 | validation: 0.05233816093902999]
	TIME [epoch: 8.29 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07186468007981318		[learning rate: 0.00018605]
	Learning Rate: 0.000186051
	LOSS [training: 0.07186468007981318 | validation: 0.04115628088844268]
	TIME [epoch: 8.28 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06881615830900116		[learning rate: 0.00018538]
	Learning Rate: 0.000185376
	LOSS [training: 0.06881615830900116 | validation: 0.036636521439980155]
	TIME [epoch: 8.27 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0725525363488413		[learning rate: 0.0001847]
	Learning Rate: 0.000184703
	LOSS [training: 0.0725525363488413 | validation: 0.04599679871869045]
	TIME [epoch: 8.26 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08028013069636743		[learning rate: 0.00018403]
	Learning Rate: 0.000184033
	LOSS [training: 0.08028013069636743 | validation: 0.04861682726699654]
	TIME [epoch: 8.29 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08277852105974672		[learning rate: 0.00018336]
	Learning Rate: 0.000183365
	LOSS [training: 0.08277852105974672 | validation: 0.049260818697660656]
	TIME [epoch: 8.28 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0792182864398245		[learning rate: 0.0001827]
	Learning Rate: 0.000182699
	LOSS [training: 0.0792182864398245 | validation: 0.029853698631001763]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_1201.pth
	Model improved!!!
EPOCH 1202/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07902545206957694		[learning rate: 0.00018204]
	Learning Rate: 0.000182036
	LOSS [training: 0.07902545206957694 | validation: 0.05011374773750994]
	TIME [epoch: 8.28 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07846654426808423		[learning rate: 0.00018138]
	Learning Rate: 0.000181376
	LOSS [training: 0.07846654426808423 | validation: 0.04345046266999489]
	TIME [epoch: 8.29 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07358056197433073		[learning rate: 0.00018072]
	Learning Rate: 0.000180717
	LOSS [training: 0.07358056197433073 | validation: 0.052576854895738157]
	TIME [epoch: 8.28 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07440156894358245		[learning rate: 0.00018006]
	Learning Rate: 0.000180062
	LOSS [training: 0.07440156894358245 | validation: 0.047354291419608625]
	TIME [epoch: 8.27 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07361695376211783		[learning rate: 0.00017941]
	Learning Rate: 0.000179408
	LOSS [training: 0.07361695376211783 | validation: 0.044551569498921154]
	TIME [epoch: 8.28 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07726076280395451		[learning rate: 0.00017876]
	Learning Rate: 0.000178757
	LOSS [training: 0.07726076280395451 | validation: 0.07386764733684514]
	TIME [epoch: 8.28 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08682529870470268		[learning rate: 0.00017811]
	Learning Rate: 0.000178108
	LOSS [training: 0.08682529870470268 | validation: 0.05020061546027976]
	TIME [epoch: 8.29 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07430266081237523		[learning rate: 0.00017746]
	Learning Rate: 0.000177462
	LOSS [training: 0.07430266081237523 | validation: 0.04174200546438543]
	TIME [epoch: 8.27 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0825429216383031		[learning rate: 0.00017682]
	Learning Rate: 0.000176818
	LOSS [training: 0.0825429216383031 | validation: 0.05762896182676363]
	TIME [epoch: 8.26 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07847197216186486		[learning rate: 0.00017618]
	Learning Rate: 0.000176176
	LOSS [training: 0.07847197216186486 | validation: 0.04471394189977792]
	TIME [epoch: 8.27 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07821352171381593		[learning rate: 0.00017554]
	Learning Rate: 0.000175537
	LOSS [training: 0.07821352171381593 | validation: 0.04266102303151853]
	TIME [epoch: 8.28 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07359517431536931		[learning rate: 0.0001749]
	Learning Rate: 0.0001749
	LOSS [training: 0.07359517431536931 | validation: 0.0697411705679858]
	TIME [epoch: 8.27 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0772406907344863		[learning rate: 0.00017427]
	Learning Rate: 0.000174265
	LOSS [training: 0.0772406907344863 | validation: 0.04764043704740638]
	TIME [epoch: 8.27 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07317327144825873		[learning rate: 0.00017363]
	Learning Rate: 0.000173633
	LOSS [training: 0.07317327144825873 | validation: 0.044620569468099655]
	TIME [epoch: 8.28 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07981131635080864		[learning rate: 0.000173]
	Learning Rate: 0.000173003
	LOSS [training: 0.07981131635080864 | validation: 0.05274428503271832]
	TIME [epoch: 8.28 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06803735250816292		[learning rate: 0.00017237]
	Learning Rate: 0.000172375
	LOSS [training: 0.06803735250816292 | validation: 0.037988618219340776]
	TIME [epoch: 8.27 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07293346611238545		[learning rate: 0.00017175]
	Learning Rate: 0.000171749
	LOSS [training: 0.07293346611238545 | validation: 0.058232240149680536]
	TIME [epoch: 8.27 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07283939311215513		[learning rate: 0.00017113]
	Learning Rate: 0.000171126
	LOSS [training: 0.07283939311215513 | validation: 0.042403640816517596]
	TIME [epoch: 8.27 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08549160222619576		[learning rate: 0.0001705]
	Learning Rate: 0.000170505
	LOSS [training: 0.08549160222619576 | validation: 0.0347890059199948]
	TIME [epoch: 8.29 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08086854897664464		[learning rate: 0.00016989]
	Learning Rate: 0.000169886
	LOSS [training: 0.08086854897664464 | validation: 0.05327821019387169]
	TIME [epoch: 8.27 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08029530508208638		[learning rate: 0.00016927]
	Learning Rate: 0.00016927
	LOSS [training: 0.08029530508208638 | validation: 0.05568853912686137]
	TIME [epoch: 8.28 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07550776012926518		[learning rate: 0.00016866]
	Learning Rate: 0.000168655
	LOSS [training: 0.07550776012926518 | validation: 0.07625399570593691]
	TIME [epoch: 8.27 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08477004119562184		[learning rate: 0.00016804]
	Learning Rate: 0.000168043
	LOSS [training: 0.08477004119562184 | validation: 0.04375904075983984]
	TIME [epoch: 8.29 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07889376654247886		[learning rate: 0.00016743]
	Learning Rate: 0.000167433
	LOSS [training: 0.07889376654247886 | validation: 0.05040536927481086]
	TIME [epoch: 8.27 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07699496352732484		[learning rate: 0.00016683]
	Learning Rate: 0.000166826
	LOSS [training: 0.07699496352732484 | validation: 0.0448597730126339]
	TIME [epoch: 8.27 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08991925168881224		[learning rate: 0.00016622]
	Learning Rate: 0.00016622
	LOSS [training: 0.08991925168881224 | validation: 0.09207967107423284]
	TIME [epoch: 8.28 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08318706141707072		[learning rate: 0.00016562]
	Learning Rate: 0.000165617
	LOSS [training: 0.08318706141707072 | validation: 0.03930145432356498]
	TIME [epoch: 8.29 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0775866638529871		[learning rate: 0.00016502]
	Learning Rate: 0.000165016
	LOSS [training: 0.0775866638529871 | validation: 0.041727915445022024]
	TIME [epoch: 8.27 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07827305568314777		[learning rate: 0.00016442]
	Learning Rate: 0.000164417
	LOSS [training: 0.07827305568314777 | validation: 0.041493045124903816]
	TIME [epoch: 8.27 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0709719812883592		[learning rate: 0.00016382]
	Learning Rate: 0.000163821
	LOSS [training: 0.0709719812883592 | validation: 0.049802160053576405]
	TIME [epoch: 8.28 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0771822398764035		[learning rate: 0.00016323]
	Learning Rate: 0.000163226
	LOSS [training: 0.0771822398764035 | validation: 0.04384051508712761]
	TIME [epoch: 8.28 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08133085948400975		[learning rate: 0.00016263]
	Learning Rate: 0.000162634
	LOSS [training: 0.08133085948400975 | validation: 0.045498454885014555]
	TIME [epoch: 8.27 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07438242952138267		[learning rate: 0.00016204]
	Learning Rate: 0.000162043
	LOSS [training: 0.07438242952138267 | validation: 0.05254085706109133]
	TIME [epoch: 8.27 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0699964314484964		[learning rate: 0.00016146]
	Learning Rate: 0.000161455
	LOSS [training: 0.0699964314484964 | validation: 0.034419636619198814]
	TIME [epoch: 8.28 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08124585366301543		[learning rate: 0.00016087]
	Learning Rate: 0.000160869
	LOSS [training: 0.08124585366301543 | validation: 0.052034504591013823]
	TIME [epoch: 8.29 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07447811410299883		[learning rate: 0.00016029]
	Learning Rate: 0.000160286
	LOSS [training: 0.07447811410299883 | validation: 0.04178575886173032]
	TIME [epoch: 8.27 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08098321132163708		[learning rate: 0.0001597]
	Learning Rate: 0.000159704
	LOSS [training: 0.08098321132163708 | validation: 0.058243461084951015]
	TIME [epoch: 8.27 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07877550008968048		[learning rate: 0.00015912]
	Learning Rate: 0.000159124
	LOSS [training: 0.07877550008968048 | validation: 0.05411142712388234]
	TIME [epoch: 8.28 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07727798884609245		[learning rate: 0.00015855]
	Learning Rate: 0.000158547
	LOSS [training: 0.07727798884609245 | validation: 0.04763188455771412]
	TIME [epoch: 8.3 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07357419010450691		[learning rate: 0.00015797]
	Learning Rate: 0.000157972
	LOSS [training: 0.07357419010450691 | validation: 0.042304977744737425]
	TIME [epoch: 8.27 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0747942817671813		[learning rate: 0.0001574]
	Learning Rate: 0.000157398
	LOSS [training: 0.0747942817671813 | validation: 0.05147622696170534]
	TIME [epoch: 8.27 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0794480815333484		[learning rate: 0.00015683]
	Learning Rate: 0.000156827
	LOSS [training: 0.0794480815333484 | validation: 0.042524320939328425]
	TIME [epoch: 8.28 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08140065258989461		[learning rate: 0.00015626]
	Learning Rate: 0.000156258
	LOSS [training: 0.08140065258989461 | validation: 0.04309755780725232]
	TIME [epoch: 8.29 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08093927468067488		[learning rate: 0.00015569]
	Learning Rate: 0.000155691
	LOSS [training: 0.08093927468067488 | validation: 0.046048520295960196]
	TIME [epoch: 8.27 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07947993205091905		[learning rate: 0.00015513]
	Learning Rate: 0.000155126
	LOSS [training: 0.07947993205091905 | validation: 0.05161860750062827]
	TIME [epoch: 8.27 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06974019495166037		[learning rate: 0.00015456]
	Learning Rate: 0.000154563
	LOSS [training: 0.06974019495166037 | validation: 0.05754149369260533]
	TIME [epoch: 8.28 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07374138454159487		[learning rate: 0.000154]
	Learning Rate: 0.000154002
	LOSS [training: 0.07374138454159487 | validation: 0.03589870285041193]
	TIME [epoch: 8.29 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07458122017485481		[learning rate: 0.00015344]
	Learning Rate: 0.000153443
	LOSS [training: 0.07458122017485481 | validation: 0.0515756078792767]
	TIME [epoch: 8.27 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06938721385918625		[learning rate: 0.00015289]
	Learning Rate: 0.000152886
	LOSS [training: 0.06938721385918625 | validation: 0.0496312442721458]
	TIME [epoch: 8.26 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08192666381074516		[learning rate: 0.00015233]
	Learning Rate: 0.000152331
	LOSS [training: 0.08192666381074516 | validation: 0.05166893705234421]
	TIME [epoch: 8.28 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08075366258006192		[learning rate: 0.00015178]
	Learning Rate: 0.000151779
	LOSS [training: 0.08075366258006192 | validation: 0.04583648498671375]
	TIME [epoch: 8.28 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07490051374649469		[learning rate: 0.00015123]
	Learning Rate: 0.000151228
	LOSS [training: 0.07490051374649469 | validation: 0.03381942984014425]
	TIME [epoch: 8.27 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0722011720534017		[learning rate: 0.00015068]
	Learning Rate: 0.000150679
	LOSS [training: 0.0722011720534017 | validation: 0.03519693225405741]
	TIME [epoch: 8.27 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08062879961396333		[learning rate: 0.00015013]
	Learning Rate: 0.000150132
	LOSS [training: 0.08062879961396333 | validation: 0.05560680303492341]
	TIME [epoch: 8.26 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07005479566395083		[learning rate: 0.00014959]
	Learning Rate: 0.000149587
	LOSS [training: 0.07005479566395083 | validation: 0.04121085077500773]
	TIME [epoch: 8.29 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06490081009525309		[learning rate: 0.00014904]
	Learning Rate: 0.000149044
	LOSS [training: 0.06490081009525309 | validation: 0.04463364542808197]
	TIME [epoch: 8.27 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0750964159288455		[learning rate: 0.0001485]
	Learning Rate: 0.000148504
	LOSS [training: 0.0750964159288455 | validation: 0.06084324710418659]
	TIME [epoch: 8.27 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06867247450889233		[learning rate: 0.00014796]
	Learning Rate: 0.000147965
	LOSS [training: 0.06867247450889233 | validation: 0.04330053907427435]
	TIME [epoch: 8.27 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06954863209026325		[learning rate: 0.00014743]
	Learning Rate: 0.000147428
	LOSS [training: 0.06954863209026325 | validation: 0.04094633023422831]
	TIME [epoch: 8.28 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0735829784890647		[learning rate: 0.00014689]
	Learning Rate: 0.000146893
	LOSS [training: 0.0735829784890647 | validation: 0.04036506050419267]
	TIME [epoch: 8.27 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07633807833594808		[learning rate: 0.00014636]
	Learning Rate: 0.00014636
	LOSS [training: 0.07633807833594808 | validation: 0.053944117116461375]
	TIME [epoch: 8.27 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07394174967141756		[learning rate: 0.00014583]
	Learning Rate: 0.000145828
	LOSS [training: 0.07394174967141756 | validation: 0.05591427797771416]
	TIME [epoch: 8.28 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07181601165505015		[learning rate: 0.0001453]
	Learning Rate: 0.000145299
	LOSS [training: 0.07181601165505015 | validation: 0.04571630140286331]
	TIME [epoch: 8.29 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06920063029794152		[learning rate: 0.00014477]
	Learning Rate: 0.000144772
	LOSS [training: 0.06920063029794152 | validation: 0.037183663380030645]
	TIME [epoch: 8.26 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07433529986815376		[learning rate: 0.00014425]
	Learning Rate: 0.000144247
	LOSS [training: 0.07433529986815376 | validation: 0.04049891264261665]
	TIME [epoch: 8.27 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07056238400073309		[learning rate: 0.00014372]
	Learning Rate: 0.000143723
	LOSS [training: 0.07056238400073309 | validation: 0.04971954187952103]
	TIME [epoch: 8.27 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07821239232537786		[learning rate: 0.0001432]
	Learning Rate: 0.000143201
	LOSS [training: 0.07821239232537786 | validation: 0.0631077617246533]
	TIME [epoch: 8.28 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08584470377568965		[learning rate: 0.00014268]
	Learning Rate: 0.000142682
	LOSS [training: 0.08584470377568965 | validation: 0.03664310750816527]
	TIME [epoch: 8.27 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07699478719586238		[learning rate: 0.00014216]
	Learning Rate: 0.000142164
	LOSS [training: 0.07699478719586238 | validation: 0.046083421469236094]
	TIME [epoch: 8.26 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06799159505988836		[learning rate: 0.00014165]
	Learning Rate: 0.000141648
	LOSS [training: 0.06799159505988836 | validation: 0.045107939933844224]
	TIME [epoch: 8.27 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07718380642717175		[learning rate: 0.00014113]
	Learning Rate: 0.000141134
	LOSS [training: 0.07718380642717175 | validation: 0.049491514453714774]
	TIME [epoch: 8.28 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07140848578113279		[learning rate: 0.00014062]
	Learning Rate: 0.000140622
	LOSS [training: 0.07140848578113279 | validation: 0.043685428666212375]
	TIME [epoch: 8.26 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07396411577860214		[learning rate: 0.00014011]
	Learning Rate: 0.000140112
	LOSS [training: 0.07396411577860214 | validation: 0.04038978805885255]
	TIME [epoch: 8.26 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07617256702056394		[learning rate: 0.0001396]
	Learning Rate: 0.000139603
	LOSS [training: 0.07617256702056394 | validation: 0.04411206271271145]
	TIME [epoch: 8.27 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0729887705820931		[learning rate: 0.0001391]
	Learning Rate: 0.000139096
	LOSS [training: 0.0729887705820931 | validation: 0.04008424221608081]
	TIME [epoch: 8.28 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07404145291877436		[learning rate: 0.00013859]
	Learning Rate: 0.000138592
	LOSS [training: 0.07404145291877436 | validation: 0.04375572193268046]
	TIME [epoch: 8.27 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0799069481174755		[learning rate: 0.00013809]
	Learning Rate: 0.000138089
	LOSS [training: 0.0799069481174755 | validation: 0.049128214481025514]
	TIME [epoch: 8.27 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07375897871354523		[learning rate: 0.00013759]
	Learning Rate: 0.000137587
	LOSS [training: 0.07375897871354523 | validation: 0.041590863142237866]
	TIME [epoch: 8.28 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0713776157919164		[learning rate: 0.00013709]
	Learning Rate: 0.000137088
	LOSS [training: 0.0713776157919164 | validation: 0.03881373206433829]
	TIME [epoch: 8.27 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07422545374768881		[learning rate: 0.00013659]
	Learning Rate: 0.000136591
	LOSS [training: 0.07422545374768881 | validation: 0.04313597652176941]
	TIME [epoch: 8.26 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07587987790620748		[learning rate: 0.00013609]
	Learning Rate: 0.000136095
	LOSS [training: 0.07587987790620748 | validation: 0.05577311995092979]
	TIME [epoch: 8.26 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0734181228628061		[learning rate: 0.0001356]
	Learning Rate: 0.000135601
	LOSS [training: 0.0734181228628061 | validation: 0.04187272336057755]
	TIME [epoch: 8.27 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08394571427336697		[learning rate: 0.00013511]
	Learning Rate: 0.000135109
	LOSS [training: 0.08394571427336697 | validation: 0.03300765270535503]
	TIME [epoch: 8.28 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07965963026459405		[learning rate: 0.00013462]
	Learning Rate: 0.000134619
	LOSS [training: 0.07965963026459405 | validation: 0.04368880072499678]
	TIME [epoch: 8.27 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07517332879483889		[learning rate: 0.00013413]
	Learning Rate: 0.00013413
	LOSS [training: 0.07517332879483889 | validation: 0.039843332060566075]
	TIME [epoch: 8.26 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07763079436676316		[learning rate: 0.00013364]
	Learning Rate: 0.000133643
	LOSS [training: 0.07763079436676316 | validation: 0.04900908994706743]
	TIME [epoch: 8.27 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07597455371893429		[learning rate: 0.00013316]
	Learning Rate: 0.000133158
	LOSS [training: 0.07597455371893429 | validation: 0.03897310830321127]
	TIME [epoch: 8.28 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06914416948225037		[learning rate: 0.00013268]
	Learning Rate: 0.000132675
	LOSS [training: 0.06914416948225037 | validation: 0.04202970673158507]
	TIME [epoch: 8.27 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08377381028332155		[learning rate: 0.00013219]
	Learning Rate: 0.000132194
	LOSS [training: 0.08377381028332155 | validation: 0.0711913881427988]
	TIME [epoch: 8.26 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08570048779995962		[learning rate: 0.00013171]
	Learning Rate: 0.000131714
	LOSS [training: 0.08570048779995962 | validation: 0.0406488243368619]
	TIME [epoch: 8.27 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07411057934270457		[learning rate: 0.00013124]
	Learning Rate: 0.000131236
	LOSS [training: 0.07411057934270457 | validation: 0.02884957956347927]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_1292.pth
	Model improved!!!
EPOCH 1293/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06803344201050263		[learning rate: 0.00013076]
	Learning Rate: 0.00013076
	LOSS [training: 0.06803344201050263 | validation: 0.04200061480045638]
	TIME [epoch: 8.27 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07564988072837894		[learning rate: 0.00013029]
	Learning Rate: 0.000130285
	LOSS [training: 0.07564988072837894 | validation: 0.033743172774438474]
	TIME [epoch: 8.27 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06744162392089796		[learning rate: 0.00012981]
	Learning Rate: 0.000129812
	LOSS [training: 0.06744162392089796 | validation: 0.03813010439254437]
	TIME [epoch: 8.27 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07360089460444347		[learning rate: 0.00012934]
	Learning Rate: 0.000129341
	LOSS [training: 0.07360089460444347 | validation: 0.030232096367017364]
	TIME [epoch: 8.29 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06438755038729406		[learning rate: 0.00012887]
	Learning Rate: 0.000128872
	LOSS [training: 0.06438755038729406 | validation: 0.034132978089335055]
	TIME [epoch: 8.27 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07378904441289351		[learning rate: 0.0001284]
	Learning Rate: 0.000128404
	LOSS [training: 0.07378904441289351 | validation: 0.03505257058793161]
	TIME [epoch: 8.31 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07492200673877891		[learning rate: 0.00012794]
	Learning Rate: 0.000127938
	LOSS [training: 0.07492200673877891 | validation: 0.04535985880357708]
	TIME [epoch: 8.26 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07689329026759659		[learning rate: 0.00012747]
	Learning Rate: 0.000127474
	LOSS [training: 0.07689329026759659 | validation: 0.03912426241348796]
	TIME [epoch: 8.29 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06745868065592864		[learning rate: 0.00012701]
	Learning Rate: 0.000127011
	LOSS [training: 0.06745868065592864 | validation: 0.03715625439521659]
	TIME [epoch: 8.27 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07144064527685048		[learning rate: 0.00012655]
	Learning Rate: 0.00012655
	LOSS [training: 0.07144064527685048 | validation: 0.03448841465548391]
	TIME [epoch: 8.27 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06922645906267845		[learning rate: 0.00012609]
	Learning Rate: 0.000126091
	LOSS [training: 0.06922645906267845 | validation: 0.055945476029484006]
	TIME [epoch: 8.27 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07168033944305467		[learning rate: 0.00012563]
	Learning Rate: 0.000125633
	LOSS [training: 0.07168033944305467 | validation: 0.04288024885591708]
	TIME [epoch: 8.29 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06732460131414844		[learning rate: 0.00012518]
	Learning Rate: 0.000125178
	LOSS [training: 0.06732460131414844 | validation: 0.06777092873703547]
	TIME [epoch: 8.27 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.08250109186112706		[learning rate: 0.00012472]
	Learning Rate: 0.000124723
	LOSS [training: 0.08250109186112706 | validation: 0.05379728429493663]
	TIME [epoch: 8.27 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0768187181047931		[learning rate: 0.00012427]
	Learning Rate: 0.000124271
	LOSS [training: 0.0768187181047931 | validation: 0.05150206353207108]
	TIME [epoch: 8.27 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0808324725541648		[learning rate: 0.00012382]
	Learning Rate: 0.00012382
	LOSS [training: 0.0808324725541648 | validation: 0.03248984538402603]
	TIME [epoch: 8.29 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06768910593114721		[learning rate: 0.00012337]
	Learning Rate: 0.00012337
	LOSS [training: 0.06768910593114721 | validation: 0.03774450775636856]
	TIME [epoch: 8.27 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06434081721451558		[learning rate: 0.00012292]
	Learning Rate: 0.000122923
	LOSS [training: 0.06434081721451558 | validation: 0.04363144959906674]
	TIME [epoch: 8.27 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06812343641831828		[learning rate: 0.00012248]
	Learning Rate: 0.000122476
	LOSS [training: 0.06812343641831828 | validation: 0.04128963233470015]
	TIME [epoch: 8.27 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06927260718080096		[learning rate: 0.00012203]
	Learning Rate: 0.000122032
	LOSS [training: 0.06927260718080096 | validation: 0.04005170063522544]
	TIME [epoch: 8.3 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07090572741364234		[learning rate: 0.00012159]
	Learning Rate: 0.000121589
	LOSS [training: 0.07090572741364234 | validation: 0.041970988940017946]
	TIME [epoch: 8.27 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07033102942523209		[learning rate: 0.00012115]
	Learning Rate: 0.000121148
	LOSS [training: 0.07033102942523209 | validation: 0.03905722058330929]
	TIME [epoch: 8.27 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0708756918755602		[learning rate: 0.00012071]
	Learning Rate: 0.000120708
	LOSS [training: 0.0708756918755602 | validation: 0.043988937560583544]
	TIME [epoch: 8.27 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06257808439983145		[learning rate: 0.00012027]
	Learning Rate: 0.00012027
	LOSS [training: 0.06257808439983145 | validation: 0.03244368479892567]
	TIME [epoch: 8.29 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06871095827389034		[learning rate: 0.00011983]
	Learning Rate: 0.000119834
	LOSS [training: 0.06871095827389034 | validation: 0.04674290223113123]
	TIME [epoch: 8.27 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07145347831062002		[learning rate: 0.0001194]
	Learning Rate: 0.000119399
	LOSS [training: 0.07145347831062002 | validation: 0.04842987829733632]
	TIME [epoch: 8.28 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07420917459365306		[learning rate: 0.00011897]
	Learning Rate: 0.000118966
	LOSS [training: 0.07420917459365306 | validation: 0.050196804249994315]
	TIME [epoch: 8.27 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07100173312150679		[learning rate: 0.00011853]
	Learning Rate: 0.000118534
	LOSS [training: 0.07100173312150679 | validation: 0.03234607240233217]
	TIME [epoch: 8.29 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06813578107200476		[learning rate: 0.0001181]
	Learning Rate: 0.000118104
	LOSS [training: 0.06813578107200476 | validation: 0.04426439633801396]
	TIME [epoch: 8.28 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07130581420124198		[learning rate: 0.00011767]
	Learning Rate: 0.000117675
	LOSS [training: 0.07130581420124198 | validation: 0.041565775661124085]
	TIME [epoch: 8.27 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06908121508738954		[learning rate: 0.00011725]
	Learning Rate: 0.000117248
	LOSS [training: 0.06908121508738954 | validation: 0.04278036979877717]
	TIME [epoch: 8.27 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07019301452080683		[learning rate: 0.00011682]
	Learning Rate: 0.000116822
	LOSS [training: 0.07019301452080683 | validation: 0.03476406840098427]
	TIME [epoch: 8.29 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06974666680911898		[learning rate: 0.0001164]
	Learning Rate: 0.000116398
	LOSS [training: 0.06974666680911898 | validation: 0.05282724205329565]
	TIME [epoch: 8.28 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06908744349617885		[learning rate: 0.00011598]
	Learning Rate: 0.000115976
	LOSS [training: 0.06908744349617885 | validation: 0.03833427497027554]
	TIME [epoch: 8.27 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06557967400843898		[learning rate: 0.00011556]
	Learning Rate: 0.000115555
	LOSS [training: 0.06557967400843898 | validation: 0.040184662669503005]
	TIME [epoch: 8.27 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07142143461464703		[learning rate: 0.00011514]
	Learning Rate: 0.000115136
	LOSS [training: 0.07142143461464703 | validation: 0.04185888318084516]
	TIME [epoch: 8.29 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06219586557349318		[learning rate: 0.00011472]
	Learning Rate: 0.000114718
	LOSS [training: 0.06219586557349318 | validation: 0.03402778799193268]
	TIME [epoch: 8.27 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06872380809694797		[learning rate: 0.0001143]
	Learning Rate: 0.000114302
	LOSS [training: 0.06872380809694797 | validation: 0.038688361698433676]
	TIME [epoch: 8.28 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07059388561216771		[learning rate: 0.00011389]
	Learning Rate: 0.000113887
	LOSS [training: 0.07059388561216771 | validation: 0.04822535877951202]
	TIME [epoch: 8.27 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06239531117099314		[learning rate: 0.00011347]
	Learning Rate: 0.000113474
	LOSS [training: 0.06239531117099314 | validation: 0.03510004211652065]
	TIME [epoch: 8.29 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06719165397415405		[learning rate: 0.00011306]
	Learning Rate: 0.000113062
	LOSS [training: 0.06719165397415405 | validation: 0.024740109955814403]
	TIME [epoch: 8.27 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_1333.pth
	Model improved!!!
EPOCH 1334/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06360508971778756		[learning rate: 0.00011265]
	Learning Rate: 0.000112651
	LOSS [training: 0.06360508971778756 | validation: 0.03843421470781991]
	TIME [epoch: 8.27 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06998187762363073		[learning rate: 0.00011224]
	Learning Rate: 0.000112243
	LOSS [training: 0.06998187762363073 | validation: 0.03963038191136023]
	TIME [epoch: 8.27 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07300516032814833		[learning rate: 0.00011184]
	Learning Rate: 0.000111835
	LOSS [training: 0.07300516032814833 | validation: 0.03227036785185967]
	TIME [epoch: 8.28 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07299012898894576		[learning rate: 0.00011143]
	Learning Rate: 0.000111429
	LOSS [training: 0.07299012898894576 | validation: 0.04308181741971155]
	TIME [epoch: 8.27 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07405971627585506		[learning rate: 0.00011103]
	Learning Rate: 0.000111025
	LOSS [training: 0.07405971627585506 | validation: 0.046006619135302816]
	TIME [epoch: 8.27 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05780332449585805		[learning rate: 0.00011062]
	Learning Rate: 0.000110622
	LOSS [training: 0.05780332449585805 | validation: 0.03899771931301475]
	TIME [epoch: 8.27 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06704715080509867		[learning rate: 0.00011022]
	Learning Rate: 0.000110221
	LOSS [training: 0.06704715080509867 | validation: 0.04382262885529117]
	TIME [epoch: 8.29 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06422730046899441		[learning rate: 0.00010982]
	Learning Rate: 0.000109821
	LOSS [training: 0.06422730046899441 | validation: 0.029768209138821146]
	TIME [epoch: 8.28 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07072463972909555		[learning rate: 0.00010942]
	Learning Rate: 0.000109422
	LOSS [training: 0.07072463972909555 | validation: 0.04791936936510723]
	TIME [epoch: 8.27 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06364232812929585		[learning rate: 0.00010903]
	Learning Rate: 0.000109025
	LOSS [training: 0.06364232812929585 | validation: 0.037433010726703686]
	TIME [epoch: 8.27 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06285013465867326		[learning rate: 0.00010863]
	Learning Rate: 0.000108629
	LOSS [training: 0.06285013465867326 | validation: 0.04540770685561406]
	TIME [epoch: 8.3 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06144952321656194		[learning rate: 0.00010824]
	Learning Rate: 0.000108235
	LOSS [training: 0.06144952321656194 | validation: 0.032511693832921705]
	TIME [epoch: 8.27 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05972847500973568		[learning rate: 0.00010784]
	Learning Rate: 0.000107842
	LOSS [training: 0.05972847500973568 | validation: 0.047388512297313265]
	TIME [epoch: 8.27 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06307564157986112		[learning rate: 0.00010745]
	Learning Rate: 0.000107451
	LOSS [training: 0.06307564157986112 | validation: 0.03509909553361938]
	TIME [epoch: 8.27 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07882841262946834		[learning rate: 0.00010706]
	Learning Rate: 0.000107061
	LOSS [training: 0.07882841262946834 | validation: 0.04765551035987289]
	TIME [epoch: 8.29 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07097727550764424		[learning rate: 0.00010667]
	Learning Rate: 0.000106673
	LOSS [training: 0.07097727550764424 | validation: 0.03337529871746799]
	TIME [epoch: 8.28 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.060482111891650606		[learning rate: 0.00010629]
	Learning Rate: 0.000106285
	LOSS [training: 0.060482111891650606 | validation: 0.031114805794178788]
	TIME [epoch: 8.28 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07355682673941828		[learning rate: 0.0001059]
	Learning Rate: 0.0001059
	LOSS [training: 0.07355682673941828 | validation: 0.03960045653968471]
	TIME [epoch: 8.27 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07187616393915532		[learning rate: 0.00010552]
	Learning Rate: 0.000105515
	LOSS [training: 0.07187616393915532 | validation: 0.0397868205912873]
	TIME [epoch: 8.29 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06682176732156973		[learning rate: 0.00010513]
	Learning Rate: 0.000105132
	LOSS [training: 0.06682176732156973 | validation: 0.0309755680069674]
	TIME [epoch: 8.28 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06413912390927576		[learning rate: 0.00010475]
	Learning Rate: 0.000104751
	LOSS [training: 0.06413912390927576 | validation: 0.03402424408555069]
	TIME [epoch: 8.27 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06721271305782138		[learning rate: 0.00010437]
	Learning Rate: 0.000104371
	LOSS [training: 0.06721271305782138 | validation: 0.052327147426556204]
	TIME [epoch: 8.27 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05865154161302148		[learning rate: 0.00010399]
	Learning Rate: 0.000103992
	LOSS [training: 0.05865154161302148 | validation: 0.03303537595442358]
	TIME [epoch: 8.29 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07033926060454072		[learning rate: 0.00010361]
	Learning Rate: 0.000103615
	LOSS [training: 0.07033926060454072 | validation: 0.0403047990570493]
	TIME [epoch: 8.28 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06470432986183569		[learning rate: 0.00010324]
	Learning Rate: 0.000103239
	LOSS [training: 0.06470432986183569 | validation: 0.04763746377897708]
	TIME [epoch: 8.27 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07147957963264079		[learning rate: 0.00010286]
	Learning Rate: 0.000102864
	LOSS [training: 0.07147957963264079 | validation: 0.05250152523184322]
	TIME [epoch: 8.27 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0693420820526027		[learning rate: 0.00010249]
	Learning Rate: 0.000102491
	LOSS [training: 0.0693420820526027 | validation: 0.047173177482810316]
	TIME [epoch: 8.29 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07093256325946933		[learning rate: 0.00010212]
	Learning Rate: 0.000102119
	LOSS [training: 0.07093256325946933 | validation: 0.04514873898602407]
	TIME [epoch: 8.27 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06516637901977462		[learning rate: 0.00010175]
	Learning Rate: 0.000101748
	LOSS [training: 0.06516637901977462 | validation: 0.040148614861614335]
	TIME [epoch: 8.27 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0685522528032879		[learning rate: 0.00010138]
	Learning Rate: 0.000101379
	LOSS [training: 0.0685522528032879 | validation: 0.03528002375630236]
	TIME [epoch: 8.26 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05995592198010663		[learning rate: 0.00010101]
	Learning Rate: 0.000101011
	LOSS [training: 0.05995592198010663 | validation: 0.03828448747205467]
	TIME [epoch: 8.29 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06482572477701959		[learning rate: 0.00010064]
	Learning Rate: 0.000100644
	LOSS [training: 0.06482572477701959 | validation: 0.031174729472946923]
	TIME [epoch: 8.28 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06413458717422159		[learning rate: 0.00010028]
	Learning Rate: 0.000100279
	LOSS [training: 0.06413458717422159 | validation: 0.04700077748238174]
	TIME [epoch: 8.27 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06740459703038339		[learning rate: 9.9915e-05]
	Learning Rate: 9.99152e-05
	LOSS [training: 0.06740459703038339 | validation: 0.04025984426293403]
	TIME [epoch: 8.26 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0679903352171411		[learning rate: 9.9553e-05]
	Learning Rate: 9.95526e-05
	LOSS [training: 0.0679903352171411 | validation: 0.03900578141108822]
	TIME [epoch: 8.29 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06020275783909512		[learning rate: 9.9191e-05]
	Learning Rate: 9.91913e-05
	LOSS [training: 0.06020275783909512 | validation: 0.05026073058514363]
	TIME [epoch: 8.27 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06697313603822273		[learning rate: 9.8831e-05]
	Learning Rate: 9.88314e-05
	LOSS [training: 0.06697313603822273 | validation: 0.03776831030264026]
	TIME [epoch: 8.26 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06024155075828769		[learning rate: 9.8473e-05]
	Learning Rate: 9.84727e-05
	LOSS [training: 0.06024155075828769 | validation: 0.04489636993621483]
	TIME [epoch: 8.27 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06265309024861891		[learning rate: 9.8115e-05]
	Learning Rate: 9.81153e-05
	LOSS [training: 0.06265309024861891 | validation: 0.02838948399923402]
	TIME [epoch: 8.29 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06640799424464959		[learning rate: 9.7759e-05]
	Learning Rate: 9.77592e-05
	LOSS [training: 0.06640799424464959 | validation: 0.0420928988831437]
	TIME [epoch: 8.27 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.057986912380519715		[learning rate: 9.7404e-05]
	Learning Rate: 9.74045e-05
	LOSS [training: 0.057986912380519715 | validation: 0.041338775876326776]
	TIME [epoch: 8.27 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0739914333007212		[learning rate: 9.7051e-05]
	Learning Rate: 9.7051e-05
	LOSS [training: 0.0739914333007212 | validation: 0.04906589760839285]
	TIME [epoch: 8.26 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0694307274780562		[learning rate: 9.6699e-05]
	Learning Rate: 9.66988e-05
	LOSS [training: 0.0694307274780562 | validation: 0.03998231935321507]
	TIME [epoch: 8.28 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07041673746861438		[learning rate: 9.6348e-05]
	Learning Rate: 9.63479e-05
	LOSS [training: 0.07041673746861438 | validation: 0.04293030993042255]
	TIME [epoch: 8.27 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06502667384242292		[learning rate: 9.5998e-05]
	Learning Rate: 9.59982e-05
	LOSS [training: 0.06502667384242292 | validation: 0.03350514061369704]
	TIME [epoch: 8.26 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06002120908770159		[learning rate: 9.565e-05]
	Learning Rate: 9.56499e-05
	LOSS [training: 0.06002120908770159 | validation: 0.03250200102500492]
	TIME [epoch: 8.26 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06693352666305749		[learning rate: 9.5303e-05]
	Learning Rate: 9.53027e-05
	LOSS [training: 0.06693352666305749 | validation: 0.03909387552817821]
	TIME [epoch: 8.28 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06793992496700649		[learning rate: 9.4957e-05]
	Learning Rate: 9.49569e-05
	LOSS [training: 0.06793992496700649 | validation: 0.04459268755007571]
	TIME [epoch: 8.28 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06512306425635699		[learning rate: 9.4612e-05]
	Learning Rate: 9.46122e-05
	LOSS [training: 0.06512306425635699 | validation: 0.04345940507229758]
	TIME [epoch: 8.27 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06955380431544564		[learning rate: 9.4269e-05]
	Learning Rate: 9.42689e-05
	LOSS [training: 0.06955380431544564 | validation: 0.038620145567614304]
	TIME [epoch: 8.27 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06632165760297296		[learning rate: 9.3927e-05]
	Learning Rate: 9.39268e-05
	LOSS [training: 0.06632165760297296 | validation: 0.03669296248439383]
	TIME [epoch: 8.29 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.059358394317277983		[learning rate: 9.3586e-05]
	Learning Rate: 9.35859e-05
	LOSS [training: 0.059358394317277983 | validation: 0.030446341361623865]
	TIME [epoch: 8.27 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06039637062102525		[learning rate: 9.3246e-05]
	Learning Rate: 9.32463e-05
	LOSS [training: 0.06039637062102525 | validation: 0.05461553488630522]
	TIME [epoch: 8.27 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.060116151333394076		[learning rate: 9.2908e-05]
	Learning Rate: 9.29079e-05
	LOSS [training: 0.060116151333394076 | validation: 0.026615116029381238]
	TIME [epoch: 8.26 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.067840376677541		[learning rate: 9.2571e-05]
	Learning Rate: 9.25707e-05
	LOSS [training: 0.067840376677541 | validation: 0.04452950340643838]
	TIME [epoch: 8.28 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.061698828405411875		[learning rate: 9.2235e-05]
	Learning Rate: 9.22348e-05
	LOSS [training: 0.061698828405411875 | validation: 0.04051666055711528]
	TIME [epoch: 8.27 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07171899472773166		[learning rate: 9.19e-05]
	Learning Rate: 9.19001e-05
	LOSS [training: 0.07171899472773166 | validation: 0.031513882377324]
	TIME [epoch: 8.26 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06179593816623223		[learning rate: 9.1567e-05]
	Learning Rate: 9.15665e-05
	LOSS [training: 0.06179593816623223 | validation: 0.037002763059861214]
	TIME [epoch: 8.26 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06447056558117022		[learning rate: 9.1234e-05]
	Learning Rate: 9.12343e-05
	LOSS [training: 0.06447056558117022 | validation: 0.055875925420098005]
	TIME [epoch: 8.28 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06612946297179516		[learning rate: 9.0903e-05]
	Learning Rate: 9.09031e-05
	LOSS [training: 0.06612946297179516 | validation: 0.04243274571214953]
	TIME [epoch: 8.27 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06182199174583982		[learning rate: 9.0573e-05]
	Learning Rate: 9.05733e-05
	LOSS [training: 0.06182199174583982 | validation: 0.03390126851969672]
	TIME [epoch: 8.27 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.060325081973895536		[learning rate: 9.0245e-05]
	Learning Rate: 9.02446e-05
	LOSS [training: 0.060325081973895536 | validation: 0.039948826001937277]
	TIME [epoch: 8.28 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06693044665735746		[learning rate: 8.9917e-05]
	Learning Rate: 8.99171e-05
	LOSS [training: 0.06693044665735746 | validation: 0.04211112220606664]
	TIME [epoch: 8.29 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06288566285292534		[learning rate: 8.9591e-05]
	Learning Rate: 8.95908e-05
	LOSS [training: 0.06288566285292534 | validation: 0.04351863383464564]
	TIME [epoch: 8.28 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06791293871447045		[learning rate: 8.9266e-05]
	Learning Rate: 8.92656e-05
	LOSS [training: 0.06791293871447045 | validation: 0.045553933918121584]
	TIME [epoch: 8.27 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06637467460209936		[learning rate: 8.8942e-05]
	Learning Rate: 8.89417e-05
	LOSS [training: 0.06637467460209936 | validation: 0.04321667971010401]
	TIME [epoch: 8.27 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05898330457941273		[learning rate: 8.8619e-05]
	Learning Rate: 8.86189e-05
	LOSS [training: 0.05898330457941273 | validation: 0.03465784390283681]
	TIME [epoch: 8.29 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0602273299731392		[learning rate: 8.8297e-05]
	Learning Rate: 8.82973e-05
	LOSS [training: 0.0602273299731392 | validation: 0.03227800761459756]
	TIME [epoch: 8.28 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06218112128305435		[learning rate: 8.7977e-05]
	Learning Rate: 8.79769e-05
	LOSS [training: 0.06218112128305435 | validation: 0.052851580915795504]
	TIME [epoch: 8.27 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06428554485917409		[learning rate: 8.7658e-05]
	Learning Rate: 8.76576e-05
	LOSS [training: 0.06428554485917409 | validation: 0.043649273824608814]
	TIME [epoch: 8.27 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.061581605623690014		[learning rate: 8.7339e-05]
	Learning Rate: 8.73395e-05
	LOSS [training: 0.061581605623690014 | validation: 0.03876412615754224]
	TIME [epoch: 8.29 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.059265852912697405		[learning rate: 8.7023e-05]
	Learning Rate: 8.70225e-05
	LOSS [training: 0.059265852912697405 | validation: 0.05355843254774669]
	TIME [epoch: 8.28 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0726903186367843		[learning rate: 8.6707e-05]
	Learning Rate: 8.67067e-05
	LOSS [training: 0.0726903186367843 | validation: 0.0673449747866634]
	TIME [epoch: 8.26 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06378465830934936		[learning rate: 8.6392e-05]
	Learning Rate: 8.63921e-05
	LOSS [training: 0.06378465830934936 | validation: 0.03939137281985955]
	TIME [epoch: 8.26 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06468677892598315		[learning rate: 8.6079e-05]
	Learning Rate: 8.60785e-05
	LOSS [training: 0.06468677892598315 | validation: 0.038358623885340774]
	TIME [epoch: 8.29 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.059457129612392204		[learning rate: 8.5766e-05]
	Learning Rate: 8.57661e-05
	LOSS [training: 0.059457129612392204 | validation: 0.039368803729119106]
	TIME [epoch: 8.27 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06540744885273636		[learning rate: 8.5455e-05]
	Learning Rate: 8.54549e-05
	LOSS [training: 0.06540744885273636 | validation: 0.03407599207089212]
	TIME [epoch: 8.27 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06691005832889121		[learning rate: 8.5145e-05]
	Learning Rate: 8.51448e-05
	LOSS [training: 0.06691005832889121 | validation: 0.03484713986061405]
	TIME [epoch: 8.27 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06546928316451026		[learning rate: 8.4836e-05]
	Learning Rate: 8.48358e-05
	LOSS [training: 0.06546928316451026 | validation: 0.031808978412462746]
	TIME [epoch: 8.28 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.057798272723249264		[learning rate: 8.4528e-05]
	Learning Rate: 8.45279e-05
	LOSS [training: 0.057798272723249264 | validation: 0.031470257337401976]
	TIME [epoch: 8.28 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06173133222018924		[learning rate: 8.4221e-05]
	Learning Rate: 8.42211e-05
	LOSS [training: 0.06173133222018924 | validation: 0.036181885154921864]
	TIME [epoch: 8.27 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0587819583572491		[learning rate: 8.3916e-05]
	Learning Rate: 8.39155e-05
	LOSS [training: 0.0587819583572491 | validation: 0.03139122119845855]
	TIME [epoch: 8.27 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06158194425918988		[learning rate: 8.3611e-05]
	Learning Rate: 8.3611e-05
	LOSS [training: 0.06158194425918988 | validation: 0.04347054239937842]
	TIME [epoch: 8.29 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.058416316262080145		[learning rate: 8.3308e-05]
	Learning Rate: 8.33075e-05
	LOSS [training: 0.058416316262080145 | validation: 0.029774467954584084]
	TIME [epoch: 8.27 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06267179524228081		[learning rate: 8.3005e-05]
	Learning Rate: 8.30052e-05
	LOSS [training: 0.06267179524228081 | validation: 0.03601487112485437]
	TIME [epoch: 8.26 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.062158828026305166		[learning rate: 8.2704e-05]
	Learning Rate: 8.2704e-05
	LOSS [training: 0.062158828026305166 | validation: 0.04155183427078478]
	TIME [epoch: 8.27 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05917581537785634		[learning rate: 8.2404e-05]
	Learning Rate: 8.24038e-05
	LOSS [training: 0.05917581537785634 | validation: 0.03724209025856573]
	TIME [epoch: 8.29 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0634072504817923		[learning rate: 8.2105e-05]
	Learning Rate: 8.21048e-05
	LOSS [training: 0.0634072504817923 | validation: 0.04524397157360069]
	TIME [epoch: 8.28 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.060994266256860094		[learning rate: 8.1807e-05]
	Learning Rate: 8.18068e-05
	LOSS [training: 0.060994266256860094 | validation: 0.03884146622337595]
	TIME [epoch: 8.27 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05669760808285943		[learning rate: 8.151e-05]
	Learning Rate: 8.15099e-05
	LOSS [training: 0.05669760808285943 | validation: 0.03535341483248971]
	TIME [epoch: 8.27 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.057815832386456725		[learning rate: 8.1214e-05]
	Learning Rate: 8.12142e-05
	LOSS [training: 0.057815832386456725 | validation: 0.03660743896970799]
	TIME [epoch: 8.29 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05844050954507353		[learning rate: 8.0919e-05]
	Learning Rate: 8.09194e-05
	LOSS [training: 0.05844050954507353 | validation: 0.04939442884759775]
	TIME [epoch: 8.28 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.061525655040752816		[learning rate: 8.0626e-05]
	Learning Rate: 8.06257e-05
	LOSS [training: 0.061525655040752816 | validation: 0.03498193480522428]
	TIME [epoch: 8.28 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06382260790168226		[learning rate: 8.0333e-05]
	Learning Rate: 8.03331e-05
	LOSS [training: 0.06382260790168226 | validation: 0.0384949829617983]
	TIME [epoch: 8.27 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.057762004527586105		[learning rate: 8.0042e-05]
	Learning Rate: 8.00416e-05
	LOSS [training: 0.057762004527586105 | validation: 0.04003644042174048]
	TIME [epoch: 8.28 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06191683514417247		[learning rate: 7.9751e-05]
	Learning Rate: 7.97511e-05
	LOSS [training: 0.06191683514417247 | validation: 0.033240733875762735]
	TIME [epoch: 8.28 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05767346137173256		[learning rate: 7.9462e-05]
	Learning Rate: 7.94617e-05
	LOSS [training: 0.05767346137173256 | validation: 0.03893144521918134]
	TIME [epoch: 8.26 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05744172593402323		[learning rate: 7.9173e-05]
	Learning Rate: 7.91733e-05
	LOSS [training: 0.05744172593402323 | validation: 0.04220645233399116]
	TIME [epoch: 8.27 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.059045751794410906		[learning rate: 7.8886e-05]
	Learning Rate: 7.8886e-05
	LOSS [training: 0.059045751794410906 | validation: 0.0404352267327072]
	TIME [epoch: 8.28 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06001995882111192		[learning rate: 7.86e-05]
	Learning Rate: 7.85997e-05
	LOSS [training: 0.06001995882111192 | validation: 0.03954024510981857]
	TIME [epoch: 8.27 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05763940232409705		[learning rate: 7.8315e-05]
	Learning Rate: 7.83145e-05
	LOSS [training: 0.05763940232409705 | validation: 0.04436624398681062]
	TIME [epoch: 8.26 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.061883621958876846		[learning rate: 7.803e-05]
	Learning Rate: 7.80303e-05
	LOSS [training: 0.061883621958876846 | validation: 0.037240971609893786]
	TIME [epoch: 8.27 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06394691180342635		[learning rate: 7.7747e-05]
	Learning Rate: 7.77471e-05
	LOSS [training: 0.06394691180342635 | validation: 0.0447850394621171]
	TIME [epoch: 8.28 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06086712032661108		[learning rate: 7.7465e-05]
	Learning Rate: 7.7465e-05
	LOSS [training: 0.06086712032661108 | validation: 0.0470092112490014]
	TIME [epoch: 8.27 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05513149369079684		[learning rate: 7.7184e-05]
	Learning Rate: 7.71838e-05
	LOSS [training: 0.05513149369079684 | validation: 0.050594571735012916]
	TIME [epoch: 8.27 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05892017476074861		[learning rate: 7.6904e-05]
	Learning Rate: 7.69037e-05
	LOSS [training: 0.05892017476074861 | validation: 0.03700990425881112]
	TIME [epoch: 8.27 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05915126095694022		[learning rate: 7.6625e-05]
	Learning Rate: 7.66246e-05
	LOSS [training: 0.05915126095694022 | validation: 0.04230734430346886]
	TIME [epoch: 8.29 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05922862830731011		[learning rate: 7.6347e-05]
	Learning Rate: 7.63466e-05
	LOSS [training: 0.05922862830731011 | validation: 0.032668727007533906]
	TIME [epoch: 8.27 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06251485797517806		[learning rate: 7.607e-05]
	Learning Rate: 7.60695e-05
	LOSS [training: 0.06251485797517806 | validation: 0.036669457980454415]
	TIME [epoch: 8.26 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06458232867850762		[learning rate: 7.5793e-05]
	Learning Rate: 7.57935e-05
	LOSS [training: 0.06458232867850762 | validation: 0.03418217918988999]
	TIME [epoch: 8.26 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06456945265633815		[learning rate: 7.5518e-05]
	Learning Rate: 7.55184e-05
	LOSS [training: 0.06456945265633815 | validation: 0.04141391349852618]
	TIME [epoch: 8.29 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06379735730322988		[learning rate: 7.5244e-05]
	Learning Rate: 7.52443e-05
	LOSS [training: 0.06379735730322988 | validation: 0.033373415360103326]
	TIME [epoch: 8.27 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05568531819891971		[learning rate: 7.4971e-05]
	Learning Rate: 7.49712e-05
	LOSS [training: 0.05568531819891971 | validation: 0.031149579313092528]
	TIME [epoch: 8.26 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06245708628767064		[learning rate: 7.4699e-05]
	Learning Rate: 7.46992e-05
	LOSS [training: 0.06245708628767064 | validation: 0.03784859030395858]
	TIME [epoch: 8.26 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05930532827562544		[learning rate: 7.4428e-05]
	Learning Rate: 7.44281e-05
	LOSS [training: 0.05930532827562544 | validation: 0.04018644368382086]
	TIME [epoch: 8.29 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.057503777803498225		[learning rate: 7.4158e-05]
	Learning Rate: 7.4158e-05
	LOSS [training: 0.057503777803498225 | validation: 0.03965556207361942]
	TIME [epoch: 8.27 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.060277248990180055		[learning rate: 7.3889e-05]
	Learning Rate: 7.38889e-05
	LOSS [training: 0.060277248990180055 | validation: 0.05005508523458708]
	TIME [epoch: 8.26 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06058186027932703		[learning rate: 7.3621e-05]
	Learning Rate: 7.36207e-05
	LOSS [training: 0.06058186027932703 | validation: 0.04316330210642094]
	TIME [epoch: 8.27 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.061408878547023		[learning rate: 7.3354e-05]
	Learning Rate: 7.33536e-05
	LOSS [training: 0.061408878547023 | validation: 0.040378873874195996]
	TIME [epoch: 8.28 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06242669996668641		[learning rate: 7.3087e-05]
	Learning Rate: 7.30873e-05
	LOSS [training: 0.06242669996668641 | validation: 0.03192046419376259]
	TIME [epoch: 8.28 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05923550013081238		[learning rate: 7.2822e-05]
	Learning Rate: 7.28221e-05
	LOSS [training: 0.05923550013081238 | validation: 0.030734621964867856]
	TIME [epoch: 8.27 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06332943224841517		[learning rate: 7.2558e-05]
	Learning Rate: 7.25578e-05
	LOSS [training: 0.06332943224841517 | validation: 0.047582419444812854]
	TIME [epoch: 8.28 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06429332963309495		[learning rate: 7.2295e-05]
	Learning Rate: 7.22945e-05
	LOSS [training: 0.06429332963309495 | validation: 0.046120376488371556]
	TIME [epoch: 8.3 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.059981666167521666		[learning rate: 7.2032e-05]
	Learning Rate: 7.20321e-05
	LOSS [training: 0.059981666167521666 | validation: 0.04982296710226631]
	TIME [epoch: 8.28 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05659276898563261		[learning rate: 7.1771e-05]
	Learning Rate: 7.17707e-05
	LOSS [training: 0.05659276898563261 | validation: 0.03043256794941822]
	TIME [epoch: 8.28 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05722822982437501		[learning rate: 7.151e-05]
	Learning Rate: 7.15103e-05
	LOSS [training: 0.05722822982437501 | validation: 0.03489478415584671]
	TIME [epoch: 8.27 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05503461234755561		[learning rate: 7.1251e-05]
	Learning Rate: 7.12508e-05
	LOSS [training: 0.05503461234755561 | validation: 0.03707051208095444]
	TIME [epoch: 8.29 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06047789673901362		[learning rate: 7.0992e-05]
	Learning Rate: 7.09922e-05
	LOSS [training: 0.06047789673901362 | validation: 0.042536697052140346]
	TIME [epoch: 8.27 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06541620301330713		[learning rate: 7.0735e-05]
	Learning Rate: 7.07345e-05
	LOSS [training: 0.06541620301330713 | validation: 0.046696210387760034]
	TIME [epoch: 8.28 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.061580936423048446		[learning rate: 7.0478e-05]
	Learning Rate: 7.04778e-05
	LOSS [training: 0.061580936423048446 | validation: 0.042391260536521314]
	TIME [epoch: 8.27 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054923665634677286		[learning rate: 7.0222e-05]
	Learning Rate: 7.02221e-05
	LOSS [training: 0.054923665634677286 | validation: 0.03109643831546348]
	TIME [epoch: 8.29 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055682392424442916		[learning rate: 6.9967e-05]
	Learning Rate: 6.99672e-05
	LOSS [training: 0.055682392424442916 | validation: 0.03215603669041195]
	TIME [epoch: 8.28 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05831271594531343		[learning rate: 6.9713e-05]
	Learning Rate: 6.97133e-05
	LOSS [training: 0.05831271594531343 | validation: 0.03203811751554453]
	TIME [epoch: 8.27 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05482901386780762		[learning rate: 6.946e-05]
	Learning Rate: 6.94603e-05
	LOSS [training: 0.05482901386780762 | validation: 0.02798844261572892]
	TIME [epoch: 8.27 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.051993081760254634		[learning rate: 6.9208e-05]
	Learning Rate: 6.92083e-05
	LOSS [training: 0.051993081760254634 | validation: 0.04417659235379464]
	TIME [epoch: 8.3 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.061220650113548845		[learning rate: 6.8957e-05]
	Learning Rate: 6.89571e-05
	LOSS [training: 0.061220650113548845 | validation: 0.03382266262865723]
	TIME [epoch: 8.28 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06618706740405109		[learning rate: 6.8707e-05]
	Learning Rate: 6.87068e-05
	LOSS [training: 0.06618706740405109 | validation: 0.03015076232533838]
	TIME [epoch: 8.27 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06294406155573233		[learning rate: 6.8457e-05]
	Learning Rate: 6.84575e-05
	LOSS [training: 0.06294406155573233 | validation: 0.032475651691749804]
	TIME [epoch: 8.27 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06099571198362821		[learning rate: 6.8209e-05]
	Learning Rate: 6.82091e-05
	LOSS [training: 0.06099571198362821 | validation: 0.044764464453582745]
	TIME [epoch: 8.3 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.060119354833141146		[learning rate: 6.7962e-05]
	Learning Rate: 6.79615e-05
	LOSS [training: 0.060119354833141146 | validation: 0.04172078967936825]
	TIME [epoch: 8.28 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.061245034055149604		[learning rate: 6.7715e-05]
	Learning Rate: 6.77149e-05
	LOSS [training: 0.061245034055149604 | validation: 0.0345824137163509]
	TIME [epoch: 8.27 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05748885692470237		[learning rate: 6.7469e-05]
	Learning Rate: 6.74692e-05
	LOSS [training: 0.05748885692470237 | validation: 0.03153648915630558]
	TIME [epoch: 8.27 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05432807811515104		[learning rate: 6.7224e-05]
	Learning Rate: 6.72243e-05
	LOSS [training: 0.05432807811515104 | validation: 0.03773126373296843]
	TIME [epoch: 8.29 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06095809849230312		[learning rate: 6.698e-05]
	Learning Rate: 6.69804e-05
	LOSS [training: 0.06095809849230312 | validation: 0.033565049197962035]
	TIME [epoch: 8.28 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05676332842623466		[learning rate: 6.6737e-05]
	Learning Rate: 6.67373e-05
	LOSS [training: 0.05676332842623466 | validation: 0.03890667885908254]
	TIME [epoch: 8.27 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.059850826907450715		[learning rate: 6.6495e-05]
	Learning Rate: 6.64951e-05
	LOSS [training: 0.059850826907450715 | validation: 0.04222886404022916]
	TIME [epoch: 8.28 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06192530176964985		[learning rate: 6.6254e-05]
	Learning Rate: 6.62538e-05
	LOSS [training: 0.06192530176964985 | validation: 0.03847986342439937]
	TIME [epoch: 8.29 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05577469879225176		[learning rate: 6.6013e-05]
	Learning Rate: 6.60133e-05
	LOSS [training: 0.05577469879225176 | validation: 0.043778645073756596]
	TIME [epoch: 8.28 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05789606621573218		[learning rate: 6.5774e-05]
	Learning Rate: 6.57738e-05
	LOSS [training: 0.05789606621573218 | validation: 0.04039793831005315]
	TIME [epoch: 8.28 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05633545524532494		[learning rate: 6.5535e-05]
	Learning Rate: 6.55351e-05
	LOSS [training: 0.05633545524532494 | validation: 0.04794331476488797]
	TIME [epoch: 8.28 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055489943841283265		[learning rate: 6.5297e-05]
	Learning Rate: 6.52972e-05
	LOSS [training: 0.055489943841283265 | validation: 0.03921290420636975]
	TIME [epoch: 8.3 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05685535823268537		[learning rate: 6.506e-05]
	Learning Rate: 6.50603e-05
	LOSS [training: 0.05685535823268537 | validation: 0.04526973985555331]
	TIME [epoch: 8.28 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05432125733767319		[learning rate: 6.4824e-05]
	Learning Rate: 6.48242e-05
	LOSS [training: 0.05432125733767319 | validation: 0.03696418958141981]
	TIME [epoch: 8.26 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05631223883666596		[learning rate: 6.4589e-05]
	Learning Rate: 6.45889e-05
	LOSS [training: 0.05631223883666596 | validation: 0.0424718864185526]
	TIME [epoch: 8.27 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05878836052908515		[learning rate: 6.4354e-05]
	Learning Rate: 6.43545e-05
	LOSS [training: 0.05878836052908515 | validation: 0.04590775225454172]
	TIME [epoch: 8.29 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05913720539575847		[learning rate: 6.4121e-05]
	Learning Rate: 6.4121e-05
	LOSS [training: 0.05913720539575847 | validation: 0.030797824644164716]
	TIME [epoch: 8.28 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05605773836698358		[learning rate: 6.3888e-05]
	Learning Rate: 6.38883e-05
	LOSS [training: 0.05605773836698358 | validation: 0.03282617801545426]
	TIME [epoch: 8.27 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06074175963242321		[learning rate: 6.3656e-05]
	Learning Rate: 6.36564e-05
	LOSS [training: 0.06074175963242321 | validation: 0.04021606168096029]
	TIME [epoch: 8.28 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.059158535461246234		[learning rate: 6.3425e-05]
	Learning Rate: 6.34254e-05
	LOSS [training: 0.059158535461246234 | validation: 0.048404908203415385]
	TIME [epoch: 8.29 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06115568128596964		[learning rate: 6.3195e-05]
	Learning Rate: 6.31952e-05
	LOSS [training: 0.06115568128596964 | validation: 0.034760241072933656]
	TIME [epoch: 8.27 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05948706607382113		[learning rate: 6.2966e-05]
	Learning Rate: 6.29659e-05
	LOSS [training: 0.05948706607382113 | validation: 0.03404913683619877]
	TIME [epoch: 8.28 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05569049163152377		[learning rate: 6.2737e-05]
	Learning Rate: 6.27374e-05
	LOSS [training: 0.05569049163152377 | validation: 0.03633262164392369]
	TIME [epoch: 8.27 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.057491007438488964		[learning rate: 6.251e-05]
	Learning Rate: 6.25097e-05
	LOSS [training: 0.057491007438488964 | validation: 0.03384316451038727]
	TIME [epoch: 8.3 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.056747410821462466		[learning rate: 6.2283e-05]
	Learning Rate: 6.22828e-05
	LOSS [training: 0.056747410821462466 | validation: 0.03137531224953361]
	TIME [epoch: 8.27 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.057587722705646494		[learning rate: 6.2057e-05]
	Learning Rate: 6.20568e-05
	LOSS [training: 0.057587722705646494 | validation: 0.04284387659944412]
	TIME [epoch: 8.27 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05451527608613541		[learning rate: 6.1832e-05]
	Learning Rate: 6.18316e-05
	LOSS [training: 0.05451527608613541 | validation: 0.03574651611539323]
	TIME [epoch: 8.27 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05560666375966561		[learning rate: 6.1607e-05]
	Learning Rate: 6.16072e-05
	LOSS [training: 0.05560666375966561 | validation: 0.03797172771796743]
	TIME [epoch: 8.29 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05760896538618758		[learning rate: 6.1384e-05]
	Learning Rate: 6.13836e-05
	LOSS [training: 0.05760896538618758 | validation: 0.04519383188505138]
	TIME [epoch: 8.27 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05914721967862072		[learning rate: 6.1161e-05]
	Learning Rate: 6.11609e-05
	LOSS [training: 0.05914721967862072 | validation: 0.030165565456814383]
	TIME [epoch: 8.27 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.056901635085389736		[learning rate: 6.0939e-05]
	Learning Rate: 6.09389e-05
	LOSS [training: 0.056901635085389736 | validation: 0.03259241564568269]
	TIME [epoch: 8.27 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052506135099651255		[learning rate: 6.0718e-05]
	Learning Rate: 6.07178e-05
	LOSS [training: 0.052506135099651255 | validation: 0.040863274428565535]
	TIME [epoch: 8.29 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06326709965137696		[learning rate: 6.0497e-05]
	Learning Rate: 6.04974e-05
	LOSS [training: 0.06326709965137696 | validation: 0.04381493082782511]
	TIME [epoch: 8.27 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06024884831078579		[learning rate: 6.0278e-05]
	Learning Rate: 6.02779e-05
	LOSS [training: 0.06024884831078579 | validation: 0.03452457278337595]
	TIME [epoch: 8.28 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05209292178988604		[learning rate: 6.0059e-05]
	Learning Rate: 6.00591e-05
	LOSS [training: 0.05209292178988604 | validation: 0.03328553535497574]
	TIME [epoch: 8.28 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05456620167691022		[learning rate: 5.9841e-05]
	Learning Rate: 5.98412e-05
	LOSS [training: 0.05456620167691022 | validation: 0.041402766397046195]
	TIME [epoch: 8.29 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06026802842164976		[learning rate: 5.9624e-05]
	Learning Rate: 5.9624e-05
	LOSS [training: 0.06026802842164976 | validation: 0.040824895749874024]
	TIME [epoch: 8.28 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05675321960248732		[learning rate: 5.9408e-05]
	Learning Rate: 5.94076e-05
	LOSS [training: 0.05675321960248732 | validation: 0.03458250814717303]
	TIME [epoch: 8.27 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.060582123650711264		[learning rate: 5.9192e-05]
	Learning Rate: 5.9192e-05
	LOSS [training: 0.060582123650711264 | validation: 0.03648766413646967]
	TIME [epoch: 8.27 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.058041663427252456		[learning rate: 5.8977e-05]
	Learning Rate: 5.89772e-05
	LOSS [training: 0.058041663427252456 | validation: 0.04222403035962745]
	TIME [epoch: 8.3 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05353694301373335		[learning rate: 5.8763e-05]
	Learning Rate: 5.87632e-05
	LOSS [training: 0.05353694301373335 | validation: 0.02398498550080657]
	TIME [epoch: 8.27 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_1513.pth
	Model improved!!!
EPOCH 1514/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.061024688245284696		[learning rate: 5.855e-05]
	Learning Rate: 5.85499e-05
	LOSS [training: 0.061024688245284696 | validation: 0.03678425773635547]
	TIME [epoch: 8.28 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05895993708150066		[learning rate: 5.8337e-05]
	Learning Rate: 5.83374e-05
	LOSS [training: 0.05895993708150066 | validation: 0.04134358572212738]
	TIME [epoch: 8.27 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05825161262493115		[learning rate: 5.8126e-05]
	Learning Rate: 5.81257e-05
	LOSS [training: 0.05825161262493115 | validation: 0.03834699636893468]
	TIME [epoch: 8.29 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06176951117840816		[learning rate: 5.7915e-05]
	Learning Rate: 5.79148e-05
	LOSS [training: 0.06176951117840816 | validation: 0.04453246797420114]
	TIME [epoch: 8.28 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.061992253520684035		[learning rate: 5.7705e-05]
	Learning Rate: 5.77046e-05
	LOSS [training: 0.061992253520684035 | validation: 0.0518765971686005]
	TIME [epoch: 8.28 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05888640287236595		[learning rate: 5.7495e-05]
	Learning Rate: 5.74952e-05
	LOSS [training: 0.05888640287236595 | validation: 0.035594583398090185]
	TIME [epoch: 8.27 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0562701403521342		[learning rate: 5.7287e-05]
	Learning Rate: 5.72866e-05
	LOSS [training: 0.0562701403521342 | validation: 0.030495633588424026]
	TIME [epoch: 8.28 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06206576345151964		[learning rate: 5.7079e-05]
	Learning Rate: 5.70787e-05
	LOSS [training: 0.06206576345151964 | validation: 0.03937913346058694]
	TIME [epoch: 8.29 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06688791409130193		[learning rate: 5.6872e-05]
	Learning Rate: 5.68715e-05
	LOSS [training: 0.06688791409130193 | validation: 0.04046720500702515]
	TIME [epoch: 8.27 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.062313586576650704		[learning rate: 5.6665e-05]
	Learning Rate: 5.66651e-05
	LOSS [training: 0.062313586576650704 | validation: 0.03300003773588513]
	TIME [epoch: 8.27 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06136331165478777		[learning rate: 5.6459e-05]
	Learning Rate: 5.64595e-05
	LOSS [training: 0.06136331165478777 | validation: 0.03504781071823178]
	TIME [epoch: 8.28 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054848033292725605		[learning rate: 5.6255e-05]
	Learning Rate: 5.62546e-05
	LOSS [training: 0.054848033292725605 | validation: 0.030212518670613177]
	TIME [epoch: 8.28 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06035218180776071		[learning rate: 5.605e-05]
	Learning Rate: 5.60504e-05
	LOSS [training: 0.06035218180776071 | validation: 0.032736813780333864]
	TIME [epoch: 8.28 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05889940746849228		[learning rate: 5.5847e-05]
	Learning Rate: 5.5847e-05
	LOSS [training: 0.05889940746849228 | validation: 0.03030500353012363]
	TIME [epoch: 8.27 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05798539785969388		[learning rate: 5.5644e-05]
	Learning Rate: 5.56444e-05
	LOSS [training: 0.05798539785969388 | validation: 0.04125969787184067]
	TIME [epoch: 8.28 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054817031263970306		[learning rate: 5.5442e-05]
	Learning Rate: 5.54424e-05
	LOSS [training: 0.054817031263970306 | validation: 0.035425081973302096]
	TIME [epoch: 8.27 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05140636160701414		[learning rate: 5.5241e-05]
	Learning Rate: 5.52412e-05
	LOSS [training: 0.05140636160701414 | validation: 0.024089490528644508]
	TIME [epoch: 8.26 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0562633919925829		[learning rate: 5.5041e-05]
	Learning Rate: 5.50407e-05
	LOSS [training: 0.0562633919925829 | validation: 0.0417670488510854]
	TIME [epoch: 8.27 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.058973553428892656		[learning rate: 5.4841e-05]
	Learning Rate: 5.4841e-05
	LOSS [training: 0.058973553428892656 | validation: 0.03756659239242116]
	TIME [epoch: 8.28 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05421333285482126		[learning rate: 5.4642e-05]
	Learning Rate: 5.4642e-05
	LOSS [training: 0.05421333285482126 | validation: 0.03477905738411899]
	TIME [epoch: 8.27 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0588095015174559		[learning rate: 5.4444e-05]
	Learning Rate: 5.44437e-05
	LOSS [training: 0.0588095015174559 | validation: 0.036895546234610666]
	TIME [epoch: 8.27 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055766344255018495		[learning rate: 5.4246e-05]
	Learning Rate: 5.42461e-05
	LOSS [training: 0.055766344255018495 | validation: 0.042890020093080405]
	TIME [epoch: 8.27 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05953673164708536		[learning rate: 5.4049e-05]
	Learning Rate: 5.40492e-05
	LOSS [training: 0.05953673164708536 | validation: 0.03666408982047029]
	TIME [epoch: 8.28 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05499420584403665		[learning rate: 5.3853e-05]
	Learning Rate: 5.38531e-05
	LOSS [training: 0.05499420584403665 | validation: 0.03904634804636445]
	TIME [epoch: 8.27 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0642422084162776		[learning rate: 5.3658e-05]
	Learning Rate: 5.36577e-05
	LOSS [training: 0.0642422084162776 | validation: 0.03570898607238375]
	TIME [epoch: 8.27 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05566585111693066		[learning rate: 5.3463e-05]
	Learning Rate: 5.34629e-05
	LOSS [training: 0.05566585111693066 | validation: 0.043851090284382166]
	TIME [epoch: 8.26 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05364735541358963		[learning rate: 5.3269e-05]
	Learning Rate: 5.32689e-05
	LOSS [training: 0.05364735541358963 | validation: 0.03956592905781929]
	TIME [epoch: 8.28 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05781118430369162		[learning rate: 5.3076e-05]
	Learning Rate: 5.30756e-05
	LOSS [training: 0.05781118430369162 | validation: 0.03255888541580674]
	TIME [epoch: 8.27 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0539289305865307		[learning rate: 5.2883e-05]
	Learning Rate: 5.2883e-05
	LOSS [training: 0.0539289305865307 | validation: 0.03181836972688233]
	TIME [epoch: 8.26 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05518924498490999		[learning rate: 5.2691e-05]
	Learning Rate: 5.26911e-05
	LOSS [training: 0.05518924498490999 | validation: 0.03576585901747721]
	TIME [epoch: 8.26 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05828406590020698		[learning rate: 5.25e-05]
	Learning Rate: 5.24998e-05
	LOSS [training: 0.05828406590020698 | validation: 0.036909878809377725]
	TIME [epoch: 8.28 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05672972774860038		[learning rate: 5.2309e-05]
	Learning Rate: 5.23093e-05
	LOSS [training: 0.05672972774860038 | validation: 0.03000650220946509]
	TIME [epoch: 8.28 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05496684792380715		[learning rate: 5.2119e-05]
	Learning Rate: 5.21195e-05
	LOSS [training: 0.05496684792380715 | validation: 0.044819815748133505]
	TIME [epoch: 8.26 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054404429829041936		[learning rate: 5.193e-05]
	Learning Rate: 5.19303e-05
	LOSS [training: 0.054404429829041936 | validation: 0.029510214635972545]
	TIME [epoch: 8.27 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05408560375584302		[learning rate: 5.1742e-05]
	Learning Rate: 5.17419e-05
	LOSS [training: 0.05408560375584302 | validation: 0.03859801330500871]
	TIME [epoch: 8.28 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05537977504606406		[learning rate: 5.1554e-05]
	Learning Rate: 5.15541e-05
	LOSS [training: 0.05537977504606406 | validation: 0.033580261515940166]
	TIME [epoch: 8.28 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05481372877773442		[learning rate: 5.1367e-05]
	Learning Rate: 5.1367e-05
	LOSS [training: 0.05481372877773442 | validation: 0.02713021862641845]
	TIME [epoch: 8.26 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.058331817601318434		[learning rate: 5.1181e-05]
	Learning Rate: 5.11806e-05
	LOSS [training: 0.058331817601318434 | validation: 0.040416154102008404]
	TIME [epoch: 8.27 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05436310293234716		[learning rate: 5.0995e-05]
	Learning Rate: 5.09949e-05
	LOSS [training: 0.05436310293234716 | validation: 0.03719732108883177]
	TIME [epoch: 8.28 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.058527645757671165		[learning rate: 5.081e-05]
	Learning Rate: 5.08098e-05
	LOSS [training: 0.058527645757671165 | validation: 0.038719988798165614]
	TIME [epoch: 8.27 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05190955762537171		[learning rate: 5.0625e-05]
	Learning Rate: 5.06254e-05
	LOSS [training: 0.05190955762537171 | validation: 0.02874208850349779]
	TIME [epoch: 8.27 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0554717259692426		[learning rate: 5.0442e-05]
	Learning Rate: 5.04417e-05
	LOSS [training: 0.0554717259692426 | validation: 0.048093264827410026]
	TIME [epoch: 8.26 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05466323624741939		[learning rate: 5.0259e-05]
	Learning Rate: 5.02586e-05
	LOSS [training: 0.05466323624741939 | validation: 0.04540702476325223]
	TIME [epoch: 8.28 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05447519151836475		[learning rate: 5.0076e-05]
	Learning Rate: 5.00762e-05
	LOSS [training: 0.05447519151836475 | validation: 0.03010567117756121]
	TIME [epoch: 8.27 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05379666918850703		[learning rate: 4.9894e-05]
	Learning Rate: 4.98945e-05
	LOSS [training: 0.05379666918850703 | validation: 0.04330112438892206]
	TIME [epoch: 8.27 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05791032923390103		[learning rate: 4.9713e-05]
	Learning Rate: 4.97134e-05
	LOSS [training: 0.05791032923390103 | validation: 0.035898077669535004]
	TIME [epoch: 8.26 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.059429873532031975		[learning rate: 4.9533e-05]
	Learning Rate: 4.9533e-05
	LOSS [training: 0.059429873532031975 | validation: 0.04040942464760851]
	TIME [epoch: 8.28 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.07153487057557469		[learning rate: 4.9353e-05]
	Learning Rate: 4.93533e-05
	LOSS [training: 0.07153487057557469 | validation: 0.04314658028263184]
	TIME [epoch: 8.27 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06270899808668234		[learning rate: 4.9174e-05]
	Learning Rate: 4.91742e-05
	LOSS [training: 0.06270899808668234 | validation: 0.03741468708325876]
	TIME [epoch: 8.26 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052073765295310284		[learning rate: 4.8996e-05]
	Learning Rate: 4.89957e-05
	LOSS [training: 0.052073765295310284 | validation: 0.04109361724068647]
	TIME [epoch: 8.27 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0612887847412706		[learning rate: 4.8818e-05]
	Learning Rate: 4.88179e-05
	LOSS [training: 0.0612887847412706 | validation: 0.03397446170360918]
	TIME [epoch: 8.28 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05815753461997762		[learning rate: 4.8641e-05]
	Learning Rate: 4.86407e-05
	LOSS [training: 0.05815753461997762 | validation: 0.027728806758696614]
	TIME [epoch: 8.27 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05831580239566889		[learning rate: 4.8464e-05]
	Learning Rate: 4.84642e-05
	LOSS [training: 0.05831580239566889 | validation: 0.032439854506271446]
	TIME [epoch: 8.27 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053257068226733706		[learning rate: 4.8288e-05]
	Learning Rate: 4.82883e-05
	LOSS [training: 0.053257068226733706 | validation: 0.03397005229826342]
	TIME [epoch: 8.26 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05833568856625123		[learning rate: 4.8113e-05]
	Learning Rate: 4.81131e-05
	LOSS [training: 0.05833568856625123 | validation: 0.02970927009997687]
	TIME [epoch: 8.28 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05934891015864226		[learning rate: 4.7938e-05]
	Learning Rate: 4.79385e-05
	LOSS [training: 0.05934891015864226 | validation: 0.03873860471924497]
	TIME [epoch: 8.27 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05486871652775398		[learning rate: 4.7765e-05]
	Learning Rate: 4.77645e-05
	LOSS [training: 0.05486871652775398 | validation: 0.023392440932964283]
	TIME [epoch: 8.26 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_1570.pth
	Model improved!!!
EPOCH 1571/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055755422190237304		[learning rate: 4.7591e-05]
	Learning Rate: 4.75912e-05
	LOSS [training: 0.055755422190237304 | validation: 0.02961238252802943]
	TIME [epoch: 8.26 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.056356013175307716		[learning rate: 4.7418e-05]
	Learning Rate: 4.74185e-05
	LOSS [training: 0.056356013175307716 | validation: 0.029637663602876327]
	TIME [epoch: 8.28 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053868300989387406		[learning rate: 4.7246e-05]
	Learning Rate: 4.72464e-05
	LOSS [training: 0.053868300989387406 | validation: 0.03042227144256749]
	TIME [epoch: 8.27 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05685934575748243		[learning rate: 4.7075e-05]
	Learning Rate: 4.70749e-05
	LOSS [training: 0.05685934575748243 | validation: 0.02896193327110815]
	TIME [epoch: 8.26 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05834308398253809		[learning rate: 4.6904e-05]
	Learning Rate: 4.69041e-05
	LOSS [training: 0.05834308398253809 | validation: 0.032511661191646196]
	TIME [epoch: 8.26 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055910699958898635		[learning rate: 4.6734e-05]
	Learning Rate: 4.67339e-05
	LOSS [training: 0.055910699958898635 | validation: 0.04092847283949028]
	TIME [epoch: 8.27 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05887046579565136		[learning rate: 4.6564e-05]
	Learning Rate: 4.65643e-05
	LOSS [training: 0.05887046579565136 | validation: 0.04591068235759049]
	TIME [epoch: 8.29 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.057324415195592116		[learning rate: 4.6395e-05]
	Learning Rate: 4.63953e-05
	LOSS [training: 0.057324415195592116 | validation: 0.03851367666375245]
	TIME [epoch: 8.26 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06030146900237473		[learning rate: 4.6227e-05]
	Learning Rate: 4.62269e-05
	LOSS [training: 0.06030146900237473 | validation: 0.027157113552349038]
	TIME [epoch: 8.27 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055889579856175965		[learning rate: 4.6059e-05]
	Learning Rate: 4.60591e-05
	LOSS [training: 0.055889579856175965 | validation: 0.03840733777182316]
	TIME [epoch: 8.26 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05976774160457682		[learning rate: 4.5892e-05]
	Learning Rate: 4.5892e-05
	LOSS [training: 0.05976774160457682 | validation: 0.03135835168432433]
	TIME [epoch: 8.29 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.051599113295881835		[learning rate: 4.5725e-05]
	Learning Rate: 4.57254e-05
	LOSS [training: 0.051599113295881835 | validation: 0.032189863524070814]
	TIME [epoch: 8.26 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06264153708953078		[learning rate: 4.556e-05]
	Learning Rate: 4.55595e-05
	LOSS [training: 0.06264153708953078 | validation: 0.04441024131939235]
	TIME [epoch: 8.27 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.061071698535179644		[learning rate: 4.5394e-05]
	Learning Rate: 4.53942e-05
	LOSS [training: 0.061071698535179644 | validation: 0.03277640250326044]
	TIME [epoch: 8.27 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05508154981567034		[learning rate: 4.5229e-05]
	Learning Rate: 4.52294e-05
	LOSS [training: 0.05508154981567034 | validation: 0.035915956163366555]
	TIME [epoch: 8.29 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05444419764318167		[learning rate: 4.5065e-05]
	Learning Rate: 4.50653e-05
	LOSS [training: 0.05444419764318167 | validation: 0.03258869878148289]
	TIME [epoch: 8.27 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052985697230489924		[learning rate: 4.4902e-05]
	Learning Rate: 4.49017e-05
	LOSS [training: 0.052985697230489924 | validation: 0.04524172584658942]
	TIME [epoch: 8.26 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05323150696739579		[learning rate: 4.4739e-05]
	Learning Rate: 4.47388e-05
	LOSS [training: 0.05323150696739579 | validation: 0.03099086563087186]
	TIME [epoch: 8.27 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.057100183213712505		[learning rate: 4.4576e-05]
	Learning Rate: 4.45764e-05
	LOSS [training: 0.057100183213712505 | validation: 0.03194829418699182]
	TIME [epoch: 8.28 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.056668512162182626		[learning rate: 4.4415e-05]
	Learning Rate: 4.44147e-05
	LOSS [training: 0.056668512162182626 | validation: 0.03709225177840088]
	TIME [epoch: 8.27 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053958329364342375		[learning rate: 4.4253e-05]
	Learning Rate: 4.42535e-05
	LOSS [training: 0.053958329364342375 | validation: 0.02867815041521433]
	TIME [epoch: 8.27 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05460435810178114		[learning rate: 4.4093e-05]
	Learning Rate: 4.40929e-05
	LOSS [training: 0.05460435810178114 | validation: 0.027875387182338583]
	TIME [epoch: 8.28 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053713398171750956		[learning rate: 4.3933e-05]
	Learning Rate: 4.39329e-05
	LOSS [training: 0.053713398171750956 | validation: 0.030901977038302816]
	TIME [epoch: 8.29 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05233835330005689		[learning rate: 4.3773e-05]
	Learning Rate: 4.37734e-05
	LOSS [training: 0.05233835330005689 | validation: 0.03530443818776418]
	TIME [epoch: 8.28 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05499442402491116		[learning rate: 4.3615e-05]
	Learning Rate: 4.36146e-05
	LOSS [training: 0.05499442402491116 | validation: 0.029396659088490425]
	TIME [epoch: 8.27 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05802527533365237		[learning rate: 4.3456e-05]
	Learning Rate: 4.34563e-05
	LOSS [training: 0.05802527533365237 | validation: 0.038753137658877806]
	TIME [epoch: 8.27 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06057529412711036		[learning rate: 4.3299e-05]
	Learning Rate: 4.32986e-05
	LOSS [training: 0.06057529412711036 | validation: 0.03145642574856486]
	TIME [epoch: 8.29 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.056029379322277105		[learning rate: 4.3141e-05]
	Learning Rate: 4.31415e-05
	LOSS [training: 0.056029379322277105 | validation: 0.038031418261280425]
	TIME [epoch: 8.27 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053882283233576786		[learning rate: 4.2985e-05]
	Learning Rate: 4.29849e-05
	LOSS [training: 0.053882283233576786 | validation: 0.03885301404721907]
	TIME [epoch: 8.28 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.059234380207505245		[learning rate: 4.2829e-05]
	Learning Rate: 4.28289e-05
	LOSS [training: 0.059234380207505245 | validation: 0.036880211056324885]
	TIME [epoch: 8.27 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05223968534098382		[learning rate: 4.2673e-05]
	Learning Rate: 4.26735e-05
	LOSS [training: 0.05223968534098382 | validation: 0.032153533520294666]
	TIME [epoch: 8.3 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05102595889259318		[learning rate: 4.2519e-05]
	Learning Rate: 4.25186e-05
	LOSS [training: 0.05102595889259318 | validation: 0.03513859386723382]
	TIME [epoch: 8.28 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05268722194348792		[learning rate: 4.2364e-05]
	Learning Rate: 4.23643e-05
	LOSS [training: 0.05268722194348792 | validation: 0.038476250361125564]
	TIME [epoch: 8.28 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054860345579125336		[learning rate: 4.2211e-05]
	Learning Rate: 4.22106e-05
	LOSS [training: 0.054860345579125336 | validation: 0.03289362852263149]
	TIME [epoch: 8.28 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05746363968665676		[learning rate: 4.2057e-05]
	Learning Rate: 4.20574e-05
	LOSS [training: 0.05746363968665676 | validation: 0.03905813991684749]
	TIME [epoch: 8.29 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05292927530238269		[learning rate: 4.1905e-05]
	Learning Rate: 4.19047e-05
	LOSS [training: 0.05292927530238269 | validation: 0.037282967928008205]
	TIME [epoch: 8.27 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05448033668512403		[learning rate: 4.1753e-05]
	Learning Rate: 4.17527e-05
	LOSS [training: 0.05448033668512403 | validation: 0.034199582520559955]
	TIME [epoch: 8.26 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054053949783603304		[learning rate: 4.1601e-05]
	Learning Rate: 4.16012e-05
	LOSS [training: 0.054053949783603304 | validation: 0.036653767917147036]
	TIME [epoch: 8.28 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05392340106834248		[learning rate: 4.145e-05]
	Learning Rate: 4.14502e-05
	LOSS [training: 0.05392340106834248 | validation: 0.03271924295320941]
	TIME [epoch: 8.3 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05587435543310598		[learning rate: 4.13e-05]
	Learning Rate: 4.12998e-05
	LOSS [training: 0.05587435543310598 | validation: 0.03141046445957814]
	TIME [epoch: 8.28 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.059422588323654345		[learning rate: 4.115e-05]
	Learning Rate: 4.11499e-05
	LOSS [training: 0.059422588323654345 | validation: 0.03400850191826583]
	TIME [epoch: 8.28 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05685706555171911		[learning rate: 4.1001e-05]
	Learning Rate: 4.10005e-05
	LOSS [training: 0.05685706555171911 | validation: 0.02505488271199874]
	TIME [epoch: 8.27 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05736937436040536		[learning rate: 4.0852e-05]
	Learning Rate: 4.08517e-05
	LOSS [training: 0.05736937436040536 | validation: 0.025141268767003858]
	TIME [epoch: 8.29 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05373115078291117		[learning rate: 4.0703e-05]
	Learning Rate: 4.07035e-05
	LOSS [training: 0.05373115078291117 | validation: 0.030717844624597174]
	TIME [epoch: 8.27 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.059709027765187536		[learning rate: 4.0556e-05]
	Learning Rate: 4.05558e-05
	LOSS [training: 0.059709027765187536 | validation: 0.03331622915110631]
	TIME [epoch: 8.27 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0536570653466233		[learning rate: 4.0409e-05]
	Learning Rate: 4.04086e-05
	LOSS [training: 0.0536570653466233 | validation: 0.03448532096748898]
	TIME [epoch: 8.27 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05583177202031246		[learning rate: 4.0262e-05]
	Learning Rate: 4.0262e-05
	LOSS [training: 0.05583177202031246 | validation: 0.03706340327674017]
	TIME [epoch: 8.3 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054984842263532466		[learning rate: 4.0116e-05]
	Learning Rate: 4.01158e-05
	LOSS [training: 0.054984842263532466 | validation: 0.03408694295323869]
	TIME [epoch: 8.27 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05162158811913022		[learning rate: 3.997e-05]
	Learning Rate: 3.99703e-05
	LOSS [training: 0.05162158811913022 | validation: 0.029473856172067436]
	TIME [epoch: 8.27 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05606974593865074		[learning rate: 3.9825e-05]
	Learning Rate: 3.98252e-05
	LOSS [training: 0.05606974593865074 | validation: 0.034933819485400155]
	TIME [epoch: 8.28 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05924993065416416		[learning rate: 3.9681e-05]
	Learning Rate: 3.96807e-05
	LOSS [training: 0.05924993065416416 | validation: 0.03820070791276185]
	TIME [epoch: 8.28 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055231377581666775		[learning rate: 3.9537e-05]
	Learning Rate: 3.95367e-05
	LOSS [training: 0.055231377581666775 | validation: 0.03731793782387484]
	TIME [epoch: 8.27 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05524063966298544		[learning rate: 3.9393e-05]
	Learning Rate: 3.93932e-05
	LOSS [training: 0.05524063966298544 | validation: 0.03175745958607378]
	TIME [epoch: 8.26 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06431170957801399		[learning rate: 3.925e-05]
	Learning Rate: 3.92502e-05
	LOSS [training: 0.06431170957801399 | validation: 0.03544931595166517]
	TIME [epoch: 8.27 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.056160938088293796		[learning rate: 3.9108e-05]
	Learning Rate: 3.91078e-05
	LOSS [training: 0.056160938088293796 | validation: 0.042402751953918794]
	TIME [epoch: 8.29 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05465386546633173		[learning rate: 3.8966e-05]
	Learning Rate: 3.89659e-05
	LOSS [training: 0.05465386546633173 | validation: 0.03203148258839516]
	TIME [epoch: 8.27 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05116147137161431		[learning rate: 3.8824e-05]
	Learning Rate: 3.88245e-05
	LOSS [training: 0.05116147137161431 | validation: 0.027274072198304453]
	TIME [epoch: 8.26 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05057411229436952		[learning rate: 3.8684e-05]
	Learning Rate: 3.86836e-05
	LOSS [training: 0.05057411229436952 | validation: 0.034634458813195754]
	TIME [epoch: 8.28 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05620066002421663		[learning rate: 3.8543e-05]
	Learning Rate: 3.85432e-05
	LOSS [training: 0.05620066002421663 | validation: 0.03426095802950187]
	TIME [epoch: 8.28 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05747944170396864		[learning rate: 3.8403e-05]
	Learning Rate: 3.84033e-05
	LOSS [training: 0.05747944170396864 | validation: 0.028856125174173346]
	TIME [epoch: 8.27 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06253951192742205		[learning rate: 3.8264e-05]
	Learning Rate: 3.82639e-05
	LOSS [training: 0.06253951192742205 | validation: 0.027642794005960876]
	TIME [epoch: 8.27 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.061795019826977285		[learning rate: 3.8125e-05]
	Learning Rate: 3.81251e-05
	LOSS [training: 0.061795019826977285 | validation: 0.03430132876519139]
	TIME [epoch: 8.27 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.058914083894289294		[learning rate: 3.7987e-05]
	Learning Rate: 3.79867e-05
	LOSS [training: 0.058914083894289294 | validation: 0.039142558068753566]
	TIME [epoch: 8.28 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06266339115512168		[learning rate: 3.7849e-05]
	Learning Rate: 3.78489e-05
	LOSS [training: 0.06266339115512168 | validation: 0.030937836835716388]
	TIME [epoch: 8.27 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05351678094050864		[learning rate: 3.7711e-05]
	Learning Rate: 3.77115e-05
	LOSS [training: 0.05351678094050864 | validation: 0.03727202508178783]
	TIME [epoch: 8.27 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06448536234036258		[learning rate: 3.7575e-05]
	Learning Rate: 3.75746e-05
	LOSS [training: 0.06448536234036258 | validation: 0.03955933653121005]
	TIME [epoch: 8.27 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05822520435294567		[learning rate: 3.7438e-05]
	Learning Rate: 3.74383e-05
	LOSS [training: 0.05822520435294567 | validation: 0.0365216083682025]
	TIME [epoch: 8.29 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.058730449652968385		[learning rate: 3.7302e-05]
	Learning Rate: 3.73024e-05
	LOSS [training: 0.058730449652968385 | validation: 0.028429743646547365]
	TIME [epoch: 8.26 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.059326547562075796		[learning rate: 3.7167e-05]
	Learning Rate: 3.7167e-05
	LOSS [training: 0.059326547562075796 | validation: 0.03433525558875023]
	TIME [epoch: 8.27 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05604962661929095		[learning rate: 3.7032e-05]
	Learning Rate: 3.70322e-05
	LOSS [training: 0.05604962661929095 | validation: 0.042815728359537134]
	TIME [epoch: 8.28 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05700364652924313		[learning rate: 3.6898e-05]
	Learning Rate: 3.68978e-05
	LOSS [training: 0.05700364652924313 | validation: 0.038754779092565646]
	TIME [epoch: 8.28 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055426652321568404		[learning rate: 3.6764e-05]
	Learning Rate: 3.67639e-05
	LOSS [training: 0.055426652321568404 | validation: 0.035050253300379225]
	TIME [epoch: 8.27 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.050718018174939414		[learning rate: 3.663e-05]
	Learning Rate: 3.66304e-05
	LOSS [training: 0.050718018174939414 | validation: 0.0497162250083832]
	TIME [epoch: 8.27 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0492158644839148		[learning rate: 3.6498e-05]
	Learning Rate: 3.64975e-05
	LOSS [training: 0.0492158644839148 | validation: 0.03777158822701306]
	TIME [epoch: 8.27 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055486144419685225		[learning rate: 3.6365e-05]
	Learning Rate: 3.63651e-05
	LOSS [training: 0.055486144419685225 | validation: 0.029695384003855328]
	TIME [epoch: 8.29 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053401810239650405		[learning rate: 3.6233e-05]
	Learning Rate: 3.62331e-05
	LOSS [training: 0.053401810239650405 | validation: 0.04210891747216747]
	TIME [epoch: 8.27 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.056785645094279136		[learning rate: 3.6102e-05]
	Learning Rate: 3.61016e-05
	LOSS [training: 0.056785645094279136 | validation: 0.03459926109022991]
	TIME [epoch: 8.27 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055334536671346676		[learning rate: 3.5971e-05]
	Learning Rate: 3.59706e-05
	LOSS [training: 0.055334536671346676 | validation: 0.0286126579185366]
	TIME [epoch: 8.27 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.049622786038688846		[learning rate: 3.584e-05]
	Learning Rate: 3.584e-05
	LOSS [training: 0.049622786038688846 | validation: 0.028757621760644354]
	TIME [epoch: 8.29 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06080149353686507		[learning rate: 3.571e-05]
	Learning Rate: 3.571e-05
	LOSS [training: 0.06080149353686507 | validation: 0.035156050270293546]
	TIME [epoch: 8.28 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0602544302848782		[learning rate: 3.558e-05]
	Learning Rate: 3.55804e-05
	LOSS [training: 0.0602544302848782 | validation: 0.031400743373921465]
	TIME [epoch: 8.28 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054027915112275046		[learning rate: 3.5451e-05]
	Learning Rate: 3.54513e-05
	LOSS [training: 0.054027915112275046 | validation: 0.028211166219668998]
	TIME [epoch: 8.28 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05112430639601332		[learning rate: 3.5323e-05]
	Learning Rate: 3.53226e-05
	LOSS [training: 0.05112430639601332 | validation: 0.03505369941292633]
	TIME [epoch: 8.29 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05517559547949036		[learning rate: 3.5194e-05]
	Learning Rate: 3.51944e-05
	LOSS [training: 0.05517559547949036 | validation: 0.03439090004483253]
	TIME [epoch: 8.27 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.059314410689445964		[learning rate: 3.5067e-05]
	Learning Rate: 3.50667e-05
	LOSS [training: 0.059314410689445964 | validation: 0.03306755963688267]
	TIME [epoch: 8.27 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.048783353833847284		[learning rate: 3.4939e-05]
	Learning Rate: 3.49394e-05
	LOSS [training: 0.048783353833847284 | validation: 0.03058902683623762]
	TIME [epoch: 8.27 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055467793537188524		[learning rate: 3.4813e-05]
	Learning Rate: 3.48126e-05
	LOSS [training: 0.055467793537188524 | validation: 0.03945843036690416]
	TIME [epoch: 8.28 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.057395430938127344		[learning rate: 3.4686e-05]
	Learning Rate: 3.46863e-05
	LOSS [training: 0.057395430938127344 | validation: 0.04254353680675606]
	TIME [epoch: 8.27 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053515975037251896		[learning rate: 3.456e-05]
	Learning Rate: 3.45604e-05
	LOSS [training: 0.053515975037251896 | validation: 0.03884066442061958]
	TIME [epoch: 8.26 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054524072467602894		[learning rate: 3.4435e-05]
	Learning Rate: 3.4435e-05
	LOSS [training: 0.054524072467602894 | validation: 0.028257921537205213]
	TIME [epoch: 8.28 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05210474326903054		[learning rate: 3.431e-05]
	Learning Rate: 3.431e-05
	LOSS [training: 0.05210474326903054 | validation: 0.033793541790421974]
	TIME [epoch: 8.29 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05510430937435532		[learning rate: 3.4186e-05]
	Learning Rate: 3.41855e-05
	LOSS [training: 0.05510430937435532 | validation: 0.03313363846938644]
	TIME [epoch: 8.28 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05462692916727215		[learning rate: 3.4061e-05]
	Learning Rate: 3.40615e-05
	LOSS [training: 0.05462692916727215 | validation: 0.037134284332095875]
	TIME [epoch: 8.27 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05233646057197376		[learning rate: 3.3938e-05]
	Learning Rate: 3.39378e-05
	LOSS [training: 0.05233646057197376 | validation: 0.03380904806796029]
	TIME [epoch: 8.28 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05673656100528068		[learning rate: 3.3815e-05]
	Learning Rate: 3.38147e-05
	LOSS [training: 0.05673656100528068 | validation: 0.032684381390887876]
	TIME [epoch: 8.28 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.056174799138029086		[learning rate: 3.3692e-05]
	Learning Rate: 3.3692e-05
	LOSS [training: 0.056174799138029086 | validation: 0.028818137298890132]
	TIME [epoch: 8.28 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05604298082868521		[learning rate: 3.357e-05]
	Learning Rate: 3.35697e-05
	LOSS [training: 0.05604298082868521 | validation: 0.04083600358242253]
	TIME [epoch: 8.27 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05409986531648662		[learning rate: 3.3448e-05]
	Learning Rate: 3.34479e-05
	LOSS [training: 0.05409986531648662 | validation: 0.041035318280129296]
	TIME [epoch: 8.28 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05171932854706178		[learning rate: 3.3326e-05]
	Learning Rate: 3.33265e-05
	LOSS [training: 0.05171932854706178 | validation: 0.031136355771380415]
	TIME [epoch: 8.3 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.051798778920337664		[learning rate: 3.3206e-05]
	Learning Rate: 3.32055e-05
	LOSS [training: 0.051798778920337664 | validation: 0.026896171596292356]
	TIME [epoch: 8.28 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05532697620620575		[learning rate: 3.3085e-05]
	Learning Rate: 3.3085e-05
	LOSS [training: 0.05532697620620575 | validation: 0.052576506165765716]
	TIME [epoch: 8.28 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06331003659521875		[learning rate: 3.2965e-05]
	Learning Rate: 3.2965e-05
	LOSS [training: 0.06331003659521875 | validation: 0.06001892427804369]
	TIME [epoch: 8.28 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06128233119494743		[learning rate: 3.2845e-05]
	Learning Rate: 3.28453e-05
	LOSS [training: 0.06128233119494743 | validation: 0.03248348608559689]
	TIME [epoch: 8.29 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05618353886133483		[learning rate: 3.2726e-05]
	Learning Rate: 3.27261e-05
	LOSS [training: 0.05618353886133483 | validation: 0.03998655875315593]
	TIME [epoch: 8.28 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.057735728292790764		[learning rate: 3.2607e-05]
	Learning Rate: 3.26074e-05
	LOSS [training: 0.057735728292790764 | validation: 0.04304441545672942]
	TIME [epoch: 8.27 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0569242715201879		[learning rate: 3.2489e-05]
	Learning Rate: 3.2489e-05
	LOSS [training: 0.0569242715201879 | validation: 0.043870229893815324]
	TIME [epoch: 8.28 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05332619280302899		[learning rate: 3.2371e-05]
	Learning Rate: 3.23711e-05
	LOSS [training: 0.05332619280302899 | validation: 0.03230743394073113]
	TIME [epoch: 8.29 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05490693687067679		[learning rate: 3.2254e-05]
	Learning Rate: 3.22537e-05
	LOSS [training: 0.05490693687067679 | validation: 0.0238579512924419]
	TIME [epoch: 8.28 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.057511972453336405		[learning rate: 3.2137e-05]
	Learning Rate: 3.21366e-05
	LOSS [training: 0.057511972453336405 | validation: 0.033367759485104655]
	TIME [epoch: 8.26 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05417303099221804		[learning rate: 3.202e-05]
	Learning Rate: 3.202e-05
	LOSS [training: 0.05417303099221804 | validation: 0.037561516484274346]
	TIME [epoch: 8.28 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05899911362293686		[learning rate: 3.1904e-05]
	Learning Rate: 3.19038e-05
	LOSS [training: 0.05899911362293686 | validation: 0.04630499414296049]
	TIME [epoch: 8.29 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05666021715460854		[learning rate: 3.1788e-05]
	Learning Rate: 3.1788e-05
	LOSS [training: 0.05666021715460854 | validation: 0.03621703755412915]
	TIME [epoch: 8.27 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05410085838137688		[learning rate: 3.1673e-05]
	Learning Rate: 3.16726e-05
	LOSS [training: 0.05410085838137688 | validation: 0.031385060218690544]
	TIME [epoch: 8.27 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05751586482241535		[learning rate: 3.1558e-05]
	Learning Rate: 3.15577e-05
	LOSS [training: 0.05751586482241535 | validation: 0.032899894692654134]
	TIME [epoch: 8.28 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.061744606604226204		[learning rate: 3.1443e-05]
	Learning Rate: 3.14432e-05
	LOSS [training: 0.061744606604226204 | validation: 0.04063486548868578]
	TIME [epoch: 8.29 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05482497428158717		[learning rate: 3.1329e-05]
	Learning Rate: 3.13291e-05
	LOSS [training: 0.05482497428158717 | validation: 0.031239793492613816]
	TIME [epoch: 8.27 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05929279354749667		[learning rate: 3.1215e-05]
	Learning Rate: 3.12154e-05
	LOSS [training: 0.05929279354749667 | validation: 0.0256362285776185]
	TIME [epoch: 8.28 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05237347368559485		[learning rate: 3.1102e-05]
	Learning Rate: 3.11021e-05
	LOSS [training: 0.05237347368559485 | validation: 0.026942792801150493]
	TIME [epoch: 8.28 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05098086276374113		[learning rate: 3.0989e-05]
	Learning Rate: 3.09892e-05
	LOSS [training: 0.05098086276374113 | validation: 0.03599240775313408]
	TIME [epoch: 8.29 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052548354578269264		[learning rate: 3.0877e-05]
	Learning Rate: 3.08768e-05
	LOSS [training: 0.052548354578269264 | validation: 0.033732524773283035]
	TIME [epoch: 8.27 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05719688035393115		[learning rate: 3.0765e-05]
	Learning Rate: 3.07647e-05
	LOSS [training: 0.05719688035393115 | validation: 0.027946688296575782]
	TIME [epoch: 8.28 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0545938104118807		[learning rate: 3.0653e-05]
	Learning Rate: 3.0653e-05
	LOSS [training: 0.0545938104118807 | validation: 0.03994212873565186]
	TIME [epoch: 8.28 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.057696252479877475		[learning rate: 3.0542e-05]
	Learning Rate: 3.05418e-05
	LOSS [training: 0.057696252479877475 | validation: 0.026897788131193312]
	TIME [epoch: 8.29 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05339783016774137		[learning rate: 3.0431e-05]
	Learning Rate: 3.0431e-05
	LOSS [training: 0.05339783016774137 | validation: 0.05816972751746658]
	TIME [epoch: 8.27 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06226027471221275		[learning rate: 3.0321e-05]
	Learning Rate: 3.03205e-05
	LOSS [training: 0.06226027471221275 | validation: 0.033182997928412235]
	TIME [epoch: 8.27 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05672791664956265		[learning rate: 3.0211e-05]
	Learning Rate: 3.02105e-05
	LOSS [training: 0.05672791664956265 | validation: 0.028677862393069877]
	TIME [epoch: 8.29 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05068996975799593		[learning rate: 3.0101e-05]
	Learning Rate: 3.01009e-05
	LOSS [training: 0.05068996975799593 | validation: 0.027540614108350014]
	TIME [epoch: 8.28 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05224569218136732		[learning rate: 2.9992e-05]
	Learning Rate: 2.99916e-05
	LOSS [training: 0.05224569218136732 | validation: 0.03634054155224191]
	TIME [epoch: 8.27 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05259624028040209		[learning rate: 2.9883e-05]
	Learning Rate: 2.98828e-05
	LOSS [training: 0.05259624028040209 | validation: 0.03310658159001381]
	TIME [epoch: 8.27 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05735301817014514		[learning rate: 2.9774e-05]
	Learning Rate: 2.97743e-05
	LOSS [training: 0.05735301817014514 | validation: 0.03834710442025498]
	TIME [epoch: 8.29 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05662562407289977		[learning rate: 2.9666e-05]
	Learning Rate: 2.96663e-05
	LOSS [training: 0.05662562407289977 | validation: 0.025519249167390554]
	TIME [epoch: 8.29 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05351576658816255		[learning rate: 2.9559e-05]
	Learning Rate: 2.95586e-05
	LOSS [training: 0.05351576658816255 | validation: 0.030673070943007197]
	TIME [epoch: 8.27 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05244699512487916		[learning rate: 2.9451e-05]
	Learning Rate: 2.94514e-05
	LOSS [training: 0.05244699512487916 | validation: 0.04146798730929371]
	TIME [epoch: 8.27 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.060200419625697374		[learning rate: 2.9344e-05]
	Learning Rate: 2.93445e-05
	LOSS [training: 0.060200419625697374 | validation: 0.02954827468330102]
	TIME [epoch: 8.29 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04894688421640571		[learning rate: 2.9238e-05]
	Learning Rate: 2.9238e-05
	LOSS [training: 0.04894688421640571 | validation: 0.030613459199309974]
	TIME [epoch: 8.28 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.051952807972965545		[learning rate: 2.9132e-05]
	Learning Rate: 2.91319e-05
	LOSS [training: 0.051952807972965545 | validation: 0.025723653327426488]
	TIME [epoch: 8.27 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05263336070618017		[learning rate: 2.9026e-05]
	Learning Rate: 2.90262e-05
	LOSS [training: 0.05263336070618017 | validation: 0.03625784325133281]
	TIME [epoch: 8.28 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055115357174963873		[learning rate: 2.8921e-05]
	Learning Rate: 2.89208e-05
	LOSS [training: 0.055115357174963873 | validation: 0.03679105829027622]
	TIME [epoch: 8.29 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05217913747399953		[learning rate: 2.8816e-05]
	Learning Rate: 2.88159e-05
	LOSS [training: 0.05217913747399953 | validation: 0.03237709576734947]
	TIME [epoch: 8.28 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0538730448691321		[learning rate: 2.8711e-05]
	Learning Rate: 2.87113e-05
	LOSS [training: 0.0538730448691321 | validation: 0.029845765877579353]
	TIME [epoch: 8.27 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055099252218063846		[learning rate: 2.8607e-05]
	Learning Rate: 2.86071e-05
	LOSS [training: 0.055099252218063846 | validation: 0.036482530715329975]
	TIME [epoch: 8.27 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05752027345877163		[learning rate: 2.8503e-05]
	Learning Rate: 2.85033e-05
	LOSS [training: 0.05752027345877163 | validation: 0.04053203039334713]
	TIME [epoch: 8.29 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05166976241614081		[learning rate: 2.84e-05]
	Learning Rate: 2.83998e-05
	LOSS [training: 0.05166976241614081 | validation: 0.03348263065462527]
	TIME [epoch: 8.29 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05553694477194733		[learning rate: 2.8297e-05]
	Learning Rate: 2.82968e-05
	LOSS [training: 0.05553694477194733 | validation: 0.028584321596632354]
	TIME [epoch: 8.27 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05347805339877569		[learning rate: 2.8194e-05]
	Learning Rate: 2.81941e-05
	LOSS [training: 0.05347805339877569 | validation: 0.03086866370429328]
	TIME [epoch: 8.27 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05242016439466087		[learning rate: 2.8092e-05]
	Learning Rate: 2.80918e-05
	LOSS [training: 0.05242016439466087 | validation: 0.03347563207886615]
	TIME [epoch: 8.29 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.057030930896862383		[learning rate: 2.799e-05]
	Learning Rate: 2.79898e-05
	LOSS [training: 0.057030930896862383 | validation: 0.034433598147111444]
	TIME [epoch: 8.28 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05345678707901906		[learning rate: 2.7888e-05]
	Learning Rate: 2.78882e-05
	LOSS [training: 0.05345678707901906 | validation: 0.04113937013982781]
	TIME [epoch: 8.28 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0548818286479509		[learning rate: 2.7787e-05]
	Learning Rate: 2.7787e-05
	LOSS [training: 0.0548818286479509 | validation: 0.0321975852061588]
	TIME [epoch: 8.27 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055211986139017254		[learning rate: 2.7686e-05]
	Learning Rate: 2.76862e-05
	LOSS [training: 0.055211986139017254 | validation: 0.02802540476969815]
	TIME [epoch: 8.29 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05167510380061269		[learning rate: 2.7586e-05]
	Learning Rate: 2.75857e-05
	LOSS [training: 0.05167510380061269 | validation: 0.04039825859864116]
	TIME [epoch: 8.28 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05490005481873102		[learning rate: 2.7486e-05]
	Learning Rate: 2.74856e-05
	LOSS [training: 0.05490005481873102 | validation: 0.026948788277656677]
	TIME [epoch: 8.27 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054235686350133164		[learning rate: 2.7386e-05]
	Learning Rate: 2.73859e-05
	LOSS [training: 0.054235686350133164 | validation: 0.03010956959351643]
	TIME [epoch: 8.27 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0574151508079793		[learning rate: 2.7286e-05]
	Learning Rate: 2.72865e-05
	LOSS [training: 0.0574151508079793 | validation: 0.03319472792732964]
	TIME [epoch: 8.28 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.056148635788129876		[learning rate: 2.7187e-05]
	Learning Rate: 2.71875e-05
	LOSS [training: 0.056148635788129876 | validation: 0.037330044467912576]
	TIME [epoch: 8.28 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06400598062175043		[learning rate: 2.7089e-05]
	Learning Rate: 2.70888e-05
	LOSS [training: 0.06400598062175043 | validation: 0.0354078706553354]
	TIME [epoch: 8.28 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05226587154205402		[learning rate: 2.699e-05]
	Learning Rate: 2.69905e-05
	LOSS [training: 0.05226587154205402 | validation: 0.027723632657681678]
	TIME [epoch: 8.28 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052654511447945015		[learning rate: 2.6893e-05]
	Learning Rate: 2.68925e-05
	LOSS [training: 0.052654511447945015 | validation: 0.03943621055459268]
	TIME [epoch: 8.28 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0550246384012331		[learning rate: 2.6795e-05]
	Learning Rate: 2.67949e-05
	LOSS [training: 0.0550246384012331 | validation: 0.02644157694520209]
	TIME [epoch: 8.28 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05626381811631555		[learning rate: 2.6698e-05]
	Learning Rate: 2.66977e-05
	LOSS [training: 0.05626381811631555 | validation: 0.03719013710019807]
	TIME [epoch: 8.27 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05333899618487535		[learning rate: 2.6601e-05]
	Learning Rate: 2.66008e-05
	LOSS [training: 0.05333899618487535 | validation: 0.021045615523684447]
	TIME [epoch: 8.27 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_1731.pth
	Model improved!!!
EPOCH 1732/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04970052808136293		[learning rate: 2.6504e-05]
	Learning Rate: 2.65043e-05
	LOSS [training: 0.04970052808136293 | validation: 0.02400057731675794]
	TIME [epoch: 8.29 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05083955988209071		[learning rate: 2.6408e-05]
	Learning Rate: 2.64081e-05
	LOSS [training: 0.05083955988209071 | validation: 0.02588152935876971]
	TIME [epoch: 8.28 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.050149267078689753		[learning rate: 2.6312e-05]
	Learning Rate: 2.63123e-05
	LOSS [training: 0.050149267078689753 | validation: 0.037167603923027814]
	TIME [epoch: 8.27 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05450676598227202		[learning rate: 2.6217e-05]
	Learning Rate: 2.62168e-05
	LOSS [training: 0.05450676598227202 | validation: 0.03615117083406568]
	TIME [epoch: 8.27 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05385691085214438		[learning rate: 2.6122e-05]
	Learning Rate: 2.61216e-05
	LOSS [training: 0.05385691085214438 | validation: 0.03326944140304381]
	TIME [epoch: 8.27 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05130449276376507		[learning rate: 2.6027e-05]
	Learning Rate: 2.60268e-05
	LOSS [training: 0.05130449276376507 | validation: 0.036100676751517985]
	TIME [epoch: 8.29 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05373650216918693		[learning rate: 2.5932e-05]
	Learning Rate: 2.59324e-05
	LOSS [training: 0.05373650216918693 | validation: 0.0298701723981585]
	TIME [epoch: 8.27 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05483367492186575		[learning rate: 2.5838e-05]
	Learning Rate: 2.58383e-05
	LOSS [training: 0.05483367492186575 | validation: 0.033518053100938826]
	TIME [epoch: 8.27 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054854433300581554		[learning rate: 2.5744e-05]
	Learning Rate: 2.57445e-05
	LOSS [training: 0.054854433300581554 | validation: 0.026538224039890777]
	TIME [epoch: 8.28 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05196219588706432		[learning rate: 2.5651e-05]
	Learning Rate: 2.56511e-05
	LOSS [training: 0.05196219588706432 | validation: 0.03504285242832646]
	TIME [epoch: 8.29 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05367718024869803		[learning rate: 2.5558e-05]
	Learning Rate: 2.5558e-05
	LOSS [training: 0.05367718024869803 | validation: 0.04139770378864939]
	TIME [epoch: 8.27 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05345996099261217		[learning rate: 2.5465e-05]
	Learning Rate: 2.54652e-05
	LOSS [training: 0.05345996099261217 | validation: 0.03911775216300493]
	TIME [epoch: 8.28 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05356274677430258		[learning rate: 2.5373e-05]
	Learning Rate: 2.53728e-05
	LOSS [training: 0.05356274677430258 | validation: 0.03812655911918258]
	TIME [epoch: 8.27 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05541389864579956		[learning rate: 2.5281e-05]
	Learning Rate: 2.52807e-05
	LOSS [training: 0.05541389864579956 | validation: 0.04329796812870195]
	TIME [epoch: 8.3 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05065303218306252		[learning rate: 2.5189e-05]
	Learning Rate: 2.5189e-05
	LOSS [training: 0.05065303218306252 | validation: 0.036390823423395224]
	TIME [epoch: 8.27 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.056800431610970724		[learning rate: 2.5098e-05]
	Learning Rate: 2.50976e-05
	LOSS [training: 0.056800431610970724 | validation: 0.03628708102077238]
	TIME [epoch: 8.28 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05310946790901455		[learning rate: 2.5006e-05]
	Learning Rate: 2.50065e-05
	LOSS [training: 0.05310946790901455 | validation: 0.040814364159678554]
	TIME [epoch: 8.27 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05105998249532504		[learning rate: 2.4916e-05]
	Learning Rate: 2.49157e-05
	LOSS [training: 0.05105998249532504 | validation: 0.0358853782684814]
	TIME [epoch: 8.3 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0612271172489137		[learning rate: 2.4825e-05]
	Learning Rate: 2.48253e-05
	LOSS [training: 0.0612271172489137 | validation: 0.030282632346222277]
	TIME [epoch: 8.28 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0539956816426197		[learning rate: 2.4735e-05]
	Learning Rate: 2.47352e-05
	LOSS [training: 0.0539956816426197 | validation: 0.03030169900109073]
	TIME [epoch: 8.27 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05371394155210799		[learning rate: 2.4645e-05]
	Learning Rate: 2.46455e-05
	LOSS [training: 0.05371394155210799 | validation: 0.034988424842626695]
	TIME [epoch: 8.27 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05272142541200463		[learning rate: 2.4556e-05]
	Learning Rate: 2.4556e-05
	LOSS [training: 0.05272142541200463 | validation: 0.03185205129025592]
	TIME [epoch: 8.3 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05678922774119634		[learning rate: 2.4467e-05]
	Learning Rate: 2.44669e-05
	LOSS [training: 0.05678922774119634 | validation: 0.03552222940697777]
	TIME [epoch: 8.28 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05641199722869951		[learning rate: 2.4378e-05]
	Learning Rate: 2.43781e-05
	LOSS [training: 0.05641199722869951 | validation: 0.03162926890223665]
	TIME [epoch: 8.28 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05365622546561014		[learning rate: 2.429e-05]
	Learning Rate: 2.42896e-05
	LOSS [training: 0.05365622546561014 | validation: 0.02972746865391778]
	TIME [epoch: 8.27 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054659837030893554		[learning rate: 2.4201e-05]
	Learning Rate: 2.42015e-05
	LOSS [training: 0.054659837030893554 | validation: 0.03448198785378459]
	TIME [epoch: 8.3 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055339140516794075		[learning rate: 2.4114e-05]
	Learning Rate: 2.41137e-05
	LOSS [training: 0.055339140516794075 | validation: 0.03481557824503012]
	TIME [epoch: 8.28 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05539457677937147		[learning rate: 2.4026e-05]
	Learning Rate: 2.40262e-05
	LOSS [training: 0.05539457677937147 | validation: 0.03693613542191779]
	TIME [epoch: 8.28 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.050049112894313005		[learning rate: 2.3939e-05]
	Learning Rate: 2.3939e-05
	LOSS [training: 0.050049112894313005 | validation: 0.020125883593103414]
	TIME [epoch: 8.27 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_1760.pth
	Model improved!!!
EPOCH 1761/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04977079510346894		[learning rate: 2.3852e-05]
	Learning Rate: 2.38521e-05
	LOSS [training: 0.04977079510346894 | validation: 0.03167161606814366]
	TIME [epoch: 8.3 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05677934084377586		[learning rate: 2.3766e-05]
	Learning Rate: 2.37655e-05
	LOSS [training: 0.05677934084377586 | validation: 0.036038270161499956]
	TIME [epoch: 8.27 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.050903420840059575		[learning rate: 2.3679e-05]
	Learning Rate: 2.36793e-05
	LOSS [training: 0.050903420840059575 | validation: 0.03191200395356751]
	TIME [epoch: 8.28 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.051747477645070185		[learning rate: 2.3593e-05]
	Learning Rate: 2.35933e-05
	LOSS [training: 0.051747477645070185 | validation: 0.03374654014475522]
	TIME [epoch: 8.27 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05363796161343413		[learning rate: 2.3508e-05]
	Learning Rate: 2.35077e-05
	LOSS [training: 0.05363796161343413 | validation: 0.03611492181574184]
	TIME [epoch: 8.3 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0537756405832225		[learning rate: 2.3422e-05]
	Learning Rate: 2.34224e-05
	LOSS [training: 0.0537756405832225 | validation: 0.034040969701880516]
	TIME [epoch: 8.28 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05017856941327999		[learning rate: 2.3337e-05]
	Learning Rate: 2.33374e-05
	LOSS [training: 0.05017856941327999 | validation: 0.03850850251338783]
	TIME [epoch: 8.27 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055653354388620103		[learning rate: 2.3253e-05]
	Learning Rate: 2.32527e-05
	LOSS [training: 0.055653354388620103 | validation: 0.03106951942806095]
	TIME [epoch: 8.27 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05253614881427872		[learning rate: 2.3168e-05]
	Learning Rate: 2.31683e-05
	LOSS [training: 0.05253614881427872 | validation: 0.021710126532585812]
	TIME [epoch: 8.29 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05381611450885184		[learning rate: 2.3084e-05]
	Learning Rate: 2.30843e-05
	LOSS [training: 0.05381611450885184 | validation: 0.03892380809990262]
	TIME [epoch: 8.29 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05478781435633626		[learning rate: 2.3e-05]
	Learning Rate: 2.30005e-05
	LOSS [training: 0.05478781435633626 | validation: 0.03149656882966609]
	TIME [epoch: 8.28 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05236659788014615		[learning rate: 2.2917e-05]
	Learning Rate: 2.2917e-05
	LOSS [training: 0.05236659788014615 | validation: 0.033841227405450244]
	TIME [epoch: 8.27 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05473130778064271		[learning rate: 2.2834e-05]
	Learning Rate: 2.28338e-05
	LOSS [training: 0.05473130778064271 | validation: 0.03575646046710851]
	TIME [epoch: 8.28 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052187861773207764		[learning rate: 2.2751e-05]
	Learning Rate: 2.2751e-05
	LOSS [training: 0.052187861773207764 | validation: 0.035182074451365355]
	TIME [epoch: 8.27 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.051047000036281785		[learning rate: 2.2668e-05]
	Learning Rate: 2.26684e-05
	LOSS [training: 0.051047000036281785 | validation: 0.030579299059209142]
	TIME [epoch: 8.26 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05516266598382589		[learning rate: 2.2586e-05]
	Learning Rate: 2.25862e-05
	LOSS [training: 0.05516266598382589 | validation: 0.028268456587526476]
	TIME [epoch: 8.27 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053900543291423145		[learning rate: 2.2504e-05]
	Learning Rate: 2.25042e-05
	LOSS [training: 0.053900543291423145 | validation: 0.033775625404354206]
	TIME [epoch: 8.29 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05471707716152312		[learning rate: 2.2423e-05]
	Learning Rate: 2.24225e-05
	LOSS [training: 0.05471707716152312 | validation: 0.03765506772182989]
	TIME [epoch: 8.28 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05048075860528302		[learning rate: 2.2341e-05]
	Learning Rate: 2.23411e-05
	LOSS [training: 0.05048075860528302 | validation: 0.030197531107445177]
	TIME [epoch: 8.28 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05307559099852155		[learning rate: 2.226e-05]
	Learning Rate: 2.22601e-05
	LOSS [training: 0.05307559099852155 | validation: 0.02995566081441587]
	TIME [epoch: 8.28 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.050461704979609945		[learning rate: 2.2179e-05]
	Learning Rate: 2.21793e-05
	LOSS [training: 0.050461704979609945 | validation: 0.024586625820553362]
	TIME [epoch: 8.29 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054409277154244894		[learning rate: 2.2099e-05]
	Learning Rate: 2.20988e-05
	LOSS [training: 0.054409277154244894 | validation: 0.03713863567260543]
	TIME [epoch: 8.27 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0546664550016152		[learning rate: 2.2019e-05]
	Learning Rate: 2.20186e-05
	LOSS [training: 0.0546664550016152 | validation: 0.03326509323451475]
	TIME [epoch: 8.27 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.056026319433253036		[learning rate: 2.1939e-05]
	Learning Rate: 2.19387e-05
	LOSS [training: 0.056026319433253036 | validation: 0.04363028788117001]
	TIME [epoch: 8.27 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052572683408662946		[learning rate: 2.1859e-05]
	Learning Rate: 2.18591e-05
	LOSS [training: 0.052572683408662946 | validation: 0.04527354975287447]
	TIME [epoch: 8.29 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05525407664382877		[learning rate: 2.178e-05]
	Learning Rate: 2.17797e-05
	LOSS [training: 0.05525407664382877 | validation: 0.031339173964162054]
	TIME [epoch: 8.28 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05139279923975078		[learning rate: 2.1701e-05]
	Learning Rate: 2.17007e-05
	LOSS [training: 0.05139279923975078 | validation: 0.022885697306514526]
	TIME [epoch: 8.27 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05503854330469351		[learning rate: 2.1622e-05]
	Learning Rate: 2.16219e-05
	LOSS [training: 0.05503854330469351 | validation: 0.037822793022757026]
	TIME [epoch: 8.28 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053934441817073894		[learning rate: 2.1543e-05]
	Learning Rate: 2.15435e-05
	LOSS [training: 0.053934441817073894 | validation: 0.03666221192042633]
	TIME [epoch: 8.29 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05573098101518721		[learning rate: 2.1465e-05]
	Learning Rate: 2.14653e-05
	LOSS [training: 0.05573098101518721 | validation: 0.034215228058196046]
	TIME [epoch: 8.28 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05827156416289012		[learning rate: 2.1387e-05]
	Learning Rate: 2.13874e-05
	LOSS [training: 0.05827156416289012 | validation: 0.029749105831175383]
	TIME [epoch: 8.27 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04965973072540421		[learning rate: 2.131e-05]
	Learning Rate: 2.13098e-05
	LOSS [training: 0.04965973072540421 | validation: 0.042026822876189744]
	TIME [epoch: 8.27 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05470618081050711		[learning rate: 2.1232e-05]
	Learning Rate: 2.12325e-05
	LOSS [training: 0.05470618081050711 | validation: 0.03510199048224882]
	TIME [epoch: 8.29 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05925221260461576		[learning rate: 2.1155e-05]
	Learning Rate: 2.11554e-05
	LOSS [training: 0.05925221260461576 | validation: 0.029699484853462917]
	TIME [epoch: 8.28 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.057365365838537664		[learning rate: 2.1079e-05]
	Learning Rate: 2.10786e-05
	LOSS [training: 0.057365365838537664 | validation: 0.02724579316962452]
	TIME [epoch: 8.28 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05473724358999488		[learning rate: 2.1002e-05]
	Learning Rate: 2.10021e-05
	LOSS [training: 0.05473724358999488 | validation: 0.031438241153254005]
	TIME [epoch: 8.27 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05332161310383694		[learning rate: 2.0926e-05]
	Learning Rate: 2.09259e-05
	LOSS [training: 0.05332161310383694 | validation: 0.03292824071876703]
	TIME [epoch: 8.3 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05148793269198797		[learning rate: 2.085e-05]
	Learning Rate: 2.085e-05
	LOSS [training: 0.05148793269198797 | validation: 0.036147924617616234]
	TIME [epoch: 8.27 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.047781688414398955		[learning rate: 2.0774e-05]
	Learning Rate: 2.07743e-05
	LOSS [training: 0.047781688414398955 | validation: 0.029830716309904838]
	TIME [epoch: 8.28 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052533456675619705		[learning rate: 2.0699e-05]
	Learning Rate: 2.06989e-05
	LOSS [training: 0.052533456675619705 | validation: 0.03555191955700966]
	TIME [epoch: 8.27 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053454214774297994		[learning rate: 2.0624e-05]
	Learning Rate: 2.06238e-05
	LOSS [training: 0.053454214774297994 | validation: 0.03887310683992642]
	TIME [epoch: 8.29 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05500563277459543		[learning rate: 2.0549e-05]
	Learning Rate: 2.05489e-05
	LOSS [training: 0.05500563277459543 | validation: 0.044793115253713614]
	TIME [epoch: 8.28 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05367171194446867		[learning rate: 2.0474e-05]
	Learning Rate: 2.04744e-05
	LOSS [training: 0.05367171194446867 | validation: 0.02531713011755808]
	TIME [epoch: 8.28 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05219505401113696		[learning rate: 2.04e-05]
	Learning Rate: 2.04001e-05
	LOSS [training: 0.05219505401113696 | validation: 0.03195602712450335]
	TIME [epoch: 8.28 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05833522308180457		[learning rate: 2.0326e-05]
	Learning Rate: 2.0326e-05
	LOSS [training: 0.05833522308180457 | validation: 0.03075041463353142]
	TIME [epoch: 8.29 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05411582821109404		[learning rate: 2.0252e-05]
	Learning Rate: 2.02523e-05
	LOSS [training: 0.05411582821109404 | validation: 0.032620272394869754]
	TIME [epoch: 8.28 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05346696076370714		[learning rate: 2.0179e-05]
	Learning Rate: 2.01788e-05
	LOSS [training: 0.05346696076370714 | validation: 0.036625715269003906]
	TIME [epoch: 8.27 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.048326838362265534		[learning rate: 2.0106e-05]
	Learning Rate: 2.01055e-05
	LOSS [training: 0.048326838362265534 | validation: 0.03644109318524941]
	TIME [epoch: 8.28 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055291421369329095		[learning rate: 2.0033e-05]
	Learning Rate: 2.00326e-05
	LOSS [training: 0.055291421369329095 | validation: 0.037793464663319926]
	TIME [epoch: 8.28 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05407404377899592		[learning rate: 1.996e-05]
	Learning Rate: 1.99599e-05
	LOSS [training: 0.05407404377899592 | validation: 0.0235461120515067]
	TIME [epoch: 8.27 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053249823115553395		[learning rate: 1.9887e-05]
	Learning Rate: 1.98874e-05
	LOSS [training: 0.053249823115553395 | validation: 0.026138987815099657]
	TIME [epoch: 8.27 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0557712671795075		[learning rate: 1.9815e-05]
	Learning Rate: 1.98153e-05
	LOSS [training: 0.0557712671795075 | validation: 0.04031220543413842]
	TIME [epoch: 8.27 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05721642924807535		[learning rate: 1.9743e-05]
	Learning Rate: 1.97434e-05
	LOSS [training: 0.05721642924807535 | validation: 0.02921880476283449]
	TIME [epoch: 8.3 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05383894372079109		[learning rate: 1.9672e-05]
	Learning Rate: 1.96717e-05
	LOSS [training: 0.05383894372079109 | validation: 0.030984030148002446]
	TIME [epoch: 8.27 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05852806308555336		[learning rate: 1.96e-05]
	Learning Rate: 1.96003e-05
	LOSS [training: 0.05852806308555336 | validation: 0.03530568186467167]
	TIME [epoch: 8.27 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05541850058222854		[learning rate: 1.9529e-05]
	Learning Rate: 1.95292e-05
	LOSS [training: 0.05541850058222854 | validation: 0.03670612376240196]
	TIME [epoch: 8.26 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.059341096999793355		[learning rate: 1.9458e-05]
	Learning Rate: 1.94583e-05
	LOSS [training: 0.059341096999793355 | validation: 0.033966135499564426]
	TIME [epoch: 8.3 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.051499064880131694		[learning rate: 1.9388e-05]
	Learning Rate: 1.93877e-05
	LOSS [training: 0.051499064880131694 | validation: 0.04336569771662909]
	TIME [epoch: 8.27 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04907663623117291		[learning rate: 1.9317e-05]
	Learning Rate: 1.93173e-05
	LOSS [training: 0.04907663623117291 | validation: 0.0390852983446079]
	TIME [epoch: 8.28 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05463825394199022		[learning rate: 1.9247e-05]
	Learning Rate: 1.92472e-05
	LOSS [training: 0.05463825394199022 | validation: 0.03751775857640479]
	TIME [epoch: 8.27 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05492838277226551		[learning rate: 1.9177e-05]
	Learning Rate: 1.91774e-05
	LOSS [training: 0.05492838277226551 | validation: 0.04658763951716659]
	TIME [epoch: 8.29 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05379030642102082		[learning rate: 1.9108e-05]
	Learning Rate: 1.91078e-05
	LOSS [training: 0.05379030642102082 | validation: 0.04089512810064358]
	TIME [epoch: 8.28 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.056583218876743745		[learning rate: 1.9038e-05]
	Learning Rate: 1.90384e-05
	LOSS [training: 0.056583218876743745 | validation: 0.0419378523480433]
	TIME [epoch: 8.27 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05521740323275407		[learning rate: 1.8969e-05]
	Learning Rate: 1.89694e-05
	LOSS [training: 0.05521740323275407 | validation: 0.0372184234489462]
	TIME [epoch: 8.28 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05352796636526995		[learning rate: 1.8901e-05]
	Learning Rate: 1.89005e-05
	LOSS [training: 0.05352796636526995 | validation: 0.0384926329117891]
	TIME [epoch: 8.29 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05647822064142349		[learning rate: 1.8832e-05]
	Learning Rate: 1.88319e-05
	LOSS [training: 0.05647822064142349 | validation: 0.03522601896808624]
	TIME [epoch: 8.27 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04973094991866678		[learning rate: 1.8764e-05]
	Learning Rate: 1.87636e-05
	LOSS [training: 0.04973094991866678 | validation: 0.03477650207364811]
	TIME [epoch: 8.26 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.049704984209149414		[learning rate: 1.8695e-05]
	Learning Rate: 1.86955e-05
	LOSS [training: 0.049704984209149414 | validation: 0.029384032011089054]
	TIME [epoch: 8.28 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055762690863824904		[learning rate: 1.8628e-05]
	Learning Rate: 1.86276e-05
	LOSS [training: 0.055762690863824904 | validation: 0.03909582920177409]
	TIME [epoch: 8.29 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05358737284253521		[learning rate: 1.856e-05]
	Learning Rate: 1.856e-05
	LOSS [training: 0.05358737284253521 | validation: 0.0335717271112942]
	TIME [epoch: 8.27 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05239553084975472		[learning rate: 1.8493e-05]
	Learning Rate: 1.84927e-05
	LOSS [training: 0.05239553084975472 | validation: 0.03564896926818938]
	TIME [epoch: 8.27 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054273509997912375		[learning rate: 1.8426e-05]
	Learning Rate: 1.84256e-05
	LOSS [training: 0.054273509997912375 | validation: 0.029611071191045113]
	TIME [epoch: 8.27 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05324993230115864		[learning rate: 1.8359e-05]
	Learning Rate: 1.83587e-05
	LOSS [training: 0.05324993230115864 | validation: 0.038832965247415696]
	TIME [epoch: 8.29 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.048876706160209574		[learning rate: 1.8292e-05]
	Learning Rate: 1.82921e-05
	LOSS [training: 0.048876706160209574 | validation: 0.03303126196242036]
	TIME [epoch: 8.27 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.051755015327985654		[learning rate: 1.8226e-05]
	Learning Rate: 1.82257e-05
	LOSS [training: 0.051755015327985654 | validation: 0.028936048413107842]
	TIME [epoch: 8.27 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05617291838465429		[learning rate: 1.816e-05]
	Learning Rate: 1.81596e-05
	LOSS [training: 0.05617291838465429 | validation: 0.03184794069212171]
	TIME [epoch: 8.27 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05459209804522611		[learning rate: 1.8094e-05]
	Learning Rate: 1.80937e-05
	LOSS [training: 0.05459209804522611 | validation: 0.0379162714624273]
	TIME [epoch: 8.29 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052260194234866754		[learning rate: 1.8028e-05]
	Learning Rate: 1.8028e-05
	LOSS [training: 0.052260194234866754 | validation: 0.03748971931008141]
	TIME [epoch: 8.27 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05352433823477053		[learning rate: 1.7963e-05]
	Learning Rate: 1.79626e-05
	LOSS [training: 0.05352433823477053 | validation: 0.027952987946145588]
	TIME [epoch: 8.27 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05415911665993811		[learning rate: 1.7897e-05]
	Learning Rate: 1.78974e-05
	LOSS [training: 0.05415911665993811 | validation: 0.03185480671141641]
	TIME [epoch: 8.27 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05197132266146106		[learning rate: 1.7832e-05]
	Learning Rate: 1.78324e-05
	LOSS [training: 0.05197132266146106 | validation: 0.033611972730044834]
	TIME [epoch: 8.29 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052254244950583714		[learning rate: 1.7768e-05]
	Learning Rate: 1.77677e-05
	LOSS [training: 0.052254244950583714 | validation: 0.027883638006177595]
	TIME [epoch: 8.28 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05593702237199875		[learning rate: 1.7703e-05]
	Learning Rate: 1.77032e-05
	LOSS [training: 0.05593702237199875 | validation: 0.026028548635813404]
	TIME [epoch: 8.27 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052574835414632724		[learning rate: 1.7639e-05]
	Learning Rate: 1.7639e-05
	LOSS [training: 0.052574835414632724 | validation: 0.02843269909637581]
	TIME [epoch: 8.28 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05402353270870584		[learning rate: 1.7575e-05]
	Learning Rate: 1.7575e-05
	LOSS [training: 0.05402353270870584 | validation: 0.03363228510568308]
	TIME [epoch: 8.3 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05172326662774128		[learning rate: 1.7511e-05]
	Learning Rate: 1.75112e-05
	LOSS [training: 0.05172326662774128 | validation: 0.03394498501825399]
	TIME [epoch: 8.28 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05399075422182988		[learning rate: 1.7448e-05]
	Learning Rate: 1.74476e-05
	LOSS [training: 0.05399075422182988 | validation: 0.037596240583237614]
	TIME [epoch: 8.28 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053422782153768035		[learning rate: 1.7384e-05]
	Learning Rate: 1.73843e-05
	LOSS [training: 0.053422782153768035 | validation: 0.03903583685737251]
	TIME [epoch: 8.28 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05465981239486164		[learning rate: 1.7321e-05]
	Learning Rate: 1.73212e-05
	LOSS [training: 0.05465981239486164 | validation: 0.036949570444265376]
	TIME [epoch: 8.29 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.051216755167286056		[learning rate: 1.7258e-05]
	Learning Rate: 1.72584e-05
	LOSS [training: 0.051216755167286056 | validation: 0.032237215783158996]
	TIME [epoch: 8.27 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052054773335791026		[learning rate: 1.7196e-05]
	Learning Rate: 1.71957e-05
	LOSS [training: 0.052054773335791026 | validation: 0.0327668879201146]
	TIME [epoch: 8.27 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052768772727171044		[learning rate: 1.7133e-05]
	Learning Rate: 1.71333e-05
	LOSS [training: 0.052768772727171044 | validation: 0.03634815844462151]
	TIME [epoch: 8.27 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05153231977496492		[learning rate: 1.7071e-05]
	Learning Rate: 1.70712e-05
	LOSS [training: 0.05153231977496492 | validation: 0.021106732818494657]
	TIME [epoch: 8.3 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054917709392185175		[learning rate: 1.7009e-05]
	Learning Rate: 1.70092e-05
	LOSS [training: 0.054917709392185175 | validation: 0.025432138609307663]
	TIME [epoch: 8.27 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054660908975426956		[learning rate: 1.6947e-05]
	Learning Rate: 1.69475e-05
	LOSS [training: 0.054660908975426956 | validation: 0.0315415007522148]
	TIME [epoch: 8.27 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054208203828536215		[learning rate: 1.6886e-05]
	Learning Rate: 1.6886e-05
	LOSS [training: 0.054208203828536215 | validation: 0.029684932398779756]
	TIME [epoch: 8.27 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05489864914307822		[learning rate: 1.6825e-05]
	Learning Rate: 1.68247e-05
	LOSS [training: 0.05489864914307822 | validation: 0.03250792084466216]
	TIME [epoch: 8.3 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05718596121093513		[learning rate: 1.6764e-05]
	Learning Rate: 1.67636e-05
	LOSS [training: 0.05718596121093513 | validation: 0.025837268209555002]
	TIME [epoch: 8.27 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05261011694228984		[learning rate: 1.6703e-05]
	Learning Rate: 1.67028e-05
	LOSS [training: 0.05261011694228984 | validation: 0.03574163546998926]
	TIME [epoch: 8.27 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05365750627880169		[learning rate: 1.6642e-05]
	Learning Rate: 1.66422e-05
	LOSS [training: 0.05365750627880169 | validation: 0.0400181075627698]
	TIME [epoch: 8.28 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04774656360640994		[learning rate: 1.6582e-05]
	Learning Rate: 1.65818e-05
	LOSS [training: 0.04774656360640994 | validation: 0.02755500937858822]
	TIME [epoch: 8.29 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05286500384034052		[learning rate: 1.6522e-05]
	Learning Rate: 1.65216e-05
	LOSS [training: 0.05286500384034052 | validation: 0.02537639046203582]
	TIME [epoch: 8.27 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05650497553464007		[learning rate: 1.6462e-05]
	Learning Rate: 1.64617e-05
	LOSS [training: 0.05650497553464007 | validation: 0.03623725912811461]
	TIME [epoch: 8.27 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05251357809720251		[learning rate: 1.6402e-05]
	Learning Rate: 1.64019e-05
	LOSS [training: 0.05251357809720251 | validation: 0.03392138927445089]
	TIME [epoch: 8.27 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04999508776878124		[learning rate: 1.6342e-05]
	Learning Rate: 1.63424e-05
	LOSS [training: 0.04999508776878124 | validation: 0.03312669081972726]
	TIME [epoch: 8.29 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054329252336417035		[learning rate: 1.6283e-05]
	Learning Rate: 1.62831e-05
	LOSS [training: 0.054329252336417035 | validation: 0.036946189981061986]
	TIME [epoch: 8.28 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05783745500702747		[learning rate: 1.6224e-05]
	Learning Rate: 1.6224e-05
	LOSS [training: 0.05783745500702747 | validation: 0.029127698218633938]
	TIME [epoch: 8.28 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05340797712723498		[learning rate: 1.6165e-05]
	Learning Rate: 1.61651e-05
	LOSS [training: 0.05340797712723498 | validation: 0.026668091003619276]
	TIME [epoch: 8.28 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.050049899532586685		[learning rate: 1.6106e-05]
	Learning Rate: 1.61065e-05
	LOSS [training: 0.050049899532586685 | validation: 0.032520386702876665]
	TIME [epoch: 8.29 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04859764849687244		[learning rate: 1.6048e-05]
	Learning Rate: 1.6048e-05
	LOSS [training: 0.04859764849687244 | validation: 0.024291514759660238]
	TIME [epoch: 8.29 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052052661219510896		[learning rate: 1.599e-05]
	Learning Rate: 1.59898e-05
	LOSS [training: 0.052052661219510896 | validation: 0.0475528559920362]
	TIME [epoch: 8.28 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05094386851205921		[learning rate: 1.5932e-05]
	Learning Rate: 1.59317e-05
	LOSS [training: 0.05094386851205921 | validation: 0.030672620757667052]
	TIME [epoch: 8.28 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.049256525172465315		[learning rate: 1.5874e-05]
	Learning Rate: 1.58739e-05
	LOSS [training: 0.049256525172465315 | validation: 0.03445016381029443]
	TIME [epoch: 8.3 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05491429521025735		[learning rate: 1.5816e-05]
	Learning Rate: 1.58163e-05
	LOSS [training: 0.05491429521025735 | validation: 0.03773895150675223]
	TIME [epoch: 8.28 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05560896773349759		[learning rate: 1.5759e-05]
	Learning Rate: 1.57589e-05
	LOSS [training: 0.05560896773349759 | validation: 0.03813012831937213]
	TIME [epoch: 8.28 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05493218869000625		[learning rate: 1.5702e-05]
	Learning Rate: 1.57017e-05
	LOSS [training: 0.05493218869000625 | validation: 0.03265253843080987]
	TIME [epoch: 8.28 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05521906443592102		[learning rate: 1.5645e-05]
	Learning Rate: 1.56447e-05
	LOSS [training: 0.05521906443592102 | validation: 0.03924294205832975]
	TIME [epoch: 8.3 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05789594377353872		[learning rate: 1.5588e-05]
	Learning Rate: 1.5588e-05
	LOSS [training: 0.05789594377353872 | validation: 0.03395198690461379]
	TIME [epoch: 8.28 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053894571493262086		[learning rate: 1.5531e-05]
	Learning Rate: 1.55314e-05
	LOSS [training: 0.053894571493262086 | validation: 0.03941983434628747]
	TIME [epoch: 8.27 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0546024846337142		[learning rate: 1.5475e-05]
	Learning Rate: 1.5475e-05
	LOSS [training: 0.0546024846337142 | validation: 0.03813234154764079]
	TIME [epoch: 8.28 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05640285222128091		[learning rate: 1.5419e-05]
	Learning Rate: 1.54189e-05
	LOSS [training: 0.05640285222128091 | validation: 0.037009287077350725]
	TIME [epoch: 8.29 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05369398525502513		[learning rate: 1.5363e-05]
	Learning Rate: 1.53629e-05
	LOSS [training: 0.05369398525502513 | validation: 0.024847663088788167]
	TIME [epoch: 8.28 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0561929544185058		[learning rate: 1.5307e-05]
	Learning Rate: 1.53072e-05
	LOSS [training: 0.0561929544185058 | validation: 0.028971764768572765]
	TIME [epoch: 8.27 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.059757063819639265		[learning rate: 1.5252e-05]
	Learning Rate: 1.52516e-05
	LOSS [training: 0.059757063819639265 | validation: 0.028361674147148786]
	TIME [epoch: 8.28 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.06413252331127581		[learning rate: 1.5196e-05]
	Learning Rate: 1.51963e-05
	LOSS [training: 0.06413252331127581 | validation: 0.02987906121408028]
	TIME [epoch: 8.3 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05460782987300879		[learning rate: 1.5141e-05]
	Learning Rate: 1.51411e-05
	LOSS [training: 0.05460782987300879 | validation: 0.03097741540706893]
	TIME [epoch: 8.28 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05328494716200009		[learning rate: 1.5086e-05]
	Learning Rate: 1.50862e-05
	LOSS [training: 0.05328494716200009 | validation: 0.026278392319430688]
	TIME [epoch: 8.28 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04936674288815134		[learning rate: 1.5031e-05]
	Learning Rate: 1.50314e-05
	LOSS [training: 0.04936674288815134 | validation: 0.03367398036377517]
	TIME [epoch: 8.28 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.058910038021069665		[learning rate: 1.4977e-05]
	Learning Rate: 1.49769e-05
	LOSS [training: 0.058910038021069665 | validation: 0.034738591153247324]
	TIME [epoch: 8.3 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05329634591411947		[learning rate: 1.4923e-05]
	Learning Rate: 1.49225e-05
	LOSS [training: 0.05329634591411947 | validation: 0.035355196302768316]
	TIME [epoch: 8.27 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053786370076937984		[learning rate: 1.4868e-05]
	Learning Rate: 1.48684e-05
	LOSS [training: 0.053786370076937984 | validation: 0.03406103827152911]
	TIME [epoch: 8.27 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05435600014217924		[learning rate: 1.4814e-05]
	Learning Rate: 1.48144e-05
	LOSS [training: 0.05435600014217924 | validation: 0.04129192628535738]
	TIME [epoch: 8.28 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.050785048055948336		[learning rate: 1.4761e-05]
	Learning Rate: 1.47606e-05
	LOSS [training: 0.050785048055948336 | validation: 0.03140239310225095]
	TIME [epoch: 8.29 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05586443150628634		[learning rate: 1.4707e-05]
	Learning Rate: 1.47071e-05
	LOSS [training: 0.05586443150628634 | validation: 0.03157939684919487]
	TIME [epoch: 8.28 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.050860457734701894		[learning rate: 1.4654e-05]
	Learning Rate: 1.46537e-05
	LOSS [training: 0.050860457734701894 | validation: 0.02346520040071958]
	TIME [epoch: 8.27 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04713298292650576		[learning rate: 1.4601e-05]
	Learning Rate: 1.46005e-05
	LOSS [training: 0.04713298292650576 | validation: 0.03716070160548242]
	TIME [epoch: 8.28 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04924703191397836		[learning rate: 1.4548e-05]
	Learning Rate: 1.45475e-05
	LOSS [training: 0.04924703191397836 | validation: 0.03098544047361429]
	TIME [epoch: 8.29 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05134893332571111		[learning rate: 1.4495e-05]
	Learning Rate: 1.44947e-05
	LOSS [training: 0.05134893332571111 | validation: 0.017670833248784594]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r2_20240219_233848/states/model_tr_study204_1898.pth
	Model improved!!!
EPOCH 1899/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05134812795004513		[learning rate: 1.4442e-05]
	Learning Rate: 1.44421e-05
	LOSS [training: 0.05134812795004513 | validation: 0.03262271906268681]
	TIME [epoch: 8.28 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.051868980592746396		[learning rate: 1.439e-05]
	Learning Rate: 1.43897e-05
	LOSS [training: 0.051868980592746396 | validation: 0.03434065126297117]
	TIME [epoch: 8.29 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.056674377673420304		[learning rate: 1.4338e-05]
	Learning Rate: 1.43375e-05
	LOSS [training: 0.056674377673420304 | validation: 0.02543167038790682]
	TIME [epoch: 8.3 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052820200890868876		[learning rate: 1.4285e-05]
	Learning Rate: 1.42855e-05
	LOSS [training: 0.052820200890868876 | validation: 0.03353382682664029]
	TIME [epoch: 8.28 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055874002168084844		[learning rate: 1.4234e-05]
	Learning Rate: 1.42336e-05
	LOSS [training: 0.055874002168084844 | validation: 0.034399138546234095]
	TIME [epoch: 8.28 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05302235254477241		[learning rate: 1.4182e-05]
	Learning Rate: 1.4182e-05
	LOSS [training: 0.05302235254477241 | validation: 0.03432812910337801]
	TIME [epoch: 8.28 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.056732676760264125		[learning rate: 1.4131e-05]
	Learning Rate: 1.41305e-05
	LOSS [training: 0.056732676760264125 | validation: 0.034192829638832294]
	TIME [epoch: 8.3 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.050081520756558195		[learning rate: 1.4079e-05]
	Learning Rate: 1.40792e-05
	LOSS [training: 0.050081520756558195 | validation: 0.025985641463148954]
	TIME [epoch: 8.29 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054134259893058624		[learning rate: 1.4028e-05]
	Learning Rate: 1.40281e-05
	LOSS [training: 0.054134259893058624 | validation: 0.03513977784739673]
	TIME [epoch: 8.28 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04928536918136412		[learning rate: 1.3977e-05]
	Learning Rate: 1.39772e-05
	LOSS [training: 0.04928536918136412 | validation: 0.0362611318090669]
	TIME [epoch: 8.28 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05348847417311795		[learning rate: 1.3927e-05]
	Learning Rate: 1.39265e-05
	LOSS [training: 0.05348847417311795 | validation: 0.030708211392853797]
	TIME [epoch: 8.3 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0549193473549703		[learning rate: 1.3876e-05]
	Learning Rate: 1.3876e-05
	LOSS [training: 0.0549193473549703 | validation: 0.021207287797867035]
	TIME [epoch: 8.29 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.051903193571781615		[learning rate: 1.3826e-05]
	Learning Rate: 1.38256e-05
	LOSS [training: 0.051903193571781615 | validation: 0.02765144753032415]
	TIME [epoch: 8.28 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05062720941190215		[learning rate: 1.3775e-05]
	Learning Rate: 1.37754e-05
	LOSS [training: 0.05062720941190215 | validation: 0.04047940566760503]
	TIME [epoch: 8.28 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055009729182275625		[learning rate: 1.3725e-05]
	Learning Rate: 1.37254e-05
	LOSS [training: 0.055009729182275625 | validation: 0.02780079664066832]
	TIME [epoch: 8.3 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05358833275450982		[learning rate: 1.3676e-05]
	Learning Rate: 1.36756e-05
	LOSS [training: 0.05358833275450982 | validation: 0.033712550681073254]
	TIME [epoch: 8.29 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05609882229459011		[learning rate: 1.3626e-05]
	Learning Rate: 1.3626e-05
	LOSS [training: 0.05609882229459011 | validation: 0.0344669251263513]
	TIME [epoch: 8.28 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05014667562994303		[learning rate: 1.3577e-05]
	Learning Rate: 1.35766e-05
	LOSS [training: 0.05014667562994303 | validation: 0.03265130704403564]
	TIME [epoch: 8.28 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05685093484730579		[learning rate: 1.3527e-05]
	Learning Rate: 1.35273e-05
	LOSS [training: 0.05685093484730579 | validation: 0.0251369659923738]
	TIME [epoch: 8.3 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0562581504581471		[learning rate: 1.3478e-05]
	Learning Rate: 1.34782e-05
	LOSS [training: 0.0562581504581471 | validation: 0.026754571547845905]
	TIME [epoch: 8.29 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.046185823104912635		[learning rate: 1.3429e-05]
	Learning Rate: 1.34293e-05
	LOSS [training: 0.046185823104912635 | validation: 0.03374153158078972]
	TIME [epoch: 8.28 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04637589048828987		[learning rate: 1.3381e-05]
	Learning Rate: 1.33805e-05
	LOSS [training: 0.04637589048828987 | validation: 0.029515820876743485]
	TIME [epoch: 8.28 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05662138721192047		[learning rate: 1.3332e-05]
	Learning Rate: 1.3332e-05
	LOSS [training: 0.05662138721192047 | validation: 0.03688594425700899]
	TIME [epoch: 8.3 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054926464630589644		[learning rate: 1.3284e-05]
	Learning Rate: 1.32836e-05
	LOSS [training: 0.054926464630589644 | validation: 0.030817787009703907]
	TIME [epoch: 8.28 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0559285339091003		[learning rate: 1.3235e-05]
	Learning Rate: 1.32354e-05
	LOSS [training: 0.0559285339091003 | validation: 0.03370768871998362]
	TIME [epoch: 8.28 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05282900389855481		[learning rate: 1.3187e-05]
	Learning Rate: 1.31874e-05
	LOSS [training: 0.05282900389855481 | validation: 0.02959065766504909]
	TIME [epoch: 8.28 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0520210433117936		[learning rate: 1.314e-05]
	Learning Rate: 1.31395e-05
	LOSS [training: 0.0520210433117936 | validation: 0.03111101951849402]
	TIME [epoch: 8.31 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05576619862446345		[learning rate: 1.3092e-05]
	Learning Rate: 1.30918e-05
	LOSS [training: 0.05576619862446345 | validation: 0.031624389539719286]
	TIME [epoch: 8.28 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05061476540739016		[learning rate: 1.3044e-05]
	Learning Rate: 1.30443e-05
	LOSS [training: 0.05061476540739016 | validation: 0.03611302736598784]
	TIME [epoch: 8.28 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05352888710336525		[learning rate: 1.2997e-05]
	Learning Rate: 1.2997e-05
	LOSS [training: 0.05352888710336525 | validation: 0.03046385420794994]
	TIME [epoch: 8.29 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052354665950812417		[learning rate: 1.295e-05]
	Learning Rate: 1.29498e-05
	LOSS [training: 0.052354665950812417 | validation: 0.03878009247080075]
	TIME [epoch: 8.31 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05504532029295446		[learning rate: 1.2903e-05]
	Learning Rate: 1.29028e-05
	LOSS [training: 0.05504532029295446 | validation: 0.028702227090935097]
	TIME [epoch: 8.28 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0567776562837351		[learning rate: 1.2856e-05]
	Learning Rate: 1.2856e-05
	LOSS [training: 0.0567776562837351 | validation: 0.037881497176991075]
	TIME [epoch: 8.28 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052642020537898904		[learning rate: 1.2809e-05]
	Learning Rate: 1.28093e-05
	LOSS [training: 0.052642020537898904 | validation: 0.029976518197085918]
	TIME [epoch: 8.29 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053526721300446486		[learning rate: 1.2763e-05]
	Learning Rate: 1.27628e-05
	LOSS [training: 0.053526721300446486 | validation: 0.036004299532952214]
	TIME [epoch: 8.31 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04910956996336093		[learning rate: 1.2717e-05]
	Learning Rate: 1.27165e-05
	LOSS [training: 0.04910956996336093 | validation: 0.03759782880448706]
	TIME [epoch: 8.28 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04871275467879356		[learning rate: 1.267e-05]
	Learning Rate: 1.26704e-05
	LOSS [training: 0.04871275467879356 | validation: 0.03746778795577557]
	TIME [epoch: 8.28 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.047179615484175194		[learning rate: 1.2624e-05]
	Learning Rate: 1.26244e-05
	LOSS [training: 0.047179615484175194 | validation: 0.039771186203254816]
	TIME [epoch: 8.28 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05068770064933984		[learning rate: 1.2579e-05]
	Learning Rate: 1.25786e-05
	LOSS [training: 0.05068770064933984 | validation: 0.041073764180623556]
	TIME [epoch: 8.3 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05080990287467071		[learning rate: 1.2533e-05]
	Learning Rate: 1.25329e-05
	LOSS [training: 0.05080990287467071 | validation: 0.034915613689478875]
	TIME [epoch: 8.28 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05197436079325972		[learning rate: 1.2487e-05]
	Learning Rate: 1.24874e-05
	LOSS [training: 0.05197436079325972 | validation: 0.024353878947280694]
	TIME [epoch: 8.28 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054123307002255164		[learning rate: 1.2442e-05]
	Learning Rate: 1.24421e-05
	LOSS [training: 0.054123307002255164 | validation: 0.04190689382901466]
	TIME [epoch: 8.28 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04971717839846606		[learning rate: 1.2397e-05]
	Learning Rate: 1.2397e-05
	LOSS [training: 0.04971717839846606 | validation: 0.03268550476880987]
	TIME [epoch: 8.31 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055006832480403944		[learning rate: 1.2352e-05]
	Learning Rate: 1.2352e-05
	LOSS [training: 0.055006832480403944 | validation: 0.031024249966493937]
	TIME [epoch: 8.28 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.049870640012515545		[learning rate: 1.2307e-05]
	Learning Rate: 1.23072e-05
	LOSS [training: 0.049870640012515545 | validation: 0.032867975565398486]
	TIME [epoch: 8.28 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05167779105535092		[learning rate: 1.2263e-05]
	Learning Rate: 1.22625e-05
	LOSS [training: 0.05167779105535092 | validation: 0.02898190062668802]
	TIME [epoch: 8.28 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052989717169734565		[learning rate: 1.2218e-05]
	Learning Rate: 1.2218e-05
	LOSS [training: 0.052989717169734565 | validation: 0.028616302727221692]
	TIME [epoch: 8.31 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05737579890692235		[learning rate: 1.2174e-05]
	Learning Rate: 1.21737e-05
	LOSS [training: 0.05737579890692235 | validation: 0.038435968199496025]
	TIME [epoch: 8.28 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05410539308449569		[learning rate: 1.2129e-05]
	Learning Rate: 1.21295e-05
	LOSS [training: 0.05410539308449569 | validation: 0.030284551029813957]
	TIME [epoch: 8.28 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05359523312141098		[learning rate: 1.2085e-05]
	Learning Rate: 1.20855e-05
	LOSS [training: 0.05359523312141098 | validation: 0.028177715029766934]
	TIME [epoch: 8.28 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05130715757257478		[learning rate: 1.2042e-05]
	Learning Rate: 1.20416e-05
	LOSS [training: 0.05130715757257478 | validation: 0.03024068658086141]
	TIME [epoch: 8.31 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05359284062451344		[learning rate: 1.1998e-05]
	Learning Rate: 1.19979e-05
	LOSS [training: 0.05359284062451344 | validation: 0.038033883798140565]
	TIME [epoch: 8.29 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05427638368820224		[learning rate: 1.1954e-05]
	Learning Rate: 1.19544e-05
	LOSS [training: 0.05427638368820224 | validation: 0.03715633535100315]
	TIME [epoch: 8.28 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05273100324669947		[learning rate: 1.1911e-05]
	Learning Rate: 1.1911e-05
	LOSS [training: 0.05273100324669947 | validation: 0.02810769393079261]
	TIME [epoch: 8.28 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04866645446464311		[learning rate: 1.1868e-05]
	Learning Rate: 1.18678e-05
	LOSS [training: 0.04866645446464311 | validation: 0.028207409473981204]
	TIME [epoch: 8.3 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052932395851903666		[learning rate: 1.1825e-05]
	Learning Rate: 1.18247e-05
	LOSS [training: 0.052932395851903666 | validation: 0.03561885176942731]
	TIME [epoch: 8.29 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05059588344915574		[learning rate: 1.1782e-05]
	Learning Rate: 1.17818e-05
	LOSS [training: 0.05059588344915574 | validation: 0.03401021552059123]
	TIME [epoch: 8.28 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05673183581868631		[learning rate: 1.1739e-05]
	Learning Rate: 1.1739e-05
	LOSS [training: 0.05673183581868631 | validation: 0.024591373483878597]
	TIME [epoch: 8.28 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05162885814150388		[learning rate: 1.1696e-05]
	Learning Rate: 1.16964e-05
	LOSS [training: 0.05162885814150388 | validation: 0.035509505012616835]
	TIME [epoch: 8.3 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05195691880165678		[learning rate: 1.1654e-05]
	Learning Rate: 1.1654e-05
	LOSS [training: 0.05195691880165678 | validation: 0.03322017720202612]
	TIME [epoch: 8.28 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05295018986758186		[learning rate: 1.1612e-05]
	Learning Rate: 1.16117e-05
	LOSS [training: 0.05295018986758186 | validation: 0.02937042552949143]
	TIME [epoch: 8.28 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.056187282014342664		[learning rate: 1.157e-05]
	Learning Rate: 1.15695e-05
	LOSS [training: 0.056187282014342664 | validation: 0.031370076336438193]
	TIME [epoch: 8.28 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.055855536217946464		[learning rate: 1.1528e-05]
	Learning Rate: 1.15275e-05
	LOSS [training: 0.055855536217946464 | validation: 0.03376440233242961]
	TIME [epoch: 8.31 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.057580100094473105		[learning rate: 1.1486e-05]
	Learning Rate: 1.14857e-05
	LOSS [training: 0.057580100094473105 | validation: 0.026699667321717743]
	TIME [epoch: 8.28 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05361406004008196		[learning rate: 1.1444e-05]
	Learning Rate: 1.1444e-05
	LOSS [training: 0.05361406004008196 | validation: 0.037963023542089955]
	TIME [epoch: 8.29 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05370275725724301		[learning rate: 1.1402e-05]
	Learning Rate: 1.14025e-05
	LOSS [training: 0.05370275725724301 | validation: 0.03332805065925324]
	TIME [epoch: 8.28 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05498995719954593		[learning rate: 1.1361e-05]
	Learning Rate: 1.13611e-05
	LOSS [training: 0.05498995719954593 | validation: 0.03447476219666312]
	TIME [epoch: 8.31 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05189222941032208		[learning rate: 1.132e-05]
	Learning Rate: 1.13199e-05
	LOSS [training: 0.05189222941032208 | validation: 0.02877315371612254]
	TIME [epoch: 8.28 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05399338304703227		[learning rate: 1.1279e-05]
	Learning Rate: 1.12788e-05
	LOSS [training: 0.05399338304703227 | validation: 0.023831339797807476]
	TIME [epoch: 8.28 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05271245321503755		[learning rate: 1.1238e-05]
	Learning Rate: 1.12379e-05
	LOSS [training: 0.05271245321503755 | validation: 0.03237024149682061]
	TIME [epoch: 8.28 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053177499601972975		[learning rate: 1.1197e-05]
	Learning Rate: 1.11971e-05
	LOSS [training: 0.053177499601972975 | validation: 0.03278195287668072]
	TIME [epoch: 8.31 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05511646200185967		[learning rate: 1.1156e-05]
	Learning Rate: 1.11565e-05
	LOSS [training: 0.05511646200185967 | validation: 0.03526916688902701]
	TIME [epoch: 8.28 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05501575052624219		[learning rate: 1.1116e-05]
	Learning Rate: 1.1116e-05
	LOSS [training: 0.05501575052624219 | validation: 0.04036955081618021]
	TIME [epoch: 8.28 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.049996252508778725		[learning rate: 1.1076e-05]
	Learning Rate: 1.10756e-05
	LOSS [training: 0.049996252508778725 | validation: 0.03174097508287451]
	TIME [epoch: 8.28 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05328848232884707		[learning rate: 1.1035e-05]
	Learning Rate: 1.10354e-05
	LOSS [training: 0.05328848232884707 | validation: 0.03619961251511765]
	TIME [epoch: 8.31 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05309551093794766		[learning rate: 1.0995e-05]
	Learning Rate: 1.09954e-05
	LOSS [training: 0.05309551093794766 | validation: 0.03861272445218612]
	TIME [epoch: 8.28 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05038302293600909		[learning rate: 1.0955e-05]
	Learning Rate: 1.09555e-05
	LOSS [training: 0.05038302293600909 | validation: 0.02979394093326334]
	TIME [epoch: 8.28 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04990545472260841		[learning rate: 1.0916e-05]
	Learning Rate: 1.09157e-05
	LOSS [training: 0.04990545472260841 | validation: 0.022906050717960008]
	TIME [epoch: 8.27 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.051975243852372156		[learning rate: 1.0876e-05]
	Learning Rate: 1.08761e-05
	LOSS [training: 0.051975243852372156 | validation: 0.03954335963047173]
	TIME [epoch: 8.3 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05051879219834924		[learning rate: 1.0837e-05]
	Learning Rate: 1.08366e-05
	LOSS [training: 0.05051879219834924 | validation: 0.03792442715380172]
	TIME [epoch: 8.28 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053668039600851425		[learning rate: 1.0797e-05]
	Learning Rate: 1.07973e-05
	LOSS [training: 0.053668039600851425 | validation: 0.038797947544114544]
	TIME [epoch: 8.28 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05448747535194924		[learning rate: 1.0758e-05]
	Learning Rate: 1.07581e-05
	LOSS [training: 0.05448747535194924 | validation: 0.03249198497718582]
	TIME [epoch: 8.28 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05130714634927186		[learning rate: 1.0719e-05]
	Learning Rate: 1.07191e-05
	LOSS [training: 0.05130714634927186 | validation: 0.023029439838586577]
	TIME [epoch: 8.3 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.051364682011528794		[learning rate: 1.068e-05]
	Learning Rate: 1.06802e-05
	LOSS [training: 0.051364682011528794 | validation: 0.03211487469955818]
	TIME [epoch: 8.29 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05400087222203137		[learning rate: 1.0641e-05]
	Learning Rate: 1.06414e-05
	LOSS [training: 0.05400087222203137 | validation: 0.043837059017093796]
	TIME [epoch: 8.28 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05059779493450476		[learning rate: 1.0603e-05]
	Learning Rate: 1.06028e-05
	LOSS [training: 0.05059779493450476 | validation: 0.03472434428539001]
	TIME [epoch: 8.29 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05344352692871		[learning rate: 1.0564e-05]
	Learning Rate: 1.05643e-05
	LOSS [training: 0.05344352692871 | validation: 0.0357082236599156]
	TIME [epoch: 8.3 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.053742553131256185		[learning rate: 1.0526e-05]
	Learning Rate: 1.0526e-05
	LOSS [training: 0.053742553131256185 | validation: 0.021592701203817413]
	TIME [epoch: 8.29 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05218465849512344		[learning rate: 1.0488e-05]
	Learning Rate: 1.04878e-05
	LOSS [training: 0.05218465849512344 | validation: 0.030312000921740835]
	TIME [epoch: 8.28 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05410635603144577		[learning rate: 1.045e-05]
	Learning Rate: 1.04497e-05
	LOSS [training: 0.05410635603144577 | validation: 0.034347219599088236]
	TIME [epoch: 8.29 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05220778090160873		[learning rate: 1.0412e-05]
	Learning Rate: 1.04118e-05
	LOSS [training: 0.05220778090160873 | validation: 0.03189067418090878]
	TIME [epoch: 8.3 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.054028182369776065		[learning rate: 1.0374e-05]
	Learning Rate: 1.0374e-05
	LOSS [training: 0.054028182369776065 | validation: 0.02448517000907348]
	TIME [epoch: 8.28 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04904625359791801		[learning rate: 1.0336e-05]
	Learning Rate: 1.03364e-05
	LOSS [training: 0.04904625359791801 | validation: 0.033981555953016135]
	TIME [epoch: 8.28 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05781132488132943		[learning rate: 1.0299e-05]
	Learning Rate: 1.02989e-05
	LOSS [training: 0.05781132488132943 | validation: 0.02963317989545934]
	TIME [epoch: 8.28 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05088787199481519		[learning rate: 1.0261e-05]
	Learning Rate: 1.02615e-05
	LOSS [training: 0.05088787199481519 | validation: 0.033055961899881775]
	TIME [epoch: 8.31 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.04743891531838267		[learning rate: 1.0224e-05]
	Learning Rate: 1.02243e-05
	LOSS [training: 0.04743891531838267 | validation: 0.03266933106436471]
	TIME [epoch: 8.28 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.052477651107712985		[learning rate: 1.0187e-05]
	Learning Rate: 1.01872e-05
	LOSS [training: 0.052477651107712985 | validation: 0.04267100353318537]
	TIME [epoch: 8.28 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05448683735716905		[learning rate: 1.015e-05]
	Learning Rate: 1.01502e-05
	LOSS [training: 0.05448683735716905 | validation: 0.031467467290286946]
	TIME [epoch: 8.28 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.0537644832944297		[learning rate: 1.0113e-05]
	Learning Rate: 1.01133e-05
	LOSS [training: 0.0537644832944297 | validation: 0.035344368762939234]
	TIME [epoch: 8.31 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.046179380648533906		[learning rate: 1.0077e-05]
	Learning Rate: 1.00766e-05
	LOSS [training: 0.046179380648533906 | validation: 0.0249740425063996]
	TIME [epoch: 8.28 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.047905025581706104		[learning rate: 1.004e-05]
	Learning Rate: 1.00401e-05
	LOSS [training: 0.047905025581706104 | validation: 0.03621197875344899]
	TIME [epoch: 8.29 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.05160172658235235		[learning rate: 1.0004e-05]
	Learning Rate: 1.00036e-05
	LOSS [training: 0.05160172658235235 | validation: 0.034750853620900676]
	TIME [epoch: 8.28 sec]
Finished training in 16744.056 seconds.
