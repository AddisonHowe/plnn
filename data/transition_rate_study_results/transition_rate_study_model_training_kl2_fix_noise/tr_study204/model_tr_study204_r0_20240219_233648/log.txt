Args:
Namespace(name='model_tr_study204', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0', training_data='data/transition_rate_studies/tr_study204/tr_study204_training/r0', validation_data='data/transition_rate_studies/tr_study204/tr_study204_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3178701081

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/10] avg loss: 11.028806016908005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.028806016908005 | validation: 11.566300414916856]
	TIME [epoch: 79.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.222439301144622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.222439301144622 | validation: 9.450766666060836]
	TIME [epoch: 8.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.099186902835196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.099186902835196 | validation: 6.8188269761973554]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.289044907866599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.289044907866599 | validation: 7.392918005004953]
	TIME [epoch: 8.54 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 10/10] avg loss: 6.02779789168416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.02779789168416 | validation: 6.2042099371057375]
	TIME [epoch: 8.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.494429591854252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.494429591854252 | validation: 6.920751340609325]
	TIME [epoch: 8.55 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.629345323343856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.629345323343856 | validation: 4.573690291576417]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.12934672020321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.12934672020321 | validation: 4.624602641460199]
	TIME [epoch: 8.55 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.376683502826335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.376683502826335 | validation: 5.309014542298328]
	TIME [epoch: 8.56 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.168579707992405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.168579707992405 | validation: 4.4404040588384]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.928774350867666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.928774350867666 | validation: 4.656808713129724]
	TIME [epoch: 8.54 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.964701913207166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.964701913207166 | validation: 4.406632335062089]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.903816572049034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.903816572049034 | validation: 4.246625031164887]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.743310230691389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.743310230691389 | validation: 3.977069580993935]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.550333483396093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.550333483396093 | validation: 4.106784185684812]
	TIME [epoch: 8.54 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.552841307655263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.552841307655263 | validation: 3.98028448203976]
	TIME [epoch: 8.55 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.4183909651584425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4183909651584425 | validation: 3.9807461499165795]
	TIME [epoch: 8.57 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.328347747035289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.328347747035289 | validation: 4.041337157106298]
	TIME [epoch: 8.55 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.315734313134412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.315734313134412 | validation: 3.67092316719861]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.7878942418681465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7878942418681465 | validation: 3.7359549330947015]
	TIME [epoch: 8.54 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.201421396480254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.201421396480254 | validation: 6.525584094442282]
	TIME [epoch: 8.56 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.568499842412544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.568499842412544 | validation: 3.5206547660723935]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.2299910816041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2299910816041 | validation: 3.6202668153069806]
	TIME [epoch: 8.55 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.167533815707334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.167533815707334 | validation: 4.243896893535554]
	TIME [epoch: 8.54 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.179108778228373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.179108778228373 | validation: 3.9226931498132505]
	TIME [epoch: 8.55 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1989832364958435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1989832364958435 | validation: 3.4115385651356167]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.138115091567788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.138115091567788 | validation: 4.228213096353003]
	TIME [epoch: 8.54 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.144630515165719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.144630515165719 | validation: 3.199727140907104]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9191314570963547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9191314570963547 | validation: 3.518264013436804]
	TIME [epoch: 8.55 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9635710712760663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9635710712760663 | validation: 3.6385928387912045]
	TIME [epoch: 8.57 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.967907734356328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.967907734356328 | validation: 3.1434600297425517]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8205544751717753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8205544751717753 | validation: 3.5456044222675693]
	TIME [epoch: 8.54 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9511995969606715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9511995969606715 | validation: 3.3713713128797127]
	TIME [epoch: 8.54 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9451716612651864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9451716612651864 | validation: 3.2862174498074577]
	TIME [epoch: 8.57 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.854028525295945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.854028525295945 | validation: 3.3600875359904787]
	TIME [epoch: 8.54 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8573084367117096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8573084367117096 | validation: 3.490168978515761]
	TIME [epoch: 8.53 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.11641397189716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.11641397189716 | validation: 5.018276008318995]
	TIME [epoch: 8.54 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.117680921358341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.117680921358341 | validation: 8.12545368666823]
	TIME [epoch: 8.57 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.8146933649112835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.8146933649112835 | validation: 3.4716826604201136]
	TIME [epoch: 8.54 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.890583670417126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.890583670417126 | validation: 3.2361061419953288]
	TIME [epoch: 8.53 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.680909022866037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.680909022866037 | validation: 3.0602666985958042]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.8293788590938584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8293788590938584 | validation: 3.684074078682149]
	TIME [epoch: 8.56 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.668645355588657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.668645355588657 | validation: 4.068905579262253]
	TIME [epoch: 8.56 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.528872277849191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.528872277849191 | validation: 3.2541451270860855]
	TIME [epoch: 8.54 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7642058051672818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7642058051672818 | validation: 3.085334652566131]
	TIME [epoch: 8.54 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7722179304904153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7722179304904153 | validation: 3.135551885657786]
	TIME [epoch: 8.54 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.670958893104056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.670958893104056 | validation: 3.1199849881287838]
	TIME [epoch: 8.56 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.719252035760237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.719252035760237 | validation: 3.3340200274090646]
	TIME [epoch: 8.54 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7535664064387406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7535664064387406 | validation: 3.247193463451695]
	TIME [epoch: 8.54 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7341589931983314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7341589931983314 | validation: 3.334792764432246]
	TIME [epoch: 8.55 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1573406461901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1573406461901 | validation: 3.799067276971226]
	TIME [epoch: 8.57 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.016805579394149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.016805579394149 | validation: 3.9075343528248383]
	TIME [epoch: 8.54 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.113349120882379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.113349120882379 | validation: 2.984539426021078]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.658840077898288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.658840077898288 | validation: 3.0076510190196766]
	TIME [epoch: 8.54 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5763150511806523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5763150511806523 | validation: 3.043725913386672]
	TIME [epoch: 8.56 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.600727470209512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.600727470209512 | validation: 3.297717924110426]
	TIME [epoch: 8.54 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5637786610907747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5637786610907747 | validation: 2.9051594562238816]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.556254525774686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.556254525774686 | validation: 3.2479560905266442]
	TIME [epoch: 8.54 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5790558686788914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5790558686788914 | validation: 2.9799899161032406]
	TIME [epoch: 8.56 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.512892611270071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.512892611270071 | validation: 2.8150155148999403]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6631111482899117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6631111482899117 | validation: 3.047886114722571]
	TIME [epoch: 8.54 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5756148537062296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5756148537062296 | validation: 3.0891929831667477]
	TIME [epoch: 8.52 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5454158153849726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5454158153849726 | validation: 2.9346634362966144]
	TIME [epoch: 8.55 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.930750980876671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.930750980876671 | validation: 4.05740419061935]
	TIME [epoch: 8.54 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.839108111762768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.839108111762768 | validation: 3.6089429889805036]
	TIME [epoch: 8.54 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8064533320241174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8064533320241174 | validation: 2.8866752861200045]
	TIME [epoch: 8.53 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5228139603642012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5228139603642012 | validation: 2.995874916148299]
	TIME [epoch: 8.53 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.551336172649642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.551336172649642 | validation: 2.8687034552346766]
	TIME [epoch: 8.55 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.040132599667589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.040132599667589 | validation: 3.193349385655632]
	TIME [epoch: 8.53 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.140648533696898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.140648533696898 | validation: 3.034724654224908]
	TIME [epoch: 8.53 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7951559745387655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7951559745387655 | validation: 2.9574987970160866]
	TIME [epoch: 8.53 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.154823940884844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.154823940884844 | validation: 2.9661500706628035]
	TIME [epoch: 8.56 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7147191585680384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7147191585680384 | validation: 2.734220355261827]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5964223573392915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5964223573392915 | validation: 3.846575580316602]
	TIME [epoch: 8.54 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8127217623456433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8127217623456433 | validation: 3.613462116367986]
	TIME [epoch: 8.53 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.657181410403966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.657181410403966 | validation: 2.7580663033072264]
	TIME [epoch: 8.56 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.413285902289305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.413285902289305 | validation: 2.9387827819553887]
	TIME [epoch: 8.53 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5126177784991386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5126177784991386 | validation: 2.6903039906669197]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.472982661836921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.472982661836921 | validation: 3.5044070026290957]
	TIME [epoch: 8.54 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.478225544759281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.478225544759281 | validation: 2.9337601802778224]
	TIME [epoch: 8.57 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6682245107271925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6682245107271925 | validation: 2.926634647739172]
	TIME [epoch: 8.54 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7052217866960815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7052217866960815 | validation: 2.9925256202034993]
	TIME [epoch: 8.53 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.878541414060629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.878541414060629 | validation: 2.6881359391895145]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5629042674522147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5629042674522147 | validation: 3.45305829295992]
	TIME [epoch: 8.87 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7182017366624875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7182017366624875 | validation: 3.6530800751166383]
	TIME [epoch: 8.56 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.91765648595105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.91765648595105 | validation: 2.6155423427942814]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.717132611948872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.717132611948872 | validation: 2.6771047621165662]
	TIME [epoch: 8.55 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7193315353778402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7193315353778402 | validation: 3.1024506437091803]
	TIME [epoch: 8.57 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.87777595971463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.87777595971463 | validation: 3.7820016457838985]
	TIME [epoch: 8.56 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.040467271934679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.040467271934679 | validation: 4.090633958568096]
	TIME [epoch: 8.55 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.668274966082825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.668274966082825 | validation: 2.6249010973988556]
	TIME [epoch: 8.55 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.2876409895137955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2876409895137955 | validation: 2.8132575806111895]
	TIME [epoch: 8.55 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.352495061954696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.352495061954696 | validation: 2.7877719637119887]
	TIME [epoch: 8.58 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5660118164110157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5660118164110157 | validation: 2.717635079800269]
	TIME [epoch: 8.55 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.455086282513914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.455086282513914 | validation: 3.9098250572866444]
	TIME [epoch: 8.55 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.709977227418277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.709977227418277 | validation: 2.7482139245101056]
	TIME [epoch: 8.55 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4586205195052697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4586205195052697 | validation: 2.685014614945163]
	TIME [epoch: 8.58 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8747187509428462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8747187509428462 | validation: 2.8641958547269297]
	TIME [epoch: 8.55 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.534301482890622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.534301482890622 | validation: 2.744532484784088]
	TIME [epoch: 8.55 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.310736096085898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.310736096085898 | validation: 3.2036675392818523]
	TIME [epoch: 8.55 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5670152075074713		[learning rate: 0.0099673]
	Learning Rate: 0.00996733
	LOSS [training: 3.5670152075074713 | validation: 2.8293407876373116]
	TIME [epoch: 8.57 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5083661773643846		[learning rate: 0.0099312]
	Learning Rate: 0.00993116
	LOSS [training: 3.5083661773643846 | validation: 2.6090501006986484]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3706653651774303		[learning rate: 0.0098951]
	Learning Rate: 0.00989512
	LOSS [training: 3.3706653651774303 | validation: 4.654299988007953]
	TIME [epoch: 8.55 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8643501463666468		[learning rate: 0.0098592]
	Learning Rate: 0.00985921
	LOSS [training: 3.8643501463666468 | validation: 3.584432112401659]
	TIME [epoch: 8.55 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.846222492486819		[learning rate: 0.0098234]
	Learning Rate: 0.00982343
	LOSS [training: 3.846222492486819 | validation: 3.365262744722081]
	TIME [epoch: 8.57 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6209928513004295		[learning rate: 0.0097878]
	Learning Rate: 0.00978778
	LOSS [training: 3.6209928513004295 | validation: 2.6143879840751647]
	TIME [epoch: 8.56 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7271662661227296		[learning rate: 0.0097523]
	Learning Rate: 0.00975226
	LOSS [training: 3.7271662661227296 | validation: 2.619694786823494]
	TIME [epoch: 8.55 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.35682882658089		[learning rate: 0.0097169]
	Learning Rate: 0.00971687
	LOSS [training: 3.35682882658089 | validation: 2.7699320721692455]
	TIME [epoch: 8.55 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.296818856952592		[learning rate: 0.0096816]
	Learning Rate: 0.0096816
	LOSS [training: 3.296818856952592 | validation: 3.11087461119335]
	TIME [epoch: 8.56 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.349352876883816		[learning rate: 0.0096465]
	Learning Rate: 0.00964647
	LOSS [training: 3.349352876883816 | validation: 2.6219008195665623]
	TIME [epoch: 8.57 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.500505708692409		[learning rate: 0.0096115]
	Learning Rate: 0.00961146
	LOSS [training: 3.500505708692409 | validation: 2.805763558509937]
	TIME [epoch: 8.55 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.313651670872234		[learning rate: 0.0095766]
	Learning Rate: 0.00957658
	LOSS [training: 3.313651670872234 | validation: 3.5821022808468777]
	TIME [epoch: 8.55 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6211084833310787		[learning rate: 0.0095418]
	Learning Rate: 0.00954183
	LOSS [training: 3.6211084833310787 | validation: 2.7856118012689874]
	TIME [epoch: 8.55 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1831217695040683		[learning rate: 0.0095072]
	Learning Rate: 0.0095072
	LOSS [training: 3.1831217695040683 | validation: 2.414131664446123]
	TIME [epoch: 8.58 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.662737102389253		[learning rate: 0.0094727]
	Learning Rate: 0.0094727
	LOSS [training: 2.662737102389253 | validation: 2.1911241808324866]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8299356509877738		[learning rate: 0.0094383]
	Learning Rate: 0.00943832
	LOSS [training: 2.8299356509877738 | validation: 2.2514456226186024]
	TIME [epoch: 8.55 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1341524025717993		[learning rate: 0.0094041]
	Learning Rate: 0.00940407
	LOSS [training: 3.1341524025717993 | validation: 2.2004644572510013]
	TIME [epoch: 8.55 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.756344484595998		[learning rate: 0.0093699]
	Learning Rate: 0.00936994
	LOSS [training: 2.756344484595998 | validation: 2.239911153752261]
	TIME [epoch: 8.58 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.845871087321334		[learning rate: 0.0093359]
	Learning Rate: 0.00933594
	LOSS [training: 2.845871087321334 | validation: 2.986426598582181]
	TIME [epoch: 8.55 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.103995782681742		[learning rate: 0.0093021]
	Learning Rate: 0.00930206
	LOSS [training: 3.103995782681742 | validation: 2.3623441419842153]
	TIME [epoch: 8.55 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0565613619619763		[learning rate: 0.0092683]
	Learning Rate: 0.0092683
	LOSS [training: 3.0565613619619763 | validation: 2.2350427349398814]
	TIME [epoch: 8.55 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.775519658011107		[learning rate: 0.0092347]
	Learning Rate: 0.00923466
	LOSS [training: 2.775519658011107 | validation: 2.745755615958613]
	TIME [epoch: 8.57 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7438627123412034		[learning rate: 0.0092011]
	Learning Rate: 0.00920115
	LOSS [training: 2.7438627123412034 | validation: 2.1423143037531434]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.819146718872365		[learning rate: 0.0091678]
	Learning Rate: 0.00916776
	LOSS [training: 2.819146718872365 | validation: 2.308005346004181]
	TIME [epoch: 8.54 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.6529726051877014		[learning rate: 0.0091345]
	Learning Rate: 0.00913449
	LOSS [training: 2.6529726051877014 | validation: 2.0874783592460937]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.298825739379791		[learning rate: 0.0091013]
	Learning Rate: 0.00910134
	LOSS [training: 3.298825739379791 | validation: 2.1138634313921445]
	TIME [epoch: 8.56 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7199537102039226		[learning rate: 0.0090683]
	Learning Rate: 0.00906831
	LOSS [training: 2.7199537102039226 | validation: 2.303106427087876]
	TIME [epoch: 8.54 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7814847095636983		[learning rate: 0.0090354]
	Learning Rate: 0.0090354
	LOSS [training: 2.7814847095636983 | validation: 2.352720579356908]
	TIME [epoch: 8.54 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.591486726348066		[learning rate: 0.0090026]
	Learning Rate: 0.00900261
	LOSS [training: 2.591486726348066 | validation: 2.0602717976281464]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.480771335985259		[learning rate: 0.0089699]
	Learning Rate: 0.00896994
	LOSS [training: 2.480771335985259 | validation: 2.038544119240426]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.447835385199159		[learning rate: 0.0089374]
	Learning Rate: 0.00893739
	LOSS [training: 2.447835385199159 | validation: 2.37518038078879]
	TIME [epoch: 8.55 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.4031705758206634		[learning rate: 0.008905]
	Learning Rate: 0.00890495
	LOSS [training: 2.4031705758206634 | validation: 2.3890884368260497]
	TIME [epoch: 8.54 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.345797785734328		[learning rate: 0.0088726]
	Learning Rate: 0.00887263
	LOSS [training: 2.345797785734328 | validation: 1.6921698926982884]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8105066258443283		[learning rate: 0.0088404]
	Learning Rate: 0.00884044
	LOSS [training: 1.8105066258443283 | validation: 2.4360723446526995]
	TIME [epoch: 8.55 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.728554689958937		[learning rate: 0.0088084]
	Learning Rate: 0.00880835
	LOSS [training: 1.728554689958937 | validation: 1.0009946586492355]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4665270662407544		[learning rate: 0.0087764]
	Learning Rate: 0.00877639
	LOSS [training: 1.4665270662407544 | validation: 2.0557896731871934]
	TIME [epoch: 8.54 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9961234370227523		[learning rate: 0.0087445]
	Learning Rate: 0.00874454
	LOSS [training: 1.9961234370227523 | validation: 1.3898716980265506]
	TIME [epoch: 8.54 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4401032492498773		[learning rate: 0.0087128]
	Learning Rate: 0.0087128
	LOSS [training: 1.4401032492498773 | validation: 1.1208745693352817]
	TIME [epoch: 8.53 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2846636832649667		[learning rate: 0.0086812]
	Learning Rate: 0.00868118
	LOSS [training: 1.2846636832649667 | validation: 0.9077615983748015]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1229470555378456		[learning rate: 0.0086497]
	Learning Rate: 0.00864968
	LOSS [training: 1.1229470555378456 | validation: 0.6605514056645031]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1755633022801362		[learning rate: 0.0086183]
	Learning Rate: 0.00861829
	LOSS [training: 1.1755633022801362 | validation: 2.3939928307639704]
	TIME [epoch: 8.53 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3258701950912506		[learning rate: 0.008587]
	Learning Rate: 0.00858701
	LOSS [training: 1.3258701950912506 | validation: 5.91141747357746]
	TIME [epoch: 8.53 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7491732293406463		[learning rate: 0.0085559]
	Learning Rate: 0.00855585
	LOSS [training: 1.7491732293406463 | validation: 0.5329218310585933]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.85984913350283		[learning rate: 0.0085248]
	Learning Rate: 0.0085248
	LOSS [training: 0.85984913350283 | validation: 0.7078258939014033]
	TIME [epoch: 8.55 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1159514284004866		[learning rate: 0.0084939]
	Learning Rate: 0.00849386
	LOSS [training: 1.1159514284004866 | validation: 0.6635344274549908]
	TIME [epoch: 8.55 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0743231661000265		[learning rate: 0.008463]
	Learning Rate: 0.00846304
	LOSS [training: 1.0743231661000265 | validation: 0.5613834596951957]
	TIME [epoch: 8.54 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9967511267957431		[learning rate: 0.0084323]
	Learning Rate: 0.00843233
	LOSS [training: 0.9967511267957431 | validation: 0.6327971799629457]
	TIME [epoch: 8.57 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0208064477755805		[learning rate: 0.0084017]
	Learning Rate: 0.00840172
	LOSS [training: 1.0208064477755805 | validation: 0.8344365437651697]
	TIME [epoch: 8.55 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0638411830521435		[learning rate: 0.0083712]
	Learning Rate: 0.00837123
	LOSS [training: 1.0638411830521435 | validation: 0.9313052678362251]
	TIME [epoch: 8.55 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9952495609239334		[learning rate: 0.0083409]
	Learning Rate: 0.00834085
	LOSS [training: 0.9952495609239334 | validation: 0.9060792175058755]
	TIME [epoch: 8.55 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9477406090300429		[learning rate: 0.0083106]
	Learning Rate: 0.00831059
	LOSS [training: 0.9477406090300429 | validation: 1.0034578449457157]
	TIME [epoch: 8.57 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9729761470820355		[learning rate: 0.0082804]
	Learning Rate: 0.00828043
	LOSS [training: 0.9729761470820355 | validation: 1.2850612615834045]
	TIME [epoch: 8.55 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9385909299199385		[learning rate: 0.0082504]
	Learning Rate: 0.00825037
	LOSS [training: 0.9385909299199385 | validation: 0.8142565495596548]
	TIME [epoch: 8.55 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9707227870690514		[learning rate: 0.0082204]
	Learning Rate: 0.00822043
	LOSS [training: 0.9707227870690514 | validation: 0.8072958238067466]
	TIME [epoch: 8.55 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9760856446645567		[learning rate: 0.0081906]
	Learning Rate: 0.0081906
	LOSS [training: 0.9760856446645567 | validation: 1.6151687388028626]
	TIME [epoch: 8.57 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0853519984099245		[learning rate: 0.0081609]
	Learning Rate: 0.00816088
	LOSS [training: 1.0853519984099245 | validation: 0.6725453635323753]
	TIME [epoch: 8.56 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.050614594412573		[learning rate: 0.0081313]
	Learning Rate: 0.00813126
	LOSS [training: 1.050614594412573 | validation: 1.3684004620579766]
	TIME [epoch: 8.55 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9183769886692185		[learning rate: 0.0081018]
	Learning Rate: 0.00810175
	LOSS [training: 0.9183769886692185 | validation: 0.4612720745308549]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8516679137792224		[learning rate: 0.0080724]
	Learning Rate: 0.00807235
	LOSS [training: 0.8516679137792224 | validation: 0.8065370754098269]
	TIME [epoch: 8.55 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.842696299112986		[learning rate: 0.0080431]
	Learning Rate: 0.00804305
	LOSS [training: 0.842696299112986 | validation: 0.5361095229511216]
	TIME [epoch: 8.56 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8099093911433002		[learning rate: 0.0080139]
	Learning Rate: 0.00801387
	LOSS [training: 0.8099093911433002 | validation: 0.47615337390606854]
	TIME [epoch: 8.55 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8607385553029177		[learning rate: 0.0079848]
	Learning Rate: 0.00798478
	LOSS [training: 0.8607385553029177 | validation: 0.9750082883223619]
	TIME [epoch: 8.54 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4575119517859183		[learning rate: 0.0079558]
	Learning Rate: 0.00795581
	LOSS [training: 1.4575119517859183 | validation: 1.7482880762714679]
	TIME [epoch: 8.54 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2364191681653312		[learning rate: 0.0079269]
	Learning Rate: 0.00792693
	LOSS [training: 1.2364191681653312 | validation: 0.7268239657056885]
	TIME [epoch: 8.57 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8176708156561219		[learning rate: 0.0078982]
	Learning Rate: 0.00789817
	LOSS [training: 0.8176708156561219 | validation: 1.1605525386133309]
	TIME [epoch: 8.54 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8309464777321521		[learning rate: 0.0078695]
	Learning Rate: 0.0078695
	LOSS [training: 0.8309464777321521 | validation: 1.267704478257893]
	TIME [epoch: 8.54 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9939486591363853		[learning rate: 0.0078409]
	Learning Rate: 0.00784094
	LOSS [training: 0.9939486591363853 | validation: 1.1361215774894156]
	TIME [epoch: 8.54 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8141912092814414		[learning rate: 0.0078125]
	Learning Rate: 0.00781249
	LOSS [training: 0.8141912092814414 | validation: 0.5029930375718772]
	TIME [epoch: 8.56 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8701646294413887		[learning rate: 0.0077841]
	Learning Rate: 0.00778414
	LOSS [training: 0.8701646294413887 | validation: 1.2861995615979072]
	TIME [epoch: 8.54 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9161272752833028		[learning rate: 0.0077559]
	Learning Rate: 0.00775589
	LOSS [training: 0.9161272752833028 | validation: 1.0124215481712755]
	TIME [epoch: 8.54 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9908697325258329		[learning rate: 0.0077277]
	Learning Rate: 0.00772774
	LOSS [training: 0.9908697325258329 | validation: 0.839450290977037]
	TIME [epoch: 8.54 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9089442942021015		[learning rate: 0.0076997]
	Learning Rate: 0.0076997
	LOSS [training: 0.9089442942021015 | validation: 0.5577218920584743]
	TIME [epoch: 8.56 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7512480912078658		[learning rate: 0.0076718]
	Learning Rate: 0.00767176
	LOSS [training: 0.7512480912078658 | validation: 1.190145213756784]
	TIME [epoch: 8.56 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.775910900815807		[learning rate: 0.0076439]
	Learning Rate: 0.00764391
	LOSS [training: 0.775910900815807 | validation: 0.6981768318099609]
	TIME [epoch: 8.54 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7511534955214455		[learning rate: 0.0076162]
	Learning Rate: 0.00761617
	LOSS [training: 0.7511534955214455 | validation: 0.48445884497370195]
	TIME [epoch: 8.54 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7301631225951484		[learning rate: 0.0075885]
	Learning Rate: 0.00758853
	LOSS [training: 0.7301631225951484 | validation: 0.482262994477176]
	TIME [epoch: 8.55 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0126416091683201		[learning rate: 0.007561]
	Learning Rate: 0.00756099
	LOSS [training: 1.0126416091683201 | validation: 1.3009996887123592]
	TIME [epoch: 8.56 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8194702865252543		[learning rate: 0.0075336]
	Learning Rate: 0.00753356
	LOSS [training: 0.8194702865252543 | validation: 1.0807855650968183]
	TIME [epoch: 8.54 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8622033104348625		[learning rate: 0.0075062]
	Learning Rate: 0.00750622
	LOSS [training: 0.8622033104348625 | validation: 0.6598180911489253]
	TIME [epoch: 8.54 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8300717138941629		[learning rate: 0.007479]
	Learning Rate: 0.00747897
	LOSS [training: 0.8300717138941629 | validation: 0.5158846049297766]
	TIME [epoch: 8.54 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8245761436814518		[learning rate: 0.0074518]
	Learning Rate: 0.00745183
	LOSS [training: 0.8245761436814518 | validation: 0.6550808898504101]
	TIME [epoch: 8.57 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7536315391112228		[learning rate: 0.0074248]
	Learning Rate: 0.00742479
	LOSS [training: 0.7536315391112228 | validation: 0.5512781453651836]
	TIME [epoch: 8.54 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7305659510055098		[learning rate: 0.0073978]
	Learning Rate: 0.00739784
	LOSS [training: 0.7305659510055098 | validation: 0.4757593428664449]
	TIME [epoch: 8.54 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8758708978460922		[learning rate: 0.007371]
	Learning Rate: 0.007371
	LOSS [training: 0.8758708978460922 | validation: 0.7902438498153053]
	TIME [epoch: 8.54 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8245908034063332		[learning rate: 0.0073442]
	Learning Rate: 0.00734425
	LOSS [training: 0.8245908034063332 | validation: 0.6143598562902359]
	TIME [epoch: 8.56 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.983225876451284		[learning rate: 0.0073176]
	Learning Rate: 0.0073176
	LOSS [training: 0.983225876451284 | validation: 0.44678404604621225]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7206364240865619		[learning rate: 0.007291]
	Learning Rate: 0.00729104
	LOSS [training: 0.7206364240865619 | validation: 0.6434894786448337]
	TIME [epoch: 8.54 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8608189051266166		[learning rate: 0.0072646]
	Learning Rate: 0.00726458
	LOSS [training: 0.8608189051266166 | validation: 0.7556981161547913]
	TIME [epoch: 8.54 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9249292201368		[learning rate: 0.0072382]
	Learning Rate: 0.00723822
	LOSS [training: 0.9249292201368 | validation: 0.5731369384002014]
	TIME [epoch: 8.56 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8060668947058559		[learning rate: 0.0072119]
	Learning Rate: 0.00721195
	LOSS [training: 0.8060668947058559 | validation: 0.5227872126692682]
	TIME [epoch: 8.55 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8835219798411309		[learning rate: 0.0071858]
	Learning Rate: 0.00718578
	LOSS [training: 0.8835219798411309 | validation: 0.4284466472105512]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8772250360897014		[learning rate: 0.0071597]
	Learning Rate: 0.0071597
	LOSS [training: 0.8772250360897014 | validation: 0.5221075881924755]
	TIME [epoch: 8.53 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8180340117795956		[learning rate: 0.0071337]
	Learning Rate: 0.00713371
	LOSS [training: 0.8180340117795956 | validation: 0.9503809467815237]
	TIME [epoch: 8.55 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0093506515380646		[learning rate: 0.0071078]
	Learning Rate: 0.00710783
	LOSS [training: 1.0093506515380646 | validation: 0.6112028398985395]
	TIME [epoch: 8.55 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8453015678970827		[learning rate: 0.007082]
	Learning Rate: 0.00708203
	LOSS [training: 0.8453015678970827 | validation: 1.5480027875662468]
	TIME [epoch: 8.54 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0698879543675424		[learning rate: 0.0070563]
	Learning Rate: 0.00705633
	LOSS [training: 1.0698879543675424 | validation: 0.49565781950613463]
	TIME [epoch: 8.54 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6997797901968749		[learning rate: 0.0070307]
	Learning Rate: 0.00703072
	LOSS [training: 0.6997797901968749 | validation: 0.7860614847469238]
	TIME [epoch: 8.54 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9387975820040599		[learning rate: 0.0070052]
	Learning Rate: 0.00700521
	LOSS [training: 0.9387975820040599 | validation: 0.4073098265494408]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8979719731612343		[learning rate: 0.0069798]
	Learning Rate: 0.00697979
	LOSS [training: 0.8979719731612343 | validation: 0.4636648183807213]
	TIME [epoch: 8.53 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7770270253869684		[learning rate: 0.0069545]
	Learning Rate: 0.00695446
	LOSS [training: 0.7770270253869684 | validation: 0.8082463723556614]
	TIME [epoch: 8.54 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8580904787084238		[learning rate: 0.0069292]
	Learning Rate: 0.00692922
	LOSS [training: 0.8580904787084238 | validation: 0.5617726980839326]
	TIME [epoch: 8.53 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7528860763820628		[learning rate: 0.0069041]
	Learning Rate: 0.00690407
	LOSS [training: 0.7528860763820628 | validation: 0.5012849594633455]
	TIME [epoch: 8.56 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.666018838945578		[learning rate: 0.006879]
	Learning Rate: 0.00687902
	LOSS [training: 0.666018838945578 | validation: 0.7079850762036277]
	TIME [epoch: 8.53 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7870094832511542		[learning rate: 0.0068541]
	Learning Rate: 0.00685405
	LOSS [training: 0.7870094832511542 | validation: 1.2607456029066137]
	TIME [epoch: 8.54 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.843861173587354		[learning rate: 0.0068292]
	Learning Rate: 0.00682918
	LOSS [training: 0.843861173587354 | validation: 1.2602672113053317]
	TIME [epoch: 8.53 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6954771145179351		[learning rate: 0.0068044]
	Learning Rate: 0.00680439
	LOSS [training: 0.6954771145179351 | validation: 0.4229494680409712]
	TIME [epoch: 8.56 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6695417239359008		[learning rate: 0.0067797]
	Learning Rate: 0.0067797
	LOSS [training: 0.6695417239359008 | validation: 0.46826262020947196]
	TIME [epoch: 8.54 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7711275158302124		[learning rate: 0.0067551]
	Learning Rate: 0.0067551
	LOSS [training: 0.7711275158302124 | validation: 1.1985322470215505]
	TIME [epoch: 8.54 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9127297079523267		[learning rate: 0.0067306]
	Learning Rate: 0.00673058
	LOSS [training: 0.9127297079523267 | validation: 0.6031596532513276]
	TIME [epoch: 8.53 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9438979553440197		[learning rate: 0.0067062]
	Learning Rate: 0.00670616
	LOSS [training: 0.9438979553440197 | validation: 0.8177778818209762]
	TIME [epoch: 8.56 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6522174132781314		[learning rate: 0.0066818]
	Learning Rate: 0.00668182
	LOSS [training: 0.6522174132781314 | validation: 0.6957916692955137]
	TIME [epoch: 8.54 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6707880978104088		[learning rate: 0.0066576]
	Learning Rate: 0.00665757
	LOSS [training: 0.6707880978104088 | validation: 0.5400923259120409]
	TIME [epoch: 8.53 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9125771493228516		[learning rate: 0.0066334]
	Learning Rate: 0.00663341
	LOSS [training: 0.9125771493228516 | validation: 0.5723712346514697]
	TIME [epoch: 8.53 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8643630881990146		[learning rate: 0.0066093]
	Learning Rate: 0.00660934
	LOSS [training: 0.8643630881990146 | validation: 1.551556541571038]
	TIME [epoch: 8.55 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8689145885989079		[learning rate: 0.0065854]
	Learning Rate: 0.00658535
	LOSS [training: 0.8689145885989079 | validation: 1.0000258389855854]
	TIME [epoch: 8.54 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.677867226495249		[learning rate: 0.0065615]
	Learning Rate: 0.00656145
	LOSS [training: 0.677867226495249 | validation: 0.6729332089867126]
	TIME [epoch: 8.54 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7203591751448156		[learning rate: 0.0065376]
	Learning Rate: 0.00653764
	LOSS [training: 0.7203591751448156 | validation: 0.6063629358353617]
	TIME [epoch: 8.53 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.680588166590684		[learning rate: 0.0065139]
	Learning Rate: 0.00651392
	LOSS [training: 0.680588166590684 | validation: 0.5364326949461402]
	TIME [epoch: 8.53 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7070834851620204		[learning rate: 0.0064903]
	Learning Rate: 0.00649028
	LOSS [training: 0.7070834851620204 | validation: 0.5685073618276273]
	TIME [epoch: 8.56 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6808067356066044		[learning rate: 0.0064667]
	Learning Rate: 0.00646672
	LOSS [training: 0.6808067356066044 | validation: 1.057397983796724]
	TIME [epoch: 8.53 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8955183681023879		[learning rate: 0.0064433]
	Learning Rate: 0.00644325
	LOSS [training: 0.8955183681023879 | validation: 1.0580978511613357]
	TIME [epoch: 8.54 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7153720745209226		[learning rate: 0.0064199]
	Learning Rate: 0.00641987
	LOSS [training: 0.7153720745209226 | validation: 0.5816128370145454]
	TIME [epoch: 8.53 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7748310413612176		[learning rate: 0.0063966]
	Learning Rate: 0.00639657
	LOSS [training: 0.7748310413612176 | validation: 0.5277715462315855]
	TIME [epoch: 8.56 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6899769260230608		[learning rate: 0.0063734]
	Learning Rate: 0.00637336
	LOSS [training: 0.6899769260230608 | validation: 0.7231992558597593]
	TIME [epoch: 8.54 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6382442472410647		[learning rate: 0.0063502]
	Learning Rate: 0.00635023
	LOSS [training: 0.6382442472410647 | validation: 1.273295589504085]
	TIME [epoch: 8.54 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7196146053682834		[learning rate: 0.0063272]
	Learning Rate: 0.00632718
	LOSS [training: 0.7196146053682834 | validation: 0.7678902416856914]
	TIME [epoch: 8.53 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6970108417019801		[learning rate: 0.0063042]
	Learning Rate: 0.00630422
	LOSS [training: 0.6970108417019801 | validation: 0.5779080219867054]
	TIME [epoch: 8.56 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.735877659687714		[learning rate: 0.0062813]
	Learning Rate: 0.00628134
	LOSS [training: 0.735877659687714 | validation: 0.5826937141078341]
	TIME [epoch: 8.54 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6821999701180264		[learning rate: 0.0062585]
	Learning Rate: 0.00625855
	LOSS [training: 0.6821999701180264 | validation: 0.41070321209135124]
	TIME [epoch: 8.54 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5880570835223022		[learning rate: 0.0062358]
	Learning Rate: 0.00623584
	LOSS [training: 0.5880570835223022 | validation: 0.6902065379263304]
	TIME [epoch: 8.53 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6462494248477115		[learning rate: 0.0062132]
	Learning Rate: 0.00621321
	LOSS [training: 0.6462494248477115 | validation: 1.0228818237008483]
	TIME [epoch: 8.55 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6942562842591062		[learning rate: 0.0061907]
	Learning Rate: 0.00619066
	LOSS [training: 0.6942562842591062 | validation: 0.8714897822058212]
	TIME [epoch: 8.55 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5602306142608827		[learning rate: 0.0061682]
	Learning Rate: 0.00616819
	LOSS [training: 0.5602306142608827 | validation: 0.6167019613649563]
	TIME [epoch: 8.54 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6551342262268275		[learning rate: 0.0061458]
	Learning Rate: 0.00614581
	LOSS [training: 0.6551342262268275 | validation: 0.39783844903656906]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5368676786843228		[learning rate: 0.0061235]
	Learning Rate: 0.0061235
	LOSS [training: 0.5368676786843228 | validation: 1.207728792001935]
	TIME [epoch: 8.54 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6469404886664509		[learning rate: 0.0061013]
	Learning Rate: 0.00610128
	LOSS [training: 0.6469404886664509 | validation: 0.48800271320744415]
	TIME [epoch: 8.56 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5870891984580537		[learning rate: 0.0060791]
	Learning Rate: 0.00607914
	LOSS [training: 0.5870891984580537 | validation: 0.4026342009950896]
	TIME [epoch: 8.53 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.652958568109794		[learning rate: 0.0060571]
	Learning Rate: 0.00605708
	LOSS [training: 0.652958568109794 | validation: 0.46223754413657514]
	TIME [epoch: 8.53 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6114397770477221		[learning rate: 0.0060351]
	Learning Rate: 0.0060351
	LOSS [training: 0.6114397770477221 | validation: 0.49007276405708344]
	TIME [epoch: 8.53 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6646764764625253		[learning rate: 0.0060132]
	Learning Rate: 0.00601319
	LOSS [training: 0.6646764764625253 | validation: 0.7368195119957728]
	TIME [epoch: 8.56 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5931717571632042		[learning rate: 0.0059914]
	Learning Rate: 0.00599137
	LOSS [training: 0.5931717571632042 | validation: 0.42273671228340304]
	TIME [epoch: 8.53 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5281995436667375		[learning rate: 0.0059696]
	Learning Rate: 0.00596963
	LOSS [training: 0.5281995436667375 | validation: 0.40987165245511636]
	TIME [epoch: 8.53 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5360394070512507		[learning rate: 0.005948]
	Learning Rate: 0.00594797
	LOSS [training: 0.5360394070512507 | validation: 0.3596935945998615]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6573511843482696		[learning rate: 0.0059264]
	Learning Rate: 0.00592638
	LOSS [training: 0.6573511843482696 | validation: 0.3260314686606264]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7154177256636625		[learning rate: 0.0059049]
	Learning Rate: 0.00590487
	LOSS [training: 0.7154177256636625 | validation: 1.3606870867952456]
	TIME [epoch: 8.53 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7486583172491625		[learning rate: 0.0058834]
	Learning Rate: 0.00588344
	LOSS [training: 0.7486583172491625 | validation: 0.5381410956786487]
	TIME [epoch: 8.53 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5615926047240323		[learning rate: 0.0058621]
	Learning Rate: 0.00586209
	LOSS [training: 0.5615926047240323 | validation: 0.48774614848991504]
	TIME [epoch: 8.52 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7147773009936149		[learning rate: 0.0058408]
	Learning Rate: 0.00584082
	LOSS [training: 0.7147773009936149 | validation: 0.5299783510171733]
	TIME [epoch: 8.55 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5529237862708432		[learning rate: 0.0058196]
	Learning Rate: 0.00581962
	LOSS [training: 0.5529237862708432 | validation: 0.5291700154812473]
	TIME [epoch: 8.53 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6258156713240953		[learning rate: 0.0057985]
	Learning Rate: 0.0057985
	LOSS [training: 0.6258156713240953 | validation: 0.4921713058459025]
	TIME [epoch: 8.53 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5582261215698088		[learning rate: 0.0057775]
	Learning Rate: 0.00577746
	LOSS [training: 0.5582261215698088 | validation: 0.5370057372819889]
	TIME [epoch: 8.53 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6689320148803203		[learning rate: 0.0057565]
	Learning Rate: 0.00575649
	LOSS [training: 0.6689320148803203 | validation: 0.37659638605919443]
	TIME [epoch: 8.54 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5035051993960304		[learning rate: 0.0057356]
	Learning Rate: 0.0057356
	LOSS [training: 0.5035051993960304 | validation: 0.35586480631513795]
	TIME [epoch: 8.53 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5645743489262871		[learning rate: 0.0057148]
	Learning Rate: 0.00571479
	LOSS [training: 0.5645743489262871 | validation: 0.42986139363279563]
	TIME [epoch: 8.53 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5483804712715352		[learning rate: 0.005694]
	Learning Rate: 0.00569405
	LOSS [training: 0.5483804712715352 | validation: 0.5306112839969155]
	TIME [epoch: 8.53 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6761714857620291		[learning rate: 0.0056734]
	Learning Rate: 0.00567338
	LOSS [training: 0.6761714857620291 | validation: 0.4118251313718977]
	TIME [epoch: 8.53 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5637673035157263		[learning rate: 0.0056528]
	Learning Rate: 0.00565279
	LOSS [training: 0.5637673035157263 | validation: 0.4703693959769807]
	TIME [epoch: 8.55 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8887005511985381		[learning rate: 0.0056323]
	Learning Rate: 0.00563228
	LOSS [training: 0.8887005511985381 | validation: 0.7442889328849278]
	TIME [epoch: 8.52 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6072004837451289		[learning rate: 0.0056118]
	Learning Rate: 0.00561184
	LOSS [training: 0.6072004837451289 | validation: 0.5848328133804299]
	TIME [epoch: 8.52 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6141767332626589		[learning rate: 0.0055915]
	Learning Rate: 0.00559147
	LOSS [training: 0.6141767332626589 | validation: 0.5165518445014633]
	TIME [epoch: 8.52 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5151908065512936		[learning rate: 0.0055712]
	Learning Rate: 0.00557118
	LOSS [training: 0.5151908065512936 | validation: 0.40805649077232814]
	TIME [epoch: 8.55 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6926888069264091		[learning rate: 0.005551]
	Learning Rate: 0.00555096
	LOSS [training: 0.6926888069264091 | validation: 0.7394374117137845]
	TIME [epoch: 8.53 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6019677362531617		[learning rate: 0.0055308]
	Learning Rate: 0.00553082
	LOSS [training: 0.6019677362531617 | validation: 2.8797700707800127]
	TIME [epoch: 8.53 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9116136008610243		[learning rate: 0.0055107]
	Learning Rate: 0.00551075
	LOSS [training: 0.9116136008610243 | validation: 0.38374683877932975]
	TIME [epoch: 8.53 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5249789061803221		[learning rate: 0.0054907]
	Learning Rate: 0.00549075
	LOSS [training: 0.5249789061803221 | validation: 0.6832657907260118]
	TIME [epoch: 8.56 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4533144261899467		[learning rate: 0.0054708]
	Learning Rate: 0.00547082
	LOSS [training: 1.4533144261899467 | validation: 0.656369408476362]
	TIME [epoch: 8.53 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5374166289578752		[learning rate: 0.005451]
	Learning Rate: 0.00545097
	LOSS [training: 0.5374166289578752 | validation: 0.45996234017120563]
	TIME [epoch: 8.52 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4779867152456392		[learning rate: 0.0054312]
	Learning Rate: 0.00543119
	LOSS [training: 0.4779867152456392 | validation: 0.38256432754194214]
	TIME [epoch: 8.52 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4220076118259436		[learning rate: 0.0054115]
	Learning Rate: 0.00541148
	LOSS [training: 0.4220076118259436 | validation: 0.5820331480635106]
	TIME [epoch: 8.54 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5849996692193956		[learning rate: 0.0053918]
	Learning Rate: 0.00539184
	LOSS [training: 0.5849996692193956 | validation: 0.39185428721515403]
	TIME [epoch: 8.54 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6477163032462113		[learning rate: 0.0053723]
	Learning Rate: 0.00537227
	LOSS [training: 0.6477163032462113 | validation: 1.4181624795301502]
	TIME [epoch: 8.53 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6611381117933666		[learning rate: 0.0053528]
	Learning Rate: 0.00535277
	LOSS [training: 0.6611381117933666 | validation: 0.3785276158247522]
	TIME [epoch: 8.53 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6408400426290184		[learning rate: 0.0053333]
	Learning Rate: 0.00533335
	LOSS [training: 0.6408400426290184 | validation: 0.3676251014959635]
	TIME [epoch: 8.53 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49480329021348035		[learning rate: 0.005314]
	Learning Rate: 0.00531399
	LOSS [training: 0.49480329021348035 | validation: 0.31718356772112266]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43801810434562316		[learning rate: 0.0052947]
	Learning Rate: 0.00529471
	LOSS [training: 0.43801810434562316 | validation: 0.500518189876126]
	TIME [epoch: 8.54 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5725939688524988		[learning rate: 0.0052755]
	Learning Rate: 0.00527549
	LOSS [training: 0.5725939688524988 | validation: 0.4748699793964904]
	TIME [epoch: 8.53 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5022726281555816		[learning rate: 0.0052563]
	Learning Rate: 0.00525635
	LOSS [training: 0.5022726281555816 | validation: 0.44581245928658775]
	TIME [epoch: 8.53 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.622148968131524		[learning rate: 0.0052373]
	Learning Rate: 0.00523727
	LOSS [training: 0.622148968131524 | validation: 1.0408868402240643]
	TIME [epoch: 8.56 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5907828735097891		[learning rate: 0.0052183]
	Learning Rate: 0.00521827
	LOSS [training: 0.5907828735097891 | validation: 0.528166556144523]
	TIME [epoch: 8.53 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2114676866967526		[learning rate: 0.0051993]
	Learning Rate: 0.00519933
	LOSS [training: 2.2114676866967526 | validation: 9.333054304344637]
	TIME [epoch: 8.53 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.95637971280905		[learning rate: 0.0051805]
	Learning Rate: 0.00518046
	LOSS [training: 8.95637971280905 | validation: 9.338976768799654]
	TIME [epoch: 8.53 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.902930667606466		[learning rate: 0.0051617]
	Learning Rate: 0.00516166
	LOSS [training: 8.902930667606466 | validation: 9.344512134177036]
	TIME [epoch: 8.56 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.953109164828168		[learning rate: 0.0051429]
	Learning Rate: 0.00514293
	LOSS [training: 8.953109164828168 | validation: 9.400387866410266]
	TIME [epoch: 8.53 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.94675358026005		[learning rate: 0.0051243]
	Learning Rate: 0.00512426
	LOSS [training: 8.94675358026005 | validation: 9.331526729020787]
	TIME [epoch: 8.53 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.926358971586705		[learning rate: 0.0051057]
	Learning Rate: 0.00510567
	LOSS [training: 8.926358971586705 | validation: 9.530771200119775]
	TIME [epoch: 8.53 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.988919204258229		[learning rate: 0.0050871]
	Learning Rate: 0.00508714
	LOSS [training: 8.988919204258229 | validation: 9.322554210462155]
	TIME [epoch: 8.55 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.19864355829267		[learning rate: 0.0050687]
	Learning Rate: 0.00506868
	LOSS [training: 9.19864355829267 | validation: 9.427872936702233]
	TIME [epoch: 8.53 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.074007076135299		[learning rate: 0.0050503]
	Learning Rate: 0.00505028
	LOSS [training: 9.074007076135299 | validation: 9.428209540895878]
	TIME [epoch: 8.53 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.06105879548485		[learning rate: 0.005032]
	Learning Rate: 0.00503196
	LOSS [training: 9.06105879548485 | validation: 9.432244055155177]
	TIME [epoch: 8.53 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.020212328672866		[learning rate: 0.0050137]
	Learning Rate: 0.00501369
	LOSS [training: 9.020212328672866 | validation: 9.385367956172]
	TIME [epoch: 8.54 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.013948879269693		[learning rate: 0.0049955]
	Learning Rate: 0.0049955
	LOSS [training: 9.013948879269693 | validation: 9.401884024689874]
	TIME [epoch: 8.54 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.148084949931654		[learning rate: 0.0049774]
	Learning Rate: 0.00497737
	LOSS [training: 9.148084949931654 | validation: 9.485935643445629]
	TIME [epoch: 8.52 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.014081010243164		[learning rate: 0.0049593]
	Learning Rate: 0.00495931
	LOSS [training: 9.014081010243164 | validation: 9.465669981026313]
	TIME [epoch: 8.53 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.04420787864154		[learning rate: 0.0049413]
	Learning Rate: 0.00494131
	LOSS [training: 9.04420787864154 | validation: 9.427694492829728]
	TIME [epoch: 8.52 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.017345618080675		[learning rate: 0.0049234]
	Learning Rate: 0.00492338
	LOSS [training: 9.017345618080675 | validation: 9.419408042509879]
	TIME [epoch: 8.55 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.978529323939512		[learning rate: 0.0049055]
	Learning Rate: 0.00490551
	LOSS [training: 8.978529323939512 | validation: 9.482374296358834]
	TIME [epoch: 8.53 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.018470328266185		[learning rate: 0.0048877]
	Learning Rate: 0.00488771
	LOSS [training: 9.018470328266185 | validation: 9.477501690213138]
	TIME [epoch: 8.53 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.062226651330231		[learning rate: 0.00487]
	Learning Rate: 0.00486997
	LOSS [training: 9.062226651330231 | validation: 9.583615263602637]
	TIME [epoch: 8.53 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.108492439281338		[learning rate: 0.0048523]
	Learning Rate: 0.0048523
	LOSS [training: 9.108492439281338 | validation: 9.403220114822988]
	TIME [epoch: 8.55 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.021936791311107		[learning rate: 0.0048347]
	Learning Rate: 0.00483469
	LOSS [training: 9.021936791311107 | validation: 9.375325512477154]
	TIME [epoch: 8.53 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.9662413300935		[learning rate: 0.0048171]
	Learning Rate: 0.00481714
	LOSS [training: 8.9662413300935 | validation: 9.374539882816592]
	TIME [epoch: 8.53 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.01169111723362		[learning rate: 0.0047997]
	Learning Rate: 0.00479966
	LOSS [training: 9.01169111723362 | validation: 9.407848243482736]
	TIME [epoch: 8.53 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.993248728981515		[learning rate: 0.0047822]
	Learning Rate: 0.00478224
	LOSS [training: 8.993248728981515 | validation: 9.375958099693992]
	TIME [epoch: 8.55 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.022897190942023		[learning rate: 0.0047649]
	Learning Rate: 0.00476489
	LOSS [training: 9.022897190942023 | validation: 9.402210043605566]
	TIME [epoch: 8.53 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.016186652490338		[learning rate: 0.0047476]
	Learning Rate: 0.00474759
	LOSS [training: 9.016186652490338 | validation: 9.375803302872214]
	TIME [epoch: 8.52 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.970858608151332		[learning rate: 0.0047304]
	Learning Rate: 0.00473037
	LOSS [training: 8.970858608151332 | validation: 9.378561559045025]
	TIME [epoch: 8.52 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.000379239715304		[learning rate: 0.0047132]
	Learning Rate: 0.0047132
	LOSS [training: 9.000379239715304 | validation: 9.466556023171382]
	TIME [epoch: 8.54 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.005971599944822		[learning rate: 0.0046961]
	Learning Rate: 0.00469609
	LOSS [training: 9.005971599944822 | validation: 9.398657595146151]
	TIME [epoch: 8.54 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.008794012233896		[learning rate: 0.0046791]
	Learning Rate: 0.00467905
	LOSS [training: 9.008794012233896 | validation: 9.378730039304287]
	TIME [epoch: 8.53 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.002559002395968		[learning rate: 0.0046621]
	Learning Rate: 0.00466207
	LOSS [training: 9.002559002395968 | validation: 9.468282510544444]
	TIME [epoch: 8.53 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.985874031545734		[learning rate: 0.0046452]
	Learning Rate: 0.00464515
	LOSS [training: 8.985874031545734 | validation: 9.453454016782274]
	TIME [epoch: 8.52 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.997197517216453		[learning rate: 0.0046283]
	Learning Rate: 0.0046283
	LOSS [training: 8.997197517216453 | validation: 9.395753003618402]
	TIME [epoch: 8.56 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.998752389418303		[learning rate: 0.0046115]
	Learning Rate: 0.0046115
	LOSS [training: 8.998752389418303 | validation: 9.387446582248824]
	TIME [epoch: 8.53 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.003655691506985		[learning rate: 0.0045948]
	Learning Rate: 0.00459476
	LOSS [training: 9.003655691506985 | validation: 9.475008497204074]
	TIME [epoch: 8.53 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.990732915002358		[learning rate: 0.0045781]
	Learning Rate: 0.00457809
	LOSS [training: 8.990732915002358 | validation: 9.417274131605026]
	TIME [epoch: 8.53 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.000613801423608		[learning rate: 0.0045615]
	Learning Rate: 0.00456148
	LOSS [training: 9.000613801423608 | validation: 9.409211092502428]
	TIME [epoch: 8.55 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.998677577180333		[learning rate: 0.0045449]
	Learning Rate: 0.00454492
	LOSS [training: 8.998677577180333 | validation: 9.366675487760727]
	TIME [epoch: 8.53 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.973766924314697		[learning rate: 0.0045284]
	Learning Rate: 0.00452843
	LOSS [training: 8.973766924314697 | validation: 9.385742215581658]
	TIME [epoch: 8.53 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.985940947141374		[learning rate: 0.004512]
	Learning Rate: 0.00451199
	LOSS [training: 8.985940947141374 | validation: 9.463596961304436]
	TIME [epoch: 8.54 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.97784336037066		[learning rate: 0.0044956]
	Learning Rate: 0.00449562
	LOSS [training: 8.97784336037066 | validation: 9.541247499365758]
	TIME [epoch: 8.56 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.999866380908722		[learning rate: 0.0044793]
	Learning Rate: 0.0044793
	LOSS [training: 8.999866380908722 | validation: 9.339766958032426]
	TIME [epoch: 8.54 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.966503913703928		[learning rate: 0.004463]
	Learning Rate: 0.00446305
	LOSS [training: 8.966503913703928 | validation: 9.384974785213364]
	TIME [epoch: 8.53 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.975697069549913		[learning rate: 0.0044469]
	Learning Rate: 0.00444685
	LOSS [training: 8.975697069549913 | validation: 9.37115993271611]
	TIME [epoch: 8.54 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.975506226281073		[learning rate: 0.0044307]
	Learning Rate: 0.00443071
	LOSS [training: 8.975506226281073 | validation: 9.382169394598787]
	TIME [epoch: 8.54 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.982583317338564		[learning rate: 0.0044146]
	Learning Rate: 0.00441463
	LOSS [training: 8.982583317338564 | validation: 9.340852714607653]
	TIME [epoch: 8.55 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.960292460307667		[learning rate: 0.0043986]
	Learning Rate: 0.00439861
	LOSS [training: 8.960292460307667 | validation: 9.41177257417621]
	TIME [epoch: 8.53 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/10] avg loss: 9.01471524780774		[learning rate: 0.0043827]
	Learning Rate: 0.00438265
	LOSS [training: 9.01471524780774 | validation: 9.374373015416726]
	TIME [epoch: 8.53 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.985599744303112		[learning rate: 0.0043667]
	Learning Rate: 0.00436675
	LOSS [training: 8.985599744303112 | validation: 9.41755991651044]
	TIME [epoch: 8.53 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.961062573995125		[learning rate: 0.0043509]
	Learning Rate: 0.0043509
	LOSS [training: 8.961062573995125 | validation: 9.381794813018772]
	TIME [epoch: 8.55 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.957492651871648		[learning rate: 0.0043351]
	Learning Rate: 0.00433511
	LOSS [training: 8.957492651871648 | validation: 9.359043855718639]
	TIME [epoch: 8.53 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.947723564693211		[learning rate: 0.0043194]
	Learning Rate: 0.00431938
	LOSS [training: 8.947723564693211 | validation: 9.345632540281073]
	TIME [epoch: 8.53 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.952022165389707		[learning rate: 0.0043037]
	Learning Rate: 0.0043037
	LOSS [training: 8.952022165389707 | validation: 9.36418739686427]
	TIME [epoch: 8.53 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.920363555423291		[learning rate: 0.0042881]
	Learning Rate: 0.00428808
	LOSS [training: 8.920363555423291 | validation: 9.256650281465465]
	TIME [epoch: 8.56 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.74154733048276		[learning rate: 0.0042725]
	Learning Rate: 0.00427252
	LOSS [training: 8.74154733048276 | validation: 8.984234786232072]
	TIME [epoch: 8.53 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.763230839967207		[learning rate: 0.004257]
	Learning Rate: 0.00425702
	LOSS [training: 7.763230839967207 | validation: 6.292623141676804]
	TIME [epoch: 8.53 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.8301358125559		[learning rate: 0.0042416]
	Learning Rate: 0.00424157
	LOSS [training: 4.8301358125559 | validation: 3.3613221609937614]
	TIME [epoch: 8.53 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.139738822030742		[learning rate: 0.0042262]
	Learning Rate: 0.00422617
	LOSS [training: 4.139738822030742 | validation: 3.1454307230254406]
	TIME [epoch: 8.55 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6503481605817285		[learning rate: 0.0042108]
	Learning Rate: 0.00421084
	LOSS [training: 3.6503481605817285 | validation: 3.5277007910856906]
	TIME [epoch: 8.54 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1666115247548716		[learning rate: 0.0041956]
	Learning Rate: 0.00419556
	LOSS [training: 3.1666115247548716 | validation: 2.785818929282724]
	TIME [epoch: 8.53 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4529939669664516		[learning rate: 0.0041803]
	Learning Rate: 0.00418033
	LOSS [training: 3.4529939669664516 | validation: 2.624430027495667]
	TIME [epoch: 8.53 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.085066486641237		[learning rate: 0.0041652]
	Learning Rate: 0.00416516
	LOSS [training: 4.085066486641237 | validation: 4.102656039710821]
	TIME [epoch: 8.54 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.449781848451986		[learning rate: 0.00415]
	Learning Rate: 0.00415004
	LOSS [training: 5.449781848451986 | validation: 4.27660637726781]
	TIME [epoch: 8.54 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.309518009172722		[learning rate: 0.004135]
	Learning Rate: 0.00413498
	LOSS [training: 4.309518009172722 | validation: 4.660865925666178]
	TIME [epoch: 8.53 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1811893095217645		[learning rate: 0.00412]
	Learning Rate: 0.00411998
	LOSS [training: 3.1811893095217645 | validation: 2.8422844132218428]
	TIME [epoch: 8.53 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.429338669112549		[learning rate: 0.004105]
	Learning Rate: 0.00410502
	LOSS [training: 2.429338669112549 | validation: 2.021935048847522]
	TIME [epoch: 8.54 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.823490783523339		[learning rate: 0.0040901]
	Learning Rate: 0.00409013
	LOSS [training: 2.823490783523339 | validation: 3.945003537473954]
	TIME [epoch: 8.54 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.0596080006693995		[learning rate: 0.0040753]
	Learning Rate: 0.00407528
	LOSS [training: 3.0596080006693995 | validation: 2.0199771868833016]
	TIME [epoch: 8.53 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.36751017623043		[learning rate: 0.0040605]
	Learning Rate: 0.00406049
	LOSS [training: 2.36751017623043 | validation: 2.1715055158578633]
	TIME [epoch: 8.53 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.5061326854107593		[learning rate: 0.0040458]
	Learning Rate: 0.00404576
	LOSS [training: 2.5061326854107593 | validation: 2.0626996392177332]
	TIME [epoch: 8.53 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.8914832177886356		[learning rate: 0.0040311]
	Learning Rate: 0.00403108
	LOSS [training: 2.8914832177886356 | validation: 2.8849645980219636]
	TIME [epoch: 8.55 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.9482970164967544		[learning rate: 0.0040164]
	Learning Rate: 0.00401645
	LOSS [training: 2.9482970164967544 | validation: 3.7284603397489344]
	TIME [epoch: 8.53 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7716315416012685		[learning rate: 0.0040019]
	Learning Rate: 0.00400187
	LOSS [training: 3.7716315416012685 | validation: 2.1165199253875544]
	TIME [epoch: 8.53 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2987846170924002		[learning rate: 0.0039873]
	Learning Rate: 0.00398735
	LOSS [training: 2.2987846170924002 | validation: 1.9299265525049782]
	TIME [epoch: 8.52 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6445215776776139		[learning rate: 0.0039729]
	Learning Rate: 0.00397288
	LOSS [training: 1.6445215776776139 | validation: 1.4257924956261938]
	TIME [epoch: 8.55 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5766900497387595		[learning rate: 0.0039585]
	Learning Rate: 0.00395846
	LOSS [training: 1.5766900497387595 | validation: 1.0990793264655279]
	TIME [epoch: 8.53 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3010187224546015		[learning rate: 0.0039441]
	Learning Rate: 0.00394409
	LOSS [training: 1.3010187224546015 | validation: 1.3758914041873647]
	TIME [epoch: 8.53 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0956746458169573		[learning rate: 0.0039298]
	Learning Rate: 0.00392978
	LOSS [training: 1.0956746458169573 | validation: 3.2113800602059763]
	TIME [epoch: 8.53 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.222633111207746		[learning rate: 0.0039155]
	Learning Rate: 0.00391552
	LOSS [training: 2.222633111207746 | validation: 1.8547263097270195]
	TIME [epoch: 8.54 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.316659819881178		[learning rate: 0.0039013]
	Learning Rate: 0.00390131
	LOSS [training: 1.316659819881178 | validation: 1.9258929835829763]
	TIME [epoch: 8.53 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7162517915090951		[learning rate: 0.0038872]
	Learning Rate: 0.00388715
	LOSS [training: 1.7162517915090951 | validation: 3.1064095831467267]
	TIME [epoch: 8.52 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8437404187948718		[learning rate: 0.003873]
	Learning Rate: 0.00387305
	LOSS [training: 1.8437404187948718 | validation: 1.2313558469979706]
	TIME [epoch: 8.53 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4248052418640884		[learning rate: 0.003859]
	Learning Rate: 0.00385899
	LOSS [training: 1.4248052418640884 | validation: 1.7311059229060564]
	TIME [epoch: 8.52 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5152579927535832		[learning rate: 0.003845]
	Learning Rate: 0.00384499
	LOSS [training: 1.5152579927535832 | validation: 1.4213908671677844]
	TIME [epoch: 8.55 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.430742246083685		[learning rate: 0.003831]
	Learning Rate: 0.00383103
	LOSS [training: 1.430742246083685 | validation: 2.2683479923267886]
	TIME [epoch: 8.53 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.150699528384306		[learning rate: 0.0038171]
	Learning Rate: 0.00381713
	LOSS [training: 2.150699528384306 | validation: 1.2174699289925397]
	TIME [epoch: 8.52 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7617041782538163		[learning rate: 0.0038033]
	Learning Rate: 0.00380328
	LOSS [training: 1.7617041782538163 | validation: 1.2117040348189139]
	TIME [epoch: 8.52 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1233715428564217		[learning rate: 0.0037895]
	Learning Rate: 0.00378947
	LOSS [training: 2.1233715428564217 | validation: 1.2907875381030218]
	TIME [epoch: 8.55 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.84723499755955		[learning rate: 0.0037757]
	Learning Rate: 0.00377572
	LOSS [training: 1.84723499755955 | validation: 1.2188060078054839]
	TIME [epoch: 8.52 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7392867279231197		[learning rate: 0.003762]
	Learning Rate: 0.00376202
	LOSS [training: 1.7392867279231197 | validation: 0.8924038686840368]
	TIME [epoch: 8.52 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2461776916841247		[learning rate: 0.0037484]
	Learning Rate: 0.00374837
	LOSS [training: 1.2461776916841247 | validation: 4.746695236741653]
	TIME [epoch: 8.52 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9216942983881826		[learning rate: 0.0037348]
	Learning Rate: 0.00373476
	LOSS [training: 1.9216942983881826 | validation: 2.6792905533470543]
	TIME [epoch: 8.55 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3115113780590046		[learning rate: 0.0037212]
	Learning Rate: 0.00372121
	LOSS [training: 1.3115113780590046 | validation: 1.7386431602826717]
	TIME [epoch: 8.53 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3248470271524604		[learning rate: 0.0037077]
	Learning Rate: 0.00370771
	LOSS [training: 1.3248470271524604 | validation: 2.001896482485612]
	TIME [epoch: 8.53 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.324128719520353		[learning rate: 0.0036943]
	Learning Rate: 0.00369425
	LOSS [training: 1.324128719520353 | validation: 0.9068411009604327]
	TIME [epoch: 8.52 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6147144048642736		[learning rate: 0.0036808]
	Learning Rate: 0.00368084
	LOSS [training: 1.6147144048642736 | validation: 2.623140173940402]
	TIME [epoch: 8.55 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1277562600212807		[learning rate: 0.0036675]
	Learning Rate: 0.00366749
	LOSS [training: 2.1277562600212807 | validation: 1.3588485765633913]
	TIME [epoch: 8.53 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1200887293270494		[learning rate: 0.0036542]
	Learning Rate: 0.00365418
	LOSS [training: 2.1200887293270494 | validation: 1.772331962442522]
	TIME [epoch: 8.52 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.608510894151428		[learning rate: 0.0036409]
	Learning Rate: 0.00364092
	LOSS [training: 1.608510894151428 | validation: 1.5092582553789704]
	TIME [epoch: 8.53 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.712672268737574		[learning rate: 0.0036277]
	Learning Rate: 0.0036277
	LOSS [training: 1.712672268737574 | validation: 1.1851745574513246]
	TIME [epoch: 8.54 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9113254310370198		[learning rate: 0.0036145]
	Learning Rate: 0.00361454
	LOSS [training: 1.9113254310370198 | validation: 4.423781053456567]
	TIME [epoch: 8.53 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9398057048951436		[learning rate: 0.0036014]
	Learning Rate: 0.00360142
	LOSS [training: 1.9398057048951436 | validation: 2.042301832909609]
	TIME [epoch: 8.53 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2649626323389542		[learning rate: 0.0035883]
	Learning Rate: 0.00358835
	LOSS [training: 1.2649626323389542 | validation: 1.6072478429569754]
	TIME [epoch: 8.53 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0941809468115191		[learning rate: 0.0035753]
	Learning Rate: 0.00357533
	LOSS [training: 1.0941809468115191 | validation: 1.2283105921039355]
	TIME [epoch: 8.52 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.655171792491951		[learning rate: 0.0035624]
	Learning Rate: 0.00356235
	LOSS [training: 1.655171792491951 | validation: 2.096004171016723]
	TIME [epoch: 8.55 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2650652918094112		[learning rate: 0.0035494]
	Learning Rate: 0.00354942
	LOSS [training: 1.2650652918094112 | validation: 1.5943596764190215]
	TIME [epoch: 8.52 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0851717398272247		[learning rate: 0.0035365]
	Learning Rate: 0.00353654
	LOSS [training: 1.0851717398272247 | validation: 2.1484422180923683]
	TIME [epoch: 8.53 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7136055529303635		[learning rate: 0.0035237]
	Learning Rate: 0.00352371
	LOSS [training: 1.7136055529303635 | validation: 4.587063848375841]
	TIME [epoch: 8.52 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9703034229139726		[learning rate: 0.0035109]
	Learning Rate: 0.00351092
	LOSS [training: 3.9703034229139726 | validation: 4.521380437430108]
	TIME [epoch: 8.55 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.583390733595819		[learning rate: 0.0034982]
	Learning Rate: 0.00349818
	LOSS [training: 4.583390733595819 | validation: 5.644212869656736]
	TIME [epoch: 8.52 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.654466294957459		[learning rate: 0.0034855]
	Learning Rate: 0.00348548
	LOSS [training: 5.654466294957459 | validation: 3.9691880181727073]
	TIME [epoch: 8.52 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.7996265530445337		[learning rate: 0.0034728]
	Learning Rate: 0.00347284
	LOSS [training: 2.7996265530445337 | validation: 1.3637231927487719]
	TIME [epoch: 8.52 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0212075008370163		[learning rate: 0.0034602]
	Learning Rate: 0.00346023
	LOSS [training: 1.0212075008370163 | validation: 1.2815952092552814]
	TIME [epoch: 8.54 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1437047399362439		[learning rate: 0.0034477]
	Learning Rate: 0.00344767
	LOSS [training: 1.1437047399362439 | validation: 1.9438167599063374]
	TIME [epoch: 8.53 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5060746313317979		[learning rate: 0.0034352]
	Learning Rate: 0.00343516
	LOSS [training: 1.5060746313317979 | validation: 1.5517223539345086]
	TIME [epoch: 8.52 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7966481605336526		[learning rate: 0.0034227]
	Learning Rate: 0.0034227
	LOSS [training: 1.7966481605336526 | validation: 2.719235840643918]
	TIME [epoch: 8.52 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3467031188540701		[learning rate: 0.0034103]
	Learning Rate: 0.00341028
	LOSS [training: 1.3467031188540701 | validation: 2.414877700777441]
	TIME [epoch: 8.54 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.175929252912701		[learning rate: 0.0033979]
	Learning Rate: 0.0033979
	LOSS [training: 1.175929252912701 | validation: 1.3134134045524108]
	TIME [epoch: 8.53 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9546647550035935		[learning rate: 0.0033856]
	Learning Rate: 0.00338557
	LOSS [training: 0.9546647550035935 | validation: 0.8265122533855361]
	TIME [epoch: 8.52 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8646770913941515		[learning rate: 0.0033733]
	Learning Rate: 0.00337328
	LOSS [training: 0.8646770913941515 | validation: 1.3358953826749635]
	TIME [epoch: 8.52 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9017195265232415		[learning rate: 0.003361]
	Learning Rate: 0.00336104
	LOSS [training: 0.9017195265232415 | validation: 0.6056338199568722]
	TIME [epoch: 8.52 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9166355781840082		[learning rate: 0.0033488]
	Learning Rate: 0.00334884
	LOSS [training: 0.9166355781840082 | validation: 0.9436391819135108]
	TIME [epoch: 8.55 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8612417250232702		[learning rate: 0.0033367]
	Learning Rate: 0.00333669
	LOSS [training: 0.8612417250232702 | validation: 0.9883624609580346]
	TIME [epoch: 8.52 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0127946933889402		[learning rate: 0.0033246]
	Learning Rate: 0.00332458
	LOSS [training: 1.0127946933889402 | validation: 0.9948167497510831]
	TIME [epoch: 8.52 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0343077988799163		[learning rate: 0.0033125]
	Learning Rate: 0.00331252
	LOSS [training: 1.0343077988799163 | validation: 0.7117444897931968]
	TIME [epoch: 8.53 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0353626120367183		[learning rate: 0.0033005]
	Learning Rate: 0.00330049
	LOSS [training: 1.0353626120367183 | validation: 1.523590787800123]
	TIME [epoch: 8.55 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8648549684854892		[learning rate: 0.0032885]
	Learning Rate: 0.00328852
	LOSS [training: 0.8648549684854892 | validation: 0.9874786932186297]
	TIME [epoch: 8.53 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7343900625187396		[learning rate: 0.0032766]
	Learning Rate: 0.00327658
	LOSS [training: 0.7343900625187396 | validation: 0.9495220212740227]
	TIME [epoch: 8.53 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7207217339577567		[learning rate: 0.0032647]
	Learning Rate: 0.00326469
	LOSS [training: 0.7207217339577567 | validation: 0.5960078979531224]
	TIME [epoch: 8.53 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7797283079153896		[learning rate: 0.0032528]
	Learning Rate: 0.00325284
	LOSS [training: 0.7797283079153896 | validation: 0.6957236672789574]
	TIME [epoch: 8.55 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8139066960743214		[learning rate: 0.003241]
	Learning Rate: 0.00324104
	LOSS [training: 0.8139066960743214 | validation: 0.5066312795413132]
	TIME [epoch: 8.53 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8281265623385711		[learning rate: 0.0032293]
	Learning Rate: 0.00322928
	LOSS [training: 0.8281265623385711 | validation: 1.288158421236862]
	TIME [epoch: 8.53 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2408973736723143		[learning rate: 0.0032176]
	Learning Rate: 0.00321756
	LOSS [training: 1.2408973736723143 | validation: 0.930152684546508]
	TIME [epoch: 8.52 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6300800824181707		[learning rate: 0.0032059]
	Learning Rate: 0.00320588
	LOSS [training: 0.6300800824181707 | validation: 1.3328611281342575]
	TIME [epoch: 8.54 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.734046211220151		[learning rate: 0.0031942]
	Learning Rate: 0.00319425
	LOSS [training: 0.734046211220151 | validation: 0.9772086337495443]
	TIME [epoch: 8.53 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6772947562644293		[learning rate: 0.0031827]
	Learning Rate: 0.00318265
	LOSS [training: 0.6772947562644293 | validation: 0.6235146451227165]
	TIME [epoch: 8.53 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7793370034750573		[learning rate: 0.0031711]
	Learning Rate: 0.0031711
	LOSS [training: 0.7793370034750573 | validation: 0.5693534588194668]
	TIME [epoch: 8.53 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7296000687004144		[learning rate: 0.0031596]
	Learning Rate: 0.0031596
	LOSS [training: 0.7296000687004144 | validation: 0.7505835871871352]
	TIME [epoch: 8.53 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7241573041306979		[learning rate: 0.0031481]
	Learning Rate: 0.00314813
	LOSS [training: 0.7241573041306979 | validation: 0.5627689018059745]
	TIME [epoch: 8.55 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6739158432868743		[learning rate: 0.0031367]
	Learning Rate: 0.00313671
	LOSS [training: 0.6739158432868743 | validation: 0.7601161222307669]
	TIME [epoch: 8.53 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7274872386265102		[learning rate: 0.0031253]
	Learning Rate: 0.00312532
	LOSS [training: 0.7274872386265102 | validation: 0.8530934044626466]
	TIME [epoch: 8.52 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6904057106033659		[learning rate: 0.003114]
	Learning Rate: 0.00311398
	LOSS [training: 0.6904057106033659 | validation: 0.5132683658904088]
	TIME [epoch: 8.52 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6314127851785785		[learning rate: 0.0031027]
	Learning Rate: 0.00310268
	LOSS [training: 0.6314127851785785 | validation: 0.537197120693466]
	TIME [epoch: 8.55 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7116531299366857		[learning rate: 0.0030914]
	Learning Rate: 0.00309142
	LOSS [training: 0.7116531299366857 | validation: 0.5011717779570308]
	TIME [epoch: 8.53 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8082011033197917		[learning rate: 0.0030802]
	Learning Rate: 0.0030802
	LOSS [training: 0.8082011033197917 | validation: 0.4690789152334627]
	TIME [epoch: 8.53 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5870473863015467		[learning rate: 0.003069]
	Learning Rate: 0.00306902
	LOSS [training: 0.5870473863015467 | validation: 0.5612070772900719]
	TIME [epoch: 8.53 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.611674018587833		[learning rate: 0.0030579]
	Learning Rate: 0.00305788
	LOSS [training: 0.611674018587833 | validation: 0.5259012954690783]
	TIME [epoch: 8.55 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.710133258063613		[learning rate: 0.0030468]
	Learning Rate: 0.00304679
	LOSS [training: 0.710133258063613 | validation: 0.49147918512039684]
	TIME [epoch: 8.52 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7380504167271849		[learning rate: 0.0030357]
	Learning Rate: 0.00303573
	LOSS [training: 0.7380504167271849 | validation: 0.6784891002982028]
	TIME [epoch: 8.52 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6778679620324889		[learning rate: 0.0030247]
	Learning Rate: 0.00302471
	LOSS [training: 0.6778679620324889 | validation: 0.5265792525465784]
	TIME [epoch: 8.52 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6297855107547468		[learning rate: 0.0030137]
	Learning Rate: 0.00301374
	LOSS [training: 0.6297855107547468 | validation: 0.7077630102545591]
	TIME [epoch: 8.54 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7747313382878773		[learning rate: 0.0030028]
	Learning Rate: 0.0030028
	LOSS [training: 0.7747313382878773 | validation: 0.4691951855764004]
	TIME [epoch: 8.53 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5482980907873415		[learning rate: 0.0029919]
	Learning Rate: 0.0029919
	LOSS [training: 0.5482980907873415 | validation: 0.42470474253070256]
	TIME [epoch: 8.53 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5534418438390033		[learning rate: 0.002981]
	Learning Rate: 0.00298104
	LOSS [training: 0.5534418438390033 | validation: 1.0407747164254468]
	TIME [epoch: 8.53 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6444247304228796		[learning rate: 0.0029702]
	Learning Rate: 0.00297023
	LOSS [training: 0.6444247304228796 | validation: 0.5784824361348699]
	TIME [epoch: 8.53 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5642841282193128		[learning rate: 0.0029594]
	Learning Rate: 0.00295945
	LOSS [training: 0.5642841282193128 | validation: 0.6515099301048832]
	TIME [epoch: 8.54 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5712351449308014		[learning rate: 0.0029487]
	Learning Rate: 0.00294871
	LOSS [training: 0.5712351449308014 | validation: 0.7060841570816453]
	TIME [epoch: 8.52 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6531780084046234		[learning rate: 0.002938]
	Learning Rate: 0.00293801
	LOSS [training: 0.6531780084046234 | validation: 0.4552915416936425]
	TIME [epoch: 8.52 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5727208925445666		[learning rate: 0.0029273]
	Learning Rate: 0.00292734
	LOSS [training: 0.5727208925445666 | validation: 0.43301431821305614]
	TIME [epoch: 8.52 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5227399158225021		[learning rate: 0.0029167]
	Learning Rate: 0.00291672
	LOSS [training: 0.5227399158225021 | validation: 0.6636845807007877]
	TIME [epoch: 8.55 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5476029511635048		[learning rate: 0.0029061]
	Learning Rate: 0.00290614
	LOSS [training: 0.5476029511635048 | validation: 0.9102597444283586]
	TIME [epoch: 8.53 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8241714531820605		[learning rate: 0.0028956]
	Learning Rate: 0.00289559
	LOSS [training: 0.8241714531820605 | validation: 0.8628248865733446]
	TIME [epoch: 8.53 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5536370015645288		[learning rate: 0.0028851]
	Learning Rate: 0.00288508
	LOSS [training: 0.5536370015645288 | validation: 0.5627095490028564]
	TIME [epoch: 8.52 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6055459557316893		[learning rate: 0.0028746]
	Learning Rate: 0.00287461
	LOSS [training: 0.6055459557316893 | validation: 0.6551934518569629]
	TIME [epoch: 8.55 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7578687506824696		[learning rate: 0.0028642]
	Learning Rate: 0.00286418
	LOSS [training: 0.7578687506824696 | validation: 0.480484159622943]
	TIME [epoch: 8.53 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6212919735308489		[learning rate: 0.0028538]
	Learning Rate: 0.00285378
	LOSS [training: 0.6212919735308489 | validation: 0.4616010894296273]
	TIME [epoch: 8.53 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7290060369499913		[learning rate: 0.0028434]
	Learning Rate: 0.00284343
	LOSS [training: 0.7290060369499913 | validation: 0.43416831067660355]
	TIME [epoch: 8.52 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5764601547539935		[learning rate: 0.0028331]
	Learning Rate: 0.00283311
	LOSS [training: 0.5764601547539935 | validation: 0.40089438345306533]
	TIME [epoch: 8.55 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7668555954176639		[learning rate: 0.0028228]
	Learning Rate: 0.00282283
	LOSS [training: 0.7668555954176639 | validation: 1.300416524926216]
	TIME [epoch: 8.53 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8471756164450254		[learning rate: 0.0028126]
	Learning Rate: 0.00281258
	LOSS [training: 0.8471756164450254 | validation: 0.7740473058458394]
	TIME [epoch: 8.53 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5098727920146516		[learning rate: 0.0028024]
	Learning Rate: 0.00280238
	LOSS [training: 0.5098727920146516 | validation: 0.7510359103580768]
	TIME [epoch: 8.53 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5819229169809115		[learning rate: 0.0027922]
	Learning Rate: 0.00279221
	LOSS [training: 0.5819229169809115 | validation: 0.5130539549515017]
	TIME [epoch: 8.54 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5810695960458077		[learning rate: 0.0027821]
	Learning Rate: 0.00278207
	LOSS [training: 0.5810695960458077 | validation: 0.519248150186341]
	TIME [epoch: 8.54 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.529870845551952		[learning rate: 0.002772]
	Learning Rate: 0.00277198
	LOSS [training: 0.529870845551952 | validation: 0.42478455076983695]
	TIME [epoch: 8.53 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5965712661277391		[learning rate: 0.0027619]
	Learning Rate: 0.00276192
	LOSS [training: 0.5965712661277391 | validation: 0.511872594969593]
	TIME [epoch: 8.53 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8050883945733288		[learning rate: 0.0027519]
	Learning Rate: 0.00275189
	LOSS [training: 0.8050883945733288 | validation: 0.49957636404399663]
	TIME [epoch: 8.54 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6107220874798014		[learning rate: 0.0027419]
	Learning Rate: 0.00274191
	LOSS [training: 0.6107220874798014 | validation: 0.620560902186556]
	TIME [epoch: 8.55 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4859630822276846		[learning rate: 0.002732]
	Learning Rate: 0.00273196
	LOSS [training: 0.4859630822276846 | validation: 0.4438635274027737]
	TIME [epoch: 8.52 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5632060219402066		[learning rate: 0.002722]
	Learning Rate: 0.00272204
	LOSS [training: 0.5632060219402066 | validation: 0.5843832040062662]
	TIME [epoch: 8.53 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.541770023690434		[learning rate: 0.0027122]
	Learning Rate: 0.00271216
	LOSS [training: 0.541770023690434 | validation: 0.461765469900561]
	TIME [epoch: 8.53 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.624590174893097		[learning rate: 0.0027023]
	Learning Rate: 0.00270232
	LOSS [training: 0.624590174893097 | validation: 0.5177732167304329]
	TIME [epoch: 8.56 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5487772536660371		[learning rate: 0.0026925]
	Learning Rate: 0.00269251
	LOSS [training: 0.5487772536660371 | validation: 0.7274775441183621]
	TIME [epoch: 8.53 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5272521394043463		[learning rate: 0.0026827]
	Learning Rate: 0.00268274
	LOSS [training: 0.5272521394043463 | validation: 0.6215454342343312]
	TIME [epoch: 8.53 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6260823518618579		[learning rate: 0.002673]
	Learning Rate: 0.00267301
	LOSS [training: 0.6260823518618579 | validation: 0.41695522732575097]
	TIME [epoch: 8.52 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4350664543545368		[learning rate: 0.0026633]
	Learning Rate: 0.00266331
	LOSS [training: 0.4350664543545368 | validation: 0.36324474420367203]
	TIME [epoch: 8.54 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5082629917869034		[learning rate: 0.0026536]
	Learning Rate: 0.00265364
	LOSS [training: 0.5082629917869034 | validation: 0.4117632006441682]
	TIME [epoch: 8.53 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5837029673063172		[learning rate: 0.002644]
	Learning Rate: 0.00264401
	LOSS [training: 0.5837029673063172 | validation: 0.6116467268432075]
	TIME [epoch: 8.53 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5552471548673081		[learning rate: 0.0026344]
	Learning Rate: 0.00263441
	LOSS [training: 0.5552471548673081 | validation: 0.3924817872607427]
	TIME [epoch: 8.52 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7739421692795821		[learning rate: 0.0026249]
	Learning Rate: 0.00262485
	LOSS [training: 0.7739421692795821 | validation: 0.42287980754080967]
	TIME [epoch: 8.54 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5088314283235549		[learning rate: 0.0026153]
	Learning Rate: 0.00261533
	LOSS [training: 0.5088314283235549 | validation: 0.5540505219940752]
	TIME [epoch: 8.54 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5498506900337455		[learning rate: 0.0026058]
	Learning Rate: 0.00260584
	LOSS [training: 0.5498506900337455 | validation: 0.43484421018993147]
	TIME [epoch: 8.53 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4793071842971595		[learning rate: 0.0025964]
	Learning Rate: 0.00259638
	LOSS [training: 0.4793071842971595 | validation: 0.55069510396957]
	TIME [epoch: 8.52 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4836723466793324		[learning rate: 0.002587]
	Learning Rate: 0.00258696
	LOSS [training: 0.4836723466793324 | validation: 0.48043887763499427]
	TIME [epoch: 8.53 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5236807877331591		[learning rate: 0.0025776]
	Learning Rate: 0.00257757
	LOSS [training: 0.5236807877331591 | validation: 0.4511486963789035]
	TIME [epoch: 8.56 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.684610989906278		[learning rate: 0.0025682]
	Learning Rate: 0.00256822
	LOSS [training: 0.684610989906278 | validation: 0.7686893949977576]
	TIME [epoch: 8.53 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4939494957513907		[learning rate: 0.0025589]
	Learning Rate: 0.0025589
	LOSS [training: 0.4939494957513907 | validation: 0.5069906911223878]
	TIME [epoch: 8.53 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4637853853255066		[learning rate: 0.0025496]
	Learning Rate: 0.00254961
	LOSS [training: 0.4637853853255066 | validation: 0.7286939980545872]
	TIME [epoch: 8.53 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4899385565847839		[learning rate: 0.0025404]
	Learning Rate: 0.00254036
	LOSS [training: 0.4899385565847839 | validation: 0.7680477503669275]
	TIME [epoch: 8.65 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49317945976659966		[learning rate: 0.0025311]
	Learning Rate: 0.00253114
	LOSS [training: 0.49317945976659966 | validation: 0.8572355986502529]
	TIME [epoch: 8.53 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5324136609329049		[learning rate: 0.002522]
	Learning Rate: 0.00252195
	LOSS [training: 0.5324136609329049 | validation: 0.36356586672185814]
	TIME [epoch: 8.53 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41922358424709144		[learning rate: 0.0025128]
	Learning Rate: 0.0025128
	LOSS [training: 0.41922358424709144 | validation: 0.4474103330750055]
	TIME [epoch: 8.53 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41355919362199084		[learning rate: 0.0025037]
	Learning Rate: 0.00250368
	LOSS [training: 0.41355919362199084 | validation: 0.6582358018914962]
	TIME [epoch: 8.56 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5761885033151624		[learning rate: 0.0024946]
	Learning Rate: 0.00249459
	LOSS [training: 0.5761885033151624 | validation: 0.7153767032822445]
	TIME [epoch: 8.53 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5969806885502866		[learning rate: 0.0024855]
	Learning Rate: 0.00248554
	LOSS [training: 0.5969806885502866 | validation: 0.6362165256340564]
	TIME [epoch: 8.53 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6405604328608896		[learning rate: 0.0024765]
	Learning Rate: 0.00247652
	LOSS [training: 0.6405604328608896 | validation: 0.5106084190069363]
	TIME [epoch: 8.53 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5960505294689358		[learning rate: 0.0024675]
	Learning Rate: 0.00246753
	LOSS [training: 0.5960505294689358 | validation: 0.5023471305440111]
	TIME [epoch: 8.54 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46906048811540957		[learning rate: 0.0024586]
	Learning Rate: 0.00245858
	LOSS [training: 0.46906048811540957 | validation: 0.5299280523343953]
	TIME [epoch: 8.54 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47138916716120133		[learning rate: 0.0024497]
	Learning Rate: 0.00244966
	LOSS [training: 0.47138916716120133 | validation: 0.3614616321087941]
	TIME [epoch: 8.53 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5095335320283847		[learning rate: 0.0024408]
	Learning Rate: 0.00244077
	LOSS [training: 0.5095335320283847 | validation: 0.42926971666190394]
	TIME [epoch: 8.53 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44689870047164026		[learning rate: 0.0024319]
	Learning Rate: 0.00243191
	LOSS [training: 0.44689870047164026 | validation: 0.6581849229644039]
	TIME [epoch: 8.54 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44183978806796337		[learning rate: 0.0024231]
	Learning Rate: 0.00242308
	LOSS [training: 0.44183978806796337 | validation: 0.6786999133680887]
	TIME [epoch: 8.55 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5133423292463493		[learning rate: 0.0024143]
	Learning Rate: 0.00241429
	LOSS [training: 0.5133423292463493 | validation: 0.38818396708803843]
	TIME [epoch: 8.53 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3911032870810377		[learning rate: 0.0024055]
	Learning Rate: 0.00240553
	LOSS [training: 0.3911032870810377 | validation: 0.541367937231799]
	TIME [epoch: 8.53 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5130601018560298		[learning rate: 0.0023968]
	Learning Rate: 0.0023968
	LOSS [training: 0.5130601018560298 | validation: 0.490849908125968]
	TIME [epoch: 8.53 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.639882419185369		[learning rate: 0.0023881]
	Learning Rate: 0.0023881
	LOSS [training: 0.639882419185369 | validation: 0.40319590844224973]
	TIME [epoch: 8.55 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4416038625902434		[learning rate: 0.0023794]
	Learning Rate: 0.00237943
	LOSS [training: 0.4416038625902434 | validation: 0.558388836087766]
	TIME [epoch: 8.53 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.504464743067928		[learning rate: 0.0023708]
	Learning Rate: 0.0023708
	LOSS [training: 0.504464743067928 | validation: 0.6369475368629179]
	TIME [epoch: 8.53 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.484968582870006		[learning rate: 0.0023622]
	Learning Rate: 0.0023622
	LOSS [training: 0.484968582870006 | validation: 0.44651626746207995]
	TIME [epoch: 8.53 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38293020783263254		[learning rate: 0.0023536]
	Learning Rate: 0.00235362
	LOSS [training: 0.38293020783263254 | validation: 1.1043390658788512]
	TIME [epoch: 8.55 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5082691108817543		[learning rate: 0.0023451]
	Learning Rate: 0.00234508
	LOSS [training: 0.5082691108817543 | validation: 0.3948055484922658]
	TIME [epoch: 8.54 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42570774504010894		[learning rate: 0.0023366]
	Learning Rate: 0.00233657
	LOSS [training: 0.42570774504010894 | validation: 0.5275976082484392]
	TIME [epoch: 8.53 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4544797802346691		[learning rate: 0.0023281]
	Learning Rate: 0.00232809
	LOSS [training: 0.4544797802346691 | validation: 0.9362691187852642]
	TIME [epoch: 8.53 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5495292121164914		[learning rate: 0.0023196]
	Learning Rate: 0.00231964
	LOSS [training: 0.5495292121164914 | validation: 0.30760751913622]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_502.pth
	Model improved!!!
EPOCH 503/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48595994739670967		[learning rate: 0.0023112]
	Learning Rate: 0.00231122
	LOSS [training: 0.48595994739670967 | validation: 0.3438832100527351]
	TIME [epoch: 8.54 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49038853148109157		[learning rate: 0.0023028]
	Learning Rate: 0.00230284
	LOSS [training: 0.49038853148109157 | validation: 0.35566864961787487]
	TIME [epoch: 8.53 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4888677909533257		[learning rate: 0.0022945]
	Learning Rate: 0.00229448
	LOSS [training: 0.4888677909533257 | validation: 0.6203342457357555]
	TIME [epoch: 8.53 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44878390194926565		[learning rate: 0.0022862]
	Learning Rate: 0.00228615
	LOSS [training: 0.44878390194926565 | validation: 0.5457198982555215]
	TIME [epoch: 8.54 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5122425755191264		[learning rate: 0.0022779]
	Learning Rate: 0.00227786
	LOSS [training: 0.5122425755191264 | validation: 0.4718704742611661]
	TIME [epoch: 8.54 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5462761211621752		[learning rate: 0.0022696]
	Learning Rate: 0.00226959
	LOSS [training: 0.5462761211621752 | validation: 0.4065840007129935]
	TIME [epoch: 8.53 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41125699522697373		[learning rate: 0.0022614]
	Learning Rate: 0.00226135
	LOSS [training: 0.41125699522697373 | validation: 0.4624539947819143]
	TIME [epoch: 8.53 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4730628362171635		[learning rate: 0.0022531]
	Learning Rate: 0.00225315
	LOSS [training: 0.4730628362171635 | validation: 0.5058033590827753]
	TIME [epoch: 8.53 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5096994199509443		[learning rate: 0.002245]
	Learning Rate: 0.00224497
	LOSS [training: 0.5096994199509443 | validation: 0.36401427753283155]
	TIME [epoch: 8.55 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5060859994855369		[learning rate: 0.0022368]
	Learning Rate: 0.00223682
	LOSS [training: 0.5060859994855369 | validation: 0.37312123233296596]
	TIME [epoch: 8.53 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41732674268791053		[learning rate: 0.0022287]
	Learning Rate: 0.00222871
	LOSS [training: 0.41732674268791053 | validation: 0.2834745975569197]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_513.pth
	Model improved!!!
EPOCH 514/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42930496784189937		[learning rate: 0.0022206]
	Learning Rate: 0.00222062
	LOSS [training: 0.42930496784189937 | validation: 0.33272507530455453]
	TIME [epoch: 8.53 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4863029085210532		[learning rate: 0.0022126]
	Learning Rate: 0.00221256
	LOSS [training: 0.4863029085210532 | validation: 0.2902082464819817]
	TIME [epoch: 8.55 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3941566601945507		[learning rate: 0.0022045]
	Learning Rate: 0.00220453
	LOSS [training: 0.3941566601945507 | validation: 0.3692124673628766]
	TIME [epoch: 8.54 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5096226086056386		[learning rate: 0.0021965]
	Learning Rate: 0.00219653
	LOSS [training: 0.5096226086056386 | validation: 0.3022745158135868]
	TIME [epoch: 8.52 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4107194086327213		[learning rate: 0.0021886]
	Learning Rate: 0.00218856
	LOSS [training: 0.4107194086327213 | validation: 0.6128248643849431]
	TIME [epoch: 8.53 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4206373545236989		[learning rate: 0.0021806]
	Learning Rate: 0.00218061
	LOSS [training: 0.4206373545236989 | validation: 0.361313471787978]
	TIME [epoch: 8.56 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5834879861372185		[learning rate: 0.0021727]
	Learning Rate: 0.0021727
	LOSS [training: 0.5834879861372185 | validation: 0.5797384305936042]
	TIME [epoch: 8.53 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41867459610633856		[learning rate: 0.0021648]
	Learning Rate: 0.00216482
	LOSS [training: 0.41867459610633856 | validation: 0.6313081155505333]
	TIME [epoch: 8.53 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.417836697191198		[learning rate: 0.002157]
	Learning Rate: 0.00215696
	LOSS [training: 0.417836697191198 | validation: 0.31385281444954316]
	TIME [epoch: 8.52 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5847833663073414		[learning rate: 0.0021491]
	Learning Rate: 0.00214913
	LOSS [training: 0.5847833663073414 | validation: 0.4175840452336168]
	TIME [epoch: 8.54 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4084524806470481		[learning rate: 0.0021413]
	Learning Rate: 0.00214133
	LOSS [training: 0.4084524806470481 | validation: 0.40017451660079084]
	TIME [epoch: 8.53 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42859403962983694		[learning rate: 0.0021336]
	Learning Rate: 0.00213356
	LOSS [training: 0.42859403962983694 | validation: 1.6979410639748622]
	TIME [epoch: 8.53 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6434065450487488		[learning rate: 0.0021258]
	Learning Rate: 0.00212582
	LOSS [training: 0.6434065450487488 | validation: 0.3013163582695494]
	TIME [epoch: 8.53 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4901815485850977		[learning rate: 0.0021181]
	Learning Rate: 0.0021181
	LOSS [training: 0.4901815485850977 | validation: 0.4228794704351126]
	TIME [epoch: 8.52 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.408555610143556		[learning rate: 0.0021104]
	Learning Rate: 0.00211042
	LOSS [training: 0.408555610143556 | validation: 0.31361814460466647]
	TIME [epoch: 8.55 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42678223285455485		[learning rate: 0.0021028]
	Learning Rate: 0.00210276
	LOSS [training: 0.42678223285455485 | validation: 0.3826045449938579]
	TIME [epoch: 8.53 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4109464351597255		[learning rate: 0.0020951]
	Learning Rate: 0.00209513
	LOSS [training: 0.4109464351597255 | validation: 0.321193309787943]
	TIME [epoch: 8.53 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37890161958627194		[learning rate: 0.0020875]
	Learning Rate: 0.00208752
	LOSS [training: 0.37890161958627194 | validation: 0.285614356515292]
	TIME [epoch: 8.52 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5220139107622535		[learning rate: 0.0020799]
	Learning Rate: 0.00207995
	LOSS [training: 0.5220139107622535 | validation: 0.3101062391136362]
	TIME [epoch: 8.55 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4215635941998933		[learning rate: 0.0020724]
	Learning Rate: 0.0020724
	LOSS [training: 0.4215635941998933 | validation: 0.38617759599141277]
	TIME [epoch: 8.53 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5613262527971074		[learning rate: 0.0020649]
	Learning Rate: 0.00206488
	LOSS [training: 0.5613262527971074 | validation: 0.4652592319806683]
	TIME [epoch: 8.52 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4205663844734412		[learning rate: 0.0020574]
	Learning Rate: 0.00205739
	LOSS [training: 0.4205663844734412 | validation: 0.3058427659918357]
	TIME [epoch: 8.53 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4127747795185668		[learning rate: 0.0020499]
	Learning Rate: 0.00204992
	LOSS [training: 0.4127747795185668 | validation: 0.583173571867448]
	TIME [epoch: 8.54 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48892435528753797		[learning rate: 0.0020425]
	Learning Rate: 0.00204248
	LOSS [training: 0.48892435528753797 | validation: 0.3704708028750819]
	TIME [epoch: 8.53 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4369527095610727		[learning rate: 0.0020351]
	Learning Rate: 0.00203507
	LOSS [training: 0.4369527095610727 | validation: 0.5119767213523321]
	TIME [epoch: 8.54 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39985002917054835		[learning rate: 0.0020277]
	Learning Rate: 0.00202768
	LOSS [training: 0.39985002917054835 | validation: 0.36533349242605184]
	TIME [epoch: 8.53 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4263116009631679		[learning rate: 0.0020203]
	Learning Rate: 0.00202032
	LOSS [training: 0.4263116009631679 | validation: 0.313023180357758]
	TIME [epoch: 8.54 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4911833298573171		[learning rate: 0.002013]
	Learning Rate: 0.00201299
	LOSS [training: 0.4911833298573171 | validation: 0.2819788399214819]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44130767783306507		[learning rate: 0.0020057]
	Learning Rate: 0.00200569
	LOSS [training: 0.44130767783306507 | validation: 0.42068360781556124]
	TIME [epoch: 8.54 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4663303989695609		[learning rate: 0.0019984]
	Learning Rate: 0.00199841
	LOSS [training: 0.4663303989695609 | validation: 0.493505132397953]
	TIME [epoch: 8.54 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42610202173218126		[learning rate: 0.0019912]
	Learning Rate: 0.00199116
	LOSS [training: 0.42610202173218126 | validation: 0.47146854125161186]
	TIME [epoch: 8.55 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3847462037196674		[learning rate: 0.0019839]
	Learning Rate: 0.00198393
	LOSS [training: 0.3847462037196674 | validation: 0.6119774462657992]
	TIME [epoch: 8.55 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.374675390479083		[learning rate: 0.0019767]
	Learning Rate: 0.00197673
	LOSS [training: 0.374675390479083 | validation: 0.38803422016718636]
	TIME [epoch: 8.54 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41421498148738173		[learning rate: 0.0019696]
	Learning Rate: 0.00196956
	LOSS [training: 0.41421498148738173 | validation: 0.35393853395307573]
	TIME [epoch: 8.54 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4170781261204003		[learning rate: 0.0019624]
	Learning Rate: 0.00196241
	LOSS [training: 0.4170781261204003 | validation: 0.7124270767586757]
	TIME [epoch: 8.54 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46900698231532756		[learning rate: 0.0019553]
	Learning Rate: 0.00195529
	LOSS [training: 0.46900698231532756 | validation: 0.4672387061458463]
	TIME [epoch: 8.57 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39042740684566685		[learning rate: 0.0019482]
	Learning Rate: 0.00194819
	LOSS [training: 0.39042740684566685 | validation: 0.3979050787048968]
	TIME [epoch: 8.54 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.356809939995266		[learning rate: 0.0019411]
	Learning Rate: 0.00194112
	LOSS [training: 0.356809939995266 | validation: 0.4144680701983965]
	TIME [epoch: 8.54 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4462371337077033		[learning rate: 0.0019341]
	Learning Rate: 0.00193408
	LOSS [training: 0.4462371337077033 | validation: 0.3988216308861231]
	TIME [epoch: 8.54 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.426685740565401		[learning rate: 0.0019271]
	Learning Rate: 0.00192706
	LOSS [training: 0.426685740565401 | validation: 0.44086707727849694]
	TIME [epoch: 8.56 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.402458431636211		[learning rate: 0.0019201]
	Learning Rate: 0.00192006
	LOSS [training: 0.402458431636211 | validation: 0.3928988219713444]
	TIME [epoch: 8.54 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37248657520027		[learning rate: 0.0019131]
	Learning Rate: 0.0019131
	LOSS [training: 0.37248657520027 | validation: 0.6079570070569702]
	TIME [epoch: 8.54 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36574948891995823		[learning rate: 0.0019062]
	Learning Rate: 0.00190615
	LOSS [training: 0.36574948891995823 | validation: 0.8885228486720316]
	TIME [epoch: 8.54 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45926314775276145		[learning rate: 0.0018992]
	Learning Rate: 0.00189924
	LOSS [training: 0.45926314775276145 | validation: 0.2706316464690144]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_557.pth
	Model improved!!!
EPOCH 558/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3996253608658463		[learning rate: 0.0018923]
	Learning Rate: 0.00189234
	LOSS [training: 0.3996253608658463 | validation: 0.3623885283852214]
	TIME [epoch: 8.54 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.386641328587419		[learning rate: 0.0018855]
	Learning Rate: 0.00188548
	LOSS [training: 0.386641328587419 | validation: 0.35365463099881533]
	TIME [epoch: 8.54 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41915128078717484		[learning rate: 0.0018786]
	Learning Rate: 0.00187863
	LOSS [training: 0.41915128078717484 | validation: 0.2416833151409496]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_560.pth
	Model improved!!!
EPOCH 561/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43206710814346144		[learning rate: 0.0018718]
	Learning Rate: 0.00187182
	LOSS [training: 0.43206710814346144 | validation: 0.6380976235269429]
	TIME [epoch: 8.56 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45897233099496965		[learning rate: 0.001865]
	Learning Rate: 0.00186502
	LOSS [training: 0.45897233099496965 | validation: 0.30323424621179745]
	TIME [epoch: 8.54 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6685715160466265		[learning rate: 0.0018583]
	Learning Rate: 0.00185825
	LOSS [training: 0.6685715160466265 | validation: 0.4429854123665269]
	TIME [epoch: 8.54 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34775267057343595		[learning rate: 0.0018515]
	Learning Rate: 0.00185151
	LOSS [training: 0.34775267057343595 | validation: 0.591566721406759]
	TIME [epoch: 8.53 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38028928848667504		[learning rate: 0.0018448]
	Learning Rate: 0.00184479
	LOSS [training: 0.38028928848667504 | validation: 0.5931262720749396]
	TIME [epoch: 8.55 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.544962789237612		[learning rate: 0.0018381]
	Learning Rate: 0.0018381
	LOSS [training: 0.544962789237612 | validation: 0.2900637221158543]
	TIME [epoch: 8.54 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41230603474934363		[learning rate: 0.0018314]
	Learning Rate: 0.00183143
	LOSS [training: 0.41230603474934363 | validation: 0.44830280297636743]
	TIME [epoch: 8.54 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33336523654767075		[learning rate: 0.0018248]
	Learning Rate: 0.00182478
	LOSS [training: 0.33336523654767075 | validation: 0.44742060013837137]
	TIME [epoch: 8.53 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6340319398266946		[learning rate: 0.0018182]
	Learning Rate: 0.00181816
	LOSS [training: 0.6340319398266946 | validation: 0.3111799652966689]
	TIME [epoch: 8.53 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34785432711364295		[learning rate: 0.0018116]
	Learning Rate: 0.00181156
	LOSS [training: 0.34785432711364295 | validation: 0.350006502415784]
	TIME [epoch: 8.56 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34085535236836817		[learning rate: 0.001805]
	Learning Rate: 0.00180499
	LOSS [training: 0.34085535236836817 | validation: 0.3949773980959127]
	TIME [epoch: 8.53 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36540293228564086		[learning rate: 0.0017984]
	Learning Rate: 0.00179844
	LOSS [training: 0.36540293228564086 | validation: 0.25587059018516045]
	TIME [epoch: 8.53 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3955291891465814		[learning rate: 0.0017919]
	Learning Rate: 0.00179191
	LOSS [training: 0.3955291891465814 | validation: 0.3727159776166198]
	TIME [epoch: 8.54 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4504790751245508		[learning rate: 0.0017854]
	Learning Rate: 0.00178541
	LOSS [training: 0.4504790751245508 | validation: 0.3370600416538468]
	TIME [epoch: 8.56 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3812926729936059		[learning rate: 0.0017789]
	Learning Rate: 0.00177893
	LOSS [training: 0.3812926729936059 | validation: 0.3179589122827518]
	TIME [epoch: 8.54 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4025647962898308		[learning rate: 0.0017725]
	Learning Rate: 0.00177247
	LOSS [training: 0.4025647962898308 | validation: 0.6579374189016289]
	TIME [epoch: 8.53 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4592155507576943		[learning rate: 0.001766]
	Learning Rate: 0.00176604
	LOSS [training: 0.4592155507576943 | validation: 0.2688886961816502]
	TIME [epoch: 8.54 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32373202694919456		[learning rate: 0.0017596]
	Learning Rate: 0.00175963
	LOSS [training: 0.32373202694919456 | validation: 0.268232209156514]
	TIME [epoch: 8.56 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43087295909285234		[learning rate: 0.0017532]
	Learning Rate: 0.00175324
	LOSS [training: 0.43087295909285234 | validation: 0.26689754359586343]
	TIME [epoch: 8.54 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40986057065941645		[learning rate: 0.0017469]
	Learning Rate: 0.00174688
	LOSS [training: 0.40986057065941645 | validation: 0.3862193969383763]
	TIME [epoch: 8.54 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3762780382513294		[learning rate: 0.0017405]
	Learning Rate: 0.00174054
	LOSS [training: 0.3762780382513294 | validation: 0.3474436005501451]
	TIME [epoch: 8.54 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42030657784322145		[learning rate: 0.0017342]
	Learning Rate: 0.00173422
	LOSS [training: 0.42030657784322145 | validation: 0.3084809019442065]
	TIME [epoch: 8.56 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3534525011789143		[learning rate: 0.0017279]
	Learning Rate: 0.00172793
	LOSS [training: 0.3534525011789143 | validation: 0.8053373580257477]
	TIME [epoch: 8.56 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4050532112449227		[learning rate: 0.0017217]
	Learning Rate: 0.00172166
	LOSS [training: 0.4050532112449227 | validation: 0.3837356050776861]
	TIME [epoch: 8.54 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41890395032022976		[learning rate: 0.0017154]
	Learning Rate: 0.00171541
	LOSS [training: 0.41890395032022976 | validation: 0.4461749945448125]
	TIME [epoch: 8.54 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3510071880231639		[learning rate: 0.0017092]
	Learning Rate: 0.00170919
	LOSS [training: 0.3510071880231639 | validation: 0.27132226081266775]
	TIME [epoch: 8.54 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3754898275998929		[learning rate: 0.001703]
	Learning Rate: 0.00170298
	LOSS [training: 0.3754898275998929 | validation: 0.28172206338465]
	TIME [epoch: 8.57 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3574866134031784		[learning rate: 0.0016968]
	Learning Rate: 0.0016968
	LOSS [training: 0.3574866134031784 | validation: 0.3170681389476659]
	TIME [epoch: 8.54 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3149237963011151		[learning rate: 0.0016906]
	Learning Rate: 0.00169065
	LOSS [training: 0.3149237963011151 | validation: 0.2861152634374081]
	TIME [epoch: 8.54 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31508099060487826		[learning rate: 0.0016845]
	Learning Rate: 0.00168451
	LOSS [training: 0.31508099060487826 | validation: 0.28064412606314415]
	TIME [epoch: 8.53 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4184220129733619		[learning rate: 0.0016784]
	Learning Rate: 0.0016784
	LOSS [training: 0.4184220129733619 | validation: 0.3904833080346374]
	TIME [epoch: 8.57 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35399850510062214		[learning rate: 0.0016723]
	Learning Rate: 0.00167231
	LOSS [training: 0.35399850510062214 | validation: 0.26669956990719373]
	TIME [epoch: 8.54 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36586446321051647		[learning rate: 0.0016662]
	Learning Rate: 0.00166624
	LOSS [training: 0.36586446321051647 | validation: 0.31574566734288334]
	TIME [epoch: 8.54 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3657059697984555		[learning rate: 0.0016602]
	Learning Rate: 0.00166019
	LOSS [training: 0.3657059697984555 | validation: 0.40275423149244105]
	TIME [epoch: 8.54 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42729164642782447		[learning rate: 0.0016542]
	Learning Rate: 0.00165417
	LOSS [training: 0.42729164642782447 | validation: 0.5928600847895049]
	TIME [epoch: 8.57 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38249884394379097		[learning rate: 0.0016482]
	Learning Rate: 0.00164816
	LOSS [training: 0.38249884394379097 | validation: 0.2739201097447846]
	TIME [epoch: 8.54 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3878075412701916		[learning rate: 0.0016422]
	Learning Rate: 0.00164218
	LOSS [training: 0.3878075412701916 | validation: 0.2971303425233073]
	TIME [epoch: 8.54 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3036655131095579		[learning rate: 0.0016362]
	Learning Rate: 0.00163622
	LOSS [training: 0.3036655131095579 | validation: 0.31800433520397775]
	TIME [epoch: 8.54 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4665739711727964		[learning rate: 0.0016303]
	Learning Rate: 0.00163028
	LOSS [training: 0.4665739711727964 | validation: 0.5245347572784126]
	TIME [epoch: 8.55 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38183204741089566		[learning rate: 0.0016244]
	Learning Rate: 0.00162437
	LOSS [training: 0.38183204741089566 | validation: 0.36947640998782566]
	TIME [epoch: 8.55 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.349111620038756		[learning rate: 0.0016185]
	Learning Rate: 0.00161847
	LOSS [training: 0.349111620038756 | validation: 0.30003683525800184]
	TIME [epoch: 8.54 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35808110668334614		[learning rate: 0.0016126]
	Learning Rate: 0.0016126
	LOSS [training: 0.35808110668334614 | validation: 0.3009331246338621]
	TIME [epoch: 8.54 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40590447306142696		[learning rate: 0.0016067]
	Learning Rate: 0.00160675
	LOSS [training: 0.40590447306142696 | validation: 0.33136409966232117]
	TIME [epoch: 8.55 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3780078220592037		[learning rate: 0.0016009]
	Learning Rate: 0.00160092
	LOSS [training: 0.3780078220592037 | validation: 0.3899982739424617]
	TIME [epoch: 8.56 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44917095427441867		[learning rate: 0.0015951]
	Learning Rate: 0.00159511
	LOSS [training: 0.44917095427441867 | validation: 0.22481163163568735]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_605.pth
	Model improved!!!
EPOCH 606/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5656629843365094		[learning rate: 0.0015893]
	Learning Rate: 0.00158932
	LOSS [training: 0.5656629843365094 | validation: 0.26136185305380966]
	TIME [epoch: 8.53 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3237057007167904		[learning rate: 0.0015835]
	Learning Rate: 0.00158355
	LOSS [training: 0.3237057007167904 | validation: 0.6420546780924785]
	TIME [epoch: 8.53 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33788940438926435		[learning rate: 0.0015778]
	Learning Rate: 0.0015778
	LOSS [training: 0.33788940438926435 | validation: 0.35009004278919853]
	TIME [epoch: 8.56 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35997686131261686		[learning rate: 0.0015721]
	Learning Rate: 0.00157208
	LOSS [training: 0.35997686131261686 | validation: 0.2733360922137209]
	TIME [epoch: 8.53 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35439905700050933		[learning rate: 0.0015664]
	Learning Rate: 0.00156637
	LOSS [training: 0.35439905700050933 | validation: 0.5393148564729564]
	TIME [epoch: 8.54 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35132925253532027		[learning rate: 0.0015607]
	Learning Rate: 0.00156069
	LOSS [training: 0.35132925253532027 | validation: 0.44315929146519856]
	TIME [epoch: 8.53 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4296661978102308		[learning rate: 0.001555]
	Learning Rate: 0.00155502
	LOSS [training: 0.4296661978102308 | validation: 0.32585282540843513]
	TIME [epoch: 8.55 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3073051517371419		[learning rate: 0.0015494]
	Learning Rate: 0.00154938
	LOSS [training: 0.3073051517371419 | validation: 0.2081417907735614]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_613.pth
	Model improved!!!
EPOCH 614/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3605575298684079		[learning rate: 0.0015438]
	Learning Rate: 0.00154376
	LOSS [training: 0.3605575298684079 | validation: 0.25727101209575687]
	TIME [epoch: 8.53 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32027909663262943		[learning rate: 0.0015382]
	Learning Rate: 0.00153815
	LOSS [training: 0.32027909663262943 | validation: 0.33561101902688006]
	TIME [epoch: 8.54 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4112158462344303		[learning rate: 0.0015326]
	Learning Rate: 0.00153257
	LOSS [training: 0.4112158462344303 | validation: 0.24085964124254708]
	TIME [epoch: 8.55 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2945490830600545		[learning rate: 0.001527]
	Learning Rate: 0.00152701
	LOSS [training: 0.2945490830600545 | validation: 0.27736396513236194]
	TIME [epoch: 8.54 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34249366000123727		[learning rate: 0.0015215]
	Learning Rate: 0.00152147
	LOSS [training: 0.34249366000123727 | validation: 0.5464466775872303]
	TIME [epoch: 8.53 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3414745277554655		[learning rate: 0.0015159]
	Learning Rate: 0.00151595
	LOSS [training: 0.3414745277554655 | validation: 0.23432180083912318]
	TIME [epoch: 8.53 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42576025321264177		[learning rate: 0.0015104]
	Learning Rate: 0.00151045
	LOSS [training: 0.42576025321264177 | validation: 0.41162476886626753]
	TIME [epoch: 8.54 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4123359736797177		[learning rate: 0.001505]
	Learning Rate: 0.00150496
	LOSS [training: 0.4123359736797177 | validation: 0.3249861794009318]
	TIME [epoch: 8.54 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3157529230182103		[learning rate: 0.0014995]
	Learning Rate: 0.0014995
	LOSS [training: 0.3157529230182103 | validation: 0.5908620760609306]
	TIME [epoch: 8.53 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36302543618522753		[learning rate: 0.0014941]
	Learning Rate: 0.00149406
	LOSS [training: 0.36302543618522753 | validation: 0.3286007742479425]
	TIME [epoch: 8.53 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31470829023858704		[learning rate: 0.0014886]
	Learning Rate: 0.00148864
	LOSS [training: 0.31470829023858704 | validation: 0.3014985708276874]
	TIME [epoch: 8.53 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29083181527660334		[learning rate: 0.0014832]
	Learning Rate: 0.00148324
	LOSS [training: 0.29083181527660334 | validation: 0.3347591786615157]
	TIME [epoch: 8.55 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2642515441075699		[learning rate: 0.0014779]
	Learning Rate: 0.00147785
	LOSS [training: 0.2642515441075699 | validation: 0.24090763773035712]
	TIME [epoch: 8.52 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.825717212732225		[learning rate: 0.0014725]
	Learning Rate: 0.00147249
	LOSS [training: 0.825717212732225 | validation: 0.4708244137638713]
	TIME [epoch: 8.53 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35539480983111454		[learning rate: 0.0014671]
	Learning Rate: 0.00146715
	LOSS [training: 0.35539480983111454 | validation: 0.26778217285131317]
	TIME [epoch: 8.53 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35196246707121465		[learning rate: 0.0014618]
	Learning Rate: 0.00146182
	LOSS [training: 0.35196246707121465 | validation: 0.2986022768957209]
	TIME [epoch: 8.55 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3693109756700682		[learning rate: 0.0014565]
	Learning Rate: 0.00145652
	LOSS [training: 0.3693109756700682 | validation: 0.3334882886825272]
	TIME [epoch: 8.53 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3077637282814511		[learning rate: 0.0014512]
	Learning Rate: 0.00145123
	LOSS [training: 0.3077637282814511 | validation: 0.24316941717101553]
	TIME [epoch: 8.53 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3198750432045957		[learning rate: 0.001446]
	Learning Rate: 0.00144597
	LOSS [training: 0.3198750432045957 | validation: 0.8985363838672635]
	TIME [epoch: 8.53 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.571572099523825		[learning rate: 0.0014407]
	Learning Rate: 0.00144072
	LOSS [training: 0.571572099523825 | validation: 0.28424859402718444]
	TIME [epoch: 8.55 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3052480869154618		[learning rate: 0.0014355]
	Learning Rate: 0.00143549
	LOSS [training: 0.3052480869154618 | validation: 0.359598578522064]
	TIME [epoch: 8.53 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3702185613237784		[learning rate: 0.0014303]
	Learning Rate: 0.00143028
	LOSS [training: 0.3702185613237784 | validation: 0.26487596364596205]
	TIME [epoch: 8.53 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2823265277330672		[learning rate: 0.0014251]
	Learning Rate: 0.00142509
	LOSS [training: 0.2823265277330672 | validation: 0.2694294659484243]
	TIME [epoch: 8.53 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29535993849925746		[learning rate: 0.0014199]
	Learning Rate: 0.00141992
	LOSS [training: 0.29535993849925746 | validation: 0.23171761547338304]
	TIME [epoch: 8.54 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3476864393080212		[learning rate: 0.0014148]
	Learning Rate: 0.00141476
	LOSS [training: 0.3476864393080212 | validation: 0.3993932485748355]
	TIME [epoch: 8.53 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30866909430626166		[learning rate: 0.0014096]
	Learning Rate: 0.00140963
	LOSS [training: 0.30866909430626166 | validation: 0.30275326899599486]
	TIME [epoch: 8.53 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2859783770760128		[learning rate: 0.0014045]
	Learning Rate: 0.00140451
	LOSS [training: 0.2859783770760128 | validation: 0.324217687331239]
	TIME [epoch: 8.53 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2879557104706827		[learning rate: 0.0013994]
	Learning Rate: 0.00139942
	LOSS [training: 0.2879557104706827 | validation: 0.3502348335441109]
	TIME [epoch: 8.54 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3907423231072387		[learning rate: 0.0013943]
	Learning Rate: 0.00139434
	LOSS [training: 0.3907423231072387 | validation: 0.28988694880070437]
	TIME [epoch: 8.55 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3035327011646532		[learning rate: 0.0013893]
	Learning Rate: 0.00138928
	LOSS [training: 0.3035327011646532 | validation: 0.3072095171843242]
	TIME [epoch: 8.53 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2817297943727502		[learning rate: 0.0013842]
	Learning Rate: 0.00138424
	LOSS [training: 0.2817297943727502 | validation: 0.33880649060748436]
	TIME [epoch: 8.53 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26961387212784893		[learning rate: 0.0013792]
	Learning Rate: 0.00137921
	LOSS [training: 0.26961387212784893 | validation: 0.2405011680338034]
	TIME [epoch: 8.53 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2727759334202299		[learning rate: 0.0013742]
	Learning Rate: 0.00137421
	LOSS [training: 0.2727759334202299 | validation: 0.2795033563231454]
	TIME [epoch: 8.55 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3103745129378455		[learning rate: 0.0013692]
	Learning Rate: 0.00136922
	LOSS [training: 0.3103745129378455 | validation: 0.41509593478392426]
	TIME [epoch: 8.53 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3798759814009433		[learning rate: 0.0013643]
	Learning Rate: 0.00136425
	LOSS [training: 0.3798759814009433 | validation: 0.2554245019450753]
	TIME [epoch: 8.53 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26750422508840666		[learning rate: 0.0013593]
	Learning Rate: 0.0013593
	LOSS [training: 0.26750422508840666 | validation: 0.2145762635131673]
	TIME [epoch: 8.53 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25783024337536764		[learning rate: 0.0013544]
	Learning Rate: 0.00135437
	LOSS [training: 0.25783024337536764 | validation: 0.27466962372222387]
	TIME [epoch: 8.55 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3528991367563095		[learning rate: 0.0013495]
	Learning Rate: 0.00134945
	LOSS [training: 0.3528991367563095 | validation: 0.4027473340196755]
	TIME [epoch: 8.53 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30037689647972654		[learning rate: 0.0013446]
	Learning Rate: 0.00134456
	LOSS [training: 0.30037689647972654 | validation: 0.2798646468453869]
	TIME [epoch: 8.53 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24973342307471347		[learning rate: 0.0013397]
	Learning Rate: 0.00133968
	LOSS [training: 0.24973342307471347 | validation: 0.4019552291204779]
	TIME [epoch: 8.53 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2818045093353667		[learning rate: 0.0013348]
	Learning Rate: 0.00133481
	LOSS [training: 0.2818045093353667 | validation: 0.26357635330475854]
	TIME [epoch: 8.55 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31008730622966707		[learning rate: 0.00133]
	Learning Rate: 0.00132997
	LOSS [training: 0.31008730622966707 | validation: 0.3895171695044078]
	TIME [epoch: 8.53 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44597182528281165		[learning rate: 0.0013251]
	Learning Rate: 0.00132514
	LOSS [training: 0.44597182528281165 | validation: 0.25594945220609394]
	TIME [epoch: 8.53 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.351429833956756		[learning rate: 0.0013203]
	Learning Rate: 0.00132034
	LOSS [training: 0.351429833956756 | validation: 0.6155584663637559]
	TIME [epoch: 8.52 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3306214982681214		[learning rate: 0.0013155]
	Learning Rate: 0.00131554
	LOSS [training: 0.3306214982681214 | validation: 0.19981886311036984]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_658.pth
	Model improved!!!
EPOCH 659/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22554978173584236		[learning rate: 0.0013108]
	Learning Rate: 0.00131077
	LOSS [training: 0.22554978173584236 | validation: 0.25501857219132557]
	TIME [epoch: 8.54 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4713753564367842		[learning rate: 0.001306]
	Learning Rate: 0.00130601
	LOSS [training: 0.4713753564367842 | validation: 0.3031421949009524]
	TIME [epoch: 8.53 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31205348760440743		[learning rate: 0.0013013]
	Learning Rate: 0.00130127
	LOSS [training: 0.31205348760440743 | validation: 0.20411338124126027]
	TIME [epoch: 8.53 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26626444591167997		[learning rate: 0.0012966]
	Learning Rate: 0.00129655
	LOSS [training: 0.26626444591167997 | validation: 0.20544659768872722]
	TIME [epoch: 8.53 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29999316782265595		[learning rate: 0.0012918]
	Learning Rate: 0.00129185
	LOSS [training: 0.29999316782265595 | validation: 0.2234261906814968]
	TIME [epoch: 8.55 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35185539739549637		[learning rate: 0.0012872]
	Learning Rate: 0.00128716
	LOSS [training: 0.35185539739549637 | validation: 0.4936433348546344]
	TIME [epoch: 8.53 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28375188536385776		[learning rate: 0.0012825]
	Learning Rate: 0.00128249
	LOSS [training: 0.28375188536385776 | validation: 0.3979920692225701]
	TIME [epoch: 8.52 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2805227941619894		[learning rate: 0.0012778]
	Learning Rate: 0.00127783
	LOSS [training: 0.2805227941619894 | validation: 0.2854932557543719]
	TIME [epoch: 8.53 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2698088262467378		[learning rate: 0.0012732]
	Learning Rate: 0.00127319
	LOSS [training: 0.2698088262467378 | validation: 0.20233845342068685]
	TIME [epoch: 8.55 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37506502146080434		[learning rate: 0.0012686]
	Learning Rate: 0.00126857
	LOSS [training: 0.37506502146080434 | validation: 0.5967817958545438]
	TIME [epoch: 8.53 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33579015917435695		[learning rate: 0.001264]
	Learning Rate: 0.00126397
	LOSS [training: 0.33579015917435695 | validation: 0.19188308423879918]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_669.pth
	Model improved!!!
EPOCH 670/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25685325557271643		[learning rate: 0.0012594]
	Learning Rate: 0.00125938
	LOSS [training: 0.25685325557271643 | validation: 0.26742138294511286]
	TIME [epoch: 8.52 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2561469504007225		[learning rate: 0.0012548]
	Learning Rate: 0.00125481
	LOSS [training: 0.2561469504007225 | validation: 0.286851100034767]
	TIME [epoch: 8.55 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2703734914436112		[learning rate: 0.0012503]
	Learning Rate: 0.00125026
	LOSS [training: 0.2703734914436112 | validation: 0.23531194520067336]
	TIME [epoch: 8.53 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3044722378255814		[learning rate: 0.0012457]
	Learning Rate: 0.00124572
	LOSS [training: 0.3044722378255814 | validation: 0.2569937822173206]
	TIME [epoch: 8.53 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31658840536235255		[learning rate: 0.0012412]
	Learning Rate: 0.0012412
	LOSS [training: 0.31658840536235255 | validation: 0.2634826927690345]
	TIME [epoch: 8.52 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28932094929856095		[learning rate: 0.0012367]
	Learning Rate: 0.0012367
	LOSS [training: 0.28932094929856095 | validation: 0.34069349433924634]
	TIME [epoch: 8.54 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33334466062350426		[learning rate: 0.0012322]
	Learning Rate: 0.00123221
	LOSS [training: 0.33334466062350426 | validation: 0.3181421821290984]
	TIME [epoch: 8.53 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3542240454699618		[learning rate: 0.0012277]
	Learning Rate: 0.00122774
	LOSS [training: 0.3542240454699618 | validation: 0.2874345791387666]
	TIME [epoch: 8.53 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26630283087193013		[learning rate: 0.0012233]
	Learning Rate: 0.00122328
	LOSS [training: 0.26630283087193013 | validation: 0.4854783620573946]
	TIME [epoch: 8.52 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3116035925903793		[learning rate: 0.0012188]
	Learning Rate: 0.00121884
	LOSS [training: 0.3116035925903793 | validation: 0.29001615166986466]
	TIME [epoch: 8.53 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28844647502920917		[learning rate: 0.0012144]
	Learning Rate: 0.00121442
	LOSS [training: 0.28844647502920917 | validation: 0.23214570906161214]
	TIME [epoch: 8.54 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2631868308949984		[learning rate: 0.00121]
	Learning Rate: 0.00121001
	LOSS [training: 0.2631868308949984 | validation: 0.27970675092320324]
	TIME [epoch: 8.52 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3274347559089995		[learning rate: 0.0012056]
	Learning Rate: 0.00120562
	LOSS [training: 0.3274347559089995 | validation: 0.34508178376072424]
	TIME [epoch: 8.52 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30529221362326997		[learning rate: 0.0012012]
	Learning Rate: 0.00120125
	LOSS [training: 0.30529221362326997 | validation: 0.23151624955011613]
	TIME [epoch: 8.51 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2706452048240643		[learning rate: 0.0011969]
	Learning Rate: 0.00119689
	LOSS [training: 0.2706452048240643 | validation: 0.2289908402641289]
	TIME [epoch: 8.55 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.301904981323793		[learning rate: 0.0011925]
	Learning Rate: 0.00119254
	LOSS [training: 0.301904981323793 | validation: 0.23425013170487358]
	TIME [epoch: 8.52 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30041858070173066		[learning rate: 0.0011882]
	Learning Rate: 0.00118821
	LOSS [training: 0.30041858070173066 | validation: 0.2572493855937964]
	TIME [epoch: 8.52 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2729978483070932		[learning rate: 0.0011839]
	Learning Rate: 0.0011839
	LOSS [training: 0.2729978483070932 | validation: 0.23349945598547395]
	TIME [epoch: 8.52 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25959071172770054		[learning rate: 0.0011796]
	Learning Rate: 0.00117961
	LOSS [training: 0.25959071172770054 | validation: 0.2277131799889218]
	TIME [epoch: 8.55 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35684083388354787		[learning rate: 0.0011753]
	Learning Rate: 0.00117532
	LOSS [training: 0.35684083388354787 | validation: 0.2977442157687781]
	TIME [epoch: 8.52 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3010998728318244		[learning rate: 0.0011711]
	Learning Rate: 0.00117106
	LOSS [training: 0.3010998728318244 | validation: 0.2809929051227249]
	TIME [epoch: 8.52 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2749263605334505		[learning rate: 0.0011668]
	Learning Rate: 0.00116681
	LOSS [training: 0.2749263605334505 | validation: 0.31661068098337797]
	TIME [epoch: 8.53 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28162467684040643		[learning rate: 0.0011626]
	Learning Rate: 0.00116258
	LOSS [training: 0.28162467684040643 | validation: 0.2882126673101848]
	TIME [epoch: 8.55 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36808221516552886		[learning rate: 0.0011584]
	Learning Rate: 0.00115836
	LOSS [training: 0.36808221516552886 | validation: 0.3721593326209443]
	TIME [epoch: 8.53 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2680189173785987		[learning rate: 0.0011542]
	Learning Rate: 0.00115415
	LOSS [training: 0.2680189173785987 | validation: 0.2435855446023466]
	TIME [epoch: 8.52 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28242992869454303		[learning rate: 0.00115]
	Learning Rate: 0.00114996
	LOSS [training: 0.28242992869454303 | validation: 0.1744264410105012]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_695.pth
	Model improved!!!
EPOCH 696/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28025279384552254		[learning rate: 0.0011458]
	Learning Rate: 0.00114579
	LOSS [training: 0.28025279384552254 | validation: 0.2456237109618878]
	TIME [epoch: 8.54 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24783194756085672		[learning rate: 0.0011416]
	Learning Rate: 0.00114163
	LOSS [training: 0.24783194756085672 | validation: 0.24853649834024705]
	TIME [epoch: 8.54 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3155995566111674		[learning rate: 0.0011375]
	Learning Rate: 0.00113749
	LOSS [training: 0.3155995566111674 | validation: 0.20144400393287695]
	TIME [epoch: 8.52 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29711290018166553		[learning rate: 0.0011334]
	Learning Rate: 0.00113336
	LOSS [training: 0.29711290018166553 | validation: 0.20783973345342602]
	TIME [epoch: 8.53 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21883635618191194		[learning rate: 0.0011292]
	Learning Rate: 0.00112925
	LOSS [training: 0.21883635618191194 | validation: 0.25339803966405516]
	TIME [epoch: 8.53 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2724446351682467		[learning rate: 0.0011252]
	Learning Rate: 0.00112515
	LOSS [training: 0.2724446351682467 | validation: 0.28373005785872973]
	TIME [epoch: 8.55 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3314922205865427		[learning rate: 0.0011211]
	Learning Rate: 0.00112107
	LOSS [training: 0.3314922205865427 | validation: 0.24777443385347347]
	TIME [epoch: 8.52 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28151892263946066		[learning rate: 0.001117]
	Learning Rate: 0.001117
	LOSS [training: 0.28151892263946066 | validation: 0.20929518555740062]
	TIME [epoch: 8.53 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28429768212841416		[learning rate: 0.0011129]
	Learning Rate: 0.00111295
	LOSS [training: 0.28429768212841416 | validation: 0.26916649323300823]
	TIME [epoch: 8.53 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27482733592319303		[learning rate: 0.0011089]
	Learning Rate: 0.00110891
	LOSS [training: 0.27482733592319303 | validation: 0.22795620809084688]
	TIME [epoch: 8.55 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2814446556114869		[learning rate: 0.0011049]
	Learning Rate: 0.00110488
	LOSS [training: 0.2814446556114869 | validation: 0.3882401461208596]
	TIME [epoch: 8.53 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2709048344429318		[learning rate: 0.0011009]
	Learning Rate: 0.00110087
	LOSS [training: 0.2709048344429318 | validation: 0.2501041358247774]
	TIME [epoch: 8.53 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22843635213854604		[learning rate: 0.0010969]
	Learning Rate: 0.00109688
	LOSS [training: 0.22843635213854604 | validation: 0.24322508390605424]
	TIME [epoch: 8.53 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27515200054666716		[learning rate: 0.0010929]
	Learning Rate: 0.0010929
	LOSS [training: 0.27515200054666716 | validation: 0.2295779575606056]
	TIME [epoch: 8.55 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2581245698614511		[learning rate: 0.0010889]
	Learning Rate: 0.00108893
	LOSS [training: 0.2581245698614511 | validation: 0.20080062406637694]
	TIME [epoch: 8.53 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.244756205984008		[learning rate: 0.001085]
	Learning Rate: 0.00108498
	LOSS [training: 0.244756205984008 | validation: 0.18682501160087076]
	TIME [epoch: 8.52 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30054622138337955		[learning rate: 0.001081]
	Learning Rate: 0.00108104
	LOSS [training: 0.30054622138337955 | validation: 0.3978338041774336]
	TIME [epoch: 8.52 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2955446935832668		[learning rate: 0.0010771]
	Learning Rate: 0.00107712
	LOSS [training: 0.2955446935832668 | validation: 0.22761315400884097]
	TIME [epoch: 8.54 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2590389857705747		[learning rate: 0.0010732]
	Learning Rate: 0.00107321
	LOSS [training: 0.2590389857705747 | validation: 0.20819175054031286]
	TIME [epoch: 8.54 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2565799756934557		[learning rate: 0.0010693]
	Learning Rate: 0.00106931
	LOSS [training: 0.2565799756934557 | validation: 0.41987140293300124]
	TIME [epoch: 8.53 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2920243535705305		[learning rate: 0.0010654]
	Learning Rate: 0.00106543
	LOSS [training: 0.2920243535705305 | validation: 0.23322994738323155]
	TIME [epoch: 8.52 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2969940776619736		[learning rate: 0.0010616]
	Learning Rate: 0.00106157
	LOSS [training: 0.2969940776619736 | validation: 0.1521747512343826]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_717.pth
	Model improved!!!
EPOCH 718/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.322007002314305		[learning rate: 0.0010577]
	Learning Rate: 0.00105771
	LOSS [training: 0.322007002314305 | validation: 0.19774778214137606]
	TIME [epoch: 8.54 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2557179418852192		[learning rate: 0.0010539]
	Learning Rate: 0.00105388
	LOSS [training: 0.2557179418852192 | validation: 0.2662920044113614]
	TIME [epoch: 8.51 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2619752506186551		[learning rate: 0.0010501]
	Learning Rate: 0.00105005
	LOSS [training: 0.2619752506186551 | validation: 0.32436285038503787]
	TIME [epoch: 8.52 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23070130965745128		[learning rate: 0.0010462]
	Learning Rate: 0.00104624
	LOSS [training: 0.23070130965745128 | validation: 0.2987520475131351]
	TIME [epoch: 8.51 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.274360218298609		[learning rate: 0.0010424]
	Learning Rate: 0.00104244
	LOSS [training: 0.274360218298609 | validation: 0.18315945263963487]
	TIME [epoch: 8.55 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2643859275895346		[learning rate: 0.0010387]
	Learning Rate: 0.00103866
	LOSS [training: 0.2643859275895346 | validation: 0.2880036585041029]
	TIME [epoch: 8.52 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24159923347618487		[learning rate: 0.0010349]
	Learning Rate: 0.00103489
	LOSS [training: 0.24159923347618487 | validation: 0.19981543669096138]
	TIME [epoch: 8.53 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26778641958848737		[learning rate: 0.0010311]
	Learning Rate: 0.00103114
	LOSS [training: 0.26778641958848737 | validation: 0.2782224978838162]
	TIME [epoch: 8.52 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22525694950109668		[learning rate: 0.0010274]
	Learning Rate: 0.00102739
	LOSS [training: 0.22525694950109668 | validation: 0.18293029180751225]
	TIME [epoch: 8.55 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23288385943511508		[learning rate: 0.0010237]
	Learning Rate: 0.00102367
	LOSS [training: 0.23288385943511508 | validation: 0.1886461037245912]
	TIME [epoch: 8.53 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22854984945697052		[learning rate: 0.00102]
	Learning Rate: 0.00101995
	LOSS [training: 0.22854984945697052 | validation: 0.21384144180108922]
	TIME [epoch: 8.52 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26322093812484865		[learning rate: 0.0010162]
	Learning Rate: 0.00101625
	LOSS [training: 0.26322093812484865 | validation: 0.15633623462436397]
	TIME [epoch: 8.52 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34282814522591487		[learning rate: 0.0010126]
	Learning Rate: 0.00101256
	LOSS [training: 0.34282814522591487 | validation: 0.25392854866668757]
	TIME [epoch: 8.54 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3059773701468876		[learning rate: 0.0010089]
	Learning Rate: 0.00100889
	LOSS [training: 0.3059773701468876 | validation: 0.3768763852688446]
	TIME [epoch: 8.53 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29532379086047245		[learning rate: 0.0010052]
	Learning Rate: 0.00100522
	LOSS [training: 0.29532379086047245 | validation: 0.2253200053121837]
	TIME [epoch: 8.53 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2676233108279439		[learning rate: 0.0010016]
	Learning Rate: 0.00100158
	LOSS [training: 0.2676233108279439 | validation: 0.16320477577710757]
	TIME [epoch: 8.52 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20927231918401606		[learning rate: 0.00099794]
	Learning Rate: 0.000997942
	LOSS [training: 0.20927231918401606 | validation: 0.23671205026392156]
	TIME [epoch: 8.55 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29105994263710394		[learning rate: 0.00099432]
	Learning Rate: 0.00099432
	LOSS [training: 0.29105994263710394 | validation: 0.15697899353199185]
	TIME [epoch: 8.54 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2513375367791805		[learning rate: 0.00099071]
	Learning Rate: 0.000990712
	LOSS [training: 0.2513375367791805 | validation: 0.20444944368207793]
	TIME [epoch: 8.53 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22994788807481337		[learning rate: 0.00098712]
	Learning Rate: 0.000987116
	LOSS [training: 0.22994788807481337 | validation: 0.39431997397606333]
	TIME [epoch: 8.53 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26180689192708323		[learning rate: 0.00098353]
	Learning Rate: 0.000983534
	LOSS [training: 0.26180689192708323 | validation: 0.19129273122257245]
	TIME [epoch: 8.53 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2209841527225231		[learning rate: 0.00097996]
	Learning Rate: 0.000979965
	LOSS [training: 0.2209841527225231 | validation: 0.2469656475819534]
	TIME [epoch: 8.55 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2572447493560931		[learning rate: 0.00097641]
	Learning Rate: 0.000976409
	LOSS [training: 0.2572447493560931 | validation: 0.20267325231152805]
	TIME [epoch: 8.53 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23639170418215943		[learning rate: 0.00097287]
	Learning Rate: 0.000972865
	LOSS [training: 0.23639170418215943 | validation: 0.2850621237436016]
	TIME [epoch: 8.52 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26349077448929814		[learning rate: 0.00096933]
	Learning Rate: 0.000969335
	LOSS [training: 0.26349077448929814 | validation: 0.2147068607850741]
	TIME [epoch: 8.53 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23268359449044987		[learning rate: 0.00096582]
	Learning Rate: 0.000965817
	LOSS [training: 0.23268359449044987 | validation: 0.24751609402525449]
	TIME [epoch: 8.54 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.244013226186991		[learning rate: 0.00096231]
	Learning Rate: 0.000962312
	LOSS [training: 0.244013226186991 | validation: 0.1920032379428796]
	TIME [epoch: 8.53 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21379329704999028		[learning rate: 0.00095882]
	Learning Rate: 0.00095882
	LOSS [training: 0.21379329704999028 | validation: 0.16968274757526092]
	TIME [epoch: 8.53 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2667745178495564		[learning rate: 0.00095534]
	Learning Rate: 0.00095534
	LOSS [training: 0.2667745178495564 | validation: 0.25226775271874247]
	TIME [epoch: 8.53 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3736252445056787		[learning rate: 0.00095187]
	Learning Rate: 0.000951873
	LOSS [training: 0.3736252445056787 | validation: 0.18151032178235216]
	TIME [epoch: 8.55 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2638563047928938		[learning rate: 0.00094842]
	Learning Rate: 0.000948419
	LOSS [training: 0.2638563047928938 | validation: 0.5787035070657209]
	TIME [epoch: 8.54 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2714355588971625		[learning rate: 0.00094498]
	Learning Rate: 0.000944977
	LOSS [training: 0.2714355588971625 | validation: 0.24442629936826543]
	TIME [epoch: 8.53 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2540910300054948		[learning rate: 0.00094155]
	Learning Rate: 0.000941547
	LOSS [training: 0.2540910300054948 | validation: 0.21999174324062826]
	TIME [epoch: 8.53 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2379010056426965		[learning rate: 0.00093813]
	Learning Rate: 0.00093813
	LOSS [training: 0.2379010056426965 | validation: 0.2576567400449258]
	TIME [epoch: 8.55 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22228417479056536		[learning rate: 0.00093473]
	Learning Rate: 0.000934726
	LOSS [training: 0.22228417479056536 | validation: 0.2115248620237635]
	TIME [epoch: 8.54 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21433139209445326		[learning rate: 0.00093133]
	Learning Rate: 0.000931334
	LOSS [training: 0.21433139209445326 | validation: 0.16869784747386618]
	TIME [epoch: 8.53 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.507445171257529		[learning rate: 0.00092795]
	Learning Rate: 0.000927954
	LOSS [training: 0.507445171257529 | validation: 0.30398008463186144]
	TIME [epoch: 8.53 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3643033905428631		[learning rate: 0.00092459]
	Learning Rate: 0.000924586
	LOSS [training: 0.3643033905428631 | validation: 0.18376813474004766]
	TIME [epoch: 8.53 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31259642687533445		[learning rate: 0.00092123]
	Learning Rate: 0.000921231
	LOSS [training: 0.31259642687533445 | validation: 0.1586638751700108]
	TIME [epoch: 8.56 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33425175119819905		[learning rate: 0.00091789]
	Learning Rate: 0.000917888
	LOSS [training: 0.33425175119819905 | validation: 0.18958846063875778]
	TIME [epoch: 8.53 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25196383130396666		[learning rate: 0.00091456]
	Learning Rate: 0.000914556
	LOSS [training: 0.25196383130396666 | validation: 0.4204430878534281]
	TIME [epoch: 8.53 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33288315250384465		[learning rate: 0.00091124]
	Learning Rate: 0.000911237
	LOSS [training: 0.33288315250384465 | validation: 0.2563332863015454]
	TIME [epoch: 8.53 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23701758072642623		[learning rate: 0.00090793]
	Learning Rate: 0.000907931
	LOSS [training: 0.23701758072642623 | validation: 0.14527698995055377]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_760.pth
	Model improved!!!
EPOCH 761/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2903949488215989		[learning rate: 0.00090464]
	Learning Rate: 0.000904636
	LOSS [training: 0.2903949488215989 | validation: 0.31771475100019525]
	TIME [epoch: 8.53 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30580853305504746		[learning rate: 0.00090135]
	Learning Rate: 0.000901353
	LOSS [training: 0.30580853305504746 | validation: 0.23487817406035572]
	TIME [epoch: 8.53 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22418186844171908		[learning rate: 0.00089808]
	Learning Rate: 0.000898082
	LOSS [training: 0.22418186844171908 | validation: 0.15296875102121577]
	TIME [epoch: 8.52 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22219900267752415		[learning rate: 0.00089482]
	Learning Rate: 0.000894822
	LOSS [training: 0.22219900267752415 | validation: 0.15992283827960013]
	TIME [epoch: 8.54 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23457593605381377		[learning rate: 0.00089157]
	Learning Rate: 0.000891575
	LOSS [training: 0.23457593605381377 | validation: 0.1394333630282676]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_765.pth
	Model improved!!!
EPOCH 766/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2715781446863691		[learning rate: 0.00088834]
	Learning Rate: 0.000888339
	LOSS [training: 0.2715781446863691 | validation: 0.31709672994166127]
	TIME [epoch: 8.53 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21672876651207415		[learning rate: 0.00088512]
	Learning Rate: 0.000885116
	LOSS [training: 0.21672876651207415 | validation: 0.352297370210801]
	TIME [epoch: 8.52 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2853879672343294		[learning rate: 0.0008819]
	Learning Rate: 0.000881903
	LOSS [training: 0.2853879672343294 | validation: 0.24977506969950303]
	TIME [epoch: 8.55 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19808090966781086		[learning rate: 0.0008787]
	Learning Rate: 0.000878703
	LOSS [training: 0.19808090966781086 | validation: 0.16706879131254812]
	TIME [epoch: 8.53 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21158879089271904		[learning rate: 0.00087551]
	Learning Rate: 0.000875514
	LOSS [training: 0.21158879089271904 | validation: 0.15813050104089937]
	TIME [epoch: 8.52 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30299991812097593		[learning rate: 0.00087234]
	Learning Rate: 0.000872337
	LOSS [training: 0.30299991812097593 | validation: 0.2863871452186745]
	TIME [epoch: 8.51 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23466810491585247		[learning rate: 0.00086917]
	Learning Rate: 0.000869171
	LOSS [training: 0.23466810491585247 | validation: 0.21339904898332598]
	TIME [epoch: 8.54 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2507432019609939		[learning rate: 0.00086602]
	Learning Rate: 0.000866017
	LOSS [training: 0.2507432019609939 | validation: 0.2872723683860774]
	TIME [epoch: 8.54 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23838098591588705		[learning rate: 0.00086287]
	Learning Rate: 0.000862874
	LOSS [training: 0.23838098591588705 | validation: 0.36025993150818003]
	TIME [epoch: 8.52 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24033478711301556		[learning rate: 0.00085974]
	Learning Rate: 0.000859743
	LOSS [training: 0.24033478711301556 | validation: 0.37616298842109575]
	TIME [epoch: 8.52 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21752246944392542		[learning rate: 0.00085662]
	Learning Rate: 0.000856623
	LOSS [training: 0.21752246944392542 | validation: 0.18110679393482815]
	TIME [epoch: 8.52 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22680395596861294		[learning rate: 0.00085351]
	Learning Rate: 0.000853514
	LOSS [training: 0.22680395596861294 | validation: 0.1782647352687637]
	TIME [epoch: 8.56 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2302679048277838		[learning rate: 0.00085042]
	Learning Rate: 0.000850416
	LOSS [training: 0.2302679048277838 | validation: 0.275632163896561]
	TIME [epoch: 8.53 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24338561055176688		[learning rate: 0.00084733]
	Learning Rate: 0.00084733
	LOSS [training: 0.24338561055176688 | validation: 0.17677212328417585]
	TIME [epoch: 8.52 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2584632478572809		[learning rate: 0.00084426]
	Learning Rate: 0.000844255
	LOSS [training: 0.2584632478572809 | validation: 0.2331007876873823]
	TIME [epoch: 8.53 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22565054946146149		[learning rate: 0.00084119]
	Learning Rate: 0.000841191
	LOSS [training: 0.22565054946146149 | validation: 0.18712550214520318]
	TIME [epoch: 8.55 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3125633621354716		[learning rate: 0.00083814]
	Learning Rate: 0.000838139
	LOSS [training: 0.3125633621354716 | validation: 0.1851303005847118]
	TIME [epoch: 8.53 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1975584103410396		[learning rate: 0.0008351]
	Learning Rate: 0.000835097
	LOSS [training: 0.1975584103410396 | validation: 0.261788303833032]
	TIME [epoch: 8.52 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2093633699326977		[learning rate: 0.00083207]
	Learning Rate: 0.000832066
	LOSS [training: 0.2093633699326977 | validation: 0.16461461439631703]
	TIME [epoch: 8.53 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2023702032700029		[learning rate: 0.00082905]
	Learning Rate: 0.000829046
	LOSS [training: 0.2023702032700029 | validation: 0.21769982266584148]
	TIME [epoch: 8.55 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23646951212735293		[learning rate: 0.00082604]
	Learning Rate: 0.000826038
	LOSS [training: 0.23646951212735293 | validation: 0.24087939963799504]
	TIME [epoch: 8.53 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2064104791873025		[learning rate: 0.00082304]
	Learning Rate: 0.00082304
	LOSS [training: 0.2064104791873025 | validation: 0.20213455117850138]
	TIME [epoch: 8.53 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2368372165474216		[learning rate: 0.00082005]
	Learning Rate: 0.000820053
	LOSS [training: 0.2368372165474216 | validation: 0.26546594754881725]
	TIME [epoch: 8.52 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2221774898381086		[learning rate: 0.00081708]
	Learning Rate: 0.000817077
	LOSS [training: 0.2221774898381086 | validation: 0.17943916166802992]
	TIME [epoch: 8.54 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24194113669297393		[learning rate: 0.00081411]
	Learning Rate: 0.000814112
	LOSS [training: 0.24194113669297393 | validation: 0.25473430564249694]
	TIME [epoch: 8.53 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23882820261435636		[learning rate: 0.00081116]
	Learning Rate: 0.000811158
	LOSS [training: 0.23882820261435636 | validation: 0.26422745818573173]
	TIME [epoch: 8.52 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24712103836551544		[learning rate: 0.00080821]
	Learning Rate: 0.000808214
	LOSS [training: 0.24712103836551544 | validation: 0.17994528687817732]
	TIME [epoch: 8.52 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25556873964612736		[learning rate: 0.00080528]
	Learning Rate: 0.000805281
	LOSS [training: 0.25556873964612736 | validation: 0.23776847828000408]
	TIME [epoch: 8.53 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20307337212283097		[learning rate: 0.00080236]
	Learning Rate: 0.000802358
	LOSS [training: 0.20307337212283097 | validation: 0.2527679344757124]
	TIME [epoch: 8.55 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25854900997089936		[learning rate: 0.00079945]
	Learning Rate: 0.000799447
	LOSS [training: 0.25854900997089936 | validation: 0.40219265904881496]
	TIME [epoch: 8.52 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22160234948575797		[learning rate: 0.00079655]
	Learning Rate: 0.000796545
	LOSS [training: 0.22160234948575797 | validation: 0.1625079771959318]
	TIME [epoch: 8.53 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24275503175554455		[learning rate: 0.00079365]
	Learning Rate: 0.000793655
	LOSS [training: 0.24275503175554455 | validation: 0.18163728851731387]
	TIME [epoch: 8.52 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21678113279049988		[learning rate: 0.00079077]
	Learning Rate: 0.000790775
	LOSS [training: 0.21678113279049988 | validation: 0.3458582776030837]
	TIME [epoch: 8.55 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25735831968203843		[learning rate: 0.0007879]
	Learning Rate: 0.000787905
	LOSS [training: 0.25735831968203843 | validation: 0.16780228267828995]
	TIME [epoch: 8.53 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20863703132157463		[learning rate: 0.00078505]
	Learning Rate: 0.000785045
	LOSS [training: 0.20863703132157463 | validation: 0.14277166958181733]
	TIME [epoch: 8.53 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20324014821757236		[learning rate: 0.0007822]
	Learning Rate: 0.000782196
	LOSS [training: 0.20324014821757236 | validation: 0.18602239914697272]
	TIME [epoch: 8.53 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27866674813055353		[learning rate: 0.00077936]
	Learning Rate: 0.000779358
	LOSS [training: 0.27866674813055353 | validation: 0.1980226340896054]
	TIME [epoch: 8.55 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24512004324913494		[learning rate: 0.00077653]
	Learning Rate: 0.000776529
	LOSS [training: 0.24512004324913494 | validation: 0.25092695634157713]
	TIME [epoch: 8.54 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23141700085670253		[learning rate: 0.00077371]
	Learning Rate: 0.000773711
	LOSS [training: 0.23141700085670253 | validation: 0.27054168859349303]
	TIME [epoch: 8.53 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2662602608530827		[learning rate: 0.0007709]
	Learning Rate: 0.000770903
	LOSS [training: 0.2662602608530827 | validation: 0.3200652332031677]
	TIME [epoch: 8.53 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2804017215909715		[learning rate: 0.00076811]
	Learning Rate: 0.000768106
	LOSS [training: 0.2804017215909715 | validation: 0.28721546733486075]
	TIME [epoch: 8.54 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24690562833088464		[learning rate: 0.00076532]
	Learning Rate: 0.000765318
	LOSS [training: 0.24690562833088464 | validation: 0.2344033809193668]
	TIME [epoch: 8.53 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23249952064225304		[learning rate: 0.00076254]
	Learning Rate: 0.000762541
	LOSS [training: 0.23249952064225304 | validation: 0.2698613174261279]
	TIME [epoch: 8.53 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23409419690716318		[learning rate: 0.00075977]
	Learning Rate: 0.000759774
	LOSS [training: 0.23409419690716318 | validation: 0.18027504641345293]
	TIME [epoch: 8.53 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21274700164028132		[learning rate: 0.00075702]
	Learning Rate: 0.000757016
	LOSS [training: 0.21274700164028132 | validation: 0.15980841426821482]
	TIME [epoch: 8.54 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2145280684593284		[learning rate: 0.00075427]
	Learning Rate: 0.000754269
	LOSS [training: 0.2145280684593284 | validation: 0.27661766974847685]
	TIME [epoch: 8.54 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25051268942085153		[learning rate: 0.00075153]
	Learning Rate: 0.000751532
	LOSS [training: 0.25051268942085153 | validation: 0.19273679485078515]
	TIME [epoch: 8.53 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2496878519060748		[learning rate: 0.0007488]
	Learning Rate: 0.000748804
	LOSS [training: 0.2496878519060748 | validation: 0.26552487340865794]
	TIME [epoch: 8.53 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20781205086754378		[learning rate: 0.00074609]
	Learning Rate: 0.000746087
	LOSS [training: 0.20781205086754378 | validation: 0.22339127405547038]
	TIME [epoch: 8.53 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23721814815733328		[learning rate: 0.00074338]
	Learning Rate: 0.000743379
	LOSS [training: 0.23721814815733328 | validation: 0.16310434161465]
	TIME [epoch: 8.55 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1894375952064235		[learning rate: 0.00074068]
	Learning Rate: 0.000740682
	LOSS [training: 0.1894375952064235 | validation: 0.14351187709354485]
	TIME [epoch: 8.53 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19466130323093933		[learning rate: 0.00073799]
	Learning Rate: 0.000737994
	LOSS [training: 0.19466130323093933 | validation: 0.16082387532709566]
	TIME [epoch: 8.52 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2209268667113097		[learning rate: 0.00073532]
	Learning Rate: 0.000735315
	LOSS [training: 0.2209268667113097 | validation: 0.21824874559040014]
	TIME [epoch: 8.52 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21346841839752387		[learning rate: 0.00073265]
	Learning Rate: 0.000732647
	LOSS [training: 0.21346841839752387 | validation: 0.26059888728655856]
	TIME [epoch: 8.55 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24371962124990693		[learning rate: 0.00072999]
	Learning Rate: 0.000729988
	LOSS [training: 0.24371962124990693 | validation: 0.20104480605631653]
	TIME [epoch: 8.53 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21186399636470132		[learning rate: 0.00072734]
	Learning Rate: 0.000727339
	LOSS [training: 0.21186399636470132 | validation: 0.1643290023904815]
	TIME [epoch: 8.52 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21055935997277925		[learning rate: 0.0007247]
	Learning Rate: 0.000724699
	LOSS [training: 0.21055935997277925 | validation: 0.3059643970096411]
	TIME [epoch: 8.53 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21139121080329426		[learning rate: 0.00072207]
	Learning Rate: 0.000722069
	LOSS [training: 0.21139121080329426 | validation: 0.17105487776980485]
	TIME [epoch: 8.54 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22417892315941676		[learning rate: 0.00071945]
	Learning Rate: 0.000719449
	LOSS [training: 0.22417892315941676 | validation: 0.226086774051748]
	TIME [epoch: 8.53 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23792767240386672		[learning rate: 0.00071684]
	Learning Rate: 0.000716838
	LOSS [training: 0.23792767240386672 | validation: 0.17764157122195917]
	TIME [epoch: 8.53 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20910757257438775		[learning rate: 0.00071424]
	Learning Rate: 0.000714237
	LOSS [training: 0.20910757257438775 | validation: 0.13653692233798706]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_826.pth
	Model improved!!!
EPOCH 827/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21222363386710114		[learning rate: 0.00071164]
	Learning Rate: 0.000711645
	LOSS [training: 0.21222363386710114 | validation: 0.16826821835299338]
	TIME [epoch: 8.54 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17084634778536834		[learning rate: 0.00070906]
	Learning Rate: 0.000709062
	LOSS [training: 0.17084634778536834 | validation: 0.14578449414004774]
	TIME [epoch: 8.54 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1961493133234236		[learning rate: 0.00070649]
	Learning Rate: 0.000706489
	LOSS [training: 0.1961493133234236 | validation: 0.2106851659730389]
	TIME [epoch: 8.52 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22921162711615634		[learning rate: 0.00070392]
	Learning Rate: 0.000703925
	LOSS [training: 0.22921162711615634 | validation: 0.24539917883761658]
	TIME [epoch: 8.52 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21251758995282333		[learning rate: 0.00070137]
	Learning Rate: 0.00070137
	LOSS [training: 0.21251758995282333 | validation: 0.2305874491603046]
	TIME [epoch: 8.52 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22807378586595237		[learning rate: 0.00069883]
	Learning Rate: 0.000698825
	LOSS [training: 0.22807378586595237 | validation: 0.20949560693347308]
	TIME [epoch: 8.54 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22141654007970907		[learning rate: 0.00069629]
	Learning Rate: 0.000696289
	LOSS [training: 0.22141654007970907 | validation: 0.16044361593105605]
	TIME [epoch: 8.52 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21580209732985683		[learning rate: 0.00069376]
	Learning Rate: 0.000693762
	LOSS [training: 0.21580209732985683 | validation: 0.15437679077984146]
	TIME [epoch: 8.53 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17849468872629454		[learning rate: 0.00069124]
	Learning Rate: 0.000691244
	LOSS [training: 0.17849468872629454 | validation: 0.17217048081322317]
	TIME [epoch: 8.52 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21283792258823858		[learning rate: 0.00068874]
	Learning Rate: 0.000688736
	LOSS [training: 0.21283792258823858 | validation: 0.27044257389067505]
	TIME [epoch: 8.55 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19545432906107535		[learning rate: 0.00068624]
	Learning Rate: 0.000686236
	LOSS [training: 0.19545432906107535 | validation: 0.1563288931565999]
	TIME [epoch: 8.53 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20815192342834848		[learning rate: 0.00068375]
	Learning Rate: 0.000683746
	LOSS [training: 0.20815192342834848 | validation: 0.2537393117194361]
	TIME [epoch: 8.52 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22738727081145876		[learning rate: 0.00068126]
	Learning Rate: 0.000681265
	LOSS [training: 0.22738727081145876 | validation: 0.17090733207741993]
	TIME [epoch: 8.52 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21551076878628925		[learning rate: 0.00067879]
	Learning Rate: 0.000678792
	LOSS [training: 0.21551076878628925 | validation: 0.22491744170004885]
	TIME [epoch: 8.54 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1858595714641371		[learning rate: 0.00067633]
	Learning Rate: 0.000676329
	LOSS [training: 0.1858595714641371 | validation: 0.2518390338443246]
	TIME [epoch: 8.53 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21287284654225497		[learning rate: 0.00067387]
	Learning Rate: 0.000673874
	LOSS [training: 0.21287284654225497 | validation: 0.6178572718840086]
	TIME [epoch: 8.52 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27779106711727697		[learning rate: 0.00067143]
	Learning Rate: 0.000671429
	LOSS [training: 0.27779106711727697 | validation: 0.1881590002686931]
	TIME [epoch: 8.53 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18103977864691484		[learning rate: 0.00066899]
	Learning Rate: 0.000668992
	LOSS [training: 0.18103977864691484 | validation: 0.17303132475476798]
	TIME [epoch: 8.55 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24747891989336268		[learning rate: 0.00066656]
	Learning Rate: 0.000666564
	LOSS [training: 0.24747891989336268 | validation: 0.23379896136951303]
	TIME [epoch: 8.54 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19648303357881874		[learning rate: 0.00066415]
	Learning Rate: 0.000664145
	LOSS [training: 0.19648303357881874 | validation: 0.16132822579953063]
	TIME [epoch: 8.53 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25822104710399774		[learning rate: 0.00066174]
	Learning Rate: 0.000661735
	LOSS [training: 0.25822104710399774 | validation: 0.16855470487689186]
	TIME [epoch: 8.53 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1707360250819627		[learning rate: 0.00065933]
	Learning Rate: 0.000659334
	LOSS [training: 0.1707360250819627 | validation: 0.17057271082709619]
	TIME [epoch: 8.53 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17209570849504802		[learning rate: 0.00065694]
	Learning Rate: 0.000656941
	LOSS [training: 0.17209570849504802 | validation: 0.17481989455544356]
	TIME [epoch: 8.55 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2113305379014585		[learning rate: 0.00065456]
	Learning Rate: 0.000654557
	LOSS [training: 0.2113305379014585 | validation: 0.1449469871273843]
	TIME [epoch: 8.52 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2781666055533835		[learning rate: 0.00065218]
	Learning Rate: 0.000652182
	LOSS [training: 0.2781666055533835 | validation: 0.21631022703754604]
	TIME [epoch: 8.52 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25091150935853246		[learning rate: 0.00064981]
	Learning Rate: 0.000649815
	LOSS [training: 0.25091150935853246 | validation: 0.18593391548219226]
	TIME [epoch: 8.52 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19576646241126294		[learning rate: 0.00064746]
	Learning Rate: 0.000647456
	LOSS [training: 0.19576646241126294 | validation: 0.2068392107227378]
	TIME [epoch: 8.55 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21889511248448193		[learning rate: 0.00064511]
	Learning Rate: 0.000645107
	LOSS [training: 0.21889511248448193 | validation: 0.24112343980112516]
	TIME [epoch: 8.53 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23166824915739107		[learning rate: 0.00064277]
	Learning Rate: 0.000642766
	LOSS [training: 0.23166824915739107 | validation: 0.19731026716070116]
	TIME [epoch: 8.53 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24932010453078815		[learning rate: 0.00064043]
	Learning Rate: 0.000640433
	LOSS [training: 0.24932010453078815 | validation: 0.1679646587151829]
	TIME [epoch: 8.52 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2417262169220257		[learning rate: 0.00063811]
	Learning Rate: 0.000638109
	LOSS [training: 0.2417262169220257 | validation: 0.21466933756097012]
	TIME [epoch: 8.54 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17935359065308148		[learning rate: 0.00063579]
	Learning Rate: 0.000635793
	LOSS [training: 0.17935359065308148 | validation: 0.1608993135011216]
	TIME [epoch: 8.54 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1984387888680861		[learning rate: 0.00063349]
	Learning Rate: 0.000633486
	LOSS [training: 0.1984387888680861 | validation: 0.20646875107239626]
	TIME [epoch: 8.52 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18754772082328183		[learning rate: 0.00063119]
	Learning Rate: 0.000631187
	LOSS [training: 0.18754772082328183 | validation: 0.15087426120925046]
	TIME [epoch: 8.53 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22364812585455093		[learning rate: 0.0006289]
	Learning Rate: 0.000628896
	LOSS [training: 0.22364812585455093 | validation: 0.2620731732561503]
	TIME [epoch: 8.54 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2648225552503002		[learning rate: 0.00062661]
	Learning Rate: 0.000626614
	LOSS [training: 0.2648225552503002 | validation: 0.15607837152036536]
	TIME [epoch: 8.53 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21062387897447862		[learning rate: 0.00062434]
	Learning Rate: 0.00062434
	LOSS [training: 0.21062387897447862 | validation: 0.20298277143616372]
	TIME [epoch: 8.52 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21591399881689494		[learning rate: 0.00062207]
	Learning Rate: 0.000622074
	LOSS [training: 0.21591399881689494 | validation: 0.190429640115627]
	TIME [epoch: 8.53 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22202804716177943		[learning rate: 0.00061982]
	Learning Rate: 0.000619816
	LOSS [training: 0.22202804716177943 | validation: 0.16452861263103025]
	TIME [epoch: 8.53 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19990649626656334		[learning rate: 0.00061757]
	Learning Rate: 0.000617567
	LOSS [training: 0.19990649626656334 | validation: 0.14698130149048733]
	TIME [epoch: 8.54 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19103313122705914		[learning rate: 0.00061533]
	Learning Rate: 0.000615326
	LOSS [training: 0.19103313122705914 | validation: 0.13124301572338937]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_867.pth
	Model improved!!!
EPOCH 868/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1667422789556099		[learning rate: 0.00061309]
	Learning Rate: 0.000613093
	LOSS [training: 0.1667422789556099 | validation: 0.14267421943899425]
	TIME [epoch: 8.52 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1811450795779232		[learning rate: 0.00061087]
	Learning Rate: 0.000610868
	LOSS [training: 0.1811450795779232 | validation: 0.20314034102258152]
	TIME [epoch: 8.52 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18310057809408065		[learning rate: 0.00060865]
	Learning Rate: 0.000608651
	LOSS [training: 0.18310057809408065 | validation: 0.15119664147039608]
	TIME [epoch: 8.55 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2265961208015554		[learning rate: 0.00060644]
	Learning Rate: 0.000606442
	LOSS [training: 0.2265961208015554 | validation: 0.24160279573112378]
	TIME [epoch: 8.52 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2017033913046864		[learning rate: 0.00060424]
	Learning Rate: 0.000604242
	LOSS [training: 0.2017033913046864 | validation: 0.21759512200187853]
	TIME [epoch: 8.52 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1729688568606508		[learning rate: 0.00060205]
	Learning Rate: 0.000602049
	LOSS [training: 0.1729688568606508 | validation: 0.1283060059578013]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_873.pth
	Model improved!!!
EPOCH 874/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17163377599156565		[learning rate: 0.00059986]
	Learning Rate: 0.000599864
	LOSS [training: 0.17163377599156565 | validation: 0.16072471837053837]
	TIME [epoch: 8.55 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20755626272389538		[learning rate: 0.00059769]
	Learning Rate: 0.000597687
	LOSS [training: 0.20755626272389538 | validation: 0.17682003294961593]
	TIME [epoch: 8.52 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1826706219356259		[learning rate: 0.00059552]
	Learning Rate: 0.000595518
	LOSS [training: 0.1826706219356259 | validation: 0.22349650452334374]
	TIME [epoch: 8.53 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2267787794070805		[learning rate: 0.00059336]
	Learning Rate: 0.000593357
	LOSS [training: 0.2267787794070805 | validation: 0.19240353984450215]
	TIME [epoch: 8.52 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20723733465879107		[learning rate: 0.0005912]
	Learning Rate: 0.000591203
	LOSS [training: 0.20723733465879107 | validation: 0.16109859125386328]
	TIME [epoch: 8.55 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19775969415683006		[learning rate: 0.00058906]
	Learning Rate: 0.000589058
	LOSS [training: 0.19775969415683006 | validation: 0.2251972170620338]
	TIME [epoch: 8.52 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19680171439232738		[learning rate: 0.00058692]
	Learning Rate: 0.00058692
	LOSS [training: 0.19680171439232738 | validation: 0.22917713401067175]
	TIME [epoch: 8.52 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19995529059677714		[learning rate: 0.00058479]
	Learning Rate: 0.00058479
	LOSS [training: 0.19995529059677714 | validation: 0.3255893854498877]
	TIME [epoch: 8.52 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19716994505357904		[learning rate: 0.00058267]
	Learning Rate: 0.000582668
	LOSS [training: 0.19716994505357904 | validation: 0.15849041762499705]
	TIME [epoch: 8.54 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21775497870046007		[learning rate: 0.00058055]
	Learning Rate: 0.000580553
	LOSS [training: 0.21775497870046007 | validation: 0.18008868753757726]
	TIME [epoch: 8.53 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22507745874738969		[learning rate: 0.00057845]
	Learning Rate: 0.000578446
	LOSS [training: 0.22507745874738969 | validation: 0.1348053831392404]
	TIME [epoch: 8.53 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18850931991532419		[learning rate: 0.00057635]
	Learning Rate: 0.000576347
	LOSS [training: 0.18850931991532419 | validation: 0.1795198154143739]
	TIME [epoch: 8.52 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25870248367912246		[learning rate: 0.00057426]
	Learning Rate: 0.000574256
	LOSS [training: 0.25870248367912246 | validation: 0.15733892105793762]
	TIME [epoch: 8.53 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22976598276208987		[learning rate: 0.00057217]
	Learning Rate: 0.000572172
	LOSS [training: 0.22976598276208987 | validation: 0.17310695870262538]
	TIME [epoch: 8.54 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20519550968953784		[learning rate: 0.0005701]
	Learning Rate: 0.000570095
	LOSS [training: 0.20519550968953784 | validation: 0.13555158442591375]
	TIME [epoch: 8.52 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21264640438810964		[learning rate: 0.00056803]
	Learning Rate: 0.000568026
	LOSS [training: 0.21264640438810964 | validation: 0.14974758824869366]
	TIME [epoch: 8.52 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2408003871965489		[learning rate: 0.00056596]
	Learning Rate: 0.000565965
	LOSS [training: 0.2408003871965489 | validation: 0.1560500298213323]
	TIME [epoch: 8.52 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1887508039440992		[learning rate: 0.00056391]
	Learning Rate: 0.000563911
	LOSS [training: 0.1887508039440992 | validation: 0.12977403097965606]
	TIME [epoch: 8.55 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15331083770090098		[learning rate: 0.00056186]
	Learning Rate: 0.000561864
	LOSS [training: 0.15331083770090098 | validation: 0.16985751601615612]
	TIME [epoch: 8.52 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16620430437016695		[learning rate: 0.00055983]
	Learning Rate: 0.000559825
	LOSS [training: 0.16620430437016695 | validation: 0.16806111403677454]
	TIME [epoch: 8.52 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1814653159938165		[learning rate: 0.00055779]
	Learning Rate: 0.000557794
	LOSS [training: 0.1814653159938165 | validation: 0.23527795200747087]
	TIME [epoch: 8.52 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18798797064420422		[learning rate: 0.00055577]
	Learning Rate: 0.00055577
	LOSS [training: 0.18798797064420422 | validation: 0.16351023277652404]
	TIME [epoch: 8.55 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19067748948384627		[learning rate: 0.00055375]
	Learning Rate: 0.000553753
	LOSS [training: 0.19067748948384627 | validation: 0.1983568334859615]
	TIME [epoch: 8.52 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19105071088733486		[learning rate: 0.00055174]
	Learning Rate: 0.000551743
	LOSS [training: 0.19105071088733486 | validation: 0.194367993735166]
	TIME [epoch: 8.52 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1651540617445028		[learning rate: 0.00054974]
	Learning Rate: 0.000549741
	LOSS [training: 0.1651540617445028 | validation: 0.1405712870663919]
	TIME [epoch: 8.52 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18944840772846483		[learning rate: 0.00054775]
	Learning Rate: 0.000547746
	LOSS [training: 0.18944840772846483 | validation: 0.14148847825447047]
	TIME [epoch: 8.54 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16311984960245982		[learning rate: 0.00054576]
	Learning Rate: 0.000545758
	LOSS [training: 0.16311984960245982 | validation: 0.13413623663003124]
	TIME [epoch: 8.53 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16307160362427792		[learning rate: 0.00054378]
	Learning Rate: 0.000543777
	LOSS [training: 0.16307160362427792 | validation: 0.1623332680992019]
	TIME [epoch: 8.52 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15918193507073933		[learning rate: 0.0005418]
	Learning Rate: 0.000541804
	LOSS [training: 0.15918193507073933 | validation: 0.14833798259374414]
	TIME [epoch: 8.52 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21414002432304358		[learning rate: 0.00053984]
	Learning Rate: 0.000539838
	LOSS [training: 0.21414002432304358 | validation: 0.15413980246270054]
	TIME [epoch: 8.53 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20100072963319718		[learning rate: 0.00053788]
	Learning Rate: 0.000537879
	LOSS [training: 0.20100072963319718 | validation: 0.3237116840803005]
	TIME [epoch: 8.54 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31346579588184625		[learning rate: 0.00053593]
	Learning Rate: 0.000535926
	LOSS [training: 0.31346579588184625 | validation: 0.1350797709430455]
	TIME [epoch: 8.52 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15829423028456616		[learning rate: 0.00053398]
	Learning Rate: 0.000533982
	LOSS [training: 0.15829423028456616 | validation: 0.13992765081025993]
	TIME [epoch: 8.52 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15643100458503173		[learning rate: 0.00053204]
	Learning Rate: 0.000532044
	LOSS [training: 0.15643100458503173 | validation: 0.1823396732911256]
	TIME [epoch: 8.52 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17477872173121028		[learning rate: 0.00053011]
	Learning Rate: 0.000530113
	LOSS [training: 0.17477872173121028 | validation: 0.1432923894474029]
	TIME [epoch: 8.55 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17816754727786177		[learning rate: 0.00052819]
	Learning Rate: 0.000528189
	LOSS [training: 0.17816754727786177 | validation: 0.21642801433691433]
	TIME [epoch: 8.52 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17241470807625564		[learning rate: 0.00052627]
	Learning Rate: 0.000526272
	LOSS [training: 0.17241470807625564 | validation: 0.1244329090024208]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_910.pth
	Model improved!!!
EPOCH 911/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18173959971454964		[learning rate: 0.00052436]
	Learning Rate: 0.000524362
	LOSS [training: 0.18173959971454964 | validation: 0.15623099455946707]
	TIME [epoch: 8.52 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19547824776644745		[learning rate: 0.00052246]
	Learning Rate: 0.00052246
	LOSS [training: 0.19547824776644745 | validation: 0.13967022674617957]
	TIME [epoch: 8.54 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16607837438036166		[learning rate: 0.00052056]
	Learning Rate: 0.000520563
	LOSS [training: 0.16607837438036166 | validation: 0.15132338669177592]
	TIME [epoch: 8.52 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20775510657861035		[learning rate: 0.00051867]
	Learning Rate: 0.000518674
	LOSS [training: 0.20775510657861035 | validation: 0.2376960828384163]
	TIME [epoch: 8.52 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15925716272936857		[learning rate: 0.00051679]
	Learning Rate: 0.000516792
	LOSS [training: 0.15925716272936857 | validation: 0.13539036597617843]
	TIME [epoch: 8.51 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21218559242210358		[learning rate: 0.00051492]
	Learning Rate: 0.000514917
	LOSS [training: 0.21218559242210358 | validation: 0.24120428313340592]
	TIME [epoch: 8.54 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18839755775317624		[learning rate: 0.00051305]
	Learning Rate: 0.000513048
	LOSS [training: 0.18839755775317624 | validation: 0.22773427928316337]
	TIME [epoch: 8.53 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17785262601729374		[learning rate: 0.00051119]
	Learning Rate: 0.000511186
	LOSS [training: 0.17785262601729374 | validation: 0.22553892724743305]
	TIME [epoch: 8.52 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2459067238665364		[learning rate: 0.00050933]
	Learning Rate: 0.000509331
	LOSS [training: 0.2459067238665364 | validation: 0.18123889994719092]
	TIME [epoch: 8.52 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18737014421627607		[learning rate: 0.00050748]
	Learning Rate: 0.000507483
	LOSS [training: 0.18737014421627607 | validation: 0.1507383830966756]
	TIME [epoch: 8.53 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17769072849506262		[learning rate: 0.00050564]
	Learning Rate: 0.000505641
	LOSS [training: 0.17769072849506262 | validation: 0.197567497657626]
	TIME [epoch: 8.53 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19355485672890765		[learning rate: 0.00050381]
	Learning Rate: 0.000503806
	LOSS [training: 0.19355485672890765 | validation: 0.16994809646979664]
	TIME [epoch: 8.52 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.175767122272639		[learning rate: 0.00050198]
	Learning Rate: 0.000501977
	LOSS [training: 0.175767122272639 | validation: 0.18192322318918358]
	TIME [epoch: 8.52 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1707124766477452		[learning rate: 0.00050016]
	Learning Rate: 0.000500156
	LOSS [training: 0.1707124766477452 | validation: 0.14683503562440012]
	TIME [epoch: 8.51 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2026879245527343		[learning rate: 0.00049834]
	Learning Rate: 0.000498341
	LOSS [training: 0.2026879245527343 | validation: 0.1726543491145399]
	TIME [epoch: 8.55 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22887823486193698		[learning rate: 0.00049653]
	Learning Rate: 0.000496532
	LOSS [training: 0.22887823486193698 | validation: 0.20446388293829215]
	TIME [epoch: 8.52 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17794111458879644		[learning rate: 0.00049473]
	Learning Rate: 0.00049473
	LOSS [training: 0.17794111458879644 | validation: 0.22271397759938405]
	TIME [epoch: 8.52 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21090805001134805		[learning rate: 0.00049293]
	Learning Rate: 0.000492935
	LOSS [training: 0.21090805001134805 | validation: 0.1821105110327825]
	TIME [epoch: 8.51 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22940060061266315		[learning rate: 0.00049115]
	Learning Rate: 0.000491146
	LOSS [training: 0.22940060061266315 | validation: 0.17275679891859025]
	TIME [epoch: 8.54 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16617477234602185		[learning rate: 0.00048936]
	Learning Rate: 0.000489364
	LOSS [training: 0.16617477234602185 | validation: 0.16413121168945607]
	TIME [epoch: 8.51 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1673074051861518		[learning rate: 0.00048759]
	Learning Rate: 0.000487588
	LOSS [training: 0.1673074051861518 | validation: 0.1434882044878655]
	TIME [epoch: 8.52 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20581880149641013		[learning rate: 0.00048582]
	Learning Rate: 0.000485818
	LOSS [training: 0.20581880149641013 | validation: 0.26184780002731234]
	TIME [epoch: 8.51 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.21314245847459606		[learning rate: 0.00048406]
	Learning Rate: 0.000484055
	LOSS [training: 0.21314245847459606 | validation: 0.20539662210750548]
	TIME [epoch: 8.54 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17971773513369071		[learning rate: 0.0004823]
	Learning Rate: 0.000482298
	LOSS [training: 0.17971773513369071 | validation: 0.1738556428700884]
	TIME [epoch: 8.52 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1922981821213462		[learning rate: 0.00048055]
	Learning Rate: 0.000480548
	LOSS [training: 0.1922981821213462 | validation: 0.21234809504738061]
	TIME [epoch: 8.51 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17271608781871384		[learning rate: 0.0004788]
	Learning Rate: 0.000478804
	LOSS [training: 0.17271608781871384 | validation: 0.15903957551810527]
	TIME [epoch: 8.52 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15839389119413613		[learning rate: 0.00047707]
	Learning Rate: 0.000477067
	LOSS [training: 0.15839389119413613 | validation: 0.13163874339583725]
	TIME [epoch: 8.53 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18528643195072111		[learning rate: 0.00047534]
	Learning Rate: 0.000475335
	LOSS [training: 0.18528643195072111 | validation: 0.1718302861900423]
	TIME [epoch: 8.53 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16332854985408246		[learning rate: 0.00047361]
	Learning Rate: 0.00047361
	LOSS [training: 0.16332854985408246 | validation: 0.1385888528309308]
	TIME [epoch: 8.51 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15328743158996622		[learning rate: 0.00047189]
	Learning Rate: 0.000471891
	LOSS [training: 0.15328743158996622 | validation: 0.1636613528725891]
	TIME [epoch: 8.51 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.22357504356880603		[learning rate: 0.00047018]
	Learning Rate: 0.000470179
	LOSS [training: 0.22357504356880603 | validation: 0.14618916466158538]
	TIME [epoch: 8.52 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16107931679377058		[learning rate: 0.00046847]
	Learning Rate: 0.000468473
	LOSS [training: 0.16107931679377058 | validation: 0.22174912376089362]
	TIME [epoch: 8.53 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18158980040927117		[learning rate: 0.00046677]
	Learning Rate: 0.000466773
	LOSS [training: 0.18158980040927117 | validation: 0.14765005238255682]
	TIME [epoch: 8.51 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16270507226606756		[learning rate: 0.00046508]
	Learning Rate: 0.000465079
	LOSS [training: 0.16270507226606756 | validation: 0.13615735949713237]
	TIME [epoch: 8.52 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17819315877703482		[learning rate: 0.00046339]
	Learning Rate: 0.000463391
	LOSS [training: 0.17819315877703482 | validation: 0.14830069271742208]
	TIME [epoch: 8.51 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17906653287941596		[learning rate: 0.00046171]
	Learning Rate: 0.000461709
	LOSS [training: 0.17906653287941596 | validation: 0.33404458760808386]
	TIME [epoch: 8.54 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18386238610743239		[learning rate: 0.00046003]
	Learning Rate: 0.000460034
	LOSS [training: 0.18386238610743239 | validation: 0.17247493589565344]
	TIME [epoch: 8.52 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20164103963936597		[learning rate: 0.00045836]
	Learning Rate: 0.000458364
	LOSS [training: 0.20164103963936597 | validation: 0.16184200673019333]
	TIME [epoch: 8.51 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1542299805372676		[learning rate: 0.0004567]
	Learning Rate: 0.000456701
	LOSS [training: 0.1542299805372676 | validation: 0.1491582722152363]
	TIME [epoch: 8.52 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17454548255329014		[learning rate: 0.00045504]
	Learning Rate: 0.000455043
	LOSS [training: 0.17454548255329014 | validation: 0.13927940504750347]
	TIME [epoch: 8.54 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17456755724070105		[learning rate: 0.00045339]
	Learning Rate: 0.000453392
	LOSS [training: 0.17456755724070105 | validation: 0.1538982773874053]
	TIME [epoch: 8.52 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.196073263850662		[learning rate: 0.00045175]
	Learning Rate: 0.000451746
	LOSS [training: 0.196073263850662 | validation: 0.14912676572691813]
	TIME [epoch: 8.51 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20440171804255286		[learning rate: 0.00045011]
	Learning Rate: 0.000450107
	LOSS [training: 0.20440171804255286 | validation: 0.2602267826581586]
	TIME [epoch: 8.52 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17792902660159918		[learning rate: 0.00044847]
	Learning Rate: 0.000448474
	LOSS [training: 0.17792902660159918 | validation: 0.20163524282265533]
	TIME [epoch: 8.52 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2170377734271253		[learning rate: 0.00044685]
	Learning Rate: 0.000446846
	LOSS [training: 0.2170377734271253 | validation: 0.13198671061021644]
	TIME [epoch: 8.52 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1592040226032595		[learning rate: 0.00044522]
	Learning Rate: 0.000445224
	LOSS [training: 0.1592040226032595 | validation: 0.14579740659445511]
	TIME [epoch: 8.51 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1629191543443443		[learning rate: 0.00044361]
	Learning Rate: 0.000443609
	LOSS [training: 0.1629191543443443 | validation: 0.29309877160044273]
	TIME [epoch: 8.51 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20442585116228623		[learning rate: 0.000442]
	Learning Rate: 0.000441999
	LOSS [training: 0.20442585116228623 | validation: 0.15705868595604133]
	TIME [epoch: 8.52 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1551693686297197		[learning rate: 0.00044039]
	Learning Rate: 0.000440395
	LOSS [training: 0.1551693686297197 | validation: 0.14923256295967796]
	TIME [epoch: 8.53 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1908505405693855		[learning rate: 0.0004388]
	Learning Rate: 0.000438797
	LOSS [training: 0.1908505405693855 | validation: 0.14000758516828177]
	TIME [epoch: 8.52 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2764858158396312		[learning rate: 0.0004372]
	Learning Rate: 0.000437204
	LOSS [training: 0.2764858158396312 | validation: 0.14026614860417336]
	TIME [epoch: 8.51 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15671574903817692		[learning rate: 0.00043562]
	Learning Rate: 0.000435617
	LOSS [training: 0.15671574903817692 | validation: 0.13958122903444814]
	TIME [epoch: 8.52 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14908066524373448		[learning rate: 0.00043404]
	Learning Rate: 0.000434037
	LOSS [training: 0.14908066524373448 | validation: 0.1538179717124578]
	TIME [epoch: 8.54 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15615608193971076		[learning rate: 0.00043246]
	Learning Rate: 0.000432461
	LOSS [training: 0.15615608193971076 | validation: 0.1280528350105121]
	TIME [epoch: 8.51 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15413659438157662		[learning rate: 0.00043089]
	Learning Rate: 0.000430892
	LOSS [training: 0.15413659438157662 | validation: 0.14922081741761606]
	TIME [epoch: 8.51 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1469187734836532		[learning rate: 0.00042933]
	Learning Rate: 0.000429328
	LOSS [training: 0.1469187734836532 | validation: 0.1532962308165296]
	TIME [epoch: 8.51 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1735202933577909		[learning rate: 0.00042777]
	Learning Rate: 0.00042777
	LOSS [training: 0.1735202933577909 | validation: 0.12678031755241673]
	TIME [epoch: 8.54 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16566388042385483		[learning rate: 0.00042622]
	Learning Rate: 0.000426218
	LOSS [training: 0.16566388042385483 | validation: 0.1436101539975272]
	TIME [epoch: 8.52 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16968494971838505		[learning rate: 0.00042467]
	Learning Rate: 0.000424671
	LOSS [training: 0.16968494971838505 | validation: 0.13846475918704712]
	TIME [epoch: 8.51 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14735181356459542		[learning rate: 0.00042313]
	Learning Rate: 0.00042313
	LOSS [training: 0.14735181356459542 | validation: 0.14175901535863095]
	TIME [epoch: 8.51 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15172006708657304		[learning rate: 0.00042159]
	Learning Rate: 0.000421594
	LOSS [training: 0.15172006708657304 | validation: 0.13134481118844613]
	TIME [epoch: 8.54 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1630514018550008		[learning rate: 0.00042006]
	Learning Rate: 0.000420064
	LOSS [training: 0.1630514018550008 | validation: 0.1353726014583484]
	TIME [epoch: 8.52 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16889607798009795		[learning rate: 0.00041854]
	Learning Rate: 0.00041854
	LOSS [training: 0.16889607798009795 | validation: 0.18197632549366854]
	TIME [epoch: 8.51 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17959089411211862		[learning rate: 0.00041702]
	Learning Rate: 0.000417021
	LOSS [training: 0.17959089411211862 | validation: 0.15904886121076894]
	TIME [epoch: 8.51 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15243079165010134		[learning rate: 0.00041551]
	Learning Rate: 0.000415508
	LOSS [training: 0.15243079165010134 | validation: 0.2023535308082161]
	TIME [epoch: 8.53 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16471813783101913		[learning rate: 0.000414]
	Learning Rate: 0.000414
	LOSS [training: 0.16471813783101913 | validation: 0.20983684280038273]
	TIME [epoch: 8.53 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1813824643602157		[learning rate: 0.0004125]
	Learning Rate: 0.000412497
	LOSS [training: 0.1813824643602157 | validation: 0.13542975602750654]
	TIME [epoch: 8.52 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1922424187127395		[learning rate: 0.000411]
	Learning Rate: 0.000411
	LOSS [training: 0.1922424187127395 | validation: 0.16093815888987914]
	TIME [epoch: 8.52 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16456605176818356		[learning rate: 0.00040951]
	Learning Rate: 0.000409509
	LOSS [training: 0.16456605176818356 | validation: 0.19386457618387665]
	TIME [epoch: 8.51 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17455012201822523		[learning rate: 0.00040802]
	Learning Rate: 0.000408023
	LOSS [training: 0.17455012201822523 | validation: 0.17371558382019725]
	TIME [epoch: 8.54 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17431430955940339		[learning rate: 0.00040654]
	Learning Rate: 0.000406542
	LOSS [training: 0.17431430955940339 | validation: 0.2263900679672989]
	TIME [epoch: 8.52 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1800215780163869		[learning rate: 0.00040507]
	Learning Rate: 0.000405067
	LOSS [training: 0.1800215780163869 | validation: 0.13256962018341922]
	TIME [epoch: 8.51 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16089541998256104		[learning rate: 0.0004036]
	Learning Rate: 0.000403597
	LOSS [training: 0.16089541998256104 | validation: 0.1363756700108705]
	TIME [epoch: 8.52 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16993682306021995		[learning rate: 0.00040213]
	Learning Rate: 0.000402132
	LOSS [training: 0.16993682306021995 | validation: 0.14935660290807656]
	TIME [epoch: 8.53 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1638885133832742		[learning rate: 0.00040067]
	Learning Rate: 0.000400672
	LOSS [training: 0.1638885133832742 | validation: 0.14981425335783397]
	TIME [epoch: 8.52 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15152102318558297		[learning rate: 0.00039922]
	Learning Rate: 0.000399218
	LOSS [training: 0.15152102318558297 | validation: 0.14081421210139652]
	TIME [epoch: 8.51 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14890359191038982		[learning rate: 0.00039777]
	Learning Rate: 0.00039777
	LOSS [training: 0.14890359191038982 | validation: 0.11222982552789582]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_987.pth
	Model improved!!!
EPOCH 988/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15808639254623574		[learning rate: 0.00039633]
	Learning Rate: 0.000396326
	LOSS [training: 0.15808639254623574 | validation: 0.1485663133771875]
	TIME [epoch: 8.54 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1737444773685358		[learning rate: 0.00039489]
	Learning Rate: 0.000394888
	LOSS [training: 0.1737444773685358 | validation: 0.148471323351312]
	TIME [epoch: 8.52 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17163686225490932		[learning rate: 0.00039345]
	Learning Rate: 0.000393455
	LOSS [training: 0.17163686225490932 | validation: 0.20193466877865665]
	TIME [epoch: 8.52 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15472060197107018		[learning rate: 0.00039203]
	Learning Rate: 0.000392027
	LOSS [training: 0.15472060197107018 | validation: 0.1312157857035244]
	TIME [epoch: 8.52 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14637755455625437		[learning rate: 0.0003906]
	Learning Rate: 0.000390604
	LOSS [training: 0.14637755455625437 | validation: 0.177354972191627]
	TIME [epoch: 8.53 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1694065792942507		[learning rate: 0.00038919]
	Learning Rate: 0.000389187
	LOSS [training: 0.1694065792942507 | validation: 0.12487379671395371]
	TIME [epoch: 8.53 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15484693620555562		[learning rate: 0.00038777]
	Learning Rate: 0.000387774
	LOSS [training: 0.15484693620555562 | validation: 0.1561293982507735]
	TIME [epoch: 8.52 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16493765733486804		[learning rate: 0.00038637]
	Learning Rate: 0.000386367
	LOSS [training: 0.16493765733486804 | validation: 0.12502142283850565]
	TIME [epoch: 8.52 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14199387783409206		[learning rate: 0.00038496]
	Learning Rate: 0.000384965
	LOSS [training: 0.14199387783409206 | validation: 0.14567634257593773]
	TIME [epoch: 8.52 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17121697748737397		[learning rate: 0.00038357]
	Learning Rate: 0.000383568
	LOSS [training: 0.17121697748737397 | validation: 0.12068133172619042]
	TIME [epoch: 8.53 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18051947476553792		[learning rate: 0.00038218]
	Learning Rate: 0.000382176
	LOSS [training: 0.18051947476553792 | validation: 0.15247942589222346]
	TIME [epoch: 8.51 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16386443933794712		[learning rate: 0.00038079]
	Learning Rate: 0.000380789
	LOSS [training: 0.16386443933794712 | validation: 0.2474946835914228]
	TIME [epoch: 8.52 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15668956941367154		[learning rate: 0.00037941]
	Learning Rate: 0.000379407
	LOSS [training: 0.15668956941367154 | validation: 0.14556615253366606]
	TIME [epoch: 8.52 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16739017496613778		[learning rate: 0.00037803]
	Learning Rate: 0.00037803
	LOSS [training: 0.16739017496613778 | validation: 0.18178022496446034]
	TIME [epoch: 8.53 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18762292794189364		[learning rate: 0.00037666]
	Learning Rate: 0.000376658
	LOSS [training: 0.18762292794189364 | validation: 0.14203082723872867]
	TIME [epoch: 8.52 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1480352736342458		[learning rate: 0.00037529]
	Learning Rate: 0.000375291
	LOSS [training: 0.1480352736342458 | validation: 0.12886129414668346]
	TIME [epoch: 8.51 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14338443855706956		[learning rate: 0.00037393]
	Learning Rate: 0.000373929
	LOSS [training: 0.14338443855706956 | validation: 0.12359080848412543]
	TIME [epoch: 8.52 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1621536850338327		[learning rate: 0.00037257]
	Learning Rate: 0.000372572
	LOSS [training: 0.1621536850338327 | validation: 0.14753197561508394]
	TIME [epoch: 8.54 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15192822140212742		[learning rate: 0.00037122]
	Learning Rate: 0.00037122
	LOSS [training: 0.15192822140212742 | validation: 0.15261756634452187]
	TIME [epoch: 8.52 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1779785808999244		[learning rate: 0.00036987]
	Learning Rate: 0.000369873
	LOSS [training: 0.1779785808999244 | validation: 0.172125032862458]
	TIME [epoch: 8.52 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15634857742528288		[learning rate: 0.00036853]
	Learning Rate: 0.000368531
	LOSS [training: 0.15634857742528288 | validation: 0.13600042853580535]
	TIME [epoch: 8.51 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15681445179405573		[learning rate: 0.00036719]
	Learning Rate: 0.000367193
	LOSS [training: 0.15681445179405573 | validation: 0.23518476806546765]
	TIME [epoch: 8.53 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1672842879944489		[learning rate: 0.00036586]
	Learning Rate: 0.000365861
	LOSS [training: 0.1672842879944489 | validation: 0.12505374843365707]
	TIME [epoch: 8.53 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14718496872075318		[learning rate: 0.00036453]
	Learning Rate: 0.000364533
	LOSS [training: 0.14718496872075318 | validation: 0.133917733206871]
	TIME [epoch: 8.51 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18197943089746457		[learning rate: 0.00036321]
	Learning Rate: 0.00036321
	LOSS [training: 0.18197943089746457 | validation: 0.16130646882658556]
	TIME [epoch: 8.51 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15441658563991273		[learning rate: 0.00036189]
	Learning Rate: 0.000361892
	LOSS [training: 0.15441658563991273 | validation: 0.1507122075832226]
	TIME [epoch: 8.52 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19056089525293662		[learning rate: 0.00036058]
	Learning Rate: 0.000360579
	LOSS [training: 0.19056089525293662 | validation: 0.15980212580754108]
	TIME [epoch: 8.53 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16063570057967977		[learning rate: 0.00035927]
	Learning Rate: 0.00035927
	LOSS [training: 0.16063570057967977 | validation: 0.15363262894574506]
	TIME [epoch: 8.51 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17578811447412995		[learning rate: 0.00035797]
	Learning Rate: 0.000357966
	LOSS [training: 0.17578811447412995 | validation: 0.11534364863122049]
	TIME [epoch: 8.51 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1609870490857961		[learning rate: 0.00035667]
	Learning Rate: 0.000356667
	LOSS [training: 0.1609870490857961 | validation: 0.1674612291946792]
	TIME [epoch: 8.51 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14207657921988776		[learning rate: 0.00035537]
	Learning Rate: 0.000355373
	LOSS [training: 0.14207657921988776 | validation: 0.1532205935326562]
	TIME [epoch: 8.54 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15405915332319678		[learning rate: 0.00035408]
	Learning Rate: 0.000354083
	LOSS [training: 0.15405915332319678 | validation: 0.14245097008096572]
	TIME [epoch: 8.51 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17556002042273833		[learning rate: 0.0003528]
	Learning Rate: 0.000352798
	LOSS [training: 0.17556002042273833 | validation: 0.3317545205396666]
	TIME [epoch: 8.51 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20167921085383625		[learning rate: 0.00035152]
	Learning Rate: 0.000351518
	LOSS [training: 0.20167921085383625 | validation: 0.17287028717990244]
	TIME [epoch: 8.51 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.20407969224710007		[learning rate: 0.00035024]
	Learning Rate: 0.000350242
	LOSS [training: 0.20407969224710007 | validation: 0.2769616723944234]
	TIME [epoch: 8.53 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18646157328549373		[learning rate: 0.00034897]
	Learning Rate: 0.000348971
	LOSS [training: 0.18646157328549373 | validation: 0.1545831746620534]
	TIME [epoch: 8.52 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17479694208066343		[learning rate: 0.0003477]
	Learning Rate: 0.000347705
	LOSS [training: 0.17479694208066343 | validation: 0.16658122519403273]
	TIME [epoch: 8.51 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16897389185839132		[learning rate: 0.00034644]
	Learning Rate: 0.000346443
	LOSS [training: 0.16897389185839132 | validation: 0.13780558246684052]
	TIME [epoch: 8.51 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16475585601983903		[learning rate: 0.00034519]
	Learning Rate: 0.000345186
	LOSS [training: 0.16475585601983903 | validation: 0.15611820625040618]
	TIME [epoch: 8.53 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17070110730529203		[learning rate: 0.00034393]
	Learning Rate: 0.000343933
	LOSS [training: 0.17070110730529203 | validation: 0.12636880992251426]
	TIME [epoch: 8.52 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1569409598636152		[learning rate: 0.00034268]
	Learning Rate: 0.000342685
	LOSS [training: 0.1569409598636152 | validation: 0.13678553686402783]
	TIME [epoch: 8.51 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15281946395951446		[learning rate: 0.00034144]
	Learning Rate: 0.000341441
	LOSS [training: 0.15281946395951446 | validation: 0.1499222775708034]
	TIME [epoch: 8.51 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1744417159124553		[learning rate: 0.0003402]
	Learning Rate: 0.000340202
	LOSS [training: 0.1744417159124553 | validation: 0.12987914261051506]
	TIME [epoch: 8.53 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14987049144192383		[learning rate: 0.00033897]
	Learning Rate: 0.000338967
	LOSS [training: 0.14987049144192383 | validation: 0.11618677833124982]
	TIME [epoch: 8.52 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15516098717187504		[learning rate: 0.00033774]
	Learning Rate: 0.000337737
	LOSS [training: 0.15516098717187504 | validation: 0.12938182089949182]
	TIME [epoch: 8.51 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1414313113619508		[learning rate: 0.00033651]
	Learning Rate: 0.000336512
	LOSS [training: 0.1414313113619508 | validation: 0.1466706212105877]
	TIME [epoch: 8.51 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17456945388174644		[learning rate: 0.00033529]
	Learning Rate: 0.00033529
	LOSS [training: 0.17456945388174644 | validation: 0.1800133077880699]
	TIME [epoch: 8.52 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16184684246363537		[learning rate: 0.00033407]
	Learning Rate: 0.000334074
	LOSS [training: 0.16184684246363537 | validation: 0.15270682516185988]
	TIME [epoch: 8.53 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1740554443761328		[learning rate: 0.00033286]
	Learning Rate: 0.000332861
	LOSS [training: 0.1740554443761328 | validation: 0.15622590010178858]
	TIME [epoch: 8.51 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1665556414165556		[learning rate: 0.00033165]
	Learning Rate: 0.000331653
	LOSS [training: 0.1665556414165556 | validation: 0.13276929894693212]
	TIME [epoch: 8.51 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14625080553529274		[learning rate: 0.00033045]
	Learning Rate: 0.00033045
	LOSS [training: 0.14625080553529274 | validation: 0.1668438204530226]
	TIME [epoch: 8.51 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1713659505906908		[learning rate: 0.00032925]
	Learning Rate: 0.00032925
	LOSS [training: 0.1713659505906908 | validation: 0.15187367021703096]
	TIME [epoch: 8.53 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15872819749252404		[learning rate: 0.00032806]
	Learning Rate: 0.000328056
	LOSS [training: 0.15872819749252404 | validation: 0.1437161315522298]
	TIME [epoch: 8.52 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1592573790109579		[learning rate: 0.00032686]
	Learning Rate: 0.000326865
	LOSS [training: 0.1592573790109579 | validation: 0.2078398838626914]
	TIME [epoch: 8.51 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19352960742612996		[learning rate: 0.00032568]
	Learning Rate: 0.000325679
	LOSS [training: 0.19352960742612996 | validation: 0.1447396868810342]
	TIME [epoch: 8.52 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14708418913542093		[learning rate: 0.0003245]
	Learning Rate: 0.000324497
	LOSS [training: 0.14708418913542093 | validation: 0.14834571551091041]
	TIME [epoch: 8.53 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14880951832588413		[learning rate: 0.00032332]
	Learning Rate: 0.000323319
	LOSS [training: 0.14880951832588413 | validation: 0.11929246033356]
	TIME [epoch: 8.52 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14019247073781105		[learning rate: 0.00032215]
	Learning Rate: 0.000322146
	LOSS [training: 0.14019247073781105 | validation: 0.13016026330111297]
	TIME [epoch: 8.51 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13968234552508735		[learning rate: 0.00032098]
	Learning Rate: 0.000320977
	LOSS [training: 0.13968234552508735 | validation: 0.1617363220091197]
	TIME [epoch: 8.51 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1894385354495121		[learning rate: 0.00031981]
	Learning Rate: 0.000319812
	LOSS [training: 0.1894385354495121 | validation: 0.17381369341352593]
	TIME [epoch: 8.53 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14789681007177607		[learning rate: 0.00031865]
	Learning Rate: 0.000318651
	LOSS [training: 0.14789681007177607 | validation: 0.1627650363728832]
	TIME [epoch: 8.53 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1422468600643542		[learning rate: 0.0003175]
	Learning Rate: 0.000317495
	LOSS [training: 0.1422468600643542 | validation: 0.11672004091058338]
	TIME [epoch: 8.51 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17021631722180514		[learning rate: 0.00031634]
	Learning Rate: 0.000316343
	LOSS [training: 0.17021631722180514 | validation: 0.14256773549909912]
	TIME [epoch: 8.51 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14151803804519397		[learning rate: 0.00031519]
	Learning Rate: 0.000315195
	LOSS [training: 0.14151803804519397 | validation: 0.11816142119607298]
	TIME [epoch: 8.52 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1426617111940198		[learning rate: 0.00031405]
	Learning Rate: 0.000314051
	LOSS [training: 0.1426617111940198 | validation: 0.1301827893551921]
	TIME [epoch: 8.54 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1530878713923043		[learning rate: 0.00031291]
	Learning Rate: 0.000312911
	LOSS [training: 0.1530878713923043 | validation: 0.1387664211458619]
	TIME [epoch: 8.52 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14493138976367587		[learning rate: 0.00031178]
	Learning Rate: 0.000311776
	LOSS [training: 0.14493138976367587 | validation: 0.15765901348084732]
	TIME [epoch: 8.52 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1444007105352832		[learning rate: 0.00031064]
	Learning Rate: 0.000310644
	LOSS [training: 0.1444007105352832 | validation: 0.1745318326772874]
	TIME [epoch: 8.53 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14543279902157089		[learning rate: 0.00030952]
	Learning Rate: 0.000309517
	LOSS [training: 0.14543279902157089 | validation: 0.1555241137656904]
	TIME [epoch: 8.54 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15928502850976983		[learning rate: 0.00030839]
	Learning Rate: 0.000308394
	LOSS [training: 0.15928502850976983 | validation: 0.1299484633537917]
	TIME [epoch: 8.52 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1722506692759304		[learning rate: 0.00030727]
	Learning Rate: 0.000307274
	LOSS [training: 0.1722506692759304 | validation: 0.21832048601180504]
	TIME [epoch: 8.52 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1668285220537284		[learning rate: 0.00030616]
	Learning Rate: 0.000306159
	LOSS [training: 0.1668285220537284 | validation: 0.17047119734524413]
	TIME [epoch: 8.52 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1535119802833033		[learning rate: 0.00030505]
	Learning Rate: 0.000305048
	LOSS [training: 0.1535119802833033 | validation: 0.17565298948287184]
	TIME [epoch: 8.54 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1743385187714918		[learning rate: 0.00030394]
	Learning Rate: 0.000303941
	LOSS [training: 0.1743385187714918 | validation: 0.14624978453253473]
	TIME [epoch: 8.52 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16384526387046122		[learning rate: 0.00030284]
	Learning Rate: 0.000302838
	LOSS [training: 0.16384526387046122 | validation: 0.14529833645017637]
	TIME [epoch: 8.51 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16386902208132575		[learning rate: 0.00030174]
	Learning Rate: 0.000301739
	LOSS [training: 0.16386902208132575 | validation: 0.143178470240129]
	TIME [epoch: 8.52 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13672793113153886		[learning rate: 0.00030064]
	Learning Rate: 0.000300644
	LOSS [training: 0.13672793113153886 | validation: 0.12964793735826935]
	TIME [epoch: 8.53 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15673656834494412		[learning rate: 0.00029955]
	Learning Rate: 0.000299553
	LOSS [training: 0.15673656834494412 | validation: 0.12116999942986913]
	TIME [epoch: 8.53 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16164053346112767		[learning rate: 0.00029847]
	Learning Rate: 0.000298466
	LOSS [training: 0.16164053346112767 | validation: 0.11234307657070296]
	TIME [epoch: 8.53 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14035371438217667		[learning rate: 0.00029738]
	Learning Rate: 0.000297383
	LOSS [training: 0.14035371438217667 | validation: 0.1377220651317344]
	TIME [epoch: 8.52 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.145618384135418		[learning rate: 0.0002963]
	Learning Rate: 0.000296304
	LOSS [training: 0.145618384135418 | validation: 0.1327970059191708]
	TIME [epoch: 8.53 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14427122637367545		[learning rate: 0.00029523]
	Learning Rate: 0.000295228
	LOSS [training: 0.14427122637367545 | validation: 0.11268723924094101]
	TIME [epoch: 8.54 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14412862034561846		[learning rate: 0.00029416]
	Learning Rate: 0.000294157
	LOSS [training: 0.14412862034561846 | validation: 0.17305841676803183]
	TIME [epoch: 8.52 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1870107201628814		[learning rate: 0.00029309]
	Learning Rate: 0.000293089
	LOSS [training: 0.1870107201628814 | validation: 0.15229536441320998]
	TIME [epoch: 8.52 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1560718777749334		[learning rate: 0.00029203]
	Learning Rate: 0.000292026
	LOSS [training: 0.1560718777749334 | validation: 0.12726969515107772]
	TIME [epoch: 8.51 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1458457874500303		[learning rate: 0.00029097]
	Learning Rate: 0.000290966
	LOSS [training: 0.1458457874500303 | validation: 0.18899978199116]
	TIME [epoch: 8.54 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1600452375205344		[learning rate: 0.00028991]
	Learning Rate: 0.00028991
	LOSS [training: 0.1600452375205344 | validation: 0.19169243482406031]
	TIME [epoch: 8.52 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.19486556076038436		[learning rate: 0.00028886]
	Learning Rate: 0.000288858
	LOSS [training: 0.19486556076038436 | validation: 0.1931377857329563]
	TIME [epoch: 8.52 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15581117556086493		[learning rate: 0.00028781]
	Learning Rate: 0.00028781
	LOSS [training: 0.15581117556086493 | validation: 0.1492236163470384]
	TIME [epoch: 8.52 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14704567692982556		[learning rate: 0.00028677]
	Learning Rate: 0.000286765
	LOSS [training: 0.14704567692982556 | validation: 0.1688843209348461]
	TIME [epoch: 8.55 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15893858170543163		[learning rate: 0.00028572]
	Learning Rate: 0.000285724
	LOSS [training: 0.15893858170543163 | validation: 0.15226218693233537]
	TIME [epoch: 8.53 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1426962846373387		[learning rate: 0.00028469]
	Learning Rate: 0.000284688
	LOSS [training: 0.1426962846373387 | validation: 0.14229555125253626]
	TIME [epoch: 8.52 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13973941633246423		[learning rate: 0.00028365]
	Learning Rate: 0.000283654
	LOSS [training: 0.13973941633246423 | validation: 0.12993071998840464]
	TIME [epoch: 8.51 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16743707199017885		[learning rate: 0.00028263]
	Learning Rate: 0.000282625
	LOSS [training: 0.16743707199017885 | validation: 0.18802090845956576]
	TIME [epoch: 8.54 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15385529065447068		[learning rate: 0.0002816]
	Learning Rate: 0.000281599
	LOSS [training: 0.15385529065447068 | validation: 0.13088626718484841]
	TIME [epoch: 8.53 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1603155909435866		[learning rate: 0.00028058]
	Learning Rate: 0.000280577
	LOSS [training: 0.1603155909435866 | validation: 0.13955969745053348]
	TIME [epoch: 8.54 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1546658690155658		[learning rate: 0.00027956]
	Learning Rate: 0.000279559
	LOSS [training: 0.1546658690155658 | validation: 0.12338367166503242]
	TIME [epoch: 8.52 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14590339009465456		[learning rate: 0.00027854]
	Learning Rate: 0.000278545
	LOSS [training: 0.14590339009465456 | validation: 0.1349202250850965]
	TIME [epoch: 8.53 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1343417096635654		[learning rate: 0.00027753]
	Learning Rate: 0.000277534
	LOSS [training: 0.1343417096635654 | validation: 0.12187075932267041]
	TIME [epoch: 8.55 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1636274809693166		[learning rate: 0.00027653]
	Learning Rate: 0.000276527
	LOSS [training: 0.1636274809693166 | validation: 0.12471478235516917]
	TIME [epoch: 8.52 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1551507675935289		[learning rate: 0.00027552]
	Learning Rate: 0.000275523
	LOSS [training: 0.1551507675935289 | validation: 0.1444875155421163]
	TIME [epoch: 8.52 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1596649173870675		[learning rate: 0.00027452]
	Learning Rate: 0.000274523
	LOSS [training: 0.1596649173870675 | validation: 0.1630627580304242]
	TIME [epoch: 8.52 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15402588818549962		[learning rate: 0.00027353]
	Learning Rate: 0.000273527
	LOSS [training: 0.15402588818549962 | validation: 0.1376475750971515]
	TIME [epoch: 8.55 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16530917882290913		[learning rate: 0.00027253]
	Learning Rate: 0.000272534
	LOSS [training: 0.16530917882290913 | validation: 0.13149692902384108]
	TIME [epoch: 8.53 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14224481129213964		[learning rate: 0.00027155]
	Learning Rate: 0.000271545
	LOSS [training: 0.14224481129213964 | validation: 0.15323940561695126]
	TIME [epoch: 8.52 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15530865854385778		[learning rate: 0.00027056]
	Learning Rate: 0.00027056
	LOSS [training: 0.15530865854385778 | validation: 0.12455397964810469]
	TIME [epoch: 8.52 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1379784755361112		[learning rate: 0.00026958]
	Learning Rate: 0.000269578
	LOSS [training: 0.1379784755361112 | validation: 0.13510776410808556]
	TIME [epoch: 8.54 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14988450719277008		[learning rate: 0.0002686]
	Learning Rate: 0.0002686
	LOSS [training: 0.14988450719277008 | validation: 0.1320172529216686]
	TIME [epoch: 8.52 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16646030832574688		[learning rate: 0.00026762]
	Learning Rate: 0.000267625
	LOSS [training: 0.16646030832574688 | validation: 0.12029162890270317]
	TIME [epoch: 8.52 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13669060974949274		[learning rate: 0.00026665]
	Learning Rate: 0.000266654
	LOSS [training: 0.13669060974949274 | validation: 0.12127430997074645]
	TIME [epoch: 8.52 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14252770508581722		[learning rate: 0.00026569]
	Learning Rate: 0.000265686
	LOSS [training: 0.14252770508581722 | validation: 0.1305806384547462]
	TIME [epoch: 8.55 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16012146998811036		[learning rate: 0.00026472]
	Learning Rate: 0.000264722
	LOSS [training: 0.16012146998811036 | validation: 0.12162277151665904]
	TIME [epoch: 8.53 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.176166498232403		[learning rate: 0.00026376]
	Learning Rate: 0.000263761
	LOSS [training: 0.176166498232403 | validation: 0.14390791845820117]
	TIME [epoch: 8.52 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16730615760209683		[learning rate: 0.0002628]
	Learning Rate: 0.000262804
	LOSS [training: 0.16730615760209683 | validation: 0.12028176514050795]
	TIME [epoch: 8.51 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13008849147758644		[learning rate: 0.00026185]
	Learning Rate: 0.00026185
	LOSS [training: 0.13008849147758644 | validation: 0.12314312526299101]
	TIME [epoch: 8.53 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1301375433703022		[learning rate: 0.0002609]
	Learning Rate: 0.0002609
	LOSS [training: 0.1301375433703022 | validation: 0.13100370973369596]
	TIME [epoch: 8.53 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16250724266675348		[learning rate: 0.00025995]
	Learning Rate: 0.000259953
	LOSS [training: 0.16250724266675348 | validation: 0.18082079711156634]
	TIME [epoch: 8.52 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15131447549301486		[learning rate: 0.00025901]
	Learning Rate: 0.00025901
	LOSS [training: 0.15131447549301486 | validation: 0.168609167488523]
	TIME [epoch: 8.52 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15611983038974292		[learning rate: 0.00025807]
	Learning Rate: 0.00025807
	LOSS [training: 0.15611983038974292 | validation: 0.18059052241932136]
	TIME [epoch: 8.52 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17775009220954507		[learning rate: 0.00025713]
	Learning Rate: 0.000257133
	LOSS [training: 0.17775009220954507 | validation: 0.13788807799883357]
	TIME [epoch: 8.54 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1557930647762334		[learning rate: 0.0002562]
	Learning Rate: 0.0002562
	LOSS [training: 0.1557930647762334 | validation: 0.15626737895725135]
	TIME [epoch: 8.52 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16612514708771037		[learning rate: 0.00025527]
	Learning Rate: 0.00025527
	LOSS [training: 0.16612514708771037 | validation: 0.11140888251727873]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_1109.pth
	Model improved!!!
EPOCH 1110/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1297220806559432		[learning rate: 0.00025434]
	Learning Rate: 0.000254344
	LOSS [training: 0.1297220806559432 | validation: 0.14630326400292618]
	TIME [epoch: 8.53 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14506098050368993		[learning rate: 0.00025342]
	Learning Rate: 0.000253421
	LOSS [training: 0.14506098050368993 | validation: 0.11579743268038944]
	TIME [epoch: 8.54 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.135102514682588		[learning rate: 0.0002525]
	Learning Rate: 0.000252501
	LOSS [training: 0.135102514682588 | validation: 0.13473999505404174]
	TIME [epoch: 8.52 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1294536202754198		[learning rate: 0.00025158]
	Learning Rate: 0.000251585
	LOSS [training: 0.1294536202754198 | validation: 0.11774328471148648]
	TIME [epoch: 8.52 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13731151672717964		[learning rate: 0.00025067]
	Learning Rate: 0.000250672
	LOSS [training: 0.13731151672717964 | validation: 0.121927729761515]
	TIME [epoch: 8.51 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1486314143303022		[learning rate: 0.00024976]
	Learning Rate: 0.000249762
	LOSS [training: 0.1486314143303022 | validation: 0.20113552429457093]
	TIME [epoch: 8.55 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1641291037596442		[learning rate: 0.00024886]
	Learning Rate: 0.000248856
	LOSS [training: 0.1641291037596442 | validation: 0.19190608692849176]
	TIME [epoch: 8.53 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16792464751576905		[learning rate: 0.00024795]
	Learning Rate: 0.000247952
	LOSS [training: 0.16792464751576905 | validation: 0.15237481085004698]
	TIME [epoch: 8.52 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17219201723043343		[learning rate: 0.00024705]
	Learning Rate: 0.000247053
	LOSS [training: 0.17219201723043343 | validation: 0.140735294734061]
	TIME [epoch: 8.52 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13556218496803052		[learning rate: 0.00024616]
	Learning Rate: 0.000246156
	LOSS [training: 0.13556218496803052 | validation: 0.1295671326543419]
	TIME [epoch: 8.53 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13846085014297332		[learning rate: 0.00024526]
	Learning Rate: 0.000245263
	LOSS [training: 0.13846085014297332 | validation: 0.11808117844031428]
	TIME [epoch: 8.53 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1317031124687812		[learning rate: 0.00024437]
	Learning Rate: 0.000244373
	LOSS [training: 0.1317031124687812 | validation: 0.13145591581325294]
	TIME [epoch: 8.53 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1472402869531945		[learning rate: 0.00024349]
	Learning Rate: 0.000243486
	LOSS [training: 0.1472402869531945 | validation: 0.11721271171364439]
	TIME [epoch: 8.52 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14127356786542009		[learning rate: 0.0002426]
	Learning Rate: 0.000242602
	LOSS [training: 0.14127356786542009 | validation: 0.12151506163794537]
	TIME [epoch: 8.53 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14665299342671428		[learning rate: 0.00024172]
	Learning Rate: 0.000241722
	LOSS [training: 0.14665299342671428 | validation: 0.13976259649199838]
	TIME [epoch: 8.53 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1530465143213088		[learning rate: 0.00024084]
	Learning Rate: 0.000240845
	LOSS [training: 0.1530465143213088 | validation: 0.15012917913004314]
	TIME [epoch: 8.52 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1491757617407922		[learning rate: 0.00023997]
	Learning Rate: 0.000239971
	LOSS [training: 0.1491757617407922 | validation: 0.11861501653169607]
	TIME [epoch: 8.52 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1349793515074975		[learning rate: 0.0002391]
	Learning Rate: 0.0002391
	LOSS [training: 0.1349793515074975 | validation: 0.18286564998409988]
	TIME [epoch: 8.52 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1600086706115675		[learning rate: 0.00023823]
	Learning Rate: 0.000238232
	LOSS [training: 0.1600086706115675 | validation: 0.1164198434233267]
	TIME [epoch: 8.54 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1373135001615453		[learning rate: 0.00023737]
	Learning Rate: 0.000237367
	LOSS [training: 0.1373135001615453 | validation: 0.13296829871934807]
	TIME [epoch: 8.52 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12939458263377115		[learning rate: 0.00023651]
	Learning Rate: 0.000236506
	LOSS [training: 0.12939458263377115 | validation: 0.12240598574840138]
	TIME [epoch: 8.51 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15421887842053345		[learning rate: 0.00023565]
	Learning Rate: 0.000235648
	LOSS [training: 0.15421887842053345 | validation: 0.15970200043244268]
	TIME [epoch: 8.52 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14590235052107686		[learning rate: 0.00023479]
	Learning Rate: 0.000234793
	LOSS [training: 0.14590235052107686 | validation: 0.13349693022979198]
	TIME [epoch: 8.53 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1536061056809831		[learning rate: 0.00023394]
	Learning Rate: 0.00023394
	LOSS [training: 0.1536061056809831 | validation: 0.12458147064899024]
	TIME [epoch: 8.53 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14127608258294572		[learning rate: 0.00023309]
	Learning Rate: 0.000233091
	LOSS [training: 0.14127608258294572 | validation: 0.1453391776884656]
	TIME [epoch: 8.52 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14017561899791509		[learning rate: 0.00023225]
	Learning Rate: 0.000232246
	LOSS [training: 0.14017561899791509 | validation: 0.11821480011639854]
	TIME [epoch: 8.53 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17909717168291425		[learning rate: 0.0002314]
	Learning Rate: 0.000231403
	LOSS [training: 0.17909717168291425 | validation: 0.22117352321286382]
	TIME [epoch: 8.53 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15253403801871976		[learning rate: 0.00023056]
	Learning Rate: 0.000230563
	LOSS [training: 0.15253403801871976 | validation: 0.11675654891227041]
	TIME [epoch: 8.53 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1334362247887216		[learning rate: 0.00022973]
	Learning Rate: 0.000229726
	LOSS [training: 0.1334362247887216 | validation: 0.12943329934853498]
	TIME [epoch: 8.52 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1442239651150072		[learning rate: 0.00022889]
	Learning Rate: 0.000228893
	LOSS [training: 0.1442239651150072 | validation: 0.12389819374146682]
	TIME [epoch: 8.52 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.125714742041657		[learning rate: 0.00022806]
	Learning Rate: 0.000228062
	LOSS [training: 0.125714742041657 | validation: 0.12500536829437026]
	TIME [epoch: 8.52 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15406540149451975		[learning rate: 0.00022723]
	Learning Rate: 0.000227234
	LOSS [training: 0.15406540149451975 | validation: 0.11389807093595568]
	TIME [epoch: 8.54 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13731370714317753		[learning rate: 0.00022641]
	Learning Rate: 0.00022641
	LOSS [training: 0.13731370714317753 | validation: 0.14055687198579186]
	TIME [epoch: 8.52 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14274117628756675		[learning rate: 0.00022559]
	Learning Rate: 0.000225588
	LOSS [training: 0.14274117628756675 | validation: 0.12588602150593894]
	TIME [epoch: 8.52 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1438081195283067		[learning rate: 0.00022477]
	Learning Rate: 0.000224769
	LOSS [training: 0.1438081195283067 | validation: 0.11050342746707312]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_1144.pth
	Model improved!!!
EPOCH 1145/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13845487122010064		[learning rate: 0.00022395]
	Learning Rate: 0.000223954
	LOSS [training: 0.13845487122010064 | validation: 0.17211460446941448]
	TIME [epoch: 8.56 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16067505272812893		[learning rate: 0.00022314]
	Learning Rate: 0.000223141
	LOSS [training: 0.16067505272812893 | validation: 0.14749196470082832]
	TIME [epoch: 8.52 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1386208039538041		[learning rate: 0.00022233]
	Learning Rate: 0.000222331
	LOSS [training: 0.1386208039538041 | validation: 0.1116963543434922]
	TIME [epoch: 8.52 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14548247514923043		[learning rate: 0.00022152]
	Learning Rate: 0.000221524
	LOSS [training: 0.14548247514923043 | validation: 0.11684678425547321]
	TIME [epoch: 8.52 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13578930851360457		[learning rate: 0.00022072]
	Learning Rate: 0.00022072
	LOSS [training: 0.13578930851360457 | validation: 0.13923150960749603]
	TIME [epoch: 8.55 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13141707636083017		[learning rate: 0.00021992]
	Learning Rate: 0.000219919
	LOSS [training: 0.13141707636083017 | validation: 0.11907305218014282]
	TIME [epoch: 8.52 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13260586613460407		[learning rate: 0.00021912]
	Learning Rate: 0.000219121
	LOSS [training: 0.13260586613460407 | validation: 0.1222244198131342]
	TIME [epoch: 8.53 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.131252269730681		[learning rate: 0.00021833]
	Learning Rate: 0.000218326
	LOSS [training: 0.131252269730681 | validation: 0.1040191331540832]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_1152.pth
	Model improved!!!
EPOCH 1153/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12347522879889587		[learning rate: 0.00021753]
	Learning Rate: 0.000217534
	LOSS [training: 0.12347522879889587 | validation: 0.10575056495390346]
	TIME [epoch: 8.54 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1259823885565053		[learning rate: 0.00021674]
	Learning Rate: 0.000216744
	LOSS [training: 0.1259823885565053 | validation: 0.15033641223514904]
	TIME [epoch: 8.52 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.17251780551372306		[learning rate: 0.00021596]
	Learning Rate: 0.000215958
	LOSS [training: 0.17251780551372306 | validation: 0.12437920434649916]
	TIME [epoch: 8.52 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12924406852223422		[learning rate: 0.00021517]
	Learning Rate: 0.000215174
	LOSS [training: 0.12924406852223422 | validation: 0.12105155493225464]
	TIME [epoch: 8.52 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1313759644210033		[learning rate: 0.00021439]
	Learning Rate: 0.000214393
	LOSS [training: 0.1313759644210033 | validation: 0.12148202672194133]
	TIME [epoch: 8.53 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12750244009453354		[learning rate: 0.00021361]
	Learning Rate: 0.000213615
	LOSS [training: 0.12750244009453354 | validation: 0.14085315473560583]
	TIME [epoch: 8.53 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13897172203139443		[learning rate: 0.00021284]
	Learning Rate: 0.00021284
	LOSS [training: 0.13897172203139443 | validation: 0.1028681428233654]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_1159.pth
	Model improved!!!
EPOCH 1160/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12451316044549028		[learning rate: 0.00021207]
	Learning Rate: 0.000212067
	LOSS [training: 0.12451316044549028 | validation: 0.11521497023630745]
	TIME [epoch: 8.52 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13572770331626788		[learning rate: 0.0002113]
	Learning Rate: 0.000211298
	LOSS [training: 0.13572770331626788 | validation: 0.11963133471516804]
	TIME [epoch: 8.52 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13533807207593132		[learning rate: 0.00021053]
	Learning Rate: 0.000210531
	LOSS [training: 0.13533807207593132 | validation: 0.11825487974671156]
	TIME [epoch: 8.52 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12420665303482367		[learning rate: 0.00020977]
	Learning Rate: 0.000209767
	LOSS [training: 0.12420665303482367 | validation: 0.13241306901397765]
	TIME [epoch: 8.5 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.131226016274194		[learning rate: 0.00020901]
	Learning Rate: 0.000209006
	LOSS [training: 0.131226016274194 | validation: 0.14251704735174203]
	TIME [epoch: 8.51 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1365344383679639		[learning rate: 0.00020825]
	Learning Rate: 0.000208247
	LOSS [training: 0.1365344383679639 | validation: 0.1401593297122095]
	TIME [epoch: 8.5 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14939205869595512		[learning rate: 0.00020749]
	Learning Rate: 0.000207491
	LOSS [training: 0.14939205869595512 | validation: 0.18770761389218105]
	TIME [epoch: 8.53 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14278011784947045		[learning rate: 0.00020674]
	Learning Rate: 0.000206738
	LOSS [training: 0.14278011784947045 | validation: 0.12385529724068925]
	TIME [epoch: 8.51 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1432953728047389		[learning rate: 0.00020599]
	Learning Rate: 0.000205988
	LOSS [training: 0.1432953728047389 | validation: 0.11680642973553146]
	TIME [epoch: 8.52 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14599354150714555		[learning rate: 0.00020524]
	Learning Rate: 0.000205241
	LOSS [training: 0.14599354150714555 | validation: 0.12177630653801558]
	TIME [epoch: 8.51 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12922330227849269		[learning rate: 0.0002045]
	Learning Rate: 0.000204496
	LOSS [training: 0.12922330227849269 | validation: 0.1302812559094646]
	TIME [epoch: 8.53 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12621262025580654		[learning rate: 0.00020375]
	Learning Rate: 0.000203754
	LOSS [training: 0.12621262025580654 | validation: 0.10940361708469853]
	TIME [epoch: 8.52 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13177554569609784		[learning rate: 0.00020301]
	Learning Rate: 0.000203014
	LOSS [training: 0.13177554569609784 | validation: 0.12980534762411827]
	TIME [epoch: 8.52 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14385243828409125		[learning rate: 0.00020228]
	Learning Rate: 0.000202277
	LOSS [training: 0.14385243828409125 | validation: 0.19176134805494446]
	TIME [epoch: 8.51 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1456683511429026		[learning rate: 0.00020154]
	Learning Rate: 0.000201543
	LOSS [training: 0.1456683511429026 | validation: 0.12813971320879883]
	TIME [epoch: 8.53 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14347778309568687		[learning rate: 0.00020081]
	Learning Rate: 0.000200812
	LOSS [training: 0.14347778309568687 | validation: 0.1609760239942617]
	TIME [epoch: 8.5 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13370991624650608		[learning rate: 0.00020008]
	Learning Rate: 0.000200083
	LOSS [training: 0.13370991624650608 | validation: 0.12668438462803483]
	TIME [epoch: 8.51 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15811290294297065		[learning rate: 0.00019936]
	Learning Rate: 0.000199357
	LOSS [training: 0.15811290294297065 | validation: 0.15934718825542005]
	TIME [epoch: 8.5 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13213318044489328		[learning rate: 0.00019863]
	Learning Rate: 0.000198634
	LOSS [training: 0.13213318044489328 | validation: 0.12320745575006646]
	TIME [epoch: 8.53 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13119112214620937		[learning rate: 0.00019791]
	Learning Rate: 0.000197913
	LOSS [training: 0.13119112214620937 | validation: 0.13366495268434525]
	TIME [epoch: 8.52 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14074978171679792		[learning rate: 0.00019719]
	Learning Rate: 0.000197194
	LOSS [training: 0.14074978171679792 | validation: 0.1480350781950578]
	TIME [epoch: 8.51 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12889163927423883		[learning rate: 0.00019648]
	Learning Rate: 0.000196479
	LOSS [training: 0.12889163927423883 | validation: 0.12107033669811135]
	TIME [epoch: 8.51 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1300282656646646		[learning rate: 0.00019577]
	Learning Rate: 0.000195766
	LOSS [training: 0.1300282656646646 | validation: 0.14923993104306016]
	TIME [epoch: 8.52 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12537156771512067		[learning rate: 0.00019506]
	Learning Rate: 0.000195055
	LOSS [training: 0.12537156771512067 | validation: 0.11686975535393404]
	TIME [epoch: 8.54 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13690367751998744		[learning rate: 0.00019435]
	Learning Rate: 0.000194347
	LOSS [training: 0.13690367751998744 | validation: 0.12680008261042028]
	TIME [epoch: 8.51 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1314355344506673		[learning rate: 0.00019364]
	Learning Rate: 0.000193642
	LOSS [training: 0.1314355344506673 | validation: 0.12498757700954052]
	TIME [epoch: 8.51 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16810455337799227		[learning rate: 0.00019294]
	Learning Rate: 0.000192939
	LOSS [training: 0.16810455337799227 | validation: 0.11394367542865852]
	TIME [epoch: 8.52 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12275028273845649		[learning rate: 0.00019224]
	Learning Rate: 0.000192239
	LOSS [training: 0.12275028273845649 | validation: 0.11933731859293431]
	TIME [epoch: 8.54 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13435597654579812		[learning rate: 0.00019154]
	Learning Rate: 0.000191542
	LOSS [training: 0.13435597654579812 | validation: 0.16870222600067764]
	TIME [epoch: 8.51 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1338501357394979		[learning rate: 0.00019085]
	Learning Rate: 0.000190846
	LOSS [training: 0.1338501357394979 | validation: 0.12961898392766974]
	TIME [epoch: 8.51 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13373141603785235		[learning rate: 0.00019015]
	Learning Rate: 0.000190154
	LOSS [training: 0.13373141603785235 | validation: 0.10590340606532422]
	TIME [epoch: 8.51 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1165778267952585		[learning rate: 0.00018946]
	Learning Rate: 0.000189464
	LOSS [training: 0.1165778267952585 | validation: 0.12130255810974598]
	TIME [epoch: 8.53 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12717407657800922		[learning rate: 0.00018878]
	Learning Rate: 0.000188776
	LOSS [training: 0.12717407657800922 | validation: 0.12468995958837822]
	TIME [epoch: 8.51 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13845155094832967		[learning rate: 0.00018809]
	Learning Rate: 0.000188091
	LOSS [training: 0.13845155094832967 | validation: 0.1567200293208031]
	TIME [epoch: 8.52 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1307253676872386		[learning rate: 0.00018741]
	Learning Rate: 0.000187409
	LOSS [training: 0.1307253676872386 | validation: 0.12658517318019158]
	TIME [epoch: 8.52 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1402395356751283		[learning rate: 0.00018673]
	Learning Rate: 0.000186729
	LOSS [training: 0.1402395356751283 | validation: 0.15108817941878286]
	TIME [epoch: 8.52 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12714345527985088		[learning rate: 0.00018605]
	Learning Rate: 0.000186051
	LOSS [training: 0.12714345527985088 | validation: 0.12433101885768016]
	TIME [epoch: 8.53 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1327977967745812		[learning rate: 0.00018538]
	Learning Rate: 0.000185376
	LOSS [training: 0.1327977967745812 | validation: 0.13338378596856715]
	TIME [epoch: 8.51 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1265853322071083		[learning rate: 0.0001847]
	Learning Rate: 0.000184703
	LOSS [training: 0.1265853322071083 | validation: 0.12197876017805126]
	TIME [epoch: 8.51 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12286238221260885		[learning rate: 0.00018403]
	Learning Rate: 0.000184033
	LOSS [training: 0.12286238221260885 | validation: 0.11171073249343517]
	TIME [epoch: 8.51 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12204489461409603		[learning rate: 0.00018336]
	Learning Rate: 0.000183365
	LOSS [training: 0.12204489461409603 | validation: 0.11239005279720127]
	TIME [epoch: 8.53 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12601561458746446		[learning rate: 0.0001827]
	Learning Rate: 0.000182699
	LOSS [training: 0.12601561458746446 | validation: 0.1357625994450105]
	TIME [epoch: 8.5 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1424591568402767		[learning rate: 0.00018204]
	Learning Rate: 0.000182036
	LOSS [training: 0.1424591568402767 | validation: 0.10738659557828922]
	TIME [epoch: 8.51 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12485671537141396		[learning rate: 0.00018138]
	Learning Rate: 0.000181376
	LOSS [training: 0.12485671537141396 | validation: 0.1182286696458876]
	TIME [epoch: 8.52 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13214027252840982		[learning rate: 0.00018072]
	Learning Rate: 0.000180717
	LOSS [training: 0.13214027252840982 | validation: 0.17731640514467575]
	TIME [epoch: 8.54 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16278390564657336		[learning rate: 0.00018006]
	Learning Rate: 0.000180062
	LOSS [training: 0.16278390564657336 | validation: 0.19206764299967416]
	TIME [epoch: 8.51 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15217095895403626		[learning rate: 0.00017941]
	Learning Rate: 0.000179408
	LOSS [training: 0.15217095895403626 | validation: 0.14050253943981708]
	TIME [epoch: 8.52 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12459653197204704		[learning rate: 0.00017876]
	Learning Rate: 0.000178757
	LOSS [training: 0.12459653197204704 | validation: 0.11618160106589434]
	TIME [epoch: 8.52 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1402548957783473		[learning rate: 0.00017811]
	Learning Rate: 0.000178108
	LOSS [training: 0.1402548957783473 | validation: 0.1216171787075052]
	TIME [epoch: 8.54 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1185006208439268		[learning rate: 0.00017746]
	Learning Rate: 0.000177462
	LOSS [training: 0.1185006208439268 | validation: 0.10848169100421834]
	TIME [epoch: 8.51 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11734218417709807		[learning rate: 0.00017682]
	Learning Rate: 0.000176818
	LOSS [training: 0.11734218417709807 | validation: 0.12658486443927824]
	TIME [epoch: 8.52 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1273344690264937		[learning rate: 0.00017618]
	Learning Rate: 0.000176176
	LOSS [training: 0.1273344690264937 | validation: 0.11484708592892107]
	TIME [epoch: 8.51 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14613285087633546		[learning rate: 0.00017554]
	Learning Rate: 0.000175537
	LOSS [training: 0.14613285087633546 | validation: 0.1270750137063212]
	TIME [epoch: 8.53 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12804236460393664		[learning rate: 0.0001749]
	Learning Rate: 0.0001749
	LOSS [training: 0.12804236460393664 | validation: 0.1786103456396533]
	TIME [epoch: 8.52 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14496594087518808		[learning rate: 0.00017427]
	Learning Rate: 0.000174265
	LOSS [training: 0.14496594087518808 | validation: 0.10826480070504388]
	TIME [epoch: 8.52 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12373455623969183		[learning rate: 0.00017363]
	Learning Rate: 0.000173633
	LOSS [training: 0.12373455623969183 | validation: 0.12182308144153481]
	TIME [epoch: 8.51 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1309033637558142		[learning rate: 0.000173]
	Learning Rate: 0.000173003
	LOSS [training: 0.1309033637558142 | validation: 0.1194035762173552]
	TIME [epoch: 8.52 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12945061467826388		[learning rate: 0.00017237]
	Learning Rate: 0.000172375
	LOSS [training: 0.12945061467826388 | validation: 0.13306531078446643]
	TIME [epoch: 8.54 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13994762351402273		[learning rate: 0.00017175]
	Learning Rate: 0.000171749
	LOSS [training: 0.13994762351402273 | validation: 0.1230359726061564]
	TIME [epoch: 8.51 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13899605852341138		[learning rate: 0.00017113]
	Learning Rate: 0.000171126
	LOSS [training: 0.13899605852341138 | validation: 0.13411451047500114]
	TIME [epoch: 8.52 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.139235663573586		[learning rate: 0.0001705]
	Learning Rate: 0.000170505
	LOSS [training: 0.139235663573586 | validation: 0.11061796888568796]
	TIME [epoch: 8.52 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12756156529777848		[learning rate: 0.00016989]
	Learning Rate: 0.000169886
	LOSS [training: 0.12756156529777848 | validation: 0.1481691949159144]
	TIME [epoch: 8.54 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13374620876157056		[learning rate: 0.00016927]
	Learning Rate: 0.00016927
	LOSS [training: 0.13374620876157056 | validation: 0.1395958918498503]
	TIME [epoch: 8.51 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1332094675975694		[learning rate: 0.00016866]
	Learning Rate: 0.000168655
	LOSS [training: 0.1332094675975694 | validation: 0.11539047702269803]
	TIME [epoch: 8.5 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15265234055091015		[learning rate: 0.00016804]
	Learning Rate: 0.000168043
	LOSS [training: 0.15265234055091015 | validation: 0.11678975640116486]
	TIME [epoch: 8.51 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12671117299522697		[learning rate: 0.00016743]
	Learning Rate: 0.000167433
	LOSS [training: 0.12671117299522697 | validation: 0.1304242021727693]
	TIME [epoch: 8.53 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15094852205805998		[learning rate: 0.00016683]
	Learning Rate: 0.000166826
	LOSS [training: 0.15094852205805998 | validation: 0.1492179479689617]
	TIME [epoch: 8.5 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12672936810183427		[learning rate: 0.00016622]
	Learning Rate: 0.00016622
	LOSS [training: 0.12672936810183427 | validation: 0.12836451892460912]
	TIME [epoch: 8.52 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15659656582028997		[learning rate: 0.00016562]
	Learning Rate: 0.000165617
	LOSS [training: 0.15659656582028997 | validation: 0.1561308154128158]
	TIME [epoch: 8.52 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13563678592737669		[learning rate: 0.00016502]
	Learning Rate: 0.000165016
	LOSS [training: 0.13563678592737669 | validation: 0.13226340850553514]
	TIME [epoch: 8.53 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13564051393853982		[learning rate: 0.00016442]
	Learning Rate: 0.000164417
	LOSS [training: 0.13564051393853982 | validation: 0.12081804313358227]
	TIME [epoch: 8.52 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12638421477837647		[learning rate: 0.00016382]
	Learning Rate: 0.000163821
	LOSS [training: 0.12638421477837647 | validation: 0.11937977315458667]
	TIME [epoch: 8.5 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1392726510306733		[learning rate: 0.00016323]
	Learning Rate: 0.000163226
	LOSS [training: 0.1392726510306733 | validation: 0.11659360342586131]
	TIME [epoch: 8.52 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13181578627119123		[learning rate: 0.00016263]
	Learning Rate: 0.000162634
	LOSS [training: 0.13181578627119123 | validation: 0.10160699679121368]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_1233.pth
	Model improved!!!
EPOCH 1234/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13603203851868587		[learning rate: 0.00016204]
	Learning Rate: 0.000162043
	LOSS [training: 0.13603203851868587 | validation: 0.11253686849902178]
	TIME [epoch: 8.54 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12554008736265648		[learning rate: 0.00016146]
	Learning Rate: 0.000161455
	LOSS [training: 0.12554008736265648 | validation: 0.10610322096726015]
	TIME [epoch: 8.52 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12244645797064832		[learning rate: 0.00016087]
	Learning Rate: 0.000160869
	LOSS [training: 0.12244645797064832 | validation: 0.11009757861580385]
	TIME [epoch: 8.51 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13048045808112024		[learning rate: 0.00016029]
	Learning Rate: 0.000160286
	LOSS [training: 0.13048045808112024 | validation: 0.1749192131412891]
	TIME [epoch: 8.52 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13653698325650004		[learning rate: 0.0001597]
	Learning Rate: 0.000159704
	LOSS [training: 0.13653698325650004 | validation: 0.13205752212978117]
	TIME [epoch: 8.54 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1311481776074434		[learning rate: 0.00015912]
	Learning Rate: 0.000159124
	LOSS [training: 0.1311481776074434 | validation: 0.11511272183599047]
	TIME [epoch: 8.52 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1341548663054071		[learning rate: 0.00015855]
	Learning Rate: 0.000158547
	LOSS [training: 0.1341548663054071 | validation: 0.1231287688896552]
	TIME [epoch: 8.52 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13215291853917563		[learning rate: 0.00015797]
	Learning Rate: 0.000157972
	LOSS [training: 0.13215291853917563 | validation: 0.1089525376101255]
	TIME [epoch: 8.52 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15699310370123795		[learning rate: 0.0001574]
	Learning Rate: 0.000157398
	LOSS [training: 0.15699310370123795 | validation: 0.1611569230193537]
	TIME [epoch: 8.53 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.18412291868035996		[learning rate: 0.00015683]
	Learning Rate: 0.000156827
	LOSS [training: 0.18412291868035996 | validation: 0.12961401403350914]
	TIME [epoch: 8.52 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.133957804763489		[learning rate: 0.00015626]
	Learning Rate: 0.000156258
	LOSS [training: 0.133957804763489 | validation: 0.09997778797465356]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_1244.pth
	Model improved!!!
EPOCH 1245/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13505252507107635		[learning rate: 0.00015569]
	Learning Rate: 0.000155691
	LOSS [training: 0.13505252507107635 | validation: 0.11083012816534094]
	TIME [epoch: 8.52 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1258105821584078		[learning rate: 0.00015513]
	Learning Rate: 0.000155126
	LOSS [training: 0.1258105821584078 | validation: 0.11162001395830314]
	TIME [epoch: 8.54 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.118821246050593		[learning rate: 0.00015456]
	Learning Rate: 0.000154563
	LOSS [training: 0.118821246050593 | validation: 0.12274389932071852]
	TIME [epoch: 8.52 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12486886774631008		[learning rate: 0.000154]
	Learning Rate: 0.000154002
	LOSS [training: 0.12486886774631008 | validation: 0.10261849085625863]
	TIME [epoch: 8.51 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12266871067820462		[learning rate: 0.00015344]
	Learning Rate: 0.000153443
	LOSS [training: 0.12266871067820462 | validation: 0.11377942399788252]
	TIME [epoch: 8.52 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13548921884945847		[learning rate: 0.00015289]
	Learning Rate: 0.000152886
	LOSS [training: 0.13548921884945847 | validation: 0.11623742009113022]
	TIME [epoch: 8.52 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12992094104162924		[learning rate: 0.00015233]
	Learning Rate: 0.000152331
	LOSS [training: 0.12992094104162924 | validation: 0.13969118678849388]
	TIME [epoch: 8.52 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12713609113230231		[learning rate: 0.00015178]
	Learning Rate: 0.000151779
	LOSS [training: 0.12713609113230231 | validation: 0.12998792251721453]
	TIME [epoch: 8.51 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13039740194444982		[learning rate: 0.00015123]
	Learning Rate: 0.000151228
	LOSS [training: 0.13039740194444982 | validation: 0.14809782321578932]
	TIME [epoch: 8.52 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1497575714885691		[learning rate: 0.00015068]
	Learning Rate: 0.000150679
	LOSS [training: 0.1497575714885691 | validation: 0.10887514976273845]
	TIME [epoch: 8.51 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12540293594359522		[learning rate: 0.00015013]
	Learning Rate: 0.000150132
	LOSS [training: 0.12540293594359522 | validation: 0.12544894806202517]
	TIME [epoch: 8.54 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13222005501041836		[learning rate: 0.00014959]
	Learning Rate: 0.000149587
	LOSS [training: 0.13222005501041836 | validation: 0.10386435189614265]
	TIME [epoch: 8.51 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11629316738495199		[learning rate: 0.00014904]
	Learning Rate: 0.000149044
	LOSS [training: 0.11629316738495199 | validation: 0.10246530581421798]
	TIME [epoch: 8.52 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12143608135921953		[learning rate: 0.0001485]
	Learning Rate: 0.000148504
	LOSS [training: 0.12143608135921953 | validation: 0.11073041626994855]
	TIME [epoch: 8.52 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12395159309915929		[learning rate: 0.00014796]
	Learning Rate: 0.000147965
	LOSS [training: 0.12395159309915929 | validation: 0.11462568526152322]
	TIME [epoch: 8.54 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13428868474326452		[learning rate: 0.00014743]
	Learning Rate: 0.000147428
	LOSS [training: 0.13428868474326452 | validation: 0.12287000285075922]
	TIME [epoch: 8.51 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13606054520416477		[learning rate: 0.00014689]
	Learning Rate: 0.000146893
	LOSS [training: 0.13606054520416477 | validation: 0.13096304355272143]
	TIME [epoch: 8.52 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13653456093344635		[learning rate: 0.00014636]
	Learning Rate: 0.00014636
	LOSS [training: 0.13653456093344635 | validation: 0.12905985874826428]
	TIME [epoch: 8.51 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12300253603366396		[learning rate: 0.00014583]
	Learning Rate: 0.000145828
	LOSS [training: 0.12300253603366396 | validation: 0.12368819503636909]
	TIME [epoch: 8.53 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13111511544414203		[learning rate: 0.0001453]
	Learning Rate: 0.000145299
	LOSS [training: 0.13111511544414203 | validation: 0.14070442276971365]
	TIME [epoch: 8.51 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12995976943903634		[learning rate: 0.00014477]
	Learning Rate: 0.000144772
	LOSS [training: 0.12995976943903634 | validation: 0.11568778216244603]
	TIME [epoch: 8.51 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11849718796490613		[learning rate: 0.00014425]
	Learning Rate: 0.000144247
	LOSS [training: 0.11849718796490613 | validation: 0.11852483935052949]
	TIME [epoch: 8.52 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12289796919194565		[learning rate: 0.00014372]
	Learning Rate: 0.000143723
	LOSS [training: 0.12289796919194565 | validation: 0.1427339224024693]
	TIME [epoch: 8.53 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12736955038840436		[learning rate: 0.0001432]
	Learning Rate: 0.000143201
	LOSS [training: 0.12736955038840436 | validation: 0.10327907708521168]
	TIME [epoch: 8.52 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1250886289019077		[learning rate: 0.00014268]
	Learning Rate: 0.000142682
	LOSS [training: 0.1250886289019077 | validation: 0.10167277556211626]
	TIME [epoch: 8.52 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11375316528289565		[learning rate: 0.00014216]
	Learning Rate: 0.000142164
	LOSS [training: 0.11375316528289565 | validation: 0.10907968120105685]
	TIME [epoch: 8.52 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11517778856760805		[learning rate: 0.00014165]
	Learning Rate: 0.000141648
	LOSS [training: 0.11517778856760805 | validation: 0.11377701605564486]
	TIME [epoch: 8.51 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1263298768951267		[learning rate: 0.00014113]
	Learning Rate: 0.000141134
	LOSS [training: 0.1263298768951267 | validation: 0.10702188572354628]
	TIME [epoch: 8.53 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11873281141064936		[learning rate: 0.00014062]
	Learning Rate: 0.000140622
	LOSS [training: 0.11873281141064936 | validation: 0.10691675210889436]
	TIME [epoch: 8.52 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13184980334210597		[learning rate: 0.00014011]
	Learning Rate: 0.000140112
	LOSS [training: 0.13184980334210597 | validation: 0.10816501598530733]
	TIME [epoch: 8.52 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12452650139126646		[learning rate: 0.0001396]
	Learning Rate: 0.000139603
	LOSS [training: 0.12452650139126646 | validation: 0.12063762270399203]
	TIME [epoch: 8.51 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1255124308789128		[learning rate: 0.0001391]
	Learning Rate: 0.000139096
	LOSS [training: 0.1255124308789128 | validation: 0.1336150607746605]
	TIME [epoch: 8.54 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14571058529551276		[learning rate: 0.00013859]
	Learning Rate: 0.000138592
	LOSS [training: 0.14571058529551276 | validation: 0.11623482745537725]
	TIME [epoch: 8.52 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1322742899955065		[learning rate: 0.00013809]
	Learning Rate: 0.000138089
	LOSS [training: 0.1322742899955065 | validation: 0.1218056381125305]
	TIME [epoch: 8.51 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12006976973354858		[learning rate: 0.00013759]
	Learning Rate: 0.000137587
	LOSS [training: 0.12006976973354858 | validation: 0.12511118176571454]
	TIME [epoch: 8.51 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12364936854764577		[learning rate: 0.00013709]
	Learning Rate: 0.000137088
	LOSS [training: 0.12364936854764577 | validation: 0.13401919709940416]
	TIME [epoch: 8.54 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1438535056739926		[learning rate: 0.00013659]
	Learning Rate: 0.000136591
	LOSS [training: 0.1438535056739926 | validation: 0.12401896916175911]
	TIME [epoch: 8.51 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1160706725308878		[learning rate: 0.00013609]
	Learning Rate: 0.000136095
	LOSS [training: 0.1160706725308878 | validation: 0.10905900378149849]
	TIME [epoch: 8.5 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13800253882659277		[learning rate: 0.0001356]
	Learning Rate: 0.000135601
	LOSS [training: 0.13800253882659277 | validation: 0.13613845686442963]
	TIME [epoch: 8.52 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12694985585981897		[learning rate: 0.00013511]
	Learning Rate: 0.000135109
	LOSS [training: 0.12694985585981897 | validation: 0.12159196083098014]
	TIME [epoch: 8.54 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11944010330795862		[learning rate: 0.00013462]
	Learning Rate: 0.000134619
	LOSS [training: 0.11944010330795862 | validation: 0.10574848901956468]
	TIME [epoch: 8.52 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11979868810712953		[learning rate: 0.00013413]
	Learning Rate: 0.00013413
	LOSS [training: 0.11979868810712953 | validation: 0.11238204214167016]
	TIME [epoch: 8.52 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12169402159837706		[learning rate: 0.00013364]
	Learning Rate: 0.000133643
	LOSS [training: 0.12169402159837706 | validation: 0.13092425952380893]
	TIME [epoch: 8.52 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12188304519928454		[learning rate: 0.00013316]
	Learning Rate: 0.000133158
	LOSS [training: 0.12188304519928454 | validation: 0.11364644091743833]
	TIME [epoch: 8.53 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12902484936278383		[learning rate: 0.00013268]
	Learning Rate: 0.000132675
	LOSS [training: 0.12902484936278383 | validation: 0.11444782031837414]
	TIME [epoch: 8.52 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12234188096047408		[learning rate: 0.00013219]
	Learning Rate: 0.000132194
	LOSS [training: 0.12234188096047408 | validation: 0.12028631105028492]
	TIME [epoch: 8.52 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12698273739475519		[learning rate: 0.00013171]
	Learning Rate: 0.000131714
	LOSS [training: 0.12698273739475519 | validation: 0.10670202336199866]
	TIME [epoch: 8.52 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11972304481947646		[learning rate: 0.00013124]
	Learning Rate: 0.000131236
	LOSS [training: 0.11972304481947646 | validation: 0.12262108391142673]
	TIME [epoch: 8.51 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12546011336964819		[learning rate: 0.00013076]
	Learning Rate: 0.00013076
	LOSS [training: 0.12546011336964819 | validation: 0.11733413570273973]
	TIME [epoch: 8.55 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12819487405732946		[learning rate: 0.00013029]
	Learning Rate: 0.000130285
	LOSS [training: 0.12819487405732946 | validation: 0.11668458551692129]
	TIME [epoch: 8.52 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12311348354260425		[learning rate: 0.00012981]
	Learning Rate: 0.000129812
	LOSS [training: 0.12311348354260425 | validation: 0.1125358355595281]
	TIME [epoch: 8.53 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13702885861809666		[learning rate: 0.00012934]
	Learning Rate: 0.000129341
	LOSS [training: 0.13702885861809666 | validation: 0.11683246014929638]
	TIME [epoch: 8.52 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1311982217186079		[learning rate: 0.00012887]
	Learning Rate: 0.000128872
	LOSS [training: 0.1311982217186079 | validation: 0.1496571858543168]
	TIME [epoch: 8.53 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16269212059674412		[learning rate: 0.0001284]
	Learning Rate: 0.000128404
	LOSS [training: 0.16269212059674412 | validation: 0.19475689805994906]
	TIME [epoch: 8.51 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.16708176679134273		[learning rate: 0.00012794]
	Learning Rate: 0.000127938
	LOSS [training: 0.16708176679134273 | validation: 0.09963584626549289]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_1299.pth
	Model improved!!!
EPOCH 1300/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12137293341354993		[learning rate: 0.00012747]
	Learning Rate: 0.000127474
	LOSS [training: 0.12137293341354993 | validation: 0.11959341539512545]
	TIME [epoch: 8.51 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11608458548640428		[learning rate: 0.00012701]
	Learning Rate: 0.000127011
	LOSS [training: 0.11608458548640428 | validation: 0.11704811215329367]
	TIME [epoch: 8.53 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12666584089076965		[learning rate: 0.00012655]
	Learning Rate: 0.00012655
	LOSS [training: 0.12666584089076965 | validation: 0.14259779207858875]
	TIME [epoch: 8.51 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1221917325726034		[learning rate: 0.00012609]
	Learning Rate: 0.000126091
	LOSS [training: 0.1221917325726034 | validation: 0.1089057914002464]
	TIME [epoch: 8.52 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12747117218334575		[learning rate: 0.00012563]
	Learning Rate: 0.000125633
	LOSS [training: 0.12747117218334575 | validation: 0.09506180761581082]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_1304.pth
	Model improved!!!
EPOCH 1305/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12211052140080685		[learning rate: 0.00012518]
	Learning Rate: 0.000125178
	LOSS [training: 0.12211052140080685 | validation: 0.10728987046062587]
	TIME [epoch: 8.53 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12523542530333126		[learning rate: 0.00012472]
	Learning Rate: 0.000124723
	LOSS [training: 0.12523542530333126 | validation: 0.1147120204606104]
	TIME [epoch: 8.55 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11893286535917869		[learning rate: 0.00012427]
	Learning Rate: 0.000124271
	LOSS [training: 0.11893286535917869 | validation: 0.11642387615396005]
	TIME [epoch: 8.54 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11973019824631323		[learning rate: 0.00012382]
	Learning Rate: 0.00012382
	LOSS [training: 0.11973019824631323 | validation: 0.10597366708862505]
	TIME [epoch: 8.54 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11660759014169012		[learning rate: 0.00012337]
	Learning Rate: 0.00012337
	LOSS [training: 0.11660759014169012 | validation: 0.10061613846987111]
	TIME [epoch: 8.55 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13011989124894913		[learning rate: 0.00012292]
	Learning Rate: 0.000122923
	LOSS [training: 0.13011989124894913 | validation: 0.1197795907335994]
	TIME [epoch: 8.54 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12218926009689682		[learning rate: 0.00012248]
	Learning Rate: 0.000122476
	LOSS [training: 0.12218926009689682 | validation: 0.10527165212047311]
	TIME [epoch: 8.54 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12474479137539081		[learning rate: 0.00012203]
	Learning Rate: 0.000122032
	LOSS [training: 0.12474479137539081 | validation: 0.14053081330630557]
	TIME [epoch: 8.54 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13100549019555058		[learning rate: 0.00012159]
	Learning Rate: 0.000121589
	LOSS [training: 0.13100549019555058 | validation: 0.10469661700583782]
	TIME [epoch: 8.53 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12279579639706961		[learning rate: 0.00012115]
	Learning Rate: 0.000121148
	LOSS [training: 0.12279579639706961 | validation: 0.1251508781663514]
	TIME [epoch: 8.56 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11474979758489696		[learning rate: 0.00012071]
	Learning Rate: 0.000120708
	LOSS [training: 0.11474979758489696 | validation: 0.11694759958308207]
	TIME [epoch: 8.54 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11943525043125		[learning rate: 0.00012027]
	Learning Rate: 0.00012027
	LOSS [training: 0.11943525043125 | validation: 0.10498638328447452]
	TIME [epoch: 8.54 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11835105463976554		[learning rate: 0.00011983]
	Learning Rate: 0.000119834
	LOSS [training: 0.11835105463976554 | validation: 0.10939778394588495]
	TIME [epoch: 8.54 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1198029988659384		[learning rate: 0.0001194]
	Learning Rate: 0.000119399
	LOSS [training: 0.1198029988659384 | validation: 0.10435159303464842]
	TIME [epoch: 8.56 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.117174442030525		[learning rate: 0.00011897]
	Learning Rate: 0.000118966
	LOSS [training: 0.117174442030525 | validation: 0.117942789032812]
	TIME [epoch: 8.54 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11801437161093002		[learning rate: 0.00011853]
	Learning Rate: 0.000118534
	LOSS [training: 0.11801437161093002 | validation: 0.10310892336340349]
	TIME [epoch: 8.54 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13016985984081833		[learning rate: 0.0001181]
	Learning Rate: 0.000118104
	LOSS [training: 0.13016985984081833 | validation: 0.13934275472229385]
	TIME [epoch: 8.53 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1364299730666022		[learning rate: 0.00011767]
	Learning Rate: 0.000117675
	LOSS [training: 0.1364299730666022 | validation: 0.10559378277325229]
	TIME [epoch: 8.56 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11648950437774153		[learning rate: 0.00011725]
	Learning Rate: 0.000117248
	LOSS [training: 0.11648950437774153 | validation: 0.11534198299132115]
	TIME [epoch: 8.54 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15613431944554476		[learning rate: 0.00011682]
	Learning Rate: 0.000116822
	LOSS [training: 0.15613431944554476 | validation: 0.18353924207609384]
	TIME [epoch: 8.53 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1556416478138959		[learning rate: 0.0001164]
	Learning Rate: 0.000116398
	LOSS [training: 0.1556416478138959 | validation: 0.12295068726930575]
	TIME [epoch: 8.53 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13072415518269984		[learning rate: 0.00011598]
	Learning Rate: 0.000115976
	LOSS [training: 0.13072415518269984 | validation: 0.11322451189526063]
	TIME [epoch: 8.55 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11715438531670974		[learning rate: 0.00011556]
	Learning Rate: 0.000115555
	LOSS [training: 0.11715438531670974 | validation: 0.11121138278037843]
	TIME [epoch: 8.54 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11550255548857738		[learning rate: 0.00011514]
	Learning Rate: 0.000115136
	LOSS [training: 0.11550255548857738 | validation: 0.1109722013846129]
	TIME [epoch: 8.54 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12158171856832004		[learning rate: 0.00011472]
	Learning Rate: 0.000114718
	LOSS [training: 0.12158171856832004 | validation: 0.11803450992518066]
	TIME [epoch: 8.54 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1159256847987421		[learning rate: 0.0001143]
	Learning Rate: 0.000114302
	LOSS [training: 0.1159256847987421 | validation: 0.1164956709688458]
	TIME [epoch: 8.54 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11549660579479633		[learning rate: 0.00011389]
	Learning Rate: 0.000113887
	LOSS [training: 0.11549660579479633 | validation: 0.10592828850567833]
	TIME [epoch: 8.55 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1184237185981731		[learning rate: 0.00011347]
	Learning Rate: 0.000113474
	LOSS [training: 0.1184237185981731 | validation: 0.10428604945873829]
	TIME [epoch: 8.53 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11569074075706773		[learning rate: 0.00011306]
	Learning Rate: 0.000113062
	LOSS [training: 0.11569074075706773 | validation: 0.1251829161286267]
	TIME [epoch: 8.53 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11729715937001033		[learning rate: 0.00011265]
	Learning Rate: 0.000112651
	LOSS [training: 0.11729715937001033 | validation: 0.10746914991707393]
	TIME [epoch: 8.53 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11978076900705528		[learning rate: 0.00011224]
	Learning Rate: 0.000112243
	LOSS [training: 0.11978076900705528 | validation: 0.11280699098797677]
	TIME [epoch: 8.56 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12392838971874773		[learning rate: 0.00011184]
	Learning Rate: 0.000111835
	LOSS [training: 0.12392838971874773 | validation: 0.10782384535143583]
	TIME [epoch: 8.54 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13092219571529648		[learning rate: 0.00011143]
	Learning Rate: 0.000111429
	LOSS [training: 0.13092219571529648 | validation: 0.10537337106789052]
	TIME [epoch: 8.53 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12786954967470204		[learning rate: 0.00011103]
	Learning Rate: 0.000111025
	LOSS [training: 0.12786954967470204 | validation: 0.11620466241692409]
	TIME [epoch: 8.53 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12857748026689036		[learning rate: 0.00011062]
	Learning Rate: 0.000110622
	LOSS [training: 0.12857748026689036 | validation: 0.12011211173187772]
	TIME [epoch: 8.55 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12887638271410548		[learning rate: 0.00011022]
	Learning Rate: 0.000110221
	LOSS [training: 0.12887638271410548 | validation: 0.12272026164707328]
	TIME [epoch: 8.54 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1344739479917035		[learning rate: 0.00010982]
	Learning Rate: 0.000109821
	LOSS [training: 0.1344739479917035 | validation: 0.12032274972356127]
	TIME [epoch: 8.53 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12567530799784216		[learning rate: 0.00010942]
	Learning Rate: 0.000109422
	LOSS [training: 0.12567530799784216 | validation: 0.11322069373222254]
	TIME [epoch: 8.53 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1267956984757332		[learning rate: 0.00010903]
	Learning Rate: 0.000109025
	LOSS [training: 0.1267956984757332 | validation: 0.11177601144351788]
	TIME [epoch: 8.54 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1215544172715854		[learning rate: 0.00010863]
	Learning Rate: 0.000108629
	LOSS [training: 0.1215544172715854 | validation: 0.10482685421022844]
	TIME [epoch: 8.54 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12112788641145711		[learning rate: 0.00010824]
	Learning Rate: 0.000108235
	LOSS [training: 0.12112788641145711 | validation: 0.11726886058471843]
	TIME [epoch: 8.53 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12024006982811564		[learning rate: 0.00010784]
	Learning Rate: 0.000107842
	LOSS [training: 0.12024006982811564 | validation: 0.12159949514999613]
	TIME [epoch: 8.53 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13367260036499218		[learning rate: 0.00010745]
	Learning Rate: 0.000107451
	LOSS [training: 0.13367260036499218 | validation: 0.10974985327858403]
	TIME [epoch: 8.54 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11169903226974616		[learning rate: 0.00010706]
	Learning Rate: 0.000107061
	LOSS [training: 0.11169903226974616 | validation: 0.12117929149416565]
	TIME [epoch: 8.55 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12681485326540282		[learning rate: 0.00010667]
	Learning Rate: 0.000106673
	LOSS [training: 0.12681485326540282 | validation: 0.1210651908034768]
	TIME [epoch: 8.53 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12210904793728194		[learning rate: 0.00010629]
	Learning Rate: 0.000106285
	LOSS [training: 0.12210904793728194 | validation: 0.10918933628385402]
	TIME [epoch: 8.53 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11328546140088139		[learning rate: 0.0001059]
	Learning Rate: 0.0001059
	LOSS [training: 0.11328546140088139 | validation: 0.10995090968652213]
	TIME [epoch: 8.53 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12269408629674836		[learning rate: 0.00010552]
	Learning Rate: 0.000105515
	LOSS [training: 0.12269408629674836 | validation: 0.10526936716622413]
	TIME [epoch: 8.56 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.126544510682404		[learning rate: 0.00010513]
	Learning Rate: 0.000105132
	LOSS [training: 0.126544510682404 | validation: 0.12506950151173926]
	TIME [epoch: 8.54 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12108808644928863		[learning rate: 0.00010475]
	Learning Rate: 0.000104751
	LOSS [training: 0.12108808644928863 | validation: 0.11770209427111764]
	TIME [epoch: 8.53 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12329457084789215		[learning rate: 0.00010437]
	Learning Rate: 0.000104371
	LOSS [training: 0.12329457084789215 | validation: 0.11571685044113703]
	TIME [epoch: 8.54 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11862587726604026		[learning rate: 0.00010399]
	Learning Rate: 0.000103992
	LOSS [training: 0.11862587726604026 | validation: 0.1044744675354336]
	TIME [epoch: 8.55 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12050252078361241		[learning rate: 0.00010361]
	Learning Rate: 0.000103615
	LOSS [training: 0.12050252078361241 | validation: 0.11306457291527358]
	TIME [epoch: 8.54 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11398967874957515		[learning rate: 0.00010324]
	Learning Rate: 0.000103239
	LOSS [training: 0.11398967874957515 | validation: 0.09994981995311783]
	TIME [epoch: 8.53 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1271195909277702		[learning rate: 0.00010286]
	Learning Rate: 0.000102864
	LOSS [training: 0.1271195909277702 | validation: 0.13707220915066398]
	TIME [epoch: 8.53 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1334626784911101		[learning rate: 0.00010249]
	Learning Rate: 0.000102491
	LOSS [training: 0.1334626784911101 | validation: 0.10478959327275968]
	TIME [epoch: 8.55 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11739246050949234		[learning rate: 0.00010212]
	Learning Rate: 0.000102119
	LOSS [training: 0.11739246050949234 | validation: 0.11772834661107234]
	TIME [epoch: 8.53 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11987805482905851		[learning rate: 0.00010175]
	Learning Rate: 0.000101748
	LOSS [training: 0.11987805482905851 | validation: 0.10866627497782319]
	TIME [epoch: 8.53 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11448863002267243		[learning rate: 0.00010138]
	Learning Rate: 0.000101379
	LOSS [training: 0.11448863002267243 | validation: 0.12685606786717674]
	TIME [epoch: 8.53 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1227833242913791		[learning rate: 0.00010101]
	Learning Rate: 0.000101011
	LOSS [training: 0.1227833242913791 | validation: 0.11126066739656446]
	TIME [epoch: 8.54 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13120703348933732		[learning rate: 0.00010064]
	Learning Rate: 0.000100644
	LOSS [training: 0.13120703348933732 | validation: 0.10451024183861916]
	TIME [epoch: 8.54 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12820565310696566		[learning rate: 0.00010028]
	Learning Rate: 0.000100279
	LOSS [training: 0.12820565310696566 | validation: 0.10733076847230172]
	TIME [epoch: 8.53 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12032187452337868		[learning rate: 9.9915e-05]
	Learning Rate: 9.99152e-05
	LOSS [training: 0.12032187452337868 | validation: 0.09906240789431828]
	TIME [epoch: 8.53 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12940222748881952		[learning rate: 9.9553e-05]
	Learning Rate: 9.95526e-05
	LOSS [training: 0.12940222748881952 | validation: 0.1339503043739647]
	TIME [epoch: 8.54 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12644473555299607		[learning rate: 9.9191e-05]
	Learning Rate: 9.91913e-05
	LOSS [training: 0.12644473555299607 | validation: 0.11360825687947021]
	TIME [epoch: 8.55 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.120106954762733		[learning rate: 9.8831e-05]
	Learning Rate: 9.88314e-05
	LOSS [training: 0.120106954762733 | validation: 0.10153955970878491]
	TIME [epoch: 8.53 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11758459310036058		[learning rate: 9.8473e-05]
	Learning Rate: 9.84727e-05
	LOSS [training: 0.11758459310036058 | validation: 0.10049329071819259]
	TIME [epoch: 8.53 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11818362634695043		[learning rate: 9.8115e-05]
	Learning Rate: 9.81153e-05
	LOSS [training: 0.11818362634695043 | validation: 0.12022809635321471]
	TIME [epoch: 8.54 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11729646137152609		[learning rate: 9.7759e-05]
	Learning Rate: 9.77592e-05
	LOSS [training: 0.11729646137152609 | validation: 0.11306972058376338]
	TIME [epoch: 8.56 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11573942051712863		[learning rate: 9.7404e-05]
	Learning Rate: 9.74045e-05
	LOSS [training: 0.11573942051712863 | validation: 0.11400619481111893]
	TIME [epoch: 8.53 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11954138662637066		[learning rate: 9.7051e-05]
	Learning Rate: 9.7051e-05
	LOSS [training: 0.11954138662637066 | validation: 0.11467267990003413]
	TIME [epoch: 8.53 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12139426843540202		[learning rate: 9.6699e-05]
	Learning Rate: 9.66988e-05
	LOSS [training: 0.12139426843540202 | validation: 0.11047992589642103]
	TIME [epoch: 8.53 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11509872788129247		[learning rate: 9.6348e-05]
	Learning Rate: 9.63479e-05
	LOSS [training: 0.11509872788129247 | validation: 0.0961363016766261]
	TIME [epoch: 8.56 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11242216184465512		[learning rate: 9.5998e-05]
	Learning Rate: 9.59982e-05
	LOSS [training: 0.11242216184465512 | validation: 0.12206674789925877]
	TIME [epoch: 8.53 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12264786048591088		[learning rate: 9.565e-05]
	Learning Rate: 9.56499e-05
	LOSS [training: 0.12264786048591088 | validation: 0.11462648476863435]
	TIME [epoch: 8.53 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10990309211626151		[learning rate: 9.5303e-05]
	Learning Rate: 9.53027e-05
	LOSS [training: 0.10990309211626151 | validation: 0.09548724275347899]
	TIME [epoch: 8.53 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1152185376983563		[learning rate: 9.4957e-05]
	Learning Rate: 9.49569e-05
	LOSS [training: 0.1152185376983563 | validation: 0.09784527055208178]
	TIME [epoch: 8.54 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11283550730249432		[learning rate: 9.4612e-05]
	Learning Rate: 9.46122e-05
	LOSS [training: 0.11283550730249432 | validation: 0.10521843990814442]
	TIME [epoch: 8.54 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11583422873614926		[learning rate: 9.4269e-05]
	Learning Rate: 9.42689e-05
	LOSS [training: 0.11583422873614926 | validation: 0.10977878085515526]
	TIME [epoch: 8.53 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11140386840930513		[learning rate: 9.3927e-05]
	Learning Rate: 9.39268e-05
	LOSS [training: 0.11140386840930513 | validation: 0.10270682209510847]
	TIME [epoch: 8.53 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11579863703601538		[learning rate: 9.3586e-05]
	Learning Rate: 9.35859e-05
	LOSS [training: 0.11579863703601538 | validation: 0.13161562699215537]
	TIME [epoch: 8.54 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13500044754213816		[learning rate: 9.3246e-05]
	Learning Rate: 9.32463e-05
	LOSS [training: 0.13500044754213816 | validation: 0.13034410971337004]
	TIME [epoch: 8.55 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11921616221830897		[learning rate: 9.2908e-05]
	Learning Rate: 9.29079e-05
	LOSS [training: 0.11921616221830897 | validation: 0.1059561213257314]
	TIME [epoch: 8.53 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1208826489070115		[learning rate: 9.2571e-05]
	Learning Rate: 9.25707e-05
	LOSS [training: 0.1208826489070115 | validation: 0.1337421063318278]
	TIME [epoch: 8.53 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12394239449255884		[learning rate: 9.2235e-05]
	Learning Rate: 9.22348e-05
	LOSS [training: 0.12394239449255884 | validation: 0.11326004218645844]
	TIME [epoch: 8.53 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12041973291728483		[learning rate: 9.19e-05]
	Learning Rate: 9.19001e-05
	LOSS [training: 0.12041973291728483 | validation: 0.09665323188305501]
	TIME [epoch: 8.55 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11316159931882167		[learning rate: 9.1567e-05]
	Learning Rate: 9.15665e-05
	LOSS [training: 0.11316159931882167 | validation: 0.11111802612707497]
	TIME [epoch: 8.54 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11081593244060954		[learning rate: 9.1234e-05]
	Learning Rate: 9.12343e-05
	LOSS [training: 0.11081593244060954 | validation: 0.12040589484021691]
	TIME [epoch: 8.54 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11525376475280011		[learning rate: 9.0903e-05]
	Learning Rate: 9.09031e-05
	LOSS [training: 0.11525376475280011 | validation: 0.10240649225351116]
	TIME [epoch: 8.53 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12874844758654386		[learning rate: 9.0573e-05]
	Learning Rate: 9.05733e-05
	LOSS [training: 0.12874844758654386 | validation: 0.1196509063063648]
	TIME [epoch: 8.55 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1172825552902691		[learning rate: 9.0245e-05]
	Learning Rate: 9.02446e-05
	LOSS [training: 0.1172825552902691 | validation: 0.10717942524778973]
	TIME [epoch: 8.54 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11743660943174852		[learning rate: 8.9917e-05]
	Learning Rate: 8.99171e-05
	LOSS [training: 0.11743660943174852 | validation: 0.11798422676156364]
	TIME [epoch: 8.53 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1263419689986325		[learning rate: 8.9591e-05]
	Learning Rate: 8.95908e-05
	LOSS [training: 0.1263419689986325 | validation: 0.14125778672841172]
	TIME [epoch: 8.53 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13137751191659197		[learning rate: 8.9266e-05]
	Learning Rate: 8.92656e-05
	LOSS [training: 0.13137751191659197 | validation: 0.11584032801872322]
	TIME [epoch: 8.55 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12487139563625942		[learning rate: 8.8942e-05]
	Learning Rate: 8.89417e-05
	LOSS [training: 0.12487139563625942 | validation: 0.10401391065469817]
	TIME [epoch: 8.55 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1248653430953183		[learning rate: 8.8619e-05]
	Learning Rate: 8.86189e-05
	LOSS [training: 0.1248653430953183 | validation: 0.12608972190332776]
	TIME [epoch: 8.53 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14007196562564292		[learning rate: 8.8297e-05]
	Learning Rate: 8.82973e-05
	LOSS [training: 0.14007196562564292 | validation: 0.1064028585566526]
	TIME [epoch: 8.53 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1419037266820795		[learning rate: 8.7977e-05]
	Learning Rate: 8.79769e-05
	LOSS [training: 0.1419037266820795 | validation: 0.12497878829958664]
	TIME [epoch: 8.54 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12115016219750736		[learning rate: 8.7658e-05]
	Learning Rate: 8.76576e-05
	LOSS [training: 0.12115016219750736 | validation: 0.10746216440440433]
	TIME [epoch: 8.54 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1151063279805433		[learning rate: 8.7339e-05]
	Learning Rate: 8.73395e-05
	LOSS [training: 0.1151063279805433 | validation: 0.11756999658397]
	TIME [epoch: 8.53 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11641647772278396		[learning rate: 8.7023e-05]
	Learning Rate: 8.70225e-05
	LOSS [training: 0.11641647772278396 | validation: 0.10624513696702285]
	TIME [epoch: 8.53 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11825920271404895		[learning rate: 8.6707e-05]
	Learning Rate: 8.67067e-05
	LOSS [training: 0.11825920271404895 | validation: 0.11310116048158592]
	TIME [epoch: 8.53 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11811221204733346		[learning rate: 8.6392e-05]
	Learning Rate: 8.63921e-05
	LOSS [training: 0.11811221204733346 | validation: 0.09449834540692095]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_1407.pth
	Model improved!!!
EPOCH 1408/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12353325087951006		[learning rate: 8.6079e-05]
	Learning Rate: 8.60785e-05
	LOSS [training: 0.12353325087951006 | validation: 0.11812512688687649]
	TIME [epoch: 8.52 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12096580397779015		[learning rate: 8.5766e-05]
	Learning Rate: 8.57661e-05
	LOSS [training: 0.12096580397779015 | validation: 0.10151480336373672]
	TIME [epoch: 8.52 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11609338297092042		[learning rate: 8.5455e-05]
	Learning Rate: 8.54549e-05
	LOSS [training: 0.11609338297092042 | validation: 0.10409230678669365]
	TIME [epoch: 8.52 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12774293028551653		[learning rate: 8.5145e-05]
	Learning Rate: 8.51448e-05
	LOSS [training: 0.12774293028551653 | validation: 0.11141799598456725]
	TIME [epoch: 8.55 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12819805374412724		[learning rate: 8.4836e-05]
	Learning Rate: 8.48358e-05
	LOSS [training: 0.12819805374412724 | validation: 0.10327798102998266]
	TIME [epoch: 8.53 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11798368050333403		[learning rate: 8.4528e-05]
	Learning Rate: 8.45279e-05
	LOSS [training: 0.11798368050333403 | validation: 0.11194883380277923]
	TIME [epoch: 8.52 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12736570562831828		[learning rate: 8.4221e-05]
	Learning Rate: 8.42211e-05
	LOSS [training: 0.12736570562831828 | validation: 0.1057214982126704]
	TIME [epoch: 8.52 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12319931834059947		[learning rate: 8.3916e-05]
	Learning Rate: 8.39155e-05
	LOSS [training: 0.12319931834059947 | validation: 0.11466358592872639]
	TIME [epoch: 8.54 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12214611839085301		[learning rate: 8.3611e-05]
	Learning Rate: 8.3611e-05
	LOSS [training: 0.12214611839085301 | validation: 0.10042046554988004]
	TIME [epoch: 8.52 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11363610494590151		[learning rate: 8.3308e-05]
	Learning Rate: 8.33075e-05
	LOSS [training: 0.11363610494590151 | validation: 0.11711774075583281]
	TIME [epoch: 8.53 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11640607625679338		[learning rate: 8.3005e-05]
	Learning Rate: 8.30052e-05
	LOSS [training: 0.11640607625679338 | validation: 0.10717513699993816]
	TIME [epoch: 8.52 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11145162333013348		[learning rate: 8.2704e-05]
	Learning Rate: 8.2704e-05
	LOSS [training: 0.11145162333013348 | validation: 0.10964665462866491]
	TIME [epoch: 8.54 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11046778266241611		[learning rate: 8.2404e-05]
	Learning Rate: 8.24038e-05
	LOSS [training: 0.11046778266241611 | validation: 0.10777206237213446]
	TIME [epoch: 8.53 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10904225125404547		[learning rate: 8.2105e-05]
	Learning Rate: 8.21048e-05
	LOSS [training: 0.10904225125404547 | validation: 0.10262824317270625]
	TIME [epoch: 8.53 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11547390249209255		[learning rate: 8.1807e-05]
	Learning Rate: 8.18068e-05
	LOSS [training: 0.11547390249209255 | validation: 0.10345911820970746]
	TIME [epoch: 8.52 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11773554530996969		[learning rate: 8.151e-05]
	Learning Rate: 8.15099e-05
	LOSS [training: 0.11773554530996969 | validation: 0.09875576810512475]
	TIME [epoch: 8.53 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12119758166546121		[learning rate: 8.1214e-05]
	Learning Rate: 8.12142e-05
	LOSS [training: 0.12119758166546121 | validation: 0.10937066618540361]
	TIME [epoch: 8.54 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1180350439828145		[learning rate: 8.0919e-05]
	Learning Rate: 8.09194e-05
	LOSS [training: 0.1180350439828145 | validation: 0.10349186670861411]
	TIME [epoch: 8.52 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11653021380046687		[learning rate: 8.0626e-05]
	Learning Rate: 8.06257e-05
	LOSS [training: 0.11653021380046687 | validation: 0.1143862762601679]
	TIME [epoch: 8.52 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11520017264391073		[learning rate: 8.0333e-05]
	Learning Rate: 8.03331e-05
	LOSS [training: 0.11520017264391073 | validation: 0.1061451113365335]
	TIME [epoch: 8.52 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12127426143588871		[learning rate: 8.0042e-05]
	Learning Rate: 8.00416e-05
	LOSS [training: 0.12127426143588871 | validation: 0.11833821373547776]
	TIME [epoch: 8.55 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12153416246704404		[learning rate: 7.9751e-05]
	Learning Rate: 7.97511e-05
	LOSS [training: 0.12153416246704404 | validation: 0.09887707241641762]
	TIME [epoch: 8.52 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11579906440232868		[learning rate: 7.9462e-05]
	Learning Rate: 7.94617e-05
	LOSS [training: 0.11579906440232868 | validation: 0.1079049477811668]
	TIME [epoch: 8.52 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11660264288036708		[learning rate: 7.9173e-05]
	Learning Rate: 7.91733e-05
	LOSS [training: 0.11660264288036708 | validation: 0.09973195004196246]
	TIME [epoch: 8.53 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1165692187482988		[learning rate: 7.8886e-05]
	Learning Rate: 7.8886e-05
	LOSS [training: 0.1165692187482988 | validation: 0.09822609090908793]
	TIME [epoch: 8.54 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11412835796474312		[learning rate: 7.86e-05]
	Learning Rate: 7.85997e-05
	LOSS [training: 0.11412835796474312 | validation: 0.10364464204982668]
	TIME [epoch: 8.52 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13122778388922698		[learning rate: 7.8315e-05]
	Learning Rate: 7.83145e-05
	LOSS [training: 0.13122778388922698 | validation: 0.12122790492719862]
	TIME [epoch: 8.52 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11800698634641257		[learning rate: 7.803e-05]
	Learning Rate: 7.80303e-05
	LOSS [training: 0.11800698634641257 | validation: 0.10899455691717339]
	TIME [epoch: 8.52 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11468109221031708		[learning rate: 7.7747e-05]
	Learning Rate: 7.77471e-05
	LOSS [training: 0.11468109221031708 | validation: 0.10496385738404115]
	TIME [epoch: 8.54 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12532398244374693		[learning rate: 7.7465e-05]
	Learning Rate: 7.7465e-05
	LOSS [training: 0.12532398244374693 | validation: 0.10788205134864787]
	TIME [epoch: 8.53 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11662008474690548		[learning rate: 7.7184e-05]
	Learning Rate: 7.71838e-05
	LOSS [training: 0.11662008474690548 | validation: 0.12083547527443755]
	TIME [epoch: 8.52 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.14338026121602504		[learning rate: 7.6904e-05]
	Learning Rate: 7.69037e-05
	LOSS [training: 0.14338026121602504 | validation: 0.1307529815796491]
	TIME [epoch: 8.52 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12552502660956147		[learning rate: 7.6625e-05]
	Learning Rate: 7.66246e-05
	LOSS [training: 0.12552502660956147 | validation: 0.11078447788979373]
	TIME [epoch: 8.54 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1146527437744898		[learning rate: 7.6347e-05]
	Learning Rate: 7.63466e-05
	LOSS [training: 0.1146527437744898 | validation: 0.10616810891755327]
	TIME [epoch: 8.53 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1143793687776262		[learning rate: 7.607e-05]
	Learning Rate: 7.60695e-05
	LOSS [training: 0.1143793687776262 | validation: 0.11539486326506782]
	TIME [epoch: 8.53 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12310943668301806		[learning rate: 7.5793e-05]
	Learning Rate: 7.57935e-05
	LOSS [training: 0.12310943668301806 | validation: 0.13064556620860196]
	TIME [epoch: 8.52 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12160932790929042		[learning rate: 7.5518e-05]
	Learning Rate: 7.55184e-05
	LOSS [training: 0.12160932790929042 | validation: 0.09948960741494386]
	TIME [epoch: 8.52 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11297153852662581		[learning rate: 7.5244e-05]
	Learning Rate: 7.52443e-05
	LOSS [training: 0.11297153852662581 | validation: 0.10627075044114287]
	TIME [epoch: 8.55 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11763081438768501		[learning rate: 7.4971e-05]
	Learning Rate: 7.49712e-05
	LOSS [training: 0.11763081438768501 | validation: 0.11488644042670225]
	TIME [epoch: 8.52 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11292538010205873		[learning rate: 7.4699e-05]
	Learning Rate: 7.46992e-05
	LOSS [training: 0.11292538010205873 | validation: 0.10127610586459301]
	TIME [epoch: 8.53 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12543813535723508		[learning rate: 7.4428e-05]
	Learning Rate: 7.44281e-05
	LOSS [training: 0.12543813535723508 | validation: 0.11126392681994537]
	TIME [epoch: 8.52 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10813743333406153		[learning rate: 7.4158e-05]
	Learning Rate: 7.4158e-05
	LOSS [training: 0.10813743333406153 | validation: 0.10099097018168116]
	TIME [epoch: 8.55 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10850079939990716		[learning rate: 7.3889e-05]
	Learning Rate: 7.38889e-05
	LOSS [training: 0.10850079939990716 | validation: 0.09999037594386144]
	TIME [epoch: 8.53 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11309938137956004		[learning rate: 7.3621e-05]
	Learning Rate: 7.36207e-05
	LOSS [training: 0.11309938137956004 | validation: 0.10445972795552047]
	TIME [epoch: 8.52 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11282652785243537		[learning rate: 7.3354e-05]
	Learning Rate: 7.33536e-05
	LOSS [training: 0.11282652785243537 | validation: 0.11925681541066185]
	TIME [epoch: 8.52 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11663043277535039		[learning rate: 7.3087e-05]
	Learning Rate: 7.30873e-05
	LOSS [training: 0.11663043277535039 | validation: 0.11427296948507201]
	TIME [epoch: 8.54 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.109085534261173		[learning rate: 7.2822e-05]
	Learning Rate: 7.28221e-05
	LOSS [training: 0.109085534261173 | validation: 0.09862197124411473]
	TIME [epoch: 8.52 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10948739608581		[learning rate: 7.2558e-05]
	Learning Rate: 7.25578e-05
	LOSS [training: 0.10948739608581 | validation: 0.09270756466792926]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_1455.pth
	Model improved!!!
EPOCH 1456/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11006028668132994		[learning rate: 7.2295e-05]
	Learning Rate: 7.22945e-05
	LOSS [training: 0.11006028668132994 | validation: 0.09890442749388707]
	TIME [epoch: 8.52 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11352770921305133		[learning rate: 7.2032e-05]
	Learning Rate: 7.20321e-05
	LOSS [training: 0.11352770921305133 | validation: 0.0989592149230415]
	TIME [epoch: 8.54 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11138968877903836		[learning rate: 7.1771e-05]
	Learning Rate: 7.17707e-05
	LOSS [training: 0.11138968877903836 | validation: 0.10949694180032732]
	TIME [epoch: 8.53 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1195283905277131		[learning rate: 7.151e-05]
	Learning Rate: 7.15103e-05
	LOSS [training: 0.1195283905277131 | validation: 0.10552852792028386]
	TIME [epoch: 8.52 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1169270496704016		[learning rate: 7.1251e-05]
	Learning Rate: 7.12508e-05
	LOSS [training: 0.1169270496704016 | validation: 0.09930278574083248]
	TIME [epoch: 8.52 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1166397107239053		[learning rate: 7.0992e-05]
	Learning Rate: 7.09922e-05
	LOSS [training: 0.1166397107239053 | validation: 0.09762035109192185]
	TIME [epoch: 8.53 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11326781105896244		[learning rate: 7.0735e-05]
	Learning Rate: 7.07345e-05
	LOSS [training: 0.11326781105896244 | validation: 0.10539441810852698]
	TIME [epoch: 8.54 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11666976630644157		[learning rate: 7.0478e-05]
	Learning Rate: 7.04778e-05
	LOSS [training: 0.11666976630644157 | validation: 0.11645411387695917]
	TIME [epoch: 8.52 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11265661506990368		[learning rate: 7.0222e-05]
	Learning Rate: 7.02221e-05
	LOSS [training: 0.11265661506990368 | validation: 0.10494314795357015]
	TIME [epoch: 8.52 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12638183018700455		[learning rate: 6.9967e-05]
	Learning Rate: 6.99672e-05
	LOSS [training: 0.12638183018700455 | validation: 0.15713976240401528]
	TIME [epoch: 8.52 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12284105360853488		[learning rate: 6.9713e-05]
	Learning Rate: 6.97133e-05
	LOSS [training: 0.12284105360853488 | validation: 0.10469878366935087]
	TIME [epoch: 8.54 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11490443963393933		[learning rate: 6.946e-05]
	Learning Rate: 6.94603e-05
	LOSS [training: 0.11490443963393933 | validation: 0.10249016062204705]
	TIME [epoch: 8.52 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11278961931310538		[learning rate: 6.9208e-05]
	Learning Rate: 6.92083e-05
	LOSS [training: 0.11278961931310538 | validation: 0.11276294655421398]
	TIME [epoch: 8.52 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11654981942841988		[learning rate: 6.8957e-05]
	Learning Rate: 6.89571e-05
	LOSS [training: 0.11654981942841988 | validation: 0.11161831436030825]
	TIME [epoch: 8.52 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11859148011977223		[learning rate: 6.8707e-05]
	Learning Rate: 6.87068e-05
	LOSS [training: 0.11859148011977223 | validation: 0.11694740047046218]
	TIME [epoch: 8.54 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10986562312386405		[learning rate: 6.8457e-05]
	Learning Rate: 6.84575e-05
	LOSS [training: 0.10986562312386405 | validation: 0.11060291922508891]
	TIME [epoch: 8.53 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11384144289215278		[learning rate: 6.8209e-05]
	Learning Rate: 6.82091e-05
	LOSS [training: 0.11384144289215278 | validation: 0.11590916981364847]
	TIME [epoch: 8.52 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1166972427919589		[learning rate: 6.7962e-05]
	Learning Rate: 6.79615e-05
	LOSS [training: 0.1166972427919589 | validation: 0.10725070975987175]
	TIME [epoch: 8.52 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11362599137754477		[learning rate: 6.7715e-05]
	Learning Rate: 6.77149e-05
	LOSS [training: 0.11362599137754477 | validation: 0.10997978507294562]
	TIME [epoch: 8.54 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11597511862757492		[learning rate: 6.7469e-05]
	Learning Rate: 6.74692e-05
	LOSS [training: 0.11597511862757492 | validation: 0.10484596745798788]
	TIME [epoch: 8.53 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11418700571387312		[learning rate: 6.7224e-05]
	Learning Rate: 6.72243e-05
	LOSS [training: 0.11418700571387312 | validation: 0.10136300762974915]
	TIME [epoch: 8.52 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11124894532518093		[learning rate: 6.698e-05]
	Learning Rate: 6.69804e-05
	LOSS [training: 0.11124894532518093 | validation: 0.10691957791439693]
	TIME [epoch: 8.53 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11621030481692836		[learning rate: 6.6737e-05]
	Learning Rate: 6.67373e-05
	LOSS [training: 0.11621030481692836 | validation: 0.10553191897124288]
	TIME [epoch: 8.54 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10977039486674525		[learning rate: 6.6495e-05]
	Learning Rate: 6.64951e-05
	LOSS [training: 0.10977039486674525 | validation: 0.10727025007836216]
	TIME [epoch: 8.53 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11591970347630731		[learning rate: 6.6254e-05]
	Learning Rate: 6.62538e-05
	LOSS [training: 0.11591970347630731 | validation: 0.10252266179980177]
	TIME [epoch: 8.53 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10959006059828598		[learning rate: 6.6013e-05]
	Learning Rate: 6.60133e-05
	LOSS [training: 0.10959006059828598 | validation: 0.11331936384594568]
	TIME [epoch: 8.52 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11569999558308701		[learning rate: 6.5774e-05]
	Learning Rate: 6.57738e-05
	LOSS [training: 0.11569999558308701 | validation: 0.11088919454339292]
	TIME [epoch: 8.52 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11599889247652335		[learning rate: 6.5535e-05]
	Learning Rate: 6.55351e-05
	LOSS [training: 0.11599889247652335 | validation: 0.11678128386887465]
	TIME [epoch: 8.55 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1318067499752042		[learning rate: 6.5297e-05]
	Learning Rate: 6.52972e-05
	LOSS [training: 0.1318067499752042 | validation: 0.10875419290926708]
	TIME [epoch: 8.52 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11316005505694911		[learning rate: 6.506e-05]
	Learning Rate: 6.50603e-05
	LOSS [training: 0.11316005505694911 | validation: 0.10096721641299845]
	TIME [epoch: 8.52 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1155715150418771		[learning rate: 6.4824e-05]
	Learning Rate: 6.48242e-05
	LOSS [training: 0.1155715150418771 | validation: 0.09982855860625889]
	TIME [epoch: 8.53 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11257366228290129		[learning rate: 6.4589e-05]
	Learning Rate: 6.45889e-05
	LOSS [training: 0.11257366228290129 | validation: 0.09885102923776434]
	TIME [epoch: 8.55 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10863459242264535		[learning rate: 6.4354e-05]
	Learning Rate: 6.43545e-05
	LOSS [training: 0.10863459242264535 | validation: 0.10449850639909766]
	TIME [epoch: 8.52 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10890715532877851		[learning rate: 6.4121e-05]
	Learning Rate: 6.4121e-05
	LOSS [training: 0.10890715532877851 | validation: 0.10866525149951306]
	TIME [epoch: 8.52 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1182353511404722		[learning rate: 6.3888e-05]
	Learning Rate: 6.38883e-05
	LOSS [training: 0.1182353511404722 | validation: 0.10478708727823235]
	TIME [epoch: 8.52 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1140073479965408		[learning rate: 6.3656e-05]
	Learning Rate: 6.36564e-05
	LOSS [training: 0.1140073479965408 | validation: 0.11105749096360182]
	TIME [epoch: 8.54 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12340555050306948		[learning rate: 6.3425e-05]
	Learning Rate: 6.34254e-05
	LOSS [training: 0.12340555050306948 | validation: 0.11379015193808929]
	TIME [epoch: 8.52 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11460073378540339		[learning rate: 6.3195e-05]
	Learning Rate: 6.31952e-05
	LOSS [training: 0.11460073378540339 | validation: 0.10136266854970988]
	TIME [epoch: 8.52 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11346605180543541		[learning rate: 6.2966e-05]
	Learning Rate: 6.29659e-05
	LOSS [training: 0.11346605180543541 | validation: 0.11096724878801287]
	TIME [epoch: 8.52 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10967286767198466		[learning rate: 6.2737e-05]
	Learning Rate: 6.27374e-05
	LOSS [training: 0.10967286767198466 | validation: 0.10169804941620723]
	TIME [epoch: 8.53 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11343364848152519		[learning rate: 6.251e-05]
	Learning Rate: 6.25097e-05
	LOSS [training: 0.11343364848152519 | validation: 0.1020382842244161]
	TIME [epoch: 8.53 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1177792518317861		[learning rate: 6.2283e-05]
	Learning Rate: 6.22828e-05
	LOSS [training: 0.1177792518317861 | validation: 0.11638314130792055]
	TIME [epoch: 8.52 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12171344733611637		[learning rate: 6.2057e-05]
	Learning Rate: 6.20568e-05
	LOSS [training: 0.12171344733611637 | validation: 0.12690939357369577]
	TIME [epoch: 8.52 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13487407603924975		[learning rate: 6.1832e-05]
	Learning Rate: 6.18316e-05
	LOSS [training: 0.13487407603924975 | validation: 0.11520290072854769]
	TIME [epoch: 8.53 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12058552739832444		[learning rate: 6.1607e-05]
	Learning Rate: 6.16072e-05
	LOSS [training: 0.12058552739832444 | validation: 0.10726176125116807]
	TIME [epoch: 8.54 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11077710134441203		[learning rate: 6.1384e-05]
	Learning Rate: 6.13836e-05
	LOSS [training: 0.11077710134441203 | validation: 0.106857207858751]
	TIME [epoch: 8.52 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11363814013990947		[learning rate: 6.1161e-05]
	Learning Rate: 6.11609e-05
	LOSS [training: 0.11363814013990947 | validation: 0.10609041959085341]
	TIME [epoch: 8.52 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11259001801426163		[learning rate: 6.0939e-05]
	Learning Rate: 6.09389e-05
	LOSS [training: 0.11259001801426163 | validation: 0.10120044918347804]
	TIME [epoch: 8.52 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1136689117766659		[learning rate: 6.0718e-05]
	Learning Rate: 6.07178e-05
	LOSS [training: 0.1136689117766659 | validation: 0.10540805527684077]
	TIME [epoch: 8.54 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11176354964802337		[learning rate: 6.0497e-05]
	Learning Rate: 6.04974e-05
	LOSS [training: 0.11176354964802337 | validation: 0.10667017604322923]
	TIME [epoch: 8.52 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11991000363074367		[learning rate: 6.0278e-05]
	Learning Rate: 6.02779e-05
	LOSS [training: 0.11991000363074367 | validation: 0.11147899490712485]
	TIME [epoch: 8.52 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11965982043230086		[learning rate: 6.0059e-05]
	Learning Rate: 6.00591e-05
	LOSS [training: 0.11965982043230086 | validation: 0.10404436169667192]
	TIME [epoch: 8.52 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11294217745298676		[learning rate: 5.9841e-05]
	Learning Rate: 5.98412e-05
	LOSS [training: 0.11294217745298676 | validation: 0.09359204472298159]
	TIME [epoch: 8.55 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11720755917193015		[learning rate: 5.9624e-05]
	Learning Rate: 5.9624e-05
	LOSS [training: 0.11720755917193015 | validation: 0.10181259797644575]
	TIME [epoch: 8.52 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11605349482692033		[learning rate: 5.9408e-05]
	Learning Rate: 5.94076e-05
	LOSS [training: 0.11605349482692033 | validation: 0.11293674412410626]
	TIME [epoch: 8.52 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11272088259854796		[learning rate: 5.9192e-05]
	Learning Rate: 5.9192e-05
	LOSS [training: 0.11272088259854796 | validation: 0.11170749251283607]
	TIME [epoch: 8.52 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10893777541520847		[learning rate: 5.8977e-05]
	Learning Rate: 5.89772e-05
	LOSS [training: 0.10893777541520847 | validation: 0.10636823714182705]
	TIME [epoch: 8.54 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11496978518179439		[learning rate: 5.8763e-05]
	Learning Rate: 5.87632e-05
	LOSS [training: 0.11496978518179439 | validation: 0.0989121553394822]
	TIME [epoch: 8.53 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10913924567901345		[learning rate: 5.855e-05]
	Learning Rate: 5.85499e-05
	LOSS [training: 0.10913924567901345 | validation: 0.09463785303783875]
	TIME [epoch: 8.52 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11608366896243658		[learning rate: 5.8337e-05]
	Learning Rate: 5.83374e-05
	LOSS [training: 0.11608366896243658 | validation: 0.09396247488988248]
	TIME [epoch: 8.52 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11038319554441908		[learning rate: 5.8126e-05]
	Learning Rate: 5.81257e-05
	LOSS [training: 0.11038319554441908 | validation: 0.09746241279390863]
	TIME [epoch: 8.53 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11043245108639513		[learning rate: 5.7915e-05]
	Learning Rate: 5.79148e-05
	LOSS [training: 0.11043245108639513 | validation: 0.10073925768229197]
	TIME [epoch: 8.54 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11301679188494866		[learning rate: 5.7705e-05]
	Learning Rate: 5.77046e-05
	LOSS [training: 0.11301679188494866 | validation: 0.0958482151084768]
	TIME [epoch: 8.52 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11677195390117241		[learning rate: 5.7495e-05]
	Learning Rate: 5.74952e-05
	LOSS [training: 0.11677195390117241 | validation: 0.10500236612365975]
	TIME [epoch: 8.52 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11329604411026883		[learning rate: 5.7287e-05]
	Learning Rate: 5.72866e-05
	LOSS [training: 0.11329604411026883 | validation: 0.10407279267095355]
	TIME [epoch: 8.52 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11521730883975981		[learning rate: 5.7079e-05]
	Learning Rate: 5.70787e-05
	LOSS [training: 0.11521730883975981 | validation: 0.10228491372491713]
	TIME [epoch: 8.54 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11832318148552659		[learning rate: 5.6872e-05]
	Learning Rate: 5.68715e-05
	LOSS [training: 0.11832318148552659 | validation: 0.11488825765352476]
	TIME [epoch: 8.52 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11637085967413832		[learning rate: 5.6665e-05]
	Learning Rate: 5.66651e-05
	LOSS [training: 0.11637085967413832 | validation: 0.09878377495421148]
	TIME [epoch: 8.52 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11376782753374433		[learning rate: 5.6459e-05]
	Learning Rate: 5.64595e-05
	LOSS [training: 0.11376782753374433 | validation: 0.09602376783302127]
	TIME [epoch: 8.52 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11323597245756019		[learning rate: 5.6255e-05]
	Learning Rate: 5.62546e-05
	LOSS [training: 0.11323597245756019 | validation: 0.11815268421105807]
	TIME [epoch: 8.55 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12237213069623831		[learning rate: 5.605e-05]
	Learning Rate: 5.60504e-05
	LOSS [training: 0.12237213069623831 | validation: 0.12524718911511026]
	TIME [epoch: 8.53 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.15002326660505672		[learning rate: 5.5847e-05]
	Learning Rate: 5.5847e-05
	LOSS [training: 0.15002326660505672 | validation: 0.12274295755716921]
	TIME [epoch: 8.52 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1188773468679855		[learning rate: 5.5644e-05]
	Learning Rate: 5.56444e-05
	LOSS [training: 0.1188773468679855 | validation: 0.10956178642590787]
	TIME [epoch: 8.52 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11109336796549545		[learning rate: 5.5442e-05]
	Learning Rate: 5.54424e-05
	LOSS [training: 0.11109336796549545 | validation: 0.10986355452900756]
	TIME [epoch: 8.54 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11100287172199368		[learning rate: 5.5241e-05]
	Learning Rate: 5.52412e-05
	LOSS [training: 0.11100287172199368 | validation: 0.11234018358450634]
	TIME [epoch: 8.53 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1091185536816702		[learning rate: 5.5041e-05]
	Learning Rate: 5.50407e-05
	LOSS [training: 0.1091185536816702 | validation: 0.11285039039896419]
	TIME [epoch: 8.53 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1139774609724955		[learning rate: 5.4841e-05]
	Learning Rate: 5.4841e-05
	LOSS [training: 0.1139774609724955 | validation: 0.10040088020191151]
	TIME [epoch: 8.52 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11292479782068128		[learning rate: 5.4642e-05]
	Learning Rate: 5.4642e-05
	LOSS [training: 0.11292479782068128 | validation: 0.099029345017565]
	TIME [epoch: 8.54 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1108768383677535		[learning rate: 5.4444e-05]
	Learning Rate: 5.44437e-05
	LOSS [training: 0.1108768383677535 | validation: 0.1073583854810037]
	TIME [epoch: 8.53 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11783956269630698		[learning rate: 5.4246e-05]
	Learning Rate: 5.42461e-05
	LOSS [training: 0.11783956269630698 | validation: 0.1184760789039941]
	TIME [epoch: 8.53 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11486981685927208		[learning rate: 5.4049e-05]
	Learning Rate: 5.40492e-05
	LOSS [training: 0.11486981685927208 | validation: 0.1121369433452084]
	TIME [epoch: 8.52 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11078957438787732		[learning rate: 5.3853e-05]
	Learning Rate: 5.38531e-05
	LOSS [training: 0.11078957438787732 | validation: 0.09982964473926426]
	TIME [epoch: 8.53 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1179632650624334		[learning rate: 5.3658e-05]
	Learning Rate: 5.36577e-05
	LOSS [training: 0.1179632650624334 | validation: 0.12179669435024683]
	TIME [epoch: 8.55 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11285692512353088		[learning rate: 5.3463e-05]
	Learning Rate: 5.34629e-05
	LOSS [training: 0.11285692512353088 | validation: 0.10802338575898762]
	TIME [epoch: 8.52 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10981185576118982		[learning rate: 5.3269e-05]
	Learning Rate: 5.32689e-05
	LOSS [training: 0.10981185576118982 | validation: 0.12227944428932264]
	TIME [epoch: 8.53 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12128762690901776		[learning rate: 5.3076e-05]
	Learning Rate: 5.30756e-05
	LOSS [training: 0.12128762690901776 | validation: 0.11811279334232014]
	TIME [epoch: 8.52 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13223833061234003		[learning rate: 5.2883e-05]
	Learning Rate: 5.2883e-05
	LOSS [training: 0.13223833061234003 | validation: 0.11608477252089149]
	TIME [epoch: 8.55 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.126610565949246		[learning rate: 5.2691e-05]
	Learning Rate: 5.26911e-05
	LOSS [training: 0.126610565949246 | validation: 0.11754318124010057]
	TIME [epoch: 8.52 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11205359250612776		[learning rate: 5.25e-05]
	Learning Rate: 5.24998e-05
	LOSS [training: 0.11205359250612776 | validation: 0.1235561184491658]
	TIME [epoch: 8.52 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11923221085134897		[learning rate: 5.2309e-05]
	Learning Rate: 5.23093e-05
	LOSS [training: 0.11923221085134897 | validation: 0.1184750844213574]
	TIME [epoch: 8.52 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11119289887455472		[learning rate: 5.2119e-05]
	Learning Rate: 5.21195e-05
	LOSS [training: 0.11119289887455472 | validation: 0.11268456539041544]
	TIME [epoch: 8.54 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11346795850695279		[learning rate: 5.193e-05]
	Learning Rate: 5.19303e-05
	LOSS [training: 0.11346795850695279 | validation: 0.1049916063741129]
	TIME [epoch: 8.53 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10784158982513652		[learning rate: 5.1742e-05]
	Learning Rate: 5.17419e-05
	LOSS [training: 0.10784158982513652 | validation: 0.10854479235863504]
	TIME [epoch: 8.53 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11191931998345253		[learning rate: 5.1554e-05]
	Learning Rate: 5.15541e-05
	LOSS [training: 0.11191931998345253 | validation: 0.09699948653886104]
	TIME [epoch: 8.53 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10977297614327253		[learning rate: 5.1367e-05]
	Learning Rate: 5.1367e-05
	LOSS [training: 0.10977297614327253 | validation: 0.10952783003684746]
	TIME [epoch: 8.54 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11826533963329014		[learning rate: 5.1181e-05]
	Learning Rate: 5.11806e-05
	LOSS [training: 0.11826533963329014 | validation: 0.1389953008633337]
	TIME [epoch: 8.53 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.125582692624844		[learning rate: 5.0995e-05]
	Learning Rate: 5.09949e-05
	LOSS [training: 0.125582692624844 | validation: 0.12475045858128139]
	TIME [epoch: 8.53 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12110023089548135		[learning rate: 5.081e-05]
	Learning Rate: 5.08098e-05
	LOSS [training: 0.12110023089548135 | validation: 0.13419591333962244]
	TIME [epoch: 8.53 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1199022952988438		[learning rate: 5.0625e-05]
	Learning Rate: 5.06254e-05
	LOSS [training: 0.1199022952988438 | validation: 0.1053252692356518]
	TIME [epoch: 8.53 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11274094662936955		[learning rate: 5.0442e-05]
	Learning Rate: 5.04417e-05
	LOSS [training: 0.11274094662936955 | validation: 0.10556903661700931]
	TIME [epoch: 8.54 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11735198793475984		[learning rate: 5.0259e-05]
	Learning Rate: 5.02586e-05
	LOSS [training: 0.11735198793475984 | validation: 0.11004981917052256]
	TIME [epoch: 8.53 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11071317894133492		[learning rate: 5.0076e-05]
	Learning Rate: 5.00762e-05
	LOSS [training: 0.11071317894133492 | validation: 0.10188842442760235]
	TIME [epoch: 8.53 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11192962720336812		[learning rate: 4.9894e-05]
	Learning Rate: 4.98945e-05
	LOSS [training: 0.11192962720336812 | validation: 0.10462389119756185]
	TIME [epoch: 8.52 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11271773064258668		[learning rate: 4.9713e-05]
	Learning Rate: 4.97134e-05
	LOSS [training: 0.11271773064258668 | validation: 0.10445744779692363]
	TIME [epoch: 8.55 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11612349179756225		[learning rate: 4.9533e-05]
	Learning Rate: 4.9533e-05
	LOSS [training: 0.11612349179756225 | validation: 0.1085429872126645]
	TIME [epoch: 8.53 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11176419327025633		[learning rate: 4.9353e-05]
	Learning Rate: 4.93533e-05
	LOSS [training: 0.11176419327025633 | validation: 0.10182707967090122]
	TIME [epoch: 8.53 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11190751925594151		[learning rate: 4.9174e-05]
	Learning Rate: 4.91742e-05
	LOSS [training: 0.11190751925594151 | validation: 0.10611161679265652]
	TIME [epoch: 8.52 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11770020753498193		[learning rate: 4.8996e-05]
	Learning Rate: 4.89957e-05
	LOSS [training: 0.11770020753498193 | validation: 0.09854566670046033]
	TIME [epoch: 8.54 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11107815275546225		[learning rate: 4.8818e-05]
	Learning Rate: 4.88179e-05
	LOSS [training: 0.11107815275546225 | validation: 0.10618402131515293]
	TIME [epoch: 8.53 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10882178028283104		[learning rate: 4.8641e-05]
	Learning Rate: 4.86407e-05
	LOSS [training: 0.10882178028283104 | validation: 0.10437348962148198]
	TIME [epoch: 8.52 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11161684844064454		[learning rate: 4.8464e-05]
	Learning Rate: 4.84642e-05
	LOSS [training: 0.11161684844064454 | validation: 0.09863764678056494]
	TIME [epoch: 8.52 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11301765693721284		[learning rate: 4.8288e-05]
	Learning Rate: 4.82883e-05
	LOSS [training: 0.11301765693721284 | validation: 0.09707023860034455]
	TIME [epoch: 8.54 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11319858035962967		[learning rate: 4.8113e-05]
	Learning Rate: 4.81131e-05
	LOSS [training: 0.11319858035962967 | validation: 0.10746786682394435]
	TIME [epoch: 8.53 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11206692978880399		[learning rate: 4.7938e-05]
	Learning Rate: 4.79385e-05
	LOSS [training: 0.11206692978880399 | validation: 0.10643094789099121]
	TIME [epoch: 8.52 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11488898637700695		[learning rate: 4.7765e-05]
	Learning Rate: 4.77645e-05
	LOSS [training: 0.11488898637700695 | validation: 0.10624321459067869]
	TIME [epoch: 8.52 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10454246883116614		[learning rate: 4.7591e-05]
	Learning Rate: 4.75912e-05
	LOSS [training: 0.10454246883116614 | validation: 0.11422952531250999]
	TIME [epoch: 8.54 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12466617489462514		[learning rate: 4.7418e-05]
	Learning Rate: 4.74185e-05
	LOSS [training: 0.12466617489462514 | validation: 0.13049271753656724]
	TIME [epoch: 8.53 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1290821714972567		[learning rate: 4.7246e-05]
	Learning Rate: 4.72464e-05
	LOSS [training: 0.1290821714972567 | validation: 0.1119404118676448]
	TIME [epoch: 8.53 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11524150231078997		[learning rate: 4.7075e-05]
	Learning Rate: 4.70749e-05
	LOSS [training: 0.11524150231078997 | validation: 0.10385863195394035]
	TIME [epoch: 8.52 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10902970976909823		[learning rate: 4.6904e-05]
	Learning Rate: 4.69041e-05
	LOSS [training: 0.10902970976909823 | validation: 0.10964490804281689]
	TIME [epoch: 8.52 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11368456877775457		[learning rate: 4.6734e-05]
	Learning Rate: 4.67339e-05
	LOSS [training: 0.11368456877775457 | validation: 0.10493094479313406]
	TIME [epoch: 8.55 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11028863853118001		[learning rate: 4.6564e-05]
	Learning Rate: 4.65643e-05
	LOSS [training: 0.11028863853118001 | validation: 0.10350228195185404]
	TIME [epoch: 8.52 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11402956436931178		[learning rate: 4.6395e-05]
	Learning Rate: 4.63953e-05
	LOSS [training: 0.11402956436931178 | validation: 0.10622154050071891]
	TIME [epoch: 8.52 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1066301249666417		[learning rate: 4.6227e-05]
	Learning Rate: 4.62269e-05
	LOSS [training: 0.1066301249666417 | validation: 0.1035117281694167]
	TIME [epoch: 8.52 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11668658528561145		[learning rate: 4.6059e-05]
	Learning Rate: 4.60591e-05
	LOSS [training: 0.11668658528561145 | validation: 0.11381068983448944]
	TIME [epoch: 8.54 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12169595518514782		[learning rate: 4.5892e-05]
	Learning Rate: 4.5892e-05
	LOSS [training: 0.12169595518514782 | validation: 0.1389967517297845]
	TIME [epoch: 8.52 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12707912371180977		[learning rate: 4.5725e-05]
	Learning Rate: 4.57254e-05
	LOSS [training: 0.12707912371180977 | validation: 0.13313656212118827]
	TIME [epoch: 8.52 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13317746350166498		[learning rate: 4.556e-05]
	Learning Rate: 4.55595e-05
	LOSS [training: 0.13317746350166498 | validation: 0.13363282670810028]
	TIME [epoch: 8.52 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11819603394212053		[learning rate: 4.5394e-05]
	Learning Rate: 4.53942e-05
	LOSS [training: 0.11819603394212053 | validation: 0.10309097432055336]
	TIME [epoch: 8.54 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10891527788571784		[learning rate: 4.5229e-05]
	Learning Rate: 4.52294e-05
	LOSS [training: 0.10891527788571784 | validation: 0.0968444428048729]
	TIME [epoch: 8.53 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11118167550675191		[learning rate: 4.5065e-05]
	Learning Rate: 4.50653e-05
	LOSS [training: 0.11118167550675191 | validation: 0.10358214408549393]
	TIME [epoch: 8.52 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11317725584590488		[learning rate: 4.4902e-05]
	Learning Rate: 4.49017e-05
	LOSS [training: 0.11317725584590488 | validation: 0.10940167555509175]
	TIME [epoch: 8.52 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10903647776074007		[learning rate: 4.4739e-05]
	Learning Rate: 4.47388e-05
	LOSS [training: 0.10903647776074007 | validation: 0.10598424551274079]
	TIME [epoch: 8.53 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10805348852808257		[learning rate: 4.4576e-05]
	Learning Rate: 4.45764e-05
	LOSS [training: 0.10805348852808257 | validation: 0.10815752934312203]
	TIME [epoch: 8.53 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10884582311905908		[learning rate: 4.4415e-05]
	Learning Rate: 4.44147e-05
	LOSS [training: 0.10884582311905908 | validation: 0.09613025345733915]
	TIME [epoch: 8.52 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11404744741227861		[learning rate: 4.4253e-05]
	Learning Rate: 4.42535e-05
	LOSS [training: 0.11404744741227861 | validation: 0.11412591560081314]
	TIME [epoch: 8.52 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10991927493465092		[learning rate: 4.4093e-05]
	Learning Rate: 4.40929e-05
	LOSS [training: 0.10991927493465092 | validation: 0.10191166042722147]
	TIME [epoch: 8.53 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11117067136631356		[learning rate: 4.3933e-05]
	Learning Rate: 4.39329e-05
	LOSS [training: 0.11117067136631356 | validation: 0.10479519216261418]
	TIME [epoch: 8.54 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11022439037456344		[learning rate: 4.3773e-05]
	Learning Rate: 4.37734e-05
	LOSS [training: 0.11022439037456344 | validation: 0.10328130508432207]
	TIME [epoch: 8.52 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10787891456759442		[learning rate: 4.3615e-05]
	Learning Rate: 4.36146e-05
	LOSS [training: 0.10787891456759442 | validation: 0.10072441652966865]
	TIME [epoch: 8.52 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10811961110467297		[learning rate: 4.3456e-05]
	Learning Rate: 4.34563e-05
	LOSS [training: 0.10811961110467297 | validation: 0.09739558359578215]
	TIME [epoch: 8.52 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10808786041760213		[learning rate: 4.3299e-05]
	Learning Rate: 4.32986e-05
	LOSS [training: 0.10808786041760213 | validation: 0.10612850813040842]
	TIME [epoch: 8.54 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10925272287495391		[learning rate: 4.3141e-05]
	Learning Rate: 4.31415e-05
	LOSS [training: 0.10925272287495391 | validation: 0.10428667425855342]
	TIME [epoch: 8.52 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11800049843579974		[learning rate: 4.2985e-05]
	Learning Rate: 4.29849e-05
	LOSS [training: 0.11800049843579974 | validation: 0.11107820701090472]
	TIME [epoch: 8.52 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11411924184295612		[learning rate: 4.2829e-05]
	Learning Rate: 4.28289e-05
	LOSS [training: 0.11411924184295612 | validation: 0.10195223007229381]
	TIME [epoch: 8.51 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10879226992938404		[learning rate: 4.2673e-05]
	Learning Rate: 4.26735e-05
	LOSS [training: 0.10879226992938404 | validation: 0.10413037279659088]
	TIME [epoch: 8.55 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11556446169639212		[learning rate: 4.2519e-05]
	Learning Rate: 4.25186e-05
	LOSS [training: 0.11556446169639212 | validation: 0.12319745900595408]
	TIME [epoch: 8.52 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11540879428365672		[learning rate: 4.2364e-05]
	Learning Rate: 4.23643e-05
	LOSS [training: 0.11540879428365672 | validation: 0.10266650613153587]
	TIME [epoch: 8.52 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10771761232282877		[learning rate: 4.2211e-05]
	Learning Rate: 4.22106e-05
	LOSS [training: 0.10771761232282877 | validation: 0.11088019854992126]
	TIME [epoch: 8.53 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11568818801088558		[learning rate: 4.2057e-05]
	Learning Rate: 4.20574e-05
	LOSS [training: 0.11568818801088558 | validation: 0.1125868023385545]
	TIME [epoch: 8.53 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11211633393768841		[learning rate: 4.1905e-05]
	Learning Rate: 4.19047e-05
	LOSS [training: 0.11211633393768841 | validation: 0.10777905103875121]
	TIME [epoch: 8.53 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.113199723577404		[learning rate: 4.1753e-05]
	Learning Rate: 4.17527e-05
	LOSS [training: 0.113199723577404 | validation: 0.11092099126150179]
	TIME [epoch: 8.53 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11757069476437965		[learning rate: 4.1601e-05]
	Learning Rate: 4.16012e-05
	LOSS [training: 0.11757069476437965 | validation: 0.10817704627586855]
	TIME [epoch: 8.52 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10504467387369613		[learning rate: 4.145e-05]
	Learning Rate: 4.14502e-05
	LOSS [training: 0.10504467387369613 | validation: 0.0976854476188091]
	TIME [epoch: 8.53 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10743217051787317		[learning rate: 4.13e-05]
	Learning Rate: 4.12998e-05
	LOSS [training: 0.10743217051787317 | validation: 0.10470643107468078]
	TIME [epoch: 8.55 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11460167304062474		[learning rate: 4.115e-05]
	Learning Rate: 4.11499e-05
	LOSS [training: 0.11460167304062474 | validation: 0.11064484999298897]
	TIME [epoch: 8.52 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11290426713260904		[learning rate: 4.1001e-05]
	Learning Rate: 4.10005e-05
	LOSS [training: 0.11290426713260904 | validation: 0.12254714684835817]
	TIME [epoch: 8.53 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1145562931239323		[learning rate: 4.0852e-05]
	Learning Rate: 4.08517e-05
	LOSS [training: 0.1145562931239323 | validation: 0.1078617159629501]
	TIME [epoch: 8.52 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11371773624189083		[learning rate: 4.0703e-05]
	Learning Rate: 4.07035e-05
	LOSS [training: 0.11371773624189083 | validation: 0.10796717554997795]
	TIME [epoch: 8.55 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10813484204772031		[learning rate: 4.0556e-05]
	Learning Rate: 4.05558e-05
	LOSS [training: 0.10813484204772031 | validation: 0.11082772419975387]
	TIME [epoch: 8.52 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11493109682106498		[learning rate: 4.0409e-05]
	Learning Rate: 4.04086e-05
	LOSS [training: 0.11493109682106498 | validation: 0.11206985395504356]
	TIME [epoch: 8.52 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11406463584302373		[learning rate: 4.0262e-05]
	Learning Rate: 4.0262e-05
	LOSS [training: 0.11406463584302373 | validation: 0.10768433782085568]
	TIME [epoch: 8.53 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11504578418733795		[learning rate: 4.0116e-05]
	Learning Rate: 4.01158e-05
	LOSS [training: 0.11504578418733795 | validation: 0.10275598253467572]
	TIME [epoch: 8.55 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11306423949045259		[learning rate: 3.997e-05]
	Learning Rate: 3.99703e-05
	LOSS [training: 0.11306423949045259 | validation: 0.1077489266750261]
	TIME [epoch: 8.53 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11212965487643294		[learning rate: 3.9825e-05]
	Learning Rate: 3.98252e-05
	LOSS [training: 0.11212965487643294 | validation: 0.10711975991646158]
	TIME [epoch: 8.53 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1104072866813263		[learning rate: 3.9681e-05]
	Learning Rate: 3.96807e-05
	LOSS [training: 0.1104072866813263 | validation: 0.10431843839905268]
	TIME [epoch: 8.52 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11109015517794767		[learning rate: 3.9537e-05]
	Learning Rate: 3.95367e-05
	LOSS [training: 0.11109015517794767 | validation: 0.09757112526157125]
	TIME [epoch: 8.54 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11307449886459979		[learning rate: 3.9393e-05]
	Learning Rate: 3.93932e-05
	LOSS [training: 0.11307449886459979 | validation: 0.10260067461993298]
	TIME [epoch: 8.53 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11383328411604512		[learning rate: 3.925e-05]
	Learning Rate: 3.92502e-05
	LOSS [training: 0.11383328411604512 | validation: 0.0990604831780486]
	TIME [epoch: 8.53 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10928114702402177		[learning rate: 3.9108e-05]
	Learning Rate: 3.91078e-05
	LOSS [training: 0.10928114702402177 | validation: 0.09445610237083976]
	TIME [epoch: 8.52 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10717180955386844		[learning rate: 3.8966e-05]
	Learning Rate: 3.89659e-05
	LOSS [training: 0.10717180955386844 | validation: 0.10570607517662226]
	TIME [epoch: 8.54 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11208484433413826		[learning rate: 3.8824e-05]
	Learning Rate: 3.88245e-05
	LOSS [training: 0.11208484433413826 | validation: 0.09297202895853399]
	TIME [epoch: 8.53 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11273151528991551		[learning rate: 3.8684e-05]
	Learning Rate: 3.86836e-05
	LOSS [training: 0.11273151528991551 | validation: 0.09500682013695978]
	TIME [epoch: 8.53 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11457114953810779		[learning rate: 3.8543e-05]
	Learning Rate: 3.85432e-05
	LOSS [training: 0.11457114953810779 | validation: 0.10071559993936242]
	TIME [epoch: 8.53 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1099722253114848		[learning rate: 3.8403e-05]
	Learning Rate: 3.84033e-05
	LOSS [training: 0.1099722253114848 | validation: 0.09889317385944821]
	TIME [epoch: 8.52 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1145650945119588		[learning rate: 3.8264e-05]
	Learning Rate: 3.82639e-05
	LOSS [training: 0.1145650945119588 | validation: 0.09995105142216235]
	TIME [epoch: 8.55 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10970330217095994		[learning rate: 3.8125e-05]
	Learning Rate: 3.81251e-05
	LOSS [training: 0.10970330217095994 | validation: 0.10902357139669801]
	TIME [epoch: 8.52 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11331486711729796		[learning rate: 3.7987e-05]
	Learning Rate: 3.79867e-05
	LOSS [training: 0.11331486711729796 | validation: 0.09835178598117125]
	TIME [epoch: 8.53 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.112724389843281		[learning rate: 3.7849e-05]
	Learning Rate: 3.78489e-05
	LOSS [training: 0.112724389843281 | validation: 0.11347384513717698]
	TIME [epoch: 8.52 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10767251565811294		[learning rate: 3.7711e-05]
	Learning Rate: 3.77115e-05
	LOSS [training: 0.10767251565811294 | validation: 0.10284593705421786]
	TIME [epoch: 8.55 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11010031532667405		[learning rate: 3.7575e-05]
	Learning Rate: 3.75746e-05
	LOSS [training: 0.11010031532667405 | validation: 0.10242749448875632]
	TIME [epoch: 8.53 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11193893961117093		[learning rate: 3.7438e-05]
	Learning Rate: 3.74383e-05
	LOSS [training: 0.11193893961117093 | validation: 0.1082471851401757]
	TIME [epoch: 8.52 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10766868016443923		[learning rate: 3.7302e-05]
	Learning Rate: 3.73024e-05
	LOSS [training: 0.10766868016443923 | validation: 0.10707296450879505]
	TIME [epoch: 8.52 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11524966725719957		[learning rate: 3.7167e-05]
	Learning Rate: 3.7167e-05
	LOSS [training: 0.11524966725719957 | validation: 0.09964380958475255]
	TIME [epoch: 8.54 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11581735233299968		[learning rate: 3.7032e-05]
	Learning Rate: 3.70322e-05
	LOSS [training: 0.11581735233299968 | validation: 0.11061143583714214]
	TIME [epoch: 8.53 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11853059814393423		[learning rate: 3.6898e-05]
	Learning Rate: 3.68978e-05
	LOSS [training: 0.11853059814393423 | validation: 0.10911623584401908]
	TIME [epoch: 8.53 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1149441762939368		[learning rate: 3.6764e-05]
	Learning Rate: 3.67639e-05
	LOSS [training: 0.1149441762939368 | validation: 0.09987798462086073]
	TIME [epoch: 8.52 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11297906002077059		[learning rate: 3.663e-05]
	Learning Rate: 3.66304e-05
	LOSS [training: 0.11297906002077059 | validation: 0.09676344360533312]
	TIME [epoch: 8.54 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1107390828610684		[learning rate: 3.6498e-05]
	Learning Rate: 3.64975e-05
	LOSS [training: 0.1107390828610684 | validation: 0.1067152147074601]
	TIME [epoch: 8.54 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10944859940274902		[learning rate: 3.6365e-05]
	Learning Rate: 3.63651e-05
	LOSS [training: 0.10944859940274902 | validation: 0.09279461255380528]
	TIME [epoch: 8.53 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10788498049784359		[learning rate: 3.6233e-05]
	Learning Rate: 3.62331e-05
	LOSS [training: 0.10788498049784359 | validation: 0.10287162853750867]
	TIME [epoch: 8.53 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11264517323923087		[learning rate: 3.6102e-05]
	Learning Rate: 3.61016e-05
	LOSS [training: 0.11264517323923087 | validation: 0.10183019920893956]
	TIME [epoch: 8.53 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11438448124678709		[learning rate: 3.5971e-05]
	Learning Rate: 3.59706e-05
	LOSS [training: 0.11438448124678709 | validation: 0.10371977874744912]
	TIME [epoch: 8.55 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10763287603128077		[learning rate: 3.584e-05]
	Learning Rate: 3.584e-05
	LOSS [training: 0.10763287603128077 | validation: 0.1047924913874776]
	TIME [epoch: 8.52 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11161127708058971		[learning rate: 3.571e-05]
	Learning Rate: 3.571e-05
	LOSS [training: 0.11161127708058971 | validation: 0.11305630056858912]
	TIME [epoch: 8.53 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1084364442274391		[learning rate: 3.558e-05]
	Learning Rate: 3.55804e-05
	LOSS [training: 0.1084364442274391 | validation: 0.10183873117105291]
	TIME [epoch: 8.53 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10843196029009827		[learning rate: 3.5451e-05]
	Learning Rate: 3.54513e-05
	LOSS [training: 0.10843196029009827 | validation: 0.09717733042395438]
	TIME [epoch: 8.55 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10938065988598944		[learning rate: 3.5323e-05]
	Learning Rate: 3.53226e-05
	LOSS [training: 0.10938065988598944 | validation: 0.09377015839731066]
	TIME [epoch: 8.52 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11379916381649562		[learning rate: 3.5194e-05]
	Learning Rate: 3.51944e-05
	LOSS [training: 0.11379916381649562 | validation: 0.1186899339498621]
	TIME [epoch: 8.52 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11608393289313476		[learning rate: 3.5067e-05]
	Learning Rate: 3.50667e-05
	LOSS [training: 0.11608393289313476 | validation: 0.10846879850804325]
	TIME [epoch: 8.52 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11393638546524795		[learning rate: 3.4939e-05]
	Learning Rate: 3.49394e-05
	LOSS [training: 0.11393638546524795 | validation: 0.09697290959856791]
	TIME [epoch: 8.54 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11211222490920099		[learning rate: 3.4813e-05]
	Learning Rate: 3.48126e-05
	LOSS [training: 0.11211222490920099 | validation: 0.10198325365179836]
	TIME [epoch: 8.53 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10899105972544407		[learning rate: 3.4686e-05]
	Learning Rate: 3.46863e-05
	LOSS [training: 0.10899105972544407 | validation: 0.11077277590304069]
	TIME [epoch: 8.52 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11525026430508416		[learning rate: 3.456e-05]
	Learning Rate: 3.45604e-05
	LOSS [training: 0.11525026430508416 | validation: 0.11486045695922757]
	TIME [epoch: 8.53 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11692375331064611		[learning rate: 3.4435e-05]
	Learning Rate: 3.4435e-05
	LOSS [training: 0.11692375331064611 | validation: 0.1195585068752546]
	TIME [epoch: 8.55 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11037791767352238		[learning rate: 3.431e-05]
	Learning Rate: 3.431e-05
	LOSS [training: 0.11037791767352238 | validation: 0.10055305102308687]
	TIME [epoch: 8.53 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11422900942762637		[learning rate: 3.4186e-05]
	Learning Rate: 3.41855e-05
	LOSS [training: 0.11422900942762637 | validation: 0.10851310551849527]
	TIME [epoch: 8.53 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10551413036581239		[learning rate: 3.4061e-05]
	Learning Rate: 3.40615e-05
	LOSS [training: 0.10551413036581239 | validation: 0.09837502694695718]
	TIME [epoch: 8.52 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1098286732488245		[learning rate: 3.3938e-05]
	Learning Rate: 3.39378e-05
	LOSS [training: 0.1098286732488245 | validation: 0.10701282410273863]
	TIME [epoch: 8.54 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.12074596053341839		[learning rate: 3.3815e-05]
	Learning Rate: 3.38147e-05
	LOSS [training: 0.12074596053341839 | validation: 0.09896592427873596]
	TIME [epoch: 8.54 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11201823326386184		[learning rate: 3.3692e-05]
	Learning Rate: 3.3692e-05
	LOSS [training: 0.11201823326386184 | validation: 0.09797471343775702]
	TIME [epoch: 8.53 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11364641765987502		[learning rate: 3.357e-05]
	Learning Rate: 3.35697e-05
	LOSS [training: 0.11364641765987502 | validation: 0.11245480511141162]
	TIME [epoch: 8.52 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11332583394490985		[learning rate: 3.3448e-05]
	Learning Rate: 3.34479e-05
	LOSS [training: 0.11332583394490985 | validation: 0.10824210561280313]
	TIME [epoch: 8.53 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11865591665250655		[learning rate: 3.3326e-05]
	Learning Rate: 3.33265e-05
	LOSS [training: 0.11865591665250655 | validation: 0.11055507394105235]
	TIME [epoch: 8.55 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11525547515333838		[learning rate: 3.3206e-05]
	Learning Rate: 3.32055e-05
	LOSS [training: 0.11525547515333838 | validation: 0.12064094576342531]
	TIME [epoch: 8.52 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11941618789290713		[learning rate: 3.3085e-05]
	Learning Rate: 3.3085e-05
	LOSS [training: 0.11941618789290713 | validation: 0.10421626796046801]
	TIME [epoch: 8.53 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11233666345026079		[learning rate: 3.2965e-05]
	Learning Rate: 3.2965e-05
	LOSS [training: 0.11233666345026079 | validation: 0.09457966690914728]
	TIME [epoch: 8.52 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10741482786523489		[learning rate: 3.2845e-05]
	Learning Rate: 3.28453e-05
	LOSS [training: 0.10741482786523489 | validation: 0.10093689751857547]
	TIME [epoch: 8.56 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1109146620925254		[learning rate: 3.2726e-05]
	Learning Rate: 3.27261e-05
	LOSS [training: 0.1109146620925254 | validation: 0.10435548382821971]
	TIME [epoch: 8.53 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10616069646887775		[learning rate: 3.2607e-05]
	Learning Rate: 3.26074e-05
	LOSS [training: 0.10616069646887775 | validation: 0.1122780197633684]
	TIME [epoch: 8.53 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1118193453820974		[learning rate: 3.2489e-05]
	Learning Rate: 3.2489e-05
	LOSS [training: 0.1118193453820974 | validation: 0.11888237917465896]
	TIME [epoch: 8.53 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.13074130533253603		[learning rate: 3.2371e-05]
	Learning Rate: 3.23711e-05
	LOSS [training: 0.13074130533253603 | validation: 0.11276675828175875]
	TIME [epoch: 8.56 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11993921809828689		[learning rate: 3.2254e-05]
	Learning Rate: 3.22537e-05
	LOSS [training: 0.11993921809828689 | validation: 0.099395894733187]
	TIME [epoch: 8.54 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11067314651298651		[learning rate: 3.2137e-05]
	Learning Rate: 3.21366e-05
	LOSS [training: 0.11067314651298651 | validation: 0.09587838169420504]
	TIME [epoch: 8.53 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10934562141259703		[learning rate: 3.202e-05]
	Learning Rate: 3.202e-05
	LOSS [training: 0.10934562141259703 | validation: 0.09809780905817367]
	TIME [epoch: 8.53 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11058619019497606		[learning rate: 3.1904e-05]
	Learning Rate: 3.19038e-05
	LOSS [training: 0.11058619019497606 | validation: 0.10252845627757351]
	TIME [epoch: 8.55 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11313599261343779		[learning rate: 3.1788e-05]
	Learning Rate: 3.1788e-05
	LOSS [training: 0.11313599261343779 | validation: 0.09734605962919951]
	TIME [epoch: 8.54 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10980929984004748		[learning rate: 3.1673e-05]
	Learning Rate: 3.16726e-05
	LOSS [training: 0.10980929984004748 | validation: 0.09901642416714022]
	TIME [epoch: 8.53 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1049192438908555		[learning rate: 3.1558e-05]
	Learning Rate: 3.15577e-05
	LOSS [training: 0.1049192438908555 | validation: 0.09613666368062995]
	TIME [epoch: 8.53 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10960570037610777		[learning rate: 3.1443e-05]
	Learning Rate: 3.14432e-05
	LOSS [training: 0.10960570037610777 | validation: 0.1054455696555551]
	TIME [epoch: 8.54 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11142249622857359		[learning rate: 3.1329e-05]
	Learning Rate: 3.13291e-05
	LOSS [training: 0.11142249622857359 | validation: 0.09936020796272005]
	TIME [epoch: 8.55 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11362214662130857		[learning rate: 3.1215e-05]
	Learning Rate: 3.12154e-05
	LOSS [training: 0.11362214662130857 | validation: 0.09979531967159253]
	TIME [epoch: 8.53 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11203463064244645		[learning rate: 3.1102e-05]
	Learning Rate: 3.11021e-05
	LOSS [training: 0.11203463064244645 | validation: 0.09670893127642596]
	TIME [epoch: 8.53 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10698821826205374		[learning rate: 3.0989e-05]
	Learning Rate: 3.09892e-05
	LOSS [training: 0.10698821826205374 | validation: 0.09995890077284603]
	TIME [epoch: 8.54 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11307209149112592		[learning rate: 3.0877e-05]
	Learning Rate: 3.08768e-05
	LOSS [training: 0.11307209149112592 | validation: 0.11147728654611312]
	TIME [epoch: 8.55 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10937704461625512		[learning rate: 3.0765e-05]
	Learning Rate: 3.07647e-05
	LOSS [training: 0.10937704461625512 | validation: 0.10434730758817189]
	TIME [epoch: 8.53 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1073509139575741		[learning rate: 3.0653e-05]
	Learning Rate: 3.0653e-05
	LOSS [training: 0.1073509139575741 | validation: 0.1082197735187069]
	TIME [epoch: 8.53 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11106088618987038		[learning rate: 3.0542e-05]
	Learning Rate: 3.05418e-05
	LOSS [training: 0.11106088618987038 | validation: 0.09523447161869228]
	TIME [epoch: 8.53 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10817164601434488		[learning rate: 3.0431e-05]
	Learning Rate: 3.0431e-05
	LOSS [training: 0.10817164601434488 | validation: 0.09395250367790806]
	TIME [epoch: 8.56 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11051255667206424		[learning rate: 3.0321e-05]
	Learning Rate: 3.03205e-05
	LOSS [training: 0.11051255667206424 | validation: 0.10358186122105548]
	TIME [epoch: 8.53 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10736290245589024		[learning rate: 3.0211e-05]
	Learning Rate: 3.02105e-05
	LOSS [training: 0.10736290245589024 | validation: 0.09757759233486674]
	TIME [epoch: 8.53 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11034639264420107		[learning rate: 3.0101e-05]
	Learning Rate: 3.01009e-05
	LOSS [training: 0.11034639264420107 | validation: 0.09746328867183313]
	TIME [epoch: 8.53 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1148779630018929		[learning rate: 2.9992e-05]
	Learning Rate: 2.99916e-05
	LOSS [training: 0.1148779630018929 | validation: 0.10591315212140648]
	TIME [epoch: 8.54 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10999599972324545		[learning rate: 2.9883e-05]
	Learning Rate: 2.98828e-05
	LOSS [training: 0.10999599972324545 | validation: 0.09887039553979585]
	TIME [epoch: 8.53 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10719350183020153		[learning rate: 2.9774e-05]
	Learning Rate: 2.97743e-05
	LOSS [training: 0.10719350183020153 | validation: 0.10125178197897469]
	TIME [epoch: 8.52 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10301265145469307		[learning rate: 2.9666e-05]
	Learning Rate: 2.96663e-05
	LOSS [training: 0.10301265145469307 | validation: 0.10198881702269044]
	TIME [epoch: 8.52 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10768956422321527		[learning rate: 2.9559e-05]
	Learning Rate: 2.95586e-05
	LOSS [training: 0.10768956422321527 | validation: 0.10767196981904344]
	TIME [epoch: 8.54 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11159901658574509		[learning rate: 2.9451e-05]
	Learning Rate: 2.94514e-05
	LOSS [training: 0.11159901658574509 | validation: 0.10340000990073898]
	TIME [epoch: 8.53 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10602788102018874		[learning rate: 2.9344e-05]
	Learning Rate: 2.93445e-05
	LOSS [training: 0.10602788102018874 | validation: 0.09940219553061722]
	TIME [epoch: 8.53 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10558292961154078		[learning rate: 2.9238e-05]
	Learning Rate: 2.9238e-05
	LOSS [training: 0.10558292961154078 | validation: 0.1007870023588073]
	TIME [epoch: 8.53 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10575074992699703		[learning rate: 2.9132e-05]
	Learning Rate: 2.91319e-05
	LOSS [training: 0.10575074992699703 | validation: 0.09743204590812676]
	TIME [epoch: 8.52 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10338766749470496		[learning rate: 2.9026e-05]
	Learning Rate: 2.90262e-05
	LOSS [training: 0.10338766749470496 | validation: 0.10446340093275355]
	TIME [epoch: 8.56 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10674214868483882		[learning rate: 2.8921e-05]
	Learning Rate: 2.89208e-05
	LOSS [training: 0.10674214868483882 | validation: 0.10143556090581149]
	TIME [epoch: 8.53 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10852103235426311		[learning rate: 2.8816e-05]
	Learning Rate: 2.88159e-05
	LOSS [training: 0.10852103235426311 | validation: 0.09857747781261593]
	TIME [epoch: 8.53 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1093402190055655		[learning rate: 2.8711e-05]
	Learning Rate: 2.87113e-05
	LOSS [training: 0.1093402190055655 | validation: 0.103852637224088]
	TIME [epoch: 8.53 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10808961980399752		[learning rate: 2.8607e-05]
	Learning Rate: 2.86071e-05
	LOSS [training: 0.10808961980399752 | validation: 0.11069646706748307]
	TIME [epoch: 8.55 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11584160850005762		[learning rate: 2.8503e-05]
	Learning Rate: 2.85033e-05
	LOSS [training: 0.11584160850005762 | validation: 0.10700278527321785]
	TIME [epoch: 8.53 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11233185058678963		[learning rate: 2.84e-05]
	Learning Rate: 2.83998e-05
	LOSS [training: 0.11233185058678963 | validation: 0.10129828611587557]
	TIME [epoch: 8.53 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11006294337966142		[learning rate: 2.8297e-05]
	Learning Rate: 2.82968e-05
	LOSS [training: 0.11006294337966142 | validation: 0.10296842785090321]
	TIME [epoch: 8.52 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11125815268553345		[learning rate: 2.8194e-05]
	Learning Rate: 2.81941e-05
	LOSS [training: 0.11125815268553345 | validation: 0.10049564344350775]
	TIME [epoch: 8.54 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10558916701793433		[learning rate: 2.8092e-05]
	Learning Rate: 2.80918e-05
	LOSS [training: 0.10558916701793433 | validation: 0.09200812830110848]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_1716.pth
	Model improved!!!
EPOCH 1717/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.111425065030569		[learning rate: 2.799e-05]
	Learning Rate: 2.79898e-05
	LOSS [training: 0.111425065030569 | validation: 0.107102843740353]
	TIME [epoch: 8.53 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1055384710272204		[learning rate: 2.7888e-05]
	Learning Rate: 2.78882e-05
	LOSS [training: 0.1055384710272204 | validation: 0.10009981317578512]
	TIME [epoch: 8.52 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10932348971884868		[learning rate: 2.7787e-05]
	Learning Rate: 2.7787e-05
	LOSS [training: 0.10932348971884868 | validation: 0.09727007719013309]
	TIME [epoch: 8.53 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10749330063631073		[learning rate: 2.7686e-05]
	Learning Rate: 2.76862e-05
	LOSS [training: 0.10749330063631073 | validation: 0.10650091468455539]
	TIME [epoch: 8.53 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11203851835369416		[learning rate: 2.7586e-05]
	Learning Rate: 2.75857e-05
	LOSS [training: 0.11203851835369416 | validation: 0.0999798260063347]
	TIME [epoch: 8.52 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10626634998063755		[learning rate: 2.7486e-05]
	Learning Rate: 2.74856e-05
	LOSS [training: 0.10626634998063755 | validation: 0.10507526133779096]
	TIME [epoch: 8.52 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10219001803047907		[learning rate: 2.7386e-05]
	Learning Rate: 2.73859e-05
	LOSS [training: 0.10219001803047907 | validation: 0.10791889919536338]
	TIME [epoch: 8.53 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10369387641926531		[learning rate: 2.7286e-05]
	Learning Rate: 2.72865e-05
	LOSS [training: 0.10369387641926531 | validation: 0.10989268597066042]
	TIME [epoch: 8.54 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10717184710962953		[learning rate: 2.7187e-05]
	Learning Rate: 2.71875e-05
	LOSS [training: 0.10717184710962953 | validation: 0.09725093602752409]
	TIME [epoch: 8.53 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10526490519259661		[learning rate: 2.7089e-05]
	Learning Rate: 2.70888e-05
	LOSS [training: 0.10526490519259661 | validation: 0.10297185402614877]
	TIME [epoch: 8.53 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10850126344402011		[learning rate: 2.699e-05]
	Learning Rate: 2.69905e-05
	LOSS [training: 0.10850126344402011 | validation: 0.09952432333641059]
	TIME [epoch: 8.52 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10784278258395705		[learning rate: 2.6893e-05]
	Learning Rate: 2.68925e-05
	LOSS [training: 0.10784278258395705 | validation: 0.1019382523653852]
	TIME [epoch: 8.55 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11399883347723147		[learning rate: 2.6795e-05]
	Learning Rate: 2.67949e-05
	LOSS [training: 0.11399883347723147 | validation: 0.1090339798839059]
	TIME [epoch: 8.52 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11124905218674638		[learning rate: 2.6698e-05]
	Learning Rate: 2.66977e-05
	LOSS [training: 0.11124905218674638 | validation: 0.09768506676585634]
	TIME [epoch: 8.52 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1064835540224208		[learning rate: 2.6601e-05]
	Learning Rate: 2.66008e-05
	LOSS [training: 0.1064835540224208 | validation: 0.09289370398549368]
	TIME [epoch: 8.52 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10746602356191122		[learning rate: 2.6504e-05]
	Learning Rate: 2.65043e-05
	LOSS [training: 0.10746602356191122 | validation: 0.09878562896085305]
	TIME [epoch: 8.55 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10839265684942227		[learning rate: 2.6408e-05]
	Learning Rate: 2.64081e-05
	LOSS [training: 0.10839265684942227 | validation: 0.10569705141380106]
	TIME [epoch: 8.53 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10306828062179944		[learning rate: 2.6312e-05]
	Learning Rate: 2.63123e-05
	LOSS [training: 0.10306828062179944 | validation: 0.10416997699358538]
	TIME [epoch: 8.52 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10673651601163969		[learning rate: 2.6217e-05]
	Learning Rate: 2.62168e-05
	LOSS [training: 0.10673651601163969 | validation: 0.09397709909013717]
	TIME [epoch: 8.52 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10562158555293377		[learning rate: 2.6122e-05]
	Learning Rate: 2.61216e-05
	LOSS [training: 0.10562158555293377 | validation: 0.10893193405831154]
	TIME [epoch: 8.54 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11481595312216822		[learning rate: 2.6027e-05]
	Learning Rate: 2.60268e-05
	LOSS [training: 0.11481595312216822 | validation: 0.10044582233042834]
	TIME [epoch: 8.53 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11166956689753496		[learning rate: 2.5932e-05]
	Learning Rate: 2.59324e-05
	LOSS [training: 0.11166956689753496 | validation: 0.09316186128392065]
	TIME [epoch: 8.53 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10673416330124959		[learning rate: 2.5838e-05]
	Learning Rate: 2.58383e-05
	LOSS [training: 0.10673416330124959 | validation: 0.0911513952395829]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_1739.pth
	Model improved!!!
EPOCH 1740/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10892544286989705		[learning rate: 2.5744e-05]
	Learning Rate: 2.57445e-05
	LOSS [training: 0.10892544286989705 | validation: 0.10263579707077347]
	TIME [epoch: 8.54 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11019813292069616		[learning rate: 2.5651e-05]
	Learning Rate: 2.56511e-05
	LOSS [training: 0.11019813292069616 | validation: 0.09968180402771928]
	TIME [epoch: 8.53 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1137067381562975		[learning rate: 2.5558e-05]
	Learning Rate: 2.5558e-05
	LOSS [training: 0.1137067381562975 | validation: 0.10530673965134235]
	TIME [epoch: 8.52 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11032825011059832		[learning rate: 2.5465e-05]
	Learning Rate: 2.54652e-05
	LOSS [training: 0.11032825011059832 | validation: 0.0988544083793137]
	TIME [epoch: 8.53 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1037573919809264		[learning rate: 2.5373e-05]
	Learning Rate: 2.53728e-05
	LOSS [training: 0.1037573919809264 | validation: 0.09355899317848465]
	TIME [epoch: 8.53 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11116386572386569		[learning rate: 2.5281e-05]
	Learning Rate: 2.52807e-05
	LOSS [training: 0.11116386572386569 | validation: 0.09993740133539251]
	TIME [epoch: 8.54 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1098473236459833		[learning rate: 2.5189e-05]
	Learning Rate: 2.5189e-05
	LOSS [training: 0.1098473236459833 | validation: 0.1018656456537326]
	TIME [epoch: 8.52 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11761988440139604		[learning rate: 2.5098e-05]
	Learning Rate: 2.50976e-05
	LOSS [training: 0.11761988440139604 | validation: 0.11483020780926675]
	TIME [epoch: 8.52 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10889929183885774		[learning rate: 2.5006e-05]
	Learning Rate: 2.50065e-05
	LOSS [training: 0.10889929183885774 | validation: 0.10910012099241137]
	TIME [epoch: 8.52 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10632721189937026		[learning rate: 2.4916e-05]
	Learning Rate: 2.49157e-05
	LOSS [training: 0.10632721189937026 | validation: 0.10279290353173594]
	TIME [epoch: 8.54 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10946672020053683		[learning rate: 2.4825e-05]
	Learning Rate: 2.48253e-05
	LOSS [training: 0.10946672020053683 | validation: 0.09614103455641557]
	TIME [epoch: 8.52 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11207135786823645		[learning rate: 2.4735e-05]
	Learning Rate: 2.47352e-05
	LOSS [training: 0.11207135786823645 | validation: 0.1033866940690684]
	TIME [epoch: 8.53 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11524243441193283		[learning rate: 2.4645e-05]
	Learning Rate: 2.46455e-05
	LOSS [training: 0.11524243441193283 | validation: 0.10141310118111113]
	TIME [epoch: 8.52 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1172062762134145		[learning rate: 2.4556e-05]
	Learning Rate: 2.4556e-05
	LOSS [training: 0.1172062762134145 | validation: 0.0997818026644881]
	TIME [epoch: 8.55 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10944059591335013		[learning rate: 2.4467e-05]
	Learning Rate: 2.44669e-05
	LOSS [training: 0.10944059591335013 | validation: 0.10329162485389706]
	TIME [epoch: 8.53 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10383116946282638		[learning rate: 2.4378e-05]
	Learning Rate: 2.43781e-05
	LOSS [training: 0.10383116946282638 | validation: 0.09073406780717189]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_1755.pth
	Model improved!!!
EPOCH 1756/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10875001568596183		[learning rate: 2.429e-05]
	Learning Rate: 2.42896e-05
	LOSS [training: 0.10875001568596183 | validation: 0.10519197692870041]
	TIME [epoch: 8.52 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10815245357754819		[learning rate: 2.4201e-05]
	Learning Rate: 2.42015e-05
	LOSS [training: 0.10815245357754819 | validation: 0.10125463706206539]
	TIME [epoch: 8.54 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11075425907415397		[learning rate: 2.4114e-05]
	Learning Rate: 2.41137e-05
	LOSS [training: 0.11075425907415397 | validation: 0.10196982681260558]
	TIME [epoch: 8.52 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11018655151656412		[learning rate: 2.4026e-05]
	Learning Rate: 2.40262e-05
	LOSS [training: 0.11018655151656412 | validation: 0.10080914221580277]
	TIME [epoch: 8.52 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11080683485860403		[learning rate: 2.3939e-05]
	Learning Rate: 2.3939e-05
	LOSS [training: 0.11080683485860403 | validation: 0.09127562461133101]
	TIME [epoch: 8.53 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10501990668726272		[learning rate: 2.3852e-05]
	Learning Rate: 2.38521e-05
	LOSS [training: 0.10501990668726272 | validation: 0.09929106744548033]
	TIME [epoch: 8.54 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1072933336035328		[learning rate: 2.3766e-05]
	Learning Rate: 2.37655e-05
	LOSS [training: 0.1072933336035328 | validation: 0.11110847553031053]
	TIME [epoch: 8.53 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11098500326747465		[learning rate: 2.3679e-05]
	Learning Rate: 2.36793e-05
	LOSS [training: 0.11098500326747465 | validation: 0.10949126578665565]
	TIME [epoch: 8.52 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11500265616812562		[learning rate: 2.3593e-05]
	Learning Rate: 2.35933e-05
	LOSS [training: 0.11500265616812562 | validation: 0.12248764185108368]
	TIME [epoch: 8.52 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10926359788770983		[learning rate: 2.3508e-05]
	Learning Rate: 2.35077e-05
	LOSS [training: 0.10926359788770983 | validation: 0.10496610901276682]
	TIME [epoch: 8.52 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10942436017575721		[learning rate: 2.3422e-05]
	Learning Rate: 2.34224e-05
	LOSS [training: 0.10942436017575721 | validation: 0.09590752568308379]
	TIME [epoch: 8.54 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10802492538491155		[learning rate: 2.3337e-05]
	Learning Rate: 2.33374e-05
	LOSS [training: 0.10802492538491155 | validation: 0.10018279530165708]
	TIME [epoch: 8.52 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10695973432730085		[learning rate: 2.3253e-05]
	Learning Rate: 2.32527e-05
	LOSS [training: 0.10695973432730085 | validation: 0.1107265327410121]
	TIME [epoch: 8.52 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11059070548319969		[learning rate: 2.3168e-05]
	Learning Rate: 2.31683e-05
	LOSS [training: 0.11059070548319969 | validation: 0.09406095403051262]
	TIME [epoch: 8.52 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10826243951771079		[learning rate: 2.3084e-05]
	Learning Rate: 2.30843e-05
	LOSS [training: 0.10826243951771079 | validation: 0.09602455891726339]
	TIME [epoch: 8.55 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1055920026418431		[learning rate: 2.3e-05]
	Learning Rate: 2.30005e-05
	LOSS [training: 0.1055920026418431 | validation: 0.09974664749046966]
	TIME [epoch: 8.52 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10531628141351115		[learning rate: 2.2917e-05]
	Learning Rate: 2.2917e-05
	LOSS [training: 0.10531628141351115 | validation: 0.09172511268541195]
	TIME [epoch: 8.52 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1093081847973972		[learning rate: 2.2834e-05]
	Learning Rate: 2.28338e-05
	LOSS [training: 0.1093081847973972 | validation: 0.10394476465404645]
	TIME [epoch: 8.52 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1086374703989571		[learning rate: 2.2751e-05]
	Learning Rate: 2.2751e-05
	LOSS [training: 0.1086374703989571 | validation: 0.10904547890012695]
	TIME [epoch: 8.55 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1089736345702145		[learning rate: 2.2668e-05]
	Learning Rate: 2.26684e-05
	LOSS [training: 0.1089736345702145 | validation: 0.10403314209212879]
	TIME [epoch: 8.53 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10656632703909921		[learning rate: 2.2586e-05]
	Learning Rate: 2.25862e-05
	LOSS [training: 0.10656632703909921 | validation: 0.10331888025979202]
	TIME [epoch: 8.52 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10647643522562807		[learning rate: 2.2504e-05]
	Learning Rate: 2.25042e-05
	LOSS [training: 0.10647643522562807 | validation: 0.0977726203856112]
	TIME [epoch: 8.52 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1028682699200408		[learning rate: 2.2423e-05]
	Learning Rate: 2.24225e-05
	LOSS [training: 0.1028682699200408 | validation: 0.09546920456280132]
	TIME [epoch: 8.54 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10818171675232242		[learning rate: 2.2341e-05]
	Learning Rate: 2.23411e-05
	LOSS [training: 0.10818171675232242 | validation: 0.09948469029052098]
	TIME [epoch: 8.54 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1094921238323818		[learning rate: 2.226e-05]
	Learning Rate: 2.22601e-05
	LOSS [training: 0.1094921238323818 | validation: 0.09283823648568318]
	TIME [epoch: 8.52 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10604319744096798		[learning rate: 2.2179e-05]
	Learning Rate: 2.21793e-05
	LOSS [training: 0.10604319744096798 | validation: 0.11444609956020782]
	TIME [epoch: 8.53 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10618977385448372		[learning rate: 2.2099e-05]
	Learning Rate: 2.20988e-05
	LOSS [training: 0.10618977385448372 | validation: 0.10624313720988243]
	TIME [epoch: 8.53 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10978559172098264		[learning rate: 2.2019e-05]
	Learning Rate: 2.20186e-05
	LOSS [training: 0.10978559172098264 | validation: 0.09773184205137189]
	TIME [epoch: 8.55 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11316476382856214		[learning rate: 2.1939e-05]
	Learning Rate: 2.19387e-05
	LOSS [training: 0.11316476382856214 | validation: 0.09944135596655548]
	TIME [epoch: 8.52 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11647175397193243		[learning rate: 2.1859e-05]
	Learning Rate: 2.18591e-05
	LOSS [training: 0.11647175397193243 | validation: 0.10451032795441662]
	TIME [epoch: 8.53 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10550544366890018		[learning rate: 2.178e-05]
	Learning Rate: 2.17797e-05
	LOSS [training: 0.10550544366890018 | validation: 0.09353976028732774]
	TIME [epoch: 8.53 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11106945997099024		[learning rate: 2.1701e-05]
	Learning Rate: 2.17007e-05
	LOSS [training: 0.11106945997099024 | validation: 0.10623011246849332]
	TIME [epoch: 8.54 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10683090761107265		[learning rate: 2.1622e-05]
	Learning Rate: 2.16219e-05
	LOSS [training: 0.10683090761107265 | validation: 0.10459873290664709]
	TIME [epoch: 8.53 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10577027075162607		[learning rate: 2.1543e-05]
	Learning Rate: 2.15435e-05
	LOSS [training: 0.10577027075162607 | validation: 0.09626431136240565]
	TIME [epoch: 8.53 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10739556203772146		[learning rate: 2.1465e-05]
	Learning Rate: 2.14653e-05
	LOSS [training: 0.10739556203772146 | validation: 0.10981340467001666]
	TIME [epoch: 8.52 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10835964652847366		[learning rate: 2.1387e-05]
	Learning Rate: 2.13874e-05
	LOSS [training: 0.10835964652847366 | validation: 0.10522900400935786]
	TIME [epoch: 8.55 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1074122448118839		[learning rate: 2.131e-05]
	Learning Rate: 2.13098e-05
	LOSS [training: 0.1074122448118839 | validation: 0.10404356573051625]
	TIME [epoch: 8.53 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1028781371920916		[learning rate: 2.1232e-05]
	Learning Rate: 2.12325e-05
	LOSS [training: 0.1028781371920916 | validation: 0.1053192341891844]
	TIME [epoch: 8.52 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10503812710834375		[learning rate: 2.1155e-05]
	Learning Rate: 2.11554e-05
	LOSS [training: 0.10503812710834375 | validation: 0.10181560912942786]
	TIME [epoch: 8.52 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1053065124590686		[learning rate: 2.1079e-05]
	Learning Rate: 2.10786e-05
	LOSS [training: 0.1053065124590686 | validation: 0.09843688090570685]
	TIME [epoch: 8.54 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10509570817374306		[learning rate: 2.1002e-05]
	Learning Rate: 2.10021e-05
	LOSS [training: 0.10509570817374306 | validation: 0.09956653463061022]
	TIME [epoch: 8.52 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10634724800425696		[learning rate: 2.0926e-05]
	Learning Rate: 2.09259e-05
	LOSS [training: 0.10634724800425696 | validation: 0.10374018568898746]
	TIME [epoch: 8.52 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10722087967386192		[learning rate: 2.085e-05]
	Learning Rate: 2.085e-05
	LOSS [training: 0.10722087967386192 | validation: 0.10756310582589607]
	TIME [epoch: 8.52 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10721109929915504		[learning rate: 2.0774e-05]
	Learning Rate: 2.07743e-05
	LOSS [training: 0.10721109929915504 | validation: 0.09628463731821264]
	TIME [epoch: 8.53 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10797415640772265		[learning rate: 2.0699e-05]
	Learning Rate: 2.06989e-05
	LOSS [training: 0.10797415640772265 | validation: 0.10671765524989776]
	TIME [epoch: 8.54 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1062751129612246		[learning rate: 2.0624e-05]
	Learning Rate: 2.06238e-05
	LOSS [training: 0.1062751129612246 | validation: 0.10231430411759015]
	TIME [epoch: 8.52 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11380278530738774		[learning rate: 2.0549e-05]
	Learning Rate: 2.05489e-05
	LOSS [training: 0.11380278530738774 | validation: 0.10420364512969402]
	TIME [epoch: 8.53 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10858196170052829		[learning rate: 2.0474e-05]
	Learning Rate: 2.04744e-05
	LOSS [training: 0.10858196170052829 | validation: 0.10566142281860355]
	TIME [epoch: 8.53 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11078104389329368		[learning rate: 2.04e-05]
	Learning Rate: 2.04001e-05
	LOSS [training: 0.11078104389329368 | validation: 0.11349129565997611]
	TIME [epoch: 8.55 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1044121669232563		[learning rate: 2.0326e-05]
	Learning Rate: 2.0326e-05
	LOSS [training: 0.1044121669232563 | validation: 0.10150528558697824]
	TIME [epoch: 8.53 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10405745414014737		[learning rate: 2.0252e-05]
	Learning Rate: 2.02523e-05
	LOSS [training: 0.10405745414014737 | validation: 0.09694700173153378]
	TIME [epoch: 8.53 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11069493276312281		[learning rate: 2.0179e-05]
	Learning Rate: 2.01788e-05
	LOSS [training: 0.11069493276312281 | validation: 0.098666756359375]
	TIME [epoch: 8.53 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11062736561561078		[learning rate: 2.0106e-05]
	Learning Rate: 2.01055e-05
	LOSS [training: 0.11062736561561078 | validation: 0.10798544943687866]
	TIME [epoch: 8.55 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10305317880623266		[learning rate: 2.0033e-05]
	Learning Rate: 2.00326e-05
	LOSS [training: 0.10305317880623266 | validation: 0.10022972553543166]
	TIME [epoch: 8.53 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10813630967007111		[learning rate: 1.996e-05]
	Learning Rate: 1.99599e-05
	LOSS [training: 0.10813630967007111 | validation: 0.09774878321557817]
	TIME [epoch: 8.52 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11141799142167401		[learning rate: 1.9887e-05]
	Learning Rate: 1.98874e-05
	LOSS [training: 0.11141799142167401 | validation: 0.11085054215150428]
	TIME [epoch: 8.52 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.107163265602685		[learning rate: 1.9815e-05]
	Learning Rate: 1.98153e-05
	LOSS [training: 0.107163265602685 | validation: 0.1009533901655209]
	TIME [epoch: 8.55 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10148770118803116		[learning rate: 1.9743e-05]
	Learning Rate: 1.97434e-05
	LOSS [training: 0.10148770118803116 | validation: 0.10020252683958945]
	TIME [epoch: 8.53 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10682186668898068		[learning rate: 1.9672e-05]
	Learning Rate: 1.96717e-05
	LOSS [training: 0.10682186668898068 | validation: 0.10749158727996111]
	TIME [epoch: 8.52 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.107950475960058		[learning rate: 1.96e-05]
	Learning Rate: 1.96003e-05
	LOSS [training: 0.107950475960058 | validation: 0.09991684924500843]
	TIME [epoch: 8.52 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11000904390317504		[learning rate: 1.9529e-05]
	Learning Rate: 1.95292e-05
	LOSS [training: 0.11000904390317504 | validation: 0.09680910060542985]
	TIME [epoch: 8.54 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10591632607906307		[learning rate: 1.9458e-05]
	Learning Rate: 1.94583e-05
	LOSS [training: 0.10591632607906307 | validation: 0.10554849204030652]
	TIME [epoch: 8.53 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10396430896321521		[learning rate: 1.9388e-05]
	Learning Rate: 1.93877e-05
	LOSS [training: 0.10396430896321521 | validation: 0.0982381811784074]
	TIME [epoch: 8.53 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10560502941755703		[learning rate: 1.9317e-05]
	Learning Rate: 1.93173e-05
	LOSS [training: 0.10560502941755703 | validation: 0.10259809601583794]
	TIME [epoch: 8.52 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10881720215834148		[learning rate: 1.9247e-05]
	Learning Rate: 1.92472e-05
	LOSS [training: 0.10881720215834148 | validation: 0.09927495069021344]
	TIME [epoch: 8.53 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11121701717470063		[learning rate: 1.9177e-05]
	Learning Rate: 1.91774e-05
	LOSS [training: 0.11121701717470063 | validation: 0.10031231577898142]
	TIME [epoch: 8.55 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10890761123266786		[learning rate: 1.9108e-05]
	Learning Rate: 1.91078e-05
	LOSS [training: 0.10890761123266786 | validation: 0.09545123141412541]
	TIME [epoch: 8.53 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10765932531164943		[learning rate: 1.9038e-05]
	Learning Rate: 1.90384e-05
	LOSS [training: 0.10765932531164943 | validation: 0.10682779043176654]
	TIME [epoch: 8.52 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10795543940289731		[learning rate: 1.8969e-05]
	Learning Rate: 1.89694e-05
	LOSS [training: 0.10795543940289731 | validation: 0.10184216194609709]
	TIME [epoch: 8.52 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10794432345848586		[learning rate: 1.8901e-05]
	Learning Rate: 1.89005e-05
	LOSS [training: 0.10794432345848586 | validation: 0.10479871554444035]
	TIME [epoch: 8.55 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10739510655917986		[learning rate: 1.8832e-05]
	Learning Rate: 1.88319e-05
	LOSS [training: 0.10739510655917986 | validation: 0.10755041939871704]
	TIME [epoch: 8.53 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.106360210439353		[learning rate: 1.8764e-05]
	Learning Rate: 1.87636e-05
	LOSS [training: 0.106360210439353 | validation: 0.0941012241577561]
	TIME [epoch: 8.52 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10634135690864624		[learning rate: 1.8695e-05]
	Learning Rate: 1.86955e-05
	LOSS [training: 0.10634135690864624 | validation: 0.10539961828983552]
	TIME [epoch: 8.53 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10214645262546207		[learning rate: 1.8628e-05]
	Learning Rate: 1.86276e-05
	LOSS [training: 0.10214645262546207 | validation: 0.10582524746708687]
	TIME [epoch: 8.55 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10900057715110072		[learning rate: 1.856e-05]
	Learning Rate: 1.856e-05
	LOSS [training: 0.10900057715110072 | validation: 0.10214745464143583]
	TIME [epoch: 8.52 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10712240332588471		[learning rate: 1.8493e-05]
	Learning Rate: 1.84927e-05
	LOSS [training: 0.10712240332588471 | validation: 0.09496799396061573]
	TIME [epoch: 8.52 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10888226384069125		[learning rate: 1.8426e-05]
	Learning Rate: 1.84256e-05
	LOSS [training: 0.10888226384069125 | validation: 0.10754146775066792]
	TIME [epoch: 8.52 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10100204492697447		[learning rate: 1.8359e-05]
	Learning Rate: 1.83587e-05
	LOSS [training: 0.10100204492697447 | validation: 0.09878825383758735]
	TIME [epoch: 8.54 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10605510669472029		[learning rate: 1.8292e-05]
	Learning Rate: 1.82921e-05
	LOSS [training: 0.10605510669472029 | validation: 0.10254182660119815]
	TIME [epoch: 8.53 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10395301381295183		[learning rate: 1.8226e-05]
	Learning Rate: 1.82257e-05
	LOSS [training: 0.10395301381295183 | validation: 0.09496567310984683]
	TIME [epoch: 8.52 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10741877768216776		[learning rate: 1.816e-05]
	Learning Rate: 1.81596e-05
	LOSS [training: 0.10741877768216776 | validation: 0.10572048337417819]
	TIME [epoch: 8.52 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10623996252192033		[learning rate: 1.8094e-05]
	Learning Rate: 1.80937e-05
	LOSS [training: 0.10623996252192033 | validation: 0.10058281096606128]
	TIME [epoch: 8.53 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10546416059003771		[learning rate: 1.8028e-05]
	Learning Rate: 1.8028e-05
	LOSS [training: 0.10546416059003771 | validation: 0.08993026302513697]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_1838.pth
	Model improved!!!
EPOCH 1839/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10642400248781894		[learning rate: 1.7963e-05]
	Learning Rate: 1.79626e-05
	LOSS [training: 0.10642400248781894 | validation: 0.0941127382994503]
	TIME [epoch: 8.53 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1105285201735002		[learning rate: 1.7897e-05]
	Learning Rate: 1.78974e-05
	LOSS [training: 0.1105285201735002 | validation: 0.10341587687154968]
	TIME [epoch: 8.52 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10409237647834375		[learning rate: 1.7832e-05]
	Learning Rate: 1.78324e-05
	LOSS [training: 0.10409237647834375 | validation: 0.10316887087979257]
	TIME [epoch: 8.52 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10898289526354046		[learning rate: 1.7768e-05]
	Learning Rate: 1.77677e-05
	LOSS [training: 0.10898289526354046 | validation: 0.09176600564025528]
	TIME [epoch: 8.54 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10887365562821771		[learning rate: 1.7703e-05]
	Learning Rate: 1.77032e-05
	LOSS [training: 0.10887365562821771 | validation: 0.09287172476739036]
	TIME [epoch: 8.52 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11162482440735504		[learning rate: 1.7639e-05]
	Learning Rate: 1.7639e-05
	LOSS [training: 0.11162482440735504 | validation: 0.11520076524090525]
	TIME [epoch: 8.52 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10896359330624769		[learning rate: 1.7575e-05]
	Learning Rate: 1.7575e-05
	LOSS [training: 0.10896359330624769 | validation: 0.10605836062000701]
	TIME [epoch: 8.52 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11032625008088123		[learning rate: 1.7511e-05]
	Learning Rate: 1.75112e-05
	LOSS [training: 0.11032625008088123 | validation: 0.10301850974276261]
	TIME [epoch: 8.54 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10699346859415286		[learning rate: 1.7448e-05]
	Learning Rate: 1.74476e-05
	LOSS [training: 0.10699346859415286 | validation: 0.10360630472530477]
	TIME [epoch: 8.52 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11064613714254934		[learning rate: 1.7384e-05]
	Learning Rate: 1.73843e-05
	LOSS [training: 0.11064613714254934 | validation: 0.1013074343362489]
	TIME [epoch: 8.52 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10803628249864936		[learning rate: 1.7321e-05]
	Learning Rate: 1.73212e-05
	LOSS [training: 0.10803628249864936 | validation: 0.1057249281143422]
	TIME [epoch: 8.52 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10800604773961417		[learning rate: 1.7258e-05]
	Learning Rate: 1.72584e-05
	LOSS [training: 0.10800604773961417 | validation: 0.10398874861601709]
	TIME [epoch: 8.54 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11054687363484299		[learning rate: 1.7196e-05]
	Learning Rate: 1.71957e-05
	LOSS [training: 0.11054687363484299 | validation: 0.0997634992904209]
	TIME [epoch: 8.52 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10721792594606112		[learning rate: 1.7133e-05]
	Learning Rate: 1.71333e-05
	LOSS [training: 0.10721792594606112 | validation: 0.09896309759293707]
	TIME [epoch: 8.52 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10812192106923169		[learning rate: 1.7071e-05]
	Learning Rate: 1.70712e-05
	LOSS [training: 0.10812192106923169 | validation: 0.09480489587026647]
	TIME [epoch: 8.52 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10820133057119956		[learning rate: 1.7009e-05]
	Learning Rate: 1.70092e-05
	LOSS [training: 0.10820133057119956 | validation: 0.1036203536805257]
	TIME [epoch: 8.54 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10744293336398818		[learning rate: 1.6947e-05]
	Learning Rate: 1.69475e-05
	LOSS [training: 0.10744293336398818 | validation: 0.11174851492578544]
	TIME [epoch: 8.53 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10863716869499469		[learning rate: 1.6886e-05]
	Learning Rate: 1.6886e-05
	LOSS [training: 0.10863716869499469 | validation: 0.09856808112934867]
	TIME [epoch: 8.52 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10644891204026292		[learning rate: 1.6825e-05]
	Learning Rate: 1.68247e-05
	LOSS [training: 0.10644891204026292 | validation: 0.09963704161924232]
	TIME [epoch: 8.52 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10556181157197979		[learning rate: 1.6764e-05]
	Learning Rate: 1.67636e-05
	LOSS [training: 0.10556181157197979 | validation: 0.10378151230245981]
	TIME [epoch: 8.53 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10353422053369114		[learning rate: 1.6703e-05]
	Learning Rate: 1.67028e-05
	LOSS [training: 0.10353422053369114 | validation: 0.09814149113335247]
	TIME [epoch: 8.54 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10696099352805274		[learning rate: 1.6642e-05]
	Learning Rate: 1.66422e-05
	LOSS [training: 0.10696099352805274 | validation: 0.10185527857003361]
	TIME [epoch: 8.52 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11060474398023608		[learning rate: 1.6582e-05]
	Learning Rate: 1.65818e-05
	LOSS [training: 0.11060474398023608 | validation: 0.0988840948399893]
	TIME [epoch: 8.52 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10413536194135609		[learning rate: 1.6522e-05]
	Learning Rate: 1.65216e-05
	LOSS [training: 0.10413536194135609 | validation: 0.102144130336211]
	TIME [epoch: 8.52 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1052257080957264		[learning rate: 1.6462e-05]
	Learning Rate: 1.64617e-05
	LOSS [training: 0.1052257080957264 | validation: 0.09345565273580282]
	TIME [epoch: 8.55 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10644689652990127		[learning rate: 1.6402e-05]
	Learning Rate: 1.64019e-05
	LOSS [training: 0.10644689652990127 | validation: 0.10352290024059832]
	TIME [epoch: 8.52 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10878823785830724		[learning rate: 1.6342e-05]
	Learning Rate: 1.63424e-05
	LOSS [training: 0.10878823785830724 | validation: 0.09824577511026758]
	TIME [epoch: 8.52 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10682464301054355		[learning rate: 1.6283e-05]
	Learning Rate: 1.62831e-05
	LOSS [training: 0.10682464301054355 | validation: 0.0972358332273606]
	TIME [epoch: 8.52 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10506223842268833		[learning rate: 1.6224e-05]
	Learning Rate: 1.6224e-05
	LOSS [training: 0.10506223842268833 | validation: 0.09316255707608137]
	TIME [epoch: 8.54 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10716418982408224		[learning rate: 1.6165e-05]
	Learning Rate: 1.61651e-05
	LOSS [training: 0.10716418982408224 | validation: 0.10310468826152429]
	TIME [epoch: 8.52 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10788332922917383		[learning rate: 1.6106e-05]
	Learning Rate: 1.61065e-05
	LOSS [training: 0.10788332922917383 | validation: 0.09474380048433292]
	TIME [epoch: 8.52 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10551942221076852		[learning rate: 1.6048e-05]
	Learning Rate: 1.6048e-05
	LOSS [training: 0.10551942221076852 | validation: 0.10232498598336587]
	TIME [epoch: 8.52 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10837469354782996		[learning rate: 1.599e-05]
	Learning Rate: 1.59898e-05
	LOSS [training: 0.10837469354782996 | validation: 0.1038512313800323]
	TIME [epoch: 8.54 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10667008921034327		[learning rate: 1.5932e-05]
	Learning Rate: 1.59317e-05
	LOSS [training: 0.10667008921034327 | validation: 0.10317900502725028]
	TIME [epoch: 8.53 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10648167255658507		[learning rate: 1.5874e-05]
	Learning Rate: 1.58739e-05
	LOSS [training: 0.10648167255658507 | validation: 0.11000234598704062]
	TIME [epoch: 8.52 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10204205331773793		[learning rate: 1.5816e-05]
	Learning Rate: 1.58163e-05
	LOSS [training: 0.10204205331773793 | validation: 0.10315196176232724]
	TIME [epoch: 8.52 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11141086112531076		[learning rate: 1.5759e-05]
	Learning Rate: 1.57589e-05
	LOSS [training: 0.11141086112531076 | validation: 0.1134833740244996]
	TIME [epoch: 8.53 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10867688873551964		[learning rate: 1.5702e-05]
	Learning Rate: 1.57017e-05
	LOSS [training: 0.10867688873551964 | validation: 0.10301830879569158]
	TIME [epoch: 8.54 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10903825463408053		[learning rate: 1.5645e-05]
	Learning Rate: 1.56447e-05
	LOSS [training: 0.10903825463408053 | validation: 0.09096083353092466]
	TIME [epoch: 8.52 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10913966915975544		[learning rate: 1.5588e-05]
	Learning Rate: 1.5588e-05
	LOSS [training: 0.10913966915975544 | validation: 0.09962389751979898]
	TIME [epoch: 8.52 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10805763867916889		[learning rate: 1.5531e-05]
	Learning Rate: 1.55314e-05
	LOSS [training: 0.10805763867916889 | validation: 0.08994628284772188]
	TIME [epoch: 8.52 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10538290583994414		[learning rate: 1.5475e-05]
	Learning Rate: 1.5475e-05
	LOSS [training: 0.10538290583994414 | validation: 0.10046102160951914]
	TIME [epoch: 8.54 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1088391847575108		[learning rate: 1.5419e-05]
	Learning Rate: 1.54189e-05
	LOSS [training: 0.1088391847575108 | validation: 0.10021345461237324]
	TIME [epoch: 8.52 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10136732787773169		[learning rate: 1.5363e-05]
	Learning Rate: 1.53629e-05
	LOSS [training: 0.10136732787773169 | validation: 0.10331277075688498]
	TIME [epoch: 8.52 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10572006873263787		[learning rate: 1.5307e-05]
	Learning Rate: 1.53072e-05
	LOSS [training: 0.10572006873263787 | validation: 0.10087174560415416]
	TIME [epoch: 8.52 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10585239539205804		[learning rate: 1.5252e-05]
	Learning Rate: 1.52516e-05
	LOSS [training: 0.10585239539205804 | validation: 0.08490930532126617]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_1884.pth
	Model improved!!!
EPOCH 1885/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10523067864790134		[learning rate: 1.5196e-05]
	Learning Rate: 1.51963e-05
	LOSS [training: 0.10523067864790134 | validation: 0.09460669183344406]
	TIME [epoch: 8.52 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1058599256851217		[learning rate: 1.5141e-05]
	Learning Rate: 1.51411e-05
	LOSS [training: 0.1058599256851217 | validation: 0.09509638989290346]
	TIME [epoch: 8.52 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10797059717899722		[learning rate: 1.5086e-05]
	Learning Rate: 1.50862e-05
	LOSS [training: 0.10797059717899722 | validation: 0.0935724437812442]
	TIME [epoch: 8.53 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10515777006267764		[learning rate: 1.5031e-05]
	Learning Rate: 1.50314e-05
	LOSS [training: 0.10515777006267764 | validation: 0.10366099339984843]
	TIME [epoch: 8.55 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10596790357352374		[learning rate: 1.4977e-05]
	Learning Rate: 1.49769e-05
	LOSS [training: 0.10596790357352374 | validation: 0.09557162996993118]
	TIME [epoch: 8.53 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10741329221640401		[learning rate: 1.4923e-05]
	Learning Rate: 1.49225e-05
	LOSS [training: 0.10741329221640401 | validation: 0.10381356733713215]
	TIME [epoch: 8.52 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10254664369030655		[learning rate: 1.4868e-05]
	Learning Rate: 1.48684e-05
	LOSS [training: 0.10254664369030655 | validation: 0.1019147949118836]
	TIME [epoch: 8.52 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1087238882658101		[learning rate: 1.4814e-05]
	Learning Rate: 1.48144e-05
	LOSS [training: 0.1087238882658101 | validation: 0.09275579862969598]
	TIME [epoch: 8.54 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10617267880499859		[learning rate: 1.4761e-05]
	Learning Rate: 1.47606e-05
	LOSS [training: 0.10617267880499859 | validation: 0.09322954080984983]
	TIME [epoch: 8.54 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10612752169683157		[learning rate: 1.4707e-05]
	Learning Rate: 1.47071e-05
	LOSS [training: 0.10612752169683157 | validation: 0.09746429459705147]
	TIME [epoch: 8.53 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10660239470985974		[learning rate: 1.4654e-05]
	Learning Rate: 1.46537e-05
	LOSS [training: 0.10660239470985974 | validation: 0.10107164184557693]
	TIME [epoch: 8.53 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11270371875885268		[learning rate: 1.4601e-05]
	Learning Rate: 1.46005e-05
	LOSS [training: 0.11270371875885268 | validation: 0.09304076073388877]
	TIME [epoch: 8.53 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10938830050785502		[learning rate: 1.4548e-05]
	Learning Rate: 1.45475e-05
	LOSS [training: 0.10938830050785502 | validation: 0.10680662326308123]
	TIME [epoch: 8.55 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10830724853159868		[learning rate: 1.4495e-05]
	Learning Rate: 1.44947e-05
	LOSS [training: 0.10830724853159868 | validation: 0.10365355643126072]
	TIME [epoch: 8.52 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10706085115611712		[learning rate: 1.4442e-05]
	Learning Rate: 1.44421e-05
	LOSS [training: 0.10706085115611712 | validation: 0.10344481999223773]
	TIME [epoch: 8.53 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10235764219388907		[learning rate: 1.439e-05]
	Learning Rate: 1.43897e-05
	LOSS [training: 0.10235764219388907 | validation: 0.10234891965412765]
	TIME [epoch: 8.53 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10903646707800563		[learning rate: 1.4338e-05]
	Learning Rate: 1.43375e-05
	LOSS [training: 0.10903646707800563 | validation: 0.10706704706769352]
	TIME [epoch: 8.55 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11010651980695044		[learning rate: 1.4285e-05]
	Learning Rate: 1.42855e-05
	LOSS [training: 0.11010651980695044 | validation: 0.09821515652665158]
	TIME [epoch: 8.53 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10845744846511396		[learning rate: 1.4234e-05]
	Learning Rate: 1.42336e-05
	LOSS [training: 0.10845744846511396 | validation: 0.10860547002044282]
	TIME [epoch: 8.52 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10989678798934968		[learning rate: 1.4182e-05]
	Learning Rate: 1.4182e-05
	LOSS [training: 0.10989678798934968 | validation: 0.09229730556877934]
	TIME [epoch: 8.52 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10520806230640198		[learning rate: 1.4131e-05]
	Learning Rate: 1.41305e-05
	LOSS [training: 0.10520806230640198 | validation: 0.10670859869203539]
	TIME [epoch: 8.54 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10338182529182711		[learning rate: 1.4079e-05]
	Learning Rate: 1.40792e-05
	LOSS [training: 0.10338182529182711 | validation: 0.11138781978932069]
	TIME [epoch: 8.53 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10711234628755831		[learning rate: 1.4028e-05]
	Learning Rate: 1.40281e-05
	LOSS [training: 0.10711234628755831 | validation: 0.10704231895686415]
	TIME [epoch: 8.52 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10736964071807742		[learning rate: 1.3977e-05]
	Learning Rate: 1.39772e-05
	LOSS [training: 0.10736964071807742 | validation: 0.10498101125110998]
	TIME [epoch: 8.52 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.105893582541948		[learning rate: 1.3927e-05]
	Learning Rate: 1.39265e-05
	LOSS [training: 0.105893582541948 | validation: 0.10284160307152446]
	TIME [epoch: 8.55 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10540983670427358		[learning rate: 1.3876e-05]
	Learning Rate: 1.3876e-05
	LOSS [training: 0.10540983670427358 | validation: 0.10166275702690719]
	TIME [epoch: 8.53 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10672242438770133		[learning rate: 1.3826e-05]
	Learning Rate: 1.38256e-05
	LOSS [training: 0.10672242438770133 | validation: 0.09561633915380524]
	TIME [epoch: 8.52 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10365743874711061		[learning rate: 1.3775e-05]
	Learning Rate: 1.37754e-05
	LOSS [training: 0.10365743874711061 | validation: 0.0973026588309659]
	TIME [epoch: 8.52 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10953864409098571		[learning rate: 1.3725e-05]
	Learning Rate: 1.37254e-05
	LOSS [training: 0.10953864409098571 | validation: 0.10196194193322848]
	TIME [epoch: 8.54 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10831651154660818		[learning rate: 1.3676e-05]
	Learning Rate: 1.36756e-05
	LOSS [training: 0.10831651154660818 | validation: 0.10525107521665183]
	TIME [epoch: 8.54 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10389449318208348		[learning rate: 1.3626e-05]
	Learning Rate: 1.3626e-05
	LOSS [training: 0.10389449318208348 | validation: 0.09749100091735904]
	TIME [epoch: 8.53 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10522554954667204		[learning rate: 1.3577e-05]
	Learning Rate: 1.35766e-05
	LOSS [training: 0.10522554954667204 | validation: 0.11355054238236642]
	TIME [epoch: 8.53 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10233723734724857		[learning rate: 1.3527e-05]
	Learning Rate: 1.35273e-05
	LOSS [training: 0.10233723734724857 | validation: 0.09953362560849002]
	TIME [epoch: 8.53 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10427896304730322		[learning rate: 1.3478e-05]
	Learning Rate: 1.34782e-05
	LOSS [training: 0.10427896304730322 | validation: 0.09943933369922789]
	TIME [epoch: 8.55 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10654668390512696		[learning rate: 1.3429e-05]
	Learning Rate: 1.34293e-05
	LOSS [training: 0.10654668390512696 | validation: 0.09788399603300832]
	TIME [epoch: 8.53 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10512212023982906		[learning rate: 1.3381e-05]
	Learning Rate: 1.33805e-05
	LOSS [training: 0.10512212023982906 | validation: 0.09046174929227592]
	TIME [epoch: 8.53 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10654574181048493		[learning rate: 1.3332e-05]
	Learning Rate: 1.3332e-05
	LOSS [training: 0.10654574181048493 | validation: 0.10500450439021135]
	TIME [epoch: 8.53 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10765050942427608		[learning rate: 1.3284e-05]
	Learning Rate: 1.32836e-05
	LOSS [training: 0.10765050942427608 | validation: 0.09771572075398666]
	TIME [epoch: 8.55 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11076034547076781		[learning rate: 1.3235e-05]
	Learning Rate: 1.32354e-05
	LOSS [training: 0.11076034547076781 | validation: 0.09673457919313203]
	TIME [epoch: 8.53 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1082747384584292		[learning rate: 1.3187e-05]
	Learning Rate: 1.31874e-05
	LOSS [training: 0.1082747384584292 | validation: 0.09480625354784815]
	TIME [epoch: 8.53 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10814722191039328		[learning rate: 1.314e-05]
	Learning Rate: 1.31395e-05
	LOSS [training: 0.10814722191039328 | validation: 0.09989295593450354]
	TIME [epoch: 8.54 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10208650039466138		[learning rate: 1.3092e-05]
	Learning Rate: 1.30918e-05
	LOSS [training: 0.10208650039466138 | validation: 0.1082492380200083]
	TIME [epoch: 8.55 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10815355939518338		[learning rate: 1.3044e-05]
	Learning Rate: 1.30443e-05
	LOSS [training: 0.10815355939518338 | validation: 0.10030468942026008]
	TIME [epoch: 8.53 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11042275712572076		[learning rate: 1.2997e-05]
	Learning Rate: 1.2997e-05
	LOSS [training: 0.11042275712572076 | validation: 0.09066160703197639]
	TIME [epoch: 8.53 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10539114284649345		[learning rate: 1.295e-05]
	Learning Rate: 1.29498e-05
	LOSS [training: 0.10539114284649345 | validation: 0.09427052568110031]
	TIME [epoch: 8.53 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10674616640548222		[learning rate: 1.2903e-05]
	Learning Rate: 1.29028e-05
	LOSS [training: 0.10674616640548222 | validation: 0.09826744749278793]
	TIME [epoch: 8.54 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10500152908301046		[learning rate: 1.2856e-05]
	Learning Rate: 1.2856e-05
	LOSS [training: 0.10500152908301046 | validation: 0.09963417997741615]
	TIME [epoch: 8.54 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10832162604450804		[learning rate: 1.2809e-05]
	Learning Rate: 1.28093e-05
	LOSS [training: 0.10832162604450804 | validation: 0.09724460880895622]
	TIME [epoch: 8.54 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10454519665981601		[learning rate: 1.2763e-05]
	Learning Rate: 1.27628e-05
	LOSS [training: 0.10454519665981601 | validation: 0.10134747378016215]
	TIME [epoch: 8.53 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10415056909944109		[learning rate: 1.2717e-05]
	Learning Rate: 1.27165e-05
	LOSS [training: 0.10415056909944109 | validation: 0.10516578224940287]
	TIME [epoch: 8.54 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.104764511293211		[learning rate: 1.267e-05]
	Learning Rate: 1.26704e-05
	LOSS [training: 0.104764511293211 | validation: 0.10521329199939261]
	TIME [epoch: 8.55 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1092892742800278		[learning rate: 1.2624e-05]
	Learning Rate: 1.26244e-05
	LOSS [training: 0.1092892742800278 | validation: 0.09109423784841716]
	TIME [epoch: 8.53 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10898943087652162		[learning rate: 1.2579e-05]
	Learning Rate: 1.25786e-05
	LOSS [training: 0.10898943087652162 | validation: 0.10137019323562496]
	TIME [epoch: 8.53 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10123106334280008		[learning rate: 1.2533e-05]
	Learning Rate: 1.25329e-05
	LOSS [training: 0.10123106334280008 | validation: 0.10142757606246985]
	TIME [epoch: 8.53 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1069395641326965		[learning rate: 1.2487e-05]
	Learning Rate: 1.24874e-05
	LOSS [training: 0.1069395641326965 | validation: 0.10247734094771159]
	TIME [epoch: 8.55 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10815997214291553		[learning rate: 1.2442e-05]
	Learning Rate: 1.24421e-05
	LOSS [training: 0.10815997214291553 | validation: 0.10650033467629086]
	TIME [epoch: 8.53 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10950698484209684		[learning rate: 1.2397e-05]
	Learning Rate: 1.2397e-05
	LOSS [training: 0.10950698484209684 | validation: 0.10313544061881147]
	TIME [epoch: 8.53 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10754492625699327		[learning rate: 1.2352e-05]
	Learning Rate: 1.2352e-05
	LOSS [training: 0.10754492625699327 | validation: 0.09868529751697408]
	TIME [epoch: 8.53 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10413974540274051		[learning rate: 1.2307e-05]
	Learning Rate: 1.23072e-05
	LOSS [training: 0.10413974540274051 | validation: 0.09534691109234635]
	TIME [epoch: 8.55 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10540902733894617		[learning rate: 1.2263e-05]
	Learning Rate: 1.22625e-05
	LOSS [training: 0.10540902733894617 | validation: 0.10040234079977674]
	TIME [epoch: 8.53 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10907456867509421		[learning rate: 1.2218e-05]
	Learning Rate: 1.2218e-05
	LOSS [training: 0.10907456867509421 | validation: 0.1016887994906522]
	TIME [epoch: 8.53 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10594216430458123		[learning rate: 1.2174e-05]
	Learning Rate: 1.21737e-05
	LOSS [training: 0.10594216430458123 | validation: 0.09668345809013587]
	TIME [epoch: 8.52 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10447560531383364		[learning rate: 1.2129e-05]
	Learning Rate: 1.21295e-05
	LOSS [training: 0.10447560531383364 | validation: 0.09427734036179043]
	TIME [epoch: 8.54 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10499538846860246		[learning rate: 1.2085e-05]
	Learning Rate: 1.20855e-05
	LOSS [training: 0.10499538846860246 | validation: 0.09379213253680149]
	TIME [epoch: 8.52 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10729185436708293		[learning rate: 1.2042e-05]
	Learning Rate: 1.20416e-05
	LOSS [training: 0.10729185436708293 | validation: 0.10035156026992034]
	TIME [epoch: 8.52 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10663158010632642		[learning rate: 1.1998e-05]
	Learning Rate: 1.19979e-05
	LOSS [training: 0.10663158010632642 | validation: 0.09597963432611786]
	TIME [epoch: 8.51 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1050644988924477		[learning rate: 1.1954e-05]
	Learning Rate: 1.19544e-05
	LOSS [training: 0.1050644988924477 | validation: 0.10291516584543894]
	TIME [epoch: 8.53 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10901102876793708		[learning rate: 1.1911e-05]
	Learning Rate: 1.1911e-05
	LOSS [training: 0.10901102876793708 | validation: 0.09593535071908822]
	TIME [epoch: 8.52 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10575591745911536		[learning rate: 1.1868e-05]
	Learning Rate: 1.18678e-05
	LOSS [training: 0.10575591745911536 | validation: 0.1065889424487999]
	TIME [epoch: 8.55 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1076656574847881		[learning rate: 1.1825e-05]
	Learning Rate: 1.18247e-05
	LOSS [training: 0.1076656574847881 | validation: 0.11246668238138689]
	TIME [epoch: 8.52 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10862966682119782		[learning rate: 1.1782e-05]
	Learning Rate: 1.17818e-05
	LOSS [training: 0.10862966682119782 | validation: 0.09931471861443583]
	TIME [epoch: 8.53 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10617363601385363		[learning rate: 1.1739e-05]
	Learning Rate: 1.1739e-05
	LOSS [training: 0.10617363601385363 | validation: 0.10502097988207817]
	TIME [epoch: 8.53 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10480616610023918		[learning rate: 1.1696e-05]
	Learning Rate: 1.16964e-05
	LOSS [training: 0.10480616610023918 | validation: 0.09185153853641832]
	TIME [epoch: 8.51 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10713564393328756		[learning rate: 1.1654e-05]
	Learning Rate: 1.1654e-05
	LOSS [training: 0.10713564393328756 | validation: 0.10134265484968809]
	TIME [epoch: 8.51 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10416603736027392		[learning rate: 1.1612e-05]
	Learning Rate: 1.16117e-05
	LOSS [training: 0.10416603736027392 | validation: 0.09785575381519435]
	TIME [epoch: 8.51 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10708571085592133		[learning rate: 1.157e-05]
	Learning Rate: 1.15695e-05
	LOSS [training: 0.10708571085592133 | validation: 0.10194114982302344]
	TIME [epoch: 8.53 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10760111858584309		[learning rate: 1.1528e-05]
	Learning Rate: 1.15275e-05
	LOSS [training: 0.10760111858584309 | validation: 0.09583131558372952]
	TIME [epoch: 8.51 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1064136713915865		[learning rate: 1.1486e-05]
	Learning Rate: 1.14857e-05
	LOSS [training: 0.1064136713915865 | validation: 0.09668339847332849]
	TIME [epoch: 8.51 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10614790361934807		[learning rate: 1.1444e-05]
	Learning Rate: 1.1444e-05
	LOSS [training: 0.10614790361934807 | validation: 0.10364679202035676]
	TIME [epoch: 8.51 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10782008692512739		[learning rate: 1.1402e-05]
	Learning Rate: 1.14025e-05
	LOSS [training: 0.10782008692512739 | validation: 0.0976891779542095]
	TIME [epoch: 8.53 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10624426459834388		[learning rate: 1.1361e-05]
	Learning Rate: 1.13611e-05
	LOSS [training: 0.10624426459834388 | validation: 0.09506052188503678]
	TIME [epoch: 8.52 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1089707796384419		[learning rate: 1.132e-05]
	Learning Rate: 1.13199e-05
	LOSS [training: 0.1089707796384419 | validation: 0.09652761258823322]
	TIME [epoch: 8.51 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10447133832878379		[learning rate: 1.1279e-05]
	Learning Rate: 1.12788e-05
	LOSS [training: 0.10447133832878379 | validation: 0.09764806225181638]
	TIME [epoch: 8.51 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10577437030083084		[learning rate: 1.1238e-05]
	Learning Rate: 1.12379e-05
	LOSS [training: 0.10577437030083084 | validation: 0.10229669389517015]
	TIME [epoch: 8.52 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10237717263964172		[learning rate: 1.1197e-05]
	Learning Rate: 1.11971e-05
	LOSS [training: 0.10237717263964172 | validation: 0.09739201319807742]
	TIME [epoch: 8.52 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10532351299822326		[learning rate: 1.1156e-05]
	Learning Rate: 1.11565e-05
	LOSS [training: 0.10532351299822326 | validation: 0.10135087277809593]
	TIME [epoch: 8.51 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10819399255717391		[learning rate: 1.1116e-05]
	Learning Rate: 1.1116e-05
	LOSS [training: 0.10819399255717391 | validation: 0.09796780397170857]
	TIME [epoch: 8.51 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10789837499705528		[learning rate: 1.1076e-05]
	Learning Rate: 1.10756e-05
	LOSS [training: 0.10789837499705528 | validation: 0.09867124204900123]
	TIME [epoch: 8.52 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10371286448687508		[learning rate: 1.1035e-05]
	Learning Rate: 1.10354e-05
	LOSS [training: 0.10371286448687508 | validation: 0.09662541787890455]
	TIME [epoch: 8.52 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10835910544398968		[learning rate: 1.0995e-05]
	Learning Rate: 1.09954e-05
	LOSS [training: 0.10835910544398968 | validation: 0.09144186076781091]
	TIME [epoch: 8.52 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10855090532833492		[learning rate: 1.0955e-05]
	Learning Rate: 1.09555e-05
	LOSS [training: 0.10855090532833492 | validation: 0.10028593386823578]
	TIME [epoch: 8.51 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10398213585352885		[learning rate: 1.0916e-05]
	Learning Rate: 1.09157e-05
	LOSS [training: 0.10398213585352885 | validation: 0.09373248989952743]
	TIME [epoch: 8.51 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10576761292871613		[learning rate: 1.0876e-05]
	Learning Rate: 1.08761e-05
	LOSS [training: 0.10576761292871613 | validation: 0.09748197537454714]
	TIME [epoch: 8.53 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10244276849019007		[learning rate: 1.0837e-05]
	Learning Rate: 1.08366e-05
	LOSS [training: 0.10244276849019007 | validation: 0.0834497291012959]
	TIME [epoch: 8.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study204/model_tr_study204_r0_20240219_233648/states/model_tr_study204_1978.pth
	Model improved!!!
EPOCH 1979/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10423614075807988		[learning rate: 1.0797e-05]
	Learning Rate: 1.07973e-05
	LOSS [training: 0.10423614075807988 | validation: 0.10463597438904979]
	TIME [epoch: 8.51 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11199930051193618		[learning rate: 1.0758e-05]
	Learning Rate: 1.07581e-05
	LOSS [training: 0.11199930051193618 | validation: 0.098167844519741]
	TIME [epoch: 8.5 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1070154855254887		[learning rate: 1.0719e-05]
	Learning Rate: 1.07191e-05
	LOSS [training: 0.1070154855254887 | validation: 0.10186302209520323]
	TIME [epoch: 8.53 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10529305287285526		[learning rate: 1.068e-05]
	Learning Rate: 1.06802e-05
	LOSS [training: 0.10529305287285526 | validation: 0.0999491749812725]
	TIME [epoch: 8.51 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1025610252515271		[learning rate: 1.0641e-05]
	Learning Rate: 1.06414e-05
	LOSS [training: 0.1025610252515271 | validation: 0.10362291587920947]
	TIME [epoch: 8.51 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10714044928512871		[learning rate: 1.0603e-05]
	Learning Rate: 1.06028e-05
	LOSS [training: 0.10714044928512871 | validation: 0.10362864050069079]
	TIME [epoch: 8.51 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10698230267735649		[learning rate: 1.0564e-05]
	Learning Rate: 1.05643e-05
	LOSS [training: 0.10698230267735649 | validation: 0.09847164585089992]
	TIME [epoch: 8.52 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11016959776979678		[learning rate: 1.0526e-05]
	Learning Rate: 1.0526e-05
	LOSS [training: 0.11016959776979678 | validation: 0.10857676143048026]
	TIME [epoch: 8.52 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10793326696240993		[learning rate: 1.0488e-05]
	Learning Rate: 1.04878e-05
	LOSS [training: 0.10793326696240993 | validation: 0.0969900219418959]
	TIME [epoch: 8.51 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11283090143282126		[learning rate: 1.045e-05]
	Learning Rate: 1.04497e-05
	LOSS [training: 0.11283090143282126 | validation: 0.10011608013133466]
	TIME [epoch: 8.52 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11139173263605842		[learning rate: 1.0412e-05]
	Learning Rate: 1.04118e-05
	LOSS [training: 0.11139173263605842 | validation: 0.09583635444224983]
	TIME [epoch: 8.53 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11149503512744689		[learning rate: 1.0374e-05]
	Learning Rate: 1.0374e-05
	LOSS [training: 0.11149503512744689 | validation: 0.10456641202297329]
	TIME [epoch: 8.52 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11836431087693859		[learning rate: 1.0336e-05]
	Learning Rate: 1.03364e-05
	LOSS [training: 0.11836431087693859 | validation: 0.09949091335177868]
	TIME [epoch: 8.52 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10974000871576775		[learning rate: 1.0299e-05]
	Learning Rate: 1.02989e-05
	LOSS [training: 0.10974000871576775 | validation: 0.10187732464447313]
	TIME [epoch: 8.51 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10607630076737733		[learning rate: 1.0261e-05]
	Learning Rate: 1.02615e-05
	LOSS [training: 0.10607630076737733 | validation: 0.097116755518763]
	TIME [epoch: 8.52 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10881915458393003		[learning rate: 1.0224e-05]
	Learning Rate: 1.02243e-05
	LOSS [training: 0.10881915458393003 | validation: 0.08843709392881371]
	TIME [epoch: 8.53 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.11123819166580813		[learning rate: 1.0187e-05]
	Learning Rate: 1.01872e-05
	LOSS [training: 0.11123819166580813 | validation: 0.0960603460058723]
	TIME [epoch: 8.51 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10910683343423302		[learning rate: 1.015e-05]
	Learning Rate: 1.01502e-05
	LOSS [training: 0.10910683343423302 | validation: 0.10427931019926635]
	TIME [epoch: 8.51 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10775307940961051		[learning rate: 1.0113e-05]
	Learning Rate: 1.01133e-05
	LOSS [training: 0.10775307940961051 | validation: 0.09170862074680577]
	TIME [epoch: 8.51 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10750818089087084		[learning rate: 1.0077e-05]
	Learning Rate: 1.00766e-05
	LOSS [training: 0.10750818089087084 | validation: 0.09216678441368097]
	TIME [epoch: 8.54 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.1102902164280313		[learning rate: 1.004e-05]
	Learning Rate: 1.00401e-05
	LOSS [training: 0.1102902164280313 | validation: 0.10285445774189714]
	TIME [epoch: 8.51 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.10881437144393584		[learning rate: 1.0004e-05]
	Learning Rate: 1.00036e-05
	LOSS [training: 0.10881437144393584 | validation: 0.10042987821821954]
	TIME [epoch: 8.52 sec]
Finished training in 17224.526 seconds.
