Args:
Namespace(name='model_tr_study201', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0', training_data='data/transition_rate_studies/tr_study201/tr_study201_training/r0', validation_data='data/transition_rate_studies/tr_study201/tr_study201_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2322745805

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 12.12860732425949		[learning rate: 0.01]
		[batch 20/20] avg loss: 10.806467049838293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.467537187048894 | validation: 9.804695347428318]
	TIME [epoch: 79 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.69789427434047		[learning rate: 0.01]
		[batch 20/20] avg loss: 9.989446428960283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.843670351650378 | validation: 10.458645693682014]
	TIME [epoch: 8.21 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.391720820412697		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.8976374741241155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.6446791472684055 | validation: 9.44836186282065]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.339538726724761		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.431788039019031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.385663382871895 | validation: 6.220197496797493]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.471084792344762		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.457861427367642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.464473109856202 | validation: 8.18575466462272]
	TIME [epoch: 8.22 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.317495138649062		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.278646003145315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.798070570897188 | validation: 4.008339774022487]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.151948923717902		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.793473011369366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.972710967543635 | validation: 4.541804512562667]
	TIME [epoch: 8.19 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.694150478301018		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.61968285617843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.656916667239724 | validation: 5.165069236390622]
	TIME [epoch: 8.19 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.4414254405969364		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.446285612852589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.943855526724763 | validation: 4.43853894931085]
	TIME [epoch: 8.21 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.408266942540234		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.175844949696109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.292055946118173 | validation: 4.8211621538376015]
	TIME [epoch: 8.24 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.615569954596469		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.372960545754831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.49426525017565 | validation: 5.990939788198067]
	TIME [epoch: 8.19 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.258593136321533		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.061538137919626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.160065637120579 | validation: 5.784819825397426]
	TIME [epoch: 8.19 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.192949917925057		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.219189018061696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.206069467993377 | validation: 3.659496063901598]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.701802935126189		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.433755277208735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.067779106167463 | validation: 6.412764687775118]
	TIME [epoch: 8.21 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.740852795512377		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.385773177647091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.563312986579734 | validation: 9.62247088896093]
	TIME [epoch: 8.19 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.995968687845926		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.067306542392119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.531637615119022 | validation: 4.833254120571237]
	TIME [epoch: 8.18 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.039906242128518		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.9370365852689178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9884714136987185 | validation: 3.6465561034815233]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.00514424979523		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.9803637969841885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.992754023389709 | validation: 4.39062084973028]
	TIME [epoch: 8.2 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.91316908795932		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.1865143896117365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0498417387855286 | validation: 3.830615498116912]
	TIME [epoch: 8.2 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.934916722215963		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.077500675994503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.006208699105233 | validation: 4.122786724145538]
	TIME [epoch: 8.18 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.917511714007643		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.772162031074059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8448368725408515 | validation: 4.713258458434655]
	TIME [epoch: 8.17 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.004817697265014		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.718463152963907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.861640425114461 | validation: 4.481259640022994]
	TIME [epoch: 8.18 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7260161910826843		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.7713064679951556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.74866132953892 | validation: 4.11013749083281]
	TIME [epoch: 8.24 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7343990323477527		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5141384628388153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.624268747593285 | validation: 3.561104702565564]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.456420896782538		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.2342702498108706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3453455732967043 | validation: 3.2817403960186393]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4034224400101083		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.078914869672415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7411686548412613 | validation: 4.7140600627631635]
	TIME [epoch: 8.19 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.594012560511684		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.343815246662981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4689139035873326 | validation: 3.830305489565009]
	TIME [epoch: 8.22 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2734104436713		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.2444418087078715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2589261261895857 | validation: 4.067505773563745]
	TIME [epoch: 8.19 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2136501553990775		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1116833482544592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.162666751826768 | validation: 4.407420108343398]
	TIME [epoch: 8.19 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7518242158664847		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.747619200473873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.749721708170179 | validation: 7.194184092634699]
	TIME [epoch: 8.21 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.201992802140101		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.561218731758294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.881605766949197 | validation: 2.8200175884693963]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.070825954033654		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.026465784480796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.048645869257226 | validation: 2.6002438167607886]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9096975418867586		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8881369809748243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8989172614307908 | validation: 2.9647052045902593]
	TIME [epoch: 8.19 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6803913486475133		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9735694011254377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8269803748864755 | validation: 2.715722114141963]
	TIME [epoch: 8.22 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.696911222578139		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7320201052371695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7144656639076543 | validation: 2.951934149390897]
	TIME [epoch: 8.19 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6465108016854777		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5604788422268117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6034948219561445 | validation: 2.737285903044273]
	TIME [epoch: 8.21 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4405843572806347		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6798694550093556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.560226906144995 | validation: 2.6508852275285215]
	TIME [epoch: 8.18 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.432893774358401		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5091831560337536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4710384651960773 | validation: 2.8008921051206364]
	TIME [epoch: 8.19 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.331687988533953		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2802902224180945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3059891054760233 | validation: 2.153812772767725]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.334499869941932		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.543182975647069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.438841422794501 | validation: 1.4763252990770739]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1370244586921983		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1489188223302227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.142971640511211 | validation: 3.0425914445694984]
	TIME [epoch: 8.18 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1594493266969144		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0307276425858714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0950884846413933 | validation: 2.420377979689361]
	TIME [epoch: 8.18 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.002383965198101		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.960861142805749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9816225540019254 | validation: 1.7336469791571802]
	TIME [epoch: 8.21 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9920594292132272		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8146093144735347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9033343718433808 | validation: 2.1451176344952074]
	TIME [epoch: 8.21 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8412250741891583		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.746138639328544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.793681856758851 | validation: 1.8799018389381517]
	TIME [epoch: 8.19 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7631518932231576		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7413844912637935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7522681922434753 | validation: 2.283951300648008]
	TIME [epoch: 8.21 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.179223222030485		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.025288218651645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1022557203410654 | validation: 3.1568916964970883]
	TIME [epoch: 8.2 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.096667599285403		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7781776393819186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.937422619333661 | validation: 1.952002856926144]
	TIME [epoch: 8.17 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7868389176770276		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.646214782799929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.716526850238478 | validation: 1.6897201891927622]
	TIME [epoch: 8.21 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7568653805127077		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5058549236201808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6313601520664442 | validation: 2.730403933371618]
	TIME [epoch: 8.17 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7543295894886684		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5304671409959028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6423983652422856 | validation: 2.0440832512306484]
	TIME [epoch: 8.18 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6983330426944199		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7379665565690534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7181497996317368 | validation: 1.7867676753770376]
	TIME [epoch: 8.18 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5435291597514857		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5382245335415274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5408768466465066 | validation: 1.95243443538124]
	TIME [epoch: 8.22 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6388796842688642		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5992336934939178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.619056688881391 | validation: 1.4091861140045472]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4532178481144378		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5804225079862686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5168201780503534 | validation: 1.6319135536117897]
	TIME [epoch: 8.2 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6325742187242862		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.408865008949794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5207196138370402 | validation: 1.8434566947001445]
	TIME [epoch: 8.19 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6098108331447034		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5532255369168666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.581518185030785 | validation: 1.7632663742942518]
	TIME [epoch: 8.24 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4808500084453526		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5631263999372869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5219882041913197 | validation: 1.482428120518313]
	TIME [epoch: 8.21 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.39028628382284		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2158833708358325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3030848273293367 | validation: 1.9918919301055422]
	TIME [epoch: 8.18 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5398953042495793		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.450953332000322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4954243181249507 | validation: 3.7128510620594097]
	TIME [epoch: 8.18 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8688961617029132		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5686917979513457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7187939798271294 | validation: 1.4812766615843689]
	TIME [epoch: 8.17 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4421717664115294		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5450562458340042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4936140061227667 | validation: 1.5900083234393936]
	TIME [epoch: 8.21 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4116780894041572		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.408362664098377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.410020376751267 | validation: 2.176465948146656]
	TIME [epoch: 8.18 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4282394820007065		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.362856035572039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3955477587863725 | validation: 1.2235225082246577]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3985354801552266		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5018407835203558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4501881318377912 | validation: 1.7626836387944167]
	TIME [epoch: 8.21 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3841635182669016		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.851100405217234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6176319617420678 | validation: 3.857467401656056]
	TIME [epoch: 8.21 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.353067222436698		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2462144114623688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.299640816949533 | validation: 3.0820681266595398]
	TIME [epoch: 8.2 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0782874866581564		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.31125768509089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1947725858745235 | validation: 3.2587815012767978]
	TIME [epoch: 8.21 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.164776898503066		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1512646605829975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1580207795430315 | validation: 3.2720869403081023]
	TIME [epoch: 8.2 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1149178703225946		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0584785586493823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0866982144859887 | validation: 2.851344131977811]
	TIME [epoch: 8.18 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1029125130723907		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.101355387782349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.10213395042737 | validation: 3.34004340630157]
	TIME [epoch: 8.21 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1061323439046147		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0729352370791165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0895337904918656 | validation: 2.5186000981475356]
	TIME [epoch: 8.17 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8391924974283174		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4612976124968546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6502450549625856 | validation: 2.2428221179224495]
	TIME [epoch: 8.18 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4925391102626362		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2861776445406827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3893583774016594 | validation: 1.893970544600141]
	TIME [epoch: 8.17 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4672597522665847		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3662256508588133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.416742701562699 | validation: 1.3576780300909166]
	TIME [epoch: 8.23 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3573305232586308		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7398211191258803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5485758211922556 | validation: 3.7316839053512623]
	TIME [epoch: 8.2 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7720139258658083		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4699795305370813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.620996728201445 | validation: 1.9893160361872302]
	TIME [epoch: 8.19 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4202442898048386		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4169075478683557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.418575918836597 | validation: 1.3915264508149499]
	TIME [epoch: 8.18 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1851237479489545		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9543389389829593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5697313434659568 | validation: 1.7054012111821821]
	TIME [epoch: 8.23 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.433779396416562		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4930262061176542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4634028012671076 | validation: 1.5931803217704634]
	TIME [epoch: 8.21 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3758202543614397		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.246383038735026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.311101646548233 | validation: 1.44451812338764]
	TIME [epoch: 8.18 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3695450695824292		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2984710389134024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3340080542479158 | validation: 1.5131037758846317]
	TIME [epoch: 8.18 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.217499276780532		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3709934377833513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2942463572819416 | validation: 1.5021653844656728]
	TIME [epoch: 8.19 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1878508158425451		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.271842565949854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2298466908961996 | validation: 1.3292985681485285]
	TIME [epoch: 8.21 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3460255413586828		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4849990534969435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.415512297427813 | validation: 2.1600630139430623]
	TIME [epoch: 8.2 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3526813445112844		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.336116148667816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3443987465895502 | validation: 1.4043232188527992]
	TIME [epoch: 8.2 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.325357153385813		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2234471305263668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2744021419560903 | validation: 1.881403431950601]
	TIME [epoch: 8.18 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2824216160107238		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3953416060159438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3388816110133337 | validation: 1.6990871665189156]
	TIME [epoch: 8.2 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6324023393149836		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3079792588925727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.470190799103778 | validation: 1.9803176776912537]
	TIME [epoch: 8.23 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3222658181885703		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1910349673623615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2566503927754658 | validation: 1.4969573399374396]
	TIME [epoch: 8.19 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.439911416036653		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3359966309231437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3879540234798986 | validation: 1.834069347340662]
	TIME [epoch: 8.18 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2801677125397408		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.223430937261872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2517993249008066 | validation: 1.32021460850837]
	TIME [epoch: 8.17 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1943171419221659		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4174644761122885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.305890809017227 | validation: 1.8402771219478269]
	TIME [epoch: 8.21 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.312985663668122		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1759369239664887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2444612938173054 | validation: 1.4717287362191636]
	TIME [epoch: 8.18 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1932340592220945		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3596435659384678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2764388125802812 | validation: 1.880897894642903]
	TIME [epoch: 8.19 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4352875969184065		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1998256147396211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3175566058290138 | validation: 1.8760861745452673]
	TIME [epoch: 8.21 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2042239310678584		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1806134676318116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1924186993498351 | validation: 1.7544028217335463]
	TIME [epoch: 8.21 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2014818299496233		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1739015235032106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.187691676726417 | validation: 1.270121579693652]
	TIME [epoch: 8.2 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1903579491573648		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1871157019578897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1887368255576274 | validation: 1.127500248692904]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2670238814172745		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1753386957434404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2211812885803577 | validation: 1.5169715619903954]
	TIME [epoch: 8.21 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2111161903451992		[learning rate: 0.0099837]
		[batch 20/20] avg loss: 1.3768277790542758		[learning rate: 0.0099655]
	Learning Rate: 0.00996552
	LOSS [training: 1.2939719846997377 | validation: 2.288328665930975]
	TIME [epoch: 8.17 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2646431159817049		[learning rate: 0.0099474]
		[batch 20/20] avg loss: 1.111013686162901		[learning rate: 0.0099294]
	Learning Rate: 0.00992935
	LOSS [training: 1.1878284010723028 | validation: 1.0369521132608128]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.060620661384021		[learning rate: 0.0099113]
		[batch 20/20] avg loss: 1.0202046695277258		[learning rate: 0.0098933]
	Learning Rate: 0.00989332
	LOSS [training: 1.0404126654558734 | validation: 1.0561855484124132]
	TIME [epoch: 8.18 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1444197566480485		[learning rate: 0.0098754]
		[batch 20/20] avg loss: 1.1510375974214813		[learning rate: 0.0098574]
	Learning Rate: 0.00985742
	LOSS [training: 1.1477286770347646 | validation: 1.4596661121684544]
	TIME [epoch: 8.17 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3575228607187664		[learning rate: 0.0098395]
		[batch 20/20] avg loss: 1.0226028766895292		[learning rate: 0.0098216]
	Learning Rate: 0.00982164
	LOSS [training: 1.1900628687041477 | validation: 1.0714755200793722]
	TIME [epoch: 8.18 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.099093015856837		[learning rate: 0.0098038]
		[batch 20/20] avg loss: 1.2467719420877057		[learning rate: 0.009786]
	Learning Rate: 0.009786
	LOSS [training: 1.1729324789722715 | validation: 0.6555316212145559]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2957258815726935		[learning rate: 0.0097682]
		[batch 20/20] avg loss: 1.0158716388424769		[learning rate: 0.0097505]
	Learning Rate: 0.00975049
	LOSS [training: 1.1557987602075852 | validation: 1.1884557571037146]
	TIME [epoch: 8.19 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1092633198912094		[learning rate: 0.0097328]
		[batch 20/20] avg loss: 1.0153048406300031		[learning rate: 0.0097151]
	Learning Rate: 0.0097151
	LOSS [training: 1.0622840802606064 | validation: 2.2309451505612223]
	TIME [epoch: 8.17 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.221748742509891		[learning rate: 0.0096975]
		[batch 20/20] avg loss: 0.9929185693996505		[learning rate: 0.0096798]
	Learning Rate: 0.00967984
	LOSS [training: 1.1073336559547706 | validation: 1.3392501967126098]
	TIME [epoch: 8.21 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1376844206400674		[learning rate: 0.0096623]
		[batch 20/20] avg loss: 1.0861637256205148		[learning rate: 0.0096447]
	Learning Rate: 0.00964472
	LOSS [training: 1.1119240731302908 | validation: 0.9742144483675896]
	TIME [epoch: 8.18 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2557934731900802		[learning rate: 0.0096272]
		[batch 20/20] avg loss: 0.9939025378944579		[learning rate: 0.0096097]
	Learning Rate: 0.00960972
	LOSS [training: 1.1248480055422692 | validation: 1.5090541193767266]
	TIME [epoch: 8.19 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.007845991598645		[learning rate: 0.0095923]
		[batch 20/20] avg loss: 0.841613723307477		[learning rate: 0.0095748]
	Learning Rate: 0.00957484
	LOSS [training: 0.924729857453061 | validation: 1.0167427829324962]
	TIME [epoch: 8.17 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3836379860819037		[learning rate: 0.0095575]
		[batch 20/20] avg loss: 1.0588530582940192		[learning rate: 0.0095401]
	Learning Rate: 0.00954009
	LOSS [training: 1.2212455221879615 | validation: 1.1606269141601746]
	TIME [epoch: 8.17 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9170779752236295		[learning rate: 0.0095228]
		[batch 20/20] avg loss: 1.1707405850548058		[learning rate: 0.0095055]
	Learning Rate: 0.00950547
	LOSS [training: 1.0439092801392176 | validation: 1.2227218961804742]
	TIME [epoch: 8.17 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0067040317594322		[learning rate: 0.0094882]
		[batch 20/20] avg loss: 1.159018680544079		[learning rate: 0.009471]
	Learning Rate: 0.00947098
	LOSS [training: 1.082861356151756 | validation: 1.261826669647479]
	TIME [epoch: 8.2 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9929405798816255		[learning rate: 0.0094538]
		[batch 20/20] avg loss: 0.9907311018647877		[learning rate: 0.0094366]
	Learning Rate: 0.0094366
	LOSS [training: 0.9918358408732066 | validation: 1.1084369077643774]
	TIME [epoch: 8.17 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8887466772613065		[learning rate: 0.0094195]
		[batch 20/20] avg loss: 0.9892305692853341		[learning rate: 0.0094024]
	Learning Rate: 0.00940236
	LOSS [training: 0.9389886232733202 | validation: 1.0142183913760083]
	TIME [epoch: 8.19 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9405041721568038		[learning rate: 0.0093853]
		[batch 20/20] avg loss: 1.1043094807533385		[learning rate: 0.0093682]
	Learning Rate: 0.00936824
	LOSS [training: 1.0224068264550712 | validation: 0.915341778142748]
	TIME [epoch: 8.19 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9724444254447857		[learning rate: 0.0093512]
		[batch 20/20] avg loss: 1.1432874058194962		[learning rate: 0.0093342]
	Learning Rate: 0.00933424
	LOSS [training: 1.057865915632141 | validation: 1.0038720823232357]
	TIME [epoch: 8.2 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9719493358392988		[learning rate: 0.0093173]
		[batch 20/20] avg loss: 1.0740774476638593		[learning rate: 0.0093004]
	Learning Rate: 0.00930036
	LOSS [training: 1.0230133917515791 | validation: 0.8356054583255689]
	TIME [epoch: 8.19 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.071288166308252		[learning rate: 0.0092835]
		[batch 20/20] avg loss: 0.9959884811835135		[learning rate: 0.0092666]
	Learning Rate: 0.00926661
	LOSS [training: 1.0336383237458828 | validation: 0.8002154732687043]
	TIME [epoch: 8.2 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9498041415696731		[learning rate: 0.0092498]
		[batch 20/20] avg loss: 0.8755669434363759		[learning rate: 0.009233]
	Learning Rate: 0.00923298
	LOSS [training: 0.9126855425030245 | validation: 0.9142413720876392]
	TIME [epoch: 8.18 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.00905116525565		[learning rate: 0.0092162]
		[batch 20/20] avg loss: 0.8806088542185885		[learning rate: 0.0091995]
	Learning Rate: 0.00919948
	LOSS [training: 0.944830009737119 | validation: 1.0739910075393702]
	TIME [epoch: 8.17 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9975733225871213		[learning rate: 0.0091828]
		[batch 20/20] avg loss: 1.194005604304841		[learning rate: 0.0091661]
	Learning Rate: 0.00916609
	LOSS [training: 1.0957894634459813 | validation: 1.2448591405686027]
	TIME [epoch: 8.2 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9300639210252827		[learning rate: 0.0091494]
		[batch 20/20] avg loss: 0.9750600476961047		[learning rate: 0.0091328]
	Learning Rate: 0.00913283
	LOSS [training: 0.9525619843606938 | validation: 1.2987723602476267]
	TIME [epoch: 8.17 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.007129048794455		[learning rate: 0.0091162]
		[batch 20/20] avg loss: 1.129907797843538		[learning rate: 0.0090997]
	Learning Rate: 0.00909968
	LOSS [training: 1.0685184233189966 | validation: 0.811725335112744]
	TIME [epoch: 8.17 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8598986137968148		[learning rate: 0.0090832]
		[batch 20/20] avg loss: 0.8648987939007252		[learning rate: 0.0090667]
	Learning Rate: 0.00906666
	LOSS [training: 0.8623987038487698 | validation: 1.2331108276291844]
	TIME [epoch: 8.18 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.191427838151928		[learning rate: 0.0090502]
		[batch 20/20] avg loss: 1.039275027306325		[learning rate: 0.0090338]
	Learning Rate: 0.00903376
	LOSS [training: 1.1153514327291265 | validation: 0.866763993865602]
	TIME [epoch: 8.22 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8078534280423346		[learning rate: 0.0090174]
		[batch 20/20] avg loss: 0.8903066042381582		[learning rate: 0.009001]
	Learning Rate: 0.00900097
	LOSS [training: 0.8490800161402465 | validation: 0.8184918256245799]
	TIME [epoch: 8.19 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7956751560267497		[learning rate: 0.0089846]
		[batch 20/20] avg loss: 0.9209837021798573		[learning rate: 0.0089683]
	Learning Rate: 0.00896831
	LOSS [training: 0.8583294291033037 | validation: 0.859050539171062]
	TIME [epoch: 8.18 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8792294315831242		[learning rate: 0.008952]
		[batch 20/20] avg loss: 0.8000863648020384		[learning rate: 0.0089358]
	Learning Rate: 0.00893576
	LOSS [training: 0.8396578981925812 | validation: 0.9956819930693657]
	TIME [epoch: 8.2 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9222860789471573		[learning rate: 0.0089195]
		[batch 20/20] avg loss: 1.0102121957847274		[learning rate: 0.0089033]
	Learning Rate: 0.00890333
	LOSS [training: 0.9662491373659423 | validation: 1.1539683876305684]
	TIME [epoch: 8.2 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9438949261202506		[learning rate: 0.0088872]
		[batch 20/20] avg loss: 0.9938519664488193		[learning rate: 0.008871]
	Learning Rate: 0.00887102
	LOSS [training: 0.9688734462845352 | validation: 0.7077902973517176]
	TIME [epoch: 8.2 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8967284872109221		[learning rate: 0.0088549]
		[batch 20/20] avg loss: 0.8573504303976932		[learning rate: 0.0088388]
	Learning Rate: 0.00883883
	LOSS [training: 0.8770394588043077 | validation: 1.1568498796823201]
	TIME [epoch: 8.16 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4933968444578265		[learning rate: 0.0088228]
		[batch 20/20] avg loss: 1.2447191453831663		[learning rate: 0.0088068]
	Learning Rate: 0.00880675
	LOSS [training: 1.3690579949204964 | validation: 1.3849946864663512]
	TIME [epoch: 8.18 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1739301480986162		[learning rate: 0.0087908]
		[batch 20/20] avg loss: 1.266292770581949		[learning rate: 0.0087748]
	Learning Rate: 0.00877479
	LOSS [training: 1.2201114593402824 | validation: 0.9847741295840433]
	TIME [epoch: 8.17 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8582847704696149		[learning rate: 0.0087589]
		[batch 20/20] avg loss: 0.8975525245374305		[learning rate: 0.0087429]
	Learning Rate: 0.00874295
	LOSS [training: 0.8779186475035228 | validation: 1.1195932042120196]
	TIME [epoch: 8.2 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.04687144450647		[learning rate: 0.0087271]
		[batch 20/20] avg loss: 0.7810688846906941		[learning rate: 0.0087112]
	Learning Rate: 0.00871122
	LOSS [training: 0.9139701645985818 | validation: 1.3620595063565928]
	TIME [epoch: 8.21 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2574319097957258		[learning rate: 0.0086954]
		[batch 20/20] avg loss: 0.7975326321758176		[learning rate: 0.0086796]
	Learning Rate: 0.00867961
	LOSS [training: 1.0274822709857718 | validation: 1.009379232806789]
	TIME [epoch: 8.18 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7874408785894562		[learning rate: 0.0086638]
		[batch 20/20] avg loss: 0.7963922621995818		[learning rate: 0.0086481]
	Learning Rate: 0.00864811
	LOSS [training: 0.791916570394519 | validation: 1.695021859665534]
	TIME [epoch: 8.18 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8512085404519413		[learning rate: 0.0086324]
		[batch 20/20] avg loss: 0.7747383064175362		[learning rate: 0.0086167]
	Learning Rate: 0.00861672
	LOSS [training: 0.8129734234347387 | validation: 1.0886459165573177]
	TIME [epoch: 8.19 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8007691376326755		[learning rate: 0.0086011]
		[batch 20/20] avg loss: 0.9112247056380449		[learning rate: 0.0085855]
	Learning Rate: 0.00858545
	LOSS [training: 0.8559969216353606 | validation: 1.2348082518109975]
	TIME [epoch: 8.24 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9195511513345475		[learning rate: 0.0085699]
		[batch 20/20] avg loss: 0.8251223347296979		[learning rate: 0.0085543]
	Learning Rate: 0.00855429
	LOSS [training: 0.8723367430321225 | validation: 0.6399153531188124]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.75111905888484		[learning rate: 0.0085388]
		[batch 20/20] avg loss: 1.0903326608259176		[learning rate: 0.0085232]
	Learning Rate: 0.00852325
	LOSS [training: 0.9207258598553789 | validation: 1.5600566243064562]
	TIME [epoch: 8.17 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4249976195298026		[learning rate: 0.0085078]
		[batch 20/20] avg loss: 0.9474535276832066		[learning rate: 0.0084923]
	Learning Rate: 0.00849232
	LOSS [training: 1.1862255736065046 | validation: 1.4493206914754069]
	TIME [epoch: 8.17 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8398234799949341		[learning rate: 0.0084769]
		[batch 20/20] avg loss: 0.9956555087886111		[learning rate: 0.0084615]
	Learning Rate: 0.0084615
	LOSS [training: 0.9177394943917726 | validation: 1.0591468994412756]
	TIME [epoch: 8.21 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0565979009820439		[learning rate: 0.0084461]
		[batch 20/20] avg loss: 1.1090247635633064		[learning rate: 0.0084308]
	Learning Rate: 0.00843079
	LOSS [training: 1.0828113322726751 | validation: 1.2591344771438275]
	TIME [epoch: 8.17 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7899483319143388		[learning rate: 0.0084155]
		[batch 20/20] avg loss: 0.915405782548544		[learning rate: 0.0084002]
	Learning Rate: 0.0084002
	LOSS [training: 0.8526770572314414 | validation: 1.0892604022813828]
	TIME [epoch: 8.18 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8370912019609028		[learning rate: 0.0083849]
		[batch 20/20] avg loss: 0.9336177268101242		[learning rate: 0.0083697]
	Learning Rate: 0.00836971
	LOSS [training: 0.8853544643855136 | validation: 1.5210729702331618]
	TIME [epoch: 8.21 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8069051965279952		[learning rate: 0.0083545]
		[batch 20/20] avg loss: 0.8522867101108156		[learning rate: 0.0083393]
	Learning Rate: 0.00833934
	LOSS [training: 0.8295959533194054 | validation: 1.0601100939655639]
	TIME [epoch: 8.2 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7629556765567116		[learning rate: 0.0083242]
		[batch 20/20] avg loss: 0.9141537751817868		[learning rate: 0.0083091]
	Learning Rate: 0.00830907
	LOSS [training: 0.8385547258692492 | validation: 1.293914854426343]
	TIME [epoch: 8.19 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8555489575239743		[learning rate: 0.008294]
		[batch 20/20] avg loss: 0.7307109961686203		[learning rate: 0.0082789]
	Learning Rate: 0.00827892
	LOSS [training: 0.7931299768462974 | validation: 1.5440631341324083]
	TIME [epoch: 8.19 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1914263909774423		[learning rate: 0.0082639]
		[batch 20/20] avg loss: 0.9099708559813534		[learning rate: 0.0082489]
	Learning Rate: 0.00824887
	LOSS [training: 1.0506986234793976 | validation: 1.1247045709664865]
	TIME [epoch: 8.21 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.022792126589696		[learning rate: 0.0082339]
		[batch 20/20] avg loss: 0.8128404892161789		[learning rate: 0.0082189]
	Learning Rate: 0.00821894
	LOSS [training: 0.9178163079029373 | validation: 0.8988287228133625]
	TIME [epoch: 8.17 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7684358285009727		[learning rate: 0.008204]
		[batch 20/20] avg loss: 0.835806239460822		[learning rate: 0.0081891]
	Learning Rate: 0.00818911
	LOSS [training: 0.8021210339808975 | validation: 0.6118815610047917]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6973477668940283		[learning rate: 0.0081742]
		[batch 20/20] avg loss: 0.8732404227774909		[learning rate: 0.0081594]
	Learning Rate: 0.00815939
	LOSS [training: 0.7852940948357595 | validation: 1.042755829990691]
	TIME [epoch: 8.2 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7553164305755631		[learning rate: 0.0081446]
		[batch 20/20] avg loss: 1.026172410538584		[learning rate: 0.0081298]
	Learning Rate: 0.00812978
	LOSS [training: 0.8907444205570737 | validation: 1.1159096044111758]
	TIME [epoch: 8.19 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8613808109508415		[learning rate: 0.008115]
		[batch 20/20] avg loss: 0.7297404819931772		[learning rate: 0.0081003]
	Learning Rate: 0.00810028
	LOSS [training: 0.7955606464720092 | validation: 0.9515286641985667]
	TIME [epoch: 8.19 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9419575263735531		[learning rate: 0.0080856]
		[batch 20/20] avg loss: 0.8609235713302891		[learning rate: 0.0080709]
	Learning Rate: 0.00807088
	LOSS [training: 0.9014405488519213 | validation: 1.0311983000232838]
	TIME [epoch: 8.23 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7369539439706347		[learning rate: 0.0080562]
		[batch 20/20] avg loss: 0.7481485111857961		[learning rate: 0.0080416]
	Learning Rate: 0.00804159
	LOSS [training: 0.7425512275782155 | validation: 0.8463261761525628]
	TIME [epoch: 8.22 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0368252957900586		[learning rate: 0.008027]
		[batch 20/20] avg loss: 0.9843355881348066		[learning rate: 0.0080124]
	Learning Rate: 0.00801241
	LOSS [training: 1.0105804419624327 | validation: 0.8923463273863474]
	TIME [epoch: 8.2 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9219187449742258		[learning rate: 0.0079979]
		[batch 20/20] avg loss: 0.8290376452949232		[learning rate: 0.0079833]
	Learning Rate: 0.00798333
	LOSS [training: 0.8754781951345743 | validation: 0.471341720138318]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7281858697445676		[learning rate: 0.0079688]
		[batch 20/20] avg loss: 0.845306214260322		[learning rate: 0.0079544]
	Learning Rate: 0.00795436
	LOSS [training: 0.786746042002445 | validation: 1.2534165905028125]
	TIME [epoch: 8.24 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.801115103699136		[learning rate: 0.0079399]
		[batch 20/20] avg loss: 0.788993048524522		[learning rate: 0.0079255]
	Learning Rate: 0.00792549
	LOSS [training: 0.795054076111829 | validation: 0.7506899420958542]
	TIME [epoch: 8.22 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1132422116739333		[learning rate: 0.0079111]
		[batch 20/20] avg loss: 0.9587561132147002		[learning rate: 0.0078967]
	Learning Rate: 0.00789673
	LOSS [training: 1.0359991624443166 | validation: 0.5242769682886801]
	TIME [epoch: 8.19 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1269907229945169		[learning rate: 0.0078824]
		[batch 20/20] avg loss: 0.9806911835618097		[learning rate: 0.0078681]
	Learning Rate: 0.00786807
	LOSS [training: 1.0538409532781632 | validation: 0.6449346194985623]
	TIME [epoch: 8.19 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7696921636284032		[learning rate: 0.0078538]
		[batch 20/20] avg loss: 0.8055287153330409		[learning rate: 0.0078395]
	Learning Rate: 0.00783952
	LOSS [training: 0.7876104394807222 | validation: 0.7718410684825764]
	TIME [epoch: 8.19 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8375630823138189		[learning rate: 0.0078253]
		[batch 20/20] avg loss: 0.6905632310796067		[learning rate: 0.0078111]
	Learning Rate: 0.00781107
	LOSS [training: 0.7640631566967129 | validation: 1.0592831986739064]
	TIME [epoch: 8.21 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1705428728011618		[learning rate: 0.0077969]
		[batch 20/20] avg loss: 0.7073790752172355		[learning rate: 0.0077827]
	Learning Rate: 0.00778272
	LOSS [training: 0.938960974009199 | validation: 0.6622841042000789]
	TIME [epoch: 8.2 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.801931346199327		[learning rate: 0.0077686]
		[batch 20/20] avg loss: 0.6994486824907256		[learning rate: 0.0077545]
	Learning Rate: 0.00775448
	LOSS [training: 0.7506900143450265 | validation: 0.45841983625530236]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8875166690380067		[learning rate: 0.0077404]
		[batch 20/20] avg loss: 0.7124116635660249		[learning rate: 0.0077263]
	Learning Rate: 0.00772634
	LOSS [training: 0.7999641663020156 | validation: 1.6489146799185657]
	TIME [epoch: 8.2 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7858878658303559		[learning rate: 0.0077123]
		[batch 20/20] avg loss: 0.6921620471005222		[learning rate: 0.0076983]
	Learning Rate: 0.0076983
	LOSS [training: 0.739024956465439 | validation: 0.7415250031164283]
	TIME [epoch: 8.21 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8108867841610861		[learning rate: 0.0076843]
		[batch 20/20] avg loss: 0.8873418114039383		[learning rate: 0.0076704]
	Learning Rate: 0.00767036
	LOSS [training: 0.8491142977825122 | validation: 1.2855946285143676]
	TIME [epoch: 8.23 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8652499828922204		[learning rate: 0.0076564]
		[batch 20/20] avg loss: 0.6813843874092482		[learning rate: 0.0076425]
	Learning Rate: 0.00764252
	LOSS [training: 0.7733171851507344 | validation: 0.6367627263025064]
	TIME [epoch: 8.2 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.050804374843434		[learning rate: 0.0076286]
		[batch 20/20] avg loss: 0.9337643461463628		[learning rate: 0.0076148]
	Learning Rate: 0.00761479
	LOSS [training: 0.9922843604948985 | validation: 1.226218829139603]
	TIME [epoch: 8.19 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.766824584599207		[learning rate: 0.007601]
		[batch 20/20] avg loss: 0.6353544432005752		[learning rate: 0.0075872]
	Learning Rate: 0.00758715
	LOSS [training: 0.7010895138998913 | validation: 0.5835174280339034]
	TIME [epoch: 8.19 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6906420244529072		[learning rate: 0.0075734]
		[batch 20/20] avg loss: 1.1220602247003773		[learning rate: 0.0075596]
	Learning Rate: 0.00755962
	LOSS [training: 0.9063511245766425 | validation: 0.7491775935965096]
	TIME [epoch: 8.23 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6799553257811286		[learning rate: 0.0075459]
		[batch 20/20] avg loss: 0.8501422608531461		[learning rate: 0.0075322]
	Learning Rate: 0.00753219
	LOSS [training: 0.7650487933171374 | validation: 0.7392985515499683]
	TIME [epoch: 8.2 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6850061066949567		[learning rate: 0.0075185]
		[batch 20/20] avg loss: 0.6324701785311275		[learning rate: 0.0075049]
	Learning Rate: 0.00750485
	LOSS [training: 0.6587381426130421 | validation: 0.672793185926272]
	TIME [epoch: 8.2 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7291092343909137		[learning rate: 0.0074912]
		[batch 20/20] avg loss: 0.8404309303815213		[learning rate: 0.0074776]
	Learning Rate: 0.00747762
	LOSS [training: 0.7847700823862176 | validation: 0.6066073198135684]
	TIME [epoch: 8.21 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1644165502258623		[learning rate: 0.007464]
		[batch 20/20] avg loss: 0.8354021065982231		[learning rate: 0.0074505]
	Learning Rate: 0.00745048
	LOSS [training: 0.9999093284120424 | validation: 1.070220694633878]
	TIME [epoch: 8.24 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8011029163548018		[learning rate: 0.0074369]
		[batch 20/20] avg loss: 0.8314874794974635		[learning rate: 0.0074234]
	Learning Rate: 0.00742344
	LOSS [training: 0.8162951979261326 | validation: 1.142141481181159]
	TIME [epoch: 8.2 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9144726596164438		[learning rate: 0.00741]
		[batch 20/20] avg loss: 0.7138286893751048		[learning rate: 0.0073965]
	Learning Rate: 0.0073965
	LOSS [training: 0.8141506744957743 | validation: 0.8837770217062295]
	TIME [epoch: 8.19 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8242871147106715		[learning rate: 0.0073831]
		[batch 20/20] avg loss: 0.7144825797557651		[learning rate: 0.0073697]
	Learning Rate: 0.00736966
	LOSS [training: 0.7693848472332184 | validation: 0.5506167279258611]
	TIME [epoch: 8.22 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6693614295931118		[learning rate: 0.0073563]
		[batch 20/20] avg loss: 0.9362850456988372		[learning rate: 0.0073429]
	Learning Rate: 0.00734291
	LOSS [training: 0.8028232376459746 | validation: 1.3424092465756783]
	TIME [epoch: 8.23 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9189834909178612		[learning rate: 0.0073296]
		[batch 20/20] avg loss: 0.79340308240419		[learning rate: 0.0073163]
	Learning Rate: 0.00731627
	LOSS [training: 0.8561932866610258 | validation: 0.9187167437493535]
	TIME [epoch: 8.2 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6875242887810918		[learning rate: 0.007303]
		[batch 20/20] avg loss: 0.6510755815733391		[learning rate: 0.0072897]
	Learning Rate: 0.00728971
	LOSS [training: 0.6692999351772155 | validation: 1.137807583638863]
	TIME [epoch: 8.2 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7832785763131143		[learning rate: 0.0072765]
		[batch 20/20] avg loss: 0.8769261158162045		[learning rate: 0.0072633]
	Learning Rate: 0.00726326
	LOSS [training: 0.8301023460646594 | validation: 0.9709436772702806]
	TIME [epoch: 8.19 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6657400146399646		[learning rate: 0.0072501]
		[batch 20/20] avg loss: 0.6738597195752656		[learning rate: 0.0072369]
	Learning Rate: 0.0072369
	LOSS [training: 0.6697998671076151 | validation: 1.0211502924219578]
	TIME [epoch: 8.19 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8755613209493689		[learning rate: 0.0072238]
		[batch 20/20] avg loss: 0.7335581922950436		[learning rate: 0.0072106]
	Learning Rate: 0.00721064
	LOSS [training: 0.8045597566222064 | validation: 0.47152716344005724]
	TIME [epoch: 8.22 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6524291635235281		[learning rate: 0.0071975]
		[batch 20/20] avg loss: 0.7119050114012412		[learning rate: 0.0071845]
	Learning Rate: 0.00718447
	LOSS [training: 0.6821670874623845 | validation: 0.8377350485444469]
	TIME [epoch: 8.22 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5956204347334213		[learning rate: 0.0071714]
		[batch 20/20] avg loss: 0.7112566108237007		[learning rate: 0.0071584]
	Learning Rate: 0.0071584
	LOSS [training: 0.6534385227785611 | validation: 0.8233844601554476]
	TIME [epoch: 8.2 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.758066284811308		[learning rate: 0.0071454]
		[batch 20/20] avg loss: 0.7884446940306693		[learning rate: 0.0071324]
	Learning Rate: 0.00713242
	LOSS [training: 0.7732554894209884 | validation: 0.7782325630653498]
	TIME [epoch: 8.19 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6427643938978951		[learning rate: 0.0071195]
		[batch 20/20] avg loss: 0.791089792505667		[learning rate: 0.0071065]
	Learning Rate: 0.00710653
	LOSS [training: 0.716927093201781 | validation: 0.7555757011105427]
	TIME [epoch: 8.22 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7909074254761124		[learning rate: 0.0070936]
		[batch 20/20] avg loss: 0.7630063180390663		[learning rate: 0.0070807]
	Learning Rate: 0.00708074
	LOSS [training: 0.7769568717575892 | validation: 0.6946117650185716]
	TIME [epoch: 8.24 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6761572796521097		[learning rate: 0.0070679]
		[batch 20/20] avg loss: 0.7543565689763605		[learning rate: 0.007055]
	Learning Rate: 0.00705505
	LOSS [training: 0.715256924314235 | validation: 0.5209493786802241]
	TIME [epoch: 8.19 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6886280615737814		[learning rate: 0.0070422]
		[batch 20/20] avg loss: 1.001304561641921		[learning rate: 0.0070294]
	Learning Rate: 0.00702945
	LOSS [training: 0.8449663116078512 | validation: 1.0103867690055353]
	TIME [epoch: 8.18 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6789267130480827		[learning rate: 0.0070167]
		[batch 20/20] avg loss: 0.6188522443262199		[learning rate: 0.0070039]
	Learning Rate: 0.00700394
	LOSS [training: 0.6488894786871513 | validation: 1.7689305958956112]
	TIME [epoch: 8.18 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.778548730948418		[learning rate: 0.0069912]
		[batch 20/20] avg loss: 0.7466804203879525		[learning rate: 0.0069785]
	Learning Rate: 0.00697852
	LOSS [training: 0.7626145756681855 | validation: 1.3325863174517516]
	TIME [epoch: 8.21 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8500661906029716		[learning rate: 0.0069658]
		[batch 20/20] avg loss: 0.7366012954668733		[learning rate: 0.0069532]
	Learning Rate: 0.00695319
	LOSS [training: 0.7933337430349224 | validation: 0.6276220368642779]
	TIME [epoch: 8.18 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9024713598172305		[learning rate: 0.0069406]
		[batch 20/20] avg loss: 0.6654538223474142		[learning rate: 0.006928]
	Learning Rate: 0.00692796
	LOSS [training: 0.7839625910823225 | validation: 0.7313469483773055]
	TIME [epoch: 8.18 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6228984902858795		[learning rate: 0.0069154]
		[batch 20/20] avg loss: 0.592448880785363		[learning rate: 0.0069028]
	Learning Rate: 0.00690282
	LOSS [training: 0.6076736855356213 | validation: 1.3586716180570564]
	TIME [epoch: 8.18 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7631522290022577		[learning rate: 0.0068903]
		[batch 20/20] avg loss: 0.6627151814168147		[learning rate: 0.0068778]
	Learning Rate: 0.00687777
	LOSS [training: 0.7129337052095361 | validation: 0.7176002341390242]
	TIME [epoch: 8.24 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6824364027469365		[learning rate: 0.0068653]
		[batch 20/20] avg loss: 0.5579091507407012		[learning rate: 0.0068528]
	Learning Rate: 0.00685281
	LOSS [training: 0.620172776743819 | validation: 0.7628040264275548]
	TIME [epoch: 8.2 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8065310041619856		[learning rate: 0.0068404]
		[batch 20/20] avg loss: 0.6124605081258043		[learning rate: 0.0068279]
	Learning Rate: 0.00682794
	LOSS [training: 0.709495756143895 | validation: 0.6251655103305175]
	TIME [epoch: 8.19 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6048695470068216		[learning rate: 0.0068155]
		[batch 20/20] avg loss: 0.7201913570898354		[learning rate: 0.0068032]
	Learning Rate: 0.00680316
	LOSS [training: 0.6625304520483286 | validation: 0.7380281002021154]
	TIME [epoch: 8.19 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6149549315681356		[learning rate: 0.0067908]
		[batch 20/20] avg loss: 0.7187752925790438		[learning rate: 0.0067785]
	Learning Rate: 0.00677847
	LOSS [training: 0.6668651120735898 | validation: 0.9376734682188727]
	TIME [epoch: 8.23 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5773299878904796		[learning rate: 0.0067662]
		[batch 20/20] avg loss: 0.6967328191856691		[learning rate: 0.0067539]
	Learning Rate: 0.00675387
	LOSS [training: 0.6370314035380742 | validation: 1.5054055486755558]
	TIME [epoch: 8.21 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6892944884137895		[learning rate: 0.0067416]
		[batch 20/20] avg loss: 0.6023139094810087		[learning rate: 0.0067294]
	Learning Rate: 0.00672936
	LOSS [training: 0.6458041989473992 | validation: 0.6713541797324966]
	TIME [epoch: 8.19 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6495164706482772		[learning rate: 0.0067171]
		[batch 20/20] avg loss: 0.5465365341062259		[learning rate: 0.0067049]
	Learning Rate: 0.00670494
	LOSS [training: 0.5980265023772516 | validation: 1.1175993945852234]
	TIME [epoch: 8.18 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8110109836798827		[learning rate: 0.0066928]
		[batch 20/20] avg loss: 0.7329872971125623		[learning rate: 0.0066806]
	Learning Rate: 0.0066806
	LOSS [training: 0.7719991403962224 | validation: 2.0357511077017536]
	TIME [epoch: 8.19 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.959303157429402		[learning rate: 0.0066685]
		[batch 20/20] avg loss: 0.6593131304817164		[learning rate: 0.0066564]
	Learning Rate: 0.00665636
	LOSS [training: 0.809308143955559 | validation: 1.0704216088269511]
	TIME [epoch: 8.21 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8029861338848704		[learning rate: 0.0066443]
		[batch 20/20] avg loss: 0.6695288408506848		[learning rate: 0.0066322]
	Learning Rate: 0.0066322
	LOSS [training: 0.7362574873677776 | validation: 0.7587337635565987]
	TIME [epoch: 8.18 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6177915507944469		[learning rate: 0.0066202]
		[batch 20/20] avg loss: 0.7644475817272465		[learning rate: 0.0066081]
	Learning Rate: 0.00660814
	LOSS [training: 0.6911195662608466 | validation: 1.0222472158089129]
	TIME [epoch: 8.2 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7517161098820309		[learning rate: 0.0065961]
		[batch 20/20] avg loss: 0.5480814171064832		[learning rate: 0.0065842]
	Learning Rate: 0.00658415
	LOSS [training: 0.6498987634942568 | validation: 0.8974612397147336]
	TIME [epoch: 8.2 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7018434164757805		[learning rate: 0.0065722]
		[batch 20/20] avg loss: 0.8888745441280786		[learning rate: 0.0065603]
	Learning Rate: 0.00656026
	LOSS [training: 0.7953589803019296 | validation: 0.554725736397761]
	TIME [epoch: 8.21 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8199887976494921		[learning rate: 0.0065483]
		[batch 20/20] avg loss: 0.8308226193047428		[learning rate: 0.0065365]
	Learning Rate: 0.00653645
	LOSS [training: 0.8254057084771175 | validation: 1.8529664851468184]
	TIME [epoch: 8.21 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8846488893360303		[learning rate: 0.0065246]
		[batch 20/20] avg loss: 0.7316691566945487		[learning rate: 0.0065127]
	Learning Rate: 0.00651273
	LOSS [training: 0.8081590230152893 | validation: 0.8815436274388866]
	TIME [epoch: 8.22 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7925518232160855		[learning rate: 0.0065009]
		[batch 20/20] avg loss: 0.9112476026975347		[learning rate: 0.0064891]
	Learning Rate: 0.0064891
	LOSS [training: 0.8518997129568102 | validation: 0.9241118757255273]
	TIME [epoch: 8.18 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.76157609623468		[learning rate: 0.0064773]
		[batch 20/20] avg loss: 0.6139447218696994		[learning rate: 0.0064655]
	Learning Rate: 0.00646555
	LOSS [training: 0.6877604090521898 | validation: 0.5042542106184904]
	TIME [epoch: 8.19 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7000083127032822		[learning rate: 0.0064538]
		[batch 20/20] avg loss: 0.6882674808477122		[learning rate: 0.0064421]
	Learning Rate: 0.00644208
	LOSS [training: 0.6941378967754973 | validation: 0.5549230121833804]
	TIME [epoch: 8.21 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6075916947849406		[learning rate: 0.0064304]
		[batch 20/20] avg loss: 0.7299227582141159		[learning rate: 0.0064187]
	Learning Rate: 0.0064187
	LOSS [training: 0.6687572264995283 | validation: 1.5944078031066]
	TIME [epoch: 8.19 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9314816748184238		[learning rate: 0.006407]
		[batch 20/20] avg loss: 0.7438089255693563		[learning rate: 0.0063954]
	Learning Rate: 0.00639541
	LOSS [training: 0.8376453001938902 | validation: 0.684565453081889]
	TIME [epoch: 8.18 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.544787320740641		[learning rate: 0.0063838]
		[batch 20/20] avg loss: 0.6417333473560933		[learning rate: 0.0063722]
	Learning Rate: 0.0063722
	LOSS [training: 0.593260334048367 | validation: 0.9497097446391729]
	TIME [epoch: 8.18 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6326255859485241		[learning rate: 0.0063606]
		[batch 20/20] avg loss: 0.7803834966777974		[learning rate: 0.0063491]
	Learning Rate: 0.00634908
	LOSS [training: 0.7065045413131608 | validation: 1.0358705291852541]
	TIME [epoch: 8.23 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.861315570186472		[learning rate: 0.0063375]
		[batch 20/20] avg loss: 0.7654292447770968		[learning rate: 0.006326]
	Learning Rate: 0.00632603
	LOSS [training: 0.8133724074817843 | validation: 0.7497336834104528]
	TIME [epoch: 8.2 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6692356173587943		[learning rate: 0.0063145]
		[batch 20/20] avg loss: 0.7900972209262241		[learning rate: 0.0063031]
	Learning Rate: 0.00630308
	LOSS [training: 0.7296664191425093 | validation: 1.3136721493908459]
	TIME [epoch: 8.19 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9655909969023915		[learning rate: 0.0062916]
		[batch 20/20] avg loss: 0.5499477971084069		[learning rate: 0.0062802]
	Learning Rate: 0.0062802
	LOSS [training: 0.7577693970053991 | validation: 0.8353493851518392]
	TIME [epoch: 8.19 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6624161093465559		[learning rate: 0.0062688]
		[batch 20/20] avg loss: 0.7417783000887167		[learning rate: 0.0062574]
	Learning Rate: 0.00625741
	LOSS [training: 0.7020972047176365 | validation: 1.7628548688174641]
	TIME [epoch: 8.24 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6176010940108141		[learning rate: 0.006246]
		[batch 20/20] avg loss: 0.6511158043985013		[learning rate: 0.0062347]
	Learning Rate: 0.0062347
	LOSS [training: 0.6343584492046578 | validation: 0.48871015307506327]
	TIME [epoch: 8.21 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6892451185547255		[learning rate: 0.0062234]
		[batch 20/20] avg loss: 0.9070059361446295		[learning rate: 0.0062121]
	Learning Rate: 0.00621208
	LOSS [training: 0.7981255273496773 | validation: 1.0144189756429238]
	TIME [epoch: 8.18 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6981594034942897		[learning rate: 0.0062008]
		[batch 20/20] avg loss: 0.599651367702346		[learning rate: 0.0061895]
	Learning Rate: 0.00618953
	LOSS [training: 0.648905385598318 | validation: 0.7437174385796086]
	TIME [epoch: 8.18 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6006227621220742		[learning rate: 0.0061783]
		[batch 20/20] avg loss: 0.5848812800613978		[learning rate: 0.0061671]
	Learning Rate: 0.00616707
	LOSS [training: 0.5927520210917359 | validation: 0.4815792619435174]
	TIME [epoch: 8.19 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9264564655005572		[learning rate: 0.0061559]
		[batch 20/20] avg loss: 0.727977888147856		[learning rate: 0.0061447]
	Learning Rate: 0.00614469
	LOSS [training: 0.8272171768242066 | validation: 0.7544666309707961]
	TIME [epoch: 8.21 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5425244740849088		[learning rate: 0.0061335]
		[batch 20/20] avg loss: 0.6037420493627537		[learning rate: 0.0061224]
	Learning Rate: 0.00612239
	LOSS [training: 0.5731332617238313 | validation: 0.6251305802665549]
	TIME [epoch: 8.19 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7706021394906746		[learning rate: 0.0061113]
		[batch 20/20] avg loss: 0.7637719280789923		[learning rate: 0.0061002]
	Learning Rate: 0.00610017
	LOSS [training: 0.7671870337848333 | validation: 0.5614390245645116]
	TIME [epoch: 8.19 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5414669610461968		[learning rate: 0.0060891]
		[batch 20/20] avg loss: 0.6000278942556603		[learning rate: 0.006078]
	Learning Rate: 0.00607803
	LOSS [training: 0.5707474276509285 | validation: 0.9768822721345489]
	TIME [epoch: 8.2 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5789262395128918		[learning rate: 0.006067]
		[batch 20/20] avg loss: 0.608147923142304		[learning rate: 0.006056]
	Learning Rate: 0.00605598
	LOSS [training: 0.5935370813275981 | validation: 0.7341745373302894]
	TIME [epoch: 8.19 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7785297653450248		[learning rate: 0.006045]
		[batch 20/20] avg loss: 0.6848791935681384		[learning rate: 0.006034]
	Learning Rate: 0.006034
	LOSS [training: 0.7317044794565816 | validation: 0.620380023440176]
	TIME [epoch: 8.22 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.752005180477443		[learning rate: 0.006023]
		[batch 20/20] avg loss: 0.6861838028441938		[learning rate: 0.0060121]
	Learning Rate: 0.0060121
	LOSS [training: 0.7190944916608183 | validation: 1.6233513496593197]
	TIME [epoch: 8.21 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8133026528803882		[learning rate: 0.0060012]
		[batch 20/20] avg loss: 0.7379136272055311		[learning rate: 0.0059903]
	Learning Rate: 0.00599028
	LOSS [training: 0.7756081400429597 | validation: 0.6574716882144619]
	TIME [epoch: 8.21 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5593837739964616		[learning rate: 0.0059794]
		[batch 20/20] avg loss: 0.6661370740572706		[learning rate: 0.0059685]
	Learning Rate: 0.00596854
	LOSS [training: 0.612760424026866 | validation: 1.352756952111021]
	TIME [epoch: 8.18 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6681597012088886		[learning rate: 0.0059577]
		[batch 20/20] avg loss: 0.7578735782692251		[learning rate: 0.0059469]
	Learning Rate: 0.00594688
	LOSS [training: 0.7130166397390567 | validation: 0.8570661747581996]
	TIME [epoch: 8.21 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5847346886233238		[learning rate: 0.0059361]
		[batch 20/20] avg loss: 0.6588797390138025		[learning rate: 0.0059253]
	Learning Rate: 0.0059253
	LOSS [training: 0.6218072138185631 | validation: 0.9259299411430106]
	TIME [epoch: 8.19 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6303082711935788		[learning rate: 0.0059145]
		[batch 20/20] avg loss: 0.5117975117328356		[learning rate: 0.0059038]
	Learning Rate: 0.0059038
	LOSS [training: 0.5710528914632073 | validation: 0.5000978560488366]
	TIME [epoch: 8.18 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.876537779077806		[learning rate: 0.0058931]
		[batch 20/20] avg loss: 0.8039534038249571		[learning rate: 0.0058824]
	Learning Rate: 0.00588237
	LOSS [training: 0.8402455914513816 | validation: 0.5803706852389396]
	TIME [epoch: 8.18 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49814944033357733		[learning rate: 0.0058717]
		[batch 20/20] avg loss: 0.5523317330666495		[learning rate: 0.005861]
	Learning Rate: 0.00586103
	LOSS [training: 0.5252405867001133 | validation: 0.804684395098276]
	TIME [epoch: 8.2 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8770739716430731		[learning rate: 0.0058504]
		[batch 20/20] avg loss: 0.8536890115288216		[learning rate: 0.0058398]
	Learning Rate: 0.00583976
	LOSS [training: 0.8653814915859475 | validation: 0.9686928663223664]
	TIME [epoch: 8.23 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7638205154536687		[learning rate: 0.0058291]
		[batch 20/20] avg loss: 0.5738360572229729		[learning rate: 0.0058186]
	Learning Rate: 0.00581856
	LOSS [training: 0.6688282863383207 | validation: 0.8000124974435572]
	TIME [epoch: 8.19 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5863084756546244		[learning rate: 0.005808]
		[batch 20/20] avg loss: 0.7233253743794492		[learning rate: 0.0057974]
	Learning Rate: 0.00579745
	LOSS [training: 0.6548169250170368 | validation: 0.8692005681769144]
	TIME [epoch: 8.22 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5873726690104439		[learning rate: 0.0057869]
		[batch 20/20] avg loss: 0.7800396915542768		[learning rate: 0.0057764]
	Learning Rate: 0.00577641
	LOSS [training: 0.6837061802823605 | validation: 0.48718270416684434]
	TIME [epoch: 8.21 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7443666489076637		[learning rate: 0.0057659]
		[batch 20/20] avg loss: 0.6199484695407358		[learning rate: 0.0057554]
	Learning Rate: 0.00575545
	LOSS [training: 0.6821575592241997 | validation: 1.59164215389985]
	TIME [epoch: 8.23 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6639946382755688		[learning rate: 0.005745]
		[batch 20/20] avg loss: 0.5468234608088509		[learning rate: 0.0057346]
	Learning Rate: 0.00573456
	LOSS [training: 0.6054090495422096 | validation: 0.6934341715323844]
	TIME [epoch: 8.18 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.576255584225785		[learning rate: 0.0057241]
		[batch 20/20] avg loss: 0.578002386477946		[learning rate: 0.0057137]
	Learning Rate: 0.00571375
	LOSS [training: 0.5771289853518657 | validation: 0.8738015009752608]
	TIME [epoch: 8.18 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4863905497252892		[learning rate: 0.0057034]
		[batch 20/20] avg loss: 0.5331093263817721		[learning rate: 0.005693]
	Learning Rate: 0.00569301
	LOSS [training: 0.5097499380535306 | validation: 0.582722872556777]
	TIME [epoch: 8.19 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5973225032880395		[learning rate: 0.0056827]
		[batch 20/20] avg loss: 0.5159110503723354		[learning rate: 0.0056724]
	Learning Rate: 0.00567235
	LOSS [training: 0.5566167768301875 | validation: 0.6520713427219272]
	TIME [epoch: 8.2 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5407016734027865		[learning rate: 0.005662]
		[batch 20/20] avg loss: 0.621312490687884		[learning rate: 0.0056518]
	Learning Rate: 0.00565177
	LOSS [training: 0.5810070820453352 | validation: 1.033866830348586]
	TIME [epoch: 8.2 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7214724912853463		[learning rate: 0.0056415]
		[batch 20/20] avg loss: 0.624452202603661		[learning rate: 0.0056313]
	Learning Rate: 0.00563126
	LOSS [training: 0.6729623469445036 | validation: 0.8938458387085428]
	TIME [epoch: 8.21 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6643339861353598		[learning rate: 0.005621]
		[batch 20/20] avg loss: 0.5023517641022947		[learning rate: 0.0056108]
	Learning Rate: 0.00561082
	LOSS [training: 0.5833428751188272 | validation: 0.5736210084882564]
	TIME [epoch: 8.19 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6155660901992603		[learning rate: 0.0056006]
		[batch 20/20] avg loss: 0.5810911072636258		[learning rate: 0.0055905]
	Learning Rate: 0.00559046
	LOSS [training: 0.598328598731443 | validation: 0.6145155028643736]
	TIME [epoch: 8.19 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9440125008318718		[learning rate: 0.0055803]
		[batch 20/20] avg loss: 0.7231136011566477		[learning rate: 0.0055702]
	Learning Rate: 0.00557017
	LOSS [training: 0.8335630509942596 | validation: 0.7528559943130497]
	TIME [epoch: 8.25 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4955831933536099		[learning rate: 0.0055601]
		[batch 20/20] avg loss: 0.537706359364633		[learning rate: 0.00555]
	Learning Rate: 0.00554996
	LOSS [training: 0.5166447763591214 | validation: 0.34548256260652643]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5493207775002621		[learning rate: 0.0055399]
		[batch 20/20] avg loss: 0.5541858530211563		[learning rate: 0.0055298]
	Learning Rate: 0.00552981
	LOSS [training: 0.5517533152607093 | validation: 0.510823222315946]
	TIME [epoch: 8.21 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5599570605853091		[learning rate: 0.0055198]
		[batch 20/20] avg loss: 0.5813898558037078		[learning rate: 0.0055097]
	Learning Rate: 0.00550975
	LOSS [training: 0.5706734581945084 | validation: 0.3679088902819483]
	TIME [epoch: 8.2 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5710811723720756		[learning rate: 0.0054997]
		[batch 20/20] avg loss: 0.5395418896627786		[learning rate: 0.0054898]
	Learning Rate: 0.00548975
	LOSS [training: 0.5553115310174271 | validation: 0.35371346705281453]
	TIME [epoch: 8.24 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4981205994613145		[learning rate: 0.0054798]
		[batch 20/20] avg loss: 0.4606726232491468		[learning rate: 0.0054698]
	Learning Rate: 0.00546983
	LOSS [training: 0.4793966113552307 | validation: 0.50066775708101]
	TIME [epoch: 8.2 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.463029150742788		[learning rate: 0.0054599]
		[batch 20/20] avg loss: 0.45100732289848333		[learning rate: 0.00545]
	Learning Rate: 0.00544998
	LOSS [training: 0.4570182368206356 | validation: 0.8341006267317836]
	TIME [epoch: 8.19 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6292812114789814		[learning rate: 0.0054401]
		[batch 20/20] avg loss: 0.6394375067293094		[learning rate: 0.0054302]
	Learning Rate: 0.0054302
	LOSS [training: 0.6343593591041456 | validation: 0.3743590870545626]
	TIME [epoch: 8.24 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5589412742020312		[learning rate: 0.0054203]
		[batch 20/20] avg loss: 0.7098850693788215		[learning rate: 0.0054105]
	Learning Rate: 0.00541049
	LOSS [training: 0.6344131717904263 | validation: 0.7460286654445889]
	TIME [epoch: 8.22 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5619614441748509		[learning rate: 0.0054007]
		[batch 20/20] avg loss: 0.4841634593545295		[learning rate: 0.0053909]
	Learning Rate: 0.00539086
	LOSS [training: 0.5230624517646902 | validation: 0.6847430921270761]
	TIME [epoch: 8.21 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5665489525726681		[learning rate: 0.0053811]
		[batch 20/20] avg loss: 0.5576601050391761		[learning rate: 0.0053713]
	Learning Rate: 0.00537129
	LOSS [training: 0.5621045288059222 | validation: 0.640783403271204]
	TIME [epoch: 8.21 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5192430771898371		[learning rate: 0.0053615]
		[batch 20/20] avg loss: 0.6801923607135822		[learning rate: 0.0053518]
	Learning Rate: 0.0053518
	LOSS [training: 0.5997177189517098 | validation: 0.6603455862688841]
	TIME [epoch: 8.23 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6767910652789402		[learning rate: 0.0053421]
		[batch 20/20] avg loss: 0.4917003170362128		[learning rate: 0.0053324]
	Learning Rate: 0.00533238
	LOSS [training: 0.5842456911575764 | validation: 0.5711592860747349]
	TIME [epoch: 8.22 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5061252860812584		[learning rate: 0.0053227]
		[batch 20/20] avg loss: 0.574902171219607		[learning rate: 0.005313]
	Learning Rate: 0.00531303
	LOSS [training: 0.5405137286504329 | validation: 1.7106398547337605]
	TIME [epoch: 8.18 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.597545919831028		[learning rate: 0.0053034]
		[batch 20/20] avg loss: 0.6100740951667678		[learning rate: 0.0052937]
	Learning Rate: 0.00529375
	LOSS [training: 0.6038100074988979 | validation: 0.7347464624902764]
	TIME [epoch: 8.16 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5321892796869083		[learning rate: 0.0052841]
		[batch 20/20] avg loss: 0.533767501200147		[learning rate: 0.0052745]
	Learning Rate: 0.00527454
	LOSS [training: 0.5329783904435277 | validation: 0.58202243553379]
	TIME [epoch: 8.15 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5540313603017649		[learning rate: 0.005265]
		[batch 20/20] avg loss: 0.574339652384922		[learning rate: 0.0052554]
	Learning Rate: 0.00525539
	LOSS [training: 0.5641855063433436 | validation: 0.4924209208783702]
	TIME [epoch: 8.19 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.498541030425623		[learning rate: 0.0052458]
		[batch 20/20] avg loss: 0.6204739694274737		[learning rate: 0.0052363]
	Learning Rate: 0.00523632
	LOSS [training: 0.5595074999265484 | validation: 1.117122402119608]
	TIME [epoch: 8.19 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5646549317236285		[learning rate: 0.0052268]
		[batch 20/20] avg loss: 0.5323923602161822		[learning rate: 0.0052173]
	Learning Rate: 0.00521732
	LOSS [training: 0.5485236459699052 | validation: 0.5131678102948687]
	TIME [epoch: 8.18 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9091664370920757		[learning rate: 0.0052078]
		[batch 20/20] avg loss: 0.5340770842272131		[learning rate: 0.0051984]
	Learning Rate: 0.00519839
	LOSS [training: 0.7216217606596443 | validation: 0.8378734177684592]
	TIME [epoch: 8.17 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6203013915505995		[learning rate: 0.0051889]
		[batch 20/20] avg loss: 0.5073758311084272		[learning rate: 0.0051795]
	Learning Rate: 0.00517952
	LOSS [training: 0.5638386113295133 | validation: 0.5222143485713454]
	TIME [epoch: 8.22 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6912408578814702		[learning rate: 0.0051701]
		[batch 20/20] avg loss: 0.605962966398266		[learning rate: 0.0051607]
	Learning Rate: 0.00516072
	LOSS [training: 0.6486019121398678 | validation: 0.78502296322318]
	TIME [epoch: 8.16 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9715226949256989		[learning rate: 0.0051513]
		[batch 20/20] avg loss: 0.45253399147619533		[learning rate: 0.005142]
	Learning Rate: 0.00514199
	LOSS [training: 0.7120283432009471 | validation: 0.6695032549202351]
	TIME [epoch: 8.18 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6105868344298307		[learning rate: 0.0051327]
		[batch 20/20] avg loss: 0.5476449361957868		[learning rate: 0.0051233]
	Learning Rate: 0.00512333
	LOSS [training: 0.5791158853128088 | validation: 0.412621735520202]
	TIME [epoch: 8.15 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6341147382438866		[learning rate: 0.005114]
		[batch 20/20] avg loss: 0.6775978772889651		[learning rate: 0.0051047]
	Learning Rate: 0.00510474
	LOSS [training: 0.6558563077664261 | validation: 0.7925106268701845]
	TIME [epoch: 8.15 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5243696322680602		[learning rate: 0.0050955]
		[batch 20/20] avg loss: 0.47442362858599774		[learning rate: 0.0050862]
	Learning Rate: 0.00508622
	LOSS [training: 0.49939663042702903 | validation: 0.8975736616961763]
	TIME [epoch: 8.15 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.585260221243056		[learning rate: 0.005077]
		[batch 20/20] avg loss: 0.5540591198242545		[learning rate: 0.0050678]
	Learning Rate: 0.00506776
	LOSS [training: 0.5696596705336552 | validation: 0.6478320872556215]
	TIME [epoch: 8.18 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5131616439752832		[learning rate: 0.0050586]
		[batch 20/20] avg loss: 0.44990317498425314		[learning rate: 0.0050494]
	Learning Rate: 0.00504937
	LOSS [training: 0.4815324094797681 | validation: 0.6031819006549857]
	TIME [epoch: 8.16 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5606132435889775		[learning rate: 0.0050402]
		[batch 20/20] avg loss: 0.4692746574086765		[learning rate: 0.005031]
	Learning Rate: 0.00503104
	LOSS [training: 0.5149439504988271 | validation: 0.6003883982692682]
	TIME [epoch: 8.18 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4853906548968864		[learning rate: 0.0050219]
		[batch 20/20] avg loss: 0.5361253830104472		[learning rate: 0.0050128]
	Learning Rate: 0.00501278
	LOSS [training: 0.5107580189536666 | validation: 0.7236098654776648]
	TIME [epoch: 8.18 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5750085901517442		[learning rate: 0.0050037]
		[batch 20/20] avg loss: 0.5574000245084785		[learning rate: 0.0049946]
	Learning Rate: 0.00499459
	LOSS [training: 0.5662043073301113 | validation: 0.8395327323899043]
	TIME [epoch: 8.17 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45863298401178987		[learning rate: 0.0049855]
		[batch 20/20] avg loss: 0.5170543937748662		[learning rate: 0.0049765]
	Learning Rate: 0.00497647
	LOSS [training: 0.48784368889332813 | validation: 0.5083300801102992]
	TIME [epoch: 8.2 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5695341900510963		[learning rate: 0.0049674]
		[batch 20/20] avg loss: 0.6796718765707762		[learning rate: 0.0049584]
	Learning Rate: 0.00495841
	LOSS [training: 0.6246030333109364 | validation: 0.8197877166592201]
	TIME [epoch: 8.2 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.543875989619571		[learning rate: 0.0049494]
		[batch 20/20] avg loss: 0.5367340739309027		[learning rate: 0.0049404]
	Learning Rate: 0.00494041
	LOSS [training: 0.5403050317752369 | validation: 0.40728745489646967]
	TIME [epoch: 8.17 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7599240678768272		[learning rate: 0.0049314]
		[batch 20/20] avg loss: 0.8090003899913029		[learning rate: 0.0049225]
	Learning Rate: 0.00492248
	LOSS [training: 0.784462228934065 | validation: 1.4502439799944642]
	TIME [epoch: 8.15 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7262158064862442		[learning rate: 0.0049135]
		[batch 20/20] avg loss: 0.5219100119848884		[learning rate: 0.0049046]
	Learning Rate: 0.00490462
	LOSS [training: 0.6240629092355663 | validation: 0.5364592291503094]
	TIME [epoch: 8.18 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4125055755840995		[learning rate: 0.0048957]
		[batch 20/20] avg loss: 0.5384279673057815		[learning rate: 0.0048868]
	Learning Rate: 0.00488682
	LOSS [training: 0.4754667714449406 | validation: 1.312102231787113]
	TIME [epoch: 8.16 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6972356475507453		[learning rate: 0.0048779]
		[batch 20/20] avg loss: 0.44251739019505365		[learning rate: 0.0048691]
	Learning Rate: 0.00486909
	LOSS [training: 0.5698765188728996 | validation: 0.8973018135193576]
	TIME [epoch: 8.15 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7125577730758585		[learning rate: 0.0048602]
		[batch 20/20] avg loss: 0.4391117094182201		[learning rate: 0.0048514]
	Learning Rate: 0.00485141
	LOSS [training: 0.5758347412470395 | validation: 0.5779703364197454]
	TIME [epoch: 8.16 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.612300419254183		[learning rate: 0.0048426]
		[batch 20/20] avg loss: 0.6124951412805129		[learning rate: 0.0048338]
	Learning Rate: 0.00483381
	LOSS [training: 0.6123977802673479 | validation: 0.6523332665594201]
	TIME [epoch: 8.19 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.536447664960451		[learning rate: 0.004825]
		[batch 20/20] avg loss: 0.6181985166107458		[learning rate: 0.0048163]
	Learning Rate: 0.00481627
	LOSS [training: 0.5773230907855984 | validation: 0.6954188118429073]
	TIME [epoch: 8.21 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4723507388558138		[learning rate: 0.0048075]
		[batch 20/20] avg loss: 0.5441904621331513		[learning rate: 0.0047988]
	Learning Rate: 0.00479879
	LOSS [training: 0.5082706004944826 | validation: 0.6191104872150612]
	TIME [epoch: 8.17 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46626576865618735		[learning rate: 0.0047901]
		[batch 20/20] avg loss: 0.47764234533291655		[learning rate: 0.0047814]
	Learning Rate: 0.00478137
	LOSS [training: 0.47195405699455195 | validation: 0.6184707146437327]
	TIME [epoch: 8.17 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4964010245411908		[learning rate: 0.0047727]
		[batch 20/20] avg loss: 0.4500826612614879		[learning rate: 0.004764]
	Learning Rate: 0.00476402
	LOSS [training: 0.4732418429013393 | validation: 0.6285375687081409]
	TIME [epoch: 8.2 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5313045421285836		[learning rate: 0.0047554]
		[batch 20/20] avg loss: 0.5629663095004125		[learning rate: 0.0047467]
	Learning Rate: 0.00474673
	LOSS [training: 0.5471354258144979 | validation: 0.7665983265249803]
	TIME [epoch: 8.21 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5639098614733872		[learning rate: 0.0047381]
		[batch 20/20] avg loss: 0.7367392819196017		[learning rate: 0.0047295]
	Learning Rate: 0.00472951
	LOSS [training: 0.6503245716964943 | validation: 0.587581705202791]
	TIME [epoch: 8.16 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45441006547112295		[learning rate: 0.0047209]
		[batch 20/20] avg loss: 0.6404167553696317		[learning rate: 0.0047123]
	Learning Rate: 0.00471234
	LOSS [training: 0.5474134104203774 | validation: 0.6589053173636471]
	TIME [epoch: 8.15 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5588843880582427		[learning rate: 0.0047038]
		[batch 20/20] avg loss: 0.5351527254101418		[learning rate: 0.0046952]
	Learning Rate: 0.00469524
	LOSS [training: 0.5470185567341923 | validation: 0.9029215518363045]
	TIME [epoch: 8.16 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6532488731657226		[learning rate: 0.0046867]
		[batch 20/20] avg loss: 0.4603370275395773		[learning rate: 0.0046782]
	Learning Rate: 0.0046782
	LOSS [training: 0.5567929503526499 | validation: 0.47530865720472004]
	TIME [epoch: 8.17 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42830263263920393		[learning rate: 0.0046697]
		[batch 20/20] avg loss: 0.6645656073349724		[learning rate: 0.0046612]
	Learning Rate: 0.00466122
	LOSS [training: 0.5464341199870882 | validation: 0.6066328826102436]
	TIME [epoch: 8.17 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6607225000615442		[learning rate: 0.0046528]
		[batch 20/20] avg loss: 0.6484855555637068		[learning rate: 0.0046443]
	Learning Rate: 0.00464431
	LOSS [training: 0.6546040278126253 | validation: 0.895236427377638]
	TIME [epoch: 8.18 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5648934069097115		[learning rate: 0.0046359]
		[batch 20/20] avg loss: 0.4371478570418814		[learning rate: 0.0046275]
	Learning Rate: 0.00462745
	LOSS [training: 0.5010206319757964 | validation: 0.6853443650230162]
	TIME [epoch: 8.19 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7670250758528818		[learning rate: 0.004619]
		[batch 20/20] avg loss: 0.5269745219396949		[learning rate: 0.0046107]
	Learning Rate: 0.00461066
	LOSS [training: 0.6469997988962882 | validation: 0.7300161147173829]
	TIME [epoch: 8.17 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5874161606766498		[learning rate: 0.0046023]
		[batch 20/20] avg loss: 0.6058639992762878		[learning rate: 0.0045939]
	Learning Rate: 0.00459393
	LOSS [training: 0.5966400799764687 | validation: 0.4343244616962234]
	TIME [epoch: 8.2 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5265157520336117		[learning rate: 0.0045856]
		[batch 20/20] avg loss: 0.5689697876919586		[learning rate: 0.0045773]
	Learning Rate: 0.00457726
	LOSS [training: 0.5477427698627853 | validation: 0.6997080930531041]
	TIME [epoch: 8.2 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5859741931935364		[learning rate: 0.0045689]
		[batch 20/20] avg loss: 0.4735247341534762		[learning rate: 0.0045606]
	Learning Rate: 0.00456065
	LOSS [training: 0.5297494636735063 | validation: 0.5437197266857201]
	TIME [epoch: 8.18 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6080393814173852		[learning rate: 0.0045524]
		[batch 20/20] avg loss: 0.5696994255590655		[learning rate: 0.0045441]
	Learning Rate: 0.00454409
	LOSS [training: 0.5888694034882254 | validation: 0.4032181248815636]
	TIME [epoch: 8.16 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47738824804167657		[learning rate: 0.0045358]
		[batch 20/20] avg loss: 0.5699413679560201		[learning rate: 0.0045276]
	Learning Rate: 0.0045276
	LOSS [training: 0.5236648079988483 | validation: 1.4156765513662177]
	TIME [epoch: 8.17 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.603541008593224		[learning rate: 0.0045194]
		[batch 20/20] avg loss: 0.444522672587528		[learning rate: 0.0045112]
	Learning Rate: 0.00451117
	LOSS [training: 0.524031840590376 | validation: 0.744481918695862]
	TIME [epoch: 8.17 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49617748360358094		[learning rate: 0.004503]
		[batch 20/20] avg loss: 0.6618916974729148		[learning rate: 0.0044948]
	Learning Rate: 0.0044948
	LOSS [training: 0.5790345905382478 | validation: 1.050356406385978]
	TIME [epoch: 8.16 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5835445191338666		[learning rate: 0.0044866]
		[batch 20/20] avg loss: 1.0326765405540512		[learning rate: 0.0044785]
	Learning Rate: 0.00447849
	LOSS [training: 0.8081105298439588 | validation: 0.5868077516460198]
	TIME [epoch: 8.16 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5201022065933373		[learning rate: 0.0044704]
		[batch 20/20] avg loss: 0.4555953492122288		[learning rate: 0.0044622]
	Learning Rate: 0.00446224
	LOSS [training: 0.48784877790278297 | validation: 0.5358929552256572]
	TIME [epoch: 8.16 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4459145837198176		[learning rate: 0.0044541]
		[batch 20/20] avg loss: 0.6557611966722457		[learning rate: 0.004446]
	Learning Rate: 0.00444604
	LOSS [training: 0.5508378901960317 | validation: 0.7405134543180961]
	TIME [epoch: 8.24 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8817420245087331		[learning rate: 0.004438]
		[batch 20/20] avg loss: 0.5073713485924387		[learning rate: 0.0044299]
	Learning Rate: 0.00442991
	LOSS [training: 0.6945566865505859 | validation: 0.6739106211864886]
	TIME [epoch: 8.17 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7755187499595156		[learning rate: 0.0044219]
		[batch 20/20] avg loss: 0.46950000598376274		[learning rate: 0.0044138]
	Learning Rate: 0.00441383
	LOSS [training: 0.6225093779716391 | validation: 0.42426633969463584]
	TIME [epoch: 8.17 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.694383203442307		[learning rate: 0.0044058]
		[batch 20/20] avg loss: 0.4584473946092813		[learning rate: 0.0043978]
	Learning Rate: 0.00439781
	LOSS [training: 0.5764152990257942 | validation: 0.6502741956448522]
	TIME [epoch: 8.17 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5757170621951685		[learning rate: 0.0043898]
		[batch 20/20] avg loss: 0.5067470264393941		[learning rate: 0.0043819]
	Learning Rate: 0.00438185
	LOSS [training: 0.5412320443172812 | validation: 0.4211406280376975]
	TIME [epoch: 8.24 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6060006424368753		[learning rate: 0.0043739]
		[batch 20/20] avg loss: 0.5245899091919012		[learning rate: 0.004366]
	Learning Rate: 0.00436595
	LOSS [training: 0.5652952758143883 | validation: 0.7380760530132618]
	TIME [epoch: 8.17 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48798717248878526		[learning rate: 0.004358]
		[batch 20/20] avg loss: 0.5668218959662104		[learning rate: 0.0043501]
	Learning Rate: 0.00435011
	LOSS [training: 0.5274045342274978 | validation: 0.3198016531412149]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.742528054578844		[learning rate: 0.0043422]
		[batch 20/20] avg loss: 0.7465440304952359		[learning rate: 0.0043343]
	Learning Rate: 0.00433432
	LOSS [training: 0.7445360425370398 | validation: 0.4064360563766482]
	TIME [epoch: 8.16 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47195019237051483		[learning rate: 0.0043264]
		[batch 20/20] avg loss: 0.5639223858483962		[learning rate: 0.0043186]
	Learning Rate: 0.00431859
	LOSS [training: 0.5179362891094554 | validation: 0.4891325436525032]
	TIME [epoch: 8.16 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5523558103136178		[learning rate: 0.0043107]
		[batch 20/20] avg loss: 0.6726535686170678		[learning rate: 0.0043029]
	Learning Rate: 0.00430292
	LOSS [training: 0.6125046894653429 | validation: 0.9443155825518736]
	TIME [epoch: 8.18 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5774541532993923		[learning rate: 0.0042951]
		[batch 20/20] avg loss: 0.4202015806798071		[learning rate: 0.0042873]
	Learning Rate: 0.0042873
	LOSS [training: 0.4988278669895997 | validation: 0.2754733480695762]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.506634162640111		[learning rate: 0.0042795]
		[batch 20/20] avg loss: 0.449658896152869		[learning rate: 0.0042717]
	Learning Rate: 0.00427174
	LOSS [training: 0.47814652939649005 | validation: 0.41081777827647287]
	TIME [epoch: 8.2 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5150945578604376		[learning rate: 0.004264]
		[batch 20/20] avg loss: 0.5458302005372845		[learning rate: 0.0042562]
	Learning Rate: 0.00425624
	LOSS [training: 0.530462379198861 | validation: 0.9855265695606701]
	TIME [epoch: 8.22 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5539748588987614		[learning rate: 0.0042485]
		[batch 20/20] avg loss: 0.4050512178550603		[learning rate: 0.0042408]
	Learning Rate: 0.0042408
	LOSS [training: 0.47951303837691084 | validation: 0.3626140368704205]
	TIME [epoch: 8.22 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46235633315022884		[learning rate: 0.0042331]
		[batch 20/20] avg loss: 0.509001634219109		[learning rate: 0.0042254]
	Learning Rate: 0.00422541
	LOSS [training: 0.4856789836846689 | validation: 0.32754751597087]
	TIME [epoch: 8.2 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5662649312623499		[learning rate: 0.0042177]
		[batch 20/20] avg loss: 0.5020887915045454		[learning rate: 0.0042101]
	Learning Rate: 0.00421007
	LOSS [training: 0.5341768613834478 | validation: 0.2993812662824611]
	TIME [epoch: 8.21 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3949055723381788		[learning rate: 0.0042024]
		[batch 20/20] avg loss: 0.49178970877586703		[learning rate: 0.0041948]
	Learning Rate: 0.00419479
	LOSS [training: 0.44334764055702286 | validation: 0.24047371992709815]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6947937455037365		[learning rate: 0.0041872]
		[batch 20/20] avg loss: 0.39986880408537806		[learning rate: 0.0041796]
	Learning Rate: 0.00417957
	LOSS [training: 0.5473312747945573 | validation: 0.321861798601512]
	TIME [epoch: 8.2 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4802907203193681		[learning rate: 0.004172]
		[batch 20/20] avg loss: 0.3767806680488239		[learning rate: 0.0041644]
	Learning Rate: 0.0041644
	LOSS [training: 0.4285356941840959 | validation: 0.49968379226389265]
	TIME [epoch: 8.22 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5702565430860531		[learning rate: 0.0041568]
		[batch 20/20] avg loss: 0.4447346489443163		[learning rate: 0.0041493]
	Learning Rate: 0.00414929
	LOSS [training: 0.5074955960151847 | validation: 0.2765020589898278]
	TIME [epoch: 8.19 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6090468619745656		[learning rate: 0.0041418]
		[batch 20/20] avg loss: 0.3987163629154766		[learning rate: 0.0041342]
	Learning Rate: 0.00413423
	LOSS [training: 0.5038816124450212 | validation: 1.2126885973064572]
	TIME [epoch: 8.18 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6063925529815646		[learning rate: 0.0041267]
		[batch 20/20] avg loss: 0.30293151095022974		[learning rate: 0.0041192]
	Learning Rate: 0.00411923
	LOSS [training: 0.4546620319658971 | validation: 0.25415540757008953]
	TIME [epoch: 8.2 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4683580503360122		[learning rate: 0.0041117]
		[batch 20/20] avg loss: 0.5666054019710267		[learning rate: 0.0041043]
	Learning Rate: 0.00410428
	LOSS [training: 0.5174817261535194 | validation: 0.3275191374807805]
	TIME [epoch: 8.22 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4061378339045838		[learning rate: 0.0040968]
		[batch 20/20] avg loss: 0.5109991117924396		[learning rate: 0.0040894]
	Learning Rate: 0.00408938
	LOSS [training: 0.45856847284851165 | validation: 0.7409323527260236]
	TIME [epoch: 8.2 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6332686226512866		[learning rate: 0.004082]
		[batch 20/20] avg loss: 0.4275651748300243		[learning rate: 0.0040745]
	Learning Rate: 0.00407454
	LOSS [training: 0.5304168987406555 | validation: 0.3117492211407391]
	TIME [epoch: 8.17 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4538102532041669		[learning rate: 0.0040671]
		[batch 20/20] avg loss: 0.4822852279586364		[learning rate: 0.0040598]
	Learning Rate: 0.00405976
	LOSS [training: 0.4680477405814017 | validation: 0.3905583283493296]
	TIME [epoch: 8.15 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4703409065937347		[learning rate: 0.0040524]
		[batch 20/20] avg loss: 0.419707131214353		[learning rate: 0.004045]
	Learning Rate: 0.00404502
	LOSS [training: 0.44502401890404386 | validation: 0.520798490830888]
	TIME [epoch: 8.17 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46411958459179325		[learning rate: 0.0040377]
		[batch 20/20] avg loss: 0.41030920283003286		[learning rate: 0.0040303]
	Learning Rate: 0.00403034
	LOSS [training: 0.4372143937109131 | validation: 0.5404050713323592]
	TIME [epoch: 8.19 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47861552224135356		[learning rate: 0.004023]
		[batch 20/20] avg loss: 0.4601244429686374		[learning rate: 0.0040157]
	Learning Rate: 0.00401572
	LOSS [training: 0.4693699826049954 | validation: 0.4470846419748655]
	TIME [epoch: 8.16 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40131965308393236		[learning rate: 0.0040084]
		[batch 20/20] avg loss: 0.4497794311603799		[learning rate: 0.0040011]
	Learning Rate: 0.00400114
	LOSS [training: 0.4255495421221561 | validation: 0.7724007299107898]
	TIME [epoch: 8.14 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36048843177667056		[learning rate: 0.0039939]
		[batch 20/20] avg loss: 0.4090165441619698		[learning rate: 0.0039866]
	Learning Rate: 0.00398662
	LOSS [training: 0.38475248796932016 | validation: 0.1793033145333692]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3925449416542068		[learning rate: 0.0039794]
		[batch 20/20] avg loss: 0.5289494854219083		[learning rate: 0.0039722]
	Learning Rate: 0.00397216
	LOSS [training: 0.4607472135380576 | validation: 0.44333917475945406]
	TIME [epoch: 8.17 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5433664323683499		[learning rate: 0.0039649]
		[batch 20/20] avg loss: 0.33911900717634297		[learning rate: 0.0039577]
	Learning Rate: 0.00395774
	LOSS [training: 0.4412427197723464 | validation: 0.42887531750678687]
	TIME [epoch: 8.13 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37469401163246024		[learning rate: 0.0039506]
		[batch 20/20] avg loss: 0.4062419142129638		[learning rate: 0.0039434]
	Learning Rate: 0.00394338
	LOSS [training: 0.390467962922712 | validation: 0.567816563067545]
	TIME [epoch: 8.13 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4456213674195354		[learning rate: 0.0039362]
		[batch 20/20] avg loss: 0.3895203767398531		[learning rate: 0.0039291]
	Learning Rate: 0.00392907
	LOSS [training: 0.41757087207969423 | validation: 0.4100654699047553]
	TIME [epoch: 8.19 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3802923625636715		[learning rate: 0.0039219]
		[batch 20/20] avg loss: 0.4280054900619322		[learning rate: 0.0039148]
	Learning Rate: 0.00391481
	LOSS [training: 0.4041489263128019 | validation: 0.4143057343439601]
	TIME [epoch: 8.17 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3787177130057887		[learning rate: 0.0039077]
		[batch 20/20] avg loss: 0.3743226339150941		[learning rate: 0.0039006]
	Learning Rate: 0.0039006
	LOSS [training: 0.3765201734604414 | validation: 0.9442468151275731]
	TIME [epoch: 8.14 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35710079371664777		[learning rate: 0.0038935]
		[batch 20/20] avg loss: 0.3413484439301891		[learning rate: 0.0038864]
	Learning Rate: 0.00388645
	LOSS [training: 0.3492246188234185 | validation: 0.4580986729660729]
	TIME [epoch: 8.14 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4082202566984314		[learning rate: 0.0038794]
		[batch 20/20] avg loss: 0.44993759089340124		[learning rate: 0.0038723]
	Learning Rate: 0.00387234
	LOSS [training: 0.4290789237959164 | validation: 0.38550048731961856]
	TIME [epoch: 8.19 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6729951464629998		[learning rate: 0.0038653]
		[batch 20/20] avg loss: 0.35218740059096143		[learning rate: 0.0038583]
	Learning Rate: 0.00385829
	LOSS [training: 0.5125912735269804 | validation: 0.41842190848146643]
	TIME [epoch: 8.15 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3566422514514249		[learning rate: 0.0038513]
		[batch 20/20] avg loss: 0.3949979136831673		[learning rate: 0.0038443]
	Learning Rate: 0.00384429
	LOSS [training: 0.37582008256729615 | validation: 0.67217694334346]
	TIME [epoch: 8.14 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.451799046686054		[learning rate: 0.0038373]
		[batch 20/20] avg loss: 0.30640353283915156		[learning rate: 0.0038303]
	Learning Rate: 0.00383034
	LOSS [training: 0.37910128976260277 | validation: 0.3308736140276548]
	TIME [epoch: 8.13 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5193432696309002		[learning rate: 0.0038234]
		[batch 20/20] avg loss: 0.36217440585304517		[learning rate: 0.0038164]
	Learning Rate: 0.00381644
	LOSS [training: 0.4407588377419728 | validation: 0.6554971880939453]
	TIME [epoch: 8.14 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4718465464108542		[learning rate: 0.0038095]
		[batch 20/20] avg loss: 0.4965398217021549		[learning rate: 0.0038026]
	Learning Rate: 0.00380258
	LOSS [training: 0.48419318405650447 | validation: 0.41863136135539253]
	TIME [epoch: 8.14 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5397531966010801		[learning rate: 0.0037957]
		[batch 20/20] avg loss: 0.47033668102550086		[learning rate: 0.0037888]
	Learning Rate: 0.00378879
	LOSS [training: 0.5050449388132905 | validation: 0.45873789816082156]
	TIME [epoch: 8.16 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3619649520922626		[learning rate: 0.0037819]
		[batch 20/20] avg loss: 0.4194446763892974		[learning rate: 0.003775]
	Learning Rate: 0.00377504
	LOSS [training: 0.39070481424077996 | validation: 0.5096899409804333]
	TIME [epoch: 8.18 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38506994560277674		[learning rate: 0.0037682]
		[batch 20/20] avg loss: 0.33431892637765653		[learning rate: 0.0037613]
	Learning Rate: 0.00376134
	LOSS [training: 0.3596944359902166 | validation: 0.5765256447973854]
	TIME [epoch: 8.14 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45899504855974343		[learning rate: 0.0037545]
		[batch 20/20] avg loss: 0.35959606857385373		[learning rate: 0.0037477]
	Learning Rate: 0.00374769
	LOSS [training: 0.4092955585667985 | validation: 0.6099291415905823]
	TIME [epoch: 8.14 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38923949851018425		[learning rate: 0.0037409]
		[batch 20/20] avg loss: 0.3822247448890675		[learning rate: 0.0037341]
	Learning Rate: 0.00373408
	LOSS [training: 0.3857321216996258 | validation: 0.40815309472330885]
	TIME [epoch: 8.17 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36408261095571814		[learning rate: 0.0037273]
		[batch 20/20] avg loss: 0.3697394192342407		[learning rate: 0.0037205]
	Learning Rate: 0.00372053
	LOSS [training: 0.3669110150949795 | validation: 0.6048840624766209]
	TIME [epoch: 8.2 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4544240672687795		[learning rate: 0.0037138]
		[batch 20/20] avg loss: 0.5002047546502032		[learning rate: 0.003707]
	Learning Rate: 0.00370703
	LOSS [training: 0.47731441095949123 | validation: 0.7792618895793668]
	TIME [epoch: 8.14 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4404886344315197		[learning rate: 0.0037003]
		[batch 20/20] avg loss: 0.5508609760955595		[learning rate: 0.0036936]
	Learning Rate: 0.00369358
	LOSS [training: 0.4956748052635396 | validation: 0.32178687109731086]
	TIME [epoch: 8.13 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3946927989593537		[learning rate: 0.0036869]
		[batch 20/20] avg loss: 0.3931117100426902		[learning rate: 0.0036802]
	Learning Rate: 0.00368017
	LOSS [training: 0.3939022545010219 | validation: 0.32674763264924256]
	TIME [epoch: 8.14 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4638741814353045		[learning rate: 0.0036735]
		[batch 20/20] avg loss: 0.3459299001752535		[learning rate: 0.0036668]
	Learning Rate: 0.00366682
	LOSS [training: 0.4049020408052789 | validation: 0.6967961855966623]
	TIME [epoch: 8.16 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3431407086848544		[learning rate: 0.0036602]
		[batch 20/20] avg loss: 0.3569323519259366		[learning rate: 0.0036535]
	Learning Rate: 0.00365351
	LOSS [training: 0.3500365303053955 | validation: 0.43640036724644693]
	TIME [epoch: 8.14 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39808173363170163		[learning rate: 0.0036469]
		[batch 20/20] avg loss: 0.4740644277010479		[learning rate: 0.0036403]
	Learning Rate: 0.00364025
	LOSS [training: 0.43607308066637473 | validation: 0.4423634941203829]
	TIME [epoch: 8.14 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4096365919911154		[learning rate: 0.0036336]
		[batch 20/20] avg loss: 0.3247039196999589		[learning rate: 0.003627]
	Learning Rate: 0.00362704
	LOSS [training: 0.3671702558455371 | validation: 0.4079034518215522]
	TIME [epoch: 8.18 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3577668561435356		[learning rate: 0.0036205]
		[batch 20/20] avg loss: 0.3386041251888865		[learning rate: 0.0036139]
	Learning Rate: 0.00361388
	LOSS [training: 0.3481854906662111 | validation: 0.5646868422771498]
	TIME [epoch: 8.18 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3959221005397906		[learning rate: 0.0036073]
		[batch 20/20] avg loss: 0.4026531599616249		[learning rate: 0.0036008]
	Learning Rate: 0.00360076
	LOSS [training: 0.39928763025070774 | validation: 0.26371905319199923]
	TIME [epoch: 8.14 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4491527428777019		[learning rate: 0.0035942]
		[batch 20/20] avg loss: 0.4695391246541874		[learning rate: 0.0035877]
	Learning Rate: 0.0035877
	LOSS [training: 0.4593459337659446 | validation: 0.3875370822983327]
	TIME [epoch: 8.14 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3824039013269559		[learning rate: 0.0035812]
		[batch 20/20] avg loss: 0.4969636239195139		[learning rate: 0.0035747]
	Learning Rate: 0.00357468
	LOSS [training: 0.43968376262323494 | validation: 1.0618031127917513]
	TIME [epoch: 8.19 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41252189996180605		[learning rate: 0.0035682]
		[batch 20/20] avg loss: 0.4088957252638233		[learning rate: 0.0035617]
	Learning Rate: 0.0035617
	LOSS [training: 0.4107088126128146 | validation: 0.3991275679645789]
	TIME [epoch: 8.15 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32356024840840814		[learning rate: 0.0035552]
		[batch 20/20] avg loss: 0.4804618160008743		[learning rate: 0.0035488]
	Learning Rate: 0.00354878
	LOSS [training: 0.4020110322046412 | validation: 1.1257340631048813]
	TIME [epoch: 8.17 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3926754300609706		[learning rate: 0.0035423]
		[batch 20/20] avg loss: 0.49461838669397923		[learning rate: 0.0035359]
	Learning Rate: 0.0035359
	LOSS [training: 0.44364690837747495 | validation: 0.5135438187537527]
	TIME [epoch: 8.14 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.443940437401731		[learning rate: 0.0035295]
		[batch 20/20] avg loss: 0.41645277365885186		[learning rate: 0.0035231]
	Learning Rate: 0.00352307
	LOSS [training: 0.43019660553029143 | validation: 0.32606966218891875]
	TIME [epoch: 8.14 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31261895777061444		[learning rate: 0.0035167]
		[batch 20/20] avg loss: 0.40212083966034085		[learning rate: 0.0035103]
	Learning Rate: 0.00351028
	LOSS [training: 0.3573698987154777 | validation: 0.39977113230537487]
	TIME [epoch: 8.13 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3306912737484064		[learning rate: 0.0035039]
		[batch 20/20] avg loss: 0.33548536324562167		[learning rate: 0.0034975]
	Learning Rate: 0.00349754
	LOSS [training: 0.33308831849701404 | validation: 0.4064280718007618]
	TIME [epoch: 8.16 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3334153869314731		[learning rate: 0.0034912]
		[batch 20/20] avg loss: 0.40996209749037915		[learning rate: 0.0034849]
	Learning Rate: 0.00348485
	LOSS [training: 0.3716887422109261 | validation: 0.46006229349884253]
	TIME [epoch: 8.14 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31496003551161267		[learning rate: 0.0034785]
		[batch 20/20] avg loss: 0.308528417172446		[learning rate: 0.0034722]
	Learning Rate: 0.0034722
	LOSS [training: 0.31174422634202936 | validation: 0.8846124284707466]
	TIME [epoch: 8.18 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38592202234747763		[learning rate: 0.0034659]
		[batch 20/20] avg loss: 0.504479960341434		[learning rate: 0.0034596]
	Learning Rate: 0.0034596
	LOSS [training: 0.4452009913444558 | validation: 0.5341386007256467]
	TIME [epoch: 8.14 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3352298755930305		[learning rate: 0.0034533]
		[batch 20/20] avg loss: 0.38136448981675897		[learning rate: 0.003447]
	Learning Rate: 0.00344705
	LOSS [training: 0.3582971827048948 | validation: 0.6864823396461941]
	TIME [epoch: 8.15 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49063316463335716		[learning rate: 0.0034408]
		[batch 20/20] avg loss: 0.418638178724284		[learning rate: 0.0034345]
	Learning Rate: 0.00343454
	LOSS [training: 0.4546356716788206 | validation: 0.21129026901045844]
	TIME [epoch: 8.18 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4419482086373098		[learning rate: 0.0034283]
		[batch 20/20] avg loss: 0.3408820240699153		[learning rate: 0.0034221]
	Learning Rate: 0.00342207
	LOSS [training: 0.3914151163536126 | validation: 0.44394320442317053]
	TIME [epoch: 8.17 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34088530752382695		[learning rate: 0.0034159]
		[batch 20/20] avg loss: 0.4231627192739462		[learning rate: 0.0034097]
	Learning Rate: 0.00340966
	LOSS [training: 0.38202401339888664 | validation: 0.5012793687346853]
	TIME [epoch: 8.14 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42854917605018805		[learning rate: 0.0034035]
		[batch 20/20] avg loss: 0.3254782953630307		[learning rate: 0.0033973]
	Learning Rate: 0.00339728
	LOSS [training: 0.3770137357066093 | validation: 0.2487707847843066]
	TIME [epoch: 8.13 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3645066718516517		[learning rate: 0.0033911]
		[batch 20/20] avg loss: 0.45910282184932444		[learning rate: 0.003385]
	Learning Rate: 0.00338495
	LOSS [training: 0.41180474685048807 | validation: 0.27039245133762824]
	TIME [epoch: 8.17 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42890885815767665		[learning rate: 0.0033788]
		[batch 20/20] avg loss: 0.34541347299064556		[learning rate: 0.0033727]
	Learning Rate: 0.00337267
	LOSS [training: 0.3871611655741612 | validation: 0.40720450336606656]
	TIME [epoch: 8.13 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38476703299992465		[learning rate: 0.0033665]
		[batch 20/20] avg loss: 0.3852550117097371		[learning rate: 0.0033604]
	Learning Rate: 0.00336043
	LOSS [training: 0.38501102235483087 | validation: 0.3694879485147601]
	TIME [epoch: 8.14 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3441743149151323		[learning rate: 0.0033543]
		[batch 20/20] avg loss: 0.4353126647018749		[learning rate: 0.0033482]
	Learning Rate: 0.00334823
	LOSS [training: 0.3897434898085036 | validation: 0.34706806381891947]
	TIME [epoch: 8.14 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3227157885587548		[learning rate: 0.0033422]
		[batch 20/20] avg loss: 0.2976894348396706		[learning rate: 0.0033361]
	Learning Rate: 0.00333608
	LOSS [training: 0.31020261169921276 | validation: 0.7199028038796226]
	TIME [epoch: 8.18 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7663068520866385		[learning rate: 0.00333]
		[batch 20/20] avg loss: 0.4720282150293979		[learning rate: 0.003324]
	Learning Rate: 0.00332398
	LOSS [training: 0.6191675335580181 | validation: 0.4397114170754174]
	TIME [epoch: 8.18 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32043937832326225		[learning rate: 0.0033179]
		[batch 20/20] avg loss: 0.4073004173991671		[learning rate: 0.0033119]
	Learning Rate: 0.00331191
	LOSS [training: 0.3638698978612147 | validation: 0.5484730908568762]
	TIME [epoch: 8.15 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3099166230846335		[learning rate: 0.0033059]
		[batch 20/20] avg loss: 0.4024605275849333		[learning rate: 0.0032999]
	Learning Rate: 0.00329989
	LOSS [training: 0.3561885753347834 | validation: 0.5007289024646824]
	TIME [epoch: 8.15 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3861567994798909		[learning rate: 0.0032939]
		[batch 20/20] avg loss: 0.4184247467579051		[learning rate: 0.0032879]
	Learning Rate: 0.00328792
	LOSS [training: 0.40229077311889805 | validation: 0.5167340610045505]
	TIME [epoch: 8.18 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3783046109106144		[learning rate: 0.0032819]
		[batch 20/20] avg loss: 0.3770654371359768		[learning rate: 0.003276]
	Learning Rate: 0.00327599
	LOSS [training: 0.3776850240232957 | validation: 0.37604844968284507]
	TIME [epoch: 8.18 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42248531016613916		[learning rate: 0.00327]
		[batch 20/20] avg loss: 0.36006979267954475		[learning rate: 0.0032641]
	Learning Rate: 0.0032641
	LOSS [training: 0.391277551422842 | validation: 0.292929252374409]
	TIME [epoch: 8.14 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3780438173133788		[learning rate: 0.0032582]
		[batch 20/20] avg loss: 0.34094785030295555		[learning rate: 0.0032523]
	Learning Rate: 0.00325225
	LOSS [training: 0.35949583380816713 | validation: 0.23984824508152885]
	TIME [epoch: 8.14 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4643834229823932		[learning rate: 0.0032463]
		[batch 20/20] avg loss: 0.5766686366713037		[learning rate: 0.0032404]
	Learning Rate: 0.00324045
	LOSS [training: 0.5205260298268486 | validation: 0.9713658040896559]
	TIME [epoch: 8.14 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4319953406080028		[learning rate: 0.0032346]
		[batch 20/20] avg loss: 0.3707780781683174		[learning rate: 0.0032287]
	Learning Rate: 0.00322869
	LOSS [training: 0.4013867093881601 | validation: 0.36593346928359083]
	TIME [epoch: 8.15 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3322950427939727		[learning rate: 0.0032228]
		[batch 20/20] avg loss: 0.31971897557094475		[learning rate: 0.003217]
	Learning Rate: 0.00321697
	LOSS [training: 0.32600700918245873 | validation: 0.7077931858786187]
	TIME [epoch: 8.17 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39575373972884426		[learning rate: 0.0032111]
		[batch 20/20] avg loss: 0.4240261225826398		[learning rate: 0.0032053]
	Learning Rate: 0.0032053
	LOSS [training: 0.409889931155742 | validation: 0.7728964967078172]
	TIME [epoch: 8.18 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3829683397619982		[learning rate: 0.0031995]
		[batch 20/20] avg loss: 0.3243048699054457		[learning rate: 0.0031937]
	Learning Rate: 0.00319367
	LOSS [training: 0.3536366048337219 | validation: 0.6387315821006212]
	TIME [epoch: 8.14 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3607032574280852		[learning rate: 0.0031879]
		[batch 20/20] avg loss: 0.45574681270434264		[learning rate: 0.0031821]
	Learning Rate: 0.00318208
	LOSS [training: 0.40822503506621394 | validation: 0.38761767366100397]
	TIME [epoch: 8.14 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3766510849802209		[learning rate: 0.0031763]
		[batch 20/20] avg loss: 0.3630146370856359		[learning rate: 0.0031705]
	Learning Rate: 0.00317053
	LOSS [training: 0.3698328610329284 | validation: 0.27079514311895936]
	TIME [epoch: 8.2 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4008125280693712		[learning rate: 0.0031648]
		[batch 20/20] avg loss: 0.34515145519205664		[learning rate: 0.003159]
	Learning Rate: 0.00315902
	LOSS [training: 0.3729819916307139 | validation: 0.35774714175808625]
	TIME [epoch: 8.17 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3912170212413371		[learning rate: 0.0031533]
		[batch 20/20] avg loss: 0.3774449337330813		[learning rate: 0.0031476]
	Learning Rate: 0.00314756
	LOSS [training: 0.38433097748720924 | validation: 0.2731067630975908]
	TIME [epoch: 8.13 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3414100274486812		[learning rate: 0.0031418]
		[batch 20/20] avg loss: 0.34206618773676023		[learning rate: 0.0031361]
	Learning Rate: 0.00313613
	LOSS [training: 0.3417381075927207 | validation: 0.8606972614676328]
	TIME [epoch: 8.14 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2879100987669354		[learning rate: 0.0031304]
		[batch 20/20] avg loss: 0.4304846414849088		[learning rate: 0.0031248]
	Learning Rate: 0.00312475
	LOSS [training: 0.359197370125922 | validation: 0.35351997869590734]
	TIME [epoch: 8.17 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4960587129436016		[learning rate: 0.0031191]
		[batch 20/20] avg loss: 0.3423516950129936		[learning rate: 0.0031134]
	Learning Rate: 0.00311341
	LOSS [training: 0.41920520397829775 | validation: 0.5931178036974784]
	TIME [epoch: 8.14 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37066806762574633		[learning rate: 0.0031078]
		[batch 20/20] avg loss: 0.7286024020666091		[learning rate: 0.0031021]
	Learning Rate: 0.00310212
	LOSS [training: 0.5496352348461776 | validation: 0.7109022212293916]
	TIME [epoch: 8.14 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3421861652992187		[learning rate: 0.0030965]
		[batch 20/20] avg loss: 0.278159567114336		[learning rate: 0.0030909]
	Learning Rate: 0.00309086
	LOSS [training: 0.31017286620677736 | validation: 0.24072612121162848]
	TIME [epoch: 8.14 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3472778983244914		[learning rate: 0.0030852]
		[batch 20/20] avg loss: 0.3361294263154494		[learning rate: 0.0030796]
	Learning Rate: 0.00307964
	LOSS [training: 0.3417036623199704 | validation: 0.3598207711193959]
	TIME [epoch: 8.18 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2796493845119242		[learning rate: 0.003074]
		[batch 20/20] avg loss: 0.3395517876470966		[learning rate: 0.0030685]
	Learning Rate: 0.00306846
	LOSS [training: 0.30960058607951046 | validation: 0.28740279746413205]
	TIME [epoch: 8.21 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33962575437633047		[learning rate: 0.0030629]
		[batch 20/20] avg loss: 0.4542037284409474		[learning rate: 0.0030573]
	Learning Rate: 0.00305733
	LOSS [training: 0.39691474140863886 | validation: 0.5680790447028712]
	TIME [epoch: 8.14 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3716079976873601		[learning rate: 0.0030518]
		[batch 20/20] avg loss: 0.34263431969616903		[learning rate: 0.0030462]
	Learning Rate: 0.00304623
	LOSS [training: 0.35712115869176453 | validation: 0.6099297844380946]
	TIME [epoch: 8.15 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35330396050721835		[learning rate: 0.0030407]
		[batch 20/20] avg loss: 0.4118017007108595		[learning rate: 0.0030352]
	Learning Rate: 0.00303518
	LOSS [training: 0.3825528306090389 | validation: 0.22717312464974293]
	TIME [epoch: 8.17 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47137708627464514		[learning rate: 0.0030297]
		[batch 20/20] avg loss: 0.2900066833830148		[learning rate: 0.0030242]
	Learning Rate: 0.00302416
	LOSS [training: 0.38069188482883 | validation: 0.31888526397946376]
	TIME [epoch: 8.2 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3281567490950786		[learning rate: 0.0030187]
		[batch 20/20] avg loss: 0.2953134059697599		[learning rate: 0.0030132]
	Learning Rate: 0.00301319
	LOSS [training: 0.31173507753241925 | validation: 0.3975484311967181]
	TIME [epoch: 8.14 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3677528276989947		[learning rate: 0.0030077]
		[batch 20/20] avg loss: 0.45517993503592685		[learning rate: 0.0030023]
	Learning Rate: 0.00300225
	LOSS [training: 0.4114663813674609 | validation: 0.26097266246863954]
	TIME [epoch: 8.15 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32187089886830067		[learning rate: 0.0029968]
		[batch 20/20] avg loss: 0.4545057842113346		[learning rate: 0.0029914]
	Learning Rate: 0.00299136
	LOSS [training: 0.3881883415398177 | validation: 0.5952083061571117]
	TIME [epoch: 8.14 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3394818714920117		[learning rate: 0.0029859]
		[batch 20/20] avg loss: 0.38920506724486675		[learning rate: 0.0029805]
	Learning Rate: 0.0029805
	LOSS [training: 0.3643434693684392 | validation: 0.37012138828341884]
	TIME [epoch: 8.15 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2988743000356112		[learning rate: 0.0029751]
		[batch 20/20] avg loss: 0.33674664063764326		[learning rate: 0.0029697]
	Learning Rate: 0.00296969
	LOSS [training: 0.31781047033662724 | validation: 0.6665894863132448]
	TIME [epoch: 8.19 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45574553035258053		[learning rate: 0.0029643]
		[batch 20/20] avg loss: 0.3834653193813149		[learning rate: 0.0029589]
	Learning Rate: 0.00295891
	LOSS [training: 0.4196054248669478 | validation: 0.27868703429337727]
	TIME [epoch: 8.15 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3152579987691484		[learning rate: 0.0029535]
		[batch 20/20] avg loss: 0.2957758659629828		[learning rate: 0.0029482]
	Learning Rate: 0.00294817
	LOSS [training: 0.3055169323660656 | validation: 0.23600357634510583]
	TIME [epoch: 8.15 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3061094157569132		[learning rate: 0.0029428]
		[batch 20/20] avg loss: 0.36227210440405744		[learning rate: 0.0029375]
	Learning Rate: 0.00293747
	LOSS [training: 0.3341907600804853 | validation: 0.2567575936100867]
	TIME [epoch: 8.15 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4203387573508864		[learning rate: 0.0029321]
		[batch 20/20] avg loss: 0.30841131373451014		[learning rate: 0.0029268]
	Learning Rate: 0.00292681
	LOSS [training: 0.36437503554269823 | validation: 0.32959050730606293]
	TIME [epoch: 8.23 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3176651486658309		[learning rate: 0.0029215]
		[batch 20/20] avg loss: 0.2831703408136847		[learning rate: 0.0029162]
	Learning Rate: 0.00291619
	LOSS [training: 0.30041774473975785 | validation: 0.26005837080949357]
	TIME [epoch: 8.14 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.407412159092899		[learning rate: 0.0029109]
		[batch 20/20] avg loss: 0.3613435652990633		[learning rate: 0.0029056]
	Learning Rate: 0.00290561
	LOSS [training: 0.3843778621959812 | validation: 0.245912590192372]
	TIME [epoch: 8.14 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3800446946069547		[learning rate: 0.0029003]
		[batch 20/20] avg loss: 0.33488361142550643		[learning rate: 0.0028951]
	Learning Rate: 0.00289506
	LOSS [training: 0.3574641530162306 | validation: 0.6040861949616179]
	TIME [epoch: 8.14 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28718897084115264		[learning rate: 0.0028898]
		[batch 20/20] avg loss: 0.3324509699558374		[learning rate: 0.0028846]
	Learning Rate: 0.00288456
	LOSS [training: 0.30981997039849496 | validation: 0.3387665220993852]
	TIME [epoch: 8.16 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2856463601317297		[learning rate: 0.0028793]
		[batch 20/20] avg loss: 0.4218794975981564		[learning rate: 0.0028741]
	Learning Rate: 0.00287409
	LOSS [training: 0.35376292886494304 | validation: 0.42562839111013123]
	TIME [epoch: 8.15 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3842847528764711		[learning rate: 0.0028689]
		[batch 20/20] avg loss: 0.2833710345345176		[learning rate: 0.0028637]
	Learning Rate: 0.00286366
	LOSS [training: 0.33382789370549437 | validation: 0.3579580889244125]
	TIME [epoch: 8.14 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29418686670782757		[learning rate: 0.0028585]
		[batch 20/20] avg loss: 0.31786624840472655		[learning rate: 0.0028533]
	Learning Rate: 0.00285326
	LOSS [training: 0.30602655755627706 | validation: 0.2476091755314359]
	TIME [epoch: 8.14 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34640809152956775		[learning rate: 0.0028481]
		[batch 20/20] avg loss: 0.3120253177075613		[learning rate: 0.0028429]
	Learning Rate: 0.00284291
	LOSS [training: 0.3292167046185645 | validation: 0.1751710599429245]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3170885799072772		[learning rate: 0.0028377]
		[batch 20/20] avg loss: 0.3143897078063737		[learning rate: 0.0028326]
	Learning Rate: 0.00283259
	LOSS [training: 0.31573914385682544 | validation: 0.17489984019042187]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_447.pth
	Model improved!!!
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41288823655437995		[learning rate: 0.0028274]
		[batch 20/20] avg loss: 0.3402741277651894		[learning rate: 0.0028223]
	Learning Rate: 0.00282231
	LOSS [training: 0.3765811821597846 | validation: 0.27644691220965006]
	TIME [epoch: 8.15 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34687700475205174		[learning rate: 0.0028172]
		[batch 20/20] avg loss: 0.4098052725457414		[learning rate: 0.0028121]
	Learning Rate: 0.00281207
	LOSS [training: 0.37834113864889657 | validation: 0.3862319093795825]
	TIME [epoch: 8.15 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3082158453303222		[learning rate: 0.002807]
		[batch 20/20] avg loss: 0.3410390393238858		[learning rate: 0.0028019]
	Learning Rate: 0.00280187
	LOSS [training: 0.32462744232710394 | validation: 0.2880829438472079]
	TIME [epoch: 8.17 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36892126393160024		[learning rate: 0.0027968]
		[batch 20/20] avg loss: 0.34902924399778235		[learning rate: 0.0027917]
	Learning Rate: 0.0027917
	LOSS [training: 0.35897525396469127 | validation: 0.3875661706106415]
	TIME [epoch: 8.18 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30975130170012344		[learning rate: 0.0027866]
		[batch 20/20] avg loss: 0.2990672364194166		[learning rate: 0.0027816]
	Learning Rate: 0.00278157
	LOSS [training: 0.30440926905977 | validation: 0.5780016964919518]
	TIME [epoch: 8.13 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3846710250568908		[learning rate: 0.0027765]
		[batch 20/20] avg loss: 0.3508969483130357		[learning rate: 0.0027715]
	Learning Rate: 0.00277147
	LOSS [training: 0.36778398668496326 | validation: 0.363119270949323]
	TIME [epoch: 8.14 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25456321817295924		[learning rate: 0.0027664]
		[batch 20/20] avg loss: 0.3425505544659968		[learning rate: 0.0027614]
	Learning Rate: 0.00276141
	LOSS [training: 0.29855688631947797 | validation: 0.18910112461801287]
	TIME [epoch: 8.13 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3228006618445776		[learning rate: 0.0027564]
		[batch 20/20] avg loss: 0.31388928183925047		[learning rate: 0.0027514]
	Learning Rate: 0.00275139
	LOSS [training: 0.318344971841914 | validation: 0.46903897102891434]
	TIME [epoch: 8.14 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3749901644312915		[learning rate: 0.0027464]
		[batch 20/20] avg loss: 0.3461309792287217		[learning rate: 0.0027414]
	Learning Rate: 0.00274141
	LOSS [training: 0.36056057183000656 | validation: 0.3715208684260897]
	TIME [epoch: 8.16 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2907894624744033		[learning rate: 0.0027364]
		[batch 20/20] avg loss: 0.30101680531811337		[learning rate: 0.0027315]
	Learning Rate: 0.00273146
	LOSS [training: 0.2959031338962583 | validation: 0.2991719066132533]
	TIME [epoch: 8.14 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31337528050739855		[learning rate: 0.0027265]
		[batch 20/20] avg loss: 0.278741151104379		[learning rate: 0.0027215]
	Learning Rate: 0.00272155
	LOSS [training: 0.2960582158058888 | validation: 0.5068653962724753]
	TIME [epoch: 8.15 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38223816331577404		[learning rate: 0.0027166]
		[batch 20/20] avg loss: 0.28684552667656166		[learning rate: 0.0027117]
	Learning Rate: 0.00271167
	LOSS [training: 0.3345418449961679 | validation: 0.4285594613856503]
	TIME [epoch: 8.16 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3671161581949944		[learning rate: 0.0027067]
		[batch 20/20] avg loss: 0.354992392537221		[learning rate: 0.0027018]
	Learning Rate: 0.00270183
	LOSS [training: 0.3610542753661077 | validation: 0.38583756561362564]
	TIME [epoch: 8.16 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2731538932310579		[learning rate: 0.0026969]
		[batch 20/20] avg loss: 0.3017012495402903		[learning rate: 0.002692]
	Learning Rate: 0.00269202
	LOSS [training: 0.2874275713856741 | validation: 0.2617030733080162]
	TIME [epoch: 8.14 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3445141920171671		[learning rate: 0.0026871]
		[batch 20/20] avg loss: 0.23413938041255067		[learning rate: 0.0026823]
	Learning Rate: 0.00268225
	LOSS [training: 0.28932678621485886 | validation: 0.32414186228578884]
	TIME [epoch: 8.18 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.297748335938892		[learning rate: 0.0026774]
		[batch 20/20] avg loss: 0.4186371226815571		[learning rate: 0.0026725]
	Learning Rate: 0.00267252
	LOSS [training: 0.35819272931022444 | validation: 0.5977105807695555]
	TIME [epoch: 8.17 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3444412379179319		[learning rate: 0.0026677]
		[batch 20/20] avg loss: 0.3625267011222627		[learning rate: 0.0026628]
	Learning Rate: 0.00266282
	LOSS [training: 0.35348396952009725 | validation: 0.4041776136735376]
	TIME [epoch: 8.15 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.305384142024811		[learning rate: 0.002658]
		[batch 20/20] avg loss: 0.3121320558355244		[learning rate: 0.0026532]
	Learning Rate: 0.00265316
	LOSS [training: 0.30875809893016765 | validation: 0.40108973829616273]
	TIME [epoch: 8.15 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3441800190483915		[learning rate: 0.0026483]
		[batch 20/20] avg loss: 0.3122439060016569		[learning rate: 0.0026435]
	Learning Rate: 0.00264353
	LOSS [training: 0.3282119625250242 | validation: 0.22703755641984866]
	TIME [epoch: 8.14 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3030469961157473		[learning rate: 0.0026387]
		[batch 20/20] avg loss: 0.3378483537720767		[learning rate: 0.0026339]
	Learning Rate: 0.00263394
	LOSS [training: 0.320447674943912 | validation: 0.24483267195691782]
	TIME [epoch: 8.13 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2620002436582671		[learning rate: 0.0026292]
		[batch 20/20] avg loss: 0.27043002782157427		[learning rate: 0.0026244]
	Learning Rate: 0.00262438
	LOSS [training: 0.2662151357399206 | validation: 0.33711542341927236]
	TIME [epoch: 8.13 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.318839623168489		[learning rate: 0.0026196]
		[batch 20/20] avg loss: 0.3859939445554562		[learning rate: 0.0026149]
	Learning Rate: 0.00261485
	LOSS [training: 0.35241678386197267 | validation: 0.3515268373429678]
	TIME [epoch: 8.16 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2523105072281089		[learning rate: 0.0026101]
		[batch 20/20] avg loss: 0.24921098119106952		[learning rate: 0.0026054]
	Learning Rate: 0.00260536
	LOSS [training: 0.2507607442095892 | validation: 0.27483611202046854]
	TIME [epoch: 8.13 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3355667702000934		[learning rate: 0.0026006]
		[batch 20/20] avg loss: 0.37277086540155885		[learning rate: 0.0025959]
	Learning Rate: 0.00259591
	LOSS [training: 0.3541688178008261 | validation: 0.5152710863981329]
	TIME [epoch: 8.17 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2670637132621857		[learning rate: 0.0025912]
		[batch 20/20] avg loss: 0.28619989850244304		[learning rate: 0.0025865]
	Learning Rate: 0.00258649
	LOSS [training: 0.27663180588231434 | validation: 0.22088532843822542]
	TIME [epoch: 8.16 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28532559946179725		[learning rate: 0.0025818]
		[batch 20/20] avg loss: 0.2573209097941974		[learning rate: 0.0025771]
	Learning Rate: 0.0025771
	LOSS [training: 0.27132325462799733 | validation: 0.18011366640175636]
	TIME [epoch: 8.16 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33118544989903936		[learning rate: 0.0025724]
		[batch 20/20] avg loss: 0.28825318011950635		[learning rate: 0.0025677]
	Learning Rate: 0.00256775
	LOSS [training: 0.30971931500927286 | validation: 0.3249163207493454]
	TIME [epoch: 8.15 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34948406248520786		[learning rate: 0.0025631]
		[batch 20/20] avg loss: 0.29788258791944877		[learning rate: 0.0025584]
	Learning Rate: 0.00255843
	LOSS [training: 0.3236833252023283 | validation: 0.48305732820127445]
	TIME [epoch: 8.19 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26999479146261934		[learning rate: 0.0025538]
		[batch 20/20] avg loss: 0.26866423723758726		[learning rate: 0.0025491]
	Learning Rate: 0.00254915
	LOSS [training: 0.2693295143501032 | validation: 0.5589427015271288]
	TIME [epoch: 8.15 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30320608929097365		[learning rate: 0.0025445]
		[batch 20/20] avg loss: 0.3314249524957037		[learning rate: 0.0025399]
	Learning Rate: 0.0025399
	LOSS [training: 0.3173155208933387 | validation: 0.38798635167656026]
	TIME [epoch: 8.14 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32084888884372414		[learning rate: 0.0025353]
		[batch 20/20] avg loss: 0.3712732733380252		[learning rate: 0.0025307]
	Learning Rate: 0.00253068
	LOSS [training: 0.34606108109087463 | validation: 0.33029184731134437]
	TIME [epoch: 8.17 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47847627783732705		[learning rate: 0.0025261]
		[batch 20/20] avg loss: 0.25061846383845543		[learning rate: 0.0025215]
	Learning Rate: 0.00252149
	LOSS [training: 0.3645473708378913 | validation: 0.3025442689844666]
	TIME [epoch: 8.13 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.265871923806961		[learning rate: 0.0025169]
		[batch 20/20] avg loss: 0.2764105211143724		[learning rate: 0.0025123]
	Learning Rate: 0.00251234
	LOSS [training: 0.2711412224606667 | validation: 0.4020903301329025]
	TIME [epoch: 8.13 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2735026652871424		[learning rate: 0.0025078]
		[batch 20/20] avg loss: 0.2511609415413835		[learning rate: 0.0025032]
	Learning Rate: 0.00250323
	LOSS [training: 0.26233180341426293 | validation: 0.3755831931044441]
	TIME [epoch: 8.12 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3910971277247195		[learning rate: 0.0024987]
		[batch 20/20] avg loss: 0.28274095233912955		[learning rate: 0.0024941]
	Learning Rate: 0.00249414
	LOSS [training: 0.3369190400319245 | validation: 0.21594438128227167]
	TIME [epoch: 8.15 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2419458330163511		[learning rate: 0.0024896]
		[batch 20/20] avg loss: 0.3982614101222798		[learning rate: 0.0024851]
	Learning Rate: 0.00248509
	LOSS [training: 0.3201036215693153 | validation: 0.16278539557296048]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_483.pth
	Model improved!!!
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2679504081551134		[learning rate: 0.0024806]
		[batch 20/20] avg loss: 0.26012525112874957		[learning rate: 0.0024761]
	Learning Rate: 0.00247607
	LOSS [training: 0.26403782964193145 | validation: 0.25384199636510063]
	TIME [epoch: 8.15 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34698315233271404		[learning rate: 0.0024716]
		[batch 20/20] avg loss: 0.2617183558731636		[learning rate: 0.0024671]
	Learning Rate: 0.00246709
	LOSS [training: 0.3043507541029389 | validation: 0.3227497215473495]
	TIME [epoch: 8.14 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24447519377147348		[learning rate: 0.0024626]
		[batch 20/20] avg loss: 0.2892316922490014		[learning rate: 0.0024581]
	Learning Rate: 0.00245813
	LOSS [training: 0.26685344301023745 | validation: 0.20863235176756723]
	TIME [epoch: 8.15 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29226525693834027		[learning rate: 0.0024537]
		[batch 20/20] avg loss: 0.3095489215503075		[learning rate: 0.0024492]
	Learning Rate: 0.00244921
	LOSS [training: 0.30090708924432386 | validation: 0.5495550355471771]
	TIME [epoch: 8.21 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2828614336504146		[learning rate: 0.0024448]
		[batch 20/20] avg loss: 0.4502752390790666		[learning rate: 0.0024403]
	Learning Rate: 0.00244032
	LOSS [training: 0.3665683363647406 | validation: 0.6419378561838498]
	TIME [epoch: 8.13 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3441186068743832		[learning rate: 0.0024359]
		[batch 20/20] avg loss: 0.3001207130383174		[learning rate: 0.0024315]
	Learning Rate: 0.00243147
	LOSS [training: 0.3221196599563502 | validation: 0.4284669943077438]
	TIME [epoch: 8.14 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2898901631704975		[learning rate: 0.0024271]
		[batch 20/20] avg loss: 0.2686418802627478		[learning rate: 0.0024226]
	Learning Rate: 0.00242264
	LOSS [training: 0.2792660217166226 | validation: 0.3251233722030046]
	TIME [epoch: 8.12 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20607419822703404		[learning rate: 0.0024182]
		[batch 20/20] avg loss: 0.3530824992238881		[learning rate: 0.0024139]
	Learning Rate: 0.00241385
	LOSS [training: 0.27957834872546117 | validation: 0.654873814168683]
	TIME [epoch: 8.15 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.337377048714194		[learning rate: 0.0024095]
		[batch 20/20] avg loss: 0.23591737610679941		[learning rate: 0.0024051]
	Learning Rate: 0.00240509
	LOSS [training: 0.2866472124104968 | validation: 0.3269658375049135]
	TIME [epoch: 8.12 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23633483281260448		[learning rate: 0.0024007]
		[batch 20/20] avg loss: 0.27747622936479865		[learning rate: 0.0023964]
	Learning Rate: 0.00239636
	LOSS [training: 0.25690553108870157 | validation: 0.3462053405598077]
	TIME [epoch: 8.13 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23559995658650013		[learning rate: 0.002392]
		[batch 20/20] avg loss: 0.27632445781189696		[learning rate: 0.0023877]
	Learning Rate: 0.00238767
	LOSS [training: 0.25596220719919854 | validation: 0.22888734139597905]
	TIME [epoch: 8.13 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3314863949974471		[learning rate: 0.0023833]
		[batch 20/20] avg loss: 0.25853988604386763		[learning rate: 0.002379]
	Learning Rate: 0.002379
	LOSS [training: 0.2950131405206574 | validation: 0.7393845879518373]
	TIME [epoch: 8.14 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37788982421831646		[learning rate: 0.0023747]
		[batch 20/20] avg loss: 0.2694931082580463		[learning rate: 0.0023704]
	Learning Rate: 0.00237037
	LOSS [training: 0.3236914662381814 | validation: 0.5284545480728555]
	TIME [epoch: 8.21 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2696595979334627		[learning rate: 0.0023661]
		[batch 20/20] avg loss: 0.32057918451241746		[learning rate: 0.0023618]
	Learning Rate: 0.00236177
	LOSS [training: 0.29511939122294006 | validation: 0.6134292855074859]
	TIME [epoch: 8.15 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3574603973769034		[learning rate: 0.0023575]
		[batch 20/20] avg loss: 0.2853556925639834		[learning rate: 0.0023532]
	Learning Rate: 0.00235319
	LOSS [training: 0.32140804497044334 | validation: 0.19040592128331185]
	TIME [epoch: 8.14 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3207861537264947		[learning rate: 0.0023489]
		[batch 20/20] avg loss: 0.26472389313698136		[learning rate: 0.0023447]
	Learning Rate: 0.00234465
	LOSS [training: 0.29275502343173804 | validation: 0.22602387802089094]
	TIME [epoch: 8.14 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.230537176153324		[learning rate: 0.0023404]
		[batch 20/20] avg loss: 0.3244833127812489		[learning rate: 0.0023361]
	Learning Rate: 0.00233615
	LOSS [training: 0.2775102444672865 | validation: 0.18627265181733826]
	TIME [epoch: 8.22 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2795326665582956		[learning rate: 0.0023319]
		[batch 20/20] avg loss: 0.22904690366431418		[learning rate: 0.0023277]
	Learning Rate: 0.00232767
	LOSS [training: 0.25428978511130496 | validation: 0.303703880460662]
	TIME [epoch: 8.14 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2580029485591878		[learning rate: 0.0023234]
		[batch 20/20] avg loss: 0.2822964434501834		[learning rate: 0.0023192]
	Learning Rate: 0.00231922
	LOSS [training: 0.2701496960046855 | validation: 0.2592586428098331]
	TIME [epoch: 8.14 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2445832794258343		[learning rate: 0.002315]
		[batch 20/20] avg loss: 0.27962560525547997		[learning rate: 0.0023108]
	Learning Rate: 0.0023108
	LOSS [training: 0.2621044423406571 | validation: 0.5977313350414207]
	TIME [epoch: 8.14 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35127060556110945		[learning rate: 0.0023066]
		[batch 20/20] avg loss: 0.2272256166302733		[learning rate: 0.0023024]
	Learning Rate: 0.00230242
	LOSS [training: 0.2892481110956915 | validation: 0.2525782551128581]
	TIME [epoch: 8.15 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22115497230878872		[learning rate: 0.0022982]
		[batch 20/20] avg loss: 0.2620766784888144		[learning rate: 0.0022941]
	Learning Rate: 0.00229406
	LOSS [training: 0.24161582539880153 | validation: 0.2320926901356731]
	TIME [epoch: 8.14 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22913584888297467		[learning rate: 0.0022899]
		[batch 20/20] avg loss: 0.2849001324061113		[learning rate: 0.0022857]
	Learning Rate: 0.00228574
	LOSS [training: 0.257017990644543 | validation: 0.18087396489151022]
	TIME [epoch: 8.14 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2644016935018808		[learning rate: 0.0022816]
		[batch 20/20] avg loss: 0.3263685013419796		[learning rate: 0.0022774]
	Learning Rate: 0.00227744
	LOSS [training: 0.2953850974219302 | validation: 0.6111576171936444]
	TIME [epoch: 8.13 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34105710709115067		[learning rate: 0.0022733]
		[batch 20/20] avg loss: 0.2644232891395183		[learning rate: 0.0022692]
	Learning Rate: 0.00226918
	LOSS [training: 0.3027401981153345 | validation: 0.5270117871159152]
	TIME [epoch: 8.19 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2564778288659203		[learning rate: 0.0022651]
		[batch 20/20] avg loss: 0.25184448557872774		[learning rate: 0.0022609]
	Learning Rate: 0.00226094
	LOSS [training: 0.254161157222324 | validation: 0.28652951149621547]
	TIME [epoch: 8.17 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23333789226501445		[learning rate: 0.0022568]
		[batch 20/20] avg loss: 0.2452985974116612		[learning rate: 0.0022527]
	Learning Rate: 0.00225274
	LOSS [training: 0.23931824483833783 | validation: 0.3758123488754789]
	TIME [epoch: 8.14 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30677040766839003		[learning rate: 0.0022486]
		[batch 20/20] avg loss: 0.23807404029271062		[learning rate: 0.0022446]
	Learning Rate: 0.00224456
	LOSS [training: 0.2724222239805503 | validation: 0.2946786709288585]
	TIME [epoch: 8.14 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28199528200669943		[learning rate: 0.0022405]
		[batch 20/20] avg loss: 0.23077831190939876		[learning rate: 0.0022364]
	Learning Rate: 0.00223642
	LOSS [training: 0.2563867969580491 | validation: 0.3231346935293913]
	TIME [epoch: 8.18 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2996312169653311		[learning rate: 0.0022324]
		[batch 20/20] avg loss: 0.255536120529993		[learning rate: 0.0022283]
	Learning Rate: 0.0022283
	LOSS [training: 0.2775836687476621 | validation: 0.4139321739468202]
	TIME [epoch: 8.14 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34526474778886335		[learning rate: 0.0022243]
		[batch 20/20] avg loss: 0.2759144239511936		[learning rate: 0.0022202]
	Learning Rate: 0.00222021
	LOSS [training: 0.3105895858700285 | validation: 0.21361382065440626]
	TIME [epoch: 8.14 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3676221083425872		[learning rate: 0.0022162]
		[batch 20/20] avg loss: 0.252912946067703		[learning rate: 0.0022122]
	Learning Rate: 0.00221216
	LOSS [training: 0.31026752720514506 | validation: 0.24052906179659997]
	TIME [epoch: 8.17 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2951163104883948		[learning rate: 0.0022081]
		[batch 20/20] avg loss: 0.25585519626708714		[learning rate: 0.0022041]
	Learning Rate: 0.00220413
	LOSS [training: 0.275485753377741 | validation: 0.5428581566811497]
	TIME [epoch: 8.14 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2978892350708291		[learning rate: 0.0022001]
		[batch 20/20] avg loss: 0.2866294010450813		[learning rate: 0.0021961]
	Learning Rate: 0.00219613
	LOSS [training: 0.2922593180579552 | validation: 0.18557025570559882]
	TIME [epoch: 8.14 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.300208554687317		[learning rate: 0.0021921]
		[batch 20/20] avg loss: 0.24654287014956458		[learning rate: 0.0021882]
	Learning Rate: 0.00218816
	LOSS [training: 0.2733757124184408 | validation: 0.32354294933837036]
	TIME [epoch: 8.18 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25465605953174564		[learning rate: 0.0021842]
		[batch 20/20] avg loss: 0.46499860108277513		[learning rate: 0.0021802]
	Learning Rate: 0.00218022
	LOSS [training: 0.35982733030726033 | validation: 0.281714708653312]
	TIME [epoch: 8.17 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23425276200984269		[learning rate: 0.0021763]
		[batch 20/20] avg loss: 0.2978265349758585		[learning rate: 0.0021723]
	Learning Rate: 0.00217231
	LOSS [training: 0.2660396484928506 | validation: 0.19838065152832662]
	TIME [epoch: 8.13 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2732464798440772		[learning rate: 0.0021684]
		[batch 20/20] avg loss: 0.23811984721671764		[learning rate: 0.0021644]
	Learning Rate: 0.00216442
	LOSS [training: 0.2556831635303974 | validation: 0.3522108006391754]
	TIME [epoch: 8.13 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2575297880493286		[learning rate: 0.0021605]
		[batch 20/20] avg loss: 0.2299145560336151		[learning rate: 0.0021566]
	Learning Rate: 0.00215657
	LOSS [training: 0.2437221720414718 | validation: 0.22125761779070074]
	TIME [epoch: 8.14 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31974375594197824		[learning rate: 0.0021527]
		[batch 20/20] avg loss: 0.2641009331470991		[learning rate: 0.0021487]
	Learning Rate: 0.00214874
	LOSS [training: 0.2919223445445386 | validation: 0.2025712777972296]
	TIME [epoch: 8.14 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2438937730704783		[learning rate: 0.0021448]
		[batch 20/20] avg loss: 0.24371297226584435		[learning rate: 0.0021409]
	Learning Rate: 0.00214094
	LOSS [training: 0.2438033726681613 | validation: 0.6662859112450872]
	TIME [epoch: 8.13 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3881521165680325		[learning rate: 0.0021371]
		[batch 20/20] avg loss: 0.24465381582398438		[learning rate: 0.0021332]
	Learning Rate: 0.00213317
	LOSS [training: 0.3164029661960085 | validation: 0.2640003087197522]
	TIME [epoch: 8.14 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31352428320132913		[learning rate: 0.0021293]
		[batch 20/20] avg loss: 0.350160047056561		[learning rate: 0.0021254]
	Learning Rate: 0.00212543
	LOSS [training: 0.33184216512894515 | validation: 0.31089408800925317]
	TIME [epoch: 8.15 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29917257653825285		[learning rate: 0.0021216]
		[batch 20/20] avg loss: 0.31268114489195914		[learning rate: 0.0021177]
	Learning Rate: 0.00211772
	LOSS [training: 0.30592686071510605 | validation: 0.34508668236362317]
	TIME [epoch: 8.16 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2306913061470163		[learning rate: 0.0021139]
		[batch 20/20] avg loss: 0.24813588308796813		[learning rate: 0.00211]
	Learning Rate: 0.00211003
	LOSS [training: 0.23941359461749218 | validation: 0.20391604388172638]
	TIME [epoch: 8.19 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2826202483036687		[learning rate: 0.0021062]
		[batch 20/20] avg loss: 0.22688769190230076		[learning rate: 0.0021024]
	Learning Rate: 0.00210238
	LOSS [training: 0.25475397010298473 | validation: 0.3667803142904924]
	TIME [epoch: 8.14 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24861683960259212		[learning rate: 0.0020986]
		[batch 20/20] avg loss: 0.2916617550715793		[learning rate: 0.0020947]
	Learning Rate: 0.00209475
	LOSS [training: 0.2701392973370857 | validation: 0.5340593197045307]
	TIME [epoch: 8.14 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2585197155542811		[learning rate: 0.0020909]
		[batch 20/20] avg loss: 0.2845298218648763		[learning rate: 0.0020871]
	Learning Rate: 0.00208714
	LOSS [training: 0.27152476870957876 | validation: 0.35663884868379436]
	TIME [epoch: 8.17 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24023634128388446		[learning rate: 0.0020834]
		[batch 20/20] avg loss: 0.29945228680569724		[learning rate: 0.0020796]
	Learning Rate: 0.00207957
	LOSS [training: 0.26984431404479087 | validation: 0.1679677619312365]
	TIME [epoch: 8.19 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23683187135569633		[learning rate: 0.0020758]
		[batch 20/20] avg loss: 0.2616431723002911		[learning rate: 0.002072]
	Learning Rate: 0.00207202
	LOSS [training: 0.2492375218279937 | validation: 0.27019280794804046]
	TIME [epoch: 8.15 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22936633072171447		[learning rate: 0.0020683]
		[batch 20/20] avg loss: 0.2466744778444651		[learning rate: 0.0020645]
	Learning Rate: 0.0020645
	LOSS [training: 0.23802040428308985 | validation: 0.4445824781900962]
	TIME [epoch: 8.13 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20130391036591694		[learning rate: 0.0020608]
		[batch 20/20] avg loss: 0.32174851767735513		[learning rate: 0.002057]
	Learning Rate: 0.00205701
	LOSS [training: 0.261526214021636 | validation: 0.2449046818461567]
	TIME [epoch: 8.14 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22929944980688471		[learning rate: 0.0020533]
		[batch 20/20] avg loss: 0.22023492005085998		[learning rate: 0.0020495]
	Learning Rate: 0.00204955
	LOSS [training: 0.22476718492887238 | validation: 0.21836584540458376]
	TIME [epoch: 8.16 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2555088991838507		[learning rate: 0.0020458]
		[batch 20/20] avg loss: 0.2451264123162126		[learning rate: 0.0020421]
	Learning Rate: 0.00204211
	LOSS [training: 0.2503176557500318 | validation: 0.2716411193309886]
	TIME [epoch: 8.13 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26113752980743143		[learning rate: 0.0020384]
		[batch 20/20] avg loss: 0.29490477901725703		[learning rate: 0.0020347]
	Learning Rate: 0.0020347
	LOSS [training: 0.2780211544123442 | validation: 0.38974283984973596]
	TIME [epoch: 8.13 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22380550353293177		[learning rate: 0.002031]
		[batch 20/20] avg loss: 0.2428028578206415		[learning rate: 0.0020273]
	Learning Rate: 0.00202731
	LOSS [training: 0.23330418067678665 | validation: 0.5211826794989513]
	TIME [epoch: 8.13 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20668107069402844		[learning rate: 0.0020236]
		[batch 20/20] avg loss: 0.2440064614091479		[learning rate: 0.00202]
	Learning Rate: 0.00201996
	LOSS [training: 0.2253437660515881 | validation: 0.2593725599757515]
	TIME [epoch: 8.21 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.270340818849057		[learning rate: 0.0020163]
		[batch 20/20] avg loss: 0.20021951889865558		[learning rate: 0.0020126]
	Learning Rate: 0.00201263
	LOSS [training: 0.23528016887385633 | validation: 0.351486222701214]
	TIME [epoch: 8.17 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25471818650122413		[learning rate: 0.002009]
		[batch 20/20] avg loss: 0.21851158924457953		[learning rate: 0.0020053]
	Learning Rate: 0.00200532
	LOSS [training: 0.23661488787290183 | validation: 0.2367959454120515]
	TIME [epoch: 8.14 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28352367714029447		[learning rate: 0.0020017]
		[batch 20/20] avg loss: 0.19252679803738593		[learning rate: 0.001998]
	Learning Rate: 0.00199805
	LOSS [training: 0.23802523758884023 | validation: 0.224341059247948]
	TIME [epoch: 8.14 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22363287088997832		[learning rate: 0.0019944]
		[batch 20/20] avg loss: 0.22083066812384353		[learning rate: 0.0019908]
	Learning Rate: 0.00199079
	LOSS [training: 0.22223176950691093 | validation: 0.40067525875573384]
	TIME [epoch: 8.19 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26728446658265836		[learning rate: 0.0019872]
		[batch 20/20] avg loss: 0.2552739928129125		[learning rate: 0.0019836]
	Learning Rate: 0.00198357
	LOSS [training: 0.26127922969778544 | validation: 0.18552285386783243]
	TIME [epoch: 8.17 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1895666508925043		[learning rate: 0.00198]
		[batch 20/20] avg loss: 0.2790768996788335		[learning rate: 0.0019764]
	Learning Rate: 0.00197637
	LOSS [training: 0.23432177528566886 | validation: 0.1486863350455218]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_546.pth
	Model improved!!!
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21949429890861735		[learning rate: 0.0019728]
		[batch 20/20] avg loss: 0.2582930327925189		[learning rate: 0.0019692]
	Learning Rate: 0.0019692
	LOSS [training: 0.23889366585056812 | validation: 0.25977573900522477]
	TIME [epoch: 8.13 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24414950112637585		[learning rate: 0.0019656]
		[batch 20/20] avg loss: 0.28977614586766737		[learning rate: 0.0019621]
	Learning Rate: 0.00196205
	LOSS [training: 0.26696282349702155 | validation: 0.30082369996478253]
	TIME [epoch: 8.14 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2853793840487553		[learning rate: 0.0019585]
		[batch 20/20] avg loss: 0.21014222862339502		[learning rate: 0.0019549]
	Learning Rate: 0.00195493
	LOSS [training: 0.24776080633607514 | validation: 0.22817921735141344]
	TIME [epoch: 8.16 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2253697585905658		[learning rate: 0.0019514]
		[batch 20/20] avg loss: 0.24898414165348		[learning rate: 0.0019478]
	Learning Rate: 0.00194784
	LOSS [training: 0.2371769501220229 | validation: 0.2674389887239343]
	TIME [epoch: 8.14 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.249006647817349		[learning rate: 0.0019443]
		[batch 20/20] avg loss: 0.19537856554696964		[learning rate: 0.0019408]
	Learning Rate: 0.00194077
	LOSS [training: 0.22219260668215934 | validation: 0.3615221974369965]
	TIME [epoch: 8.14 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2036693941229574		[learning rate: 0.0019372]
		[batch 20/20] avg loss: 0.24383059987376593		[learning rate: 0.0019337]
	Learning Rate: 0.00193373
	LOSS [training: 0.2237499969983617 | validation: 0.9866219049064754]
	TIME [epoch: 8.16 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38862184660525145		[learning rate: 0.0019302]
		[batch 20/20] avg loss: 0.22113139761630468		[learning rate: 0.0019267]
	Learning Rate: 0.00192671
	LOSS [training: 0.3048766221107781 | validation: 0.28586267426590206]
	TIME [epoch: 8.19 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21620742956210487		[learning rate: 0.0019232]
		[batch 20/20] avg loss: 0.19817689984579442		[learning rate: 0.0019197]
	Learning Rate: 0.00191972
	LOSS [training: 0.2071921647039497 | validation: 0.2433583752292872]
	TIME [epoch: 8.15 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22050289357581762		[learning rate: 0.0019162]
		[batch 20/20] avg loss: 0.22289848609723198		[learning rate: 0.0019127]
	Learning Rate: 0.00191275
	LOSS [training: 0.22170068983652474 | validation: 0.20951575910181364]
	TIME [epoch: 8.14 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22826761411751662		[learning rate: 0.0019093]
		[batch 20/20] avg loss: 0.35179304096454794		[learning rate: 0.0019058]
	Learning Rate: 0.00190581
	LOSS [training: 0.2900303275410323 | validation: 0.3936322185176031]
	TIME [epoch: 8.17 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23751703514081454		[learning rate: 0.0019023]
		[batch 20/20] avg loss: 0.24785673124432658		[learning rate: 0.0018989]
	Learning Rate: 0.00189889
	LOSS [training: 0.2426868831925706 | validation: 0.24419726031009048]
	TIME [epoch: 8.17 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19624776638515337		[learning rate: 0.0018954]
		[batch 20/20] avg loss: 0.25490564896940887		[learning rate: 0.001892]
	Learning Rate: 0.001892
	LOSS [training: 0.22557670767728105 | validation: 0.15272238100225816]
	TIME [epoch: 8.16 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17354974378308954		[learning rate: 0.0018886]
		[batch 20/20] avg loss: 0.2661435572546994		[learning rate: 0.0018851]
	Learning Rate: 0.00188513
	LOSS [training: 0.21984665051889452 | validation: 0.3418134011199594]
	TIME [epoch: 8.14 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2533967283444181		[learning rate: 0.0018817]
		[batch 20/20] avg loss: 0.34003521837085493		[learning rate: 0.0018783]
	Learning Rate: 0.00187829
	LOSS [training: 0.29671597335763644 | validation: 0.15447262796234598]
	TIME [epoch: 8.13 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21836444705732472		[learning rate: 0.0018749]
		[batch 20/20] avg loss: 0.21723252485763309		[learning rate: 0.0018715]
	Learning Rate: 0.00187148
	LOSS [training: 0.21779848595747886 | validation: 0.18717684764989095]
	TIME [epoch: 8.13 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22867759561440862		[learning rate: 0.0018681]
		[batch 20/20] avg loss: 0.26661933641176694		[learning rate: 0.0018647]
	Learning Rate: 0.00186468
	LOSS [training: 0.2476484660130877 | validation: 0.3274093024652644]
	TIME [epoch: 8.15 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2199263840789063		[learning rate: 0.0018613]
		[batch 20/20] avg loss: 0.36042292836949286		[learning rate: 0.0018579]
	Learning Rate: 0.00185792
	LOSS [training: 0.29017465622419963 | validation: 0.5684213495832591]
	TIME [epoch: 8.15 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2506323492341116		[learning rate: 0.0018545]
		[batch 20/20] avg loss: 0.23043286944308		[learning rate: 0.0018512]
	Learning Rate: 0.00185117
	LOSS [training: 0.24053260933859583 | validation: 0.23317414336864567]
	TIME [epoch: 8.14 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22551081601908188		[learning rate: 0.0018478]
		[batch 20/20] avg loss: 0.2102001030950753		[learning rate: 0.0018445]
	Learning Rate: 0.00184446
	LOSS [training: 0.21785545955707858 | validation: 0.4858733119225591]
	TIME [epoch: 8.19 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2744574146749177		[learning rate: 0.0018411]
		[batch 20/20] avg loss: 0.21246401362645306		[learning rate: 0.0018378]
	Learning Rate: 0.00183776
	LOSS [training: 0.24346071415068535 | validation: 0.546520328705673]
	TIME [epoch: 8.14 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2405192165611103		[learning rate: 0.0018344]
		[batch 20/20] avg loss: 0.2366656557313617		[learning rate: 0.0018311]
	Learning Rate: 0.00183109
	LOSS [training: 0.23859243614623601 | validation: 0.46545321795450506]
	TIME [epoch: 8.17 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25106661491079457		[learning rate: 0.0018278]
		[batch 20/20] avg loss: 0.20801132104991815		[learning rate: 0.0018244]
	Learning Rate: 0.00182445
	LOSS [training: 0.2295389679803564 | validation: 0.3289242003817765]
	TIME [epoch: 8.16 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24882572724320737		[learning rate: 0.0018211]
		[batch 20/20] avg loss: 0.17378821998816546		[learning rate: 0.0018178]
	Learning Rate: 0.00181783
	LOSS [training: 0.21130697361568643 | validation: 0.3002222177506092]
	TIME [epoch: 8.17 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28472905851743546		[learning rate: 0.0018145]
		[batch 20/20] avg loss: 0.21106361050406477		[learning rate: 0.0018112]
	Learning Rate: 0.00181123
	LOSS [training: 0.2478963345107501 | validation: 0.41252293726724115]
	TIME [epoch: 8.13 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23795488885765512		[learning rate: 0.0018079]
		[batch 20/20] avg loss: 0.19478269180536217		[learning rate: 0.0018047]
	Learning Rate: 0.00180466
	LOSS [training: 0.21636879033150863 | validation: 0.12601690257446718]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_571.pth
	Model improved!!!
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21563683374623627		[learning rate: 0.0018014]
		[batch 20/20] avg loss: 0.22009286661824481		[learning rate: 0.0017981]
	Learning Rate: 0.00179811
	LOSS [training: 0.21786485018224053 | validation: 0.18947875561733843]
	TIME [epoch: 8.17 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20967479722970522		[learning rate: 0.0017948]
		[batch 20/20] avg loss: 0.25967682138734766		[learning rate: 0.0017916]
	Learning Rate: 0.00179158
	LOSS [training: 0.23467580930852647 | validation: 0.23670253513030368]
	TIME [epoch: 8.15 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24774516024267493		[learning rate: 0.0017883]
		[batch 20/20] avg loss: 0.2199965028835411		[learning rate: 0.0017851]
	Learning Rate: 0.00178508
	LOSS [training: 0.233870831563108 | validation: 0.21873863774520755]
	TIME [epoch: 8.15 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16116268552375562		[learning rate: 0.0017818]
		[batch 20/20] avg loss: 0.2291567469765275		[learning rate: 0.0017786]
	Learning Rate: 0.0017786
	LOSS [training: 0.19515971625014153 | validation: 0.20926371251579992]
	TIME [epoch: 8.16 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20740446686646186		[learning rate: 0.0017754]
		[batch 20/20] avg loss: 0.24255318907927648		[learning rate: 0.0017721]
	Learning Rate: 0.00177215
	LOSS [training: 0.22497882797286914 | validation: 0.5020742285011872]
	TIME [epoch: 8.19 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23570469241314287		[learning rate: 0.0017689]
		[batch 20/20] avg loss: 0.22892721189650436		[learning rate: 0.0017657]
	Learning Rate: 0.00176572
	LOSS [training: 0.23231595215482365 | validation: 0.2995014589294162]
	TIME [epoch: 8.18 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17472822787889644		[learning rate: 0.0017625]
		[batch 20/20] avg loss: 0.2193863447508652		[learning rate: 0.0017593]
	Learning Rate: 0.00175931
	LOSS [training: 0.19705728631488084 | validation: 0.23766495726861034]
	TIME [epoch: 8.16 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2170246334715576		[learning rate: 0.0017561]
		[batch 20/20] avg loss: 0.2218742373352467		[learning rate: 0.0017529]
	Learning Rate: 0.00175292
	LOSS [training: 0.21944943540340217 | validation: 0.15693441349334633]
	TIME [epoch: 8.17 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20443595617787316		[learning rate: 0.0017497]
		[batch 20/20] avg loss: 0.2360658165947926		[learning rate: 0.0017466]
	Learning Rate: 0.00174656
	LOSS [training: 0.2202508863863329 | validation: 0.210654713202109]
	TIME [epoch: 8.22 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2110913601838774		[learning rate: 0.0017434]
		[batch 20/20] avg loss: 0.21401259523746838		[learning rate: 0.0017402]
	Learning Rate: 0.00174022
	LOSS [training: 0.21255197771067294 | validation: 0.40263576776244325]
	TIME [epoch: 8.19 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.259130764821746		[learning rate: 0.0017371]
		[batch 20/20] avg loss: 0.2437286324726995		[learning rate: 0.0017339]
	Learning Rate: 0.00173391
	LOSS [training: 0.25142969864722275 | validation: 0.2921771468189533]
	TIME [epoch: 8.15 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2604523324602094		[learning rate: 0.0017308]
		[batch 20/20] avg loss: 0.21791116870206312		[learning rate: 0.0017276]
	Learning Rate: 0.00172762
	LOSS [training: 0.2391817505811363 | validation: 0.19953793999534045]
	TIME [epoch: 8.15 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25019934539639715		[learning rate: 0.0017245]
		[batch 20/20] avg loss: 0.26382709184971376		[learning rate: 0.0017213]
	Learning Rate: 0.00172135
	LOSS [training: 0.25701321862305543 | validation: 0.21928628831517272]
	TIME [epoch: 8.17 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21448522013217883		[learning rate: 0.0017182]
		[batch 20/20] avg loss: 0.17787288100627457		[learning rate: 0.0017151]
	Learning Rate: 0.0017151
	LOSS [training: 0.19617905056922672 | validation: 0.18025114219150495]
	TIME [epoch: 8.17 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1945472069825562		[learning rate: 0.001712]
		[batch 20/20] avg loss: 0.1656517430262521		[learning rate: 0.0017089]
	Learning Rate: 0.00170888
	LOSS [training: 0.1800994750044041 | validation: 0.3873158324523841]
	TIME [epoch: 8.16 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20609807195857477		[learning rate: 0.0017058]
		[batch 20/20] avg loss: 0.20085092305390456		[learning rate: 0.0017027]
	Learning Rate: 0.00170267
	LOSS [training: 0.2034744975062397 | validation: 0.28099170271439144]
	TIME [epoch: 8.16 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23352458335803225		[learning rate: 0.0016996]
		[batch 20/20] avg loss: 0.2185705967522109		[learning rate: 0.0016965]
	Learning Rate: 0.0016965
	LOSS [training: 0.2260475900551216 | validation: 0.666948580471177]
	TIME [epoch: 8.21 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22180191814664396		[learning rate: 0.0016934]
		[batch 20/20] avg loss: 0.19611531519079256		[learning rate: 0.0016903]
	Learning Rate: 0.00169034
	LOSS [training: 0.2089586166687183 | validation: 0.16795526014020334]
	TIME [epoch: 8.19 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24274922711660177		[learning rate: 0.0016873]
		[batch 20/20] avg loss: 0.21035806817597433		[learning rate: 0.0016842]
	Learning Rate: 0.0016842
	LOSS [training: 0.22655364764628802 | validation: 0.15652100167708186]
	TIME [epoch: 8.16 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4591810918783419		[learning rate: 0.0016811]
		[batch 20/20] avg loss: 0.20107112755773798		[learning rate: 0.0016781]
	Learning Rate: 0.00167809
	LOSS [training: 0.3301261097180399 | validation: 0.14705680088643372]
	TIME [epoch: 8.19 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22706977436569137		[learning rate: 0.001675]
		[batch 20/20] avg loss: 0.17946616525735504		[learning rate: 0.001672]
	Learning Rate: 0.001672
	LOSS [training: 0.20326796981152317 | validation: 0.20226905584749064]
	TIME [epoch: 8.2 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1904778783824634		[learning rate: 0.001669]
		[batch 20/20] avg loss: 0.2050698750446523		[learning rate: 0.0016659]
	Learning Rate: 0.00166593
	LOSS [training: 0.19777387671355784 | validation: 0.2453743403858701]
	TIME [epoch: 8.18 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21457938841422322		[learning rate: 0.0016629]
		[batch 20/20] avg loss: 0.19086955785056292		[learning rate: 0.0016599]
	Learning Rate: 0.00165989
	LOSS [training: 0.20272447313239308 | validation: 0.2981954333352258]
	TIME [epoch: 8.16 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18812611664247086		[learning rate: 0.0016569]
		[batch 20/20] avg loss: 0.2793246087790485		[learning rate: 0.0016539]
	Learning Rate: 0.00165387
	LOSS [training: 0.23372536271075967 | validation: 0.30174776452636454]
	TIME [epoch: 8.15 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22753644798847686		[learning rate: 0.0016509]
		[batch 20/20] avg loss: 0.3677129529424378		[learning rate: 0.0016479]
	Learning Rate: 0.00164786
	LOSS [training: 0.2976247004654572 | validation: 0.19547121771080675]
	TIME [epoch: 8.16 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19262436171569133		[learning rate: 0.0016449]
		[batch 20/20] avg loss: 0.21449670205462062		[learning rate: 0.0016419]
	Learning Rate: 0.00164188
	LOSS [training: 0.20356053188515602 | validation: 0.19341757704391074]
	TIME [epoch: 8.15 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23569168728018922		[learning rate: 0.0016389]
		[batch 20/20] avg loss: 0.17286019590954924		[learning rate: 0.0016359]
	Learning Rate: 0.00163592
	LOSS [training: 0.20427594159486923 | validation: 0.3514598315404356]
	TIME [epoch: 8.18 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2220251046719394		[learning rate: 0.001633]
		[batch 20/20] avg loss: 0.20012138811966937		[learning rate: 0.00163]
	Learning Rate: 0.00162999
	LOSS [training: 0.21107324639580433 | validation: 0.34808747192055345]
	TIME [epoch: 8.16 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22492672253609877		[learning rate: 0.001627]
		[batch 20/20] avg loss: 0.2047481873067602		[learning rate: 0.0016241]
	Learning Rate: 0.00162407
	LOSS [training: 0.2148374549214295 | validation: 0.1477051956487216]
	TIME [epoch: 8.16 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17539504605929354		[learning rate: 0.0016211]
		[batch 20/20] avg loss: 0.222901331442605		[learning rate: 0.0016182]
	Learning Rate: 0.00161818
	LOSS [training: 0.19914818875094928 | validation: 0.36222316090654705]
	TIME [epoch: 8.22 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23609221473768568		[learning rate: 0.0016152]
		[batch 20/20] avg loss: 0.22429353148352557		[learning rate: 0.0016123]
	Learning Rate: 0.00161231
	LOSS [training: 0.23019287311060568 | validation: 0.21308377065386705]
	TIME [epoch: 8.19 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18207575044080168		[learning rate: 0.0016094]
		[batch 20/20] avg loss: 0.2107067399117856		[learning rate: 0.0016065]
	Learning Rate: 0.00160645
	LOSS [training: 0.19639124517629364 | validation: 0.24157859174578633]
	TIME [epoch: 8.17 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24604485374455476		[learning rate: 0.0016035]
		[batch 20/20] avg loss: 0.21117681653176823		[learning rate: 0.0016006]
	Learning Rate: 0.00160062
	LOSS [training: 0.2286108351381615 | validation: 0.20392227976504954]
	TIME [epoch: 8.17 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17549623962366498		[learning rate: 0.0015977]
		[batch 20/20] avg loss: 0.2375118116905869		[learning rate: 0.0015948]
	Learning Rate: 0.00159482
	LOSS [training: 0.20650402565712594 | validation: 0.14135739963108856]
	TIME [epoch: 8.2 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16620799202560504		[learning rate: 0.0015919]
		[batch 20/20] avg loss: 0.27971843490508086		[learning rate: 0.001589]
	Learning Rate: 0.00158903
	LOSS [training: 0.22296321346534292 | validation: 0.37291575861315474]
	TIME [epoch: 8.17 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19775636455276835		[learning rate: 0.0015861]
		[batch 20/20] avg loss: 0.2503106527133358		[learning rate: 0.0015833]
	Learning Rate: 0.00158326
	LOSS [training: 0.22403350863305213 | validation: 0.263948322102008]
	TIME [epoch: 8.17 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18808498069848606		[learning rate: 0.0015804]
		[batch 20/20] avg loss: 0.21954104145427045		[learning rate: 0.0015775]
	Learning Rate: 0.00157752
	LOSS [training: 0.20381301107637823 | validation: 0.20278316858879875]
	TIME [epoch: 8.16 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17807826499308674		[learning rate: 0.0015747]
		[batch 20/20] avg loss: 0.17301957946562702		[learning rate: 0.0015718]
	Learning Rate: 0.00157179
	LOSS [training: 0.1755489222293568 | validation: 0.23193807529344324]
	TIME [epoch: 8.16 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21225550906854468		[learning rate: 0.0015689]
		[batch 20/20] avg loss: 0.1854861297673925		[learning rate: 0.0015661]
	Learning Rate: 0.00156609
	LOSS [training: 0.1988708194179686 | validation: 0.1469378195452407]
	TIME [epoch: 8.16 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2479573070092424		[learning rate: 0.0015632]
		[batch 20/20] avg loss: 0.18669516262509514		[learning rate: 0.0015604]
	Learning Rate: 0.0015604
	LOSS [training: 0.21732623481716878 | validation: 0.19386896759671848]
	TIME [epoch: 8.18 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23560012459572377		[learning rate: 0.0015576]
		[batch 20/20] avg loss: 0.15867466386952933		[learning rate: 0.0015547]
	Learning Rate: 0.00155474
	LOSS [training: 0.19713739423262655 | validation: 0.19831359914432758]
	TIME [epoch: 8.16 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19562481738847987		[learning rate: 0.0015519]
		[batch 20/20] avg loss: 0.27429875856410824		[learning rate: 0.0015491]
	Learning Rate: 0.0015491
	LOSS [training: 0.2349617879762941 | validation: 0.249128389080909]
	TIME [epoch: 8.2 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19059365146593352		[learning rate: 0.0015463]
		[batch 20/20] avg loss: 0.20657763559477854		[learning rate: 0.0015435]
	Learning Rate: 0.00154348
	LOSS [training: 0.19858564353035604 | validation: 0.17307964296118528]
	TIME [epoch: 8.19 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20601393531639495		[learning rate: 0.0015407]
		[batch 20/20] avg loss: 0.2081274989401543		[learning rate: 0.0015379]
	Learning Rate: 0.00153788
	LOSS [training: 0.20707071712827463 | validation: 0.2680665939658562]
	TIME [epoch: 8.18 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2225239364799816		[learning rate: 0.0015351]
		[batch 20/20] avg loss: 0.20695372971082043		[learning rate: 0.0015323]
	Learning Rate: 0.00153229
	LOSS [training: 0.21473883309540104 | validation: 0.20115908487488565]
	TIME [epoch: 8.18 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18524300068488628		[learning rate: 0.0015295]
		[batch 20/20] avg loss: 0.21286048296196886		[learning rate: 0.0015267]
	Learning Rate: 0.00152673
	LOSS [training: 0.1990517418234276 | validation: 0.2191557771744447]
	TIME [epoch: 8.21 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21540004502057677		[learning rate: 0.001524]
		[batch 20/20] avg loss: 0.2322301903172322		[learning rate: 0.0015212]
	Learning Rate: 0.00152119
	LOSS [training: 0.2238151176689045 | validation: 0.15940811374572778]
	TIME [epoch: 8.16 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21637899767695826		[learning rate: 0.0015184]
		[batch 20/20] avg loss: 0.24386694832635775		[learning rate: 0.0015157]
	Learning Rate: 0.00151567
	LOSS [training: 0.230122973001658 | validation: 0.18626289616218006]
	TIME [epoch: 8.15 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19371730210710872		[learning rate: 0.0015129]
		[batch 20/20] avg loss: 0.19327876465906155		[learning rate: 0.0015102]
	Learning Rate: 0.00151017
	LOSS [training: 0.19349803338308516 | validation: 0.18161987128471674]
	TIME [epoch: 8.18 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22337830436481693		[learning rate: 0.0015074]
		[batch 20/20] avg loss: 0.2268008456584228		[learning rate: 0.0015047]
	Learning Rate: 0.00150469
	LOSS [training: 0.22508957501161989 | validation: 0.36189580603935356]
	TIME [epoch: 8.16 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23923090394526994		[learning rate: 0.001502]
		[batch 20/20] avg loss: 0.19915745798510348		[learning rate: 0.0014992]
	Learning Rate: 0.00149923
	LOSS [training: 0.21919418096518672 | validation: 0.32133854299087805]
	TIME [epoch: 8.15 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2509487662395905		[learning rate: 0.0014965]
		[batch 20/20] avg loss: 0.18671857410439932		[learning rate: 0.0014938]
	Learning Rate: 0.00149379
	LOSS [training: 0.21883367017199493 | validation: 0.3770421592525781]
	TIME [epoch: 8.15 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2237760025563877		[learning rate: 0.0014911]
		[batch 20/20] avg loss: 0.24612178712547927		[learning rate: 0.0014884]
	Learning Rate: 0.00148837
	LOSS [training: 0.23494889484093345 | validation: 0.20682940742659384]
	TIME [epoch: 8.18 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21210200431890916		[learning rate: 0.0014857]
		[batch 20/20] avg loss: 0.23008956873885725		[learning rate: 0.001483]
	Learning Rate: 0.00148297
	LOSS [training: 0.22109578652888318 | validation: 0.22303304017681685]
	TIME [epoch: 8.17 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1799752409388015		[learning rate: 0.0014803]
		[batch 20/20] avg loss: 0.2649302243952915		[learning rate: 0.0014776]
	Learning Rate: 0.00147759
	LOSS [training: 0.22245273266704652 | validation: 0.6837161724847778]
	TIME [epoch: 8.21 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2970832825823551		[learning rate: 0.0014749]
		[batch 20/20] avg loss: 0.19425340833537433		[learning rate: 0.0014722]
	Learning Rate: 0.00147222
	LOSS [training: 0.2456683454588647 | validation: 0.17434093365265374]
	TIME [epoch: 8.16 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20715507572856734		[learning rate: 0.0014695]
		[batch 20/20] avg loss: 0.19988445593399082		[learning rate: 0.0014669]
	Learning Rate: 0.00146688
	LOSS [training: 0.20351976583127906 | validation: 0.25527221975167164]
	TIME [epoch: 8.16 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16741064217178242		[learning rate: 0.0014642]
		[batch 20/20] avg loss: 0.17783070771570214		[learning rate: 0.0014616]
	Learning Rate: 0.00146156
	LOSS [training: 0.17262067494374225 | validation: 0.1746593351932518]
	TIME [epoch: 8.21 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18074186763698233		[learning rate: 0.0014589]
		[batch 20/20] avg loss: 0.19757717066317543		[learning rate: 0.0014563]
	Learning Rate: 0.00145625
	LOSS [training: 0.1891595191500789 | validation: 0.6080410755561634]
	TIME [epoch: 8.19 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2523787967566786		[learning rate: 0.0014536]
		[batch 20/20] avg loss: 0.23217406005214264		[learning rate: 0.001451]
	Learning Rate: 0.00145097
	LOSS [training: 0.2422764284044106 | validation: 0.29060914111473163]
	TIME [epoch: 8.15 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21229758136252946		[learning rate: 0.0014483]
		[batch 20/20] avg loss: 0.21434470480373005		[learning rate: 0.0014457]
	Learning Rate: 0.0014457
	LOSS [training: 0.21332114308312974 | validation: 0.28169857879927584]
	TIME [epoch: 8.15 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16629281534424944		[learning rate: 0.0014431]
		[batch 20/20] avg loss: 0.1785766915225978		[learning rate: 0.0014405]
	Learning Rate: 0.00144046
	LOSS [training: 0.17243475343342363 | validation: 0.4421052620615372]
	TIME [epoch: 8.18 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1933213002287219		[learning rate: 0.0014378]
		[batch 20/20] avg loss: 0.20463075052680796		[learning rate: 0.0014352]
	Learning Rate: 0.00143523
	LOSS [training: 0.19897602537776488 | validation: 0.22879972418312766]
	TIME [epoch: 8.15 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21454703317418633		[learning rate: 0.0014326]
		[batch 20/20] avg loss: 0.19024470098899116		[learning rate: 0.00143]
	Learning Rate: 0.00143002
	LOSS [training: 0.20239586708158877 | validation: 0.23839585542457042]
	TIME [epoch: 8.15 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.193230271350529		[learning rate: 0.0014274]
		[batch 20/20] avg loss: 0.1614127204137424		[learning rate: 0.0014248]
	Learning Rate: 0.00142483
	LOSS [training: 0.1773214958821357 | validation: 0.19058667353883935]
	TIME [epoch: 8.15 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20600238400850085		[learning rate: 0.0014222]
		[batch 20/20] avg loss: 0.18693081066347828		[learning rate: 0.0014197]
	Learning Rate: 0.00141966
	LOSS [training: 0.19646659733598953 | validation: 0.15789124043796862]
	TIME [epoch: 8.18 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2309637188379939		[learning rate: 0.0014171]
		[batch 20/20] avg loss: 0.2160666609299946		[learning rate: 0.0014145]
	Learning Rate: 0.00141451
	LOSS [training: 0.2235151898839943 | validation: 0.2220985236306922]
	TIME [epoch: 8.23 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2289820888192094		[learning rate: 0.0014119]
		[batch 20/20] avg loss: 0.2040461196848446		[learning rate: 0.0014094]
	Learning Rate: 0.00140937
	LOSS [training: 0.21651410425202697 | validation: 0.34477980517553974]
	TIME [epoch: 8.16 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18736062731826086		[learning rate: 0.0014068]
		[batch 20/20] avg loss: 0.16776692999185264		[learning rate: 0.0014043]
	Learning Rate: 0.00140426
	LOSS [training: 0.17756377865505674 | validation: 0.20284855496003398]
	TIME [epoch: 8.17 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1902178478339415		[learning rate: 0.0014017]
		[batch 20/20] avg loss: 0.20066637503087054		[learning rate: 0.0013992]
	Learning Rate: 0.00139916
	LOSS [training: 0.19544211143240597 | validation: 0.23121888855334616]
	TIME [epoch: 8.19 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21009218591827666		[learning rate: 0.0013966]
		[batch 20/20] avg loss: 0.1887263449512241		[learning rate: 0.0013941]
	Learning Rate: 0.00139409
	LOSS [training: 0.19940926543475038 | validation: 0.20377416202318577]
	TIME [epoch: 8.21 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19456261744252012		[learning rate: 0.0013916]
		[batch 20/20] avg loss: 0.21816631775270548		[learning rate: 0.001389]
	Learning Rate: 0.00138903
	LOSS [training: 0.2063644675976128 | validation: 0.2107378556083025]
	TIME [epoch: 8.16 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22295638099633167		[learning rate: 0.0013865]
		[batch 20/20] avg loss: 0.22293870470099023		[learning rate: 0.001384]
	Learning Rate: 0.00138399
	LOSS [training: 0.22294754284866092 | validation: 0.44300578086881715]
	TIME [epoch: 8.16 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23647372120657767		[learning rate: 0.0013815]
		[batch 20/20] avg loss: 0.22120782360148827		[learning rate: 0.001379]
	Learning Rate: 0.00137896
	LOSS [training: 0.22884077240403297 | validation: 0.2003595748019506]
	TIME [epoch: 8.15 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18873751863626528		[learning rate: 0.0013765]
		[batch 20/20] avg loss: 0.2387792740809454		[learning rate: 0.001374]
	Learning Rate: 0.00137396
	LOSS [training: 0.21375839635860533 | validation: 0.3422299635391565]
	TIME [epoch: 8.17 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17996678440824962		[learning rate: 0.0013715]
		[batch 20/20] avg loss: 0.19245829753326837		[learning rate: 0.001369]
	Learning Rate: 0.00136897
	LOSS [training: 0.18621254097075898 | validation: 0.21430245049456892]
	TIME [epoch: 8.17 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18228798407965852		[learning rate: 0.0013665]
		[batch 20/20] avg loss: 0.21278054224132287		[learning rate: 0.001364]
	Learning Rate: 0.001364
	LOSS [training: 0.19753426316049064 | validation: 0.23887507634244853]
	TIME [epoch: 8.17 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2486161413165901		[learning rate: 0.0013615]
		[batch 20/20] avg loss: 0.20540069185128754		[learning rate: 0.0013591]
	Learning Rate: 0.00135905
	LOSS [training: 0.22700841658393886 | validation: 0.14927676219160294]
	TIME [epoch: 8.19 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15413638460138285		[learning rate: 0.0013566]
		[batch 20/20] avg loss: 0.27130587497145175		[learning rate: 0.0013541]
	Learning Rate: 0.00135412
	LOSS [training: 0.21272112978641727 | validation: 0.18760820463160507]
	TIME [epoch: 8.16 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17589595029151353		[learning rate: 0.0013517]
		[batch 20/20] avg loss: 0.20406194924416038		[learning rate: 0.0013492]
	Learning Rate: 0.00134921
	LOSS [training: 0.18997894976783697 | validation: 0.2726662803552676]
	TIME [epoch: 8.19 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1857757152076705		[learning rate: 0.0013468]
		[batch 20/20] avg loss: 0.19522503827697055		[learning rate: 0.0013443]
	Learning Rate: 0.00134431
	LOSS [training: 0.19050037674232048 | validation: 0.16171527901268465]
	TIME [epoch: 8.19 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3329542586707378		[learning rate: 0.0013419]
		[batch 20/20] avg loss: 0.23403072322662136		[learning rate: 0.0013394]
	Learning Rate: 0.00133943
	LOSS [training: 0.2834924909486795 | validation: 0.3023913393378302]
	TIME [epoch: 8.18 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21377365996813893		[learning rate: 0.001337]
		[batch 20/20] avg loss: 0.2461118088411073		[learning rate: 0.0013346]
	Learning Rate: 0.00133457
	LOSS [training: 0.22994273440462315 | validation: 0.16718537837561226]
	TIME [epoch: 8.16 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23924871247465734		[learning rate: 0.0013321]
		[batch 20/20] avg loss: 0.19295088254515785		[learning rate: 0.0013297]
	Learning Rate: 0.00132973
	LOSS [training: 0.21609979750990763 | validation: 0.19160064698089022]
	TIME [epoch: 8.17 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17960301298017242		[learning rate: 0.0013273]
		[batch 20/20] avg loss: 0.20025385569018034		[learning rate: 0.0013249]
	Learning Rate: 0.0013249
	LOSS [training: 0.1899284343351764 | validation: 0.18733887975203367]
	TIME [epoch: 8.17 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1733213840380068		[learning rate: 0.0013225]
		[batch 20/20] avg loss: 0.2168336918148805		[learning rate: 0.0013201]
	Learning Rate: 0.0013201
	LOSS [training: 0.1950775379264436 | validation: 0.24792519675128105]
	TIME [epoch: 8.15 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16110499634114056		[learning rate: 0.0013177]
		[batch 20/20] avg loss: 0.23182092266570486		[learning rate: 0.0013153]
	Learning Rate: 0.0013153
	LOSS [training: 0.1964629595034227 | validation: 0.19113974278277912]
	TIME [epoch: 8.16 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1915693466175414		[learning rate: 0.0013129]
		[batch 20/20] avg loss: 0.20289521260771534		[learning rate: 0.0013105]
	Learning Rate: 0.00131053
	LOSS [training: 0.19723227961262835 | validation: 0.1901950853462062]
	TIME [epoch: 8.16 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1777452651358093		[learning rate: 0.0013082]
		[batch 20/20] avg loss: 0.1978689879542867		[learning rate: 0.0013058]
	Learning Rate: 0.00130578
	LOSS [training: 0.187807126545048 | validation: 0.22667565968499143]
	TIME [epoch: 8.19 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15906394420581735		[learning rate: 0.0013034]
		[batch 20/20] avg loss: 0.2409265451521201		[learning rate: 0.001301]
	Learning Rate: 0.00130104
	LOSS [training: 0.19999524467896876 | validation: 0.1602144621817109]
	TIME [epoch: 8.22 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3390440492648259		[learning rate: 0.0012987]
		[batch 20/20] avg loss: 0.16379076144334637		[learning rate: 0.0012963]
	Learning Rate: 0.00129631
	LOSS [training: 0.25141740535408613 | validation: 0.2435329387857207]
	TIME [epoch: 8.16 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18854586160211087		[learning rate: 0.001294]
		[batch 20/20] avg loss: 0.1902459033816133		[learning rate: 0.0012916]
	Learning Rate: 0.00129161
	LOSS [training: 0.18939588249186207 | validation: 0.4609620529465582]
	TIME [epoch: 8.16 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17635374257897127		[learning rate: 0.0012893]
		[batch 20/20] avg loss: 0.21817091035768837		[learning rate: 0.0012869]
	Learning Rate: 0.00128692
	LOSS [training: 0.1972623264683298 | validation: 0.3565145367051569]
	TIME [epoch: 8.19 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17121471446191017		[learning rate: 0.0012846]
		[batch 20/20] avg loss: 0.18047723657586864		[learning rate: 0.0012823]
	Learning Rate: 0.00128225
	LOSS [training: 0.1758459755188894 | validation: 0.32053877649179635]
	TIME [epoch: 8.22 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17774472863122517		[learning rate: 0.0012799]
		[batch 20/20] avg loss: 0.17437396148488923		[learning rate: 0.0012776]
	Learning Rate: 0.0012776
	LOSS [training: 0.1760593450580572 | validation: 0.16259056305109257]
	TIME [epoch: 8.15 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16451576348521216		[learning rate: 0.0012753]
		[batch 20/20] avg loss: 0.21242896642363712		[learning rate: 0.001273]
	Learning Rate: 0.00127296
	LOSS [training: 0.18847236495442468 | validation: 0.23065290895771123]
	TIME [epoch: 8.16 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21531886955925134		[learning rate: 0.0012707]
		[batch 20/20] avg loss: 0.3139095754557911		[learning rate: 0.0012683]
	Learning Rate: 0.00126834
	LOSS [training: 0.26461422250752126 | validation: 0.17980967163602538]
	TIME [epoch: 8.16 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2254863656762524		[learning rate: 0.001266]
		[batch 20/20] avg loss: 0.2306494142666217		[learning rate: 0.0012637]
	Learning Rate: 0.00126374
	LOSS [training: 0.228067889971437 | validation: 0.23696687613060197]
	TIME [epoch: 8.18 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1549077172107604		[learning rate: 0.0012614]
		[batch 20/20] avg loss: 0.17690472952156922		[learning rate: 0.0012592]
	Learning Rate: 0.00125915
	LOSS [training: 0.1659062233661648 | validation: 0.2436136842546013]
	TIME [epoch: 8.15 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16883814456219717		[learning rate: 0.0012569]
		[batch 20/20] avg loss: 0.1548609191669892		[learning rate: 0.0012546]
	Learning Rate: 0.00125458
	LOSS [training: 0.16184953186459322 | validation: 0.2817488179615133]
	TIME [epoch: 8.16 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2236004068390897		[learning rate: 0.0012523]
		[batch 20/20] avg loss: 0.27726746538317515		[learning rate: 0.00125]
	Learning Rate: 0.00125003
	LOSS [training: 0.2504339361111324 | validation: 0.1592035580912403]
	TIME [epoch: 8.15 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.186415607826463		[learning rate: 0.0012478]
		[batch 20/20] avg loss: 0.2196533683736805		[learning rate: 0.0012455]
	Learning Rate: 0.0012455
	LOSS [training: 0.20303448810007177 | validation: 0.36049057880084967]
	TIME [epoch: 8.18 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20362308809700158		[learning rate: 0.0012432]
		[batch 20/20] avg loss: 0.16936692151121152		[learning rate: 0.001241]
	Learning Rate: 0.00124098
	LOSS [training: 0.18649500480410655 | validation: 0.2478169967303351]
	TIME [epoch: 8.18 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17212482573581606		[learning rate: 0.0012387]
		[batch 20/20] avg loss: 0.19259921832458804		[learning rate: 0.0012365]
	Learning Rate: 0.00123647
	LOSS [training: 0.18236202203020208 | validation: 0.12031130714681151]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_675.pth
	Model improved!!!
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22334764303966162		[learning rate: 0.0012342]
		[batch 20/20] avg loss: 0.1895337072730289		[learning rate: 0.001232]
	Learning Rate: 0.00123198
	LOSS [training: 0.2064406751563453 | validation: 0.1415705096714395]
	TIME [epoch: 8.17 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17881473198396267		[learning rate: 0.0012297]
		[batch 20/20] avg loss: 0.16907153196100982		[learning rate: 0.0012275]
	Learning Rate: 0.00122751
	LOSS [training: 0.1739431319724862 | validation: 0.14150604521216964]
	TIME [epoch: 8.17 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2640979100318655		[learning rate: 0.0012253]
		[batch 20/20] avg loss: 0.182523666420563		[learning rate: 0.0012231]
	Learning Rate: 0.00122306
	LOSS [training: 0.22331078822621428 | validation: 0.12737323470495035]
	TIME [epoch: 8.21 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17799482315797344		[learning rate: 0.0012208]
		[batch 20/20] avg loss: 0.16156574561858345		[learning rate: 0.0012186]
	Learning Rate: 0.00121862
	LOSS [training: 0.16978028438827844 | validation: 0.24545811169550863]
	TIME [epoch: 8.17 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2111484023314077		[learning rate: 0.0012164]
		[batch 20/20] avg loss: 0.18268658233723162		[learning rate: 0.0012142]
	Learning Rate: 0.0012142
	LOSS [training: 0.19691749233431963 | validation: 0.27618099515913164]
	TIME [epoch: 8.15 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18998862106439343		[learning rate: 0.001212]
		[batch 20/20] avg loss: 0.15110203170513475		[learning rate: 0.0012098]
	Learning Rate: 0.00120979
	LOSS [training: 0.1705453263847641 | validation: 0.13335869129001882]
	TIME [epoch: 8.15 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16750363307015123		[learning rate: 0.0012076]
		[batch 20/20] avg loss: 0.16427791225764704		[learning rate: 0.0012054]
	Learning Rate: 0.0012054
	LOSS [training: 0.16589077266389912 | validation: 0.2667006080337361]
	TIME [epoch: 8.18 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13590396883033842		[learning rate: 0.0012032]
		[batch 20/20] avg loss: 0.1704031982343237		[learning rate: 0.001201]
	Learning Rate: 0.00120103
	LOSS [training: 0.15315358353233108 | validation: 0.2182659813379289]
	TIME [epoch: 8.15 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15135212127948253		[learning rate: 0.0011988]
		[batch 20/20] avg loss: 0.18327565338256202		[learning rate: 0.0011967]
	Learning Rate: 0.00119667
	LOSS [training: 0.16731388733102232 | validation: 0.2766889845696447]
	TIME [epoch: 8.15 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20832226140657378		[learning rate: 0.0011945]
		[batch 20/20] avg loss: 0.21689137811092335		[learning rate: 0.0011923]
	Learning Rate: 0.00119233
	LOSS [training: 0.2126068197587486 | validation: 0.1686200503782348]
	TIME [epoch: 8.18 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2165990612593076		[learning rate: 0.0011902]
		[batch 20/20] avg loss: 0.20249189276731347		[learning rate: 0.001188]
	Learning Rate: 0.001188
	LOSS [training: 0.20954547701331055 | validation: 0.21857175522597244]
	TIME [epoch: 8.2 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1968919696205667		[learning rate: 0.0011858]
		[batch 20/20] avg loss: 0.1809395673541561		[learning rate: 0.0011837]
	Learning Rate: 0.00118369
	LOSS [training: 0.1889157684873614 | validation: 0.130052787125386]
	TIME [epoch: 8.17 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1704669460158641		[learning rate: 0.0011815]
		[batch 20/20] avg loss: 0.18135668765619772		[learning rate: 0.0011794]
	Learning Rate: 0.00117939
	LOSS [training: 0.17591181683603088 | validation: 0.17383667696774635]
	TIME [epoch: 8.15 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21755933122273358		[learning rate: 0.0011772]
		[batch 20/20] avg loss: 0.19095378086822862		[learning rate: 0.0011751]
	Learning Rate: 0.00117511
	LOSS [training: 0.20425655604548112 | validation: 0.15693658674641403]
	TIME [epoch: 8.19 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18690832126870022		[learning rate: 0.001173]
		[batch 20/20] avg loss: 0.146407649573331		[learning rate: 0.0011708]
	Learning Rate: 0.00117085
	LOSS [training: 0.1666579854210156 | validation: 0.12133316501348507]
	TIME [epoch: 8.17 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14412873405714244		[learning rate: 0.0011687]
		[batch 20/20] avg loss: 0.19617553439265523		[learning rate: 0.0011666]
	Learning Rate: 0.0011666
	LOSS [training: 0.17015213422489883 | validation: 0.22465484824237206]
	TIME [epoch: 8.19 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16260523053171344		[learning rate: 0.0011645]
		[batch 20/20] avg loss: 0.15757587751908472		[learning rate: 0.0011624]
	Learning Rate: 0.00116236
	LOSS [training: 0.16009055402539912 | validation: 0.1609352383959302]
	TIME [epoch: 8.15 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15989483564898638		[learning rate: 0.0011603]
		[batch 20/20] avg loss: 0.16246519519827046		[learning rate: 0.0011581]
	Learning Rate: 0.00115815
	LOSS [training: 0.16118001542362842 | validation: 0.28222686503221234]
	TIME [epoch: 8.15 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2004191504251057		[learning rate: 0.001156]
		[batch 20/20] avg loss: 0.1708045918609093		[learning rate: 0.0011539]
	Learning Rate: 0.00115394
	LOSS [training: 0.1856118711430075 | validation: 0.15819802089686844]
	TIME [epoch: 8.15 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1554857421078674		[learning rate: 0.0011518]
		[batch 20/20] avg loss: 0.18257426030858856		[learning rate: 0.0011498]
	Learning Rate: 0.00114975
	LOSS [training: 0.169030001208228 | validation: 0.15050587702567397]
	TIME [epoch: 8.18 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18950071210459082		[learning rate: 0.0011477]
		[batch 20/20] avg loss: 0.16374009032117284		[learning rate: 0.0011456]
	Learning Rate: 0.00114558
	LOSS [training: 0.17662040121288183 | validation: 0.3648404443729983]
	TIME [epoch: 8.21 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22326789554695153		[learning rate: 0.0011435]
		[batch 20/20] avg loss: 0.20292843795081833		[learning rate: 0.0011414]
	Learning Rate: 0.00114142
	LOSS [training: 0.21309816674888488 | validation: 0.20305780454297578]
	TIME [epoch: 8.16 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18457263130975612		[learning rate: 0.0011394]
		[batch 20/20] avg loss: 0.21032010723703345		[learning rate: 0.0011373]
	Learning Rate: 0.00113728
	LOSS [training: 0.1974463692733948 | validation: 0.313189768949437]
	TIME [epoch: 8.16 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1689545156215261		[learning rate: 0.0011352]
		[batch 20/20] avg loss: 0.19640506636943483		[learning rate: 0.0011332]
	Learning Rate: 0.00113316
	LOSS [training: 0.18267979099548043 | validation: 0.2555755404536603]
	TIME [epoch: 8.17 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16238128182561032		[learning rate: 0.0011311]
		[batch 20/20] avg loss: 0.16633060268985328		[learning rate: 0.001129]
	Learning Rate: 0.00112904
	LOSS [training: 0.1643559422577318 | validation: 0.1904620755158198]
	TIME [epoch: 8.23 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15577443601265215		[learning rate: 0.001127]
		[batch 20/20] avg loss: 0.2166944119848786		[learning rate: 0.0011249]
	Learning Rate: 0.00112495
	LOSS [training: 0.18623442399876536 | validation: 0.15274093459165938]
	TIME [epoch: 8.15 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2027778619640947		[learning rate: 0.0011229]
		[batch 20/20] avg loss: 0.179799850140781		[learning rate: 0.0011209]
	Learning Rate: 0.00112086
	LOSS [training: 0.19128885605243787 | validation: 0.2516708469530696]
	TIME [epoch: 8.16 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20376389717705762		[learning rate: 0.0011188]
		[batch 20/20] avg loss: 0.13322631867683418		[learning rate: 0.0011168]
	Learning Rate: 0.0011168
	LOSS [training: 0.16849510792694594 | validation: 0.18485074660304113]
	TIME [epoch: 8.16 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1620503714294566		[learning rate: 0.0011148]
		[batch 20/20] avg loss: 0.2399782794559775		[learning rate: 0.0011127]
	Learning Rate: 0.00111274
	LOSS [training: 0.20101432544271702 | validation: 0.21286689133456324]
	TIME [epoch: 8.18 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22021481761851117		[learning rate: 0.0011107]
		[batch 20/20] avg loss: 0.20503226950781692		[learning rate: 0.0011087]
	Learning Rate: 0.0011087
	LOSS [training: 0.21262354356316404 | validation: 0.5552948950407084]
	TIME [epoch: 8.16 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1772969905446045		[learning rate: 0.0011067]
		[batch 20/20] avg loss: 0.1947827443943745		[learning rate: 0.0011047]
	Learning Rate: 0.00110468
	LOSS [training: 0.18603986746948947 | validation: 0.20787619406578442]
	TIME [epoch: 8.17 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16627770135849537		[learning rate: 0.0011027]
		[batch 20/20] avg loss: 0.16995023248955127		[learning rate: 0.0011007]
	Learning Rate: 0.00110067
	LOSS [training: 0.16811396692402325 | validation: 0.1621298815083593]
	TIME [epoch: 8.21 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21604840524908045		[learning rate: 0.0010987]
		[batch 20/20] avg loss: 0.18536806695399308		[learning rate: 0.0010967]
	Learning Rate: 0.00109668
	LOSS [training: 0.2007082361015368 | validation: 0.23068686447047657]
	TIME [epoch: 8.18 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1576881448259267		[learning rate: 0.0010947]
		[batch 20/20] avg loss: 0.2609921989440237		[learning rate: 0.0010927]
	Learning Rate: 0.0010927
	LOSS [training: 0.20934017188497528 | validation: 0.11682441687518873]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_709.pth
	Model improved!!!
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14621094402182358		[learning rate: 0.0010907]
		[batch 20/20] avg loss: 0.2479557759256216		[learning rate: 0.0010887]
	Learning Rate: 0.00108873
	LOSS [training: 0.1970833599737226 | validation: 0.1880212459874817]
	TIME [epoch: 8.17 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19039437729528796		[learning rate: 0.0010868]
		[batch 20/20] avg loss: 0.29373066980001644		[learning rate: 0.0010848]
	Learning Rate: 0.00108478
	LOSS [training: 0.2420625235476522 | validation: 0.2537473549387596]
	TIME [epoch: 8.19 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1874475461322353		[learning rate: 0.0010828]
		[batch 20/20] avg loss: 0.17307988831136328		[learning rate: 0.0010808]
	Learning Rate: 0.00108084
	LOSS [training: 0.1802637172217993 | validation: 0.1247969756352586]
	TIME [epoch: 8.14 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17366906904741747		[learning rate: 0.0010789]
		[batch 20/20] avg loss: 0.15270195425436564		[learning rate: 0.0010769]
	Learning Rate: 0.00107692
	LOSS [training: 0.1631855116508915 | validation: 0.12977348594479637]
	TIME [epoch: 8.17 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22468784793068766		[learning rate: 0.001075]
		[batch 20/20] avg loss: 0.2056737947804025		[learning rate: 0.001073]
	Learning Rate: 0.00107301
	LOSS [training: 0.21518082135554506 | validation: 0.19583102280558773]
	TIME [epoch: 8.14 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16619099639690632		[learning rate: 0.0010711]
		[batch 20/20] avg loss: 0.16967889138821945		[learning rate: 0.0010691]
	Learning Rate: 0.00106912
	LOSS [training: 0.1679349438925629 | validation: 0.2596243426528937]
	TIME [epoch: 8.15 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16235888983234323		[learning rate: 0.0010672]
		[batch 20/20] avg loss: 0.21118316836250464		[learning rate: 0.0010652]
	Learning Rate: 0.00106524
	LOSS [training: 0.18677102909742394 | validation: 0.26635102167567454]
	TIME [epoch: 8.14 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16715768718613389		[learning rate: 0.0010633]
		[batch 20/20] avg loss: 0.1451232156540474		[learning rate: 0.0010614]
	Learning Rate: 0.00106137
	LOSS [training: 0.15614045142009064 | validation: 0.17186383551173268]
	TIME [epoch: 8.17 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2194806278003222		[learning rate: 0.0010594]
		[batch 20/20] avg loss: 0.17192031180077044		[learning rate: 0.0010575]
	Learning Rate: 0.00105752
	LOSS [training: 0.19570046980054628 | validation: 0.14950263781519843]
	TIME [epoch: 8.2 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.193534755605073		[learning rate: 0.0010556]
		[batch 20/20] avg loss: 0.15705705773218556		[learning rate: 0.0010537]
	Learning Rate: 0.00105368
	LOSS [training: 0.17529590666862926 | validation: 0.13831086114087476]
	TIME [epoch: 8.15 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1547561684720725		[learning rate: 0.0010518]
		[batch 20/20] avg loss: 0.1933315400117972		[learning rate: 0.0010499]
	Learning Rate: 0.00104986
	LOSS [training: 0.17404385424193486 | validation: 0.15028115260374925]
	TIME [epoch: 8.16 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14237024088111455		[learning rate: 0.001048]
		[batch 20/20] avg loss: 0.16299070733069992		[learning rate: 0.0010461]
	Learning Rate: 0.00104605
	LOSS [training: 0.15268047410590724 | validation: 0.16995050884161803]
	TIME [epoch: 8.15 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18421069435368384		[learning rate: 0.0010442]
		[batch 20/20] avg loss: 0.20261766955760102		[learning rate: 0.0010423]
	Learning Rate: 0.00104225
	LOSS [training: 0.19341418195564244 | validation: 0.2677871896140598]
	TIME [epoch: 8.23 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1779732667190777		[learning rate: 0.0010404]
		[batch 20/20] avg loss: 0.17123652689407703		[learning rate: 0.0010385]
	Learning Rate: 0.00103847
	LOSS [training: 0.17460489680657737 | validation: 0.20904006215868443]
	TIME [epoch: 8.15 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15529205787217512		[learning rate: 0.0010366]
		[batch 20/20] avg loss: 0.15758174253255133		[learning rate: 0.0010347]
	Learning Rate: 0.0010347
	LOSS [training: 0.15643690020236325 | validation: 0.28678601098989476]
	TIME [epoch: 8.15 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20061848528610496		[learning rate: 0.0010328]
		[batch 20/20] avg loss: 0.18607572086584953		[learning rate: 0.0010309]
	Learning Rate: 0.00103095
	LOSS [training: 0.19334710307597724 | validation: 0.14471765664474284]
	TIME [epoch: 8.14 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1592455909471085		[learning rate: 0.0010291]
		[batch 20/20] avg loss: 0.23031001839369653		[learning rate: 0.0010272]
	Learning Rate: 0.00102721
	LOSS [training: 0.1947778046704025 | validation: 0.15449366082501975]
	TIME [epoch: 8.18 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15705514193886344		[learning rate: 0.0010253]
		[batch 20/20] avg loss: 0.160135800301875		[learning rate: 0.0010235]
	Learning Rate: 0.00102348
	LOSS [training: 0.15859547112036926 | validation: 0.5997534733181312]
	TIME [epoch: 8.15 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21059303138699073		[learning rate: 0.0010216]
		[batch 20/20] avg loss: 0.15492520517287817		[learning rate: 0.0010198]
	Learning Rate: 0.00101976
	LOSS [training: 0.18275911827993446 | validation: 0.14854227276615006]
	TIME [epoch: 8.15 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1498218265680628		[learning rate: 0.0010179]
		[batch 20/20] avg loss: 0.16990210005927442		[learning rate: 0.0010161]
	Learning Rate: 0.00101606
	LOSS [training: 0.15986196331366864 | validation: 0.14498360589709436]
	TIME [epoch: 8.15 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16911551471995154		[learning rate: 0.0010142]
		[batch 20/20] avg loss: 0.22027630517701713		[learning rate: 0.0010124]
	Learning Rate: 0.00101238
	LOSS [training: 0.19469590994848435 | validation: 0.1540398980652883]
	TIME [epoch: 8.17 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12015437982889958		[learning rate: 0.0010105]
		[batch 20/20] avg loss: 0.1885464782010392		[learning rate: 0.0010087]
	Learning Rate: 0.0010087
	LOSS [training: 0.15435042901496937 | validation: 0.20720558430108235]
	TIME [epoch: 8.22 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16469888031138152		[learning rate: 0.0010069]
		[batch 20/20] avg loss: 0.2062254961416372		[learning rate: 0.001005]
	Learning Rate: 0.00100504
	LOSS [training: 0.18546218822650937 | validation: 0.5195234330316643]
	TIME [epoch: 8.15 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2024970515144433		[learning rate: 0.0010032]
		[batch 20/20] avg loss: 0.17379274336123954		[learning rate: 0.0010014]
	Learning Rate: 0.00100139
	LOSS [training: 0.18814489743784144 | validation: 0.3568899299601669]
	TIME [epoch: 8.15 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17649715041345201		[learning rate: 0.00099958]
		[batch 20/20] avg loss: 0.1506850692236442		[learning rate: 0.00099776]
	Learning Rate: 0.00099776
	LOSS [training: 0.16359110981854813 | validation: 0.22239575609180373]
	TIME [epoch: 8.18 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14676226808434217		[learning rate: 0.00099595]
		[batch 20/20] avg loss: 0.16386367293293552		[learning rate: 0.00099414]
	Learning Rate: 0.00099414
	LOSS [training: 0.15531297050863885 | validation: 0.1373383209552109]
	TIME [epoch: 8.2 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16724257419018906		[learning rate: 0.00099233]
		[batch 20/20] avg loss: 0.2101333255471849		[learning rate: 0.00099053]
	Learning Rate: 0.000990532
	LOSS [training: 0.18868794986868695 | validation: 0.17394860929541786]
	TIME [epoch: 8.15 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19094268152541827		[learning rate: 0.00098873]
		[batch 20/20] avg loss: 0.16962163301494756		[learning rate: 0.00098694]
	Learning Rate: 0.000986937
	LOSS [training: 0.18028215727018293 | validation: 0.19666960291773838]
	TIME [epoch: 8.15 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20617905922414176		[learning rate: 0.00098514]
		[batch 20/20] avg loss: 0.16018873269504968		[learning rate: 0.00098336]
	Learning Rate: 0.000983355
	LOSS [training: 0.18318389595959572 | validation: 0.1601854698865483]
	TIME [epoch: 8.15 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18669370483917067		[learning rate: 0.00098157]
		[batch 20/20] avg loss: 0.1667787626498716		[learning rate: 0.00097979]
	Learning Rate: 0.000979787
	LOSS [training: 0.17673623374452113 | validation: 0.1418295361851589]
	TIME [epoch: 8.16 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2043966324390783		[learning rate: 0.00097801]
		[batch 20/20] avg loss: 0.14390736451687725		[learning rate: 0.00097623]
	Learning Rate: 0.000976231
	LOSS [training: 0.1741519984779778 | validation: 0.2911895502005811]
	TIME [epoch: 8.16 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1691082716560951		[learning rate: 0.00097446]
		[batch 20/20] avg loss: 0.18466580621978113		[learning rate: 0.00097269]
	Learning Rate: 0.000972688
	LOSS [training: 0.17688703893793808 | validation: 0.12890688492826222]
	TIME [epoch: 8.15 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1714004805159762		[learning rate: 0.00097092]
		[batch 20/20] avg loss: 0.27163368999882564		[learning rate: 0.00096916]
	Learning Rate: 0.000969158
	LOSS [training: 0.22151708525740094 | validation: 0.39472347519526535]
	TIME [epoch: 8.2 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18533921525035352		[learning rate: 0.0009674]
		[batch 20/20] avg loss: 0.14044135248010592		[learning rate: 0.00096564]
	Learning Rate: 0.000965641
	LOSS [training: 0.1628902838652297 | validation: 0.13374104274389162]
	TIME [epoch: 8.16 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15206759218788304		[learning rate: 0.00096389]
		[batch 20/20] avg loss: 0.22577907490477284		[learning rate: 0.00096214]
	Learning Rate: 0.000962137
	LOSS [training: 0.18892333354632793 | validation: 0.211069644603601]
	TIME [epoch: 8.18 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15428453373335307		[learning rate: 0.00096039]
		[batch 20/20] avg loss: 0.18118443031264847		[learning rate: 0.00095865]
	Learning Rate: 0.000958645
	LOSS [training: 0.16773448202300073 | validation: 0.306501256600795]
	TIME [epoch: 8.16 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17813916293271712		[learning rate: 0.0009569]
		[batch 20/20] avg loss: 0.1831731688437051		[learning rate: 0.00095517]
	Learning Rate: 0.000955166
	LOSS [training: 0.18065616588821115 | validation: 0.21756426604854087]
	TIME [epoch: 8.19 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2021658437643688		[learning rate: 0.00095343]
		[batch 20/20] avg loss: 0.1892280148522438		[learning rate: 0.0009517]
	Learning Rate: 0.0009517
	LOSS [training: 0.19569692930830632 | validation: 0.1508083610413417]
	TIME [epoch: 8.14 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15317815779203509		[learning rate: 0.00094997]
		[batch 20/20] avg loss: 0.17692260741079843		[learning rate: 0.00094825]
	Learning Rate: 0.000948246
	LOSS [training: 0.16505038260141675 | validation: 0.22275183008382288]
	TIME [epoch: 8.16 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17119303154723922		[learning rate: 0.00094652]
		[batch 20/20] avg loss: 0.16087087043054593		[learning rate: 0.0009448]
	Learning Rate: 0.000944805
	LOSS [training: 0.16603195098889256 | validation: 0.21385712793232564]
	TIME [epoch: 8.16 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15520893328499438		[learning rate: 0.00094309]
		[batch 20/20] avg loss: 0.16583436490965248		[learning rate: 0.00094138]
	Learning Rate: 0.000941376
	LOSS [training: 0.16052164909732344 | validation: 0.18213849166343332]
	TIME [epoch: 8.15 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17855545443953738		[learning rate: 0.00093967]
		[batch 20/20] avg loss: 0.1449615499820121		[learning rate: 0.00093796]
	Learning Rate: 0.00093796
	LOSS [training: 0.16175850221077473 | validation: 0.18712954235605223]
	TIME [epoch: 8.15 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15899533042814168		[learning rate: 0.00093626]
		[batch 20/20] avg loss: 0.16567496531532383		[learning rate: 0.00093456]
	Learning Rate: 0.000934556
	LOSS [training: 0.1623351478717327 | validation: 0.20189267645977357]
	TIME [epoch: 8.16 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19378091602536912		[learning rate: 0.00093286]
		[batch 20/20] avg loss: 0.21469661040547		[learning rate: 0.00093116]
	Learning Rate: 0.000931164
	LOSS [training: 0.20423876321541953 | validation: 0.467785235934406]
	TIME [epoch: 8.22 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23214153290415726		[learning rate: 0.00092947]
		[batch 20/20] avg loss: 0.1643003648291479		[learning rate: 0.00092779]
	Learning Rate: 0.000927785
	LOSS [training: 0.1982209488666526 | validation: 0.1650958832548295]
	TIME [epoch: 8.16 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15528696378922371		[learning rate: 0.0009261]
		[batch 20/20] avg loss: 0.1524296953039377		[learning rate: 0.00092442]
	Learning Rate: 0.000924418
	LOSS [training: 0.15385832954658069 | validation: 0.28180430761174236]
	TIME [epoch: 8.16 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21063304987994544		[learning rate: 0.00092274]
		[batch 20/20] avg loss: 0.1902413984515701		[learning rate: 0.00092106]
	Learning Rate: 0.000921063
	LOSS [training: 0.20043722416575777 | validation: 0.13483110278558172]
	TIME [epoch: 8.18 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14608779413672074		[learning rate: 0.00091939]
		[batch 20/20] avg loss: 0.1493711528468823		[learning rate: 0.00091772]
	Learning Rate: 0.000917721
	LOSS [training: 0.1477294734918015 | validation: 0.23805496364108628]
	TIME [epoch: 8.2 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17285772204780284		[learning rate: 0.00091605]
		[batch 20/20] avg loss: 0.1646589875872344		[learning rate: 0.00091439]
	Learning Rate: 0.00091439
	LOSS [training: 0.16875835481751864 | validation: 0.18318371309655065]
	TIME [epoch: 8.15 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22156463530151677		[learning rate: 0.00091273]
		[batch 20/20] avg loss: 0.15028221072906658		[learning rate: 0.00091107]
	Learning Rate: 0.000911072
	LOSS [training: 0.1859234230152917 | validation: 0.18696938364991636]
	TIME [epoch: 8.15 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15576321777560448		[learning rate: 0.00090942]
		[batch 20/20] avg loss: 0.1677060546363433		[learning rate: 0.00090777]
	Learning Rate: 0.000907766
	LOSS [training: 0.1617346362059739 | validation: 0.4460839710745202]
	TIME [epoch: 8.15 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20448191553353476		[learning rate: 0.00090612]
		[batch 20/20] avg loss: 0.16204648581531017		[learning rate: 0.00090447]
	Learning Rate: 0.000904471
	LOSS [training: 0.18326420067442245 | validation: 0.13733812399556322]
	TIME [epoch: 8.16 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17240847449577396		[learning rate: 0.00090283]
		[batch 20/20] avg loss: 0.17222329034428296		[learning rate: 0.00090119]
	Learning Rate: 0.000901189
	LOSS [training: 0.17231588242002843 | validation: 0.2314248265411458]
	TIME [epoch: 8.17 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1666410262225112		[learning rate: 0.00089955]
		[batch 20/20] avg loss: 0.20949417566430242		[learning rate: 0.00089792]
	Learning Rate: 0.000897918
	LOSS [training: 0.18806760094340683 | validation: 0.45172279208099925]
	TIME [epoch: 8.17 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17622024339029715		[learning rate: 0.00089629]
		[batch 20/20] avg loss: 0.18316028996433514		[learning rate: 0.00089466]
	Learning Rate: 0.00089466
	LOSS [training: 0.1796902666773161 | validation: 0.17316582336164504]
	TIME [epoch: 8.19 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17032968695217673		[learning rate: 0.00089303]
		[batch 20/20] avg loss: 0.15369743797450866		[learning rate: 0.00089141]
	Learning Rate: 0.000891413
	LOSS [training: 0.1620135624633427 | validation: 0.2608377799192353]
	TIME [epoch: 8.15 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17260221521197155		[learning rate: 0.00088979]
		[batch 20/20] avg loss: 0.13298806281081235		[learning rate: 0.00088818]
	Learning Rate: 0.000888178
	LOSS [training: 0.1527951390113919 | validation: 0.37091988745479326]
	TIME [epoch: 8.18 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17313531415200323		[learning rate: 0.00088656]
		[batch 20/20] avg loss: 0.17574146021119458		[learning rate: 0.00088495]
	Learning Rate: 0.000884955
	LOSS [training: 0.17443838718159893 | validation: 0.32219254078865533]
	TIME [epoch: 8.18 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16400951825928806		[learning rate: 0.00088335]
		[batch 20/20] avg loss: 0.1503562882358203		[learning rate: 0.00088174]
	Learning Rate: 0.000881743
	LOSS [training: 0.15718290324755418 | validation: 0.2272059401028255]
	TIME [epoch: 8.19 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17464773780867482		[learning rate: 0.00088014]
		[batch 20/20] avg loss: 0.19691435595000056		[learning rate: 0.00087854]
	Learning Rate: 0.000878543
	LOSS [training: 0.1857810468793377 | validation: 0.20535201772161013]
	TIME [epoch: 8.14 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.172625008094294		[learning rate: 0.00087695]
		[batch 20/20] avg loss: 0.17870411106420322		[learning rate: 0.00087536]
	Learning Rate: 0.000875355
	LOSS [training: 0.17566455957924862 | validation: 0.24190449226392027]
	TIME [epoch: 8.17 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2079230807106133		[learning rate: 0.00087377]
		[batch 20/20] avg loss: 0.14789321830519161		[learning rate: 0.00087218]
	Learning Rate: 0.000872178
	LOSS [training: 0.17790814950790246 | validation: 0.16243782064201154]
	TIME [epoch: 8.16 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1577344524917173		[learning rate: 0.00087059]
		[batch 20/20] avg loss: 0.155173335264297		[learning rate: 0.00086901]
	Learning Rate: 0.000869013
	LOSS [training: 0.15645389387800718 | validation: 0.17846389673428995]
	TIME [epoch: 8.15 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14741765661005474		[learning rate: 0.00086743]
		[batch 20/20] avg loss: 0.16669015659186087		[learning rate: 0.00086586]
	Learning Rate: 0.000865859
	LOSS [training: 0.1570539066009578 | validation: 0.2622725442865749]
	TIME [epoch: 8.15 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16044191191905025		[learning rate: 0.00086429]
		[batch 20/20] avg loss: 0.1486799315249196		[learning rate: 0.00086272]
	Learning Rate: 0.000862717
	LOSS [training: 0.1545609217219849 | validation: 0.16597031719191718]
	TIME [epoch: 8.15 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15787156251311907		[learning rate: 0.00086115]
		[batch 20/20] avg loss: 0.1410258656395491		[learning rate: 0.00085959]
	Learning Rate: 0.000859586
	LOSS [training: 0.1494487140763341 | validation: 0.1346280807213262]
	TIME [epoch: 8.18 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16468250058181974		[learning rate: 0.00085803]
		[batch 20/20] avg loss: 0.14305427504768373		[learning rate: 0.00085647]
	Learning Rate: 0.000856467
	LOSS [training: 0.15386838781475176 | validation: 0.23454602506723032]
	TIME [epoch: 8.19 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13638840215423365		[learning rate: 0.00085491]
		[batch 20/20] avg loss: 0.206151995502744		[learning rate: 0.00085336]
	Learning Rate: 0.000853359
	LOSS [training: 0.17127019882848882 | validation: 0.269384428984629]
	TIME [epoch: 8.15 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16156192796353389		[learning rate: 0.00085181]
		[batch 20/20] avg loss: 0.17685338089379965		[learning rate: 0.00085026]
	Learning Rate: 0.000850262
	LOSS [training: 0.16920765442866678 | validation: 0.1688063237190286]
	TIME [epoch: 8.15 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1823957351439496		[learning rate: 0.00084872]
		[batch 20/20] avg loss: 0.16184377396156993		[learning rate: 0.00084718]
	Learning Rate: 0.000847176
	LOSS [training: 0.17211975455275977 | validation: 0.30786505510446494]
	TIME [epoch: 8.2 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21271466751617055		[learning rate: 0.00084564]
		[batch 20/20] avg loss: 0.14119054989195373		[learning rate: 0.0008441]
	Learning Rate: 0.000844102
	LOSS [training: 0.17695260870406213 | validation: 0.22148533936478243]
	TIME [epoch: 8.19 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1568750023700613		[learning rate: 0.00084257]
		[batch 20/20] avg loss: 0.1689271409898461		[learning rate: 0.00084104]
	Learning Rate: 0.000841038
	LOSS [training: 0.16290107167995369 | validation: 0.21229173012061253]
	TIME [epoch: 8.15 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2035626076554876		[learning rate: 0.00083951]
		[batch 20/20] avg loss: 0.15753265229524227		[learning rate: 0.00083799]
	Learning Rate: 0.000837986
	LOSS [training: 0.18054762997536494 | validation: 0.1765373464179039]
	TIME [epoch: 8.15 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16582303589995145		[learning rate: 0.00083646]
		[batch 20/20] avg loss: 0.18508620534654796		[learning rate: 0.00083495]
	Learning Rate: 0.000834945
	LOSS [training: 0.1754546206232497 | validation: 0.20033407415996085]
	TIME [epoch: 8.14 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19389898496547786		[learning rate: 0.00083343]
		[batch 20/20] avg loss: 0.1682023291556129		[learning rate: 0.00083192]
	Learning Rate: 0.000831915
	LOSS [training: 0.1810506570605454 | validation: 0.18709473310955693]
	TIME [epoch: 8.17 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14900221285455717		[learning rate: 0.0008304]
		[batch 20/20] avg loss: 0.21987265810214374		[learning rate: 0.0008289]
	Learning Rate: 0.000828896
	LOSS [training: 0.18443743547835045 | validation: 0.15905189601448996]
	TIME [epoch: 8.14 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14525524262080514		[learning rate: 0.00082739]
		[batch 20/20] avg loss: 0.18876407581432197		[learning rate: 0.00082589]
	Learning Rate: 0.000825888
	LOSS [training: 0.16700965921756356 | validation: 0.3600534008210559]
	TIME [epoch: 8.15 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17905304359519603		[learning rate: 0.00082439]
		[batch 20/20] avg loss: 0.1852858066244984		[learning rate: 0.00082289]
	Learning Rate: 0.000822891
	LOSS [training: 0.1821694251098472 | validation: 0.18531654778910028]
	TIME [epoch: 8.14 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1678511303117396		[learning rate: 0.0008214]
		[batch 20/20] avg loss: 0.16468722598830893		[learning rate: 0.0008199]
	Learning Rate: 0.000819904
	LOSS [training: 0.16626917815002426 | validation: 0.17418790347114702]
	TIME [epoch: 8.17 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17333182954848317		[learning rate: 0.00081842]
		[batch 20/20] avg loss: 0.15463895106423914		[learning rate: 0.00081693]
	Learning Rate: 0.000816929
	LOSS [training: 0.16398539030636114 | validation: 0.11214490881169803]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_789.pth
	Model improved!!!
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13688041902464523		[learning rate: 0.00081545]
		[batch 20/20] avg loss: 0.1507383869009586		[learning rate: 0.00081396]
	Learning Rate: 0.000813964
	LOSS [training: 0.14380940296280192 | validation: 0.3123250209054415]
	TIME [epoch: 8.15 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.318460236306947		[learning rate: 0.00081249]
		[batch 20/20] avg loss: 0.1780299075948068		[learning rate: 0.00081101]
	Learning Rate: 0.00081101
	LOSS [training: 0.24824507195087692 | validation: 0.24461344569341698]
	TIME [epoch: 8.17 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1450789480143966		[learning rate: 0.00080954]
		[batch 20/20] avg loss: 0.14619706855756176		[learning rate: 0.00080807]
	Learning Rate: 0.000808067
	LOSS [training: 0.14563800828597917 | validation: 0.1475379017966177]
	TIME [epoch: 8.17 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21516141740453937		[learning rate: 0.0008066]
		[batch 20/20] avg loss: 0.1632185011990124		[learning rate: 0.00080513]
	Learning Rate: 0.000805135
	LOSS [training: 0.18918995930177587 | validation: 0.13693734799670612]
	TIME [epoch: 8.17 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18904972420925184		[learning rate: 0.00080367]
		[batch 20/20] avg loss: 0.16448080574298962		[learning rate: 0.00080221]
	Learning Rate: 0.000802213
	LOSS [training: 0.17676526497612075 | validation: 0.18582971745546273]
	TIME [epoch: 8.15 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22228480118586535		[learning rate: 0.00080076]
		[batch 20/20] avg loss: 0.17732053433216025		[learning rate: 0.0007993]
	Learning Rate: 0.000799301
	LOSS [training: 0.19980266775901281 | validation: 0.12832920822659916]
	TIME [epoch: 8.19 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2169060905124774		[learning rate: 0.00079785]
		[batch 20/20] avg loss: 0.16229361635821885		[learning rate: 0.0007964]
	Learning Rate: 0.000796401
	LOSS [training: 0.1895998534353481 | validation: 0.18394424408120408]
	TIME [epoch: 8.16 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15943333950409833		[learning rate: 0.00079495]
		[batch 20/20] avg loss: 0.17482835037652975		[learning rate: 0.00079351]
	Learning Rate: 0.000793511
	LOSS [training: 0.16713084494031408 | validation: 0.15248782464158747]
	TIME [epoch: 8.17 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1837773096389772		[learning rate: 0.00079207]
		[batch 20/20] avg loss: 0.15548331506773944		[learning rate: 0.00079063]
	Learning Rate: 0.000790631
	LOSS [training: 0.1696303123533583 | validation: 0.14304047549164706]
	TIME [epoch: 8.15 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1421763054617725		[learning rate: 0.00078919]
		[batch 20/20] avg loss: 0.159022214902157		[learning rate: 0.00078776]
	Learning Rate: 0.000787761
	LOSS [training: 0.15059926018196473 | validation: 0.1580373879550426]
	TIME [epoch: 8.15 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15133944524494689		[learning rate: 0.00078633]
		[batch 20/20] avg loss: 0.1728062399775713		[learning rate: 0.0007849]
	Learning Rate: 0.000784903
	LOSS [training: 0.16207284261125915 | validation: 0.20373996727103666]
	TIME [epoch: 8.14 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17343021613491424		[learning rate: 0.00078348]
		[batch 20/20] avg loss: 0.13077133974362112		[learning rate: 0.00078205]
	Learning Rate: 0.000782054
	LOSS [training: 0.15210077793926768 | validation: 0.2639130412744123]
	TIME [epoch: 8.17 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1713464060878122		[learning rate: 0.00078063]
		[batch 20/20] avg loss: 0.16835845095412888		[learning rate: 0.00077922]
	Learning Rate: 0.000779216
	LOSS [training: 0.16985242852097057 | validation: 0.19316953933011297]
	TIME [epoch: 8.2 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1759823193684597		[learning rate: 0.0007778]
		[batch 20/20] avg loss: 0.14909709519935666		[learning rate: 0.00077639]
	Learning Rate: 0.000776388
	LOSS [training: 0.16253970728390824 | validation: 0.3260202180944329]
	TIME [epoch: 8.15 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27351900560032705		[learning rate: 0.00077498]
		[batch 20/20] avg loss: 0.16975382968462957		[learning rate: 0.00077357]
	Learning Rate: 0.000773571
	LOSS [training: 0.22163641764247838 | validation: 0.1981063837292032]
	TIME [epoch: 8.15 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16995633239149469		[learning rate: 0.00077217]
		[batch 20/20] avg loss: 0.16835266429148713		[learning rate: 0.00077076]
	Learning Rate: 0.000770763
	LOSS [training: 0.1691544983414909 | validation: 0.2722589906650792]
	TIME [epoch: 8.17 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15691056243920593		[learning rate: 0.00076936]
		[batch 20/20] avg loss: 0.16041681079417208		[learning rate: 0.00076797]
	Learning Rate: 0.000767966
	LOSS [training: 0.158663686616689 | validation: 0.1452238740020199]
	TIME [epoch: 8.2 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1481055838960464		[learning rate: 0.00076657]
		[batch 20/20] avg loss: 0.1558573668090067		[learning rate: 0.00076518]
	Learning Rate: 0.000765179
	LOSS [training: 0.15198147535252654 | validation: 0.24282359561217487]
	TIME [epoch: 8.14 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1569932426745669		[learning rate: 0.00076379]
		[batch 20/20] avg loss: 0.13041236790538957		[learning rate: 0.0007624]
	Learning Rate: 0.000762402
	LOSS [training: 0.14370280528997825 | validation: 0.16085510682135132]
	TIME [epoch: 8.14 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16928785098210702		[learning rate: 0.00076102]
		[batch 20/20] avg loss: 0.15738100756878337		[learning rate: 0.00075964]
	Learning Rate: 0.000759636
	LOSS [training: 0.1633344292754452 | validation: 0.26304493667490714]
	TIME [epoch: 8.14 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14098056924870717		[learning rate: 0.00075826]
		[batch 20/20] avg loss: 0.15625303799207246		[learning rate: 0.00075688]
	Learning Rate: 0.000756879
	LOSS [training: 0.14861680362038981 | validation: 0.2212942949471634]
	TIME [epoch: 8.16 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16551913477268024		[learning rate: 0.0007555]
		[batch 20/20] avg loss: 0.13606516070184702		[learning rate: 0.00075413]
	Learning Rate: 0.000754132
	LOSS [training: 0.15079214773726363 | validation: 0.22389714207325923]
	TIME [epoch: 8.15 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14406272944459483		[learning rate: 0.00075276]
		[batch 20/20] avg loss: 0.14455596791576872		[learning rate: 0.0007514]
	Learning Rate: 0.000751395
	LOSS [training: 0.1443093486801818 | validation: 0.17741151105366015]
	TIME [epoch: 8.14 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1421768303443199		[learning rate: 0.00075003]
		[batch 20/20] avg loss: 0.13867627246075798		[learning rate: 0.00074867]
	Learning Rate: 0.000748668
	LOSS [training: 0.14042655140253896 | validation: 0.1351056705294678]
	TIME [epoch: 8.18 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1367982119546231		[learning rate: 0.00074731]
		[batch 20/20] avg loss: 0.14447970040527933		[learning rate: 0.00074595]
	Learning Rate: 0.000745951
	LOSS [training: 0.1406389561799512 | validation: 0.277287620602595]
	TIME [epoch: 8.17 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15512760786392593		[learning rate: 0.0007446]
		[batch 20/20] avg loss: 0.13729522549089085		[learning rate: 0.00074324]
	Learning Rate: 0.000743244
	LOSS [training: 0.1462114166774084 | validation: 0.4096803473084345]
	TIME [epoch: 8.17 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17082712511722628		[learning rate: 0.00074189]
		[batch 20/20] avg loss: 0.14884050536147386		[learning rate: 0.00074055]
	Learning Rate: 0.000740547
	LOSS [training: 0.15983381523935009 | validation: 0.20837282761014134]
	TIME [epoch: 8.14 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14865372531924187		[learning rate: 0.0007392]
		[batch 20/20] avg loss: 0.17407240535787966		[learning rate: 0.00073786]
	Learning Rate: 0.00073786
	LOSS [training: 0.16136306533856076 | validation: 0.22371812703099328]
	TIME [epoch: 8.18 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15731654049135574		[learning rate: 0.00073652]
		[batch 20/20] avg loss: 0.16218522145502412		[learning rate: 0.00073518]
	Learning Rate: 0.000735182
	LOSS [training: 0.15975088097318996 | validation: 0.21183324937988068]
	TIME [epoch: 8.17 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17312394479482218		[learning rate: 0.00073385]
		[batch 20/20] avg loss: 0.1509778448583263		[learning rate: 0.00073251]
	Learning Rate: 0.000732514
	LOSS [training: 0.16205089482657425 | validation: 0.14972900776296574]
	TIME [epoch: 8.17 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1473216765642921		[learning rate: 0.00073118]
		[batch 20/20] avg loss: 0.15136621200032838		[learning rate: 0.00072986]
	Learning Rate: 0.000729855
	LOSS [training: 0.14934394428231024 | validation: 0.16538942685354482]
	TIME [epoch: 8.15 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15535949412982322		[learning rate: 0.00072853]
		[batch 20/20] avg loss: 0.1402568991252736		[learning rate: 0.00072721]
	Learning Rate: 0.000727207
	LOSS [training: 0.1478081966275484 | validation: 0.19984209484625642]
	TIME [epoch: 8.14 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14142831987140264		[learning rate: 0.00072589]
		[batch 20/20] avg loss: 0.17233832641878405		[learning rate: 0.00072457]
	Learning Rate: 0.000724568
	LOSS [training: 0.15688332314509337 | validation: 0.17516630776135192]
	TIME [epoch: 8.14 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13265701031970306		[learning rate: 0.00072325]
		[batch 20/20] avg loss: 0.17791899434286373		[learning rate: 0.00072194]
	Learning Rate: 0.000721938
	LOSS [training: 0.15528800233128343 | validation: 0.24590111008152465]
	TIME [epoch: 8.15 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19452202621721756		[learning rate: 0.00072063]
		[batch 20/20] avg loss: 0.17222535686579402		[learning rate: 0.00071932]
	Learning Rate: 0.000719318
	LOSS [training: 0.1833736915415058 | validation: 0.20364711360319215]
	TIME [epoch: 8.17 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16843034151608427		[learning rate: 0.00071801]
		[batch 20/20] avg loss: 0.1719606463148311		[learning rate: 0.00071671]
	Learning Rate: 0.000716708
	LOSS [training: 0.17019549391545766 | validation: 0.1341575485133977]
	TIME [epoch: 8.14 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1837995291266071		[learning rate: 0.00071541]
		[batch 20/20] avg loss: 0.16226286376082016		[learning rate: 0.00071411]
	Learning Rate: 0.000714107
	LOSS [training: 0.17303119644371362 | validation: 0.14426981110765724]
	TIME [epoch: 8.2 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13341157530233955		[learning rate: 0.00071281]
		[batch 20/20] avg loss: 0.16959460319746913		[learning rate: 0.00071152]
	Learning Rate: 0.000711515
	LOSS [training: 0.15150308924990433 | validation: 0.2049269968620996]
	TIME [epoch: 8.14 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14079131079493132		[learning rate: 0.00071022]
		[batch 20/20] avg loss: 0.19132548762434823		[learning rate: 0.00070893]
	Learning Rate: 0.000708933
	LOSS [training: 0.16605839920963977 | validation: 0.20304965097213024]
	TIME [epoch: 8.17 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14701634722528892		[learning rate: 0.00070765]
		[batch 20/20] avg loss: 0.14468841448122247		[learning rate: 0.00070636]
	Learning Rate: 0.00070636
	LOSS [training: 0.1458523808532557 | validation: 0.12346452556714183]
	TIME [epoch: 8.15 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1369388174077418		[learning rate: 0.00070508]
		[batch 20/20] avg loss: 0.14345885455537127		[learning rate: 0.0007038]
	Learning Rate: 0.000703797
	LOSS [training: 0.14019883598155652 | validation: 0.16920748957800866]
	TIME [epoch: 8.19 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13682078308768827		[learning rate: 0.00070252]
		[batch 20/20] avg loss: 0.1700334344430284		[learning rate: 0.00070124]
	Learning Rate: 0.000701243
	LOSS [training: 0.15342710876535834 | validation: 0.19711456831950847]
	TIME [epoch: 8.14 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13306593686115428		[learning rate: 0.00069997]
		[batch 20/20] avg loss: 0.15482924914669408		[learning rate: 0.0006987]
	Learning Rate: 0.000698698
	LOSS [training: 0.14394759300392418 | validation: 0.27303274586267445]
	TIME [epoch: 8.14 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19406741734754046		[learning rate: 0.00069743]
		[batch 20/20] avg loss: 0.16855355246154083		[learning rate: 0.00069616]
	Learning Rate: 0.000696162
	LOSS [training: 0.18131048490454066 | validation: 0.12164630045602767]
	TIME [epoch: 8.16 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13543677301102397		[learning rate: 0.0006949]
		[batch 20/20] avg loss: 0.17768255814276737		[learning rate: 0.00069364]
	Learning Rate: 0.000693636
	LOSS [training: 0.15655966557689566 | validation: 0.12639641372856042]
	TIME [epoch: 8.14 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15032464024296793		[learning rate: 0.00069238]
		[batch 20/20] avg loss: 0.14701782829289922		[learning rate: 0.00069112]
	Learning Rate: 0.000691119
	LOSS [training: 0.14867123426793355 | validation: 0.16654319507752133]
	TIME [epoch: 8.14 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13941756964465005		[learning rate: 0.00068986]
		[batch 20/20] avg loss: 0.13708024232672283		[learning rate: 0.00068861]
	Learning Rate: 0.000688611
	LOSS [training: 0.13824890598568643 | validation: 0.1635959727854434]
	TIME [epoch: 8.19 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15510266673616335		[learning rate: 0.00068736]
		[batch 20/20] avg loss: 0.14948417263038236		[learning rate: 0.00068611]
	Learning Rate: 0.000686112
	LOSS [training: 0.15229341968327287 | validation: 0.18649606272410127]
	TIME [epoch: 8.17 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13058380507505243		[learning rate: 0.00068487]
		[batch 20/20] avg loss: 0.16292276368811204		[learning rate: 0.00068362]
	Learning Rate: 0.000683622
	LOSS [training: 0.14675328438158222 | validation: 0.17810751699051525]
	TIME [epoch: 8.15 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13726200109427505		[learning rate: 0.00068238]
		[batch 20/20] avg loss: 0.20259804591906252		[learning rate: 0.00068114]
	Learning Rate: 0.000681141
	LOSS [training: 0.1699300235066688 | validation: 0.19012016050636502]
	TIME [epoch: 8.15 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13905531220423178		[learning rate: 0.0006799]
		[batch 20/20] avg loss: 0.14456071154476086		[learning rate: 0.00067867]
	Learning Rate: 0.000678669
	LOSS [training: 0.1418080118744963 | validation: 0.14521587149077092]
	TIME [epoch: 8.19 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14282446056126857		[learning rate: 0.00067744]
		[batch 20/20] avg loss: 0.19940726197481878		[learning rate: 0.00067621]
	Learning Rate: 0.000676206
	LOSS [training: 0.17111586126804365 | validation: 0.1359878036800237]
	TIME [epoch: 8.16 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1646855083816702		[learning rate: 0.00067498]
		[batch 20/20] avg loss: 0.14636872380930216		[learning rate: 0.00067375]
	Learning Rate: 0.000673752
	LOSS [training: 0.15552711609548614 | validation: 0.2671069274872704]
	TIME [epoch: 8.15 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13667851084645072		[learning rate: 0.00067253]
		[batch 20/20] avg loss: 0.1317320624857897		[learning rate: 0.00067131]
	Learning Rate: 0.000671307
	LOSS [training: 0.1342052866661202 | validation: 0.21334901055428207]
	TIME [epoch: 8.14 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15104909640793585		[learning rate: 0.00067009]
		[batch 20/20] avg loss: 0.14781903833148105		[learning rate: 0.00066887]
	Learning Rate: 0.000668871
	LOSS [training: 0.14943406736970846 | validation: 0.15783522759784713]
	TIME [epoch: 8.14 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1655688071525438		[learning rate: 0.00066766]
		[batch 20/20] avg loss: 0.13890594117998264		[learning rate: 0.00066644]
	Learning Rate: 0.000666443
	LOSS [training: 0.1522373741662632 | validation: 0.23622529032906214]
	TIME [epoch: 8.14 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17068551839438567		[learning rate: 0.00066523]
		[batch 20/20] avg loss: 0.1575392202974311		[learning rate: 0.00066402]
	Learning Rate: 0.000664025
	LOSS [training: 0.1641123693459084 | validation: 0.17672879942320918]
	TIME [epoch: 8.16 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15781933280250451		[learning rate: 0.00066282]
		[batch 20/20] avg loss: 0.1356740604526055		[learning rate: 0.00066161]
	Learning Rate: 0.000661615
	LOSS [training: 0.14674669662755496 | validation: 0.14173236889028668]
	TIME [epoch: 8.14 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18440237689457806		[learning rate: 0.00066041]
		[batch 20/20] avg loss: 0.15411606483136636		[learning rate: 0.00065921]
	Learning Rate: 0.000659214
	LOSS [training: 0.16925922086297218 | validation: 0.16216230213379845]
	TIME [epoch: 8.14 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17892722758666665		[learning rate: 0.00065802]
		[batch 20/20] avg loss: 0.1356399000819456		[learning rate: 0.00065682]
	Learning Rate: 0.000656822
	LOSS [training: 0.15728356383430614 | validation: 0.1822039946202]
	TIME [epoch: 8.19 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1506797090549417		[learning rate: 0.00065563]
		[batch 20/20] avg loss: 0.16122301339753464		[learning rate: 0.00065444]
	Learning Rate: 0.000654438
	LOSS [training: 0.15595136122623815 | validation: 0.1893506134143747]
	TIME [epoch: 8.16 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16471220311280693		[learning rate: 0.00065325]
		[batch 20/20] avg loss: 0.16095550020160554		[learning rate: 0.00065206]
	Learning Rate: 0.000652063
	LOSS [training: 0.1628338516572062 | validation: 0.14978456559479156]
	TIME [epoch: 8.15 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14429876046610007		[learning rate: 0.00065088]
		[batch 20/20] avg loss: 0.18782440714506896		[learning rate: 0.0006497]
	Learning Rate: 0.000649697
	LOSS [training: 0.16606158380558453 | validation: 0.15263351978727463]
	TIME [epoch: 8.16 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16982814530760887		[learning rate: 0.00064852]
		[batch 20/20] avg loss: 0.12533940119457693		[learning rate: 0.00064734]
	Learning Rate: 0.000647339
	LOSS [training: 0.1475837732510929 | validation: 0.23676046367612819]
	TIME [epoch: 8.18 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18873100335224818		[learning rate: 0.00064616]
		[batch 20/20] avg loss: 0.16777806693072195		[learning rate: 0.00064499]
	Learning Rate: 0.00064499
	LOSS [training: 0.17825453514148507 | validation: 0.17546245599626023]
	TIME [epoch: 8.14 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13305739834452696		[learning rate: 0.00064382]
		[batch 20/20] avg loss: 0.13113099159662522		[learning rate: 0.00064265]
	Learning Rate: 0.000642649
	LOSS [training: 0.13209419497057612 | validation: 0.20717882438178625]
	TIME [epoch: 8.16 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21893350990837296		[learning rate: 0.00064148]
		[batch 20/20] avg loss: 0.23995198205144286		[learning rate: 0.00064032]
	Learning Rate: 0.000640317
	LOSS [training: 0.2294427459799079 | validation: 0.23005083660679282]
	TIME [epoch: 8.14 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19953312002256512		[learning rate: 0.00063915]
		[batch 20/20] avg loss: 0.17338276324479407		[learning rate: 0.00063799]
	Learning Rate: 0.000637993
	LOSS [training: 0.1864579416336796 | validation: 0.17117898773609297]
	TIME [epoch: 8.14 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16842601808034666		[learning rate: 0.00063683]
		[batch 20/20] avg loss: 0.13127547003137943		[learning rate: 0.00063568]
	Learning Rate: 0.000635677
	LOSS [training: 0.14985074405586302 | validation: 0.12933552646866725]
	TIME [epoch: 8.14 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13523126554421802		[learning rate: 0.00063452]
		[batch 20/20] avg loss: 0.13374204794677805		[learning rate: 0.00063337]
	Learning Rate: 0.000633371
	LOSS [training: 0.13448665674549803 | validation: 0.1492271835010725]
	TIME [epoch: 8.17 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1415277153224011		[learning rate: 0.00063222]
		[batch 20/20] avg loss: 0.15747034948824193		[learning rate: 0.00063107]
	Learning Rate: 0.000631072
	LOSS [training: 0.1494990324053215 | validation: 0.18873972242966677]
	TIME [epoch: 8.14 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16620751403498396		[learning rate: 0.00062993]
		[batch 20/20] avg loss: 0.13367466344862056		[learning rate: 0.00062878]
	Learning Rate: 0.000628782
	LOSS [training: 0.1499410887418023 | validation: 0.1562757599094886]
	TIME [epoch: 8.14 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16145178279687236		[learning rate: 0.00062764]
		[batch 20/20] avg loss: 0.16108726076318003		[learning rate: 0.0006265]
	Learning Rate: 0.0006265
	LOSS [training: 0.1612695217800262 | validation: 0.15386852016533478]
	TIME [epoch: 8.14 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1513983007503263		[learning rate: 0.00062536]
		[batch 20/20] avg loss: 0.12990579498627225		[learning rate: 0.00062423]
	Learning Rate: 0.000624226
	LOSS [training: 0.14065204786829927 | validation: 0.12379566324932333]
	TIME [epoch: 8.14 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13533039968763885		[learning rate: 0.00062309]
		[batch 20/20] avg loss: 0.1677650399888256		[learning rate: 0.00062196]
	Learning Rate: 0.000621961
	LOSS [training: 0.15154771983823226 | validation: 0.21022424448423097]
	TIME [epoch: 8.16 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13676710163456074		[learning rate: 0.00062083]
		[batch 20/20] avg loss: 0.12932881967680343		[learning rate: 0.0006197]
	Learning Rate: 0.000619704
	LOSS [training: 0.13304796065568208 | validation: 0.19158836709075933]
	TIME [epoch: 8.14 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13841300766818798		[learning rate: 0.00061858]
		[batch 20/20] avg loss: 0.13323293463394878		[learning rate: 0.00061745]
	Learning Rate: 0.000617455
	LOSS [training: 0.1358229711510684 | validation: 0.15935967838528747]
	TIME [epoch: 8.14 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13003496284405877		[learning rate: 0.00061633]
		[batch 20/20] avg loss: 0.12112104168331712		[learning rate: 0.00061521]
	Learning Rate: 0.000615214
	LOSS [training: 0.12557800226368793 | validation: 0.1290129107738086]
	TIME [epoch: 8.2 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19275157557159167		[learning rate: 0.0006141]
		[batch 20/20] avg loss: 0.12642502493975746		[learning rate: 0.00061298]
	Learning Rate: 0.000612982
	LOSS [training: 0.1595883002556746 | validation: 0.15029982874691566]
	TIME [epoch: 8.17 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21040643737084172		[learning rate: 0.00061187]
		[batch 20/20] avg loss: 0.18223509575396754		[learning rate: 0.00061076]
	Learning Rate: 0.000610757
	LOSS [training: 0.19632076656240466 | validation: 0.22125973454158165]
	TIME [epoch: 8.15 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17525797894346232		[learning rate: 0.00060965]
		[batch 20/20] avg loss: 0.13571359142544853		[learning rate: 0.00060854]
	Learning Rate: 0.00060854
	LOSS [training: 0.1554857851844554 | validation: 0.21408984693794372]
	TIME [epoch: 8.15 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13713325760752007		[learning rate: 0.00060744]
		[batch 20/20] avg loss: 0.12137454479485148		[learning rate: 0.00060633]
	Learning Rate: 0.000606332
	LOSS [training: 0.1292539012011858 | validation: 0.1482527291658747]
	TIME [epoch: 8.2 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1585011587565735		[learning rate: 0.00060523]
		[batch 20/20] avg loss: 0.14936329760511852		[learning rate: 0.00060413]
	Learning Rate: 0.000604132
	LOSS [training: 0.153932228180846 | validation: 0.17883789304229386]
	TIME [epoch: 8.15 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1431322107269725		[learning rate: 0.00060303]
		[batch 20/20] avg loss: 0.12581457954675893		[learning rate: 0.00060194]
	Learning Rate: 0.000601939
	LOSS [training: 0.13447339513686568 | validation: 0.13479605624768198]
	TIME [epoch: 8.15 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13731686337538973		[learning rate: 0.00060085]
		[batch 20/20] avg loss: 0.13541047795160208		[learning rate: 0.00059975]
	Learning Rate: 0.000599755
	LOSS [training: 0.13636367066349592 | validation: 0.13609337245275266]
	TIME [epoch: 8.14 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1661964808733815		[learning rate: 0.00059867]
		[batch 20/20] avg loss: 0.1253680804150294		[learning rate: 0.00059758]
	Learning Rate: 0.000597578
	LOSS [training: 0.14578228064420543 | validation: 0.18000715601741168]
	TIME [epoch: 8.14 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11199272164934755		[learning rate: 0.00059649]
		[batch 20/20] avg loss: 0.16028670548206775		[learning rate: 0.00059541]
	Learning Rate: 0.00059541
	LOSS [training: 0.13613971356570767 | validation: 0.19825592671642508]
	TIME [epoch: 8.14 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14101281547932118		[learning rate: 0.00059433]
		[batch 20/20] avg loss: 0.14720641202467066		[learning rate: 0.00059325]
	Learning Rate: 0.000593249
	LOSS [training: 0.14410961375199596 | validation: 0.1831881208642323]
	TIME [epoch: 8.16 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11318634252265466		[learning rate: 0.00059217]
		[batch 20/20] avg loss: 0.1430711043558027		[learning rate: 0.0005911]
	Learning Rate: 0.000591096
	LOSS [training: 0.12812872343922868 | validation: 0.16217655160948083]
	TIME [epoch: 8.14 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1490135675213558		[learning rate: 0.00059002]
		[batch 20/20] avg loss: 0.12068653860181171		[learning rate: 0.00058895]
	Learning Rate: 0.000588951
	LOSS [training: 0.13485005306158374 | validation: 0.15341466249900207]
	TIME [epoch: 8.14 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1260750846882467		[learning rate: 0.00058788]
		[batch 20/20] avg loss: 0.12066211403342911		[learning rate: 0.00058681]
	Learning Rate: 0.000586813
	LOSS [training: 0.12336859936083792 | validation: 0.14156693808984078]
	TIME [epoch: 8.19 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13548283342627715		[learning rate: 0.00058575]
		[batch 20/20] avg loss: 0.14148536620370789		[learning rate: 0.00058468]
	Learning Rate: 0.000584684
	LOSS [training: 0.13848409981499252 | validation: 0.12183015032448036]
	TIME [epoch: 8.17 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13514202489224197		[learning rate: 0.00058362]
		[batch 20/20] avg loss: 0.1478403110103172		[learning rate: 0.00058256]
	Learning Rate: 0.000582562
	LOSS [training: 0.1414911679512796 | validation: 0.17772207704156218]
	TIME [epoch: 8.16 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17286125726905405		[learning rate: 0.0005815]
		[batch 20/20] avg loss: 0.13149910604443055		[learning rate: 0.00058045]
	Learning Rate: 0.000580448
	LOSS [training: 0.15218018165674232 | validation: 0.16439387272437567]
	TIME [epoch: 8.15 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12768323793750905		[learning rate: 0.00057939]
		[batch 20/20] avg loss: 0.13992858984439235		[learning rate: 0.00057834]
	Learning Rate: 0.000578341
	LOSS [training: 0.1338059138909507 | validation: 0.1864685603827776]
	TIME [epoch: 8.19 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12571884023235755		[learning rate: 0.00057729]
		[batch 20/20] avg loss: 0.12294491827963294		[learning rate: 0.00057624]
	Learning Rate: 0.000576243
	LOSS [training: 0.12433187925599525 | validation: 0.1475974674146664]
	TIME [epoch: 8.14 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12198727832466619		[learning rate: 0.0005752]
		[batch 20/20] avg loss: 0.17187480377268147		[learning rate: 0.00057415]
	Learning Rate: 0.000574151
	LOSS [training: 0.14693104104867383 | validation: 0.2060913883319716]
	TIME [epoch: 8.16 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13892957426498928		[learning rate: 0.00057311]
		[batch 20/20] avg loss: 0.12978498322865378		[learning rate: 0.00057207]
	Learning Rate: 0.000572068
	LOSS [training: 0.13435727874682152 | validation: 0.26612583412728325]
	TIME [epoch: 8.14 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14951079462113098		[learning rate: 0.00057103]
		[batch 20/20] avg loss: 0.1299597826236979		[learning rate: 0.00056999]
	Learning Rate: 0.000569992
	LOSS [training: 0.1397352886224144 | validation: 0.14847763529611868]
	TIME [epoch: 8.14 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14438085436775455		[learning rate: 0.00056896]
		[batch 20/20] avg loss: 0.12085706684111801		[learning rate: 0.00056792]
	Learning Rate: 0.000567923
	LOSS [training: 0.13261896060443626 | validation: 0.15236273854669613]
	TIME [epoch: 8.14 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12052145404932105		[learning rate: 0.00056689]
		[batch 20/20] avg loss: 0.13864837449157277		[learning rate: 0.00056586]
	Learning Rate: 0.000565862
	LOSS [training: 0.12958491427044688 | validation: 0.1485105671819486]
	TIME [epoch: 8.15 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14634259237960312		[learning rate: 0.00056483]
		[batch 20/20] avg loss: 0.14015613931393942		[learning rate: 0.00056381]
	Learning Rate: 0.000563808
	LOSS [training: 0.14324936584677128 | validation: 0.14464495509128386]
	TIME [epoch: 8.2 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12518630973656528		[learning rate: 0.00056278]
		[batch 20/20] avg loss: 0.1350351981934059		[learning rate: 0.00056176]
	Learning Rate: 0.000561762
	LOSS [training: 0.13011075396498556 | validation: 0.1508019518850755]
	TIME [epoch: 8.14 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1542559099159514		[learning rate: 0.00056074]
		[batch 20/20] avg loss: 0.2093009773473987		[learning rate: 0.00055972]
	Learning Rate: 0.000559724
	LOSS [training: 0.18177844363167503 | validation: 0.16863915725210313]
	TIME [epoch: 8.14 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13299643599017916		[learning rate: 0.00055871]
		[batch 20/20] avg loss: 0.10603857165218733		[learning rate: 0.00055769]
	Learning Rate: 0.000557692
	LOSS [training: 0.11951750382118323 | validation: 0.2462205960178894]
	TIME [epoch: 8.16 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13275528629980252		[learning rate: 0.00055668]
		[batch 20/20] avg loss: 0.1167566668492808		[learning rate: 0.00055567]
	Learning Rate: 0.000555669
	LOSS [training: 0.12475597657454163 | validation: 0.18277042758791387]
	TIME [epoch: 8.21 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12040365704634652		[learning rate: 0.00055466]
		[batch 20/20] avg loss: 0.16622656866806046		[learning rate: 0.00055365]
	Learning Rate: 0.000553652
	LOSS [training: 0.14331511285720347 | validation: 0.26506511215398876]
	TIME [epoch: 8.14 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13282791310179604		[learning rate: 0.00055265]
		[batch 20/20] avg loss: 0.14447653774779412		[learning rate: 0.00055164]
	Learning Rate: 0.000551643
	LOSS [training: 0.13865222542479508 | validation: 0.15312392549758833]
	TIME [epoch: 8.13 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12014019648670513		[learning rate: 0.00055064]
		[batch 20/20] avg loss: 0.1115655102648702		[learning rate: 0.00054964]
	Learning Rate: 0.000549641
	LOSS [training: 0.11585285337578768 | validation: 0.25132667295548133]
	TIME [epoch: 8.13 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13759475627947082		[learning rate: 0.00054864]
		[batch 20/20] avg loss: 0.13160506353736218		[learning rate: 0.00054765]
	Learning Rate: 0.000547646
	LOSS [training: 0.1345999099084165 | validation: 0.13524957247513097]
	TIME [epoch: 8.16 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1388893364654428		[learning rate: 0.00054665]
		[batch 20/20] avg loss: 0.13554516907385356		[learning rate: 0.00054566]
	Learning Rate: 0.000545659
	LOSS [training: 0.13721725276964816 | validation: 0.19814740550017193]
	TIME [epoch: 8.14 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18252038977368104		[learning rate: 0.00054467]
		[batch 20/20] avg loss: 0.13570549159767226		[learning rate: 0.00054368]
	Learning Rate: 0.000543678
	LOSS [training: 0.15911294068567666 | validation: 0.2162276460879426]
	TIME [epoch: 8.14 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14601575071084044		[learning rate: 0.00054269]
		[batch 20/20] avg loss: 0.14591359341304538		[learning rate: 0.00054171]
	Learning Rate: 0.000541705
	LOSS [training: 0.1459646720619429 | validation: 0.11352473711615832]
	TIME [epoch: 8.14 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12573275162704514		[learning rate: 0.00054072]
		[batch 20/20] avg loss: 0.1269023270523141		[learning rate: 0.00053974]
	Learning Rate: 0.00053974
	LOSS [training: 0.12631753933967965 | validation: 0.176353278137708]
	TIME [epoch: 8.14 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14972032413152586		[learning rate: 0.00053876]
		[batch 20/20] avg loss: 0.12664516521854924		[learning rate: 0.00053778]
	Learning Rate: 0.000537781
	LOSS [training: 0.13818274467503752 | validation: 0.1808869099166759]
	TIME [epoch: 8.21 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13579484655106314		[learning rate: 0.0005368]
		[batch 20/20] avg loss: 0.13142553492945236		[learning rate: 0.00053583]
	Learning Rate: 0.000535829
	LOSS [training: 0.13361019074025773 | validation: 0.10916449838791509]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_905.pth
	Model improved!!!
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13158759865906347		[learning rate: 0.00053486]
		[batch 20/20] avg loss: 0.14450730694640507		[learning rate: 0.00053388]
	Learning Rate: 0.000533885
	LOSS [training: 0.13804745280273428 | validation: 0.15639522553237453]
	TIME [epoch: 8.15 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13667208715928764		[learning rate: 0.00053291]
		[batch 20/20] avg loss: 0.11306119673807472		[learning rate: 0.00053195]
	Learning Rate: 0.000531947
	LOSS [training: 0.12486664194868118 | validation: 0.11770533259436682]
	TIME [epoch: 8.14 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1270190152123807		[learning rate: 0.00053098]
		[batch 20/20] avg loss: 0.12582685277627342		[learning rate: 0.00053002]
	Learning Rate: 0.000530017
	LOSS [training: 0.12642293399432708 | validation: 0.1568129035134651]
	TIME [epoch: 8.22 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15496436032398422		[learning rate: 0.00052905]
		[batch 20/20] avg loss: 0.1296544406827453		[learning rate: 0.00052809]
	Learning Rate: 0.000528093
	LOSS [training: 0.1423094005033647 | validation: 0.15612674361779844]
	TIME [epoch: 8.14 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1844545321688696		[learning rate: 0.00052713]
		[batch 20/20] avg loss: 0.11586708490131103		[learning rate: 0.00052618]
	Learning Rate: 0.000526177
	LOSS [training: 0.15016080853509034 | validation: 0.12097032951031311]
	TIME [epoch: 8.13 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14748492288775866		[learning rate: 0.00052522]
		[batch 20/20] avg loss: 0.13730401675080742		[learning rate: 0.00052427]
	Learning Rate: 0.000524267
	LOSS [training: 0.14239446981928303 | validation: 0.24566658283913975]
	TIME [epoch: 8.14 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15967435300652658		[learning rate: 0.00052332]
		[batch 20/20] avg loss: 0.1342137736970686		[learning rate: 0.00052236]
	Learning Rate: 0.000522365
	LOSS [training: 0.14694406335179755 | validation: 0.18084652146793762]
	TIME [epoch: 8.15 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15363092741806342		[learning rate: 0.00052142]
		[batch 20/20] avg loss: 0.1157831732265644		[learning rate: 0.00052047]
	Learning Rate: 0.000520469
	LOSS [training: 0.13470705032231392 | validation: 0.12082264705376167]
	TIME [epoch: 8.14 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1193894390510019		[learning rate: 0.00051952]
		[batch 20/20] avg loss: 0.11612171126561308		[learning rate: 0.00051858]
	Learning Rate: 0.00051858
	LOSS [training: 0.11775557515830745 | validation: 0.1568533055098417]
	TIME [epoch: 8.13 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1338754686517772		[learning rate: 0.00051764]
		[batch 20/20] avg loss: 0.13221908016057718		[learning rate: 0.0005167]
	Learning Rate: 0.000516698
	LOSS [training: 0.1330472744061772 | validation: 0.18822849132308847]
	TIME [epoch: 8.13 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12238133372268653		[learning rate: 0.00051576]
		[batch 20/20] avg loss: 0.13397246796328185		[learning rate: 0.00051482]
	Learning Rate: 0.000514823
	LOSS [training: 0.12817690084298422 | validation: 0.1128507293282075]
	TIME [epoch: 8.15 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11370530074136767		[learning rate: 0.00051389]
		[batch 20/20] avg loss: 0.12647622806962625		[learning rate: 0.00051295]
	Learning Rate: 0.000512955
	LOSS [training: 0.12009076440549697 | validation: 0.14211505203467667]
	TIME [epoch: 8.19 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11634705619859131		[learning rate: 0.00051202]
		[batch 20/20] avg loss: 0.14596173303651966		[learning rate: 0.00051109]
	Learning Rate: 0.000511093
	LOSS [training: 0.13115439461755551 | validation: 0.1513821876159706]
	TIME [epoch: 8.14 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15082102403463113		[learning rate: 0.00051016]
		[batch 20/20] avg loss: 0.15593571593065586		[learning rate: 0.00050924]
	Learning Rate: 0.000509238
	LOSS [training: 0.1533783699826435 | validation: 0.4114883016169243]
	TIME [epoch: 8.13 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18922729471180627		[learning rate: 0.00050831]
		[batch 20/20] avg loss: 0.12909476775742015		[learning rate: 0.00050739]
	Learning Rate: 0.00050739
	LOSS [training: 0.15916103123461323 | validation: 0.21218590606788196]
	TIME [epoch: 8.17 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1233485319016149		[learning rate: 0.00050647]
		[batch 20/20] avg loss: 0.1358131735770115		[learning rate: 0.00050555]
	Learning Rate: 0.000505549
	LOSS [training: 0.1295808527393132 | validation: 0.15567252653774377]
	TIME [epoch: 8.17 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11349794975228049		[learning rate: 0.00050463]
		[batch 20/20] avg loss: 0.12568162936689434		[learning rate: 0.00050371]
	Learning Rate: 0.000503714
	LOSS [training: 0.11958978955958743 | validation: 0.1320146796643972]
	TIME [epoch: 8.15 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14540512663362323		[learning rate: 0.0005028]
		[batch 20/20] avg loss: 0.149090648854245		[learning rate: 0.00050189]
	Learning Rate: 0.000501886
	LOSS [training: 0.14724788774393413 | validation: 0.1591343030137866]
	TIME [epoch: 8.13 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15164281229608384		[learning rate: 0.00050097]
		[batch 20/20] avg loss: 0.14762623025013627		[learning rate: 0.00050006]
	Learning Rate: 0.000500065
	LOSS [training: 0.14963452127311003 | validation: 0.13904719927889556]
	TIME [epoch: 8.13 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12202961819537197		[learning rate: 0.00049916]
		[batch 20/20] avg loss: 0.1476798864685272		[learning rate: 0.00049825]
	Learning Rate: 0.00049825
	LOSS [training: 0.1348547523319496 | validation: 0.14953890495077643]
	TIME [epoch: 8.13 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13600790596455276		[learning rate: 0.00049735]
		[batch 20/20] avg loss: 0.15626888954022172		[learning rate: 0.00049644]
	Learning Rate: 0.000496442
	LOSS [training: 0.1461383977523873 | validation: 0.1638167118943258]
	TIME [epoch: 8.16 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11828859718306621		[learning rate: 0.00049554]
		[batch 20/20] avg loss: 0.14244119025719063		[learning rate: 0.00049464]
	Learning Rate: 0.00049464
	LOSS [training: 0.13036489372012844 | validation: 0.17985429178035992]
	TIME [epoch: 8.13 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13127169518264167		[learning rate: 0.00049374]
		[batch 20/20] avg loss: 0.1451058881202994		[learning rate: 0.00049285]
	Learning Rate: 0.000492845
	LOSS [training: 0.13818879165147052 | validation: 0.1472929034874701]
	TIME [epoch: 8.14 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13775631291992416		[learning rate: 0.00049195]
		[batch 20/20] avg loss: 0.1585784653061863		[learning rate: 0.00049106]
	Learning Rate: 0.000491057
	LOSS [training: 0.14816738911305524 | validation: 0.17532791478044174]
	TIME [epoch: 8.13 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12587297580904694		[learning rate: 0.00049016]
		[batch 20/20] avg loss: 0.13192373596545737		[learning rate: 0.00048927]
	Learning Rate: 0.000489275
	LOSS [training: 0.12889835588725215 | validation: 0.13880556347762643]
	TIME [epoch: 8.15 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11245699408057992		[learning rate: 0.00048839]
		[batch 20/20] avg loss: 0.13036991100474327		[learning rate: 0.0004875]
	Learning Rate: 0.000487499
	LOSS [training: 0.1214134525426616 | validation: 0.1830224444313312]
	TIME [epoch: 8.14 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13719142184864316		[learning rate: 0.00048661]
		[batch 20/20] avg loss: 0.12743830917746396		[learning rate: 0.00048573]
	Learning Rate: 0.00048573
	LOSS [training: 0.13231486551305355 | validation: 0.1757713769993291]
	TIME [epoch: 8.13 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12484871927986954		[learning rate: 0.00048485]
		[batch 20/20] avg loss: 0.1344289454095047		[learning rate: 0.00048397]
	Learning Rate: 0.000483967
	LOSS [training: 0.1296388323446871 | validation: 0.15275492535825697]
	TIME [epoch: 8.14 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1451827644095081		[learning rate: 0.00048309]
		[batch 20/20] avg loss: 0.13519976046010962		[learning rate: 0.00048221]
	Learning Rate: 0.000482211
	LOSS [training: 0.1401912624348089 | validation: 0.14323805734804165]
	TIME [epoch: 8.13 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13828924220610522		[learning rate: 0.00048133]
		[batch 20/20] avg loss: 0.13350145469562646		[learning rate: 0.00048046]
	Learning Rate: 0.000480461
	LOSS [training: 0.13589534845086584 | validation: 0.16151448900922358]
	TIME [epoch: 8.16 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1274583057772998		[learning rate: 0.00047959]
		[batch 20/20] avg loss: 0.1159327157059783		[learning rate: 0.00047872]
	Learning Rate: 0.000478717
	LOSS [training: 0.12169551074163905 | validation: 0.14831349235578564]
	TIME [epoch: 8.14 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12275693906241329		[learning rate: 0.00047785]
		[batch 20/20] avg loss: 0.12220485036100903		[learning rate: 0.00047698]
	Learning Rate: 0.00047698
	LOSS [training: 0.12248089471171117 | validation: 0.1595092946552835]
	TIME [epoch: 8.13 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14442277748366206		[learning rate: 0.00047611]
		[batch 20/20] avg loss: 0.1459440463389535		[learning rate: 0.00047525]
	Learning Rate: 0.000475249
	LOSS [training: 0.14518341191130776 | validation: 0.1348790015662066]
	TIME [epoch: 8.14 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11580509507368567		[learning rate: 0.00047439]
		[batch 20/20] avg loss: 0.1638758945571139		[learning rate: 0.00047352]
	Learning Rate: 0.000473524
	LOSS [training: 0.13984049481539979 | validation: 0.13144551572250138]
	TIME [epoch: 8.15 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11481838806883171		[learning rate: 0.00047266]
		[batch 20/20] avg loss: 0.1309375709949925		[learning rate: 0.00047181]
	Learning Rate: 0.000471806
	LOSS [training: 0.12287797953191211 | validation: 0.22587784401944241]
	TIME [epoch: 8.14 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14159156754405122		[learning rate: 0.00047095]
		[batch 20/20] avg loss: 0.12841035740391685		[learning rate: 0.00047009]
	Learning Rate: 0.000470093
	LOSS [training: 0.13500096247398402 | validation: 0.11784373279651428]
	TIME [epoch: 8.13 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12201287800575134		[learning rate: 0.00046924]
		[batch 20/20] avg loss: 0.12052524237595322		[learning rate: 0.00046839]
	Learning Rate: 0.000468388
	LOSS [training: 0.12126906019085226 | validation: 0.17265871518214768]
	TIME [epoch: 8.14 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15197043297185062		[learning rate: 0.00046754]
		[batch 20/20] avg loss: 0.1260618772878469		[learning rate: 0.00046669]
	Learning Rate: 0.000466688
	LOSS [training: 0.13901615512984875 | validation: 0.19774620949410943]
	TIME [epoch: 8.15 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1450695150963705		[learning rate: 0.00046584]
		[batch 20/20] avg loss: 0.14609053107175107		[learning rate: 0.00046499]
	Learning Rate: 0.000464994
	LOSS [training: 0.1455800230840608 | validation: 0.19020122919523283]
	TIME [epoch: 8.2 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12229726140501393		[learning rate: 0.00046415]
		[batch 20/20] avg loss: 0.13044252174018572		[learning rate: 0.00046331]
	Learning Rate: 0.000463307
	LOSS [training: 0.12636989157259987 | validation: 0.15858599455721475]
	TIME [epoch: 8.13 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14510976049647312		[learning rate: 0.00046247]
		[batch 20/20] avg loss: 0.11839680137046642		[learning rate: 0.00046163]
	Learning Rate: 0.000461625
	LOSS [training: 0.13175328093346977 | validation: 0.14877416447273753]
	TIME [epoch: 8.14 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13352631683522906		[learning rate: 0.00046079]
		[batch 20/20] avg loss: 0.10848816917346733		[learning rate: 0.00045995]
	Learning Rate: 0.00045995
	LOSS [training: 0.1210072430043482 | validation: 0.15774905855014368]
	TIME [epoch: 8.16 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12178437495134505		[learning rate: 0.00045911]
		[batch 20/20] avg loss: 0.14022419090784816		[learning rate: 0.00045828]
	Learning Rate: 0.000458281
	LOSS [training: 0.1310042829295966 | validation: 0.1364160145442187]
	TIME [epoch: 8.19 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12192696680004884		[learning rate: 0.00045745]
		[batch 20/20] avg loss: 0.15716491602988503		[learning rate: 0.00045662]
	Learning Rate: 0.000456618
	LOSS [training: 0.13954594141496696 | validation: 0.2780574423350655]
	TIME [epoch: 8.14 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1291196087217546		[learning rate: 0.00045579]
		[batch 20/20] avg loss: 0.11598512688353797		[learning rate: 0.00045496]
	Learning Rate: 0.000454961
	LOSS [training: 0.12255236780264629 | validation: 0.1506359944619551]
	TIME [epoch: 8.13 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11552152618332308		[learning rate: 0.00045413]
		[batch 20/20] avg loss: 0.1577097484676569		[learning rate: 0.00045331]
	Learning Rate: 0.000453309
	LOSS [training: 0.13661563732549 | validation: 0.250822208730179]
	TIME [epoch: 8.13 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14395154086683565		[learning rate: 0.00045249]
		[batch 20/20] avg loss: 0.12801563972201688		[learning rate: 0.00045166]
	Learning Rate: 0.000451664
	LOSS [training: 0.13598359029442625 | validation: 0.15810590809580946]
	TIME [epoch: 8.14 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14451212567727048		[learning rate: 0.00045084]
		[batch 20/20] avg loss: 0.11839101209323695		[learning rate: 0.00045003]
	Learning Rate: 0.000450025
	LOSS [training: 0.13145156888525372 | validation: 0.12882619550859178]
	TIME [epoch: 8.15 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13107892742825805		[learning rate: 0.00044921]
		[batch 20/20] avg loss: 0.14472617193839094		[learning rate: 0.00044839]
	Learning Rate: 0.000448392
	LOSS [training: 0.1379025496833245 | validation: 0.14174058465574182]
	TIME [epoch: 8.13 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1593296822110598		[learning rate: 0.00044758]
		[batch 20/20] avg loss: 0.12967488621278755		[learning rate: 0.00044676]
	Learning Rate: 0.000446765
	LOSS [training: 0.14450228421192365 | validation: 0.1730260234555]
	TIME [epoch: 8.14 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1398804628773589		[learning rate: 0.00044595]
		[batch 20/20] avg loss: 0.12678657812660346		[learning rate: 0.00044514]
	Learning Rate: 0.000445143
	LOSS [training: 0.13333352050198116 | validation: 0.14516338114191152]
	TIME [epoch: 8.18 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12579406554120337		[learning rate: 0.00044434]
		[batch 20/20] avg loss: 0.16238694471726528		[learning rate: 0.00044353]
	Learning Rate: 0.000443528
	LOSS [training: 0.1440905051292343 | validation: 0.14017665306218832]
	TIME [epoch: 8.16 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14812034352745745		[learning rate: 0.00044272]
		[batch 20/20] avg loss: 0.1370427467759765		[learning rate: 0.00044192]
	Learning Rate: 0.000441918
	LOSS [training: 0.14258154515171698 | validation: 0.11965471269339141]
	TIME [epoch: 8.13 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1818453358813456		[learning rate: 0.00044112]
		[batch 20/20] avg loss: 0.10337331229366212		[learning rate: 0.00044031]
	Learning Rate: 0.000440315
	LOSS [training: 0.14260932408750385 | validation: 0.14064667493357502]
	TIME [epoch: 8.14 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11329784734836894		[learning rate: 0.00043952]
		[batch 20/20] avg loss: 0.1266757399960627		[learning rate: 0.00043872]
	Learning Rate: 0.000438717
	LOSS [training: 0.11998679367221585 | validation: 0.14179454184233325]
	TIME [epoch: 8.19 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1395019249903577		[learning rate: 0.00043792]
		[batch 20/20] avg loss: 0.10783581452477423		[learning rate: 0.00043712]
	Learning Rate: 0.000437125
	LOSS [training: 0.12366886975756594 | validation: 0.12922654089752666]
	TIME [epoch: 8.14 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14301262692126943		[learning rate: 0.00043633]
		[batch 20/20] avg loss: 0.14946574050893774		[learning rate: 0.00043554]
	Learning Rate: 0.000435538
	LOSS [training: 0.14623918371510353 | validation: 0.1454946460478374]
	TIME [epoch: 8.14 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14255187250859863		[learning rate: 0.00043475]
		[batch 20/20] avg loss: 0.12625993833775742		[learning rate: 0.00043396]
	Learning Rate: 0.000433958
	LOSS [training: 0.13440590542317804 | validation: 0.12152053855259647]
	TIME [epoch: 8.13 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12832711554114667		[learning rate: 0.00043317]
		[batch 20/20] avg loss: 0.13514985237817384		[learning rate: 0.00043238]
	Learning Rate: 0.000432383
	LOSS [training: 0.13173848395966026 | validation: 0.12337238135518706]
	TIME [epoch: 8.13 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13946270786224182		[learning rate: 0.0004316]
		[batch 20/20] avg loss: 0.1296223415124928		[learning rate: 0.00043081]
	Learning Rate: 0.000430814
	LOSS [training: 0.13454252468736733 | validation: 0.16432788458266226]
	TIME [epoch: 8.13 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14935551208436504		[learning rate: 0.00043003]
		[batch 20/20] avg loss: 0.10761468924196342		[learning rate: 0.00042925]
	Learning Rate: 0.00042925
	LOSS [training: 0.1284851006631642 | validation: 0.11831091433238682]
	TIME [epoch: 8.16 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13409868927593308		[learning rate: 0.00042847]
		[batch 20/20] avg loss: 0.1179122687868688		[learning rate: 0.00042769]
	Learning Rate: 0.000427692
	LOSS [training: 0.12600547903140094 | validation: 0.20567399325563274]
	TIME [epoch: 8.18 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1291489216867649		[learning rate: 0.00042692]
		[batch 20/20] avg loss: 0.14085832184088798		[learning rate: 0.00042614]
	Learning Rate: 0.00042614
	LOSS [training: 0.1350036217638264 | validation: 0.30537008995875553]
	TIME [epoch: 8.14 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13745447532138816		[learning rate: 0.00042537]
		[batch 20/20] avg loss: 0.11852157312770546		[learning rate: 0.00042459]
	Learning Rate: 0.000424594
	LOSS [training: 0.12798802422454678 | validation: 0.1781369187396749]
	TIME [epoch: 8.14 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13107262284531201		[learning rate: 0.00042382]
		[batch 20/20] avg loss: 0.12283181775778558		[learning rate: 0.00042305]
	Learning Rate: 0.000423053
	LOSS [training: 0.12695222030154882 | validation: 0.2237201618040104]
	TIME [epoch: 8.17 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1474079276766774		[learning rate: 0.00042228]
		[batch 20/20] avg loss: 0.13052337942501321		[learning rate: 0.00042152]
	Learning Rate: 0.000421518
	LOSS [training: 0.13896565355084528 | validation: 0.16666934161901112]
	TIME [epoch: 8.19 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13179324593910763		[learning rate: 0.00042075]
		[batch 20/20] avg loss: 0.1434006580590605		[learning rate: 0.00041999]
	Learning Rate: 0.000419988
	LOSS [training: 0.1375969519990841 | validation: 0.14468711137948054]
	TIME [epoch: 8.13 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12181979910439636		[learning rate: 0.00041923]
		[batch 20/20] avg loss: 0.12498831844079997		[learning rate: 0.00041846]
	Learning Rate: 0.000418464
	LOSS [training: 0.12340405877259815 | validation: 0.21856054635949257]
	TIME [epoch: 8.13 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.153511909228622		[learning rate: 0.0004177]
		[batch 20/20] avg loss: 0.15399880786097037		[learning rate: 0.00041695]
	Learning Rate: 0.000416945
	LOSS [training: 0.15375535854479622 | validation: 0.13446909827447837]
	TIME [epoch: 8.13 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10678523906255714		[learning rate: 0.00041619]
		[batch 20/20] avg loss: 0.16049673101127765		[learning rate: 0.00041543]
	Learning Rate: 0.000415432
	LOSS [training: 0.13364098503691738 | validation: 0.13218131788083234]
	TIME [epoch: 8.15 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10847501511839883		[learning rate: 0.00041468]
		[batch 20/20] avg loss: 0.14415884037800594		[learning rate: 0.00041392]
	Learning Rate: 0.000413924
	LOSS [training: 0.12631692774820238 | validation: 0.13381980634307655]
	TIME [epoch: 8.13 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12220383054254032		[learning rate: 0.00041317]
		[batch 20/20] avg loss: 0.11927666658735114		[learning rate: 0.00041242]
	Learning Rate: 0.000412422
	LOSS [training: 0.12074024856494572 | validation: 0.19653955245301072]
	TIME [epoch: 8.13 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16163955437558317		[learning rate: 0.00041167]
		[batch 20/20] avg loss: 0.13342837421717796		[learning rate: 0.00041093]
	Learning Rate: 0.000410926
	LOSS [training: 0.14753396429638058 | validation: 0.13406766746714063]
	TIME [epoch: 8.17 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13453260703914288		[learning rate: 0.00041018]
		[batch 20/20] avg loss: 0.12618839786140817		[learning rate: 0.00040943]
	Learning Rate: 0.000409434
	LOSS [training: 0.13036050245027553 | validation: 0.15743626371895242]
	TIME [epoch: 8.17 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12738324540765106		[learning rate: 0.00040869]
		[batch 20/20] avg loss: 0.1180952220566962		[learning rate: 0.00040795]
	Learning Rate: 0.000407948
	LOSS [training: 0.12273923373217366 | validation: 0.14409007141790586]
	TIME [epoch: 8.14 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12199869271146788		[learning rate: 0.00040721]
		[batch 20/20] avg loss: 0.1132826303146365		[learning rate: 0.00040647]
	Learning Rate: 0.000406468
	LOSS [training: 0.11764066151305219 | validation: 0.15951818947582846]
	TIME [epoch: 8.13 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1383857573049858		[learning rate: 0.00040573]
		[batch 20/20] avg loss: 0.1681418513347785		[learning rate: 0.00040499]
	Learning Rate: 0.000404993
	LOSS [training: 0.15326380431988215 | validation: 0.184696817865656]
	TIME [epoch: 8.2 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13364895231002655		[learning rate: 0.00040426]
		[batch 20/20] avg loss: 0.13292982873316667		[learning rate: 0.00040352]
	Learning Rate: 0.000403523
	LOSS [training: 0.1332893905215966 | validation: 0.17646852768184249]
	TIME [epoch: 8.14 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15044953004861875		[learning rate: 0.00040279]
		[batch 20/20] avg loss: 0.12938348231801416		[learning rate: 0.00040206]
	Learning Rate: 0.000402059
	LOSS [training: 0.13991650618331644 | validation: 0.11565558129833524]
	TIME [epoch: 8.14 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17080905318832254		[learning rate: 0.00040133]
		[batch 20/20] avg loss: 0.15392492194921534		[learning rate: 0.0004006]
	Learning Rate: 0.0004006
	LOSS [training: 0.16236698756876894 | validation: 0.13229830591153638]
	TIME [epoch: 8.13 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13115825186936142		[learning rate: 0.00039987]
		[batch 20/20] avg loss: 0.12583109653843733		[learning rate: 0.00039915]
	Learning Rate: 0.000399146
	LOSS [training: 0.12849467420389935 | validation: 0.14494597134748965]
	TIME [epoch: 8.13 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12182654103365853		[learning rate: 0.00039842]
		[batch 20/20] avg loss: 0.13743134699312004		[learning rate: 0.0003977]
	Learning Rate: 0.000397697
	LOSS [training: 0.12962894401338929 | validation: 0.12817143281045695]
	TIME [epoch: 8.13 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1337085014845007		[learning rate: 0.00039698]
		[batch 20/20] avg loss: 0.1228081356728958		[learning rate: 0.00039625]
	Learning Rate: 0.000396254
	LOSS [training: 0.12825831857869824 | validation: 0.13197594090471598]
	TIME [epoch: 8.16 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15035218085341628		[learning rate: 0.00039553]
		[batch 20/20] avg loss: 0.11134432525665716		[learning rate: 0.00039482]
	Learning Rate: 0.000394816
	LOSS [training: 0.13084825305503672 | validation: 0.12529298067854666]
	TIME [epoch: 8.18 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1041138903421905		[learning rate: 0.0003941]
		[batch 20/20] avg loss: 0.13376938084569406		[learning rate: 0.00039338]
	Learning Rate: 0.000393383
	LOSS [training: 0.11894163559394229 | validation: 0.11440579999892035]
	TIME [epoch: 8.14 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11523249995654965		[learning rate: 0.00039267]
		[batch 20/20] avg loss: 0.10784537673834331		[learning rate: 0.00039196]
	Learning Rate: 0.000391956
	LOSS [training: 0.11153893834744648 | validation: 0.1155278645052353]
	TIME [epoch: 8.13 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10760131431763949		[learning rate: 0.00039124]
		[batch 20/20] avg loss: 0.16255593383560524		[learning rate: 0.00039053]
	Learning Rate: 0.000390533
	LOSS [training: 0.13507862407662236 | validation: 0.20985760389785685]
	TIME [epoch: 8.17 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14097694038841707		[learning rate: 0.00038982]
		[batch 20/20] avg loss: 0.11277064489493419		[learning rate: 0.00038912]
	Learning Rate: 0.000389116
	LOSS [training: 0.1268737926416756 | validation: 0.12354237484184435]
	TIME [epoch: 8.17 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12273581150766685		[learning rate: 0.00038841]
		[batch 20/20] avg loss: 0.11157343914155463		[learning rate: 0.0003877]
	Learning Rate: 0.000387704
	LOSS [training: 0.11715462532461071 | validation: 0.14526450632560478]
	TIME [epoch: 8.13 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10779277857194902		[learning rate: 0.000387]
		[batch 20/20] avg loss: 0.12743632161680413		[learning rate: 0.0003863]
	Learning Rate: 0.000386297
	LOSS [training: 0.1176145500943766 | validation: 0.1548302383180781]
	TIME [epoch: 8.14 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1470443746927686		[learning rate: 0.0003856]
		[batch 20/20] avg loss: 0.11736034310783475		[learning rate: 0.00038489]
	Learning Rate: 0.000384895
	LOSS [training: 0.1322023589003017 | validation: 0.1421677573385906]
	TIME [epoch: 8.12 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12014081574127919		[learning rate: 0.0003842]
		[batch 20/20] avg loss: 0.12353842809907134		[learning rate: 0.0003835]
	Learning Rate: 0.000383498
	LOSS [training: 0.12183962192017528 | validation: 0.12321139031962677]
	TIME [epoch: 8.14 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15281711079863447		[learning rate: 0.0003828]
		[batch 20/20] avg loss: 0.14782249767598676		[learning rate: 0.00038211]
	Learning Rate: 0.000382106
	LOSS [training: 0.15031980423731062 | validation: 0.14522382694353658]
	TIME [epoch: 8.13 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10450925438791892		[learning rate: 0.00038141]
		[batch 20/20] avg loss: 0.1489279690864471		[learning rate: 0.00038072]
	Learning Rate: 0.00038072
	LOSS [training: 0.12671861173718305 | validation: 0.13371154698297813]
	TIME [epoch: 8.16 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14064390078201844		[learning rate: 0.00038003]
		[batch 20/20] avg loss: 0.11029915333215864		[learning rate: 0.00037934]
	Learning Rate: 0.000379338
	LOSS [training: 0.12547152705708856 | validation: 0.13611421410729263]
	TIME [epoch: 8.12 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12892616137652482		[learning rate: 0.00037865]
		[batch 20/20] avg loss: 0.10305734080394809		[learning rate: 0.00037796]
	Learning Rate: 0.000377961
	LOSS [training: 0.11599175109023645 | validation: 0.11284059048627902]
	TIME [epoch: 8.15 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1125925264264526		[learning rate: 0.00037727]
		[batch 20/20] avg loss: 0.1159448200400444		[learning rate: 0.00037659]
	Learning Rate: 0.00037659
	LOSS [training: 0.11426867323324849 | validation: 0.13983799675576816]
	TIME [epoch: 8.13 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11526912458741696		[learning rate: 0.00037591]
		[batch 20/20] avg loss: 0.11666283600599792		[learning rate: 0.00037522]
	Learning Rate: 0.000375223
	LOSS [training: 0.11596598029670743 | validation: 0.1864068774119228]
	TIME [epoch: 8.19 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13359881843679547		[learning rate: 0.00037454]
		[batch 20/20] avg loss: 0.12869002345908795		[learning rate: 0.00037386]
	Learning Rate: 0.000373861
	LOSS [training: 0.13114442094794168 | validation: 0.1286833789632604]
	TIME [epoch: 8.13 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11579541236457094		[learning rate: 0.00037318]
		[batch 20/20] avg loss: 0.11862253015435416		[learning rate: 0.0003725]
	Learning Rate: 0.000372505
	LOSS [training: 0.11720897125946254 | validation: 0.17334435443453078]
	TIME [epoch: 8.14 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1259756373549083		[learning rate: 0.00037183]
		[batch 20/20] avg loss: 0.12215957176624062		[learning rate: 0.00037115]
	Learning Rate: 0.000371153
	LOSS [training: 0.12406760456057446 | validation: 0.12057178027596141]
	TIME [epoch: 8.15 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1053250278321121		[learning rate: 0.00037048]
		[batch 20/20] avg loss: 0.1134677634128615		[learning rate: 0.00036981]
	Learning Rate: 0.000369806
	LOSS [training: 0.10939639562248679 | validation: 0.13699551712067337]
	TIME [epoch: 8.12 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1156187510771822		[learning rate: 0.00036913]
		[batch 20/20] avg loss: 0.14059960119354256		[learning rate: 0.00036846]
	Learning Rate: 0.000368464
	LOSS [training: 0.1281091761353624 | validation: 0.10428477580936485]
	TIME [epoch: 8.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_1008.pth
	Model improved!!!
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13954447264950925		[learning rate: 0.00036779]
		[batch 20/20] avg loss: 0.12590309600263827		[learning rate: 0.00036713]
	Learning Rate: 0.000367127
	LOSS [training: 0.13272378432607373 | validation: 0.11585334936456644]
	TIME [epoch: 8.16 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11137856340356747		[learning rate: 0.00036646]
		[batch 20/20] avg loss: 0.1432292122091251		[learning rate: 0.00036579]
	Learning Rate: 0.000365794
	LOSS [training: 0.12730388780634627 | validation: 0.2068210365649351]
	TIME [epoch: 8.17 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12830445500797202		[learning rate: 0.00036513]
		[batch 20/20] avg loss: 0.1254119033097123		[learning rate: 0.00036447]
	Learning Rate: 0.000364467
	LOSS [training: 0.12685817915884218 | validation: 0.1381946157763349]
	TIME [epoch: 8.13 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12247143530981217		[learning rate: 0.0003638]
		[batch 20/20] avg loss: 0.11711493803361055		[learning rate: 0.00036314]
	Learning Rate: 0.000363144
	LOSS [training: 0.11979318667171135 | validation: 0.1488841088095171]
	TIME [epoch: 8.13 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10631787378260893		[learning rate: 0.00036248]
		[batch 20/20] avg loss: 0.11158933244469141		[learning rate: 0.00036183]
	Learning Rate: 0.000361826
	LOSS [training: 0.10895360311365017 | validation: 0.11863603991535433]
	TIME [epoch: 8.16 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13006380385297336		[learning rate: 0.00036117]
		[batch 20/20] avg loss: 0.1301624095537393		[learning rate: 0.00036051]
	Learning Rate: 0.000360513
	LOSS [training: 0.13011310670335635 | validation: 0.1664797980677302]
	TIME [epoch: 8.14 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10795017331288445		[learning rate: 0.00035986]
		[batch 20/20] avg loss: 0.14926167947512098		[learning rate: 0.0003592]
	Learning Rate: 0.000359205
	LOSS [training: 0.12860592639400273 | validation: 0.14370819542521962]
	TIME [epoch: 8.14 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14928477480022587		[learning rate: 0.00035855]
		[batch 20/20] avg loss: 0.11503287671272208		[learning rate: 0.0003579]
	Learning Rate: 0.000357901
	LOSS [training: 0.13215882575647397 | validation: 0.16449691032003033]
	TIME [epoch: 8.11 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13721676298359792		[learning rate: 0.00035725]
		[batch 20/20] avg loss: 0.10408176569089903		[learning rate: 0.0003566]
	Learning Rate: 0.000356602
	LOSS [training: 0.12064926433724847 | validation: 0.1232977062788267]
	TIME [epoch: 8.12 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11085958728334408		[learning rate: 0.00035595]
		[batch 20/20] avg loss: 0.11758417156779166		[learning rate: 0.00035531]
	Learning Rate: 0.000355308
	LOSS [training: 0.11422187942556787 | validation: 0.13644384771491055]
	TIME [epoch: 8.12 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10683668671225807		[learning rate: 0.00035466]
		[batch 20/20] avg loss: 0.14251461848548538		[learning rate: 0.00035402]
	Learning Rate: 0.000354019
	LOSS [training: 0.12467565259887174 | validation: 0.15915164520689273]
	TIME [epoch: 8.15 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13119102051066522		[learning rate: 0.00035338]
		[batch 20/20] avg loss: 0.12806878617143663		[learning rate: 0.00035273]
	Learning Rate: 0.000352734
	LOSS [training: 0.1296299033410509 | validation: 0.1819173530182421]
	TIME [epoch: 8.16 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1343628205686343		[learning rate: 0.00035209]
		[batch 20/20] avg loss: 0.10197199688054674		[learning rate: 0.00035145]
	Learning Rate: 0.000351454
	LOSS [training: 0.11816740872459049 | validation: 0.1392682008001886]
	TIME [epoch: 8.13 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11724253393277953		[learning rate: 0.00035082]
		[batch 20/20] avg loss: 0.11338187997094022		[learning rate: 0.00035018]
	Learning Rate: 0.000350179
	LOSS [training: 0.11531220695185991 | validation: 0.12012022158334096]
	TIME [epoch: 8.12 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10889070039644669		[learning rate: 0.00034954]
		[batch 20/20] avg loss: 0.12096130501060487		[learning rate: 0.00034891]
	Learning Rate: 0.000348908
	LOSS [training: 0.11492600270352575 | validation: 0.11938348794799146]
	TIME [epoch: 8.14 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11396822395097601		[learning rate: 0.00034827]
		[batch 20/20] avg loss: 0.11451196880202122		[learning rate: 0.00034764]
	Learning Rate: 0.000347641
	LOSS [training: 0.11424009637649865 | validation: 0.13551593152854224]
	TIME [epoch: 8.18 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11459535954690156		[learning rate: 0.00034701]
		[batch 20/20] avg loss: 0.12138008876879507		[learning rate: 0.00034638]
	Learning Rate: 0.00034638
	LOSS [training: 0.1179877241578483 | validation: 0.13247363593801756]
	TIME [epoch: 8.12 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12616182897164338		[learning rate: 0.00034575]
		[batch 20/20] avg loss: 0.14214517233428833		[learning rate: 0.00034512]
	Learning Rate: 0.000345123
	LOSS [training: 0.13415350065296586 | validation: 0.20673433572329616]
	TIME [epoch: 8.12 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11543566315499283		[learning rate: 0.0003445]
		[batch 20/20] avg loss: 0.12742968907062524		[learning rate: 0.00034387]
	Learning Rate: 0.00034387
	LOSS [training: 0.12143267611280902 | validation: 0.1637784665156609]
	TIME [epoch: 8.12 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1281822438464265		[learning rate: 0.00034325]
		[batch 20/20] avg loss: 0.10926646065986763		[learning rate: 0.00034262]
	Learning Rate: 0.000342622
	LOSS [training: 0.11872435225314706 | validation: 0.13263641445153246]
	TIME [epoch: 8.15 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12138410859616515		[learning rate: 0.000342]
		[batch 20/20] avg loss: 0.17020774230046726		[learning rate: 0.00034138]
	Learning Rate: 0.000341379
	LOSS [training: 0.1457959254483162 | validation: 0.16497385682932436]
	TIME [epoch: 8.12 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1505706629189322		[learning rate: 0.00034076]
		[batch 20/20] avg loss: 0.12474086849440247		[learning rate: 0.00034014]
	Learning Rate: 0.00034014
	LOSS [training: 0.13765576570666732 | validation: 0.13759575710492594]
	TIME [epoch: 8.12 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15539896835433092		[learning rate: 0.00033952]
		[batch 20/20] avg loss: 0.10464870769019016		[learning rate: 0.00033891]
	Learning Rate: 0.000338906
	LOSS [training: 0.13002383802226058 | validation: 0.16739045700825825]
	TIME [epoch: 8.13 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13391644031644645		[learning rate: 0.00033829]
		[batch 20/20] avg loss: 0.1350886937677038		[learning rate: 0.00033768]
	Learning Rate: 0.000337676
	LOSS [training: 0.1345025670420751 | validation: 0.1546532621813931]
	TIME [epoch: 8.17 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14453645828838121		[learning rate: 0.00033706]
		[batch 20/20] avg loss: 0.14967489564225733		[learning rate: 0.00033645]
	Learning Rate: 0.00033645
	LOSS [training: 0.14710567696531934 | validation: 0.1258563602426105]
	TIME [epoch: 8.14 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12212221379588648		[learning rate: 0.00033584]
		[batch 20/20] avg loss: 0.14889286430160348		[learning rate: 0.00033523]
	Learning Rate: 0.000335229
	LOSS [training: 0.135507539048745 | validation: 0.1323043836869939]
	TIME [epoch: 8.13 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1340130167898726		[learning rate: 0.00033462]
		[batch 20/20] avg loss: 0.14683714574660806		[learning rate: 0.00033401]
	Learning Rate: 0.000334013
	LOSS [training: 0.14042508126824033 | validation: 0.15823676620121396]
	TIME [epoch: 8.15 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1354973112436664		[learning rate: 0.00033341]
		[batch 20/20] avg loss: 0.14202439970142727		[learning rate: 0.0003328]
	Learning Rate: 0.000332801
	LOSS [training: 0.13876085547254682 | validation: 0.11433067468738953]
	TIME [epoch: 8.15 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11204948006355908		[learning rate: 0.0003322]
		[batch 20/20] avg loss: 0.11898106406005673		[learning rate: 0.00033159]
	Learning Rate: 0.000331593
	LOSS [training: 0.11551527206180792 | validation: 0.13344909905636915]
	TIME [epoch: 8.15 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1340765350137968		[learning rate: 0.00033099]
		[batch 20/20] avg loss: 0.11579287773013178		[learning rate: 0.00033039]
	Learning Rate: 0.00033039
	LOSS [training: 0.12493470637196427 | validation: 0.12394787688692074]
	TIME [epoch: 8.12 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10466168962073771		[learning rate: 0.00032979]
		[batch 20/20] avg loss: 0.12806702229859562		[learning rate: 0.00032919]
	Learning Rate: 0.000329191
	LOSS [training: 0.11636435595966665 | validation: 0.13697054772772901]
	TIME [epoch: 8.13 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1332141506179313		[learning rate: 0.00032859]
		[batch 20/20] avg loss: 0.11276730362219037		[learning rate: 0.000328]
	Learning Rate: 0.000327996
	LOSS [training: 0.12299072712006083 | validation: 0.15617793423242332]
	TIME [epoch: 8.13 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13235085123567808		[learning rate: 0.0003274]
		[batch 20/20] avg loss: 0.11904040498982212		[learning rate: 0.00032681]
	Learning Rate: 0.000326806
	LOSS [training: 0.1256956281127501 | validation: 0.1599714848557345]
	TIME [epoch: 8.15 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1343394149203746		[learning rate: 0.00032621]
		[batch 20/20] avg loss: 0.12629235648528142		[learning rate: 0.00032562]
	Learning Rate: 0.00032562
	LOSS [training: 0.13031588570282804 | validation: 0.12071197098881749]
	TIME [epoch: 8.14 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15705462214534607		[learning rate: 0.00032503]
		[batch 20/20] avg loss: 0.13793750811969896		[learning rate: 0.00032444]
	Learning Rate: 0.000324438
	LOSS [training: 0.14749606513252256 | validation: 0.12027116629112164]
	TIME [epoch: 8.13 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13361352707367585		[learning rate: 0.00032385]
		[batch 20/20] avg loss: 0.12501457405418806		[learning rate: 0.00032326]
	Learning Rate: 0.00032326
	LOSS [training: 0.12931405056393194 | validation: 0.2692741361086943]
	TIME [epoch: 8.16 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14130263425079928		[learning rate: 0.00032267]
		[batch 20/20] avg loss: 0.12371672684242085		[learning rate: 0.00032209]
	Learning Rate: 0.000322087
	LOSS [training: 0.13250968054661005 | validation: 0.13115001556449546]
	TIME [epoch: 8.13 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11225682392700369		[learning rate: 0.0003215]
		[batch 20/20] avg loss: 0.11736834539753782		[learning rate: 0.00032092]
	Learning Rate: 0.000320918
	LOSS [training: 0.11481258466227073 | validation: 0.12678633868401679]
	TIME [epoch: 8.15 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11902496017025865		[learning rate: 0.00032034]
		[batch 20/20] avg loss: 0.10558839038083405		[learning rate: 0.00031975]
	Learning Rate: 0.000319754
	LOSS [training: 0.11230667527554636 | validation: 0.14565085958073454]
	TIME [epoch: 8.13 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10893048277949384		[learning rate: 0.00031917]
		[batch 20/20] avg loss: 0.12786853401708204		[learning rate: 0.00031859]
	Learning Rate: 0.000318593
	LOSS [training: 0.11839950839828794 | validation: 0.1465710889685546]
	TIME [epoch: 8.18 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1132422394853256		[learning rate: 0.00031801]
		[batch 20/20] avg loss: 0.12778879693038678		[learning rate: 0.00031744]
	Learning Rate: 0.000317437
	LOSS [training: 0.12051551820785618 | validation: 0.21497408915879054]
	TIME [epoch: 8.13 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11766955429003649		[learning rate: 0.00031686]
		[batch 20/20] avg loss: 0.11308767924236349		[learning rate: 0.00031629]
	Learning Rate: 0.000316285
	LOSS [training: 0.11537861676619998 | validation: 0.16421435243480154]
	TIME [epoch: 8.15 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1053509327439072		[learning rate: 0.00031571]
		[batch 20/20] avg loss: 0.1377438763139341		[learning rate: 0.00031514]
	Learning Rate: 0.000315137
	LOSS [training: 0.12154740452892061 | validation: 0.12641074059908614]
	TIME [epoch: 8.12 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11718854938972967		[learning rate: 0.00031457]
		[batch 20/20] avg loss: 0.10409995882228554		[learning rate: 0.00031399]
	Learning Rate: 0.000313994
	LOSS [training: 0.1106442541060076 | validation: 0.11062412617007598]
	TIME [epoch: 8.13 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12194555869294652		[learning rate: 0.00031342]
		[batch 20/20] avg loss: 0.10282788206677163		[learning rate: 0.00031285]
	Learning Rate: 0.000312854
	LOSS [training: 0.11238672037985908 | validation: 0.13256584747306438]
	TIME [epoch: 8.12 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11228016295225149		[learning rate: 0.00031229]
		[batch 20/20] avg loss: 0.10675699868871728		[learning rate: 0.00031172]
	Learning Rate: 0.000311719
	LOSS [training: 0.10951858082048438 | validation: 0.1435143453134468]
	TIME [epoch: 8.12 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1411479115018454		[learning rate: 0.00031115]
		[batch 20/20] avg loss: 0.11018909259870333		[learning rate: 0.00031059]
	Learning Rate: 0.000310588
	LOSS [training: 0.12566850205027436 | validation: 0.1604465668496221]
	TIME [epoch: 8.18 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11082047071556285		[learning rate: 0.00031002]
		[batch 20/20] avg loss: 0.11285660052421506		[learning rate: 0.00030946]
	Learning Rate: 0.000309461
	LOSS [training: 0.11183853561988892 | validation: 0.13415411318489678]
	TIME [epoch: 8.15 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10631067952498882		[learning rate: 0.0003089]
		[batch 20/20] avg loss: 0.09859864080113555		[learning rate: 0.00030834]
	Learning Rate: 0.000308338
	LOSS [training: 0.10245466016306218 | validation: 0.11036034832853009]
	TIME [epoch: 8.13 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11105579035219515		[learning rate: 0.00030778]
		[batch 20/20] avg loss: 0.10831602685174399		[learning rate: 0.00030722]
	Learning Rate: 0.000307219
	LOSS [training: 0.10968590860196956 | validation: 0.15604891696236073]
	TIME [epoch: 8.13 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11572365481994715		[learning rate: 0.00030666]
		[batch 20/20] avg loss: 0.11890341849395403		[learning rate: 0.0003061]
	Learning Rate: 0.000306104
	LOSS [training: 0.11731353665695059 | validation: 0.11437143808497247]
	TIME [epoch: 8.2 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12000594461147569		[learning rate: 0.00030555]
		[batch 20/20] avg loss: 0.1396359875255638		[learning rate: 0.00030499]
	Learning Rate: 0.000304993
	LOSS [training: 0.12982096606851976 | validation: 0.13297107631883007]
	TIME [epoch: 8.14 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11727562372936617		[learning rate: 0.00030444]
		[batch 20/20] avg loss: 0.09986788137095709		[learning rate: 0.00030389]
	Learning Rate: 0.000303886
	LOSS [training: 0.10857175255016165 | validation: 0.14163201355166763]
	TIME [epoch: 8.13 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11878586499324198		[learning rate: 0.00030333]
		[batch 20/20] avg loss: 0.1070797869047428		[learning rate: 0.00030278]
	Learning Rate: 0.000302783
	LOSS [training: 0.11293282594899241 | validation: 0.13371314862595401]
	TIME [epoch: 8.12 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13500489020045364		[learning rate: 0.00030223]
		[batch 20/20] avg loss: 0.10586743250621375		[learning rate: 0.00030168]
	Learning Rate: 0.000301684
	LOSS [training: 0.12043616135333365 | validation: 0.12790669161056756]
	TIME [epoch: 8.14 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11228355734810155		[learning rate: 0.00030114]
		[batch 20/20] avg loss: 0.11614204992762352		[learning rate: 0.00030059]
	Learning Rate: 0.000300589
	LOSS [training: 0.11421280363786253 | validation: 0.11687439866233378]
	TIME [epoch: 8.14 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14785387499581917		[learning rate: 0.00030004]
		[batch 20/20] avg loss: 0.10827741465762197		[learning rate: 0.0002995]
	Learning Rate: 0.000299499
	LOSS [training: 0.1280656448267206 | validation: 0.13546184636724765]
	TIME [epoch: 8.12 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13552086755829168		[learning rate: 0.00029895]
		[batch 20/20] avg loss: 0.13757732806098805		[learning rate: 0.00029841]
	Learning Rate: 0.000298412
	LOSS [training: 0.1365490978096399 | validation: 0.15319320511842205]
	TIME [epoch: 8.17 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12103854748505534		[learning rate: 0.00029787]
		[batch 20/20] avg loss: 0.16900279557119977		[learning rate: 0.00029733]
	Learning Rate: 0.000297329
	LOSS [training: 0.14502067152812753 | validation: 0.18292220679226556]
	TIME [epoch: 8.13 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14192018773590345		[learning rate: 0.00029679]
		[batch 20/20] avg loss: 0.13700950431246592		[learning rate: 0.00029625]
	Learning Rate: 0.00029625
	LOSS [training: 0.13946484602418469 | validation: 0.1565601911452928]
	TIME [epoch: 8.15 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13181700961918144		[learning rate: 0.00029571]
		[batch 20/20] avg loss: 0.10296878415161452		[learning rate: 0.00029517]
	Learning Rate: 0.000295175
	LOSS [training: 0.117392896885398 | validation: 0.12029200083649487]
	TIME [epoch: 8.13 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1377766245887333		[learning rate: 0.00029464]
		[batch 20/20] avg loss: 0.11006251601498059		[learning rate: 0.0002941]
	Learning Rate: 0.000294103
	LOSS [training: 0.12391957030185694 | validation: 0.11616145932726701]
	TIME [epoch: 8.18 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13352466263981538		[learning rate: 0.00029357]
		[batch 20/20] avg loss: 0.10727740100830671		[learning rate: 0.00029304]
	Learning Rate: 0.000293036
	LOSS [training: 0.12040103182406107 | validation: 0.13004306170468202]
	TIME [epoch: 8.14 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10979595601598897		[learning rate: 0.0002925]
		[batch 20/20] avg loss: 0.0935239782522265		[learning rate: 0.00029197]
	Learning Rate: 0.000291973
	LOSS [training: 0.10165996713410774 | validation: 0.10937222985472993]
	TIME [epoch: 8.14 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12863680768872882		[learning rate: 0.00029144]
		[batch 20/20] avg loss: 0.13354470461023757		[learning rate: 0.00029091]
	Learning Rate: 0.000290913
	LOSS [training: 0.1310907561494832 | validation: 0.11771522691627358]
	TIME [epoch: 8.14 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1281145947174831		[learning rate: 0.00029038]
		[batch 20/20] avg loss: 0.1097365572616668		[learning rate: 0.00028986]
	Learning Rate: 0.000289857
	LOSS [training: 0.11892557598957496 | validation: 0.13369432082595076]
	TIME [epoch: 8.13 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10204437163626663		[learning rate: 0.00028933]
		[batch 20/20] avg loss: 0.11896970202509252		[learning rate: 0.00028881]
	Learning Rate: 0.000288805
	LOSS [training: 0.11050703683067957 | validation: 0.13995669521528237]
	TIME [epoch: 8.12 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10877105850575845		[learning rate: 0.00028828]
		[batch 20/20] avg loss: 0.12516708866144016		[learning rate: 0.00028776]
	Learning Rate: 0.000287757
	LOSS [training: 0.11696907358359931 | validation: 0.13477444378330228]
	TIME [epoch: 8.16 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12132138446948008		[learning rate: 0.00028723]
		[batch 20/20] avg loss: 0.14144810709770234		[learning rate: 0.00028671]
	Learning Rate: 0.000286713
	LOSS [training: 0.13138474578359122 | validation: 0.12644843553901355]
	TIME [epoch: 8.16 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17648881392326168		[learning rate: 0.00028619]
		[batch 20/20] avg loss: 0.10600526894203628		[learning rate: 0.00028567]
	Learning Rate: 0.000285672
	LOSS [training: 0.14124704143264896 | validation: 0.16644932744768764]
	TIME [epoch: 8.14 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13300086012158846		[learning rate: 0.00028515]
		[batch 20/20] avg loss: 0.168363552656499		[learning rate: 0.00028464]
	Learning Rate: 0.000284636
	LOSS [training: 0.15068220638904375 | validation: 0.14830636959502597]
	TIME [epoch: 8.13 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1557299964588479		[learning rate: 0.00028412]
		[batch 20/20] avg loss: 0.12222749416799097		[learning rate: 0.0002836]
	Learning Rate: 0.000283603
	LOSS [training: 0.13897874531341944 | validation: 0.13305894706908722]
	TIME [epoch: 8.18 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14525378846963966		[learning rate: 0.00028309]
		[batch 20/20] avg loss: 0.1409499730086806		[learning rate: 0.00028257]
	Learning Rate: 0.000282574
	LOSS [training: 0.14310188073916014 | validation: 0.10184321389010331]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_1081.pth
	Model improved!!!
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13238560500536464		[learning rate: 0.00028206]
		[batch 20/20] avg loss: 0.16601604705981046		[learning rate: 0.00028155]
	Learning Rate: 0.000281548
	LOSS [training: 0.14920082603258755 | validation: 0.16397131769258394]
	TIME [epoch: 8.14 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14357931788449035		[learning rate: 0.00028104]
		[batch 20/20] avg loss: 0.13796635940070628		[learning rate: 0.00028053]
	Learning Rate: 0.000280526
	LOSS [training: 0.14077283864259832 | validation: 0.11942957039252862]
	TIME [epoch: 8.12 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14095974924135285		[learning rate: 0.00028002]
		[batch 20/20] avg loss: 0.1087391466815563		[learning rate: 0.00027951]
	Learning Rate: 0.000279508
	LOSS [training: 0.12484944796145458 | validation: 0.11244637284194321]
	TIME [epoch: 8.12 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13711942731301913		[learning rate: 0.000279]
		[batch 20/20] avg loss: 0.11559244687779864		[learning rate: 0.00027849]
	Learning Rate: 0.000278494
	LOSS [training: 0.12635593709540888 | validation: 0.09767059181278978]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_1085.pth
	Model improved!!!
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10466265433260011		[learning rate: 0.00027799]
		[batch 20/20] avg loss: 0.1410047135804604		[learning rate: 0.00027748]
	Learning Rate: 0.000277483
	LOSS [training: 0.1228336839565303 | validation: 0.1702084746990852]
	TIME [epoch: 8.16 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11623266027651954		[learning rate: 0.00027698]
		[batch 20/20] avg loss: 0.128126965380162		[learning rate: 0.00027648]
	Learning Rate: 0.000276476
	LOSS [training: 0.12217981282834076 | validation: 0.11414006926555964]
	TIME [epoch: 8.13 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09776498007941384		[learning rate: 0.00027597]
		[batch 20/20] avg loss: 0.10925778423212015		[learning rate: 0.00027547]
	Learning Rate: 0.000275473
	LOSS [training: 0.10351138215576701 | validation: 0.11971818117595726]
	TIME [epoch: 8.13 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10124902867039261		[learning rate: 0.00027497]
		[batch 20/20] avg loss: 0.13497686127652286		[learning rate: 0.00027447]
	Learning Rate: 0.000274473
	LOSS [training: 0.11811294497345777 | validation: 0.15132923696458006]
	TIME [epoch: 8.18 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13961954458770803		[learning rate: 0.00027397]
		[batch 20/20] avg loss: 0.10730986358093494		[learning rate: 0.00027348]
	Learning Rate: 0.000273477
	LOSS [training: 0.12346470408432146 | validation: 0.13804413976100596]
	TIME [epoch: 8.14 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11614570319268835		[learning rate: 0.00027298]
		[batch 20/20] avg loss: 0.12393175205512508		[learning rate: 0.00027248]
	Learning Rate: 0.000272485
	LOSS [training: 0.12003872762390669 | validation: 0.13026163352039097]
	TIME [epoch: 8.13 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13224890311620502		[learning rate: 0.00027199]
		[batch 20/20] avg loss: 0.1207162037161917		[learning rate: 0.0002715]
	Learning Rate: 0.000271496
	LOSS [training: 0.12648255341619835 | validation: 0.18644726196686048]
	TIME [epoch: 8.12 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13862048480256867		[learning rate: 0.000271]
		[batch 20/20] avg loss: 0.11500363309573688		[learning rate: 0.00027051]
	Learning Rate: 0.000270511
	LOSS [training: 0.1268120589491528 | validation: 0.1802258782458092]
	TIME [epoch: 8.12 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1143607348728762		[learning rate: 0.00027002]
		[batch 20/20] avg loss: 0.12041956318420899		[learning rate: 0.00026953]
	Learning Rate: 0.000269529
	LOSS [training: 0.11739014902854258 | validation: 0.15564033108082306]
	TIME [epoch: 8.13 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13544137143316975		[learning rate: 0.00026904]
		[batch 20/20] avg loss: 0.1154613800972812		[learning rate: 0.00026855]
	Learning Rate: 0.000268551
	LOSS [training: 0.12545137576522547 | validation: 0.12746848867719585]
	TIME [epoch: 8.14 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11212960608063098		[learning rate: 0.00026806]
		[batch 20/20] avg loss: 0.1802039951152568		[learning rate: 0.00026758]
	Learning Rate: 0.000267576
	LOSS [training: 0.14616680059794387 | validation: 0.11977864868207806]
	TIME [epoch: 8.18 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11610580384487175		[learning rate: 0.00026709]
		[batch 20/20] avg loss: 0.10783386429011144		[learning rate: 0.00026661]
	Learning Rate: 0.000266605
	LOSS [training: 0.11196983406749159 | validation: 0.11840275166985367]
	TIME [epoch: 8.12 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11101775023651098		[learning rate: 0.00026612]
		[batch 20/20] avg loss: 0.10612866488114372		[learning rate: 0.00026564]
	Learning Rate: 0.000265638
	LOSS [training: 0.10857320755882735 | validation: 0.15434825406761368]
	TIME [epoch: 8.13 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12477546079112474		[learning rate: 0.00026516]
		[batch 20/20] avg loss: 0.1280762715175523		[learning rate: 0.00026467]
	Learning Rate: 0.000264674
	LOSS [training: 0.12642586615433854 | validation: 0.15356266277106773]
	TIME [epoch: 8.15 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11706266916937966		[learning rate: 0.00026419]
		[batch 20/20] avg loss: 0.10326823665326099		[learning rate: 0.00026371]
	Learning Rate: 0.000263713
	LOSS [training: 0.11016545291132032 | validation: 0.12456061810096117]
	TIME [epoch: 8.18 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10967854696330118		[learning rate: 0.00026323]
		[batch 20/20] avg loss: 0.11392063635185863		[learning rate: 0.00026276]
	Learning Rate: 0.000262756
	LOSS [training: 0.11179959165757991 | validation: 0.1857367568170311]
	TIME [epoch: 8.12 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11686802751518773		[learning rate: 0.00026228]
		[batch 20/20] avg loss: 0.12871878769614015		[learning rate: 0.0002618]
	Learning Rate: 0.000261802
	LOSS [training: 0.12279340760566393 | validation: 0.12250938499773614]
	TIME [epoch: 8.12 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13883140047879286		[learning rate: 0.00026133]
		[batch 20/20] avg loss: 0.12177927444825161		[learning rate: 0.00026085]
	Learning Rate: 0.000260852
	LOSS [training: 0.13030533746352224 | validation: 0.13916709291949683]
	TIME [epoch: 8.14 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11026041331615044		[learning rate: 0.00026038]
		[batch 20/20] avg loss: 0.10983816990334014		[learning rate: 0.00025991]
	Learning Rate: 0.000259906
	LOSS [training: 0.11004929160974528 | validation: 0.16895078442913294]
	TIME [epoch: 8.13 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1201444481055904		[learning rate: 0.00025943]
		[batch 20/20] avg loss: 0.09372137567287175		[learning rate: 0.00025896]
	Learning Rate: 0.000258962
	LOSS [training: 0.10693291188923108 | validation: 0.13701577035618]
	TIME [epoch: 8.12 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11180911827790149		[learning rate: 0.00025849]
		[batch 20/20] avg loss: 0.11025603023065415		[learning rate: 0.00025802]
	Learning Rate: 0.000258023
	LOSS [training: 0.11103257425427786 | validation: 0.11300992893119148]
	TIME [epoch: 8.13 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10181301607239639		[learning rate: 0.00025755]
		[batch 20/20] avg loss: 0.11226087405346245		[learning rate: 0.00025709]
	Learning Rate: 0.000257086
	LOSS [training: 0.10703694506292942 | validation: 0.126614946128383]
	TIME [epoch: 8.21 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1043876987530294		[learning rate: 0.00025662]
		[batch 20/20] avg loss: 0.09903189668702839		[learning rate: 0.00025615]
	Learning Rate: 0.000256153
	LOSS [training: 0.10170979772002889 | validation: 0.12779162296338892]
	TIME [epoch: 8.15 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09572417342764775		[learning rate: 0.00025569]
		[batch 20/20] avg loss: 0.11350636420202206		[learning rate: 0.00025522]
	Learning Rate: 0.000255224
	LOSS [training: 0.10461526881483492 | validation: 0.12920405340847158]
	TIME [epoch: 8.13 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10746294760871736		[learning rate: 0.00025476]
		[batch 20/20] avg loss: 0.11907786956033395		[learning rate: 0.0002543]
	Learning Rate: 0.000254298
	LOSS [training: 0.11327040858452564 | validation: 0.11789356030290166]
	TIME [epoch: 8.15 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11946595704189325		[learning rate: 0.00025384]
		[batch 20/20] avg loss: 0.1074572547988936		[learning rate: 0.00025337]
	Learning Rate: 0.000253375
	LOSS [training: 0.11346160592039342 | validation: 0.12794896652208174]
	TIME [epoch: 8.17 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14461266371264953		[learning rate: 0.00025291]
		[batch 20/20] avg loss: 0.10227346532689255		[learning rate: 0.00025246]
	Learning Rate: 0.000252455
	LOSS [training: 0.12344306451977102 | validation: 0.1460998501428403]
	TIME [epoch: 8.15 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11567426195514227		[learning rate: 0.000252]
		[batch 20/20] avg loss: 0.13827084851217003		[learning rate: 0.00025154]
	Learning Rate: 0.000251539
	LOSS [training: 0.12697255523365616 | validation: 0.11107266129840604]
	TIME [epoch: 8.14 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10570789331407338		[learning rate: 0.00025108]
		[batch 20/20] avg loss: 0.1035508545373971		[learning rate: 0.00025063]
	Learning Rate: 0.000250626
	LOSS [training: 0.10462937392573521 | validation: 0.11337079720023208]
	TIME [epoch: 8.13 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13334954838421367		[learning rate: 0.00025017]
		[batch 20/20] avg loss: 0.09950780303944765		[learning rate: 0.00024972]
	Learning Rate: 0.000249717
	LOSS [training: 0.11642867571183065 | validation: 0.10775846700970568]
	TIME [epoch: 8.11 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12216980880608488		[learning rate: 0.00024926]
		[batch 20/20] avg loss: 0.11180197072036586		[learning rate: 0.00024881]
	Learning Rate: 0.00024881
	LOSS [training: 0.11698588976322537 | validation: 0.11350637609336212]
	TIME [epoch: 8.12 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12520424861631563		[learning rate: 0.00024836]
		[batch 20/20] avg loss: 0.12057599955944384		[learning rate: 0.00024791]
	Learning Rate: 0.000247907
	LOSS [training: 0.12289012408787976 | validation: 0.143212586283389]
	TIME [epoch: 8.15 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10682800761143016		[learning rate: 0.00024746]
		[batch 20/20] avg loss: 0.10584493738763419		[learning rate: 0.00024701]
	Learning Rate: 0.000247008
	LOSS [training: 0.10633647249953218 | validation: 0.12826029124656696]
	TIME [epoch: 8.16 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10332543705183983		[learning rate: 0.00024656]
		[batch 20/20] avg loss: 0.10069297511470246		[learning rate: 0.00024611]
	Learning Rate: 0.000246111
	LOSS [training: 0.10200920608327113 | validation: 0.11016734539314839]
	TIME [epoch: 8.14 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11448506847255402		[learning rate: 0.00024566]
		[batch 20/20] avg loss: 0.10199710909716715		[learning rate: 0.00024522]
	Learning Rate: 0.000245218
	LOSS [training: 0.10824108878486056 | validation: 0.13101015437973193]
	TIME [epoch: 8.13 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10926656673282549		[learning rate: 0.00024477]
		[batch 20/20] avg loss: 0.13074656260753675		[learning rate: 0.00024433]
	Learning Rate: 0.000244328
	LOSS [training: 0.12000656467018114 | validation: 0.1659943814758946]
	TIME [epoch: 8.14 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10000089734309907		[learning rate: 0.00024388]
		[batch 20/20] avg loss: 0.10665772012290671		[learning rate: 0.00024344]
	Learning Rate: 0.000243442
	LOSS [training: 0.1033293087330029 | validation: 0.14313485109010046]
	TIME [epoch: 8.19 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11351791588001581		[learning rate: 0.000243]
		[batch 20/20] avg loss: 0.11410576962089584		[learning rate: 0.00024256]
	Learning Rate: 0.000242558
	LOSS [training: 0.11381184275045582 | validation: 0.12031767159429294]
	TIME [epoch: 8.13 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10651977101714553		[learning rate: 0.00024212]
		[batch 20/20] avg loss: 0.10719237721422029		[learning rate: 0.00024168]
	Learning Rate: 0.000241678
	LOSS [training: 0.1068560741156829 | validation: 0.13604420086576374]
	TIME [epoch: 8.12 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09233579298550339		[learning rate: 0.00024124]
		[batch 20/20] avg loss: 0.1338193589573953		[learning rate: 0.0002408]
	Learning Rate: 0.000240801
	LOSS [training: 0.11307757597144934 | validation: 0.18482478942221517]
	TIME [epoch: 8.13 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1207764317238837		[learning rate: 0.00024036]
		[batch 20/20] avg loss: 0.10707045560250097		[learning rate: 0.00023993]
	Learning Rate: 0.000239927
	LOSS [training: 0.11392344366319233 | validation: 0.12406609490084074]
	TIME [epoch: 8.15 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10877772141129352		[learning rate: 0.00023949]
		[batch 20/20] avg loss: 0.11944599264435311		[learning rate: 0.00023906]
	Learning Rate: 0.000239056
	LOSS [training: 0.1141118570278233 | validation: 0.108910512226196]
	TIME [epoch: 8.13 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11859950366493299		[learning rate: 0.00023862]
		[batch 20/20] avg loss: 0.0966115796887186		[learning rate: 0.00023819]
	Learning Rate: 0.000238189
	LOSS [training: 0.10760554167682579 | validation: 0.12430133031998919]
	TIME [epoch: 8.13 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1077877252743733		[learning rate: 0.00023776]
		[batch 20/20] avg loss: 0.10705564196059396		[learning rate: 0.00023732]
	Learning Rate: 0.000237324
	LOSS [training: 0.10742168361748365 | validation: 0.1203168370322141]
	TIME [epoch: 8.13 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10641391836520395		[learning rate: 0.00023689]
		[batch 20/20] avg loss: 0.09275738121385187		[learning rate: 0.00023646]
	Learning Rate: 0.000236463
	LOSS [training: 0.09958564978952791 | validation: 0.1217850733911512]
	TIME [epoch: 8.15 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09994344485814598		[learning rate: 0.00023603]
		[batch 20/20] avg loss: 0.12162755701044878		[learning rate: 0.0002356]
	Learning Rate: 0.000235605
	LOSS [training: 0.11078550093429737 | validation: 0.1298487978865454]
	TIME [epoch: 8.17 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09882744693948467		[learning rate: 0.00023518]
		[batch 20/20] avg loss: 0.11911470412013714		[learning rate: 0.00023475]
	Learning Rate: 0.00023475
	LOSS [training: 0.10897107552981092 | validation: 0.16815645120561687]
	TIME [epoch: 8.13 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1079187716669379		[learning rate: 0.00023432]
		[batch 20/20] avg loss: 0.10689579963444847		[learning rate: 0.0002339]
	Learning Rate: 0.000233898
	LOSS [training: 0.10740728565069317 | validation: 0.11893627465120414]
	TIME [epoch: 8.13 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09123025777586971		[learning rate: 0.00023347]
		[batch 20/20] avg loss: 0.10528052214998222		[learning rate: 0.00023305]
	Learning Rate: 0.000233049
	LOSS [training: 0.09825538996292596 | validation: 0.10802753056712108]
	TIME [epoch: 8.15 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10718427933775179		[learning rate: 0.00023263]
		[batch 20/20] avg loss: 0.12386949312225448		[learning rate: 0.0002322]
	Learning Rate: 0.000232203
	LOSS [training: 0.11552688623000315 | validation: 0.14257490134010514]
	TIME [epoch: 8.17 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10784267668812383		[learning rate: 0.00023178]
		[batch 20/20] avg loss: 0.12334998964835646		[learning rate: 0.00023136]
	Learning Rate: 0.000231361
	LOSS [training: 0.11559633316824018 | validation: 0.16109542413750538]
	TIME [epoch: 8.12 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14638415881207825		[learning rate: 0.00023094]
		[batch 20/20] avg loss: 0.10535422096079658		[learning rate: 0.00023052]
	Learning Rate: 0.000230521
	LOSS [training: 0.12586918988643742 | validation: 0.15095726704409013]
	TIME [epoch: 8.12 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11533341895760585		[learning rate: 0.0002301]
		[batch 20/20] avg loss: 0.12430798510508023		[learning rate: 0.00022968]
	Learning Rate: 0.000229685
	LOSS [training: 0.11982070203134303 | validation: 0.13317177910232295]
	TIME [epoch: 8.12 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12541821747261706		[learning rate: 0.00022927]
		[batch 20/20] avg loss: 0.11618627669199315		[learning rate: 0.00022885]
	Learning Rate: 0.000228851
	LOSS [training: 0.12080224708230511 | validation: 0.13934631882887838]
	TIME [epoch: 8.15 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1165052909240217		[learning rate: 0.00022844]
		[batch 20/20] avg loss: 0.12939819991535403		[learning rate: 0.00022802]
	Learning Rate: 0.00022802
	LOSS [training: 0.12295174541968786 | validation: 0.16374510814125348]
	TIME [epoch: 8.13 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11281798611136144		[learning rate: 0.00022761]
		[batch 20/20] avg loss: 0.11702683401499928		[learning rate: 0.00022719]
	Learning Rate: 0.000227193
	LOSS [training: 0.11492241006318035 | validation: 0.12097136922341249]
	TIME [epoch: 8.12 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.108118609440677		[learning rate: 0.00022678]
		[batch 20/20] avg loss: 0.1139687333899579		[learning rate: 0.00022637]
	Learning Rate: 0.000226368
	LOSS [training: 0.11104367141531746 | validation: 0.13898275140969027]
	TIME [epoch: 8.14 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10376428259001447		[learning rate: 0.00022596]
		[batch 20/20] avg loss: 0.11062240963936192		[learning rate: 0.00022555]
	Learning Rate: 0.000225547
	LOSS [training: 0.10719334611468818 | validation: 0.160031106682762]
	TIME [epoch: 8.17 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10613671789607608		[learning rate: 0.00022514]
		[batch 20/20] avg loss: 0.09967678631729869		[learning rate: 0.00022473]
	Learning Rate: 0.000224728
	LOSS [training: 0.1029067521066874 | validation: 0.13111225026346643]
	TIME [epoch: 8.15 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09477348030725836		[learning rate: 0.00022432]
		[batch 20/20] avg loss: 0.10565200736220196		[learning rate: 0.00022391]
	Learning Rate: 0.000223913
	LOSS [training: 0.10021274383473015 | validation: 0.19081727359420958]
	TIME [epoch: 8.13 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11938864108982217		[learning rate: 0.00022351]
		[batch 20/20] avg loss: 0.09732786679591444		[learning rate: 0.0002231]
	Learning Rate: 0.0002231
	LOSS [training: 0.10835825394286831 | validation: 0.11320563435199915]
	TIME [epoch: 8.16 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11505496072695187		[learning rate: 0.0002227]
		[batch 20/20] avg loss: 0.13907752788302025		[learning rate: 0.00022229]
	Learning Rate: 0.000222291
	LOSS [training: 0.12706624430498606 | validation: 0.13037346689470344]
	TIME [epoch: 8.15 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.100962986892745		[learning rate: 0.00022189]
		[batch 20/20] avg loss: 0.11159462616897228		[learning rate: 0.00022148]
	Learning Rate: 0.000221484
	LOSS [training: 0.10627880653085864 | validation: 0.12593767946475878]
	TIME [epoch: 8.14 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1035738583184003		[learning rate: 0.00022108]
		[batch 20/20] avg loss: 0.11592864598101808		[learning rate: 0.00022068]
	Learning Rate: 0.00022068
	LOSS [training: 0.1097512521497092 | validation: 0.1377752249064767]
	TIME [epoch: 8.13 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12296113083113949		[learning rate: 0.00022028]
		[batch 20/20] avg loss: 0.09824550371469551		[learning rate: 0.00021988]
	Learning Rate: 0.000219879
	LOSS [training: 0.11060331727291753 | validation: 0.15255801545700576]
	TIME [epoch: 8.13 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10666648052328184		[learning rate: 0.00021948]
		[batch 20/20] avg loss: 0.09940764548767837		[learning rate: 0.00021908]
	Learning Rate: 0.000219081
	LOSS [training: 0.1030370630054801 | validation: 0.12826099716346076]
	TIME [epoch: 8.13 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10752667145868489		[learning rate: 0.00021868]
		[batch 20/20] avg loss: 0.10058763563508799		[learning rate: 0.00021829]
	Learning Rate: 0.000218286
	LOSS [training: 0.10405715354688645 | validation: 0.14044176951072063]
	TIME [epoch: 8.14 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1051691194428451		[learning rate: 0.00021789]
		[batch 20/20] avg loss: 0.1111709709531993		[learning rate: 0.00021749]
	Learning Rate: 0.000217494
	LOSS [training: 0.1081700451980222 | validation: 0.15162108987860515]
	TIME [epoch: 8.14 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10908374034748916		[learning rate: 0.0002171]
		[batch 20/20] avg loss: 0.10442432761753002		[learning rate: 0.0002167]
	Learning Rate: 0.000216705
	LOSS [training: 0.1067540339825096 | validation: 0.13509941399936642]
	TIME [epoch: 8.13 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09973585667995577		[learning rate: 0.00021631]
		[batch 20/20] avg loss: 0.11792362034623789		[learning rate: 0.00021592]
	Learning Rate: 0.000215918
	LOSS [training: 0.10882973851309682 | validation: 0.1540762045408941]
	TIME [epoch: 8.13 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11053390259045966		[learning rate: 0.00021553]
		[batch 20/20] avg loss: 0.1054501937486696		[learning rate: 0.00021513]
	Learning Rate: 0.000215135
	LOSS [training: 0.1079920481695646 | validation: 0.14177612940864362]
	TIME [epoch: 8.17 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10403033374634496		[learning rate: 0.00021474]
		[batch 20/20] avg loss: 0.13808967830671454		[learning rate: 0.00021435]
	Learning Rate: 0.000214354
	LOSS [training: 0.12106000602652978 | validation: 0.1637602289269641]
	TIME [epoch: 8.16 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10294439105456885		[learning rate: 0.00021396]
		[batch 20/20] avg loss: 0.11620875149905743		[learning rate: 0.00021358]
	Learning Rate: 0.000213576
	LOSS [training: 0.10957657127681313 | validation: 0.14086462576662698]
	TIME [epoch: 8.12 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10762251471055177		[learning rate: 0.00021319]
		[batch 20/20] avg loss: 0.1023865459221355		[learning rate: 0.0002128]
	Learning Rate: 0.000212801
	LOSS [training: 0.10500453031634363 | validation: 0.16288392403678706]
	TIME [epoch: 8.14 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13122656576814726		[learning rate: 0.00021241]
		[batch 20/20] avg loss: 0.119182506830727		[learning rate: 0.00021203]
	Learning Rate: 0.000212029
	LOSS [training: 0.12520453629943712 | validation: 0.12114756392514515]
	TIME [epoch: 8.17 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12006691343405981		[learning rate: 0.00021164]
		[batch 20/20] avg loss: 0.1020769938532791		[learning rate: 0.00021126]
	Learning Rate: 0.000211259
	LOSS [training: 0.11107195364366947 | validation: 0.1259184170225504]
	TIME [epoch: 8.15 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10550459289534113		[learning rate: 0.00021088]
		[batch 20/20] avg loss: 0.09889796903879047		[learning rate: 0.00021049]
	Learning Rate: 0.000210493
	LOSS [training: 0.10220128096706578 | validation: 0.12782035353846605]
	TIME [epoch: 8.13 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11251745062308535		[learning rate: 0.00021011]
		[batch 20/20] avg loss: 0.09466621010719609		[learning rate: 0.00020973]
	Learning Rate: 0.000209729
	LOSS [training: 0.1035918303651407 | validation: 0.12121736117698506]
	TIME [epoch: 8.12 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08738798352805907		[learning rate: 0.00020935]
		[batch 20/20] avg loss: 0.11926266492791193		[learning rate: 0.00020897]
	Learning Rate: 0.000208968
	LOSS [training: 0.1033253242279855 | validation: 0.12260017586492296]
	TIME [epoch: 8.13 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10350039549911302		[learning rate: 0.00020859]
		[batch 20/20] avg loss: 0.11562461839323368		[learning rate: 0.00020821]
	Learning Rate: 0.000208209
	LOSS [training: 0.10956250694617335 | validation: 0.13423055766405545]
	TIME [epoch: 8.13 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09722282009834651		[learning rate: 0.00020783]
		[batch 20/20] avg loss: 0.10431509055108487		[learning rate: 0.00020745]
	Learning Rate: 0.000207454
	LOSS [training: 0.1007689553247157 | validation: 0.13144283412274096]
	TIME [epoch: 8.15 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09738994603784677		[learning rate: 0.00020708]
		[batch 20/20] avg loss: 0.10053980835747167		[learning rate: 0.0002067]
	Learning Rate: 0.000206701
	LOSS [training: 0.09896487719765922 | validation: 0.12623832742222035]
	TIME [epoch: 8.12 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09993363875675709		[learning rate: 0.00020633]
		[batch 20/20] avg loss: 0.1164379704840163		[learning rate: 0.00020595]
	Learning Rate: 0.000205951
	LOSS [training: 0.10818580462038667 | validation: 0.1540395719919275]
	TIME [epoch: 8.17 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1258857440555839		[learning rate: 0.00020558]
		[batch 20/20] avg loss: 0.09944855712117681		[learning rate: 0.0002052]
	Learning Rate: 0.000205203
	LOSS [training: 0.11266715058838037 | validation: 0.11323839245690111]
	TIME [epoch: 8.13 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08760400023570517		[learning rate: 0.00020483]
		[batch 20/20] avg loss: 0.1172188632283095		[learning rate: 0.00020446]
	Learning Rate: 0.000204459
	LOSS [training: 0.10241143173200733 | validation: 0.16069133366973765]
	TIME [epoch: 8.16 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13195606517308717		[learning rate: 0.00020409]
		[batch 20/20] avg loss: 0.09647821055643972		[learning rate: 0.00020372]
	Learning Rate: 0.000203717
	LOSS [training: 0.11421713786476347 | validation: 0.13273200878716485]
	TIME [epoch: 8.14 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10599304272476014		[learning rate: 0.00020335]
		[batch 20/20] avg loss: 0.09033328432259938		[learning rate: 0.00020298]
	Learning Rate: 0.000202977
	LOSS [training: 0.09816316352367976 | validation: 0.11289007532582634]
	TIME [epoch: 8.16 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11049177283737155		[learning rate: 0.00020261]
		[batch 20/20] avg loss: 0.10609379467467839		[learning rate: 0.00020224]
	Learning Rate: 0.000202241
	LOSS [training: 0.10829278375602494 | validation: 0.12098675187474824]
	TIME [epoch: 8.13 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09038591417910316		[learning rate: 0.00020187]
		[batch 20/20] avg loss: 0.11368276286121723		[learning rate: 0.00020151]
	Learning Rate: 0.000201507
	LOSS [training: 0.10203433852016017 | validation: 0.12875627033075304]
	TIME [epoch: 8.13 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09322179883186973		[learning rate: 0.00020114]
		[batch 20/20] avg loss: 0.10132706559969498		[learning rate: 0.00020078]
	Learning Rate: 0.000200775
	LOSS [training: 0.09727443221578236 | validation: 0.12273522103310586]
	TIME [epoch: 8.15 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09994027743893405		[learning rate: 0.00020041]
		[batch 20/20] avg loss: 0.11376213994511505		[learning rate: 0.00020005]
	Learning Rate: 0.000200047
	LOSS [training: 0.10685120869202454 | validation: 0.17458854317256817]
	TIME [epoch: 8.13 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12032561415599548		[learning rate: 0.00019968]
		[batch 20/20] avg loss: 0.11405594294823895		[learning rate: 0.00019932]
	Learning Rate: 0.000199321
	LOSS [training: 0.11719077855211721 | validation: 0.13106012576674816]
	TIME [epoch: 8.13 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09701646758045854		[learning rate: 0.00019896]
		[batch 20/20] avg loss: 0.10429415550000753		[learning rate: 0.0001986]
	Learning Rate: 0.000198597
	LOSS [training: 0.10065531154023304 | validation: 0.1593793149425293]
	TIME [epoch: 8.12 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1035693996037099		[learning rate: 0.00019824]
		[batch 20/20] avg loss: 0.09757058533206142		[learning rate: 0.00019788]
	Learning Rate: 0.000197877
	LOSS [training: 0.10056999246788564 | validation: 0.11356680814576552]
	TIME [epoch: 8.18 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11121095441911277		[learning rate: 0.00019752]
		[batch 20/20] avg loss: 0.11620430194541229		[learning rate: 0.00019716]
	Learning Rate: 0.000197159
	LOSS [training: 0.11370762818226254 | validation: 0.20372802380330718]
	TIME [epoch: 8.14 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11880783503735334		[learning rate: 0.0001968]
		[batch 20/20] avg loss: 0.10648057155932467		[learning rate: 0.00019644]
	Learning Rate: 0.000196443
	LOSS [training: 0.11264420329833902 | validation: 0.13185016143010586]
	TIME [epoch: 8.13 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10011204421313684		[learning rate: 0.00019609]
		[batch 20/20] avg loss: 0.10443330717273809		[learning rate: 0.00019573]
	Learning Rate: 0.00019573
	LOSS [training: 0.10227267569293748 | validation: 0.12341025831234761]
	TIME [epoch: 8.14 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10322561610120928		[learning rate: 0.00019537]
		[batch 20/20] avg loss: 0.10905170937102085		[learning rate: 0.00019502]
	Learning Rate: 0.00019502
	LOSS [training: 0.10613866273611508 | validation: 0.14559655695774942]
	TIME [epoch: 8.19 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11657030426948575		[learning rate: 0.00019467]
		[batch 20/20] avg loss: 0.10241481264836447		[learning rate: 0.00019431]
	Learning Rate: 0.000194312
	LOSS [training: 0.10949255845892511 | validation: 0.12218424378286488]
	TIME [epoch: 8.15 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10760940531638397		[learning rate: 0.00019396]
		[batch 20/20] avg loss: 0.10029340169792325		[learning rate: 0.00019361]
	Learning Rate: 0.000193607
	LOSS [training: 0.1039514035071536 | validation: 0.12274095139473455]
	TIME [epoch: 8.13 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10561456073968471		[learning rate: 0.00019326]
		[batch 20/20] avg loss: 0.12423794016547725		[learning rate: 0.0001929]
	Learning Rate: 0.000192904
	LOSS [training: 0.11492625045258101 | validation: 0.11795863128960263]
	TIME [epoch: 8.15 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11648862466544144		[learning rate: 0.00019255]
		[batch 20/20] avg loss: 0.10316883652998594		[learning rate: 0.0001922]
	Learning Rate: 0.000192204
	LOSS [training: 0.1098287305977137 | validation: 0.11756264962347257]
	TIME [epoch: 8.12 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09443615839316666		[learning rate: 0.00019186]
		[batch 20/20] avg loss: 0.14120142056623208		[learning rate: 0.00019151]
	Learning Rate: 0.000191507
	LOSS [training: 0.11781878947969937 | validation: 0.12418380626995654]
	TIME [epoch: 8.14 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12760919453767855		[learning rate: 0.00019116]
		[batch 20/20] avg loss: 0.12471235983136877		[learning rate: 0.00019081]
	Learning Rate: 0.000190812
	LOSS [training: 0.12616077718452368 | validation: 0.13274944785485887]
	TIME [epoch: 8.12 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09677356471166047		[learning rate: 0.00019047]
		[batch 20/20] avg loss: 0.14127027453460766		[learning rate: 0.00019012]
	Learning Rate: 0.000190119
	LOSS [training: 0.11902191962313405 | validation: 0.14201519151313843]
	TIME [epoch: 8.13 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13557586163702615		[learning rate: 0.00018977]
		[batch 20/20] avg loss: 0.1308772755767657		[learning rate: 0.00018943]
	Learning Rate: 0.000189429
	LOSS [training: 0.1332265686068959 | validation: 0.13498382775353787]
	TIME [epoch: 8.18 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10444966120768495		[learning rate: 0.00018909]
		[batch 20/20] avg loss: 0.11970034081942202		[learning rate: 0.00018874]
	Learning Rate: 0.000188742
	LOSS [training: 0.1120750010135535 | validation: 0.11073197231151645]
	TIME [epoch: 8.15 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11485148694762323		[learning rate: 0.0001884]
		[batch 20/20] avg loss: 0.11051596758956891		[learning rate: 0.00018806]
	Learning Rate: 0.000188057
	LOSS [training: 0.11268372726859607 | validation: 0.11989324013986846]
	TIME [epoch: 8.15 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11058198998136755		[learning rate: 0.00018772]
		[batch 20/20] avg loss: 0.10271220926789855		[learning rate: 0.00018737]
	Learning Rate: 0.000187375
	LOSS [training: 0.10664709962463306 | validation: 0.1608625213337605]
	TIME [epoch: 8.14 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11526948031798823		[learning rate: 0.00018703]
		[batch 20/20] avg loss: 0.11392284238789847		[learning rate: 0.00018669]
	Learning Rate: 0.000186695
	LOSS [training: 0.11459616135294334 | validation: 0.11592969712325481]
	TIME [epoch: 8.18 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09449973787352786		[learning rate: 0.00018636]
		[batch 20/20] avg loss: 0.12072946250584052		[learning rate: 0.00018602]
	Learning Rate: 0.000186017
	LOSS [training: 0.10761460018968419 | validation: 0.11787870603975883]
	TIME [epoch: 8.13 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12454815155657768		[learning rate: 0.00018568]
		[batch 20/20] avg loss: 0.1005936828449703		[learning rate: 0.00018534]
	Learning Rate: 0.000185342
	LOSS [training: 0.11257091720077399 | validation: 0.14011808331344539]
	TIME [epoch: 8.15 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10556927536347234		[learning rate: 0.00018501]
		[batch 20/20] avg loss: 0.10354800882845532		[learning rate: 0.00018467]
	Learning Rate: 0.000184669
	LOSS [training: 0.10455864209596381 | validation: 0.10853483235480996]
	TIME [epoch: 8.12 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09886588159477368		[learning rate: 0.00018433]
		[batch 20/20] avg loss: 0.09117380142892093		[learning rate: 0.000184]
	Learning Rate: 0.000183999
	LOSS [training: 0.09501984151184731 | validation: 0.11843845275321299]
	TIME [epoch: 8.13 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10398388488104093		[learning rate: 0.00018367]
		[batch 20/20] avg loss: 0.11703666947004177		[learning rate: 0.00018333]
	Learning Rate: 0.000183331
	LOSS [training: 0.11051027717554136 | validation: 0.1288559756365394]
	TIME [epoch: 8.12 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09705261022735452		[learning rate: 0.000183]
		[batch 20/20] avg loss: 0.10395821229910658		[learning rate: 0.00018267]
	Learning Rate: 0.000182666
	LOSS [training: 0.10050541126323054 | validation: 0.12122204427040889]
	TIME [epoch: 8.14 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10324177816895277		[learning rate: 0.00018233]
		[batch 20/20] avg loss: 0.10055879698974905		[learning rate: 0.000182]
	Learning Rate: 0.000182003
	LOSS [training: 0.1019002875793509 | validation: 0.12257854835929045]
	TIME [epoch: 8.12 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10549118448208325		[learning rate: 0.00018167]
		[batch 20/20] avg loss: 0.09385809732503739		[learning rate: 0.00018134]
	Learning Rate: 0.000181343
	LOSS [training: 0.09967464090356029 | validation: 0.10766831728272484]
	TIME [epoch: 8.11 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10707872432181711		[learning rate: 0.00018101]
		[batch 20/20] avg loss: 0.10554550501563689		[learning rate: 0.00018068]
	Learning Rate: 0.000180685
	LOSS [training: 0.10631211466872699 | validation: 0.10754272677663973]
	TIME [epoch: 8.16 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09802852664338668		[learning rate: 0.00018036]
		[batch 20/20] avg loss: 0.1107071387942491		[learning rate: 0.00018003]
	Learning Rate: 0.000180029
	LOSS [training: 0.10436783271881789 | validation: 0.12440675708688435]
	TIME [epoch: 8.13 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10957487808765709		[learning rate: 0.0001797]
		[batch 20/20] avg loss: 0.13173078867939716		[learning rate: 0.00017938]
	Learning Rate: 0.000179376
	LOSS [training: 0.1206528333835271 | validation: 0.12959203656216545]
	TIME [epoch: 8.15 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09110351218369073		[learning rate: 0.00017905]
		[batch 20/20] avg loss: 0.13093969634431818		[learning rate: 0.00017872]
	Learning Rate: 0.000178725
	LOSS [training: 0.11102160426400445 | validation: 0.21423239277152895]
	TIME [epoch: 8.12 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1682497895984355		[learning rate: 0.0001784]
		[batch 20/20] avg loss: 0.1402878891063928		[learning rate: 0.00017808]
	Learning Rate: 0.000178076
	LOSS [training: 0.15426883935241414 | validation: 0.21351145870394667]
	TIME [epoch: 8.17 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10278110626598207		[learning rate: 0.00017775]
		[batch 20/20] avg loss: 0.13878169769332097		[learning rate: 0.00017743]
	Learning Rate: 0.00017743
	LOSS [training: 0.12078140197965152 | validation: 0.16180261877694246]
	TIME [epoch: 8.12 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13063911451824928		[learning rate: 0.00017711]
		[batch 20/20] avg loss: 0.11978403246784845		[learning rate: 0.00017679]
	Learning Rate: 0.000176786
	LOSS [training: 0.1252115734930489 | validation: 0.13025662489122358]
	TIME [epoch: 8.14 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12075791704088941		[learning rate: 0.00017646]
		[batch 20/20] avg loss: 0.0995779638023799		[learning rate: 0.00017614]
	Learning Rate: 0.000176144
	LOSS [training: 0.11016794042163462 | validation: 0.11333736155202227]
	TIME [epoch: 8.11 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12722195521663424		[learning rate: 0.00017582]
		[batch 20/20] avg loss: 0.11095514443772861		[learning rate: 0.0001755]
	Learning Rate: 0.000175505
	LOSS [training: 0.11908854982718142 | validation: 0.17669017353364044]
	TIME [epoch: 8.12 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11699167240652404		[learning rate: 0.00017519]
		[batch 20/20] avg loss: 0.11790579556022637		[learning rate: 0.00017487]
	Learning Rate: 0.000174868
	LOSS [training: 0.11744873398337523 | validation: 0.11994625604735781]
	TIME [epoch: 8.12 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10401274400234659		[learning rate: 0.00017455]
		[batch 20/20] avg loss: 0.12277192181867844		[learning rate: 0.00017423]
	Learning Rate: 0.000174233
	LOSS [training: 0.11339233291051251 | validation: 0.13233612438928202]
	TIME [epoch: 8.14 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11172087665901857		[learning rate: 0.00017392]
		[batch 20/20] avg loss: 0.10112227963881107		[learning rate: 0.0001736]
	Learning Rate: 0.000173601
	LOSS [training: 0.1064215781489148 | validation: 0.2004781522399618]
	TIME [epoch: 8.14 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21081662592530268		[learning rate: 0.00017329]
		[batch 20/20] avg loss: 0.13203390498264367		[learning rate: 0.00017297]
	Learning Rate: 0.000172971
	LOSS [training: 0.17142526545397313 | validation: 0.14668188667312743]
	TIME [epoch: 8.13 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11563990056765146		[learning rate: 0.00017266]
		[batch 20/20] avg loss: 0.10350503485590448		[learning rate: 0.00017234]
	Learning Rate: 0.000172343
	LOSS [training: 0.10957246771177798 | validation: 0.11592766899119408]
	TIME [epoch: 8.12 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10910157538410185		[learning rate: 0.00017203]
		[batch 20/20] avg loss: 0.12900949745522433		[learning rate: 0.00017172]
	Learning Rate: 0.000171718
	LOSS [training: 0.11905553641966307 | validation: 0.1802484437037415]
	TIME [epoch: 8.13 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10002444124589471		[learning rate: 0.00017141]
		[batch 20/20] avg loss: 0.1428857347886549		[learning rate: 0.00017109]
	Learning Rate: 0.000171095
	LOSS [training: 0.12145508801727481 | validation: 0.1275237243371217]
	TIME [epoch: 8.19 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13867453014789957		[learning rate: 0.00017078]
		[batch 20/20] avg loss: 0.12982756256882352		[learning rate: 0.00017047]
	Learning Rate: 0.000170474
	LOSS [training: 0.13425104635836158 | validation: 0.13407475655566464]
	TIME [epoch: 8.14 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12301762643641709		[learning rate: 0.00017016]
		[batch 20/20] avg loss: 0.12810568643169007		[learning rate: 0.00016986]
	Learning Rate: 0.000169855
	LOSS [training: 0.12556165643405356 | validation: 0.1339445119902873]
	TIME [epoch: 8.13 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11093775274650805		[learning rate: 0.00016955]
		[batch 20/20] avg loss: 0.14979945850229365		[learning rate: 0.00016924]
	Learning Rate: 0.000169239
	LOSS [training: 0.13036860562440084 | validation: 0.1325886526751194]
	TIME [epoch: 8.12 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11999816221487687		[learning rate: 0.00016893]
		[batch 20/20] avg loss: 0.12786919423964452		[learning rate: 0.00016862]
	Learning Rate: 0.000168625
	LOSS [training: 0.12393367822726069 | validation: 0.13592843520708914]
	TIME [epoch: 8.19 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11949135322710988		[learning rate: 0.00016832]
		[batch 20/20] avg loss: 0.12193896777091766		[learning rate: 0.00016801]
	Learning Rate: 0.000168013
	LOSS [training: 0.12071516049901379 | validation: 0.23249735117757309]
	TIME [epoch: 8.13 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1403839883517141		[learning rate: 0.00016771]
		[batch 20/20] avg loss: 0.12574358804023195		[learning rate: 0.0001674]
	Learning Rate: 0.000167403
	LOSS [training: 0.13306378819597303 | validation: 0.11672321954067429]
	TIME [epoch: 8.12 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11872265397469188		[learning rate: 0.0001671]
		[batch 20/20] avg loss: 0.10887282444962501		[learning rate: 0.0001668]
	Learning Rate: 0.000166795
	LOSS [training: 0.11379773921215844 | validation: 0.1020522979754056]
	TIME [epoch: 8.12 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12105013280446401		[learning rate: 0.00016649]
		[batch 20/20] avg loss: 0.11547955774135907		[learning rate: 0.00016619]
	Learning Rate: 0.00016619
	LOSS [training: 0.11826484527291156 | validation: 0.11897932791146767]
	TIME [epoch: 8.13 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11401538283520814		[learning rate: 0.00016589]
		[batch 20/20] avg loss: 0.16008608666206997		[learning rate: 0.00016559]
	Learning Rate: 0.000165587
	LOSS [training: 0.13705073474863902 | validation: 0.1595818330574308]
	TIME [epoch: 8.15 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12182956446322216		[learning rate: 0.00016529]
		[batch 20/20] avg loss: 0.11652166614852792		[learning rate: 0.00016499]
	Learning Rate: 0.000164986
	LOSS [training: 0.11917561530587502 | validation: 0.11100029869545329]
	TIME [epoch: 8.12 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11925509481549368		[learning rate: 0.00016469]
		[batch 20/20] avg loss: 0.11498226777198595		[learning rate: 0.00016439]
	Learning Rate: 0.000164387
	LOSS [training: 0.11711868129373981 | validation: 0.12977491000910277]
	TIME [epoch: 8.13 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14218151863584255		[learning rate: 0.00016409]
		[batch 20/20] avg loss: 0.10318120959618186		[learning rate: 0.00016379]
	Learning Rate: 0.000163791
	LOSS [training: 0.12268136411601219 | validation: 0.12045610414693253]
	TIME [epoch: 8.15 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10807426632187907		[learning rate: 0.00016349]
		[batch 20/20] avg loss: 0.11685568969382283		[learning rate: 0.0001632]
	Learning Rate: 0.000163196
	LOSS [training: 0.11246497800785096 | validation: 0.12065210537221253]
	TIME [epoch: 8.15 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12522652510090945		[learning rate: 0.0001629]
		[batch 20/20] avg loss: 0.10848166837809739		[learning rate: 0.0001626]
	Learning Rate: 0.000162604
	LOSS [training: 0.11685409673950342 | validation: 0.10674518725266512]
	TIME [epoch: 8.13 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10993066904777304		[learning rate: 0.00016231]
		[batch 20/20] avg loss: 0.14539527486377316		[learning rate: 0.00016201]
	Learning Rate: 0.000162014
	LOSS [training: 0.12766297195577309 | validation: 0.13191385657350632]
	TIME [epoch: 8.13 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12595376438096764		[learning rate: 0.00016172]
		[batch 20/20] avg loss: 0.10685359884111849		[learning rate: 0.00016143]
	Learning Rate: 0.000161426
	LOSS [training: 0.11640368161104306 | validation: 0.1043204383520615]
	TIME [epoch: 8.16 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11598338102552723		[learning rate: 0.00016113]
		[batch 20/20] avg loss: 0.0982503806941655		[learning rate: 0.00016084]
	Learning Rate: 0.00016084
	LOSS [training: 0.10711688085984636 | validation: 0.10735230859825304]
	TIME [epoch: 8.15 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10110457836374846		[learning rate: 0.00016055]
		[batch 20/20] avg loss: 0.13858481482895038		[learning rate: 0.00016026]
	Learning Rate: 0.000160257
	LOSS [training: 0.11984469659634941 | validation: 0.12431365006884346]
	TIME [epoch: 8.16 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09479506680343727		[learning rate: 0.00015997]
		[batch 20/20] avg loss: 0.11822937384041898		[learning rate: 0.00015967]
	Learning Rate: 0.000159675
	LOSS [training: 0.10651222032192813 | validation: 0.14846737520922648]
	TIME [epoch: 8.13 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12105635383547297		[learning rate: 0.00015938]
		[batch 20/20] avg loss: 0.1264067103192125		[learning rate: 0.0001591]
	Learning Rate: 0.000159096
	LOSS [training: 0.12373153207734273 | validation: 0.12472443936753036]
	TIME [epoch: 8.13 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10201664169916047		[learning rate: 0.00015881]
		[batch 20/20] avg loss: 0.11782611106050755		[learning rate: 0.00015852]
	Learning Rate: 0.000158518
	LOSS [training: 0.10992137637983401 | validation: 0.12396981121269147]
	TIME [epoch: 8.13 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1333692612003837		[learning rate: 0.00015823]
		[batch 20/20] avg loss: 0.09433459300701871		[learning rate: 0.00015794]
	Learning Rate: 0.000157943
	LOSS [training: 0.11385192710370122 | validation: 0.12437806397023582]
	TIME [epoch: 8.14 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10643060369052279		[learning rate: 0.00015766]
		[batch 20/20] avg loss: 0.0915911637764324		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.09901088373347758 | validation: 0.1137291703298927]
	TIME [epoch: 8.13 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09265061224256917		[learning rate: 0.00015708]
		[batch 20/20] avg loss: 0.1171225931439247		[learning rate: 0.0001568]
	Learning Rate: 0.000156799
	LOSS [training: 0.10488660269324693 | validation: 0.12136918473100643]
	TIME [epoch: 8.12 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12330745111935722		[learning rate: 0.00015651]
		[batch 20/20] avg loss: 0.10554908796570021		[learning rate: 0.00015623]
	Learning Rate: 0.00015623
	LOSS [training: 0.11442826954252872 | validation: 0.1789304625072479]
	TIME [epoch: 8.12 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10824708063385909		[learning rate: 0.00015595]
		[batch 20/20] avg loss: 0.09797970705111468		[learning rate: 0.00015566]
	Learning Rate: 0.000155663
	LOSS [training: 0.1031133938424869 | validation: 0.10163318112996296]
	TIME [epoch: 8.13 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10617107490472477		[learning rate: 0.00015538]
		[batch 20/20] avg loss: 0.10451096433666177		[learning rate: 0.0001551]
	Learning Rate: 0.000155098
	LOSS [training: 0.10534101962069327 | validation: 0.11383980991553856]
	TIME [epoch: 8.14 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10747885044922648		[learning rate: 0.00015482]
		[batch 20/20] avg loss: 0.09480375380669645		[learning rate: 0.00015453]
	Learning Rate: 0.000154535
	LOSS [training: 0.10114130212796146 | validation: 0.11671194517040642]
	TIME [epoch: 8.13 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09533707215301881		[learning rate: 0.00015425]
		[batch 20/20] avg loss: 0.1094118711834233		[learning rate: 0.00015397]
	Learning Rate: 0.000153974
	LOSS [training: 0.10237447166822104 | validation: 0.11938758346632417]
	TIME [epoch: 8.12 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09872483201694889		[learning rate: 0.00015369]
		[batch 20/20] avg loss: 0.1016214977689613		[learning rate: 0.00015342]
	Learning Rate: 0.000153415
	LOSS [training: 0.10017316489295507 | validation: 0.14405823952739855]
	TIME [epoch: 8.12 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11186238458212612		[learning rate: 0.00015314]
		[batch 20/20] avg loss: 0.0967385798395014		[learning rate: 0.00015286]
	Learning Rate: 0.000152858
	LOSS [training: 0.10430048221081378 | validation: 0.10488544620770393]
	TIME [epoch: 8.15 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1138215008637475		[learning rate: 0.00015258]
		[batch 20/20] avg loss: 0.10609464149894836		[learning rate: 0.0001523]
	Learning Rate: 0.000152304
	LOSS [training: 0.10995807118134793 | validation: 0.1307992095191536]
	TIME [epoch: 8.13 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11160342229263191		[learning rate: 0.00015203]
		[batch 20/20] avg loss: 0.09832478621117098		[learning rate: 0.00015175]
	Learning Rate: 0.000151751
	LOSS [training: 0.10496410425190146 | validation: 0.14547600824196383]
	TIME [epoch: 8.13 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11783196253944608		[learning rate: 0.00015148]
		[batch 20/20] avg loss: 0.11906839055253271		[learning rate: 0.0001512]
	Learning Rate: 0.0001512
	LOSS [training: 0.1184501765459894 | validation: 0.11087514801433228]
	TIME [epoch: 8.12 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10949284740950624		[learning rate: 0.00015093]
		[batch 20/20] avg loss: 0.10673975360737428		[learning rate: 0.00015065]
	Learning Rate: 0.000150652
	LOSS [training: 0.10811630050844026 | validation: 0.101856351821399]
	TIME [epoch: 8.12 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0836714772622839		[learning rate: 0.00015038]
		[batch 20/20] avg loss: 0.11894829324496056		[learning rate: 0.0001501]
	Learning Rate: 0.000150105
	LOSS [training: 0.10130988525362224 | validation: 0.10848028909738133]
	TIME [epoch: 8.14 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10617208652369237		[learning rate: 0.00014983]
		[batch 20/20] avg loss: 0.10526152523325397		[learning rate: 0.00014956]
	Learning Rate: 0.00014956
	LOSS [training: 0.10571680587847317 | validation: 0.1277753136168291]
	TIME [epoch: 8.13 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11924779365486546		[learning rate: 0.00014929]
		[batch 20/20] avg loss: 0.13612828728286477		[learning rate: 0.00014902]
	Learning Rate: 0.000149017
	LOSS [training: 0.12768804046886514 | validation: 0.13277262282241528]
	TIME [epoch: 8.12 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10985871259882955		[learning rate: 0.00014875]
		[batch 20/20] avg loss: 0.09693005920124832		[learning rate: 0.00014848]
	Learning Rate: 0.000148477
	LOSS [training: 0.10339438590003894 | validation: 0.102565845592693]
	TIME [epoch: 8.13 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09556144700369798		[learning rate: 0.00014821]
		[batch 20/20] avg loss: 0.1279125750494287		[learning rate: 0.00014794]
	Learning Rate: 0.000147938
	LOSS [training: 0.11173701102656333 | validation: 0.10259393008046383]
	TIME [epoch: 8.13 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12195987658099468		[learning rate: 0.00014767]
		[batch 20/20] avg loss: 0.11388954794550032		[learning rate: 0.0001474]
	Learning Rate: 0.000147401
	LOSS [training: 0.11792471226324748 | validation: 0.11573459205679436]
	TIME [epoch: 8.13 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11057043480212052		[learning rate: 0.00014713]
		[batch 20/20] avg loss: 0.10796035312812957		[learning rate: 0.00014687]
	Learning Rate: 0.000146866
	LOSS [training: 0.10926539396512507 | validation: 0.1190516898693799]
	TIME [epoch: 8.12 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12434211938864151		[learning rate: 0.0001466]
		[batch 20/20] avg loss: 0.1170291166363479		[learning rate: 0.00014633]
	Learning Rate: 0.000146333
	LOSS [training: 0.1206856180124947 | validation: 0.10350907791493529]
	TIME [epoch: 8.12 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11159904385529382		[learning rate: 0.00014607]
		[batch 20/20] avg loss: 0.10492105182162739		[learning rate: 0.0001458]
	Learning Rate: 0.000145802
	LOSS [training: 0.10826004783846059 | validation: 0.1134955758088421]
	TIME [epoch: 8.14 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10795792980950716		[learning rate: 0.00014554]
		[batch 20/20] avg loss: 0.11289771902863419		[learning rate: 0.00014527]
	Learning Rate: 0.000145273
	LOSS [training: 0.11042782441907069 | validation: 0.10128435717360978]
	TIME [epoch: 8.14 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09388730311837232		[learning rate: 0.00014501]
		[batch 20/20] avg loss: 0.10752506409756915		[learning rate: 0.00014475]
	Learning Rate: 0.000144746
	LOSS [training: 0.10070618360797072 | validation: 0.10842984039281559]
	TIME [epoch: 8.12 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09475577028961034		[learning rate: 0.00014448]
		[batch 20/20] avg loss: 0.12056950478310631		[learning rate: 0.00014422]
	Learning Rate: 0.00014422
	LOSS [training: 0.10766263753635837 | validation: 0.1053426622439567]
	TIME [epoch: 8.13 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1004394347610437		[learning rate: 0.00014396]
		[batch 20/20] avg loss: 0.11395781635224714		[learning rate: 0.0001437]
	Learning Rate: 0.000143697
	LOSS [training: 0.10719862555664542 | validation: 0.10706366122656719]
	TIME [epoch: 8.12 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11486006805547119		[learning rate: 0.00014344]
		[batch 20/20] avg loss: 0.10526861032103399		[learning rate: 0.00014318]
	Learning Rate: 0.000143175
	LOSS [training: 0.1100643391882526 | validation: 0.11775238915448769]
	TIME [epoch: 8.14 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12120432423159433		[learning rate: 0.00014292]
		[batch 20/20] avg loss: 0.098670344989028		[learning rate: 0.00014266]
	Learning Rate: 0.000142656
	LOSS [training: 0.10993733461031116 | validation: 0.10767355022427368]
	TIME [epoch: 8.13 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10765268550788085		[learning rate: 0.0001424]
		[batch 20/20] avg loss: 0.10458384569971972		[learning rate: 0.00014214]
	Learning Rate: 0.000142138
	LOSS [training: 0.10611826560380029 | validation: 0.11212229854991682]
	TIME [epoch: 8.12 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11105602375390075		[learning rate: 0.00014188]
		[batch 20/20] avg loss: 0.10095081275516686		[learning rate: 0.00014162]
	Learning Rate: 0.000141622
	LOSS [training: 0.10600341825453381 | validation: 0.12261228490750604]
	TIME [epoch: 8.12 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10638966810792108		[learning rate: 0.00014137]
		[batch 20/20] avg loss: 0.1257163806497492		[learning rate: 0.00014111]
	Learning Rate: 0.000141108
	LOSS [training: 0.11605302437883513 | validation: 0.14956034597288434]
	TIME [epoch: 8.13 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12826537890606207		[learning rate: 0.00014085]
		[batch 20/20] avg loss: 0.10145284858834162		[learning rate: 0.0001406]
	Learning Rate: 0.000140596
	LOSS [training: 0.11485911374720185 | validation: 0.11267463689659776]
	TIME [epoch: 8.12 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1050354160016405		[learning rate: 0.00014034]
		[batch 20/20] avg loss: 0.07903920375176374		[learning rate: 0.00014009]
	Learning Rate: 0.000140086
	LOSS [training: 0.09203730987670213 | validation: 0.10405278953171274]
	TIME [epoch: 8.11 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10272627836507811		[learning rate: 0.00013983]
		[batch 20/20] avg loss: 0.13834864821647203		[learning rate: 0.00013958]
	Learning Rate: 0.000139578
	LOSS [training: 0.12053746329077508 | validation: 0.09463187497821088]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_1275.pth
	Model improved!!!
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10099240322356248		[learning rate: 0.00013932]
		[batch 20/20] avg loss: 0.10338980588073202		[learning rate: 0.00013907]
	Learning Rate: 0.000139071
	LOSS [training: 0.10219110455214726 | validation: 0.12110060030574163]
	TIME [epoch: 8.17 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09410376428192771		[learning rate: 0.00013882]
		[batch 20/20] avg loss: 0.12148167572966007		[learning rate: 0.00013857]
	Learning Rate: 0.000138566
	LOSS [training: 0.10779272000579387 | validation: 0.13216306823798496]
	TIME [epoch: 8.14 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09338157138283416		[learning rate: 0.00013831]
		[batch 20/20] avg loss: 0.09966044662061405		[learning rate: 0.00013806]
	Learning Rate: 0.000138064
	LOSS [training: 0.09652100900172408 | validation: 0.11193519584795097]
	TIME [epoch: 8.12 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0979027846285466		[learning rate: 0.00013781]
		[batch 20/20] avg loss: 0.10735178896069053		[learning rate: 0.00013756]
	Learning Rate: 0.000137562
	LOSS [training: 0.10262728679461856 | validation: 0.11593467168078786]
	TIME [epoch: 8.12 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09508346149488257		[learning rate: 0.00013731]
		[batch 20/20] avg loss: 0.09132349644935536		[learning rate: 0.00013706]
	Learning Rate: 0.000137063
	LOSS [training: 0.09320347897211897 | validation: 0.10069710816673216]
	TIME [epoch: 8.18 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09671870430141163		[learning rate: 0.00013681]
		[batch 20/20] avg loss: 0.08903269041088052		[learning rate: 0.00013657]
	Learning Rate: 0.000136566
	LOSS [training: 0.09287569735614606 | validation: 0.10833006349338656]
	TIME [epoch: 8.13 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09845406060690101		[learning rate: 0.00013632]
		[batch 20/20] avg loss: 0.09933003977276268		[learning rate: 0.00013607]
	Learning Rate: 0.00013607
	LOSS [training: 0.09889205018983187 | validation: 0.18667585222500044]
	TIME [epoch: 8.12 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14036612791165692		[learning rate: 0.00013582]
		[batch 20/20] avg loss: 0.11897062488598616		[learning rate: 0.00013558]
	Learning Rate: 0.000135576
	LOSS [training: 0.12966837639882153 | validation: 0.1348307831441007]
	TIME [epoch: 8.11 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09241612538064906		[learning rate: 0.00013533]
		[batch 20/20] avg loss: 0.1162958849798312		[learning rate: 0.00013508]
	Learning Rate: 0.000135084
	LOSS [training: 0.10435600518024013 | validation: 0.12822809617209735]
	TIME [epoch: 8.11 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10251586125039851		[learning rate: 0.00013484]
		[batch 20/20] avg loss: 0.11255339357650362		[learning rate: 0.00013459]
	Learning Rate: 0.000134594
	LOSS [training: 0.10753462741345107 | validation: 0.13350517787400218]
	TIME [epoch: 8.11 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11792634657047349		[learning rate: 0.00013435]
		[batch 20/20] avg loss: 0.09108407089056628		[learning rate: 0.00013411]
	Learning Rate: 0.000134106
	LOSS [training: 0.1045052087305199 | validation: 0.12473378275531073]
	TIME [epoch: 8.14 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09635686246792431		[learning rate: 0.00013386]
		[batch 20/20] avg loss: 0.13500470816576615		[learning rate: 0.00013362]
	Learning Rate: 0.000133619
	LOSS [training: 0.11568078531684522 | validation: 0.12581244153478527]
	TIME [epoch: 8.13 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08913868734245962		[learning rate: 0.00013338]
		[batch 20/20] avg loss: 0.1209635631601936		[learning rate: 0.00013313]
	Learning Rate: 0.000133134
	LOSS [training: 0.10505112525132661 | validation: 0.13928103217906554]
	TIME [epoch: 8.16 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09757258040871693		[learning rate: 0.00013289]
		[batch 20/20] avg loss: 0.11496760444588558		[learning rate: 0.00013265]
	Learning Rate: 0.000132651
	LOSS [training: 0.10627009242730126 | validation: 0.11564184756181621]
	TIME [epoch: 8.13 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11920294689447027		[learning rate: 0.00013241]
		[batch 20/20] avg loss: 0.10592460340657787		[learning rate: 0.00013217]
	Learning Rate: 0.00013217
	LOSS [training: 0.11256377515052407 | validation: 0.10820721405787924]
	TIME [epoch: 8.15 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11761624076765889		[learning rate: 0.00013193]
		[batch 20/20] avg loss: 0.11448056813596438		[learning rate: 0.00013169]
	Learning Rate: 0.00013169
	LOSS [training: 0.11604840445181164 | validation: 0.12305102525795324]
	TIME [epoch: 8.15 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09567529702011139		[learning rate: 0.00013145]
		[batch 20/20] avg loss: 0.13902195938515585		[learning rate: 0.00013121]
	Learning Rate: 0.000131212
	LOSS [training: 0.11734862820263363 | validation: 0.16034943809023]
	TIME [epoch: 8.15 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1179484494207573		[learning rate: 0.00013097]
		[batch 20/20] avg loss: 0.10390074421484956		[learning rate: 0.00013074]
	Learning Rate: 0.000130736
	LOSS [training: 0.11092459681780342 | validation: 0.12722681063519622]
	TIME [epoch: 8.11 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1035073079115737		[learning rate: 0.0001305]
		[batch 20/20] avg loss: 0.10713575287395842		[learning rate: 0.00013026]
	Learning Rate: 0.000130261
	LOSS [training: 0.10532153039276607 | validation: 0.11351588678389338]
	TIME [epoch: 8.12 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10480663938677344		[learning rate: 0.00013002]
		[batch 20/20] avg loss: 0.0932140056002241		[learning rate: 0.00012979]
	Learning Rate: 0.000129789
	LOSS [training: 0.09901032249349875 | validation: 0.109878808923633]
	TIME [epoch: 8.14 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09558572524428617		[learning rate: 0.00012955]
		[batch 20/20] avg loss: 0.12773984368035995		[learning rate: 0.00012932]
	Learning Rate: 0.000129318
	LOSS [training: 0.11166278446232306 | validation: 0.10902531585926342]
	TIME [epoch: 8.12 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11691567621064736		[learning rate: 0.00012908]
		[batch 20/20] avg loss: 0.11439362243477394		[learning rate: 0.00012885]
	Learning Rate: 0.000128848
	LOSS [training: 0.11565464932271063 | validation: 0.1348185613822753]
	TIME [epoch: 8.11 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11682352524481485		[learning rate: 0.00012861]
		[batch 20/20] avg loss: 0.09336924989835853		[learning rate: 0.00012838]
	Learning Rate: 0.000128381
	LOSS [training: 0.10509638757158671 | validation: 0.11964080973990047]
	TIME [epoch: 8.12 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10952411674318038		[learning rate: 0.00012815]
		[batch 20/20] avg loss: 0.09562819913593439		[learning rate: 0.00012791]
	Learning Rate: 0.000127915
	LOSS [training: 0.10257615793955739 | validation: 0.11829830423224576]
	TIME [epoch: 8.19 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11209320775515678		[learning rate: 0.00012768]
		[batch 20/20] avg loss: 0.08840396367585461		[learning rate: 0.00012745]
	Learning Rate: 0.000127451
	LOSS [training: 0.10024858571550573 | validation: 0.10867872842918058]
	TIME [epoch: 8.13 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10075247781274924		[learning rate: 0.00012722]
		[batch 20/20] avg loss: 0.10987883261012601		[learning rate: 0.00012699]
	Learning Rate: 0.000126988
	LOSS [training: 0.10531565521143763 | validation: 0.10888656112922623]
	TIME [epoch: 8.13 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08937630946956417		[learning rate: 0.00012676]
		[batch 20/20] avg loss: 0.09845733321464553		[learning rate: 0.00012653]
	Learning Rate: 0.000126527
	LOSS [training: 0.09391682134210486 | validation: 0.10933503293269384]
	TIME [epoch: 8.12 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10518887427525717		[learning rate: 0.0001263]
		[batch 20/20] avg loss: 0.1210052372424671		[learning rate: 0.00012607]
	Learning Rate: 0.000126068
	LOSS [training: 0.11309705575886213 | validation: 0.13565556209739446]
	TIME [epoch: 8.19 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10247071459373494		[learning rate: 0.00012584]
		[batch 20/20] avg loss: 0.09111263140886515		[learning rate: 0.00012561]
	Learning Rate: 0.000125611
	LOSS [training: 0.09679167300130005 | validation: 0.13296410462711555]
	TIME [epoch: 8.14 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09414373449669727		[learning rate: 0.00012538]
		[batch 20/20] avg loss: 0.10932074757367094		[learning rate: 0.00012515]
	Learning Rate: 0.000125155
	LOSS [training: 0.1017322410351841 | validation: 0.1028335640121458]
	TIME [epoch: 8.12 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09525257086822034		[learning rate: 0.00012493]
		[batch 20/20] avg loss: 0.0955484199302318		[learning rate: 0.0001247]
	Learning Rate: 0.000124701
	LOSS [training: 0.09540049539922606 | validation: 0.09957722627964541]
	TIME [epoch: 8.12 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09180639896237128		[learning rate: 0.00012447]
		[batch 20/20] avg loss: 0.10519382153313497		[learning rate: 0.00012425]
	Learning Rate: 0.000124248
	LOSS [training: 0.09850011024775311 | validation: 0.11206461510388509]
	TIME [epoch: 8.12 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10077982652100356		[learning rate: 0.00012402]
		[batch 20/20] avg loss: 0.09340162860157575		[learning rate: 0.0001238]
	Learning Rate: 0.000123797
	LOSS [training: 0.09709072756128964 | validation: 0.13249559449980083]
	TIME [epoch: 8.14 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10737598521081068		[learning rate: 0.00012357]
		[batch 20/20] avg loss: 0.0931141726604006		[learning rate: 0.00012335]
	Learning Rate: 0.000123348
	LOSS [training: 0.10024507893560568 | validation: 0.11587729190187536]
	TIME [epoch: 8.14 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10997141569054517		[learning rate: 0.00012312]
		[batch 20/20] avg loss: 0.09556851382610014		[learning rate: 0.0001229]
	Learning Rate: 0.0001229
	LOSS [training: 0.10276996475832265 | validation: 0.10962376382573735]
	TIME [epoch: 8.13 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1015881976119329		[learning rate: 0.00012268]
		[batch 20/20] avg loss: 0.10769128351891125		[learning rate: 0.00012245]
	Learning Rate: 0.000122454
	LOSS [training: 0.10463974056542207 | validation: 0.11131859281710293]
	TIME [epoch: 8.12 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10408288674471847		[learning rate: 0.00012223]
		[batch 20/20] avg loss: 0.1045495018584478		[learning rate: 0.00012201]
	Learning Rate: 0.00012201
	LOSS [training: 0.10431619430158315 | validation: 0.10827382523138118]
	TIME [epoch: 8.14 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09802992985431688		[learning rate: 0.00012179]
		[batch 20/20] avg loss: 0.09515352185827404		[learning rate: 0.00012157]
	Learning Rate: 0.000121567
	LOSS [training: 0.09659172585629545 | validation: 0.10336709363177665]
	TIME [epoch: 8.17 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08589683651083171		[learning rate: 0.00012135]
		[batch 20/20] avg loss: 0.09497817524400123		[learning rate: 0.00012113]
	Learning Rate: 0.000121126
	LOSS [training: 0.09043750587741647 | validation: 0.11151823656062294]
	TIME [epoch: 8.12 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1111809112790599		[learning rate: 0.00012091]
		[batch 20/20] avg loss: 0.08731165524888482		[learning rate: 0.00012069]
	Learning Rate: 0.000120686
	LOSS [training: 0.09924628326397235 | validation: 0.11458538268711134]
	TIME [epoch: 8.12 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10041108767248878		[learning rate: 0.00012047]
		[batch 20/20] avg loss: 0.10474094103077172		[learning rate: 0.00012025]
	Learning Rate: 0.000120248
	LOSS [training: 0.10257601435163026 | validation: 0.12130383827069402]
	TIME [epoch: 8.12 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10847225877862213		[learning rate: 0.00012003]
		[batch 20/20] avg loss: 0.11131491386538746		[learning rate: 0.00011981]
	Learning Rate: 0.000119812
	LOSS [training: 0.10989358632200481 | validation: 0.10930338101288377]
	TIME [epoch: 8.15 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0892686380517459		[learning rate: 0.00011959]
		[batch 20/20] avg loss: 0.09396853239648542		[learning rate: 0.00011938]
	Learning Rate: 0.000119377
	LOSS [training: 0.09161858522411567 | validation: 0.11992002745390906]
	TIME [epoch: 8.11 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1063723626904787		[learning rate: 0.00011916]
		[batch 20/20] avg loss: 0.10701127352351288		[learning rate: 0.00011894]
	Learning Rate: 0.000118944
	LOSS [training: 0.1066918181069958 | validation: 0.11191165834369393]
	TIME [epoch: 8.12 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12306519748220675		[learning rate: 0.00011873]
		[batch 20/20] avg loss: 0.09734538291925166		[learning rate: 0.00011851]
	Learning Rate: 0.000118512
	LOSS [training: 0.1102052902007292 | validation: 0.11134127474937724]
	TIME [epoch: 8.12 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10281397098361708		[learning rate: 0.0001183]
		[batch 20/20] avg loss: 0.10647082203502363		[learning rate: 0.00011808]
	Learning Rate: 0.000118082
	LOSS [training: 0.10464239650932035 | validation: 0.10292316616851885]
	TIME [epoch: 8.13 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09313110867810415		[learning rate: 0.00011787]
		[batch 20/20] avg loss: 0.10150020504674866		[learning rate: 0.00011765]
	Learning Rate: 0.000117654
	LOSS [training: 0.09731565686242641 | validation: 0.11409106984897051]
	TIME [epoch: 8.15 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08919937983889613		[learning rate: 0.00011744]
		[batch 20/20] avg loss: 0.10629678612861986		[learning rate: 0.00011723]
	Learning Rate: 0.000117227
	LOSS [training: 0.097748082983758 | validation: 0.1231579223911822]
	TIME [epoch: 8.14 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08934429794742296		[learning rate: 0.00011701]
		[batch 20/20] avg loss: 0.10076352780843172		[learning rate: 0.0001168]
	Learning Rate: 0.000116801
	LOSS [training: 0.09505391287792735 | validation: 0.12274095176423883]
	TIME [epoch: 8.13 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11549165042068724		[learning rate: 0.00011659]
		[batch 20/20] avg loss: 0.10564669583158177		[learning rate: 0.00011638]
	Learning Rate: 0.000116377
	LOSS [training: 0.11056917312613448 | validation: 0.13489845298549188]
	TIME [epoch: 8.13 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10119662214508196		[learning rate: 0.00011617]
		[batch 20/20] avg loss: 0.09723375990868174		[learning rate: 0.00011595]
	Learning Rate: 0.000115955
	LOSS [training: 0.09921519102688184 | validation: 0.13942974124232121]
	TIME [epoch: 8.19 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10506414776557968		[learning rate: 0.00011574]
		[batch 20/20] avg loss: 0.09627951852171493		[learning rate: 0.00011553]
	Learning Rate: 0.000115534
	LOSS [training: 0.10067183314364729 | validation: 0.11527168404561469]
	TIME [epoch: 8.14 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10013479999069883		[learning rate: 0.00011532]
		[batch 20/20] avg loss: 0.1069224659530457		[learning rate: 0.00011511]
	Learning Rate: 0.000115115
	LOSS [training: 0.10352863297187227 | validation: 0.1503233220770349]
	TIME [epoch: 8.1 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10300108238548969		[learning rate: 0.00011491]
		[batch 20/20] avg loss: 0.10870173198184771		[learning rate: 0.0001147]
	Learning Rate: 0.000114697
	LOSS [training: 0.1058514071836687 | validation: 0.09923732715540293]
	TIME [epoch: 8.11 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0889733729021123		[learning rate: 0.00011449]
		[batch 20/20] avg loss: 0.10015552664326718		[learning rate: 0.00011428]
	Learning Rate: 0.000114281
	LOSS [training: 0.09456444977268973 | validation: 0.1217528549329197]
	TIME [epoch: 8.14 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08037482089703654		[learning rate: 0.00011407]
		[batch 20/20] avg loss: 0.10479957499641998		[learning rate: 0.00011387]
	Learning Rate: 0.000113866
	LOSS [training: 0.09258719794672826 | validation: 0.11680607325671029]
	TIME [epoch: 8.13 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09748647753787783		[learning rate: 0.00011366]
		[batch 20/20] avg loss: 0.0995676417443657		[learning rate: 0.00011345]
	Learning Rate: 0.000113453
	LOSS [training: 0.09852705964112177 | validation: 0.11490294055527214]
	TIME [epoch: 8.13 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09096380919420896		[learning rate: 0.00011325]
		[batch 20/20] avg loss: 0.10776070266124371		[learning rate: 0.00011304]
	Learning Rate: 0.000113041
	LOSS [training: 0.09936225592772638 | validation: 0.12239231150229216]
	TIME [epoch: 8.12 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10234135372830011		[learning rate: 0.00011284]
		[batch 20/20] avg loss: 0.1015315446697217		[learning rate: 0.00011263]
	Learning Rate: 0.000112631
	LOSS [training: 0.1019364491990109 | validation: 0.11314117975495919]
	TIME [epoch: 8.14 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10706195825461987		[learning rate: 0.00011243]
		[batch 20/20] avg loss: 0.09434344272422791		[learning rate: 0.00011222]
	Learning Rate: 0.000112222
	LOSS [training: 0.10070270048942387 | validation: 0.10883034095883765]
	TIME [epoch: 8.19 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10249980716684841		[learning rate: 0.00011202]
		[batch 20/20] avg loss: 0.10251776686305265		[learning rate: 0.00011181]
	Learning Rate: 0.000111815
	LOSS [training: 0.10250878701495054 | validation: 0.11964377214659175]
	TIME [epoch: 8.13 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1116264721110493		[learning rate: 0.00011161]
		[batch 20/20] avg loss: 0.08785385578536412		[learning rate: 0.00011141]
	Learning Rate: 0.000111409
	LOSS [training: 0.0997401639482067 | validation: 0.11575700308692585]
	TIME [epoch: 8.13 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10203047883716714		[learning rate: 0.00011121]
		[batch 20/20] avg loss: 0.09972586694470678		[learning rate: 0.000111]
	Learning Rate: 0.000111005
	LOSS [training: 0.10087817289093697 | validation: 0.1093085274830313]
	TIME [epoch: 8.15 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09595078582124687		[learning rate: 0.0001108]
		[batch 20/20] avg loss: 0.1257932032823071		[learning rate: 0.0001106]
	Learning Rate: 0.000110602
	LOSS [training: 0.11087199455177701 | validation: 0.14410974728443915]
	TIME [epoch: 8.18 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10303365603589967		[learning rate: 0.0001104]
		[batch 20/20] avg loss: 0.11180020709211227		[learning rate: 0.0001102]
	Learning Rate: 0.000110201
	LOSS [training: 0.10741693156400597 | validation: 0.1188592160757053]
	TIME [epoch: 8.13 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10118270388954764		[learning rate: 0.00011]
		[batch 20/20] avg loss: 0.10546190255706908		[learning rate: 0.0001098]
	Learning Rate: 0.000109801
	LOSS [training: 0.10332230322330835 | validation: 0.11057834704079772]
	TIME [epoch: 8.12 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09617893458462454		[learning rate: 0.0001096]
		[batch 20/20] avg loss: 0.09752192054062239		[learning rate: 0.0001094]
	Learning Rate: 0.000109402
	LOSS [training: 0.09685042756262345 | validation: 0.10677212879466515]
	TIME [epoch: 8.13 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09924612336808485		[learning rate: 0.0001092]
		[batch 20/20] avg loss: 0.08795324292642744		[learning rate: 0.00010901]
	Learning Rate: 0.000109005
	LOSS [training: 0.09359968314725614 | validation: 0.10993176354035832]
	TIME [epoch: 8.13 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08178409017338241		[learning rate: 0.00010881]
		[batch 20/20] avg loss: 0.10677265195704178		[learning rate: 0.00010861]
	Learning Rate: 0.00010861
	LOSS [training: 0.09427837106521209 | validation: 0.12916416886489993]
	TIME [epoch: 8.18 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09355259692733159		[learning rate: 0.00010841]
		[batch 20/20] avg loss: 0.1144435940473526		[learning rate: 0.00010822]
	Learning Rate: 0.000108215
	LOSS [training: 0.10399809548734207 | validation: 0.10472895190772769]
	TIME [epoch: 8.16 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10607654940846434		[learning rate: 0.00010802]
		[batch 20/20] avg loss: 0.10454970211168128		[learning rate: 0.00010782]
	Learning Rate: 0.000107823
	LOSS [training: 0.10531312576007282 | validation: 0.1270786784457935]
	TIME [epoch: 8.13 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11462305997588407		[learning rate: 0.00010763]
		[batch 20/20] avg loss: 0.10130106732155604		[learning rate: 0.00010743]
	Learning Rate: 0.000107432
	LOSS [training: 0.10796206364872003 | validation: 0.12666361522729974]
	TIME [epoch: 8.12 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0972861103472423		[learning rate: 0.00010724]
		[batch 20/20] avg loss: 0.12335720048163057		[learning rate: 0.00010704]
	Learning Rate: 0.000107042
	LOSS [training: 0.11032165541443639 | validation: 0.09772277274870649]
	TIME [epoch: 8.17 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10584565237738716		[learning rate: 0.00010685]
		[batch 20/20] avg loss: 0.11344431643798134		[learning rate: 0.00010665]
	Learning Rate: 0.000106653
	LOSS [training: 0.10964498440768426 | validation: 0.12594793591502154]
	TIME [epoch: 8.16 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09785046701265981		[learning rate: 0.00010646]
		[batch 20/20] avg loss: 0.12347061789836751		[learning rate: 0.00010627]
	Learning Rate: 0.000106266
	LOSS [training: 0.11066054245551368 | validation: 0.10140708867041938]
	TIME [epoch: 8.12 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10637907781969576		[learning rate: 0.00010607]
		[batch 20/20] avg loss: 0.08899573387129597		[learning rate: 0.00010588]
	Learning Rate: 0.00010588
	LOSS [training: 0.09768740584549587 | validation: 0.10086161718356783]
	TIME [epoch: 8.12 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11551933339755456		[learning rate: 0.00010569]
		[batch 20/20] avg loss: 0.09587289741039726		[learning rate: 0.0001055]
	Learning Rate: 0.000105496
	LOSS [training: 0.10569611540397592 | validation: 0.09497778050747224]
	TIME [epoch: 8.13 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10201550297814661		[learning rate: 0.0001053]
		[batch 20/20] avg loss: 0.0880291810795641		[learning rate: 0.00010511]
	Learning Rate: 0.000105113
	LOSS [training: 0.09502234202885536 | validation: 0.12877947029390308]
	TIME [epoch: 8.14 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10346911896496776		[learning rate: 0.00010492]
		[batch 20/20] avg loss: 0.10003104078284109		[learning rate: 0.00010473]
	Learning Rate: 0.000104732
	LOSS [training: 0.10175007987390443 | validation: 0.10887029389202268]
	TIME [epoch: 8.12 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09010424284768871		[learning rate: 0.00010454]
		[batch 20/20] avg loss: 0.1036094035379719		[learning rate: 0.00010435]
	Learning Rate: 0.000104352
	LOSS [training: 0.09685682319283032 | validation: 0.11491218024162533]
	TIME [epoch: 8.12 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10618437069734188		[learning rate: 0.00010416]
		[batch 20/20] avg loss: 0.10745490125463227		[learning rate: 0.00010397]
	Learning Rate: 0.000103973
	LOSS [training: 0.10681963597598707 | validation: 0.10944733611772223]
	TIME [epoch: 8.12 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10496913353156423		[learning rate: 0.00010378]
		[batch 20/20] avg loss: 0.09034212096882507		[learning rate: 0.0001036]
	Learning Rate: 0.000103596
	LOSS [training: 0.09765562725019465 | validation: 0.10208407577671344]
	TIME [epoch: 8.2 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09809861999462491		[learning rate: 0.00010341]
		[batch 20/20] avg loss: 0.09909555789286428		[learning rate: 0.00010322]
	Learning Rate: 0.00010322
	LOSS [training: 0.09859708894374461 | validation: 0.10872084088147782]
	TIME [epoch: 8.13 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10477892089242939		[learning rate: 0.00010303]
		[batch 20/20] avg loss: 0.09882137182467575		[learning rate: 0.00010285]
	Learning Rate: 0.000102845
	LOSS [training: 0.10180014635855257 | validation: 0.10130929321823984]
	TIME [epoch: 8.13 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11507337285293884		[learning rate: 0.00010266]
		[batch 20/20] avg loss: 0.09713240899520863		[learning rate: 0.00010247]
	Learning Rate: 0.000102472
	LOSS [training: 0.10610289092407373 | validation: 0.10513629364251363]
	TIME [epoch: 8.13 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10475264982388566		[learning rate: 0.00010229]
		[batch 20/20] avg loss: 0.11560531597295572		[learning rate: 0.0001021]
	Learning Rate: 0.0001021
	LOSS [training: 0.1101789828984207 | validation: 0.10846062607414059]
	TIME [epoch: 8.19 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12320163427913071		[learning rate: 0.00010191]
		[batch 20/20] avg loss: 0.10119122831339933		[learning rate: 0.00010173]
	Learning Rate: 0.00010173
	LOSS [training: 0.11219643129626503 | validation: 0.09534550137544948]
	TIME [epoch: 8.14 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10300711046060744		[learning rate: 0.00010154]
		[batch 20/20] avg loss: 0.11374037857966879		[learning rate: 0.00010136]
	Learning Rate: 0.00010136
	LOSS [training: 0.10837374452013812 | validation: 0.12253421609721848]
	TIME [epoch: 8.13 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1180592558535321		[learning rate: 0.00010118]
		[batch 20/20] avg loss: 0.11518343548051527		[learning rate: 0.00010099]
	Learning Rate: 0.000100993
	LOSS [training: 0.11662134566702369 | validation: 0.1002490412697484]
	TIME [epoch: 8.12 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0981551456985822		[learning rate: 0.00010081]
		[batch 20/20] avg loss: 0.11434623065780725		[learning rate: 0.00010063]
	Learning Rate: 0.000100626
	LOSS [training: 0.10625068817819472 | validation: 0.10280460619884414]
	TIME [epoch: 8.13 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08866991803057486		[learning rate: 0.00010044]
		[batch 20/20] avg loss: 0.12369424948803223		[learning rate: 0.00010026]
	Learning Rate: 0.000100261
	LOSS [training: 0.10618208375930352 | validation: 0.16015935703229237]
	TIME [epoch: 8.16 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12951109914034267		[learning rate: 0.00010008]
		[batch 20/20] avg loss: 0.09185630526307519		[learning rate: 9.9897e-05]
	Learning Rate: 9.98971e-05
	LOSS [training: 0.11068370220170894 | validation: 0.10117978428775298]
	TIME [epoch: 8.13 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09465313046792048		[learning rate: 9.9716e-05]
		[batch 20/20] avg loss: 0.11599804801082522		[learning rate: 9.9535e-05]
	Learning Rate: 9.95345e-05
	LOSS [training: 0.10532558923937282 | validation: 0.1101834746559931]
	TIME [epoch: 8.12 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10360514306272725		[learning rate: 9.9354e-05]
		[batch 20/20] avg loss: 0.09519838253835863		[learning rate: 9.9173e-05]
	Learning Rate: 9.91733e-05
	LOSS [training: 0.09940176280054294 | validation: 0.10203428421251919]
	TIME [epoch: 8.12 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08601144665387375		[learning rate: 9.8993e-05]
		[batch 20/20] avg loss: 0.10434011454724038		[learning rate: 9.8813e-05]
	Learning Rate: 9.88134e-05
	LOSS [training: 0.09517578060055708 | validation: 0.10591885068611523]
	TIME [epoch: 8.15 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09143821216081946		[learning rate: 9.8634e-05]
		[batch 20/20] avg loss: 0.09938768901978613		[learning rate: 9.8455e-05]
	Learning Rate: 9.84548e-05
	LOSS [training: 0.09541295059030278 | validation: 0.12070845235702396]
	TIME [epoch: 8.13 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10128331008011507		[learning rate: 9.8276e-05]
		[batch 20/20] avg loss: 0.10878108124030092		[learning rate: 9.8098e-05]
	Learning Rate: 9.80975e-05
	LOSS [training: 0.10503219566020798 | validation: 0.11373537818263692]
	TIME [epoch: 8.12 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11432820299804873		[learning rate: 9.7919e-05]
		[batch 20/20] avg loss: 0.10064320967734589		[learning rate: 9.7742e-05]
	Learning Rate: 9.77415e-05
	LOSS [training: 0.10748570633769731 | validation: 0.11717737770948385]
	TIME [epoch: 8.12 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09858400877263171		[learning rate: 9.7564e-05]
		[batch 20/20] avg loss: 0.10098581797102371		[learning rate: 9.7387e-05]
	Learning Rate: 9.73868e-05
	LOSS [training: 0.0997849133718277 | validation: 0.10724211404628604]
	TIME [epoch: 8.13 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09534867908071351		[learning rate: 9.721e-05]
		[batch 20/20] avg loss: 0.10536277622679799		[learning rate: 9.7033e-05]
	Learning Rate: 9.70334e-05
	LOSS [training: 0.10035572765375575 | validation: 0.11080178795203119]
	TIME [epoch: 8.14 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0942958467716683		[learning rate: 9.6857e-05]
		[batch 20/20] avg loss: 0.10650412123245875		[learning rate: 9.6681e-05]
	Learning Rate: 9.66812e-05
	LOSS [training: 0.10039998400206354 | validation: 0.10843459985176723]
	TIME [epoch: 8.12 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08724942260604167		[learning rate: 9.6506e-05]
		[batch 20/20] avg loss: 0.0983715127354207		[learning rate: 9.633e-05]
	Learning Rate: 9.63304e-05
	LOSS [training: 0.09281046767073119 | validation: 0.13061325181422853]
	TIME [epoch: 8.16 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08941020971048844		[learning rate: 9.6155e-05]
		[batch 20/20] avg loss: 0.11092705292676792		[learning rate: 9.5981e-05]
	Learning Rate: 9.59808e-05
	LOSS [training: 0.10016863131862816 | validation: 0.13746586110203055]
	TIME [epoch: 8.13 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12020655279193541		[learning rate: 9.5806e-05]
		[batch 20/20] avg loss: 0.10863345415969616		[learning rate: 9.5632e-05]
	Learning Rate: 9.56324e-05
	LOSS [training: 0.1144200034758158 | validation: 0.1364463629451643]
	TIME [epoch: 8.15 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09621514699307454		[learning rate: 9.5459e-05]
		[batch 20/20] avg loss: 0.12101702983071412		[learning rate: 9.5285e-05]
	Learning Rate: 9.52854e-05
	LOSS [training: 0.10861608841189437 | validation: 0.11157769705755577]
	TIME [epoch: 8.14 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10803433821278012		[learning rate: 9.5112e-05]
		[batch 20/20] avg loss: 0.12052647574929498		[learning rate: 9.494e-05]
	Learning Rate: 9.49396e-05
	LOSS [training: 0.11428040698103752 | validation: 0.2182071411982905]
	TIME [epoch: 8.17 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1076242445256376		[learning rate: 9.4767e-05]
		[batch 20/20] avg loss: 0.14307368623638733		[learning rate: 9.4595e-05]
	Learning Rate: 9.45951e-05
	LOSS [training: 0.12534896538101245 | validation: 0.14104348202296918]
	TIME [epoch: 8.13 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12101020082108516		[learning rate: 9.4423e-05]
		[batch 20/20] avg loss: 0.12551179986289274		[learning rate: 9.4252e-05]
	Learning Rate: 9.42518e-05
	LOSS [training: 0.12326100034198892 | validation: 0.22581973707736624]
	TIME [epoch: 8.13 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13573610914664389		[learning rate: 9.4081e-05]
		[batch 20/20] avg loss: 0.11178760143195758		[learning rate: 9.391e-05]
	Learning Rate: 9.39097e-05
	LOSS [training: 0.12376185528930075 | validation: 0.11938170569930696]
	TIME [epoch: 8.15 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11868972481784881		[learning rate: 9.3739e-05]
		[batch 20/20] avg loss: 0.1258312783479726		[learning rate: 9.3569e-05]
	Learning Rate: 9.35689e-05
	LOSS [training: 0.12226050158291069 | validation: 0.1792331378014084]
	TIME [epoch: 8.12 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10588359049606963		[learning rate: 9.3399e-05]
		[batch 20/20] avg loss: 0.1206766329656924		[learning rate: 9.3229e-05]
	Learning Rate: 9.32294e-05
	LOSS [training: 0.11328011173088101 | validation: 0.12406956672701702]
	TIME [epoch: 8.13 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11255227035003501		[learning rate: 9.306e-05]
		[batch 20/20] avg loss: 0.09980698517268839		[learning rate: 9.2891e-05]
	Learning Rate: 9.2891e-05
	LOSS [training: 0.10617962776136172 | validation: 0.10843544332671701]
	TIME [epoch: 8.12 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09999484912819376		[learning rate: 9.2722e-05]
		[batch 20/20] avg loss: 0.1052429121009674		[learning rate: 9.2554e-05]
	Learning Rate: 9.25539e-05
	LOSS [training: 0.10261888061458058 | validation: 0.1080791785814571]
	TIME [epoch: 8.16 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09902111470323041		[learning rate: 9.2386e-05]
		[batch 20/20] avg loss: 0.08675550646095113		[learning rate: 9.2218e-05]
	Learning Rate: 9.2218e-05
	LOSS [training: 0.09288831058209077 | validation: 0.11079592767083642]
	TIME [epoch: 8.18 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08916710389875164		[learning rate: 9.2051e-05]
		[batch 20/20] avg loss: 0.10327227438669304		[learning rate: 9.1883e-05]
	Learning Rate: 9.18834e-05
	LOSS [training: 0.09621968914272234 | validation: 0.10813776247783083]
	TIME [epoch: 8.13 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0925224122622439		[learning rate: 9.1716e-05]
		[batch 20/20] avg loss: 0.10292834614380468		[learning rate: 9.155e-05]
	Learning Rate: 9.15499e-05
	LOSS [training: 0.09772537920302429 | validation: 0.09634748906052232]
	TIME [epoch: 8.13 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09813442117546506		[learning rate: 9.1384e-05]
		[batch 20/20] avg loss: 0.09161817859469436		[learning rate: 9.1218e-05]
	Learning Rate: 9.12177e-05
	LOSS [training: 0.0948762998850797 | validation: 0.10755182700562396]
	TIME [epoch: 8.17 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09883066774468263		[learning rate: 9.1052e-05]
		[batch 20/20] avg loss: 0.09456525810330114		[learning rate: 9.0887e-05]
	Learning Rate: 9.08866e-05
	LOSS [training: 0.09669796292399188 | validation: 0.12684067104330196]
	TIME [epoch: 8.18 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09066659798191334		[learning rate: 9.0722e-05]
		[batch 20/20] avg loss: 0.10638277545144581		[learning rate: 9.0557e-05]
	Learning Rate: 9.05568e-05
	LOSS [training: 0.09852468671667955 | validation: 0.10958446270495537]
	TIME [epoch: 8.13 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08883622103100215		[learning rate: 9.0392e-05]
		[batch 20/20] avg loss: 0.09963596661330983		[learning rate: 9.0228e-05]
	Learning Rate: 9.02281e-05
	LOSS [training: 0.09423609382215598 | validation: 0.11269796781134965]
	TIME [epoch: 8.13 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0912603764231251		[learning rate: 9.0064e-05]
		[batch 20/20] avg loss: 0.09983878450837455		[learning rate: 8.9901e-05]
	Learning Rate: 8.99007e-05
	LOSS [training: 0.09554958046574984 | validation: 0.10414965745098127]
	TIME [epoch: 8.14 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08740731966740231		[learning rate: 8.9737e-05]
		[batch 20/20] avg loss: 0.11472609857992082		[learning rate: 8.9574e-05]
	Learning Rate: 8.95745e-05
	LOSS [training: 0.10106670912366156 | validation: 0.12015644174140248]
	TIME [epoch: 8.15 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09903950530150982		[learning rate: 8.9412e-05]
		[batch 20/20] avg loss: 0.10396550260810891		[learning rate: 8.9249e-05]
	Learning Rate: 8.92494e-05
	LOSS [training: 0.10150250395480935 | validation: 0.10522420278628869]
	TIME [epoch: 8.13 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09149571966473773		[learning rate: 8.9087e-05]
		[batch 20/20] avg loss: 0.09812187378094231		[learning rate: 8.8926e-05]
	Learning Rate: 8.89255e-05
	LOSS [training: 0.09480879672284004 | validation: 0.1316958528128792]
	TIME [epoch: 8.13 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09439068655039748		[learning rate: 8.8764e-05]
		[batch 20/20] avg loss: 0.09359848323900308		[learning rate: 8.8603e-05]
	Learning Rate: 8.86028e-05
	LOSS [training: 0.09399458489470029 | validation: 0.12114486969233053]
	TIME [epoch: 8.12 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0860235321641693		[learning rate: 8.8442e-05]
		[batch 20/20] avg loss: 0.1043767022738042		[learning rate: 8.8281e-05]
	Learning Rate: 8.82813e-05
	LOSS [training: 0.09520011721898675 | validation: 0.10181145801131719]
	TIME [epoch: 8.19 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0918012020842982		[learning rate: 8.8121e-05]
		[batch 20/20] avg loss: 0.09020813814446646		[learning rate: 8.7961e-05]
	Learning Rate: 8.79609e-05
	LOSS [training: 0.09100467011438232 | validation: 0.09762485273003617]
	TIME [epoch: 8.14 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07935476013063575		[learning rate: 8.7801e-05]
		[batch 20/20] avg loss: 0.10607033871159813		[learning rate: 8.7642e-05]
	Learning Rate: 8.76417e-05
	LOSS [training: 0.09271254942111692 | validation: 0.09422753415978372]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_1403.pth
	Model improved!!!
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08753385356479554		[learning rate: 8.7482e-05]
		[batch 20/20] avg loss: 0.09916512700860766		[learning rate: 8.7324e-05]
	Learning Rate: 8.73236e-05
	LOSS [training: 0.09334949028670161 | validation: 0.10699561695232535]
	TIME [epoch: 8.16 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09211816489323768		[learning rate: 8.7165e-05]
		[batch 20/20] avg loss: 0.08615829475133381		[learning rate: 8.7007e-05]
	Learning Rate: 8.70067e-05
	LOSS [training: 0.08913822982228575 | validation: 0.10021593384796527]
	TIME [epoch: 8.18 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1077024303073719		[learning rate: 8.6849e-05]
		[batch 20/20] avg loss: 0.09968141113765833		[learning rate: 8.6691e-05]
	Learning Rate: 8.66909e-05
	LOSS [training: 0.1036919207225151 | validation: 0.10754599238286905]
	TIME [epoch: 8.15 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09152250199062939		[learning rate: 8.6533e-05]
		[batch 20/20] avg loss: 0.09463995542419487		[learning rate: 8.6376e-05]
	Learning Rate: 8.63763e-05
	LOSS [training: 0.09308122870741213 | validation: 0.09997163929924102]
	TIME [epoch: 8.12 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08821602674446313		[learning rate: 8.6219e-05]
		[batch 20/20] avg loss: 0.09745731672979648		[learning rate: 8.6063e-05]
	Learning Rate: 8.60629e-05
	LOSS [training: 0.09283667173712981 | validation: 0.0982180468805081]
	TIME [epoch: 8.12 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09180875570643503		[learning rate: 8.5907e-05]
		[batch 20/20] avg loss: 0.11443711574354154		[learning rate: 8.5751e-05]
	Learning Rate: 8.57505e-05
	LOSS [training: 0.10312293572498828 | validation: 0.11386600895753185]
	TIME [epoch: 8.12 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09403758668287024		[learning rate: 8.5595e-05]
		[batch 20/20] avg loss: 0.10215438172516418		[learning rate: 8.5439e-05]
	Learning Rate: 8.54394e-05
	LOSS [training: 0.0980959842040172 | validation: 0.10497400045725343]
	TIME [epoch: 8.15 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09005856529010839		[learning rate: 8.5284e-05]
		[batch 20/20] avg loss: 0.10477135017961625		[learning rate: 8.5129e-05]
	Learning Rate: 8.51293e-05
	LOSS [training: 0.09741495773486233 | validation: 0.13484626003699074]
	TIME [epoch: 8.12 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10106205963920967		[learning rate: 8.4975e-05]
		[batch 20/20] avg loss: 0.08487353518411223		[learning rate: 8.482e-05]
	Learning Rate: 8.48204e-05
	LOSS [training: 0.09296779741166097 | validation: 0.10910177367357385]
	TIME [epoch: 8.13 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09807313039825138		[learning rate: 8.4666e-05]
		[batch 20/20] avg loss: 0.08673250052290707		[learning rate: 8.4513e-05]
	Learning Rate: 8.45125e-05
	LOSS [training: 0.0924028154605792 | validation: 0.10619249283605742]
	TIME [epoch: 8.12 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10908411682232104		[learning rate: 8.4359e-05]
		[batch 20/20] avg loss: 0.09224368282744097		[learning rate: 8.4206e-05]
	Learning Rate: 8.42058e-05
	LOSS [training: 0.10066389982488103 | validation: 0.10652995267402587]
	TIME [epoch: 8.13 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10074997277678488		[learning rate: 8.4053e-05]
		[batch 20/20] avg loss: 0.09727607035075025		[learning rate: 8.39e-05]
	Learning Rate: 8.39002e-05
	LOSS [training: 0.09901302156376758 | validation: 0.0993235717171007]
	TIME [epoch: 8.19 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09433166666699745		[learning rate: 8.3748e-05]
		[batch 20/20] avg loss: 0.08957569527734248		[learning rate: 8.3596e-05]
	Learning Rate: 8.35958e-05
	LOSS [training: 0.09195368097216997 | validation: 0.11161677275815865]
	TIME [epoch: 8.14 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09197752430411026		[learning rate: 8.3444e-05]
		[batch 20/20] avg loss: 0.10095380398283313		[learning rate: 8.3292e-05]
	Learning Rate: 8.32924e-05
	LOSS [training: 0.0964656641434717 | validation: 0.09918483736558942]
	TIME [epoch: 8.13 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10506663638726585		[learning rate: 8.3141e-05]
		[batch 20/20] avg loss: 0.09111396115583296		[learning rate: 8.299e-05]
	Learning Rate: 8.29901e-05
	LOSS [training: 0.09809029877154941 | validation: 0.09663913867940568]
	TIME [epoch: 8.12 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08693799312905573		[learning rate: 8.2839e-05]
		[batch 20/20] avg loss: 0.09522435459028829		[learning rate: 8.2689e-05]
	Learning Rate: 8.26889e-05
	LOSS [training: 0.091081173859672 | validation: 0.09258044922644297]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_1419.pth
	Model improved!!!
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09146495442746497		[learning rate: 8.2539e-05]
		[batch 20/20] avg loss: 0.11121091174074453		[learning rate: 8.2389e-05]
	Learning Rate: 8.23889e-05
	LOSS [training: 0.10133793308410474 | validation: 0.09466760739271474]
	TIME [epoch: 8.45 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08557701222129968		[learning rate: 8.2239e-05]
		[batch 20/20] avg loss: 0.10507674365186064		[learning rate: 8.209e-05]
	Learning Rate: 8.20899e-05
	LOSS [training: 0.09532687793658016 | validation: 0.10461897701211696]
	TIME [epoch: 8.14 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09565975791645767		[learning rate: 8.1941e-05]
		[batch 20/20] avg loss: 0.09895078334025863		[learning rate: 8.1792e-05]
	Learning Rate: 8.17919e-05
	LOSS [training: 0.09730527062835817 | validation: 0.11128690354412737]
	TIME [epoch: 8.13 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10612269657513809		[learning rate: 8.1643e-05]
		[batch 20/20] avg loss: 0.10027762509483278		[learning rate: 8.1495e-05]
	Learning Rate: 8.14951e-05
	LOSS [training: 0.10320016083498544 | validation: 0.09672982559150593]
	TIME [epoch: 8.15 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09994965187087955		[learning rate: 8.1347e-05]
		[batch 20/20] avg loss: 0.09561329326792886		[learning rate: 8.1199e-05]
	Learning Rate: 8.11994e-05
	LOSS [training: 0.0977814725694042 | validation: 0.08950683465538205]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_1424.pth
	Model improved!!!
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11418504231588203		[learning rate: 8.1052e-05]
		[batch 20/20] avg loss: 0.0826833629889726		[learning rate: 8.0905e-05]
	Learning Rate: 8.09047e-05
	LOSS [training: 0.09843420265242729 | validation: 0.09933677427217069]
	TIME [epoch: 8.14 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11206104909076038		[learning rate: 8.0758e-05]
		[batch 20/20] avg loss: 0.08262404860349509		[learning rate: 8.0611e-05]
	Learning Rate: 8.06111e-05
	LOSS [training: 0.09734254884712774 | validation: 0.11352348978853843]
	TIME [epoch: 8.14 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1033464566947919		[learning rate: 8.0465e-05]
		[batch 20/20] avg loss: 0.10307648678473322		[learning rate: 8.0319e-05]
	Learning Rate: 8.03186e-05
	LOSS [training: 0.10321147173976256 | validation: 0.0998413958525162]
	TIME [epoch: 8.18 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10805009675825236		[learning rate: 8.0173e-05]
		[batch 20/20] avg loss: 0.1111181200570254		[learning rate: 8.0027e-05]
	Learning Rate: 8.00271e-05
	LOSS [training: 0.10958410840763885 | validation: 0.10908829562967261]
	TIME [epoch: 8.17 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09774782561310856		[learning rate: 7.9882e-05]
		[batch 20/20] avg loss: 0.0938204300030709		[learning rate: 7.9737e-05]
	Learning Rate: 7.97367e-05
	LOSS [training: 0.09578412780808973 | validation: 0.10381529115376877]
	TIME [epoch: 8.14 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10482989069933399		[learning rate: 7.9592e-05]
		[batch 20/20] avg loss: 0.08983501408136244		[learning rate: 7.9447e-05]
	Learning Rate: 7.94473e-05
	LOSS [training: 0.09733245239034823 | validation: 0.09724747261751235]
	TIME [epoch: 8.17 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10010818170609397		[learning rate: 7.9303e-05]
		[batch 20/20] avg loss: 0.0894919834000749		[learning rate: 7.9159e-05]
	Learning Rate: 7.91589e-05
	LOSS [training: 0.09480008255308445 | validation: 0.10687855946219019]
	TIME [epoch: 8.18 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09490589229711067		[learning rate: 7.9015e-05]
		[batch 20/20] avg loss: 0.09956943126504353		[learning rate: 7.8872e-05]
	Learning Rate: 7.88717e-05
	LOSS [training: 0.09723766178107711 | validation: 0.09745464587417649]
	TIME [epoch: 8.16 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1093321866231872		[learning rate: 7.8728e-05]
		[batch 20/20] avg loss: 0.08123209362845304		[learning rate: 7.8585e-05]
	Learning Rate: 7.85854e-05
	LOSS [training: 0.09528214012582012 | validation: 0.116364006856826]
	TIME [epoch: 8.14 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08809453324898182		[learning rate: 7.8443e-05]
		[batch 20/20] avg loss: 0.09762755397894071		[learning rate: 7.83e-05]
	Learning Rate: 7.83003e-05
	LOSS [training: 0.09286104361396125 | validation: 0.11708641512556824]
	TIME [epoch: 8.14 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08297255733393478		[learning rate: 7.8158e-05]
		[batch 20/20] avg loss: 0.09799963159665329		[learning rate: 7.8016e-05]
	Learning Rate: 7.80161e-05
	LOSS [training: 0.09048609446529401 | validation: 0.10767123809664861]
	TIME [epoch: 8.14 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09810159628626147		[learning rate: 7.7874e-05]
		[batch 20/20] avg loss: 0.09047514901342672		[learning rate: 7.7733e-05]
	Learning Rate: 7.7733e-05
	LOSS [training: 0.0942883726498441 | validation: 0.10564253395646148]
	TIME [epoch: 8.14 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09386123340297466		[learning rate: 7.7592e-05]
		[batch 20/20] avg loss: 0.08677410042973115		[learning rate: 7.7451e-05]
	Learning Rate: 7.74509e-05
	LOSS [training: 0.0903176669163529 | validation: 0.10613718742195627]
	TIME [epoch: 8.16 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.091075533609714		[learning rate: 7.731e-05]
		[batch 20/20] avg loss: 0.08945493146187107		[learning rate: 7.717e-05]
	Learning Rate: 7.71698e-05
	LOSS [training: 0.09026523253579254 | validation: 0.1128391113334608]
	TIME [epoch: 8.19 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08351441983631913		[learning rate: 7.703e-05]
		[batch 20/20] avg loss: 0.10440195293805532		[learning rate: 7.689e-05]
	Learning Rate: 7.68897e-05
	LOSS [training: 0.0939581863871872 | validation: 0.0988726896654734]
	TIME [epoch: 8.15 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08963268262910523		[learning rate: 7.675e-05]
		[batch 20/20] avg loss: 0.1022400720501957		[learning rate: 7.6611e-05]
	Learning Rate: 7.66107e-05
	LOSS [training: 0.09593637733965046 | validation: 0.11931882094662119]
	TIME [epoch: 8.14 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10353831325420926		[learning rate: 7.6472e-05]
		[batch 20/20] avg loss: 0.08785943174631927		[learning rate: 7.6333e-05]
	Learning Rate: 7.63327e-05
	LOSS [training: 0.09569887250026425 | validation: 0.11228722434561417]
	TIME [epoch: 8.17 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08294411043185809		[learning rate: 7.6194e-05]
		[batch 20/20] avg loss: 0.10063798969634548		[learning rate: 7.6056e-05]
	Learning Rate: 7.60557e-05
	LOSS [training: 0.09179105006410176 | validation: 0.1135190044827277]
	TIME [epoch: 8.2 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09696631249807389		[learning rate: 7.5918e-05]
		[batch 20/20] avg loss: 0.09379257732062773		[learning rate: 7.578e-05]
	Learning Rate: 7.57797e-05
	LOSS [training: 0.0953794449093508 | validation: 0.10315577533207072]
	TIME [epoch: 8.14 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08625412031572455		[learning rate: 7.5642e-05]
		[batch 20/20] avg loss: 0.09626446075792201		[learning rate: 7.5505e-05]
	Learning Rate: 7.55047e-05
	LOSS [training: 0.09125929053682327 | validation: 0.09971124703788906]
	TIME [epoch: 8.13 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09214176092358424		[learning rate: 7.5368e-05]
		[batch 20/20] avg loss: 0.10859671041980339		[learning rate: 7.5231e-05]
	Learning Rate: 7.52307e-05
	LOSS [training: 0.1003692356716938 | validation: 0.1061328605009735]
	TIME [epoch: 8.16 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1120221304251671		[learning rate: 7.5094e-05]
		[batch 20/20] avg loss: 0.09562804375412857		[learning rate: 7.4958e-05]
	Learning Rate: 7.49576e-05
	LOSS [training: 0.10382508708964786 | validation: 0.0948518638434146]
	TIME [epoch: 8.15 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09791250893382615		[learning rate: 7.4822e-05]
		[batch 20/20] avg loss: 0.09395709666581528		[learning rate: 7.4686e-05]
	Learning Rate: 7.46856e-05
	LOSS [training: 0.09593480279982071 | validation: 0.09699754595377186]
	TIME [epoch: 8.14 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0877119574790354		[learning rate: 7.455e-05]
		[batch 20/20] avg loss: 0.08951328385878878		[learning rate: 7.4415e-05]
	Learning Rate: 7.44146e-05
	LOSS [training: 0.0886126206689121 | validation: 0.10145853256819806]
	TIME [epoch: 8.14 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10027212929923342		[learning rate: 7.4279e-05]
		[batch 20/20] avg loss: 0.09577854656063223		[learning rate: 7.4144e-05]
	Learning Rate: 7.41445e-05
	LOSS [training: 0.09802533792993283 | validation: 0.13253132050074273]
	TIME [epoch: 8.14 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1024977798409912		[learning rate: 7.401e-05]
		[batch 20/20] avg loss: 0.09506680106653972		[learning rate: 7.3875e-05]
	Learning Rate: 7.38754e-05
	LOSS [training: 0.09878229045376546 | validation: 0.09990752774902963]
	TIME [epoch: 8.17 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08848192176524385		[learning rate: 7.3741e-05]
		[batch 20/20] avg loss: 0.10952390502530203		[learning rate: 7.3607e-05]
	Learning Rate: 7.36073e-05
	LOSS [training: 0.09900291339527292 | validation: 0.0996968738244225]
	TIME [epoch: 8.18 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08934348784907872		[learning rate: 7.3474e-05]
		[batch 20/20] avg loss: 0.10106785920962595		[learning rate: 7.334e-05]
	Learning Rate: 7.33402e-05
	LOSS [training: 0.09520567352935234 | validation: 0.11587331117156636]
	TIME [epoch: 8.14 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11548092790825185		[learning rate: 7.3207e-05]
		[batch 20/20] avg loss: 0.07980873164475212		[learning rate: 7.3074e-05]
	Learning Rate: 7.30741e-05
	LOSS [training: 0.09764482977650198 | validation: 0.09601305411962419]
	TIME [epoch: 8.15 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08377623319224978		[learning rate: 7.2941e-05]
		[batch 20/20] avg loss: 0.10117952800147864		[learning rate: 7.2809e-05]
	Learning Rate: 7.28089e-05
	LOSS [training: 0.09247788059686421 | validation: 0.10574309562096906]
	TIME [epoch: 8.18 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0861271019569618		[learning rate: 7.2677e-05]
		[batch 20/20] avg loss: 0.10519206402853361		[learning rate: 7.2545e-05]
	Learning Rate: 7.25447e-05
	LOSS [training: 0.09565958299274771 | validation: 0.10118954057768001]
	TIME [epoch: 8.18 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09670718574042159		[learning rate: 7.2413e-05]
		[batch 20/20] avg loss: 0.1058652316598813		[learning rate: 7.2281e-05]
	Learning Rate: 7.22814e-05
	LOSS [training: 0.10128620870015145 | validation: 0.10347440165579502]
	TIME [epoch: 8.14 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09102398494697582		[learning rate: 7.215e-05]
		[batch 20/20] avg loss: 0.08593823880755576		[learning rate: 7.2019e-05]
	Learning Rate: 7.2019e-05
	LOSS [training: 0.08848111187726579 | validation: 0.09612582584892483]
	TIME [epoch: 8.14 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1069361204113797		[learning rate: 7.1888e-05]
		[batch 20/20] avg loss: 0.08752000937348295		[learning rate: 7.1758e-05]
	Learning Rate: 7.17577e-05
	LOSS [training: 0.09722806489243133 | validation: 0.1050063958973681]
	TIME [epoch: 8.14 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1098715590976977		[learning rate: 7.1627e-05]
		[batch 20/20] avg loss: 0.0881672260198679		[learning rate: 7.1497e-05]
	Learning Rate: 7.14973e-05
	LOSS [training: 0.0990193925587828 | validation: 0.10282891067116881]
	TIME [epoch: 8.16 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10595885045724329		[learning rate: 7.1367e-05]
		[batch 20/20] avg loss: 0.08346534448404906		[learning rate: 7.1238e-05]
	Learning Rate: 7.12378e-05
	LOSS [training: 0.09471209747064616 | validation: 0.1135892218345475]
	TIME [epoch: 8.17 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10391419424399177		[learning rate: 7.1108e-05]
		[batch 20/20] avg loss: 0.09290009531484773		[learning rate: 7.0979e-05]
	Learning Rate: 7.09793e-05
	LOSS [training: 0.09840714477941975 | validation: 0.10545697008692283]
	TIME [epoch: 8.14 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10266440913585222		[learning rate: 7.085e-05]
		[batch 20/20] avg loss: 0.08494668173381953		[learning rate: 7.0722e-05]
	Learning Rate: 7.07217e-05
	LOSS [training: 0.09380554543483585 | validation: 0.10546044652704757]
	TIME [epoch: 8.13 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09549274514162046		[learning rate: 7.0593e-05]
		[batch 20/20] avg loss: 0.0962242971900791		[learning rate: 7.0465e-05]
	Learning Rate: 7.0465e-05
	LOSS [training: 0.09585852116584978 | validation: 0.11262149250746734]
	TIME [epoch: 8.22 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10381240596903045		[learning rate: 7.0337e-05]
		[batch 20/20] avg loss: 0.08755331710900469		[learning rate: 7.0209e-05]
	Learning Rate: 7.02093e-05
	LOSS [training: 0.09568286153901759 | validation: 0.10379015357825544]
	TIME [epoch: 8.15 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09137538167858389		[learning rate: 7.0082e-05]
		[batch 20/20] avg loss: 0.10274482997491852		[learning rate: 6.9955e-05]
	Learning Rate: 6.99545e-05
	LOSS [training: 0.0970601058267512 | validation: 0.11431342850346796]
	TIME [epoch: 8.14 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08238449213067159		[learning rate: 6.9827e-05]
		[batch 20/20] avg loss: 0.1013170551840564		[learning rate: 6.9701e-05]
	Learning Rate: 6.97006e-05
	LOSS [training: 0.09185077365736402 | validation: 0.09965431639279064]
	TIME [epoch: 8.14 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08221551581003485		[learning rate: 6.9574e-05]
		[batch 20/20] avg loss: 0.09530363437650091		[learning rate: 6.9448e-05]
	Learning Rate: 6.94477e-05
	LOSS [training: 0.08875957509326787 | validation: 0.09445703740172284]
	TIME [epoch: 8.2 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08482726264063296		[learning rate: 6.9322e-05]
		[batch 20/20] avg loss: 0.09881264064188433		[learning rate: 6.9196e-05]
	Learning Rate: 6.91957e-05
	LOSS [training: 0.09181995164125864 | validation: 0.10621894826460676]
	TIME [epoch: 8.16 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09907597193883719		[learning rate: 6.907e-05]
		[batch 20/20] avg loss: 0.09347485509035779		[learning rate: 6.8945e-05]
	Learning Rate: 6.89446e-05
	LOSS [training: 0.0962754135145975 | validation: 0.11128917514465889]
	TIME [epoch: 8.14 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0973718015416227		[learning rate: 6.8819e-05]
		[batch 20/20] avg loss: 0.09563009008254435		[learning rate: 6.8694e-05]
	Learning Rate: 6.86944e-05
	LOSS [training: 0.09650094581208354 | validation: 0.10808877364157382]
	TIME [epoch: 8.14 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10116883981220268		[learning rate: 6.857e-05]
		[batch 20/20] avg loss: 0.07945753424358595		[learning rate: 6.8445e-05]
	Learning Rate: 6.84451e-05
	LOSS [training: 0.0903131870278943 | validation: 0.09476143537243349]
	TIME [epoch: 8.14 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09864820608835671		[learning rate: 6.8321e-05]
		[batch 20/20] avg loss: 0.08222311551111805		[learning rate: 6.8197e-05]
	Learning Rate: 6.81967e-05
	LOSS [training: 0.09043566079973739 | validation: 0.09770901670750648]
	TIME [epoch: 8.16 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09613454542724935		[learning rate: 6.8073e-05]
		[batch 20/20] avg loss: 0.08641937179820051		[learning rate: 6.7949e-05]
	Learning Rate: 6.79492e-05
	LOSS [training: 0.09127695861272493 | validation: 0.1017070774509443]
	TIME [epoch: 8.14 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10074280457163305		[learning rate: 6.7826e-05]
		[batch 20/20] avg loss: 0.09385659163006775		[learning rate: 6.7703e-05]
	Learning Rate: 6.77026e-05
	LOSS [training: 0.0972996981008504 | validation: 0.10425026163980729]
	TIME [epoch: 8.16 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0960566686574331		[learning rate: 6.758e-05]
		[batch 20/20] avg loss: 0.088615068127999		[learning rate: 6.7457e-05]
	Learning Rate: 6.74569e-05
	LOSS [training: 0.09233586839271606 | validation: 0.11549172784445906]
	TIME [epoch: 8.17 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0955132946713678		[learning rate: 6.7334e-05]
		[batch 20/20] avg loss: 0.08704194737683829		[learning rate: 6.7212e-05]
	Learning Rate: 6.72121e-05
	LOSS [training: 0.09127762102410306 | validation: 0.10507273529306138]
	TIME [epoch: 8.15 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08586078574213576		[learning rate: 6.709e-05]
		[batch 20/20] avg loss: 0.0842231877718718		[learning rate: 6.6968e-05]
	Learning Rate: 6.69682e-05
	LOSS [training: 0.08504198675700378 | validation: 0.10917996941063195]
	TIME [epoch: 8.16 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09058888066361366		[learning rate: 6.6847e-05]
		[batch 20/20] avg loss: 0.09254012656935598		[learning rate: 6.6725e-05]
	Learning Rate: 6.67251e-05
	LOSS [training: 0.09156450361648484 | validation: 0.12002082174768315]
	TIME [epoch: 8.18 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08818121731899567		[learning rate: 6.6604e-05]
		[batch 20/20] avg loss: 0.0945544541568496		[learning rate: 6.6483e-05]
	Learning Rate: 6.6483e-05
	LOSS [training: 0.09136783573792265 | validation: 0.10618694201978021]
	TIME [epoch: 8.16 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09365646845477285		[learning rate: 6.6362e-05]
		[batch 20/20] avg loss: 0.10215290024305199		[learning rate: 6.6242e-05]
	Learning Rate: 6.62417e-05
	LOSS [training: 0.0979046843489124 | validation: 0.10300060531043642]
	TIME [epoch: 8.14 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08806804252018648		[learning rate: 6.6121e-05]
		[batch 20/20] avg loss: 0.09417092229210015		[learning rate: 6.6001e-05]
	Learning Rate: 6.60013e-05
	LOSS [training: 0.09111948240614333 | validation: 0.10004722539746516]
	TIME [epoch: 8.16 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09106829126625031		[learning rate: 6.5881e-05]
		[batch 20/20] avg loss: 0.09325139464250674		[learning rate: 6.5762e-05]
	Learning Rate: 6.57618e-05
	LOSS [training: 0.09215984295437853 | validation: 0.11406518646343294]
	TIME [epoch: 8.14 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09497289118929131		[learning rate: 6.5642e-05]
		[batch 20/20] avg loss: 0.10389453342532595		[learning rate: 6.5523e-05]
	Learning Rate: 6.55232e-05
	LOSS [training: 0.09943371230730862 | validation: 0.11402511007308394]
	TIME [epoch: 8.14 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09928714978458461		[learning rate: 6.5404e-05]
		[batch 20/20] avg loss: 0.08792118615535505		[learning rate: 6.5285e-05]
	Learning Rate: 6.52854e-05
	LOSS [training: 0.09360416796996983 | validation: 0.1159209854543001]
	TIME [epoch: 8.13 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09265280000747961		[learning rate: 6.5167e-05]
		[batch 20/20] avg loss: 0.10034250820665447		[learning rate: 6.5048e-05]
	Learning Rate: 6.50484e-05
	LOSS [training: 0.09649765410706704 | validation: 0.10253830849067277]
	TIME [epoch: 8.15 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08837110022477222		[learning rate: 6.493e-05]
		[batch 20/20] avg loss: 0.10225948641579363		[learning rate: 6.4812e-05]
	Learning Rate: 6.48124e-05
	LOSS [training: 0.09531529332028292 | validation: 0.10434095292860789]
	TIME [epoch: 8.15 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.082585204048267		[learning rate: 6.4695e-05]
		[batch 20/20] avg loss: 0.09451688428756605		[learning rate: 6.4577e-05]
	Learning Rate: 6.45772e-05
	LOSS [training: 0.08855104416791654 | validation: 0.11631632681550749]
	TIME [epoch: 8.14 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10598736919602311		[learning rate: 6.446e-05]
		[batch 20/20] avg loss: 0.07364508445002715		[learning rate: 6.4343e-05]
	Learning Rate: 6.43428e-05
	LOSS [training: 0.08981622682302517 | validation: 0.10525667370750758]
	TIME [epoch: 8.14 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11158552233470612		[learning rate: 6.4226e-05]
		[batch 20/20] avg loss: 0.09247862010352499		[learning rate: 6.4109e-05]
	Learning Rate: 6.41093e-05
	LOSS [training: 0.10203207121911553 | validation: 0.11464942184347729]
	TIME [epoch: 8.16 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08955226609278596		[learning rate: 6.3993e-05]
		[batch 20/20] avg loss: 0.0969760816292623		[learning rate: 6.3877e-05]
	Learning Rate: 6.38767e-05
	LOSS [training: 0.09326417386102413 | validation: 0.10387573278155396]
	TIME [epoch: 8.2 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09440591027274868		[learning rate: 6.3761e-05]
		[batch 20/20] avg loss: 0.07771577242638536		[learning rate: 6.3645e-05]
	Learning Rate: 6.36449e-05
	LOSS [training: 0.08606084134956701 | validation: 0.10217224935244991]
	TIME [epoch: 8.14 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09032628135552075		[learning rate: 6.3529e-05]
		[batch 20/20] avg loss: 0.11440202341529052		[learning rate: 6.3414e-05]
	Learning Rate: 6.34139e-05
	LOSS [training: 0.10236415238540561 | validation: 0.1254495694314795]
	TIME [epoch: 8.14 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09902218969786454		[learning rate: 6.3299e-05]
		[batch 20/20] avg loss: 0.10390113899907064		[learning rate: 6.3184e-05]
	Learning Rate: 6.31837e-05
	LOSS [training: 0.1014616643484676 | validation: 0.10712458285924138]
	TIME [epoch: 8.17 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08982731101102542		[learning rate: 6.3069e-05]
		[batch 20/20] avg loss: 0.10794660679258083		[learning rate: 6.2954e-05]
	Learning Rate: 6.29544e-05
	LOSS [training: 0.09888695890180313 | validation: 0.132165798484081]
	TIME [epoch: 8.2 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09158535513838845		[learning rate: 6.284e-05]
		[batch 20/20] avg loss: 0.08282412381253663		[learning rate: 6.2726e-05]
	Learning Rate: 6.2726e-05
	LOSS [training: 0.08720473947546253 | validation: 0.10914979626270482]
	TIME [epoch: 8.14 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08712696090880477		[learning rate: 6.2612e-05]
		[batch 20/20] avg loss: 0.10069855373843746		[learning rate: 6.2498e-05]
	Learning Rate: 6.24983e-05
	LOSS [training: 0.09391275732362112 | validation: 0.09916158875891389]
	TIME [epoch: 8.14 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09878756515227177		[learning rate: 6.2385e-05]
		[batch 20/20] avg loss: 0.09816581181234828		[learning rate: 6.2272e-05]
	Learning Rate: 6.22715e-05
	LOSS [training: 0.09847668848231003 | validation: 0.1064632566116038]
	TIME [epoch: 8.13 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10178899205088017		[learning rate: 6.2158e-05]
		[batch 20/20] avg loss: 0.08903320482168661		[learning rate: 6.2046e-05]
	Learning Rate: 6.20455e-05
	LOSS [training: 0.0954110984362834 | validation: 0.09653491514574772]
	TIME [epoch: 8.14 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08971290679982248		[learning rate: 6.1933e-05]
		[batch 20/20] avg loss: 0.10102390959514478		[learning rate: 6.182e-05]
	Learning Rate: 6.18204e-05
	LOSS [training: 0.09536840819748363 | validation: 0.1042787913181186]
	TIME [epoch: 8.15 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09695404785273762		[learning rate: 6.1708e-05]
		[batch 20/20] avg loss: 0.10027154780479794		[learning rate: 6.1596e-05]
	Learning Rate: 6.1596e-05
	LOSS [training: 0.09861279782876779 | validation: 0.10242776765107134]
	TIME [epoch: 8.14 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09843570147513954		[learning rate: 6.1484e-05]
		[batch 20/20] avg loss: 0.09008131579126241		[learning rate: 6.1372e-05]
	Learning Rate: 6.13725e-05
	LOSS [training: 0.09425850863320098 | validation: 0.11042439494478472]
	TIME [epoch: 8.13 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09349467551089355		[learning rate: 6.1261e-05]
		[batch 20/20] avg loss: 0.099654531908304		[learning rate: 6.115e-05]
	Learning Rate: 6.11498e-05
	LOSS [training: 0.09657460370959878 | validation: 0.09559213290544921]
	TIME [epoch: 8.13 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08948274757273347		[learning rate: 6.1039e-05]
		[batch 20/20] avg loss: 0.10066891017424946		[learning rate: 6.0928e-05]
	Learning Rate: 6.09278e-05
	LOSS [training: 0.09507582887349146 | validation: 0.11459420512100317]
	TIME [epoch: 8.19 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08595291869184535		[learning rate: 6.0817e-05]
		[batch 20/20] avg loss: 0.10800396943778397		[learning rate: 6.0707e-05]
	Learning Rate: 6.07067e-05
	LOSS [training: 0.09697844406481465 | validation: 0.1054860206023471]
	TIME [epoch: 8.15 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07914883572971963		[learning rate: 6.0596e-05]
		[batch 20/20] avg loss: 0.10934471040564178		[learning rate: 6.0486e-05]
	Learning Rate: 6.04864e-05
	LOSS [training: 0.0942467730676807 | validation: 0.10018181227989353]
	TIME [epoch: 8.15 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0857676739491908		[learning rate: 6.0377e-05]
		[batch 20/20] avg loss: 0.11171630337099955		[learning rate: 6.0267e-05]
	Learning Rate: 6.02669e-05
	LOSS [training: 0.09874198866009518 | validation: 0.09040833993480434]
	TIME [epoch: 8.14 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1041859639745583		[learning rate: 6.0157e-05]
		[batch 20/20] avg loss: 0.10359602055906547		[learning rate: 6.0048e-05]
	Learning Rate: 6.00482e-05
	LOSS [training: 0.1038909922668119 | validation: 0.09645344368775947]
	TIME [epoch: 8.19 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09620380342249626		[learning rate: 5.9939e-05]
		[batch 20/20] avg loss: 0.09421831062327066		[learning rate: 5.983e-05]
	Learning Rate: 5.98303e-05
	LOSS [training: 0.09521105702288346 | validation: 0.11314226589544595]
	TIME [epoch: 8.18 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08842573498615738		[learning rate: 5.9722e-05]
		[batch 20/20] avg loss: 0.09938886295915784		[learning rate: 5.9613e-05]
	Learning Rate: 5.96132e-05
	LOSS [training: 0.0939072989726576 | validation: 0.10781789180518944]
	TIME [epoch: 8.13 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09605999283690467		[learning rate: 5.9505e-05]
		[batch 20/20] avg loss: 0.08712154898729497		[learning rate: 5.9397e-05]
	Learning Rate: 5.93968e-05
	LOSS [training: 0.09159077091209983 | validation: 0.11550345144355001]
	TIME [epoch: 8.13 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08913486813359936		[learning rate: 5.9289e-05]
		[batch 20/20] avg loss: 0.08922568035240924		[learning rate: 5.9181e-05]
	Learning Rate: 5.91813e-05
	LOSS [training: 0.0891802742430043 | validation: 0.09967478689158579]
	TIME [epoch: 8.14 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09171905101688281		[learning rate: 5.9074e-05]
		[batch 20/20] avg loss: 0.09392969370184437		[learning rate: 5.8966e-05]
	Learning Rate: 5.89665e-05
	LOSS [training: 0.09282437235936358 | validation: 0.10500379681813]
	TIME [epoch: 8.16 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09993813550779855		[learning rate: 5.8859e-05]
		[batch 20/20] avg loss: 0.09221207436934785		[learning rate: 5.8752e-05]
	Learning Rate: 5.87525e-05
	LOSS [training: 0.09607510493857319 | validation: 0.10120540623080229]
	TIME [epoch: 8.13 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09005778489047472		[learning rate: 5.8646e-05]
		[batch 20/20] avg loss: 0.08972972173111508		[learning rate: 5.8539e-05]
	Learning Rate: 5.85393e-05
	LOSS [training: 0.0898937533107949 | validation: 0.10799649609270863]
	TIME [epoch: 8.13 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08927909089749587		[learning rate: 5.8433e-05]
		[batch 20/20] avg loss: 0.10004373099694393		[learning rate: 5.8327e-05]
	Learning Rate: 5.83268e-05
	LOSS [training: 0.09466141094721992 | validation: 0.10988283353040726]
	TIME [epoch: 8.13 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08939735187684804		[learning rate: 5.8221e-05]
		[batch 20/20] avg loss: 0.09471976720644841		[learning rate: 5.8115e-05]
	Learning Rate: 5.81152e-05
	LOSS [training: 0.09205855954164824 | validation: 0.10629861234305168]
	TIME [epoch: 8.16 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09405304687995575		[learning rate: 5.801e-05]
		[batch 20/20] avg loss: 0.0884438328627824		[learning rate: 5.7904e-05]
	Learning Rate: 5.79043e-05
	LOSS [training: 0.09124843987136907 | validation: 0.10201250177942006]
	TIME [epoch: 8.19 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08423186465439961		[learning rate: 5.7799e-05]
		[batch 20/20] avg loss: 0.09777654776952438		[learning rate: 5.7694e-05]
	Learning Rate: 5.76941e-05
	LOSS [training: 0.09100420621196201 | validation: 0.1066873479127073]
	TIME [epoch: 8.15 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09187669926084754		[learning rate: 5.7589e-05]
		[batch 20/20] avg loss: 0.0860412953163642		[learning rate: 5.7485e-05]
	Learning Rate: 5.74847e-05
	LOSS [training: 0.08895899728860586 | validation: 0.10200080929319497]
	TIME [epoch: 8.14 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0933864489199278		[learning rate: 5.738e-05]
		[batch 20/20] avg loss: 0.08157367127755102		[learning rate: 5.7276e-05]
	Learning Rate: 5.72761e-05
	LOSS [training: 0.0874800600987394 | validation: 0.09972353824138369]
	TIME [epoch: 8.14 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08597829453958653		[learning rate: 5.7172e-05]
		[batch 20/20] avg loss: 0.08938080803274964		[learning rate: 5.7068e-05]
	Learning Rate: 5.70683e-05
	LOSS [training: 0.08767955128616807 | validation: 0.09696709003576072]
	TIME [epoch: 8.22 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09466881697025455		[learning rate: 5.6965e-05]
		[batch 20/20] avg loss: 0.08291114568729226		[learning rate: 5.6861e-05]
	Learning Rate: 5.68612e-05
	LOSS [training: 0.0887899813287734 | validation: 0.10794039782766926]
	TIME [epoch: 8.14 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0835999076428218		[learning rate: 5.6758e-05]
		[batch 20/20] avg loss: 0.09919861401352645		[learning rate: 5.6655e-05]
	Learning Rate: 5.66548e-05
	LOSS [training: 0.09139926082817412 | validation: 0.1055923963605325]
	TIME [epoch: 8.13 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0917720577932151		[learning rate: 5.6552e-05]
		[batch 20/20] avg loss: 0.08891865819970451		[learning rate: 5.6449e-05]
	Learning Rate: 5.64492e-05
	LOSS [training: 0.0903453579964598 | validation: 0.09762557203263231]
	TIME [epoch: 8.13 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0875413940110734		[learning rate: 5.6347e-05]
		[batch 20/20] avg loss: 0.09013384004219954		[learning rate: 5.6244e-05]
	Learning Rate: 5.62444e-05
	LOSS [training: 0.08883761702663646 | validation: 0.10283258219116338]
	TIME [epoch: 8.15 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09087520556980562		[learning rate: 5.6142e-05]
		[batch 20/20] avg loss: 0.08230147983534383		[learning rate: 5.604e-05]
	Learning Rate: 5.60403e-05
	LOSS [training: 0.08658834270257473 | validation: 0.09695480934323061]
	TIME [epoch: 8.14 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08402079152233133		[learning rate: 5.5938e-05]
		[batch 20/20] avg loss: 0.08946534989879297		[learning rate: 5.5837e-05]
	Learning Rate: 5.58369e-05
	LOSS [training: 0.08674307071056217 | validation: 0.10320692369882863]
	TIME [epoch: 8.13 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09256348366938892		[learning rate: 5.5735e-05]
		[batch 20/20] avg loss: 0.08868199009379797		[learning rate: 5.5634e-05]
	Learning Rate: 5.56342e-05
	LOSS [training: 0.09062273688159345 | validation: 0.10379777389009313]
	TIME [epoch: 8.15 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08113589376153335		[learning rate: 5.5533e-05]
		[batch 20/20] avg loss: 0.08435829547508808		[learning rate: 5.5432e-05]
	Learning Rate: 5.54323e-05
	LOSS [training: 0.08274709461831072 | validation: 0.102423147869023]
	TIME [epoch: 8.17 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09499861350661679		[learning rate: 5.5332e-05]
		[batch 20/20] avg loss: 0.08085168714076597		[learning rate: 5.5231e-05]
	Learning Rate: 5.52312e-05
	LOSS [training: 0.08792515032369139 | validation: 0.10165625175128751]
	TIME [epoch: 8.16 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0913575386830986		[learning rate: 5.5131e-05]
		[batch 20/20] avg loss: 0.08862711007920566		[learning rate: 5.5031e-05]
	Learning Rate: 5.50307e-05
	LOSS [training: 0.08999232438115212 | validation: 0.11181759486883959]
	TIME [epoch: 8.14 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0824766076340419		[learning rate: 5.4931e-05]
		[batch 20/20] avg loss: 0.1053857558268027		[learning rate: 5.4831e-05]
	Learning Rate: 5.4831e-05
	LOSS [training: 0.0939311817304223 | validation: 0.0960083563943007]
	TIME [epoch: 8.18 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10090809937574596		[learning rate: 5.4731e-05]
		[batch 20/20] avg loss: 0.07426233532669678		[learning rate: 5.4632e-05]
	Learning Rate: 5.4632e-05
	LOSS [training: 0.08758521735122135 | validation: 0.1007041172977848]
	TIME [epoch: 8.14 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08136908880869145		[learning rate: 5.4533e-05]
		[batch 20/20] avg loss: 0.09853223814270985		[learning rate: 5.4434e-05]
	Learning Rate: 5.44338e-05
	LOSS [training: 0.08995066347570066 | validation: 0.11247797481214553]
	TIME [epoch: 8.15 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1011703281103861		[learning rate: 5.4335e-05]
		[batch 20/20] avg loss: 0.10509536737427239		[learning rate: 5.4236e-05]
	Learning Rate: 5.42362e-05
	LOSS [training: 0.10313284774232927 | validation: 0.10563033605617332]
	TIME [epoch: 8.15 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0830611961729736		[learning rate: 5.4138e-05]
		[batch 20/20] avg loss: 0.09988161857174432		[learning rate: 5.4039e-05]
	Learning Rate: 5.40394e-05
	LOSS [training: 0.09147140737235895 | validation: 0.10081632786244035]
	TIME [epoch: 8.13 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09561567947129682		[learning rate: 5.3941e-05]
		[batch 20/20] avg loss: 0.09007501475025517		[learning rate: 5.3843e-05]
	Learning Rate: 5.38433e-05
	LOSS [training: 0.092845347110776 | validation: 0.09939909035033671]
	TIME [epoch: 8.13 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07905149596657447		[learning rate: 5.3746e-05]
		[batch 20/20] avg loss: 0.09558593742767893		[learning rate: 5.3648e-05]
	Learning Rate: 5.36479e-05
	LOSS [training: 0.0873187166971267 | validation: 0.11456032555569617]
	TIME [epoch: 8.13 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08542690912212132		[learning rate: 5.355e-05]
		[batch 20/20] avg loss: 0.08471191295278936		[learning rate: 5.3453e-05]
	Learning Rate: 5.34532e-05
	LOSS [training: 0.08506941103745534 | validation: 0.10966352199256979]
	TIME [epoch: 8.2 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08710312800962688		[learning rate: 5.3356e-05]
		[batch 20/20] avg loss: 0.08964294485671241		[learning rate: 5.3259e-05]
	Learning Rate: 5.32592e-05
	LOSS [training: 0.08837303643316965 | validation: 0.10326724513727074]
	TIME [epoch: 8.15 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08562269263379618		[learning rate: 5.3162e-05]
		[batch 20/20] avg loss: 0.11259456431061562		[learning rate: 5.3066e-05]
	Learning Rate: 5.30659e-05
	LOSS [training: 0.09910862847220592 | validation: 0.1281328475450261]
	TIME [epoch: 8.14 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09817197539962175		[learning rate: 5.297e-05]
		[batch 20/20] avg loss: 0.09382758999396752		[learning rate: 5.2873e-05]
	Learning Rate: 5.28734e-05
	LOSS [training: 0.09599978269679466 | validation: 0.1147852800131047]
	TIME [epoch: 8.14 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08804693011625779		[learning rate: 5.2777e-05]
		[batch 20/20] avg loss: 0.09146209821871133		[learning rate: 5.2681e-05]
	Learning Rate: 5.26815e-05
	LOSS [training: 0.08975451416748456 | validation: 0.10168545535490757]
	TIME [epoch: 8.2 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08824944901695513		[learning rate: 5.2586e-05]
		[batch 20/20] avg loss: 0.08354088779694291		[learning rate: 5.249e-05]
	Learning Rate: 5.24903e-05
	LOSS [training: 0.08589516840694902 | validation: 0.10134353485055224]
	TIME [epoch: 8.14 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0942582192520507		[learning rate: 5.2395e-05]
		[batch 20/20] avg loss: 0.09221147185483078		[learning rate: 5.23e-05]
	Learning Rate: 5.22998e-05
	LOSS [training: 0.09323484555344073 | validation: 0.10576244611300631]
	TIME [epoch: 8.13 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07972307296123185		[learning rate: 5.2205e-05]
		[batch 20/20] avg loss: 0.10106128624825454		[learning rate: 5.211e-05]
	Learning Rate: 5.211e-05
	LOSS [training: 0.09039217960474322 | validation: 0.09260691918603445]
	TIME [epoch: 8.13 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08475367636531958		[learning rate: 5.2015e-05]
		[batch 20/20] avg loss: 0.09415250861086201		[learning rate: 5.1921e-05]
	Learning Rate: 5.19209e-05
	LOSS [training: 0.08945309248809077 | validation: 0.1128616356899287]
	TIME [epoch: 8.14 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08475762501900033		[learning rate: 5.1827e-05]
		[batch 20/20] avg loss: 0.09355703603725461		[learning rate: 5.1732e-05]
	Learning Rate: 5.17325e-05
	LOSS [training: 0.08915733052812747 | validation: 0.11405697406670885]
	TIME [epoch: 8.14 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09031302540514181		[learning rate: 5.1639e-05]
		[batch 20/20] avg loss: 0.08655858991079267		[learning rate: 5.1545e-05]
	Learning Rate: 5.15447e-05
	LOSS [training: 0.08843580765796724 | validation: 0.10782653284750064]
	TIME [epoch: 8.13 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08066158998854403		[learning rate: 5.1451e-05]
		[batch 20/20] avg loss: 0.09787086536109427		[learning rate: 5.1358e-05]
	Learning Rate: 5.13577e-05
	LOSS [training: 0.08926622767481915 | validation: 0.09758267033548776]
	TIME [epoch: 8.13 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06925881566135925		[learning rate: 5.1264e-05]
		[batch 20/20] avg loss: 0.1026270059102536		[learning rate: 5.1171e-05]
	Learning Rate: 5.11713e-05
	LOSS [training: 0.08594291078580643 | validation: 0.10303863884661221]
	TIME [epoch: 8.13 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09323509940067912		[learning rate: 5.1078e-05]
		[batch 20/20] avg loss: 0.10248126763960537		[learning rate: 5.0986e-05]
	Learning Rate: 5.09856e-05
	LOSS [training: 0.09785818352014224 | validation: 0.1137044227259034]
	TIME [epoch: 8.2 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09588672458871576		[learning rate: 5.0893e-05]
		[batch 20/20] avg loss: 0.08482621778215207		[learning rate: 5.0801e-05]
	Learning Rate: 5.08006e-05
	LOSS [training: 0.09035647118543391 | validation: 0.10664234714149316]
	TIME [epoch: 8.15 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09515187728138988		[learning rate: 5.0708e-05]
		[batch 20/20] avg loss: 0.08411135495777616		[learning rate: 5.0616e-05]
	Learning Rate: 5.06162e-05
	LOSS [training: 0.08963161611958301 | validation: 0.11054858858816559]
	TIME [epoch: 8.13 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08851212606531515		[learning rate: 5.0524e-05]
		[batch 20/20] avg loss: 0.09160097075752391		[learning rate: 5.0433e-05]
	Learning Rate: 5.04325e-05
	LOSS [training: 0.09005654841141952 | validation: 0.10374039330316866]
	TIME [epoch: 8.14 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08415574518435423		[learning rate: 5.0341e-05]
		[batch 20/20] avg loss: 0.0948966953927937		[learning rate: 5.0249e-05]
	Learning Rate: 5.02495e-05
	LOSS [training: 0.08952622028857396 | validation: 0.10660557100676585]
	TIME [epoch: 8.19 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08540734201759732		[learning rate: 5.0158e-05]
		[batch 20/20] avg loss: 0.08508021234788257		[learning rate: 5.0067e-05]
	Learning Rate: 5.00671e-05
	LOSS [training: 0.08524377718273994 | validation: 0.107598050946288]
	TIME [epoch: 8.15 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08625797164582487		[learning rate: 4.9976e-05]
		[batch 20/20] avg loss: 0.09119727996150126		[learning rate: 4.9885e-05]
	Learning Rate: 4.98854e-05
	LOSS [training: 0.08872762580366307 | validation: 0.10559482515776653]
	TIME [epoch: 8.13 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09197227224976809		[learning rate: 4.9795e-05]
		[batch 20/20] avg loss: 0.08502767483558356		[learning rate: 4.9704e-05]
	Learning Rate: 4.97044e-05
	LOSS [training: 0.08849997354267582 | validation: 0.10264185320442909]
	TIME [epoch: 8.13 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09519582377563805		[learning rate: 4.9614e-05]
		[batch 20/20] avg loss: 0.08373998298471202		[learning rate: 4.9524e-05]
	Learning Rate: 4.9524e-05
	LOSS [training: 0.08946790338017505 | validation: 0.10213983733568303]
	TIME [epoch: 8.13 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09028014145520892		[learning rate: 4.9434e-05]
		[batch 20/20] avg loss: 0.07711143932449012		[learning rate: 4.9344e-05]
	Learning Rate: 4.93443e-05
	LOSS [training: 0.08369579038984955 | validation: 0.10775619200631324]
	TIME [epoch: 8.15 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09194053879139766		[learning rate: 4.9255e-05]
		[batch 20/20] avg loss: 0.07927913151341738		[learning rate: 4.9165e-05]
	Learning Rate: 4.91652e-05
	LOSS [training: 0.08560983515240754 | validation: 0.10581916710543735]
	TIME [epoch: 8.18 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0868557940660897		[learning rate: 4.9076e-05]
		[batch 20/20] avg loss: 0.08291655007667502		[learning rate: 4.8987e-05]
	Learning Rate: 4.89868e-05
	LOSS [training: 0.08488617207138235 | validation: 0.10290757997928729]
	TIME [epoch: 8.14 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09550973512477487		[learning rate: 4.8898e-05]
		[batch 20/20] avg loss: 0.08141060140503653		[learning rate: 4.8809e-05]
	Learning Rate: 4.8809e-05
	LOSS [training: 0.0884601682649057 | validation: 0.10498779008038452]
	TIME [epoch: 8.14 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07379283287192925		[learning rate: 4.872e-05]
		[batch 20/20] avg loss: 0.10393228870227278		[learning rate: 4.8632e-05]
	Learning Rate: 4.86319e-05
	LOSS [training: 0.08886256078710103 | validation: 0.10281181361907876]
	TIME [epoch: 8.15 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08040198492962215		[learning rate: 4.8544e-05]
		[batch 20/20] avg loss: 0.09467115841176706		[learning rate: 4.8455e-05]
	Learning Rate: 4.84554e-05
	LOSS [training: 0.0875365716706946 | validation: 0.10610880285949496]
	TIME [epoch: 8.2 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08008813357884625		[learning rate: 4.8367e-05]
		[batch 20/20] avg loss: 0.09001058177890395		[learning rate: 4.828e-05]
	Learning Rate: 4.82795e-05
	LOSS [training: 0.08504935767887511 | validation: 0.10094768877167581]
	TIME [epoch: 8.14 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08868479679459329		[learning rate: 4.8192e-05]
		[batch 20/20] avg loss: 0.0846021253221657		[learning rate: 4.8104e-05]
	Learning Rate: 4.81043e-05
	LOSS [training: 0.08664346105837947 | validation: 0.1102285357217615]
	TIME [epoch: 8.13 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08152143930581784		[learning rate: 4.8017e-05]
		[batch 20/20] avg loss: 0.09443138817688983		[learning rate: 4.793e-05]
	Learning Rate: 4.79298e-05
	LOSS [training: 0.08797641374135381 | validation: 0.09605518481483362]
	TIME [epoch: 8.13 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08636556251620266		[learning rate: 4.7843e-05]
		[batch 20/20] avg loss: 0.08438654843376112		[learning rate: 4.7756e-05]
	Learning Rate: 4.77558e-05
	LOSS [training: 0.08537605547498188 | validation: 0.11738994276572688]
	TIME [epoch: 8.15 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08554959571055994		[learning rate: 4.7669e-05]
		[batch 20/20] avg loss: 0.08875816980127929		[learning rate: 4.7583e-05]
	Learning Rate: 4.75825e-05
	LOSS [training: 0.08715388275591963 | validation: 0.11862151197242826]
	TIME [epoch: 8.13 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08409372605715915		[learning rate: 4.7496e-05]
		[batch 20/20] avg loss: 0.09292911287435326		[learning rate: 4.741e-05]
	Learning Rate: 4.74098e-05
	LOSS [training: 0.0885114194657562 | validation: 0.10841955035657855]
	TIME [epoch: 8.13 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08369742076375493		[learning rate: 4.7324e-05]
		[batch 20/20] avg loss: 0.09195679681850473		[learning rate: 4.7238e-05]
	Learning Rate: 4.72378e-05
	LOSS [training: 0.08782710879112984 | validation: 0.11302828584346282]
	TIME [epoch: 8.13 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08894218810307607		[learning rate: 4.7152e-05]
		[batch 20/20] avg loss: 0.09272598633239573		[learning rate: 4.7066e-05]
	Learning Rate: 4.70664e-05
	LOSS [training: 0.09083408721773589 | validation: 0.1028804560668966]
	TIME [epoch: 8.16 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07792407860933966		[learning rate: 4.6981e-05]
		[batch 20/20] avg loss: 0.09175184199980872		[learning rate: 4.6896e-05]
	Learning Rate: 4.68955e-05
	LOSS [training: 0.08483796030457419 | validation: 0.0990681161310356]
	TIME [epoch: 8.18 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07433705785266184		[learning rate: 4.681e-05]
		[batch 20/20] avg loss: 0.0898344245148692		[learning rate: 4.6725e-05]
	Learning Rate: 4.67254e-05
	LOSS [training: 0.08208574118376552 | validation: 0.09815475545083979]
	TIME [epoch: 8.14 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08568970850356182		[learning rate: 4.6641e-05]
		[batch 20/20] avg loss: 0.08690862683799971		[learning rate: 4.6556e-05]
	Learning Rate: 4.65558e-05
	LOSS [training: 0.08629916767078077 | validation: 0.10854065828897638]
	TIME [epoch: 8.14 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08097017651135233		[learning rate: 4.6471e-05]
		[batch 20/20] avg loss: 0.0923338366957133		[learning rate: 4.6387e-05]
	Learning Rate: 4.63868e-05
	LOSS [training: 0.0866520066035328 | validation: 0.10519141514419646]
	TIME [epoch: 8.16 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08899369469380482		[learning rate: 4.6303e-05]
		[batch 20/20] avg loss: 0.08484944472461967		[learning rate: 4.6219e-05]
	Learning Rate: 4.62185e-05
	LOSS [training: 0.08692156970921223 | validation: 0.09934377976843985]
	TIME [epoch: 8.18 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08417572512602353		[learning rate: 4.6135e-05]
		[batch 20/20] avg loss: 0.08729612303621345		[learning rate: 4.6051e-05]
	Learning Rate: 4.60508e-05
	LOSS [training: 0.08573592408111849 | validation: 0.11550036354281258]
	TIME [epoch: 8.13 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09094312618618003		[learning rate: 4.5967e-05]
		[batch 20/20] avg loss: 0.08927742765229843		[learning rate: 4.5884e-05]
	Learning Rate: 4.58836e-05
	LOSS [training: 0.09011027691923923 | validation: 0.1001172002589696]
	TIME [epoch: 8.12 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08797052388973295		[learning rate: 4.58e-05]
		[batch 20/20] avg loss: 0.08517429468609118		[learning rate: 4.5717e-05]
	Learning Rate: 4.57171e-05
	LOSS [training: 0.08657240928791206 | validation: 0.10427124895622489]
	TIME [epoch: 8.13 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09247250797322368		[learning rate: 4.5634e-05]
		[batch 20/20] avg loss: 0.08495981395776		[learning rate: 4.5551e-05]
	Learning Rate: 4.55512e-05
	LOSS [training: 0.08871616096549183 | validation: 0.10945511512287805]
	TIME [epoch: 8.14 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09591067857630789		[learning rate: 4.5468e-05]
		[batch 20/20] avg loss: 0.09700872179570363		[learning rate: 4.5386e-05]
	Learning Rate: 4.53859e-05
	LOSS [training: 0.09645970018600575 | validation: 0.11189944454947029]
	TIME [epoch: 8.13 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08823993633466917		[learning rate: 4.5303e-05]
		[batch 20/20] avg loss: 0.07953734684098732		[learning rate: 4.5221e-05]
	Learning Rate: 4.52212e-05
	LOSS [training: 0.08388864158782824 | validation: 0.09587167558506615]
	TIME [epoch: 8.12 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08051052333303355		[learning rate: 4.5139e-05]
		[batch 20/20] avg loss: 0.08773633677357462		[learning rate: 4.5057e-05]
	Learning Rate: 4.50571e-05
	LOSS [training: 0.08412343005330408 | validation: 0.10491622637478934]
	TIME [epoch: 8.14 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09056558005466545		[learning rate: 4.4975e-05]
		[batch 20/20] avg loss: 0.07952927753061642		[learning rate: 4.4894e-05]
	Learning Rate: 4.48936e-05
	LOSS [training: 0.08504742879264092 | validation: 0.10108324718500286]
	TIME [epoch: 8.17 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08694178769757059		[learning rate: 4.4812e-05]
		[batch 20/20] avg loss: 0.09024677604828861		[learning rate: 4.4731e-05]
	Learning Rate: 4.47307e-05
	LOSS [training: 0.08859428187292959 | validation: 0.10069121159631027]
	TIME [epoch: 8.15 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09007886061045126		[learning rate: 4.4649e-05]
		[batch 20/20] avg loss: 0.09421740099476747		[learning rate: 4.4568e-05]
	Learning Rate: 4.45683e-05
	LOSS [training: 0.09214813080260935 | validation: 0.10794848697910443]
	TIME [epoch: 8.13 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09678405585085137		[learning rate: 4.4487e-05]
		[batch 20/20] avg loss: 0.08598335571091449		[learning rate: 4.4407e-05]
	Learning Rate: 4.44066e-05
	LOSS [training: 0.09138370578088292 | validation: 0.10726660124666926]
	TIME [epoch: 8.16 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0901366718249352		[learning rate: 4.4326e-05]
		[batch 20/20] avg loss: 0.08779240537059499		[learning rate: 4.4245e-05]
	Learning Rate: 4.42454e-05
	LOSS [training: 0.08896453859776508 | validation: 0.10240834321698819]
	TIME [epoch: 8.15 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08824912985971237		[learning rate: 4.4165e-05]
		[batch 20/20] avg loss: 0.0869470822443679		[learning rate: 4.4085e-05]
	Learning Rate: 4.40849e-05
	LOSS [training: 0.08759810605204013 | validation: 0.10987303235505508]
	TIME [epoch: 8.15 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08796100972536242		[learning rate: 4.4005e-05]
		[batch 20/20] avg loss: 0.09152979982435053		[learning rate: 4.3925e-05]
	Learning Rate: 4.39249e-05
	LOSS [training: 0.08974540477485646 | validation: 0.10453565251887045]
	TIME [epoch: 8.13 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09375303261846224		[learning rate: 4.3845e-05]
		[batch 20/20] avg loss: 0.08408660855734344		[learning rate: 4.3765e-05]
	Learning Rate: 4.37655e-05
	LOSS [training: 0.08891982058790282 | validation: 0.1056568245127761]
	TIME [epoch: 8.13 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08328793827747517		[learning rate: 4.3686e-05]
		[batch 20/20] avg loss: 0.09970435177049433		[learning rate: 4.3607e-05]
	Learning Rate: 4.36067e-05
	LOSS [training: 0.09149614502398476 | validation: 0.12371036827598852]
	TIME [epoch: 8.12 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0963310013041858		[learning rate: 4.3527e-05]
		[batch 20/20] avg loss: 0.08875379366498105		[learning rate: 4.3448e-05]
	Learning Rate: 4.34484e-05
	LOSS [training: 0.09254239748458341 | validation: 0.10485491072485428]
	TIME [epoch: 8.14 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0926871037270609		[learning rate: 4.3369e-05]
		[batch 20/20] avg loss: 0.08634018275961203		[learning rate: 4.3291e-05]
	Learning Rate: 4.32907e-05
	LOSS [training: 0.08951364324333647 | validation: 0.10651767501095954]
	TIME [epoch: 8.13 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09473037881531106		[learning rate: 4.3212e-05]
		[batch 20/20] avg loss: 0.07851823551412522		[learning rate: 4.3134e-05]
	Learning Rate: 4.31336e-05
	LOSS [training: 0.08662430716471814 | validation: 0.10964210651475212]
	TIME [epoch: 8.17 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09069861554938645		[learning rate: 4.3055e-05]
		[batch 20/20] avg loss: 0.08432262100624437		[learning rate: 4.2977e-05]
	Learning Rate: 4.29771e-05
	LOSS [training: 0.08751061827781541 | validation: 0.10883566051269633]
	TIME [epoch: 8.13 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08897828209399088		[learning rate: 4.2899e-05]
		[batch 20/20] avg loss: 0.08745974801077588		[learning rate: 4.2821e-05]
	Learning Rate: 4.28211e-05
	LOSS [training: 0.08821901505238337 | validation: 0.10215221595998085]
	TIME [epoch: 8.14 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08796868336771067		[learning rate: 4.2743e-05]
		[batch 20/20] avg loss: 0.09375796429093067		[learning rate: 4.2666e-05]
	Learning Rate: 4.26657e-05
	LOSS [training: 0.09086332382932068 | validation: 0.10746136745912108]
	TIME [epoch: 8.16 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09292944783606177		[learning rate: 4.2588e-05]
		[batch 20/20] avg loss: 0.09154285878726096		[learning rate: 4.2511e-05]
	Learning Rate: 4.25109e-05
	LOSS [training: 0.09223615331166132 | validation: 0.10453513640772022]
	TIME [epoch: 8.18 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09653658194281932		[learning rate: 4.2434e-05]
		[batch 20/20] avg loss: 0.09445063967966569		[learning rate: 4.2357e-05]
	Learning Rate: 4.23566e-05
	LOSS [training: 0.09549361081124251 | validation: 0.10269532507463024]
	TIME [epoch: 8.13 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09731308475977038		[learning rate: 4.228e-05]
		[batch 20/20] avg loss: 0.082777499701056		[learning rate: 4.2203e-05]
	Learning Rate: 4.22029e-05
	LOSS [training: 0.09004529223041316 | validation: 0.09145150934542073]
	TIME [epoch: 8.12 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08511683919332716		[learning rate: 4.2126e-05]
		[batch 20/20] avg loss: 0.08818660837513086		[learning rate: 4.205e-05]
	Learning Rate: 4.20497e-05
	LOSS [training: 0.086651723784229 | validation: 0.09559224799305187]
	TIME [epoch: 8.14 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09083338607410182		[learning rate: 4.1973e-05]
		[batch 20/20] avg loss: 0.09276020151641315		[learning rate: 4.1897e-05]
	Learning Rate: 4.18971e-05
	LOSS [training: 0.09179679379525749 | validation: 0.09136010282951643]
	TIME [epoch: 8.13 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0963371718463414		[learning rate: 4.1821e-05]
		[batch 20/20] avg loss: 0.07563074336630447		[learning rate: 4.1745e-05]
	Learning Rate: 4.17451e-05
	LOSS [training: 0.08598395760632295 | validation: 0.10007423621080108]
	TIME [epoch: 8.12 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09040243380356411		[learning rate: 4.1669e-05]
		[batch 20/20] avg loss: 0.08577458159569916		[learning rate: 4.1594e-05]
	Learning Rate: 4.15936e-05
	LOSS [training: 0.08808850769963164 | validation: 0.10628152861936449]
	TIME [epoch: 8.12 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08578458313273193		[learning rate: 4.1518e-05]
		[batch 20/20] avg loss: 0.09232616970274195		[learning rate: 4.1443e-05]
	Learning Rate: 4.14426e-05
	LOSS [training: 0.08905537641773698 | validation: 0.10606084504622737]
	TIME [epoch: 8.12 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08511735251503036		[learning rate: 4.1367e-05]
		[batch 20/20] avg loss: 0.09084131873059215		[learning rate: 4.1292e-05]
	Learning Rate: 4.12922e-05
	LOSS [training: 0.08797933562281127 | validation: 0.10099833487591078]
	TIME [epoch: 8.15 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08018297757095791		[learning rate: 4.1217e-05]
		[batch 20/20] avg loss: 0.09159779523633903		[learning rate: 4.1142e-05]
	Learning Rate: 4.11424e-05
	LOSS [training: 0.08589038640364846 | validation: 0.10849289575054644]
	TIME [epoch: 8.17 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09873998932879688		[learning rate: 4.1068e-05]
		[batch 20/20] avg loss: 0.07709830870186589		[learning rate: 4.0993e-05]
	Learning Rate: 4.09931e-05
	LOSS [training: 0.08791914901533139 | validation: 0.10351611092324817]
	TIME [epoch: 8.13 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08737661662964749		[learning rate: 4.0919e-05]
		[batch 20/20] avg loss: 0.08704049456169535		[learning rate: 4.0844e-05]
	Learning Rate: 4.08443e-05
	LOSS [training: 0.08720855559567144 | validation: 0.10362884350229032]
	TIME [epoch: 8.14 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07815908423512852		[learning rate: 4.077e-05]
		[batch 20/20] avg loss: 0.10045092803484043		[learning rate: 4.0696e-05]
	Learning Rate: 4.06961e-05
	LOSS [training: 0.08930500613498449 | validation: 0.09739756065838691]
	TIME [epoch: 8.16 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09315578193016424		[learning rate: 4.0622e-05]
		[batch 20/20] avg loss: 0.08596585676185438		[learning rate: 4.0548e-05]
	Learning Rate: 4.05484e-05
	LOSS [training: 0.08956081934600932 | validation: 0.10313833998800037]
	TIME [epoch: 8.19 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0761776482810952		[learning rate: 4.0475e-05]
		[batch 20/20] avg loss: 0.0941003074071179		[learning rate: 4.0401e-05]
	Learning Rate: 4.04012e-05
	LOSS [training: 0.08513897784410654 | validation: 0.0975648821962044]
	TIME [epoch: 8.12 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08601047124712677		[learning rate: 4.0328e-05]
		[batch 20/20] avg loss: 0.0843865901315682		[learning rate: 4.0255e-05]
	Learning Rate: 4.02546e-05
	LOSS [training: 0.08519853068934746 | validation: 0.10752147116569154]
	TIME [epoch: 8.12 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09085994758405255		[learning rate: 4.0182e-05]
		[batch 20/20] avg loss: 0.09350996166562228		[learning rate: 4.0109e-05]
	Learning Rate: 4.01085e-05
	LOSS [training: 0.09218495462483742 | validation: 0.11227120358492218]
	TIME [epoch: 8.13 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08545759126628556		[learning rate: 4.0036e-05]
		[batch 20/20] avg loss: 0.09569761863213562		[learning rate: 3.9963e-05]
	Learning Rate: 3.9963e-05
	LOSS [training: 0.09057760494921058 | validation: 0.1017627907542945]
	TIME [epoch: 8.14 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10163939251481136		[learning rate: 3.989e-05]
		[batch 20/20] avg loss: 0.10489659888991212		[learning rate: 3.9818e-05]
	Learning Rate: 3.9818e-05
	LOSS [training: 0.10326799570236171 | validation: 0.1226443893986817]
	TIME [epoch: 8.12 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10243758420404037		[learning rate: 3.9746e-05]
		[batch 20/20] avg loss: 0.08328384500684463		[learning rate: 3.9673e-05]
	Learning Rate: 3.96735e-05
	LOSS [training: 0.09286071460544248 | validation: 0.11506633915121901]
	TIME [epoch: 8.13 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08281015831972006		[learning rate: 3.9601e-05]
		[batch 20/20] avg loss: 0.08764844900358296		[learning rate: 3.9529e-05]
	Learning Rate: 3.95295e-05
	LOSS [training: 0.0852293036616515 | validation: 0.10720854388709365]
	TIME [epoch: 8.12 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0833742170325888		[learning rate: 3.9458e-05]
		[batch 20/20] avg loss: 0.08553979130987957		[learning rate: 3.9386e-05]
	Learning Rate: 3.9386e-05
	LOSS [training: 0.08445700417123417 | validation: 0.10155721336159503]
	TIME [epoch: 8.15 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0816094495763642		[learning rate: 3.9315e-05]
		[batch 20/20] avg loss: 0.08856545347789664		[learning rate: 3.9243e-05]
	Learning Rate: 3.92431e-05
	LOSS [training: 0.0850874515271304 | validation: 0.1068704456447043]
	TIME [epoch: 8.18 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09289532946963898		[learning rate: 3.9172e-05]
		[batch 20/20] avg loss: 0.08182280190845428		[learning rate: 3.9101e-05]
	Learning Rate: 3.91007e-05
	LOSS [training: 0.08735906568904664 | validation: 0.09725197810601593]
	TIME [epoch: 8.13 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09188142473924643		[learning rate: 3.903e-05]
		[batch 20/20] avg loss: 0.08237315736080245		[learning rate: 3.8959e-05]
	Learning Rate: 3.89588e-05
	LOSS [training: 0.08712729105002444 | validation: 0.10578398545775669]
	TIME [epoch: 8.13 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0865065109153125		[learning rate: 3.8888e-05]
		[batch 20/20] avg loss: 0.08379835633425513		[learning rate: 3.8817e-05]
	Learning Rate: 3.88174e-05
	LOSS [training: 0.08515243362478378 | validation: 0.10270440156719482]
	TIME [epoch: 8.14 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08501992577454465		[learning rate: 3.8747e-05]
		[batch 20/20] avg loss: 0.08692327018787362		[learning rate: 3.8677e-05]
	Learning Rate: 3.86765e-05
	LOSS [training: 0.08597159798120912 | validation: 0.10337533259547449]
	TIME [epoch: 8.2 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08059215229924724		[learning rate: 3.8606e-05]
		[batch 20/20] avg loss: 0.10703797848694722		[learning rate: 3.8536e-05]
	Learning Rate: 3.85362e-05
	LOSS [training: 0.09381506539309722 | validation: 0.13725142256366102]
	TIME [epoch: 8.12 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10672999223495024		[learning rate: 3.8466e-05]
		[batch 20/20] avg loss: 0.09343320551419734		[learning rate: 3.8396e-05]
	Learning Rate: 3.83963e-05
	LOSS [training: 0.10008159887457382 | validation: 0.10701142282559763]
	TIME [epoch: 8.12 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07773785233858103		[learning rate: 3.8327e-05]
		[batch 20/20] avg loss: 0.08539732497498813		[learning rate: 3.8257e-05]
	Learning Rate: 3.8257e-05
	LOSS [training: 0.08156758865678458 | validation: 0.10104640161515474]
	TIME [epoch: 8.12 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09267086331635113		[learning rate: 3.8187e-05]
		[batch 20/20] avg loss: 0.08042240136784037		[learning rate: 3.8118e-05]
	Learning Rate: 3.81181e-05
	LOSS [training: 0.08654663234209575 | validation: 0.10013806916490381]
	TIME [epoch: 8.15 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08449443538566082		[learning rate: 3.8049e-05]
		[batch 20/20] avg loss: 0.09171859517826417		[learning rate: 3.798e-05]
	Learning Rate: 3.79798e-05
	LOSS [training: 0.0881065152819625 | validation: 0.10356362146637216]
	TIME [epoch: 8.13 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08631540143355476		[learning rate: 3.7911e-05]
		[batch 20/20] avg loss: 0.08894027925895234		[learning rate: 3.7842e-05]
	Learning Rate: 3.7842e-05
	LOSS [training: 0.08762784034625357 | validation: 0.0953259250238705]
	TIME [epoch: 8.12 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09709344019705043		[learning rate: 3.7773e-05]
		[batch 20/20] avg loss: 0.075392663018507		[learning rate: 3.7705e-05]
	Learning Rate: 3.77046e-05
	LOSS [training: 0.08624305160777875 | validation: 0.09651894281082277]
	TIME [epoch: 8.13 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07837976007929479		[learning rate: 3.7636e-05]
		[batch 20/20] avg loss: 0.09924480001230411		[learning rate: 3.7568e-05]
	Learning Rate: 3.75678e-05
	LOSS [training: 0.08881228004579947 | validation: 0.11349835981856175]
	TIME [epoch: 8.14 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1013271081651624		[learning rate: 3.75e-05]
		[batch 20/20] avg loss: 0.08639997093804948		[learning rate: 3.7431e-05]
	Learning Rate: 3.74315e-05
	LOSS [training: 0.09386353955160591 | validation: 0.10280890268096968]
	TIME [epoch: 8.17 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08251675098324619		[learning rate: 3.7363e-05]
		[batch 20/20] avg loss: 0.08992046370719858		[learning rate: 3.7296e-05]
	Learning Rate: 3.72956e-05
	LOSS [training: 0.08621860734522238 | validation: 0.1061849842556592]
	TIME [epoch: 8.13 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0862520702254233		[learning rate: 3.7228e-05]
		[batch 20/20] avg loss: 0.08229602721024844		[learning rate: 3.716e-05]
	Learning Rate: 3.71603e-05
	LOSS [training: 0.08427404871783588 | validation: 0.09727928810903724]
	TIME [epoch: 8.15 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08339816153294563		[learning rate: 3.7093e-05]
		[batch 20/20] avg loss: 0.09089625372518319		[learning rate: 3.7025e-05]
	Learning Rate: 3.70254e-05
	LOSS [training: 0.0871472076290644 | validation: 0.11770466612377137]
	TIME [epoch: 8.16 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07557728300694425		[learning rate: 3.6958e-05]
		[batch 20/20] avg loss: 0.09270112841574578		[learning rate: 3.6891e-05]
	Learning Rate: 3.68911e-05
	LOSS [training: 0.08413920571134502 | validation: 0.0993288718321764]
	TIME [epoch: 8.18 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09681194354997505		[learning rate: 3.6824e-05]
		[batch 20/20] avg loss: 0.08424787163811377		[learning rate: 3.6757e-05]
	Learning Rate: 3.67572e-05
	LOSS [training: 0.0905299075940444 | validation: 0.10579321532961165]
	TIME [epoch: 8.12 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09777305501225583		[learning rate: 3.669e-05]
		[batch 20/20] avg loss: 0.09568723005974178		[learning rate: 3.6624e-05]
	Learning Rate: 3.66238e-05
	LOSS [training: 0.09673014253599879 | validation: 0.1138316117819919]
	TIME [epoch: 8.13 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08444734731292483		[learning rate: 3.6557e-05]
		[batch 20/20] avg loss: 0.0978009520210503		[learning rate: 3.6491e-05]
	Learning Rate: 3.64909e-05
	LOSS [training: 0.09112414966698754 | validation: 0.11353853204321487]
	TIME [epoch: 8.12 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08048071793719287		[learning rate: 3.6425e-05]
		[batch 20/20] avg loss: 0.10944325008866777		[learning rate: 3.6358e-05]
	Learning Rate: 3.63584e-05
	LOSS [training: 0.09496198401293035 | validation: 0.1098614905054758]
	TIME [epoch: 8.15 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08722071512673812		[learning rate: 3.6292e-05]
		[batch 20/20] avg loss: 0.09143099767395217		[learning rate: 3.6226e-05]
	Learning Rate: 3.62265e-05
	LOSS [training: 0.08932585640034515 | validation: 0.10496481444847269]
	TIME [epoch: 8.13 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09579080788869311		[learning rate: 3.6161e-05]
		[batch 20/20] avg loss: 0.08332717542163903		[learning rate: 3.6095e-05]
	Learning Rate: 3.6095e-05
	LOSS [training: 0.08955899165516608 | validation: 0.09289432357161945]
	TIME [epoch: 8.12 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08391346135311678		[learning rate: 3.6029e-05]
		[batch 20/20] avg loss: 0.09436522290875193		[learning rate: 3.5964e-05]
	Learning Rate: 3.5964e-05
	LOSS [training: 0.08913934213093437 | validation: 0.10336935793108443]
	TIME [epoch: 8.12 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08752419901775856		[learning rate: 3.5899e-05]
		[batch 20/20] avg loss: 0.08776675079511029		[learning rate: 3.5834e-05]
	Learning Rate: 3.58335e-05
	LOSS [training: 0.0876454749064344 | validation: 0.08877365336473932]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_1649.pth
	Model improved!!!
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09598547946758539		[learning rate: 3.5768e-05]
		[batch 20/20] avg loss: 0.08418282941161144		[learning rate: 3.5703e-05]
	Learning Rate: 3.57035e-05
	LOSS [training: 0.09008415443959841 | validation: 0.10039505423981826]
	TIME [epoch: 8.15 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0863403208939544		[learning rate: 3.5639e-05]
		[batch 20/20] avg loss: 0.09181229518341874		[learning rate: 3.5574e-05]
	Learning Rate: 3.55739e-05
	LOSS [training: 0.08907630803868657 | validation: 0.1020511503268623]
	TIME [epoch: 8.12 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09203731664614909		[learning rate: 3.5509e-05]
		[batch 20/20] avg loss: 0.08599205601323869		[learning rate: 3.5445e-05]
	Learning Rate: 3.54448e-05
	LOSS [training: 0.08901468632969387 | validation: 0.10723123448736369]
	TIME [epoch: 8.13 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10095728542700233		[learning rate: 3.538e-05]
		[batch 20/20] avg loss: 0.07643003006918186		[learning rate: 3.5316e-05]
	Learning Rate: 3.53162e-05
	LOSS [training: 0.0886936577480921 | validation: 0.09259474028234069]
	TIME [epoch: 8.14 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08675135118131624		[learning rate: 3.5252e-05]
		[batch 20/20] avg loss: 0.08832223393793007		[learning rate: 3.5188e-05]
	Learning Rate: 3.5188e-05
	LOSS [training: 0.08753679255962318 | validation: 0.09850470942259426]
	TIME [epoch: 8.19 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09829197927024753		[learning rate: 3.5124e-05]
		[batch 20/20] avg loss: 0.09256739821330925		[learning rate: 3.506e-05]
	Learning Rate: 3.50603e-05
	LOSS [training: 0.0954296887417784 | validation: 0.09728035176804035]
	TIME [epoch: 8.14 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0980504254058922		[learning rate: 3.4997e-05]
		[batch 20/20] avg loss: 0.0828772308885847		[learning rate: 3.4933e-05]
	Learning Rate: 3.49331e-05
	LOSS [training: 0.09046382814723844 | validation: 0.10732700946458124]
	TIME [epoch: 8.12 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09322002605051108		[learning rate: 3.487e-05]
		[batch 20/20] avg loss: 0.0939462612278249		[learning rate: 3.4806e-05]
	Learning Rate: 3.48063e-05
	LOSS [training: 0.09358314363916799 | validation: 0.1009829814373086]
	TIME [epoch: 8.14 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0928344952275893		[learning rate: 3.4743e-05]
		[batch 20/20] avg loss: 0.09244836213211767		[learning rate: 3.468e-05]
	Learning Rate: 3.468e-05
	LOSS [training: 0.09264142867985348 | validation: 0.10648343229323204]
	TIME [epoch: 8.18 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09047024913379266		[learning rate: 3.4617e-05]
		[batch 20/20] avg loss: 0.10030398296870505		[learning rate: 3.4554e-05]
	Learning Rate: 3.45541e-05
	LOSS [training: 0.09538711605124886 | validation: 0.09563662775279609]
	TIME [epoch: 8.14 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09989276511651635		[learning rate: 3.4491e-05]
		[batch 20/20] avg loss: 0.08197512476794122		[learning rate: 3.4429e-05]
	Learning Rate: 3.44287e-05
	LOSS [training: 0.09093394494222878 | validation: 0.09684109263043451]
	TIME [epoch: 8.12 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09681691520872088		[learning rate: 3.4366e-05]
		[batch 20/20] avg loss: 0.08733633262336041		[learning rate: 3.4304e-05]
	Learning Rate: 3.43038e-05
	LOSS [training: 0.09207662391604063 | validation: 0.10660342917951325]
	TIME [epoch: 8.12 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07754283483984137		[learning rate: 3.4241e-05]
		[batch 20/20] avg loss: 0.09721402223777775		[learning rate: 3.4179e-05]
	Learning Rate: 3.41793e-05
	LOSS [training: 0.08737842853880959 | validation: 0.10151298135886543]
	TIME [epoch: 8.12 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0954750717599047		[learning rate: 3.4117e-05]
		[batch 20/20] avg loss: 0.07602349684867622		[learning rate: 3.4055e-05]
	Learning Rate: 3.40553e-05
	LOSS [training: 0.08574928430429046 | validation: 0.10724451349207621]
	TIME [epoch: 8.14 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08542992695546331		[learning rate: 3.3993e-05]
		[batch 20/20] avg loss: 0.09202679140183666		[learning rate: 3.3932e-05]
	Learning Rate: 3.39317e-05
	LOSS [training: 0.08872835917864999 | validation: 0.09825936774040069]
	TIME [epoch: 8.12 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09258244840875715		[learning rate: 3.387e-05]
		[batch 20/20] avg loss: 0.08190868107054158		[learning rate: 3.3809e-05]
	Learning Rate: 3.38085e-05
	LOSS [training: 0.08724556473964937 | validation: 0.10281731758670482]
	TIME [epoch: 8.11 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08787606240365184		[learning rate: 3.3747e-05]
		[batch 20/20] avg loss: 0.08341920716243187		[learning rate: 3.3686e-05]
	Learning Rate: 3.36858e-05
	LOSS [training: 0.08564763478304185 | validation: 0.10365174475169678]
	TIME [epoch: 8.13 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08539430827578251		[learning rate: 3.3625e-05]
		[batch 20/20] avg loss: 0.07844468397563338		[learning rate: 3.3564e-05]
	Learning Rate: 3.35636e-05
	LOSS [training: 0.08191949612570794 | validation: 0.09761836715221257]
	TIME [epoch: 8.17 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0825047024335352		[learning rate: 3.3503e-05]
		[batch 20/20] avg loss: 0.08680576268459031		[learning rate: 3.3442e-05]
	Learning Rate: 3.34418e-05
	LOSS [training: 0.08465523255906274 | validation: 0.09711854150436106]
	TIME [epoch: 8.13 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08346747315438845		[learning rate: 3.3381e-05]
		[batch 20/20] avg loss: 0.08953374348792056		[learning rate: 3.332e-05]
	Learning Rate: 3.33204e-05
	LOSS [training: 0.08650060832115451 | validation: 0.1065482279833406]
	TIME [epoch: 8.12 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08151618657725128		[learning rate: 3.326e-05]
		[batch 20/20] avg loss: 0.09436825126702819		[learning rate: 3.32e-05]
	Learning Rate: 3.31995e-05
	LOSS [training: 0.08794221892213974 | validation: 0.10166867268797963]
	TIME [epoch: 8.15 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09397392134048102		[learning rate: 3.3139e-05]
		[batch 20/20] avg loss: 0.08927334746742131		[learning rate: 3.3079e-05]
	Learning Rate: 3.3079e-05
	LOSS [training: 0.09162363440395113 | validation: 0.1021334056350051]
	TIME [epoch: 8.15 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0952335038148012		[learning rate: 3.3019e-05]
		[batch 20/20] avg loss: 0.07501180509480386		[learning rate: 3.2959e-05]
	Learning Rate: 3.2959e-05
	LOSS [training: 0.08512265445480253 | validation: 0.1006679195424825]
	TIME [epoch: 8.15 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08409559539655448		[learning rate: 3.2899e-05]
		[batch 20/20] avg loss: 0.09278365607922347		[learning rate: 3.2839e-05]
	Learning Rate: 3.28394e-05
	LOSS [training: 0.08843962573788898 | validation: 0.09249188880001644]
	TIME [epoch: 8.11 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08072306268934552		[learning rate: 3.278e-05]
		[batch 20/20] avg loss: 0.0924313219050017		[learning rate: 3.272e-05]
	Learning Rate: 3.27202e-05
	LOSS [training: 0.08657719229717362 | validation: 0.10596902775302189]
	TIME [epoch: 8.12 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10360863620193606		[learning rate: 3.2661e-05]
		[batch 20/20] avg loss: 0.08047478209496445		[learning rate: 3.2601e-05]
	Learning Rate: 3.26014e-05
	LOSS [training: 0.09204170914845025 | validation: 0.103003535041612]
	TIME [epoch: 8.11 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08680990549828241		[learning rate: 3.2542e-05]
		[batch 20/20] avg loss: 0.09285439176693408		[learning rate: 3.2483e-05]
	Learning Rate: 3.24831e-05
	LOSS [training: 0.08983214863260823 | validation: 0.10610386409776962]
	TIME [epoch: 8.14 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0810725993897293		[learning rate: 3.2424e-05]
		[batch 20/20] avg loss: 0.0920603553342442		[learning rate: 3.2365e-05]
	Learning Rate: 3.23652e-05
	LOSS [training: 0.08656647736198675 | validation: 0.10969508067082032]
	TIME [epoch: 8.13 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08869857988822111		[learning rate: 3.2306e-05]
		[batch 20/20] avg loss: 0.08552718622516185		[learning rate: 3.2248e-05]
	Learning Rate: 3.22478e-05
	LOSS [training: 0.08711288305669146 | validation: 0.10322477157367134]
	TIME [epoch: 8.12 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08839386781052852		[learning rate: 3.2189e-05]
		[batch 20/20] avg loss: 0.08525034073029504		[learning rate: 3.2131e-05]
	Learning Rate: 3.21308e-05
	LOSS [training: 0.08682210427041179 | validation: 0.10331603465191412]
	TIME [epoch: 8.12 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0872385228351698		[learning rate: 3.2072e-05]
		[batch 20/20] avg loss: 0.09208207450383586		[learning rate: 3.2014e-05]
	Learning Rate: 3.20142e-05
	LOSS [training: 0.08966029866950284 | validation: 0.09955821430370791]
	TIME [epoch: 8.17 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08854224751993126		[learning rate: 3.1956e-05]
		[batch 20/20] avg loss: 0.08949686020917673		[learning rate: 3.1898e-05]
	Learning Rate: 3.1898e-05
	LOSS [training: 0.08901955386455399 | validation: 0.10956735296258122]
	TIME [epoch: 8.15 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08684101918506419		[learning rate: 3.184e-05]
		[batch 20/20] avg loss: 0.09098357591510088		[learning rate: 3.1782e-05]
	Learning Rate: 3.17822e-05
	LOSS [training: 0.08891229755008254 | validation: 0.10833820599761507]
	TIME [epoch: 8.11 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09220441978551633		[learning rate: 3.1725e-05]
		[batch 20/20] avg loss: 0.08085876122224543		[learning rate: 3.1667e-05]
	Learning Rate: 3.16669e-05
	LOSS [training: 0.08653159050388089 | validation: 0.0971405015969178]
	TIME [epoch: 8.13 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08050425635813616		[learning rate: 3.1609e-05]
		[batch 20/20] avg loss: 0.0837081704990885		[learning rate: 3.1552e-05]
	Learning Rate: 3.1552e-05
	LOSS [training: 0.08210621342861234 | validation: 0.09772628471354741]
	TIME [epoch: 8.19 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09005295194916843		[learning rate: 3.1495e-05]
		[batch 20/20] avg loss: 0.0867671175932251		[learning rate: 3.1437e-05]
	Learning Rate: 3.14375e-05
	LOSS [training: 0.08841003477119677 | validation: 0.1060948984979939]
	TIME [epoch: 8.15 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08817907857397558		[learning rate: 3.138e-05]
		[batch 20/20] avg loss: 0.08570415002428224		[learning rate: 3.1323e-05]
	Learning Rate: 3.13234e-05
	LOSS [training: 0.0869416142991289 | validation: 0.10014411448878316]
	TIME [epoch: 8.13 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08304424484926837		[learning rate: 3.1266e-05]
		[batch 20/20] avg loss: 0.08920012499437027		[learning rate: 3.121e-05]
	Learning Rate: 3.12097e-05
	LOSS [training: 0.0861221849218193 | validation: 0.09507525844322931]
	TIME [epoch: 8.12 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07630632697190944		[learning rate: 3.1153e-05]
		[batch 20/20] avg loss: 0.09111340306455265		[learning rate: 3.1096e-05]
	Learning Rate: 3.10964e-05
	LOSS [training: 0.08370986501823104 | validation: 0.09297946933938961]
	TIME [epoch: 8.12 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07998712811195127		[learning rate: 3.104e-05]
		[batch 20/20] avg loss: 0.08871687711350724		[learning rate: 3.0984e-05]
	Learning Rate: 3.09836e-05
	LOSS [training: 0.08435200261272927 | validation: 0.09991662066356592]
	TIME [epoch: 8.13 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08733224612090464		[learning rate: 3.0927e-05]
		[batch 20/20] avg loss: 0.0967382413394054		[learning rate: 3.0871e-05]
	Learning Rate: 3.08711e-05
	LOSS [training: 0.09203524373015501 | validation: 0.10017603323268501]
	TIME [epoch: 8.14 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0813885000819409		[learning rate: 3.0815e-05]
		[batch 20/20] avg loss: 0.09100288652247036		[learning rate: 3.0759e-05]
	Learning Rate: 3.07591e-05
	LOSS [training: 0.08619569330220564 | validation: 0.10144741121200529]
	TIME [epoch: 8.13 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09843740657532321		[learning rate: 3.0703e-05]
		[batch 20/20] avg loss: 0.08252564420889325		[learning rate: 3.0647e-05]
	Learning Rate: 3.06475e-05
	LOSS [training: 0.09048152539210824 | validation: 0.10107318053607374]
	TIME [epoch: 8.16 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10089117459175836		[learning rate: 3.0592e-05]
		[batch 20/20] avg loss: 0.079493662698955		[learning rate: 3.0536e-05]
	Learning Rate: 3.05363e-05
	LOSS [training: 0.09019241864535668 | validation: 0.09531832008342651]
	TIME [epoch: 8.13 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09255817087049709		[learning rate: 3.0481e-05]
		[batch 20/20] avg loss: 0.08464450753993252		[learning rate: 3.0425e-05]
	Learning Rate: 3.04254e-05
	LOSS [training: 0.0886013392052148 | validation: 0.11560453001816723]
	TIME [epoch: 8.15 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08964634585302074		[learning rate: 3.037e-05]
		[batch 20/20] avg loss: 0.0937538436518923		[learning rate: 3.0315e-05]
	Learning Rate: 3.0315e-05
	LOSS [training: 0.09170009475245654 | validation: 0.1191913647345846]
	TIME [epoch: 8.15 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08734000754557411		[learning rate: 3.026e-05]
		[batch 20/20] avg loss: 0.11274330721547979		[learning rate: 3.0205e-05]
	Learning Rate: 3.0205e-05
	LOSS [training: 0.10004165738052695 | validation: 0.1157705344123228]
	TIME [epoch: 8.16 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08966618967349291		[learning rate: 3.015e-05]
		[batch 20/20] avg loss: 0.08611942850539986		[learning rate: 3.0095e-05]
	Learning Rate: 3.00954e-05
	LOSS [training: 0.08789280908944638 | validation: 0.10813921216729216]
	TIME [epoch: 8.12 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08583774705947975		[learning rate: 3.0041e-05]
		[batch 20/20] avg loss: 0.0815623623484597		[learning rate: 2.9986e-05]
	Learning Rate: 2.99862e-05
	LOSS [training: 0.08370005470396971 | validation: 0.0923397998001029]
	TIME [epoch: 8.14 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08982248484183435		[learning rate: 2.9932e-05]
		[batch 20/20] avg loss: 0.08672090845486725		[learning rate: 2.9877e-05]
	Learning Rate: 2.98774e-05
	LOSS [training: 0.0882716966483508 | validation: 0.09267872330729987]
	TIME [epoch: 8.13 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09866435619424993		[learning rate: 2.9823e-05]
		[batch 20/20] avg loss: 0.07497884764881431		[learning rate: 2.9769e-05]
	Learning Rate: 2.97689e-05
	LOSS [training: 0.0868216019215321 | validation: 0.09152974177020798]
	TIME [epoch: 8.12 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08364700744849286		[learning rate: 2.9715e-05]
		[batch 20/20] avg loss: 0.10282199641705066		[learning rate: 2.9661e-05]
	Learning Rate: 2.96609e-05
	LOSS [training: 0.09323450193277175 | validation: 0.10430001780793094]
	TIME [epoch: 8.12 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0923660945317684		[learning rate: 2.9607e-05]
		[batch 20/20] avg loss: 0.0833191309261621		[learning rate: 2.9553e-05]
	Learning Rate: 2.95533e-05
	LOSS [training: 0.08784261272896525 | validation: 0.09779343450833715]
	TIME [epoch: 8.16 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07405008505349454		[learning rate: 2.95e-05]
		[batch 20/20] avg loss: 0.09772710125880164		[learning rate: 2.9446e-05]
	Learning Rate: 2.9446e-05
	LOSS [training: 0.08588859315614811 | validation: 0.09994813201696665]
	TIME [epoch: 8.15 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08974102362996769		[learning rate: 2.9393e-05]
		[batch 20/20] avg loss: 0.08589784129413061		[learning rate: 2.9339e-05]
	Learning Rate: 2.93391e-05
	LOSS [training: 0.08781943246204915 | validation: 0.10167632318353528]
	TIME [epoch: 8.13 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10145482402207724		[learning rate: 2.9286e-05]
		[batch 20/20] avg loss: 0.08415268915793642		[learning rate: 2.9233e-05]
	Learning Rate: 2.92327e-05
	LOSS [training: 0.09280375659000684 | validation: 0.1043722914005873]
	TIME [epoch: 8.13 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08137017085313289		[learning rate: 2.918e-05]
		[batch 20/20] avg loss: 0.09750801887367173		[learning rate: 2.9127e-05]
	Learning Rate: 2.91266e-05
	LOSS [training: 0.08943909486340232 | validation: 0.11025652212896868]
	TIME [epoch: 8.16 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09928455252650364		[learning rate: 2.9074e-05]
		[batch 20/20] avg loss: 0.08005354427846585		[learning rate: 2.9021e-05]
	Learning Rate: 2.90209e-05
	LOSS [training: 0.08966904840248474 | validation: 0.09193693173317471]
	TIME [epoch: 8.15 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07925682108149348		[learning rate: 2.8968e-05]
		[batch 20/20] avg loss: 0.08979475693218059		[learning rate: 2.8916e-05]
	Learning Rate: 2.89156e-05
	LOSS [training: 0.08452578900683702 | validation: 0.10603592559164163]
	TIME [epoch: 8.13 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0912177640929941		[learning rate: 2.8863e-05]
		[batch 20/20] avg loss: 0.08176597627993079		[learning rate: 2.8811e-05]
	Learning Rate: 2.88106e-05
	LOSS [training: 0.08649187018646244 | validation: 0.09578273367925935]
	TIME [epoch: 8.12 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08625170412870536		[learning rate: 2.8758e-05]
		[batch 20/20] avg loss: 0.07955013747429893		[learning rate: 2.8706e-05]
	Learning Rate: 2.87061e-05
	LOSS [training: 0.08290092080150215 | validation: 0.09505830338335636]
	TIME [epoch: 8.12 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09183597112171818		[learning rate: 2.8654e-05]
		[batch 20/20] avg loss: 0.07442637164911814		[learning rate: 2.8602e-05]
	Learning Rate: 2.86019e-05
	LOSS [training: 0.08313117138541815 | validation: 0.09786413975469858]
	TIME [epoch: 8.12 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0801994856707794		[learning rate: 2.855e-05]
		[batch 20/20] avg loss: 0.08661003525476815		[learning rate: 2.8498e-05]
	Learning Rate: 2.84981e-05
	LOSS [training: 0.08340476046277379 | validation: 0.10035158575643585]
	TIME [epoch: 8.14 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08312339138085764		[learning rate: 2.8446e-05]
		[batch 20/20] avg loss: 0.0843157718433657		[learning rate: 2.8395e-05]
	Learning Rate: 2.83947e-05
	LOSS [training: 0.08371958161211167 | validation: 0.09663661871132123]
	TIME [epoch: 8.17 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0845214786630902		[learning rate: 2.8343e-05]
		[batch 20/20] avg loss: 0.08522632167165263		[learning rate: 2.8292e-05]
	Learning Rate: 2.82916e-05
	LOSS [training: 0.08487390016737141 | validation: 0.10116829988571774]
	TIME [epoch: 8.13 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08263415018948099		[learning rate: 2.824e-05]
		[batch 20/20] avg loss: 0.09236249085082818		[learning rate: 2.8189e-05]
	Learning Rate: 2.8189e-05
	LOSS [training: 0.08749832052015459 | validation: 0.09624883787815267]
	TIME [epoch: 8.13 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09487470218179414		[learning rate: 2.8138e-05]
		[batch 20/20] avg loss: 0.08020489330737723		[learning rate: 2.8087e-05]
	Learning Rate: 2.80867e-05
	LOSS [training: 0.0875397977445857 | validation: 0.1113173343626517]
	TIME [epoch: 8.15 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08810163216903436		[learning rate: 2.8036e-05]
		[batch 20/20] avg loss: 0.0806115133076852		[learning rate: 2.7985e-05]
	Learning Rate: 2.79847e-05
	LOSS [training: 0.08435657273835977 | validation: 0.10206750919293758]
	TIME [epoch: 8.18 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0807707620107673		[learning rate: 2.7934e-05]
		[batch 20/20] avg loss: 0.09000771399731333		[learning rate: 2.7883e-05]
	Learning Rate: 2.78832e-05
	LOSS [training: 0.08538923800404033 | validation: 0.10271175422202619]
	TIME [epoch: 8.12 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08610666552830372		[learning rate: 2.7833e-05]
		[batch 20/20] avg loss: 0.08115261418462262		[learning rate: 2.7782e-05]
	Learning Rate: 2.7782e-05
	LOSS [training: 0.08362963985646318 | validation: 0.09588780850297869]
	TIME [epoch: 8.12 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08521969694559905		[learning rate: 2.7732e-05]
		[batch 20/20] avg loss: 0.08177646099512965		[learning rate: 2.7681e-05]
	Learning Rate: 2.76812e-05
	LOSS [training: 0.08349807897036435 | validation: 0.10162153254831507]
	TIME [epoch: 8.13 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08619346431905629		[learning rate: 2.7631e-05]
		[batch 20/20] avg loss: 0.0796773167391811		[learning rate: 2.7581e-05]
	Learning Rate: 2.75807e-05
	LOSS [training: 0.0829353905291187 | validation: 0.10280416622482123]
	TIME [epoch: 8.15 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09003894354275586		[learning rate: 2.7531e-05]
		[batch 20/20] avg loss: 0.07983930145053614		[learning rate: 2.7481e-05]
	Learning Rate: 2.74806e-05
	LOSS [training: 0.08493912249664601 | validation: 0.10427223959344681]
	TIME [epoch: 8.11 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.079018320305226		[learning rate: 2.7431e-05]
		[batch 20/20] avg loss: 0.08508750293721044		[learning rate: 2.7381e-05]
	Learning Rate: 2.73809e-05
	LOSS [training: 0.08205291162121824 | validation: 0.10048210152606714]
	TIME [epoch: 8.13 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08254098165225594		[learning rate: 2.7331e-05]
		[batch 20/20] avg loss: 0.08550178058003856		[learning rate: 2.7282e-05]
	Learning Rate: 2.72815e-05
	LOSS [training: 0.08402138111614725 | validation: 0.09981329942827753]
	TIME [epoch: 8.12 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08285283634032935		[learning rate: 2.7232e-05]
		[batch 20/20] avg loss: 0.09123123973671621		[learning rate: 2.7183e-05]
	Learning Rate: 2.71825e-05
	LOSS [training: 0.08704203803852278 | validation: 0.10410257276663085]
	TIME [epoch: 8.15 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0932241792061991		[learning rate: 2.7133e-05]
		[batch 20/20] avg loss: 0.07846591501767539		[learning rate: 2.7084e-05]
	Learning Rate: 2.70839e-05
	LOSS [training: 0.08584504711193724 | validation: 0.10299377549366848]
	TIME [epoch: 8.17 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08116473689906391		[learning rate: 2.7035e-05]
		[batch 20/20] avg loss: 0.0898364534970858		[learning rate: 2.6986e-05]
	Learning Rate: 2.69856e-05
	LOSS [training: 0.08550059519807485 | validation: 0.09512180902465542]
	TIME [epoch: 8.13 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0886215847263537		[learning rate: 2.6937e-05]
		[batch 20/20] avg loss: 0.088580852422122		[learning rate: 2.6888e-05]
	Learning Rate: 2.68876e-05
	LOSS [training: 0.08860121857423786 | validation: 0.10133068148655722]
	TIME [epoch: 8.13 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08332546721537974		[learning rate: 2.6839e-05]
		[batch 20/20] avg loss: 0.09138198230193605		[learning rate: 2.679e-05]
	Learning Rate: 2.67901e-05
	LOSS [training: 0.08735372475865791 | validation: 0.101554577466563]
	TIME [epoch: 8.15 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08199775406311807		[learning rate: 2.6741e-05]
		[batch 20/20] avg loss: 0.08689334434783218		[learning rate: 2.6693e-05]
	Learning Rate: 2.66928e-05
	LOSS [training: 0.08444554920547512 | validation: 0.10178125996199755]
	TIME [epoch: 8.17 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08404940068832957		[learning rate: 2.6644e-05]
		[batch 20/20] avg loss: 0.08576379898244933		[learning rate: 2.6596e-05]
	Learning Rate: 2.6596e-05
	LOSS [training: 0.08490659983538945 | validation: 0.09192432330965977]
	TIME [epoch: 8.12 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08437056184993712		[learning rate: 2.6548e-05]
		[batch 20/20] avg loss: 0.08342241285008038		[learning rate: 2.6499e-05]
	Learning Rate: 2.64994e-05
	LOSS [training: 0.08389648735000875 | validation: 0.10057440695307951]
	TIME [epoch: 8.13 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08070969649011668		[learning rate: 2.6451e-05]
		[batch 20/20] avg loss: 0.09061716283433022		[learning rate: 2.6403e-05]
	Learning Rate: 2.64033e-05
	LOSS [training: 0.08566342966222346 | validation: 0.10150428483748686]
	TIME [epoch: 8.12 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07951238092345311		[learning rate: 2.6355e-05]
		[batch 20/20] avg loss: 0.0863908617427616		[learning rate: 2.6307e-05]
	Learning Rate: 2.63075e-05
	LOSS [training: 0.08295162133310736 | validation: 0.09881478403219317]
	TIME [epoch: 8.17 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07710445105385637		[learning rate: 2.626e-05]
		[batch 20/20] avg loss: 0.08989737886936616		[learning rate: 2.6212e-05]
	Learning Rate: 2.6212e-05
	LOSS [training: 0.08350091496161124 | validation: 0.0947817475436116]
	TIME [epoch: 8.13 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08972656304913537		[learning rate: 2.6164e-05]
		[batch 20/20] avg loss: 0.07555775784720793		[learning rate: 2.6117e-05]
	Learning Rate: 2.61169e-05
	LOSS [training: 0.08264216044817164 | validation: 0.09577967602884561]
	TIME [epoch: 8.13 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08268154116384858		[learning rate: 2.6069e-05]
		[batch 20/20] avg loss: 0.0911773603226031		[learning rate: 2.6022e-05]
	Learning Rate: 2.60221e-05
	LOSS [training: 0.08692945074322583 | validation: 0.09357658317183576]
	TIME [epoch: 8.13 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08128472443108838		[learning rate: 2.5975e-05]
		[batch 20/20] avg loss: 0.09925713471579271		[learning rate: 2.5928e-05]
	Learning Rate: 2.59277e-05
	LOSS [training: 0.09027092957344053 | validation: 0.1034880373225581]
	TIME [epoch: 8.18 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09713403210017006		[learning rate: 2.5881e-05]
		[batch 20/20] avg loss: 0.08207710872779503		[learning rate: 2.5834e-05]
	Learning Rate: 2.58336e-05
	LOSS [training: 0.08960557041398257 | validation: 0.11345086556323009]
	TIME [epoch: 8.14 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08416081856940087		[learning rate: 2.5787e-05]
		[batch 20/20] avg loss: 0.10135780861095271		[learning rate: 2.574e-05]
	Learning Rate: 2.57398e-05
	LOSS [training: 0.09275931359017678 | validation: 0.11111727074281788]
	TIME [epoch: 8.13 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09229430713198597		[learning rate: 2.5693e-05]
		[batch 20/20] avg loss: 0.09366950835745452		[learning rate: 2.5646e-05]
	Learning Rate: 2.56464e-05
	LOSS [training: 0.09298190774472023 | validation: 0.0972327858137178]
	TIME [epoch: 8.15 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09480951315538029		[learning rate: 2.56e-05]
		[batch 20/20] avg loss: 0.08040985450939986		[learning rate: 2.5553e-05]
	Learning Rate: 2.55533e-05
	LOSS [training: 0.08760968383239007 | validation: 0.10097053197504458]
	TIME [epoch: 8.18 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08133901165585446		[learning rate: 2.5507e-05]
		[batch 20/20] avg loss: 0.0918406402983063		[learning rate: 2.5461e-05]
	Learning Rate: 2.54606e-05
	LOSS [training: 0.08658982597708038 | validation: 0.09753767603006658]
	TIME [epoch: 8.14 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07983923763970342		[learning rate: 2.5414e-05]
		[batch 20/20] avg loss: 0.08935298901781098		[learning rate: 2.5368e-05]
	Learning Rate: 2.53682e-05
	LOSS [training: 0.08459611332875719 | validation: 0.10709170398960777]
	TIME [epoch: 8.13 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09143534988706119		[learning rate: 2.5322e-05]
		[batch 20/20] avg loss: 0.08138273559540128		[learning rate: 2.5276e-05]
	Learning Rate: 2.52761e-05
	LOSS [training: 0.08640904274123125 | validation: 0.09153589346784977]
	TIME [epoch: 8.12 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08809145752096016		[learning rate: 2.523e-05]
		[batch 20/20] avg loss: 0.09025014837053273		[learning rate: 2.5184e-05]
	Learning Rate: 2.51844e-05
	LOSS [training: 0.08917080294574645 | validation: 0.09848279674019939]
	TIME [epoch: 8.12 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0932213052024797		[learning rate: 2.5139e-05]
		[batch 20/20] avg loss: 0.08335389370087663		[learning rate: 2.5093e-05]
	Learning Rate: 2.5093e-05
	LOSS [training: 0.08828759945167818 | validation: 0.10214167443534355]
	TIME [epoch: 8.13 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08978226447113727		[learning rate: 2.5047e-05]
		[batch 20/20] avg loss: 0.07746170678417827		[learning rate: 2.5002e-05]
	Learning Rate: 2.50019e-05
	LOSS [training: 0.08362198562765776 | validation: 0.09956993828713347]
	TIME [epoch: 8.13 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08931078312539387		[learning rate: 2.4957e-05]
		[batch 20/20] avg loss: 0.07899571388076101		[learning rate: 2.4911e-05]
	Learning Rate: 2.49112e-05
	LOSS [training: 0.08415324850307744 | validation: 0.09907052742266675]
	TIME [epoch: 8.13 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07606711499788396		[learning rate: 2.4866e-05]
		[batch 20/20] avg loss: 0.1045644073615671		[learning rate: 2.4821e-05]
	Learning Rate: 2.48208e-05
	LOSS [training: 0.09031576117972553 | validation: 0.10728861076316695]
	TIME [epoch: 8.16 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08604565054597033		[learning rate: 2.4776e-05]
		[batch 20/20] avg loss: 0.08955543693726176		[learning rate: 2.4731e-05]
	Learning Rate: 2.47307e-05
	LOSS [training: 0.08780054374161604 | validation: 0.10423864479442999]
	TIME [epoch: 8.12 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09069740084981937		[learning rate: 2.4686e-05]
		[batch 20/20] avg loss: 0.07882379671178312		[learning rate: 2.4641e-05]
	Learning Rate: 2.4641e-05
	LOSS [training: 0.08476059878080125 | validation: 0.10352880211672257]
	TIME [epoch: 8.15 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0847759836201667		[learning rate: 2.4596e-05]
		[batch 20/20] avg loss: 0.0907519882920719		[learning rate: 2.4552e-05]
	Learning Rate: 2.45516e-05
	LOSS [training: 0.0877639859561193 | validation: 0.09010489422076305]
	TIME [epoch: 8.15 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09207127731443411		[learning rate: 2.4507e-05]
		[batch 20/20] avg loss: 0.08683142259546127		[learning rate: 2.4462e-05]
	Learning Rate: 2.44625e-05
	LOSS [training: 0.08945134995494768 | validation: 0.09436202635749685]
	TIME [epoch: 8.15 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09995251437595985		[learning rate: 2.4418e-05]
		[batch 20/20] avg loss: 0.07594537918796843		[learning rate: 2.4374e-05]
	Learning Rate: 2.43737e-05
	LOSS [training: 0.08794894678196416 | validation: 0.10375340857487114]
	TIME [epoch: 8.11 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08584547768726287		[learning rate: 2.4329e-05]
		[batch 20/20] avg loss: 0.08430698434396945		[learning rate: 2.4285e-05]
	Learning Rate: 2.42852e-05
	LOSS [training: 0.08507623101561616 | validation: 0.094424675077891]
	TIME [epoch: 8.14 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09102099873617983		[learning rate: 2.4241e-05]
		[batch 20/20] avg loss: 0.08330549604374048		[learning rate: 2.4197e-05]
	Learning Rate: 2.41971e-05
	LOSS [training: 0.08716324738996015 | validation: 0.09443407245685934]
	TIME [epoch: 8.12 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08434523323703577		[learning rate: 2.4153e-05]
		[batch 20/20] avg loss: 0.0853496598437583		[learning rate: 2.4109e-05]
	Learning Rate: 2.41093e-05
	LOSS [training: 0.08484744654039704 | validation: 0.09325888565932561]
	TIME [epoch: 8.12 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09374031471418347		[learning rate: 2.4065e-05]
		[batch 20/20] avg loss: 0.07768438874511227		[learning rate: 2.4022e-05]
	Learning Rate: 2.40218e-05
	LOSS [training: 0.08571235172964786 | validation: 0.10514930333653083]
	TIME [epoch: 8.12 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09125933027202267		[learning rate: 2.3978e-05]
		[batch 20/20] avg loss: 0.08180594463775158		[learning rate: 2.3935e-05]
	Learning Rate: 2.39346e-05
	LOSS [training: 0.08653263745488712 | validation: 0.09913734042477729]
	TIME [epoch: 8.12 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0879831898717067		[learning rate: 2.3891e-05]
		[batch 20/20] avg loss: 0.09031487611184252		[learning rate: 2.3848e-05]
	Learning Rate: 2.38477e-05
	LOSS [training: 0.0891490329917746 | validation: 0.10634289530957938]
	TIME [epoch: 8.14 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0826257970390309		[learning rate: 2.3804e-05]
		[batch 20/20] avg loss: 0.09224032391314332		[learning rate: 2.3761e-05]
	Learning Rate: 2.37612e-05
	LOSS [training: 0.08743306047608712 | validation: 0.09932424605701215]
	TIME [epoch: 8.13 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08844571550507632		[learning rate: 2.3718e-05]
		[batch 20/20] avg loss: 0.09488581325666871		[learning rate: 2.3675e-05]
	Learning Rate: 2.3675e-05
	LOSS [training: 0.09166576438087251 | validation: 0.10698753110128513]
	TIME [epoch: 8.14 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08573346803231358		[learning rate: 2.3632e-05]
		[batch 20/20] avg loss: 0.08962547610161228		[learning rate: 2.3589e-05]
	Learning Rate: 2.35891e-05
	LOSS [training: 0.08767947206696294 | validation: 0.10013226359710202]
	TIME [epoch: 8.16 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08605424546549384		[learning rate: 2.3546e-05]
		[batch 20/20] avg loss: 0.09700195576088028		[learning rate: 2.3503e-05]
	Learning Rate: 2.35034e-05
	LOSS [training: 0.09152810061318706 | validation: 0.09912782633562213]
	TIME [epoch: 8.15 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.091092222997637		[learning rate: 2.3461e-05]
		[batch 20/20] avg loss: 0.08470874381677004		[learning rate: 2.3418e-05]
	Learning Rate: 2.34182e-05
	LOSS [training: 0.08790048340720354 | validation: 0.10438279463370426]
	TIME [epoch: 8.13 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09556608377182807		[learning rate: 2.3376e-05]
		[batch 20/20] avg loss: 0.0754796601198546		[learning rate: 2.3333e-05]
	Learning Rate: 2.33332e-05
	LOSS [training: 0.08552287194584134 | validation: 0.09799066271708945]
	TIME [epoch: 8.15 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08548604435379534		[learning rate: 2.3291e-05]
		[batch 20/20] avg loss: 0.08114692550460789		[learning rate: 2.3248e-05]
	Learning Rate: 2.32485e-05
	LOSS [training: 0.08331648492920163 | validation: 0.09531497919829525]
	TIME [epoch: 8.14 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09465672300437068		[learning rate: 2.3206e-05]
		[batch 20/20] avg loss: 0.07567603731504728		[learning rate: 2.3164e-05]
	Learning Rate: 2.31641e-05
	LOSS [training: 0.08516638015970897 | validation: 0.09647840491138315]
	TIME [epoch: 8.13 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08243460393632474		[learning rate: 2.3122e-05]
		[batch 20/20] avg loss: 0.0868861691392118		[learning rate: 2.308e-05]
	Learning Rate: 2.30801e-05
	LOSS [training: 0.08466038653776826 | validation: 0.09307833867981863]
	TIME [epoch: 8.14 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09527269189373987		[learning rate: 2.3038e-05]
		[batch 20/20] avg loss: 0.0791484329574783		[learning rate: 2.2996e-05]
	Learning Rate: 2.29963e-05
	LOSS [training: 0.0872105624256091 | validation: 0.09949123455928499]
	TIME [epoch: 8.12 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08796649700114662		[learning rate: 2.2955e-05]
		[batch 20/20] avg loss: 0.08603104404396533		[learning rate: 2.2913e-05]
	Learning Rate: 2.29128e-05
	LOSS [training: 0.08699877052255597 | validation: 0.09534667096750617]
	TIME [epoch: 8.12 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09044785153840243		[learning rate: 2.2871e-05]
		[batch 20/20] avg loss: 0.07874846277155403		[learning rate: 2.283e-05]
	Learning Rate: 2.28297e-05
	LOSS [training: 0.08459815715497825 | validation: 0.09603797996489384]
	TIME [epoch: 8.12 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08154534390899168		[learning rate: 2.2788e-05]
		[batch 20/20] avg loss: 0.09366621435702746		[learning rate: 2.2747e-05]
	Learning Rate: 2.27468e-05
	LOSS [training: 0.08760577913300958 | validation: 0.09777972288471246]
	TIME [epoch: 8.14 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07671306303781286		[learning rate: 2.2706e-05]
		[batch 20/20] avg loss: 0.09160001922094727		[learning rate: 2.2664e-05]
	Learning Rate: 2.26643e-05
	LOSS [training: 0.08415654112938005 | validation: 0.09387646468156043]
	TIME [epoch: 8.12 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0889531262898563		[learning rate: 2.2623e-05]
		[batch 20/20] avg loss: 0.08354360321658476		[learning rate: 2.2582e-05]
	Learning Rate: 2.2582e-05
	LOSS [training: 0.08624836475322054 | validation: 0.10007467580742992]
	TIME [epoch: 8.11 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0961126221505158		[learning rate: 2.2541e-05]
		[batch 20/20] avg loss: 0.07801501653444746		[learning rate: 2.25e-05]
	Learning Rate: 2.25001e-05
	LOSS [training: 0.08706381934248163 | validation: 0.09855303590429992]
	TIME [epoch: 8.17 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0920385236521886		[learning rate: 2.2459e-05]
		[batch 20/20] avg loss: 0.08112038296882865		[learning rate: 2.2418e-05]
	Learning Rate: 2.24184e-05
	LOSS [training: 0.08657945331050862 | validation: 0.09194470098151115]
	TIME [epoch: 8.14 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08189698964433959		[learning rate: 2.2378e-05]
		[batch 20/20] avg loss: 0.08735720073528917		[learning rate: 2.2337e-05]
	Learning Rate: 2.23371e-05
	LOSS [training: 0.08462709518981437 | validation: 0.09907362103888703]
	TIME [epoch: 8.13 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07947505245612035		[learning rate: 2.2297e-05]
		[batch 20/20] avg loss: 0.10075906753420164		[learning rate: 2.2256e-05]
	Learning Rate: 2.2256e-05
	LOSS [training: 0.09011705999516098 | validation: 0.10007120160614773]
	TIME [epoch: 8.13 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08340492168509611		[learning rate: 2.2216e-05]
		[batch 20/20] avg loss: 0.08464553633109345		[learning rate: 2.2175e-05]
	Learning Rate: 2.21753e-05
	LOSS [training: 0.08402522900809478 | validation: 0.09482904552406997]
	TIME [epoch: 8.17 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08694646346203821		[learning rate: 2.2135e-05]
		[batch 20/20] avg loss: 0.07911629136857692		[learning rate: 2.2095e-05]
	Learning Rate: 2.20948e-05
	LOSS [training: 0.08303137741530756 | validation: 0.10025453178015649]
	TIME [epoch: 8.12 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08152547881843411		[learning rate: 2.2055e-05]
		[batch 20/20] avg loss: 0.08937877285742893		[learning rate: 2.2015e-05]
	Learning Rate: 2.20146e-05
	LOSS [training: 0.08545212583793152 | validation: 0.10334315979540856]
	TIME [epoch: 8.14 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08477324433068602		[learning rate: 2.1975e-05]
		[batch 20/20] avg loss: 0.08626360884059588		[learning rate: 2.1935e-05]
	Learning Rate: 2.19347e-05
	LOSS [training: 0.08551842658564095 | validation: 0.09751597270095123]
	TIME [epoch: 8.13 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09606777365926289		[learning rate: 2.1895e-05]
		[batch 20/20] avg loss: 0.07544966602347837		[learning rate: 2.1855e-05]
	Learning Rate: 2.18551e-05
	LOSS [training: 0.08575871984137062 | validation: 0.10195215190376236]
	TIME [epoch: 8.12 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08523529639610303		[learning rate: 2.1815e-05]
		[batch 20/20] avg loss: 0.08858000948245734		[learning rate: 2.1776e-05]
	Learning Rate: 2.17758e-05
	LOSS [training: 0.08690765293928018 | validation: 0.09293796467025552]
	TIME [epoch: 8.12 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07533518900033256		[learning rate: 2.1736e-05]
		[batch 20/20] avg loss: 0.10258274954357612		[learning rate: 2.1697e-05]
	Learning Rate: 2.16968e-05
	LOSS [training: 0.08895896927195437 | validation: 0.0972503599549781]
	TIME [epoch: 8.13 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07409688469443229		[learning rate: 2.1657e-05]
		[batch 20/20] avg loss: 0.09293180722798132		[learning rate: 2.1618e-05]
	Learning Rate: 2.1618e-05
	LOSS [training: 0.0835143459612068 | validation: 0.09702076378571606]
	TIME [epoch: 8.13 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08701282998896394		[learning rate: 2.1579e-05]
		[batch 20/20] avg loss: 0.09411546046540643		[learning rate: 2.154e-05]
	Learning Rate: 2.15396e-05
	LOSS [training: 0.09056414522718517 | validation: 0.10498712814497187]
	TIME [epoch: 8.11 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0764422129781912		[learning rate: 2.15e-05]
		[batch 20/20] avg loss: 0.09878399546658398		[learning rate: 2.1461e-05]
	Learning Rate: 2.14614e-05
	LOSS [training: 0.08761310422238759 | validation: 0.09995518913637089]
	TIME [epoch: 8.12 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0828398190720532		[learning rate: 2.1422e-05]
		[batch 20/20] avg loss: 0.08745634795073468		[learning rate: 2.1384e-05]
	Learning Rate: 2.13835e-05
	LOSS [training: 0.08514808351139394 | validation: 0.10066763506530431]
	TIME [epoch: 8.12 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10816132458486354		[learning rate: 2.1345e-05]
		[batch 20/20] avg loss: 0.0898892076316103		[learning rate: 2.1306e-05]
	Learning Rate: 2.13059e-05
	LOSS [training: 0.09902526610823693 | validation: 0.1014173172424915]
	TIME [epoch: 8.14 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0972933401652916		[learning rate: 2.1267e-05]
		[batch 20/20] avg loss: 0.08482250916900434		[learning rate: 2.1229e-05]
	Learning Rate: 2.12286e-05
	LOSS [training: 0.09105792466714797 | validation: 0.0908131315853892]
	TIME [epoch: 8.11 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08230538847434159		[learning rate: 2.119e-05]
		[batch 20/20] avg loss: 0.09526618658896319		[learning rate: 2.1152e-05]
	Learning Rate: 2.11515e-05
	LOSS [training: 0.0887857875316524 | validation: 0.10348577452984387]
	TIME [epoch: 8.12 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08744428837670458		[learning rate: 2.1113e-05]
		[batch 20/20] avg loss: 0.08560006458637186		[learning rate: 2.1075e-05]
	Learning Rate: 2.10748e-05
	LOSS [training: 0.08652217648153823 | validation: 0.09888593311228272]
	TIME [epoch: 8.13 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08372580933842982		[learning rate: 2.1037e-05]
		[batch 20/20] avg loss: 0.09035089995688658		[learning rate: 2.0998e-05]
	Learning Rate: 2.09983e-05
	LOSS [training: 0.08703835464765822 | validation: 0.09835648417150394]
	TIME [epoch: 8.14 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10324801896181299		[learning rate: 2.096e-05]
		[batch 20/20] avg loss: 0.09075705084213956		[learning rate: 2.0922e-05]
	Learning Rate: 2.09221e-05
	LOSS [training: 0.09700253490197627 | validation: 0.09601946581261459]
	TIME [epoch: 8.12 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09135015851834147		[learning rate: 2.0884e-05]
		[batch 20/20] avg loss: 0.0882406658438045		[learning rate: 2.0846e-05]
	Learning Rate: 2.08462e-05
	LOSS [training: 0.08979541218107301 | validation: 0.09370566283914207]
	TIME [epoch: 8.13 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09278939930494648		[learning rate: 2.0808e-05]
		[batch 20/20] avg loss: 0.0845142234576318		[learning rate: 2.0771e-05]
	Learning Rate: 2.07705e-05
	LOSS [training: 0.08865181138128914 | validation: 0.09074086426018192]
	TIME [epoch: 8.12 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08689332116703184		[learning rate: 2.0733e-05]
		[batch 20/20] avg loss: 0.09231083162381419		[learning rate: 2.0695e-05]
	Learning Rate: 2.06951e-05
	LOSS [training: 0.08960207639542303 | validation: 0.10110042539222806]
	TIME [epoch: 8.13 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07902696696397862		[learning rate: 2.0658e-05]
		[batch 20/20] avg loss: 0.10064742896187813		[learning rate: 2.062e-05]
	Learning Rate: 2.062e-05
	LOSS [training: 0.08983719796292837 | validation: 0.1032907210817269]
	TIME [epoch: 8.15 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08946545200836607		[learning rate: 2.0583e-05]
		[batch 20/20] avg loss: 0.07795396803525069		[learning rate: 2.0545e-05]
	Learning Rate: 2.05452e-05
	LOSS [training: 0.08370971002180837 | validation: 0.09965844653594766]
	TIME [epoch: 8.12 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08945953949824445		[learning rate: 2.0508e-05]
		[batch 20/20] avg loss: 0.08090869004793878		[learning rate: 2.0471e-05]
	Learning Rate: 2.04706e-05
	LOSS [training: 0.0851841147730916 | validation: 0.09993924925644151]
	TIME [epoch: 8.11 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08571273266588286		[learning rate: 2.0433e-05]
		[batch 20/20] avg loss: 0.0814994858002095		[learning rate: 2.0396e-05]
	Learning Rate: 2.03964e-05
	LOSS [training: 0.08360610923304619 | validation: 0.09582791363219807]
	TIME [epoch: 8.12 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08738895077589565		[learning rate: 2.0359e-05]
		[batch 20/20] avg loss: 0.08363356360059734		[learning rate: 2.0322e-05]
	Learning Rate: 2.03223e-05
	LOSS [training: 0.0855112571882465 | validation: 0.1002089378028983]
	TIME [epoch: 8.15 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08293820888854055		[learning rate: 2.0285e-05]
		[batch 20/20] avg loss: 0.10230333806890021		[learning rate: 2.0249e-05]
	Learning Rate: 2.02486e-05
	LOSS [training: 0.09262077347872039 | validation: 0.10448876488233197]
	TIME [epoch: 8.13 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0950980359244338		[learning rate: 2.0212e-05]
		[batch 20/20] avg loss: 0.08268159696498363		[learning rate: 2.0175e-05]
	Learning Rate: 2.01751e-05
	LOSS [training: 0.08888981644470874 | validation: 0.09977148432558995]
	TIME [epoch: 8.13 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09443313661350201		[learning rate: 2.0138e-05]
		[batch 20/20] avg loss: 0.07781914049351238		[learning rate: 2.0102e-05]
	Learning Rate: 2.01019e-05
	LOSS [training: 0.08612613855350719 | validation: 0.09806047164562853]
	TIME [epoch: 8.13 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09665723463702647		[learning rate: 2.0065e-05]
		[batch 20/20] avg loss: 0.07569746702727317		[learning rate: 2.0029e-05]
	Learning Rate: 2.00289e-05
	LOSS [training: 0.08617735083214981 | validation: 0.09841333160977622]
	TIME [epoch: 8.13 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08316947588268367		[learning rate: 1.9993e-05]
		[batch 20/20] avg loss: 0.08479140062171266		[learning rate: 1.9956e-05]
	Learning Rate: 1.99563e-05
	LOSS [training: 0.08398043825219816 | validation: 0.10037969672617558]
	TIME [epoch: 8.14 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08757639327783526		[learning rate: 1.992e-05]
		[batch 20/20] avg loss: 0.0787388324979582		[learning rate: 1.9884e-05]
	Learning Rate: 1.98838e-05
	LOSS [training: 0.08315761288789672 | validation: 0.10157792188508163]
	TIME [epoch: 8.12 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08015135079015642		[learning rate: 1.9848e-05]
		[batch 20/20] avg loss: 0.0921707919144302		[learning rate: 1.9812e-05]
	Learning Rate: 1.98117e-05
	LOSS [training: 0.0861610713522933 | validation: 0.09925529195248914]
	TIME [epoch: 8.12 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08280544796443531		[learning rate: 1.9776e-05]
		[batch 20/20] avg loss: 0.08484360694142504		[learning rate: 1.974e-05]
	Learning Rate: 1.97398e-05
	LOSS [training: 0.08382452745293016 | validation: 0.09224939633975791]
	TIME [epoch: 8.13 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08897063941106295		[learning rate: 1.9704e-05]
		[batch 20/20] avg loss: 0.08368166392562472		[learning rate: 1.9668e-05]
	Learning Rate: 1.96681e-05
	LOSS [training: 0.08632615166834386 | validation: 0.09717928882894111]
	TIME [epoch: 8.15 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0956985866252311		[learning rate: 1.9632e-05]
		[batch 20/20] avg loss: 0.0732479651265952		[learning rate: 1.9597e-05]
	Learning Rate: 1.95968e-05
	LOSS [training: 0.08447327587591316 | validation: 0.09997450144639655]
	TIME [epoch: 8.13 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08324268151770867		[learning rate: 1.9561e-05]
		[batch 20/20] avg loss: 0.08772263187963761		[learning rate: 1.9526e-05]
	Learning Rate: 1.95256e-05
	LOSS [training: 0.08548265669867312 | validation: 0.09963132992150403]
	TIME [epoch: 8.12 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08795004825822055		[learning rate: 1.949e-05]
		[batch 20/20] avg loss: 0.08097265057926796		[learning rate: 1.9455e-05]
	Learning Rate: 1.94548e-05
	LOSS [training: 0.08446134941874425 | validation: 0.10803429674760481]
	TIME [epoch: 8.13 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08296211192933312		[learning rate: 1.9419e-05]
		[batch 20/20] avg loss: 0.08666506601711668		[learning rate: 1.9384e-05]
	Learning Rate: 1.93842e-05
	LOSS [training: 0.08481358897322491 | validation: 0.09444142245650386]
	TIME [epoch: 8.15 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08900616642400486		[learning rate: 1.9349e-05]
		[batch 20/20] avg loss: 0.07719917033149158		[learning rate: 1.9314e-05]
	Learning Rate: 1.93138e-05
	LOSS [training: 0.08310266837774821 | validation: 0.0916692411937854]
	TIME [epoch: 8.14 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08100490457715567		[learning rate: 1.9279e-05]
		[batch 20/20] avg loss: 0.08784999430873497		[learning rate: 1.9244e-05]
	Learning Rate: 1.92437e-05
	LOSS [training: 0.08442744944294531 | validation: 0.09019798120278509]
	TIME [epoch: 8.12 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07795141999975565		[learning rate: 1.9209e-05]
		[batch 20/20] avg loss: 0.08598093219503514		[learning rate: 1.9174e-05]
	Learning Rate: 1.91739e-05
	LOSS [training: 0.08196617609739539 | validation: 0.09257788336525961]
	TIME [epoch: 8.13 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08035767696110543		[learning rate: 1.9139e-05]
		[batch 20/20] avg loss: 0.08490430937489282		[learning rate: 1.9104e-05]
	Learning Rate: 1.91043e-05
	LOSS [training: 0.08263099316799913 | validation: 0.10124223889405318]
	TIME [epoch: 8.12 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07263655628988888		[learning rate: 1.907e-05]
		[batch 20/20] avg loss: 0.09732132461954986		[learning rate: 1.9035e-05]
	Learning Rate: 1.9035e-05
	LOSS [training: 0.08497894045471935 | validation: 0.09072247219373891]
	TIME [epoch: 8.15 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09029389689862781		[learning rate: 1.9e-05]
		[batch 20/20] avg loss: 0.09276166248349371		[learning rate: 1.8966e-05]
	Learning Rate: 1.89659e-05
	LOSS [training: 0.09152777969106077 | validation: 0.09946133778508612]
	TIME [epoch: 8.13 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08926934204226435		[learning rate: 1.8931e-05]
		[batch 20/20] avg loss: 0.0813933291141887		[learning rate: 1.8897e-05]
	Learning Rate: 1.88971e-05
	LOSS [training: 0.08533133557822652 | validation: 0.09577349858392176]
	TIME [epoch: 8.13 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08241208176481428		[learning rate: 1.8863e-05]
		[batch 20/20] avg loss: 0.09180580728659335		[learning rate: 1.8829e-05]
	Learning Rate: 1.88285e-05
	LOSS [training: 0.08710894452570382 | validation: 0.09443610157644107]
	TIME [epoch: 8.12 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08358108990914623		[learning rate: 1.8794e-05]
		[batch 20/20] avg loss: 0.09242128706404847		[learning rate: 1.876e-05]
	Learning Rate: 1.87602e-05
	LOSS [training: 0.08800118848659735 | validation: 0.09331234197384902]
	TIME [epoch: 8.12 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08774367284892363		[learning rate: 1.8726e-05]
		[batch 20/20] avg loss: 0.08547104280658743		[learning rate: 1.8692e-05]
	Learning Rate: 1.86921e-05
	LOSS [training: 0.08660735782775554 | validation: 0.09739464617301463]
	TIME [epoch: 8.14 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0908748254452993		[learning rate: 1.8658e-05]
		[batch 20/20] avg loss: 0.07942689958770803		[learning rate: 1.8624e-05]
	Learning Rate: 1.86243e-05
	LOSS [training: 0.0851508625165037 | validation: 0.09766829737973988]
	TIME [epoch: 8.12 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08493450605331526		[learning rate: 1.859e-05]
		[batch 20/20] avg loss: 0.0872641436874209		[learning rate: 1.8557e-05]
	Learning Rate: 1.85567e-05
	LOSS [training: 0.08609932487036806 | validation: 0.10120444869559643]
	TIME [epoch: 8.13 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08636022964837574		[learning rate: 1.8523e-05]
		[batch 20/20] avg loss: 0.08726929792964314		[learning rate: 1.8489e-05]
	Learning Rate: 1.84893e-05
	LOSS [training: 0.08681476378900944 | validation: 0.09942038652615615]
	TIME [epoch: 8.12 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08686851410737248		[learning rate: 1.8456e-05]
		[batch 20/20] avg loss: 0.0844900619617115		[learning rate: 1.8422e-05]
	Learning Rate: 1.84222e-05
	LOSS [training: 0.08567928803454196 | validation: 0.09974158080183038]
	TIME [epoch: 8.16 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.081156421243498		[learning rate: 1.8389e-05]
		[batch 20/20] avg loss: 0.09614747810080253		[learning rate: 1.8355e-05]
	Learning Rate: 1.83554e-05
	LOSS [training: 0.08865194967215027 | validation: 0.09608937367501871]
	TIME [epoch: 8.12 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08588362940856809		[learning rate: 1.8322e-05]
		[batch 20/20] avg loss: 0.0813409465812325		[learning rate: 1.8289e-05]
	Learning Rate: 1.82888e-05
	LOSS [training: 0.08361228799490031 | validation: 0.09099195153373699]
	TIME [epoch: 8.12 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0884372178329835		[learning rate: 1.8256e-05]
		[batch 20/20] avg loss: 0.08106407523007243		[learning rate: 1.8222e-05]
	Learning Rate: 1.82224e-05
	LOSS [training: 0.08475064653152796 | validation: 0.10643864361046995]
	TIME [epoch: 8.12 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07867673905354439		[learning rate: 1.8189e-05]
		[batch 20/20] avg loss: 0.09016677097140523		[learning rate: 1.8156e-05]
	Learning Rate: 1.81563e-05
	LOSS [training: 0.08442175501247481 | validation: 0.09932279959642269]
	TIME [epoch: 8.14 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08290950173370572		[learning rate: 1.8123e-05]
		[batch 20/20] avg loss: 0.08993469125298972		[learning rate: 1.809e-05]
	Learning Rate: 1.80904e-05
	LOSS [training: 0.08642209649334771 | validation: 0.10203229845174125]
	TIME [epoch: 8.13 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08968955786071434		[learning rate: 1.8058e-05]
		[batch 20/20] avg loss: 0.08666673557021778		[learning rate: 1.8025e-05]
	Learning Rate: 1.80247e-05
	LOSS [training: 0.08817814671546605 | validation: 0.10174216952681919]
	TIME [epoch: 8.12 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08861898559981786		[learning rate: 1.7992e-05]
		[batch 20/20] avg loss: 0.08636379603078634		[learning rate: 1.7959e-05]
	Learning Rate: 1.79593e-05
	LOSS [training: 0.08749139081530208 | validation: 0.09761643482031732]
	TIME [epoch: 8.13 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08482188973956639		[learning rate: 1.7927e-05]
		[batch 20/20] avg loss: 0.09296285610560115		[learning rate: 1.7894e-05]
	Learning Rate: 1.78941e-05
	LOSS [training: 0.08889237292258376 | validation: 0.10931716766161297]
	TIME [epoch: 8.13 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09122596357130129		[learning rate: 1.7862e-05]
		[batch 20/20] avg loss: 0.08488514301411544		[learning rate: 1.7829e-05]
	Learning Rate: 1.78292e-05
	LOSS [training: 0.08805555329270838 | validation: 0.10638525164191759]
	TIME [epoch: 8.15 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0859852474001029		[learning rate: 1.7797e-05]
		[batch 20/20] avg loss: 0.08596327146874183		[learning rate: 1.7764e-05]
	Learning Rate: 1.77645e-05
	LOSS [training: 0.08597425943442237 | validation: 0.09562969289261192]
	TIME [epoch: 8.12 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0918326049989872		[learning rate: 1.7732e-05]
		[batch 20/20] avg loss: 0.08420827043632173		[learning rate: 1.77e-05]
	Learning Rate: 1.77e-05
	LOSS [training: 0.08802043771765447 | validation: 0.0996075214092993]
	TIME [epoch: 8.13 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07477410993425518		[learning rate: 1.7668e-05]
		[batch 20/20] avg loss: 0.09393377361173336		[learning rate: 1.7636e-05]
	Learning Rate: 1.76358e-05
	LOSS [training: 0.08435394177299428 | validation: 0.09782879257464208]
	TIME [epoch: 8.13 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09189425203839101		[learning rate: 1.7604e-05]
		[batch 20/20] avg loss: 0.07493872032720943		[learning rate: 1.7572e-05]
	Learning Rate: 1.75718e-05
	LOSS [training: 0.08341648618280022 | validation: 0.09460128532820893]
	TIME [epoch: 8.14 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08337203346964585		[learning rate: 1.754e-05]
		[batch 20/20] avg loss: 0.0852492722423225		[learning rate: 1.7508e-05]
	Learning Rate: 1.7508e-05
	LOSS [training: 0.08431065285598419 | validation: 0.09849834920444138]
	TIME [epoch: 8.13 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0805944601690483		[learning rate: 1.7476e-05]
		[batch 20/20] avg loss: 0.08421418389088989		[learning rate: 1.7444e-05]
	Learning Rate: 1.74445e-05
	LOSS [training: 0.08240432202996911 | validation: 0.09419791554021864]
	TIME [epoch: 8.12 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08103529706612553		[learning rate: 1.7413e-05]
		[batch 20/20] avg loss: 0.08868864841380233		[learning rate: 1.7381e-05]
	Learning Rate: 1.73812e-05
	LOSS [training: 0.0848619727399639 | validation: 0.0964868576975093]
	TIME [epoch: 8.13 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08350186376282312		[learning rate: 1.735e-05]
		[batch 20/20] avg loss: 0.08963440308868863		[learning rate: 1.7318e-05]
	Learning Rate: 1.73181e-05
	LOSS [training: 0.08656813342575587 | validation: 0.09967266871675562]
	TIME [epoch: 8.12 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0837646950947942		[learning rate: 1.7287e-05]
		[batch 20/20] avg loss: 0.08664111055593113		[learning rate: 1.7255e-05]
	Learning Rate: 1.72552e-05
	LOSS [training: 0.08520290282536266 | validation: 0.09948799692020911]
	TIME [epoch: 8.14 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10160300079745517		[learning rate: 1.7224e-05]
		[batch 20/20] avg loss: 0.07390514406167627		[learning rate: 1.7193e-05]
	Learning Rate: 1.71926e-05
	LOSS [training: 0.0877540724295657 | validation: 0.0927531243622591]
	TIME [epoch: 8.11 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08129547244836448		[learning rate: 1.7161e-05]
		[batch 20/20] avg loss: 0.08707063312216864		[learning rate: 1.713e-05]
	Learning Rate: 1.71302e-05
	LOSS [training: 0.08418305278526657 | validation: 0.10276872843548274]
	TIME [epoch: 8.13 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0870706873156379		[learning rate: 1.7099e-05]
		[batch 20/20] avg loss: 0.07688915417269111		[learning rate: 1.7068e-05]
	Learning Rate: 1.70681e-05
	LOSS [training: 0.08197992074416449 | validation: 0.1002710601790397]
	TIME [epoch: 8.12 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08556706193582084		[learning rate: 1.7037e-05]
		[batch 20/20] avg loss: 0.08347495524721145		[learning rate: 1.7006e-05]
	Learning Rate: 1.70061e-05
	LOSS [training: 0.08452100859151614 | validation: 0.0940257862676579]
	TIME [epoch: 8.15 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09299249690736211		[learning rate: 1.6975e-05]
		[batch 20/20] avg loss: 0.07703909657560273		[learning rate: 1.6944e-05]
	Learning Rate: 1.69444e-05
	LOSS [training: 0.08501579674148244 | validation: 0.09566361334197868]
	TIME [epoch: 8.13 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08781199371329536		[learning rate: 1.6914e-05]
		[batch 20/20] avg loss: 0.08507906162324402		[learning rate: 1.6883e-05]
	Learning Rate: 1.68829e-05
	LOSS [training: 0.08644552766826971 | validation: 0.08755524460423422]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_1856.pth
	Model improved!!!
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08158778824192962		[learning rate: 1.6852e-05]
		[batch 20/20] avg loss: 0.08714933302340902		[learning rate: 1.6822e-05]
	Learning Rate: 1.68216e-05
	LOSS [training: 0.08436856063266931 | validation: 0.10503096331869091]
	TIME [epoch: 8.12 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08185881336919161		[learning rate: 1.6791e-05]
		[batch 20/20] avg loss: 0.090437983263218		[learning rate: 1.6761e-05]
	Learning Rate: 1.67606e-05
	LOSS [training: 0.08614839831620479 | validation: 0.09844643534758923]
	TIME [epoch: 8.13 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08263713253522946		[learning rate: 1.673e-05]
		[batch 20/20] avg loss: 0.09055225971562482		[learning rate: 1.67e-05]
	Learning Rate: 1.66998e-05
	LOSS [training: 0.08659469612542711 | validation: 0.09756005772680387]
	TIME [epoch: 8.13 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07764382967125191		[learning rate: 1.6669e-05]
		[batch 20/20] avg loss: 0.09094272270306772		[learning rate: 1.6639e-05]
	Learning Rate: 1.66392e-05
	LOSS [training: 0.08429327618715984 | validation: 0.10057068084819708]
	TIME [epoch: 8.12 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07954521351019858		[learning rate: 1.6609e-05]
		[batch 20/20] avg loss: 0.08894758056296478		[learning rate: 1.6579e-05]
	Learning Rate: 1.65788e-05
	LOSS [training: 0.08424639703658168 | validation: 0.0925925889885596]
	TIME [epoch: 8.13 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08206856956328942		[learning rate: 1.6549e-05]
		[batch 20/20] avg loss: 0.08685849201122538		[learning rate: 1.6519e-05]
	Learning Rate: 1.65186e-05
	LOSS [training: 0.0844635307872574 | validation: 0.09427189732156287]
	TIME [epoch: 8.12 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09264495500174194		[learning rate: 1.6489e-05]
		[batch 20/20] avg loss: 0.07978495868371899		[learning rate: 1.6459e-05]
	Learning Rate: 1.64587e-05
	LOSS [training: 0.08621495684273046 | validation: 0.09237388324405121]
	TIME [epoch: 8.15 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07831021746150704		[learning rate: 1.6429e-05]
		[batch 20/20] avg loss: 0.09114229486034611		[learning rate: 1.6399e-05]
	Learning Rate: 1.63989e-05
	LOSS [training: 0.08472625616092658 | validation: 0.09227811791938707]
	TIME [epoch: 8.12 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08638485257831612		[learning rate: 1.6369e-05]
		[batch 20/20] avg loss: 0.0819868112442287		[learning rate: 1.6339e-05]
	Learning Rate: 1.63394e-05
	LOSS [training: 0.0841858319112724 | validation: 0.09996163109645254]
	TIME [epoch: 8.12 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09270207055378323		[learning rate: 1.631e-05]
		[batch 20/20] avg loss: 0.07320773551400246		[learning rate: 1.628e-05]
	Learning Rate: 1.62801e-05
	LOSS [training: 0.08295490303389284 | validation: 0.09520164977194405]
	TIME [epoch: 8.12 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07049811221755642		[learning rate: 1.6251e-05]
		[batch 20/20] avg loss: 0.09149962269122991		[learning rate: 1.6221e-05]
	Learning Rate: 1.62211e-05
	LOSS [training: 0.08099886745439316 | validation: 0.08588429424756862]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r0_20240219_233648/states/model_tr_study201_1867.pth
	Model improved!!!
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09533538981518115		[learning rate: 1.6192e-05]
		[batch 20/20] avg loss: 0.07446398242448729		[learning rate: 1.6162e-05]
	Learning Rate: 1.61622e-05
	LOSS [training: 0.08489968611983421 | validation: 0.09258504232100451]
	TIME [epoch: 8.14 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07714457136444894		[learning rate: 1.6133e-05]
		[batch 20/20] avg loss: 0.0899735170235452		[learning rate: 1.6104e-05]
	Learning Rate: 1.61035e-05
	LOSS [training: 0.08355904419399707 | validation: 0.09133114909066539]
	TIME [epoch: 8.12 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08439126318208243		[learning rate: 1.6074e-05]
		[batch 20/20] avg loss: 0.0821169383866634		[learning rate: 1.6045e-05]
	Learning Rate: 1.60451e-05
	LOSS [training: 0.08325410078437293 | validation: 0.09136318733788683]
	TIME [epoch: 8.13 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07816261352034912		[learning rate: 1.6016e-05]
		[batch 20/20] avg loss: 0.08501483273005987		[learning rate: 1.5987e-05]
	Learning Rate: 1.59869e-05
	LOSS [training: 0.08158872312520449 | validation: 0.10152435600687601]
	TIME [epoch: 8.13 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08477288834329537		[learning rate: 1.5958e-05]
		[batch 20/20] avg loss: 0.08382051786468808		[learning rate: 1.5929e-05]
	Learning Rate: 1.59288e-05
	LOSS [training: 0.08429670310399172 | validation: 0.10117094580615994]
	TIME [epoch: 8.15 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08791432664976895		[learning rate: 1.59e-05]
		[batch 20/20] avg loss: 0.0802455581669449		[learning rate: 1.5871e-05]
	Learning Rate: 1.5871e-05
	LOSS [training: 0.08407994240835692 | validation: 0.10076328273890822]
	TIME [epoch: 8.12 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08198579984644784		[learning rate: 1.5842e-05]
		[batch 20/20] avg loss: 0.08652882982744153		[learning rate: 1.5813e-05]
	Learning Rate: 1.58134e-05
	LOSS [training: 0.0842573148369447 | validation: 0.0988433762234945]
	TIME [epoch: 8.13 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08985572861818128		[learning rate: 1.5785e-05]
		[batch 20/20] avg loss: 0.08763760727946349		[learning rate: 1.5756e-05]
	Learning Rate: 1.57561e-05
	LOSS [training: 0.08874666794882237 | validation: 0.09962678371584963]
	TIME [epoch: 8.12 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0816616659089569		[learning rate: 1.5727e-05]
		[batch 20/20] avg loss: 0.08955938870339898		[learning rate: 1.5699e-05]
	Learning Rate: 1.56989e-05
	LOSS [training: 0.08561052730617794 | validation: 0.09753303611069439]
	TIME [epoch: 8.14 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0926421551605462		[learning rate: 1.567e-05]
		[batch 20/20] avg loss: 0.08317379591863176		[learning rate: 1.5642e-05]
	Learning Rate: 1.56419e-05
	LOSS [training: 0.08790797553958898 | validation: 0.0980401758898265]
	TIME [epoch: 8.12 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08324842092876031		[learning rate: 1.5613e-05]
		[batch 20/20] avg loss: 0.08450559870166566		[learning rate: 1.5585e-05]
	Learning Rate: 1.55851e-05
	LOSS [training: 0.08387700981521298 | validation: 0.09051379615996456]
	TIME [epoch: 8.12 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08484991146056813		[learning rate: 1.5557e-05]
		[batch 20/20] avg loss: 0.08350394714052757		[learning rate: 1.5529e-05]
	Learning Rate: 1.55286e-05
	LOSS [training: 0.08417692930054785 | validation: 0.10502477304425852]
	TIME [epoch: 8.12 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08675382278462848		[learning rate: 1.55e-05]
		[batch 20/20] avg loss: 0.07918251708956484		[learning rate: 1.5472e-05]
	Learning Rate: 1.54722e-05
	LOSS [training: 0.08296816993709664 | validation: 0.09892043738060675]
	TIME [epoch: 8.13 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08310640165137255		[learning rate: 1.5444e-05]
		[batch 20/20] avg loss: 0.0850416236300804		[learning rate: 1.5416e-05]
	Learning Rate: 1.54161e-05
	LOSS [training: 0.08407401264072649 | validation: 0.10728682439700685]
	TIME [epoch: 8.15 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07991176829676172		[learning rate: 1.5388e-05]
		[batch 20/20] avg loss: 0.08957117776134164		[learning rate: 1.536e-05]
	Learning Rate: 1.53601e-05
	LOSS [training: 0.08474147302905169 | validation: 0.09947366046321446]
	TIME [epoch: 8.12 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08191704365140093		[learning rate: 1.5332e-05]
		[batch 20/20] avg loss: 0.08810698306187495		[learning rate: 1.5304e-05]
	Learning Rate: 1.53044e-05
	LOSS [training: 0.08501201335663797 | validation: 0.09914495381705851]
	TIME [epoch: 8.12 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08532932849162533		[learning rate: 1.5277e-05]
		[batch 20/20] avg loss: 0.08371247509616757		[learning rate: 1.5249e-05]
	Learning Rate: 1.52488e-05
	LOSS [training: 0.08452090179389646 | validation: 0.09811282293216445]
	TIME [epoch: 8.12 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09000835446033552		[learning rate: 1.5221e-05]
		[batch 20/20] avg loss: 0.07949017565397311		[learning rate: 1.5194e-05]
	Learning Rate: 1.51935e-05
	LOSS [training: 0.0847492650571543 | validation: 0.10247547293056551]
	TIME [epoch: 8.15 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0853425100445894		[learning rate: 1.5166e-05]
		[batch 20/20] avg loss: 0.08638938485269829		[learning rate: 1.5138e-05]
	Learning Rate: 1.51384e-05
	LOSS [training: 0.08586594744864384 | validation: 0.0930939935292467]
	TIME [epoch: 8.12 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07989909946715801		[learning rate: 1.5111e-05]
		[batch 20/20] avg loss: 0.08715611759099234		[learning rate: 1.5083e-05]
	Learning Rate: 1.50834e-05
	LOSS [training: 0.08352760852907518 | validation: 0.10007732406773341]
	TIME [epoch: 8.12 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08241540835933539		[learning rate: 1.5056e-05]
		[batch 20/20] avg loss: 0.085640281997907		[learning rate: 1.5029e-05]
	Learning Rate: 1.50287e-05
	LOSS [training: 0.08402784517862119 | validation: 0.10196598211027574]
	TIME [epoch: 8.11 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08267752854681462		[learning rate: 1.5001e-05]
		[batch 20/20] avg loss: 0.08830312034549352		[learning rate: 1.4974e-05]
	Learning Rate: 1.49741e-05
	LOSS [training: 0.08549032444615408 | validation: 0.10225991898998403]
	TIME [epoch: 8.13 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08537364383540688		[learning rate: 1.4947e-05]
		[batch 20/20] avg loss: 0.08179923136334029		[learning rate: 1.492e-05]
	Learning Rate: 1.49198e-05
	LOSS [training: 0.08358643759937355 | validation: 0.09542984471969866]
	TIME [epoch: 8.13 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08513019555715629		[learning rate: 1.4893e-05]
		[batch 20/20] avg loss: 0.08074501003752611		[learning rate: 1.4866e-05]
	Learning Rate: 1.48657e-05
	LOSS [training: 0.08293760279734119 | validation: 0.09692031984675915]
	TIME [epoch: 8.12 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08095021925459608		[learning rate: 1.4839e-05]
		[batch 20/20] avg loss: 0.08503509889424428		[learning rate: 1.4812e-05]
	Learning Rate: 1.48117e-05
	LOSS [training: 0.08299265907442019 | validation: 0.10427047539779012]
	TIME [epoch: 8.13 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08895512390653246		[learning rate: 1.4785e-05]
		[batch 20/20] avg loss: 0.09472785264409311		[learning rate: 1.4758e-05]
	Learning Rate: 1.4758e-05
	LOSS [training: 0.0918414882753128 | validation: 0.1014666820113536]
	TIME [epoch: 8.12 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08717822135766229		[learning rate: 1.4731e-05]
		[batch 20/20] avg loss: 0.08565158442715715		[learning rate: 1.4704e-05]
	Learning Rate: 1.47044e-05
	LOSS [training: 0.08641490289240972 | validation: 0.10863358608953445]
	TIME [epoch: 8.16 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0789470432251084		[learning rate: 1.4678e-05]
		[batch 20/20] avg loss: 0.09486909447928059		[learning rate: 1.4651e-05]
	Learning Rate: 1.4651e-05
	LOSS [training: 0.08690806885219449 | validation: 0.09572841812614738]
	TIME [epoch: 8.13 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08479816871983714		[learning rate: 1.4624e-05]
		[batch 20/20] avg loss: 0.09094055474860886		[learning rate: 1.4598e-05]
	Learning Rate: 1.45979e-05
	LOSS [training: 0.087869361734223 | validation: 0.10381724633584902]
	TIME [epoch: 8.13 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10049430479044348		[learning rate: 1.4571e-05]
		[batch 20/20] avg loss: 0.08339619311265273		[learning rate: 1.4545e-05]
	Learning Rate: 1.45449e-05
	LOSS [training: 0.0919452489515481 | validation: 0.10494825626900961]
	TIME [epoch: 8.12 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08815479928010646		[learning rate: 1.4518e-05]
		[batch 20/20] avg loss: 0.08495515022192172		[learning rate: 1.4492e-05]
	Learning Rate: 1.44921e-05
	LOSS [training: 0.08655497475101409 | validation: 0.1007738644274882]
	TIME [epoch: 8.12 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07619227763036676		[learning rate: 1.4466e-05]
		[batch 20/20] avg loss: 0.09346401173647126		[learning rate: 1.444e-05]
	Learning Rate: 1.44395e-05
	LOSS [training: 0.08482814468341898 | validation: 0.10393826207846496]
	TIME [epoch: 8.13 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09148920194293095		[learning rate: 1.4413e-05]
		[batch 20/20] avg loss: 0.08243458483334339		[learning rate: 1.4387e-05]
	Learning Rate: 1.43871e-05
	LOSS [training: 0.08696189338813717 | validation: 0.10072859107439165]
	TIME [epoch: 8.12 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09533277513686556		[learning rate: 1.4361e-05]
		[batch 20/20] avg loss: 0.0734853623191085		[learning rate: 1.4335e-05]
	Learning Rate: 1.43349e-05
	LOSS [training: 0.08440906872798702 | validation: 0.09467829675613934]
	TIME [epoch: 8.12 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08812519819002064		[learning rate: 1.4309e-05]
		[batch 20/20] avg loss: 0.08146481972281158		[learning rate: 1.4283e-05]
	Learning Rate: 1.42829e-05
	LOSS [training: 0.08479500895641609 | validation: 0.08993758261361343]
	TIME [epoch: 8.14 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08317506028593841		[learning rate: 1.4257e-05]
		[batch 20/20] avg loss: 0.08709330517131016		[learning rate: 1.4231e-05]
	Learning Rate: 1.4231e-05
	LOSS [training: 0.08513418272862427 | validation: 0.09283771218432693]
	TIME [epoch: 8.14 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09040627717692187		[learning rate: 1.4205e-05]
		[batch 20/20] avg loss: 0.07964214031152904		[learning rate: 1.4179e-05]
	Learning Rate: 1.41794e-05
	LOSS [training: 0.08502420874422546 | validation: 0.0965231482219119]
	TIME [epoch: 8.12 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08956555868308755		[learning rate: 1.4154e-05]
		[batch 20/20] avg loss: 0.08067028927034965		[learning rate: 1.4128e-05]
	Learning Rate: 1.41279e-05
	LOSS [training: 0.08511792397671859 | validation: 0.10282099244252034]
	TIME [epoch: 8.13 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08763937168179962		[learning rate: 1.4102e-05]
		[batch 20/20] avg loss: 0.08454350251372438		[learning rate: 1.4077e-05]
	Learning Rate: 1.40767e-05
	LOSS [training: 0.08609143709776203 | validation: 0.10190075574700364]
	TIME [epoch: 8.13 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07805353456008343		[learning rate: 1.4051e-05]
		[batch 20/20] avg loss: 0.08919549008009045		[learning rate: 1.4026e-05]
	Learning Rate: 1.40256e-05
	LOSS [training: 0.08362451232008693 | validation: 0.10794150991844935]
	TIME [epoch: 8.15 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09026101610842742		[learning rate: 1.4e-05]
		[batch 20/20] avg loss: 0.08398258833939728		[learning rate: 1.3975e-05]
	Learning Rate: 1.39747e-05
	LOSS [training: 0.08712180222391237 | validation: 0.09728813665246672]
	TIME [epoch: 8.13 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07782816255040728		[learning rate: 1.3949e-05]
		[batch 20/20] avg loss: 0.09377600615360712		[learning rate: 1.3924e-05]
	Learning Rate: 1.3924e-05
	LOSS [training: 0.08580208435200723 | validation: 0.10220682887795479]
	TIME [epoch: 8.12 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08321380313776777		[learning rate: 1.3899e-05]
		[batch 20/20] avg loss: 0.0834597556833101		[learning rate: 1.3873e-05]
	Learning Rate: 1.38734e-05
	LOSS [training: 0.08333677941053894 | validation: 0.0998029909777279]
	TIME [epoch: 8.13 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07783507820330259		[learning rate: 1.3848e-05]
		[batch 20/20] avg loss: 0.0938710028401569		[learning rate: 1.3823e-05]
	Learning Rate: 1.38231e-05
	LOSS [training: 0.08585304052172973 | validation: 0.10282863239327678]
	TIME [epoch: 8.12 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08498353532380507		[learning rate: 1.3798e-05]
		[batch 20/20] avg loss: 0.08206322193150012		[learning rate: 1.3773e-05]
	Learning Rate: 1.37729e-05
	LOSS [training: 0.0835233786276526 | validation: 0.09541109660462237]
	TIME [epoch: 8.15 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08782016920412664		[learning rate: 1.3748e-05]
		[batch 20/20] avg loss: 0.08143443544888239		[learning rate: 1.3723e-05]
	Learning Rate: 1.37229e-05
	LOSS [training: 0.08462730232650453 | validation: 0.10346101019873746]
	TIME [epoch: 8.12 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07191734177233575		[learning rate: 1.3698e-05]
		[batch 20/20] avg loss: 0.09596705322346344		[learning rate: 1.3673e-05]
	Learning Rate: 1.36731e-05
	LOSS [training: 0.08394219749789958 | validation: 0.09528114234308749]
	TIME [epoch: 8.12 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08849556463142788		[learning rate: 1.3648e-05]
		[batch 20/20] avg loss: 0.08435149661809027		[learning rate: 1.3624e-05]
	Learning Rate: 1.36235e-05
	LOSS [training: 0.08642353062475908 | validation: 0.10240243582876707]
	TIME [epoch: 8.12 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08049545722069797		[learning rate: 1.3599e-05]
		[batch 20/20] avg loss: 0.08786680125100657		[learning rate: 1.3574e-05]
	Learning Rate: 1.35741e-05
	LOSS [training: 0.08418112923585228 | validation: 0.09940701363854071]
	TIME [epoch: 8.15 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08450226499780034		[learning rate: 1.3549e-05]
		[batch 20/20] avg loss: 0.08582296747321114		[learning rate: 1.3525e-05]
	Learning Rate: 1.35248e-05
	LOSS [training: 0.08516261623550572 | validation: 0.10155609574865093]
	TIME [epoch: 8.12 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08426001996416496		[learning rate: 1.35e-05]
		[batch 20/20] avg loss: 0.08665027010664948		[learning rate: 1.3476e-05]
	Learning Rate: 1.34757e-05
	LOSS [training: 0.08545514503540723 | validation: 0.09399981464754828]
	TIME [epoch: 8.12 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07723653816869755		[learning rate: 1.3451e-05]
		[batch 20/20] avg loss: 0.09312306561730611		[learning rate: 1.3427e-05]
	Learning Rate: 1.34268e-05
	LOSS [training: 0.08517980189300181 | validation: 0.1026878898192417]
	TIME [epoch: 8.12 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09264230276187585		[learning rate: 1.3402e-05]
		[batch 20/20] avg loss: 0.0791282899823127		[learning rate: 1.3378e-05]
	Learning Rate: 1.33781e-05
	LOSS [training: 0.08588529637209426 | validation: 0.1026751143667339]
	TIME [epoch: 8.14 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08367973846244049		[learning rate: 1.3354e-05]
		[batch 20/20] avg loss: 0.08457490950803648		[learning rate: 1.333e-05]
	Learning Rate: 1.33296e-05
	LOSS [training: 0.08412732398523849 | validation: 0.09796217845502742]
	TIME [epoch: 8.14 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08286062544722196		[learning rate: 1.3305e-05]
		[batch 20/20] avg loss: 0.08501823659324566		[learning rate: 1.3281e-05]
	Learning Rate: 1.32812e-05
	LOSS [training: 0.0839394310202338 | validation: 0.09963944314219995]
	TIME [epoch: 8.13 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08467816829118224		[learning rate: 1.3257e-05]
		[batch 20/20] avg loss: 0.08496291537167201		[learning rate: 1.3233e-05]
	Learning Rate: 1.3233e-05
	LOSS [training: 0.08482054183142713 | validation: 0.09572467475595897]
	TIME [epoch: 8.12 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0815880346592724		[learning rate: 1.3209e-05]
		[batch 20/20] avg loss: 0.08326787477263815		[learning rate: 1.3185e-05]
	Learning Rate: 1.3185e-05
	LOSS [training: 0.08242795471595529 | validation: 0.0926283535306652]
	TIME [epoch: 8.13 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08569775874058674		[learning rate: 1.3161e-05]
		[batch 20/20] avg loss: 0.08249941157659904		[learning rate: 1.3137e-05]
	Learning Rate: 1.31371e-05
	LOSS [training: 0.08409858515859288 | validation: 0.09879116383773806]
	TIME [epoch: 8.15 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0757441322989506		[learning rate: 1.3113e-05]
		[batch 20/20] avg loss: 0.08620794712155193		[learning rate: 1.3089e-05]
	Learning Rate: 1.30894e-05
	LOSS [training: 0.08097603971025127 | validation: 0.09756385157895064]
	TIME [epoch: 8.12 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07591160488068137		[learning rate: 1.3066e-05]
		[batch 20/20] avg loss: 0.08804852644650565		[learning rate: 1.3042e-05]
	Learning Rate: 1.30419e-05
	LOSS [training: 0.0819800656635935 | validation: 0.09778430187545172]
	TIME [epoch: 8.12 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0771552154821266		[learning rate: 1.3018e-05]
		[batch 20/20] avg loss: 0.08832839423833627		[learning rate: 1.2995e-05]
	Learning Rate: 1.29946e-05
	LOSS [training: 0.08274180486023144 | validation: 0.09833913931462751]
	TIME [epoch: 8.12 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08729054009271424		[learning rate: 1.2971e-05]
		[batch 20/20] avg loss: 0.08558036861975256		[learning rate: 1.2947e-05]
	Learning Rate: 1.29475e-05
	LOSS [training: 0.08643545435623341 | validation: 0.09817096847306114]
	TIME [epoch: 8.14 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07633209234920543		[learning rate: 1.2924e-05]
		[batch 20/20] avg loss: 0.09208968177568343		[learning rate: 1.29e-05]
	Learning Rate: 1.29005e-05
	LOSS [training: 0.08421088706244442 | validation: 0.10171628911290242]
	TIME [epoch: 8.14 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08247617322414214		[learning rate: 1.2877e-05]
		[batch 20/20] avg loss: 0.08923962849363358		[learning rate: 1.2854e-05]
	Learning Rate: 1.28536e-05
	LOSS [training: 0.08585790085888786 | validation: 0.09908718410117381]
	TIME [epoch: 8.12 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09217151263924127		[learning rate: 1.283e-05]
		[batch 20/20] avg loss: 0.07497965059609349		[learning rate: 1.2807e-05]
	Learning Rate: 1.2807e-05
	LOSS [training: 0.08357558161766739 | validation: 0.0926385744245447]
	TIME [epoch: 8.13 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0983385118821056		[learning rate: 1.2784e-05]
		[batch 20/20] avg loss: 0.06580022180690798		[learning rate: 1.2761e-05]
	Learning Rate: 1.27605e-05
	LOSS [training: 0.08206936684450679 | validation: 0.09702241752806824]
	TIME [epoch: 8.12 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07616891415490137		[learning rate: 1.2737e-05]
		[batch 20/20] avg loss: 0.0979244934690275		[learning rate: 1.2714e-05]
	Learning Rate: 1.27142e-05
	LOSS [training: 0.08704670381196443 | validation: 0.09133685587031282]
	TIME [epoch: 8.14 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07843850930334709		[learning rate: 1.2691e-05]
		[batch 20/20] avg loss: 0.09171252112481645		[learning rate: 1.2668e-05]
	Learning Rate: 1.26681e-05
	LOSS [training: 0.08507551521408176 | validation: 0.09308800495212001]
	TIME [epoch: 8.12 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08300561151497907		[learning rate: 1.2645e-05]
		[batch 20/20] avg loss: 0.08648361688694398		[learning rate: 1.2622e-05]
	Learning Rate: 1.26221e-05
	LOSS [training: 0.08474461420096152 | validation: 0.102742562271765]
	TIME [epoch: 8.12 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08792912169210228		[learning rate: 1.2599e-05]
		[batch 20/20] avg loss: 0.08353308027842257		[learning rate: 1.2576e-05]
	Learning Rate: 1.25763e-05
	LOSS [training: 0.08573110098526242 | validation: 0.09106660161560073]
	TIME [epoch: 8.11 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07925783124307144		[learning rate: 1.2553e-05]
		[batch 20/20] avg loss: 0.08737625621704796		[learning rate: 1.2531e-05]
	Learning Rate: 1.25307e-05
	LOSS [training: 0.0833170437300597 | validation: 0.0972024594969435]
	TIME [epoch: 8.14 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08461285439778561		[learning rate: 1.2508e-05]
		[batch 20/20] avg loss: 0.08908223048658251		[learning rate: 1.2485e-05]
	Learning Rate: 1.24852e-05
	LOSS [training: 0.08684754244218404 | validation: 0.10987117864728484]
	TIME [epoch: 8.13 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08826263964499524		[learning rate: 1.2463e-05]
		[batch 20/20] avg loss: 0.08807831906432893		[learning rate: 1.244e-05]
	Learning Rate: 1.24399e-05
	LOSS [training: 0.0881704793546621 | validation: 0.10884719865095059]
	TIME [epoch: 8.13 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07332728434360347		[learning rate: 1.2417e-05]
		[batch 20/20] avg loss: 0.09792671802459788		[learning rate: 1.2395e-05]
	Learning Rate: 1.23947e-05
	LOSS [training: 0.08562700118410066 | validation: 0.09961062655150385]
	TIME [epoch: 8.11 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0795809726208879		[learning rate: 1.2372e-05]
		[batch 20/20] avg loss: 0.09809589972939933		[learning rate: 1.235e-05]
	Learning Rate: 1.23497e-05
	LOSS [training: 0.08883843617514361 | validation: 0.0972244411019722]
	TIME [epoch: 8.12 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07798292868021688		[learning rate: 1.2327e-05]
		[batch 20/20] avg loss: 0.08527883361652613		[learning rate: 1.2305e-05]
	Learning Rate: 1.23049e-05
	LOSS [training: 0.0816308811483715 | validation: 0.0969876872733327]
	TIME [epoch: 8.15 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07535339470854815		[learning rate: 1.2283e-05]
		[batch 20/20] avg loss: 0.09438480547299798		[learning rate: 1.226e-05]
	Learning Rate: 1.22603e-05
	LOSS [training: 0.08486910009077306 | validation: 0.10070129389403468]
	TIME [epoch: 8.12 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08336572544325786		[learning rate: 1.2238e-05]
		[batch 20/20] avg loss: 0.08126916810969197		[learning rate: 1.2216e-05]
	Learning Rate: 1.22158e-05
	LOSS [training: 0.0823174467764749 | validation: 0.09385725091667406]
	TIME [epoch: 8.12 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08211820972621554		[learning rate: 1.2194e-05]
		[batch 20/20] avg loss: 0.08616468490690639		[learning rate: 1.2171e-05]
	Learning Rate: 1.21714e-05
	LOSS [training: 0.08414144731656097 | validation: 0.09383751952882385]
	TIME [epoch: 8.12 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09484624908233069		[learning rate: 1.2149e-05]
		[batch 20/20] avg loss: 0.07932924659140292		[learning rate: 1.2127e-05]
	Learning Rate: 1.21273e-05
	LOSS [training: 0.08708774783686682 | validation: 0.10175170285520097]
	TIME [epoch: 8.15 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09362251365945104		[learning rate: 1.2105e-05]
		[batch 20/20] avg loss: 0.08132410257322513		[learning rate: 1.2083e-05]
	Learning Rate: 1.20833e-05
	LOSS [training: 0.0874733081163381 | validation: 0.10029602038744845]
	TIME [epoch: 8.12 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09058249264815098		[learning rate: 1.2061e-05]
		[batch 20/20] avg loss: 0.08118373301311782		[learning rate: 1.2039e-05]
	Learning Rate: 1.20394e-05
	LOSS [training: 0.0858831128306344 | validation: 0.09551701035597115]
	TIME [epoch: 8.12 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09565163241029707		[learning rate: 1.2018e-05]
		[batch 20/20] avg loss: 0.07950445613647945		[learning rate: 1.1996e-05]
	Learning Rate: 1.19957e-05
	LOSS [training: 0.08757804427338825 | validation: 0.10817167000163083]
	TIME [epoch: 8.12 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09053537502693645		[learning rate: 1.1974e-05]
		[batch 20/20] avg loss: 0.08006621822331075		[learning rate: 1.1952e-05]
	Learning Rate: 1.19522e-05
	LOSS [training: 0.08530079662512362 | validation: 0.09735280824581036]
	TIME [epoch: 8.12 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08464086189601043		[learning rate: 1.193e-05]
		[batch 20/20] avg loss: 0.08835172167359795		[learning rate: 1.1909e-05]
	Learning Rate: 1.19088e-05
	LOSS [training: 0.0864962917848042 | validation: 0.08955875964171466]
	TIME [epoch: 8.15 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08141811724388279		[learning rate: 1.1887e-05]
		[batch 20/20] avg loss: 0.08796947819174365		[learning rate: 1.1866e-05]
	Learning Rate: 1.18656e-05
	LOSS [training: 0.0846937977178132 | validation: 0.09683270796459298]
	TIME [epoch: 8.11 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08358232676271748		[learning rate: 1.1844e-05]
		[batch 20/20] avg loss: 0.08761830612816572		[learning rate: 1.1823e-05]
	Learning Rate: 1.18225e-05
	LOSS [training: 0.0856003164454416 | validation: 0.09380765481751363]
	TIME [epoch: 8.12 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09330058628435225		[learning rate: 1.1801e-05]
		[batch 20/20] avg loss: 0.07744696772459368		[learning rate: 1.178e-05]
	Learning Rate: 1.17796e-05
	LOSS [training: 0.08537377700447295 | validation: 0.09500313236910293]
	TIME [epoch: 8.12 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0866790232317446		[learning rate: 1.1758e-05]
		[batch 20/20] avg loss: 0.0826571325683422		[learning rate: 1.1737e-05]
	Learning Rate: 1.17369e-05
	LOSS [training: 0.08466807790004341 | validation: 0.0946673036295098]
	TIME [epoch: 8.15 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08769390134980418		[learning rate: 1.1716e-05]
		[batch 20/20] avg loss: 0.0855271665808327		[learning rate: 1.1694e-05]
	Learning Rate: 1.16943e-05
	LOSS [training: 0.08661053396531844 | validation: 0.10216740441691641]
	TIME [epoch: 8.12 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09306400655818084		[learning rate: 1.1673e-05]
		[batch 20/20] avg loss: 0.07914414893626762		[learning rate: 1.1652e-05]
	Learning Rate: 1.16518e-05
	LOSS [training: 0.08610407774722421 | validation: 0.0976877486459556]
	TIME [epoch: 8.12 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07507182536815422		[learning rate: 1.1631e-05]
		[batch 20/20] avg loss: 0.09205236081970693		[learning rate: 1.161e-05]
	Learning Rate: 1.16096e-05
	LOSS [training: 0.08356209309393058 | validation: 0.09948599988856037]
	TIME [epoch: 8.12 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08587900017821917		[learning rate: 1.1588e-05]
		[batch 20/20] avg loss: 0.08082996749081768		[learning rate: 1.1567e-05]
	Learning Rate: 1.15674e-05
	LOSS [training: 0.0833544838345184 | validation: 0.09988992124409865]
	TIME [epoch: 8.14 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08321297978160345		[learning rate: 1.1546e-05]
		[batch 20/20] avg loss: 0.08778575898434966		[learning rate: 1.1525e-05]
	Learning Rate: 1.15255e-05
	LOSS [training: 0.08549936938297656 | validation: 0.09960697925789422]
	TIME [epoch: 8.15 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08605811435018075		[learning rate: 1.1505e-05]
		[batch 20/20] avg loss: 0.08538655783783908		[learning rate: 1.1484e-05]
	Learning Rate: 1.14836e-05
	LOSS [training: 0.08572233609400991 | validation: 0.09894769055513807]
	TIME [epoch: 8.13 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0877610376776001		[learning rate: 1.1463e-05]
		[batch 20/20] avg loss: 0.08622712886263752		[learning rate: 1.1442e-05]
	Learning Rate: 1.1442e-05
	LOSS [training: 0.08699408327011882 | validation: 0.08828087124430205]
	TIME [epoch: 8.12 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10277261757718728		[learning rate: 1.1421e-05]
		[batch 20/20] avg loss: 0.06463682651290759		[learning rate: 1.14e-05]
	Learning Rate: 1.14004e-05
	LOSS [training: 0.08370472204504745 | validation: 0.0983265691704412]
	TIME [epoch: 8.13 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0793637460385485		[learning rate: 1.138e-05]
		[batch 20/20] avg loss: 0.09185073387217982		[learning rate: 1.1359e-05]
	Learning Rate: 1.13591e-05
	LOSS [training: 0.08560723995536416 | validation: 0.0961648103324109]
	TIME [epoch: 8.15 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08641386633031847		[learning rate: 1.1338e-05]
		[batch 20/20] avg loss: 0.08510460429270861		[learning rate: 1.1318e-05]
	Learning Rate: 1.13178e-05
	LOSS [training: 0.08575923531151354 | validation: 0.09433024072005793]
	TIME [epoch: 8.13 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0850671269066881		[learning rate: 1.1297e-05]
		[batch 20/20] avg loss: 0.08762852432311694		[learning rate: 1.1277e-05]
	Learning Rate: 1.12768e-05
	LOSS [training: 0.08634782561490252 | validation: 0.09840079413908086]
	TIME [epoch: 8.13 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0820645199498022		[learning rate: 1.1256e-05]
		[batch 20/20] avg loss: 0.08771309360093814		[learning rate: 1.1236e-05]
	Learning Rate: 1.12358e-05
	LOSS [training: 0.08488880677537017 | validation: 0.09469061023724815]
	TIME [epoch: 8.13 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0897723475375396		[learning rate: 1.1215e-05]
		[batch 20/20] avg loss: 0.07501709498688333		[learning rate: 1.1195e-05]
	Learning Rate: 1.11951e-05
	LOSS [training: 0.08239472126221146 | validation: 0.10199736592266417]
	TIME [epoch: 8.14 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08749273148983294		[learning rate: 1.1175e-05]
		[batch 20/20] avg loss: 0.07218611450624032		[learning rate: 1.1154e-05]
	Learning Rate: 1.11544e-05
	LOSS [training: 0.07983942299803662 | validation: 0.09197037863713055]
	TIME [epoch: 8.14 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08225489905799574		[learning rate: 1.1134e-05]
		[batch 20/20] avg loss: 0.08486529804273515		[learning rate: 1.1114e-05]
	Learning Rate: 1.1114e-05
	LOSS [training: 0.08356009855036545 | validation: 0.09543563191872674]
	TIME [epoch: 8.13 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08198660385331034		[learning rate: 1.1094e-05]
		[batch 20/20] avg loss: 0.08710737437945351		[learning rate: 1.1074e-05]
	Learning Rate: 1.10736e-05
	LOSS [training: 0.08454698911638192 | validation: 0.09562637673723201]
	TIME [epoch: 8.13 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07916842009711457		[learning rate: 1.1054e-05]
		[batch 20/20] avg loss: 0.08624379774541743		[learning rate: 1.1033e-05]
	Learning Rate: 1.10334e-05
	LOSS [training: 0.08270610892126601 | validation: 0.09476373402910096]
	TIME [epoch: 8.12 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07897108893084466		[learning rate: 1.1013e-05]
		[batch 20/20] avg loss: 0.08826878698752257		[learning rate: 1.0993e-05]
	Learning Rate: 1.09934e-05
	LOSS [training: 0.08361993795918363 | validation: 0.09200164923162907]
	TIME [epoch: 8.14 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09054404625078201		[learning rate: 1.0973e-05]
		[batch 20/20] avg loss: 0.07478329182606053		[learning rate: 1.0953e-05]
	Learning Rate: 1.09535e-05
	LOSS [training: 0.08266366903842126 | validation: 0.09698373762015627]
	TIME [epoch: 8.13 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08316098075129436		[learning rate: 1.0934e-05]
		[batch 20/20] avg loss: 0.08739743900926103		[learning rate: 1.0914e-05]
	Learning Rate: 1.09137e-05
	LOSS [training: 0.08527920988027769 | validation: 0.10315251362463056]
	TIME [epoch: 8.13 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08968107179482723		[learning rate: 1.0894e-05]
		[batch 20/20] avg loss: 0.08478460716052785		[learning rate: 1.0874e-05]
	Learning Rate: 1.08741e-05
	LOSS [training: 0.08723283947767754 | validation: 0.10294309282031675]
	TIME [epoch: 8.12 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08397115013756926		[learning rate: 1.0854e-05]
		[batch 20/20] avg loss: 0.08886172070299139		[learning rate: 1.0835e-05]
	Learning Rate: 1.08347e-05
	LOSS [training: 0.08641643542028032 | validation: 0.1060783805228438]
	TIME [epoch: 8.14 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09458621712925414		[learning rate: 1.0815e-05]
		[batch 20/20] avg loss: 0.08143192697415792		[learning rate: 1.0795e-05]
	Learning Rate: 1.07954e-05
	LOSS [training: 0.08800907205170602 | validation: 0.09771131342833299]
	TIME [epoch: 8.13 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08330548851979025		[learning rate: 1.0776e-05]
		[batch 20/20] avg loss: 0.08976727931125028		[learning rate: 1.0756e-05]
	Learning Rate: 1.07562e-05
	LOSS [training: 0.08653638391552027 | validation: 0.10499314539480475]
	TIME [epoch: 8.13 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08446290067001663		[learning rate: 1.0737e-05]
		[batch 20/20] avg loss: 0.09169608883098515		[learning rate: 1.0717e-05]
	Learning Rate: 1.07171e-05
	LOSS [training: 0.08807949475050089 | validation: 0.09817561850568876]
	TIME [epoch: 8.12 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08734465309057213		[learning rate: 1.0698e-05]
		[batch 20/20] avg loss: 0.07935828139020747		[learning rate: 1.0678e-05]
	Learning Rate: 1.06782e-05
	LOSS [training: 0.0833514672403898 | validation: 0.10121314518875124]
	TIME [epoch: 8.14 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08089088833379564		[learning rate: 1.0659e-05]
		[batch 20/20] avg loss: 0.08538921813541203		[learning rate: 1.0639e-05]
	Learning Rate: 1.06395e-05
	LOSS [training: 0.08314005323460384 | validation: 0.09525479265042519]
	TIME [epoch: 8.15 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09329918450190951		[learning rate: 1.062e-05]
		[batch 20/20] avg loss: 0.07288868119519065		[learning rate: 1.0601e-05]
	Learning Rate: 1.06009e-05
	LOSS [training: 0.08309393284855006 | validation: 0.09467323074883652]
	TIME [epoch: 8.11 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08818432089017617		[learning rate: 1.0582e-05]
		[batch 20/20] avg loss: 0.08039074312165642		[learning rate: 1.0562e-05]
	Learning Rate: 1.05624e-05
	LOSS [training: 0.0842875320059163 | validation: 0.10074620387786751]
	TIME [epoch: 8.11 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08695310080499266		[learning rate: 1.0543e-05]
		[batch 20/20] avg loss: 0.08655590709643182		[learning rate: 1.0524e-05]
	Learning Rate: 1.05241e-05
	LOSS [training: 0.08675450395071223 | validation: 0.09427913447756407]
	TIME [epoch: 8.12 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08544435696131475		[learning rate: 1.0505e-05]
		[batch 20/20] avg loss: 0.07665266117211912		[learning rate: 1.0486e-05]
	Learning Rate: 1.04859e-05
	LOSS [training: 0.08104850906671694 | validation: 0.08835824933095127]
	TIME [epoch: 8.14 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09049903622444011		[learning rate: 1.0467e-05]
		[batch 20/20] avg loss: 0.07928161896313238		[learning rate: 1.0448e-05]
	Learning Rate: 1.04478e-05
	LOSS [training: 0.08489032759378624 | validation: 0.09759557627351753]
	TIME [epoch: 8.13 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08301675314005189		[learning rate: 1.0429e-05]
		[batch 20/20] avg loss: 0.08728649384203466		[learning rate: 1.041e-05]
	Learning Rate: 1.04099e-05
	LOSS [training: 0.08515162349104327 | validation: 0.09405008545521239]
	TIME [epoch: 8.12 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08603505687912985		[learning rate: 1.0391e-05]
		[batch 20/20] avg loss: 0.07723876556451768		[learning rate: 1.0372e-05]
	Learning Rate: 1.03721e-05
	LOSS [training: 0.08163691122182379 | validation: 0.09750159327032044]
	TIME [epoch: 8.12 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0927072725387719		[learning rate: 1.0353e-05]
		[batch 20/20] avg loss: 0.07674030839077009		[learning rate: 1.0335e-05]
	Learning Rate: 1.03345e-05
	LOSS [training: 0.08472379046477098 | validation: 0.10111642123220908]
	TIME [epoch: 8.12 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08704989658923519		[learning rate: 1.0316e-05]
		[batch 20/20] avg loss: 0.08753598910212376		[learning rate: 1.0297e-05]
	Learning Rate: 1.0297e-05
	LOSS [training: 0.08729294284567948 | validation: 0.09821770495205664]
	TIME [epoch: 8.16 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07840917930153335		[learning rate: 1.0278e-05]
		[batch 20/20] avg loss: 0.0907245091134946		[learning rate: 1.026e-05]
	Learning Rate: 1.02596e-05
	LOSS [training: 0.08456684420751398 | validation: 0.10004456691095107]
	TIME [epoch: 8.12 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10204612649970377		[learning rate: 1.0241e-05]
		[batch 20/20] avg loss: 0.078151084185511		[learning rate: 1.0222e-05]
	Learning Rate: 1.02224e-05
	LOSS [training: 0.09009860534260739 | validation: 0.09732601863704303]
	TIME [epoch: 8.12 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08149320841264977		[learning rate: 1.0204e-05]
		[batch 20/20] avg loss: 0.08763783888341879		[learning rate: 1.0185e-05]
	Learning Rate: 1.01853e-05
	LOSS [training: 0.0845655236480343 | validation: 0.10146243888512396]
	TIME [epoch: 8.12 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.098190775964918		[learning rate: 1.0167e-05]
		[batch 20/20] avg loss: 0.07162406916968231		[learning rate: 1.0148e-05]
	Learning Rate: 1.01483e-05
	LOSS [training: 0.08490742256730013 | validation: 0.10027411138410462]
	TIME [epoch: 8.15 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08956892756996344		[learning rate: 1.013e-05]
		[batch 20/20] avg loss: 0.08025492005743562		[learning rate: 1.0112e-05]
	Learning Rate: 1.01115e-05
	LOSS [training: 0.08491192381369952 | validation: 0.09284780026662101]
	TIME [epoch: 8.13 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0781413080836183		[learning rate: 1.0093e-05]
		[batch 20/20] avg loss: 0.0918828591204486		[learning rate: 1.0075e-05]
	Learning Rate: 1.00748e-05
	LOSS [training: 0.08501208360203347 | validation: 0.0969195638849075]
	TIME [epoch: 8.13 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08160381089080113		[learning rate: 1.0057e-05]
		[batch 20/20] avg loss: 0.0868306867513152		[learning rate: 1.0038e-05]
	Learning Rate: 1.00382e-05
	LOSS [training: 0.08421724882105817 | validation: 0.0996924987484998]
	TIME [epoch: 8.12 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08512314833311321		[learning rate: 1.002e-05]
		[batch 20/20] avg loss: 0.08406217988844425		[learning rate: 1.0002e-05]
	Learning Rate: 1.00018e-05
	LOSS [training: 0.08459266411077873 | validation: 0.09375319682266967]
	TIME [epoch: 8.13 sec]
Finished training in 16455.007 seconds.
