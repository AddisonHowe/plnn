Args:
Namespace(name='model_tr_study201', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2', training_data='data/transition_rate_studies/tr_study201/tr_study201_training/r2', validation_data='data/transition_rate_studies/tr_study201/tr_study201_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1270915095

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.847232207230347		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.507020637987393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.67712642260887 | validation: 5.405659952905442]
	TIME [epoch: 78.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.335940544588423		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.50103198124722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.91848626291782 | validation: 3.9712590683914337]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.637694393700775		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.205216409828327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.42145540176455 | validation: 4.17150636298488]
	TIME [epoch: 8.32 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.465456922449295		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.91576431417133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.190610618310311 | validation: 4.247996115117116]
	TIME [epoch: 8.35 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.651395145216854		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.606153276461337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6287742108390955 | validation: 3.754825844255874]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.514226770399216		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.565664637660113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.539945704029664 | validation: 9.022169982628185]
	TIME [epoch: 8.35 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.377717115271096		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.341232433936799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.859474774603948 | validation: 3.9230905929384963]
	TIME [epoch: 8.33 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.417291136305389		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.418956981378146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.418124058841768 | validation: 5.227071256267655]
	TIME [epoch: 8.35 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.715557980223067		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.559928653853518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.637743317038292 | validation: 5.266895587398107]
	TIME [epoch: 8.34 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.451455137011198		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.266877178809463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.359166157910329 | validation: 3.462795195739896]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.059997064000671		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.871544536490263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9657708002454664 | validation: 3.0285631507750925]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.808345989894174		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8441146444614214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.826230317177798 | validation: 3.4787194254524363]
	TIME [epoch: 8.33 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.043612938292145		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8223934958289107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.933003217060528 | validation: 3.0058295224255924]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8927131334678733		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.380933868257607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6368235008627408 | validation: 2.8428748890287308]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.203561614793931		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.2761213430002925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2398414788971115 | validation: 2.454005462863981]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0839703717686375		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.133452004054835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1087111879117364 | validation: 2.573377236373682]
	TIME [epoch: 8.35 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1089608103464994		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.979999335610262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0444800729783816 | validation: 2.2140978046147737]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9845525340883654		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8649925732137205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.424772553651043 | validation: 3.2953180106519624]
	TIME [epoch: 8.33 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.564727422984209		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8951123709207263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.229919896952468 | validation: 2.48896220183221]
	TIME [epoch: 8.35 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.655913961152828		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7595446167412985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.707729288947063 | validation: 2.3259784523786724]
	TIME [epoch: 8.34 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.701940664480526		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.582447200557622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.642193932519074 | validation: 1.6948230546378018]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5833783077236454		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.650260817635801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6168195626797233 | validation: 2.6561167306677618]
	TIME [epoch: 8.35 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6307125408006797		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6074276895922823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6190701151964815 | validation: 1.621697939575895]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4297009302932095		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5474771071376243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.488589018715417 | validation: 2.513590065485992]
	TIME [epoch: 8.34 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.34796671532998		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3183094119565206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3331380636432506 | validation: 2.3928032390770024]
	TIME [epoch: 8.31 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5466648893151653		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.343934532626227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4452997109706964 | validation: 1.8245142194976216]
	TIME [epoch: 8.32 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3826925703571034		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.266649055445883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3246708129014935 | validation: 2.0976101473818045]
	TIME [epoch: 8.32 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2763928733273704		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.422149560048092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.349271216687731 | validation: 1.9158965178409153]
	TIME [epoch: 8.34 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.17173030555044		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.457747250345133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.314738777947787 | validation: 1.4392890806732037]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3817299397431		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.038491170260226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.710110555001663 | validation: 1.6141822500530385]
	TIME [epoch: 8.35 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4783781084622745		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1843006165779792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3313393625201266 | validation: 1.4594712990130203]
	TIME [epoch: 8.32 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.183661562198993		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3990357462756697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.291348654237331 | validation: 1.9456493087659064]
	TIME [epoch: 8.35 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6717425066999687		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1357484744568445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.403745490578406 | validation: 1.6000316152527294]
	TIME [epoch: 8.34 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.155663789517177		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.240486313607861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.198075051562519 | validation: 1.2920787581028268]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7004158331588854		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1413116511508514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.420863742154869 | validation: 1.3282471853877864]
	TIME [epoch: 8.31 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.432934074546783		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.690452509921309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.561693292234046 | validation: 1.6756187369025275]
	TIME [epoch: 8.35 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1333364155563594		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1179468572442355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1256416364002977 | validation: 1.3126853324007155]
	TIME [epoch: 8.31 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1946321323106313		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.567902606647446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3812673694790383 | validation: 3.0141363511565045]
	TIME [epoch: 8.31 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3271470291480396		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.241583843034544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2843654360912917 | validation: 1.5686142504487888]
	TIME [epoch: 8.31 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.328736471055329		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.094625895333925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2116811831946275 | validation: 1.6059201263999163]
	TIME [epoch: 8.33 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0700184800229016		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5170684454952914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.293543462759096 | validation: 1.7508465260526156]
	TIME [epoch: 8.31 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.019112594083659		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.096010307204481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0575614506440694 | validation: 1.6820125787724753]
	TIME [epoch: 8.34 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.14097913388745		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2839304585277778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2124547962076133 | validation: 1.8342385464626685]
	TIME [epoch: 8.31 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0507189392074343		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4380770555400892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2443979973737616 | validation: 2.598424202737701]
	TIME [epoch: 8.35 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.154442181321248		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.134684476008701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.144563328664974 | validation: 1.8745104149558653]
	TIME [epoch: 8.33 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9403192469849764		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1341485312275705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0372338891062736 | validation: 1.9175201754580575]
	TIME [epoch: 8.34 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0617846561543676		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9706424613572082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.016213558755788 | validation: 1.7429571060035118]
	TIME [epoch: 8.31 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.027188510551734		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.078651477660816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0529199941062757 | validation: 1.9502932754330589]
	TIME [epoch: 8.33 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9118043257677808		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.19896105199183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0553826888798055 | validation: 1.5894222919024537]
	TIME [epoch: 8.31 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0207819390292463		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9029187995478545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9618503692885505 | validation: 1.4550898094979494]
	TIME [epoch: 8.3 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9570130745617713		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5452602738238435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7511366741928076 | validation: 0.9201083884792453]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6645951802237868		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0537632292070778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.859179204715432 | validation: 2.2404121547553757]
	TIME [epoch: 8.36 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7667429362789686		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5312554589148293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6489991975968992 | validation: 0.9206361914798962]
	TIME [epoch: 8.33 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6052458891129948		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7861083789090262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6956771340110106 | validation: 1.2672708568562316]
	TIME [epoch: 8.33 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.624678745183525		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6664207410217997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6455497431026622 | validation: 1.00119564364349]
	TIME [epoch: 8.33 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5550229488541225		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5441859710757888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5496044599649559 | validation: 1.2561458974848374]
	TIME [epoch: 8.38 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.613019094746073		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.055196039493653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8341075671198628 | validation: 0.8531291130413529]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.649110870576143		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5073568605544683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5782338655653059 | validation: 1.0544904997345488]
	TIME [epoch: 8.31 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3812743382846926		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.055257235328934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7182657868068134 | validation: 1.008408161219744]
	TIME [epoch: 8.32 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4608892308033314		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4472173792722711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4540533050378015 | validation: 1.2838240598607165]
	TIME [epoch: 8.32 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.512033450971371		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.413831692702614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4629325718369923 | validation: 1.7257601987532445]
	TIME [epoch: 8.3 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5715713948314667		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6091722514323092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.590371823131888 | validation: 2.0999250788827806]
	TIME [epoch: 8.3 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.782849773703619		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5681859064405592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6755178400720894 | validation: 0.7232734058764458]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9317385065874801		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8644771633891484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.898107834988314 | validation: 0.6861020487215983]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6782143026140797		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6054815821150186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6418479423645493 | validation: 1.0326829918414884]
	TIME [epoch: 8.31 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7456129737909372		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.796811423478515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.771212198634726 | validation: 0.8105854998271643]
	TIME [epoch: 8.31 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3653982225368009		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9816660935904988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6735321580636497 | validation: 0.6238075677545557]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5929685235182953		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5212671686748758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5571178460965855 | validation: 0.7580543457064224]
	TIME [epoch: 8.32 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3712566786247264		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4338294921523473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.402543085388537 | validation: 0.8987637342322522]
	TIME [epoch: 8.29 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.503442144967199		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.451588708906668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4775154269369335 | validation: 1.3286802384304488]
	TIME [epoch: 8.3 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5372133858478325		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5144679986968788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.525840692272356 | validation: 2.2717177759825606]
	TIME [epoch: 8.32 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6361214367062047		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.216611619286966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9263665279965854 | validation: 1.2284305633004986]
	TIME [epoch: 8.32 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9899512335498346		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6587884704193065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8243698519845704 | validation: 0.5937503495053189]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4639384413645966		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6288781296974382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5464082855310173 | validation: 1.3750015918073584]
	TIME [epoch: 8.32 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6318397178165331		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.491219783534045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.561529750675289 | validation: 0.832854859261313]
	TIME [epoch: 8.32 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5328283968138667		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6756776805248332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6042530386693499 | validation: 0.7938546928985992]
	TIME [epoch: 8.32 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4831774975464842		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4049589125210833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4440682050337839 | validation: 0.8354573371413447]
	TIME [epoch: 8.32 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5368933249058863		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5590845945617533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5479889597338201 | validation: 1.1011233643103597]
	TIME [epoch: 8.31 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7131216489410832		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.817702630509014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7654121397250488 | validation: 0.7246894156974943]
	TIME [epoch: 8.32 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.524437827033166		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5121718368927326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5183048319629493 | validation: 0.6804192327851294]
	TIME [epoch: 8.31 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.082036501612944		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.398508114662621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7402723081377822 | validation: 0.5836661057954875]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7054207577620393		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8662194339959197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7858200958789798 | validation: 2.1496862299132817]
	TIME [epoch: 8.33 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7072835384655487		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8479346601949764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7776090993302625 | validation: 0.7773332407629128]
	TIME [epoch: 8.34 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6549490357478878		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0039384005078125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8294437181278504 | validation: 1.0306549533906504]
	TIME [epoch: 8.36 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1423931950229855		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.840601682807041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9914974389150129 | validation: 1.1523770755587663]
	TIME [epoch: 8.33 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4588134509137425		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4216069881393796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.440210219526561 | validation: 1.179497113873404]
	TIME [epoch: 8.33 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.376996552184573		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7641520438836544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5705742980341137 | validation: 0.6977537985178298]
	TIME [epoch: 8.36 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8108288342399685		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9249394145712053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8678841244055875 | validation: 0.9124060811515067]
	TIME [epoch: 8.36 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5731744355519113		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4918168251215964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5324956303367538 | validation: 1.369666340029239]
	TIME [epoch: 8.32 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.916134591050645		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5148223040068673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2154784475287563 | validation: 1.1400048153619176]
	TIME [epoch: 8.32 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.933404834839737		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.75701601466514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8452104247524386 | validation: 0.6851016621183383]
	TIME [epoch: 8.35 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.469023414794978		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4262350398592427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4476292273271105 | validation: 0.7797315001609163]
	TIME [epoch: 8.33 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4401757551297032		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6485848577274305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5443803064285668 | validation: 1.3543597700549634]
	TIME [epoch: 8.33 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6398727617812523		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.658164429722148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6490185957516998 | validation: 0.7622272670537376]
	TIME [epoch: 8.36 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8124349249879952		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.493737244735422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6530860848617084 | validation: 0.8999589479354342]
	TIME [epoch: 8.36 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3273184098595598		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5188671373730664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.423092773616313 | validation: 1.1468272051159412]
	TIME [epoch: 8.33 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3589722661670258		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6296483544285283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.494310310297777 | validation: 1.946898183832919]
	TIME [epoch: 8.33 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5003554438973992		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.537714227212513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5190348355549568 | validation: 1.0543702093012037]
	TIME [epoch: 8.36 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6325593959471636		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4816684336164188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5571139147817912 | validation: 1.1189318630038843]
	TIME [epoch: 8.35 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3061677289713973		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4363057880055552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3712367584884764 | validation: 0.7088203778676101]
	TIME [epoch: 8.33 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5171214782157176		[learning rate: 0.0099837]
		[batch 20/20] avg loss: 1.4511562396587974		[learning rate: 0.0099655]
	Learning Rate: 0.00996552
	LOSS [training: 1.4841388589372575 | validation: 0.7717397969292468]
	TIME [epoch: 8.33 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4019079348269998		[learning rate: 0.0099474]
		[batch 20/20] avg loss: 1.3531036326506785		[learning rate: 0.0099294]
	Learning Rate: 0.00992935
	LOSS [training: 1.3775057837388387 | validation: 1.3813185331407427]
	TIME [epoch: 8.32 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8359532672063221		[learning rate: 0.0099113]
		[batch 20/20] avg loss: 1.3468757324912792		[learning rate: 0.0098933]
	Learning Rate: 0.00989332
	LOSS [training: 1.5914144998488005 | validation: 1.2277948427181682]
	TIME [epoch: 8.35 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3080665829553575		[learning rate: 0.0098754]
		[batch 20/20] avg loss: 1.834561088870656		[learning rate: 0.0098574]
	Learning Rate: 0.00985742
	LOSS [training: 1.5713138359130068 | validation: 1.7123563037480394]
	TIME [epoch: 8.35 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6557494290989982		[learning rate: 0.0098395]
		[batch 20/20] avg loss: 2.1016703168928546		[learning rate: 0.0098216]
	Learning Rate: 0.00982164
	LOSS [training: 1.8787098729959262 | validation: 0.6889235219658655]
	TIME [epoch: 8.33 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9957615779586295		[learning rate: 0.0098038]
		[batch 20/20] avg loss: 1.471449394386192		[learning rate: 0.009786]
	Learning Rate: 0.009786
	LOSS [training: 1.7336054861724108 | validation: 0.7425113242974173]
	TIME [epoch: 8.33 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.515164333051692		[learning rate: 0.0097682]
		[batch 20/20] avg loss: 1.3915476497138162		[learning rate: 0.0097505]
	Learning Rate: 0.00975049
	LOSS [training: 1.4533559913827538 | validation: 0.600346839776255]
	TIME [epoch: 8.37 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.498343937769103		[learning rate: 0.0097328]
		[batch 20/20] avg loss: 1.408302561383224		[learning rate: 0.0097151]
	Learning Rate: 0.0097151
	LOSS [training: 1.453323249576164 | validation: 1.0889590425737918]
	TIME [epoch: 8.34 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3285306825822976		[learning rate: 0.0096975]
		[batch 20/20] avg loss: 1.5486681520435432		[learning rate: 0.0096798]
	Learning Rate: 0.00967984
	LOSS [training: 1.4385994173129206 | validation: 1.267940505610853]
	TIME [epoch: 8.32 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.520571324981152		[learning rate: 0.0096623]
		[batch 20/20] avg loss: 1.4539808162926797		[learning rate: 0.0096447]
	Learning Rate: 0.00964472
	LOSS [training: 1.4872760706369157 | validation: 0.8368060094452072]
	TIME [epoch: 8.31 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5983147970695721		[learning rate: 0.0096272]
		[batch 20/20] avg loss: 1.534933547815847		[learning rate: 0.0096097]
	Learning Rate: 0.00960972
	LOSS [training: 1.5666241724427097 | validation: 1.1348497384636387]
	TIME [epoch: 8.35 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5210917669387503		[learning rate: 0.0095923]
		[batch 20/20] avg loss: 1.3594650593297941		[learning rate: 0.0095748]
	Learning Rate: 0.00957484
	LOSS [training: 1.440278413134272 | validation: 0.7422917372125684]
	TIME [epoch: 8.33 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4412234911770918		[learning rate: 0.0095575]
		[batch 20/20] avg loss: 1.4853421333365864		[learning rate: 0.0095401]
	Learning Rate: 0.00954009
	LOSS [training: 1.4632828122568389 | validation: 0.7384734747207162]
	TIME [epoch: 8.32 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5639411465295971		[learning rate: 0.0095228]
		[batch 20/20] avg loss: 1.3277199583307708		[learning rate: 0.0095055]
	Learning Rate: 0.00950547
	LOSS [training: 1.4458305524301844 | validation: 1.530565089959199]
	TIME [epoch: 8.32 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8116062913699351		[learning rate: 0.0094882]
		[batch 20/20] avg loss: 1.8074713054835665		[learning rate: 0.009471]
	Learning Rate: 0.00947098
	LOSS [training: 1.809538798426751 | validation: 2.0341560222146677]
	TIME [epoch: 8.37 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9937774481198047		[learning rate: 0.0094538]
		[batch 20/20] avg loss: 1.6615212038808935		[learning rate: 0.0094366]
	Learning Rate: 0.0094366
	LOSS [training: 1.827649326000349 | validation: 1.538269969359368]
	TIME [epoch: 8.34 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3417178169027082		[learning rate: 0.0094195]
		[batch 20/20] avg loss: 1.5242853636691558		[learning rate: 0.0094024]
	Learning Rate: 0.00940236
	LOSS [training: 1.4330015902859319 | validation: 0.5504033120656797]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3038732600260248		[learning rate: 0.0093853]
		[batch 20/20] avg loss: 1.8190416408032888		[learning rate: 0.0093682]
	Learning Rate: 0.00936824
	LOSS [training: 1.561457450414657 | validation: 0.6794828488189565]
	TIME [epoch: 8.33 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5403059876417846		[learning rate: 0.0093512]
		[batch 20/20] avg loss: 1.4264871629268774		[learning rate: 0.0093342]
	Learning Rate: 0.00933424
	LOSS [training: 1.4833965752843308 | validation: 1.7429106134154415]
	TIME [epoch: 8.37 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2442308690590411		[learning rate: 0.0093173]
		[batch 20/20] avg loss: 1.4394057473694089		[learning rate: 0.0093004]
	Learning Rate: 0.00930036
	LOSS [training: 1.341818308214225 | validation: 0.4824588911275337]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.317218759622961		[learning rate: 0.0092835]
		[batch 20/20] avg loss: 1.1765410021782692		[learning rate: 0.0092666]
	Learning Rate: 0.00926661
	LOSS [training: 1.2468798809006147 | validation: 0.9093470592392545]
	TIME [epoch: 8.32 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3095232637932643		[learning rate: 0.0092498]
		[batch 20/20] avg loss: 1.1628720692156516		[learning rate: 0.009233]
	Learning Rate: 0.00923298
	LOSS [training: 1.236197666504458 | validation: 0.8904436100642716]
	TIME [epoch: 8.31 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.436173182137873		[learning rate: 0.0092162]
		[batch 20/20] avg loss: 1.4147691461216176		[learning rate: 0.0091995]
	Learning Rate: 0.00919948
	LOSS [training: 1.425471164129745 | validation: 0.9061623008818119]
	TIME [epoch: 8.33 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.238966225297548		[learning rate: 0.0091828]
		[batch 20/20] avg loss: 1.3780206259878596		[learning rate: 0.0091661]
	Learning Rate: 0.00916609
	LOSS [training: 1.3084934256427037 | validation: 1.4013388422004027]
	TIME [epoch: 8.31 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.246909637549793		[learning rate: 0.0091494]
		[batch 20/20] avg loss: 1.2150710985293551		[learning rate: 0.0091328]
	Learning Rate: 0.00913283
	LOSS [training: 1.2309903680395742 | validation: 0.9535401912090085]
	TIME [epoch: 8.33 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4882737490573743		[learning rate: 0.0091162]
		[batch 20/20] avg loss: 1.2562517661191714		[learning rate: 0.0090997]
	Learning Rate: 0.00909968
	LOSS [training: 1.3722627575882729 | validation: 0.5611091626318434]
	TIME [epoch: 8.33 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6516703970129485		[learning rate: 0.0090832]
		[batch 20/20] avg loss: 1.1094857283451733		[learning rate: 0.0090667]
	Learning Rate: 0.00906666
	LOSS [training: 1.380578062679061 | validation: 0.9786458772400417]
	TIME [epoch: 8.34 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4164692169194635		[learning rate: 0.0090502]
		[batch 20/20] avg loss: 1.2041922506148568		[learning rate: 0.0090338]
	Learning Rate: 0.00903376
	LOSS [training: 1.3103307337671601 | validation: 0.8931169467391622]
	TIME [epoch: 8.32 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2809099178532588		[learning rate: 0.0090174]
		[batch 20/20] avg loss: 1.30208656636287		[learning rate: 0.009001]
	Learning Rate: 0.00900097
	LOSS [training: 1.2914982421080643 | validation: 1.0156240541353063]
	TIME [epoch: 8.34 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9859533191880562		[learning rate: 0.0089846]
		[batch 20/20] avg loss: 0.9522407872130747		[learning rate: 0.0089683]
	Learning Rate: 0.00896831
	LOSS [training: 0.9690970532005652 | validation: 0.864009824638436]
	TIME [epoch: 8.33 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9728626962760346		[learning rate: 0.008952]
		[batch 20/20] avg loss: 0.9387999564942586		[learning rate: 0.0089358]
	Learning Rate: 0.00893576
	LOSS [training: 0.9558313263851467 | validation: 1.233709722075197]
	TIME [epoch: 8.34 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9285964926170507		[learning rate: 0.0089195]
		[batch 20/20] avg loss: 0.9110039673509753		[learning rate: 0.0089033]
	Learning Rate: 0.00890333
	LOSS [training: 0.9198002299840132 | validation: 0.5992580560052605]
	TIME [epoch: 8.31 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1748604172108754		[learning rate: 0.0088872]
		[batch 20/20] avg loss: 0.8723935474298777		[learning rate: 0.008871]
	Learning Rate: 0.00887102
	LOSS [training: 1.0236269823203767 | validation: 1.40582984148926]
	TIME [epoch: 8.3 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.003024672215409		[learning rate: 0.0088549]
		[batch 20/20] avg loss: 1.1607548314624583		[learning rate: 0.0088388]
	Learning Rate: 0.00883883
	LOSS [training: 1.0818897518389339 | validation: 0.39926543819491356]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8204301048369096		[learning rate: 0.0088228]
		[batch 20/20] avg loss: 1.2768598174208836		[learning rate: 0.0088068]
	Learning Rate: 0.00880675
	LOSS [training: 1.0486449611288966 | validation: 0.5288004983230519]
	TIME [epoch: 8.35 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7501891692541964		[learning rate: 0.0087908]
		[batch 20/20] avg loss: 0.894417255866869		[learning rate: 0.0087748]
	Learning Rate: 0.00877479
	LOSS [training: 0.8223032125605325 | validation: 0.6251835765068136]
	TIME [epoch: 8.33 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9837183276280562		[learning rate: 0.0087589]
		[batch 20/20] avg loss: 0.881080653672756		[learning rate: 0.0087429]
	Learning Rate: 0.00874295
	LOSS [training: 0.9323994906504061 | validation: 0.5999382262790234]
	TIME [epoch: 8.31 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.816852289278309		[learning rate: 0.0087271]
		[batch 20/20] avg loss: 0.9121694886904637		[learning rate: 0.0087112]
	Learning Rate: 0.00871122
	LOSS [training: 0.8645108889843863 | validation: 2.0609302835380303]
	TIME [epoch: 8.31 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0948269380654367		[learning rate: 0.0086954]
		[batch 20/20] avg loss: 0.8508413434509879		[learning rate: 0.0086796]
	Learning Rate: 0.00867961
	LOSS [training: 0.9728341407582123 | validation: 1.8357395767336195]
	TIME [epoch: 8.37 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0905491356688992		[learning rate: 0.0086638]
		[batch 20/20] avg loss: 0.9886042591922279		[learning rate: 0.0086481]
	Learning Rate: 0.00864811
	LOSS [training: 1.0395766974305638 | validation: 0.6563883147102982]
	TIME [epoch: 8.31 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8396807596880386		[learning rate: 0.0086324]
		[batch 20/20] avg loss: 1.0946136731137375		[learning rate: 0.0086167]
	Learning Rate: 0.00861672
	LOSS [training: 0.9671472164008881 | validation: 1.03829589433952]
	TIME [epoch: 8.3 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8173344943023875		[learning rate: 0.0086011]
		[batch 20/20] avg loss: 1.0723168855173864		[learning rate: 0.0085855]
	Learning Rate: 0.00858545
	LOSS [training: 0.9448256899098867 | validation: 0.5672863783547173]
	TIME [epoch: 8.3 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8313193039789484		[learning rate: 0.0085699]
		[batch 20/20] avg loss: 1.2297039156556517		[learning rate: 0.0085543]
	Learning Rate: 0.00855429
	LOSS [training: 1.0305116098173 | validation: 1.2144631631354526]
	TIME [epoch: 8.33 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.974090884896472		[learning rate: 0.0085388]
		[batch 20/20] avg loss: 0.6859369817696142		[learning rate: 0.0085232]
	Learning Rate: 0.00852325
	LOSS [training: 0.8300139333330427 | validation: 1.0824343150937648]
	TIME [epoch: 8.31 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.96515108641556		[learning rate: 0.0085078]
		[batch 20/20] avg loss: 1.0545896125168945		[learning rate: 0.0084923]
	Learning Rate: 0.00849232
	LOSS [training: 1.0098703494662273 | validation: 0.6346348548746421]
	TIME [epoch: 8.3 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8879093767314495		[learning rate: 0.0084769]
		[batch 20/20] avg loss: 0.7581586105528555		[learning rate: 0.0084615]
	Learning Rate: 0.0084615
	LOSS [training: 0.8230339936421526 | validation: 1.4734552632053943]
	TIME [epoch: 8.33 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0792879869199672		[learning rate: 0.0084461]
		[batch 20/20] avg loss: 0.8991578303334116		[learning rate: 0.0084308]
	Learning Rate: 0.00843079
	LOSS [training: 0.9892229086266893 | validation: 1.7772950324614465]
	TIME [epoch: 8.35 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.000696492166596		[learning rate: 0.0084155]
		[batch 20/20] avg loss: 0.9226786431174876		[learning rate: 0.0084002]
	Learning Rate: 0.0084002
	LOSS [training: 0.9616875676420419 | validation: 0.46606921465393447]
	TIME [epoch: 8.31 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.791052326870744		[learning rate: 0.0083849]
		[batch 20/20] avg loss: 0.8046630944014076		[learning rate: 0.0083697]
	Learning Rate: 0.00836971
	LOSS [training: 0.7978577106360758 | validation: 0.668597420855011]
	TIME [epoch: 8.31 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6619355001512479		[learning rate: 0.0083545]
		[batch 20/20] avg loss: 0.8317280215033207		[learning rate: 0.0083393]
	Learning Rate: 0.00833934
	LOSS [training: 0.7468317608272843 | validation: 0.9420537639648987]
	TIME [epoch: 8.34 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2581548172497548		[learning rate: 0.0083242]
		[batch 20/20] avg loss: 0.9930535251225191		[learning rate: 0.0083091]
	Learning Rate: 0.00830907
	LOSS [training: 1.1256041711861369 | validation: 0.5806238053660602]
	TIME [epoch: 8.33 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9107049780433012		[learning rate: 0.008294]
		[batch 20/20] avg loss: 1.1823546393998305		[learning rate: 0.0082789]
	Learning Rate: 0.00827892
	LOSS [training: 1.046529808721566 | validation: 1.7942856966551446]
	TIME [epoch: 8.3 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.143682824422917		[learning rate: 0.0082639]
		[batch 20/20] avg loss: 0.9773762393526736		[learning rate: 0.0082489]
	Learning Rate: 0.00824887
	LOSS [training: 1.0605295318877954 | validation: 0.49656556922126754]
	TIME [epoch: 8.31 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8965042424875309		[learning rate: 0.0082339]
		[batch 20/20] avg loss: 1.325062791399608		[learning rate: 0.0082189]
	Learning Rate: 0.00821894
	LOSS [training: 1.1107835169435694 | validation: 0.6096149156635562]
	TIME [epoch: 8.3 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8137781978533771		[learning rate: 0.008204]
		[batch 20/20] avg loss: 0.811777098151334		[learning rate: 0.0081891]
	Learning Rate: 0.00818911
	LOSS [training: 0.8127776480023556 | validation: 0.5398016522492541]
	TIME [epoch: 8.32 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7823320367495116		[learning rate: 0.0081742]
		[batch 20/20] avg loss: 0.8245581181151677		[learning rate: 0.0081594]
	Learning Rate: 0.00815939
	LOSS [training: 0.8034450774323396 | validation: 0.6830781536090405]
	TIME [epoch: 8.31 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.794862140072691		[learning rate: 0.0081446]
		[batch 20/20] avg loss: 1.010783363235927		[learning rate: 0.0081298]
	Learning Rate: 0.00812978
	LOSS [training: 0.902822751654309 | validation: 0.555688378434155]
	TIME [epoch: 8.33 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4588040653249592		[learning rate: 0.008115]
		[batch 20/20] avg loss: 0.9511975503445311		[learning rate: 0.0081003]
	Learning Rate: 0.00810028
	LOSS [training: 1.2050008078347454 | validation: 0.8051753363603594]
	TIME [epoch: 8.31 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9080818031428433		[learning rate: 0.0080856]
		[batch 20/20] avg loss: 0.9004512836013399		[learning rate: 0.0080709]
	Learning Rate: 0.00807088
	LOSS [training: 0.9042665433720914 | validation: 0.755662028460863]
	TIME [epoch: 8.33 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7517428662904257		[learning rate: 0.0080562]
		[batch 20/20] avg loss: 1.0464035021780635		[learning rate: 0.0080416]
	Learning Rate: 0.00804159
	LOSS [training: 0.8990731842342445 | validation: 0.5830308711739104]
	TIME [epoch: 8.33 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3803485651962029		[learning rate: 0.008027]
		[batch 20/20] avg loss: 0.7210509589244939		[learning rate: 0.0080124]
	Learning Rate: 0.00801241
	LOSS [training: 1.0506997620603484 | validation: 0.9201662061553367]
	TIME [epoch: 8.33 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8317703449220943		[learning rate: 0.0079979]
		[batch 20/20] avg loss: 0.9521371928980156		[learning rate: 0.0079833]
	Learning Rate: 0.00798333
	LOSS [training: 0.891953768910055 | validation: 0.5490742094604135]
	TIME [epoch: 8.3 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7649810503513255		[learning rate: 0.0079688]
		[batch 20/20] avg loss: 0.9306358681740532		[learning rate: 0.0079544]
	Learning Rate: 0.00795436
	LOSS [training: 0.8478084592626892 | validation: 0.940093272875204]
	TIME [epoch: 8.33 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6452313776446124		[learning rate: 0.0079399]
		[batch 20/20] avg loss: 0.9297175806505432		[learning rate: 0.0079255]
	Learning Rate: 0.00792549
	LOSS [training: 0.7874744791475778 | validation: 0.5540143531137228]
	TIME [epoch: 8.3 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1871265257951615		[learning rate: 0.0079111]
		[batch 20/20] avg loss: 1.2617880145314655		[learning rate: 0.0078967]
	Learning Rate: 0.00789673
	LOSS [training: 1.2244572701633136 | validation: 0.4054599442456508]
	TIME [epoch: 8.3 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0461284065671728		[learning rate: 0.0078824]
		[batch 20/20] avg loss: 0.737620217897214		[learning rate: 0.0078681]
	Learning Rate: 0.00786807
	LOSS [training: 0.8918743122321933 | validation: 0.5316245643862577]
	TIME [epoch: 8.31 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.049626167536519		[learning rate: 0.0078538]
		[batch 20/20] avg loss: 0.783563497431675		[learning rate: 0.0078395]
	Learning Rate: 0.00783952
	LOSS [training: 0.9165948324840969 | validation: 0.9802407026626494]
	TIME [epoch: 8.35 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8035069288413871		[learning rate: 0.0078253]
		[batch 20/20] avg loss: 1.3531954419968806		[learning rate: 0.0078111]
	Learning Rate: 0.00781107
	LOSS [training: 1.0783511854191334 | validation: 0.903336129486541]
	TIME [epoch: 8.32 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9145344989921487		[learning rate: 0.0077969]
		[batch 20/20] avg loss: 1.3278310372170798		[learning rate: 0.0077827]
	Learning Rate: 0.00778272
	LOSS [training: 1.1211827681046143 | validation: 2.2257899117885387]
	TIME [epoch: 8.31 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1588552047044645		[learning rate: 0.0077686]
		[batch 20/20] avg loss: 1.1504074356397234		[learning rate: 0.0077545]
	Learning Rate: 0.00775448
	LOSS [training: 1.154631320172094 | validation: 0.9150812163867774]
	TIME [epoch: 8.33 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.635984641667148		[learning rate: 0.0077404]
		[batch 20/20] avg loss: 1.025274560361241		[learning rate: 0.0077263]
	Learning Rate: 0.00772634
	LOSS [training: 0.8306296010141944 | validation: 0.4813959980484508]
	TIME [epoch: 8.35 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8864371650038739		[learning rate: 0.0077123]
		[batch 20/20] avg loss: 0.7604103802785598		[learning rate: 0.0076983]
	Learning Rate: 0.0076983
	LOSS [training: 0.8234237726412168 | validation: 0.9852733759934181]
	TIME [epoch: 8.31 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4452441619273313		[learning rate: 0.0076843]
		[batch 20/20] avg loss: 0.8404345731782736		[learning rate: 0.0076704]
	Learning Rate: 0.00767036
	LOSS [training: 1.1428393675528024 | validation: 0.7885233555955202]
	TIME [epoch: 8.3 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8906546859860958		[learning rate: 0.0076564]
		[batch 20/20] avg loss: 1.6715635157668793		[learning rate: 0.0076425]
	Learning Rate: 0.00764252
	LOSS [training: 1.2811091008764874 | validation: 0.3837897135069954]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.017073628164657		[learning rate: 0.0076286]
		[batch 20/20] avg loss: 1.0739673326634847		[learning rate: 0.0076148]
	Learning Rate: 0.00761479
	LOSS [training: 1.045520480414071 | validation: 0.5654629285115885]
	TIME [epoch: 8.34 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9249377321578726		[learning rate: 0.007601]
		[batch 20/20] avg loss: 1.3085418075135722		[learning rate: 0.0075872]
	Learning Rate: 0.00758715
	LOSS [training: 1.1167397698357224 | validation: 1.2760403569468481]
	TIME [epoch: 8.31 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8295358877799934		[learning rate: 0.0075734]
		[batch 20/20] avg loss: 0.910941208791056		[learning rate: 0.0075596]
	Learning Rate: 0.00755962
	LOSS [training: 0.8702385482855245 | validation: 1.0324665217756168]
	TIME [epoch: 8.33 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7625940584900495		[learning rate: 0.0075459]
		[batch 20/20] avg loss: 0.9430309392349651		[learning rate: 0.0075322]
	Learning Rate: 0.00753219
	LOSS [training: 0.8528124988625073 | validation: 0.5401266583569012]
	TIME [epoch: 8.34 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7084472401135162		[learning rate: 0.0075185]
		[batch 20/20] avg loss: 0.7268514625520512		[learning rate: 0.0075049]
	Learning Rate: 0.00750485
	LOSS [training: 0.7176493513327837 | validation: 1.1357124044960438]
	TIME [epoch: 8.35 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8128140177823177		[learning rate: 0.0074912]
		[batch 20/20] avg loss: 0.820156065690349		[learning rate: 0.0074776]
	Learning Rate: 0.00747762
	LOSS [training: 0.8164850417363334 | validation: 1.4867662563830075]
	TIME [epoch: 8.31 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0172960355206022		[learning rate: 0.007464]
		[batch 20/20] avg loss: 0.7522840074058708		[learning rate: 0.0074505]
	Learning Rate: 0.00745048
	LOSS [training: 0.8847900214632363 | validation: 0.44154264818408046]
	TIME [epoch: 8.34 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6817387606596423		[learning rate: 0.0074369]
		[batch 20/20] avg loss: 0.9927183437900325		[learning rate: 0.0074234]
	Learning Rate: 0.00742344
	LOSS [training: 0.8372285522248376 | validation: 0.5961350576433174]
	TIME [epoch: 8.33 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7603560711207198		[learning rate: 0.00741]
		[batch 20/20] avg loss: 0.9044021499122336		[learning rate: 0.0073965]
	Learning Rate: 0.0073965
	LOSS [training: 0.8323791105164767 | validation: 1.0614460012626368]
	TIME [epoch: 8.33 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.055719625594382		[learning rate: 0.0073831]
		[batch 20/20] avg loss: 1.0064428075511749		[learning rate: 0.0073697]
	Learning Rate: 0.00736966
	LOSS [training: 1.0310812165727783 | validation: 0.5637290791633897]
	TIME [epoch: 8.32 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8056644582449047		[learning rate: 0.0073563]
		[batch 20/20] avg loss: 0.732148113401302		[learning rate: 0.0073429]
	Learning Rate: 0.00734291
	LOSS [training: 0.7689062858231035 | validation: 0.5395145364297652]
	TIME [epoch: 8.31 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.790485414237384		[learning rate: 0.0073296]
		[batch 20/20] avg loss: 0.7900642858527156		[learning rate: 0.0073163]
	Learning Rate: 0.00731627
	LOSS [training: 0.7902748500450498 | validation: 1.0323742062402552]
	TIME [epoch: 8.34 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9421408224271758		[learning rate: 0.007303]
		[batch 20/20] avg loss: 0.8456787013938069		[learning rate: 0.0072897]
	Learning Rate: 0.00728971
	LOSS [training: 0.8939097619104912 | validation: 1.5636718054072138]
	TIME [epoch: 8.34 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8089160130370441		[learning rate: 0.0072765]
		[batch 20/20] avg loss: 1.17899481609836		[learning rate: 0.0072633]
	Learning Rate: 0.00726326
	LOSS [training: 0.9939554145677022 | validation: 0.7374153516088754]
	TIME [epoch: 8.34 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7864952253475176		[learning rate: 0.0072501]
		[batch 20/20] avg loss: 0.7744848086934824		[learning rate: 0.0072369]
	Learning Rate: 0.0072369
	LOSS [training: 0.7804900170205 | validation: 0.2610004393924543]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8811530677385999		[learning rate: 0.0072238]
		[batch 20/20] avg loss: 0.725016091296579		[learning rate: 0.0072106]
	Learning Rate: 0.00721064
	LOSS [training: 0.8030845795175896 | validation: 0.6538951061945394]
	TIME [epoch: 8.32 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9615320042928241		[learning rate: 0.0071975]
		[batch 20/20] avg loss: 1.0471079976986606		[learning rate: 0.0071845]
	Learning Rate: 0.00718447
	LOSS [training: 1.0043200009957425 | validation: 0.7669686348810393]
	TIME [epoch: 8.35 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8639318222559996		[learning rate: 0.0071714]
		[batch 20/20] avg loss: 1.3414487859020219		[learning rate: 0.0071584]
	Learning Rate: 0.0071584
	LOSS [training: 1.1026903040790104 | validation: 0.37283191677461264]
	TIME [epoch: 8.34 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8425165449207649		[learning rate: 0.0071454]
		[batch 20/20] avg loss: 0.8502101683286336		[learning rate: 0.0071324]
	Learning Rate: 0.00713242
	LOSS [training: 0.8463633566246991 | validation: 0.37064826918870725]
	TIME [epoch: 8.3 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8079760404344004		[learning rate: 0.0071195]
		[batch 20/20] avg loss: 1.1702806716227114		[learning rate: 0.0071065]
	Learning Rate: 0.00710653
	LOSS [training: 0.9891283560285558 | validation: 1.7417292810380438]
	TIME [epoch: 8.3 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.274406759169833		[learning rate: 0.0070936]
		[batch 20/20] avg loss: 1.4519468034939707		[learning rate: 0.0070807]
	Learning Rate: 0.00708074
	LOSS [training: 1.3631767813319018 | validation: 1.145195236598819]
	TIME [epoch: 8.33 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0318261797317747		[learning rate: 0.0070679]
		[batch 20/20] avg loss: 0.9060346770631604		[learning rate: 0.007055]
	Learning Rate: 0.00705505
	LOSS [training: 0.9689304283974677 | validation: 0.49055297554463484]
	TIME [epoch: 8.3 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8222507245777418		[learning rate: 0.0070422]
		[batch 20/20] avg loss: 0.8528309629612479		[learning rate: 0.0070294]
	Learning Rate: 0.00702945
	LOSS [training: 0.8375408437694949 | validation: 0.6080565618640263]
	TIME [epoch: 8.31 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9944039510773688		[learning rate: 0.0070167]
		[batch 20/20] avg loss: 0.7817883139014896		[learning rate: 0.0070039]
	Learning Rate: 0.00700394
	LOSS [training: 0.8880961324894292 | validation: 0.723653580155259]
	TIME [epoch: 8.31 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1703896626191506		[learning rate: 0.0069912]
		[batch 20/20] avg loss: 0.7085048734334778		[learning rate: 0.0069785]
	Learning Rate: 0.00697852
	LOSS [training: 0.9394472680263142 | validation: 0.5153693140231305]
	TIME [epoch: 8.34 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5942229324744452		[learning rate: 0.0069658]
		[batch 20/20] avg loss: 0.8453628396585662		[learning rate: 0.0069532]
	Learning Rate: 0.00695319
	LOSS [training: 0.7197928860665056 | validation: 0.3745524684731264]
	TIME [epoch: 8.35 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7346926092004604		[learning rate: 0.0069406]
		[batch 20/20] avg loss: 1.1387769722405112		[learning rate: 0.006928]
	Learning Rate: 0.00692796
	LOSS [training: 0.9367347907204859 | validation: 0.8233076162824302]
	TIME [epoch: 8.32 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8224360183232562		[learning rate: 0.0069154]
		[batch 20/20] avg loss: 0.7727550824708069		[learning rate: 0.0069028]
	Learning Rate: 0.00690282
	LOSS [training: 0.7975955503970317 | validation: 0.5218178100576233]
	TIME [epoch: 8.31 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8476994255108365		[learning rate: 0.0068903]
		[batch 20/20] avg loss: 0.8852618506651083		[learning rate: 0.0068778]
	Learning Rate: 0.00687777
	LOSS [training: 0.8664806380879725 | validation: 0.9623068854519518]
	TIME [epoch: 8.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7997088390444305		[learning rate: 0.0068653]
		[batch 20/20] avg loss: 0.6357885538322983		[learning rate: 0.0068528]
	Learning Rate: 0.00685281
	LOSS [training: 0.7177486964383644 | validation: 0.7347620051915889]
	TIME [epoch: 8.34 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9792486196618508		[learning rate: 0.0068404]
		[batch 20/20] avg loss: 0.7681738458597643		[learning rate: 0.0068279]
	Learning Rate: 0.00682794
	LOSS [training: 0.8737112327608075 | validation: 0.5072372403633277]
	TIME [epoch: 8.3 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9074129024713027		[learning rate: 0.0068155]
		[batch 20/20] avg loss: 0.7411392502540627		[learning rate: 0.0068032]
	Learning Rate: 0.00680316
	LOSS [training: 0.8242760763626826 | validation: 0.6765556820196593]
	TIME [epoch: 8.3 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8463771576319028		[learning rate: 0.0067908]
		[batch 20/20] avg loss: 1.2430164391500156		[learning rate: 0.0067785]
	Learning Rate: 0.00677847
	LOSS [training: 1.0446967983909592 | validation: 0.6286511916574355]
	TIME [epoch: 8.34 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8425380198759063		[learning rate: 0.0067662]
		[batch 20/20] avg loss: 0.9882315453359644		[learning rate: 0.0067539]
	Learning Rate: 0.00675387
	LOSS [training: 0.9153847826059351 | validation: 0.7914613895699033]
	TIME [epoch: 8.32 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6660876842623257		[learning rate: 0.0067416]
		[batch 20/20] avg loss: 0.8712067718487807		[learning rate: 0.0067294]
	Learning Rate: 0.00672936
	LOSS [training: 0.7686472280555532 | validation: 0.9116143541479781]
	TIME [epoch: 8.32 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6458736604182718		[learning rate: 0.0067171]
		[batch 20/20] avg loss: 1.1322712926040661		[learning rate: 0.0067049]
	Learning Rate: 0.00670494
	LOSS [training: 1.3890724765111693 | validation: 0.6060125945066732]
	TIME [epoch: 8.32 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2895846567408813		[learning rate: 0.0066928]
		[batch 20/20] avg loss: 1.3908071136236693		[learning rate: 0.0066806]
	Learning Rate: 0.0066806
	LOSS [training: 1.340195885182275 | validation: 0.7300290458658265]
	TIME [epoch: 8.37 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9099495516761861		[learning rate: 0.0066685]
		[batch 20/20] avg loss: 0.9778283123846702		[learning rate: 0.0066564]
	Learning Rate: 0.00665636
	LOSS [training: 0.943888932030428 | validation: 1.2477009269335422]
	TIME [epoch: 8.32 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0360065300478032		[learning rate: 0.0066443]
		[batch 20/20] avg loss: 1.0825217376390208		[learning rate: 0.0066322]
	Learning Rate: 0.0066322
	LOSS [training: 1.0592641338434121 | validation: 0.7571325954397257]
	TIME [epoch: 8.32 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4523020781428084		[learning rate: 0.0066202]
		[batch 20/20] avg loss: 1.0651778344758511		[learning rate: 0.0066081]
	Learning Rate: 0.00660814
	LOSS [training: 1.2587399563093298 | validation: 0.6076667134189343]
	TIME [epoch: 8.34 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0625263647592473		[learning rate: 0.0065961]
		[batch 20/20] avg loss: 1.0411706212490184		[learning rate: 0.0065842]
	Learning Rate: 0.00658415
	LOSS [training: 2.051848493004133 | validation: 0.7404677077087563]
	TIME [epoch: 8.36 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0117387517166647		[learning rate: 0.0065722]
		[batch 20/20] avg loss: 1.4272958118559287		[learning rate: 0.0065603]
	Learning Rate: 0.00656026
	LOSS [training: 1.2195172817862967 | validation: 0.6952898391329556]
	TIME [epoch: 8.31 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9992089255599707		[learning rate: 0.0065483]
		[batch 20/20] avg loss: 0.766204956762768		[learning rate: 0.0065365]
	Learning Rate: 0.00653645
	LOSS [training: 0.8827069411613694 | validation: 1.9432349410782432]
	TIME [epoch: 8.31 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9829271737996181		[learning rate: 0.0065246]
		[batch 20/20] avg loss: 0.6995401637993739		[learning rate: 0.0065127]
	Learning Rate: 0.00651273
	LOSS [training: 0.8412336687994959 | validation: 0.3795590621456223]
	TIME [epoch: 8.3 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7594433002980517		[learning rate: 0.0065009]
		[batch 20/20] avg loss: 0.8956878420491545		[learning rate: 0.0064891]
	Learning Rate: 0.0064891
	LOSS [training: 0.8275655711736032 | validation: 1.0384344927604605]
	TIME [epoch: 8.33 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8186601459710499		[learning rate: 0.0064773]
		[batch 20/20] avg loss: 0.7762186751537822		[learning rate: 0.0064655]
	Learning Rate: 0.00646555
	LOSS [training: 0.7974394105624161 | validation: 0.8613574939498184]
	TIME [epoch: 8.31 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4111907914080293		[learning rate: 0.0064538]
		[batch 20/20] avg loss: 1.2853209619883175		[learning rate: 0.0064421]
	Learning Rate: 0.00644208
	LOSS [training: 1.8482558766981736 | validation: 0.6268324378293323]
	TIME [epoch: 8.33 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1169830564015462		[learning rate: 0.0064304]
		[batch 20/20] avg loss: 1.2041017628040016		[learning rate: 0.0064187]
	Learning Rate: 0.0064187
	LOSS [training: 1.1605424096027739 | validation: 0.7978486353374329]
	TIME [epoch: 8.34 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8714391300949842		[learning rate: 0.006407]
		[batch 20/20] avg loss: 2.067110249307084		[learning rate: 0.0063954]
	Learning Rate: 0.00639541
	LOSS [training: 1.469274689701034 | validation: 2.8947248773790157]
	TIME [epoch: 8.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5470464823075694		[learning rate: 0.0063838]
		[batch 20/20] avg loss: 0.9135993045980897		[learning rate: 0.0063722]
	Learning Rate: 0.0063722
	LOSS [training: 1.2303228934528292 | validation: 0.747889338680737]
	TIME [epoch: 8.33 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8830980670436681		[learning rate: 0.0063606]
		[batch 20/20] avg loss: 0.7487385510443885		[learning rate: 0.0063491]
	Learning Rate: 0.00634908
	LOSS [training: 0.8159183090440282 | validation: 0.4313054149843186]
	TIME [epoch: 8.34 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7033463280081409		[learning rate: 0.0063375]
		[batch 20/20] avg loss: 0.7537335779259806		[learning rate: 0.006326]
	Learning Rate: 0.00632603
	LOSS [training: 0.7285399529670606 | validation: 0.4115232841208534]
	TIME [epoch: 8.33 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6962401931964368		[learning rate: 0.0063145]
		[batch 20/20] avg loss: 1.4638829760044032		[learning rate: 0.0063031]
	Learning Rate: 0.00630308
	LOSS [training: 1.0800615846004202 | validation: 0.37296333847057]
	TIME [epoch: 8.34 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9983479397562964		[learning rate: 0.0062916]
		[batch 20/20] avg loss: 1.9334096909795988		[learning rate: 0.0062802]
	Learning Rate: 0.0062802
	LOSS [training: 1.4658788153679476 | validation: 0.6410260815678323]
	TIME [epoch: 8.31 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0071607223984327		[learning rate: 0.0062688]
		[batch 20/20] avg loss: 0.8962068588651363		[learning rate: 0.0062574]
	Learning Rate: 0.00625741
	LOSS [training: 0.9516837906317844 | validation: 1.5888511198493114]
	TIME [epoch: 8.31 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9104168041017278		[learning rate: 0.006246]
		[batch 20/20] avg loss: 0.9640743012117385		[learning rate: 0.0062347]
	Learning Rate: 0.0062347
	LOSS [training: 0.937245552656733 | validation: 0.3123948180822668]
	TIME [epoch: 8.31 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6232807636586947		[learning rate: 0.0062234]
		[batch 20/20] avg loss: 0.7420819165194107		[learning rate: 0.0062121]
	Learning Rate: 0.00621208
	LOSS [training: 0.6826813400890528 | validation: 0.41849469182137367]
	TIME [epoch: 8.35 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8439576476165742		[learning rate: 0.0062008]
		[batch 20/20] avg loss: 0.7934547939115036		[learning rate: 0.0061895]
	Learning Rate: 0.00618953
	LOSS [training: 0.818706220764039 | validation: 0.769481717948657]
	TIME [epoch: 8.32 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6796050530122041		[learning rate: 0.0061783]
		[batch 20/20] avg loss: 0.7294948433461991		[learning rate: 0.0061671]
	Learning Rate: 0.00616707
	LOSS [training: 0.7045499481792017 | validation: 0.6419821102951269]
	TIME [epoch: 8.34 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7661338440354635		[learning rate: 0.0061559]
		[batch 20/20] avg loss: 0.7846293196426701		[learning rate: 0.0061447]
	Learning Rate: 0.00614469
	LOSS [training: 0.7753815818390667 | validation: 0.20717695420777305]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study201/model_tr_study201_r2_20240219_233648/states/model_tr_study201_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7653498565983211		[learning rate: 0.0061335]
		[batch 20/20] avg loss: 0.8182869212699275		[learning rate: 0.0061224]
	Learning Rate: 0.00612239
	LOSS [training: 0.7918183889341244 | validation: 0.27249319088891266]
	TIME [epoch: 8.34 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7528230377312985		[learning rate: 0.0061113]
		[batch 20/20] avg loss: 0.5812216228680593		[learning rate: 0.0061002]
	Learning Rate: 0.00610017
	LOSS [training: 0.6670223302996791 | validation: 0.3348453191225022]
	TIME [epoch: 8.34 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6299987319620802		[learning rate: 0.0060891]
		[batch 20/20] avg loss: 0.6965359320397267		[learning rate: 0.006078]
	Learning Rate: 0.00607803
	LOSS [training: 0.6632673320009035 | validation: 0.4175916910156158]
	TIME [epoch: 8.32 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8687374409258443		[learning rate: 0.006067]
		[batch 20/20] avg loss: 0.8625208448293085		[learning rate: 0.006056]
	Learning Rate: 0.00605598
	LOSS [training: 0.8656291428775763 | validation: 0.2839139675347761]
	TIME [epoch: 8.3 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.681386231830551		[learning rate: 0.006045]
		[batch 20/20] avg loss: 0.8595365621850684		[learning rate: 0.006034]
	Learning Rate: 0.006034
	LOSS [training: 0.7704613970078097 | validation: 0.9532426656855338]
	TIME [epoch: 8.33 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0656128456064005		[learning rate: 0.006023]
		[batch 20/20] avg loss: 0.8436999574613445		[learning rate: 0.0060121]
	Learning Rate: 0.0060121
	LOSS [training: 0.9546564015338725 | validation: 1.3358898406063016]
	TIME [epoch: 8.31 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7347887709837273		[learning rate: 0.0060012]
		[batch 20/20] avg loss: 1.0486909821995736		[learning rate: 0.0059903]
	Learning Rate: 0.00599028
	LOSS [training: 0.8917398765916505 | validation: 0.3513714253484734]
	TIME [epoch: 8.3 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8741289760993386		[learning rate: 0.0059794]
		[batch 20/20] avg loss: 0.7090935382762383		[learning rate: 0.0059685]
	Learning Rate: 0.00596854
	LOSS [training: 0.7916112571877885 | validation: 0.9785477885125057]
	TIME [epoch: 8.31 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.831101389659952		[learning rate: 0.0059577]
		[batch 20/20] avg loss: 0.718945364469587		[learning rate: 0.0059469]
	Learning Rate: 0.00594688
	LOSS [training: 0.7750233770647694 | validation: 0.7470511482096476]
	TIME [epoch: 8.33 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6831808820266804		[learning rate: 0.0059361]
		[batch 20/20] avg loss: 0.7450221625587226		[learning rate: 0.0059253]
	Learning Rate: 0.0059253
	LOSS [training: 0.7141015222927013 | validation: 0.537555514272812]
	TIME [epoch: 8.34 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7489255271577454		[learning rate: 0.0059145]
		[batch 20/20] avg loss: 0.841893432486535		[learning rate: 0.0059038]
	Learning Rate: 0.0059038
	LOSS [training: 0.7954094798221403 | validation: 1.8554013528507993]
	TIME [epoch: 8.32 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8694487360324409		[learning rate: 0.0058931]
		[batch 20/20] avg loss: 0.6409940372141727		[learning rate: 0.0058824]
	Learning Rate: 0.00588237
	LOSS [training: 0.7552213866233068 | validation: 0.3972607701703391]
	TIME [epoch: 8.32 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7575896528368817		[learning rate: 0.0058717]
		[batch 20/20] avg loss: 0.631310806625402		[learning rate: 0.005861]
	Learning Rate: 0.00586103
	LOSS [training: 0.6944502297311419 | validation: 0.8032147660387455]
	TIME [epoch: 8.36 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7909314257424009		[learning rate: 0.0058504]
		[batch 20/20] avg loss: 0.8276363538361048		[learning rate: 0.0058398]
	Learning Rate: 0.00583976
	LOSS [training: 0.8092838897892529 | validation: 1.6374159795552679]
	TIME [epoch: 8.34 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9714608810390452		[learning rate: 0.0058291]
		[batch 20/20] avg loss: 0.831086071204848		[learning rate: 0.0058186]
	Learning Rate: 0.00581856
	LOSS [training: 0.9012734761219464 | validation: 0.9086709843017837]
	TIME [epoch: 8.3 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7143288963685498		[learning rate: 0.005808]
		[batch 20/20] avg loss: 0.6474742325634029		[learning rate: 0.0057974]
	Learning Rate: 0.00579745
	LOSS [training: 0.6809015644659763 | validation: 1.1974665017619412]
	TIME [epoch: 8.32 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8059579510087304		[learning rate: 0.0057869]
		[batch 20/20] avg loss: 0.7061021084499127		[learning rate: 0.0057764]
	Learning Rate: 0.00577641
	LOSS [training: 0.7560300297293215 | validation: 0.5080478621094928]
	TIME [epoch: 8.34 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.338521478093704		[learning rate: 0.0057659]
		[batch 20/20] avg loss: 0.9037905869849118		[learning rate: 0.0057554]
	Learning Rate: 0.00575545
	LOSS [training: 1.1211560325393082 | validation: 1.3125434127501407]
	TIME [epoch: 8.32 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9311117119088304		[learning rate: 0.005745]
		[batch 20/20] avg loss: 0.7700463104562342		[learning rate: 0.0057346]
	Learning Rate: 0.00573456
	LOSS [training: 0.8505790111825322 | validation: 0.615026192346331]
	TIME [epoch: 8.31 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8990024644407828		[learning rate: 0.0057241]
		[batch 20/20] avg loss: 1.058943759095993		[learning rate: 0.0057137]
	Learning Rate: 0.00571375
	LOSS [training: 0.9789731117683879 | validation: 0.4770274373633453]
	TIME [epoch: 8.35 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.857075799522258		[learning rate: 0.0057034]
		[batch 20/20] avg loss: 0.9903285199328696		[learning rate: 0.005693]
	Learning Rate: 0.00569301
	LOSS [training: 0.9237021597275639 | validation: 0.5726273459317256]
	TIME [epoch: 8.34 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0143949470828275		[learning rate: 0.0056827]
		[batch 20/20] avg loss: 0.808115110082006		[learning rate: 0.0056724]
	Learning Rate: 0.00567235
	LOSS [training: 0.9112550285824168 | validation: 1.1522040301671836]
	TIME [epoch: 8.32 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.865939700143261		[learning rate: 0.005662]
		[batch 20/20] avg loss: 0.6861283043362526		[learning rate: 0.0056518]
	Learning Rate: 0.00565177
	LOSS [training: 0.776034002239757 | validation: 1.1693603336647436]
	TIME [epoch: 8.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8327094265293675		[learning rate: 0.0056415]
		[batch 20/20] avg loss: 0.89977380354271		[learning rate: 0.0056313]
	Learning Rate: 0.00563126
	LOSS [training: 0.8662416150360388 | validation: 0.5374438008955094]
	TIME [epoch: 8.37 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8147148399057687		[learning rate: 0.005621]
		[batch 20/20] avg loss: 0.9048320540510574		[learning rate: 0.0056108]
	Learning Rate: 0.00561082
	LOSS [training: 0.8597734469784131 | validation: 0.765235921274394]
	TIME [epoch: 8.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7136483823586351		[learning rate: 0.0056006]
		[batch 20/20] avg loss: 0.6637667318840199		[learning rate: 0.0055905]
	Learning Rate: 0.00559046
	LOSS [training: 0.6887075571213276 | validation: 0.5211548478467918]
	TIME [epoch: 8.33 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9600672066776585		[learning rate: 0.0055803]
		[batch 20/20] avg loss: 1.6150179975616343		[learning rate: 0.0055702]
	Learning Rate: 0.00557017
	LOSS [training: 1.287542602119646 | validation: 2.1331373817067774]
	TIME [epoch: 8.34 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.166215276062145		[learning rate: 0.0055601]
		[batch 20/20] avg loss: 0.9736704574885116		[learning rate: 0.00555]
	Learning Rate: 0.00554996
	LOSS [training: 1.0699428667753283 | validation: 0.3600516945063861]
	TIME [epoch: 8.34 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7193069204558697		[learning rate: 0.0055399]
		[batch 20/20] avg loss: 0.7659282984820148		[learning rate: 0.0055298]
	Learning Rate: 0.00552981
	LOSS [training: 0.7426176094689421 | validation: 0.26780152962586945]
	TIME [epoch: 8.36 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8914452042370525		[learning rate: 0.0055198]
		[batch 20/20] avg loss: 1.0310347112116032		[learning rate: 0.0055097]
	Learning Rate: 0.00550975
	LOSS [training: 0.961239957724328 | validation: 0.7624788594641791]
	TIME [epoch: 8.37 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7760010710009123		[learning rate: 0.0054997]
		[batch 20/20] avg loss: 0.7802943464009108		[learning rate: 0.0054898]
	Learning Rate: 0.00548975
	LOSS [training: 0.7781477087009117 | validation: 0.7118563799743333]
	TIME [epoch: 8.34 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7834483146754094		[learning rate: 0.0054798]
		[batch 20/20] avg loss: 0.7356915017357555		[learning rate: 0.0054698]
	Learning Rate: 0.00546983
	LOSS [training: 0.7595699082055826 | validation: 0.7413907724179899]
	TIME [epoch: 8.34 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7928540261329501		[learning rate: 0.0054599]
		[batch 20/20] avg loss: 0.7520879877148616		[learning rate: 0.00545]
	Learning Rate: 0.00544998
	LOSS [training: 0.7724710069239059 | validation: 0.4686158406161256]
	TIME [epoch: 8.38 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.868039049650713		[learning rate: 0.0054401]
		[batch 20/20] avg loss: 0.7984874242035437		[learning rate: 0.0054302]
	Learning Rate: 0.0054302
	LOSS [training: 0.8332632369271284 | validation: 0.4700069386625188]
	TIME [epoch: 8.34 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7424017743862201		[learning rate: 0.0054203]
		[batch 20/20] avg loss: 0.8241168658030844		[learning rate: 0.0054105]
	Learning Rate: 0.00541049
	LOSS [training: 0.7832593200946523 | validation: 0.8365947599242032]
	TIME [epoch: 8.33 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7258478917402884		[learning rate: 0.0054007]
		[batch 20/20] avg loss: 0.7029802429643707		[learning rate: 0.0053909]
	Learning Rate: 0.00539086
	LOSS [training: 0.7144140673523295 | validation: 0.7774742357553078]
	TIME [epoch: 8.3 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7177547812595652		[learning rate: 0.0053811]
		[batch 20/20] avg loss: 0.7741133235671173		[learning rate: 0.0053713]
	Learning Rate: 0.00537129
	LOSS [training: 0.7459340524133412 | validation: 0.4726050890753211]
	TIME [epoch: 8.31 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6907335050180917		[learning rate: 0.0053615]
		[batch 20/20] avg loss: 0.7399529454533746		[learning rate: 0.0053518]
	Learning Rate: 0.0053518
	LOSS [training: 0.7153432252357331 | validation: 1.3793486311413425]
	TIME [epoch: 8.3 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4689718856120684		[learning rate: 0.0053421]
		[batch 20/20] avg loss: 1.0324300548454475		[learning rate: 0.0053324]
	Learning Rate: 0.00533238
	LOSS [training: 1.2507009702287581 | validation: 0.6625885799760467]
	TIME [epoch: 8.34 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7288564444434678		[learning rate: 0.0053227]
		[batch 20/20] avg loss: 0.7407819087034189		[learning rate: 0.005313]
	Learning Rate: 0.00531303
	LOSS [training: 0.7348191765734434 | validation: 0.47130065980885344]
	TIME [epoch: 8.32 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6081762332204803		[learning rate: 0.0053034]
		[batch 20/20] avg loss: 0.788508200358063		[learning rate: 0.0052937]
	Learning Rate: 0.00529375
	LOSS [training: 0.6983422167892717 | validation: 0.34218623668549425]
	TIME [epoch: 8.32 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8640698493581708		[learning rate: 0.0052841]
		[batch 20/20] avg loss: 0.7408358714356243		[learning rate: 0.0052745]
	Learning Rate: 0.00527454
	LOSS [training: 0.8024528603968977 | validation: 0.9229868071450309]
	TIME [epoch: 8.33 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.617015597926304		[learning rate: 0.005265]
		[batch 20/20] avg loss: 0.7553372598617644		[learning rate: 0.0052554]
	Learning Rate: 0.00525539
	LOSS [training: 1.1861764288940342 | validation: 0.3118880469682629]
	TIME [epoch: 8.31 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7442709925037527		[learning rate: 0.0052458]
		[batch 20/20] avg loss: 0.6622906448492076		[learning rate: 0.0052363]
	Learning Rate: 0.00523632
	LOSS [training: 0.7032808186764803 | validation: 0.9759250407219175]
	TIME [epoch: 8.3 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7510693161235371		[learning rate: 0.0052268]
		[batch 20/20] avg loss: 0.690742339806529		[learning rate: 0.0052173]
	Learning Rate: 0.00521732
	LOSS [training: 0.7209058279650331 | validation: 0.9534993907483265]
	TIME [epoch: 8.3 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8000060384587563		[learning rate: 0.0052078]
		[batch 20/20] avg loss: 0.6903118848424434		[learning rate: 0.0051984]
	Learning Rate: 0.00519839
	LOSS [training: 0.7451589616505999 | validation: 0.677517913896452]
	TIME [epoch: 8.29 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6577621531438604		[learning rate: 0.0051889]
		[batch 20/20] avg loss: 0.8515342774515474		[learning rate: 0.0051795]
	Learning Rate: 0.00517952
	LOSS [training: 0.754648215297704 | validation: 0.5703282690466236]
	TIME [epoch: 8.29 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.680279325563782		[learning rate: 0.0051701]
		[batch 20/20] avg loss: 0.6841058142208635		[learning rate: 0.0051607]
	Learning Rate: 0.00516072
	LOSS [training: 0.6821925698923228 | validation: 0.42029460681611197]
	TIME [epoch: 8.29 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8642962836690439		[learning rate: 0.0051513]
		[batch 20/20] avg loss: 0.7996328757619748		[learning rate: 0.005142]
	Learning Rate: 0.00514199
	LOSS [training: 0.8319645797155092 | validation: 0.4868814185649938]
	TIME [epoch: 8.31 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6824127565205054		[learning rate: 0.0051327]
		[batch 20/20] avg loss: 1.1254307427920243		[learning rate: 0.0051233]
	Learning Rate: 0.00512333
	LOSS [training: 0.9039217496562648 | validation: 2.0497467918716055]
	TIME [epoch: 8.31 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.900193148852504		[learning rate: 0.005114]
		[batch 20/20] avg loss: 0.7522361445533435		[learning rate: 0.0051047]
	Learning Rate: 0.00510474
	LOSS [training: 0.8262146467029237 | validation: 0.8953415801227294]
	TIME [epoch: 8.34 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7028404283932864		[learning rate: 0.0050955]
		[batch 20/20] avg loss: 0.7290040620026873		[learning rate: 0.0050862]
	Learning Rate: 0.00508622
	LOSS [training: 0.715922245197987 | validation: 1.5818777233351624]
	TIME [epoch: 8.31 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.084832835472045		[learning rate: 0.005077]
		[batch 20/20] avg loss: 0.6072034268771593		[learning rate: 0.0050678]
	Learning Rate: 0.00506776
	LOSS [training: 0.846018131174602 | validation: 0.2859871811737945]
	TIME [epoch: 8.32 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7003022312914478		[learning rate: 0.0050586]
		[batch 20/20] avg loss: 0.6773902201628175		[learning rate: 0.0050494]
	Learning Rate: 0.00504937
	LOSS [training: 0.6888462257271326 | validation: 0.5342849912030676]
	TIME [epoch: 8.32 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7454516084920931		[learning rate: 0.0050402]
		[batch 20/20] avg loss: 0.7496895222745681		[learning rate: 0.005031]
	Learning Rate: 0.00503104
	LOSS [training: 0.7475705653833306 | validation: 1.8042441360458157]
	TIME [epoch: 8.32 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8348294575525632		[learning rate: 0.0050219]
		[batch 20/20] avg loss: 0.8284944244482204		[learning rate: 0.0050128]
	Learning Rate: 0.00501278
	LOSS [training: 0.8316619410003916 | validation: 0.42498546461752484]
	TIME [epoch: 8.29 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6403757378304591		[learning rate: 0.0050037]
		[batch 20/20] avg loss: 0.8381645974051795		[learning rate: 0.0049946]
	Learning Rate: 0.00499459
	LOSS [training: 0.7392701676178193 | validation: 0.47919508182349935]
	TIME [epoch: 8.3 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7120173311949775		[learning rate: 0.0049855]
		[batch 20/20] avg loss: 0.775531537966178		[learning rate: 0.0049765]
	Learning Rate: 0.00497647
	LOSS [training: 0.7437744345805777 | validation: 0.46123973539278196]
	TIME [epoch: 8.29 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0446426979652583		[learning rate: 0.0049674]
		[batch 20/20] avg loss: 0.753882220502195		[learning rate: 0.0049584]
	Learning Rate: 0.00495841
	LOSS [training: 0.8992624592337266 | validation: 0.3185867532681398]
	TIME [epoch: 8.28 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7268272826415683		[learning rate: 0.0049494]
		[batch 20/20] avg loss: 0.7845167886930321		[learning rate: 0.0049404]
	Learning Rate: 0.00494041
	LOSS [training: 0.7556720356673002 | validation: 0.44482345554836095]
	TIME [epoch: 8.3 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.694290524479864		[learning rate: 0.0049314]
		[batch 20/20] avg loss: 0.8555571596822344		[learning rate: 0.0049225]
	Learning Rate: 0.00492248
	LOSS [training: 0.7749238420810493 | validation: 0.48495532743679903]
	TIME [epoch: 8.31 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9169412956952268		[learning rate: 0.0049135]
		[batch 20/20] avg loss: 0.8523229592782318		[learning rate: 0.0049046]
	Learning Rate: 0.00490462
	LOSS [training: 0.8846321274867293 | validation: 0.6180681558557128]
	TIME [epoch: 8.33 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8317007244020636		[learning rate: 0.0048957]
		[batch 20/20] avg loss: 1.4034489179058187		[learning rate: 0.0048868]
	Learning Rate: 0.00488682
	LOSS [training: 1.1175748211539411 | validation: 1.509651253219512]
	TIME [epoch: 8.29 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8968336252526058		[learning rate: 0.0048779]
		[batch 20/20] avg loss: 1.0908352670013115		[learning rate: 0.0048691]
	Learning Rate: 0.00486909
	LOSS [training: 0.9938344461269587 | validation: 0.6101385457637161]
	TIME [epoch: 8.31 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.471051961190111		[learning rate: 0.0048602]
		[batch 20/20] avg loss: 1.1439629253862627		[learning rate: 0.0048514]
	Learning Rate: 0.00485141
	LOSS [training: 1.3075074432881868 | validation: 0.6519703965846891]
	TIME [epoch: 8.36 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9971543389902445		[learning rate: 0.0048426]
		[batch 20/20] avg loss: 1.1381869539915832		[learning rate: 0.0048338]
	Learning Rate: 0.00483381
	LOSS [training: 1.0676706464909138 | validation: 3.6118747194911283]
	TIME [epoch: 8.32 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8653516734792894		[learning rate: 0.004825]
		[batch 20/20] avg loss: 1.0049346060145539		[learning rate: 0.0048163]
	Learning Rate: 0.00481627
	LOSS [training: 1.4351431397469216 | validation: 0.3817535650356651]
	TIME [epoch: 8.29 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8233723214606027		[learning rate: 0.0048075]
		[batch 20/20] avg loss: 0.8315551554181144		[learning rate: 0.0047988]
	Learning Rate: 0.00479879
	LOSS [training: 0.8274637384393586 | validation: 0.6206527626927455]
	TIME [epoch: 8.3 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7657255434064405		[learning rate: 0.0047901]
		[batch 20/20] avg loss: 0.8426150240610937		[learning rate: 0.0047814]
	Learning Rate: 0.00478137
	LOSS [training: 0.8041702837337672 | validation: 0.6472202730814732]
	TIME [epoch: 8.32 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7592646059594542		[learning rate: 0.0047727]
		[batch 20/20] avg loss: 0.7881919952818582		[learning rate: 0.004764]
	Learning Rate: 0.00476402
	LOSS [training: 0.7737283006206561 | validation: 1.0299792313894074]
	TIME [epoch: 8.3 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7726156771821305		[learning rate: 0.0047554]
		[batch 20/20] avg loss: 0.7634324048041043		[learning rate: 0.0047467]
	Learning Rate: 0.00474673
	LOSS [training: 0.7680240409931174 | validation: 0.563759249156436]
	TIME [epoch: 8.3 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.700841203516777		[learning rate: 0.0047381]
		[batch 20/20] avg loss: 0.6825032331400382		[learning rate: 0.0047295]
	Learning Rate: 0.00472951
	LOSS [training: 0.6916722183284078 | validation: 0.6448880821043075]
	TIME [epoch: 8.33 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7853725501330906		[learning rate: 0.0047209]
		[batch 20/20] avg loss: 0.7134982506727104		[learning rate: 0.0047123]
	Learning Rate: 0.00471234
	LOSS [training: 0.7494354004029005 | validation: 0.6017523747531464]
	TIME [epoch: 8.34 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.981137206333851		[learning rate: 0.0047038]
		[batch 20/20] avg loss: 1.225278042131414		[learning rate: 0.0046952]
	Learning Rate: 0.00469524
	LOSS [training: 1.1032076242326325 | validation: 0.5820504350682346]
	TIME [epoch: 8.3 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6343411169077219		[learning rate: 0.0046867]
		[batch 20/20] avg loss: 1.2611351189433369		[learning rate: 0.0046782]
	Learning Rate: 0.0046782
	LOSS [training: 0.9477381179255293 | validation: 0.3787770248426914]
	TIME [epoch: 8.3 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9770511393095148		[learning rate: 0.0046697]
		[batch 20/20] avg loss: 0.6740251464494601		[learning rate: 0.0046612]
	Learning Rate: 0.00466122
	LOSS [training: 0.8255381428794873 | validation: 0.5276496743372081]
	TIME [epoch: 8.34 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7722373535648407		[learning rate: 0.0046528]
		[batch 20/20] avg loss: 1.3738084737732361		[learning rate: 0.0046443]
	Learning Rate: 0.00464431
	LOSS [training: 1.0730229136690383 | validation: 1.3040913891507322]
	TIME [epoch: 8.33 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2168945613773299		[learning rate: 0.0046359]
		[batch 20/20] avg loss: 1.3485890151170223		[learning rate: 0.0046275]
	Learning Rate: 0.00462745
	LOSS [training: 1.282741788247176 | validation: 0.5770009895195097]
	TIME [epoch: 8.29 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8368599265246883		[learning rate: 0.004619]
		[batch 20/20] avg loss: 0.7869226985879912		[learning rate: 0.0046107]
	Learning Rate: 0.00461066
	LOSS [training: 0.8118913125563397 | validation: 0.5104708603960528]
	TIME [epoch: 8.29 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7174986038371584		[learning rate: 0.0046023]
		[batch 20/20] avg loss: 0.7837316890736556		[learning rate: 0.0045939]
	Learning Rate: 0.00459393
	LOSS [training: 0.750615146455407 | validation: 0.6687120048482509]
	TIME [epoch: 8.3 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.90386877198114		[learning rate: 0.0045856]
		[batch 20/20] avg loss: 1.4075089268248875		[learning rate: 0.0045773]
	Learning Rate: 0.00457726
	LOSS [training: 1.6556888494030138 | validation: 2.2601787594170877]
	TIME [epoch: 8.31 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4021740787182266		[learning rate: 0.0045689]
		[batch 20/20] avg loss: 1.3757901609094254		[learning rate: 0.0045606]
	Learning Rate: 0.00456065
	LOSS [training: 1.388982119813826 | validation: 1.849995754873915]
	TIME [epoch: 8.3 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.588681669413983		[learning rate: 0.0045524]
		[batch 20/20] avg loss: 0.9720521291938302		[learning rate: 0.0045441]
	Learning Rate: 0.00454409
	LOSS [training: 1.280366899303907 | validation: 0.8623363895169658]
	TIME [epoch: 8.3 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.4070281378875995		[learning rate: 0.0045358]
		[batch 20/20] avg loss: 7.029732967647296		[learning rate: 0.0045276]
	Learning Rate: 0.0045276
	LOSS [training: 5.718380552767448 | validation: 2.6936953551144853]
	TIME [epoch: 8.35 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.52908300857937		[learning rate: 0.0045194]
		[batch 20/20] avg loss: 1.2263037646828026		[learning rate: 0.0045112]
	Learning Rate: 0.00451117
	LOSS [training: 1.8776933866310865 | validation: 1.4295708753144662]
	TIME [epoch: 8.32 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.435751941651958		[learning rate: 0.004503]
		[batch 20/20] avg loss: 1.719040278608071		[learning rate: 0.0044948]
	Learning Rate: 0.0044948
	LOSS [training: 2.5773961101300147 | validation: 1.202969273812844]
	TIME [epoch: 8.3 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6266242779150446		[learning rate: 0.0044866]
		[batch 20/20] avg loss: 1.540858030164522		[learning rate: 0.0044785]
	Learning Rate: 0.00447849
	LOSS [training: 1.5837411540397832 | validation: 2.0379075014039003]
	TIME [epoch: 8.31 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0672807791493826		[learning rate: 0.0044704]
		[batch 20/20] avg loss: 7.380271573720968		[learning rate: 0.0044622]
	Learning Rate: 0.00446224
	LOSS [training: 4.723776176435175 | validation: 10.793814629846073]
	TIME [epoch: 8.33 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.344952609323784		[learning rate: 0.0044541]
		[batch 20/20] avg loss: 6.130360208555062		[learning rate: 0.004446]
	Learning Rate: 0.00444604
	LOSS [training: 8.237656408939424 | validation: 8.792527186400717]
	TIME [epoch: 8.31 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 12.288122646448098		[learning rate: 0.004438]
		[batch 20/20] avg loss: 13.597595379485664		[learning rate: 0.0044299]
	Learning Rate: 0.00442991
	LOSS [training: 12.942859012966881 | validation: 13.641366685026295]
	TIME [epoch: 8.28 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 13.541262883179863		[learning rate: 0.0044219]
		[batch 20/20] avg loss: 10.857049828880069		[learning rate: 0.0044138]
	Learning Rate: 0.00441383
	LOSS [training: 12.199156356029967 | validation: 10.096731626971634]
	TIME [epoch: 8.29 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 12.332908086889944		[learning rate: 0.0044058]
		[batch 20/20] avg loss: 11.92225131862328		[learning rate: 0.0043978]
	Learning Rate: 0.00439781
	LOSS [training: 12.127579702756613 | validation: 10.840378381503037]
	TIME [epoch: 8.28 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.727186180284603		[learning rate: 0.0043898]
		[batch 20/20] avg loss: 9.826203328555609		[learning rate: 0.0043819]
	Learning Rate: 0.00438185
	LOSS [training: 10.276694754420106 | validation: 8.462591537600366]
	TIME [epoch: 8.32 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.252358944157342		[learning rate: 0.0043739]
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.05
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.025
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.0125
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.00625
ERROR:
Encountered nan in loss and reached the maximum number of model alterations: 4.
