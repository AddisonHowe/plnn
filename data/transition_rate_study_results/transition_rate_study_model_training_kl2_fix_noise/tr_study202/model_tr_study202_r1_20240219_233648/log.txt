Args:
Namespace(name='model_tr_study202', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1', training_data='data/transition_rate_studies/tr_study202/tr_study202_training/r1', validation_data='data/transition_rate_studies/tr_study202/tr_study202_validation/r1', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1989686345

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.955160877343639		[learning rate: 0.01]
		[batch 20/20] avg loss: 9.342274304708123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.148717591025882 | validation: 8.563337305812073]
	TIME [epoch: 77.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.08563455841782		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.929943623969852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.507789091193837 | validation: 9.900768369803163]
	TIME [epoch: 8.2 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.459350589174444		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.650119145645344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.054734867409894 | validation: 3.6022522040935123]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.075940154213461		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.9065458492212803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.991243001717371 | validation: 4.253928781939789]
	TIME [epoch: 8.17 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.150979373526365		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5772871066316725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.864133240079019 | validation: 4.080610351492055]
	TIME [epoch: 8.19 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.752544281149625		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.587417351759554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.669980816454589 | validation: 4.428347583555093]
	TIME [epoch: 8.18 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.670905105122622		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.61369029927151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.642297702197065 | validation: 4.360954160517437]
	TIME [epoch: 8.17 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5491337640802585		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.7776557944008595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6633947792405595 | validation: 3.823955916146686]
	TIME [epoch: 8.21 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3908757938888856		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.419406945683985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.405141369786435 | validation: 3.550503242037152]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7177576735000137		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.057481271634757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3876194725673847 | validation: 3.557986417277879]
	TIME [epoch: 8.18 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1398672941364643		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0950574771841017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.117462385660283 | validation: 3.379218714002258]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.013279389041832		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0758397754911058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0445595822664684 | validation: 3.2909840302854168]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8844491438890665		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.983007841762031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9337284928255487 | validation: 3.083734029620895]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7835115541655435		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8923608789774393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8379362165714914 | validation: 3.0414223290926468]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7646163461514788		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.536169900860725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6503931235061016 | validation: 2.841198467847915]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8784019972064407		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3547423401084364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.616572168657438 | validation: 1.1999095644866997]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6429438869663682		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7198103258819184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6813771064241432 | validation: 1.044537193663583]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3463540539880736		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.348570236582568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3474621452853208 | validation: 2.2266979603620145]
	TIME [epoch: 8.19 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4824886013338379		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2688353271452586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3756619642395482 | validation: 2.972035470415352]
	TIME [epoch: 8.16 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.719267074827167		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2090368015149222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4641519381710446 | validation: 1.0814736613793616]
	TIME [epoch: 8.16 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.209890289467014		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2573440080408245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2336171487539191 | validation: 1.5263054299117658]
	TIME [epoch: 8.17 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.17961121954841		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0786034551133095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1291073373308598 | validation: 1.8358952197391303]
	TIME [epoch: 8.19 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.138792206679717		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0895586043035463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1141754054916313 | validation: 0.7555837963098568]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1072853237988263		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3055908772895863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2064381005442062 | validation: 1.5081649627920406]
	TIME [epoch: 8.16 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9884875232664442		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9966634862676301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9925755047670375 | validation: 0.7756554530256922]
	TIME [epoch: 8.16 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0499697461290087		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3269261251410165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1884479356350128 | validation: 0.8372354066938186]
	TIME [epoch: 8.16 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0971910383108219		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9564068184738282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0267989283923251 | validation: 1.131157067404702]
	TIME [epoch: 8.19 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0895234504995668		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1297625537834384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.109643002141502 | validation: 0.8996526951877196]
	TIME [epoch: 8.16 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1264504591650206		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8649161246332652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9956832918991431 | validation: 1.1258103526119108]
	TIME [epoch: 8.17 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9776550479465038		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8363504757316325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.907002761839068 | validation: 1.140941894289057]
	TIME [epoch: 8.16 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8974225759302623		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8954768448662007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8964497103982316 | validation: 1.091242460029024]
	TIME [epoch: 8.19 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9347067332698258		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7819789259740906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8583428296219582 | validation: 0.7223083763230856]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8665645334207674		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9420359496084035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9043002415145855 | validation: 0.9056419749567302]
	TIME [epoch: 8.16 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9313361514354522		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.816935129895948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8741356406657002 | validation: 0.8961793762841723]
	TIME [epoch: 8.16 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8195178172225793		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.897081648728234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8582997329754066 | validation: 0.8561866036003827]
	TIME [epoch: 8.18 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8727070921150599		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2016557659958202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.03718142905544 | validation: 1.2237505797496853]
	TIME [epoch: 8.17 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8871363813899962		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8372695596849585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8622029705374773 | validation: 0.5195197654661325]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8513933039091854		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8738168029799109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8626050534445483 | validation: 0.5528802165682632]
	TIME [epoch: 8.18 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.849940565913945		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7853086304619147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8176245981879298 | validation: 0.5594318843689158]
	TIME [epoch: 8.17 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8650192186629064		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.781745193484397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8233822060736516 | validation: 0.6213926970880146]
	TIME [epoch: 8.2 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8357127997976873		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8266268835995062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8311698416985968 | validation: 1.2704122907320863]
	TIME [epoch: 8.15 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9388833674436523		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7661035498043834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.852493458624018 | validation: 0.6100943012084781]
	TIME [epoch: 8.17 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.837400087784793		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1101040222761573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9737520550304752 | validation: 1.7235625715081058]
	TIME [epoch: 8.17 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.178509027773306		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7717053144673881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9751071711203471 | validation: 1.3885519740835495]
	TIME [epoch: 8.19 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8030331992749229		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8009207736992302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8019769864870767 | validation: 0.5199748342104733]
	TIME [epoch: 8.18 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8958206886740092		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.824172700579106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8599966946265575 | validation: 0.8753130214374609]
	TIME [epoch: 8.16 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8117933872489085		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7349281556945859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7733607714717472 | validation: 0.8195461582817449]
	TIME [epoch: 8.16 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7477250815957258		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9568307819532034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8522779317744646 | validation: 0.6926428435751298]
	TIME [epoch: 8.17 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.784709898526655		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6993621470304312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.742036022778543 | validation: 0.6265271836796802]
	TIME [epoch: 8.2 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8745943445499152		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8939014474683434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8842478960091293 | validation: 0.5304752594327311]
	TIME [epoch: 8.17 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7828256830639643		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.72406489755094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7534452903074522 | validation: 0.7426559497681926]
	TIME [epoch: 8.17 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7928134725299689		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7808705313181624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7868420019240657 | validation: 0.4517307144920047]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7989104775902884		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6999898721130424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7494501748516653 | validation: 0.7911885046834689]
	TIME [epoch: 8.19 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7242047826560949		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7575695264351929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7408871545456439 | validation: 0.48050806555566433]
	TIME [epoch: 8.16 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7109297657926257		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7226877729952859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7168087693939558 | validation: 0.7253016835534325]
	TIME [epoch: 8.15 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8205828369157577		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7329953929433971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7767891149295776 | validation: 0.5502710084180961]
	TIME [epoch: 8.16 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7582667172533959		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6308957270120853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6945812221327404 | validation: 1.3577198508820107]
	TIME [epoch: 8.18 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8448515928880024		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8901994978054585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8675255453467304 | validation: 0.5567855214451846]
	TIME [epoch: 8.18 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6567584211616238		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.70497972618704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6808690736743319 | validation: 1.1804193176280164]
	TIME [epoch: 8.16 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7454508077267685		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7700151472450762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7577329774859225 | validation: 1.1240360772773506]
	TIME [epoch: 8.16 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9840475610914161		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7452434529016061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8646455069965111 | validation: 0.5999251820476054]
	TIME [epoch: 8.16 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8469251873841008		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7895883364738244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8182567619289625 | validation: 0.443898504355268]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6764670877689025		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8160090853602391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7462380865645709 | validation: 0.5196277998702882]
	TIME [epoch: 8.16 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8518540778669104		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8380936924389382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8449738851529242 | validation: 0.5410686482300003]
	TIME [epoch: 8.16 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7865976928338767		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7645719877395376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7755848402867073 | validation: 0.44186995625804154]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8680777761269087		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6284492741274554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7482635251271822 | validation: 0.5302787832102129]
	TIME [epoch: 8.19 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7071954153712173		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6764854856538113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6918404505125143 | validation: 0.6783021126220876]
	TIME [epoch: 8.16 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7201115397111624		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6783027531703724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6992071464407673 | validation: 0.5278127093764724]
	TIME [epoch: 8.17 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6789876090894971		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7078075238303814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6933975664599393 | validation: 0.6805808692381128]
	TIME [epoch: 8.17 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6591942478850672		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6264188265648916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6428065372249794 | validation: 0.4104105032505974]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7065881436231678		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7502682873330361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7284282154781019 | validation: 1.0175596788482901]
	TIME [epoch: 8.19 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6695807131484102		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7013000992285605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6854404061884856 | validation: 0.5042418028208036]
	TIME [epoch: 8.16 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6621040964573489		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7271247126771853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6946144045672671 | validation: 0.4874309427608088]
	TIME [epoch: 8.16 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5812374621291755		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7084673190164053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6448523905727904 | validation: 0.6516223175733971]
	TIME [epoch: 8.16 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5896752929508253		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0348394610770264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8122573770139259 | validation: 0.7509077523770106]
	TIME [epoch: 8.19 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6660184020130584		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8070659734144121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7365421877137351 | validation: 0.5973106328970113]
	TIME [epoch: 8.17 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8050442606417029		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6956024208207596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7503233407312313 | validation: 0.6073535315457185]
	TIME [epoch: 8.16 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8855800906580045		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6969077629186091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7912439267883068 | validation: 0.68240483880791]
	TIME [epoch: 8.16 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8679411779200878		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6715742435509451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7697577107355164 | validation: 0.7236491439600793]
	TIME [epoch: 8.17 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6174351731407904		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6766476807964348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6470414269686127 | validation: 1.459695908466675]
	TIME [epoch: 8.17 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8078365287690007		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6000023280841585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7039194284265797 | validation: 0.4110773234044057]
	TIME [epoch: 8.15 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.621728496897118		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6315290774547352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6266287871759265 | validation: 0.409029290256361]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6834430851347716		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.579352566489551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6313978258121614 | validation: 0.5135196826571612]
	TIME [epoch: 8.17 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7308608807000954		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6751521386346003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.703006509667348 | validation: 0.9188582396079007]
	TIME [epoch: 8.2 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.65332598735993		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5364489650133875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5948874761866587 | validation: 0.9959517767778797]
	TIME [epoch: 8.16 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5913665066233942		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.565486611079211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5784265588513026 | validation: 0.43786587749581174]
	TIME [epoch: 8.16 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6012979295763202		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7149492730815824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6581236013289512 | validation: 0.4102905149183357]
	TIME [epoch: 8.17 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6878192631755013		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.569177133215727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6284981981956141 | validation: 0.523119971458502]
	TIME [epoch: 8.17 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6617612674736619		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5303431073973008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5960521874354814 | validation: 1.0205034288801276]
	TIME [epoch: 8.18 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7063343635627392		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5830926469593934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6447135052610662 | validation: 0.6609498324857663]
	TIME [epoch: 8.15 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6701235714275224		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5436191211018173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.60687134626467 | validation: 0.5582135541773899]
	TIME [epoch: 8.15 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.690710513027525		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8984102844366136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7945603987320694 | validation: 0.8298082821318814]
	TIME [epoch: 8.15 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.648087258343382		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6402359898130032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6441616240781926 | validation: 0.47982472211745436]
	TIME [epoch: 8.19 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5746710079002135		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7022803669443606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6384756874222871 | validation: 0.41432274664000923]
	TIME [epoch: 8.17 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6206299912142262		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.623558620091051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6220943056526387 | validation: 0.6071232768543617]
	TIME [epoch: 8.19 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5253957574335959		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.777873323209721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6516345403216584 | validation: 0.4611897993073333]
	TIME [epoch: 8.16 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5924154124217529		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5458429387817183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5691291756017356 | validation: 0.46722857826804454]
	TIME [epoch: 8.19 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7054112686718562		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7095442975523327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7074777831120944 | validation: 1.1139772298845458]
	TIME [epoch: 8.16 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6322801523931035		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.60541283273814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6188464925656216 | validation: 0.3443288438198762]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6081125485976966		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6281035029481077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6181080257729021 | validation: 0.4689627628134712]
	TIME [epoch: 8.18 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5808700553846319		[learning rate: 0.0099837]
		[batch 20/20] avg loss: 0.640354577750132		[learning rate: 0.0099655]
	Learning Rate: 0.00996552
	LOSS [training: 0.610612316567382 | validation: 0.3599828311979695]
	TIME [epoch: 8.19 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6468376096672592		[learning rate: 0.0099474]
		[batch 20/20] avg loss: 0.5967978139700223		[learning rate: 0.0099294]
	Learning Rate: 0.00992935
	LOSS [training: 0.6218177118186409 | validation: 0.7067688701029209]
	TIME [epoch: 8.2 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.588931428163306		[learning rate: 0.0099113]
		[batch 20/20] avg loss: 0.6676022585783317		[learning rate: 0.0098933]
	Learning Rate: 0.00989332
	LOSS [training: 0.6282668433708188 | validation: 0.4374300911238809]
	TIME [epoch: 8.19 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7256027865747539		[learning rate: 0.0098754]
		[batch 20/20] avg loss: 0.5513618267853982		[learning rate: 0.0098574]
	Learning Rate: 0.00985742
	LOSS [training: 0.638482306680076 | validation: 0.44907905862804764]
	TIME [epoch: 8.19 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6730641350101259		[learning rate: 0.0098395]
		[batch 20/20] avg loss: 0.6356699733555351		[learning rate: 0.0098216]
	Learning Rate: 0.00982164
	LOSS [training: 0.6543670541828305 | validation: 0.647535134045726]
	TIME [epoch: 8.18 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5732847784316073		[learning rate: 0.0098038]
		[batch 20/20] avg loss: 0.7860807583479621		[learning rate: 0.009786]
	Learning Rate: 0.009786
	LOSS [training: 0.6796827683897848 | validation: 0.7732303629384172]
	TIME [epoch: 8.22 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6002483127836588		[learning rate: 0.0097682]
		[batch 20/20] avg loss: 0.6292592203541043		[learning rate: 0.0097505]
	Learning Rate: 0.00975049
	LOSS [training: 0.6147537665688816 | validation: 0.6303442738093952]
	TIME [epoch: 8.18 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6126653976813949		[learning rate: 0.0097328]
		[batch 20/20] avg loss: 0.63124864833252		[learning rate: 0.0097151]
	Learning Rate: 0.0097151
	LOSS [training: 0.6219570230069577 | validation: 1.186958672524912]
	TIME [epoch: 8.18 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8461542529341821		[learning rate: 0.0096975]
		[batch 20/20] avg loss: 0.555976598925555		[learning rate: 0.0096798]
	Learning Rate: 0.00967984
	LOSS [training: 0.7010654259298686 | validation: 0.5437481308892271]
	TIME [epoch: 8.19 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5607600065887552		[learning rate: 0.0096623]
		[batch 20/20] avg loss: 0.6577452669702655		[learning rate: 0.0096447]
	Learning Rate: 0.00964472
	LOSS [training: 0.6092526367795104 | validation: 0.3689519115710781]
	TIME [epoch: 8.2 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.580812969790857		[learning rate: 0.0096272]
		[batch 20/20] avg loss: 0.6344622666740379		[learning rate: 0.0096097]
	Learning Rate: 0.00960972
	LOSS [training: 0.6076376182324476 | validation: 0.42210411720564217]
	TIME [epoch: 8.2 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6429294407730828		[learning rate: 0.0095923]
		[batch 20/20] avg loss: 0.5956557265316921		[learning rate: 0.0095748]
	Learning Rate: 0.00957484
	LOSS [training: 0.6192925836523874 | validation: 1.5161506379678862]
	TIME [epoch: 8.18 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6169066485575149		[learning rate: 0.0095575]
		[batch 20/20] avg loss: 0.6179807869275848		[learning rate: 0.0095401]
	Learning Rate: 0.00954009
	LOSS [training: 0.6174437177425498 | validation: 0.8086100274374305]
	TIME [epoch: 8.19 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7109616993699669		[learning rate: 0.0095228]
		[batch 20/20] avg loss: 0.5711322272402108		[learning rate: 0.0095055]
	Learning Rate: 0.00950547
	LOSS [training: 0.6410469633050889 | validation: 0.5734360819455845]
	TIME [epoch: 8.18 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6630854742811245		[learning rate: 0.0094882]
		[batch 20/20] avg loss: 0.515773448873721		[learning rate: 0.009471]
	Learning Rate: 0.00947098
	LOSS [training: 0.5894294615774228 | validation: 0.8016757668823729]
	TIME [epoch: 8.22 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6449940646450925		[learning rate: 0.0094538]
		[batch 20/20] avg loss: 0.6255349377489761		[learning rate: 0.0094366]
	Learning Rate: 0.0094366
	LOSS [training: 0.6352645011970341 | validation: 0.5299470354566292]
	TIME [epoch: 8.19 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.542529977162146		[learning rate: 0.0094195]
		[batch 20/20] avg loss: 0.7118298120645534		[learning rate: 0.0094024]
	Learning Rate: 0.00940236
	LOSS [training: 0.6271798946133498 | validation: 1.0664258864886307]
	TIME [epoch: 8.19 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5250146977425167		[learning rate: 0.0093853]
		[batch 20/20] avg loss: 0.5268320356041712		[learning rate: 0.0093682]
	Learning Rate: 0.00936824
	LOSS [training: 0.525923366673344 | validation: 0.352949420516796]
	TIME [epoch: 8.19 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.680215646205119		[learning rate: 0.0093512]
		[batch 20/20] avg loss: 0.5834161217159526		[learning rate: 0.0093342]
	Learning Rate: 0.00933424
	LOSS [training: 0.6318158839605358 | validation: 0.44947554443625914]
	TIME [epoch: 8.2 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6114167628508956		[learning rate: 0.0093173]
		[batch 20/20] avg loss: 0.5297230582691712		[learning rate: 0.0093004]
	Learning Rate: 0.00930036
	LOSS [training: 0.5705699105600335 | validation: 0.7079970302977263]
	TIME [epoch: 8.19 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6129821475839353		[learning rate: 0.0092835]
		[batch 20/20] avg loss: 0.6557701327876815		[learning rate: 0.0092666]
	Learning Rate: 0.00926661
	LOSS [training: 0.6343761401858083 | validation: 1.4231715648266414]
	TIME [epoch: 8.18 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6524788550669457		[learning rate: 0.0092498]
		[batch 20/20] avg loss: 0.5908849829423708		[learning rate: 0.009233]
	Learning Rate: 0.00923298
	LOSS [training: 0.6216819190046583 | validation: 0.5244147089552567]
	TIME [epoch: 8.18 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6590731113648559		[learning rate: 0.0092162]
		[batch 20/20] avg loss: 0.5689623136490664		[learning rate: 0.0091995]
	Learning Rate: 0.00919948
	LOSS [training: 0.6140177125069612 | validation: 0.5091521690245852]
	TIME [epoch: 8.19 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5190414260308192		[learning rate: 0.0091828]
		[batch 20/20] avg loss: 0.5122399663275233		[learning rate: 0.0091661]
	Learning Rate: 0.00916609
	LOSS [training: 0.5156406961791713 | validation: 0.5420164497783349]
	TIME [epoch: 8.21 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5858722818077997		[learning rate: 0.0091494]
		[batch 20/20] avg loss: 0.5133260190882807		[learning rate: 0.0091328]
	Learning Rate: 0.00913283
	LOSS [training: 0.5495991504480402 | validation: 0.6692226079993348]
	TIME [epoch: 8.18 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.544889209228008		[learning rate: 0.0091162]
		[batch 20/20] avg loss: 0.5229249222952654		[learning rate: 0.0090997]
	Learning Rate: 0.00909968
	LOSS [training: 0.5339070657616367 | validation: 0.4438748982220621]
	TIME [epoch: 8.18 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5141831989357873		[learning rate: 0.0090832]
		[batch 20/20] avg loss: 0.7007899003701459		[learning rate: 0.0090667]
	Learning Rate: 0.00906666
	LOSS [training: 0.6074865496529664 | validation: 0.46566067987585136]
	TIME [epoch: 8.17 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5015212285745309		[learning rate: 0.0090502]
		[batch 20/20] avg loss: 0.5527984498787853		[learning rate: 0.0090338]
	Learning Rate: 0.00903376
	LOSS [training: 0.5271598392266581 | validation: 0.898460881339991]
	TIME [epoch: 8.21 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5108343403528004		[learning rate: 0.0090174]
		[batch 20/20] avg loss: 0.6458119052808696		[learning rate: 0.009001]
	Learning Rate: 0.00900097
	LOSS [training: 0.578323122816835 | validation: 0.4145947004699847]
	TIME [epoch: 8.19 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5185452344447916		[learning rate: 0.0089846]
		[batch 20/20] avg loss: 0.5254258370135443		[learning rate: 0.0089683]
	Learning Rate: 0.00896831
	LOSS [training: 0.5219855357291678 | validation: 0.7569949330747454]
	TIME [epoch: 8.18 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6629855816209232		[learning rate: 0.008952]
		[batch 20/20] avg loss: 0.6144587348589889		[learning rate: 0.0089358]
	Learning Rate: 0.00893576
	LOSS [training: 0.638722158239956 | validation: 0.805136320191939]
	TIME [epoch: 8.18 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6457189946946029		[learning rate: 0.0089195]
		[batch 20/20] avg loss: 0.5528366621755059		[learning rate: 0.0089033]
	Learning Rate: 0.00890333
	LOSS [training: 0.5992778284350545 | validation: 0.36896911584767633]
	TIME [epoch: 8.19 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6864693810786091		[learning rate: 0.0088872]
		[batch 20/20] avg loss: 0.4700785971793298		[learning rate: 0.008871]
	Learning Rate: 0.00887102
	LOSS [training: 0.5782739891289694 | validation: 0.4877317171670715]
	TIME [epoch: 8.21 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49580509190295496		[learning rate: 0.0088549]
		[batch 20/20] avg loss: 0.5776366935810572		[learning rate: 0.0088388]
	Learning Rate: 0.00883883
	LOSS [training: 0.5367208927420062 | validation: 0.5680781221454885]
	TIME [epoch: 8.18 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5025104547712596		[learning rate: 0.0088228]
		[batch 20/20] avg loss: 0.5487140584208507		[learning rate: 0.0088068]
	Learning Rate: 0.00880675
	LOSS [training: 0.5256122565960551 | validation: 0.3476631427112236]
	TIME [epoch: 8.19 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5320292260616358		[learning rate: 0.0087908]
		[batch 20/20] avg loss: 0.5043089816199412		[learning rate: 0.0087748]
	Learning Rate: 0.00877479
	LOSS [training: 0.5181691038407884 | validation: 0.6912702498265194]
	TIME [epoch: 8.18 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6672040905212495		[learning rate: 0.0087589]
		[batch 20/20] avg loss: 0.5777545717756981		[learning rate: 0.0087429]
	Learning Rate: 0.00874295
	LOSS [training: 0.6224793311484739 | validation: 0.42955605480740455]
	TIME [epoch: 8.21 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.521996517491089		[learning rate: 0.0087271]
		[batch 20/20] avg loss: 0.4770939028188724		[learning rate: 0.0087112]
	Learning Rate: 0.00871122
	LOSS [training: 0.4995452101549809 | validation: 0.6269616767918078]
	TIME [epoch: 8.18 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5269922529605222		[learning rate: 0.0086954]
		[batch 20/20] avg loss: 0.6121542821026588		[learning rate: 0.0086796]
	Learning Rate: 0.00867961
	LOSS [training: 0.5695732675315904 | validation: 0.7879368009209208]
	TIME [epoch: 8.18 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5620562420482159		[learning rate: 0.0086638]
		[batch 20/20] avg loss: 0.6036029679980881		[learning rate: 0.0086481]
	Learning Rate: 0.00864811
	LOSS [training: 0.582829605023152 | validation: 0.4223761406977119]
	TIME [epoch: 8.18 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5061825535083598		[learning rate: 0.0086324]
		[batch 20/20] avg loss: 0.5292593466038592		[learning rate: 0.0086167]
	Learning Rate: 0.00861672
	LOSS [training: 0.5177209500561095 | validation: 0.39446490012159674]
	TIME [epoch: 8.2 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5387010791124014		[learning rate: 0.0086011]
		[batch 20/20] avg loss: 0.5429870869446027		[learning rate: 0.0085855]
	Learning Rate: 0.00858545
	LOSS [training: 0.540844083028502 | validation: 0.4716929806477406]
	TIME [epoch: 8.19 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48054508164799314		[learning rate: 0.0085699]
		[batch 20/20] avg loss: 0.5126551979889851		[learning rate: 0.0085543]
	Learning Rate: 0.00855429
	LOSS [training: 0.49660013981848905 | validation: 0.46844990111709983]
	TIME [epoch: 8.18 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45944420399041563		[learning rate: 0.0085388]
		[batch 20/20] avg loss: 0.5583905294278735		[learning rate: 0.0085232]
	Learning Rate: 0.00852325
	LOSS [training: 0.5089173667091447 | validation: 0.3602793034893851]
	TIME [epoch: 8.18 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5687780321305557		[learning rate: 0.0085078]
		[batch 20/20] avg loss: 0.5421641344919418		[learning rate: 0.0084923]
	Learning Rate: 0.00849232
	LOSS [training: 0.5554710833112486 | validation: 0.3848374228343496]
	TIME [epoch: 8.18 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5397604915314653		[learning rate: 0.0084769]
		[batch 20/20] avg loss: 0.4969750074535674		[learning rate: 0.0084615]
	Learning Rate: 0.0084615
	LOSS [training: 0.5183677494925163 | validation: 0.40226404176027386]
	TIME [epoch: 8.21 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5597395669070272		[learning rate: 0.0084461]
		[batch 20/20] avg loss: 0.7159990827740208		[learning rate: 0.0084308]
	Learning Rate: 0.00843079
	LOSS [training: 0.637869324840524 | validation: 0.7087584651440995]
	TIME [epoch: 8.18 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6099999667981294		[learning rate: 0.0084155]
		[batch 20/20] avg loss: 0.427574881455213		[learning rate: 0.0084002]
	Learning Rate: 0.0084002
	LOSS [training: 0.5187874241266711 | validation: 0.6716657309283083]
	TIME [epoch: 8.18 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4899140418178341		[learning rate: 0.0083849]
		[batch 20/20] avg loss: 0.4741285907891747		[learning rate: 0.0083697]
	Learning Rate: 0.00836971
	LOSS [training: 0.4820213163035043 | validation: 0.2734952833529329]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48108046196444854		[learning rate: 0.0083545]
		[batch 20/20] avg loss: 0.5100095406018766		[learning rate: 0.0083393]
	Learning Rate: 0.00833934
	LOSS [training: 0.49554500128316264 | validation: 0.43542985919532906]
	TIME [epoch: 8.21 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5863148621959529		[learning rate: 0.0083242]
		[batch 20/20] avg loss: 0.49969676505699284		[learning rate: 0.0083091]
	Learning Rate: 0.00830907
	LOSS [training: 0.5430058136264728 | validation: 0.7221735398443991]
	TIME [epoch: 8.18 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8580138193965539		[learning rate: 0.008294]
		[batch 20/20] avg loss: 0.5117407347772309		[learning rate: 0.0082789]
	Learning Rate: 0.00827892
	LOSS [training: 0.6848772770868923 | validation: 0.5886830866182808]
	TIME [epoch: 8.18 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4607861124058476		[learning rate: 0.0082639]
		[batch 20/20] avg loss: 0.521228380192485		[learning rate: 0.0082489]
	Learning Rate: 0.00824887
	LOSS [training: 0.4910072462991663 | validation: 0.3636556799285218]
	TIME [epoch: 8.18 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4661778313139191		[learning rate: 0.0082339]
		[batch 20/20] avg loss: 0.5744619291241209		[learning rate: 0.0082189]
	Learning Rate: 0.00821894
	LOSS [training: 0.52031988021902 | validation: 0.3132903837602381]
	TIME [epoch: 8.18 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4749279423949508		[learning rate: 0.008204]
		[batch 20/20] avg loss: 0.5405136134982964		[learning rate: 0.0081891]
	Learning Rate: 0.00818911
	LOSS [training: 0.5077207779466237 | validation: 0.55287762173663]
	TIME [epoch: 8.21 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5386432150799129		[learning rate: 0.0081742]
		[batch 20/20] avg loss: 0.5576444257966984		[learning rate: 0.0081594]
	Learning Rate: 0.00815939
	LOSS [training: 0.5481438204383056 | validation: 0.46252900873188857]
	TIME [epoch: 8.18 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4649013883575209		[learning rate: 0.0081446]
		[batch 20/20] avg loss: 0.5018280890396533		[learning rate: 0.0081298]
	Learning Rate: 0.00812978
	LOSS [training: 0.483364738698587 | validation: 0.4621675893963225]
	TIME [epoch: 8.18 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5078171846117738		[learning rate: 0.008115]
		[batch 20/20] avg loss: 0.4988666858945824		[learning rate: 0.0081003]
	Learning Rate: 0.00810028
	LOSS [training: 0.5033419352531782 | validation: 0.49525181014189373]
	TIME [epoch: 8.17 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44810644970793534		[learning rate: 0.0080856]
		[batch 20/20] avg loss: 0.5003528933627315		[learning rate: 0.0080709]
	Learning Rate: 0.00807088
	LOSS [training: 0.47422967153533346 | validation: 0.3676650752824754]
	TIME [epoch: 8.21 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5029521426204695		[learning rate: 0.0080562]
		[batch 20/20] avg loss: 0.44261968212545916		[learning rate: 0.0080416]
	Learning Rate: 0.00804159
	LOSS [training: 0.4727859123729644 | validation: 0.2959560986916731]
	TIME [epoch: 8.18 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5447044886094988		[learning rate: 0.008027]
		[batch 20/20] avg loss: 0.5352304436399551		[learning rate: 0.0080124]
	Learning Rate: 0.00801241
	LOSS [training: 0.5399674661247269 | validation: 0.41062463038768954]
	TIME [epoch: 8.18 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45241737163279544		[learning rate: 0.0079979]
		[batch 20/20] avg loss: 0.5693727518723913		[learning rate: 0.0079833]
	Learning Rate: 0.00798333
	LOSS [training: 0.5108950617525935 | validation: 0.596753607483868]
	TIME [epoch: 8.18 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4767329775751349		[learning rate: 0.0079688]
		[batch 20/20] avg loss: 0.5325770767417463		[learning rate: 0.0079544]
	Learning Rate: 0.00795436
	LOSS [training: 0.5046550271584405 | validation: 0.5665667703719789]
	TIME [epoch: 8.18 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.499106729925621		[learning rate: 0.0079399]
		[batch 20/20] avg loss: 0.4408124158655163		[learning rate: 0.0079255]
	Learning Rate: 0.00792549
	LOSS [training: 0.46995957289556867 | validation: 0.3717544685949409]
	TIME [epoch: 8.21 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6757192413346396		[learning rate: 0.0079111]
		[batch 20/20] avg loss: 0.45545508242637		[learning rate: 0.0078967]
	Learning Rate: 0.00789673
	LOSS [training: 0.5655871618805046 | validation: 0.38190229747599636]
	TIME [epoch: 8.17 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49461083745904544		[learning rate: 0.0078824]
		[batch 20/20] avg loss: 0.48355778249756315		[learning rate: 0.0078681]
	Learning Rate: 0.00786807
	LOSS [training: 0.48908430997830427 | validation: 0.6856696607232642]
	TIME [epoch: 8.17 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6951182446612627		[learning rate: 0.0078538]
		[batch 20/20] avg loss: 0.43438321924101847		[learning rate: 0.0078395]
	Learning Rate: 0.00783952
	LOSS [training: 0.5647507319511406 | validation: 0.8018273734855847]
	TIME [epoch: 8.17 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4474598852666887		[learning rate: 0.0078253]
		[batch 20/20] avg loss: 0.7796092661504306		[learning rate: 0.0078111]
	Learning Rate: 0.00781107
	LOSS [training: 0.6135345757085597 | validation: 0.28242604064175914]
	TIME [epoch: 8.21 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4163570461037517		[learning rate: 0.0077969]
		[batch 20/20] avg loss: 0.4538977407960891		[learning rate: 0.0077827]
	Learning Rate: 0.00778272
	LOSS [training: 0.4351273934499204 | validation: 0.2694764811374385]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39447705297304514		[learning rate: 0.0077686]
		[batch 20/20] avg loss: 0.572637389990008		[learning rate: 0.0077545]
	Learning Rate: 0.00775448
	LOSS [training: 0.4835572214815266 | validation: 0.2668392832951831]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.404377286322214		[learning rate: 0.0077404]
		[batch 20/20] avg loss: 0.4244980915520637		[learning rate: 0.0077263]
	Learning Rate: 0.00772634
	LOSS [training: 0.41443768893713884 | validation: 0.2856198093423249]
	TIME [epoch: 8.18 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4723756939565993		[learning rate: 0.0077123]
		[batch 20/20] avg loss: 0.42880018555323673		[learning rate: 0.0076983]
	Learning Rate: 0.0076983
	LOSS [training: 0.45058793975491807 | validation: 0.318200978875667]
	TIME [epoch: 8.19 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33898690788788843		[learning rate: 0.0076843]
		[batch 20/20] avg loss: 0.472380622788667		[learning rate: 0.0076704]
	Learning Rate: 0.00767036
	LOSS [training: 0.4056837653382777 | validation: 0.2472805681393982]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4500677629939295		[learning rate: 0.0076564]
		[batch 20/20] avg loss: 0.4628287078160797		[learning rate: 0.0076425]
	Learning Rate: 0.00764252
	LOSS [training: 0.4564482354050045 | validation: 0.5143586606220831]
	TIME [epoch: 8.17 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44121007308110183		[learning rate: 0.0076286]
		[batch 20/20] avg loss: 0.38259141474093084		[learning rate: 0.0076148]
	Learning Rate: 0.00761479
	LOSS [training: 0.4119007439110162 | validation: 0.4916198967889992]
	TIME [epoch: 8.17 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4827779158211717		[learning rate: 0.007601]
		[batch 20/20] avg loss: 0.4215651665434514		[learning rate: 0.0075872]
	Learning Rate: 0.00758715
	LOSS [training: 0.45217154118231156 | validation: 0.24985063785133796]
	TIME [epoch: 8.16 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5579557575204573		[learning rate: 0.0075734]
		[batch 20/20] avg loss: 0.42251302828990084		[learning rate: 0.0075596]
	Learning Rate: 0.00755962
	LOSS [training: 0.4902343929051791 | validation: 0.5904959267271715]
	TIME [epoch: 8.2 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4177386921898095		[learning rate: 0.0075459]
		[batch 20/20] avg loss: 0.37124608542595244		[learning rate: 0.0075322]
	Learning Rate: 0.00753219
	LOSS [training: 0.39449238880788096 | validation: 0.5014382799171448]
	TIME [epoch: 8.16 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3954450169610312		[learning rate: 0.0075185]
		[batch 20/20] avg loss: 0.34862490030414867		[learning rate: 0.0075049]
	Learning Rate: 0.00750485
	LOSS [training: 0.37203495863258984 | validation: 0.33217908896118703]
	TIME [epoch: 8.17 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.642898288850726		[learning rate: 0.0074912]
		[batch 20/20] avg loss: 0.38107953474983297		[learning rate: 0.0074776]
	Learning Rate: 0.00747762
	LOSS [training: 0.5119889118002794 | validation: 0.47402437877592896]
	TIME [epoch: 8.16 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5982291600580737		[learning rate: 0.007464]
		[batch 20/20] avg loss: 0.45018347860411884		[learning rate: 0.0074505]
	Learning Rate: 0.00745048
	LOSS [training: 0.5242063193310962 | validation: 0.3019608996349648]
	TIME [epoch: 8.2 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4656189783416818		[learning rate: 0.0074369]
		[batch 20/20] avg loss: 0.4657121151194306		[learning rate: 0.0074234]
	Learning Rate: 0.00742344
	LOSS [training: 0.4656655467305562 | validation: 0.4018425290999911]
	TIME [epoch: 8.18 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4849393724169865		[learning rate: 0.00741]
		[batch 20/20] avg loss: 0.45561936116313256		[learning rate: 0.0073965]
	Learning Rate: 0.0073965
	LOSS [training: 0.47027936679005944 | validation: 0.2724924881088071]
	TIME [epoch: 8.17 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39538703911923256		[learning rate: 0.0073831]
		[batch 20/20] avg loss: 0.4497830981872576		[learning rate: 0.0073697]
	Learning Rate: 0.00736966
	LOSS [training: 0.4225850686532451 | validation: 0.5187589995705427]
	TIME [epoch: 8.17 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5621255132127753		[learning rate: 0.0073563]
		[batch 20/20] avg loss: 0.44113637575614495		[learning rate: 0.0073429]
	Learning Rate: 0.00734291
	LOSS [training: 0.5016309444844602 | validation: 0.3876003304705278]
	TIME [epoch: 8.18 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5086748897985525		[learning rate: 0.0073296]
		[batch 20/20] avg loss: 0.4894171314392149		[learning rate: 0.0073163]
	Learning Rate: 0.00731627
	LOSS [training: 0.4990460106188837 | validation: 0.7252618256866401]
	TIME [epoch: 8.19 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39994548705122174		[learning rate: 0.007303]
		[batch 20/20] avg loss: 0.41521200991273777		[learning rate: 0.0072897]
	Learning Rate: 0.00728971
	LOSS [training: 0.4075787484819798 | validation: 0.3784099656103034]
	TIME [epoch: 8.16 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.430429835274344		[learning rate: 0.0072765]
		[batch 20/20] avg loss: 0.4384437094612211		[learning rate: 0.0072633]
	Learning Rate: 0.00726326
	LOSS [training: 0.43443677236778255 | validation: 0.3965912378080567]
	TIME [epoch: 8.17 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39278594915059106		[learning rate: 0.0072501]
		[batch 20/20] avg loss: 0.36719944420242234		[learning rate: 0.0072369]
	Learning Rate: 0.0072369
	LOSS [training: 0.3799926966765066 | validation: 0.3981074151078591]
	TIME [epoch: 8.16 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48025882246958124		[learning rate: 0.0072238]
		[batch 20/20] avg loss: 0.3938001304847426		[learning rate: 0.0072106]
	Learning Rate: 0.00721064
	LOSS [training: 0.4370294764771619 | validation: 0.247709486354059]
	TIME [epoch: 8.19 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49147485463027224		[learning rate: 0.0071975]
		[batch 20/20] avg loss: 0.4093425710514776		[learning rate: 0.0071845]
	Learning Rate: 0.00718447
	LOSS [training: 0.4504087128408748 | validation: 0.4737069186265867]
	TIME [epoch: 8.16 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47239555473444084		[learning rate: 0.0071714]
		[batch 20/20] avg loss: 0.36552803143103485		[learning rate: 0.0071584]
	Learning Rate: 0.0071584
	LOSS [training: 0.4189617930827378 | validation: 0.3166625765496519]
	TIME [epoch: 8.17 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35450163182474426		[learning rate: 0.0071454]
		[batch 20/20] avg loss: 0.3973499855245846		[learning rate: 0.0071324]
	Learning Rate: 0.00713242
	LOSS [training: 0.3759258086746644 | validation: 0.6357360020907182]
	TIME [epoch: 8.17 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37976473443297143		[learning rate: 0.0071195]
		[batch 20/20] avg loss: 0.6409963548995141		[learning rate: 0.0071065]
	Learning Rate: 0.00710653
	LOSS [training: 0.5103805446662427 | validation: 0.2897497373747583]
	TIME [epoch: 8.18 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3727269983066637		[learning rate: 0.0070936]
		[batch 20/20] avg loss: 0.4183246397787241		[learning rate: 0.0070807]
	Learning Rate: 0.00708074
	LOSS [training: 0.39552581904269396 | validation: 0.24604685154693018]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4416832994876991		[learning rate: 0.0070679]
		[batch 20/20] avg loss: 0.3473414366485389		[learning rate: 0.007055]
	Learning Rate: 0.00705505
	LOSS [training: 0.39451236806811907 | validation: 0.6466658729879309]
	TIME [epoch: 8.17 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39826125688764746		[learning rate: 0.0070422]
		[batch 20/20] avg loss: 0.3994751463175105		[learning rate: 0.0070294]
	Learning Rate: 0.00702945
	LOSS [training: 0.39886820160257896 | validation: 0.36318547708842225]
	TIME [epoch: 8.17 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5692152763743936		[learning rate: 0.0070167]
		[batch 20/20] avg loss: 0.39690806737056805		[learning rate: 0.0070039]
	Learning Rate: 0.00700394
	LOSS [training: 0.48306167187248095 | validation: 0.21890092081860058]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46669656280382865		[learning rate: 0.0069912]
		[batch 20/20] avg loss: 0.45124126594175984		[learning rate: 0.0069785]
	Learning Rate: 0.00697852
	LOSS [training: 0.4589689143727943 | validation: 0.2631659626389834]
	TIME [epoch: 8.2 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4003506847760055		[learning rate: 0.0069658]
		[batch 20/20] avg loss: 0.4154282377336268		[learning rate: 0.0069532]
	Learning Rate: 0.00695319
	LOSS [training: 0.4078894612548162 | validation: 0.4436949836547003]
	TIME [epoch: 8.16 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3658606785558203		[learning rate: 0.0069406]
		[batch 20/20] avg loss: 0.32533061944055425		[learning rate: 0.006928]
	Learning Rate: 0.00692796
	LOSS [training: 0.34559564899818723 | validation: 0.2543734755645825]
	TIME [epoch: 8.16 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38472695558420605		[learning rate: 0.0069154]
		[batch 20/20] avg loss: 0.43211577499827747		[learning rate: 0.0069028]
	Learning Rate: 0.00690282
	LOSS [training: 0.40842136529124184 | validation: 0.6838162642881779]
	TIME [epoch: 8.16 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5636209733902008		[learning rate: 0.0068903]
		[batch 20/20] avg loss: 0.43815209693230794		[learning rate: 0.0068778]
	Learning Rate: 0.00687777
	LOSS [training: 0.5008865351612544 | validation: 0.17907174713126736]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3154326984031715		[learning rate: 0.0068653]
		[batch 20/20] avg loss: 0.39980655629136336		[learning rate: 0.0068528]
	Learning Rate: 0.00685281
	LOSS [training: 0.35761962734726743 | validation: 0.49345869806069037]
	TIME [epoch: 8.17 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40482352170525954		[learning rate: 0.0068404]
		[batch 20/20] avg loss: 0.36199614139684705		[learning rate: 0.0068279]
	Learning Rate: 0.00682794
	LOSS [training: 0.38340983155105335 | validation: 0.48597109562876395]
	TIME [epoch: 8.16 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3664929279822218		[learning rate: 0.0068155]
		[batch 20/20] avg loss: 0.4809207835679075		[learning rate: 0.0068032]
	Learning Rate: 0.00680316
	LOSS [training: 0.42370685577506456 | validation: 0.2389641850257927]
	TIME [epoch: 8.16 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44229331780628434		[learning rate: 0.0067908]
		[batch 20/20] avg loss: 0.35665131914448506		[learning rate: 0.0067785]
	Learning Rate: 0.00677847
	LOSS [training: 0.39947231847538467 | validation: 0.3974784080535422]
	TIME [epoch: 8.17 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34170020045658156		[learning rate: 0.0067662]
		[batch 20/20] avg loss: 0.40077716520777573		[learning rate: 0.0067539]
	Learning Rate: 0.00675387
	LOSS [training: 0.37123868283217865 | validation: 0.3094099126066517]
	TIME [epoch: 8.19 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4556399365535603		[learning rate: 0.0067416]
		[batch 20/20] avg loss: 0.3945021070179152		[learning rate: 0.0067294]
	Learning Rate: 0.00672936
	LOSS [training: 0.42507102178573763 | validation: 0.2866781771048391]
	TIME [epoch: 8.16 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3941014273978084		[learning rate: 0.0067171]
		[batch 20/20] avg loss: 0.5426635628434843		[learning rate: 0.0067049]
	Learning Rate: 0.00670494
	LOSS [training: 0.46838249512064634 | validation: 0.41703312428566686]
	TIME [epoch: 8.16 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39785621583330066		[learning rate: 0.0066928]
		[batch 20/20] avg loss: 0.36257145351590914		[learning rate: 0.0066806]
	Learning Rate: 0.0066806
	LOSS [training: 0.3802138346746049 | validation: 0.2594034033688313]
	TIME [epoch: 8.17 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4068055439953645		[learning rate: 0.0066685]
		[batch 20/20] avg loss: 0.3248645781796705		[learning rate: 0.0066564]
	Learning Rate: 0.00665636
	LOSS [training: 0.3658350610875175 | validation: 0.2720503909240106]
	TIME [epoch: 8.2 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4038119843867117		[learning rate: 0.0066443]
		[batch 20/20] avg loss: 0.8528435531910912		[learning rate: 0.0066322]
	Learning Rate: 0.0066322
	LOSS [training: 0.6283277687889014 | validation: 0.4368640159231359]
	TIME [epoch: 8.17 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3992319987682727		[learning rate: 0.0066202]
		[batch 20/20] avg loss: 0.4508305867385884		[learning rate: 0.0066081]
	Learning Rate: 0.00660814
	LOSS [training: 0.42503129275343043 | validation: 0.23314528020994207]
	TIME [epoch: 8.16 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27684975468885153		[learning rate: 0.0065961]
		[batch 20/20] avg loss: 0.46360818280174787		[learning rate: 0.0065842]
	Learning Rate: 0.00658415
	LOSS [training: 0.37022896874529965 | validation: 0.6588756876286673]
	TIME [epoch: 8.16 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37988363215107335		[learning rate: 0.0065722]
		[batch 20/20] avg loss: 0.40207308079568216		[learning rate: 0.0065603]
	Learning Rate: 0.00656026
	LOSS [training: 0.39097835647337786 | validation: 0.5482038901895463]
	TIME [epoch: 8.18 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3935450074225336		[learning rate: 0.0065483]
		[batch 20/20] avg loss: 0.34414156721407185		[learning rate: 0.0065365]
	Learning Rate: 0.00653645
	LOSS [training: 0.36884328731830274 | validation: 0.33578869160918123]
	TIME [epoch: 8.18 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31718305122323276		[learning rate: 0.0065246]
		[batch 20/20] avg loss: 0.2905856129483052		[learning rate: 0.0065127]
	Learning Rate: 0.00651273
	LOSS [training: 0.3038843320857689 | validation: 0.3216415752198724]
	TIME [epoch: 8.17 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36633194705973426		[learning rate: 0.0065009]
		[batch 20/20] avg loss: 0.3681060050989568		[learning rate: 0.0064891]
	Learning Rate: 0.0064891
	LOSS [training: 0.3672189760793456 | validation: 0.17189031392992002]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32177838035254147		[learning rate: 0.0064773]
		[batch 20/20] avg loss: 0.33479833795164493		[learning rate: 0.0064655]
	Learning Rate: 0.00646555
	LOSS [training: 0.3282883591520931 | validation: 0.5234460241137038]
	TIME [epoch: 8.16 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32233733808457177		[learning rate: 0.0064538]
		[batch 20/20] avg loss: 0.3260142129164493		[learning rate: 0.0064421]
	Learning Rate: 0.00644208
	LOSS [training: 0.3241757755005106 | validation: 0.3595508690313184]
	TIME [epoch: 8.19 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36478231025078517		[learning rate: 0.0064304]
		[batch 20/20] avg loss: 0.26362021261770197		[learning rate: 0.0064187]
	Learning Rate: 0.0064187
	LOSS [training: 0.31420126143424354 | validation: 0.27759182918138087]
	TIME [epoch: 8.16 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30068510599264997		[learning rate: 0.006407]
		[batch 20/20] avg loss: 0.31861591095941894		[learning rate: 0.0063954]
	Learning Rate: 0.00639541
	LOSS [training: 0.3096505084760344 | validation: 0.30947467586758315]
	TIME [epoch: 8.16 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2823985317727318		[learning rate: 0.0063838]
		[batch 20/20] avg loss: 0.27972887412211883		[learning rate: 0.0063722]
	Learning Rate: 0.0063722
	LOSS [training: 0.28106370294742533 | validation: 0.3484952890198394]
	TIME [epoch: 8.16 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3216049840819031		[learning rate: 0.0063606]
		[batch 20/20] avg loss: 0.28506403031482613		[learning rate: 0.0063491]
	Learning Rate: 0.00634908
	LOSS [training: 0.3033345071983647 | validation: 0.5958040996772898]
	TIME [epoch: 8.19 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3990849359470975		[learning rate: 0.0063375]
		[batch 20/20] avg loss: 0.3576749215075901		[learning rate: 0.006326]
	Learning Rate: 0.00632603
	LOSS [training: 0.37837992872734383 | validation: 0.2857859718805421]
	TIME [epoch: 8.16 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2864838479088483		[learning rate: 0.0063145]
		[batch 20/20] avg loss: 0.3552109777607715		[learning rate: 0.0063031]
	Learning Rate: 0.00630308
	LOSS [training: 0.3208474128348099 | validation: 0.2882832432563546]
	TIME [epoch: 8.17 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34736038247988366		[learning rate: 0.0062916]
		[batch 20/20] avg loss: 0.2532712169887027		[learning rate: 0.0062802]
	Learning Rate: 0.0062802
	LOSS [training: 0.3003157997342931 | validation: 0.8287885399769351]
	TIME [epoch: 8.17 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3836097985732819		[learning rate: 0.0062688]
		[batch 20/20] avg loss: 0.3373574596969396		[learning rate: 0.0062574]
	Learning Rate: 0.00625741
	LOSS [training: 0.36048362913511073 | validation: 0.151160284694809]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5004236548378214		[learning rate: 0.006246]
		[batch 20/20] avg loss: 0.26684631096369565		[learning rate: 0.0062347]
	Learning Rate: 0.0062347
	LOSS [training: 0.3836349829007585 | validation: 0.1571198877228124]
	TIME [epoch: 8.19 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3545536136797052		[learning rate: 0.0062234]
		[batch 20/20] avg loss: 0.40153026952409754		[learning rate: 0.0062121]
	Learning Rate: 0.00621208
	LOSS [training: 0.3780419416019014 | validation: 0.37142907796298913]
	TIME [epoch: 8.16 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44168161581074095		[learning rate: 0.0062008]
		[batch 20/20] avg loss: 0.3271051073612668		[learning rate: 0.0061895]
	Learning Rate: 0.00618953
	LOSS [training: 0.38439336158600385 | validation: 0.15578650107115996]
	TIME [epoch: 8.16 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2753900568044646		[learning rate: 0.0061783]
		[batch 20/20] avg loss: 0.29320057905761054		[learning rate: 0.0061671]
	Learning Rate: 0.00616707
	LOSS [training: 0.28429531793103757 | validation: 0.20207203544191443]
	TIME [epoch: 8.16 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28383705495887074		[learning rate: 0.0061559]
		[batch 20/20] avg loss: 0.30011198890327506		[learning rate: 0.0061447]
	Learning Rate: 0.00614469
	LOSS [training: 0.2919745219310729 | validation: 0.20075798147417911]
	TIME [epoch: 8.19 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3528509313900313		[learning rate: 0.0061335]
		[batch 20/20] avg loss: 0.4069860035476663		[learning rate: 0.0061224]
	Learning Rate: 0.00612239
	LOSS [training: 0.37991846746884883 | validation: 0.22765983232004366]
	TIME [epoch: 8.16 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.236511528846877		[learning rate: 0.0061113]
		[batch 20/20] avg loss: 0.265679230428254		[learning rate: 0.0061002]
	Learning Rate: 0.00610017
	LOSS [training: 0.2510953796375655 | validation: 0.3253295200415837]
	TIME [epoch: 8.15 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4168931587320007		[learning rate: 0.0060891]
		[batch 20/20] avg loss: 0.5274158631786025		[learning rate: 0.006078]
	Learning Rate: 0.00607803
	LOSS [training: 0.47215451095530164 | validation: 0.34202858939069164]
	TIME [epoch: 8.16 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30149900222523884		[learning rate: 0.006067]
		[batch 20/20] avg loss: 0.47952788309589367		[learning rate: 0.006056]
	Learning Rate: 0.00605598
	LOSS [training: 0.39051344266056626 | validation: 0.24289823196620558]
	TIME [epoch: 8.17 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4251863633096846		[learning rate: 0.006045]
		[batch 20/20] avg loss: 0.28724598419000524		[learning rate: 0.006034]
	Learning Rate: 0.006034
	LOSS [training: 0.35621617374984504 | validation: 0.21016330151821333]
	TIME [epoch: 8.17 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35917259724108386		[learning rate: 0.006023]
		[batch 20/20] avg loss: 0.4246466541018733		[learning rate: 0.0060121]
	Learning Rate: 0.0060121
	LOSS [training: 0.3919096256714786 | validation: 0.34657516465689275]
	TIME [epoch: 8.16 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2605516142126314		[learning rate: 0.0060012]
		[batch 20/20] avg loss: 0.3398779672277652		[learning rate: 0.0059903]
	Learning Rate: 0.00599028
	LOSS [training: 0.30021479072019835 | validation: 0.22235960502873944]
	TIME [epoch: 8.16 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2701317728598739		[learning rate: 0.0059794]
		[batch 20/20] avg loss: 0.33372667307926857		[learning rate: 0.0059685]
	Learning Rate: 0.00596854
	LOSS [training: 0.30192922296957125 | validation: 0.2618048229596822]
	TIME [epoch: 8.17 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3445182616601443		[learning rate: 0.0059577]
		[batch 20/20] avg loss: 0.2890371404281985		[learning rate: 0.0059469]
	Learning Rate: 0.00594688
	LOSS [training: 0.3167777010441714 | validation: 0.23413057586096556]
	TIME [epoch: 8.19 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3143413870475551		[learning rate: 0.0059361]
		[batch 20/20] avg loss: 0.31864546560684215		[learning rate: 0.0059253]
	Learning Rate: 0.0059253
	LOSS [training: 0.3164934263271987 | validation: 0.12121831648750295]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4988253319954758		[learning rate: 0.0059145]
		[batch 20/20] avg loss: 0.3116843517689872		[learning rate: 0.0059038]
	Learning Rate: 0.0059038
	LOSS [training: 0.40525484188223143 | validation: 0.16734780573426386]
	TIME [epoch: 8.16 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28590445873944814		[learning rate: 0.0058931]
		[batch 20/20] avg loss: 0.3337927772489049		[learning rate: 0.0058824]
	Learning Rate: 0.00588237
	LOSS [training: 0.30984861799417646 | validation: 0.740602524232753]
	TIME [epoch: 8.17 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39685810959950396		[learning rate: 0.0058717]
		[batch 20/20] avg loss: 0.2969225419739196		[learning rate: 0.005861]
	Learning Rate: 0.00586103
	LOSS [training: 0.3468903257867118 | validation: 0.2816879838588858]
	TIME [epoch: 8.18 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3010351724021153		[learning rate: 0.0058504]
		[batch 20/20] avg loss: 0.31847224885414505		[learning rate: 0.0058398]
	Learning Rate: 0.00583976
	LOSS [training: 0.3097537106281302 | validation: 0.26291761379988876]
	TIME [epoch: 8.18 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2947746793185807		[learning rate: 0.0058291]
		[batch 20/20] avg loss: 0.2307218607695988		[learning rate: 0.0058186]
	Learning Rate: 0.00581856
	LOSS [training: 0.2627482700440897 | validation: 0.1559060597612567]
	TIME [epoch: 8.16 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3357522907696215		[learning rate: 0.005808]
		[batch 20/20] avg loss: 0.24817598798658097		[learning rate: 0.0057974]
	Learning Rate: 0.00579745
	LOSS [training: 0.29196413937810123 | validation: 0.1766711344429805]
	TIME [epoch: 8.16 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22917974503794808		[learning rate: 0.0057869]
		[batch 20/20] avg loss: 0.34268840245661286		[learning rate: 0.0057764]
	Learning Rate: 0.00577641
	LOSS [training: 0.2859340737472804 | validation: 0.25402574214014184]
	TIME [epoch: 8.16 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.279263212276044		[learning rate: 0.0057659]
		[batch 20/20] avg loss: 0.30715100586929656		[learning rate: 0.0057554]
	Learning Rate: 0.00575545
	LOSS [training: 0.29320710907267034 | validation: 0.20911805377262482]
	TIME [epoch: 8.18 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3330644130579946		[learning rate: 0.005745]
		[batch 20/20] avg loss: 0.2700725170803151		[learning rate: 0.0057346]
	Learning Rate: 0.00573456
	LOSS [training: 0.3015684650691548 | validation: 0.560059164837061]
	TIME [epoch: 8.16 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31942890660855505		[learning rate: 0.0057241]
		[batch 20/20] avg loss: 0.30333474068525373		[learning rate: 0.0057137]
	Learning Rate: 0.00571375
	LOSS [training: 0.3113818236469043 | validation: 0.13718902545016634]
	TIME [epoch: 8.15 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27045451733916737		[learning rate: 0.0057034]
		[batch 20/20] avg loss: 0.27227676353595964		[learning rate: 0.005693]
	Learning Rate: 0.00569301
	LOSS [training: 0.2713656404375635 | validation: 0.36648662021447803]
	TIME [epoch: 8.16 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45637017369935445		[learning rate: 0.0056827]
		[batch 20/20] avg loss: 0.30778490885989623		[learning rate: 0.0056724]
	Learning Rate: 0.00567235
	LOSS [training: 0.38207754127962534 | validation: 0.362667502096701]
	TIME [epoch: 8.18 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28702564260840135		[learning rate: 0.005662]
		[batch 20/20] avg loss: 0.27608638528699825		[learning rate: 0.0056518]
	Learning Rate: 0.00565177
	LOSS [training: 0.28155601394769975 | validation: 0.218266042698081]
	TIME [epoch: 8.16 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2617980476451153		[learning rate: 0.0056415]
		[batch 20/20] avg loss: 0.31850228502691186		[learning rate: 0.0056313]
	Learning Rate: 0.00563126
	LOSS [training: 0.29015016633601365 | validation: 0.15631564460411224]
	TIME [epoch: 8.16 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3258740521242501		[learning rate: 0.005621]
		[batch 20/20] avg loss: 0.35391728031546943		[learning rate: 0.0056108]
	Learning Rate: 0.00561082
	LOSS [training: 0.3398956662198598 | validation: 0.4212346406141083]
	TIME [epoch: 8.16 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3204453879575639		[learning rate: 0.0056006]
		[batch 20/20] avg loss: 0.27804673550634135		[learning rate: 0.0055905]
	Learning Rate: 0.00559046
	LOSS [training: 0.2992460617319526 | validation: 0.368495561740888]
	TIME [epoch: 8.15 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3633419115658689		[learning rate: 0.0055803]
		[batch 20/20] avg loss: 0.4482690643227829		[learning rate: 0.0055702]
	Learning Rate: 0.00557017
	LOSS [training: 0.4058054879443259 | validation: 0.12972888106183295]
	TIME [epoch: 8.19 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24215847393467854		[learning rate: 0.0055601]
		[batch 20/20] avg loss: 0.24777224245777835		[learning rate: 0.00555]
	Learning Rate: 0.00554996
	LOSS [training: 0.24496535819622842 | validation: 0.384451313804646]
	TIME [epoch: 8.16 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3015253897909297		[learning rate: 0.0055399]
		[batch 20/20] avg loss: 0.21338638961361983		[learning rate: 0.0055298]
	Learning Rate: 0.00552981
	LOSS [training: 0.2574558897022748 | validation: 0.16437144372633017]
	TIME [epoch: 8.16 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29291799488241105		[learning rate: 0.0055198]
		[batch 20/20] avg loss: 0.26519666914423373		[learning rate: 0.0055097]
	Learning Rate: 0.00550975
	LOSS [training: 0.2790573320133224 | validation: 0.31240250808440884]
	TIME [epoch: 8.16 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23823028007211783		[learning rate: 0.0054997]
		[batch 20/20] avg loss: 0.31070844412990917		[learning rate: 0.0054898]
	Learning Rate: 0.00548975
	LOSS [training: 0.2744693621010135 | validation: 0.16488772602380858]
	TIME [epoch: 8.2 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35361652539916544		[learning rate: 0.0054798]
		[batch 20/20] avg loss: 0.30378257835912714		[learning rate: 0.0054698]
	Learning Rate: 0.00546983
	LOSS [training: 0.32869955187914635 | validation: 0.26579399385276326]
	TIME [epoch: 8.17 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28505413606327856		[learning rate: 0.0054599]
		[batch 20/20] avg loss: 0.2861026611768972		[learning rate: 0.00545]
	Learning Rate: 0.00544998
	LOSS [training: 0.2855783986200878 | validation: 0.2097328114930334]
	TIME [epoch: 8.15 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2916181231656735		[learning rate: 0.0054401]
		[batch 20/20] avg loss: 0.2928566485700107		[learning rate: 0.0054302]
	Learning Rate: 0.0054302
	LOSS [training: 0.2922373858678421 | validation: 0.47354804468364986]
	TIME [epoch: 8.16 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24050383144451298		[learning rate: 0.0054203]
		[batch 20/20] avg loss: 0.24060673978515984		[learning rate: 0.0054105]
	Learning Rate: 0.00541049
	LOSS [training: 0.24055528561483644 | validation: 0.3347776370826844]
	TIME [epoch: 8.16 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2836453834029266		[learning rate: 0.0054007]
		[batch 20/20] avg loss: 0.29253500173590063		[learning rate: 0.0053909]
	Learning Rate: 0.00539086
	LOSS [training: 0.28809019256941365 | validation: 0.15851113180556564]
	TIME [epoch: 8.19 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35526416488225043		[learning rate: 0.0053811]
		[batch 20/20] avg loss: 0.26082390210621187		[learning rate: 0.0053713]
	Learning Rate: 0.00537129
	LOSS [training: 0.30804403349423104 | validation: 0.22088343651935716]
	TIME [epoch: 8.16 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25934450447695334		[learning rate: 0.0053615]
		[batch 20/20] avg loss: 0.3100695444538769		[learning rate: 0.0053518]
	Learning Rate: 0.0053518
	LOSS [training: 0.2847070244654151 | validation: 0.10131936381291239]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24084331463784436		[learning rate: 0.0053421]
		[batch 20/20] avg loss: 0.3608998349853981		[learning rate: 0.0053324]
	Learning Rate: 0.00533238
	LOSS [training: 0.30087157481162125 | validation: 0.32395089580583325]
	TIME [epoch: 8.16 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20932942792196524		[learning rate: 0.0053227]
		[batch 20/20] avg loss: 0.2650638052020847		[learning rate: 0.005313]
	Learning Rate: 0.00531303
	LOSS [training: 0.23719661656202495 | validation: 0.45645152904415853]
	TIME [epoch: 8.18 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.361733267076506		[learning rate: 0.0053034]
		[batch 20/20] avg loss: 0.3626667567304026		[learning rate: 0.0052937]
	Learning Rate: 0.00529375
	LOSS [training: 0.36220001190345436 | validation: 0.2845058605096896]
	TIME [epoch: 8.15 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2697067750981298		[learning rate: 0.0052841]
		[batch 20/20] avg loss: 0.26952120914728656		[learning rate: 0.0052745]
	Learning Rate: 0.00527454
	LOSS [training: 0.26961399212270815 | validation: 0.1474288446388208]
	TIME [epoch: 8.16 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40550687867292384		[learning rate: 0.005265]
		[batch 20/20] avg loss: 0.3018993370779765		[learning rate: 0.0052554]
	Learning Rate: 0.00525539
	LOSS [training: 0.3537031078754502 | validation: 0.10817890865517642]
	TIME [epoch: 8.17 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28036632406897294		[learning rate: 0.0052458]
		[batch 20/20] avg loss: 0.29725325445229167		[learning rate: 0.0052363]
	Learning Rate: 0.00523632
	LOSS [training: 0.2888097892606322 | validation: 0.2963961051331691]
	TIME [epoch: 8.18 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25029091243608703		[learning rate: 0.0052268]
		[batch 20/20] avg loss: 0.24856708812079858		[learning rate: 0.0052173]
	Learning Rate: 0.00521732
	LOSS [training: 0.24942900027844278 | validation: 0.2729331624303863]
	TIME [epoch: 8.17 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28120120946449234		[learning rate: 0.0052078]
		[batch 20/20] avg loss: 0.24439584464547895		[learning rate: 0.0051984]
	Learning Rate: 0.00519839
	LOSS [training: 0.26279852705498563 | validation: 0.44647106892852395]
	TIME [epoch: 8.16 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2720471585560623		[learning rate: 0.0051889]
		[batch 20/20] avg loss: 0.32840500914431586		[learning rate: 0.0051795]
	Learning Rate: 0.00517952
	LOSS [training: 0.30022608385018906 | validation: 0.19720401676654886]
	TIME [epoch: 8.15 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2728803661597341		[learning rate: 0.0051701]
		[batch 20/20] avg loss: 0.28963946862446477		[learning rate: 0.0051607]
	Learning Rate: 0.00516072
	LOSS [training: 0.28125991739209943 | validation: 0.4748911236454051]
	TIME [epoch: 8.15 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3471411187702125		[learning rate: 0.0051513]
		[batch 20/20] avg loss: 0.4420952284851226		[learning rate: 0.005142]
	Learning Rate: 0.00514199
	LOSS [training: 0.39461817362766755 | validation: 0.6389418608875834]
	TIME [epoch: 8.18 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3137165566609828		[learning rate: 0.0051327]
		[batch 20/20] avg loss: 0.30451151059244436		[learning rate: 0.0051233]
	Learning Rate: 0.00512333
	LOSS [training: 0.30911403362671364 | validation: 0.40992697793428834]
	TIME [epoch: 8.14 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4477527247957994		[learning rate: 0.005114]
		[batch 20/20] avg loss: 0.3185679155213004		[learning rate: 0.0051047]
	Learning Rate: 0.00510474
	LOSS [training: 0.3831603201585499 | validation: 0.19087770799243936]
	TIME [epoch: 8.16 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29919094091986004		[learning rate: 0.0050955]
		[batch 20/20] avg loss: 0.35213934114468887		[learning rate: 0.0050862]
	Learning Rate: 0.00508622
	LOSS [training: 0.3256651410322745 | validation: 0.1790593047411072]
	TIME [epoch: 8.16 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2922355672263247		[learning rate: 0.005077]
		[batch 20/20] avg loss: 0.19225470698603125		[learning rate: 0.0050678]
	Learning Rate: 0.00506776
	LOSS [training: 0.24224513710617793 | validation: 0.30251731174435836]
	TIME [epoch: 8.18 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30506855809599775		[learning rate: 0.0050586]
		[batch 20/20] avg loss: 0.2635927511950072		[learning rate: 0.0050494]
	Learning Rate: 0.00504937
	LOSS [training: 0.2843306546455025 | validation: 0.4606955217019846]
	TIME [epoch: 8.17 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2624043542461395		[learning rate: 0.0050402]
		[batch 20/20] avg loss: 0.2824243653827277		[learning rate: 0.005031]
	Learning Rate: 0.00503104
	LOSS [training: 0.2724143598144336 | validation: 0.15871588441182327]
	TIME [epoch: 8.16 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30393966723770316		[learning rate: 0.0050219]
		[batch 20/20] avg loss: 0.24200399094597452		[learning rate: 0.0050128]
	Learning Rate: 0.00501278
	LOSS [training: 0.27297182909183887 | validation: 0.10454684861129078]
	TIME [epoch: 8.17 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2938521620269558		[learning rate: 0.0050037]
		[batch 20/20] avg loss: 0.31879591607218094		[learning rate: 0.0049946]
	Learning Rate: 0.00499459
	LOSS [training: 0.3063240390495684 | validation: 0.17222822093236603]
	TIME [epoch: 8.17 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2578478333387508		[learning rate: 0.0049855]
		[batch 20/20] avg loss: 0.19765273409822873		[learning rate: 0.0049765]
	Learning Rate: 0.00497647
	LOSS [training: 0.2277502837184898 | validation: 0.2838000350784067]
	TIME [epoch: 8.18 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44166241308014625		[learning rate: 0.0049674]
		[batch 20/20] avg loss: 0.38550183992542414		[learning rate: 0.0049584]
	Learning Rate: 0.00495841
	LOSS [training: 0.4135821265027853 | validation: 0.14822540838164566]
	TIME [epoch: 8.15 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31488000763018975		[learning rate: 0.0049494]
		[batch 20/20] avg loss: 0.3637275908968184		[learning rate: 0.0049404]
	Learning Rate: 0.00494041
	LOSS [training: 0.33930379926350407 | validation: 0.6191713796928802]
	TIME [epoch: 8.15 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2449113470198972		[learning rate: 0.0049314]
		[batch 20/20] avg loss: 0.3418712438940047		[learning rate: 0.0049225]
	Learning Rate: 0.00492248
	LOSS [training: 0.293391295456951 | validation: 0.3907766896047764]
	TIME [epoch: 8.15 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2781151274097461		[learning rate: 0.0049135]
		[batch 20/20] avg loss: 0.3511712969234182		[learning rate: 0.0049046]
	Learning Rate: 0.00490462
	LOSS [training: 0.31464321216658214 | validation: 0.16669218334648372]
	TIME [epoch: 8.17 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32132077905215134		[learning rate: 0.0048957]
		[batch 20/20] avg loss: 0.2642947877276317		[learning rate: 0.0048868]
	Learning Rate: 0.00488682
	LOSS [training: 0.29280778338989155 | validation: 0.17849080223224167]
	TIME [epoch: 8.15 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3920915708894487		[learning rate: 0.0048779]
		[batch 20/20] avg loss: 0.3059095762855593		[learning rate: 0.0048691]
	Learning Rate: 0.00486909
	LOSS [training: 0.349000573587504 | validation: 0.2486099865181396]
	TIME [epoch: 8.15 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22321061929121044		[learning rate: 0.0048602]
		[batch 20/20] avg loss: 0.276595954947798		[learning rate: 0.0048514]
	Learning Rate: 0.00485141
	LOSS [training: 0.2499032871195042 | validation: 0.3660411143785344]
	TIME [epoch: 8.16 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3346014407200686		[learning rate: 0.0048426]
		[batch 20/20] avg loss: 0.28813361488716793		[learning rate: 0.0048338]
	Learning Rate: 0.00483381
	LOSS [training: 0.3113675278036183 | validation: 0.25846250322253167]
	TIME [epoch: 8.16 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2680029335747369		[learning rate: 0.004825]
		[batch 20/20] avg loss: 0.2975606465107195		[learning rate: 0.0048163]
	Learning Rate: 0.00481627
	LOSS [training: 0.2827817900427282 | validation: 0.12789039267859792]
	TIME [epoch: 8.18 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.281770761211008		[learning rate: 0.0048075]
		[batch 20/20] avg loss: 0.26121387546528163		[learning rate: 0.0047988]
	Learning Rate: 0.00479879
	LOSS [training: 0.27149231833814486 | validation: 0.21122313777860977]
	TIME [epoch: 8.14 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2823938766700254		[learning rate: 0.0047901]
		[batch 20/20] avg loss: 0.33092079876992586		[learning rate: 0.0047814]
	Learning Rate: 0.00478137
	LOSS [training: 0.3066573377199756 | validation: 0.08280895138345652]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r1_20240219_233648/states/model_tr_study202_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3041308947436522		[learning rate: 0.0047727]
		[batch 20/20] avg loss: 0.458281575658284		[learning rate: 0.004764]
	Learning Rate: 0.00476402
	LOSS [training: 0.38120623520096814 | validation: 0.45253445589158353]
	TIME [epoch: 8.17 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34982267878338724		[learning rate: 0.0047554]
		[batch 20/20] avg loss: 0.3710718594522502		[learning rate: 0.0047467]
	Learning Rate: 0.00474673
	LOSS [training: 0.36044726911781877 | validation: 0.23800646060035313]
	TIME [epoch: 8.19 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26114884678958494		[learning rate: 0.0047381]
		[batch 20/20] avg loss: 0.2810078411791079		[learning rate: 0.0047295]
	Learning Rate: 0.00472951
	LOSS [training: 0.27107834398434644 | validation: 0.11974991287576807]
	TIME [epoch: 8.17 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4553075315568025		[learning rate: 0.0047209]
		[batch 20/20] avg loss: 0.2727323670139479		[learning rate: 0.0047123]
	Learning Rate: 0.00471234
	LOSS [training: 0.3640199492853752 | validation: 0.1817098773987782]
	TIME [epoch: 8.16 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22444503513719855		[learning rate: 0.0047038]
		[batch 20/20] avg loss: 0.2713918438269508		[learning rate: 0.0046952]
	Learning Rate: 0.00469524
	LOSS [training: 0.24791843948207473 | validation: 0.13948835865609255]
	TIME [epoch: 8.17 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3451492764698013		[learning rate: 0.0046867]
		[batch 20/20] avg loss: 0.3022486571867213		[learning rate: 0.0046782]
	Learning Rate: 0.0046782
	LOSS [training: 0.3236989668282614 | validation: 0.6449117931340151]
	TIME [epoch: 8.18 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3036323091577767		[learning rate: 0.0046697]
		[batch 20/20] avg loss: 0.28244452654106705		[learning rate: 0.0046612]
	Learning Rate: 0.00466122
	LOSS [training: 0.2930384178494219 | validation: 0.2067275804162182]
	TIME [epoch: 8.18 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3577884064159152		[learning rate: 0.0046528]
		[batch 20/20] avg loss: 0.3745213938718596		[learning rate: 0.0046443]
	Learning Rate: 0.00464431
	LOSS [training: 0.3661549001438875 | validation: 0.22627925768082585]
	TIME [epoch: 8.16 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42818515764658605		[learning rate: 0.0046359]
		[batch 20/20] avg loss: 0.5084678781814997		[learning rate: 0.0046275]
	Learning Rate: 0.00462745
	LOSS [training: 0.46832651791404273 | validation: 0.39571207215041315]
	TIME [epoch: 8.17 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34996672098862946		[learning rate: 0.004619]
		[batch 20/20] avg loss: 0.2744873549053076		[learning rate: 0.0046107]
	Learning Rate: 0.00461066
	LOSS [training: 0.3122270379469686 | validation: 0.22284913991228433]
	TIME [epoch: 8.17 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5008405779037952		[learning rate: 0.0046023]
		[batch 20/20] avg loss: 0.3042692183242298		[learning rate: 0.0045939]
	Learning Rate: 0.00459393
	LOSS [training: 0.4025548981140125 | validation: 0.1776027037800343]
	TIME [epoch: 8.2 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2860901840472204		[learning rate: 0.0045856]
		[batch 20/20] avg loss: 0.4708144895139895		[learning rate: 0.0045773]
	Learning Rate: 0.00457726
	LOSS [training: 0.378452336780605 | validation: 0.4538817604109239]
	TIME [epoch: 8.16 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37068039162062993		[learning rate: 0.0045689]
		[batch 20/20] avg loss: 0.2811421694898064		[learning rate: 0.0045606]
	Learning Rate: 0.00456065
	LOSS [training: 0.3259112805552181 | validation: 0.22034945414289053]
	TIME [epoch: 8.17 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3484683240316273		[learning rate: 0.0045524]
		[batch 20/20] avg loss: 0.3980595360711498		[learning rate: 0.0045441]
	Learning Rate: 0.00454409
	LOSS [training: 0.3732639300513885 | validation: 0.3358072084100734]
	TIME [epoch: 8.16 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3536036743675496		[learning rate: 0.0045358]
		[batch 20/20] avg loss: 0.27222958995304347		[learning rate: 0.0045276]
	Learning Rate: 0.0045276
	LOSS [training: 0.31291663216029664 | validation: 0.33575567180664995]
	TIME [epoch: 8.18 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3147864732162388		[learning rate: 0.0045194]
		[batch 20/20] avg loss: 0.41770910106422726		[learning rate: 0.0045112]
	Learning Rate: 0.00451117
	LOSS [training: 0.3662477871402331 | validation: 0.7131758122683218]
	TIME [epoch: 8.17 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4299266494711049		[learning rate: 0.004503]
		[batch 20/20] avg loss: 0.2534060754357147		[learning rate: 0.0044948]
	Learning Rate: 0.0044948
	LOSS [training: 0.34166636245340976 | validation: 0.14692595903930727]
	TIME [epoch: 8.17 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3267264021337045		[learning rate: 0.0044866]
		[batch 20/20] avg loss: 0.3580239280608348		[learning rate: 0.0044785]
	Learning Rate: 0.00447849
	LOSS [training: 0.3423751650972696 | validation: 0.3192073750830318]
	TIME [epoch: 8.16 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28372865505234807		[learning rate: 0.0044704]
		[batch 20/20] avg loss: 0.26434126623075693		[learning rate: 0.0044622]
	Learning Rate: 0.00446224
	LOSS [training: 0.27403496064155247 | validation: 0.22436202864282676]
	TIME [epoch: 8.16 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32956764943633543		[learning rate: 0.0044541]
		[batch 20/20] avg loss: 0.3770783058600159		[learning rate: 0.004446]
	Learning Rate: 0.00444604
	LOSS [training: 0.35332297764817566 | validation: 0.5423996091790374]
	TIME [epoch: 8.19 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.612823993535793		[learning rate: 0.004438]
		[batch 20/20] avg loss: 0.4835756071620875		[learning rate: 0.0044299]
	Learning Rate: 0.00442991
	LOSS [training: 0.5481998003489402 | validation: 0.3591803250823895]
	TIME [epoch: 8.16 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29593955505891356		[learning rate: 0.0044219]
		[batch 20/20] avg loss: 0.423622909448757		[learning rate: 0.0044138]
	Learning Rate: 0.00441383
	LOSS [training: 0.35978123225383535 | validation: 0.14681664546476383]
	TIME [epoch: 8.16 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4308320160115393		[learning rate: 0.0044058]
		[batch 20/20] avg loss: 0.29436614861885513		[learning rate: 0.0043978]
	Learning Rate: 0.00439781
	LOSS [training: 0.36259908231519716 | validation: 0.35936880489874634]
	TIME [epoch: 8.16 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32742788880465123		[learning rate: 0.0043898]
		[batch 20/20] avg loss: 0.45446423425862986		[learning rate: 0.0043819]
	Learning Rate: 0.00438185
	LOSS [training: 0.39094606153164047 | validation: 0.3441791891496091]
	TIME [epoch: 8.2 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3443511561011293		[learning rate: 0.0043739]
		[batch 20/20] avg loss: 0.38511780428304687		[learning rate: 0.004366]
	Learning Rate: 0.00436595
	LOSS [training: 0.3647344801920881 | validation: 0.6443142789701612]
	TIME [epoch: 8.16 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.391176750178956		[learning rate: 0.004358]
		[batch 20/20] avg loss: 0.4999510671380113		[learning rate: 0.0043501]
	Learning Rate: 0.00435011
	LOSS [training: 0.4455639086584836 | validation: 0.45612089095000585]
	TIME [epoch: 8.16 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6464902429461237		[learning rate: 0.0043422]
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.05
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.025
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.0125
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.00625
ERROR:
Encountered nan in loss and reached the maximum number of model alterations: 4.
