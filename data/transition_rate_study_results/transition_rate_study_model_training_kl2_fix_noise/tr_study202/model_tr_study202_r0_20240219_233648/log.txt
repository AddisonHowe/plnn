Args:
Namespace(name='model_tr_study202', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0', training_data='data/transition_rate_studies/tr_study202/tr_study202_training/r0', validation_data='data/transition_rate_studies/tr_study202/tr_study202_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2636867462

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.46795229792288		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.098794009201468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.283373153562176 | validation: 7.247394091540038]
	TIME [epoch: 78.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.1214941745434		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.179843914272064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.650669044407732 | validation: 5.78708007346059]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.158964873221574		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.541500295855247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.850232584538411 | validation: 5.296971230543843]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.321765925793376		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.335167407919212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.328466666856293 | validation: 3.5183999457231403]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.801318652449538		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.580885977712788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6911023150811624 | validation: 3.8732358529200273]
	TIME [epoch: 8.34 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5447392074404815		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.2444281820487575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3945836947446195 | validation: 2.890872583962514]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.182322290827719		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0151827500786803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0987525204532 | validation: 2.6595019345704953]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.996029186131868		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9867915679720523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9914103770519604 | validation: 2.3059435666440304]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.991546594712717		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0688624986661397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0302045466894283 | validation: 3.0262245988148155]
	TIME [epoch: 8.34 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1121022603035082		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7883120265902104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.950207143446859 | validation: 2.4364412971342833]
	TIME [epoch: 8.33 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8418936714197125		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.048117782096557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.945005726758134 | validation: 2.317300820658383]
	TIME [epoch: 8.32 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7689163846603764		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8102673860845124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7895918853724444 | validation: 2.2765173183424476]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.770522367630342		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7512763814621906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.760899374546266 | validation: 2.2054917018280853]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.021687738227665		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.716468163202947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8690779507153055 | validation: 2.661024925397109]
	TIME [epoch: 8.34 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.012987234834454		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.969220583959039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.991103909396746 | validation: 2.100347044211762]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7407112446630637		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.704649263101464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.722680253882264 | validation: 2.334777088942733]
	TIME [epoch: 8.31 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.766114963700101		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5981535378561724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.682134250778137 | validation: 2.017169916185531]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4540996573348117		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.540911404511715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.497505530923264 | validation: 1.871313702429803]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.515850656054098		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5012632367712184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.508556946412658 | validation: 2.70109637831882]
	TIME [epoch: 8.31 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3909613998782397		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.476448966915485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.433705183396862 | validation: 3.7268717277362184]
	TIME [epoch: 8.3 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5044488569283616		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2275498655310555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3659993612297083 | validation: 2.5654379862508088]
	TIME [epoch: 8.3 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1232884880754344		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5316684882198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.327478488147617 | validation: 1.919712843392832]
	TIME [epoch: 8.33 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.275688617033691		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.225530041945876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.250609329489783 | validation: 1.9004336375378323]
	TIME [epoch: 8.3 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.272870410787031		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.125834025157845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.199352217972438 | validation: 2.444842803206861]
	TIME [epoch: 8.3 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.265136808305274		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0210385032107174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.143087655757996 | validation: 1.997688255220666]
	TIME [epoch: 8.3 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0919212647254435		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9712468235299851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0315840441277144 | validation: 2.0517996300991355]
	TIME [epoch: 8.32 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0276582257418316		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2048487684316864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.116253497086759 | validation: 1.9632363971019922]
	TIME [epoch: 8.32 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.901489888934469		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0046814780737283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9530856835040986 | validation: 1.7841184524250746]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.891222907093237		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9318529056724167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9115379063828268 | validation: 1.6364901225919586]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7955279441297538		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9359475922005778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.865737768165166 | validation: 1.596634252157644]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6994130770359377		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5476299423619073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6235215096989222 | validation: 1.0381012029869994]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0677209409133204		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9499371550090012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0088290479611608 | validation: 0.9652373372132603]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0456241898173049		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9354348149360685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9905295023766868 | validation: 1.1792571227897555]
	TIME [epoch: 8.29 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.012619510828664		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0087869644599983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.010703237644331 | validation: 0.8431470361805293]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8945540840270748		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9063502132180007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9004521486225376 | validation: 0.5333285644004528]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7795090178426152		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0753140580988332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9274115379707242 | validation: 0.5828189415786893]
	TIME [epoch: 8.3 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9841516115612559		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9361681565766201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.960159884068938 | validation: 0.8459544599585435]
	TIME [epoch: 8.29 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8821945938678146		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8019601799142084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8420773868910117 | validation: 0.7153050924278495]
	TIME [epoch: 8.29 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7897560219159719		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0184337003352635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9040948611256179 | validation: 0.4256315986696439]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.805852214958454		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9920412478960772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8989467314272657 | validation: 0.9539327866681245]
	TIME [epoch: 8.33 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9181119941820771		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2982253769041108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1081686855430941 | validation: 0.8880579926091285]
	TIME [epoch: 8.33 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9030512891180906		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7682969259142769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8356741075161838 | validation: 0.7590660671565729]
	TIME [epoch: 8.32 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8396363220557692		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7987860923828127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8192112072192911 | validation: 0.7718038869718359]
	TIME [epoch: 8.34 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8079125768330044		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8767453984967206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8423289876648624 | validation: 0.9880599199848692]
	TIME [epoch: 8.33 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7862560381795861		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6696766263209711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7279663322502786 | validation: 0.7848412410754673]
	TIME [epoch: 8.32 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7860972925891113		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.692989782851505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7395435377203081 | validation: 0.9720282423437339]
	TIME [epoch: 8.32 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7402502664844582		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.752376735024196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7463135007543269 | validation: 1.0054943165242294]
	TIME [epoch: 8.32 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6355723143889113		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.715993223183176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6757827687860436 | validation: 0.8717506407562806]
	TIME [epoch: 8.35 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8652908381473665		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8854816106146327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8753862243809996 | validation: 1.0389705923862]
	TIME [epoch: 8.32 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9678680679966727		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9717803069703391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9698241874835061 | validation: 1.3175126381794664]
	TIME [epoch: 8.32 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.860207282609554		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.894890829196609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8775490559030814 | validation: 0.9806305490537157]
	TIME [epoch: 8.32 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6659713549625718		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7185020893780456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6922367221703086 | validation: 0.9655489220628019]
	TIME [epoch: 8.35 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7877952265370282		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6897694143445923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7387823204408102 | validation: 0.7781515762566503]
	TIME [epoch: 8.32 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7941306804721396		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2769140196376338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.035522350054887 | validation: 0.9426887723507498]
	TIME [epoch: 8.32 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7865143305614204		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.198089230538338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.992301780549879 | validation: 0.5896233625844147]
	TIME [epoch: 8.32 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6004653705961601		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7540289487368321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6772471596664962 | validation: 0.7491563204977303]
	TIME [epoch: 8.33 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7251742491167976		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6752940293640646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.700234139240431 | validation: 0.5737921126454247]
	TIME [epoch: 8.33 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.714758384490086		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7808441649137924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7478012747019391 | validation: 0.45562373294620734]
	TIME [epoch: 8.32 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.57619438808894		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6596856333899755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6179400107394577 | validation: 0.6042431445732404]
	TIME [epoch: 8.32 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8391878997612953		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8807399057555638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8599639027584294 | validation: 0.7350111192748154]
	TIME [epoch: 8.32 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6836073381127594		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7321478753280084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7078776067203838 | validation: 0.5787680378045329]
	TIME [epoch: 8.35 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7435138014006443		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.729698096837138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7366059491188912 | validation: 0.4323769432530072]
	TIME [epoch: 8.31 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7031979088868835		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6311771070041823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6671875079455329 | validation: 1.0471830985143245]
	TIME [epoch: 8.32 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7278939453739379		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6693597012916789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6986268233328086 | validation: 0.5918402932374448]
	TIME [epoch: 8.31 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0281483490471766		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0013794995633722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0147639243052744 | validation: 0.6420029998846216]
	TIME [epoch: 8.34 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7614527339137476		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7617805494312806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7616166416725141 | validation: 0.5253521545970689]
	TIME [epoch: 8.31 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7107088835293538		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.727566964637131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7191379240832422 | validation: 0.4868882832377843]
	TIME [epoch: 8.32 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8596543643684864		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7165448441409525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7880996042547195 | validation: 0.49395043661194665]
	TIME [epoch: 8.31 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6011711899103555		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6258283301484495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6134997600294024 | validation: 1.1302723132157932]
	TIME [epoch: 8.32 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6948601862864535		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7235481984239593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7092041923552064 | validation: 0.38639524640409756]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7904551637170035		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5687640644618759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6796096140894397 | validation: 0.8716906557039016]
	TIME [epoch: 8.32 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6118889252314271		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5528621333103965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5823755292709116 | validation: 1.1765134591150523]
	TIME [epoch: 8.32 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6780838628630573		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7130168248095475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6955503438363025 | validation: 0.49729681527771]
	TIME [epoch: 8.31 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.803240294388965		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9925195582281459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8978799263085555 | validation: 1.204488590497981]
	TIME [epoch: 8.34 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9881722150499905		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7846004078557466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8863863114528685 | validation: 0.5646830244324282]
	TIME [epoch: 8.32 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8580311743802145		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7594932453208718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8087622098505433 | validation: 0.692027941503375]
	TIME [epoch: 8.31 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7721299101459602		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5688811468763351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6705055285111478 | validation: 0.6982352604601919]
	TIME [epoch: 8.31 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8767048973644167		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9957463556068497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.936225626485633 | validation: 0.8245223178128727]
	TIME [epoch: 8.33 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6268101500091213		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6993843432510408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6630972466300811 | validation: 0.45307212426976096]
	TIME [epoch: 8.32 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7532067859281211		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0085345402061932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8808706630671572 | validation: 1.1511785186300951]
	TIME [epoch: 8.32 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7127407114567795		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7083277400377149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7105342257472471 | validation: 0.5649947639261232]
	TIME [epoch: 8.31 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9702389342927639		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7231309812382907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8466849577655274 | validation: 0.35033600330633885]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7390280368733051		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9569419400439921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8479849884586489 | validation: 0.733767867658788]
	TIME [epoch: 8.32 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7326596062571091		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6798854009757382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7062725036164237 | validation: 4.089053360569206]
	TIME [epoch: 8.3 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.029715700982513		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7941871806602279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9119514408213704 | validation: 1.5217026512259735]
	TIME [epoch: 8.3 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8276682517576704		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7636589865102408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7956636191339557 | validation: 0.5684249234521662]
	TIME [epoch: 8.29 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2510630864271328		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9967124797955924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1238877831113627 | validation: 0.6452139712182275]
	TIME [epoch: 8.32 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6761223027425288		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6947398073547395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6854310550486342 | validation: 0.3604134707598551]
	TIME [epoch: 8.3 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6103651622318205		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6710284280839042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6406967951578624 | validation: 0.36348041783011287]
	TIME [epoch: 8.29 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6060831464736636		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6137986657088021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6099409060912329 | validation: 1.6198630671030207]
	TIME [epoch: 8.28 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7824332194923345		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5778335155565746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6801333675244545 | validation: 0.3912492395823882]
	TIME [epoch: 8.31 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6687850207062558		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6385113087431087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6536481647246821 | validation: 0.41684943180983125]
	TIME [epoch: 8.31 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7095222408517762		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.662492985067157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6860076129594666 | validation: 0.7882752513484284]
	TIME [epoch: 8.29 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6335445004381156		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8415341353111445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.73753931787463 | validation: 0.6577105944424269]
	TIME [epoch: 8.31 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6087040011446053		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6811897595163054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6449468803304554 | validation: 0.44292309528923335]
	TIME [epoch: 8.3 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5987259731293271		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5732072395491541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5859666063392407 | validation: 0.7038522891451516]
	TIME [epoch: 8.33 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6466303252658268		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5755200632004487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6110751942331374 | validation: 0.42935586918873486]
	TIME [epoch: 8.3 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.763421179649686		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6056263588485754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6845237692491307 | validation: 0.38131882058542527]
	TIME [epoch: 8.31 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5193710852140659		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5110869279558559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.515229006584961 | validation: 0.4731118022455877]
	TIME [epoch: 8.3 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5579346616562098		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8452848038651505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7016097327606801 | validation: 0.6921046286660921]
	TIME [epoch: 8.33 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6451176826137587		[learning rate: 0.0099837]
		[batch 20/20] avg loss: 0.5912305588328164		[learning rate: 0.0099655]
	Learning Rate: 0.00996552
	LOSS [training: 0.6181741207232876 | validation: 0.46788937334426434]
	TIME [epoch: 8.3 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7531279951664802		[learning rate: 0.0099474]
		[batch 20/20] avg loss: 0.5770261456755709		[learning rate: 0.0099294]
	Learning Rate: 0.00992935
	LOSS [training: 0.6650770704210256 | validation: 1.0747713652573834]
	TIME [epoch: 8.3 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6208076187582569		[learning rate: 0.0099113]
		[batch 20/20] avg loss: 0.6069309482722727		[learning rate: 0.0098933]
	Learning Rate: 0.00989332
	LOSS [training: 0.6138692835152648 | validation: 0.46349192144820417]
	TIME [epoch: 8.3 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5559892267170008		[learning rate: 0.0098754]
		[batch 20/20] avg loss: 0.5723223439305845		[learning rate: 0.0098574]
	Learning Rate: 0.00985742
	LOSS [training: 0.5641557853237927 | validation: 0.3315399921161594]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48272815898651994		[learning rate: 0.0098395]
		[batch 20/20] avg loss: 0.510498090528077		[learning rate: 0.0098216]
	Learning Rate: 0.00982164
	LOSS [training: 0.4966131247572986 | validation: 0.7814097855326136]
	TIME [epoch: 8.31 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5779773533213531		[learning rate: 0.0098038]
		[batch 20/20] avg loss: 0.5648294795192672		[learning rate: 0.009786]
	Learning Rate: 0.009786
	LOSS [training: 0.5714034164203102 | validation: 0.8587262622415516]
	TIME [epoch: 8.3 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5569537374088884		[learning rate: 0.0097682]
		[batch 20/20] avg loss: 0.6515331834401792		[learning rate: 0.0097505]
	Learning Rate: 0.00975049
	LOSS [training: 0.6042434604245338 | validation: 0.4537872413798735]
	TIME [epoch: 8.3 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6197333990363021		[learning rate: 0.0097328]
		[batch 20/20] avg loss: 0.6688017179391731		[learning rate: 0.0097151]
	Learning Rate: 0.0097151
	LOSS [training: 0.6442675584877375 | validation: 0.6528983253500154]
	TIME [epoch: 8.29 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.082955902765723		[learning rate: 0.0096975]
		[batch 20/20] avg loss: 0.8132554396049967		[learning rate: 0.0096798]
	Learning Rate: 0.00967984
	LOSS [training: 0.9481056711853597 | validation: 0.5858828093374671]
	TIME [epoch: 8.32 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7917448188654997		[learning rate: 0.0096623]
		[batch 20/20] avg loss: 0.5828042804147116		[learning rate: 0.0096447]
	Learning Rate: 0.00964472
	LOSS [training: 0.6872745496401056 | validation: 0.7488551988871248]
	TIME [epoch: 8.3 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5819204700761963		[learning rate: 0.0096272]
		[batch 20/20] avg loss: 0.5858344231012647		[learning rate: 0.0096097]
	Learning Rate: 0.00960972
	LOSS [training: 0.5838774465887305 | validation: 0.33639844658300305]
	TIME [epoch: 8.29 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6553530305820123		[learning rate: 0.0095923]
		[batch 20/20] avg loss: 0.6125735050564545		[learning rate: 0.0095748]
	Learning Rate: 0.00957484
	LOSS [training: 0.6339632678192334 | validation: 0.6497082783202676]
	TIME [epoch: 8.29 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7191022618316261		[learning rate: 0.0095575]
		[batch 20/20] avg loss: 0.6131863529060919		[learning rate: 0.0095401]
	Learning Rate: 0.00954009
	LOSS [training: 0.6661443073688591 | validation: 0.41174787742002306]
	TIME [epoch: 8.31 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5726759301376562		[learning rate: 0.0095228]
		[batch 20/20] avg loss: 0.6530236277343057		[learning rate: 0.0095055]
	Learning Rate: 0.00950547
	LOSS [training: 0.6128497789359809 | validation: 0.3998797897626564]
	TIME [epoch: 8.3 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5921464036195964		[learning rate: 0.0094882]
		[batch 20/20] avg loss: 0.7670148901847952		[learning rate: 0.009471]
	Learning Rate: 0.00947098
	LOSS [training: 0.6795806469021958 | validation: 0.4127893046400247]
	TIME [epoch: 8.3 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48870550467103435		[learning rate: 0.0094538]
		[batch 20/20] avg loss: 0.3999066837237234		[learning rate: 0.0094366]
	Learning Rate: 0.0094366
	LOSS [training: 0.44430609419737876 | validation: 0.6271196782297747]
	TIME [epoch: 8.3 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.692689476726556		[learning rate: 0.0094195]
		[batch 20/20] avg loss: 0.5879830562357252		[learning rate: 0.0094024]
	Learning Rate: 0.00940236
	LOSS [training: 0.6403362664811407 | validation: 0.45091413787518175]
	TIME [epoch: 8.31 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5264304880441102		[learning rate: 0.0093853]
		[batch 20/20] avg loss: 0.5377819349913823		[learning rate: 0.0093682]
	Learning Rate: 0.00936824
	LOSS [training: 0.5321062115177462 | validation: 0.40400094238655515]
	TIME [epoch: 8.3 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5480002083027545		[learning rate: 0.0093512]
		[batch 20/20] avg loss: 0.5045404981385812		[learning rate: 0.0093342]
	Learning Rate: 0.00933424
	LOSS [training: 0.5262703532206678 | validation: 1.508949742181759]
	TIME [epoch: 8.3 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8483937664156114		[learning rate: 0.0093173]
		[batch 20/20] avg loss: 0.548126254564433		[learning rate: 0.0093004]
	Learning Rate: 0.00930036
	LOSS [training: 0.6982600104900222 | validation: 0.28157813274997223]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5049171124297429		[learning rate: 0.0092835]
		[batch 20/20] avg loss: 0.5092624042158269		[learning rate: 0.0092666]
	Learning Rate: 0.00926661
	LOSS [training: 0.5070897583227849 | validation: 1.4098278887254938]
	TIME [epoch: 8.3 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5518210892034328		[learning rate: 0.0092498]
		[batch 20/20] avg loss: 0.5457513286838201		[learning rate: 0.009233]
	Learning Rate: 0.00923298
	LOSS [training: 0.5487862089436264 | validation: 0.7119974891992276]
	TIME [epoch: 8.32 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5347257763957479		[learning rate: 0.0092162]
		[batch 20/20] avg loss: 0.820053774430796		[learning rate: 0.0091995]
	Learning Rate: 0.00919948
	LOSS [training: 0.677389775413272 | validation: 0.4183242947945958]
	TIME [epoch: 8.3 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44469054097695954		[learning rate: 0.0091828]
		[batch 20/20] avg loss: 0.6668119236133148		[learning rate: 0.0091661]
	Learning Rate: 0.00916609
	LOSS [training: 0.555751232295137 | validation: 0.41942400944594255]
	TIME [epoch: 8.3 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5788079815024387		[learning rate: 0.0091494]
		[batch 20/20] avg loss: 0.5627877307258655		[learning rate: 0.0091328]
	Learning Rate: 0.00913283
	LOSS [training: 0.5707978561141521 | validation: 0.6295986047963277]
	TIME [epoch: 8.3 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6053772860490579		[learning rate: 0.0091162]
		[batch 20/20] avg loss: 0.6255866115917794		[learning rate: 0.0090997]
	Learning Rate: 0.00909968
	LOSS [training: 0.6154819488204187 | validation: 0.7342800719871294]
	TIME [epoch: 8.33 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5912104237754086		[learning rate: 0.0090832]
		[batch 20/20] avg loss: 0.6656079408307176		[learning rate: 0.0090667]
	Learning Rate: 0.00906666
	LOSS [training: 0.6284091823030631 | validation: 1.0883206414392297]
	TIME [epoch: 8.3 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7056540980456019		[learning rate: 0.0090502]
		[batch 20/20] avg loss: 0.5176780089472819		[learning rate: 0.0090338]
	Learning Rate: 0.00903376
	LOSS [training: 0.6116660534964418 | validation: 0.4430304656883934]
	TIME [epoch: 8.3 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49827657512490225		[learning rate: 0.0090174]
		[batch 20/20] avg loss: 0.480503007024436		[learning rate: 0.009001]
	Learning Rate: 0.00900097
	LOSS [training: 0.48938979107466896 | validation: 0.663812553404859]
	TIME [epoch: 8.3 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6522577924044939		[learning rate: 0.0089846]
		[batch 20/20] avg loss: 0.4304433289729773		[learning rate: 0.0089683]
	Learning Rate: 0.00896831
	LOSS [training: 0.5413505606887354 | validation: 0.36491424168077674]
	TIME [epoch: 8.32 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8402431685905336		[learning rate: 0.008952]
		[batch 20/20] avg loss: 0.4342273310503101		[learning rate: 0.0089358]
	Learning Rate: 0.00893576
	LOSS [training: 0.6372352498204219 | validation: 0.670492223184992]
	TIME [epoch: 8.32 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7304328006780528		[learning rate: 0.0089195]
		[batch 20/20] avg loss: 0.6470359249225325		[learning rate: 0.0089033]
	Learning Rate: 0.00890333
	LOSS [training: 0.6887343628002925 | validation: 0.9263707901907302]
	TIME [epoch: 8.29 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6052207816960464		[learning rate: 0.0088872]
		[batch 20/20] avg loss: 0.4694028656487149		[learning rate: 0.008871]
	Learning Rate: 0.00887102
	LOSS [training: 0.5373118236723808 | validation: 0.31925322127278283]
	TIME [epoch: 8.3 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5256236958609881		[learning rate: 0.0088549]
		[batch 20/20] avg loss: 0.6077706866847208		[learning rate: 0.0088388]
	Learning Rate: 0.00883883
	LOSS [training: 0.5666971912728543 | validation: 0.8595760218757718]
	TIME [epoch: 8.29 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5792239909598192		[learning rate: 0.0088228]
		[batch 20/20] avg loss: 0.4965358081003721		[learning rate: 0.0088068]
	Learning Rate: 0.00880675
	LOSS [training: 0.5378798995300957 | validation: 0.5001078927829915]
	TIME [epoch: 8.33 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6368279020102703		[learning rate: 0.0087908]
		[batch 20/20] avg loss: 0.5694811409193428		[learning rate: 0.0087748]
	Learning Rate: 0.00877479
	LOSS [training: 0.6031545214648065 | validation: 0.44921273390180183]
	TIME [epoch: 8.3 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.661432635797327		[learning rate: 0.0087589]
		[batch 20/20] avg loss: 0.6029066646071304		[learning rate: 0.0087429]
	Learning Rate: 0.00874295
	LOSS [training: 0.6321696502022287 | validation: 0.36545147167956876]
	TIME [epoch: 8.3 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46266129404445155		[learning rate: 0.0087271]
		[batch 20/20] avg loss: 0.6000660416326006		[learning rate: 0.0087112]
	Learning Rate: 0.00871122
	LOSS [training: 0.5313636678385261 | validation: 0.6868332817549916]
	TIME [epoch: 8.29 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.453992489413353		[learning rate: 0.0086954]
		[batch 20/20] avg loss: 0.5752221503244679		[learning rate: 0.0086796]
	Learning Rate: 0.00867961
	LOSS [training: 0.5146073198689104 | validation: 0.23779992712377593]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7007020046452224		[learning rate: 0.0086638]
		[batch 20/20] avg loss: 0.6589249832494415		[learning rate: 0.0086481]
	Learning Rate: 0.00864811
	LOSS [training: 0.6798134939473319 | validation: 0.26937188453911415]
	TIME [epoch: 8.3 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7167621645765709		[learning rate: 0.0086324]
		[batch 20/20] avg loss: 0.6206302055109872		[learning rate: 0.0086167]
	Learning Rate: 0.00861672
	LOSS [training: 0.6686961850437789 | validation: 0.7237923659241959]
	TIME [epoch: 8.29 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46133717874222413		[learning rate: 0.0086011]
		[batch 20/20] avg loss: 0.5769482773685313		[learning rate: 0.0085855]
	Learning Rate: 0.00858545
	LOSS [training: 0.5191427280553776 | validation: 0.43256097869188687]
	TIME [epoch: 8.3 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4483981666634237		[learning rate: 0.0085699]
		[batch 20/20] avg loss: 0.5567139690327514		[learning rate: 0.0085543]
	Learning Rate: 0.00855429
	LOSS [training: 0.5025560678480875 | validation: 0.6702223403288261]
	TIME [epoch: 8.31 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5463420373667296		[learning rate: 0.0085388]
		[batch 20/20] avg loss: 0.46675945468011265		[learning rate: 0.0085232]
	Learning Rate: 0.00852325
	LOSS [training: 0.5065507460234212 | validation: 0.39183552088071816]
	TIME [epoch: 8.31 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5475604642919845		[learning rate: 0.0085078]
		[batch 20/20] avg loss: 0.6534070899392288		[learning rate: 0.0084923]
	Learning Rate: 0.00849232
	LOSS [training: 0.6004837771156065 | validation: 0.24972640140015934]
	TIME [epoch: 8.3 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4854145778620377		[learning rate: 0.0084769]
		[batch 20/20] avg loss: 0.4689610214185268		[learning rate: 0.0084615]
	Learning Rate: 0.0084615
	LOSS [training: 0.47718779964028224 | validation: 0.2951769675325664]
	TIME [epoch: 8.3 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6746849428286453		[learning rate: 0.0084461]
		[batch 20/20] avg loss: 0.5101220388751426		[learning rate: 0.0084308]
	Learning Rate: 0.00843079
	LOSS [training: 0.5924034908518941 | validation: 0.4569721287397202]
	TIME [epoch: 8.29 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5962426213242112		[learning rate: 0.0084155]
		[batch 20/20] avg loss: 0.5369558476368124		[learning rate: 0.0084002]
	Learning Rate: 0.0084002
	LOSS [training: 0.5665992344805119 | validation: 0.6075432085915844]
	TIME [epoch: 8.32 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5191667943522413		[learning rate: 0.0083849]
		[batch 20/20] avg loss: 0.7461586454265077		[learning rate: 0.0083697]
	Learning Rate: 0.00836971
	LOSS [training: 0.6326627198893745 | validation: 1.0653407738664922]
	TIME [epoch: 8.3 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6365324947294682		[learning rate: 0.0083545]
		[batch 20/20] avg loss: 0.5172220056438505		[learning rate: 0.0083393]
	Learning Rate: 0.00833934
	LOSS [training: 0.5768772501866593 | validation: 0.5117787839314608]
	TIME [epoch: 8.3 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7106942958976921		[learning rate: 0.0083242]
		[batch 20/20] avg loss: 0.4393141891866006		[learning rate: 0.0083091]
	Learning Rate: 0.00830907
	LOSS [training: 0.5750042425421462 | validation: 0.3363779397850672]
	TIME [epoch: 8.3 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6013351975706164		[learning rate: 0.008294]
		[batch 20/20] avg loss: 0.6866125848885076		[learning rate: 0.0082789]
	Learning Rate: 0.00827892
	LOSS [training: 0.6439738912295622 | validation: 0.9675348164942377]
	TIME [epoch: 8.31 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5556718573711137		[learning rate: 0.0082639]
		[batch 20/20] avg loss: 0.6363953504290073		[learning rate: 0.0082489]
	Learning Rate: 0.00824887
	LOSS [training: 0.5960336039000605 | validation: 0.3776251442554225]
	TIME [epoch: 8.31 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4441968085364487		[learning rate: 0.0082339]
		[batch 20/20] avg loss: 0.6145876472080556		[learning rate: 0.0082189]
	Learning Rate: 0.00821894
	LOSS [training: 0.5293922278722523 | validation: 0.4227856734317084]
	TIME [epoch: 8.3 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44912416039730874		[learning rate: 0.008204]
		[batch 20/20] avg loss: 0.4248771948579929		[learning rate: 0.0081891]
	Learning Rate: 0.00818911
	LOSS [training: 0.4370006776276507 | validation: 0.6528618005629038]
	TIME [epoch: 8.3 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4872366885633409		[learning rate: 0.0081742]
		[batch 20/20] avg loss: 0.5452343088128461		[learning rate: 0.0081594]
	Learning Rate: 0.00815939
	LOSS [training: 0.5162354986880934 | validation: 0.45679273849081414]
	TIME [epoch: 8.3 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38955806209167515		[learning rate: 0.0081446]
		[batch 20/20] avg loss: 0.40488935547476046		[learning rate: 0.0081298]
	Learning Rate: 0.00812978
	LOSS [training: 0.39722370878321783 | validation: 0.5248712797758912]
	TIME [epoch: 8.32 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5562562820450633		[learning rate: 0.008115]
		[batch 20/20] avg loss: 0.37524267143855294		[learning rate: 0.0081003]
	Learning Rate: 0.00810028
	LOSS [training: 0.4657494767418081 | validation: 0.3239169124812741]
	TIME [epoch: 8.3 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3415568184678319		[learning rate: 0.0080856]
		[batch 20/20] avg loss: 0.37382245557175925		[learning rate: 0.0080709]
	Learning Rate: 0.00807088
	LOSS [training: 0.3576896370197956 | validation: 0.2803930032638642]
	TIME [epoch: 8.3 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4412111012039489		[learning rate: 0.0080562]
		[batch 20/20] avg loss: 0.3810532693265963		[learning rate: 0.0080416]
	Learning Rate: 0.00804159
	LOSS [training: 0.41113218526527257 | validation: 0.9734950479788486]
	TIME [epoch: 8.3 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4692718114955737		[learning rate: 0.008027]
		[batch 20/20] avg loss: 0.4007398668484039		[learning rate: 0.0080124]
	Learning Rate: 0.00801241
	LOSS [training: 0.43500583917198876 | validation: 0.3591097799527293]
	TIME [epoch: 8.33 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34593753158807805		[learning rate: 0.0079979]
		[batch 20/20] avg loss: 0.4408443559936659		[learning rate: 0.0079833]
	Learning Rate: 0.00798333
	LOSS [training: 0.39339094379087197 | validation: 0.37092436092751735]
	TIME [epoch: 8.3 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6207907076675181		[learning rate: 0.0079688]
		[batch 20/20] avg loss: 0.5264123518456898		[learning rate: 0.0079544]
	Learning Rate: 0.00795436
	LOSS [training: 0.5736015297566038 | validation: 0.24689070501440705]
	TIME [epoch: 8.3 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.454270456660132		[learning rate: 0.0079399]
		[batch 20/20] avg loss: 0.6018651282496688		[learning rate: 0.0079255]
	Learning Rate: 0.00792549
	LOSS [training: 0.5280677924549003 | validation: 0.9122740800540786]
	TIME [epoch: 8.31 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5662858740790618		[learning rate: 0.0079111]
		[batch 20/20] avg loss: 0.5456080882439223		[learning rate: 0.0078967]
	Learning Rate: 0.00789673
	LOSS [training: 0.5559469811614921 | validation: 0.299917902409619]
	TIME [epoch: 8.32 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6187841150940392		[learning rate: 0.0078824]
		[batch 20/20] avg loss: 0.37131612834213096		[learning rate: 0.0078681]
	Learning Rate: 0.00786807
	LOSS [training: 0.4950501217180851 | validation: 0.6514156337324507]
	TIME [epoch: 8.31 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4345043765287982		[learning rate: 0.0078538]
		[batch 20/20] avg loss: 0.4376131015911334		[learning rate: 0.0078395]
	Learning Rate: 0.00783952
	LOSS [training: 0.43605873905996584 | validation: 0.18615138570199719]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5104644755768611		[learning rate: 0.0078253]
		[batch 20/20] avg loss: 0.49909713847034487		[learning rate: 0.0078111]
	Learning Rate: 0.00781107
	LOSS [training: 0.5047808070236031 | validation: 0.7509032466941301]
	TIME [epoch: 8.3 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5809313991317311		[learning rate: 0.0077969]
		[batch 20/20] avg loss: 0.8257207839981318		[learning rate: 0.0077827]
	Learning Rate: 0.00778272
	LOSS [training: 0.7033260915649314 | validation: 0.4650223441721218]
	TIME [epoch: 8.31 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5829161983410335		[learning rate: 0.0077686]
		[batch 20/20] avg loss: 0.4359110555283534		[learning rate: 0.0077545]
	Learning Rate: 0.00775448
	LOSS [training: 0.5094136269346934 | validation: 0.3098898868689922]
	TIME [epoch: 8.33 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42764213756719166		[learning rate: 0.0077404]
		[batch 20/20] avg loss: 0.5369987899060252		[learning rate: 0.0077263]
	Learning Rate: 0.00772634
	LOSS [training: 0.4823204637366085 | validation: 0.4092532186613018]
	TIME [epoch: 8.31 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5254010822084115		[learning rate: 0.0077123]
		[batch 20/20] avg loss: 0.47576871361132644		[learning rate: 0.0076983]
	Learning Rate: 0.0076983
	LOSS [training: 0.500584897909869 | validation: 0.21562770081337926]
	TIME [epoch: 8.31 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37098661420304446		[learning rate: 0.0076843]
		[batch 20/20] avg loss: 0.4735269931970218		[learning rate: 0.0076704]
	Learning Rate: 0.00767036
	LOSS [training: 0.42225680370003316 | validation: 0.1864540282561392]
	TIME [epoch: 8.31 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.417832244490911		[learning rate: 0.0076564]
		[batch 20/20] avg loss: 0.48979964257975184		[learning rate: 0.0076425]
	Learning Rate: 0.00764252
	LOSS [training: 0.45381594353533145 | validation: 0.2515914184334395]
	TIME [epoch: 8.33 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5611575141714027		[learning rate: 0.0076286]
		[batch 20/20] avg loss: 0.4573820267316228		[learning rate: 0.0076148]
	Learning Rate: 0.00761479
	LOSS [training: 0.5092697704515128 | validation: 0.22046988258202346]
	TIME [epoch: 8.31 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5347122217533418		[learning rate: 0.007601]
		[batch 20/20] avg loss: 0.45144053794791955		[learning rate: 0.0075872]
	Learning Rate: 0.00758715
	LOSS [training: 0.49307637985063063 | validation: 0.35812324528108264]
	TIME [epoch: 8.29 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41950433858086267		[learning rate: 0.0075734]
		[batch 20/20] avg loss: 0.6286829689024418		[learning rate: 0.0075596]
	Learning Rate: 0.00755962
	LOSS [training: 0.5240936537416523 | validation: 1.2208329004427578]
	TIME [epoch: 8.29 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5928606454667651		[learning rate: 0.0075459]
		[batch 20/20] avg loss: 0.4853987871667166		[learning rate: 0.0075322]
	Learning Rate: 0.00753219
	LOSS [training: 0.5391297163167408 | validation: 0.4291247347961876]
	TIME [epoch: 8.31 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4566186250979453		[learning rate: 0.0075185]
		[batch 20/20] avg loss: 0.4080943182756272		[learning rate: 0.0075049]
	Learning Rate: 0.00750485
	LOSS [training: 0.4323564716867862 | validation: 2.3433393629397514]
	TIME [epoch: 8.31 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0733917777373898		[learning rate: 0.0074912]
		[batch 20/20] avg loss: 0.7995691369684603		[learning rate: 0.0074776]
	Learning Rate: 0.00747762
	LOSS [training: 0.9364804573529251 | validation: 0.41172459527725846]
	TIME [epoch: 8.29 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8704066818411407		[learning rate: 0.007464]
		[batch 20/20] avg loss: 0.8679909104417944		[learning rate: 0.0074505]
	Learning Rate: 0.00745048
	LOSS [training: 0.8691987961414677 | validation: 0.5036659990977452]
	TIME [epoch: 8.3 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1015389685549777		[learning rate: 0.0074369]
		[batch 20/20] avg loss: 0.8586219481424446		[learning rate: 0.0074234]
	Learning Rate: 0.00742344
	LOSS [training: 0.9800804583487113 | validation: 4.0577068017204425]
	TIME [epoch: 8.29 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2391974276921218		[learning rate: 0.00741]
		[batch 20/20] avg loss: 0.8104409835494364		[learning rate: 0.0073965]
	Learning Rate: 0.0073965
	LOSS [training: 1.0248192056207792 | validation: 0.730825489787553]
	TIME [epoch: 8.31 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7601567668287867		[learning rate: 0.0073831]
		[batch 20/20] avg loss: 0.8755451847489413		[learning rate: 0.0073697]
	Learning Rate: 0.00736966
	LOSS [training: 0.8178509757888639 | validation: 0.3979645043953204]
	TIME [epoch: 8.36 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5702616124023585		[learning rate: 0.0073563]
		[batch 20/20] avg loss: 0.74284615417523		[learning rate: 0.0073429]
	Learning Rate: 0.00734291
	LOSS [training: 0.6565538832887944 | validation: 0.9430703008906252]
	TIME [epoch: 8.41 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7978876557598746		[learning rate: 0.0073296]
		[batch 20/20] avg loss: 1.4363065592521458		[learning rate: 0.0073163]
	Learning Rate: 0.00731627
	LOSS [training: 1.1170971075060105 | validation: 0.343199972214257]
	TIME [epoch: 8.35 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0746845475252766		[learning rate: 0.007303]
		[batch 20/20] avg loss: 1.0934753897944636		[learning rate: 0.0072897]
	Learning Rate: 0.00728971
	LOSS [training: 1.08407996865987 | validation: 1.4388202581777558]
	TIME [epoch: 8.36 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8318951513760385		[learning rate: 0.0072765]
		[batch 20/20] avg loss: 0.6334725366238791		[learning rate: 0.0072633]
	Learning Rate: 0.00726326
	LOSS [training: 0.7326838439999588 | validation: 0.3857509550554716]
	TIME [epoch: 8.43 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7283809060506699		[learning rate: 0.0072501]
		[batch 20/20] avg loss: 1.332032437717746		[learning rate: 0.0072369]
	Learning Rate: 0.0072369
	LOSS [training: 1.030206671884208 | validation: 0.27250737642622536]
	TIME [epoch: 8.43 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8506399920792663		[learning rate: 0.0072238]
		[batch 20/20] avg loss: 0.49106648170200284		[learning rate: 0.0072106]
	Learning Rate: 0.00721064
	LOSS [training: 0.6708532368906346 | validation: 1.6505712173902336]
	TIME [epoch: 8.39 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9024399700511572		[learning rate: 0.0071975]
		[batch 20/20] avg loss: 0.6872374541119558		[learning rate: 0.0071845]
	Learning Rate: 0.00718447
	LOSS [training: 0.7948387120815563 | validation: 0.30459128220419596]
	TIME [epoch: 8.44 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9237746587208046		[learning rate: 0.0071714]
		[batch 20/20] avg loss: 0.7010082444948798		[learning rate: 0.0071584]
	Learning Rate: 0.0071584
	LOSS [training: 0.8123914516078422 | validation: 1.010065372569044]
	TIME [epoch: 8.5 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.86754194854174		[learning rate: 0.0071454]
		[batch 20/20] avg loss: 0.7646197887357539		[learning rate: 0.0071324]
	Learning Rate: 0.00713242
	LOSS [training: 0.8160808686387468 | validation: 0.7059631520613486]
	TIME [epoch: 8.5 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.040384532126991		[learning rate: 0.0071195]
		[batch 20/20] avg loss: 0.61318556370695		[learning rate: 0.0071065]
	Learning Rate: 0.00710653
	LOSS [training: 0.8267850479169704 | validation: 0.9451595819711378]
	TIME [epoch: 8.39 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7636615028583383		[learning rate: 0.0070936]
		[batch 20/20] avg loss: 0.6249856632700515		[learning rate: 0.0070807]
	Learning Rate: 0.00708074
	LOSS [training: 0.6943235830641948 | validation: 0.5036057178835311]
	TIME [epoch: 8.39 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5210419877705157		[learning rate: 0.0070679]
		[batch 20/20] avg loss: 0.6605274956706357		[learning rate: 0.007055]
	Learning Rate: 0.00705505
	LOSS [training: 0.5907847417205756 | validation: 0.39008556042675213]
	TIME [epoch: 8.4 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5289047846954037		[learning rate: 0.0070422]
		[batch 20/20] avg loss: 0.6213686097624902		[learning rate: 0.0070294]
	Learning Rate: 0.00702945
	LOSS [training: 0.5751366972289469 | validation: 0.4103842594323204]
	TIME [epoch: 8.39 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5546518650853013		[learning rate: 0.0070167]
		[batch 20/20] avg loss: 0.6761597545879845		[learning rate: 0.0070039]
	Learning Rate: 0.00700394
	LOSS [training: 0.6154058098366428 | validation: 1.3034698131760427]
	TIME [epoch: 8.4 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6575216576344681		[learning rate: 0.0069912]
		[batch 20/20] avg loss: 0.6755818412487207		[learning rate: 0.0069785]
	Learning Rate: 0.00697852
	LOSS [training: 0.6665517494415945 | validation: 0.5262581274339229]
	TIME [epoch: 8.44 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.826010640436704		[learning rate: 0.0069658]
		[batch 20/20] avg loss: 0.6074856911423678		[learning rate: 0.0069532]
	Learning Rate: 0.00695319
	LOSS [training: 0.7167481657895359 | validation: 0.43480827843268455]
	TIME [epoch: 8.42 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5843014246417235		[learning rate: 0.0069406]
		[batch 20/20] avg loss: 0.6170988254656204		[learning rate: 0.006928]
	Learning Rate: 0.00692796
	LOSS [training: 0.600700125053672 | validation: 0.5660460917448815]
	TIME [epoch: 8.38 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6125434530721938		[learning rate: 0.0069154]
		[batch 20/20] avg loss: 0.6046663311046225		[learning rate: 0.0069028]
	Learning Rate: 0.00690282
	LOSS [training: 0.6086048920884083 | validation: 0.3354699520863618]
	TIME [epoch: 8.47 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6652002169437162		[learning rate: 0.0068903]
		[batch 20/20] avg loss: 0.5685425429356227		[learning rate: 0.0068778]
	Learning Rate: 0.00687777
	LOSS [training: 0.6168713799396695 | validation: 0.40992340788311493]
	TIME [epoch: 8.47 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5182309147611146		[learning rate: 0.0068653]
		[batch 20/20] avg loss: 0.793601225240997		[learning rate: 0.0068528]
	Learning Rate: 0.00685281
	LOSS [training: 0.6559160700010558 | validation: 0.7008614067547299]
	TIME [epoch: 8.53 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5950100709879		[learning rate: 0.0068404]
		[batch 20/20] avg loss: 0.5623717360904882		[learning rate: 0.0068279]
	Learning Rate: 0.00682794
	LOSS [training: 0.5786909035391943 | validation: 1.0392419872612189]
	TIME [epoch: 8.51 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7716972742536795		[learning rate: 0.0068155]
		[batch 20/20] avg loss: 0.8141206092631503		[learning rate: 0.0068032]
	Learning Rate: 0.00680316
	LOSS [training: 0.7929089417584148 | validation: 0.5418924288232997]
	TIME [epoch: 8.53 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6993804056942888		[learning rate: 0.0067908]
		[batch 20/20] avg loss: 1.4735173095855572		[learning rate: 0.0067785]
	Learning Rate: 0.00677847
	LOSS [training: 1.086448857639923 | validation: 7.867558164840901]
	TIME [epoch: 8.58 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.693160267114832		[learning rate: 0.0067662]
		[batch 20/20] avg loss: 5.065838932327287		[learning rate: 0.0067539]
	Learning Rate: 0.00675387
	LOSS [training: 6.37949959972106 | validation: 3.101473821156241]
	TIME [epoch: 8.57 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4127994333291705		[learning rate: 0.0067416]
		[batch 20/20] avg loss: 2.4756575148188857		[learning rate: 0.0067294]
	Learning Rate: 0.00672936
	LOSS [training: 2.9442284740740283 | validation: 1.521465294675473]
	TIME [epoch: 8.54 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2997052933597766		[learning rate: 0.0067171]
		[batch 20/20] avg loss: 1.3600609477912968		[learning rate: 0.0067049]
	Learning Rate: 0.00670494
	LOSS [training: 1.3298831205755368 | validation: 3.0769143177956932]
	TIME [epoch: 8.49 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5425149186145377		[learning rate: 0.0066928]
		[batch 20/20] avg loss: 1.6415649364882186		[learning rate: 0.0066806]
	Learning Rate: 0.0066806
	LOSS [training: 1.5920399275513781 | validation: 0.7509038400429224]
	TIME [epoch: 8.48 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6419182801629297		[learning rate: 0.0066685]
		[batch 20/20] avg loss: 1.0389278292616948		[learning rate: 0.0066564]
	Learning Rate: 0.00665636
	LOSS [training: 1.3404230547123122 | validation: 0.8635998878246578]
	TIME [epoch: 8.5 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8965330944676271		[learning rate: 0.0066443]
		[batch 20/20] avg loss: 0.9703441127589475		[learning rate: 0.0066322]
	Learning Rate: 0.0066322
	LOSS [training: 0.9334386036132871 | validation: 0.3618309301578309]
	TIME [epoch: 8.56 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7858176036858368		[learning rate: 0.0066202]
		[batch 20/20] avg loss: 0.931541849486089		[learning rate: 0.0066081]
	Learning Rate: 0.00660814
	LOSS [training: 0.8586797265859628 | validation: 2.416037757049742]
	TIME [epoch: 8.51 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2760141086064798		[learning rate: 0.0065961]
		[batch 20/20] avg loss: 1.1401060759959767		[learning rate: 0.0065842]
	Learning Rate: 0.00658415
	LOSS [training: 1.2080600923012281 | validation: 0.9041418077580654]
	TIME [epoch: 8.51 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8601909221802314		[learning rate: 0.0065722]
		[batch 20/20] avg loss: 0.7679643891788814		[learning rate: 0.0065603]
	Learning Rate: 0.00656026
	LOSS [training: 0.8140776556795565 | validation: 0.6233816257495897]
	TIME [epoch: 8.43 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7459347158525109		[learning rate: 0.0065483]
		[batch 20/20] avg loss: 0.6551377914058036		[learning rate: 0.0065365]
	Learning Rate: 0.00653645
	LOSS [training: 0.7005362536291574 | validation: 0.6612272502105712]
	TIME [epoch: 8.41 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.725106892843973		[learning rate: 0.0065246]
		[batch 20/20] avg loss: 0.6677835697655021		[learning rate: 0.0065127]
	Learning Rate: 0.00651273
	LOSS [training: 0.6964452313047376 | validation: 4.656179616618253]
	TIME [epoch: 8.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3854782864530424		[learning rate: 0.0065009]
		[batch 20/20] avg loss: 0.6058431466326382		[learning rate: 0.0064891]
	Learning Rate: 0.0064891
	LOSS [training: 0.9956607165428402 | validation: 0.7067186135941239]
	TIME [epoch: 8.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6358887691398001		[learning rate: 0.0064773]
		[batch 20/20] avg loss: 0.5954848929216225		[learning rate: 0.0064655]
	Learning Rate: 0.00646555
	LOSS [training: 0.6156868310307112 | validation: 0.35188581103538746]
	TIME [epoch: 8.34 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6879459056905117		[learning rate: 0.0064538]
		[batch 20/20] avg loss: 0.5919890541901286		[learning rate: 0.0064421]
	Learning Rate: 0.00644208
	LOSS [training: 0.6399674799403201 | validation: 1.4587689754423332]
	TIME [epoch: 8.5 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6419818675350049		[learning rate: 0.0064304]
		[batch 20/20] avg loss: 0.5594988065041494		[learning rate: 0.0064187]
	Learning Rate: 0.0064187
	LOSS [training: 0.6007403370195771 | validation: 0.6479483922107212]
	TIME [epoch: 8.59 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2142655073941484		[learning rate: 0.006407]
		[batch 20/20] avg loss: 0.5322802098008022		[learning rate: 0.0063954]
	Learning Rate: 0.00639541
	LOSS [training: 0.8732728585974752 | validation: 0.4043370420035145]
	TIME [epoch: 8.52 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.62667838891464		[learning rate: 0.0063838]
		[batch 20/20] avg loss: 0.47045548301688606		[learning rate: 0.0063722]
	Learning Rate: 0.0063722
	LOSS [training: 0.5485669359657631 | validation: 0.48932713546508744]
	TIME [epoch: 8.39 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6323630212211964		[learning rate: 0.0063606]
		[batch 20/20] avg loss: 0.534285033732553		[learning rate: 0.0063491]
	Learning Rate: 0.00634908
	LOSS [training: 0.5833240274768747 | validation: 0.6458409215605277]
	TIME [epoch: 8.45 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5053295544741674		[learning rate: 0.0063375]
		[batch 20/20] avg loss: 0.48254048236329644		[learning rate: 0.006326]
	Learning Rate: 0.00632603
	LOSS [training: 0.493935018418732 | validation: 0.35062348434322244]
	TIME [epoch: 8.47 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47999381700561267		[learning rate: 0.0063145]
		[batch 20/20] avg loss: 0.9180474848982219		[learning rate: 0.0063031]
	Learning Rate: 0.00630308
	LOSS [training: 0.6990206509519171 | validation: 0.333641260993472]
	TIME [epoch: 8.52 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5236724765171044		[learning rate: 0.0062916]
		[batch 20/20] avg loss: 1.1254109328576354		[learning rate: 0.0062802]
	Learning Rate: 0.0062802
	LOSS [training: 0.82454170468737 | validation: 1.371138446419891]
	TIME [epoch: 8.47 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6405850483582427		[learning rate: 0.0062688]
		[batch 20/20] avg loss: 0.5233095528156763		[learning rate: 0.0062574]
	Learning Rate: 0.00625741
	LOSS [training: 0.5819473005869595 | validation: 0.5987981078497564]
	TIME [epoch: 8.39 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5498622601426961		[learning rate: 0.006246]
		[batch 20/20] avg loss: 0.6592227466563256		[learning rate: 0.0062347]
	Learning Rate: 0.0062347
	LOSS [training: 0.6045425033995108 | validation: 0.47564875171606535]
	TIME [epoch: 8.38 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6274142831144763		[learning rate: 0.0062234]
		[batch 20/20] avg loss: 0.7982157578865734		[learning rate: 0.0062121]
	Learning Rate: 0.00621208
	LOSS [training: 0.7128150205005248 | validation: 0.3455625390974923]
	TIME [epoch: 8.34 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5279241698042512		[learning rate: 0.0062008]
		[batch 20/20] avg loss: 0.543422815686034		[learning rate: 0.0061895]
	Learning Rate: 0.00618953
	LOSS [training: 0.5356734927451426 | validation: 0.6982272446412079]
	TIME [epoch: 8.49 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5012982654100773		[learning rate: 0.0061783]
		[batch 20/20] avg loss: 0.4835288962264042		[learning rate: 0.0061671]
	Learning Rate: 0.00616707
	LOSS [training: 0.4924135808182407 | validation: 0.9268299076367763]
	TIME [epoch: 8.51 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6365396935719441		[learning rate: 0.0061559]
		[batch 20/20] avg loss: 0.40467079995823996		[learning rate: 0.0061447]
	Learning Rate: 0.00614469
	LOSS [training: 0.520605246765092 | validation: 0.8625972247611953]
	TIME [epoch: 8.5 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5246035685311774		[learning rate: 0.0061335]
		[batch 20/20] avg loss: 0.7124921738438663		[learning rate: 0.0061224]
	Learning Rate: 0.00612239
	LOSS [training: 0.6185478711875219 | validation: 0.8791873965508233]
	TIME [epoch: 8.51 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5085699296319253		[learning rate: 0.0061113]
		[batch 20/20] avg loss: 0.46282933672803417		[learning rate: 0.0061002]
	Learning Rate: 0.00610017
	LOSS [training: 0.4856996331799797 | validation: 0.18831453434111048]
	TIME [epoch: 8.54 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6055083289622106		[learning rate: 0.0060891]
		[batch 20/20] avg loss: 0.46951953801646545		[learning rate: 0.006078]
	Learning Rate: 0.00607803
	LOSS [training: 0.5375139334893382 | validation: 0.7297695915427235]
	TIME [epoch: 8.54 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.478403811651838		[learning rate: 0.006067]
		[batch 20/20] avg loss: 0.5595459104753944		[learning rate: 0.006056]
	Learning Rate: 0.00605598
	LOSS [training: 0.5189748610636161 | validation: 0.41985540411605343]
	TIME [epoch: 8.4 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6832196452459329		[learning rate: 0.006045]
		[batch 20/20] avg loss: 0.6300011631648312		[learning rate: 0.006034]
	Learning Rate: 0.006034
	LOSS [training: 0.656610404205382 | validation: 0.46596358199006827]
	TIME [epoch: 8.4 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8353781860550564		[learning rate: 0.006023]
		[batch 20/20] avg loss: 0.6768803993190772		[learning rate: 0.0060121]
	Learning Rate: 0.0060121
	LOSS [training: 0.7561292926870667 | validation: 0.2920382159803636]
	TIME [epoch: 8.4 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5338047762543544		[learning rate: 0.0060012]
		[batch 20/20] avg loss: 0.4835725170024495		[learning rate: 0.0059903]
	Learning Rate: 0.00599028
	LOSS [training: 0.508688646628402 | validation: 0.2745946242044021]
	TIME [epoch: 8.41 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9620179004456585		[learning rate: 0.0059794]
		[batch 20/20] avg loss: 0.9481312928213083		[learning rate: 0.0059685]
	Learning Rate: 0.00596854
	LOSS [training: 0.9550745966334834 | validation: 0.4307823610168811]
	TIME [epoch: 8.37 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.71788560146831		[learning rate: 0.0059577]
		[batch 20/20] avg loss: 0.6075826083247331		[learning rate: 0.0059469]
	Learning Rate: 0.00594688
	LOSS [training: 0.6627341048965216 | validation: 0.14147542261341794]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5418045440910205		[learning rate: 0.0059361]
		[batch 20/20] avg loss: 1.9304157228827716		[learning rate: 0.0059253]
	Learning Rate: 0.0059253
	LOSS [training: 1.2361101334868962 | validation: 0.223854545094116]
	TIME [epoch: 8.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39798607630206756		[learning rate: 0.0059145]
		[batch 20/20] avg loss: 0.49531480649518655		[learning rate: 0.0059038]
	Learning Rate: 0.0059038
	LOSS [training: 0.4466504413986271 | validation: 0.24102923843760501]
	TIME [epoch: 8.34 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46424986160734216		[learning rate: 0.0058931]
		[batch 20/20] avg loss: 0.5161677443630251		[learning rate: 0.0058824]
	Learning Rate: 0.00588237
	LOSS [training: 0.49020880298518366 | validation: 0.41700405094875026]
	TIME [epoch: 8.38 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47929583854178154		[learning rate: 0.0058717]
		[batch 20/20] avg loss: 0.42791374774563656		[learning rate: 0.005861]
	Learning Rate: 0.00586103
	LOSS [training: 0.45360479314370894 | validation: 0.5048786094581177]
	TIME [epoch: 8.47 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5062361033605213		[learning rate: 0.0058504]
		[batch 20/20] avg loss: 0.472442085205431		[learning rate: 0.0058398]
	Learning Rate: 0.00583976
	LOSS [training: 0.4893390942829762 | validation: 0.4640311159985733]
	TIME [epoch: 8.61 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46972771056738505		[learning rate: 0.0058291]
		[batch 20/20] avg loss: 0.45604796602297365		[learning rate: 0.0058186]
	Learning Rate: 0.00581856
	LOSS [training: 0.46288783829517943 | validation: 0.37007273450845585]
	TIME [epoch: 8.57 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43586509593414924		[learning rate: 0.005808]
		[batch 20/20] avg loss: 0.5092352407131779		[learning rate: 0.0057974]
	Learning Rate: 0.00579745
	LOSS [training: 0.4725501683236636 | validation: 0.12805781398341393]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45295110211988093		[learning rate: 0.0057869]
		[batch 20/20] avg loss: 0.49658010877724124		[learning rate: 0.0057764]
	Learning Rate: 0.00577641
	LOSS [training: 0.474765605448561 | validation: 0.21170029965400455]
	TIME [epoch: 8.51 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.723198095559984		[learning rate: 0.0057659]
		[batch 20/20] avg loss: 0.5289087291015384		[learning rate: 0.0057554]
	Learning Rate: 0.00575545
	LOSS [training: 0.6260534123307612 | validation: 0.6859827213800772]
	TIME [epoch: 8.49 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4766117969383732		[learning rate: 0.005745]
		[batch 20/20] avg loss: 0.5114327789000697		[learning rate: 0.0057346]
	Learning Rate: 0.00573456
	LOSS [training: 0.4940222879192214 | validation: 0.3302936273569612]
	TIME [epoch: 8.49 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5118167438271265		[learning rate: 0.0057241]
		[batch 20/20] avg loss: 0.48169679347128735		[learning rate: 0.0057137]
	Learning Rate: 0.00571375
	LOSS [training: 0.4967567686492069 | validation: 0.2539197137963846]
	TIME [epoch: 8.49 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5432996037931158		[learning rate: 0.0057034]
		[batch 20/20] avg loss: 0.5604241845001366		[learning rate: 0.005693]
	Learning Rate: 0.00569301
	LOSS [training: 0.5518618941466261 | validation: 0.22852105063746075]
	TIME [epoch: 8.49 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6868760917494227		[learning rate: 0.0056827]
		[batch 20/20] avg loss: 0.6256792726332795		[learning rate: 0.0056724]
	Learning Rate: 0.00567235
	LOSS [training: 0.656277682191351 | validation: 0.37217188353043856]
	TIME [epoch: 8.51 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5579872994563162		[learning rate: 0.005662]
		[batch 20/20] avg loss: 0.5804132415755656		[learning rate: 0.0056518]
	Learning Rate: 0.00565177
	LOSS [training: 0.5692002705159409 | validation: 0.6689408144081775]
	TIME [epoch: 8.48 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5205015692180689		[learning rate: 0.0056415]
		[batch 20/20] avg loss: 0.7065617396575189		[learning rate: 0.0056313]
	Learning Rate: 0.00563126
	LOSS [training: 0.6135316544377939 | validation: 0.6360171820764582]
	TIME [epoch: 8.45 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.586675006215375		[learning rate: 0.005621]
		[batch 20/20] avg loss: 0.5934727964891178		[learning rate: 0.0056108]
	Learning Rate: 0.00561082
	LOSS [training: 0.5900739013522464 | validation: 0.47232381525602385]
	TIME [epoch: 8.48 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5048567478792225		[learning rate: 0.0056006]
		[batch 20/20] avg loss: 0.3841637500810758		[learning rate: 0.0055905]
	Learning Rate: 0.00559046
	LOSS [training: 0.44451024898014924 | validation: 0.4930091698257024]
	TIME [epoch: 8.45 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39074485171871126		[learning rate: 0.0055803]
		[batch 20/20] avg loss: 0.6507621276906193		[learning rate: 0.0055702]
	Learning Rate: 0.00557017
	LOSS [training: 0.5207534897046652 | validation: 0.5476186181538159]
	TIME [epoch: 8.35 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5903779754875554		[learning rate: 0.0055601]
		[batch 20/20] avg loss: 0.7207658416743421		[learning rate: 0.00555]
	Learning Rate: 0.00554996
	LOSS [training: 0.6555719085809486 | validation: 0.660728892263893]
	TIME [epoch: 8.36 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8430224259453929		[learning rate: 0.0055399]
		[batch 20/20] avg loss: 0.4836791447001154		[learning rate: 0.0055298]
	Learning Rate: 0.00552981
	LOSS [training: 0.663350785322754 | validation: 0.16654351659511957]
	TIME [epoch: 8.37 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4768896657132493		[learning rate: 0.0055198]
		[batch 20/20] avg loss: 0.8031515942897792		[learning rate: 0.0055097]
	Learning Rate: 0.00550975
	LOSS [training: 0.6400206300015143 | validation: 1.718319465015919]
	TIME [epoch: 8.39 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7297409598906132		[learning rate: 0.0054997]
		[batch 20/20] avg loss: 0.4054457801058228		[learning rate: 0.0054898]
	Learning Rate: 0.00548975
	LOSS [training: 0.567593369998218 | validation: 0.7070790494804339]
	TIME [epoch: 8.45 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3878508799534151		[learning rate: 0.0054798]
		[batch 20/20] avg loss: 0.9699641241513361		[learning rate: 0.0054698]
	Learning Rate: 0.00546983
	LOSS [training: 0.6789075020523757 | validation: 0.6773546422899828]
	TIME [epoch: 8.57 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.865034677489062		[learning rate: 0.0054599]
		[batch 20/20] avg loss: 0.48538100691022307		[learning rate: 0.00545]
	Learning Rate: 0.00544998
	LOSS [training: 0.6752078421996426 | validation: 0.5247555559821558]
	TIME [epoch: 8.61 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5861617923291769		[learning rate: 0.0054401]
		[batch 20/20] avg loss: 0.618251833621703		[learning rate: 0.0054302]
	Learning Rate: 0.0054302
	LOSS [training: 0.6022068129754401 | validation: 0.4405088148931082]
	TIME [epoch: 8.54 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.929584419893908		[learning rate: 0.0054203]
		[batch 20/20] avg loss: 0.589192134578117		[learning rate: 0.0054105]
	Learning Rate: 0.00541049
	LOSS [training: 0.7593882772360125 | validation: 0.7875742336870505]
	TIME [epoch: 8.56 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.804610669865131		[learning rate: 0.0054007]
		[batch 20/20] avg loss: 0.5215704347737269		[learning rate: 0.0053909]
	Learning Rate: 0.00539086
	LOSS [training: 0.6630905523194288 | validation: 0.6962936816491856]
	TIME [epoch: 8.4 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7207855523131205		[learning rate: 0.0053811]
		[batch 20/20] avg loss: 0.6041454012642192		[learning rate: 0.0053713]
	Learning Rate: 0.00537129
	LOSS [training: 0.6624654767886698 | validation: 0.5655510721135678]
	TIME [epoch: 8.4 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.510425309687174		[learning rate: 0.0053615]
		[batch 20/20] avg loss: 0.4914687614709351		[learning rate: 0.0053518]
	Learning Rate: 0.0053518
	LOSS [training: 0.5009470355790546 | validation: 0.253590870252276]
	TIME [epoch: 8.45 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6629830994155346		[learning rate: 0.0053421]
		[batch 20/20] avg loss: 0.8233328578307786		[learning rate: 0.0053324]
	Learning Rate: 0.00533238
	LOSS [training: 0.7431579786231567 | validation: 0.16442366542792528]
	TIME [epoch: 8.43 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.548323027566671		[learning rate: 0.0053227]
		[batch 20/20] avg loss: 0.8173335748617969		[learning rate: 0.005313]
	Learning Rate: 0.00531303
	LOSS [training: 0.682828301214234 | validation: 0.49642070564742785]
	TIME [epoch: 8.44 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5530701833789566		[learning rate: 0.0053034]
		[batch 20/20] avg loss: 0.5871112999936782		[learning rate: 0.0052937]
	Learning Rate: 0.00529375
	LOSS [training: 0.5700907416863175 | validation: 0.4333266087367301]
	TIME [epoch: 8.45 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9840350508947291		[learning rate: 0.0052841]
		[batch 20/20] avg loss: 0.543272967619375		[learning rate: 0.0052745]
	Learning Rate: 0.00527454
	LOSS [training: 0.7636540092570523 | validation: 0.3141314380495939]
	TIME [epoch: 8.45 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5142781977577165		[learning rate: 0.005265]
		[batch 20/20] avg loss: 0.7279720849051419		[learning rate: 0.0052554]
	Learning Rate: 0.00525539
	LOSS [training: 0.6211251413314293 | validation: 0.3792097245279551]
	TIME [epoch: 8.33 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6001228017935226		[learning rate: 0.0052458]
		[batch 20/20] avg loss: 0.4912002957827422		[learning rate: 0.0052363]
	Learning Rate: 0.00523632
	LOSS [training: 0.5456615487881324 | validation: 0.4063620881443264]
	TIME [epoch: 8.42 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7965184228802394		[learning rate: 0.0052268]
		[batch 20/20] avg loss: 0.612946147852633		[learning rate: 0.0052173]
	Learning Rate: 0.00521732
	LOSS [training: 0.7047322853664362 | validation: 0.4552569841258548]
	TIME [epoch: 8.51 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5369031840763556		[learning rate: 0.0052078]
		[batch 20/20] avg loss: 0.37570519410467823		[learning rate: 0.0051984]
	Learning Rate: 0.00519839
	LOSS [training: 0.45630418909051695 | validation: 0.45594415226190554]
	TIME [epoch: 8.51 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.592042581411766		[learning rate: 0.0051889]
		[batch 20/20] avg loss: 0.5917064835732433		[learning rate: 0.0051795]
	Learning Rate: 0.00517952
	LOSS [training: 0.5918745324925044 | validation: 0.1956991505249913]
	TIME [epoch: 8.53 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6860071173449984		[learning rate: 0.0051701]
		[batch 20/20] avg loss: 0.46310979103889294		[learning rate: 0.0051607]
	Learning Rate: 0.00516072
	LOSS [training: 0.5745584541919457 | validation: 0.5848029881529034]
	TIME [epoch: 8.55 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5226261775636855		[learning rate: 0.0051513]
		[batch 20/20] avg loss: 0.6526913077636985		[learning rate: 0.005142]
	Learning Rate: 0.00514199
	LOSS [training: 0.587658742663692 | validation: 0.6191518327200153]
	TIME [epoch: 8.59 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5291514626008271		[learning rate: 0.0051327]
		[batch 20/20] avg loss: 0.6122875262316206		[learning rate: 0.0051233]
	Learning Rate: 0.00512333
	LOSS [training: 0.570719494416224 | validation: 1.0946910087218729]
	TIME [epoch: 8.7 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8236840141065114		[learning rate: 0.005114]
		[batch 20/20] avg loss: 0.5837777238289595		[learning rate: 0.0051047]
	Learning Rate: 0.00510474
	LOSS [training: 0.7037308689677355 | validation: 0.4469106313654791]
	TIME [epoch: 8.58 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5108871215548433		[learning rate: 0.0050955]
		[batch 20/20] avg loss: 0.5653839876858814		[learning rate: 0.0050862]
	Learning Rate: 0.00508622
	LOSS [training: 0.5381355546203622 | validation: 0.790696721584947]
	TIME [epoch: 8.5 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6355381155689288		[learning rate: 0.005077]
		[batch 20/20] avg loss: 0.6854420119639529		[learning rate: 0.0050678]
	Learning Rate: 0.00506776
	LOSS [training: 0.660490063766441 | validation: 0.23650658453197626]
	TIME [epoch: 8.41 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5905272832758148		[learning rate: 0.0050586]
		[batch 20/20] avg loss: 0.4899763508211312		[learning rate: 0.0050494]
	Learning Rate: 0.00504937
	LOSS [training: 0.540251817048473 | validation: 0.4084947169318938]
	TIME [epoch: 8.48 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5361575179928723		[learning rate: 0.0050402]
		[batch 20/20] avg loss: 0.45405427313307944		[learning rate: 0.005031]
	Learning Rate: 0.00503104
	LOSS [training: 0.4951058955629758 | validation: 0.676748347995545]
	TIME [epoch: 8.46 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4833860445499097		[learning rate: 0.0050219]
		[batch 20/20] avg loss: 0.6691150744836918		[learning rate: 0.0050128]
	Learning Rate: 0.00501278
	LOSS [training: 0.5762505595168008 | validation: 0.4121315265066083]
	TIME [epoch: 8.52 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49545115467805784		[learning rate: 0.0050037]
		[batch 20/20] avg loss: 0.7395530588771869		[learning rate: 0.0049946]
	Learning Rate: 0.00499459
	LOSS [training: 0.6175021067776223 | validation: 0.38985339596812535]
	TIME [epoch: 8.54 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5687368967493958		[learning rate: 0.0049855]
		[batch 20/20] avg loss: 0.4347080664941617		[learning rate: 0.0049765]
	Learning Rate: 0.00497647
	LOSS [training: 0.5017224816217787 | validation: 0.4058816424454666]
	TIME [epoch: 8.44 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4939055793738586		[learning rate: 0.0049674]
		[batch 20/20] avg loss: 0.4230075697883887		[learning rate: 0.0049584]
	Learning Rate: 0.00495841
	LOSS [training: 0.45845657458112365 | validation: 0.8179754719888758]
	TIME [epoch: 8.43 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.655197948532813		[learning rate: 0.0049494]
		[batch 20/20] avg loss: 0.4070619710247677		[learning rate: 0.0049404]
	Learning Rate: 0.00494041
	LOSS [training: 0.5311299597787904 | validation: 0.35429961804023824]
	TIME [epoch: 8.7 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39601261260607973		[learning rate: 0.0049314]
		[batch 20/20] avg loss: 0.4247396607210462		[learning rate: 0.0049225]
	Learning Rate: 0.00492248
	LOSS [training: 0.410376136663563 | validation: 0.2096316484190496]
	TIME [epoch: 8.57 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44439057481252575		[learning rate: 0.0049135]
		[batch 20/20] avg loss: 0.42133843610891447		[learning rate: 0.0049046]
	Learning Rate: 0.00490462
	LOSS [training: 0.43286450546072014 | validation: 0.3898540255813669]
	TIME [epoch: 8.47 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38717814039191956		[learning rate: 0.0048957]
		[batch 20/20] avg loss: 0.4998022474905457		[learning rate: 0.0048868]
	Learning Rate: 0.00488682
	LOSS [training: 0.44349019394123257 | validation: 0.18825708047588705]
	TIME [epoch: 8.53 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3555989319837867		[learning rate: 0.0048779]
		[batch 20/20] avg loss: 0.3634284487782375		[learning rate: 0.0048691]
	Learning Rate: 0.00486909
	LOSS [training: 0.3595136903810121 | validation: 0.261308183117016]
	TIME [epoch: 8.52 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3620277946539706		[learning rate: 0.0048602]
		[batch 20/20] avg loss: 0.3298569696316726		[learning rate: 0.0048514]
	Learning Rate: 0.00485141
	LOSS [training: 0.34594238214282164 | validation: 0.20324034891752066]
	TIME [epoch: 8.46 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32950659628999446		[learning rate: 0.0048426]
		[batch 20/20] avg loss: 0.3912695910157463		[learning rate: 0.0048338]
	Learning Rate: 0.00483381
	LOSS [training: 0.36038809365287033 | validation: 0.47175658497442463]
	TIME [epoch: 8.6 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46814358585540033		[learning rate: 0.004825]
		[batch 20/20] avg loss: 0.4559643040664202		[learning rate: 0.0048163]
	Learning Rate: 0.00481627
	LOSS [training: 0.4620539449609103 | validation: 0.13622847263446913]
	TIME [epoch: 8.53 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4929104866152404		[learning rate: 0.0048075]
		[batch 20/20] avg loss: 0.38789640939319175		[learning rate: 0.0047988]
	Learning Rate: 0.00479879
	LOSS [training: 0.440403448004216 | validation: 0.42473287938111004]
	TIME [epoch: 8.41 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28978940736405384		[learning rate: 0.0047901]
		[batch 20/20] avg loss: 0.3254284284348309		[learning rate: 0.0047814]
	Learning Rate: 0.00478137
	LOSS [training: 0.3076089178994424 | validation: 0.2710792980095915]
	TIME [epoch: 8.62 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3804134335536678		[learning rate: 0.0047727]
		[batch 20/20] avg loss: 0.6080993294904244		[learning rate: 0.004764]
	Learning Rate: 0.00476402
	LOSS [training: 0.4942563815220461 | validation: 0.2474481438879329]
	TIME [epoch: 8.39 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39766602659061473		[learning rate: 0.0047554]
		[batch 20/20] avg loss: 0.29892830097692985		[learning rate: 0.0047467]
	Learning Rate: 0.00474673
	LOSS [training: 0.3482971637837723 | validation: 0.12472074687825581]
	TIME [epoch: 8.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4067725827783171		[learning rate: 0.0047381]
		[batch 20/20] avg loss: 0.3923670838561423		[learning rate: 0.0047295]
	Learning Rate: 0.00472951
	LOSS [training: 0.3995698333172297 | validation: 0.3335531167572959]
	TIME [epoch: 8.51 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3560093533497105		[learning rate: 0.0047209]
		[batch 20/20] avg loss: 0.3245827599361032		[learning rate: 0.0047123]
	Learning Rate: 0.00471234
	LOSS [training: 0.3402960566429068 | validation: 0.6918916249107793]
	TIME [epoch: 8.54 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3119615978272101		[learning rate: 0.0047038]
		[batch 20/20] avg loss: 0.3981379310897033		[learning rate: 0.0046952]
	Learning Rate: 0.00469524
	LOSS [training: 0.35504976445845676 | validation: 0.6601240689437453]
	TIME [epoch: 8.5 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4598954155796508		[learning rate: 0.0046867]
		[batch 20/20] avg loss: 0.4283819236387111		[learning rate: 0.0046782]
	Learning Rate: 0.0046782
	LOSS [training: 0.444138669609181 | validation: 0.22074738131682292]
	TIME [epoch: 8.38 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4943197680283986		[learning rate: 0.0046697]
		[batch 20/20] avg loss: 0.40708103529182693		[learning rate: 0.0046612]
	Learning Rate: 0.00466122
	LOSS [training: 0.45070040166011277 | validation: 1.0040840403741782]
	TIME [epoch: 8.62 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45637442371558395		[learning rate: 0.0046528]
		[batch 20/20] avg loss: 0.29644585185081673		[learning rate: 0.0046443]
	Learning Rate: 0.00464431
	LOSS [training: 0.3764101377832004 | validation: 0.23784818906396893]
	TIME [epoch: 8.57 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37988422035608443		[learning rate: 0.0046359]
		[batch 20/20] avg loss: 0.3522534590806677		[learning rate: 0.0046275]
	Learning Rate: 0.00462745
	LOSS [training: 0.36606883971837606 | validation: 0.566638076021409]
	TIME [epoch: 8.49 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4101742997334131		[learning rate: 0.004619]
		[batch 20/20] avg loss: 0.33875115445499404		[learning rate: 0.0046107]
	Learning Rate: 0.00461066
	LOSS [training: 0.3744627270942037 | validation: 0.41047344237590094]
	TIME [epoch: 8.35 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29712965169672445		[learning rate: 0.0046023]
		[batch 20/20] avg loss: 0.38108877754685644		[learning rate: 0.0045939]
	Learning Rate: 0.00459393
	LOSS [training: 0.33910921462179044 | validation: 0.3607851773538855]
	TIME [epoch: 8.38 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39930983151922206		[learning rate: 0.0045856]
		[batch 20/20] avg loss: 0.37066621439096636		[learning rate: 0.0045773]
	Learning Rate: 0.00457726
	LOSS [training: 0.38498802295509427 | validation: 0.39318695221600997]
	TIME [epoch: 8.51 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31070719176187694		[learning rate: 0.0045689]
		[batch 20/20] avg loss: 0.26866956358598404		[learning rate: 0.0045606]
	Learning Rate: 0.00456065
	LOSS [training: 0.2896883776739304 | validation: 0.31594586637512756]
	TIME [epoch: 8.56 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3823769957767852		[learning rate: 0.0045524]
		[batch 20/20] avg loss: 0.29202289014574007		[learning rate: 0.0045441]
	Learning Rate: 0.00454409
	LOSS [training: 0.33719994296126266 | validation: 0.5644601831823719]
	TIME [epoch: 8.44 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30283889517391993		[learning rate: 0.0045358]
		[batch 20/20] avg loss: 0.276009588108265		[learning rate: 0.0045276]
	Learning Rate: 0.0045276
	LOSS [training: 0.2894242416410925 | validation: 0.26441030636151425]
	TIME [epoch: 8.49 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3267733810368271		[learning rate: 0.0045194]
		[batch 20/20] avg loss: 0.3171995216971248		[learning rate: 0.0045112]
	Learning Rate: 0.00451117
	LOSS [training: 0.321986451366976 | validation: 0.39699345946571807]
	TIME [epoch: 8.49 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36103660545290867		[learning rate: 0.004503]
		[batch 20/20] avg loss: 0.3570266644159899		[learning rate: 0.0044948]
	Learning Rate: 0.0044948
	LOSS [training: 0.3590316349344493 | validation: 0.15766870359438173]
	TIME [epoch: 8.48 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24724106631855425		[learning rate: 0.0044866]
		[batch 20/20] avg loss: 0.38807378288664024		[learning rate: 0.0044785]
	Learning Rate: 0.00447849
	LOSS [training: 0.31765742460259727 | validation: 0.39571215318526454]
	TIME [epoch: 8.39 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33828787780235975		[learning rate: 0.0044704]
		[batch 20/20] avg loss: 0.4039941986195249		[learning rate: 0.0044622]
	Learning Rate: 0.00446224
	LOSS [training: 0.37114103821094235 | validation: 0.29428703779816634]
	TIME [epoch: 8.37 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3498515688333324		[learning rate: 0.0044541]
		[batch 20/20] avg loss: 0.3656923719566204		[learning rate: 0.004446]
	Learning Rate: 0.00444604
	LOSS [training: 0.3577719703949764 | validation: 0.24195665900263258]
	TIME [epoch: 8.46 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28472472926666165		[learning rate: 0.004438]
		[batch 20/20] avg loss: 0.32147175058985356		[learning rate: 0.0044299]
	Learning Rate: 0.00442991
	LOSS [training: 0.3030982399282576 | validation: 0.7556329008237509]
	TIME [epoch: 8.55 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6901626492978735		[learning rate: 0.0044219]
		[batch 20/20] avg loss: 0.301649942538001		[learning rate: 0.0044138]
	Learning Rate: 0.00441383
	LOSS [training: 0.49590629591793717 | validation: 0.1828819958922862]
	TIME [epoch: 8.57 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28194009963426464		[learning rate: 0.0044058]
		[batch 20/20] avg loss: 0.44061931285956113		[learning rate: 0.0043978]
	Learning Rate: 0.00439781
	LOSS [training: 0.36127970624691286 | validation: 0.22407699998642314]
	TIME [epoch: 8.46 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37730110037166265		[learning rate: 0.0043898]
		[batch 20/20] avg loss: 0.3066052238972346		[learning rate: 0.0043819]
	Learning Rate: 0.00438185
	LOSS [training: 0.3419531621344486 | validation: 0.2682750020504191]
	TIME [epoch: 8.49 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5103924572874353		[learning rate: 0.0043739]
		[batch 20/20] avg loss: 0.4198922512663536		[learning rate: 0.004366]
	Learning Rate: 0.00436595
	LOSS [training: 0.4651423542768945 | validation: 0.4085601876987601]
	TIME [epoch: 8.39 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4515231166907091		[learning rate: 0.004358]
		[batch 20/20] avg loss: 0.3519273132501339		[learning rate: 0.0043501]
	Learning Rate: 0.00435011
	LOSS [training: 0.4017252149704215 | validation: 0.1942695465618678]
	TIME [epoch: 8.32 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3410452994660106		[learning rate: 0.0043422]
		[batch 20/20] avg loss: 0.4344201396790961		[learning rate: 0.0043343]
	Learning Rate: 0.00433432
	LOSS [training: 0.3877327195725534 | validation: 0.16139610166390744]
	TIME [epoch: 8.48 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3394681991003177		[learning rate: 0.0043264]
		[batch 20/20] avg loss: 0.5253556524091902		[learning rate: 0.0043186]
	Learning Rate: 0.00431859
	LOSS [training: 0.43241192575475396 | validation: 0.2323063484568652]
	TIME [epoch: 8.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4099577210158637		[learning rate: 0.0043107]
		[batch 20/20] avg loss: 0.2614273814313931		[learning rate: 0.0043029]
	Learning Rate: 0.00430292
	LOSS [training: 0.33569255122362834 | validation: 0.7828718401658236]
	TIME [epoch: 8.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35271263912320877		[learning rate: 0.0042951]
		[batch 20/20] avg loss: 0.369102919723418		[learning rate: 0.0042873]
	Learning Rate: 0.0042873
	LOSS [training: 0.3609077794233134 | validation: 0.4305846641273941]
	TIME [epoch: 8.46 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41771238523809756		[learning rate: 0.0042795]
		[batch 20/20] avg loss: 0.3246749489784172		[learning rate: 0.0042717]
	Learning Rate: 0.00427174
	LOSS [training: 0.37119366710825735 | validation: 0.20375448024943976]
	TIME [epoch: 8.35 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39114441555778495		[learning rate: 0.004264]
		[batch 20/20] avg loss: 0.42780099045961634		[learning rate: 0.0042562]
	Learning Rate: 0.00425624
	LOSS [training: 0.40947270300870064 | validation: 0.40513188778814546]
	TIME [epoch: 8.38 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3045756550794283		[learning rate: 0.0042485]
		[batch 20/20] avg loss: 0.2649456064174386		[learning rate: 0.0042408]
	Learning Rate: 0.0042408
	LOSS [training: 0.28476063074843344 | validation: 0.3306012368878168]
	TIME [epoch: 8.39 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3446917944649289		[learning rate: 0.0042331]
		[batch 20/20] avg loss: 0.30514306827936355		[learning rate: 0.0042254]
	Learning Rate: 0.00422541
	LOSS [training: 0.3249174313721461 | validation: 0.31723134677258813]
	TIME [epoch: 8.35 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33150698287670105		[learning rate: 0.0042177]
		[batch 20/20] avg loss: 0.27767336599102566		[learning rate: 0.0042101]
	Learning Rate: 0.00421007
	LOSS [training: 0.3045901744338634 | validation: 0.48684435151634187]
	TIME [epoch: 8.32 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3737051330520943		[learning rate: 0.0042024]
		[batch 20/20] avg loss: 0.2839531536758482		[learning rate: 0.0041948]
	Learning Rate: 0.00419479
	LOSS [training: 0.3288291433639713 | validation: 0.6231393427488082]
	TIME [epoch: 8.35 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4166788298186453		[learning rate: 0.0041872]
		[batch 20/20] avg loss: 0.31914981964011635		[learning rate: 0.0041796]
	Learning Rate: 0.00417957
	LOSS [training: 0.36791432472938074 | validation: 0.40086315378551285]
	TIME [epoch: 8.36 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3500481595683106		[learning rate: 0.004172]
		[batch 20/20] avg loss: 0.3304112598102506		[learning rate: 0.0041644]
	Learning Rate: 0.0041644
	LOSS [training: 0.3402297096892806 | validation: 0.2942581144883145]
	TIME [epoch: 8.34 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2464026950780341		[learning rate: 0.0041568]
		[batch 20/20] avg loss: 0.45721608423225657		[learning rate: 0.0041493]
	Learning Rate: 0.00414929
	LOSS [training: 0.3518093896551453 | validation: 0.5161312532464442]
	TIME [epoch: 8.36 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3273588935209415		[learning rate: 0.0041418]
		[batch 20/20] avg loss: 0.28774773640194684		[learning rate: 0.0041342]
	Learning Rate: 0.00413423
	LOSS [training: 0.30755331496144417 | validation: 0.18063582660603608]
	TIME [epoch: 8.37 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31304706262388826		[learning rate: 0.0041267]
		[batch 20/20] avg loss: 0.3886572057635284		[learning rate: 0.0041192]
	Learning Rate: 0.00411923
	LOSS [training: 0.3508521341937083 | validation: 0.23593115704839246]
	TIME [epoch: 8.4 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3552099704379047		[learning rate: 0.0041117]
		[batch 20/20] avg loss: 0.6085795979876714		[learning rate: 0.0041043]
	Learning Rate: 0.00410428
	LOSS [training: 0.481894784212788 | validation: 0.772548838035243]
	TIME [epoch: 8.38 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35498111070291444		[learning rate: 0.0040968]
		[batch 20/20] avg loss: 0.2772828954151847		[learning rate: 0.0040894]
	Learning Rate: 0.00408938
	LOSS [training: 0.31613200305904954 | validation: 0.35450907434154955]
	TIME [epoch: 8.42 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3813314981213712		[learning rate: 0.004082]
		[batch 20/20] avg loss: 0.33330375540569324		[learning rate: 0.0040745]
	Learning Rate: 0.00407454
	LOSS [training: 0.3573176267635322 | validation: 0.2858643853264244]
	TIME [epoch: 8.41 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.304528014760057		[learning rate: 0.0040671]
		[batch 20/20] avg loss: 0.2595608430366608		[learning rate: 0.0040598]
	Learning Rate: 0.00405976
	LOSS [training: 0.28204442889835885 | validation: 0.6082023461889235]
	TIME [epoch: 8.39 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.473562921793017		[learning rate: 0.0040524]
		[batch 20/20] avg loss: 0.28654400604760016		[learning rate: 0.004045]
	Learning Rate: 0.00404502
	LOSS [training: 0.38005346392030853 | validation: 0.22068985974853883]
	TIME [epoch: 8.41 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3951653188666552		[learning rate: 0.0040377]
		[batch 20/20] avg loss: 0.25458871192198523		[learning rate: 0.0040303]
	Learning Rate: 0.00403034
	LOSS [training: 0.32487701539432023 | validation: 0.5009989451247685]
	TIME [epoch: 8.42 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29637068922508586		[learning rate: 0.004023]
		[batch 20/20] avg loss: 0.5105050825008686		[learning rate: 0.0040157]
	Learning Rate: 0.00401572
	LOSS [training: 0.4034378858629773 | validation: 0.2259987577400474]
	TIME [epoch: 8.38 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30342287682790703		[learning rate: 0.0040084]
		[batch 20/20] avg loss: 0.41230499087427336		[learning rate: 0.0040011]
	Learning Rate: 0.00400114
	LOSS [training: 0.3578639338510902 | validation: 0.26181384258704654]
	TIME [epoch: 8.31 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27031981795976895		[learning rate: 0.0039939]
		[batch 20/20] avg loss: 0.4968204376061737		[learning rate: 0.0039866]
	Learning Rate: 0.00398662
	LOSS [training: 0.38357012778297134 | validation: 0.5488251955991885]
	TIME [epoch: 8.28 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3285266772341416		[learning rate: 0.0039794]
		[batch 20/20] avg loss: 0.2810192760857565		[learning rate: 0.0039722]
	Learning Rate: 0.00397216
	LOSS [training: 0.304772976659949 | validation: 0.16096483890372007]
	TIME [epoch: 8.29 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6740349591164879		[learning rate: 0.0039649]
		[batch 20/20] avg loss: 0.2644352390780867		[learning rate: 0.0039577]
	Learning Rate: 0.00395774
	LOSS [training: 0.4692350990972872 | validation: 0.4532221052927748]
	TIME [epoch: 8.4 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25806839735706466		[learning rate: 0.0039506]
		[batch 20/20] avg loss: 0.396797230213194		[learning rate: 0.0039434]
	Learning Rate: 0.00394338
	LOSS [training: 0.32743281378512934 | validation: 0.15345284989982394]
	TIME [epoch: 8.38 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4108414041581991		[learning rate: 0.0039362]
		[batch 20/20] avg loss: 0.29196230630704095		[learning rate: 0.0039291]
	Learning Rate: 0.00392907
	LOSS [training: 0.3514018552326201 | validation: 0.20808605449667428]
	TIME [epoch: 8.33 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26743969350549085		[learning rate: 0.0039219]
		[batch 20/20] avg loss: 0.3400028464225147		[learning rate: 0.0039148]
	Learning Rate: 0.00391481
	LOSS [training: 0.3037212699640028 | validation: 0.2581823343820636]
	TIME [epoch: 8.35 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2768847909517646		[learning rate: 0.0039077]
		[batch 20/20] avg loss: 0.507853706847875		[learning rate: 0.0039006]
	Learning Rate: 0.0039006
	LOSS [training: 0.39236924889981983 | validation: 0.3444828710205192]
	TIME [epoch: 8.39 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36342919146746805		[learning rate: 0.0038935]
		[batch 20/20] avg loss: 0.39255685692745695		[learning rate: 0.0038864]
	Learning Rate: 0.00388645
	LOSS [training: 0.37799302419746256 | validation: 0.22889984397054985]
	TIME [epoch: 8.35 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42124086685219453		[learning rate: 0.0038794]
		[batch 20/20] avg loss: 0.3465025790733136		[learning rate: 0.0038723]
	Learning Rate: 0.00387234
	LOSS [training: 0.38387172296275396 | validation: 0.23960380982205548]
	TIME [epoch: 8.29 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3014608395643576		[learning rate: 0.0038653]
		[batch 20/20] avg loss: 0.37296775595453546		[learning rate: 0.0038583]
	Learning Rate: 0.00385829
	LOSS [training: 0.33721429775944645 | validation: 0.18994338806370148]
	TIME [epoch: 8.27 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29051558824127044		[learning rate: 0.0038513]
		[batch 20/20] avg loss: 0.36166925284008833		[learning rate: 0.0038443]
	Learning Rate: 0.00384429
	LOSS [training: 0.3260924205406794 | validation: 0.13175844337458562]
	TIME [epoch: 8.39 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3625755780858213		[learning rate: 0.0038373]
		[batch 20/20] avg loss: 0.27918653052628994		[learning rate: 0.0038303]
	Learning Rate: 0.00383034
	LOSS [training: 0.32088105430605557 | validation: 0.41596100771036204]
	TIME [epoch: 8.35 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.433855195495635		[learning rate: 0.0038234]
		[batch 20/20] avg loss: 0.3478044579107917		[learning rate: 0.0038164]
	Learning Rate: 0.00381644
	LOSS [training: 0.3908298267032134 | validation: 0.2125057300657812]
	TIME [epoch: 8.36 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3296756877893178		[learning rate: 0.0038095]
		[batch 20/20] avg loss: 0.32033038274507525		[learning rate: 0.0038026]
	Learning Rate: 0.00380258
	LOSS [training: 0.3250030352671965 | validation: 0.32509537376541064]
	TIME [epoch: 8.37 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33674436008090325		[learning rate: 0.0037957]
		[batch 20/20] avg loss: 0.35302968530758744		[learning rate: 0.0037888]
	Learning Rate: 0.00378879
	LOSS [training: 0.34488702269424526 | validation: 0.29643429202229155]
	TIME [epoch: 8.29 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.309492445891911		[learning rate: 0.0037819]
		[batch 20/20] avg loss: 0.24775118948223668		[learning rate: 0.003775]
	Learning Rate: 0.00377504
	LOSS [training: 0.27862181768707384 | validation: 0.3415917382122749]
	TIME [epoch: 8.35 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.466071113990756		[learning rate: 0.0037682]
		[batch 20/20] avg loss: 0.418676068818268		[learning rate: 0.0037613]
	Learning Rate: 0.00376134
	LOSS [training: 0.44237359140451205 | validation: 0.5741441731064976]
	TIME [epoch: 8.31 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3969337829442606		[learning rate: 0.0037545]
		[batch 20/20] avg loss: 0.3742779380120004		[learning rate: 0.0037477]
	Learning Rate: 0.00374769
	LOSS [training: 0.3856058604781305 | validation: 0.26662446858300504]
	TIME [epoch: 8.31 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2702024300115896		[learning rate: 0.0037409]
		[batch 20/20] avg loss: 0.30290036516786994		[learning rate: 0.0037341]
	Learning Rate: 0.00373408
	LOSS [training: 0.28655139758972975 | validation: 0.5509158245959155]
	TIME [epoch: 8.29 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36371561788389806		[learning rate: 0.0037273]
		[batch 20/20] avg loss: 0.32609038620701286		[learning rate: 0.0037205]
	Learning Rate: 0.00372053
	LOSS [training: 0.3449030020454555 | validation: 0.34281156546285446]
	TIME [epoch: 8.31 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28028405177761695		[learning rate: 0.0037138]
		[batch 20/20] avg loss: 0.4118352700034925		[learning rate: 0.003707]
	Learning Rate: 0.00370703
	LOSS [training: 0.34605966089055473 | validation: 0.5692940268002438]
	TIME [epoch: 8.33 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40670028089359783		[learning rate: 0.0037003]
		[batch 20/20] avg loss: 0.3406140162964119		[learning rate: 0.0036936]
	Learning Rate: 0.00369358
	LOSS [training: 0.3736571485950049 | validation: 0.1589697249917238]
	TIME [epoch: 8.39 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32033738649929544		[learning rate: 0.0036869]
		[batch 20/20] avg loss: 0.27538401004489893		[learning rate: 0.0036802]
	Learning Rate: 0.00368017
	LOSS [training: 0.2978606982720972 | validation: 0.2614343308002234]
	TIME [epoch: 8.4 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33816043818084174		[learning rate: 0.0036735]
		[batch 20/20] avg loss: 0.3360529099868852		[learning rate: 0.0036668]
	Learning Rate: 0.00366682
	LOSS [training: 0.33710667408386347 | validation: 0.5395188095238074]
	TIME [epoch: 8.45 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30928008608044305		[learning rate: 0.0036602]
		[batch 20/20] avg loss: 0.31935493820941185		[learning rate: 0.0036535]
	Learning Rate: 0.00365351
	LOSS [training: 0.3143175121449274 | validation: 0.28241785784737566]
	TIME [epoch: 8.44 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34888656134326274		[learning rate: 0.0036469]
		[batch 20/20] avg loss: 0.2667462610808715		[learning rate: 0.0036403]
	Learning Rate: 0.00364025
	LOSS [training: 0.3078164112120672 | validation: 0.21828077979863802]
	TIME [epoch: 8.43 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32935490637093745		[learning rate: 0.0036336]
		[batch 20/20] avg loss: 0.36964754144685297		[learning rate: 0.003627]
	Learning Rate: 0.00362704
	LOSS [training: 0.3495012239088952 | validation: 0.1301960187095299]
	TIME [epoch: 8.43 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42212074931712945		[learning rate: 0.0036205]
		[batch 20/20] avg loss: 0.2150855462778269		[learning rate: 0.0036139]
	Learning Rate: 0.00361388
	LOSS [training: 0.3186031477974782 | validation: 0.09468524545815642]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33401890512800525		[learning rate: 0.0036073]
		[batch 20/20] avg loss: 0.2692361526011452		[learning rate: 0.0036008]
	Learning Rate: 0.00360076
	LOSS [training: 0.3016275288645752 | validation: 0.3906377372299573]
	TIME [epoch: 8.4 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3053983316770297		[learning rate: 0.0035942]
		[batch 20/20] avg loss: 0.4177993321522603		[learning rate: 0.0035877]
	Learning Rate: 0.0035877
	LOSS [training: 0.361598831914645 | validation: 0.15451893258312072]
	TIME [epoch: 8.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3119124934039753		[learning rate: 0.0035812]
		[batch 20/20] avg loss: 0.3708994011898279		[learning rate: 0.0035747]
	Learning Rate: 0.00357468
	LOSS [training: 0.34140594729690166 | validation: 0.21245936887018974]
	TIME [epoch: 8.27 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2529187672376058		[learning rate: 0.0035682]
		[batch 20/20] avg loss: 0.3111927425565883		[learning rate: 0.0035617]
	Learning Rate: 0.0035617
	LOSS [training: 0.282055754897097 | validation: 0.25340645479527457]
	TIME [epoch: 8.3 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31013142046216163		[learning rate: 0.0035552]
		[batch 20/20] avg loss: 0.3688211474584541		[learning rate: 0.0035488]
	Learning Rate: 0.00354878
	LOSS [training: 0.3394762839603079 | validation: 0.26107044357502085]
	TIME [epoch: 8.36 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3596623031178877		[learning rate: 0.0035423]
		[batch 20/20] avg loss: 0.2884717065916851		[learning rate: 0.0035359]
	Learning Rate: 0.0035359
	LOSS [training: 0.32406700485478646 | validation: 0.23648702878605513]
	TIME [epoch: 8.38 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.587028942847996		[learning rate: 0.0035295]
		[batch 20/20] avg loss: 0.42242349046674493		[learning rate: 0.0035231]
	Learning Rate: 0.00352307
	LOSS [training: 0.5047262166573705 | validation: 0.15660042886013167]
	TIME [epoch: 8.38 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37626738639883744		[learning rate: 0.0035167]
		[batch 20/20] avg loss: 0.34825110819367067		[learning rate: 0.0035103]
	Learning Rate: 0.00351028
	LOSS [training: 0.3622592472962541 | validation: 0.34346134205033885]
	TIME [epoch: 8.37 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3753470591285054		[learning rate: 0.0035039]
		[batch 20/20] avg loss: 0.40474938236405367		[learning rate: 0.0034975]
	Learning Rate: 0.00349754
	LOSS [training: 0.3900482207462795 | validation: 0.22158280223893378]
	TIME [epoch: 8.37 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.357722350113992		[learning rate: 0.0034912]
		[batch 20/20] avg loss: 0.2819835112893428		[learning rate: 0.0034849]
	Learning Rate: 0.00348485
	LOSS [training: 0.3198529307016675 | validation: 0.40207802071705034]
	TIME [epoch: 8.37 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3457235698197705		[learning rate: 0.0034785]
		[batch 20/20] avg loss: 0.31625985356712627		[learning rate: 0.0034722]
	Learning Rate: 0.0034722
	LOSS [training: 0.3309917116934483 | validation: 0.17999783190835567]
	TIME [epoch: 8.41 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3708257740090816		[learning rate: 0.0034659]
		[batch 20/20] avg loss: 0.332959506683399		[learning rate: 0.0034596]
	Learning Rate: 0.0034596
	LOSS [training: 0.35189264034624035 | validation: 0.17235955386695062]
	TIME [epoch: 8.4 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2477824942517335		[learning rate: 0.0034533]
		[batch 20/20] avg loss: 0.2730704807518984		[learning rate: 0.003447]
	Learning Rate: 0.00344705
	LOSS [training: 0.26042648750181596 | validation: 0.1557873166409775]
	TIME [epoch: 8.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3202067767727482		[learning rate: 0.0034408]
		[batch 20/20] avg loss: 0.23430613599495298		[learning rate: 0.0034345]
	Learning Rate: 0.00343454
	LOSS [training: 0.27725645638385055 | validation: 0.16642238671279616]
	TIME [epoch: 8.3 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2958252626946064		[learning rate: 0.0034283]
		[batch 20/20] avg loss: 0.3194714812383098		[learning rate: 0.0034221]
	Learning Rate: 0.00342207
	LOSS [training: 0.3076483719664581 | validation: 0.4401538128675583]
	TIME [epoch: 8.28 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33559460164401017		[learning rate: 0.0034159]
		[batch 20/20] avg loss: 0.3418953275685456		[learning rate: 0.0034097]
	Learning Rate: 0.00340966
	LOSS [training: 0.33874496460627784 | validation: 0.1440746890103939]
	TIME [epoch: 8.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5493108274558794		[learning rate: 0.0034035]
		[batch 20/20] avg loss: 0.2852205570377248		[learning rate: 0.0033973]
	Learning Rate: 0.00339728
	LOSS [training: 0.4172656922468022 | validation: 0.23999534859795554]
	TIME [epoch: 8.37 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3136766889617147		[learning rate: 0.0033911]
		[batch 20/20] avg loss: 0.3090197809000352		[learning rate: 0.003385]
	Learning Rate: 0.00338495
	LOSS [training: 0.31134823493087493 | validation: 0.3605214085234964]
	TIME [epoch: 8.33 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4070915300017452		[learning rate: 0.0033788]
		[batch 20/20] avg loss: 0.3793483945547956		[learning rate: 0.0033727]
	Learning Rate: 0.00337267
	LOSS [training: 0.39321996227827033 | validation: 0.5054114199977477]
	TIME [epoch: 8.32 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4109172017627822		[learning rate: 0.0033665]
		[batch 20/20] avg loss: 0.21903899071207977		[learning rate: 0.0033604]
	Learning Rate: 0.00336043
	LOSS [training: 0.31497809623743095 | validation: 0.32666539908765857]
	TIME [epoch: 8.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3400986448588645		[learning rate: 0.0033543]
		[batch 20/20] avg loss: 0.23149604729730971		[learning rate: 0.0033482]
	Learning Rate: 0.00334823
	LOSS [training: 0.2857973460780871 | validation: 0.3927349378694629]
	TIME [epoch: 8.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37478371824343587		[learning rate: 0.0033422]
		[batch 20/20] avg loss: 0.4153919364170046		[learning rate: 0.0033361]
	Learning Rate: 0.00333608
	LOSS [training: 0.39508782733022024 | validation: 0.1834611562488631]
	TIME [epoch: 8.37 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2744717199453385		[learning rate: 0.00333]
		[batch 20/20] avg loss: 0.2932904331636169		[learning rate: 0.003324]
	Learning Rate: 0.00332398
	LOSS [training: 0.28388107655447764 | validation: 0.27504638053390057]
	TIME [epoch: 8.29 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3673996965584939		[learning rate: 0.0033179]
		[batch 20/20] avg loss: 0.22348580835346601		[learning rate: 0.0033119]
	Learning Rate: 0.00331191
	LOSS [training: 0.29544275245598 | validation: 0.4656458887551107]
	TIME [epoch: 8.35 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3329140742499555		[learning rate: 0.0033059]
		[batch 20/20] avg loss: 0.2856688584458352		[learning rate: 0.0032999]
	Learning Rate: 0.00329989
	LOSS [training: 0.30929146634789534 | validation: 0.24161770040595892]
	TIME [epoch: 8.37 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2548108157985927		[learning rate: 0.0032939]
		[batch 20/20] avg loss: 0.2255929350220232		[learning rate: 0.0032879]
	Learning Rate: 0.00328792
	LOSS [training: 0.24020187541030796 | validation: 0.2904784819857575]
	TIME [epoch: 8.4 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3720475095731315		[learning rate: 0.0032819]
		[batch 20/20] avg loss: 0.34973255417611165		[learning rate: 0.003276]
	Learning Rate: 0.00327599
	LOSS [training: 0.36089003187462154 | validation: 0.27073476432277555]
	TIME [epoch: 8.35 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2418475762131731		[learning rate: 0.00327]
		[batch 20/20] avg loss: 0.32114657932039004		[learning rate: 0.0032641]
	Learning Rate: 0.0032641
	LOSS [training: 0.2814970777667817 | validation: 0.13394069058505775]
	TIME [epoch: 8.34 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3951348437803485		[learning rate: 0.0032582]
		[batch 20/20] avg loss: 0.22134115730235243		[learning rate: 0.0032523]
	Learning Rate: 0.00325225
	LOSS [training: 0.3082380005413504 | validation: 0.19443930178993613]
	TIME [epoch: 8.39 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3912674344624233		[learning rate: 0.0032463]
		[batch 20/20] avg loss: 0.2728801871817915		[learning rate: 0.0032404]
	Learning Rate: 0.00324045
	LOSS [training: 0.33207381082210746 | validation: 0.545399722860824]
	TIME [epoch: 8.4 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4429125727214947		[learning rate: 0.0032346]
		[batch 20/20] avg loss: 0.3255344184151282		[learning rate: 0.0032287]
	Learning Rate: 0.00322869
	LOSS [training: 0.3842234955683114 | validation: 0.23582282169110785]
	TIME [epoch: 8.43 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44604186554870007		[learning rate: 0.0032228]
		[batch 20/20] avg loss: 0.21897176242275082		[learning rate: 0.003217]
	Learning Rate: 0.00321697
	LOSS [training: 0.33250681398572546 | validation: 0.5407748522667843]
	TIME [epoch: 8.39 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4195111718077699		[learning rate: 0.0032111]
		[batch 20/20] avg loss: 0.27411336139937725		[learning rate: 0.0032053]
	Learning Rate: 0.0032053
	LOSS [training: 0.3468122666035736 | validation: 0.701992812897529]
	TIME [epoch: 8.34 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3131125155063821		[learning rate: 0.0031995]
		[batch 20/20] avg loss: 0.25611457295880646		[learning rate: 0.0031937]
	Learning Rate: 0.00319367
	LOSS [training: 0.2846135442325943 | validation: 0.1968749595939121]
	TIME [epoch: 8.34 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23780046092509766		[learning rate: 0.0031879]
		[batch 20/20] avg loss: 0.31000133805375674		[learning rate: 0.0031821]
	Learning Rate: 0.00318208
	LOSS [training: 0.2739008994894272 | validation: 0.09735193046826253]
	TIME [epoch: 8.33 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4309170510173109		[learning rate: 0.0031763]
		[batch 20/20] avg loss: 0.23630244908573755		[learning rate: 0.0031705]
	Learning Rate: 0.00317053
	LOSS [training: 0.33360975005152416 | validation: 0.08583082438084948]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24291433650935187		[learning rate: 0.0031648]
		[batch 20/20] avg loss: 0.2295851960921822		[learning rate: 0.003159]
	Learning Rate: 0.00315902
	LOSS [training: 0.23624976630076708 | validation: 0.16138332793240046]
	TIME [epoch: 8.32 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2797105852029966		[learning rate: 0.0031533]
		[batch 20/20] avg loss: 0.2737216940778389		[learning rate: 0.0031476]
	Learning Rate: 0.00314756
	LOSS [training: 0.2767161396404177 | validation: 0.23757552799778964]
	TIME [epoch: 8.41 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24957957494824737		[learning rate: 0.0031418]
		[batch 20/20] avg loss: 0.27605313818098515		[learning rate: 0.0031361]
	Learning Rate: 0.00313613
	LOSS [training: 0.2628163565646163 | validation: 0.15034882659946674]
	TIME [epoch: 8.48 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3123564694510004		[learning rate: 0.0031304]
		[batch 20/20] avg loss: 0.2811514021820257		[learning rate: 0.0031248]
	Learning Rate: 0.00312475
	LOSS [training: 0.29675393581651305 | validation: 0.09464447662410708]
	TIME [epoch: 8.48 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33094666190323113		[learning rate: 0.0031191]
		[batch 20/20] avg loss: 0.2696772024807009		[learning rate: 0.0031134]
	Learning Rate: 0.00311341
	LOSS [training: 0.30031193219196606 | validation: 0.15706442584606015]
	TIME [epoch: 8.49 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30290754323437363		[learning rate: 0.0031078]
		[batch 20/20] avg loss: 0.30050680723441736		[learning rate: 0.0031021]
	Learning Rate: 0.00310212
	LOSS [training: 0.3017071752343955 | validation: 0.17150859420955472]
	TIME [epoch: 8.48 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2471332840040465		[learning rate: 0.0030965]
		[batch 20/20] avg loss: 0.30864816833324304		[learning rate: 0.0030909]
	Learning Rate: 0.00309086
	LOSS [training: 0.2778907261686448 | validation: 0.12536738999482477]
	TIME [epoch: 8.48 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3224679790685585		[learning rate: 0.0030852]
		[batch 20/20] avg loss: 0.21796604731333816		[learning rate: 0.0030796]
	Learning Rate: 0.00307964
	LOSS [training: 0.2702170131909483 | validation: 0.1623301082833974]
	TIME [epoch: 8.51 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24797054719662018		[learning rate: 0.003074]
		[batch 20/20] avg loss: 0.22627161212292632		[learning rate: 0.0030685]
	Learning Rate: 0.00306846
	LOSS [training: 0.23712107965977322 | validation: 0.12318178120966616]
	TIME [epoch: 8.47 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23518169382953302		[learning rate: 0.0030629]
		[batch 20/20] avg loss: 0.31066536884032303		[learning rate: 0.0030573]
	Learning Rate: 0.00305733
	LOSS [training: 0.272923531334928 | validation: 0.22772373925469397]
	TIME [epoch: 8.47 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2481100542334474		[learning rate: 0.0030518]
		[batch 20/20] avg loss: 0.3197970526198707		[learning rate: 0.0030462]
	Learning Rate: 0.00304623
	LOSS [training: 0.28395355342665907 | validation: 0.3292275671293007]
	TIME [epoch: 8.46 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2768841019631847		[learning rate: 0.0030407]
		[batch 20/20] avg loss: 0.2691221381821658		[learning rate: 0.0030352]
	Learning Rate: 0.00303518
	LOSS [training: 0.2730031200726753 | validation: 0.37385812114942696]
	TIME [epoch: 8.39 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2611574485106025		[learning rate: 0.0030297]
		[batch 20/20] avg loss: 0.29713554747558374		[learning rate: 0.0030242]
	Learning Rate: 0.00302416
	LOSS [training: 0.27914649799309316 | validation: 0.5576719265147788]
	TIME [epoch: 8.47 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30674235048034887		[learning rate: 0.0030187]
		[batch 20/20] avg loss: 0.3013410304976989		[learning rate: 0.0030132]
	Learning Rate: 0.00301319
	LOSS [training: 0.3040416904890239 | validation: 0.2686568112925789]
	TIME [epoch: 8.39 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2674439431139486		[learning rate: 0.0030077]
		[batch 20/20] avg loss: 0.2665211985573248		[learning rate: 0.0030023]
	Learning Rate: 0.00300225
	LOSS [training: 0.2669825708356367 | validation: 0.13346284253723845]
	TIME [epoch: 8.4 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20656121067154526		[learning rate: 0.0029968]
		[batch 20/20] avg loss: 0.20841926547488768		[learning rate: 0.0029914]
	Learning Rate: 0.00299136
	LOSS [training: 0.20749023807321648 | validation: 0.2549379065937982]
	TIME [epoch: 8.38 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1889978038502731		[learning rate: 0.0029859]
		[batch 20/20] avg loss: 0.19370346845044828		[learning rate: 0.0029805]
	Learning Rate: 0.0029805
	LOSS [training: 0.1913506361503607 | validation: 0.09918289897177031]
	TIME [epoch: 8.32 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24240677591649637		[learning rate: 0.0029751]
		[batch 20/20] avg loss: 0.29981078481211726		[learning rate: 0.0029697]
	Learning Rate: 0.00296969
	LOSS [training: 0.2711087803643068 | validation: 0.19810485786922158]
	TIME [epoch: 8.27 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2739539083499954		[learning rate: 0.0029643]
		[batch 20/20] avg loss: 0.3519866595323141		[learning rate: 0.0029589]
	Learning Rate: 0.00295891
	LOSS [training: 0.3129702839411548 | validation: 0.1776419467809922]
	TIME [epoch: 8.26 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30651791697731445		[learning rate: 0.0029535]
		[batch 20/20] avg loss: 0.17867710276369392		[learning rate: 0.0029482]
	Learning Rate: 0.00294817
	LOSS [training: 0.24259750987050416 | validation: 0.22947622888940666]
	TIME [epoch: 8.28 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2702139080507581		[learning rate: 0.0029428]
		[batch 20/20] avg loss: 0.31737647213846676		[learning rate: 0.0029375]
	Learning Rate: 0.00293747
	LOSS [training: 0.2937951900946124 | validation: 0.35907310678364457]
	TIME [epoch: 8.29 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30383316214181694		[learning rate: 0.0029321]
		[batch 20/20] avg loss: 0.4890721952824483		[learning rate: 0.0029268]
	Learning Rate: 0.00292681
	LOSS [training: 0.39645267871213263 | validation: 0.3899526381696551]
	TIME [epoch: 8.29 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33192682046252076		[learning rate: 0.0029215]
		[batch 20/20] avg loss: 0.2957510062836792		[learning rate: 0.0029162]
	Learning Rate: 0.00291619
	LOSS [training: 0.31383891337309994 | validation: 0.13674406880707762]
	TIME [epoch: 8.28 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37144004953568516		[learning rate: 0.0029109]
		[batch 20/20] avg loss: 0.20268328731170454		[learning rate: 0.0029056]
	Learning Rate: 0.00290561
	LOSS [training: 0.2870616684236949 | validation: 0.1993812501397777]
	TIME [epoch: 8.27 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2677151875007847		[learning rate: 0.0029003]
		[batch 20/20] avg loss: 0.359278147348936		[learning rate: 0.0028951]
	Learning Rate: 0.00289506
	LOSS [training: 0.3134966674248604 | validation: 0.3160299185447159]
	TIME [epoch: 8.3 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28105854238838635		[learning rate: 0.0028898]
		[batch 20/20] avg loss: 0.21515233265064654		[learning rate: 0.0028846]
	Learning Rate: 0.00288456
	LOSS [training: 0.24810543751951647 | validation: 0.18222204855891155]
	TIME [epoch: 8.4 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19833246494540674		[learning rate: 0.0028793]
		[batch 20/20] avg loss: 0.31766546360460335		[learning rate: 0.0028741]
	Learning Rate: 0.00287409
	LOSS [training: 0.257998964275005 | validation: 0.5172673322554069]
	TIME [epoch: 8.46 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2682909529085476		[learning rate: 0.0028689]
		[batch 20/20] avg loss: 0.2575095875847123		[learning rate: 0.0028637]
	Learning Rate: 0.00286366
	LOSS [training: 0.26290027024663 | validation: 0.3737496908913666]
	TIME [epoch: 8.34 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2948714747130956		[learning rate: 0.0028585]
		[batch 20/20] avg loss: 0.3207680419318784		[learning rate: 0.0028533]
	Learning Rate: 0.00285326
	LOSS [training: 0.307819758322487 | validation: 0.23590094833782624]
	TIME [epoch: 8.37 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3497954282647156		[learning rate: 0.0028481]
		[batch 20/20] avg loss: 0.24723937035412363		[learning rate: 0.0028429]
	Learning Rate: 0.00284291
	LOSS [training: 0.29851739930941956 | validation: 0.18295723295814115]
	TIME [epoch: 8.46 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35029331031442296		[learning rate: 0.0028377]
		[batch 20/20] avg loss: 0.21651178953325986		[learning rate: 0.0028326]
	Learning Rate: 0.00283259
	LOSS [training: 0.2834025499238414 | validation: 0.15322388785441904]
	TIME [epoch: 8.45 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48303315226565297		[learning rate: 0.0028274]
		[batch 20/20] avg loss: 0.3025481195147526		[learning rate: 0.0028223]
	Learning Rate: 0.00282231
	LOSS [training: 0.3927906358902028 | validation: 0.35970358894876786]
	TIME [epoch: 8.37 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3057139256301499		[learning rate: 0.0028172]
		[batch 20/20] avg loss: 0.24330113140443857		[learning rate: 0.0028121]
	Learning Rate: 0.00281207
	LOSS [training: 0.2745075285172942 | validation: 0.1260440698171539]
	TIME [epoch: 8.44 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20645287861388653		[learning rate: 0.002807]
		[batch 20/20] avg loss: 0.3113874088846044		[learning rate: 0.0028019]
	Learning Rate: 0.00280187
	LOSS [training: 0.25892014374924555 | validation: 0.16077541013055208]
	TIME [epoch: 8.52 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2269778962266782		[learning rate: 0.0027968]
		[batch 20/20] avg loss: 0.20977485054313175		[learning rate: 0.0027917]
	Learning Rate: 0.0027917
	LOSS [training: 0.21837637338490495 | validation: 0.5432920477812171]
	TIME [epoch: 8.5 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3466229448356546		[learning rate: 0.0027866]
		[batch 20/20] avg loss: 0.337766891157438		[learning rate: 0.0027816]
	Learning Rate: 0.00278157
	LOSS [training: 0.34219491799654633 | validation: 0.12289713551387782]
	TIME [epoch: 8.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22024081908714294		[learning rate: 0.0027765]
		[batch 20/20] avg loss: 0.19767337614701197		[learning rate: 0.0027715]
	Learning Rate: 0.00277147
	LOSS [training: 0.20895709761707743 | validation: 0.21242827704501577]
	TIME [epoch: 8.41 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2774440101173363		[learning rate: 0.0027664]
		[batch 20/20] avg loss: 0.25422576555016546		[learning rate: 0.0027614]
	Learning Rate: 0.00276141
	LOSS [training: 0.26583488783375087 | validation: 0.20727982157789077]
	TIME [epoch: 8.39 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3492294413580326		[learning rate: 0.0027564]
		[batch 20/20] avg loss: 0.23795610781722237		[learning rate: 0.0027514]
	Learning Rate: 0.00275139
	LOSS [training: 0.2935927745876275 | validation: 0.16346559354766102]
	TIME [epoch: 8.41 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3275028953440018		[learning rate: 0.0027464]
		[batch 20/20] avg loss: 0.30969238962725126		[learning rate: 0.0027414]
	Learning Rate: 0.00274141
	LOSS [training: 0.31859764248562644 | validation: 0.5305661997144278]
	TIME [epoch: 8.4 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2386563515881411		[learning rate: 0.0027364]
		[batch 20/20] avg loss: 0.2737286805413921		[learning rate: 0.0027315]
	Learning Rate: 0.00273146
	LOSS [training: 0.25619251606476656 | validation: 0.19226942375038544]
	TIME [epoch: 8.42 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3895598756010688		[learning rate: 0.0027265]
		[batch 20/20] avg loss: 0.3406895230595162		[learning rate: 0.0027215]
	Learning Rate: 0.00272155
	LOSS [training: 0.36512469933029246 | validation: 0.10768422963062838]
	TIME [epoch: 8.39 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26873353067604694		[learning rate: 0.0027166]
		[batch 20/20] avg loss: 0.2521571214656754		[learning rate: 0.0027117]
	Learning Rate: 0.00271167
	LOSS [training: 0.2604453260708612 | validation: 0.288213568671573]
	TIME [epoch: 8.41 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32687756829362347		[learning rate: 0.0027067]
		[batch 20/20] avg loss: 0.1977364974916176		[learning rate: 0.0027018]
	Learning Rate: 0.00270183
	LOSS [training: 0.2623070328926206 | validation: 0.6025343382871211]
	TIME [epoch: 8.44 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2564156569102342		[learning rate: 0.0026969]
		[batch 20/20] avg loss: 0.2595205773819561		[learning rate: 0.002692]
	Learning Rate: 0.00269202
	LOSS [training: 0.2579681171460952 | validation: 0.19774066696601392]
	TIME [epoch: 8.43 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28740776827575076		[learning rate: 0.0026871]
		[batch 20/20] avg loss: 0.2606286315999631		[learning rate: 0.0026823]
	Learning Rate: 0.00268225
	LOSS [training: 0.2740181999378569 | validation: 0.6497649729490599]
	TIME [epoch: 8.45 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30294052446309533		[learning rate: 0.0026774]
		[batch 20/20] avg loss: 0.30862752252139236		[learning rate: 0.0026725]
	Learning Rate: 0.00267252
	LOSS [training: 0.3057840234922439 | validation: 0.1694674769387972]
	TIME [epoch: 8.51 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2884043459866209		[learning rate: 0.0026677]
		[batch 20/20] avg loss: 0.22045572160420246		[learning rate: 0.0026628]
	Learning Rate: 0.00266282
	LOSS [training: 0.2544300337954117 | validation: 0.17610911527895223]
	TIME [epoch: 8.43 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47091950952931316		[learning rate: 0.002658]
		[batch 20/20] avg loss: 0.23832125644479857		[learning rate: 0.0026532]
	Learning Rate: 0.00265316
	LOSS [training: 0.3546203829870559 | validation: 0.1878484952143695]
	TIME [epoch: 8.44 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20435227766965047		[learning rate: 0.0026483]
		[batch 20/20] avg loss: 0.2526836276085147		[learning rate: 0.0026435]
	Learning Rate: 0.00264353
	LOSS [training: 0.2285179526390826 | validation: 0.12753367575236998]
	TIME [epoch: 8.44 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1777066124315241		[learning rate: 0.0026387]
		[batch 20/20] avg loss: 0.21188155398748237		[learning rate: 0.0026339]
	Learning Rate: 0.00263394
	LOSS [training: 0.19479408320950325 | validation: 0.148307753760629]
	TIME [epoch: 8.47 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2636106245068301		[learning rate: 0.0026292]
		[batch 20/20] avg loss: 0.25704366694347447		[learning rate: 0.0026244]
	Learning Rate: 0.00262438
	LOSS [training: 0.26032714572515225 | validation: 0.17589811590756765]
	TIME [epoch: 8.44 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.290907853940396		[learning rate: 0.0026196]
		[batch 20/20] avg loss: 0.18903549947554288		[learning rate: 0.0026149]
	Learning Rate: 0.00261485
	LOSS [training: 0.23997167670796943 | validation: 0.1404989488834945]
	TIME [epoch: 8.43 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28523847051522055		[learning rate: 0.0026101]
		[batch 20/20] avg loss: 0.2691838718444855		[learning rate: 0.0026054]
	Learning Rate: 0.00260536
	LOSS [training: 0.27721117117985306 | validation: 0.22107811585765674]
	TIME [epoch: 8.42 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1918983167209028		[learning rate: 0.0026006]
		[batch 20/20] avg loss: 0.19549348013550555		[learning rate: 0.0025959]
	Learning Rate: 0.00259591
	LOSS [training: 0.19369589842820414 | validation: 0.12865184887247844]
	TIME [epoch: 8.45 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27867343083133134		[learning rate: 0.0025912]
		[batch 20/20] avg loss: 0.23242546871927322		[learning rate: 0.0025865]
	Learning Rate: 0.00258649
	LOSS [training: 0.25554944977530225 | validation: 0.267066515900019]
	TIME [epoch: 8.45 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26455384028272366		[learning rate: 0.0025818]
		[batch 20/20] avg loss: 0.22534185093897316		[learning rate: 0.0025771]
	Learning Rate: 0.0025771
	LOSS [training: 0.2449478456108484 | validation: 0.6404235714379269]
	TIME [epoch: 8.43 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3319771903524352		[learning rate: 0.0025724]
		[batch 20/20] avg loss: 0.261251739309451		[learning rate: 0.0025677]
	Learning Rate: 0.00256775
	LOSS [training: 0.2966144648309432 | validation: 0.09743310406411393]
	TIME [epoch: 8.41 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30075760593731105		[learning rate: 0.0025631]
		[batch 20/20] avg loss: 0.3913118654178379		[learning rate: 0.0025584]
	Learning Rate: 0.00255843
	LOSS [training: 0.3460347356775745 | validation: 0.1746246160915427]
	TIME [epoch: 8.34 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3400149186704565		[learning rate: 0.0025538]
		[batch 20/20] avg loss: 0.2605316741535581		[learning rate: 0.0025491]
	Learning Rate: 0.00254915
	LOSS [training: 0.3002732964120073 | validation: 0.17464556371142648]
	TIME [epoch: 8.33 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23160287612173414		[learning rate: 0.0025445]
		[batch 20/20] avg loss: 0.21440552654500297		[learning rate: 0.0025399]
	Learning Rate: 0.0025399
	LOSS [training: 0.22300420133336857 | validation: 0.1599754776273038]
	TIME [epoch: 8.32 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2376090673316658		[learning rate: 0.0025353]
		[batch 20/20] avg loss: 0.241822929902545		[learning rate: 0.0025307]
	Learning Rate: 0.00253068
	LOSS [training: 0.23971599861710535 | validation: 0.10408144680911754]
	TIME [epoch: 8.34 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19930285258326205		[learning rate: 0.0025261]
		[batch 20/20] avg loss: 0.18536802945819025		[learning rate: 0.0025215]
	Learning Rate: 0.00252149
	LOSS [training: 0.19233544102072614 | validation: 1.102401760732045]
	TIME [epoch: 8.42 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3074800642168788		[learning rate: 0.0025169]
		[batch 20/20] avg loss: 0.24869192185429675		[learning rate: 0.0025123]
	Learning Rate: 0.00251234
	LOSS [training: 0.2780859930355878 | validation: 0.10084796296179485]
	TIME [epoch: 8.49 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2709202130415468		[learning rate: 0.0025078]
		[batch 20/20] avg loss: 0.1874859770677862		[learning rate: 0.0025032]
	Learning Rate: 0.00250323
	LOSS [training: 0.2292030950546665 | validation: 0.2806831566304344]
	TIME [epoch: 8.43 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19369845892089912		[learning rate: 0.0024987]
		[batch 20/20] avg loss: 0.23811825452201835		[learning rate: 0.0024941]
	Learning Rate: 0.00249414
	LOSS [training: 0.21590835672145875 | validation: 0.13979123261913257]
	TIME [epoch: 8.36 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22757504580629676		[learning rate: 0.0024896]
		[batch 20/20] avg loss: 0.22770913456759612		[learning rate: 0.0024851]
	Learning Rate: 0.00248509
	LOSS [training: 0.2276420901869464 | validation: 0.22068589330409044]
	TIME [epoch: 8.39 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2267682417286176		[learning rate: 0.0024806]
		[batch 20/20] avg loss: 0.20919717590731687		[learning rate: 0.0024761]
	Learning Rate: 0.00247607
	LOSS [training: 0.21798270881796725 | validation: 0.38896423854348083]
	TIME [epoch: 8.4 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2380834640869564		[learning rate: 0.0024716]
		[batch 20/20] avg loss: 0.37944219220063957		[learning rate: 0.0024671]
	Learning Rate: 0.00246709
	LOSS [training: 0.30876282814379796 | validation: 0.148655508868625]
	TIME [epoch: 8.42 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36193367078485694		[learning rate: 0.0024626]
		[batch 20/20] avg loss: 0.15470521219944788		[learning rate: 0.0024581]
	Learning Rate: 0.00245813
	LOSS [training: 0.25831944149215247 | validation: 0.20624474931583328]
	TIME [epoch: 8.4 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30193353323637595		[learning rate: 0.0024537]
		[batch 20/20] avg loss: 0.2509277593251771		[learning rate: 0.0024492]
	Learning Rate: 0.00244921
	LOSS [training: 0.27643064628077646 | validation: 0.1795770091580179]
	TIME [epoch: 8.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20201562729427408		[learning rate: 0.0024448]
		[batch 20/20] avg loss: 0.2783540758328911		[learning rate: 0.0024403]
	Learning Rate: 0.00244032
	LOSS [training: 0.24018485156358263 | validation: 0.1295367256891527]
	TIME [epoch: 8.33 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2630740500106891		[learning rate: 0.0024359]
		[batch 20/20] avg loss: 0.408806516965518		[learning rate: 0.0024315]
	Learning Rate: 0.00243147
	LOSS [training: 0.33594028348810356 | validation: 0.2998381227198811]
	TIME [epoch: 8.34 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22890304957913649		[learning rate: 0.0024271]
		[batch 20/20] avg loss: 0.20590697536502356		[learning rate: 0.0024226]
	Learning Rate: 0.00242264
	LOSS [training: 0.21740501247207997 | validation: 0.14066860851881466]
	TIME [epoch: 8.44 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22121314824750113		[learning rate: 0.0024182]
		[batch 20/20] avg loss: 0.2517675810315447		[learning rate: 0.0024139]
	Learning Rate: 0.00241385
	LOSS [training: 0.2364903646395229 | validation: 0.1618085109430129]
	TIME [epoch: 8.44 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2541721549911073		[learning rate: 0.0024095]
		[batch 20/20] avg loss: 0.22167627524101294		[learning rate: 0.0024051]
	Learning Rate: 0.00240509
	LOSS [training: 0.2379242151160601 | validation: 0.2143407526659679]
	TIME [epoch: 8.42 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2704689924712655		[learning rate: 0.0024007]
		[batch 20/20] avg loss: 0.26761084813480185		[learning rate: 0.0023964]
	Learning Rate: 0.00239636
	LOSS [training: 0.2690399203030337 | validation: 0.15027889991064075]
	TIME [epoch: 8.46 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19980244530090133		[learning rate: 0.002392]
		[batch 20/20] avg loss: 0.18410763154919324		[learning rate: 0.0023877]
	Learning Rate: 0.00238767
	LOSS [training: 0.1919550384250473 | validation: 0.10525950943170663]
	TIME [epoch: 8.46 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27365107900655616		[learning rate: 0.0023833]
		[batch 20/20] avg loss: 0.2959008553701636		[learning rate: 0.002379]
	Learning Rate: 0.002379
	LOSS [training: 0.2847759671883599 | validation: 0.34700819234043223]
	TIME [epoch: 8.46 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2581852327548583		[learning rate: 0.0023747]
		[batch 20/20] avg loss: 0.24432720662413435		[learning rate: 0.0023704]
	Learning Rate: 0.00237037
	LOSS [training: 0.2512562196894963 | validation: 0.2972734801432303]
	TIME [epoch: 8.32 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18439336571974127		[learning rate: 0.0023661]
		[batch 20/20] avg loss: 0.38193656270709486		[learning rate: 0.0023618]
	Learning Rate: 0.00236177
	LOSS [training: 0.2831649642134181 | validation: 0.14667534284956976]
	TIME [epoch: 8.39 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1716988377334798		[learning rate: 0.0023575]
		[batch 20/20] avg loss: 0.2511225837143561		[learning rate: 0.0023532]
	Learning Rate: 0.00235319
	LOSS [training: 0.21141071072391796 | validation: 0.5971911349385337]
	TIME [epoch: 8.39 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2478373779939924		[learning rate: 0.0023489]
		[batch 20/20] avg loss: 0.2246857164454167		[learning rate: 0.0023447]
	Learning Rate: 0.00234465
	LOSS [training: 0.23626154721970463 | validation: 0.18520197973661023]
	TIME [epoch: 8.38 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21553549335847544		[learning rate: 0.0023404]
		[batch 20/20] avg loss: 0.22464078305323207		[learning rate: 0.0023361]
	Learning Rate: 0.00233615
	LOSS [training: 0.22008813820585377 | validation: 0.27012575158857516]
	TIME [epoch: 8.34 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23342762105281487		[learning rate: 0.0023319]
		[batch 20/20] avg loss: 0.22561850867764854		[learning rate: 0.0023277]
	Learning Rate: 0.00232767
	LOSS [training: 0.2295230648652317 | validation: 0.31125999912897273]
	TIME [epoch: 8.36 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25910395517322976		[learning rate: 0.0023234]
		[batch 20/20] avg loss: 0.3381889470365824		[learning rate: 0.0023192]
	Learning Rate: 0.00231922
	LOSS [training: 0.2986464511049061 | validation: 0.2691559499029928]
	TIME [epoch: 8.33 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18269675792826806		[learning rate: 0.002315]
		[batch 20/20] avg loss: 0.22562898856447014		[learning rate: 0.0023108]
	Learning Rate: 0.0023108
	LOSS [training: 0.20416287324636911 | validation: 0.14923321332394618]
	TIME [epoch: 8.31 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21236537987456167		[learning rate: 0.0023066]
		[batch 20/20] avg loss: 0.24769189727170504		[learning rate: 0.0023024]
	Learning Rate: 0.00230242
	LOSS [training: 0.23002863857313333 | validation: 0.13002570126887916]
	TIME [epoch: 8.37 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16362105703073007		[learning rate: 0.0022982]
		[batch 20/20] avg loss: 0.28516204633447284		[learning rate: 0.0022941]
	Learning Rate: 0.00229406
	LOSS [training: 0.22439155168260144 | validation: 0.1541866562318821]
	TIME [epoch: 8.37 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3063940890885103		[learning rate: 0.0022899]
		[batch 20/20] avg loss: 0.2033916868791268		[learning rate: 0.0022857]
	Learning Rate: 0.00228574
	LOSS [training: 0.25489288798381854 | validation: 0.1773460836045055]
	TIME [epoch: 8.46 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22312607308164498		[learning rate: 0.0022816]
		[batch 20/20] avg loss: 0.2964601814956308		[learning rate: 0.0022774]
	Learning Rate: 0.00227744
	LOSS [training: 0.2597931272886379 | validation: 0.14459595090979036]
	TIME [epoch: 8.46 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22040921437571867		[learning rate: 0.0022733]
		[batch 20/20] avg loss: 0.27276721297065576		[learning rate: 0.0022692]
	Learning Rate: 0.00226918
	LOSS [training: 0.24658821367318717 | validation: 0.3042997211295068]
	TIME [epoch: 8.47 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23662793917453845		[learning rate: 0.0022651]
		[batch 20/20] avg loss: 0.2374019104382598		[learning rate: 0.0022609]
	Learning Rate: 0.00226094
	LOSS [training: 0.2370149248063992 | validation: 0.5555847429152531]
	TIME [epoch: 8.49 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28738923537808625		[learning rate: 0.0022568]
		[batch 20/20] avg loss: 0.23501358958209334		[learning rate: 0.0022527]
	Learning Rate: 0.00225274
	LOSS [training: 0.2612014124800899 | validation: 0.18923464773320017]
	TIME [epoch: 8.51 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21037813384694398		[learning rate: 0.0022486]
		[batch 20/20] avg loss: 0.23841631521294473		[learning rate: 0.0022446]
	Learning Rate: 0.00224456
	LOSS [training: 0.22439722452994432 | validation: 0.18206839162090316]
	TIME [epoch: 8.49 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24207367543813593		[learning rate: 0.0022405]
		[batch 20/20] avg loss: 0.209188217520312		[learning rate: 0.0022364]
	Learning Rate: 0.00223642
	LOSS [training: 0.22563094647922394 | validation: 0.32517541285860213]
	TIME [epoch: 8.49 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2592724020537117		[learning rate: 0.0022324]
		[batch 20/20] avg loss: 0.29080700583052166		[learning rate: 0.0022283]
	Learning Rate: 0.0022283
	LOSS [training: 0.27503970394211663 | validation: 0.1017777131778372]
	TIME [epoch: 8.48 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21952788337261903		[learning rate: 0.0022243]
		[batch 20/20] avg loss: 0.21389769909985018		[learning rate: 0.0022202]
	Learning Rate: 0.00222021
	LOSS [training: 0.21671279123623455 | validation: 0.16551236623759782]
	TIME [epoch: 8.51 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20412565952283143		[learning rate: 0.0022162]
		[batch 20/20] avg loss: 0.23272882426589586		[learning rate: 0.0022122]
	Learning Rate: 0.00221216
	LOSS [training: 0.21842724189436366 | validation: 0.16471972864824008]
	TIME [epoch: 14.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16842579685950404		[learning rate: 0.0022081]
		[batch 20/20] avg loss: 0.23043108760655023		[learning rate: 0.0022041]
	Learning Rate: 0.00220413
	LOSS [training: 0.1994284422330271 | validation: 0.4424111552304374]
	TIME [epoch: 8.42 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2031229513045162		[learning rate: 0.0022001]
		[batch 20/20] avg loss: 0.3073699293287565		[learning rate: 0.0021961]
	Learning Rate: 0.00219613
	LOSS [training: 0.2552464403166364 | validation: 0.2738904577776153]
	TIME [epoch: 8.43 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2387899128283597		[learning rate: 0.0021921]
		[batch 20/20] avg loss: 0.2716769529450984		[learning rate: 0.0021882]
	Learning Rate: 0.00218816
	LOSS [training: 0.2552334328867291 | validation: 0.24139085637747168]
	TIME [epoch: 8.39 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2763943201946411		[learning rate: 0.0021842]
		[batch 20/20] avg loss: 0.21904924774776893		[learning rate: 0.0021802]
	Learning Rate: 0.00218022
	LOSS [training: 0.24772178397120498 | validation: 0.08204972258789192]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_519.pth
	Model improved!!!
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2726748611968951		[learning rate: 0.0021763]
		[batch 20/20] avg loss: 0.16775264145250346		[learning rate: 0.0021723]
	Learning Rate: 0.00217231
	LOSS [training: 0.22021375132469928 | validation: 0.17881354225817409]
	TIME [epoch: 8.36 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21954772391199215		[learning rate: 0.0021684]
		[batch 20/20] avg loss: 0.1774713838703614		[learning rate: 0.0021644]
	Learning Rate: 0.00216442
	LOSS [training: 0.1985095538911768 | validation: 0.15216228178504368]
	TIME [epoch: 8.33 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22171925026237807		[learning rate: 0.0021605]
		[batch 20/20] avg loss: 0.17243905270781887		[learning rate: 0.0021566]
	Learning Rate: 0.00215657
	LOSS [training: 0.19707915148509852 | validation: 0.16200525675731975]
	TIME [epoch: 8.36 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1921759027627487		[learning rate: 0.0021527]
		[batch 20/20] avg loss: 0.22969971787132196		[learning rate: 0.0021487]
	Learning Rate: 0.00214874
	LOSS [training: 0.21093781031703532 | validation: 0.16199324270681834]
	TIME [epoch: 8.4 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18627895869239774		[learning rate: 0.0021448]
		[batch 20/20] avg loss: 0.23796857691866183		[learning rate: 0.0021409]
	Learning Rate: 0.00214094
	LOSS [training: 0.2121237678055298 | validation: 0.07540811648766121]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_524.pth
	Model improved!!!
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18047143623291245		[learning rate: 0.0021371]
		[batch 20/20] avg loss: 0.22183628235783126		[learning rate: 0.0021332]
	Learning Rate: 0.00213317
	LOSS [training: 0.2011538592953718 | validation: 0.16196909773747145]
	TIME [epoch: 8.43 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20187450116656808		[learning rate: 0.0021293]
		[batch 20/20] avg loss: 0.1781009424419761		[learning rate: 0.0021254]
	Learning Rate: 0.00212543
	LOSS [training: 0.18998772180427204 | validation: 0.08829006540796944]
	TIME [epoch: 8.41 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32394703460289825		[learning rate: 0.0021216]
		[batch 20/20] avg loss: 0.23297394809459573		[learning rate: 0.0021177]
	Learning Rate: 0.00211772
	LOSS [training: 0.27846049134874706 | validation: 0.18943804417247956]
	TIME [epoch: 8.38 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1705348467342022		[learning rate: 0.0021139]
		[batch 20/20] avg loss: 0.17792804348535546		[learning rate: 0.00211]
	Learning Rate: 0.00211003
	LOSS [training: 0.1742314451097788 | validation: 0.15922244272542904]
	TIME [epoch: 8.34 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1691756566758869		[learning rate: 0.0021062]
		[batch 20/20] avg loss: 0.21255630372559692		[learning rate: 0.0021024]
	Learning Rate: 0.00210238
	LOSS [training: 0.19086598020074189 | validation: 0.15850166590879894]
	TIME [epoch: 8.34 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3130237316012335		[learning rate: 0.0020986]
		[batch 20/20] avg loss: 0.16225393932550775		[learning rate: 0.0020947]
	Learning Rate: 0.00209475
	LOSS [training: 0.23763883546337067 | validation: 0.19645857637053832]
	TIME [epoch: 8.3 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2223176378301706		[learning rate: 0.0020909]
		[batch 20/20] avg loss: 0.19257212004847016		[learning rate: 0.0020871]
	Learning Rate: 0.00208714
	LOSS [training: 0.20744487893932034 | validation: 0.1046791055849861]
	TIME [epoch: 8.39 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25338012382726754		[learning rate: 0.0020834]
		[batch 20/20] avg loss: 0.22418402305343688		[learning rate: 0.0020796]
	Learning Rate: 0.00207957
	LOSS [training: 0.23878207344035215 | validation: 0.11264240762681195]
	TIME [epoch: 8.38 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1990674666500003		[learning rate: 0.0020758]
		[batch 20/20] avg loss: 0.272912810513672		[learning rate: 0.002072]
	Learning Rate: 0.00207202
	LOSS [training: 0.23599013858183615 | validation: 0.17034489545600567]
	TIME [epoch: 8.39 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2761546482806816		[learning rate: 0.0020683]
		[batch 20/20] avg loss: 0.2796945659274471		[learning rate: 0.0020645]
	Learning Rate: 0.0020645
	LOSS [training: 0.2779246071040643 | validation: 0.09480114791893705]
	TIME [epoch: 8.39 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21681130586172057		[learning rate: 0.0020608]
		[batch 20/20] avg loss: 0.19731054520575322		[learning rate: 0.002057]
	Learning Rate: 0.00205701
	LOSS [training: 0.20706092553373687 | validation: 0.14062544683420786]
	TIME [epoch: 8.37 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1716633657510765		[learning rate: 0.0020533]
		[batch 20/20] avg loss: 0.2696294769612958		[learning rate: 0.0020495]
	Learning Rate: 0.00204955
	LOSS [training: 0.22064642135618612 | validation: 0.17594235467158903]
	TIME [epoch: 8.42 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23886057040049286		[learning rate: 0.0020458]
		[batch 20/20] avg loss: 0.18168748408275315		[learning rate: 0.0020421]
	Learning Rate: 0.00204211
	LOSS [training: 0.21027402724162303 | validation: 0.5106038756642803]
	TIME [epoch: 8.43 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2951332962691529		[learning rate: 0.0020384]
		[batch 20/20] avg loss: 0.16143303523864289		[learning rate: 0.0020347]
	Learning Rate: 0.0020347
	LOSS [training: 0.22828316575389787 | validation: 0.20853429931147177]
	TIME [epoch: 8.41 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3489657181748975		[learning rate: 0.002031]
		[batch 20/20] avg loss: 0.16515424535653941		[learning rate: 0.0020273]
	Learning Rate: 0.00202731
	LOSS [training: 0.2570599817657184 | validation: 0.17222265404187637]
	TIME [epoch: 8.41 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1890033444664953		[learning rate: 0.0020236]
		[batch 20/20] avg loss: 0.20218340250660244		[learning rate: 0.00202]
	Learning Rate: 0.00201996
	LOSS [training: 0.19559337348654887 | validation: 0.25243637342978764]
	TIME [epoch: 8.42 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21660541015922452		[learning rate: 0.0020163]
		[batch 20/20] avg loss: 0.16751068997972735		[learning rate: 0.0020126]
	Learning Rate: 0.00201263
	LOSS [training: 0.1920580500694759 | validation: 0.11425857902393305]
	TIME [epoch: 8.42 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27995012546011777		[learning rate: 0.002009]
		[batch 20/20] avg loss: 0.22623219019062243		[learning rate: 0.0020053]
	Learning Rate: 0.00200532
	LOSS [training: 0.2530911578253701 | validation: 0.0776418914254429]
	TIME [epoch: 8.42 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20065446775885004		[learning rate: 0.0020017]
		[batch 20/20] avg loss: 0.239901507764987		[learning rate: 0.001998]
	Learning Rate: 0.00199805
	LOSS [training: 0.22027798776191848 | validation: 0.09809115489736966]
	TIME [epoch: 8.4 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16365529160954634		[learning rate: 0.0019944]
		[batch 20/20] avg loss: 0.15031529510630254		[learning rate: 0.0019908]
	Learning Rate: 0.00199079
	LOSS [training: 0.1569852933579244 | validation: 0.1074908524165765]
	TIME [epoch: 8.37 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1940895862688396		[learning rate: 0.0019872]
		[batch 20/20] avg loss: 0.22771968982445903		[learning rate: 0.0019836]
	Learning Rate: 0.00198357
	LOSS [training: 0.21090463804664933 | validation: 0.09857524621486316]
	TIME [epoch: 8.42 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14351451777515115		[learning rate: 0.00198]
		[batch 20/20] avg loss: 0.23617880058304475		[learning rate: 0.0019764]
	Learning Rate: 0.00197637
	LOSS [training: 0.18984665917909793 | validation: 0.2900403631848283]
	TIME [epoch: 8.4 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2677617886413869		[learning rate: 0.0019728]
		[batch 20/20] avg loss: 0.17596002409789963		[learning rate: 0.0019692]
	Learning Rate: 0.0019692
	LOSS [training: 0.2218609063696432 | validation: 0.14116758991352749]
	TIME [epoch: 8.35 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1555194417925184		[learning rate: 0.0019656]
		[batch 20/20] avg loss: 0.23197305010228889		[learning rate: 0.0019621]
	Learning Rate: 0.00196205
	LOSS [training: 0.19374624594740364 | validation: 0.15854152875595595]
	TIME [epoch: 8.39 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2370255645710529		[learning rate: 0.0019585]
		[batch 20/20] avg loss: 0.14274764713692525		[learning rate: 0.0019549]
	Learning Rate: 0.00195493
	LOSS [training: 0.1898866058539891 | validation: 0.147924917957671]
	TIME [epoch: 8.41 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15531339437139507		[learning rate: 0.0019514]
		[batch 20/20] avg loss: 0.17430618846908602		[learning rate: 0.0019478]
	Learning Rate: 0.00194784
	LOSS [training: 0.16480979142024055 | validation: 0.11402201073315733]
	TIME [epoch: 8.37 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12882991553461215		[learning rate: 0.0019443]
		[batch 20/20] avg loss: 0.22013808979303245		[learning rate: 0.0019408]
	Learning Rate: 0.00194077
	LOSS [training: 0.1744840026638223 | validation: 0.11387951110302542]
	TIME [epoch: 8.42 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.173800693693322		[learning rate: 0.0019372]
		[batch 20/20] avg loss: 0.1839265316420048		[learning rate: 0.0019337]
	Learning Rate: 0.00193373
	LOSS [training: 0.17886361266766337 | validation: 0.20092216668502352]
	TIME [epoch: 8.44 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2052940670210103		[learning rate: 0.0019302]
		[batch 20/20] avg loss: 0.1732156013865614		[learning rate: 0.0019267]
	Learning Rate: 0.00192671
	LOSS [training: 0.1892548342037858 | validation: 0.1716899562484375]
	TIME [epoch: 8.4 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11853393952292954		[learning rate: 0.0019232]
		[batch 20/20] avg loss: 0.20369539052691307		[learning rate: 0.0019197]
	Learning Rate: 0.00191972
	LOSS [training: 0.16111466502492133 | validation: 0.08861197960544459]
	TIME [epoch: 8.45 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20149635196350518		[learning rate: 0.0019162]
		[batch 20/20] avg loss: 0.16068873730690228		[learning rate: 0.0019127]
	Learning Rate: 0.00191275
	LOSS [training: 0.18109254463520374 | validation: 0.09679519981279104]
	TIME [epoch: 8.42 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12975541480194436		[learning rate: 0.0019093]
		[batch 20/20] avg loss: 0.30089576916544714		[learning rate: 0.0019058]
	Learning Rate: 0.00190581
	LOSS [training: 0.21532559198369575 | validation: 0.28354034835032704]
	TIME [epoch: 8.39 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20680709953831627		[learning rate: 0.0019023]
		[batch 20/20] avg loss: 0.1664289037963741		[learning rate: 0.0018989]
	Learning Rate: 0.00189889
	LOSS [training: 0.18661800166734516 | validation: 0.13876561275860846]
	TIME [epoch: 8.51 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1724533195624022		[learning rate: 0.0018954]
		[batch 20/20] avg loss: 0.19814198207022712		[learning rate: 0.001892]
	Learning Rate: 0.001892
	LOSS [training: 0.18529765081631466 | validation: 0.12661532174351586]
	TIME [epoch: 8.47 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13289857187190246		[learning rate: 0.0018886]
		[batch 20/20] avg loss: 0.2256023311595759		[learning rate: 0.0018851]
	Learning Rate: 0.00188513
	LOSS [training: 0.1792504515157392 | validation: 0.05833465026662249]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_559.pth
	Model improved!!!
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1204133791086693		[learning rate: 0.0018817]
		[batch 20/20] avg loss: 0.2813624951213025		[learning rate: 0.0018783]
	Learning Rate: 0.00187829
	LOSS [training: 0.20088793711498587 | validation: 0.20659222257863416]
	TIME [epoch: 8.38 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14107917020337327		[learning rate: 0.0018749]
		[batch 20/20] avg loss: 0.15761093542455576		[learning rate: 0.0018715]
	Learning Rate: 0.00187148
	LOSS [training: 0.14934505281396449 | validation: 0.3592637652442052]
	TIME [epoch: 8.36 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17108509272218625		[learning rate: 0.0018681]
		[batch 20/20] avg loss: 0.16012707151956448		[learning rate: 0.0018647]
	Learning Rate: 0.00186468
	LOSS [training: 0.16560608212087538 | validation: 0.18040063960833624]
	TIME [epoch: 8.43 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14647004223409985		[learning rate: 0.0018613]
		[batch 20/20] avg loss: 0.14830962486425192		[learning rate: 0.0018579]
	Learning Rate: 0.00185792
	LOSS [training: 0.14738983354917584 | validation: 0.11289068028926558]
	TIME [epoch: 8.41 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20212516042093537		[learning rate: 0.0018545]
		[batch 20/20] avg loss: 0.19117794243966713		[learning rate: 0.0018512]
	Learning Rate: 0.00185117
	LOSS [training: 0.19665155143030125 | validation: 0.1568657598626084]
	TIME [epoch: 8.42 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2221471965909955		[learning rate: 0.0018478]
		[batch 20/20] avg loss: 0.15771695890938117		[learning rate: 0.0018445]
	Learning Rate: 0.00184446
	LOSS [training: 0.18993207775018833 | validation: 0.12657958597277452]
	TIME [epoch: 8.39 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18467627442527262		[learning rate: 0.0018411]
		[batch 20/20] avg loss: 0.13329468856294943		[learning rate: 0.0018378]
	Learning Rate: 0.00183776
	LOSS [training: 0.15898548149411104 | validation: 0.201784924764851]
	TIME [epoch: 8.38 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1360077853644318		[learning rate: 0.0018344]
		[batch 20/20] avg loss: 0.13409835551966198		[learning rate: 0.0018311]
	Learning Rate: 0.00183109
	LOSS [training: 0.1350530704420469 | validation: 0.1267463884208017]
	TIME [epoch: 8.41 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13717750103596055		[learning rate: 0.0018278]
		[batch 20/20] avg loss: 0.12764275516396859		[learning rate: 0.0018244]
	Learning Rate: 0.00182445
	LOSS [training: 0.13241012809996458 | validation: 0.13555800813469285]
	TIME [epoch: 8.41 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14307557826135583		[learning rate: 0.0018211]
		[batch 20/20] avg loss: 0.26590480029100677		[learning rate: 0.0018178]
	Learning Rate: 0.00181783
	LOSS [training: 0.2044901892761813 | validation: 0.114278941715277]
	TIME [epoch: 8.39 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16511235138630176		[learning rate: 0.0018145]
		[batch 20/20] avg loss: 0.18782485321382691		[learning rate: 0.0018112]
	Learning Rate: 0.00181123
	LOSS [training: 0.17646860230006434 | validation: 0.1227447605394775]
	TIME [epoch: 8.33 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1827289897305588		[learning rate: 0.0018079]
		[batch 20/20] avg loss: 0.15945174662474815		[learning rate: 0.0018047]
	Learning Rate: 0.00180466
	LOSS [training: 0.1710903681776535 | validation: 0.10779027340531895]
	TIME [epoch: 8.36 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17259855834259508		[learning rate: 0.0018014]
		[batch 20/20] avg loss: 0.21456528654970794		[learning rate: 0.0017981]
	Learning Rate: 0.00179811
	LOSS [training: 0.19358192244615152 | validation: 0.08294719212582799]
	TIME [epoch: 8.39 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23430712488875574		[learning rate: 0.0017948]
		[batch 20/20] avg loss: 0.1897951792229275		[learning rate: 0.0017916]
	Learning Rate: 0.00179158
	LOSS [training: 0.2120511520558416 | validation: 0.21232929314501206]
	TIME [epoch: 8.39 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2013528559855931		[learning rate: 0.0017883]
		[batch 20/20] avg loss: 0.14830391261657158		[learning rate: 0.0017851]
	Learning Rate: 0.00178508
	LOSS [training: 0.17482838430108236 | validation: 0.6039849126924952]
	TIME [epoch: 8.34 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26078220687472864		[learning rate: 0.0017818]
		[batch 20/20] avg loss: 0.17145343034856267		[learning rate: 0.0017786]
	Learning Rate: 0.0017786
	LOSS [training: 0.2161178186116456 | validation: 0.17899685042378774]
	TIME [epoch: 8.37 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11893747502432232		[learning rate: 0.0017754]
		[batch 20/20] avg loss: 0.1458259998266988		[learning rate: 0.0017721]
	Learning Rate: 0.00177215
	LOSS [training: 0.13238173742551057 | validation: 0.21417800848045415]
	TIME [epoch: 8.33 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23369721316938788		[learning rate: 0.0017689]
		[batch 20/20] avg loss: 0.16605505093522305		[learning rate: 0.0017657]
	Learning Rate: 0.00176572
	LOSS [training: 0.19987613205230548 | validation: 0.31796831156768424]
	TIME [epoch: 8.29 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19767039040112366		[learning rate: 0.0017625]
		[batch 20/20] avg loss: 0.19008751497820636		[learning rate: 0.0017593]
	Learning Rate: 0.00175931
	LOSS [training: 0.19387895268966499 | validation: 0.20723233229226723]
	TIME [epoch: 8.29 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15328555795620583		[learning rate: 0.0017561]
		[batch 20/20] avg loss: 0.17043456053466433		[learning rate: 0.0017529]
	Learning Rate: 0.00175292
	LOSS [training: 0.16186005924543506 | validation: 0.2718789850525376]
	TIME [epoch: 8.4 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1471959881150072		[learning rate: 0.0017497]
		[batch 20/20] avg loss: 0.14204372525648343		[learning rate: 0.0017466]
	Learning Rate: 0.00174656
	LOSS [training: 0.1446198566857453 | validation: 0.35860283982870145]
	TIME [epoch: 8.4 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19199966144132036		[learning rate: 0.0017434]
		[batch 20/20] avg loss: 0.18614213568244747		[learning rate: 0.0017402]
	Learning Rate: 0.00174022
	LOSS [training: 0.18907089856188392 | validation: 0.22839793075233467]
	TIME [epoch: 8.46 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17073471330224094		[learning rate: 0.0017371]
		[batch 20/20] avg loss: 0.212377981107015		[learning rate: 0.0017339]
	Learning Rate: 0.00173391
	LOSS [training: 0.19155634720462797 | validation: 0.18613418869330783]
	TIME [epoch: 8.41 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13660790949038298		[learning rate: 0.0017308]
		[batch 20/20] avg loss: 0.14650243295417661		[learning rate: 0.0017276]
	Learning Rate: 0.00172762
	LOSS [training: 0.14155517122227979 | validation: 0.08168282185796176]
	TIME [epoch: 8.42 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13677254376854622		[learning rate: 0.0017245]
		[batch 20/20] avg loss: 0.19811058059265668		[learning rate: 0.0017213]
	Learning Rate: 0.00172135
	LOSS [training: 0.1674415621806014 | validation: 0.265395549980895]
	TIME [epoch: 8.38 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2146603033248657		[learning rate: 0.0017182]
		[batch 20/20] avg loss: 0.11961344809456691		[learning rate: 0.0017151]
	Learning Rate: 0.0017151
	LOSS [training: 0.1671368757097163 | validation: 0.10087983756990841]
	TIME [epoch: 8.3 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14762395311956272		[learning rate: 0.001712]
		[batch 20/20] avg loss: 0.1428694632670909		[learning rate: 0.0017089]
	Learning Rate: 0.00170888
	LOSS [training: 0.1452467081933268 | validation: 0.07549758410318455]
	TIME [epoch: 8.25 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15486580509896716		[learning rate: 0.0017058]
		[batch 20/20] avg loss: 0.16573337780531		[learning rate: 0.0017027]
	Learning Rate: 0.00170267
	LOSS [training: 0.16029959145213862 | validation: 0.08090253880878147]
	TIME [epoch: 8.25 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12655048677008282		[learning rate: 0.0016996]
		[batch 20/20] avg loss: 0.26877562374509		[learning rate: 0.0016965]
	Learning Rate: 0.0016965
	LOSS [training: 0.19766305525758643 | validation: 0.10107887814905352]
	TIME [epoch: 8.28 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13234617536605806		[learning rate: 0.0016934]
		[batch 20/20] avg loss: 0.12806396122785554		[learning rate: 0.0016903]
	Learning Rate: 0.00169034
	LOSS [training: 0.1302050682969568 | validation: 0.07044833284895646]
	TIME [epoch: 8.27 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14057677010303057		[learning rate: 0.0016873]
		[batch 20/20] avg loss: 0.12895318549824192		[learning rate: 0.0016842]
	Learning Rate: 0.0016842
	LOSS [training: 0.1347649778006363 | validation: 0.08540563656387742]
	TIME [epoch: 8.25 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1448470481185334		[learning rate: 0.0016811]
		[batch 20/20] avg loss: 0.16579547892121022		[learning rate: 0.0016781]
	Learning Rate: 0.00167809
	LOSS [training: 0.15532126351987185 | validation: 0.08151799297297473]
	TIME [epoch: 8.28 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1388701833653691		[learning rate: 0.001675]
		[batch 20/20] avg loss: 0.14301325808416526		[learning rate: 0.001672]
	Learning Rate: 0.001672
	LOSS [training: 0.14094172072476724 | validation: 0.30717553770530825]
	TIME [epoch: 8.4 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2696741103973491		[learning rate: 0.001669]
		[batch 20/20] avg loss: 0.14155532931898254		[learning rate: 0.0016659]
	Learning Rate: 0.00166593
	LOSS [training: 0.2056147198581658 | validation: 0.17415122677131387]
	TIME [epoch: 8.3 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20225799524739516		[learning rate: 0.0016629]
		[batch 20/20] avg loss: 0.1432866323834845		[learning rate: 0.0016599]
	Learning Rate: 0.00165989
	LOSS [training: 0.1727723138154398 | validation: 0.09840319006474169]
	TIME [epoch: 8.36 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12114757508967185		[learning rate: 0.0016569]
		[batch 20/20] avg loss: 0.15662819311910375		[learning rate: 0.0016539]
	Learning Rate: 0.00165387
	LOSS [training: 0.1388878841043878 | validation: 0.2057941759153429]
	TIME [epoch: 8.39 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1227281754762207		[learning rate: 0.0016509]
		[batch 20/20] avg loss: 0.10873428425437248		[learning rate: 0.0016479]
	Learning Rate: 0.00164786
	LOSS [training: 0.11573122986529658 | validation: 0.12756926234871177]
	TIME [epoch: 8.34 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15472372057388384		[learning rate: 0.0016449]
		[batch 20/20] avg loss: 0.13355678770766832		[learning rate: 0.0016419]
	Learning Rate: 0.00164188
	LOSS [training: 0.14414025414077608 | validation: 0.24214318431735632]
	TIME [epoch: 8.43 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1619990949661187		[learning rate: 0.0016389]
		[batch 20/20] avg loss: 0.12918731476065104		[learning rate: 0.0016359]
	Learning Rate: 0.00163592
	LOSS [training: 0.14559320486338487 | validation: 0.07031982534355807]
	TIME [epoch: 8.37 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17402819227933683		[learning rate: 0.001633]
		[batch 20/20] avg loss: 0.11548237039525178		[learning rate: 0.00163]
	Learning Rate: 0.00162999
	LOSS [training: 0.14475528133729434 | validation: 0.1623062775935109]
	TIME [epoch: 8.34 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.133011852778641		[learning rate: 0.001627]
		[batch 20/20] avg loss: 0.14022942470105884		[learning rate: 0.0016241]
	Learning Rate: 0.00162407
	LOSS [training: 0.13662063873984992 | validation: 0.19548594988916343]
	TIME [epoch: 8.32 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22176601999439916		[learning rate: 0.0016211]
		[batch 20/20] avg loss: 0.1263965819783342		[learning rate: 0.0016182]
	Learning Rate: 0.00161818
	LOSS [training: 0.17408130098636665 | validation: 0.07176057403544357]
	TIME [epoch: 8.4 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17582037838510373		[learning rate: 0.0016152]
		[batch 20/20] avg loss: 0.1771650792524456		[learning rate: 0.0016123]
	Learning Rate: 0.00161231
	LOSS [training: 0.17649272881877462 | validation: 0.08086182636605972]
	TIME [epoch: 8.39 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09406103322769228		[learning rate: 0.0016094]
		[batch 20/20] avg loss: 0.15203624332451707		[learning rate: 0.0016065]
	Learning Rate: 0.00160645
	LOSS [training: 0.12304863827610465 | validation: 0.2665502616644409]
	TIME [epoch: 8.34 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15869668207892818		[learning rate: 0.0016035]
		[batch 20/20] avg loss: 0.14286090531406442		[learning rate: 0.0016006]
	Learning Rate: 0.00160062
	LOSS [training: 0.1507787936964963 | validation: 0.08983073540616818]
	TIME [epoch: 8.38 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11703540542358756		[learning rate: 0.0015977]
		[batch 20/20] avg loss: 0.1433100456430511		[learning rate: 0.0015948]
	Learning Rate: 0.00159482
	LOSS [training: 0.13017272553331932 | validation: 0.07918474628666247]
	TIME [epoch: 8.42 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1083106630207468		[learning rate: 0.0015919]
		[batch 20/20] avg loss: 0.18231550118672893		[learning rate: 0.001589]
	Learning Rate: 0.00158903
	LOSS [training: 0.1453130821037379 | validation: 0.10840738070608891]
	TIME [epoch: 8.4 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13355183508118512		[learning rate: 0.0015861]
		[batch 20/20] avg loss: 0.14623393587652558		[learning rate: 0.0015833]
	Learning Rate: 0.00158326
	LOSS [training: 0.13989288547885534 | validation: 0.1403898085742625]
	TIME [epoch: 8.49 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13384408420383914		[learning rate: 0.0015804]
		[batch 20/20] avg loss: 0.13614659320448147		[learning rate: 0.0015775]
	Learning Rate: 0.00157752
	LOSS [training: 0.1349953387041603 | validation: 0.07920624956092184]
	TIME [epoch: 8.44 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14486989165665493		[learning rate: 0.0015747]
		[batch 20/20] avg loss: 0.1605432980105414		[learning rate: 0.0015718]
	Learning Rate: 0.00157179
	LOSS [training: 0.15270659483359822 | validation: 0.11140642290323874]
	TIME [epoch: 8.44 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10481553598081075		[learning rate: 0.0015689]
		[batch 20/20] avg loss: 0.19858583774037897		[learning rate: 0.0015661]
	Learning Rate: 0.00156609
	LOSS [training: 0.15170068686059487 | validation: 0.11098159081441202]
	TIME [epoch: 8.42 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13618297184291084		[learning rate: 0.0015632]
		[batch 20/20] avg loss: 0.1672787094005058		[learning rate: 0.0015604]
	Learning Rate: 0.0015604
	LOSS [training: 0.15173084062170833 | validation: 0.12144629937156727]
	TIME [epoch: 8.38 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1723665491007067		[learning rate: 0.0015576]
		[batch 20/20] avg loss: 0.14786011766236012		[learning rate: 0.0015547]
	Learning Rate: 0.00155474
	LOSS [training: 0.1601133333815334 | validation: 0.23822247014853676]
	TIME [epoch: 8.38 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12207791238137608		[learning rate: 0.0015519]
		[batch 20/20] avg loss: 0.196917615227661		[learning rate: 0.0015491]
	Learning Rate: 0.0015491
	LOSS [training: 0.15949776380451852 | validation: 0.0912394715159459]
	TIME [epoch: 8.37 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1149097895512556		[learning rate: 0.0015463]
		[batch 20/20] avg loss: 0.1092671940370999		[learning rate: 0.0015435]
	Learning Rate: 0.00154348
	LOSS [training: 0.11208849179417775 | validation: 0.0702944553388595]
	TIME [epoch: 8.32 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11353003272113223		[learning rate: 0.0015407]
		[batch 20/20] avg loss: 0.13296205157952704		[learning rate: 0.0015379]
	Learning Rate: 0.00153788
	LOSS [training: 0.12324604215032961 | validation: 0.1187014579732167]
	TIME [epoch: 8.29 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12721790522999127		[learning rate: 0.0015351]
		[batch 20/20] avg loss: 0.18836801233965006		[learning rate: 0.0015323]
	Learning Rate: 0.00153229
	LOSS [training: 0.15779295878482066 | validation: 0.10293197338208082]
	TIME [epoch: 8.33 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11691158028018452		[learning rate: 0.0015295]
		[batch 20/20] avg loss: 0.1197957611003411		[learning rate: 0.0015267]
	Learning Rate: 0.00152673
	LOSS [training: 0.11835367069026281 | validation: 0.05535648858030496]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_617.pth
	Model improved!!!
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13078251040392116		[learning rate: 0.001524]
		[batch 20/20] avg loss: 0.14091610076213118		[learning rate: 0.0015212]
	Learning Rate: 0.00152119
	LOSS [training: 0.1358493055830262 | validation: 0.09064071050561619]
	TIME [epoch: 8.38 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1382912418587622		[learning rate: 0.0015184]
		[batch 20/20] avg loss: 0.14557721870949825		[learning rate: 0.0015157]
	Learning Rate: 0.00151567
	LOSS [training: 0.14193423028413024 | validation: 0.2625841130037652]
	TIME [epoch: 8.34 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14096087529950538		[learning rate: 0.0015129]
		[batch 20/20] avg loss: 0.12212013260545189		[learning rate: 0.0015102]
	Learning Rate: 0.00151017
	LOSS [training: 0.13154050395247863 | validation: 0.13048206503610105]
	TIME [epoch: 8.35 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1339517058898284		[learning rate: 0.0015074]
		[batch 20/20] avg loss: 0.15622424107901756		[learning rate: 0.0015047]
	Learning Rate: 0.00150469
	LOSS [training: 0.14508797348442298 | validation: 0.14545962486474678]
	TIME [epoch: 8.36 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12065416223845134		[learning rate: 0.001502]
		[batch 20/20] avg loss: 0.14680543125871656		[learning rate: 0.0014992]
	Learning Rate: 0.00149923
	LOSS [training: 0.13372979674858396 | validation: 0.053285219003341405]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_622.pth
	Model improved!!!
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16333638868867478		[learning rate: 0.0014965]
		[batch 20/20] avg loss: 0.14004777497242477		[learning rate: 0.0014938]
	Learning Rate: 0.00149379
	LOSS [training: 0.15169208183054977 | validation: 0.12091148747337524]
	TIME [epoch: 8.29 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12278104393223215		[learning rate: 0.0014911]
		[batch 20/20] avg loss: 0.17859226115665627		[learning rate: 0.0014884]
	Learning Rate: 0.00148837
	LOSS [training: 0.1506866525444442 | validation: 0.10892609411593673]
	TIME [epoch: 8.33 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1129758505219558		[learning rate: 0.0014857]
		[batch 20/20] avg loss: 0.11803692729256135		[learning rate: 0.001483]
	Learning Rate: 0.00148297
	LOSS [training: 0.11550638890725858 | validation: 0.03328655321984858]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13455153854563892		[learning rate: 0.0014803]
		[batch 20/20] avg loss: 0.11075490048208532		[learning rate: 0.0014776]
	Learning Rate: 0.00147759
	LOSS [training: 0.12265321951386213 | validation: 0.09242672078539789]
	TIME [epoch: 8.37 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15793740761316344		[learning rate: 0.0014749]
		[batch 20/20] avg loss: 0.1358641328622687		[learning rate: 0.0014722]
	Learning Rate: 0.00147222
	LOSS [training: 0.14690077023771606 | validation: 0.2673850152692341]
	TIME [epoch: 8.41 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16775932516968886		[learning rate: 0.0014695]
		[batch 20/20] avg loss: 0.19597288761269077		[learning rate: 0.0014669]
	Learning Rate: 0.00146688
	LOSS [training: 0.1818661063911898 | validation: 0.12072169136818064]
	TIME [epoch: 8.3 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15493582465458539		[learning rate: 0.0014642]
		[batch 20/20] avg loss: 0.14063394736014587		[learning rate: 0.0014616]
	Learning Rate: 0.00146156
	LOSS [training: 0.14778488600736567 | validation: 0.08182347726490793]
	TIME [epoch: 8.31 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14598502152793433		[learning rate: 0.0014589]
		[batch 20/20] avg loss: 0.1370397585022054		[learning rate: 0.0014563]
	Learning Rate: 0.00145625
	LOSS [training: 0.14151239001506985 | validation: 0.09935996793362249]
	TIME [epoch: 8.33 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.144705019256295		[learning rate: 0.0014536]
		[batch 20/20] avg loss: 0.11420176391404135		[learning rate: 0.001451]
	Learning Rate: 0.00145097
	LOSS [training: 0.12945339158516816 | validation: 0.2679663598099812]
	TIME [epoch: 8.33 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16054283547028506		[learning rate: 0.0014483]
		[batch 20/20] avg loss: 0.10524735134139737		[learning rate: 0.0014457]
	Learning Rate: 0.0014457
	LOSS [training: 0.1328950934058412 | validation: 0.3061472641968348]
	TIME [epoch: 8.3 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15723513019720234		[learning rate: 0.0014431]
		[batch 20/20] avg loss: 0.1681721932939067		[learning rate: 0.0014405]
	Learning Rate: 0.00144046
	LOSS [training: 0.16270366174555453 | validation: 0.3329876258279449]
	TIME [epoch: 8.28 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19278284447636387		[learning rate: 0.0014378]
		[batch 20/20] avg loss: 0.17564077626930835		[learning rate: 0.0014352]
	Learning Rate: 0.00143523
	LOSS [training: 0.18421181037283613 | validation: 0.09362350380938349]
	TIME [epoch: 8.32 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25265880371232746		[learning rate: 0.0014326]
		[batch 20/20] avg loss: 0.13761062743973448		[learning rate: 0.00143]
	Learning Rate: 0.00143002
	LOSS [training: 0.19513471557603096 | validation: 0.09829881100859758]
	TIME [epoch: 8.38 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12314072236041598		[learning rate: 0.0014274]
		[batch 20/20] avg loss: 0.13405330855138076		[learning rate: 0.0014248]
	Learning Rate: 0.00142483
	LOSS [training: 0.12859701545589836 | validation: 0.10750343830792333]
	TIME [epoch: 8.43 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17825740035480936		[learning rate: 0.0014222]
		[batch 20/20] avg loss: 0.09232582869650721		[learning rate: 0.0014197]
	Learning Rate: 0.00141966
	LOSS [training: 0.13529161452565827 | validation: 0.08433294285791622]
	TIME [epoch: 8.46 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11616959378867171		[learning rate: 0.0014171]
		[batch 20/20] avg loss: 0.18010284051621386		[learning rate: 0.0014145]
	Learning Rate: 0.00141451
	LOSS [training: 0.1481362171524428 | validation: 0.1029533862955722]
	TIME [epoch: 8.45 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1257706366417425		[learning rate: 0.0014119]
		[batch 20/20] avg loss: 0.13739494002516037		[learning rate: 0.0014094]
	Learning Rate: 0.00140937
	LOSS [training: 0.1315827883334514 | validation: 0.11808216002899943]
	TIME [epoch: 8.44 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11851073567699903		[learning rate: 0.0014068]
		[batch 20/20] avg loss: 0.15331996989247146		[learning rate: 0.0014043]
	Learning Rate: 0.00140426
	LOSS [training: 0.1359153527847353 | validation: 0.18625541477499016]
	TIME [epoch: 8.46 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11691219423706038		[learning rate: 0.0014017]
		[batch 20/20] avg loss: 0.17580556719521723		[learning rate: 0.0013992]
	Learning Rate: 0.00139916
	LOSS [training: 0.14635888071613884 | validation: 0.10785491400629786]
	TIME [epoch: 8.42 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16373037432622262		[learning rate: 0.0013966]
		[batch 20/20] avg loss: 0.1126222925551184		[learning rate: 0.0013941]
	Learning Rate: 0.00139409
	LOSS [training: 0.13817633344067048 | validation: 0.07909246047514264]
	TIME [epoch: 8.36 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18717725239081018		[learning rate: 0.0013916]
		[batch 20/20] avg loss: 0.19217147971786203		[learning rate: 0.001389]
	Learning Rate: 0.00138903
	LOSS [training: 0.18967436605433613 | validation: 0.07671721235837715]
	TIME [epoch: 8.34 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1503636480434199		[learning rate: 0.0013865]
		[batch 20/20] avg loss: 0.11268940470334358		[learning rate: 0.001384]
	Learning Rate: 0.00138399
	LOSS [training: 0.13152652637338172 | validation: 0.17535516118080657]
	TIME [epoch: 8.29 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1194284552052595		[learning rate: 0.0013815]
		[batch 20/20] avg loss: 0.13619637231603401		[learning rate: 0.001379]
	Learning Rate: 0.00137896
	LOSS [training: 0.12781241376064675 | validation: 0.1867339168877169]
	TIME [epoch: 8.31 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13513863895372605		[learning rate: 0.0013765]
		[batch 20/20] avg loss: 0.15160602075070803		[learning rate: 0.001374]
	Learning Rate: 0.00137396
	LOSS [training: 0.14337232985221698 | validation: 0.10577708215519786]
	TIME [epoch: 8.31 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13709118711119234		[learning rate: 0.0013715]
		[batch 20/20] avg loss: 0.1172474272825633		[learning rate: 0.001369]
	Learning Rate: 0.00136897
	LOSS [training: 0.12716930719687783 | validation: 0.057386028497212985]
	TIME [epoch: 8.37 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09884126140823746		[learning rate: 0.0013665]
		[batch 20/20] avg loss: 0.15174578482865678		[learning rate: 0.001364]
	Learning Rate: 0.001364
	LOSS [training: 0.12529352311844713 | validation: 0.11000864101886168]
	TIME [epoch: 8.35 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17853492829966394		[learning rate: 0.0013615]
		[batch 20/20] avg loss: 0.1541466918802014		[learning rate: 0.0013591]
	Learning Rate: 0.00135905
	LOSS [training: 0.16634081008993268 | validation: 0.07831492087849165]
	TIME [epoch: 8.32 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.128646140101684		[learning rate: 0.0013566]
		[batch 20/20] avg loss: 0.13233553252918137		[learning rate: 0.0013541]
	Learning Rate: 0.00135412
	LOSS [training: 0.13049083631543268 | validation: 0.08877302890819602]
	TIME [epoch: 8.34 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18470411967303973		[learning rate: 0.0013517]
		[batch 20/20] avg loss: 0.17940519731670074		[learning rate: 0.0013492]
	Learning Rate: 0.00134921
	LOSS [training: 0.18205465849487026 | validation: 0.07141501249882694]
	TIME [epoch: 8.34 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14081892768219878		[learning rate: 0.0013468]
		[batch 20/20] avg loss: 0.14095097673794355		[learning rate: 0.0013443]
	Learning Rate: 0.00134431
	LOSS [training: 0.1408849522100712 | validation: 0.1958961554554179]
	TIME [epoch: 8.37 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1376879401069035		[learning rate: 0.0013419]
		[batch 20/20] avg loss: 0.15415525214729014		[learning rate: 0.0013394]
	Learning Rate: 0.00133943
	LOSS [training: 0.14592159612709682 | validation: 0.07648372369546629]
	TIME [epoch: 8.4 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10018175251097287		[learning rate: 0.001337]
		[batch 20/20] avg loss: 0.1406931514561783		[learning rate: 0.0013346]
	Learning Rate: 0.00133457
	LOSS [training: 0.12043745198357561 | validation: 0.17489355542260898]
	TIME [epoch: 8.33 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1423922770815074		[learning rate: 0.0013321]
		[batch 20/20] avg loss: 0.12159492939515666		[learning rate: 0.0013297]
	Learning Rate: 0.00132973
	LOSS [training: 0.131993603238332 | validation: 0.5094887308005747]
	TIME [epoch: 8.26 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2017037135776182		[learning rate: 0.0013273]
		[batch 20/20] avg loss: 0.13123947115494788		[learning rate: 0.0013249]
	Learning Rate: 0.0013249
	LOSS [training: 0.16647159236628303 | validation: 0.07570266173050956]
	TIME [epoch: 8.28 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1504987633628509		[learning rate: 0.0013225]
		[batch 20/20] avg loss: 0.1559565509197099		[learning rate: 0.0013201]
	Learning Rate: 0.0013201
	LOSS [training: 0.1532276571412804 | validation: 0.14536129118196425]
	TIME [epoch: 8.35 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11349578953272074		[learning rate: 0.0013177]
		[batch 20/20] avg loss: 0.1276386118485054		[learning rate: 0.0013153]
	Learning Rate: 0.0013153
	LOSS [training: 0.12056720069061304 | validation: 0.09496054329077444]
	TIME [epoch: 8.35 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14079704518514974		[learning rate: 0.0013129]
		[batch 20/20] avg loss: 0.1312557704973764		[learning rate: 0.0013105]
	Learning Rate: 0.00131053
	LOSS [training: 0.13602640784126305 | validation: 0.13518601398079097]
	TIME [epoch: 8.29 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1285057548698053		[learning rate: 0.0013082]
		[batch 20/20] avg loss: 0.13560123758349746		[learning rate: 0.0013058]
	Learning Rate: 0.00130578
	LOSS [training: 0.13205349622665138 | validation: 0.19258011633178154]
	TIME [epoch: 8.32 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11319470840334553		[learning rate: 0.0013034]
		[batch 20/20] avg loss: 0.07867020633264213		[learning rate: 0.001301]
	Learning Rate: 0.00130104
	LOSS [training: 0.09593245736799383 | validation: 0.10435327398510498]
	TIME [epoch: 8.37 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11027074326250919		[learning rate: 0.0012987]
		[batch 20/20] avg loss: 0.11726428274720171		[learning rate: 0.0012963]
	Learning Rate: 0.00129631
	LOSS [training: 0.11376751300485546 | validation: 0.10461028978489705]
	TIME [epoch: 8.36 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13396725332012563		[learning rate: 0.001294]
		[batch 20/20] avg loss: 0.11279659160033124		[learning rate: 0.0012916]
	Learning Rate: 0.00129161
	LOSS [training: 0.12338192246022846 | validation: 0.08084667561871815]
	TIME [epoch: 8.32 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10195037773188842		[learning rate: 0.0012893]
		[batch 20/20] avg loss: 0.12396536524864596		[learning rate: 0.0012869]
	Learning Rate: 0.00128692
	LOSS [training: 0.11295787149026719 | validation: 0.2593255868821398]
	TIME [epoch: 8.3 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19521719715007693		[learning rate: 0.0012846]
		[batch 20/20] avg loss: 0.1688721496238979		[learning rate: 0.0012823]
	Learning Rate: 0.00128225
	LOSS [training: 0.18204467338698743 | validation: 0.10950158994067986]
	TIME [epoch: 8.34 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12764603977898348		[learning rate: 0.0012799]
		[batch 20/20] avg loss: 0.10718980500721834		[learning rate: 0.0012776]
	Learning Rate: 0.0012776
	LOSS [training: 0.1174179223931009 | validation: 0.08830243206508434]
	TIME [epoch: 8.41 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11519925897893475		[learning rate: 0.0012753]
		[batch 20/20] avg loss: 0.16243041458548207		[learning rate: 0.001273]
	Learning Rate: 0.00127296
	LOSS [training: 0.13881483678220843 | validation: 0.10325441157291157]
	TIME [epoch: 8.39 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11846898275899798		[learning rate: 0.0012707]
		[batch 20/20] avg loss: 0.1021519120025874		[learning rate: 0.0012683]
	Learning Rate: 0.00126834
	LOSS [training: 0.11031044738079265 | validation: 0.09075102709767771]
	TIME [epoch: 8.3 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09744862957302804		[learning rate: 0.001266]
		[batch 20/20] avg loss: 0.13323752457013663		[learning rate: 0.0012637]
	Learning Rate: 0.00126374
	LOSS [training: 0.11534307707158235 | validation: 0.0667310123098257]
	TIME [epoch: 8.36 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13987235931286426		[learning rate: 0.0012614]
		[batch 20/20] avg loss: 0.1555856109282829		[learning rate: 0.0012592]
	Learning Rate: 0.00125915
	LOSS [training: 0.14772898512057359 | validation: 0.2704703770354362]
	TIME [epoch: 8.43 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1277711192245753		[learning rate: 0.0012569]
		[batch 20/20] avg loss: 0.24428201241571706		[learning rate: 0.0012546]
	Learning Rate: 0.00125458
	LOSS [training: 0.18602656582014618 | validation: 0.05140999077088783]
	TIME [epoch: 8.4 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12154529897068156		[learning rate: 0.0012523]
		[batch 20/20] avg loss: 0.11102438227780827		[learning rate: 0.00125]
	Learning Rate: 0.00125003
	LOSS [training: 0.1162848406242449 | validation: 0.06925213795531754]
	TIME [epoch: 8.4 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11973213806852898		[learning rate: 0.0012478]
		[batch 20/20] avg loss: 0.13639457577400743		[learning rate: 0.0012455]
	Learning Rate: 0.0012455
	LOSS [training: 0.1280633569212682 | validation: 0.07501241120853765]
	TIME [epoch: 8.39 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1487453895610938		[learning rate: 0.0012432]
		[batch 20/20] avg loss: 0.12484837772957676		[learning rate: 0.001241]
	Learning Rate: 0.00124098
	LOSS [training: 0.1367968836453353 | validation: 0.038230982972361646]
	TIME [epoch: 8.32 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11911910395501484		[learning rate: 0.0012387]
		[batch 20/20] avg loss: 0.10986639063827822		[learning rate: 0.0012365]
	Learning Rate: 0.00123647
	LOSS [training: 0.11449274729664653 | validation: 0.05076641539080256]
	TIME [epoch: 8.32 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13278489161359644		[learning rate: 0.0012342]
		[batch 20/20] avg loss: 0.11815262229841128		[learning rate: 0.001232]
	Learning Rate: 0.00123198
	LOSS [training: 0.12546875695600385 | validation: 0.13922705449346218]
	TIME [epoch: 8.31 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12113310615675203		[learning rate: 0.0012297]
		[batch 20/20] avg loss: 0.11345169326771609		[learning rate: 0.0012275]
	Learning Rate: 0.00122751
	LOSS [training: 0.11729239971223404 | validation: 0.2647416303848003]
	TIME [epoch: 8.29 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1719653084092233		[learning rate: 0.0012253]
		[batch 20/20] avg loss: 0.10976920511021519		[learning rate: 0.0012231]
	Learning Rate: 0.00122306
	LOSS [training: 0.14086725675971928 | validation: 0.25353758277465915]
	TIME [epoch: 8.28 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15694286200855306		[learning rate: 0.0012208]
		[batch 20/20] avg loss: 0.12367465309532796		[learning rate: 0.0012186]
	Learning Rate: 0.00121862
	LOSS [training: 0.1403087575519405 | validation: 0.06803474064512162]
	TIME [epoch: 8.4 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09889656827959485		[learning rate: 0.0012164]
		[batch 20/20] avg loss: 0.14038991164974354		[learning rate: 0.0012142]
	Learning Rate: 0.0012142
	LOSS [training: 0.11964323996466922 | validation: 0.15431133171900047]
	TIME [epoch: 8.42 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12924548488763737		[learning rate: 0.001212]
		[batch 20/20] avg loss: 0.10539180709042587		[learning rate: 0.0012098]
	Learning Rate: 0.00120979
	LOSS [training: 0.1173186459890316 | validation: 0.05474757138346267]
	TIME [epoch: 8.43 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10993729824856188		[learning rate: 0.0012076]
		[batch 20/20] avg loss: 0.161376090491621		[learning rate: 0.0012054]
	Learning Rate: 0.0012054
	LOSS [training: 0.13565669437009145 | validation: 0.151874180857211]
	TIME [epoch: 8.43 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11346633330927644		[learning rate: 0.0012032]
		[batch 20/20] avg loss: 0.15833896917684348		[learning rate: 0.001201]
	Learning Rate: 0.00120103
	LOSS [training: 0.13590265124305997 | validation: 0.0950151041613323]
	TIME [epoch: 8.44 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11225580024960571		[learning rate: 0.0011988]
		[batch 20/20] avg loss: 0.102539727945027		[learning rate: 0.0011967]
	Learning Rate: 0.00119667
	LOSS [training: 0.10739776409731636 | validation: 0.09685656115356356]
	TIME [epoch: 8.43 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10677825016496745		[learning rate: 0.0011945]
		[batch 20/20] avg loss: 0.12236910444782893		[learning rate: 0.0011923]
	Learning Rate: 0.00119233
	LOSS [training: 0.11457367730639818 | validation: 0.09946092013826296]
	TIME [epoch: 8.41 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1441401548617314		[learning rate: 0.0011902]
		[batch 20/20] avg loss: 0.16132019266354383		[learning rate: 0.001188]
	Learning Rate: 0.001188
	LOSS [training: 0.15273017376263764 | validation: 0.09126336653454452]
	TIME [epoch: 8.41 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1234177314844888		[learning rate: 0.0011858]
		[batch 20/20] avg loss: 0.12934356955787232		[learning rate: 0.0011837]
	Learning Rate: 0.00118369
	LOSS [training: 0.12638065052118058 | validation: 0.0510684973269132]
	TIME [epoch: 8.43 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12836511214281593		[learning rate: 0.0011815]
		[batch 20/20] avg loss: 0.14575224175124252		[learning rate: 0.0011794]
	Learning Rate: 0.00117939
	LOSS [training: 0.13705867694702922 | validation: 0.09397902148331372]
	TIME [epoch: 8.43 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11371474038844527		[learning rate: 0.0011772]
		[batch 20/20] avg loss: 0.09550451775439255		[learning rate: 0.0011751]
	Learning Rate: 0.00117511
	LOSS [training: 0.10460962907141891 | validation: 0.1300232561084093]
	TIME [epoch: 8.31 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11503212708771444		[learning rate: 0.001173]
		[batch 20/20] avg loss: 0.11228956123747089		[learning rate: 0.0011708]
	Learning Rate: 0.00117085
	LOSS [training: 0.11366084416259263 | validation: 0.06139702145926004]
	TIME [epoch: 8.39 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07782908835837943		[learning rate: 0.0011687]
		[batch 20/20] avg loss: 0.10662816398145095		[learning rate: 0.0011666]
	Learning Rate: 0.0011666
	LOSS [training: 0.09222862616991517 | validation: 0.11243374396956765]
	TIME [epoch: 8.33 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14574402462349556		[learning rate: 0.0011645]
		[batch 20/20] avg loss: 0.15730666906887641		[learning rate: 0.0011624]
	Learning Rate: 0.00116236
	LOSS [training: 0.151525346846186 | validation: 0.0894419956847927]
	TIME [epoch: 8.37 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11244242196229134		[learning rate: 0.0011603]
		[batch 20/20] avg loss: 0.09174961735314638		[learning rate: 0.0011581]
	Learning Rate: 0.00115815
	LOSS [training: 0.10209601965771886 | validation: 0.14869621448070164]
	TIME [epoch: 8.33 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13999995554525352		[learning rate: 0.001156]
		[batch 20/20] avg loss: 0.10790412605679538		[learning rate: 0.0011539]
	Learning Rate: 0.00115394
	LOSS [training: 0.12395204080102444 | validation: 0.13718219398816306]
	TIME [epoch: 8.26 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15918700350459045		[learning rate: 0.0011518]
		[batch 20/20] avg loss: 0.12094041047360424		[learning rate: 0.0011498]
	Learning Rate: 0.00114975
	LOSS [training: 0.14006370698909731 | validation: 0.12855987670897492]
	TIME [epoch: 8.24 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1285635223696616		[learning rate: 0.0011477]
		[batch 20/20] avg loss: 0.15815053616986274		[learning rate: 0.0011456]
	Learning Rate: 0.00114558
	LOSS [training: 0.14335702926976218 | validation: 0.06299989928630667]
	TIME [epoch: 8.26 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07890895541598794		[learning rate: 0.0011435]
		[batch 20/20] avg loss: 0.12992819427668068		[learning rate: 0.0011414]
	Learning Rate: 0.00114142
	LOSS [training: 0.10441857484633432 | validation: 0.04669282994128328]
	TIME [epoch: 8.27 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10406107280745518		[learning rate: 0.0011394]
		[batch 20/20] avg loss: 0.16153556705856612		[learning rate: 0.0011373]
	Learning Rate: 0.00113728
	LOSS [training: 0.1327983199330107 | validation: 0.07703168894929674]
	TIME [epoch: 8.25 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10104669092462645		[learning rate: 0.0011352]
		[batch 20/20] avg loss: 0.14902032691952202		[learning rate: 0.0011332]
	Learning Rate: 0.00113316
	LOSS [training: 0.12503350892207424 | validation: 0.34061885388678204]
	TIME [epoch: 8.24 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15139252211145576		[learning rate: 0.0011311]
		[batch 20/20] avg loss: 0.10069654271143928		[learning rate: 0.001129]
	Learning Rate: 0.00112904
	LOSS [training: 0.12604453241144747 | validation: 0.13047011224927907]
	TIME [epoch: 8.26 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10659685232913582		[learning rate: 0.001127]
		[batch 20/20] avg loss: 0.13930699967029211		[learning rate: 0.0011249]
	Learning Rate: 0.00112495
	LOSS [training: 0.12295192599971398 | validation: 0.12211640439182869]
	TIME [epoch: 8.26 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10528993002752832		[learning rate: 0.0011229]
		[batch 20/20] avg loss: 0.08053293143123726		[learning rate: 0.0011209]
	Learning Rate: 0.00112086
	LOSS [training: 0.09291143072938278 | validation: 0.14373916287141614]
	TIME [epoch: 8.37 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22074870288851334		[learning rate: 0.0011188]
		[batch 20/20] avg loss: 0.09103852431526852		[learning rate: 0.0011168]
	Learning Rate: 0.0011168
	LOSS [training: 0.1558936136018909 | validation: 0.04952301778874829]
	TIME [epoch: 8.39 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11797895555331908		[learning rate: 0.0011148]
		[batch 20/20] avg loss: 0.1306446493700513		[learning rate: 0.0011127]
	Learning Rate: 0.00111274
	LOSS [training: 0.12431180246168516 | validation: 0.05479724375808423]
	TIME [epoch: 8.38 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13029579687198273		[learning rate: 0.0011107]
		[batch 20/20] avg loss: 0.10551326119213245		[learning rate: 0.0011087]
	Learning Rate: 0.0011087
	LOSS [training: 0.11790452903205757 | validation: 0.061224086251340404]
	TIME [epoch: 8.3 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09157458705636783		[learning rate: 0.0011067]
		[batch 20/20] avg loss: 0.14257442978727444		[learning rate: 0.0011047]
	Learning Rate: 0.00110468
	LOSS [training: 0.11707450842182114 | validation: 0.1599969212635606]
	TIME [epoch: 8.33 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13141423111019507		[learning rate: 0.0011027]
		[batch 20/20] avg loss: 0.14779749714698526		[learning rate: 0.0011007]
	Learning Rate: 0.00110067
	LOSS [training: 0.1396058641285902 | validation: 0.085714004120084]
	TIME [epoch: 8.36 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11706548980457368		[learning rate: 0.0010987]
		[batch 20/20] avg loss: 0.15616231130226102		[learning rate: 0.0010967]
	Learning Rate: 0.00109668
	LOSS [training: 0.13661390055341732 | validation: 0.12095696715321437]
	TIME [epoch: 8.38 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1005782204337422		[learning rate: 0.0010947]
		[batch 20/20] avg loss: 0.11923694517700596		[learning rate: 0.0010927]
	Learning Rate: 0.0010927
	LOSS [training: 0.10990758280537408 | validation: 0.10703030713272471]
	TIME [epoch: 8.4 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13329280195285173		[learning rate: 0.0010907]
		[batch 20/20] avg loss: 0.0855944558470029		[learning rate: 0.0010887]
	Learning Rate: 0.00108873
	LOSS [training: 0.10944362889992731 | validation: 0.09251598335522068]
	TIME [epoch: 8.39 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12325645794363697		[learning rate: 0.0010868]
		[batch 20/20] avg loss: 0.09053648820735491		[learning rate: 0.0010848]
	Learning Rate: 0.00108478
	LOSS [training: 0.10689647307549595 | validation: 0.18074191914659507]
	TIME [epoch: 8.28 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21236906952886087		[learning rate: 0.0010828]
		[batch 20/20] avg loss: 0.12023615525858725		[learning rate: 0.0010808]
	Learning Rate: 0.00108084
	LOSS [training: 0.16630261239372401 | validation: 0.1048780213009333]
	TIME [epoch: 8.37 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16684316033097635		[learning rate: 0.0010789]
		[batch 20/20] avg loss: 0.10545623804692643		[learning rate: 0.0010769]
	Learning Rate: 0.00107692
	LOSS [training: 0.13614969918895142 | validation: 0.07973108728500694]
	TIME [epoch: 8.42 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13475048701906608		[learning rate: 0.001075]
		[batch 20/20] avg loss: 0.14582827372207832		[learning rate: 0.001073]
	Learning Rate: 0.00107301
	LOSS [training: 0.14028938037057226 | validation: 0.09777680169837431]
	TIME [epoch: 8.45 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11614091587028433		[learning rate: 0.0010711]
		[batch 20/20] avg loss: 0.10978195788219051		[learning rate: 0.0010691]
	Learning Rate: 0.00106912
	LOSS [training: 0.11296143687623743 | validation: 0.06690761175960842]
	TIME [epoch: 8.43 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08889562803650329		[learning rate: 0.0010672]
		[batch 20/20] avg loss: 0.09957215850295517		[learning rate: 0.0010652]
	Learning Rate: 0.00106524
	LOSS [training: 0.09423389326972922 | validation: 0.07928482692140978]
	TIME [epoch: 8.41 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12851406914774352		[learning rate: 0.0010633]
		[batch 20/20] avg loss: 0.11948367748507396		[learning rate: 0.0010614]
	Learning Rate: 0.00106137
	LOSS [training: 0.12399887331640877 | validation: 0.051246905174945345]
	TIME [epoch: 8.3 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1714289602950103		[learning rate: 0.0010594]
		[batch 20/20] avg loss: 0.08643577784251238		[learning rate: 0.0010575]
	Learning Rate: 0.00105752
	LOSS [training: 0.12893236906876135 | validation: 0.08465926753188216]
	TIME [epoch: 8.38 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09449382767166342		[learning rate: 0.0010556]
		[batch 20/20] avg loss: 0.1076186718474134		[learning rate: 0.0010537]
	Learning Rate: 0.00105368
	LOSS [training: 0.10105624975953842 | validation: 0.06722398758936662]
	TIME [epoch: 8.34 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1361636032299243		[learning rate: 0.0010518]
		[batch 20/20] avg loss: 0.15809424716224102		[learning rate: 0.0010499]
	Learning Rate: 0.00104986
	LOSS [training: 0.1471289251960827 | validation: 0.09307934997174067]
	TIME [epoch: 8.32 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13025714437681893		[learning rate: 0.001048]
		[batch 20/20] avg loss: 0.1413908434025955		[learning rate: 0.0010461]
	Learning Rate: 0.00104605
	LOSS [training: 0.13582399388970717 | validation: 0.19847048089598884]
	TIME [epoch: 8.37 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12118988935835133		[learning rate: 0.0010442]
		[batch 20/20] avg loss: 0.10374755488808503		[learning rate: 0.0010423]
	Learning Rate: 0.00104225
	LOSS [training: 0.11246872212321819 | validation: 0.2070382237782894]
	TIME [epoch: 8.37 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24241280680288152		[learning rate: 0.0010404]
		[batch 20/20] avg loss: 0.10187056373796208		[learning rate: 0.0010385]
	Learning Rate: 0.00103847
	LOSS [training: 0.1721416852704218 | validation: 0.0934302752875796]
	TIME [epoch: 8.38 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10013608737332207		[learning rate: 0.0010366]
		[batch 20/20] avg loss: 0.15757339621929584		[learning rate: 0.0010347]
	Learning Rate: 0.0010347
	LOSS [training: 0.12885474179630893 | validation: 0.20670603699051915]
	TIME [epoch: 8.39 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.161169305697285		[learning rate: 0.0010328]
		[batch 20/20] avg loss: 0.11019873298938902		[learning rate: 0.0010309]
	Learning Rate: 0.00103095
	LOSS [training: 0.13568401934333701 | validation: 0.10736756620997]
	TIME [epoch: 8.33 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11402150574094824		[learning rate: 0.0010291]
		[batch 20/20] avg loss: 0.136577187810964		[learning rate: 0.0010272]
	Learning Rate: 0.00102721
	LOSS [training: 0.12529934677595614 | validation: 0.1417817637120712]
	TIME [epoch: 8.33 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1867519982483918		[learning rate: 0.0010253]
		[batch 20/20] avg loss: 0.10303462053079482		[learning rate: 0.0010235]
	Learning Rate: 0.00102348
	LOSS [training: 0.1448933093895933 | validation: 0.1489688709663805]
	TIME [epoch: 8.36 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1491538382795777		[learning rate: 0.0010216]
		[batch 20/20] avg loss: 0.13913143878310563		[learning rate: 0.0010198]
	Learning Rate: 0.00101976
	LOSS [training: 0.14414263853134165 | validation: 0.20789045561443686]
	TIME [epoch: 8.39 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10033774040016012		[learning rate: 0.0010179]
		[batch 20/20] avg loss: 0.11139295344192904		[learning rate: 0.0010161]
	Learning Rate: 0.00101606
	LOSS [training: 0.10586534692104459 | validation: 0.0892643496699835]
	TIME [epoch: 8.38 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13357666376655936		[learning rate: 0.0010142]
		[batch 20/20] avg loss: 0.10245980618231434		[learning rate: 0.0010124]
	Learning Rate: 0.00101238
	LOSS [training: 0.11801823497443682 | validation: 0.11008669623787776]
	TIME [epoch: 8.35 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11397973749016854		[learning rate: 0.0010105]
		[batch 20/20] avg loss: 0.11170841659680113		[learning rate: 0.0010087]
	Learning Rate: 0.0010087
	LOSS [training: 0.11284407704348483 | validation: 0.10298683223539203]
	TIME [epoch: 8.39 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11432701051981604		[learning rate: 0.0010069]
		[batch 20/20] avg loss: 0.08955060901201359		[learning rate: 0.001005]
	Learning Rate: 0.00100504
	LOSS [training: 0.10193880976591481 | validation: 0.10724982115099935]
	TIME [epoch: 8.37 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08704502399263203		[learning rate: 0.0010032]
		[batch 20/20] avg loss: 0.08464926021705009		[learning rate: 0.0010014]
	Learning Rate: 0.00100139
	LOSS [training: 0.08584714210484104 | validation: 0.09730249783957569]
	TIME [epoch: 8.35 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09753832032395189		[learning rate: 0.00099958]
		[batch 20/20] avg loss: 0.12811659053176505		[learning rate: 0.00099776]
	Learning Rate: 0.00099776
	LOSS [training: 0.11282745542785848 | validation: 0.07418311559884673]
	TIME [epoch: 8.37 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11487207797112284		[learning rate: 0.00099595]
		[batch 20/20] avg loss: 0.09522754197264788		[learning rate: 0.00099414]
	Learning Rate: 0.00099414
	LOSS [training: 0.10504980997188536 | validation: 0.07812940975654907]
	TIME [epoch: 8.39 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10616830239306749		[learning rate: 0.00099233]
		[batch 20/20] avg loss: 0.1603919000876879		[learning rate: 0.00099053]
	Learning Rate: 0.000990532
	LOSS [training: 0.1332801012403777 | validation: 0.08021895889560843]
	TIME [epoch: 8.37 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09652051711913787		[learning rate: 0.00098873]
		[batch 20/20] avg loss: 0.08997399941310441		[learning rate: 0.00098694]
	Learning Rate: 0.000986937
	LOSS [training: 0.09324725826612115 | validation: 0.11242176855508593]
	TIME [epoch: 8.38 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12058714075463262		[learning rate: 0.00098514]
		[batch 20/20] avg loss: 0.09410211692385168		[learning rate: 0.00098336]
	Learning Rate: 0.000983355
	LOSS [training: 0.10734462883924215 | validation: 0.07333129599160836]
	TIME [epoch: 8.38 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08986633120566243		[learning rate: 0.00098157]
		[batch 20/20] avg loss: 0.1498284020946175		[learning rate: 0.00097979]
	Learning Rate: 0.000979787
	LOSS [training: 0.11984736665013998 | validation: 0.05556107514150391]
	TIME [epoch: 8.4 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0956954838007229		[learning rate: 0.00097801]
		[batch 20/20] avg loss: 0.1318969212649226		[learning rate: 0.00097623]
	Learning Rate: 0.000976231
	LOSS [training: 0.11379620253282277 | validation: 0.07694089833603597]
	TIME [epoch: 8.41 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10176029608936213		[learning rate: 0.00097446]
		[batch 20/20] avg loss: 0.10308417791081095		[learning rate: 0.00097269]
	Learning Rate: 0.000972688
	LOSS [training: 0.10242223700008653 | validation: 0.11892299805414541]
	TIME [epoch: 8.4 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10034137837575016		[learning rate: 0.00097092]
		[batch 20/20] avg loss: 0.18068689679021727		[learning rate: 0.00096916]
	Learning Rate: 0.000969158
	LOSS [training: 0.1405141375829837 | validation: 0.12554971108311053]
	TIME [epoch: 8.39 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1035380162616653		[learning rate: 0.0009674]
		[batch 20/20] avg loss: 0.14709864866115696		[learning rate: 0.00096564]
	Learning Rate: 0.000965641
	LOSS [training: 0.12531833246141114 | validation: 0.0611938161328464]
	TIME [epoch: 8.41 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10845908269199966		[learning rate: 0.00096389]
		[batch 20/20] avg loss: 0.11378372578806999		[learning rate: 0.00096214]
	Learning Rate: 0.000962137
	LOSS [training: 0.11112140424003483 | validation: 0.0506820530882381]
	TIME [epoch: 8.42 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11367443690770451		[learning rate: 0.00096039]
		[batch 20/20] avg loss: 0.11433668122518222		[learning rate: 0.00095865]
	Learning Rate: 0.000958645
	LOSS [training: 0.11400555906644338 | validation: 0.12244551343049563]
	TIME [epoch: 8.4 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12911913837892186		[learning rate: 0.0009569]
		[batch 20/20] avg loss: 0.08798912011125275		[learning rate: 0.00095517]
	Learning Rate: 0.000955166
	LOSS [training: 0.10855412924508731 | validation: 0.05962717246093426]
	TIME [epoch: 8.39 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11944428561133638		[learning rate: 0.00095343]
		[batch 20/20] avg loss: 0.13036945890479884		[learning rate: 0.0009517]
	Learning Rate: 0.0009517
	LOSS [training: 0.1249068722580676 | validation: 0.10089102797642581]
	TIME [epoch: 8.4 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11794750049845137		[learning rate: 0.00094997]
		[batch 20/20] avg loss: 0.11714854968228099		[learning rate: 0.00094825]
	Learning Rate: 0.000948246
	LOSS [training: 0.11754802509036617 | validation: 0.06255252208756965]
	TIME [epoch: 8.39 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11088053557864155		[learning rate: 0.00094652]
		[batch 20/20] avg loss: 0.10520290721045329		[learning rate: 0.0009448]
	Learning Rate: 0.000944805
	LOSS [training: 0.10804172139454742 | validation: 0.054110729695457514]
	TIME [epoch: 8.37 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09770945071953148		[learning rate: 0.00094309]
		[batch 20/20] avg loss: 0.08236450454848628		[learning rate: 0.00094138]
	Learning Rate: 0.000941376
	LOSS [training: 0.09003697763400889 | validation: 0.06730498301593726]
	TIME [epoch: 8.33 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0938189459533437		[learning rate: 0.00093967]
		[batch 20/20] avg loss: 0.11940351726907247		[learning rate: 0.00093796]
	Learning Rate: 0.00093796
	LOSS [training: 0.10661123161120808 | validation: 0.2790711350193687]
	TIME [epoch: 8.3 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1486636627563946		[learning rate: 0.00093626]
		[batch 20/20] avg loss: 0.09864134493141723		[learning rate: 0.00093456]
	Learning Rate: 0.000934556
	LOSS [training: 0.12365250384390591 | validation: 0.0680197850133838]
	TIME [epoch: 8.29 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10955925580355608		[learning rate: 0.00093286]
		[batch 20/20] avg loss: 0.09733505839357201		[learning rate: 0.00093116]
	Learning Rate: 0.000931164
	LOSS [training: 0.10344715709856403 | validation: 0.04267986055157995]
	TIME [epoch: 8.33 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14945480974241		[learning rate: 0.00092947]
		[batch 20/20] avg loss: 0.08883827359372572		[learning rate: 0.00092779]
	Learning Rate: 0.000927785
	LOSS [training: 0.11914654166806785 | validation: 0.08898117843808066]
	TIME [epoch: 8.31 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09253929675750137		[learning rate: 0.0009261]
		[batch 20/20] avg loss: 0.10444861087424388		[learning rate: 0.00092442]
	Learning Rate: 0.000924418
	LOSS [training: 0.09849395381587264 | validation: 0.06097059514037662]
	TIME [epoch: 8.33 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17732727539230314		[learning rate: 0.00092274]
		[batch 20/20] avg loss: 0.08571997802494205		[learning rate: 0.00092106]
	Learning Rate: 0.000921063
	LOSS [training: 0.13152362670862258 | validation: 0.26168280473121097]
	TIME [epoch: 8.34 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20213614250788692		[learning rate: 0.00091939]
		[batch 20/20] avg loss: 0.09590943365391		[learning rate: 0.00091772]
	Learning Rate: 0.000917721
	LOSS [training: 0.14902278808089847 | validation: 0.10186341872258223]
	TIME [epoch: 8.34 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0969481942743706		[learning rate: 0.00091605]
		[batch 20/20] avg loss: 0.11836311835591136		[learning rate: 0.00091439]
	Learning Rate: 0.00091439
	LOSS [training: 0.10765565631514096 | validation: 0.11090901114882123]
	TIME [epoch: 8.34 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07804281098636097		[learning rate: 0.00091273]
		[batch 20/20] avg loss: 0.09341432069932001		[learning rate: 0.00091107]
	Learning Rate: 0.000911072
	LOSS [training: 0.0857285658428405 | validation: 0.08614550338022506]
	TIME [epoch: 8.39 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14053892548093852		[learning rate: 0.00090942]
		[batch 20/20] avg loss: 0.09946361162405667		[learning rate: 0.00090777]
	Learning Rate: 0.000907766
	LOSS [training: 0.1200012685524976 | validation: 0.08935530575132476]
	TIME [epoch: 8.4 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1190192068663443		[learning rate: 0.00090612]
		[batch 20/20] avg loss: 0.09787200597505157		[learning rate: 0.00090447]
	Learning Rate: 0.000904471
	LOSS [training: 0.10844560642069793 | validation: 0.07013394826013104]
	TIME [epoch: 8.43 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09653426231717227		[learning rate: 0.00090283]
		[batch 20/20] avg loss: 0.0877411808019256		[learning rate: 0.00090119]
	Learning Rate: 0.000901189
	LOSS [training: 0.09213772155954894 | validation: 0.11831318264818819]
	TIME [epoch: 8.33 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11931921392539158		[learning rate: 0.00089955]
		[batch 20/20] avg loss: 0.0926404026737621		[learning rate: 0.00089792]
	Learning Rate: 0.000897918
	LOSS [training: 0.1059798082995768 | validation: 0.046188522279524025]
	TIME [epoch: 8.36 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14055879357882745		[learning rate: 0.00089629]
		[batch 20/20] avg loss: 0.12453407257669213		[learning rate: 0.00089466]
	Learning Rate: 0.00089466
	LOSS [training: 0.1325464330777598 | validation: 0.1226242364002764]
	TIME [epoch: 8.39 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0935814439695815		[learning rate: 0.00089303]
		[batch 20/20] avg loss: 0.1128588299710643		[learning rate: 0.00089141]
	Learning Rate: 0.000891413
	LOSS [training: 0.10322013697032291 | validation: 0.17844405689205645]
	TIME [epoch: 8.4 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08674493625622133		[learning rate: 0.00088979]
		[batch 20/20] avg loss: 0.08009006621934625		[learning rate: 0.00088818]
	Learning Rate: 0.000888178
	LOSS [training: 0.0834175012377838 | validation: 0.043434045750443205]
	TIME [epoch: 8.39 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09186458617747535		[learning rate: 0.00088656]
		[batch 20/20] avg loss: 0.1216100821036662		[learning rate: 0.00088495]
	Learning Rate: 0.000884955
	LOSS [training: 0.10673733414057077 | validation: 0.09608982128678609]
	TIME [epoch: 8.31 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1303930828611729		[learning rate: 0.00088335]
		[batch 20/20] avg loss: 0.10990536177887947		[learning rate: 0.00088174]
	Learning Rate: 0.000881743
	LOSS [training: 0.12014922232002616 | validation: 0.22534586945229876]
	TIME [epoch: 8.36 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10188346134799782		[learning rate: 0.00088014]
		[batch 20/20] avg loss: 0.09277649037597487		[learning rate: 0.00087854]
	Learning Rate: 0.000878543
	LOSS [training: 0.09732997586198634 | validation: 0.0380578027714629]
	TIME [epoch: 8.36 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11064756035391723		[learning rate: 0.00087695]
		[batch 20/20] avg loss: 0.12905664217626017		[learning rate: 0.00087536]
	Learning Rate: 0.000875355
	LOSS [training: 0.11985210126508872 | validation: 0.06186797443573565]
	TIME [epoch: 8.39 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07475292534861527		[learning rate: 0.00087377]
		[batch 20/20] avg loss: 0.08747328058185963		[learning rate: 0.00087218]
	Learning Rate: 0.000872178
	LOSS [training: 0.08111310296523744 | validation: 0.053651266298215955]
	TIME [epoch: 8.35 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07829074798798563		[learning rate: 0.00087059]
		[batch 20/20] avg loss: 0.1279311707541043		[learning rate: 0.00086901]
	Learning Rate: 0.000869013
	LOSS [training: 0.10311095937104495 | validation: 0.3383173266309116]
	TIME [epoch: 8.35 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14541024734305488		[learning rate: 0.00086743]
		[batch 20/20] avg loss: 0.0836835712328732		[learning rate: 0.00086586]
	Learning Rate: 0.000865859
	LOSS [training: 0.114546909287964 | validation: 0.06572856605315722]
	TIME [epoch: 8.35 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10515647580471862		[learning rate: 0.00086429]
		[batch 20/20] avg loss: 0.08224603193765498		[learning rate: 0.00086272]
	Learning Rate: 0.000862717
	LOSS [training: 0.0937012538711868 | validation: 0.07463673569113642]
	TIME [epoch: 8.37 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13880181612755188		[learning rate: 0.00086115]
		[batch 20/20] avg loss: 0.09665631778355495		[learning rate: 0.00085959]
	Learning Rate: 0.000859586
	LOSS [training: 0.11772906695555344 | validation: 0.05205749857967283]
	TIME [epoch: 8.36 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15387781432248251		[learning rate: 0.00085803]
		[batch 20/20] avg loss: 0.11261099508553171		[learning rate: 0.00085647]
	Learning Rate: 0.000856467
	LOSS [training: 0.13324440470400709 | validation: 0.10183669741584211]
	TIME [epoch: 8.39 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13407299887766927		[learning rate: 0.00085491]
		[batch 20/20] avg loss: 0.0930275311682062		[learning rate: 0.00085336]
	Learning Rate: 0.000853359
	LOSS [training: 0.11355026502293775 | validation: 0.11018509856348559]
	TIME [epoch: 8.39 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11689044350210405		[learning rate: 0.00085181]
		[batch 20/20] avg loss: 0.17430101095434933		[learning rate: 0.00085026]
	Learning Rate: 0.000850262
	LOSS [training: 0.1455957272282267 | validation: 0.24951031144389674]
	TIME [epoch: 8.41 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1561721549602315		[learning rate: 0.00084872]
		[batch 20/20] avg loss: 0.13138073967619437		[learning rate: 0.00084718]
	Learning Rate: 0.000847176
	LOSS [training: 0.14377644731821296 | validation: 0.04882936146862201]
	TIME [epoch: 8.41 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07722609052433636		[learning rate: 0.00084564]
		[batch 20/20] avg loss: 0.09886683766525953		[learning rate: 0.0008441]
	Learning Rate: 0.000844102
	LOSS [training: 0.08804646409479792 | validation: 0.08331092442612197]
	TIME [epoch: 8.39 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08399796592194506		[learning rate: 0.00084257]
		[batch 20/20] avg loss: 0.08538715703986868		[learning rate: 0.00084104]
	Learning Rate: 0.000841038
	LOSS [training: 0.08469256148090687 | validation: 0.09123101103174971]
	TIME [epoch: 8.38 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09500274611216714		[learning rate: 0.00083951]
		[batch 20/20] avg loss: 0.1269491061199969		[learning rate: 0.00083799]
	Learning Rate: 0.000837986
	LOSS [training: 0.11097592611608201 | validation: 0.0854251805504531]
	TIME [epoch: 8.34 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11834596947557548		[learning rate: 0.00083646]
		[batch 20/20] avg loss: 0.09871014025033915		[learning rate: 0.00083495]
	Learning Rate: 0.000834945
	LOSS [training: 0.10852805486295732 | validation: 0.0846293768368841]
	TIME [epoch: 8.35 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10731803107418332		[learning rate: 0.00083343]
		[batch 20/20] avg loss: 0.12585467638453574		[learning rate: 0.00083192]
	Learning Rate: 0.000831915
	LOSS [training: 0.11658635372935952 | validation: 0.049014076814351895]
	TIME [epoch: 8.29 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08061672110718396		[learning rate: 0.0008304]
		[batch 20/20] avg loss: 0.09860622608983922		[learning rate: 0.0008289]
	Learning Rate: 0.000828896
	LOSS [training: 0.08961147359851158 | validation: 0.28689350464871943]
	TIME [epoch: 8.33 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1097494915264063		[learning rate: 0.00082739]
		[batch 20/20] avg loss: 0.08841214867069618		[learning rate: 0.00082589]
	Learning Rate: 0.000825888
	LOSS [training: 0.09908082009855126 | validation: 0.0782125204713435]
	TIME [epoch: 8.36 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08850288453767226		[learning rate: 0.00082439]
		[batch 20/20] avg loss: 0.07809470226528435		[learning rate: 0.00082289]
	Learning Rate: 0.000822891
	LOSS [training: 0.08329879340147829 | validation: 0.13230677142134253]
	TIME [epoch: 8.39 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0896038192231963		[learning rate: 0.0008214]
		[batch 20/20] avg loss: 0.08824462983240773		[learning rate: 0.0008199]
	Learning Rate: 0.000819904
	LOSS [training: 0.088924224527802 | validation: 0.10745225408800603]
	TIME [epoch: 8.35 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10698230034616663		[learning rate: 0.00081842]
		[batch 20/20] avg loss: 0.06658982860538236		[learning rate: 0.00081693]
	Learning Rate: 0.000816929
	LOSS [training: 0.08678606447577449 | validation: 0.09852910063023008]
	TIME [epoch: 8.35 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0974796654997212		[learning rate: 0.00081545]
		[batch 20/20] avg loss: 0.08830554482035693		[learning rate: 0.00081396]
	Learning Rate: 0.000813964
	LOSS [training: 0.09289260516003907 | validation: 0.02959274443364273]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_790.pth
	Model improved!!!
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06607763456541312		[learning rate: 0.00081249]
		[batch 20/20] avg loss: 0.10187822724691845		[learning rate: 0.00081101]
	Learning Rate: 0.00081101
	LOSS [training: 0.08397793090616577 | validation: 0.06524538870827118]
	TIME [epoch: 8.41 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07608331996751398		[learning rate: 0.00080954]
		[batch 20/20] avg loss: 0.07644908626443023		[learning rate: 0.00080807]
	Learning Rate: 0.000808067
	LOSS [training: 0.07626620311597208 | validation: 0.1286505020157978]
	TIME [epoch: 8.42 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09529532918953414		[learning rate: 0.0008066]
		[batch 20/20] avg loss: 0.11244502129399736		[learning rate: 0.00080513]
	Learning Rate: 0.000805135
	LOSS [training: 0.10387017524176574 | validation: 0.05644485510833828]
	TIME [epoch: 8.31 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08983639422132109		[learning rate: 0.00080367]
		[batch 20/20] avg loss: 0.09348160144278368		[learning rate: 0.00080221]
	Learning Rate: 0.000802213
	LOSS [training: 0.09165899783205238 | validation: 0.028545622961947445]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_794.pth
	Model improved!!!
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09743704134512968		[learning rate: 0.00080076]
		[batch 20/20] avg loss: 0.13600686464724482		[learning rate: 0.0007993]
	Learning Rate: 0.000799301
	LOSS [training: 0.11672195299618723 | validation: 0.06462120287270208]
	TIME [epoch: 8.43 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08218825739688125		[learning rate: 0.00079785]
		[batch 20/20] avg loss: 0.1159517047676959		[learning rate: 0.0007964]
	Learning Rate: 0.000796401
	LOSS [training: 0.09906998108228857 | validation: 0.08625098590728791]
	TIME [epoch: 8.47 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10984045275862056		[learning rate: 0.00079495]
		[batch 20/20] avg loss: 0.11703522338618785		[learning rate: 0.00079351]
	Learning Rate: 0.000793511
	LOSS [training: 0.11343783807240422 | validation: 0.07295963100847708]
	TIME [epoch: 8.47 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07271365034502887		[learning rate: 0.00079207]
		[batch 20/20] avg loss: 0.0773824456674519		[learning rate: 0.00079063]
	Learning Rate: 0.000790631
	LOSS [training: 0.07504804800624039 | validation: 0.047666168866560936]
	TIME [epoch: 8.44 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08994921679302312		[learning rate: 0.00078919]
		[batch 20/20] avg loss: 0.10600827253189431		[learning rate: 0.00078776]
	Learning Rate: 0.000787761
	LOSS [training: 0.09797874466245873 | validation: 0.08887771313987719]
	TIME [epoch: 8.42 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09086350117164183		[learning rate: 0.00078633]
		[batch 20/20] avg loss: 0.08789073157608764		[learning rate: 0.0007849]
	Learning Rate: 0.000784903
	LOSS [training: 0.08937711637386474 | validation: 0.11699963855228156]
	TIME [epoch: 8.38 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09686243803553432		[learning rate: 0.00078348]
		[batch 20/20] avg loss: 0.08432278587249702		[learning rate: 0.00078205]
	Learning Rate: 0.000782054
	LOSS [training: 0.09059261195401565 | validation: 0.14332352400651238]
	TIME [epoch: 8.33 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08809941403137045		[learning rate: 0.00078063]
		[batch 20/20] avg loss: 0.05806908681712806		[learning rate: 0.00077922]
	Learning Rate: 0.000779216
	LOSS [training: 0.07308425042424925 | validation: 0.08217907877256092]
	TIME [epoch: 8.33 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10657979405954336		[learning rate: 0.0007778]
		[batch 20/20] avg loss: 0.12035227937515987		[learning rate: 0.00077639]
	Learning Rate: 0.000776388
	LOSS [training: 0.1134660367173516 | validation: 0.10492599616414329]
	TIME [epoch: 8.38 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08282330742452995		[learning rate: 0.00077498]
		[batch 20/20] avg loss: 0.13216096663159108		[learning rate: 0.00077357]
	Learning Rate: 0.000773571
	LOSS [training: 0.10749213702806053 | validation: 0.08398156841806724]
	TIME [epoch: 8.41 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1522834326055587		[learning rate: 0.00077217]
		[batch 20/20] avg loss: 0.09183887437664942		[learning rate: 0.00077076]
	Learning Rate: 0.000770763
	LOSS [training: 0.1220611534911041 | validation: 0.10389755696101047]
	TIME [epoch: 8.42 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09887885533585919		[learning rate: 0.00076936]
		[batch 20/20] avg loss: 0.07034886620023967		[learning rate: 0.00076797]
	Learning Rate: 0.000767966
	LOSS [training: 0.08461386076804943 | validation: 0.034658960989299784]
	TIME [epoch: 8.4 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08962113551865955		[learning rate: 0.00076657]
		[batch 20/20] avg loss: 0.06178595437452916		[learning rate: 0.00076518]
	Learning Rate: 0.000765179
	LOSS [training: 0.07570354494659434 | validation: 0.06005433724806018]
	TIME [epoch: 8.4 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0757890910402261		[learning rate: 0.00076379]
		[batch 20/20] avg loss: 0.11593679532736494		[learning rate: 0.0007624]
	Learning Rate: 0.000762402
	LOSS [training: 0.09586294318379554 | validation: 0.10700882485741914]
	TIME [epoch: 8.41 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06854194944514912		[learning rate: 0.00076102]
		[batch 20/20] avg loss: 0.1026628313563086		[learning rate: 0.00075964]
	Learning Rate: 0.000759636
	LOSS [training: 0.08560239040072885 | validation: 0.10764068428228554]
	TIME [epoch: 8.38 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08212722428046915		[learning rate: 0.00075826]
		[batch 20/20] avg loss: 0.08337406398806824		[learning rate: 0.00075688]
	Learning Rate: 0.000756879
	LOSS [training: 0.08275064413426868 | validation: 0.0915189489359824]
	TIME [epoch: 8.36 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09289277959145205		[learning rate: 0.0007555]
		[batch 20/20] avg loss: 0.10753979757565393		[learning rate: 0.00075413]
	Learning Rate: 0.000754132
	LOSS [training: 0.10021628858355298 | validation: 0.04960517452731857]
	TIME [epoch: 8.38 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08411910195774325		[learning rate: 0.00075276]
		[batch 20/20] avg loss: 0.08220761473794144		[learning rate: 0.0007514]
	Learning Rate: 0.000751395
	LOSS [training: 0.08316335834784234 | validation: 0.12441487538862425]
	TIME [epoch: 8.4 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11208000533014628		[learning rate: 0.00075003]
		[batch 20/20] avg loss: 0.11590542209348773		[learning rate: 0.00074867]
	Learning Rate: 0.000748668
	LOSS [training: 0.113992713711817 | validation: 0.08923196122708169]
	TIME [epoch: 8.32 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13948839870537988		[learning rate: 0.00074731]
		[batch 20/20] avg loss: 0.12465082024526337		[learning rate: 0.00074595]
	Learning Rate: 0.000745951
	LOSS [training: 0.13206960947532162 | validation: 0.07568133059306877]
	TIME [epoch: 8.34 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11293962308049277		[learning rate: 0.0007446]
		[batch 20/20] avg loss: 0.09971274999597597		[learning rate: 0.00074324]
	Learning Rate: 0.000743244
	LOSS [training: 0.10632618653823434 | validation: 0.21290002818712334]
	TIME [epoch: 8.36 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11729270284691454		[learning rate: 0.00074189]
		[batch 20/20] avg loss: 0.09432794532955599		[learning rate: 0.00074055]
	Learning Rate: 0.000740547
	LOSS [training: 0.10581032408823525 | validation: 0.08542441203059992]
	TIME [epoch: 8.38 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12727749913227654		[learning rate: 0.0007392]
		[batch 20/20] avg loss: 0.07765142433601137		[learning rate: 0.00073786]
	Learning Rate: 0.00073786
	LOSS [training: 0.10246446173414395 | validation: 0.07899530215331341]
	TIME [epoch: 8.37 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06488515495206962		[learning rate: 0.00073652]
		[batch 20/20] avg loss: 0.07950708798477339		[learning rate: 0.00073518]
	Learning Rate: 0.000735182
	LOSS [training: 0.07219612146842151 | validation: 0.068517119983526]
	TIME [epoch: 8.37 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10431515510049247		[learning rate: 0.00073385]
		[batch 20/20] avg loss: 0.0794341890363782		[learning rate: 0.00073251]
	Learning Rate: 0.000732514
	LOSS [training: 0.09187467206843532 | validation: 0.1166738130361343]
	TIME [epoch: 8.37 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08942873313793381		[learning rate: 0.00073118]
		[batch 20/20] avg loss: 0.07684846521789693		[learning rate: 0.00072986]
	Learning Rate: 0.000729855
	LOSS [training: 0.08313859917791536 | validation: 0.05670103589371023]
	TIME [epoch: 8.38 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10195698907052371		[learning rate: 0.00072853]
		[batch 20/20] avg loss: 0.06784339630702181		[learning rate: 0.00072721]
	Learning Rate: 0.000727207
	LOSS [training: 0.08490019268877276 | validation: 0.07424060831106379]
	TIME [epoch: 8.38 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06979565644650482		[learning rate: 0.00072589]
		[batch 20/20] avg loss: 0.0940109795660254		[learning rate: 0.00072457]
	Learning Rate: 0.000724568
	LOSS [training: 0.08190331800626512 | validation: 0.389366530406876]
	TIME [epoch: 8.38 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1412601522759399		[learning rate: 0.00072325]
		[batch 20/20] avg loss: 0.11069243519107445		[learning rate: 0.00072194]
	Learning Rate: 0.000721938
	LOSS [training: 0.12597629373350722 | validation: 0.037066779547499595]
	TIME [epoch: 8.3 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09022340954002643		[learning rate: 0.00072063]
		[batch 20/20] avg loss: 0.10004537738078464		[learning rate: 0.00071932]
	Learning Rate: 0.000719318
	LOSS [training: 0.09513439346040554 | validation: 0.08930055203942648]
	TIME [epoch: 8.36 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06965296373683265		[learning rate: 0.00071801]
		[batch 20/20] avg loss: 0.16494484993279504		[learning rate: 0.00071671]
	Learning Rate: 0.000716708
	LOSS [training: 0.11729890683481384 | validation: 0.11668731696076551]
	TIME [epoch: 8.37 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09781389232892443		[learning rate: 0.00071541]
		[batch 20/20] avg loss: 0.055046640920819546		[learning rate: 0.00071411]
	Learning Rate: 0.000714107
	LOSS [training: 0.07643026662487198 | validation: 0.032164524332975286]
	TIME [epoch: 8.4 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09287021023235018		[learning rate: 0.00071281]
		[batch 20/20] avg loss: 0.060750140275105544		[learning rate: 0.00071152]
	Learning Rate: 0.000711515
	LOSS [training: 0.07681017525372784 | validation: 0.04573998269749695]
	TIME [epoch: 8.38 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09211190446130102		[learning rate: 0.00071022]
		[batch 20/20] avg loss: 0.0818784328445005		[learning rate: 0.00070893]
	Learning Rate: 0.000708933
	LOSS [training: 0.08699516865290076 | validation: 0.039286910015555156]
	TIME [epoch: 8.38 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08428967729433975		[learning rate: 0.00070765]
		[batch 20/20] avg loss: 0.1055544437402379		[learning rate: 0.00070636]
	Learning Rate: 0.00070636
	LOSS [training: 0.09492206051728884 | validation: 0.10868374008420187]
	TIME [epoch: 8.38 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09253953647138022		[learning rate: 0.00070508]
		[batch 20/20] avg loss: 0.09791716018400978		[learning rate: 0.0007038]
	Learning Rate: 0.000703797
	LOSS [training: 0.09522834832769499 | validation: 0.15282820011744083]
	TIME [epoch: 8.36 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11138614337293493		[learning rate: 0.00070252]
		[batch 20/20] avg loss: 0.10461312331214441		[learning rate: 0.00070124]
	Learning Rate: 0.000701243
	LOSS [training: 0.10799963334253966 | validation: 0.24981326567480877]
	TIME [epoch: 8.35 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08128997818408681		[learning rate: 0.00069997]
		[batch 20/20] avg loss: 0.09013531505491813		[learning rate: 0.0006987]
	Learning Rate: 0.000698698
	LOSS [training: 0.08571264661950247 | validation: 0.049966370671382876]
	TIME [epoch: 8.33 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05780917273916507		[learning rate: 0.00069743]
		[batch 20/20] avg loss: 0.08183053157474846		[learning rate: 0.00069616]
	Learning Rate: 0.000696162
	LOSS [training: 0.06981985215695677 | validation: 0.22115211139548147]
	TIME [epoch: 8.33 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17489335064554676		[learning rate: 0.0006949]
		[batch 20/20] avg loss: 0.07762660209244404		[learning rate: 0.00069364]
	Learning Rate: 0.000693636
	LOSS [training: 0.12625997636899539 | validation: 0.20620901753757093]
	TIME [epoch: 8.33 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10445498726077644		[learning rate: 0.00069238]
		[batch 20/20] avg loss: 0.06371361376030102		[learning rate: 0.00069112]
	Learning Rate: 0.000691119
	LOSS [training: 0.08408430051053874 | validation: 0.04154825541567225]
	TIME [epoch: 8.33 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05361635555839885		[learning rate: 0.00068986]
		[batch 20/20] avg loss: 0.06971634994182119		[learning rate: 0.00068861]
	Learning Rate: 0.000688611
	LOSS [training: 0.06166635275011002 | validation: 0.02955310260906354]
	TIME [epoch: 8.31 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11085707727284799		[learning rate: 0.00068736]
		[batch 20/20] avg loss: 0.10544192940102678		[learning rate: 0.00068611]
	Learning Rate: 0.000686112
	LOSS [training: 0.10814950333693737 | validation: 0.03209696278227357]
	TIME [epoch: 8.35 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09762116235877948		[learning rate: 0.00068487]
		[batch 20/20] avg loss: 0.061989277162715795		[learning rate: 0.00068362]
	Learning Rate: 0.000683622
	LOSS [training: 0.07980521976074764 | validation: 0.03756492117869877]
	TIME [epoch: 8.35 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06864305064386315		[learning rate: 0.00068238]
		[batch 20/20] avg loss: 0.07025207204840131		[learning rate: 0.00068114]
	Learning Rate: 0.000681141
	LOSS [training: 0.06944756134613224 | validation: 0.13602899086248493]
	TIME [epoch: 8.34 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08939396559904382		[learning rate: 0.0006799]
		[batch 20/20] avg loss: 0.07441454615308231		[learning rate: 0.00067867]
	Learning Rate: 0.000678669
	LOSS [training: 0.08190425587606306 | validation: 0.10666530147152609]
	TIME [epoch: 8.34 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10322044664950387		[learning rate: 0.00067744]
		[batch 20/20] avg loss: 0.08856710128452758		[learning rate: 0.00067621]
	Learning Rate: 0.000676206
	LOSS [training: 0.09589377396701572 | validation: 0.06651007780844212]
	TIME [epoch: 8.35 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07524819062443198		[learning rate: 0.00067498]
		[batch 20/20] avg loss: 0.11864832104383498		[learning rate: 0.00067375]
	Learning Rate: 0.000673752
	LOSS [training: 0.0969482558341335 | validation: 0.10244106688717974]
	TIME [epoch: 8.34 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08734790478140896		[learning rate: 0.00067253]
		[batch 20/20] avg loss: 0.1077013739387026		[learning rate: 0.00067131]
	Learning Rate: 0.000671307
	LOSS [training: 0.09752463936005576 | validation: 0.09216764240443834]
	TIME [epoch: 8.35 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0705042270448935		[learning rate: 0.00067009]
		[batch 20/20] avg loss: 0.07807396599061087		[learning rate: 0.00066887]
	Learning Rate: 0.000668871
	LOSS [training: 0.07428909651775217 | validation: 0.058957173677295825]
	TIME [epoch: 8.34 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06948279380813577		[learning rate: 0.00066766]
		[batch 20/20] avg loss: 0.08225184542980582		[learning rate: 0.00066644]
	Learning Rate: 0.000666443
	LOSS [training: 0.07586731961897078 | validation: 0.04446204336643521]
	TIME [epoch: 8.34 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06731305358945795		[learning rate: 0.00066523]
		[batch 20/20] avg loss: 0.10852527754611405		[learning rate: 0.00066402]
	Learning Rate: 0.000664025
	LOSS [training: 0.08791916556778599 | validation: 0.19610850233149585]
	TIME [epoch: 8.3 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09231433318183044		[learning rate: 0.00066282]
		[batch 20/20] avg loss: 0.06144490584103117		[learning rate: 0.00066161]
	Learning Rate: 0.000661615
	LOSS [training: 0.07687961951143082 | validation: 0.08954414915499014]
	TIME [epoch: 8.28 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0863141438156177		[learning rate: 0.00066041]
		[batch 20/20] avg loss: 0.08246422029985331		[learning rate: 0.00065921]
	Learning Rate: 0.000659214
	LOSS [training: 0.08438918205773552 | validation: 0.057123890930001246]
	TIME [epoch: 8.29 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06904697178435788		[learning rate: 0.00065802]
		[batch 20/20] avg loss: 0.07763081433254382		[learning rate: 0.00065682]
	Learning Rate: 0.000656822
	LOSS [training: 0.07333889305845086 | validation: 0.05934825799131182]
	TIME [epoch: 8.29 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07984188730579535		[learning rate: 0.00065563]
		[batch 20/20] avg loss: 0.0854572806015794		[learning rate: 0.00065444]
	Learning Rate: 0.000654438
	LOSS [training: 0.08264958395368738 | validation: 0.045997769877960774]
	TIME [epoch: 8.31 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08221877515814169		[learning rate: 0.00065325]
		[batch 20/20] avg loss: 0.060549182281785144		[learning rate: 0.00065206]
	Learning Rate: 0.000652063
	LOSS [training: 0.07138397871996342 | validation: 0.04717062836011375]
	TIME [epoch: 8.33 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08053804794603271		[learning rate: 0.00065088]
		[batch 20/20] avg loss: 0.06691812612376076		[learning rate: 0.0006497]
	Learning Rate: 0.000649697
	LOSS [training: 0.07372808703489672 | validation: 0.05881269463873145]
	TIME [epoch: 8.35 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06749636042347548		[learning rate: 0.00064852]
		[batch 20/20] avg loss: 0.0892852571652029		[learning rate: 0.00064734]
	Learning Rate: 0.000647339
	LOSS [training: 0.0783908087943392 | validation: 0.054852853677696566]
	TIME [epoch: 8.31 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07645475366482236		[learning rate: 0.00064616]
		[batch 20/20] avg loss: 0.07719021180632253		[learning rate: 0.00064499]
	Learning Rate: 0.00064499
	LOSS [training: 0.07682248273557243 | validation: 0.10338264322576689]
	TIME [epoch: 8.33 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07462828867310288		[learning rate: 0.00064382]
		[batch 20/20] avg loss: 0.08050311550262189		[learning rate: 0.00064265]
	Learning Rate: 0.000642649
	LOSS [training: 0.0775657020878624 | validation: 0.0988842721195503]
	TIME [epoch: 8.34 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07668336165988973		[learning rate: 0.00064148]
		[batch 20/20] avg loss: 0.08696759855888514		[learning rate: 0.00064032]
	Learning Rate: 0.000640317
	LOSS [training: 0.08182548010938744 | validation: 0.11919588802909686]
	TIME [epoch: 8.34 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08088257041507477		[learning rate: 0.00063915]
		[batch 20/20] avg loss: 0.1232030263631535		[learning rate: 0.00063799]
	Learning Rate: 0.000637993
	LOSS [training: 0.10204279838911415 | validation: 0.132437523524804]
	TIME [epoch: 8.33 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07382289605741474		[learning rate: 0.00063683]
		[batch 20/20] avg loss: 0.05556197818840121		[learning rate: 0.00063568]
	Learning Rate: 0.000635677
	LOSS [training: 0.06469243712290797 | validation: 0.048398452464595176]
	TIME [epoch: 8.34 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06733587396248308		[learning rate: 0.00063452]
		[batch 20/20] avg loss: 0.07128964229888204		[learning rate: 0.00063337]
	Learning Rate: 0.000633371
	LOSS [training: 0.06931275813068256 | validation: 0.10193317216671258]
	TIME [epoch: 8.34 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07831295643891412		[learning rate: 0.00063222]
		[batch 20/20] avg loss: 0.0648081734140264		[learning rate: 0.00063107]
	Learning Rate: 0.000631072
	LOSS [training: 0.07156056492647025 | validation: 0.06354622173310112]
	TIME [epoch: 8.34 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06769931800457422		[learning rate: 0.00062993]
		[batch 20/20] avg loss: 0.07803065303383147		[learning rate: 0.00062878]
	Learning Rate: 0.000628782
	LOSS [training: 0.07286498551920284 | validation: 0.08726097504309498]
	TIME [epoch: 8.38 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0690576572129604		[learning rate: 0.00062764]
		[batch 20/20] avg loss: 0.09844109389937548		[learning rate: 0.0006265]
	Learning Rate: 0.0006265
	LOSS [training: 0.08374937555616795 | validation: 0.06633705166221424]
	TIME [epoch: 8.36 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.082604909238054		[learning rate: 0.00062536]
		[batch 20/20] avg loss: 0.0953427356711562		[learning rate: 0.00062423]
	Learning Rate: 0.000624226
	LOSS [training: 0.08897382245460508 | validation: 0.07844175710636202]
	TIME [epoch: 8.35 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06324902202738546		[learning rate: 0.00062309]
		[batch 20/20] avg loss: 0.07532918526936117		[learning rate: 0.00062196]
	Learning Rate: 0.000621961
	LOSS [training: 0.06928910364837332 | validation: 0.12929997141218713]
	TIME [epoch: 8.31 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08778860870805413		[learning rate: 0.00062083]
		[batch 20/20] avg loss: 0.06512709879540307		[learning rate: 0.0006197]
	Learning Rate: 0.000619704
	LOSS [training: 0.07645785375172862 | validation: 0.039070838124696194]
	TIME [epoch: 8.3 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08653754404755297		[learning rate: 0.00061858]
		[batch 20/20] avg loss: 0.08893128561906329		[learning rate: 0.00061745]
	Learning Rate: 0.000617455
	LOSS [training: 0.08773441483330813 | validation: 0.10834369994254446]
	TIME [epoch: 8.34 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06080947737213581		[learning rate: 0.00061633]
		[batch 20/20] avg loss: 0.0900116613834781		[learning rate: 0.00061521]
	Learning Rate: 0.000615214
	LOSS [training: 0.07541056937780695 | validation: 0.05751795867214713]
	TIME [epoch: 8.36 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07727984392230203		[learning rate: 0.0006141]
		[batch 20/20] avg loss: 0.08256135619787229		[learning rate: 0.00061298]
	Learning Rate: 0.000612982
	LOSS [training: 0.07992060006008717 | validation: 0.07331521368484135]
	TIME [epoch: 8.36 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0653530126025055		[learning rate: 0.00061187]
		[batch 20/20] avg loss: 0.06828635380522738		[learning rate: 0.00061076]
	Learning Rate: 0.000610757
	LOSS [training: 0.06681968320386647 | validation: 0.029599199633503447]
	TIME [epoch: 8.38 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07368030458702038		[learning rate: 0.00060965]
		[batch 20/20] avg loss: 0.07421267880369804		[learning rate: 0.00060854]
	Learning Rate: 0.00060854
	LOSS [training: 0.07394649169535922 | validation: 0.03557705186109156]
	TIME [epoch: 8.28 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06404416292375663		[learning rate: 0.00060744]
		[batch 20/20] avg loss: 0.07012135175323418		[learning rate: 0.00060633]
	Learning Rate: 0.000606332
	LOSS [training: 0.06708275733849539 | validation: 0.08068618203068091]
	TIME [epoch: 8.31 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07269841669601687		[learning rate: 0.00060523]
		[batch 20/20] avg loss: 0.09694346274835279		[learning rate: 0.00060413]
	Learning Rate: 0.000604132
	LOSS [training: 0.08482093972218484 | validation: 0.04168451388889524]
	TIME [epoch: 8.35 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06666770498708996		[learning rate: 0.00060303]
		[batch 20/20] avg loss: 0.08178313307987599		[learning rate: 0.00060194]
	Learning Rate: 0.000601939
	LOSS [training: 0.07422541903348297 | validation: 0.06413639896448156]
	TIME [epoch: 8.34 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05314360276457675		[learning rate: 0.00060085]
		[batch 20/20] avg loss: 0.06069653442259332		[learning rate: 0.00059975]
	Learning Rate: 0.000599755
	LOSS [training: 0.056920068593585026 | validation: 0.08667605838797028]
	TIME [epoch: 8.35 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05555821997014563		[learning rate: 0.00059867]
		[batch 20/20] avg loss: 0.12442328081612988		[learning rate: 0.00059758]
	Learning Rate: 0.000597578
	LOSS [training: 0.08999075039313777 | validation: 0.041998544864055694]
	TIME [epoch: 8.37 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042101200796885736		[learning rate: 0.00059649]
		[batch 20/20] avg loss: 0.05892473556733667		[learning rate: 0.00059541]
	Learning Rate: 0.00059541
	LOSS [training: 0.050512968182111206 | validation: 0.07743882372607522]
	TIME [epoch: 8.37 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09676889420664406		[learning rate: 0.00059433]
		[batch 20/20] avg loss: 0.10329154179860263		[learning rate: 0.00059325]
	Learning Rate: 0.000593249
	LOSS [training: 0.10003021800262335 | validation: 0.0290634134667382]
	TIME [epoch: 8.37 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05926482535171347		[learning rate: 0.00059217]
		[batch 20/20] avg loss: 0.09187780874554402		[learning rate: 0.0005911]
	Learning Rate: 0.000591096
	LOSS [training: 0.07557131704862875 | validation: 0.036980848646424425]
	TIME [epoch: 8.37 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06964853997314091		[learning rate: 0.00059002]
		[batch 20/20] avg loss: 0.07644175799170722		[learning rate: 0.00058895]
	Learning Rate: 0.000588951
	LOSS [training: 0.07304514898242404 | validation: 0.04492444841951161]
	TIME [epoch: 8.36 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06578422775117185		[learning rate: 0.00058788]
		[batch 20/20] avg loss: 0.06979619653208986		[learning rate: 0.00058681]
	Learning Rate: 0.000586813
	LOSS [training: 0.06779021214163086 | validation: 0.11068270366422756]
	TIME [epoch: 8.28 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07489663070088595		[learning rate: 0.00058575]
		[batch 20/20] avg loss: 0.09882818312623332		[learning rate: 0.00058468]
	Learning Rate: 0.000584684
	LOSS [training: 0.08686240691355965 | validation: 0.09504402050746105]
	TIME [epoch: 8.36 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07698338254478854		[learning rate: 0.00058362]
		[batch 20/20] avg loss: 0.05533371994810232		[learning rate: 0.00058256]
	Learning Rate: 0.000582562
	LOSS [training: 0.06615855124644543 | validation: 0.06378803212108855]
	TIME [epoch: 8.38 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06485910686130122		[learning rate: 0.0005815]
		[batch 20/20] avg loss: 0.0945674233835673		[learning rate: 0.00058045]
	Learning Rate: 0.000580448
	LOSS [training: 0.07971326512243428 | validation: 0.04836439739010228]
	TIME [epoch: 8.4 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0611099480612158		[learning rate: 0.00057939]
		[batch 20/20] avg loss: 0.05907190324868048		[learning rate: 0.00057834]
	Learning Rate: 0.000578341
	LOSS [training: 0.06009092565494815 | validation: 0.11362247667400467]
	TIME [epoch: 8.32 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07101279965227582		[learning rate: 0.00057729]
		[batch 20/20] avg loss: 0.04739501107675914		[learning rate: 0.00057624]
	Learning Rate: 0.000576243
	LOSS [training: 0.05920390536451747 | validation: 0.03135961164005283]
	TIME [epoch: 8.32 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057492083531732685		[learning rate: 0.0005752]
		[batch 20/20] avg loss: 0.07531237417761552		[learning rate: 0.00057415]
	Learning Rate: 0.000574151
	LOSS [training: 0.0664022288546741 | validation: 0.03326552242375826]
	TIME [epoch: 8.31 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06246136823057367		[learning rate: 0.00057311]
		[batch 20/20] avg loss: 0.07584169935575202		[learning rate: 0.00057207]
	Learning Rate: 0.000572068
	LOSS [training: 0.06915153379316286 | validation: 0.0304489737090396]
	TIME [epoch: 8.34 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05756630359087568		[learning rate: 0.00057103]
		[batch 20/20] avg loss: 0.07996756419767562		[learning rate: 0.00056999]
	Learning Rate: 0.000569992
	LOSS [training: 0.06876693389427564 | validation: 0.044438658342315494]
	TIME [epoch: 8.24 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07707435146986838		[learning rate: 0.00056896]
		[batch 20/20] avg loss: 0.07743980680576831		[learning rate: 0.00056792]
	Learning Rate: 0.000567923
	LOSS [training: 0.07725707913781835 | validation: 0.08133037267029672]
	TIME [epoch: 8.24 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07280585176946469		[learning rate: 0.00056689]
		[batch 20/20] avg loss: 0.058063788552676424		[learning rate: 0.00056586]
	Learning Rate: 0.000565862
	LOSS [training: 0.06543482016107055 | validation: 0.06754039993779894]
	TIME [epoch: 8.25 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07814456343226887		[learning rate: 0.00056483]
		[batch 20/20] avg loss: 0.06608364900371104		[learning rate: 0.00056381]
	Learning Rate: 0.000563808
	LOSS [training: 0.07211410621798997 | validation: 0.039494393304012845]
	TIME [epoch: 8.26 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06662660855088916		[learning rate: 0.00056278]
		[batch 20/20] avg loss: 0.07259330883833291		[learning rate: 0.00056176]
	Learning Rate: 0.000561762
	LOSS [training: 0.06960995869461103 | validation: 0.2015570282241112]
	TIME [epoch: 8.25 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10417402400007345		[learning rate: 0.00056074]
		[batch 20/20] avg loss: 0.0773983041503255		[learning rate: 0.00055972]
	Learning Rate: 0.000559724
	LOSS [training: 0.0907861640751995 | validation: 0.03856982444289195]
	TIME [epoch: 8.25 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06902252053256488		[learning rate: 0.00055871]
		[batch 20/20] avg loss: 0.05798529712289506		[learning rate: 0.00055769]
	Learning Rate: 0.000557692
	LOSS [training: 0.06350390882772997 | validation: 0.04720623566151039]
	TIME [epoch: 8.24 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08217195682041273		[learning rate: 0.00055668]
		[batch 20/20] avg loss: 0.04574513132997356		[learning rate: 0.00055567]
	Learning Rate: 0.000555669
	LOSS [training: 0.06395854407519315 | validation: 0.04165491507776319]
	TIME [epoch: 8.24 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061046004112420525		[learning rate: 0.00055466]
		[batch 20/20] avg loss: 0.06166943024247512		[learning rate: 0.00055365]
	Learning Rate: 0.000553652
	LOSS [training: 0.061357717177447826 | validation: 0.050502369774894834]
	TIME [epoch: 8.26 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060179551031234124		[learning rate: 0.00055265]
		[batch 20/20] avg loss: 0.056123915443053826		[learning rate: 0.00055164]
	Learning Rate: 0.000551643
	LOSS [training: 0.05815173323714399 | validation: 0.03077923522069562]
	TIME [epoch: 8.23 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06525298635327628		[learning rate: 0.00055064]
		[batch 20/20] avg loss: 0.07068907372551479		[learning rate: 0.00054964]
	Learning Rate: 0.000549641
	LOSS [training: 0.06797103003939554 | validation: 0.05292654922263317]
	TIME [epoch: 8.23 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06684671828959571		[learning rate: 0.00054864]
		[batch 20/20] avg loss: 0.0677463524944297		[learning rate: 0.00054765]
	Learning Rate: 0.000547646
	LOSS [training: 0.06729653539201269 | validation: 0.062422148779950136]
	TIME [epoch: 8.22 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06914632717028114		[learning rate: 0.00054665]
		[batch 20/20] avg loss: 0.056252106890071395		[learning rate: 0.00054566]
	Learning Rate: 0.000545659
	LOSS [training: 0.06269921703017627 | validation: 0.06399475100776261]
	TIME [epoch: 8.26 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06607299365949006		[learning rate: 0.00054467]
		[batch 20/20] avg loss: 0.07857602498082879		[learning rate: 0.00054368]
	Learning Rate: 0.000543678
	LOSS [training: 0.07232450932015941 | validation: 0.04168661323416425]
	TIME [epoch: 8.23 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07388265300839411		[learning rate: 0.00054269]
		[batch 20/20] avg loss: 0.06442276709104111		[learning rate: 0.00054171]
	Learning Rate: 0.000541705
	LOSS [training: 0.06915271004971761 | validation: 0.029223093122821033]
	TIME [epoch: 8.23 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0578238476669225		[learning rate: 0.00054072]
		[batch 20/20] avg loss: 0.12211573308933547		[learning rate: 0.00053974]
	Learning Rate: 0.00053974
	LOSS [training: 0.089969790378129 | validation: 0.1259572381905062]
	TIME [epoch: 8.24 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05934647310485931		[learning rate: 0.00053876]
		[batch 20/20] avg loss: 0.056781157882983005		[learning rate: 0.00053778]
	Learning Rate: 0.000537781
	LOSS [training: 0.058063815493921166 | validation: 0.023811359770084743]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_904.pth
	Model improved!!!
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04948111115979119		[learning rate: 0.0005368]
		[batch 20/20] avg loss: 0.09646659170209838		[learning rate: 0.00053583]
	Learning Rate: 0.000535829
	LOSS [training: 0.0729738514309448 | validation: 0.04694587500446994]
	TIME [epoch: 8.25 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07353829931452532		[learning rate: 0.00053486]
		[batch 20/20] avg loss: 0.043960727659471927		[learning rate: 0.00053388]
	Learning Rate: 0.000533885
	LOSS [training: 0.058749513486998625 | validation: 0.032405867673080964]
	TIME [epoch: 8.23 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0347530012002347		[learning rate: 0.00053291]
		[batch 20/20] avg loss: 0.05578543481589843		[learning rate: 0.00053195]
	Learning Rate: 0.000531947
	LOSS [training: 0.04526921800806656 | validation: 0.12044075354444207]
	TIME [epoch: 8.24 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06442154745414459		[learning rate: 0.00053098]
		[batch 20/20] avg loss: 0.05532395550525354		[learning rate: 0.00053002]
	Learning Rate: 0.000530017
	LOSS [training: 0.05987275147969906 | validation: 0.040763934476638966]
	TIME [epoch: 8.24 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06164461991328936		[learning rate: 0.00052905]
		[batch 20/20] avg loss: 0.06935042131726972		[learning rate: 0.00052809]
	Learning Rate: 0.000528093
	LOSS [training: 0.06549752061527953 | validation: 0.04846492033004146]
	TIME [epoch: 8.27 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0713117250460353		[learning rate: 0.00052713]
		[batch 20/20] avg loss: 0.062273415035258003		[learning rate: 0.00052618]
	Learning Rate: 0.000526177
	LOSS [training: 0.06679257004064666 | validation: 0.070743579697506]
	TIME [epoch: 8.23 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07902875576147061		[learning rate: 0.00052522]
		[batch 20/20] avg loss: 0.06428010716124324		[learning rate: 0.00052427]
	Learning Rate: 0.000524267
	LOSS [training: 0.07165443146135694 | validation: 0.031753381256450046]
	TIME [epoch: 8.24 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05303802364388044		[learning rate: 0.00052332]
		[batch 20/20] avg loss: 0.06912241063325164		[learning rate: 0.00052236]
	Learning Rate: 0.000522365
	LOSS [training: 0.06108021713856604 | validation: 0.06772043641287813]
	TIME [epoch: 8.23 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05559194427562297		[learning rate: 0.00052142]
		[batch 20/20] avg loss: 0.10714319259726483		[learning rate: 0.00052047]
	Learning Rate: 0.000520469
	LOSS [training: 0.08136756843644391 | validation: 0.08469652252992067]
	TIME [epoch: 8.26 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06460563186179866		[learning rate: 0.00051952]
		[batch 20/20] avg loss: 0.058123969468010475		[learning rate: 0.00051858]
	Learning Rate: 0.00051858
	LOSS [training: 0.06136480066490456 | validation: 0.06698543553330659]
	TIME [epoch: 8.25 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09103513752248092		[learning rate: 0.00051764]
		[batch 20/20] avg loss: 0.06864178480264219		[learning rate: 0.0005167]
	Learning Rate: 0.000516698
	LOSS [training: 0.07983846116256156 | validation: 0.06309314628286934]
	TIME [epoch: 8.24 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061161082724364504		[learning rate: 0.00051576]
		[batch 20/20] avg loss: 0.07041743947037704		[learning rate: 0.00051482]
	Learning Rate: 0.000514823
	LOSS [training: 0.0657892610973708 | validation: 0.09320444401457198]
	TIME [epoch: 8.24 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06965354713664279		[learning rate: 0.00051389]
		[batch 20/20] avg loss: 0.07757503901162068		[learning rate: 0.00051295]
	Learning Rate: 0.000512955
	LOSS [training: 0.07361429307413173 | validation: 0.06652260309051772]
	TIME [epoch: 8.25 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05922377920840298		[learning rate: 0.00051202]
		[batch 20/20] avg loss: 0.05330804773201821		[learning rate: 0.00051109]
	Learning Rate: 0.000511093
	LOSS [training: 0.05626591347021059 | validation: 0.13389545686123672]
	TIME [epoch: 8.26 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08049146579798143		[learning rate: 0.00051016]
		[batch 20/20] avg loss: 0.06474196163904151		[learning rate: 0.00050924]
	Learning Rate: 0.000509238
	LOSS [training: 0.07261671371851149 | validation: 0.04461686472898489]
	TIME [epoch: 8.23 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07426634044328842		[learning rate: 0.00050831]
		[batch 20/20] avg loss: 0.056993661750823044		[learning rate: 0.00050739]
	Learning Rate: 0.00050739
	LOSS [training: 0.06563000109705575 | validation: 0.09252053035681009]
	TIME [epoch: 8.24 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062110856099480735		[learning rate: 0.00050647]
		[batch 20/20] avg loss: 0.05394897486051421		[learning rate: 0.00050555]
	Learning Rate: 0.000505549
	LOSS [training: 0.05802991547999746 | validation: 0.03910731319990873]
	TIME [epoch: 8.24 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0844496366287184		[learning rate: 0.00050463]
		[batch 20/20] avg loss: 0.07068776331160245		[learning rate: 0.00050371]
	Learning Rate: 0.000503714
	LOSS [training: 0.07756869997016042 | validation: 0.06591747610281116]
	TIME [epoch: 8.25 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06080072732711084		[learning rate: 0.0005028]
		[batch 20/20] avg loss: 0.0604364474237831		[learning rate: 0.00050189]
	Learning Rate: 0.000501886
	LOSS [training: 0.06061858737544695 | validation: 0.046093352970032024]
	TIME [epoch: 8.24 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05688070739621599		[learning rate: 0.00050097]
		[batch 20/20] avg loss: 0.0853491765614654		[learning rate: 0.00050006]
	Learning Rate: 0.000500065
	LOSS [training: 0.07111494197884069 | validation: 0.06852489529596655]
	TIME [epoch: 8.23 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05198609846686146		[learning rate: 0.00049916]
		[batch 20/20] avg loss: 0.09035368890197745		[learning rate: 0.00049825]
	Learning Rate: 0.00049825
	LOSS [training: 0.07116989368441948 | validation: 0.028513263632491643]
	TIME [epoch: 8.25 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07406892613381924		[learning rate: 0.00049735]
		[batch 20/20] avg loss: 0.06249456093179477		[learning rate: 0.00049644]
	Learning Rate: 0.000496442
	LOSS [training: 0.068281743532807 | validation: 0.037425288708308205]
	TIME [epoch: 8.25 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04189940893196102		[learning rate: 0.00049554]
		[batch 20/20] avg loss: 0.07161075820119496		[learning rate: 0.00049464]
	Learning Rate: 0.00049464
	LOSS [training: 0.05675508356657799 | validation: 0.0831428782586093]
	TIME [epoch: 8.24 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0619543362032723		[learning rate: 0.00049374]
		[batch 20/20] avg loss: 0.054133175110422635		[learning rate: 0.00049285]
	Learning Rate: 0.000492845
	LOSS [training: 0.058043755656847475 | validation: 0.06330050801956161]
	TIME [epoch: 8.23 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07423757674549092		[learning rate: 0.00049195]
		[batch 20/20] avg loss: 0.06950002293183745		[learning rate: 0.00049106]
	Learning Rate: 0.000491057
	LOSS [training: 0.0718687998386642 | validation: 0.06746372673091681]
	TIME [epoch: 8.23 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08286666040976584		[learning rate: 0.00049016]
		[batch 20/20] avg loss: 0.0690504438047381		[learning rate: 0.00048927]
	Learning Rate: 0.000489275
	LOSS [training: 0.07595855210725198 | validation: 0.11420071985537386]
	TIME [epoch: 8.25 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07448437003441258		[learning rate: 0.00048839]
		[batch 20/20] avg loss: 0.06143064130654728		[learning rate: 0.0004875]
	Learning Rate: 0.000487499
	LOSS [training: 0.06795750567047994 | validation: 0.051553913438006714]
	TIME [epoch: 8.26 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06728060375210587		[learning rate: 0.00048661]
		[batch 20/20] avg loss: 0.060763265543796054		[learning rate: 0.00048573]
	Learning Rate: 0.00048573
	LOSS [training: 0.06402193464795096 | validation: 0.04273567091550955]
	TIME [epoch: 8.24 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07068534516415163		[learning rate: 0.00048485]
		[batch 20/20] avg loss: 0.06953479373527473		[learning rate: 0.00048397]
	Learning Rate: 0.000483967
	LOSS [training: 0.07011006944971318 | validation: 0.04756515747132854]
	TIME [epoch: 8.23 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06391015447315859		[learning rate: 0.00048309]
		[batch 20/20] avg loss: 0.06947339227104868		[learning rate: 0.00048221]
	Learning Rate: 0.000482211
	LOSS [training: 0.06669177337210364 | validation: 0.03995848170973793]
	TIME [epoch: 8.24 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07149992136825897		[learning rate: 0.00048133]
		[batch 20/20] avg loss: 0.08019113103997715		[learning rate: 0.00048046]
	Learning Rate: 0.000480461
	LOSS [training: 0.07584552620411807 | validation: 0.050628071957025476]
	TIME [epoch: 8.26 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06622157229984182		[learning rate: 0.00047959]
		[batch 20/20] avg loss: 0.06386776469227187		[learning rate: 0.00047872]
	Learning Rate: 0.000478717
	LOSS [training: 0.06504466849605683 | validation: 0.10218621106561655]
	TIME [epoch: 8.24 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07103456586357934		[learning rate: 0.00047785]
		[batch 20/20] avg loss: 0.04775368424582758		[learning rate: 0.00047698]
	Learning Rate: 0.00047698
	LOSS [training: 0.05939412505470346 | validation: 0.03176602537591293]
	TIME [epoch: 8.23 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06812741529898816		[learning rate: 0.00047611]
		[batch 20/20] avg loss: 0.0716839612462162		[learning rate: 0.00047525]
	Learning Rate: 0.000475249
	LOSS [training: 0.06990568827260218 | validation: 0.056406994066255475]
	TIME [epoch: 8.24 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07477584401465734		[learning rate: 0.00047439]
		[batch 20/20] avg loss: 0.06408996041244008		[learning rate: 0.00047352]
	Learning Rate: 0.000473524
	LOSS [training: 0.06943290221354871 | validation: 0.047940754218037146]
	TIME [epoch: 8.25 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08126862113723372		[learning rate: 0.00047266]
		[batch 20/20] avg loss: 0.08254210815040705		[learning rate: 0.00047181]
	Learning Rate: 0.000471806
	LOSS [training: 0.08190536464382037 | validation: 0.042176921679925705]
	TIME [epoch: 8.26 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05281359230832964		[learning rate: 0.00047095]
		[batch 20/20] avg loss: 0.056280036558824645		[learning rate: 0.00047009]
	Learning Rate: 0.000470093
	LOSS [training: 0.05454681443357714 | validation: 0.0484162562828005]
	TIME [epoch: 8.24 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04477938838782298		[learning rate: 0.00046924]
		[batch 20/20] avg loss: 0.05989554982150562		[learning rate: 0.00046839]
	Learning Rate: 0.000468388
	LOSS [training: 0.0523374691046643 | validation: 0.06663272528709727]
	TIME [epoch: 8.24 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06586861321590662		[learning rate: 0.00046754]
		[batch 20/20] avg loss: 0.05957798830966678		[learning rate: 0.00046669]
	Learning Rate: 0.000466688
	LOSS [training: 0.06272330076278668 | validation: 0.07799288143817681]
	TIME [epoch: 8.24 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0734806619021108		[learning rate: 0.00046584]
		[batch 20/20] avg loss: 0.07366489528825507		[learning rate: 0.00046499]
	Learning Rate: 0.000464994
	LOSS [training: 0.07357277859518294 | validation: 0.03801431364251146]
	TIME [epoch: 8.26 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056670600302401494		[learning rate: 0.00046415]
		[batch 20/20] avg loss: 0.059029750651011126		[learning rate: 0.00046331]
	Learning Rate: 0.000463307
	LOSS [training: 0.05785017547670631 | validation: 0.03907814529546565]
	TIME [epoch: 8.24 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060269013172448746		[learning rate: 0.00046247]
		[batch 20/20] avg loss: 0.06046936061804455		[learning rate: 0.00046163]
	Learning Rate: 0.000461625
	LOSS [training: 0.06036918689524665 | validation: 0.05349369806744812]
	TIME [epoch: 8.24 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06643177238650454		[learning rate: 0.00046079]
		[batch 20/20] avg loss: 0.06456264141859037		[learning rate: 0.00045995]
	Learning Rate: 0.00045995
	LOSS [training: 0.06549720690254746 | validation: 0.05221688862505786]
	TIME [epoch: 8.25 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06304060630012906		[learning rate: 0.00045911]
		[batch 20/20] avg loss: 0.07379610213367652		[learning rate: 0.00045828]
	Learning Rate: 0.000458281
	LOSS [training: 0.0684183542169028 | validation: 0.02853309179685087]
	TIME [epoch: 8.25 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0544443196084359		[learning rate: 0.00045745]
		[batch 20/20] avg loss: 0.05506033223004875		[learning rate: 0.00045662]
	Learning Rate: 0.000456618
	LOSS [training: 0.054752325919242326 | validation: 0.021426207973733173]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_949.pth
	Model improved!!!
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050201846613398396		[learning rate: 0.00045579]
		[batch 20/20] avg loss: 0.0536980219550051		[learning rate: 0.00045496]
	Learning Rate: 0.000454961
	LOSS [training: 0.05194993428420175 | validation: 0.04883725326462226]
	TIME [epoch: 8.25 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05301442954802069		[learning rate: 0.00045413]
		[batch 20/20] avg loss: 0.07331300362428603		[learning rate: 0.00045331]
	Learning Rate: 0.000453309
	LOSS [training: 0.06316371658615336 | validation: 0.0859051971956112]
	TIME [epoch: 8.24 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07030992287283286		[learning rate: 0.00045249]
		[batch 20/20] avg loss: 0.062198940969454455		[learning rate: 0.00045166]
	Learning Rate: 0.000451664
	LOSS [training: 0.06625443192114365 | validation: 0.06033535475370447]
	TIME [epoch: 8.24 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04155894845374809		[learning rate: 0.00045084]
		[batch 20/20] avg loss: 0.05675206390845039		[learning rate: 0.00045003]
	Learning Rate: 0.000450025
	LOSS [training: 0.04915550618109924 | validation: 0.04080652850472053]
	TIME [epoch: 8.26 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06932558938844281		[learning rate: 0.00044921]
		[batch 20/20] avg loss: 0.042709396087940846		[learning rate: 0.00044839]
	Learning Rate: 0.000448392
	LOSS [training: 0.05601749273819183 | validation: 0.02431791684465347]
	TIME [epoch: 8.24 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053967136225985125		[learning rate: 0.00044758]
		[batch 20/20] avg loss: 0.06380963071236159		[learning rate: 0.00044676]
	Learning Rate: 0.000446765
	LOSS [training: 0.058888383469173355 | validation: 0.048843864713804995]
	TIME [epoch: 8.24 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05189230086328991		[learning rate: 0.00044595]
		[batch 20/20] avg loss: 0.054528784419426615		[learning rate: 0.00044514]
	Learning Rate: 0.000445143
	LOSS [training: 0.05321054264135826 | validation: 0.031071686110035104]
	TIME [epoch: 8.24 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06710896561178024		[learning rate: 0.00044434]
		[batch 20/20] avg loss: 0.052643305935294815		[learning rate: 0.00044353]
	Learning Rate: 0.000443528
	LOSS [training: 0.059876135773537544 | validation: 0.0563123392295276]
	TIME [epoch: 8.26 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05063803811587677		[learning rate: 0.00044272]
		[batch 20/20] avg loss: 0.04827165960053541		[learning rate: 0.00044192]
	Learning Rate: 0.000441918
	LOSS [training: 0.04945484885820609 | validation: 0.0367975199028732]
	TIME [epoch: 8.24 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058400645156070394		[learning rate: 0.00044112]
		[batch 20/20] avg loss: 0.07692726138459918		[learning rate: 0.00044031]
	Learning Rate: 0.000440315
	LOSS [training: 0.06766395327033478 | validation: 0.026805343289171286]
	TIME [epoch: 8.24 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08686845197774143		[learning rate: 0.00043952]
		[batch 20/20] avg loss: 0.04629797151321497		[learning rate: 0.00043872]
	Learning Rate: 0.000438717
	LOSS [training: 0.06658321174547822 | validation: 0.07881132349885465]
	TIME [epoch: 8.23 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06747493322290615		[learning rate: 0.00043792]
		[batch 20/20] avg loss: 0.050638758478701716		[learning rate: 0.00043712]
	Learning Rate: 0.000437125
	LOSS [training: 0.05905684585080394 | validation: 0.030814348493185335]
	TIME [epoch: 8.25 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06956432768412091		[learning rate: 0.00043633]
		[batch 20/20] avg loss: 0.03466612828190037		[learning rate: 0.00043554]
	Learning Rate: 0.000435538
	LOSS [training: 0.05211522798301064 | validation: 0.015572091522178247]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_962.pth
	Model improved!!!
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049385582995012015		[learning rate: 0.00043475]
		[batch 20/20] avg loss: 0.041319027801071995		[learning rate: 0.00043396]
	Learning Rate: 0.000433958
	LOSS [training: 0.045352305398042005 | validation: 0.0348457691882116]
	TIME [epoch: 8.26 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04725320235072446		[learning rate: 0.00043317]
		[batch 20/20] avg loss: 0.06358876919278675		[learning rate: 0.00043238]
	Learning Rate: 0.000432383
	LOSS [training: 0.055420985771755596 | validation: 0.03652560098633088]
	TIME [epoch: 8.26 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05501601316519674		[learning rate: 0.0004316]
		[batch 20/20] avg loss: 0.06126320262381826		[learning rate: 0.00043081]
	Learning Rate: 0.000430814
	LOSS [training: 0.05813960789450749 | validation: 0.04677415700934806]
	TIME [epoch: 8.26 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051822882533249036		[learning rate: 0.00043003]
		[batch 20/20] avg loss: 0.046462646879348396		[learning rate: 0.00042925]
	Learning Rate: 0.00042925
	LOSS [training: 0.04914276470629872 | validation: 0.02148537380062211]
	TIME [epoch: 8.28 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04905117865655571		[learning rate: 0.00042847]
		[batch 20/20] avg loss: 0.05637010060866744		[learning rate: 0.00042769]
	Learning Rate: 0.000427692
	LOSS [training: 0.05271063963261158 | validation: 0.030331419649484704]
	TIME [epoch: 8.26 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05152495854243121		[learning rate: 0.00042692]
		[batch 20/20] avg loss: 0.05741630109992295		[learning rate: 0.00042614]
	Learning Rate: 0.00042614
	LOSS [training: 0.05447062982117708 | validation: 0.034521770623336646]
	TIME [epoch: 8.26 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06905110968372456		[learning rate: 0.00042537]
		[batch 20/20] avg loss: 0.061092569717898934		[learning rate: 0.00042459]
	Learning Rate: 0.000424594
	LOSS [training: 0.06507183970081173 | validation: 0.03869137465096767]
	TIME [epoch: 8.25 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04009283793278427		[learning rate: 0.00042382]
		[batch 20/20] avg loss: 0.0445684132943721		[learning rate: 0.00042305]
	Learning Rate: 0.000423053
	LOSS [training: 0.042330625613578185 | validation: 0.04118170720237688]
	TIME [epoch: 8.29 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0566439009136611		[learning rate: 0.00042228]
		[batch 20/20] avg loss: 0.05025912834482915		[learning rate: 0.00042152]
	Learning Rate: 0.000421518
	LOSS [training: 0.05345151462924512 | validation: 0.03327904309330275]
	TIME [epoch: 8.26 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04829222035880529		[learning rate: 0.00042075]
		[batch 20/20] avg loss: 0.058471067290008616		[learning rate: 0.00041999]
	Learning Rate: 0.000419988
	LOSS [training: 0.053381643824406945 | validation: 0.033456487081083106]
	TIME [epoch: 8.26 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08317734828873359		[learning rate: 0.00041923]
		[batch 20/20] avg loss: 0.06969437600594776		[learning rate: 0.00041846]
	Learning Rate: 0.000418464
	LOSS [training: 0.07643586214734069 | validation: 0.026091393871010796]
	TIME [epoch: 8.26 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04000152797156577		[learning rate: 0.0004177]
		[batch 20/20] avg loss: 0.07553318759865538		[learning rate: 0.00041695]
	Learning Rate: 0.000416945
	LOSS [training: 0.05776735778511057 | validation: 0.04079117065612623]
	TIME [epoch: 8.28 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05735818468345048		[learning rate: 0.00041619]
		[batch 20/20] avg loss: 0.041210357005179304		[learning rate: 0.00041543]
	Learning Rate: 0.000415432
	LOSS [training: 0.0492842708443149 | validation: 0.08025680131638942]
	TIME [epoch: 8.27 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05267017834130028		[learning rate: 0.00041468]
		[batch 20/20] avg loss: 0.041525290024356384		[learning rate: 0.00041392]
	Learning Rate: 0.000413924
	LOSS [training: 0.04709773418282834 | validation: 0.039682488835552074]
	TIME [epoch: 8.26 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055921536847845524		[learning rate: 0.00041317]
		[batch 20/20] avg loss: 0.0469754795461642		[learning rate: 0.00041242]
	Learning Rate: 0.000412422
	LOSS [training: 0.051448508197004875 | validation: 0.1400884740616295]
	TIME [epoch: 8.26 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09101718090151628		[learning rate: 0.00041167]
		[batch 20/20] avg loss: 0.06691645985333775		[learning rate: 0.00041093]
	Learning Rate: 0.000410926
	LOSS [training: 0.078966820377427 | validation: 0.041429716972407195]
	TIME [epoch: 8.26 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040618620123266364		[learning rate: 0.00041018]
		[batch 20/20] avg loss: 0.05835849695618054		[learning rate: 0.00040943]
	Learning Rate: 0.000409434
	LOSS [training: 0.04948855853972346 | validation: 0.049476544709752324]
	TIME [epoch: 8.29 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05675143222197049		[learning rate: 0.00040869]
		[batch 20/20] avg loss: 0.05245298131477939		[learning rate: 0.00040795]
	Learning Rate: 0.000407948
	LOSS [training: 0.05460220676837494 | validation: 0.046604287203505665]
	TIME [epoch: 8.27 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049382171870926805		[learning rate: 0.00040721]
		[batch 20/20] avg loss: 0.030106774973563243		[learning rate: 0.00040647]
	Learning Rate: 0.000406468
	LOSS [training: 0.03974447342224501 | validation: 0.027038874946990758]
	TIME [epoch: 8.26 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048352897461204844		[learning rate: 0.00040573]
		[batch 20/20] avg loss: 0.06886103166381159		[learning rate: 0.00040499]
	Learning Rate: 0.000404993
	LOSS [training: 0.05860696456250821 | validation: 0.06777083884639398]
	TIME [epoch: 8.26 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08253074034978311		[learning rate: 0.00040426]
		[batch 20/20] avg loss: 0.061933170307283916		[learning rate: 0.00040352]
	Learning Rate: 0.000403523
	LOSS [training: 0.0722319553285335 | validation: 0.054624104428069586]
	TIME [epoch: 8.28 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05338116281142167		[learning rate: 0.00040279]
		[batch 20/20] avg loss: 0.06902701411429393		[learning rate: 0.00040206]
	Learning Rate: 0.000402059
	LOSS [training: 0.06120408846285781 | validation: 0.040353195732242664]
	TIME [epoch: 8.27 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04072185520871822		[learning rate: 0.00040133]
		[batch 20/20] avg loss: 0.07071714134356157		[learning rate: 0.0004006]
	Learning Rate: 0.0004006
	LOSS [training: 0.05571949827613989 | validation: 0.10387538338741964]
	TIME [epoch: 8.26 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0443031896855325		[learning rate: 0.00039987]
		[batch 20/20] avg loss: 0.0336111260621477		[learning rate: 0.00039915]
	Learning Rate: 0.000399146
	LOSS [training: 0.0389571578738401 | validation: 0.04289347691102802]
	TIME [epoch: 8.27 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05593575454479036		[learning rate: 0.00039842]
		[batch 20/20] avg loss: 0.057949799308830915		[learning rate: 0.0003977]
	Learning Rate: 0.000397697
	LOSS [training: 0.056942776926810657 | validation: 0.039950028906094934]
	TIME [epoch: 8.26 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05983059463734645		[learning rate: 0.00039698]
		[batch 20/20] avg loss: 0.04076186375249517		[learning rate: 0.00039625]
	Learning Rate: 0.000396254
	LOSS [training: 0.050296229194920807 | validation: 0.019264746449054997]
	TIME [epoch: 8.29 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045216597967113296		[learning rate: 0.00039553]
		[batch 20/20] avg loss: 0.06003351136969255		[learning rate: 0.00039482]
	Learning Rate: 0.000394816
	LOSS [training: 0.05262505466840293 | validation: 0.0333809465400721]
	TIME [epoch: 8.26 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03663991315600715		[learning rate: 0.0003941]
		[batch 20/20] avg loss: 0.05733995181456523		[learning rate: 0.00039338]
	Learning Rate: 0.000393383
	LOSS [training: 0.04698993248528619 | validation: 0.030619914901480028]
	TIME [epoch: 8.27 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05659716715940057		[learning rate: 0.00039267]
		[batch 20/20] avg loss: 0.04875085229279635		[learning rate: 0.00039196]
	Learning Rate: 0.000391956
	LOSS [training: 0.05267400972609846 | validation: 0.06678386792864975]
	TIME [epoch: 8.25 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05694693966746249		[learning rate: 0.00039124]
		[batch 20/20] avg loss: 0.04502320343583638		[learning rate: 0.00039053]
	Learning Rate: 0.000390533
	LOSS [training: 0.05098507155164943 | validation: 0.031325844501293346]
	TIME [epoch: 8.29 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058582047697088205		[learning rate: 0.00038982]
		[batch 20/20] avg loss: 0.050688142413368234		[learning rate: 0.00038912]
	Learning Rate: 0.000389116
	LOSS [training: 0.05463509505522822 | validation: 0.02363425314459074]
	TIME [epoch: 8.26 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03892259863860463		[learning rate: 0.00038841]
		[batch 20/20] avg loss: 0.047423147272473515		[learning rate: 0.0003877]
	Learning Rate: 0.000387704
	LOSS [training: 0.04317287295553907 | validation: 0.04989343952424133]
	TIME [epoch: 8.26 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04437960770304754		[learning rate: 0.000387]
		[batch 20/20] avg loss: 0.07814007349411307		[learning rate: 0.0003863]
	Learning Rate: 0.000386297
	LOSS [training: 0.061259840598580306 | validation: 0.04230438043347815]
	TIME [epoch: 8.26 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06074915873720295		[learning rate: 0.0003856]
		[batch 20/20] avg loss: 0.05119554653779167		[learning rate: 0.00038489]
	Learning Rate: 0.000384895
	LOSS [training: 0.0559723526374973 | validation: 0.027055150218468434]
	TIME [epoch: 8.28 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04602059546410141		[learning rate: 0.0003842]
		[batch 20/20] avg loss: 0.04282554562565925		[learning rate: 0.0003835]
	Learning Rate: 0.000383498
	LOSS [training: 0.04442307054488033 | validation: 0.06634417204255874]
	TIME [epoch: 8.27 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04792665847693249		[learning rate: 0.0003828]
		[batch 20/20] avg loss: 0.05569342965074554		[learning rate: 0.00038211]
	Learning Rate: 0.000382106
	LOSS [training: 0.051810044063839024 | validation: 0.032961456089652244]
	TIME [epoch: 8.26 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05810259811688042		[learning rate: 0.00038141]
		[batch 20/20] avg loss: 0.04667525442832802		[learning rate: 0.00038072]
	Learning Rate: 0.00038072
	LOSS [training: 0.05238892627260422 | validation: 0.02932396955626915]
	TIME [epoch: 8.26 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052477740660048844		[learning rate: 0.00038003]
		[batch 20/20] avg loss: 0.063487493405365		[learning rate: 0.00037934]
	Learning Rate: 0.000379338
	LOSS [training: 0.05798261703270691 | validation: 0.05547544891069366]
	TIME [epoch: 8.26 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06422029388244031		[learning rate: 0.00037865]
		[batch 20/20] avg loss: 0.05222522707359055		[learning rate: 0.00037796]
	Learning Rate: 0.000377961
	LOSS [training: 0.05822276047801543 | validation: 0.023745011055270823]
	TIME [epoch: 8.29 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03757695985305138		[learning rate: 0.00037727]
		[batch 20/20] avg loss: 0.042104082769506695		[learning rate: 0.00037659]
	Learning Rate: 0.00037659
	LOSS [training: 0.03984052131127905 | validation: 0.04881079501426269]
	TIME [epoch: 8.26 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03583126265036067		[learning rate: 0.00037591]
		[batch 20/20] avg loss: 0.04552555514431006		[learning rate: 0.00037522]
	Learning Rate: 0.000375223
	LOSS [training: 0.040678408897335365 | validation: 0.0396573339530753]
	TIME [epoch: 8.27 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0765169355480072		[learning rate: 0.00037454]
		[batch 20/20] avg loss: 0.047951686347544056		[learning rate: 0.00037386]
	Learning Rate: 0.000373861
	LOSS [training: 0.06223431094777563 | validation: 0.04806786313310395]
	TIME [epoch: 8.26 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05444067064425059		[learning rate: 0.00037318]
		[batch 20/20] avg loss: 0.06719792248700877		[learning rate: 0.0003725]
	Learning Rate: 0.000372505
	LOSS [training: 0.06081929656562969 | validation: 0.04154218525062662]
	TIME [epoch: 8.28 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044791751811857544		[learning rate: 0.00037183]
		[batch 20/20] avg loss: 0.055239441406649804		[learning rate: 0.00037115]
	Learning Rate: 0.000371153
	LOSS [training: 0.050015596609253664 | validation: 0.026718789463315986]
	TIME [epoch: 8.26 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05567309309647942		[learning rate: 0.00037048]
		[batch 20/20] avg loss: 0.05708167203904084		[learning rate: 0.00036981]
	Learning Rate: 0.000369806
	LOSS [training: 0.05637738256776013 | validation: 0.03227117577489997]
	TIME [epoch: 8.26 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051894773049017226		[learning rate: 0.00036913]
		[batch 20/20] avg loss: 0.03248015556702276		[learning rate: 0.00036846]
	Learning Rate: 0.000368464
	LOSS [training: 0.04218746430802 | validation: 0.02737127172803052]
	TIME [epoch: 8.26 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048994579377503886		[learning rate: 0.00036779]
		[batch 20/20] avg loss: 0.05797965158699207		[learning rate: 0.00036713]
	Learning Rate: 0.000367127
	LOSS [training: 0.05348711548224797 | validation: 0.04319785525126092]
	TIME [epoch: 8.27 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0405642985191607		[learning rate: 0.00036646]
		[batch 20/20] avg loss: 0.05700670213160949		[learning rate: 0.00036579]
	Learning Rate: 0.000365794
	LOSS [training: 0.048785500325385096 | validation: 0.027201113071631738]
	TIME [epoch: 8.28 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05297908495748109		[learning rate: 0.00036513]
		[batch 20/20] avg loss: 0.04314522657202038		[learning rate: 0.00036447]
	Learning Rate: 0.000364467
	LOSS [training: 0.048062155764750744 | validation: 0.07462449313043046]
	TIME [epoch: 8.26 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06277459522166304		[learning rate: 0.0003638]
		[batch 20/20] avg loss: 0.04593056885927725		[learning rate: 0.00036314]
	Learning Rate: 0.000363144
	LOSS [training: 0.05435258204047015 | validation: 0.06004005124184016]
	TIME [epoch: 8.26 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06820682924123515		[learning rate: 0.00036248]
		[batch 20/20] avg loss: 0.04365217010754921		[learning rate: 0.00036183]
	Learning Rate: 0.000361826
	LOSS [training: 0.05592949967439219 | validation: 0.019891226942457513]
	TIME [epoch: 8.26 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045506809721023674		[learning rate: 0.00036117]
		[batch 20/20] avg loss: 0.055679462244531616		[learning rate: 0.00036051]
	Learning Rate: 0.000360513
	LOSS [training: 0.05059313598277765 | validation: 0.024506402652662223]
	TIME [epoch: 8.28 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04317224989810888		[learning rate: 0.00035986]
		[batch 20/20] avg loss: 0.049029977751435215		[learning rate: 0.0003592]
	Learning Rate: 0.000359205
	LOSS [training: 0.046101113824772055 | validation: 0.025932918601483532]
	TIME [epoch: 8.27 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0359141160049054		[learning rate: 0.00035855]
		[batch 20/20] avg loss: 0.05433400170370697		[learning rate: 0.0003579]
	Learning Rate: 0.000357901
	LOSS [training: 0.045124058854306176 | validation: 0.02599334190818137]
	TIME [epoch: 8.26 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05822468305276361		[learning rate: 0.00035725]
		[batch 20/20] avg loss: 0.048832758053839076		[learning rate: 0.0003566]
	Learning Rate: 0.000356602
	LOSS [training: 0.053528720553301344 | validation: 0.02723811327650351]
	TIME [epoch: 8.26 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041601920336972825		[learning rate: 0.00035595]
		[batch 20/20] avg loss: 0.040471444387806685		[learning rate: 0.00035531]
	Learning Rate: 0.000355308
	LOSS [training: 0.04103668236238974 | validation: 0.03471433528491078]
	TIME [epoch: 8.28 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05098145660176281		[learning rate: 0.00035466]
		[batch 20/20] avg loss: 0.05204241411996755		[learning rate: 0.00035402]
	Learning Rate: 0.000354019
	LOSS [training: 0.05151193536086518 | validation: 0.1979753158708325]
	TIME [epoch: 8.27 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08981032945711856		[learning rate: 0.00035338]
		[batch 20/20] avg loss: 0.03215783875391974		[learning rate: 0.00035273]
	Learning Rate: 0.000352734
	LOSS [training: 0.06098408410551914 | validation: 0.018075048382331024]
	TIME [epoch: 8.26 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0435372244787144		[learning rate: 0.00035209]
		[batch 20/20] avg loss: 0.04014858910835496		[learning rate: 0.00035145]
	Learning Rate: 0.000351454
	LOSS [training: 0.041842906793534675 | validation: 0.022030886198769375]
	TIME [epoch: 8.26 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04377841222551832		[learning rate: 0.00035082]
		[batch 20/20] avg loss: 0.06208484076906843		[learning rate: 0.00035018]
	Learning Rate: 0.000350179
	LOSS [training: 0.05293162649729338 | validation: 0.026719908725182158]
	TIME [epoch: 8.25 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042592444126481215		[learning rate: 0.00034954]
		[batch 20/20] avg loss: 0.0464365018986371		[learning rate: 0.00034891]
	Learning Rate: 0.000348908
	LOSS [training: 0.04451447301255916 | validation: 0.05072991725334475]
	TIME [epoch: 8.29 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07396565551564353		[learning rate: 0.00034827]
		[batch 20/20] avg loss: 0.034815041343925324		[learning rate: 0.00034764]
	Learning Rate: 0.000347641
	LOSS [training: 0.05439034842978443 | validation: 0.010065258580899652]
	TIME [epoch: 8.26 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_1024.pth
	Model improved!!!
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03935273960175474		[learning rate: 0.00034701]
		[batch 20/20] avg loss: 0.04408005343409998		[learning rate: 0.00034638]
	Learning Rate: 0.00034638
	LOSS [training: 0.041716396517927364 | validation: 0.038355099369589356]
	TIME [epoch: 8.25 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05347633392638533		[learning rate: 0.00034575]
		[batch 20/20] avg loss: 0.04968770086546578		[learning rate: 0.00034512]
	Learning Rate: 0.000345123
	LOSS [training: 0.051582017395925564 | validation: 0.037985087539591796]
	TIME [epoch: 8.25 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04485521999707574		[learning rate: 0.0003445]
		[batch 20/20] avg loss: 0.04393133407082702		[learning rate: 0.00034387]
	Learning Rate: 0.00034387
	LOSS [training: 0.04439327703395138 | validation: 0.030615606915698087]
	TIME [epoch: 8.28 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0553559451510091		[learning rate: 0.00034325]
		[batch 20/20] avg loss: 0.07726447482212395		[learning rate: 0.00034262]
	Learning Rate: 0.000342622
	LOSS [training: 0.06631020998656652 | validation: 0.06395900677237155]
	TIME [epoch: 8.26 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042444944645331426		[learning rate: 0.000342]
		[batch 20/20] avg loss: 0.04296645522956843		[learning rate: 0.00034138]
	Learning Rate: 0.000341379
	LOSS [training: 0.04270569993744992 | validation: 0.023916572755080127]
	TIME [epoch: 8.25 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03315280330189187		[learning rate: 0.00034076]
		[batch 20/20] avg loss: 0.040256320871447884		[learning rate: 0.00034014]
	Learning Rate: 0.00034014
	LOSS [training: 0.03670456208666987 | validation: 0.029431010518048546]
	TIME [epoch: 8.26 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04516332934022034		[learning rate: 0.00033952]
		[batch 20/20] avg loss: 0.03643136115313068		[learning rate: 0.00033891]
	Learning Rate: 0.000338906
	LOSS [training: 0.04079734524667552 | validation: 0.07754718883793313]
	TIME [epoch: 8.27 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06266309322305676		[learning rate: 0.00033829]
		[batch 20/20] avg loss: 0.03648111664225848		[learning rate: 0.00033768]
	Learning Rate: 0.000337676
	LOSS [training: 0.049572104932657624 | validation: 0.02056527772866618]
	TIME [epoch: 8.26 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03707987859843683		[learning rate: 0.00033706]
		[batch 20/20] avg loss: 0.05058837472405148		[learning rate: 0.00033645]
	Learning Rate: 0.00033645
	LOSS [training: 0.04383412666124416 | validation: 0.03265028974453233]
	TIME [epoch: 8.26 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07380378852339078		[learning rate: 0.00033584]
		[batch 20/20] avg loss: 0.054815043048825826		[learning rate: 0.00033523]
	Learning Rate: 0.000335229
	LOSS [training: 0.0643094157861083 | validation: 0.035029060060281836]
	TIME [epoch: 8.25 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056996146374541354		[learning rate: 0.00033462]
		[batch 20/20] avg loss: 0.05290865802501945		[learning rate: 0.00033401]
	Learning Rate: 0.000334013
	LOSS [training: 0.05495240219978041 | validation: 0.024100534167700208]
	TIME [epoch: 8.26 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03801411143262286		[learning rate: 0.00033341]
		[batch 20/20] avg loss: 0.04355822668611378		[learning rate: 0.0003328]
	Learning Rate: 0.000332801
	LOSS [training: 0.04078616905936833 | validation: 0.07000601406986509]
	TIME [epoch: 8.28 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04244180368872959		[learning rate: 0.0003322]
		[batch 20/20] avg loss: 0.04983409015828899		[learning rate: 0.00033159]
	Learning Rate: 0.000331593
	LOSS [training: 0.04613794692350928 | validation: 0.031224288419610592]
	TIME [epoch: 8.26 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046838492706309076		[learning rate: 0.00033099]
		[batch 20/20] avg loss: 0.02636936733152135		[learning rate: 0.00033039]
	Learning Rate: 0.00033039
	LOSS [training: 0.036603930018915216 | validation: 0.02180253208773198]
	TIME [epoch: 8.25 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0547756299934727		[learning rate: 0.00032979]
		[batch 20/20] avg loss: 0.04235487542634253		[learning rate: 0.00032919]
	Learning Rate: 0.000329191
	LOSS [training: 0.04856525270990761 | validation: 0.04950365226842514]
	TIME [epoch: 8.25 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0452879597365445		[learning rate: 0.00032859]
		[batch 20/20] avg loss: 0.05558078346323474		[learning rate: 0.000328]
	Learning Rate: 0.000327996
	LOSS [training: 0.050434371599889616 | validation: 0.04034115410673506]
	TIME [epoch: 8.27 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06112795289517109		[learning rate: 0.0003274]
		[batch 20/20] avg loss: 0.04378753559307394		[learning rate: 0.00032681]
	Learning Rate: 0.000326806
	LOSS [training: 0.05245774424412253 | validation: 0.09776297679452725]
	TIME [epoch: 8.26 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06822514729549035		[learning rate: 0.00032621]
		[batch 20/20] avg loss: 0.056546498703764436		[learning rate: 0.00032562]
	Learning Rate: 0.00032562
	LOSS [training: 0.062385822999627395 | validation: 0.07725975160979334]
	TIME [epoch: 8.26 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05346641492463572		[learning rate: 0.00032503]
		[batch 20/20] avg loss: 0.045226966848432124		[learning rate: 0.00032444]
	Learning Rate: 0.000324438
	LOSS [training: 0.049346690886533914 | validation: 0.05842549671709068]
	TIME [epoch: 8.25 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04000283286698235		[learning rate: 0.00032385]
		[batch 20/20] avg loss: 0.05727981546782849		[learning rate: 0.00032326]
	Learning Rate: 0.00032326
	LOSS [training: 0.048641324167405416 | validation: 0.030024481539512805]
	TIME [epoch: 8.27 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046751949147022125		[learning rate: 0.00032267]
		[batch 20/20] avg loss: 0.05247703783831187		[learning rate: 0.00032209]
	Learning Rate: 0.000322087
	LOSS [training: 0.04961449349266699 | validation: 0.022338328674788275]
	TIME [epoch: 8.27 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03879599345353472		[learning rate: 0.0003215]
		[batch 20/20] avg loss: 0.05821404008980019		[learning rate: 0.00032092]
	Learning Rate: 0.000320918
	LOSS [training: 0.04850501677166745 | validation: 0.02514934491428858]
	TIME [epoch: 8.25 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034034735601017656		[learning rate: 0.00032034]
		[batch 20/20] avg loss: 0.036827528701948595		[learning rate: 0.00031975]
	Learning Rate: 0.000319754
	LOSS [training: 0.03543113215148312 | validation: 0.05565146543255099]
	TIME [epoch: 8.25 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04674807100777543		[learning rate: 0.00031917]
		[batch 20/20] avg loss: 0.04619262641052489		[learning rate: 0.00031859]
	Learning Rate: 0.000318593
	LOSS [training: 0.04647034870915016 | validation: 0.02788919199599263]
	TIME [epoch: 8.25 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05418629247314224		[learning rate: 0.00031801]
		[batch 20/20] avg loss: 0.046185966777587885		[learning rate: 0.00031744]
	Learning Rate: 0.000317437
	LOSS [training: 0.05018612962536506 | validation: 0.059953960983472665]
	TIME [epoch: 8.27 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0543115649218842		[learning rate: 0.00031686]
		[batch 20/20] avg loss: 0.051813464491760094		[learning rate: 0.00031629]
	Learning Rate: 0.000316285
	LOSS [training: 0.05306251470682215 | validation: 0.020379423040679034]
	TIME [epoch: 8.26 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03491563274702066		[learning rate: 0.00031571]
		[batch 20/20] avg loss: 0.05554720326522674		[learning rate: 0.00031514]
	Learning Rate: 0.000315137
	LOSS [training: 0.045231418006123694 | validation: 0.03329036031361192]
	TIME [epoch: 8.25 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08010848350828359		[learning rate: 0.00031457]
		[batch 20/20] avg loss: 0.06060873038694562		[learning rate: 0.00031399]
	Learning Rate: 0.000313994
	LOSS [training: 0.07035860694761462 | validation: 0.048186149687354934]
	TIME [epoch: 8.25 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041698858360207765		[learning rate: 0.00031342]
		[batch 20/20] avg loss: 0.048939034965014776		[learning rate: 0.00031285]
	Learning Rate: 0.000312854
	LOSS [training: 0.04531894666261127 | validation: 0.035483030783227835]
	TIME [epoch: 8.26 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03612720141682946		[learning rate: 0.00031229]
		[batch 20/20] avg loss: 0.03409740451465583		[learning rate: 0.00031172]
	Learning Rate: 0.000311719
	LOSS [training: 0.035112302965742644 | validation: 0.02826674860655156]
	TIME [epoch: 8.26 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05412927513617475		[learning rate: 0.00031115]
		[batch 20/20] avg loss: 0.032309890863210074		[learning rate: 0.00031059]
	Learning Rate: 0.000310588
	LOSS [training: 0.04321958299969241 | validation: 0.02769300838082369]
	TIME [epoch: 8.25 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04969589390411634		[learning rate: 0.00031002]
		[batch 20/20] avg loss: 0.04795195777145258		[learning rate: 0.00030946]
	Learning Rate: 0.000309461
	LOSS [training: 0.04882392583778446 | validation: 0.016231935921852774]
	TIME [epoch: 8.25 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03255867573674652		[learning rate: 0.0003089]
		[batch 20/20] avg loss: 0.048928586012963855		[learning rate: 0.00030834]
	Learning Rate: 0.000308338
	LOSS [training: 0.040743630874855184 | validation: 0.0581886226470667]
	TIME [epoch: 8.25 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04969622992823633		[learning rate: 0.00030778]
		[batch 20/20] avg loss: 0.04986154023674305		[learning rate: 0.00030722]
	Learning Rate: 0.000307219
	LOSS [training: 0.04977888508248969 | validation: 0.015032895613991855]
	TIME [epoch: 8.28 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0411877653640192		[learning rate: 0.00030666]
		[batch 20/20] avg loss: 0.04807509549116447		[learning rate: 0.0003061]
	Learning Rate: 0.000306104
	LOSS [training: 0.04463143042759184 | validation: 0.03880249528397033]
	TIME [epoch: 8.25 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05554985317328422		[learning rate: 0.00030555]
		[batch 20/20] avg loss: 0.05435565830622373		[learning rate: 0.00030499]
	Learning Rate: 0.000304993
	LOSS [training: 0.05495275573975398 | validation: 0.04974894440538077]
	TIME [epoch: 8.25 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05693411820889506		[learning rate: 0.00030444]
		[batch 20/20] avg loss: 0.052987111492113825		[learning rate: 0.00030389]
	Learning Rate: 0.000303886
	LOSS [training: 0.05496061485050444 | validation: 0.02367674958605767]
	TIME [epoch: 8.24 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05113505941913358		[learning rate: 0.00030333]
		[batch 20/20] avg loss: 0.08672421994809446		[learning rate: 0.00030278]
	Learning Rate: 0.000302783
	LOSS [training: 0.06892963968361403 | validation: 0.03836106573368752]
	TIME [epoch: 8.28 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04287861456806731		[learning rate: 0.00030223]
		[batch 20/20] avg loss: 0.07199294313950784		[learning rate: 0.00030168]
	Learning Rate: 0.000301684
	LOSS [training: 0.057435778853787575 | validation: 0.035486866555392585]
	TIME [epoch: 8.26 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03401834444149654		[learning rate: 0.00030114]
		[batch 20/20] avg loss: 0.040447121397553326		[learning rate: 0.00030059]
	Learning Rate: 0.000300589
	LOSS [training: 0.037232732919524926 | validation: 0.03592008305656256]
	TIME [epoch: 8.26 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037945381328906846		[learning rate: 0.00030004]
		[batch 20/20] avg loss: 0.049950931674958544		[learning rate: 0.0002995]
	Learning Rate: 0.000299499
	LOSS [training: 0.043948156501932685 | validation: 0.05282720787492436]
	TIME [epoch: 8.25 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04822063018355429		[learning rate: 0.00029895]
		[batch 20/20] avg loss: 0.03776684182175424		[learning rate: 0.00029841]
	Learning Rate: 0.000298412
	LOSS [training: 0.04299373600265427 | validation: 0.028309633039175983]
	TIME [epoch: 8.27 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04219387560951586		[learning rate: 0.00029787]
		[batch 20/20] avg loss: 0.03653904984583829		[learning rate: 0.00029733]
	Learning Rate: 0.000297329
	LOSS [training: 0.03936646272767706 | validation: 0.021922394227327946]
	TIME [epoch: 8.26 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05322161533118001		[learning rate: 0.00029679]
		[batch 20/20] avg loss: 0.030896181401847377		[learning rate: 0.00029625]
	Learning Rate: 0.00029625
	LOSS [training: 0.0420588983665137 | validation: 0.07172860319501796]
	TIME [epoch: 8.26 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05420903070519588		[learning rate: 0.00029571]
		[batch 20/20] avg loss: 0.04467683804742892		[learning rate: 0.00029517]
	Learning Rate: 0.000295175
	LOSS [training: 0.04944293437631239 | validation: 0.10754694756975736]
	TIME [epoch: 8.25 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07297433300716916		[learning rate: 0.00029464]
		[batch 20/20] avg loss: 0.027313340824621918		[learning rate: 0.0002941]
	Learning Rate: 0.000294103
	LOSS [training: 0.05014383691589554 | validation: 0.02196701032870116]
	TIME [epoch: 8.25 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03620806833834363		[learning rate: 0.00029357]
		[batch 20/20] avg loss: 0.03346823707694117		[learning rate: 0.00029304]
	Learning Rate: 0.000293036
	LOSS [training: 0.034838152707642386 | validation: 0.02803492057949673]
	TIME [epoch: 8.28 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040305574433664254		[learning rate: 0.0002925]
		[batch 20/20] avg loss: 0.05325785677197971		[learning rate: 0.00029197]
	Learning Rate: 0.000291973
	LOSS [training: 0.04678171560282198 | validation: 0.03736978552043108]
	TIME [epoch: 8.25 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.063252740324547		[learning rate: 0.00029144]
		[batch 20/20] avg loss: 0.045668929441445454		[learning rate: 0.00029091]
	Learning Rate: 0.000290913
	LOSS [training: 0.054460834882996224 | validation: 0.06753318628576332]
	TIME [epoch: 8.26 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03706804134285863		[learning rate: 0.00029038]
		[batch 20/20] avg loss: 0.027974164676573137		[learning rate: 0.00028986]
	Learning Rate: 0.000289857
	LOSS [training: 0.03252110300971589 | validation: 0.04421057728846749]
	TIME [epoch: 8.25 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04373750269410548		[learning rate: 0.00028933]
		[batch 20/20] avg loss: 0.032641543443170454		[learning rate: 0.00028881]
	Learning Rate: 0.000288805
	LOSS [training: 0.03818952306863797 | validation: 0.05466759002123519]
	TIME [epoch: 8.28 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048460165642870076		[learning rate: 0.00028828]
		[batch 20/20] avg loss: 0.055448114104384126		[learning rate: 0.00028776]
	Learning Rate: 0.000287757
	LOSS [training: 0.051954139873627105 | validation: 0.08213419433997035]
	TIME [epoch: 8.25 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05334283613883697		[learning rate: 0.00028723]
		[batch 20/20] avg loss: 0.04843006318274365		[learning rate: 0.00028671]
	Learning Rate: 0.000286713
	LOSS [training: 0.0508864496607903 | validation: 0.031143296168277986]
	TIME [epoch: 8.26 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04072188917595773		[learning rate: 0.00028619]
		[batch 20/20] avg loss: 0.035697172009707526		[learning rate: 0.00028567]
	Learning Rate: 0.000285672
	LOSS [training: 0.03820953059283262 | validation: 0.040040838108725565]
	TIME [epoch: 8.25 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05092470146062509		[learning rate: 0.00028515]
		[batch 20/20] avg loss: 0.03685932595879432		[learning rate: 0.00028464]
	Learning Rate: 0.000284636
	LOSS [training: 0.04389201370970971 | validation: 0.033122577167289766]
	TIME [epoch: 8.26 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05649168213560278		[learning rate: 0.00028412]
		[batch 20/20] avg loss: 0.04333020893925397		[learning rate: 0.0002836]
	Learning Rate: 0.000283603
	LOSS [training: 0.04991094553742838 | validation: 0.04695425838987437]
	TIME [epoch: 8.28 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045251037314019346		[learning rate: 0.00028309]
		[batch 20/20] avg loss: 0.0603449839599457		[learning rate: 0.00028257]
	Learning Rate: 0.000282574
	LOSS [training: 0.05279801063698253 | validation: 0.026004697954234626]
	TIME [epoch: 8.26 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061740594156807035		[learning rate: 0.00028206]
		[batch 20/20] avg loss: 0.04558208925210563		[learning rate: 0.00028155]
	Learning Rate: 0.000281548
	LOSS [training: 0.05366134170445632 | validation: 0.07477900678318387]
	TIME [epoch: 8.25 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05177043465458094		[learning rate: 0.00028104]
		[batch 20/20] avg loss: 0.04147823662945281		[learning rate: 0.00028053]
	Learning Rate: 0.000280526
	LOSS [training: 0.04662433564201688 | validation: 0.04106692619315645]
	TIME [epoch: 8.25 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04350567279988014		[learning rate: 0.00028002]
		[batch 20/20] avg loss: 0.038462607090314314		[learning rate: 0.00027951]
	Learning Rate: 0.000279508
	LOSS [training: 0.04098413994509723 | validation: 0.04660204524528583]
	TIME [epoch: 8.28 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042429004248392035		[learning rate: 0.000279]
		[batch 20/20] avg loss: 0.07095361170951094		[learning rate: 0.00027849]
	Learning Rate: 0.000278494
	LOSS [training: 0.056691307978951465 | validation: 0.039364429079409]
	TIME [epoch: 8.26 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04889956006854483		[learning rate: 0.00027799]
		[batch 20/20] avg loss: 0.040032185971721514		[learning rate: 0.00027748]
	Learning Rate: 0.000277483
	LOSS [training: 0.044465873020133176 | validation: 0.08227038625335273]
	TIME [epoch: 8.25 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045070283243937956		[learning rate: 0.00027698]
		[batch 20/20] avg loss: 0.04683260250871065		[learning rate: 0.00027648]
	Learning Rate: 0.000276476
	LOSS [training: 0.0459514428763243 | validation: 0.026368805623083212]
	TIME [epoch: 8.25 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03203824480166936		[learning rate: 0.00027597]
		[batch 20/20] avg loss: 0.03417964019276271		[learning rate: 0.00027547]
	Learning Rate: 0.000275473
	LOSS [training: 0.033108942497216035 | validation: 0.025408007997750514]
	TIME [epoch: 8.27 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04908116721107266		[learning rate: 0.00027497]
		[batch 20/20] avg loss: 0.061876082003002854		[learning rate: 0.00027447]
	Learning Rate: 0.000274473
	LOSS [training: 0.05547862460703777 | validation: 0.025920514213161796]
	TIME [epoch: 8.26 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04418388694806476		[learning rate: 0.00027397]
		[batch 20/20] avg loss: 0.05784929279537528		[learning rate: 0.00027348]
	Learning Rate: 0.000273477
	LOSS [training: 0.05101658987172002 | validation: 0.021189442666969614]
	TIME [epoch: 8.25 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0326254645068542		[learning rate: 0.00027298]
		[batch 20/20] avg loss: 0.04160561583106422		[learning rate: 0.00027248]
	Learning Rate: 0.000272485
	LOSS [training: 0.03711554016895922 | validation: 0.019655009151890446]
	TIME [epoch: 8.25 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047580239934009526		[learning rate: 0.00027199]
		[batch 20/20] avg loss: 0.03703557538868713		[learning rate: 0.0002715]
	Learning Rate: 0.000271496
	LOSS [training: 0.042307907661348315 | validation: 0.033214863981055796]
	TIME [epoch: 8.25 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04217992956780584		[learning rate: 0.000271]
		[batch 20/20] avg loss: 0.04086848410843999		[learning rate: 0.00027051]
	Learning Rate: 0.000270511
	LOSS [training: 0.041524206838122905 | validation: 0.030692163368231648]
	TIME [epoch: 8.28 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05484154901516776		[learning rate: 0.00027002]
		[batch 20/20] avg loss: 0.041074067268991286		[learning rate: 0.00026953]
	Learning Rate: 0.000269529
	LOSS [training: 0.04795780814207952 | validation: 0.021486057342525008]
	TIME [epoch: 8.25 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09503529938337105		[learning rate: 0.00026904]
		[batch 20/20] avg loss: 0.0349146558820145		[learning rate: 0.00026855]
	Learning Rate: 0.000268551
	LOSS [training: 0.06497497763269278 | validation: 0.030451446473300708]
	TIME [epoch: 8.26 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03137745309948127		[learning rate: 0.00026806]
		[batch 20/20] avg loss: 0.03842886745569639		[learning rate: 0.00026758]
	Learning Rate: 0.000267576
	LOSS [training: 0.034903160277588836 | validation: 0.08313855375668373]
	TIME [epoch: 8.25 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040113436950761264		[learning rate: 0.00026709]
		[batch 20/20] avg loss: 0.04092414358014281		[learning rate: 0.00026661]
	Learning Rate: 0.000266605
	LOSS [training: 0.04051879026545204 | validation: 0.03774564968914812]
	TIME [epoch: 8.27 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039044159790692964		[learning rate: 0.00026612]
		[batch 20/20] avg loss: 0.020011742652590803		[learning rate: 0.00026564]
	Learning Rate: 0.000265638
	LOSS [training: 0.02952795122164188 | validation: 0.016724849913976936]
	TIME [epoch: 8.25 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04332962178978535		[learning rate: 0.00026516]
		[batch 20/20] avg loss: 0.04405264927064691		[learning rate: 0.00026467]
	Learning Rate: 0.000264674
	LOSS [training: 0.04369113553021613 | validation: 0.023149165441388606]
	TIME [epoch: 8.25 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04220611626201619		[learning rate: 0.00026419]
		[batch 20/20] avg loss: 0.038716808032768606		[learning rate: 0.00026371]
	Learning Rate: 0.000263713
	LOSS [training: 0.0404614621473924 | validation: 0.022079028607223595]
	TIME [epoch: 8.25 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03132025665043286		[learning rate: 0.00026323]
		[batch 20/20] avg loss: 0.03711490623630283		[learning rate: 0.00026276]
	Learning Rate: 0.000262756
	LOSS [training: 0.03421758144336784 | validation: 0.01608386334097335]
	TIME [epoch: 8.27 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028745904456356237		[learning rate: 0.00026228]
		[batch 20/20] avg loss: 0.045075297323360006		[learning rate: 0.0002618]
	Learning Rate: 0.000261802
	LOSS [training: 0.03691060088985812 | validation: 0.027402734046830978]
	TIME [epoch: 8.26 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03172208059925196		[learning rate: 0.00026133]
		[batch 20/20] avg loss: 0.035390195613432394		[learning rate: 0.00026085]
	Learning Rate: 0.000260852
	LOSS [training: 0.03355613810634218 | validation: 0.0140579154779379]
	TIME [epoch: 8.25 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03645359768951862		[learning rate: 0.00026038]
		[batch 20/20] avg loss: 0.04252355987598744		[learning rate: 0.00025991]
	Learning Rate: 0.000259906
	LOSS [training: 0.039488578782753025 | validation: 0.0646853406087846]
	TIME [epoch: 8.25 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05749983168377332		[learning rate: 0.00025943]
		[batch 20/20] avg loss: 0.04054999848360339		[learning rate: 0.00025896]
	Learning Rate: 0.000258962
	LOSS [training: 0.04902491508368835 | validation: 0.026283585061057397]
	TIME [epoch: 8.25 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029592861942064824		[learning rate: 0.00025849]
		[batch 20/20] avg loss: 0.057878086395907316		[learning rate: 0.00025802]
	Learning Rate: 0.000258023
	LOSS [training: 0.043735474168986065 | validation: 0.04613934982608615]
	TIME [epoch: 8.28 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04110356424435915		[learning rate: 0.00025755]
		[batch 20/20] avg loss: 0.03860767601991759		[learning rate: 0.00025709]
	Learning Rate: 0.000257086
	LOSS [training: 0.03985562013213837 | validation: 0.027839502434247508]
	TIME [epoch: 8.25 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04108777668541865		[learning rate: 0.00025662]
		[batch 20/20] avg loss: 0.05205642827408339		[learning rate: 0.00025615]
	Learning Rate: 0.000256153
	LOSS [training: 0.04657210247975102 | validation: 0.017361353895139958]
	TIME [epoch: 8.25 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05008790418053405		[learning rate: 0.00025569]
		[batch 20/20] avg loss: 0.040151738123458285		[learning rate: 0.00025522]
	Learning Rate: 0.000255224
	LOSS [training: 0.04511982115199616 | validation: 0.03405088984145242]
	TIME [epoch: 8.25 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03665411293012171		[learning rate: 0.00025476]
		[batch 20/20] avg loss: 0.030634989670347917		[learning rate: 0.0002543]
	Learning Rate: 0.000254298
	LOSS [training: 0.03364455130023481 | validation: 0.014871760522960312]
	TIME [epoch: 8.26 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03154460805757117		[learning rate: 0.00025384]
		[batch 20/20] avg loss: 0.03694298419191459		[learning rate: 0.00025337]
	Learning Rate: 0.000253375
	LOSS [training: 0.034243796124742876 | validation: 0.02793588567505718]
	TIME [epoch: 8.26 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03846553157881162		[learning rate: 0.00025291]
		[batch 20/20] avg loss: 0.03384756595920076		[learning rate: 0.00025246]
	Learning Rate: 0.000252455
	LOSS [training: 0.036156548769006196 | validation: 0.01945181692945663]
	TIME [epoch: 8.25 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03977376282690221		[learning rate: 0.000252]
		[batch 20/20] avg loss: 0.03232033640426789		[learning rate: 0.00025154]
	Learning Rate: 0.000251539
	LOSS [training: 0.03604704961558505 | validation: 0.04965100402752018]
	TIME [epoch: 8.25 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04859523623075942		[learning rate: 0.00025108]
		[batch 20/20] avg loss: 0.03747514964755676		[learning rate: 0.00025063]
	Learning Rate: 0.000250626
	LOSS [training: 0.04303519293915809 | validation: 0.03464823874036141]
	TIME [epoch: 8.25 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03224893349018581		[learning rate: 0.00025017]
		[batch 20/20] avg loss: 0.07644964758070467		[learning rate: 0.00024972]
	Learning Rate: 0.000249717
	LOSS [training: 0.05434929053544524 | validation: 0.010571504758767713]
	TIME [epoch: 8.27 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03289744581485025		[learning rate: 0.00024926]
		[batch 20/20] avg loss: 0.03262831457304377		[learning rate: 0.00024881]
	Learning Rate: 0.00024881
	LOSS [training: 0.032762880193947005 | validation: 0.036462985589206034]
	TIME [epoch: 8.24 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04323246059295394		[learning rate: 0.00024836]
		[batch 20/20] avg loss: 0.034024334850343764		[learning rate: 0.00024791]
	Learning Rate: 0.000247907
	LOSS [training: 0.03862839772164885 | validation: 0.021768687449872152]
	TIME [epoch: 8.25 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05038149381793148		[learning rate: 0.00024746]
		[batch 20/20] avg loss: 0.043137807182247856		[learning rate: 0.00024701]
	Learning Rate: 0.000247008
	LOSS [training: 0.046759650500089664 | validation: 0.06374051759033866]
	TIME [epoch: 8.25 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06680605526342018		[learning rate: 0.00024656]
		[batch 20/20] avg loss: 0.03315232821133762		[learning rate: 0.00024611]
	Learning Rate: 0.000246111
	LOSS [training: 0.0499791917373789 | validation: 0.03838386929244448]
	TIME [epoch: 8.27 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05075785502814443		[learning rate: 0.00024566]
		[batch 20/20] avg loss: 0.056765313135236396		[learning rate: 0.00024522]
	Learning Rate: 0.000245218
	LOSS [training: 0.053761584081690426 | validation: 0.02584082665001597]
	TIME [epoch: 8.25 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03686190212906695		[learning rate: 0.00024477]
		[batch 20/20] avg loss: 0.0399384404530119		[learning rate: 0.00024433]
	Learning Rate: 0.000244328
	LOSS [training: 0.03840017129103942 | validation: 0.022955529930395516]
	TIME [epoch: 8.25 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03860332261443729		[learning rate: 0.00024388]
		[batch 20/20] avg loss: 0.05336249701526705		[learning rate: 0.00024344]
	Learning Rate: 0.000243442
	LOSS [training: 0.04598290981485218 | validation: 0.03307912018632759]
	TIME [epoch: 8.25 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04781213920956626		[learning rate: 0.000243]
		[batch 20/20] avg loss: 0.02898560816019931		[learning rate: 0.00024256]
	Learning Rate: 0.000242558
	LOSS [training: 0.03839887368488278 | validation: 0.05486023325602385]
	TIME [epoch: 8.26 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059089035509369114		[learning rate: 0.00024212]
		[batch 20/20] avg loss: 0.03328280878169254		[learning rate: 0.00024168]
	Learning Rate: 0.000241678
	LOSS [training: 0.04618592214553084 | validation: 0.020248219878204323]
	TIME [epoch: 8.26 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04112127490635279		[learning rate: 0.00024124]
		[batch 20/20] avg loss: 0.03053889153189455		[learning rate: 0.0002408]
	Learning Rate: 0.000240801
	LOSS [training: 0.035830083219123676 | validation: 0.044798238580432304]
	TIME [epoch: 8.25 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042835272164618896		[learning rate: 0.00024036]
		[batch 20/20] avg loss: 0.031744314343945285		[learning rate: 0.00023993]
	Learning Rate: 0.000239927
	LOSS [training: 0.0372897932542821 | validation: 0.020633586321357155]
	TIME [epoch: 8.25 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04139360681577347		[learning rate: 0.00023949]
		[batch 20/20] avg loss: 0.0424227999954014		[learning rate: 0.00023906]
	Learning Rate: 0.000239056
	LOSS [training: 0.04190820340558743 | validation: 0.027111205891672666]
	TIME [epoch: 8.25 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049872053516099406		[learning rate: 0.00023862]
		[batch 20/20] avg loss: 0.040979485886234325		[learning rate: 0.00023819]
	Learning Rate: 0.000238189
	LOSS [training: 0.04542576970116688 | validation: 0.04654322459847677]
	TIME [epoch: 8.27 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045273452542977204		[learning rate: 0.00023776]
		[batch 20/20] avg loss: 0.06395466435883561		[learning rate: 0.00023732]
	Learning Rate: 0.000237324
	LOSS [training: 0.05461405845090642 | validation: 0.04693311430775038]
	TIME [epoch: 8.24 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04353087224488893		[learning rate: 0.00023689]
		[batch 20/20] avg loss: 0.0366760710593134		[learning rate: 0.00023646]
	Learning Rate: 0.000236463
	LOSS [training: 0.04010347165210117 | validation: 0.016384748660253443]
	TIME [epoch: 8.24 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028912548633111664		[learning rate: 0.00023603]
		[batch 20/20] avg loss: 0.06195663255734194		[learning rate: 0.0002356]
	Learning Rate: 0.000235605
	LOSS [training: 0.0454345905952268 | validation: 0.028195284080280155]
	TIME [epoch: 8.24 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05474249682082877		[learning rate: 0.00023518]
		[batch 20/20] avg loss: 0.040132525243074244		[learning rate: 0.00023475]
	Learning Rate: 0.00023475
	LOSS [training: 0.04743751103195151 | validation: 0.03337299754486305]
	TIME [epoch: 8.26 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02856013068344595		[learning rate: 0.00023432]
		[batch 20/20] avg loss: 0.04253462327983458		[learning rate: 0.0002339]
	Learning Rate: 0.000233898
	LOSS [training: 0.035547376981640275 | validation: 0.02169960574474886]
	TIME [epoch: 8.25 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025801718133534112		[learning rate: 0.00023347]
		[batch 20/20] avg loss: 0.04289742065574857		[learning rate: 0.00023305]
	Learning Rate: 0.000233049
	LOSS [training: 0.034349569394641344 | validation: 0.037911294264153884]
	TIME [epoch: 8.25 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031981119639097604		[learning rate: 0.00023263]
		[batch 20/20] avg loss: 0.041560075006518093		[learning rate: 0.0002322]
	Learning Rate: 0.000232203
	LOSS [training: 0.036770597322807845 | validation: 0.03572926009003197]
	TIME [epoch: 8.24 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0404692493299225		[learning rate: 0.00023178]
		[batch 20/20] avg loss: 0.05056741690747135		[learning rate: 0.00023136]
	Learning Rate: 0.000231361
	LOSS [training: 0.04551833311869693 | validation: 0.022371847042617972]
	TIME [epoch: 8.25 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028244302472807552		[learning rate: 0.00023094]
		[batch 20/20] avg loss: 0.06622604422957945		[learning rate: 0.00023052]
	Learning Rate: 0.000230521
	LOSS [training: 0.0472351733511935 | validation: 0.09469591477943559]
	TIME [epoch: 8.26 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06264964216194532		[learning rate: 0.0002301]
		[batch 20/20] avg loss: 0.045883655049978676		[learning rate: 0.00022968]
	Learning Rate: 0.000229685
	LOSS [training: 0.05426664860596201 | validation: 0.05989158346452732]
	TIME [epoch: 8.24 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04419584211490067		[learning rate: 0.00022927]
		[batch 20/20] avg loss: 0.05824272270573767		[learning rate: 0.00022885]
	Learning Rate: 0.000228851
	LOSS [training: 0.051219282410319165 | validation: 0.027677772014913404]
	TIME [epoch: 8.24 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04502043745243836		[learning rate: 0.00022844]
		[batch 20/20] avg loss: 0.025504324375392178		[learning rate: 0.00022802]
	Learning Rate: 0.00022802
	LOSS [training: 0.035262380913915266 | validation: 0.014840165172266087]
	TIME [epoch: 8.24 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02827157914491074		[learning rate: 0.00022761]
		[batch 20/20] avg loss: 0.04259430447924313		[learning rate: 0.00022719]
	Learning Rate: 0.000227193
	LOSS [training: 0.03543294181207693 | validation: 0.02830801454501964]
	TIME [epoch: 8.27 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039903213417859726		[learning rate: 0.00022678]
		[batch 20/20] avg loss: 0.030292545405649886		[learning rate: 0.00022637]
	Learning Rate: 0.000226368
	LOSS [training: 0.035097879411754795 | validation: 0.023645133598847273]
	TIME [epoch: 8.25 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03610102429054918		[learning rate: 0.00022596]
		[batch 20/20] avg loss: 0.022574958791162315		[learning rate: 0.00022555]
	Learning Rate: 0.000225547
	LOSS [training: 0.029337991540855746 | validation: 0.02222628370425254]
	TIME [epoch: 8.24 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029534407185820268		[learning rate: 0.00022514]
		[batch 20/20] avg loss: 0.03585108482731882		[learning rate: 0.00022473]
	Learning Rate: 0.000224728
	LOSS [training: 0.032692746006569545 | validation: 0.035574745881822134]
	TIME [epoch: 8.25 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030242970535747333		[learning rate: 0.00022432]
		[batch 20/20] avg loss: 0.03855566831959635		[learning rate: 0.00022391]
	Learning Rate: 0.000223913
	LOSS [training: 0.03439931942767183 | validation: 0.036122275099306905]
	TIME [epoch: 8.26 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04513519711481114		[learning rate: 0.00022351]
		[batch 20/20] avg loss: 0.03908638720794204		[learning rate: 0.0002231]
	Learning Rate: 0.0002231
	LOSS [training: 0.042110792161376594 | validation: 0.028611786559228726]
	TIME [epoch: 8.26 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033966969505112854		[learning rate: 0.0002227]
		[batch 20/20] avg loss: 0.025876047338717078		[learning rate: 0.00022229]
	Learning Rate: 0.000222291
	LOSS [training: 0.02992150842191497 | validation: 0.01785495486349689]
	TIME [epoch: 8.25 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02901150892300728		[learning rate: 0.00022189]
		[batch 20/20] avg loss: 0.05698301027979349		[learning rate: 0.00022148]
	Learning Rate: 0.000221484
	LOSS [training: 0.042997259601400384 | validation: 0.03699497749333892]
	TIME [epoch: 8.24 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0276973175789271		[learning rate: 0.00022108]
		[batch 20/20] avg loss: 0.03330308326895835		[learning rate: 0.00022068]
	Learning Rate: 0.00022068
	LOSS [training: 0.03050020042394273 | validation: 0.01889167281976471]
	TIME [epoch: 8.25 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030232020359826188		[learning rate: 0.00022028]
		[batch 20/20] avg loss: 0.027326605947907333		[learning rate: 0.00021988]
	Learning Rate: 0.000219879
	LOSS [training: 0.02877931315386676 | validation: 0.030246930816914645]
	TIME [epoch: 8.26 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03639667931244552		[learning rate: 0.00021948]
		[batch 20/20] avg loss: 0.03665356231099441		[learning rate: 0.00021908]
	Learning Rate: 0.000219081
	LOSS [training: 0.03652512081171996 | validation: 0.013740193969043616]
	TIME [epoch: 8.24 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03660664851435056		[learning rate: 0.00021868]
		[batch 20/20] avg loss: 0.03111397681308281		[learning rate: 0.00021829]
	Learning Rate: 0.000218286
	LOSS [training: 0.03386031266371669 | validation: 0.018737070621884906]
	TIME [epoch: 8.24 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04508325423156525		[learning rate: 0.00021789]
		[batch 20/20] avg loss: 0.03853806057265498		[learning rate: 0.00021749]
	Learning Rate: 0.000217494
	LOSS [training: 0.0418106574021101 | validation: 0.018737368429133913]
	TIME [epoch: 8.24 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03178107297947538		[learning rate: 0.0002171]
		[batch 20/20] avg loss: 0.059642157631199996		[learning rate: 0.0002167]
	Learning Rate: 0.000216705
	LOSS [training: 0.04571161530533769 | validation: 0.013828040709835404]
	TIME [epoch: 8.26 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04572628196209892		[learning rate: 0.00021631]
		[batch 20/20] avg loss: 0.0369226135585717		[learning rate: 0.00021592]
	Learning Rate: 0.000215918
	LOSS [training: 0.0413244477603353 | validation: 0.02176881767759631]
	TIME [epoch: 8.24 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03516891106657154		[learning rate: 0.00021553]
		[batch 20/20] avg loss: 0.05687522886472788		[learning rate: 0.00021513]
	Learning Rate: 0.000215135
	LOSS [training: 0.04602206996564971 | validation: 0.022881786669799925]
	TIME [epoch: 8.24 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035617810289267235		[learning rate: 0.00021474]
		[batch 20/20] avg loss: 0.026994295173932998		[learning rate: 0.00021435]
	Learning Rate: 0.000214354
	LOSS [training: 0.031306052731600115 | validation: 0.03299841598114767]
	TIME [epoch: 8.24 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034076840768352115		[learning rate: 0.00021396]
		[batch 20/20] avg loss: 0.04030399644396079		[learning rate: 0.00021358]
	Learning Rate: 0.000213576
	LOSS [training: 0.037190418606156454 | validation: 0.0311080508171317]
	TIME [epoch: 8.26 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035250063144831076		[learning rate: 0.00021319]
		[batch 20/20] avg loss: 0.025057185346426335		[learning rate: 0.0002128]
	Learning Rate: 0.000212801
	LOSS [training: 0.03015362424562871 | validation: 0.019579331044994458]
	TIME [epoch: 8.25 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03083572464699629		[learning rate: 0.00021241]
		[batch 20/20] avg loss: 0.039985407571635524		[learning rate: 0.00021203]
	Learning Rate: 0.000212029
	LOSS [training: 0.03541056610931591 | validation: 0.03431537231539389]
	TIME [epoch: 8.24 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03989308423432692		[learning rate: 0.00021164]
		[batch 20/20] avg loss: 0.03266471066408082		[learning rate: 0.00021126]
	Learning Rate: 0.000211259
	LOSS [training: 0.03627889744920387 | validation: 0.034627390179078205]
	TIME [epoch: 8.24 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03873353882260444		[learning rate: 0.00021088]
		[batch 20/20] avg loss: 0.031308689983117935		[learning rate: 0.00021049]
	Learning Rate: 0.000210493
	LOSS [training: 0.035021114402861195 | validation: 0.020856328668876736]
	TIME [epoch: 8.25 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043538341591253596		[learning rate: 0.00021011]
		[batch 20/20] avg loss: 0.030731612324282086		[learning rate: 0.00020973]
	Learning Rate: 0.000209729
	LOSS [training: 0.03713497695776784 | validation: 0.021412859685751892]
	TIME [epoch: 8.26 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05624092056310226		[learning rate: 0.00020935]
		[batch 20/20] avg loss: 0.034763747046632175		[learning rate: 0.00020897]
	Learning Rate: 0.000208968
	LOSS [training: 0.045502333804867214 | validation: 0.019564080907802]
	TIME [epoch: 8.25 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025988997293604786		[learning rate: 0.00020859]
		[batch 20/20] avg loss: 0.04856476038610993		[learning rate: 0.00020821]
	Learning Rate: 0.000208209
	LOSS [training: 0.03727687883985736 | validation: 0.02482618852795146]
	TIME [epoch: 8.24 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04370562619030614		[learning rate: 0.00020783]
		[batch 20/20] avg loss: 0.026043223363326255		[learning rate: 0.00020745]
	Learning Rate: 0.000207454
	LOSS [training: 0.034874424776816196 | validation: 0.017103406633319555]
	TIME [epoch: 8.24 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044323029255808846		[learning rate: 0.00020708]
		[batch 20/20] avg loss: 0.03139644049738173		[learning rate: 0.0002067]
	Learning Rate: 0.000206701
	LOSS [training: 0.03785973487659529 | validation: 0.0259421272976864]
	TIME [epoch: 8.26 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03883136707890534		[learning rate: 0.00020633]
		[batch 20/20] avg loss: 0.04769236245548356		[learning rate: 0.00020595]
	Learning Rate: 0.000205951
	LOSS [training: 0.043261864767194444 | validation: 0.03871565359614008]
	TIME [epoch: 8.25 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036902413523362966		[learning rate: 0.00020558]
		[batch 20/20] avg loss: 0.028825848752234446		[learning rate: 0.0002052]
	Learning Rate: 0.000205203
	LOSS [training: 0.0328641311377987 | validation: 0.02192861114759154]
	TIME [epoch: 8.24 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03524178231522259		[learning rate: 0.00020483]
		[batch 20/20] avg loss: 0.03668252698845947		[learning rate: 0.00020446]
	Learning Rate: 0.000204459
	LOSS [training: 0.03596215465184103 | validation: 0.013871706720890624]
	TIME [epoch: 8.24 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03696108971757169		[learning rate: 0.00020409]
		[batch 20/20] avg loss: 0.036079673972448156		[learning rate: 0.00020372]
	Learning Rate: 0.000203717
	LOSS [training: 0.036520381845009914 | validation: 0.03192053438010321]
	TIME [epoch: 8.25 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030860633331264424		[learning rate: 0.00020335]
		[batch 20/20] avg loss: 0.03657789594273428		[learning rate: 0.00020298]
	Learning Rate: 0.000202977
	LOSS [training: 0.033719264636999355 | validation: 0.015248553898498288]
	TIME [epoch: 8.27 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03908855236145685		[learning rate: 0.00020261]
		[batch 20/20] avg loss: 0.0314734525870563		[learning rate: 0.00020224]
	Learning Rate: 0.000202241
	LOSS [training: 0.03528100247425657 | validation: 0.012364058052054677]
	TIME [epoch: 8.24 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03819240764892703		[learning rate: 0.00020187]
		[batch 20/20] avg loss: 0.03469434242288795		[learning rate: 0.00020151]
	Learning Rate: 0.000201507
	LOSS [training: 0.036443375035907485 | validation: 0.019217352910304464]
	TIME [epoch: 8.24 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043350377137027885		[learning rate: 0.00020114]
		[batch 20/20] avg loss: 0.03396748729161529		[learning rate: 0.00020078]
	Learning Rate: 0.000200775
	LOSS [training: 0.03865893221432159 | validation: 0.035518203319787396]
	TIME [epoch: 8.24 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04277859040870158		[learning rate: 0.00020041]
		[batch 20/20] avg loss: 0.036699367807598596		[learning rate: 0.00020005]
	Learning Rate: 0.000200047
	LOSS [training: 0.03973897910815009 | validation: 0.025970750790388016]
	TIME [epoch: 8.27 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03148233519279474		[learning rate: 0.00019968]
		[batch 20/20] avg loss: 0.023253671448464516		[learning rate: 0.00019932]
	Learning Rate: 0.000199321
	LOSS [training: 0.02736800332062963 | validation: 0.03258140167462999]
	TIME [epoch: 8.25 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036802047487949664		[learning rate: 0.00019896]
		[batch 20/20] avg loss: 0.017853725065421558		[learning rate: 0.0001986]
	Learning Rate: 0.000198597
	LOSS [training: 0.027327886276685615 | validation: 0.012690200276212286]
	TIME [epoch: 8.25 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03661614779391869		[learning rate: 0.00019824]
		[batch 20/20] avg loss: 0.040188283716740235		[learning rate: 0.00019788]
	Learning Rate: 0.000197877
	LOSS [training: 0.03840221575532947 | validation: 0.01592620721522321]
	TIME [epoch: 8.24 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031601453453770564		[learning rate: 0.00019752]
		[batch 20/20] avg loss: 0.03416503149549175		[learning rate: 0.00019716]
	Learning Rate: 0.000197159
	LOSS [training: 0.032883242474631154 | validation: 0.024763313949237767]
	TIME [epoch: 8.26 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03022937310962544		[learning rate: 0.0001968]
		[batch 20/20] avg loss: 0.051526800658883834		[learning rate: 0.00019644]
	Learning Rate: 0.000196443
	LOSS [training: 0.040878086884254636 | validation: 0.041498812083661574]
	TIME [epoch: 8.25 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030231380335067965		[learning rate: 0.00019609]
		[batch 20/20] avg loss: 0.030320527446569916		[learning rate: 0.00019573]
	Learning Rate: 0.00019573
	LOSS [training: 0.03027595389081894 | validation: 0.04348275940090728]
	TIME [epoch: 8.25 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04458569465318041		[learning rate: 0.00019537]
		[batch 20/20] avg loss: 0.020901461500956193		[learning rate: 0.00019502]
	Learning Rate: 0.00019502
	LOSS [training: 0.0327435780770683 | validation: 0.020232702771189628]
	TIME [epoch: 8.24 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03642947120146779		[learning rate: 0.00019467]
		[batch 20/20] avg loss: 0.05547227230851105		[learning rate: 0.00019431]
	Learning Rate: 0.000194312
	LOSS [training: 0.04595087175498942 | validation: 0.040577800747572584]
	TIME [epoch: 8.25 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04921887682156135		[learning rate: 0.00019396]
		[batch 20/20] avg loss: 0.0258761725038531		[learning rate: 0.00019361]
	Learning Rate: 0.000193607
	LOSS [training: 0.03754752466270723 | validation: 0.022692694734057977]
	TIME [epoch: 8.27 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02478759034372322		[learning rate: 0.00019326]
		[batch 20/20] avg loss: 0.028094562709716768		[learning rate: 0.0001929]
	Learning Rate: 0.000192904
	LOSS [training: 0.02644107652671999 | validation: 0.02159169427127987]
	TIME [epoch: 8.24 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03343413063220169		[learning rate: 0.00019255]
		[batch 20/20] avg loss: 0.030536322013632183		[learning rate: 0.0001922]
	Learning Rate: 0.000192204
	LOSS [training: 0.031985226322916935 | validation: 0.04063536726963905]
	TIME [epoch: 8.25 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0401461166769589		[learning rate: 0.00019186]
		[batch 20/20] avg loss: 0.021343877309438965		[learning rate: 0.00019151]
	Learning Rate: 0.000191507
	LOSS [training: 0.03074499699319893 | validation: 0.01307423626786202]
	TIME [epoch: 8.24 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04026946182211448		[learning rate: 0.00019116]
		[batch 20/20] avg loss: 0.026483258614539058		[learning rate: 0.00019081]
	Learning Rate: 0.000190812
	LOSS [training: 0.033376360218326774 | validation: 0.03921455610356529]
	TIME [epoch: 8.27 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03326743081823216		[learning rate: 0.00019047]
		[batch 20/20] avg loss: 0.0307004747129812		[learning rate: 0.00019012]
	Learning Rate: 0.000190119
	LOSS [training: 0.031983952765606685 | validation: 0.018938838080538208]
	TIME [epoch: 8.25 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03368631620071935		[learning rate: 0.00018977]
		[batch 20/20] avg loss: 0.03292154680020429		[learning rate: 0.00018943]
	Learning Rate: 0.000189429
	LOSS [training: 0.03330393150046182 | validation: 0.019725237506560547]
	TIME [epoch: 8.25 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03050357978362171		[learning rate: 0.00018909]
		[batch 20/20] avg loss: 0.030957943029765983		[learning rate: 0.00018874]
	Learning Rate: 0.000188742
	LOSS [training: 0.03073076140669385 | validation: 0.03949848949542171]
	TIME [epoch: 8.24 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029648907825154374		[learning rate: 0.0001884]
		[batch 20/20] avg loss: 0.041601789700967576		[learning rate: 0.00018806]
	Learning Rate: 0.000188057
	LOSS [training: 0.035625348763060984 | validation: 0.030479876102243026]
	TIME [epoch: 8.26 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03950396405859848		[learning rate: 0.00018772]
		[batch 20/20] avg loss: 0.034010952865131734		[learning rate: 0.00018737]
	Learning Rate: 0.000187375
	LOSS [training: 0.03675745846186511 | validation: 0.047209694514352495]
	TIME [epoch: 8.27 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044123688978229145		[learning rate: 0.00018703]
		[batch 20/20] avg loss: 0.0389777552697604		[learning rate: 0.00018669]
	Learning Rate: 0.000186695
	LOSS [training: 0.041550722123994764 | validation: 0.01094588788147686]
	TIME [epoch: 8.24 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037779572063439563		[learning rate: 0.00018636]
		[batch 20/20] avg loss: 0.02215936366034043		[learning rate: 0.00018602]
	Learning Rate: 0.000186017
	LOSS [training: 0.029969467861890003 | validation: 0.019931815096950297]
	TIME [epoch: 8.25 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024707765542969044		[learning rate: 0.00018568]
		[batch 20/20] avg loss: 0.032449834894072646		[learning rate: 0.00018534]
	Learning Rate: 0.000185342
	LOSS [training: 0.02857880021852084 | validation: 0.04881923719893712]
	TIME [epoch: 8.24 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043267108050176036		[learning rate: 0.00018501]
		[batch 20/20] avg loss: 0.0336369905208558		[learning rate: 0.00018467]
	Learning Rate: 0.000184669
	LOSS [training: 0.03845204928551592 | validation: 0.06888611341422067]
	TIME [epoch: 8.27 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04478390967488833		[learning rate: 0.00018433]
		[batch 20/20] avg loss: 0.026138250909555266		[learning rate: 0.000184]
	Learning Rate: 0.000183999
	LOSS [training: 0.0354610802922218 | validation: 0.01623610775541798]
	TIME [epoch: 8.24 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029932109147815474		[learning rate: 0.00018367]
		[batch 20/20] avg loss: 0.03769548117521337		[learning rate: 0.00018333]
	Learning Rate: 0.000183331
	LOSS [training: 0.03381379516151443 | validation: 0.03156855661751332]
	TIME [epoch: 8.25 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02714008169211841		[learning rate: 0.000183]
		[batch 20/20] avg loss: 0.03042251795252378		[learning rate: 0.00018267]
	Learning Rate: 0.000182666
	LOSS [training: 0.028781299822321093 | validation: 0.01614472758148618]
	TIME [epoch: 8.24 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024138484662464987		[learning rate: 0.00018233]
		[batch 20/20] avg loss: 0.03487704865381772		[learning rate: 0.000182]
	Learning Rate: 0.000182003
	LOSS [training: 0.029507766658141355 | validation: 0.05821555491861291]
	TIME [epoch: 8.26 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04820113703305316		[learning rate: 0.00018167]
		[batch 20/20] avg loss: 0.03834262284252339		[learning rate: 0.00018134]
	Learning Rate: 0.000181343
	LOSS [training: 0.04327187993778828 | validation: 0.024858625642156164]
	TIME [epoch: 8.25 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045019645703139884		[learning rate: 0.00018101]
		[batch 20/20] avg loss: 0.03752982617379489		[learning rate: 0.00018068]
	Learning Rate: 0.000180685
	LOSS [training: 0.04127473593846738 | validation: 0.013032752275397803]
	TIME [epoch: 8.25 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02568799854623651		[learning rate: 0.00018036]
		[batch 20/20] avg loss: 0.024542609829203657		[learning rate: 0.00018003]
	Learning Rate: 0.000180029
	LOSS [training: 0.025115304187720087 | validation: 0.017314309109036165]
	TIME [epoch: 8.24 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038400584304593796		[learning rate: 0.0001797]
		[batch 20/20] avg loss: 0.03635758602553934		[learning rate: 0.00017938]
	Learning Rate: 0.000179376
	LOSS [training: 0.037379085165066564 | validation: 0.017775849685220863]
	TIME [epoch: 8.24 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025149755990300827		[learning rate: 0.00017905]
		[batch 20/20] avg loss: 0.0303522889836234		[learning rate: 0.00017872]
	Learning Rate: 0.000178725
	LOSS [training: 0.027751022486962113 | validation: 0.029313449665823374]
	TIME [epoch: 8.27 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02868296335397611		[learning rate: 0.0001784]
		[batch 20/20] avg loss: 0.03394202310084834		[learning rate: 0.00017808]
	Learning Rate: 0.000178076
	LOSS [training: 0.03131249322741223 | validation: 0.021986615388351963]
	TIME [epoch: 8.24 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03253285762064352		[learning rate: 0.00017775]
		[batch 20/20] avg loss: 0.03913215019135981		[learning rate: 0.00017743]
	Learning Rate: 0.00017743
	LOSS [training: 0.035832503906001664 | validation: 0.06495409651559267]
	TIME [epoch: 8.24 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05088188188032239		[learning rate: 0.00017711]
		[batch 20/20] avg loss: 0.04133343950089626		[learning rate: 0.00017679]
	Learning Rate: 0.000176786
	LOSS [training: 0.046107660690609314 | validation: 0.01491904769670216]
	TIME [epoch: 8.24 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022710717466359755		[learning rate: 0.00017646]
		[batch 20/20] avg loss: 0.04539221903111151		[learning rate: 0.00017614]
	Learning Rate: 0.000176144
	LOSS [training: 0.03405146824873563 | validation: 0.06083007115814895]
	TIME [epoch: 8.27 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06559265374294435		[learning rate: 0.00017582]
		[batch 20/20] avg loss: 0.02521893940559014		[learning rate: 0.0001755]
	Learning Rate: 0.000175505
	LOSS [training: 0.04540579657426726 | validation: 0.01708404269763357]
	TIME [epoch: 8.24 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02458159949401766		[learning rate: 0.00017519]
		[batch 20/20] avg loss: 0.03771949097135679		[learning rate: 0.00017487]
	Learning Rate: 0.000174868
	LOSS [training: 0.031150545232687222 | validation: 0.017306394115011964]
	TIME [epoch: 8.24 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03245329207647348		[learning rate: 0.00017455]
		[batch 20/20] avg loss: 0.021786979043013917		[learning rate: 0.00017423]
	Learning Rate: 0.000174233
	LOSS [training: 0.027120135559743703 | validation: 0.02025934530212234]
	TIME [epoch: 8.24 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027861944967717062		[learning rate: 0.00017392]
		[batch 20/20] avg loss: 0.019559989080323924		[learning rate: 0.0001736]
	Learning Rate: 0.000173601
	LOSS [training: 0.023710967024020497 | validation: 0.07012102308372135]
	TIME [epoch: 8.25 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0558109248563622		[learning rate: 0.00017329]
		[batch 20/20] avg loss: 0.05738752591721251		[learning rate: 0.00017297]
	Learning Rate: 0.000172971
	LOSS [training: 0.056599225386787355 | validation: 0.04719870972308758]
	TIME [epoch: 8.26 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025470575401472084		[learning rate: 0.00017266]
		[batch 20/20] avg loss: 0.020820328507482597		[learning rate: 0.00017234]
	Learning Rate: 0.000172343
	LOSS [training: 0.02314545195447735 | validation: 0.02921102340886018]
	TIME [epoch: 8.24 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03182716288119744		[learning rate: 0.00017203]
		[batch 20/20] avg loss: 0.03931706491820311		[learning rate: 0.00017172]
	Learning Rate: 0.000171718
	LOSS [training: 0.03557211389970026 | validation: 0.009710723621659383]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_1218.pth
	Model improved!!!
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038569960143063645		[learning rate: 0.00017141]
		[batch 20/20] avg loss: 0.05041582180478109		[learning rate: 0.00017109]
	Learning Rate: 0.000171095
	LOSS [training: 0.044492890973922374 | validation: 0.01773903780232746]
	TIME [epoch: 8.24 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028679369585924364		[learning rate: 0.00017078]
		[batch 20/20] avg loss: 0.016725288953371963		[learning rate: 0.00017047]
	Learning Rate: 0.000170474
	LOSS [training: 0.02270232926964816 | validation: 0.02215785159673694]
	TIME [epoch: 8.26 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03340962606098147		[learning rate: 0.00017016]
		[batch 20/20] avg loss: 0.03592316397980633		[learning rate: 0.00016986]
	Learning Rate: 0.000169855
	LOSS [training: 0.03466639502039389 | validation: 0.033051906728677714]
	TIME [epoch: 8.23 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0337529135305724		[learning rate: 0.00016955]
		[batch 20/20] avg loss: 0.03405916058456358		[learning rate: 0.00016924]
	Learning Rate: 0.000169239
	LOSS [training: 0.03390603705756799 | validation: 0.02427470220655994]
	TIME [epoch: 8.24 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053921867302826644		[learning rate: 0.00016893]
		[batch 20/20] avg loss: 0.029417014929340263		[learning rate: 0.00016862]
	Learning Rate: 0.000168625
	LOSS [training: 0.041669441116083464 | validation: 0.018915341730568136]
	TIME [epoch: 8.24 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025449046576876723		[learning rate: 0.00016832]
		[batch 20/20] avg loss: 0.024145201273301404		[learning rate: 0.00016801]
	Learning Rate: 0.000168013
	LOSS [training: 0.024797123925089065 | validation: 0.02458665262544392]
	TIME [epoch: 8.26 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02986462249861969		[learning rate: 0.00016771]
		[batch 20/20] avg loss: 0.027979444329446067		[learning rate: 0.0001674]
	Learning Rate: 0.000167403
	LOSS [training: 0.028922033414032884 | validation: 0.01797103211477548]
	TIME [epoch: 8.24 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03875768718025564		[learning rate: 0.0001671]
		[batch 20/20] avg loss: 0.02937826495009646		[learning rate: 0.0001668]
	Learning Rate: 0.000166795
	LOSS [training: 0.034067976065176045 | validation: 0.012258293582460444]
	TIME [epoch: 8.23 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026904704568516197		[learning rate: 0.00016649]
		[batch 20/20] avg loss: 0.035029021943270455		[learning rate: 0.00016619]
	Learning Rate: 0.00016619
	LOSS [training: 0.030966863255893325 | validation: 0.01670039557692567]
	TIME [epoch: 8.23 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02453913790890693		[learning rate: 0.00016589]
		[batch 20/20] avg loss: 0.019099326131329503		[learning rate: 0.00016559]
	Learning Rate: 0.000165587
	LOSS [training: 0.021819232020118214 | validation: 0.017269189552627528]
	TIME [epoch: 8.23 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022325355848833415		[learning rate: 0.00016529]
		[batch 20/20] avg loss: 0.029251642390050285		[learning rate: 0.00016499]
	Learning Rate: 0.000164986
	LOSS [training: 0.025788499119441855 | validation: 0.014529057333333173]
	TIME [epoch: 8.26 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024144224166086605		[learning rate: 0.00016469]
		[batch 20/20] avg loss: 0.022170750714088038		[learning rate: 0.00016439]
	Learning Rate: 0.000164387
	LOSS [training: 0.02315748744008732 | validation: 0.02638633159426807]
	TIME [epoch: 8.23 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02604890377412395		[learning rate: 0.00016409]
		[batch 20/20] avg loss: 0.05611929485254251		[learning rate: 0.00016379]
	Learning Rate: 0.000163791
	LOSS [training: 0.04108409931333323 | validation: 0.0330689807743193]
	TIME [epoch: 8.23 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05446033781877446		[learning rate: 0.00016349]
		[batch 20/20] avg loss: 0.03042554798981596		[learning rate: 0.0001632]
	Learning Rate: 0.000163196
	LOSS [training: 0.042442942904295206 | validation: 0.019350634463425824]
	TIME [epoch: 8.23 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024910241270031275		[learning rate: 0.0001629]
		[batch 20/20] avg loss: 0.024322395595636752		[learning rate: 0.0001626]
	Learning Rate: 0.000162604
	LOSS [training: 0.02461631843283401 | validation: 0.0321489506699799]
	TIME [epoch: 8.26 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026612108287270935		[learning rate: 0.00016231]
		[batch 20/20] avg loss: 0.025415729441536878		[learning rate: 0.00016201]
	Learning Rate: 0.000162014
	LOSS [training: 0.026013918864403913 | validation: 0.013652705920999085]
	TIME [epoch: 8.24 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03295034700112604		[learning rate: 0.00016172]
		[batch 20/20] avg loss: 0.032268886329646064		[learning rate: 0.00016143]
	Learning Rate: 0.000161426
	LOSS [training: 0.03260961666538607 | validation: 0.04523672068242383]
	TIME [epoch: 8.23 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03727513421588452		[learning rate: 0.00016113]
		[batch 20/20] avg loss: 0.03105505151943642		[learning rate: 0.00016084]
	Learning Rate: 0.00016084
	LOSS [training: 0.034165092867660465 | validation: 0.0218283465617765]
	TIME [epoch: 8.23 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04199436005334837		[learning rate: 0.00016055]
		[batch 20/20] avg loss: 0.03909405407587087		[learning rate: 0.00016026]
	Learning Rate: 0.000160257
	LOSS [training: 0.040544207064609615 | validation: 0.019279632950625163]
	TIME [epoch: 8.24 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03526798559762383		[learning rate: 0.00015997]
		[batch 20/20] avg loss: 0.04511870108714909		[learning rate: 0.00015967]
	Learning Rate: 0.000159675
	LOSS [training: 0.04019334334238646 | validation: 0.011935620997285405]
	TIME [epoch: 8.24 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029056890929821617		[learning rate: 0.00015938]
		[batch 20/20] avg loss: 0.02693531267153938		[learning rate: 0.0001591]
	Learning Rate: 0.000159096
	LOSS [training: 0.027996101800680497 | validation: 0.0228898173958515]
	TIME [epoch: 8.23 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027304346145593884		[learning rate: 0.00015881]
		[batch 20/20] avg loss: 0.0192268743074539		[learning rate: 0.00015852]
	Learning Rate: 0.000158518
	LOSS [training: 0.023265610226523892 | validation: 0.009168088788889769]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_1240.pth
	Model improved!!!
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02684518043533348		[learning rate: 0.00015823]
		[batch 20/20] avg loss: 0.020258393219294308		[learning rate: 0.00015794]
	Learning Rate: 0.000157943
	LOSS [training: 0.023551786827313893 | validation: 0.009910787702809586]
	TIME [epoch: 8.23 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03244342925294521		[learning rate: 0.00015766]
		[batch 20/20] avg loss: 0.02116539445734154		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.02680441185514338 | validation: 0.02063994423820282]
	TIME [epoch: 8.25 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023779181363363152		[learning rate: 0.00015708]
		[batch 20/20] avg loss: 0.028930012683743716		[learning rate: 0.0001568]
	Learning Rate: 0.000156799
	LOSS [training: 0.026354597023553434 | validation: 0.036965420314916314]
	TIME [epoch: 8.23 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02793756346771568		[learning rate: 0.00015651]
		[batch 20/20] avg loss: 0.03118262926772894		[learning rate: 0.00015623]
	Learning Rate: 0.00015623
	LOSS [training: 0.029560096367722322 | validation: 0.017655715779764058]
	TIME [epoch: 8.23 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02845750982357496		[learning rate: 0.00015595]
		[batch 20/20] avg loss: 0.029243274807899854		[learning rate: 0.00015566]
	Learning Rate: 0.000155663
	LOSS [training: 0.02885039231573741 | validation: 0.02564956460705862]
	TIME [epoch: 8.23 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03256425504793728		[learning rate: 0.00015538]
		[batch 20/20] avg loss: 0.026983901490721506		[learning rate: 0.0001551]
	Learning Rate: 0.000155098
	LOSS [training: 0.029774078269329395 | validation: 0.02950485272221378]
	TIME [epoch: 8.25 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0240109390799284		[learning rate: 0.00015482]
		[batch 20/20] avg loss: 0.042678114657492716		[learning rate: 0.00015453]
	Learning Rate: 0.000154535
	LOSS [training: 0.03334452686871055 | validation: 0.036148663405181296]
	TIME [epoch: 8.23 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029371905038928836		[learning rate: 0.00015425]
		[batch 20/20] avg loss: 0.03505585429154477		[learning rate: 0.00015397]
	Learning Rate: 0.000153974
	LOSS [training: 0.0322138796652368 | validation: 0.0525551204731213]
	TIME [epoch: 8.23 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039537991494809084		[learning rate: 0.00015369]
		[batch 20/20] avg loss: 0.03216258158571186		[learning rate: 0.00015342]
	Learning Rate: 0.000153415
	LOSS [training: 0.03585028654026047 | validation: 0.03365739277566257]
	TIME [epoch: 8.23 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03824773715126256		[learning rate: 0.00015314]
		[batch 20/20] avg loss: 0.021469668919607495		[learning rate: 0.00015286]
	Learning Rate: 0.000152858
	LOSS [training: 0.029858703035435023 | validation: 0.019522873023457777]
	TIME [epoch: 8.24 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0247937954722746		[learning rate: 0.00015258]
		[batch 20/20] avg loss: 0.026062244406591058		[learning rate: 0.0001523]
	Learning Rate: 0.000152304
	LOSS [training: 0.025428019939432823 | validation: 0.008879209145003737]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_1251.pth
	Model improved!!!
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025873498186223115		[learning rate: 0.00015203]
		[batch 20/20] avg loss: 0.02890527060637222		[learning rate: 0.00015175]
	Learning Rate: 0.000151751
	LOSS [training: 0.02738938439629767 | validation: 0.01479363351882738]
	TIME [epoch: 8.24 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023841667923361808		[learning rate: 0.00015148]
		[batch 20/20] avg loss: 0.05082040448614404		[learning rate: 0.0001512]
	Learning Rate: 0.0001512
	LOSS [training: 0.037331036204752914 | validation: 0.01691963196820421]
	TIME [epoch: 8.23 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03832855482856172		[learning rate: 0.00015093]
		[batch 20/20] avg loss: 0.02321398782599618		[learning rate: 0.00015065]
	Learning Rate: 0.000150652
	LOSS [training: 0.030771271327278943 | validation: 0.020610370375667247]
	TIME [epoch: 8.24 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03301729356794067		[learning rate: 0.00015038]
		[batch 20/20] avg loss: 0.02665998655996419		[learning rate: 0.0001501]
	Learning Rate: 0.000150105
	LOSS [training: 0.029838640063952426 | validation: 0.012696126029846233]
	TIME [epoch: 8.25 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043971087738738306		[learning rate: 0.00014983]
		[batch 20/20] avg loss: 0.025367697685671698		[learning rate: 0.00014956]
	Learning Rate: 0.00014956
	LOSS [training: 0.034669392712205004 | validation: 0.005121303439099511]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_1256.pth
	Model improved!!!
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021130381434958562		[learning rate: 0.00014929]
		[batch 20/20] avg loss: 0.02905084057805347		[learning rate: 0.00014902]
	Learning Rate: 0.000149017
	LOSS [training: 0.02509061100650601 | validation: 0.023296670090248415]
	TIME [epoch: 8.24 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02315224228518263		[learning rate: 0.00014875]
		[batch 20/20] avg loss: 0.026620254163990137		[learning rate: 0.00014848]
	Learning Rate: 0.000148477
	LOSS [training: 0.024886248224586384 | validation: 0.0116287219574076]
	TIME [epoch: 8.24 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03579257414254487		[learning rate: 0.00014821]
		[batch 20/20] avg loss: 0.027450550494565296		[learning rate: 0.00014794]
	Learning Rate: 0.000147938
	LOSS [training: 0.031621562318555084 | validation: 0.013717046581353927]
	TIME [epoch: 8.25 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01717503408525286		[learning rate: 0.00014767]
		[batch 20/20] avg loss: 0.021273266165821845		[learning rate: 0.0001474]
	Learning Rate: 0.000147401
	LOSS [training: 0.01922415012553735 | validation: 0.0241865104836168]
	TIME [epoch: 8.24 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02791044743531736		[learning rate: 0.00014713]
		[batch 20/20] avg loss: 0.0400485848114921		[learning rate: 0.00014687]
	Learning Rate: 0.000146866
	LOSS [training: 0.03397951612340472 | validation: 0.012825478014037878]
	TIME [epoch: 8.23 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05280831272432259		[learning rate: 0.0001466]
		[batch 20/20] avg loss: 0.017426795321711618		[learning rate: 0.00014633]
	Learning Rate: 0.000146333
	LOSS [training: 0.03511755402301712 | validation: 0.014697555151285421]
	TIME [epoch: 8.24 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022004981288564502		[learning rate: 0.00014607]
		[batch 20/20] avg loss: 0.022323451128973777		[learning rate: 0.0001458]
	Learning Rate: 0.000145802
	LOSS [training: 0.022164216208769138 | validation: 0.014733256769862679]
	TIME [epoch: 8.24 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013699563771479472		[learning rate: 0.00014554]
		[batch 20/20] avg loss: 0.02840116786435351		[learning rate: 0.00014527]
	Learning Rate: 0.000145273
	LOSS [training: 0.021050365817916493 | validation: 0.0189907474262384]
	TIME [epoch: 8.25 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04252483994187634		[learning rate: 0.00014501]
		[batch 20/20] avg loss: 0.030581852152138832		[learning rate: 0.00014475]
	Learning Rate: 0.000144746
	LOSS [training: 0.036553346047007586 | validation: 0.020403711325051064]
	TIME [epoch: 8.23 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023513076071193757		[learning rate: 0.00014448]
		[batch 20/20] avg loss: 0.02485632778609523		[learning rate: 0.00014422]
	Learning Rate: 0.00014422
	LOSS [training: 0.024184701928644494 | validation: 0.03232521278613036]
	TIME [epoch: 8.24 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021932596358103695		[learning rate: 0.00014396]
		[batch 20/20] avg loss: 0.03331438120604205		[learning rate: 0.0001437]
	Learning Rate: 0.000143697
	LOSS [training: 0.027623488782072873 | validation: 0.02677871200887405]
	TIME [epoch: 8.23 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027115332767613225		[learning rate: 0.00014344]
		[batch 20/20] avg loss: 0.018816633000252055		[learning rate: 0.00014318]
	Learning Rate: 0.000143175
	LOSS [training: 0.02296598288393264 | validation: 0.01781330245816555]
	TIME [epoch: 8.25 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023709014557413845		[learning rate: 0.00014292]
		[batch 20/20] avg loss: 0.033198112949238315		[learning rate: 0.00014266]
	Learning Rate: 0.000142656
	LOSS [training: 0.028453563753326076 | validation: 0.01915139743377825]
	TIME [epoch: 8.22 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023032259189161138		[learning rate: 0.0001424]
		[batch 20/20] avg loss: 0.042297629043924596		[learning rate: 0.00014214]
	Learning Rate: 0.000142138
	LOSS [training: 0.03266494411654287 | validation: 0.016715782673264984]
	TIME [epoch: 8.22 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02786375408730795		[learning rate: 0.00014188]
		[batch 20/20] avg loss: 0.02395385307963887		[learning rate: 0.00014162]
	Learning Rate: 0.000141622
	LOSS [training: 0.02590880358347341 | validation: 0.021723798923960504]
	TIME [epoch: 8.22 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0228816787127193		[learning rate: 0.00014137]
		[batch 20/20] avg loss: 0.030752316613986815		[learning rate: 0.00014111]
	Learning Rate: 0.000141108
	LOSS [training: 0.02681699766335306 | validation: 0.02205629757866464]
	TIME [epoch: 8.25 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024972825040604243		[learning rate: 0.00014085]
		[batch 20/20] avg loss: 0.025948106784960828		[learning rate: 0.0001406]
	Learning Rate: 0.000140596
	LOSS [training: 0.02546046591278254 | validation: 0.013727930903158294]
	TIME [epoch: 8.24 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02125611455490322		[learning rate: 0.00014034]
		[batch 20/20] avg loss: 0.030443418257651456		[learning rate: 0.00014009]
	Learning Rate: 0.000140086
	LOSS [training: 0.02584976640627734 | validation: 0.008348442008628268]
	TIME [epoch: 8.24 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01629615704790152		[learning rate: 0.00013983]
		[batch 20/20] avg loss: 0.03368469085077399		[learning rate: 0.00013958]
	Learning Rate: 0.000139578
	LOSS [training: 0.024990423949337755 | validation: 0.01283161754301804]
	TIME [epoch: 8.23 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02570050577724341		[learning rate: 0.00013932]
		[batch 20/20] avg loss: 0.03521119270689454		[learning rate: 0.00013907]
	Learning Rate: 0.000139071
	LOSS [training: 0.030455849242068976 | validation: 0.011140949601740632]
	TIME [epoch: 8.24 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026023177073697938		[learning rate: 0.00013882]
		[batch 20/20] avg loss: 0.02485291618673265		[learning rate: 0.00013857]
	Learning Rate: 0.000138566
	LOSS [training: 0.025438046630215295 | validation: 0.0298032927375808]
	TIME [epoch: 8.25 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058384870851332905		[learning rate: 0.00013831]
		[batch 20/20] avg loss: 0.025522021620746314		[learning rate: 0.00013806]
	Learning Rate: 0.000138064
	LOSS [training: 0.04195344623603962 | validation: 0.012851038310729174]
	TIME [epoch: 8.24 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026860533938227675		[learning rate: 0.00013781]
		[batch 20/20] avg loss: 0.02541523654004611		[learning rate: 0.00013756]
	Learning Rate: 0.000137562
	LOSS [training: 0.026137885239136894 | validation: 0.005599215603186711]
	TIME [epoch: 8.23 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02382954863067948		[learning rate: 0.00013731]
		[batch 20/20] avg loss: 0.022984541954949526		[learning rate: 0.00013706]
	Learning Rate: 0.000137063
	LOSS [training: 0.023407045292814503 | validation: 0.017212832734867768]
	TIME [epoch: 8.23 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02498094203102469		[learning rate: 0.00013681]
		[batch 20/20] avg loss: 0.04210150248392201		[learning rate: 0.00013657]
	Learning Rate: 0.000136566
	LOSS [training: 0.03354122225747336 | validation: 0.015892515680754317]
	TIME [epoch: 8.25 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030684013332991422		[learning rate: 0.00013632]
		[batch 20/20] avg loss: 0.025833240504922528		[learning rate: 0.00013607]
	Learning Rate: 0.00013607
	LOSS [training: 0.028258626918956975 | validation: 0.019314160870195007]
	TIME [epoch: 8.23 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027634482446610632		[learning rate: 0.00013582]
		[batch 20/20] avg loss: 0.023069387643202325		[learning rate: 0.00013558]
	Learning Rate: 0.000135576
	LOSS [training: 0.025351935044906477 | validation: 0.050399929587975796]
	TIME [epoch: 8.23 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03710177365360402		[learning rate: 0.00013533]
		[batch 20/20] avg loss: 0.02773417935804038		[learning rate: 0.00013508]
	Learning Rate: 0.000135084
	LOSS [training: 0.032417976505822205 | validation: 0.038901040073834645]
	TIME [epoch: 8.22 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028626823728052486		[learning rate: 0.00013484]
		[batch 20/20] avg loss: 0.022664172871851455		[learning rate: 0.00013459]
	Learning Rate: 0.000134594
	LOSS [training: 0.02564549829995197 | validation: 0.010116785700869236]
	TIME [epoch: 8.24 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02378408815496839		[learning rate: 0.00013435]
		[batch 20/20] avg loss: 0.02195348400975579		[learning rate: 0.00013411]
	Learning Rate: 0.000134106
	LOSS [training: 0.0228687860823621 | validation: 0.02345826365856791]
	TIME [epoch: 8.24 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02617980221245052		[learning rate: 0.00013386]
		[batch 20/20] avg loss: 0.024946290373609945		[learning rate: 0.00013362]
	Learning Rate: 0.000133619
	LOSS [training: 0.025563046293030234 | validation: 0.016968485567020162]
	TIME [epoch: 8.22 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02147309267385317		[learning rate: 0.00013338]
		[batch 20/20] avg loss: 0.026355795336586017		[learning rate: 0.00013313]
	Learning Rate: 0.000133134
	LOSS [training: 0.023914444005219596 | validation: 0.011770317613468213]
	TIME [epoch: 8.22 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024810332472848112		[learning rate: 0.00013289]
		[batch 20/20] avg loss: 0.03127881744817233		[learning rate: 0.00013265]
	Learning Rate: 0.000132651
	LOSS [training: 0.028044574960510223 | validation: 0.026055701892450656]
	TIME [epoch: 8.23 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027966593944521466		[learning rate: 0.00013241]
		[batch 20/20] avg loss: 0.017830754078331408		[learning rate: 0.00013217]
	Learning Rate: 0.00013217
	LOSS [training: 0.022898674011426437 | validation: 0.022282966647583627]
	TIME [epoch: 8.25 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02422406034743283		[learning rate: 0.00013193]
		[batch 20/20] avg loss: 0.027455357690323184		[learning rate: 0.00013169]
	Learning Rate: 0.00013169
	LOSS [training: 0.025839709018878006 | validation: 0.012534571071622641]
	TIME [epoch: 8.23 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026553418463162996		[learning rate: 0.00013145]
		[batch 20/20] avg loss: 0.024590779256915413		[learning rate: 0.00013121]
	Learning Rate: 0.000131212
	LOSS [training: 0.025572098860039204 | validation: 0.01434734841250105]
	TIME [epoch: 8.23 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02263373697614983		[learning rate: 0.00013097]
		[batch 20/20] avg loss: 0.02432930311485019		[learning rate: 0.00013074]
	Learning Rate: 0.000130736
	LOSS [training: 0.02348152004550001 | validation: 0.015243673554603593]
	TIME [epoch: 8.23 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02756937608569896		[learning rate: 0.0001305]
		[batch 20/20] avg loss: 0.027565156949440693		[learning rate: 0.00013026]
	Learning Rate: 0.000130261
	LOSS [training: 0.02756726651756982 | validation: 0.021508752447606444]
	TIME [epoch: 8.24 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02346173076806803		[learning rate: 0.00013002]
		[batch 20/20] avg loss: 0.016665651632684367		[learning rate: 0.00012979]
	Learning Rate: 0.000129789
	LOSS [training: 0.0200636912003762 | validation: 0.018645742112930318]
	TIME [epoch: 8.25 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024126012326866727		[learning rate: 0.00012955]
		[batch 20/20] avg loss: 0.022890805745436567		[learning rate: 0.00012932]
	Learning Rate: 0.000129318
	LOSS [training: 0.023508409036151652 | validation: 0.01787923187768338]
	TIME [epoch: 8.23 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02253957194718774		[learning rate: 0.00012908]
		[batch 20/20] avg loss: 0.025943139342289014		[learning rate: 0.00012885]
	Learning Rate: 0.000128848
	LOSS [training: 0.024241355644738373 | validation: 0.0208677695719077]
	TIME [epoch: 8.22 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028479214825658693		[learning rate: 0.00012861]
		[batch 20/20] avg loss: 0.021678758528790375		[learning rate: 0.00012838]
	Learning Rate: 0.000128381
	LOSS [training: 0.02507898667722454 | validation: 0.013069463339878108]
	TIME [epoch: 8.23 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02306937672776504		[learning rate: 0.00012815]
		[batch 20/20] avg loss: 0.021607186714308807		[learning rate: 0.00012791]
	Learning Rate: 0.000127915
	LOSS [training: 0.02233828172103692 | validation: 0.0257159609986834]
	TIME [epoch: 8.26 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03481738605558889		[learning rate: 0.00012768]
		[batch 20/20] avg loss: 0.03131929691166353		[learning rate: 0.00012745]
	Learning Rate: 0.000127451
	LOSS [training: 0.03306834148362621 | validation: 0.019193528501387988]
	TIME [epoch: 8.23 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02299172795735651		[learning rate: 0.00012722]
		[batch 20/20] avg loss: 0.022643488436725263		[learning rate: 0.00012699]
	Learning Rate: 0.000126988
	LOSS [training: 0.022817608197040887 | validation: 0.034590699458320676]
	TIME [epoch: 8.22 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03539531403097483		[learning rate: 0.00012676]
		[batch 20/20] avg loss: 0.023293754470833372		[learning rate: 0.00012653]
	Learning Rate: 0.000126527
	LOSS [training: 0.0293445342509041 | validation: 0.016774555514929457]
	TIME [epoch: 8.23 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02081034540627143		[learning rate: 0.0001263]
		[batch 20/20] avg loss: 0.021684723470075017		[learning rate: 0.00012607]
	Learning Rate: 0.000126068
	LOSS [training: 0.021247534438173225 | validation: 0.01643182999942959]
	TIME [epoch: 8.25 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03373739385078492		[learning rate: 0.00012584]
		[batch 20/20] avg loss: 0.028432318314291605		[learning rate: 0.00012561]
	Learning Rate: 0.000125611
	LOSS [training: 0.031084856082538265 | validation: 0.015445626228438513]
	TIME [epoch: 8.23 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019359009966800823		[learning rate: 0.00012538]
		[batch 20/20] avg loss: 0.027575884265947242		[learning rate: 0.00012515]
	Learning Rate: 0.000125155
	LOSS [training: 0.02346744711637403 | validation: 0.046641969136598044]
	TIME [epoch: 8.23 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030665810614358312		[learning rate: 0.00012493]
		[batch 20/20] avg loss: 0.02221064964079809		[learning rate: 0.0001247]
	Learning Rate: 0.000124701
	LOSS [training: 0.026438230127578198 | validation: 0.012664505891758503]
	TIME [epoch: 8.24 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024483464230425984		[learning rate: 0.00012447]
		[batch 20/20] avg loss: 0.042349118546992956		[learning rate: 0.00012425]
	Learning Rate: 0.000124248
	LOSS [training: 0.03341629138870947 | validation: 0.02194081831045537]
	TIME [epoch: 8.24 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020718818049378286		[learning rate: 0.00012402]
		[batch 20/20] avg loss: 0.028592293131861547		[learning rate: 0.0001238]
	Learning Rate: 0.000123797
	LOSS [training: 0.024655555590619917 | validation: 0.01398477030996303]
	TIME [epoch: 8.24 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020513702377902825		[learning rate: 0.00012357]
		[batch 20/20] avg loss: 0.029639607573143795		[learning rate: 0.00012335]
	Learning Rate: 0.000123348
	LOSS [training: 0.025076654975523312 | validation: 0.027616547006040746]
	TIME [epoch: 8.24 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027229893289281392		[learning rate: 0.00012312]
		[batch 20/20] avg loss: 0.021520791767373905		[learning rate: 0.0001229]
	Learning Rate: 0.0001229
	LOSS [training: 0.024375342528327647 | validation: 0.01686907886668324]
	TIME [epoch: 8.24 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020332766706530354		[learning rate: 0.00012268]
		[batch 20/20] avg loss: 0.023621412597781197		[learning rate: 0.00012245]
	Learning Rate: 0.000122454
	LOSS [training: 0.021977089652155772 | validation: 0.018039697050811657]
	TIME [epoch: 8.23 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024970594774793013		[learning rate: 0.00012223]
		[batch 20/20] avg loss: 0.03197198728820532		[learning rate: 0.00012201]
	Learning Rate: 0.00012201
	LOSS [training: 0.028471291031499167 | validation: 0.024597234718098195]
	TIME [epoch: 8.25 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03986888075950418		[learning rate: 0.00012179]
		[batch 20/20] avg loss: 0.02334024695790097		[learning rate: 0.00012157]
	Learning Rate: 0.000121567
	LOSS [training: 0.031604563858702577 | validation: 0.017822587503103598]
	TIME [epoch: 8.25 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025329902105121815		[learning rate: 0.00012135]
		[batch 20/20] avg loss: 0.02191452854742158		[learning rate: 0.00012113]
	Learning Rate: 0.000121126
	LOSS [training: 0.0236222153262717 | validation: 0.01529633945672184]
	TIME [epoch: 8.23 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023708105934654358		[learning rate: 0.00012091]
		[batch 20/20] avg loss: 0.03038612578176686		[learning rate: 0.00012069]
	Learning Rate: 0.000120686
	LOSS [training: 0.027047115858210607 | validation: 0.026065915465444883]
	TIME [epoch: 8.23 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01925910699832576		[learning rate: 0.00012047]
		[batch 20/20] avg loss: 0.034304863854079784		[learning rate: 0.00012025]
	Learning Rate: 0.000120248
	LOSS [training: 0.02678198542620277 | validation: 0.008226953730330035]
	TIME [epoch: 8.25 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018790175990297968		[learning rate: 0.00012003]
		[batch 20/20] avg loss: 0.0245103261165911		[learning rate: 0.00011981]
	Learning Rate: 0.000119812
	LOSS [training: 0.021650251053444532 | validation: 0.018954767681036155]
	TIME [epoch: 8.24 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04547287082482807		[learning rate: 0.00011959]
		[batch 20/20] avg loss: 0.023952745717907314		[learning rate: 0.00011938]
	Learning Rate: 0.000119377
	LOSS [training: 0.03471280827136769 | validation: 0.022516051525668098]
	TIME [epoch: 8.23 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025949730343075617		[learning rate: 0.00011916]
		[batch 20/20] avg loss: 0.020887679554151627		[learning rate: 0.00011894]
	Learning Rate: 0.000118944
	LOSS [training: 0.023418704948613624 | validation: 0.0237225111776475]
	TIME [epoch: 8.23 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020046497257707012		[learning rate: 0.00011873]
		[batch 20/20] avg loss: 0.02069865943581744		[learning rate: 0.00011851]
	Learning Rate: 0.000118512
	LOSS [training: 0.020372578346762225 | validation: 0.01471337196930744]
	TIME [epoch: 8.23 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01832179531671945		[learning rate: 0.0001183]
		[batch 20/20] avg loss: 0.01914214416530275		[learning rate: 0.00011808]
	Learning Rate: 0.000118082
	LOSS [training: 0.018731969741011097 | validation: 0.017149947945422992]
	TIME [epoch: 8.26 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025535954213311347		[learning rate: 0.00011787]
		[batch 20/20] avg loss: 0.021754926228904754		[learning rate: 0.00011765]
	Learning Rate: 0.000117654
	LOSS [training: 0.023645440221108052 | validation: 0.009657265190945851]
	TIME [epoch: 8.23 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02211076031971		[learning rate: 0.00011744]
		[batch 20/20] avg loss: 0.024314708079471874		[learning rate: 0.00011723]
	Learning Rate: 0.000117227
	LOSS [training: 0.02321273419959094 | validation: 0.01544769403694487]
	TIME [epoch: 8.23 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033612571708084		[learning rate: 0.00011701]
		[batch 20/20] avg loss: 0.02264489715435577		[learning rate: 0.0001168]
	Learning Rate: 0.000116801
	LOSS [training: 0.028128734431219892 | validation: 0.02694439416628643]
	TIME [epoch: 8.24 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039387102600629956		[learning rate: 0.00011659]
		[batch 20/20] avg loss: 0.02494329729580395		[learning rate: 0.00011638]
	Learning Rate: 0.000116377
	LOSS [training: 0.03216519994821696 | validation: 0.01662648184457112]
	TIME [epoch: 8.25 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023410097439785706		[learning rate: 0.00011617]
		[batch 20/20] avg loss: 0.042507200918407144		[learning rate: 0.00011595]
	Learning Rate: 0.000115955
	LOSS [training: 0.032958649179096425 | validation: 0.009802284932805291]
	TIME [epoch: 8.23 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021547995581314608		[learning rate: 0.00011574]
		[batch 20/20] avg loss: 0.02926313086518601		[learning rate: 0.00011553]
	Learning Rate: 0.000115534
	LOSS [training: 0.02540556322325031 | validation: 0.011987235891272138]
	TIME [epoch: 8.23 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02274777039894728		[learning rate: 0.00011532]
		[batch 20/20] avg loss: 0.02431954469084438		[learning rate: 0.00011511]
	Learning Rate: 0.000115115
	LOSS [training: 0.02353365754489583 | validation: 0.013595233196495202]
	TIME [epoch: 8.22 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019604705292791336		[learning rate: 0.00011491]
		[batch 20/20] avg loss: 0.017518002546890583		[learning rate: 0.0001147]
	Learning Rate: 0.000114697
	LOSS [training: 0.018561353919840956 | validation: 0.01676735112960718]
	TIME [epoch: 8.24 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0236263661169743		[learning rate: 0.00011449]
		[batch 20/20] avg loss: 0.01930535112894973		[learning rate: 0.00011428]
	Learning Rate: 0.000114281
	LOSS [training: 0.021465858622962013 | validation: 0.023334652689132256]
	TIME [epoch: 8.24 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01806886975300512		[learning rate: 0.00011407]
		[batch 20/20] avg loss: 0.028622329280822693		[learning rate: 0.00011387]
	Learning Rate: 0.000113866
	LOSS [training: 0.023345599516913906 | validation: 0.012962143946070936]
	TIME [epoch: 8.23 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03618921797359117		[learning rate: 0.00011366]
		[batch 20/20] avg loss: 0.02503990425013622		[learning rate: 0.00011345]
	Learning Rate: 0.000113453
	LOSS [training: 0.03061456111186369 | validation: 0.006165694616617122]
	TIME [epoch: 8.23 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0186775962239049		[learning rate: 0.00011325]
		[batch 20/20] avg loss: 0.030291954156136513		[learning rate: 0.00011304]
	Learning Rate: 0.000113041
	LOSS [training: 0.024484775190020708 | validation: 0.0162018019553288]
	TIME [epoch: 8.22 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021464638433812244		[learning rate: 0.00011284]
		[batch 20/20] avg loss: 0.028309068377090657		[learning rate: 0.00011263]
	Learning Rate: 0.000112631
	LOSS [training: 0.02488685340545145 | validation: 0.011409818352350078]
	TIME [epoch: 8.26 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02536917390394621		[learning rate: 0.00011243]
		[batch 20/20] avg loss: 0.023169952384942804		[learning rate: 0.00011222]
	Learning Rate: 0.000112222
	LOSS [training: 0.02426956314444451 | validation: 0.02386662305327228]
	TIME [epoch: 8.23 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029256511090023585		[learning rate: 0.00011202]
		[batch 20/20] avg loss: 0.020947899800904158		[learning rate: 0.00011181]
	Learning Rate: 0.000111815
	LOSS [training: 0.025102205445463877 | validation: 0.021331988663493257]
	TIME [epoch: 8.23 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02731606314347886		[learning rate: 0.00011161]
		[batch 20/20] avg loss: 0.016330126389192977		[learning rate: 0.00011141]
	Learning Rate: 0.000111409
	LOSS [training: 0.02182309476633592 | validation: 0.01679609732909234]
	TIME [epoch: 8.23 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021536534066339215		[learning rate: 0.00011121]
		[batch 20/20] avg loss: 0.024067354683539585		[learning rate: 0.000111]
	Learning Rate: 0.000111005
	LOSS [training: 0.0228019443749394 | validation: 0.01989272769012496]
	TIME [epoch: 8.24 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030368945460872022		[learning rate: 0.0001108]
		[batch 20/20] avg loss: 0.0206093302147048		[learning rate: 0.0001106]
	Learning Rate: 0.000110602
	LOSS [training: 0.025489137837788417 | validation: 0.01357920539042725]
	TIME [epoch: 8.24 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03584741121986927		[learning rate: 0.0001104]
		[batch 20/20] avg loss: 0.024937319130154297		[learning rate: 0.0001102]
	Learning Rate: 0.000110201
	LOSS [training: 0.03039236517501178 | validation: 0.015122490400874669]
	TIME [epoch: 8.23 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016809871480253043		[learning rate: 0.00011]
		[batch 20/20] avg loss: 0.017660181757157025		[learning rate: 0.0001098]
	Learning Rate: 0.000109801
	LOSS [training: 0.01723502661870503 | validation: 0.021176960480160996]
	TIME [epoch: 8.23 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025183519903451773		[learning rate: 0.0001096]
		[batch 20/20] avg loss: 0.021689882928523467		[learning rate: 0.0001094]
	Learning Rate: 0.000109402
	LOSS [training: 0.023436701415987624 | validation: 0.023446773968268605]
	TIME [epoch: 8.23 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023077154846634744		[learning rate: 0.0001092]
		[batch 20/20] avg loss: 0.021332202901602343		[learning rate: 0.00010901]
	Learning Rate: 0.000109005
	LOSS [training: 0.02220467887411854 | validation: 0.008358850328632128]
	TIME [epoch: 8.25 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021232851759614278		[learning rate: 0.00010881]
		[batch 20/20] avg loss: 0.029711539400569274		[learning rate: 0.00010861]
	Learning Rate: 0.00010861
	LOSS [training: 0.025472195580091776 | validation: 0.0145470274812958]
	TIME [epoch: 8.23 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03265249848289291		[learning rate: 0.00010841]
		[batch 20/20] avg loss: 0.02433263445982016		[learning rate: 0.00010822]
	Learning Rate: 0.000108215
	LOSS [training: 0.02849256647135654 | validation: 0.013492330136152865]
	TIME [epoch: 8.23 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017991028780709774		[learning rate: 0.00010802]
		[batch 20/20] avg loss: 0.023299888420479857		[learning rate: 0.00010782]
	Learning Rate: 0.000107823
	LOSS [training: 0.020645458600594812 | validation: 0.014351206875424651]
	TIME [epoch: 8.23 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01592321347342205		[learning rate: 0.00010763]
		[batch 20/20] avg loss: 0.023843653534405627		[learning rate: 0.00010743]
	Learning Rate: 0.000107432
	LOSS [training: 0.01988343350391384 | validation: 0.018282669979698497]
	TIME [epoch: 8.25 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025341611741982357		[learning rate: 0.00010724]
		[batch 20/20] avg loss: 0.018767621349072266		[learning rate: 0.00010704]
	Learning Rate: 0.000107042
	LOSS [training: 0.02205461654552731 | validation: 0.017046949904836694]
	TIME [epoch: 8.23 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024069391807523457		[learning rate: 0.00010685]
		[batch 20/20] avg loss: 0.033564316622412965		[learning rate: 0.00010665]
	Learning Rate: 0.000106653
	LOSS [training: 0.028816854214968206 | validation: 0.011491267478910862]
	TIME [epoch: 8.22 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023741728753668992		[learning rate: 0.00010646]
		[batch 20/20] avg loss: 0.02439625468268266		[learning rate: 0.00010627]
	Learning Rate: 0.000106266
	LOSS [training: 0.02406899171817583 | validation: 0.01582503344649632]
	TIME [epoch: 8.22 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029257269532618306		[learning rate: 0.00010607]
		[batch 20/20] avg loss: 0.023545570244723375		[learning rate: 0.00010588]
	Learning Rate: 0.00010588
	LOSS [training: 0.02640141988867084 | validation: 0.009571717290525123]
	TIME [epoch: 8.24 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029140803216583865		[learning rate: 0.00010569]
		[batch 20/20] avg loss: 0.021084050746404952		[learning rate: 0.0001055]
	Learning Rate: 0.000105496
	LOSS [training: 0.025112426981494407 | validation: 0.005113873104550464]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_1352.pth
	Model improved!!!
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026785884928631175		[learning rate: 0.0001053]
		[batch 20/20] avg loss: 0.044193384813118214		[learning rate: 0.00010511]
	Learning Rate: 0.000105113
	LOSS [training: 0.0354896348708747 | validation: 0.031301304588040084]
	TIME [epoch: 8.23 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027971703599087926		[learning rate: 0.00010492]
		[batch 20/20] avg loss: 0.025223314084647404		[learning rate: 0.00010473]
	Learning Rate: 0.000104732
	LOSS [training: 0.02659750884186766 | validation: 0.022516206442525576]
	TIME [epoch: 8.23 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03252872695242631		[learning rate: 0.00010454]
		[batch 20/20] avg loss: 0.014505921485384838		[learning rate: 0.00010435]
	Learning Rate: 0.000104352
	LOSS [training: 0.023517324218905573 | validation: 0.015696798888338637]
	TIME [epoch: 8.23 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020851569128845483		[learning rate: 0.00010416]
		[batch 20/20] avg loss: 0.019197923151135672		[learning rate: 0.00010397]
	Learning Rate: 0.000103973
	LOSS [training: 0.020024746139990578 | validation: 0.013394670319531605]
	TIME [epoch: 8.25 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022341280132855443		[learning rate: 0.00010378]
		[batch 20/20] avg loss: 0.027973193145091935		[learning rate: 0.0001036]
	Learning Rate: 0.000103596
	LOSS [training: 0.02515723663897369 | validation: 0.02291095325883294]
	TIME [epoch: 8.23 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026450345995432594		[learning rate: 0.00010341]
		[batch 20/20] avg loss: 0.020100426364224912		[learning rate: 0.00010322]
	Learning Rate: 0.00010322
	LOSS [training: 0.02327538617982875 | validation: 0.012980547660039414]
	TIME [epoch: 8.22 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022849599178551915		[learning rate: 0.00010303]
		[batch 20/20] avg loss: 0.023786501309425542		[learning rate: 0.00010285]
	Learning Rate: 0.000102845
	LOSS [training: 0.02331805024398873 | validation: 0.015460330818035538]
	TIME [epoch: 8.23 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02298868392121179		[learning rate: 0.00010266]
		[batch 20/20] avg loss: 0.0237601540295095		[learning rate: 0.00010247]
	Learning Rate: 0.000102472
	LOSS [training: 0.023374418975360644 | validation: 0.02117348388465025]
	TIME [epoch: 8.25 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022127541889053078		[learning rate: 0.00010229]
		[batch 20/20] avg loss: 0.02251731356240114		[learning rate: 0.0001021]
	Learning Rate: 0.0001021
	LOSS [training: 0.02232242772572711 | validation: 0.01852514792209773]
	TIME [epoch: 8.23 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020665868928213642		[learning rate: 0.00010191]
		[batch 20/20] avg loss: 0.02243745804232517		[learning rate: 0.00010173]
	Learning Rate: 0.00010173
	LOSS [training: 0.021551663485269407 | validation: 0.010178586359050232]
	TIME [epoch: 8.22 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02110203722269091		[learning rate: 0.00010154]
		[batch 20/20] avg loss: 0.02109632138671545		[learning rate: 0.00010136]
	Learning Rate: 0.00010136
	LOSS [training: 0.02109917930470318 | validation: 0.013170625843824285]
	TIME [epoch: 8.23 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025806880596760055		[learning rate: 0.00010118]
		[batch 20/20] avg loss: 0.04031694618205137		[learning rate: 0.00010099]
	Learning Rate: 0.000100993
	LOSS [training: 0.033061913389405714 | validation: 0.01172959461047441]
	TIME [epoch: 8.22 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02099695822792407		[learning rate: 0.00010081]
		[batch 20/20] avg loss: 0.02997439918728051		[learning rate: 0.00010063]
	Learning Rate: 0.000100626
	LOSS [training: 0.025485678707602294 | validation: 0.008319015214919906]
	TIME [epoch: 8.25 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01763537593132506		[learning rate: 0.00010044]
		[batch 20/20] avg loss: 0.02078767401964006		[learning rate: 0.00010026]
	Learning Rate: 0.000100261
	LOSS [training: 0.01921152497548256 | validation: 0.016039284362142636]
	TIME [epoch: 8.23 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021341121664051367		[learning rate: 0.00010008]
		[batch 20/20] avg loss: 0.017876283296722466		[learning rate: 9.9897e-05]
	Learning Rate: 9.98971e-05
	LOSS [training: 0.01960870248038692 | validation: 0.002545707909500384]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_1367.pth
	Model improved!!!
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022180582031983653		[learning rate: 9.9716e-05]
		[batch 20/20] avg loss: 0.023280815780125648		[learning rate: 9.9535e-05]
	Learning Rate: 9.95345e-05
	LOSS [training: 0.02273069890605465 | validation: 0.013953339969663586]
	TIME [epoch: 8.24 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015931072284262097		[learning rate: 9.9354e-05]
		[batch 20/20] avg loss: 0.018430183500409848		[learning rate: 9.9173e-05]
	Learning Rate: 9.91733e-05
	LOSS [training: 0.01718062789233597 | validation: 0.009569791197718818]
	TIME [epoch: 8.25 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025383855050430996		[learning rate: 9.8993e-05]
		[batch 20/20] avg loss: 0.022356618090127803		[learning rate: 9.8813e-05]
	Learning Rate: 9.88134e-05
	LOSS [training: 0.023870236570279396 | validation: 0.015377396989656443]
	TIME [epoch: 8.24 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026648364568674855		[learning rate: 9.8634e-05]
		[batch 20/20] avg loss: 0.017202481703311526		[learning rate: 9.8455e-05]
	Learning Rate: 9.84548e-05
	LOSS [training: 0.02192542313599319 | validation: 0.003968092234384819]
	TIME [epoch: 8.23 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02184014704391097		[learning rate: 9.8276e-05]
		[batch 20/20] avg loss: 0.020680419575611293		[learning rate: 9.8098e-05]
	Learning Rate: 9.80975e-05
	LOSS [training: 0.021260283309761128 | validation: 0.03844058878734417]
	TIME [epoch: 8.22 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025336385703206298		[learning rate: 9.7919e-05]
		[batch 20/20] avg loss: 0.016157251882197635		[learning rate: 9.7742e-05]
	Learning Rate: 9.77415e-05
	LOSS [training: 0.020746818792701967 | validation: 0.011118439610495261]
	TIME [epoch: 8.24 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01936690241583037		[learning rate: 9.7564e-05]
		[batch 20/20] avg loss: 0.013909161056776536		[learning rate: 9.7387e-05]
	Learning Rate: 9.73868e-05
	LOSS [training: 0.01663803173630345 | validation: 0.01144694777073878]
	TIME [epoch: 8.24 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017153420959913165		[learning rate: 9.721e-05]
		[batch 20/20] avg loss: 0.02758885349017129		[learning rate: 9.7033e-05]
	Learning Rate: 9.70334e-05
	LOSS [training: 0.022371137225042227 | validation: 0.013038860110098959]
	TIME [epoch: 8.23 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02909752179974247		[learning rate: 9.6857e-05]
		[batch 20/20] avg loss: 0.018856009436376123		[learning rate: 9.6681e-05]
	Learning Rate: 9.66812e-05
	LOSS [training: 0.023976765618059295 | validation: 0.01408434976726906]
	TIME [epoch: 8.23 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019913657952203227		[learning rate: 9.6506e-05]
		[batch 20/20] avg loss: 0.02231310013925914		[learning rate: 9.633e-05]
	Learning Rate: 9.63304e-05
	LOSS [training: 0.021113379045731188 | validation: 0.020970714312024775]
	TIME [epoch: 8.24 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018547852425039372		[learning rate: 9.6155e-05]
		[batch 20/20] avg loss: 0.03252170934236157		[learning rate: 9.5981e-05]
	Learning Rate: 9.59808e-05
	LOSS [training: 0.025534780883700474 | validation: 0.03053684189062504]
	TIME [epoch: 8.25 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019088669733446206		[learning rate: 9.5806e-05]
		[batch 20/20] avg loss: 0.021786552167006777		[learning rate: 9.5632e-05]
	Learning Rate: 9.56324e-05
	LOSS [training: 0.020437610950226497 | validation: 0.010575942128490785]
	TIME [epoch: 8.24 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015080603496612346		[learning rate: 9.5459e-05]
		[batch 20/20] avg loss: 0.026107639180108333		[learning rate: 9.5285e-05]
	Learning Rate: 9.52854e-05
	LOSS [training: 0.02059412133836034 | validation: 0.013431247547439485]
	TIME [epoch: 8.23 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018094881304255694		[learning rate: 9.5112e-05]
		[batch 20/20] avg loss: 0.015566612713160339		[learning rate: 9.494e-05]
	Learning Rate: 9.49396e-05
	LOSS [training: 0.016830747008708017 | validation: 0.017762593058774305]
	TIME [epoch: 8.24 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020670950275825474		[learning rate: 9.4767e-05]
		[batch 20/20] avg loss: 0.023508248034128328		[learning rate: 9.4595e-05]
	Learning Rate: 9.45951e-05
	LOSS [training: 0.022089599154976897 | validation: 0.01901248626024803]
	TIME [epoch: 8.26 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028583802999072407		[learning rate: 9.4423e-05]
		[batch 20/20] avg loss: 0.016307993143929597		[learning rate: 9.4252e-05]
	Learning Rate: 9.42518e-05
	LOSS [training: 0.022445898071501005 | validation: 0.014664951898926009]
	TIME [epoch: 8.24 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018612498461020942		[learning rate: 9.4081e-05]
		[batch 20/20] avg loss: 0.023316127817798092		[learning rate: 9.391e-05]
	Learning Rate: 9.39097e-05
	LOSS [training: 0.020964313139409513 | validation: 0.008025223296422584]
	TIME [epoch: 8.23 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015996987995467647		[learning rate: 9.3739e-05]
		[batch 20/20] avg loss: 0.01645326720481287		[learning rate: 9.3569e-05]
	Learning Rate: 9.35689e-05
	LOSS [training: 0.016225127600140256 | validation: 0.009686335040959553]
	TIME [epoch: 8.24 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023546529011002747		[learning rate: 9.3399e-05]
		[batch 20/20] avg loss: 0.020899230207906207		[learning rate: 9.3229e-05]
	Learning Rate: 9.32294e-05
	LOSS [training: 0.02222287960945448 | validation: 0.0123633766058713]
	TIME [epoch: 8.25 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023908185338297586		[learning rate: 9.306e-05]
		[batch 20/20] avg loss: 0.020517854230205846		[learning rate: 9.2891e-05]
	Learning Rate: 9.2891e-05
	LOSS [training: 0.022213019784251713 | validation: 0.01596863326503118]
	TIME [epoch: 8.24 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0233379393788962		[learning rate: 9.2722e-05]
		[batch 20/20] avg loss: 0.0248522417902953		[learning rate: 9.2554e-05]
	Learning Rate: 9.25539e-05
	LOSS [training: 0.02409509058459575 | validation: 0.01984668496104743]
	TIME [epoch: 8.24 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015779596603646494		[learning rate: 9.2386e-05]
		[batch 20/20] avg loss: 0.02021487348289632		[learning rate: 9.2218e-05]
	Learning Rate: 9.2218e-05
	LOSS [training: 0.017997235043271408 | validation: 0.026306529791706595]
	TIME [epoch: 8.22 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02341533374161119		[learning rate: 9.2051e-05]
		[batch 20/20] avg loss: 0.017342138291038086		[learning rate: 9.1883e-05]
	Learning Rate: 9.18834e-05
	LOSS [training: 0.020378736016324634 | validation: 0.00704519382751757]
	TIME [epoch: 8.23 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020072548862140466		[learning rate: 9.1716e-05]
		[batch 20/20] avg loss: 0.028553680907159595		[learning rate: 9.155e-05]
	Learning Rate: 9.15499e-05
	LOSS [training: 0.02431311488465003 | validation: 0.014313665180430866]
	TIME [epoch: 8.26 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025122615730715152		[learning rate: 9.1384e-05]
		[batch 20/20] avg loss: 0.0202064011021435		[learning rate: 9.1218e-05]
	Learning Rate: 9.12177e-05
	LOSS [training: 0.022664508416429332 | validation: 0.012947883928595931]
	TIME [epoch: 8.23 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017421648709001238		[learning rate: 9.1052e-05]
		[batch 20/20] avg loss: 0.016026547216865684		[learning rate: 9.0887e-05]
	Learning Rate: 9.08866e-05
	LOSS [training: 0.01672409796293346 | validation: 0.012704334060160211]
	TIME [epoch: 8.24 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022711632721978358		[learning rate: 9.0722e-05]
		[batch 20/20] avg loss: 0.028156175034990023		[learning rate: 9.0557e-05]
	Learning Rate: 9.05568e-05
	LOSS [training: 0.02543390387848419 | validation: 0.022213580348870172]
	TIME [epoch: 8.24 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02886889394234391		[learning rate: 9.0392e-05]
		[batch 20/20] avg loss: 0.020757436537013188		[learning rate: 9.0228e-05]
	Learning Rate: 9.02281e-05
	LOSS [training: 0.024813165239678546 | validation: 0.007916774655120418]
	TIME [epoch: 8.25 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021706190571697096		[learning rate: 9.0064e-05]
		[batch 20/20] avg loss: 0.019299294588186848		[learning rate: 8.9901e-05]
	Learning Rate: 8.99007e-05
	LOSS [training: 0.02050274257994197 | validation: 0.01198620660762189]
	TIME [epoch: 8.23 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036192219221609506		[learning rate: 8.9737e-05]
		[batch 20/20] avg loss: 0.010636795384182201		[learning rate: 8.9574e-05]
	Learning Rate: 8.95745e-05
	LOSS [training: 0.023414507302895855 | validation: 0.010822946138002821]
	TIME [epoch: 8.23 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016426905191340765		[learning rate: 8.9412e-05]
		[batch 20/20] avg loss: 0.017757205479040066		[learning rate: 8.9249e-05]
	Learning Rate: 8.92494e-05
	LOSS [training: 0.01709205533519042 | validation: 0.01975791404423459]
	TIME [epoch: 8.23 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024445997034079462		[learning rate: 8.9087e-05]
		[batch 20/20] avg loss: 0.01987517142533154		[learning rate: 8.8926e-05]
	Learning Rate: 8.89255e-05
	LOSS [training: 0.022160584229705505 | validation: 0.017934499471082173]
	TIME [epoch: 8.23 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025565410034022316		[learning rate: 8.8764e-05]
		[batch 20/20] avg loss: 0.02793523973104095		[learning rate: 8.8603e-05]
	Learning Rate: 8.86028e-05
	LOSS [training: 0.026750324882531634 | validation: 0.019946863181075504]
	TIME [epoch: 8.26 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021506879762624692		[learning rate: 8.8442e-05]
		[batch 20/20] avg loss: 0.021964570094732173		[learning rate: 8.8281e-05]
	Learning Rate: 8.82813e-05
	LOSS [training: 0.021735724928678427 | validation: 0.01947359174286744]
	TIME [epoch: 8.24 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022341256166996754		[learning rate: 8.8121e-05]
		[batch 20/20] avg loss: 0.03703305403226677		[learning rate: 8.7961e-05]
	Learning Rate: 8.79609e-05
	LOSS [training: 0.02968715509963176 | validation: 0.02018007222657461]
	TIME [epoch: 8.24 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023109149066253328		[learning rate: 8.7801e-05]
		[batch 20/20] avg loss: 0.03942757327205058		[learning rate: 8.7642e-05]
	Learning Rate: 8.76417e-05
	LOSS [training: 0.03126836116915196 | validation: 0.011842752909888761]
	TIME [epoch: 8.23 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0121692970391243		[learning rate: 8.7482e-05]
		[batch 20/20] avg loss: 0.030678161042062564		[learning rate: 8.7324e-05]
	Learning Rate: 8.73236e-05
	LOSS [training: 0.02142372904059343 | validation: 0.019034286293683374]
	TIME [epoch: 8.25 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030244085805280773		[learning rate: 8.7165e-05]
		[batch 20/20] avg loss: 0.02656650598715221		[learning rate: 8.7007e-05]
	Learning Rate: 8.70067e-05
	LOSS [training: 0.028405295896216494 | validation: 0.03727377963806069]
	TIME [epoch: 8.24 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021679844013120068		[learning rate: 8.6849e-05]
		[batch 20/20] avg loss: 0.03833262938715219		[learning rate: 8.6691e-05]
	Learning Rate: 8.66909e-05
	LOSS [training: 0.030006236700136125 | validation: 0.0087068630972076]
	TIME [epoch: 8.23 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01671262718202628		[learning rate: 8.6533e-05]
		[batch 20/20] avg loss: 0.020007359646432486		[learning rate: 8.6376e-05]
	Learning Rate: 8.63763e-05
	LOSS [training: 0.01835999341422938 | validation: 0.012719882828495156]
	TIME [epoch: 8.22 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02399584397936424		[learning rate: 8.6219e-05]
		[batch 20/20] avg loss: 0.029666445746957502		[learning rate: 8.6063e-05]
	Learning Rate: 8.60629e-05
	LOSS [training: 0.02683114486316087 | validation: 0.018492624306056256]
	TIME [epoch: 8.24 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022235639244051742		[learning rate: 8.5907e-05]
		[batch 20/20] avg loss: 0.013677243622876884		[learning rate: 8.5751e-05]
	Learning Rate: 8.57505e-05
	LOSS [training: 0.01795644143346431 | validation: 0.014761985484867988]
	TIME [epoch: 8.23 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015822932040777844		[learning rate: 8.5595e-05]
		[batch 20/20] avg loss: 0.01675886611355048		[learning rate: 8.5439e-05]
	Learning Rate: 8.54394e-05
	LOSS [training: 0.01629089907716416 | validation: 0.008590157047106979]
	TIME [epoch: 8.23 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015715459215358364		[learning rate: 8.5284e-05]
		[batch 20/20] avg loss: 0.019719491112388726		[learning rate: 8.5129e-05]
	Learning Rate: 8.51293e-05
	LOSS [training: 0.017717475163873543 | validation: 0.016417765555791716]
	TIME [epoch: 8.24 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024060317319745828		[learning rate: 8.4975e-05]
		[batch 20/20] avg loss: 0.0186522987771716		[learning rate: 8.482e-05]
	Learning Rate: 8.48204e-05
	LOSS [training: 0.02135630804845871 | validation: 0.007683725200649391]
	TIME [epoch: 8.23 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014500855962603459		[learning rate: 8.4666e-05]
		[batch 20/20] avg loss: 0.01853558036129246		[learning rate: 8.4513e-05]
	Learning Rate: 8.45125e-05
	LOSS [training: 0.01651821816194796 | validation: 0.010913004732625665]
	TIME [epoch: 8.25 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01685665785859921		[learning rate: 8.4359e-05]
		[batch 20/20] avg loss: 0.015618145472841979		[learning rate: 8.4206e-05]
	Learning Rate: 8.42058e-05
	LOSS [training: 0.016237401665720595 | validation: 0.02523861428975963]
	TIME [epoch: 8.25 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028925403851557417		[learning rate: 8.4053e-05]
		[batch 20/20] avg loss: 0.024307183749767394		[learning rate: 8.39e-05]
	Learning Rate: 8.39002e-05
	LOSS [training: 0.026616293800662404 | validation: 0.01839696624244174]
	TIME [epoch: 8.24 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010571005728938451		[learning rate: 8.3748e-05]
		[batch 20/20] avg loss: 0.022754477517695014		[learning rate: 8.3596e-05]
	Learning Rate: 8.35958e-05
	LOSS [training: 0.016662741623316733 | validation: -0.0024654171930338495]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r0_20240219_233648/states/model_tr_study202_1416.pth
	Model improved!!!
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02117477356330032		[learning rate: 8.3444e-05]
		[batch 20/20] avg loss: 0.02038701352932406		[learning rate: 8.3292e-05]
	Learning Rate: 8.32924e-05
	LOSS [training: 0.02078089354631219 | validation: 0.007618548442894372]
	TIME [epoch: 8.26 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013405622877942211		[learning rate: 8.3141e-05]
		[batch 20/20] avg loss: 0.016855828897214375		[learning rate: 8.299e-05]
	Learning Rate: 8.29901e-05
	LOSS [training: 0.015130725887578295 | validation: 0.00836166756134551]
	TIME [epoch: 8.24 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02226909131696211		[learning rate: 8.2839e-05]
		[batch 20/20] avg loss: 0.01629176234087734		[learning rate: 8.2689e-05]
	Learning Rate: 8.26889e-05
	LOSS [training: 0.019280426828919728 | validation: 0.011649429540579456]
	TIME [epoch: 8.23 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019765461644463075		[learning rate: 8.2539e-05]
		[batch 20/20] avg loss: 0.01516196465210804		[learning rate: 8.2389e-05]
	Learning Rate: 8.23889e-05
	LOSS [training: 0.017463713148285556 | validation: 0.00994068824503834]
	TIME [epoch: 8.23 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017693010139281404		[learning rate: 8.2239e-05]
		[batch 20/20] avg loss: 0.02186990684254706		[learning rate: 8.209e-05]
	Learning Rate: 8.20899e-05
	LOSS [training: 0.019781458490914235 | validation: 0.0355450206698135]
	TIME [epoch: 8.25 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01768825574676536		[learning rate: 8.1941e-05]
		[batch 20/20] avg loss: 0.01809612346540666		[learning rate: 8.1792e-05]
	Learning Rate: 8.17919e-05
	LOSS [training: 0.01789218960608601 | validation: 0.019690587833064502]
	TIME [epoch: 8.24 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020270107997187856		[learning rate: 8.1643e-05]
		[batch 20/20] avg loss: 0.012686618290066972		[learning rate: 8.1495e-05]
	Learning Rate: 8.14951e-05
	LOSS [training: 0.016478363143627415 | validation: 0.01719935682573501]
	TIME [epoch: 8.24 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02481282742926872		[learning rate: 8.1347e-05]
		[batch 20/20] avg loss: 0.03389165034606505		[learning rate: 8.1199e-05]
	Learning Rate: 8.11994e-05
	LOSS [training: 0.029352238887666887 | validation: 0.008663137299357531]
	TIME [epoch: 8.24 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015472460490341589		[learning rate: 8.1052e-05]
		[batch 20/20] avg loss: 0.018593139251750455		[learning rate: 8.0905e-05]
	Learning Rate: 8.09047e-05
	LOSS [training: 0.01703279987104602 | validation: 0.012306820331909893]
	TIME [epoch: 8.24 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02056217154645474		[learning rate: 8.0758e-05]
		[batch 20/20] avg loss: 0.026755545681005168		[learning rate: 8.0611e-05]
	Learning Rate: 8.06111e-05
	LOSS [training: 0.023658858613729954 | validation: 0.00814179458740912]
	TIME [epoch: 8.26 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019124173057877496		[learning rate: 8.0465e-05]
		[batch 20/20] avg loss: 0.018332372876146696		[learning rate: 8.0319e-05]
	Learning Rate: 8.03186e-05
	LOSS [training: 0.018728272967012093 | validation: 0.014764477607892838]
	TIME [epoch: 8.24 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020034859602222718		[learning rate: 8.0173e-05]
		[batch 20/20] avg loss: 0.024387893667269657		[learning rate: 8.0027e-05]
	Learning Rate: 8.00271e-05
	LOSS [training: 0.022211376634746188 | validation: 0.016972417795446768]
	TIME [epoch: 8.23 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026565515154577234		[learning rate: 7.9882e-05]
		[batch 20/20] avg loss: 0.018070606072409342		[learning rate: 7.9737e-05]
	Learning Rate: 7.97367e-05
	LOSS [training: 0.02231806061349329 | validation: 0.010712792899322546]
	TIME [epoch: 8.23 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014965781177050946		[learning rate: 7.9592e-05]
		[batch 20/20] avg loss: 0.024504181395546134		[learning rate: 7.9447e-05]
	Learning Rate: 7.94473e-05
	LOSS [training: 0.019734981286298543 | validation: 0.011073674506477112]
	TIME [epoch: 8.25 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031939499358880144		[learning rate: 7.9303e-05]
		[batch 20/20] avg loss: 0.018437749812458926		[learning rate: 7.9159e-05]
	Learning Rate: 7.91589e-05
	LOSS [training: 0.025188624585669533 | validation: 0.022838840769142926]
	TIME [epoch: 8.24 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015668256975987685		[learning rate: 7.9015e-05]
		[batch 20/20] avg loss: 0.03547259967340613		[learning rate: 7.8872e-05]
	Learning Rate: 7.88717e-05
	LOSS [training: 0.025570428324696907 | validation: 0.013706989912143645]
	TIME [epoch: 8.23 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02881523811700277		[learning rate: 7.8728e-05]
		[batch 20/20] avg loss: 0.016172548633598148		[learning rate: 7.8585e-05]
	Learning Rate: 7.85854e-05
	LOSS [training: 0.02249389337530045 | validation: 0.010307549949312806]
	TIME [epoch: 8.23 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027267015349841883		[learning rate: 7.8443e-05]
		[batch 20/20] avg loss: 0.020879836818466554		[learning rate: 7.83e-05]
	Learning Rate: 7.83003e-05
	LOSS [training: 0.024073426084154217 | validation: 0.010281373328692306]
	TIME [epoch: 8.23 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021369253878723744		[learning rate: 7.8158e-05]
		[batch 20/20] avg loss: 0.023967754898812192		[learning rate: 7.8016e-05]
	Learning Rate: 7.80161e-05
	LOSS [training: 0.022668504388767968 | validation: 0.005365133290164512]
	TIME [epoch: 8.26 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02160542517207113		[learning rate: 7.7874e-05]
		[batch 20/20] avg loss: 0.020023368749988817		[learning rate: 7.7733e-05]
	Learning Rate: 7.7733e-05
	LOSS [training: 0.020814396961029973 | validation: 0.016204884772312918]
	TIME [epoch: 8.23 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02272512213896027		[learning rate: 7.7592e-05]
		[batch 20/20] avg loss: 0.025624841513627337		[learning rate: 7.7451e-05]
	Learning Rate: 7.74509e-05
	LOSS [training: 0.02417498182629381 | validation: 0.017715488950651156]
	TIME [epoch: 8.22 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021687448435883553		[learning rate: 7.731e-05]
		[batch 20/20] avg loss: 0.020778981630320874		[learning rate: 7.717e-05]
	Learning Rate: 7.71698e-05
	LOSS [training: 0.021233215033102215 | validation: 0.009523216128276332]
	TIME [epoch: 8.21 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025175313477613287		[learning rate: 7.703e-05]
		[batch 20/20] avg loss: 0.020092216848729298		[learning rate: 7.689e-05]
	Learning Rate: 7.68897e-05
	LOSS [training: 0.022633765163171282 | validation: 0.01634189977746906]
	TIME [epoch: 8.26 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017968616972695393		[learning rate: 7.675e-05]
		[batch 20/20] avg loss: 0.016088080001728768		[learning rate: 7.6611e-05]
	Learning Rate: 7.66107e-05
	LOSS [training: 0.017028348487212082 | validation: 0.01320019752040425]
	TIME [epoch: 8.23 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0343126248092839		[learning rate: 7.6472e-05]
		[batch 20/20] avg loss: 0.028919399129690392		[learning rate: 7.6333e-05]
	Learning Rate: 7.63327e-05
	LOSS [training: 0.03161601196948714 | validation: 0.008970098309970396]
	TIME [epoch: 8.23 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027219188042192772		[learning rate: 7.6194e-05]
		[batch 20/20] avg loss: 0.02011279434940429		[learning rate: 7.6056e-05]
	Learning Rate: 7.60557e-05
	LOSS [training: 0.023665991195798528 | validation: 0.017785150007935187]
	TIME [epoch: 8.22 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021634335022159744		[learning rate: 7.5918e-05]
		[batch 20/20] avg loss: 0.018102880621962796		[learning rate: 7.578e-05]
	Learning Rate: 7.57797e-05
	LOSS [training: 0.01986860782206127 | validation: 0.01354448371251229]
	TIME [epoch: 8.24 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016807065551515336		[learning rate: 7.5642e-05]
		[batch 20/20] avg loss: 0.013093066892083425		[learning rate: 7.5505e-05]
	Learning Rate: 7.55047e-05
	LOSS [training: 0.014950066221799382 | validation: 0.006188688737075028]
	TIME [epoch: 8.24 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01918691923658565		[learning rate: 7.5368e-05]
		[batch 20/20] avg loss: 0.027026023237089337		[learning rate: 7.5231e-05]
	Learning Rate: 7.52307e-05
	LOSS [training: 0.0231064712368375 | validation: 0.014756145469847671]
	TIME [epoch: 8.23 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03561359468201394		[learning rate: 7.5094e-05]
		[batch 20/20] avg loss: 0.02083066254333643		[learning rate: 7.4958e-05]
	Learning Rate: 7.49576e-05
	LOSS [training: 0.02822212861267518 | validation: 0.004752852310416875]
	TIME [epoch: 8.23 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02102053609938336		[learning rate: 7.4822e-05]
		[batch 20/20] avg loss: 0.024675641769372283		[learning rate: 7.4686e-05]
	Learning Rate: 7.46856e-05
	LOSS [training: 0.022848088934377814 | validation: 0.015823973640945982]
	TIME [epoch: 8.23 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020379167002135		[learning rate: 7.455e-05]
		[batch 20/20] avg loss: 0.020865784929857144		[learning rate: 7.4415e-05]
	Learning Rate: 7.44146e-05
	LOSS [training: 0.02062247596599607 | validation: 0.014778609927520572]
	TIME [epoch: 8.26 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019635946142303892		[learning rate: 7.4279e-05]
		[batch 20/20] avg loss: 0.01693965931877402		[learning rate: 7.4144e-05]
	Learning Rate: 7.41445e-05
	LOSS [training: 0.018287802730538955 | validation: 0.011310679597388667]
	TIME [epoch: 8.23 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02519816144399852		[learning rate: 7.401e-05]
		[batch 20/20] avg loss: 0.019967172886737612		[learning rate: 7.3875e-05]
	Learning Rate: 7.38754e-05
	LOSS [training: 0.02258266716536806 | validation: 0.02302694238895407]
	TIME [epoch: 8.22 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021716133417484906		[learning rate: 7.3741e-05]
		[batch 20/20] avg loss: 0.013617387594225958		[learning rate: 7.3607e-05]
	Learning Rate: 7.36073e-05
	LOSS [training: 0.017666760505855434 | validation: 0.010782640290819741]
	TIME [epoch: 8.24 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01619023602034287		[learning rate: 7.3474e-05]
		[batch 20/20] avg loss: 0.01414365703492557		[learning rate: 7.334e-05]
	Learning Rate: 7.33402e-05
	LOSS [training: 0.015166946527634223 | validation: 0.010789546196620608]
	TIME [epoch: 8.25 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01700832981773997		[learning rate: 7.3207e-05]
		[batch 20/20] avg loss: 0.02514733358572912		[learning rate: 7.3074e-05]
	Learning Rate: 7.30741e-05
	LOSS [training: 0.021077831701734544 | validation: 0.0026219739664266146]
	TIME [epoch: 8.25 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02108439089765916		[learning rate: 7.2941e-05]
		[batch 20/20] avg loss: 0.02191384282979857		[learning rate: 7.2809e-05]
	Learning Rate: 7.28089e-05
	LOSS [training: 0.02149911686372887 | validation: 0.010404422152388845]
	TIME [epoch: 8.24 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024495901322018503		[learning rate: 7.2677e-05]
		[batch 20/20] avg loss: 0.03248605007501605		[learning rate: 7.2545e-05]
	Learning Rate: 7.25447e-05
	LOSS [training: 0.028490975698517274 | validation: 0.013335535097303172]
	TIME [epoch: 8.24 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020508307973094532		[learning rate: 7.2413e-05]
		[batch 20/20] avg loss: 0.01689791661782376		[learning rate: 7.2281e-05]
	Learning Rate: 7.22814e-05
	LOSS [training: 0.018703112295459148 | validation: 0.02129484030628462]
	TIME [epoch: 8.23 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013793176123137648		[learning rate: 7.215e-05]
		[batch 20/20] avg loss: 0.01881174398257451		[learning rate: 7.2019e-05]
	Learning Rate: 7.2019e-05
	LOSS [training: 0.016302460052856082 | validation: 0.015984076941104968]
	TIME [epoch: 8.27 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019832854768381248		[learning rate: 7.1888e-05]
		[batch 20/20] avg loss: 0.02095228944276672		[learning rate: 7.1758e-05]
	Learning Rate: 7.17577e-05
	LOSS [training: 0.020392572105573988 | validation: 0.016193768281040657]
	TIME [epoch: 8.23 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01743936006431774		[learning rate: 7.1627e-05]
		[batch 20/20] avg loss: 0.01639037108384537		[learning rate: 7.1497e-05]
	Learning Rate: 7.14973e-05
	LOSS [training: 0.016914865574081554 | validation: 0.01158337091581851]
	TIME [epoch: 8.24 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018569170338807606		[learning rate: 7.1367e-05]
		[batch 20/20] avg loss: 0.023661627452030466		[learning rate: 7.1238e-05]
	Learning Rate: 7.12378e-05
	LOSS [training: 0.021115398895419035 | validation: 0.013409706915431334]
	TIME [epoch: 8.24 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020986653983274667		[learning rate: 7.1108e-05]
		[batch 20/20] avg loss: 0.017359079568149298		[learning rate: 7.0979e-05]
	Learning Rate: 7.09793e-05
	LOSS [training: 0.019172866775711984 | validation: 0.017630492250270375]
	TIME [epoch: 8.26 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013727563304629792		[learning rate: 7.085e-05]
		[batch 20/20] avg loss: 0.019680078368022776		[learning rate: 7.0722e-05]
	Learning Rate: 7.07217e-05
	LOSS [training: 0.01670382083632628 | validation: 0.01490376397820557]
	TIME [epoch: 8.23 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020074767607892462		[learning rate: 7.0593e-05]
		[batch 20/20] avg loss: 0.018974014108142857		[learning rate: 7.0465e-05]
	Learning Rate: 7.0465e-05
	LOSS [training: 0.019524390858017663 | validation: 0.008271607257948723]
	TIME [epoch: 8.23 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019768123943704596		[learning rate: 7.0337e-05]
		[batch 20/20] avg loss: 0.03207881132052311		[learning rate: 7.0209e-05]
	Learning Rate: 7.02093e-05
	LOSS [training: 0.025923467632113856 | validation: 0.014367553536069262]
	TIME [epoch: 8.23 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022948289438083184		[learning rate: 7.0082e-05]
		[batch 20/20] avg loss: 0.015373148066863176		[learning rate: 6.9955e-05]
	Learning Rate: 6.99545e-05
	LOSS [training: 0.01916071875247318 | validation: 0.011246317835454092]
	TIME [epoch: 8.24 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021933722859000675		[learning rate: 6.9827e-05]
		[batch 20/20] avg loss: 0.013265819769871276		[learning rate: 6.9701e-05]
	Learning Rate: 6.97006e-05
	LOSS [training: 0.01759977131443597 | validation: 0.015355301414422188]
	TIME [epoch: 8.25 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016935439744227787		[learning rate: 6.9574e-05]
		[batch 20/20] avg loss: 0.014988520193992263		[learning rate: 6.9448e-05]
	Learning Rate: 6.94477e-05
	LOSS [training: 0.015961979969110023 | validation: 0.0073458063010644285]
	TIME [epoch: 8.25 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0215320594222181		[learning rate: 6.9322e-05]
		[batch 20/20] avg loss: 0.01751063102654511		[learning rate: 6.9196e-05]
	Learning Rate: 6.91957e-05
	LOSS [training: 0.0195213452243816 | validation: 0.011804991402389092]
	TIME [epoch: 8.24 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021308561946166673		[learning rate: 6.907e-05]
		[batch 20/20] avg loss: 0.011824529657625807		[learning rate: 6.8945e-05]
	Learning Rate: 6.89446e-05
	LOSS [training: 0.01656654580189624 | validation: 0.013865901148174344]
	TIME [epoch: 8.24 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02085140723830358		[learning rate: 6.8819e-05]
		[batch 20/20] avg loss: 0.023974237129598754		[learning rate: 6.8694e-05]
	Learning Rate: 6.86944e-05
	LOSS [training: 0.022412822183951166 | validation: 0.010544263696646402]
	TIME [epoch: 8.26 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023440542578018933		[learning rate: 6.857e-05]
		[batch 20/20] avg loss: 0.01632193416243544		[learning rate: 6.8445e-05]
	Learning Rate: 6.84451e-05
	LOSS [training: 0.019881238370227185 | validation: 0.023411115604508037]
	TIME [epoch: 8.23 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01987326859231353		[learning rate: 6.8321e-05]
		[batch 20/20] avg loss: 0.022192329475922028		[learning rate: 6.8197e-05]
	Learning Rate: 6.81967e-05
	LOSS [training: 0.021032799034117776 | validation: 0.012839110920854824]
	TIME [epoch: 8.24 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015234353931763736		[learning rate: 6.8073e-05]
		[batch 20/20] avg loss: 0.028820977740248103		[learning rate: 6.7949e-05]
	Learning Rate: 6.79492e-05
	LOSS [training: 0.022027665836005923 | validation: 0.013737394508641214]
	TIME [epoch: 8.23 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016921164411599715		[learning rate: 6.7826e-05]
		[batch 20/20] avg loss: 0.017972669874523662		[learning rate: 6.7703e-05]
	Learning Rate: 6.77026e-05
	LOSS [training: 0.017446917143061687 | validation: 0.016897013451208576]
	TIME [epoch: 8.27 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019028990987091366		[learning rate: 6.758e-05]
		[batch 20/20] avg loss: 0.01692885799139431		[learning rate: 6.7457e-05]
	Learning Rate: 6.74569e-05
	LOSS [training: 0.017978924489242838 | validation: 0.009106625467366661]
	TIME [epoch: 8.24 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017839158983710284		[learning rate: 6.7334e-05]
		[batch 20/20] avg loss: 0.019692641222365463		[learning rate: 6.7212e-05]
	Learning Rate: 6.72121e-05
	LOSS [training: 0.018765900103037872 | validation: 0.023929124746604198]
	TIME [epoch: 8.23 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02651084388619667		[learning rate: 6.709e-05]
		[batch 20/20] avg loss: 0.020747671830083674		[learning rate: 6.6968e-05]
	Learning Rate: 6.69682e-05
	LOSS [training: 0.02362925785814017 | validation: 0.017418004820289427]
	TIME [epoch: 8.24 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02021799108383026		[learning rate: 6.6847e-05]
		[batch 20/20] avg loss: 0.017965832025810577		[learning rate: 6.6725e-05]
	Learning Rate: 6.67251e-05
	LOSS [training: 0.01909191155482042 | validation: 0.009318815136489415]
	TIME [epoch: 8.24 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0248469935689119		[learning rate: 6.6604e-05]
		[batch 20/20] avg loss: 0.012614561717388667		[learning rate: 6.6483e-05]
	Learning Rate: 6.6483e-05
	LOSS [training: 0.018730777643150284 | validation: 0.010693463436958385]
	TIME [epoch: 8.26 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01949170611687693		[learning rate: 6.6362e-05]
		[batch 20/20] avg loss: 0.01796294093514621		[learning rate: 6.6242e-05]
	Learning Rate: 6.62417e-05
	LOSS [training: 0.01872732352601157 | validation: 0.006597078157917776]
	TIME [epoch: 8.23 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011266695894508443		[learning rate: 6.6121e-05]
		[batch 20/20] avg loss: 0.017318083706244373		[learning rate: 6.6001e-05]
	Learning Rate: 6.60013e-05
	LOSS [training: 0.014292389800376407 | validation: 0.0062548375493052906]
	TIME [epoch: 8.24 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01574062437726422		[learning rate: 6.5881e-05]
		[batch 20/20] avg loss: 0.01659529672828702		[learning rate: 6.5762e-05]
	Learning Rate: 6.57618e-05
	LOSS [training: 0.016167960552775622 | validation: 0.008803996983763062]
	TIME [epoch: 8.23 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014119931271987653		[learning rate: 6.5642e-05]
		[batch 20/20] avg loss: 0.02582001150488506		[learning rate: 6.5523e-05]
	Learning Rate: 6.55232e-05
	LOSS [training: 0.019969971388436356 | validation: 0.02271129927897098]
	TIME [epoch: 8.26 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0172966395510036		[learning rate: 6.5404e-05]
		[batch 20/20] avg loss: 0.01920213807682452		[learning rate: 6.5285e-05]
	Learning Rate: 6.52854e-05
	LOSS [training: 0.018249388813914063 | validation: 0.007953826848934979]
	TIME [epoch: 8.24 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01765931618069541		[learning rate: 6.5167e-05]
		[batch 20/20] avg loss: 0.017019068101402735		[learning rate: 6.5048e-05]
	Learning Rate: 6.50484e-05
	LOSS [training: 0.017339192141049075 | validation: 0.005529070028770983]
	TIME [epoch: 8.24 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016265779449658596		[learning rate: 6.493e-05]
		[batch 20/20] avg loss: 0.01857922107935952		[learning rate: 6.4812e-05]
	Learning Rate: 6.48124e-05
	LOSS [training: 0.017422500264509058 | validation: 0.021261307934829724]
	TIME [epoch: 8.24 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02270005269238305		[learning rate: 6.4695e-05]
		[batch 20/20] avg loss: 0.018098137269703114		[learning rate: 6.4577e-05]
	Learning Rate: 6.45772e-05
	LOSS [training: 0.020399094981043083 | validation: 0.01687273192104701]
	TIME [epoch: 8.25 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01729807387767447		[learning rate: 6.446e-05]
		[batch 20/20] avg loss: 0.017028904952268158		[learning rate: 6.4343e-05]
	Learning Rate: 6.43428e-05
	LOSS [training: 0.017163489414971318 | validation: 0.013062181634697644]
	TIME [epoch: 8.25 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02529932322410906		[learning rate: 6.4226e-05]
		[batch 20/20] avg loss: 0.0221543078743094		[learning rate: 6.4109e-05]
	Learning Rate: 6.41093e-05
	LOSS [training: 0.02372681554920923 | validation: 0.014584509998579045]
	TIME [epoch: 8.23 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010928824041217878		[learning rate: 6.3993e-05]
		[batch 20/20] avg loss: 0.021711603981802993		[learning rate: 6.3877e-05]
	Learning Rate: 6.38767e-05
	LOSS [training: 0.01632021401151043 | validation: 0.007979171794536126]
	TIME [epoch: 8.23 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01666056979001558		[learning rate: 6.3761e-05]
		[batch 20/20] avg loss: 0.026781148951545587		[learning rate: 6.3645e-05]
	Learning Rate: 6.36449e-05
	LOSS [training: 0.021720859370780582 | validation: 0.016456055058487285]
	TIME [epoch: 8.24 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013162436818058875		[learning rate: 6.3529e-05]
		[batch 20/20] avg loss: 0.021539379831702937		[learning rate: 6.3414e-05]
	Learning Rate: 6.34139e-05
	LOSS [training: 0.017350908324880905 | validation: 0.01350320910028309]
	TIME [epoch: 8.25 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016666833747814205		[learning rate: 6.3299e-05]
		[batch 20/20] avg loss: 0.01961730911797026		[learning rate: 6.3184e-05]
	Learning Rate: 6.31837e-05
	LOSS [training: 0.01814207143289224 | validation: 0.014461409360869613]
	TIME [epoch: 8.23 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027210104983096273		[learning rate: 6.3069e-05]
		[batch 20/20] avg loss: 0.018350080477304357		[learning rate: 6.2954e-05]
	Learning Rate: 6.29544e-05
	LOSS [training: 0.022780092730200308 | validation: 0.009474081782061348]
	TIME [epoch: 8.23 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017756861768743394		[learning rate: 6.284e-05]
		[batch 20/20] avg loss: 0.015308000750263598		[learning rate: 6.2726e-05]
	Learning Rate: 6.2726e-05
	LOSS [training: 0.016532431259503492 | validation: 0.011300870972933114]
	TIME [epoch: 8.23 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017710062356426172		[learning rate: 6.2612e-05]
		[batch 20/20] avg loss: 0.015429424342238518		[learning rate: 6.2498e-05]
	Learning Rate: 6.24983e-05
	LOSS [training: 0.016569743349332342 | validation: 0.018305551076177665]
	TIME [epoch: 8.26 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01948401886513308		[learning rate: 6.2385e-05]
		[batch 20/20] avg loss: 0.017130267387283564		[learning rate: 6.2272e-05]
	Learning Rate: 6.22715e-05
	LOSS [training: 0.018307143126208326 | validation: 0.017368345736026712]
	TIME [epoch: 8.24 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022690986919504507		[learning rate: 6.2158e-05]
		[batch 20/20] avg loss: 0.01665303441278971		[learning rate: 6.2046e-05]
	Learning Rate: 6.20455e-05
	LOSS [training: 0.019672010666147106 | validation: 0.012716343857973019]
	TIME [epoch: 8.23 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019161811799788268		[learning rate: 6.1933e-05]
		[batch 20/20] avg loss: 0.016780385481062386		[learning rate: 6.182e-05]
	Learning Rate: 6.18204e-05
	LOSS [training: 0.01797109864042533 | validation: 0.014080983769924544]
	TIME [epoch: 8.23 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018991990149989684		[learning rate: 6.1708e-05]
		[batch 20/20] avg loss: 0.026873674515827894		[learning rate: 6.1596e-05]
	Learning Rate: 6.1596e-05
	LOSS [training: 0.02293283233290879 | validation: 0.009109700188635675]
	TIME [epoch: 8.24 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01905758082203473		[learning rate: 6.1484e-05]
		[batch 20/20] avg loss: 0.02216822744305633		[learning rate: 6.1372e-05]
	Learning Rate: 6.13725e-05
	LOSS [training: 0.020612904132545528 | validation: 0.007127532948375435]
	TIME [epoch: 8.25 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020990991468546473		[learning rate: 6.1261e-05]
		[batch 20/20] avg loss: 0.020988733785579642		[learning rate: 6.115e-05]
	Learning Rate: 6.11498e-05
	LOSS [training: 0.020989862627063056 | validation: 0.022248144517096103]
	TIME [epoch: 8.23 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029960872527588268		[learning rate: 6.1039e-05]
		[batch 20/20] avg loss: 0.016892950883051844		[learning rate: 6.0928e-05]
	Learning Rate: 6.09278e-05
	LOSS [training: 0.023426911705320054 | validation: 0.0080102804735434]
	TIME [epoch: 8.22 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01582842357604867		[learning rate: 6.0817e-05]
		[batch 20/20] avg loss: 0.017627946173631018		[learning rate: 6.0707e-05]
	Learning Rate: 6.07067e-05
	LOSS [training: 0.01672818487483984 | validation: 0.005448575825366804]
	TIME [epoch: 8.23 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023650991480293537		[learning rate: 6.0596e-05]
		[batch 20/20] avg loss: 0.014511220957581231		[learning rate: 6.0486e-05]
	Learning Rate: 6.04864e-05
	LOSS [training: 0.019081106218937386 | validation: 0.01053068153930604]
	TIME [epoch: 8.25 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021630896137912525		[learning rate: 6.0377e-05]
		[batch 20/20] avg loss: 0.014613636180891143		[learning rate: 6.0267e-05]
	Learning Rate: 6.02669e-05
	LOSS [training: 0.018122266159401833 | validation: 0.017388318527812186]
	TIME [epoch: 8.24 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017484459281973636		[learning rate: 6.0157e-05]
		[batch 20/20] avg loss: 0.01520822797713776		[learning rate: 6.0048e-05]
	Learning Rate: 6.00482e-05
	LOSS [training: 0.016346343629555702 | validation: 0.004244364101541167]
	TIME [epoch: 8.23 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011187969979952606		[learning rate: 5.9939e-05]
		[batch 20/20] avg loss: 0.019583108848520745		[learning rate: 5.983e-05]
	Learning Rate: 5.98303e-05
	LOSS [training: 0.015385539414236673 | validation: 0.011041000987657063]
	TIME [epoch: 8.24 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01284270695289427		[learning rate: 5.9722e-05]
		[batch 20/20] avg loss: 0.017420416582111055		[learning rate: 5.9613e-05]
	Learning Rate: 5.96132e-05
	LOSS [training: 0.01513156176750266 | validation: 0.012041912730409271]
	TIME [epoch: 8.25 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013487135159414548		[learning rate: 5.9505e-05]
		[batch 20/20] avg loss: 0.021172639855231084		[learning rate: 5.9397e-05]
	Learning Rate: 5.93968e-05
	LOSS [training: 0.017329887507322814 | validation: 0.006109450066648781]
	TIME [epoch: 8.25 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01611848736230107		[learning rate: 5.9289e-05]
		[batch 20/20] avg loss: 0.02266467262730506		[learning rate: 5.9181e-05]
	Learning Rate: 5.91813e-05
	LOSS [training: 0.019391579994803065 | validation: 0.013822452655116455]
	TIME [epoch: 8.23 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019615820914670144		[learning rate: 5.9074e-05]
		[batch 20/20] avg loss: 0.015231460493177825		[learning rate: 5.8966e-05]
	Learning Rate: 5.89665e-05
	LOSS [training: 0.01742364070392399 | validation: 0.005642425908311599]
	TIME [epoch: 8.23 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021054059659838992		[learning rate: 5.8859e-05]
		[batch 20/20] avg loss: 0.014363288287019566		[learning rate: 5.8752e-05]
	Learning Rate: 5.87525e-05
	LOSS [training: 0.01770867397342928 | validation: 0.011999071780136667]
	TIME [epoch: 8.24 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014827252225075211		[learning rate: 5.8646e-05]
		[batch 20/20] avg loss: 0.0197942328253694		[learning rate: 5.8539e-05]
	Learning Rate: 5.85393e-05
	LOSS [training: 0.017310742525222307 | validation: 0.009854970098062854]
	TIME [epoch: 8.25 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008131396781025849		[learning rate: 5.8433e-05]
		[batch 20/20] avg loss: 0.01803449405049708		[learning rate: 5.8327e-05]
	Learning Rate: 5.83268e-05
	LOSS [training: 0.013082945415761466 | validation: 0.003981987862614524]
	TIME [epoch: 8.23 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010184609416443498		[learning rate: 5.8221e-05]
		[batch 20/20] avg loss: 0.019954111631476718		[learning rate: 5.8115e-05]
	Learning Rate: 5.81152e-05
	LOSS [training: 0.01506936052396011 | validation: 0.01294744377357904]
	TIME [epoch: 8.23 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013986737104209057		[learning rate: 5.801e-05]
		[batch 20/20] avg loss: 0.02161799601394276		[learning rate: 5.7904e-05]
	Learning Rate: 5.79043e-05
	LOSS [training: 0.017802366559075904 | validation: 0.012991852761491608]
	TIME [epoch: 8.24 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01905168637681624		[learning rate: 5.7799e-05]
		[batch 20/20] avg loss: 0.04288534464164012		[learning rate: 5.7694e-05]
	Learning Rate: 5.76941e-05
	LOSS [training: 0.030968515509228182 | validation: 0.01344863341640244]
	TIME [epoch: 8.26 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023457980528524863		[learning rate: 5.7589e-05]
		[batch 20/20] avg loss: 0.016436467555562037		[learning rate: 5.7485e-05]
	Learning Rate: 5.74847e-05
	LOSS [training: 0.019947224042043448 | validation: 0.007691860277490451]
	TIME [epoch: 8.24 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024287259216676987		[learning rate: 5.738e-05]
		[batch 20/20] avg loss: 0.019241160976048874		[learning rate: 5.7276e-05]
	Learning Rate: 5.72761e-05
	LOSS [training: 0.02176421009636293 | validation: 0.011584811643389214]
	TIME [epoch: 8.23 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011898059853853763		[learning rate: 5.7172e-05]
		[batch 20/20] avg loss: 0.02519330948713026		[learning rate: 5.7068e-05]
	Learning Rate: 5.70683e-05
	LOSS [training: 0.018545684670492005 | validation: 0.010490311166910085]
	TIME [epoch: 8.23 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018111298719181778		[learning rate: 5.6965e-05]
		[batch 20/20] avg loss: 0.016895658737182472		[learning rate: 5.6861e-05]
	Learning Rate: 5.68612e-05
	LOSS [training: 0.017503478728182125 | validation: 0.014392681761525788]
	TIME [epoch: 8.26 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015906549787285205		[learning rate: 5.6758e-05]
		[batch 20/20] avg loss: 0.01774148916385467		[learning rate: 5.6655e-05]
	Learning Rate: 5.66548e-05
	LOSS [training: 0.016824019475569933 | validation: 0.013317059890505405]
	TIME [epoch: 8.24 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01634684550116019		[learning rate: 5.6552e-05]
		[batch 20/20] avg loss: 0.015156710762308696		[learning rate: 5.6449e-05]
	Learning Rate: 5.64492e-05
	LOSS [training: 0.015751778131734437 | validation: 0.010998775365559606]
	TIME [epoch: 8.24 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017463345240576546		[learning rate: 5.6347e-05]
		[batch 20/20] avg loss: 0.020111274465108332		[learning rate: 5.6244e-05]
	Learning Rate: 5.62444e-05
	LOSS [training: 0.01878730985284244 | validation: 0.014906147940352795]
	TIME [epoch: 8.24 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01640368872769348		[learning rate: 5.6142e-05]
		[batch 20/20] avg loss: 0.017304711924785186		[learning rate: 5.604e-05]
	Learning Rate: 5.60403e-05
	LOSS [training: 0.01685420032623933 | validation: 0.013907456423252933]
	TIME [epoch: 8.24 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016450558905805945		[learning rate: 5.5938e-05]
		[batch 20/20] avg loss: 0.017864633564669143		[learning rate: 5.5837e-05]
	Learning Rate: 5.58369e-05
	LOSS [training: 0.017157596235237542 | validation: 0.021531445878642707]
	TIME [epoch: 8.26 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015829158894110667		[learning rate: 5.5735e-05]
		[batch 20/20] avg loss: 0.013596831430036625		[learning rate: 5.5634e-05]
	Learning Rate: 5.56342e-05
	LOSS [training: 0.014712995162073647 | validation: 0.00743642834297755]
	TIME [epoch: 8.23 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012895099076458142		[learning rate: 5.5533e-05]
		[batch 20/20] avg loss: 0.016965001953801843		[learning rate: 5.5432e-05]
	Learning Rate: 5.54323e-05
	LOSS [training: 0.014930050515129994 | validation: 0.006581644221462429]
	TIME [epoch: 8.24 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021416748235398405		[learning rate: 5.5332e-05]
		[batch 20/20] avg loss: 0.019270245982986683		[learning rate: 5.5231e-05]
	Learning Rate: 5.52312e-05
	LOSS [training: 0.020343497109192542 | validation: 0.012501532899090688]
	TIME [epoch: 8.23 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027332903741721704		[learning rate: 5.5131e-05]
		[batch 20/20] avg loss: 0.02903720135244172		[learning rate: 5.5031e-05]
	Learning Rate: 5.50307e-05
	LOSS [training: 0.028185052547081712 | validation: 0.012126201317607423]
	TIME [epoch: 8.26 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013566999299223026		[learning rate: 5.4931e-05]
		[batch 20/20] avg loss: 0.02475857240554602		[learning rate: 5.4831e-05]
	Learning Rate: 5.4831e-05
	LOSS [training: 0.01916278585238452 | validation: 0.010438729365942433]
	TIME [epoch: 8.24 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012513476642515755		[learning rate: 5.4731e-05]
		[batch 20/20] avg loss: 0.027305479665598036		[learning rate: 5.4632e-05]
	Learning Rate: 5.4632e-05
	LOSS [training: 0.019909478154056894 | validation: 0.017459341113273952]
	TIME [epoch: 8.24 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016447288692797372		[learning rate: 5.4533e-05]
		[batch 20/20] avg loss: 0.019895514554031435		[learning rate: 5.4434e-05]
	Learning Rate: 5.44338e-05
	LOSS [training: 0.0181714016234144 | validation: 0.010257053296814105]
	TIME [epoch: 8.23 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012054700113653307		[learning rate: 5.4335e-05]
		[batch 20/20] avg loss: 0.013765711310130125		[learning rate: 5.4236e-05]
	Learning Rate: 5.42362e-05
	LOSS [training: 0.012910205711891715 | validation: 0.011665566835058573]
	TIME [epoch: 8.24 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015805084576739174		[learning rate: 5.4138e-05]
		[batch 20/20] avg loss: 0.022794533422507514		[learning rate: 5.4039e-05]
	Learning Rate: 5.40394e-05
	LOSS [training: 0.019299808999623344 | validation: 0.009351759644403487]
	TIME [epoch: 8.26 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01896420341814039		[learning rate: 5.3941e-05]
		[batch 20/20] avg loss: 0.016462759384298822		[learning rate: 5.3843e-05]
	Learning Rate: 5.38433e-05
	LOSS [training: 0.0177134814012196 | validation: 0.01172505591157895]
	TIME [epoch: 8.24 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014433486735739872		[learning rate: 5.3746e-05]
		[batch 20/20] avg loss: 0.015530122811902077		[learning rate: 5.3648e-05]
	Learning Rate: 5.36479e-05
	LOSS [training: 0.014981804773820972 | validation: 0.009406044146382858]
	TIME [epoch: 8.24 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015018863804195667		[learning rate: 5.355e-05]
		[batch 20/20] avg loss: 0.01714623366274375		[learning rate: 5.3453e-05]
	Learning Rate: 5.34532e-05
	LOSS [training: 0.01608254873346971 | validation: 0.008139658617624893]
	TIME [epoch: 8.24 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02155400667798472		[learning rate: 5.3356e-05]
		[batch 20/20] avg loss: 0.015065750791751122		[learning rate: 5.3259e-05]
	Learning Rate: 5.32592e-05
	LOSS [training: 0.018309878734867925 | validation: 0.006248511253116085]
	TIME [epoch: 8.26 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02052691852184286		[learning rate: 5.3162e-05]
		[batch 20/20] avg loss: 0.011449476133614103		[learning rate: 5.3066e-05]
	Learning Rate: 5.30659e-05
	LOSS [training: 0.01598819732772848 | validation: 0.014534617023108296]
	TIME [epoch: 8.24 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0149014153395262		[learning rate: 5.297e-05]
		[batch 20/20] avg loss: 0.020940684771798272		[learning rate: 5.2873e-05]
	Learning Rate: 5.28734e-05
	LOSS [training: 0.017921050055662236 | validation: 0.013648768501800112]
	TIME [epoch: 8.23 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018285537333818914		[learning rate: 5.2777e-05]
		[batch 20/20] avg loss: 0.016548151326276832		[learning rate: 5.2681e-05]
	Learning Rate: 5.26815e-05
	LOSS [training: 0.017416844330047875 | validation: 0.019225186998285203]
	TIME [epoch: 8.24 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020327539463033967		[learning rate: 5.2586e-05]
		[batch 20/20] avg loss: 0.013694380635615625		[learning rate: 5.249e-05]
	Learning Rate: 5.24903e-05
	LOSS [training: 0.017010960049324793 | validation: 0.00011457692067654533]
	TIME [epoch: 8.26 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017933142966113924		[learning rate: 5.2395e-05]
		[batch 20/20] avg loss: 0.013620985690966422		[learning rate: 5.23e-05]
	Learning Rate: 5.22998e-05
	LOSS [training: 0.015777064328540168 | validation: 0.017321553615789197]
	TIME [epoch: 8.24 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013920264385853653		[learning rate: 5.2205e-05]
		[batch 20/20] avg loss: 0.023659868817237622		[learning rate: 5.211e-05]
	Learning Rate: 5.211e-05
	LOSS [training: 0.01879006660154564 | validation: 0.013669208779068335]
	TIME [epoch: 8.24 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018269824643744794		[learning rate: 5.2015e-05]
		[batch 20/20] avg loss: 0.014982088458170636		[learning rate: 5.1921e-05]
	Learning Rate: 5.19209e-05
	LOSS [training: 0.016625956550957718 | validation: 0.009986647364131495]
	TIME [epoch: 8.23 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014355274268134311		[learning rate: 5.1827e-05]
		[batch 20/20] avg loss: 0.022805127869596608		[learning rate: 5.1732e-05]
	Learning Rate: 5.17325e-05
	LOSS [training: 0.018580201068865456 | validation: 0.030170922568490556]
	TIME [epoch: 8.24 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02184111643756397		[learning rate: 5.1639e-05]
		[batch 20/20] avg loss: 0.01372148601961091		[learning rate: 5.1545e-05]
	Learning Rate: 5.15447e-05
	LOSS [training: 0.01778130122858744 | validation: 0.01354438972965605]
	TIME [epoch: 8.26 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02146230144389802		[learning rate: 5.1451e-05]
		[batch 20/20] avg loss: 0.0121838816755435		[learning rate: 5.1358e-05]
	Learning Rate: 5.13577e-05
	LOSS [training: 0.016823091559720758 | validation: 0.011747400993175894]
	TIME [epoch: 8.23 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01796599431423212		[learning rate: 5.1264e-05]
		[batch 20/20] avg loss: 0.014651155573480285		[learning rate: 5.1171e-05]
	Learning Rate: 5.11713e-05
	LOSS [training: 0.016308574943856197 | validation: 0.010783782922042753]
	TIME [epoch: 8.23 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011240559109787052		[learning rate: 5.1078e-05]
		[batch 20/20] avg loss: 0.024839096829172687		[learning rate: 5.0986e-05]
	Learning Rate: 5.09856e-05
	LOSS [training: 0.018039827969479873 | validation: 0.01157923779432984]
	TIME [epoch: 8.23 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013899675254566451		[learning rate: 5.0893e-05]
		[batch 20/20] avg loss: 0.01488165684849295		[learning rate: 5.0801e-05]
	Learning Rate: 5.08006e-05
	LOSS [training: 0.014390666051529699 | validation: 0.008073055187267395]
	TIME [epoch: 8.26 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011527162482799915		[learning rate: 5.0708e-05]
		[batch 20/20] avg loss: 0.019658642477966993		[learning rate: 5.0616e-05]
	Learning Rate: 5.06162e-05
	LOSS [training: 0.015592902480383457 | validation: 0.012700700564357528]
	TIME [epoch: 8.24 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021129794918403097		[learning rate: 5.0524e-05]
		[batch 20/20] avg loss: 0.024512487546267205		[learning rate: 5.0433e-05]
	Learning Rate: 5.04325e-05
	LOSS [training: 0.022821141232335154 | validation: 0.0088482424721719]
	TIME [epoch: 8.24 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01934505174887328		[learning rate: 5.0341e-05]
		[batch 20/20] avg loss: 0.021785026234596674		[learning rate: 5.0249e-05]
	Learning Rate: 5.02495e-05
	LOSS [training: 0.02056503899173498 | validation: 0.01960618060702065]
	TIME [epoch: 8.24 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01817415248397648		[learning rate: 5.0158e-05]
		[batch 20/20] avg loss: 0.016257798862205207		[learning rate: 5.0067e-05]
	Learning Rate: 5.00671e-05
	LOSS [training: 0.017215975673090845 | validation: 0.016608417285244325]
	TIME [epoch: 8.24 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020525039417331824		[learning rate: 4.9976e-05]
		[batch 20/20] avg loss: 0.013718008438962842		[learning rate: 4.9885e-05]
	Learning Rate: 4.98854e-05
	LOSS [training: 0.01712152392814734 | validation: 0.008222638575104897]
	TIME [epoch: 8.26 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018289837533614274		[learning rate: 4.9795e-05]
		[batch 20/20] avg loss: 0.015366911100901206		[learning rate: 4.9704e-05]
	Learning Rate: 4.97044e-05
	LOSS [training: 0.01682837431725774 | validation: 0.009102129931840673]
	TIME [epoch: 8.24 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015661335870775582		[learning rate: 4.9614e-05]
		[batch 20/20] avg loss: 0.02687800477360306		[learning rate: 4.9524e-05]
	Learning Rate: 4.9524e-05
	LOSS [training: 0.02126967032218932 | validation: 0.012145560348424799]
	TIME [epoch: 8.24 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020120318107823464		[learning rate: 4.9434e-05]
		[batch 20/20] avg loss: 0.0189494473691681		[learning rate: 4.9344e-05]
	Learning Rate: 4.93443e-05
	LOSS [training: 0.01953488273849578 | validation: 0.02424293094035334]
	TIME [epoch: 8.23 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01650670803125376		[learning rate: 4.9255e-05]
		[batch 20/20] avg loss: 0.01961538822594492		[learning rate: 4.9165e-05]
	Learning Rate: 4.91652e-05
	LOSS [training: 0.018061048128599336 | validation: 0.010432096836606387]
	TIME [epoch: 8.26 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012058775986643278		[learning rate: 4.9076e-05]
		[batch 20/20] avg loss: 0.019231295687000292		[learning rate: 4.8987e-05]
	Learning Rate: 4.89868e-05
	LOSS [training: 0.015645035836821787 | validation: 0.01522099637305603]
	TIME [epoch: 8.24 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01738571403808356		[learning rate: 4.8898e-05]
		[batch 20/20] avg loss: 0.013712514016750795		[learning rate: 4.8809e-05]
	Learning Rate: 4.8809e-05
	LOSS [training: 0.015549114027417179 | validation: 0.0123884206985885]
	TIME [epoch: 8.24 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016045630271274855		[learning rate: 4.872e-05]
		[batch 20/20] avg loss: 0.013268292201906309		[learning rate: 4.8632e-05]
	Learning Rate: 4.86319e-05
	LOSS [training: 0.014656961236590587 | validation: 0.007103848830962232]
	TIME [epoch: 8.23 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01178191758636643		[learning rate: 4.8544e-05]
		[batch 20/20] avg loss: 0.020802970053426496		[learning rate: 4.8455e-05]
	Learning Rate: 4.84554e-05
	LOSS [training: 0.016292443819896463 | validation: 0.005262384701216886]
	TIME [epoch: 8.25 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014673387778525773		[learning rate: 4.8367e-05]
		[batch 20/20] avg loss: 0.016209619135136653		[learning rate: 4.828e-05]
	Learning Rate: 4.82795e-05
	LOSS [training: 0.01544150345683121 | validation: 0.015687461979945555]
	TIME [epoch: 8.24 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0223547223336876		[learning rate: 4.8192e-05]
		[batch 20/20] avg loss: 0.018782577070970956		[learning rate: 4.8104e-05]
	Learning Rate: 4.81043e-05
	LOSS [training: 0.020568649702329272 | validation: 0.008812982191904139]
	TIME [epoch: 8.23 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020708944763805907		[learning rate: 4.8017e-05]
		[batch 20/20] avg loss: 0.028741367901388676		[learning rate: 4.793e-05]
	Learning Rate: 4.79298e-05
	LOSS [training: 0.024725156332597288 | validation: 0.013230720378458788]
	TIME [epoch: 8.23 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020095144986259774		[learning rate: 4.7843e-05]
		[batch 20/20] avg loss: 0.015875678983374546		[learning rate: 4.7756e-05]
	Learning Rate: 4.77558e-05
	LOSS [training: 0.017985411984817158 | validation: 0.006593624251176299]
	TIME [epoch: 8.23 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014158007271365989		[learning rate: 4.7669e-05]
		[batch 20/20] avg loss: 0.02103400491444789		[learning rate: 4.7583e-05]
	Learning Rate: 4.75825e-05
	LOSS [training: 0.017596006092906936 | validation: 0.004630910738908384]
	TIME [epoch: 8.26 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01327800818003878		[learning rate: 4.7496e-05]
		[batch 20/20] avg loss: 0.012947271447347617		[learning rate: 4.741e-05]
	Learning Rate: 4.74098e-05
	LOSS [training: 0.0131126398136932 | validation: 0.009616315363041734]
	TIME [epoch: 8.23 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010236323563561498		[learning rate: 4.7324e-05]
		[batch 20/20] avg loss: 0.016149178368362736		[learning rate: 4.7238e-05]
	Learning Rate: 4.72378e-05
	LOSS [training: 0.013192750965962119 | validation: 0.010659828566851416]
	TIME [epoch: 8.23 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02388845552859239		[learning rate: 4.7152e-05]
		[batch 20/20] avg loss: 0.013779365391442552		[learning rate: 4.7066e-05]
	Learning Rate: 4.70664e-05
	LOSS [training: 0.01883391046001747 | validation: 0.015061475931286128]
	TIME [epoch: 8.23 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016882698390743557		[learning rate: 4.6981e-05]
		[batch 20/20] avg loss: 0.011911624944551452		[learning rate: 4.6896e-05]
	Learning Rate: 4.68955e-05
	LOSS [training: 0.014397161667647507 | validation: 0.007130606123696021]
	TIME [epoch: 8.26 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020172822719165522		[learning rate: 4.681e-05]
		[batch 20/20] avg loss: 0.015940023731415252		[learning rate: 4.6725e-05]
	Learning Rate: 4.67254e-05
	LOSS [training: 0.01805642322529039 | validation: 0.015836116131858414]
	TIME [epoch: 8.24 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01702266832855122		[learning rate: 4.6641e-05]
		[batch 20/20] avg loss: 0.014806007971973578		[learning rate: 4.6556e-05]
	Learning Rate: 4.65558e-05
	LOSS [training: 0.0159143381502624 | validation: 0.016784507685819385]
	TIME [epoch: 8.24 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021700447759722873		[learning rate: 4.6471e-05]
		[batch 20/20] avg loss: 0.015280256621071728		[learning rate: 4.6387e-05]
	Learning Rate: 4.63868e-05
	LOSS [training: 0.0184903521903973 | validation: 0.003814495289262906]
	TIME [epoch: 8.24 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010736644717482181		[learning rate: 4.6303e-05]
		[batch 20/20] avg loss: 0.016504998076258184		[learning rate: 4.6219e-05]
	Learning Rate: 4.62185e-05
	LOSS [training: 0.013620821396870184 | validation: 0.008546108126268988]
	TIME [epoch: 8.25 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01608098010207161		[learning rate: 4.6135e-05]
		[batch 20/20] avg loss: 0.019142899141882067		[learning rate: 4.6051e-05]
	Learning Rate: 4.60508e-05
	LOSS [training: 0.017611939621976835 | validation: 0.012750188406836439]
	TIME [epoch: 8.25 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013846685380907841		[learning rate: 4.5967e-05]
		[batch 20/20] avg loss: 0.018603098907245035		[learning rate: 4.5884e-05]
	Learning Rate: 4.58836e-05
	LOSS [training: 0.016224892144076436 | validation: 0.011503534987141614]
	TIME [epoch: 8.24 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01759642443618459		[learning rate: 4.58e-05]
		[batch 20/20] avg loss: 0.01658988372555582		[learning rate: 4.5717e-05]
	Learning Rate: 4.57171e-05
	LOSS [training: 0.017093154080870206 | validation: 0.009547673757922565]
	TIME [epoch: 8.24 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012096255531725334		[learning rate: 4.5634e-05]
		[batch 20/20] avg loss: 0.013830751828818027		[learning rate: 4.5551e-05]
	Learning Rate: 4.55512e-05
	LOSS [training: 0.012963503680271679 | validation: 0.009962896927420329]
	TIME [epoch: 8.24 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023188912205017627		[learning rate: 4.5468e-05]
		[batch 20/20] avg loss: 0.015974446986403733		[learning rate: 4.5386e-05]
	Learning Rate: 4.53859e-05
	LOSS [training: 0.01958167959571068 | validation: 0.010823328863139596]
	TIME [epoch: 8.26 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0160207543201507		[learning rate: 4.5303e-05]
		[batch 20/20] avg loss: 0.01354899579377854		[learning rate: 4.5221e-05]
	Learning Rate: 4.52212e-05
	LOSS [training: 0.01478487505696462 | validation: 0.005187235130434511]
	TIME [epoch: 8.23 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014365321802183126		[learning rate: 4.5139e-05]
		[batch 20/20] avg loss: 0.019154555243663815		[learning rate: 4.5057e-05]
	Learning Rate: 4.50571e-05
	LOSS [training: 0.01675993852292347 | validation: 0.007277061024065109]
	TIME [epoch: 8.23 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020636571920637052		[learning rate: 4.4975e-05]
		[batch 20/20] avg loss: 0.018710044776240473		[learning rate: 4.4894e-05]
	Learning Rate: 4.48936e-05
	LOSS [training: 0.019673308348438764 | validation: 0.011358975753673868]
	TIME [epoch: 8.23 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017905001389931376		[learning rate: 4.4812e-05]
		[batch 20/20] avg loss: 0.014593713408284154		[learning rate: 4.4731e-05]
	Learning Rate: 4.47307e-05
	LOSS [training: 0.01624935739910776 | validation: 0.01370540349897771]
	TIME [epoch: 8.25 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009923674847500747		[learning rate: 4.4649e-05]
		[batch 20/20] avg loss: 0.01969802475163904		[learning rate: 4.4568e-05]
	Learning Rate: 4.45683e-05
	LOSS [training: 0.014810849799569894 | validation: 0.008656563619290342]
	TIME [epoch: 8.25 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011836756027926838		[learning rate: 4.4487e-05]
		[batch 20/20] avg loss: 0.020311804195879614		[learning rate: 4.4407e-05]
	Learning Rate: 4.44066e-05
	LOSS [training: 0.016074280111903226 | validation: 0.008478075386664868]
	TIME [epoch: 8.23 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014192250829221875		[learning rate: 4.4326e-05]
		[batch 20/20] avg loss: 0.01911589829637571		[learning rate: 4.4245e-05]
	Learning Rate: 4.42454e-05
	LOSS [training: 0.01665407456279879 | validation: 0.007398343574891036]
	TIME [epoch: 8.23 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014018336402398687		[learning rate: 4.4165e-05]
		[batch 20/20] avg loss: 0.024072133316500696		[learning rate: 4.4085e-05]
	Learning Rate: 4.40849e-05
	LOSS [training: 0.01904523485944969 | validation: 0.00940263402868399]
	TIME [epoch: 8.23 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014498761294019405		[learning rate: 4.4005e-05]
		[batch 20/20] avg loss: 0.020898913145263282		[learning rate: 4.3925e-05]
	Learning Rate: 4.39249e-05
	LOSS [training: 0.017698837219641345 | validation: 0.01226114248658756]
	TIME [epoch: 8.26 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016870420284384225		[learning rate: 4.3845e-05]
		[batch 20/20] avg loss: 0.01241997808046124		[learning rate: 4.3765e-05]
	Learning Rate: 4.37655e-05
	LOSS [training: 0.014645199182422733 | validation: 0.01245558427647207]
	TIME [epoch: 8.24 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013394980554442576		[learning rate: 4.3686e-05]
		[batch 20/20] avg loss: 0.011399976212491104		[learning rate: 4.3607e-05]
	Learning Rate: 4.36067e-05
	LOSS [training: 0.012397478383466842 | validation: 0.011434261635055145]
	TIME [epoch: 8.23 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01399274814206212		[learning rate: 4.3527e-05]
		[batch 20/20] avg loss: 0.017123717036877557		[learning rate: 4.3448e-05]
	Learning Rate: 4.34484e-05
	LOSS [training: 0.01555823258946984 | validation: 0.006311529497853887]
	TIME [epoch: 8.23 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010378171997300166		[learning rate: 4.3369e-05]
		[batch 20/20] avg loss: 0.017133292673443913		[learning rate: 4.3291e-05]
	Learning Rate: 4.32907e-05
	LOSS [training: 0.013755732335372039 | validation: 0.010480023588313622]
	TIME [epoch: 8.26 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012378936128315605		[learning rate: 4.3212e-05]
		[batch 20/20] avg loss: 0.015714706258458513		[learning rate: 4.3134e-05]
	Learning Rate: 4.31336e-05
	LOSS [training: 0.014046821193387063 | validation: 0.01637561846492428]
	TIME [epoch: 8.24 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010269835555113427		[learning rate: 4.3055e-05]
		[batch 20/20] avg loss: 0.022875355491610357		[learning rate: 4.2977e-05]
	Learning Rate: 4.29771e-05
	LOSS [training: 0.01657259552336189 | validation: 0.009469040277214093]
	TIME [epoch: 8.23 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01915140290289754		[learning rate: 4.2899e-05]
		[batch 20/20] avg loss: 0.017079789374901018		[learning rate: 4.2821e-05]
	Learning Rate: 4.28211e-05
	LOSS [training: 0.01811559613889928 | validation: 0.012035889516894344]
	TIME [epoch: 8.24 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016227959519053793		[learning rate: 4.2743e-05]
		[batch 20/20] avg loss: 0.019537659926956573		[learning rate: 4.2666e-05]
	Learning Rate: 4.26657e-05
	LOSS [training: 0.017882809723005183 | validation: 0.013360394810876993]
	TIME [epoch: 8.25 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02122613098238537		[learning rate: 4.2588e-05]
		[batch 20/20] avg loss: 0.0184102668138598		[learning rate: 4.2511e-05]
	Learning Rate: 4.25109e-05
	LOSS [training: 0.01981819889812259 | validation: 0.010670005608429092]
	TIME [epoch: 8.25 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016465258528937748		[learning rate: 4.2434e-05]
		[batch 20/20] avg loss: 0.01510634897771057		[learning rate: 4.2357e-05]
	Learning Rate: 4.23566e-05
	LOSS [training: 0.015785803753324158 | validation: 0.012913046254315417]
	TIME [epoch: 8.23 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020606894319431045		[learning rate: 4.228e-05]
		[batch 20/20] avg loss: 0.019888860329108977		[learning rate: 4.2203e-05]
	Learning Rate: 4.22029e-05
	LOSS [training: 0.020247877324270008 | validation: 0.0137449589539689]
	TIME [epoch: 8.24 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023079568169112684		[learning rate: 4.2126e-05]
		[batch 20/20] avg loss: 0.01428498086637121		[learning rate: 4.205e-05]
	Learning Rate: 4.20497e-05
	LOSS [training: 0.01868227451774195 | validation: 0.006719469614268234]
	TIME [epoch: 8.23 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015158791891855632		[learning rate: 4.1973e-05]
		[batch 20/20] avg loss: 0.016316410429454326		[learning rate: 4.1897e-05]
	Learning Rate: 4.18971e-05
	LOSS [training: 0.015737601160654976 | validation: 0.01769223475094186]
	TIME [epoch: 8.26 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019681679112944307		[learning rate: 4.1821e-05]
		[batch 20/20] avg loss: 0.026069427307979936		[learning rate: 4.1745e-05]
	Learning Rate: 4.17451e-05
	LOSS [training: 0.022875553210462127 | validation: 0.022385798598152783]
	TIME [epoch: 8.23 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020070886569611614		[learning rate: 4.1669e-05]
		[batch 20/20] avg loss: 0.021336716682087562		[learning rate: 4.1594e-05]
	Learning Rate: 4.15936e-05
	LOSS [training: 0.020703801625849585 | validation: 0.00946424360386065]
	TIME [epoch: 8.24 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01224651422094041		[learning rate: 4.1518e-05]
		[batch 20/20] avg loss: 0.017651488782318054		[learning rate: 4.1443e-05]
	Learning Rate: 4.14426e-05
	LOSS [training: 0.014949001501629231 | validation: 0.010267141802056276]
	TIME [epoch: 8.23 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019353044190363895		[learning rate: 4.1367e-05]
		[batch 20/20] avg loss: 0.014041576416881762		[learning rate: 4.1292e-05]
	Learning Rate: 4.12922e-05
	LOSS [training: 0.016697310303622825 | validation: 0.013189394491823667]
	TIME [epoch: 8.25 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01842174689598467		[learning rate: 4.1217e-05]
		[batch 20/20] avg loss: 0.013135689549859025		[learning rate: 4.1142e-05]
	Learning Rate: 4.11424e-05
	LOSS [training: 0.015778718222921848 | validation: 0.011286203259248413]
	TIME [epoch: 8.24 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017896365000407144		[learning rate: 4.1068e-05]
		[batch 20/20] avg loss: 0.013222204026743942		[learning rate: 4.0993e-05]
	Learning Rate: 4.09931e-05
	LOSS [training: 0.015559284513575541 | validation: 0.007744046185375403]
	TIME [epoch: 8.23 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014882707257630306		[learning rate: 4.0919e-05]
		[batch 20/20] avg loss: 0.016848317749824496		[learning rate: 4.0844e-05]
	Learning Rate: 4.08443e-05
	LOSS [training: 0.0158655125037274 | validation: 0.017669460708365996]
	TIME [epoch: 8.23 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024196865309864396		[learning rate: 4.077e-05]
		[batch 20/20] avg loss: 0.018997639966394368		[learning rate: 4.0696e-05]
	Learning Rate: 4.06961e-05
	LOSS [training: 0.021597252638129383 | validation: 0.01284832243649781]
	TIME [epoch: 8.23 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009986485224478028		[learning rate: 4.0622e-05]
		[batch 20/20] avg loss: 0.011654298169612953		[learning rate: 4.0548e-05]
	Learning Rate: 4.05484e-05
	LOSS [training: 0.010820391697045491 | validation: 0.003831900835604985]
	TIME [epoch: 8.25 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013904238347752378		[learning rate: 4.0475e-05]
		[batch 20/20] avg loss: 0.014979376298165537		[learning rate: 4.0401e-05]
	Learning Rate: 4.04012e-05
	LOSS [training: 0.014441807322958961 | validation: 0.013578565993151619]
	TIME [epoch: 8.23 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010593378050940756		[learning rate: 4.0328e-05]
		[batch 20/20] avg loss: 0.018817150247610253		[learning rate: 4.0255e-05]
	Learning Rate: 4.02546e-05
	LOSS [training: 0.014705264149275504 | validation: 0.003357370311489579]
	TIME [epoch: 8.24 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013621982256937873		[learning rate: 4.0182e-05]
		[batch 20/20] avg loss: 0.014303952205610756		[learning rate: 4.0109e-05]
	Learning Rate: 4.01085e-05
	LOSS [training: 0.013962967231274317 | validation: 0.011211239359758053]
	TIME [epoch: 8.23 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015310620565771843		[learning rate: 4.0036e-05]
		[batch 20/20] avg loss: 0.019400218127069793		[learning rate: 3.9963e-05]
	Learning Rate: 3.9963e-05
	LOSS [training: 0.01735541934642082 | validation: 0.007295692560381297]
	TIME [epoch: 8.25 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012086830490453285		[learning rate: 3.989e-05]
		[batch 20/20] avg loss: 0.01739807536606975		[learning rate: 3.9818e-05]
	Learning Rate: 3.9818e-05
	LOSS [training: 0.014742452928261518 | validation: 0.0031991200024879705]
	TIME [epoch: 8.23 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010797601206140265		[learning rate: 3.9746e-05]
		[batch 20/20] avg loss: 0.01832269174467691		[learning rate: 3.9673e-05]
	Learning Rate: 3.96735e-05
	LOSS [training: 0.014560146475408587 | validation: 0.01192978917265916]
	TIME [epoch: 8.23 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01690163454156033		[learning rate: 3.9601e-05]
		[batch 20/20] avg loss: 0.022788614573681074		[learning rate: 3.9529e-05]
	Learning Rate: 3.95295e-05
	LOSS [training: 0.0198451245576207 | validation: 0.012165052581035738]
	TIME [epoch: 8.23 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012281341893888714		[learning rate: 3.9458e-05]
		[batch 20/20] avg loss: 0.019936438379889475		[learning rate: 3.9386e-05]
	Learning Rate: 3.9386e-05
	LOSS [training: 0.016108890136889098 | validation: 0.009272605114505873]
	TIME [epoch: 8.25 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01756204781891089		[learning rate: 3.9315e-05]
		[batch 20/20] avg loss: 0.018468711258363576		[learning rate: 3.9243e-05]
	Learning Rate: 3.92431e-05
	LOSS [training: 0.018015379538637234 | validation: 0.007718126425658082]
	TIME [epoch: 8.24 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020311256953639452		[learning rate: 3.9172e-05]
		[batch 20/20] avg loss: 0.014990520208632735		[learning rate: 3.9101e-05]
	Learning Rate: 3.91007e-05
	LOSS [training: 0.017650888581136098 | validation: 0.011783699310321643]
	TIME [epoch: 8.23 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011874160395527484		[learning rate: 3.903e-05]
		[batch 20/20] avg loss: 0.018869198576444722		[learning rate: 3.8959e-05]
	Learning Rate: 3.89588e-05
	LOSS [training: 0.015371679485986103 | validation: 0.0094847198747878]
	TIME [epoch: 8.23 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014359372337785978		[learning rate: 3.8888e-05]
		[batch 20/20] avg loss: 0.014661753495541833		[learning rate: 3.8817e-05]
	Learning Rate: 3.88174e-05
	LOSS [training: 0.014510562916663906 | validation: 0.007619464812754093]
	TIME [epoch: 8.24 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017251347652844702		[learning rate: 3.8747e-05]
		[batch 20/20] avg loss: 0.016419263124181356		[learning rate: 3.8677e-05]
	Learning Rate: 3.86765e-05
	LOSS [training: 0.01683530538851303 | validation: 0.005454123122449593]
	TIME [epoch: 8.25 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013818206860388912		[learning rate: 3.8606e-05]
		[batch 20/20] avg loss: 0.013968949371259958		[learning rate: 3.8536e-05]
	Learning Rate: 3.85362e-05
	LOSS [training: 0.013893578115824434 | validation: 0.0035970595329237417]
	TIME [epoch: 8.24 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016974420719851523		[learning rate: 3.8466e-05]
		[batch 20/20] avg loss: 0.014213916443692572		[learning rate: 3.8396e-05]
	Learning Rate: 3.83963e-05
	LOSS [training: 0.01559416858177205 | validation: 0.015008808221412148]
	TIME [epoch: 8.24 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017126277407712033		[learning rate: 3.8327e-05]
		[batch 20/20] avg loss: 0.017584845639406665		[learning rate: 3.8257e-05]
	Learning Rate: 3.8257e-05
	LOSS [training: 0.017355561523559347 | validation: 0.009206911739748143]
	TIME [epoch: 8.23 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021330310511521546		[learning rate: 3.8187e-05]
		[batch 20/20] avg loss: 0.015429056761763515		[learning rate: 3.8118e-05]
	Learning Rate: 3.81181e-05
	LOSS [training: 0.018379683636642528 | validation: 0.005816349177450831]
	TIME [epoch: 8.26 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014121794243624485		[learning rate: 3.8049e-05]
		[batch 20/20] avg loss: 0.018389347406239907		[learning rate: 3.798e-05]
	Learning Rate: 3.79798e-05
	LOSS [training: 0.016255570824932196 | validation: 0.010136058831599267]
	TIME [epoch: 8.24 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02227276466333424		[learning rate: 3.7911e-05]
		[batch 20/20] avg loss: 0.028582852072396588		[learning rate: 3.7842e-05]
	Learning Rate: 3.7842e-05
	LOSS [training: 0.02542780836786541 | validation: 0.01995727658327945]
	TIME [epoch: 8.23 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016225846349153146		[learning rate: 3.7773e-05]
		[batch 20/20] avg loss: 0.015072715742365388		[learning rate: 3.7705e-05]
	Learning Rate: 3.77046e-05
	LOSS [training: 0.015649281045759265 | validation: 0.013391804631141535]
	TIME [epoch: 8.24 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017663985224066602		[learning rate: 3.7636e-05]
		[batch 20/20] avg loss: 0.01844968060477183		[learning rate: 3.7568e-05]
	Learning Rate: 3.75678e-05
	LOSS [training: 0.01805683291441922 | validation: 0.007688654202907796]
	TIME [epoch: 8.24 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023989473861544437		[learning rate: 3.75e-05]
		[batch 20/20] avg loss: 0.011282528080599971		[learning rate: 3.7431e-05]
	Learning Rate: 3.74315e-05
	LOSS [training: 0.017636000971072204 | validation: 0.006340422732068466]
	TIME [epoch: 8.25 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021790815052436643		[learning rate: 3.7363e-05]
		[batch 20/20] avg loss: 0.02143149403331291		[learning rate: 3.7296e-05]
	Learning Rate: 3.72956e-05
	LOSS [training: 0.021611154542874777 | validation: 0.002177276226251146]
	TIME [epoch: 8.23 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019723835066256443		[learning rate: 3.7228e-05]
		[batch 20/20] avg loss: 0.012320052356102187		[learning rate: 3.716e-05]
	Learning Rate: 3.71603e-05
	LOSS [training: 0.016021943711179314 | validation: 0.011105571787267064]
	TIME [epoch: 8.23 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010736308633669598		[learning rate: 3.7093e-05]
		[batch 20/20] avg loss: 0.015455944315320023		[learning rate: 3.7025e-05]
	Learning Rate: 3.70254e-05
	LOSS [training: 0.01309612647449481 | validation: 0.009351915312581026]
	TIME [epoch: 8.24 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014705048628899408		[learning rate: 3.6958e-05]
		[batch 20/20] avg loss: 0.007704474760283295		[learning rate: 3.6891e-05]
	Learning Rate: 3.68911e-05
	LOSS [training: 0.011204761694591354 | validation: 0.003255622346800941]
	TIME [epoch: 8.26 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014304685342712712		[learning rate: 3.6824e-05]
		[batch 20/20] avg loss: 0.01219573653797876		[learning rate: 3.6757e-05]
	Learning Rate: 3.67572e-05
	LOSS [training: 0.013250210940345735 | validation: 0.01221534811896877]
	TIME [epoch: 8.24 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011270504458914945		[learning rate: 3.669e-05]
		[batch 20/20] avg loss: 0.0142617756172142		[learning rate: 3.6624e-05]
	Learning Rate: 3.66238e-05
	LOSS [training: 0.012766140038064571 | validation: 0.0051569819355774356]
	TIME [epoch: 8.24 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014539260004005428		[learning rate: 3.6557e-05]
		[batch 20/20] avg loss: 0.01285465518236375		[learning rate: 3.6491e-05]
	Learning Rate: 3.64909e-05
	LOSS [training: 0.013696957593184586 | validation: 0.014968539598418968]
	TIME [epoch: 8.24 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02112647216779589		[learning rate: 3.6425e-05]
		[batch 20/20] avg loss: 0.015735004026112213		[learning rate: 3.6358e-05]
	Learning Rate: 3.63584e-05
	LOSS [training: 0.018430738096954054 | validation: 0.011341800709225348]
	TIME [epoch: 8.25 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019959403308767325		[learning rate: 3.6292e-05]
		[batch 20/20] avg loss: 0.014086752115350718		[learning rate: 3.6226e-05]
	Learning Rate: 3.62265e-05
	LOSS [training: 0.01702307771205902 | validation: 0.0072114811841302055]
	TIME [epoch: 8.24 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012694965136419333		[learning rate: 3.6161e-05]
		[batch 20/20] avg loss: 0.011938057658445463		[learning rate: 3.6095e-05]
	Learning Rate: 3.6095e-05
	LOSS [training: 0.012316511397432396 | validation: 0.0060194399777907595]
	TIME [epoch: 8.23 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010419162547569109		[learning rate: 3.6029e-05]
		[batch 20/20] avg loss: 0.01958225898394482		[learning rate: 3.5964e-05]
	Learning Rate: 3.5964e-05
	LOSS [training: 0.015000710765756967 | validation: 0.015566146374715376]
	TIME [epoch: 8.22 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014906302620039008		[learning rate: 3.5899e-05]
		[batch 20/20] avg loss: 0.014810039747867886		[learning rate: 3.5834e-05]
	Learning Rate: 3.58335e-05
	LOSS [training: 0.014858171183953448 | validation: 0.010032882824345855]
	TIME [epoch: 8.23 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017015936118106107		[learning rate: 3.5768e-05]
		[batch 20/20] avg loss: 0.019336646615262253		[learning rate: 3.5703e-05]
	Learning Rate: 3.57035e-05
	LOSS [training: 0.01817629136668418 | validation: 0.012767667986622933]
	TIME [epoch: 8.26 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019162134129323168		[learning rate: 3.5639e-05]
		[batch 20/20] avg loss: 0.016865423411996292		[learning rate: 3.5574e-05]
	Learning Rate: 3.55739e-05
	LOSS [training: 0.018013778770659728 | validation: 0.015028485910722327]
	TIME [epoch: 8.23 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022284331858306573		[learning rate: 3.5509e-05]
		[batch 20/20] avg loss: 0.0179779060829515		[learning rate: 3.5445e-05]
	Learning Rate: 3.54448e-05
	LOSS [training: 0.02013111897062904 | validation: 0.018044900761460578]
	TIME [epoch: 8.24 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021167286615064434		[learning rate: 3.538e-05]
		[batch 20/20] avg loss: 0.019299958850416687		[learning rate: 3.5316e-05]
	Learning Rate: 3.53162e-05
	LOSS [training: 0.02023362273274056 | validation: 0.011745151740758512]
	TIME [epoch: 8.24 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018706275809618055		[learning rate: 3.5252e-05]
		[batch 20/20] avg loss: 0.02078863566765289		[learning rate: 3.5188e-05]
	Learning Rate: 3.5188e-05
	LOSS [training: 0.019747455738635473 | validation: 0.01276778229525498]
	TIME [epoch: 8.27 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021684523417190217		[learning rate: 3.5124e-05]
		[batch 20/20] avg loss: 0.018875769010508848		[learning rate: 3.506e-05]
	Learning Rate: 3.50603e-05
	LOSS [training: 0.020280146213849538 | validation: 0.014522297291468067]
	TIME [epoch: 8.25 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01284914071810983		[learning rate: 3.4997e-05]
		[batch 20/20] avg loss: 0.018193162845355458		[learning rate: 3.4933e-05]
	Learning Rate: 3.49331e-05
	LOSS [training: 0.015521151781732644 | validation: 0.013472327161319773]
	TIME [epoch: 8.24 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01344255564781141		[learning rate: 3.487e-05]
		[batch 20/20] avg loss: 0.014581754837048289		[learning rate: 3.4806e-05]
	Learning Rate: 3.48063e-05
	LOSS [training: 0.014012155242429849 | validation: 0.009858432422476044]
	TIME [epoch: 8.23 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0211292814159662		[learning rate: 3.4743e-05]
		[batch 20/20] avg loss: 0.010990672195474652		[learning rate: 3.468e-05]
	Learning Rate: 3.468e-05
	LOSS [training: 0.016059976805720428 | validation: 0.013405179502854293]
	TIME [epoch: 8.25 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018107543753582256		[learning rate: 3.4617e-05]
		[batch 20/20] avg loss: 0.01372946038427525		[learning rate: 3.4554e-05]
	Learning Rate: 3.45541e-05
	LOSS [training: 0.015918502068928753 | validation: 0.00827190907069978]
	TIME [epoch: 8.24 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010024292108471463		[learning rate: 3.4491e-05]
		[batch 20/20] avg loss: 0.02027935232322751		[learning rate: 3.4429e-05]
	Learning Rate: 3.44287e-05
	LOSS [training: 0.015151822215849486 | validation: 0.010093822604934733]
	TIME [epoch: 8.24 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014855760546421312		[learning rate: 3.4366e-05]
		[batch 20/20] avg loss: 0.015760820848684076		[learning rate: 3.4304e-05]
	Learning Rate: 3.43038e-05
	LOSS [training: 0.015308290697552695 | validation: 0.007196714371478528]
	TIME [epoch: 8.24 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008720052753381502		[learning rate: 3.4241e-05]
		[batch 20/20] avg loss: 0.015838144626155258		[learning rate: 3.4179e-05]
	Learning Rate: 3.41793e-05
	LOSS [training: 0.01227909868976838 | validation: 0.006774823862621828]
	TIME [epoch: 8.24 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01068690620959482		[learning rate: 3.4117e-05]
		[batch 20/20] avg loss: 0.01956966624410375		[learning rate: 3.4055e-05]
	Learning Rate: 3.40553e-05
	LOSS [training: 0.015128286226849285 | validation: 0.017193491444234786]
	TIME [epoch: 8.25 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013071345437081999		[learning rate: 3.3993e-05]
		[batch 20/20] avg loss: 0.011203032400471042		[learning rate: 3.3932e-05]
	Learning Rate: 3.39317e-05
	LOSS [training: 0.01213718891877652 | validation: 0.008327377088407504]
	TIME [epoch: 8.24 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016325904668191473		[learning rate: 3.387e-05]
		[batch 20/20] avg loss: 0.014967249411235607		[learning rate: 3.3809e-05]
	Learning Rate: 3.38085e-05
	LOSS [training: 0.01564657703971354 | validation: 0.0023742609709672255]
	TIME [epoch: 8.23 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015750110093397853		[learning rate: 3.3747e-05]
		[batch 20/20] avg loss: 0.01763367279672985		[learning rate: 3.3686e-05]
	Learning Rate: 3.36858e-05
	LOSS [training: 0.016691891445063854 | validation: 0.008674284835346721]
	TIME [epoch: 8.24 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014966855365178963		[learning rate: 3.3625e-05]
		[batch 20/20] avg loss: 0.016824924225455735		[learning rate: 3.3564e-05]
	Learning Rate: 3.35636e-05
	LOSS [training: 0.015895889795317353 | validation: 0.014286686359829973]
	TIME [epoch: 8.25 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011954781187784145		[learning rate: 3.3503e-05]
		[batch 20/20] avg loss: 0.01879111571799438		[learning rate: 3.3442e-05]
	Learning Rate: 3.34418e-05
	LOSS [training: 0.015372948452889269 | validation: 0.016015627509151243]
	TIME [epoch: 8.24 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02136234459182846		[learning rate: 3.3381e-05]
		[batch 20/20] avg loss: 0.015020775372020147		[learning rate: 3.332e-05]
	Learning Rate: 3.33204e-05
	LOSS [training: 0.018191559981924305 | validation: 0.013262064765632173]
	TIME [epoch: 8.24 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013934800016269719		[learning rate: 3.326e-05]
		[batch 20/20] avg loss: 0.016533871669089227		[learning rate: 3.32e-05]
	Learning Rate: 3.31995e-05
	LOSS [training: 0.01523433584267947 | validation: 0.0077204321431252766]
	TIME [epoch: 8.23 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014522729663030583		[learning rate: 3.3139e-05]
		[batch 20/20] avg loss: 0.015371169293026682		[learning rate: 3.3079e-05]
	Learning Rate: 3.3079e-05
	LOSS [training: 0.014946949478028635 | validation: 0.015090702277448659]
	TIME [epoch: 8.24 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020702522327546814		[learning rate: 3.3019e-05]
		[batch 20/20] avg loss: 0.012414496512468797		[learning rate: 3.2959e-05]
	Learning Rate: 3.2959e-05
	LOSS [training: 0.016558509420007802 | validation: 0.012037783434305406]
	TIME [epoch: 8.25 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014666025679866154		[learning rate: 3.2899e-05]
		[batch 20/20] avg loss: 0.015459511773392515		[learning rate: 3.2839e-05]
	Learning Rate: 3.28394e-05
	LOSS [training: 0.015062768726629336 | validation: 0.009973306554303596]
	TIME [epoch: 8.23 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009336626416476746		[learning rate: 3.278e-05]
		[batch 20/20] avg loss: 0.01598384278035759		[learning rate: 3.272e-05]
	Learning Rate: 3.27202e-05
	LOSS [training: 0.012660234598417164 | validation: 0.0012354094785039064]
	TIME [epoch: 8.23 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013238861409013009		[learning rate: 3.2661e-05]
		[batch 20/20] avg loss: 0.016996461796929097		[learning rate: 3.2601e-05]
	Learning Rate: 3.26014e-05
	LOSS [training: 0.015117661602971053 | validation: 0.013839099768327744]
	TIME [epoch: 8.24 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016450599172480564		[learning rate: 3.2542e-05]
		[batch 20/20] avg loss: 0.015325505749921906		[learning rate: 3.2483e-05]
	Learning Rate: 3.24831e-05
	LOSS [training: 0.015888052461201233 | validation: 0.007981472497172615]
	TIME [epoch: 8.26 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011796672165594322		[learning rate: 3.2424e-05]
		[batch 20/20] avg loss: 0.013965065098745599		[learning rate: 3.2365e-05]
	Learning Rate: 3.23652e-05
	LOSS [training: 0.012880868632169962 | validation: 0.006633646023151607]
	TIME [epoch: 8.24 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013548586400959142		[learning rate: 3.2306e-05]
		[batch 20/20] avg loss: 0.008940074158609883		[learning rate: 3.2248e-05]
	Learning Rate: 3.22478e-05
	LOSS [training: 0.011244330279784514 | validation: 0.000863125140725441]
	TIME [epoch: 8.24 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01361008631969233		[learning rate: 3.2189e-05]
		[batch 20/20] avg loss: 0.021197182700743555		[learning rate: 3.2131e-05]
	Learning Rate: 3.21308e-05
	LOSS [training: 0.017403634510217945 | validation: 0.016637910816961355]
	TIME [epoch: 8.23 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017331919816940267		[learning rate: 3.2072e-05]
		[batch 20/20] avg loss: 0.013555581355914479		[learning rate: 3.2014e-05]
	Learning Rate: 3.20142e-05
	LOSS [training: 0.015443750586427371 | validation: 0.007610305022979264]
	TIME [epoch: 8.24 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015409349904510053		[learning rate: 3.1956e-05]
		[batch 20/20] avg loss: 0.014388341034327682		[learning rate: 3.1898e-05]
	Learning Rate: 3.1898e-05
	LOSS [training: 0.014898845469418864 | validation: 0.0075387346409628506]
	TIME [epoch: 8.24 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013639360516747221		[learning rate: 3.184e-05]
		[batch 20/20] avg loss: 0.012791196421679672		[learning rate: 3.1782e-05]
	Learning Rate: 3.17822e-05
	LOSS [training: 0.013215278469213446 | validation: 0.006752831384092758]
	TIME [epoch: 8.24 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01748085912233769		[learning rate: 3.1725e-05]
		[batch 20/20] avg loss: 0.010064582074682197		[learning rate: 3.1667e-05]
	Learning Rate: 3.16669e-05
	LOSS [training: 0.013772720598509943 | validation: 0.004199778815364126]
	TIME [epoch: 8.23 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012459899787117512		[learning rate: 3.1609e-05]
		[batch 20/20] avg loss: 0.013764642331207954		[learning rate: 3.1552e-05]
	Learning Rate: 3.1552e-05
	LOSS [training: 0.01311227105916273 | validation: 0.010425914534120692]
	TIME [epoch: 8.24 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012233391460017225		[learning rate: 3.1495e-05]
		[batch 20/20] avg loss: 0.01780481297737828		[learning rate: 3.1437e-05]
	Learning Rate: 3.14375e-05
	LOSS [training: 0.015019102218697752 | validation: 0.010473270729875398]
	TIME [epoch: 8.26 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015741022918293997		[learning rate: 3.138e-05]
		[batch 20/20] avg loss: 0.021238467032058304		[learning rate: 3.1323e-05]
	Learning Rate: 3.13234e-05
	LOSS [training: 0.01848974497517615 | validation: 0.011988698208382147]
	TIME [epoch: 8.22 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016863273358698826		[learning rate: 3.1266e-05]
		[batch 20/20] avg loss: 0.015075015065361097		[learning rate: 3.121e-05]
	Learning Rate: 3.12097e-05
	LOSS [training: 0.015969144212029964 | validation: 0.010104048345856096]
	TIME [epoch: 8.23 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015550706212194228		[learning rate: 3.1153e-05]
		[batch 20/20] avg loss: 0.01232400626867095		[learning rate: 3.1096e-05]
	Learning Rate: 3.10964e-05
	LOSS [training: 0.013937356240432588 | validation: 0.014801724630882192]
	TIME [epoch: 8.23 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016867409922191167		[learning rate: 3.104e-05]
		[batch 20/20] avg loss: 0.015602546604623175		[learning rate: 3.0984e-05]
	Learning Rate: 3.09836e-05
	LOSS [training: 0.016234978263407175 | validation: 0.011776289198931079]
	TIME [epoch: 8.26 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015372872372437086		[learning rate: 3.0927e-05]
		[batch 20/20] avg loss: 0.011671495392354866		[learning rate: 3.0871e-05]
	Learning Rate: 3.08711e-05
	LOSS [training: 0.013522183882395974 | validation: -0.0005756391954688119]
	TIME [epoch: 8.23 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011457399275792087		[learning rate: 3.0815e-05]
		[batch 20/20] avg loss: 0.015908045951883616		[learning rate: 3.0759e-05]
	Learning Rate: 3.07591e-05
	LOSS [training: 0.013682722613837853 | validation: 0.012654306781035221]
	TIME [epoch: 8.23 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011800087605530854		[learning rate: 3.0703e-05]
		[batch 20/20] avg loss: 0.018066348668206174		[learning rate: 3.0647e-05]
	Learning Rate: 3.06475e-05
	LOSS [training: 0.014933218136868511 | validation: 0.013399803019801149]
	TIME [epoch: 8.23 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014821545550832297		[learning rate: 3.0592e-05]
		[batch 20/20] avg loss: 0.01650152959745077		[learning rate: 3.0536e-05]
	Learning Rate: 3.05363e-05
	LOSS [training: 0.015661537574141536 | validation: 0.004021705676059767]
	TIME [epoch: 8.24 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01765641163225829		[learning rate: 3.0481e-05]
		[batch 20/20] avg loss: 0.0094160914647747		[learning rate: 3.0425e-05]
	Learning Rate: 3.04254e-05
	LOSS [training: 0.013536251548516495 | validation: 0.013658859346822795]
	TIME [epoch: 8.26 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015168586060294184		[learning rate: 3.037e-05]
		[batch 20/20] avg loss: 0.013307132168830258		[learning rate: 3.0315e-05]
	Learning Rate: 3.0315e-05
	LOSS [training: 0.014237859114562221 | validation: 0.006942528461439563]
	TIME [epoch: 8.24 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012137111180538576		[learning rate: 3.026e-05]
		[batch 20/20] avg loss: 0.013907115279777898		[learning rate: 3.0205e-05]
	Learning Rate: 3.0205e-05
	LOSS [training: 0.013022113230158242 | validation: 0.007914792423590772]
	TIME [epoch: 8.24 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020880323386126783		[learning rate: 3.015e-05]
		[batch 20/20] avg loss: 0.011656842984476095		[learning rate: 3.0095e-05]
	Learning Rate: 3.00954e-05
	LOSS [training: 0.016268583185301437 | validation: 0.01453999124391632]
	TIME [epoch: 8.23 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02478150697750507		[learning rate: 3.0041e-05]
		[batch 20/20] avg loss: 0.014306122858662626		[learning rate: 2.9986e-05]
	Learning Rate: 2.99862e-05
	LOSS [training: 0.019543814918083845 | validation: -0.0015614758237293674]
	TIME [epoch: 8.26 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0055037720189457405		[learning rate: 2.9932e-05]
		[batch 20/20] avg loss: 0.018729144324153835		[learning rate: 2.9877e-05]
	Learning Rate: 2.98774e-05
	LOSS [training: 0.012116458171549788 | validation: 0.007716031795147407]
	TIME [epoch: 8.24 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020695790663042467		[learning rate: 2.9823e-05]
		[batch 20/20] avg loss: 0.012069506515250225		[learning rate: 2.9769e-05]
	Learning Rate: 2.97689e-05
	LOSS [training: 0.01638264858914635 | validation: 0.013217438142085314]
	TIME [epoch: 8.24 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015668183677057752		[learning rate: 2.9715e-05]
		[batch 20/20] avg loss: 0.015899194592486378		[learning rate: 2.9661e-05]
	Learning Rate: 2.96609e-05
	LOSS [training: 0.015783689134772065 | validation: 0.0006859367805697928]
	TIME [epoch: 8.23 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01594865403001177		[learning rate: 2.9607e-05]
		[batch 20/20] avg loss: 0.011993171891397848		[learning rate: 2.9553e-05]
	Learning Rate: 2.95533e-05
	LOSS [training: 0.01397091296070481 | validation: 0.0097177426046844]
	TIME [epoch: 8.25 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013644251813826372		[learning rate: 2.95e-05]
		[batch 20/20] avg loss: 0.016046006276997784		[learning rate: 2.9446e-05]
	Learning Rate: 2.9446e-05
	LOSS [training: 0.01484512904541208 | validation: 0.010826091849784069]
	TIME [epoch: 8.24 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017832031720611312		[learning rate: 2.9393e-05]
		[batch 20/20] avg loss: 0.015896500905956422		[learning rate: 2.9339e-05]
	Learning Rate: 2.93391e-05
	LOSS [training: 0.016864266313283867 | validation: 0.010796955879770765]
	TIME [epoch: 8.23 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013470571865666862		[learning rate: 2.9286e-05]
		[batch 20/20] avg loss: 0.010455229139086692		[learning rate: 2.9233e-05]
	Learning Rate: 2.92327e-05
	LOSS [training: 0.011962900502376776 | validation: 0.007560710354595484]
	TIME [epoch: 8.23 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014628202720319703		[learning rate: 2.918e-05]
		[batch 20/20] avg loss: 0.02060562354911315		[learning rate: 2.9127e-05]
	Learning Rate: 2.91266e-05
	LOSS [training: 0.017616913134716423 | validation: 0.0041907464502994795]
	TIME [epoch: 8.23 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0071730849095067465		[learning rate: 2.9074e-05]
		[batch 20/20] avg loss: 0.010413843247808655		[learning rate: 2.9021e-05]
	Learning Rate: 2.90209e-05
	LOSS [training: 0.008793464078657701 | validation: 0.012585126340484502]
	TIME [epoch: 8.26 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017640567272822368		[learning rate: 2.8968e-05]
		[batch 20/20] avg loss: 0.01588645400968686		[learning rate: 2.8916e-05]
	Learning Rate: 2.89156e-05
	LOSS [training: 0.016763510641254614 | validation: 0.0123007929040058]
	TIME [epoch: 8.24 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015771861215796965		[learning rate: 2.8863e-05]
		[batch 20/20] avg loss: 0.013249935560724731		[learning rate: 2.8811e-05]
	Learning Rate: 2.88106e-05
	LOSS [training: 0.014510898388260852 | validation: 0.004973539363973056]
	TIME [epoch: 8.23 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013284538098243157		[learning rate: 2.8758e-05]
		[batch 20/20] avg loss: 0.02328788175556838		[learning rate: 2.8706e-05]
	Learning Rate: 2.87061e-05
	LOSS [training: 0.018286209926905765 | validation: 0.010629004578048514]
	TIME [epoch: 8.24 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01814857624851311		[learning rate: 2.8654e-05]
		[batch 20/20] avg loss: 0.013372471644867956		[learning rate: 2.8602e-05]
	Learning Rate: 2.86019e-05
	LOSS [training: 0.01576052394669053 | validation: 0.011914788302153043]
	TIME [epoch: 8.26 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019232628827528322		[learning rate: 2.855e-05]
		[batch 20/20] avg loss: 0.013861947387485187		[learning rate: 2.8498e-05]
	Learning Rate: 2.84981e-05
	LOSS [training: 0.016547288107506754 | validation: 0.010796225610851291]
	TIME [epoch: 8.23 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01259905748446367		[learning rate: 2.8446e-05]
		[batch 20/20] avg loss: 0.01623557498381364		[learning rate: 2.8395e-05]
	Learning Rate: 2.83947e-05
	LOSS [training: 0.014417316234138661 | validation: 0.009226260720127093]
	TIME [epoch: 8.23 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015991438951479583		[learning rate: 2.8343e-05]
		[batch 20/20] avg loss: 0.01345709674751841		[learning rate: 2.8292e-05]
	Learning Rate: 2.82916e-05
	LOSS [training: 0.014724267849498998 | validation: 0.006405764517058075]
	TIME [epoch: 8.24 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01612666902409678		[learning rate: 2.824e-05]
		[batch 20/20] avg loss: 0.017390830358876907		[learning rate: 2.8189e-05]
	Learning Rate: 2.8189e-05
	LOSS [training: 0.016758749691486847 | validation: 0.005464581599601104]
	TIME [epoch: 8.23 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021862111157456107		[learning rate: 2.8138e-05]
		[batch 20/20] avg loss: 0.010975775013535647		[learning rate: 2.8087e-05]
	Learning Rate: 2.80867e-05
	LOSS [training: 0.01641894308549588 | validation: 0.00548298403625574]
	TIME [epoch: 8.26 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0118296644101693		[learning rate: 2.8036e-05]
		[batch 20/20] avg loss: 0.015161152139640111		[learning rate: 2.7985e-05]
	Learning Rate: 2.79847e-05
	LOSS [training: 0.013495408274904705 | validation: 0.007339546876886499]
	TIME [epoch: 8.24 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01365232253759692		[learning rate: 2.7934e-05]
		[batch 20/20] avg loss: 0.01335530322317469		[learning rate: 2.7883e-05]
	Learning Rate: 2.78832e-05
	LOSS [training: 0.013503812880385805 | validation: 0.010823415840718302]
	TIME [epoch: 8.24 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014215566617163356		[learning rate: 2.7833e-05]
		[batch 20/20] avg loss: 0.014339456148381544		[learning rate: 2.7782e-05]
	Learning Rate: 2.7782e-05
	LOSS [training: 0.01427751138277245 | validation: 0.0062478966497615295]
	TIME [epoch: 8.23 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012767000580362056		[learning rate: 2.7732e-05]
		[batch 20/20] avg loss: 0.015203729161238511		[learning rate: 2.7681e-05]
	Learning Rate: 2.76812e-05
	LOSS [training: 0.013985364870800284 | validation: 0.01469593941853977]
	TIME [epoch: 8.25 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01895947185642445		[learning rate: 2.7631e-05]
		[batch 20/20] avg loss: 0.01573038551167019		[learning rate: 2.7581e-05]
	Learning Rate: 2.75807e-05
	LOSS [training: 0.01734492868404732 | validation: 0.004982084025395]
	TIME [epoch: 8.23 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01403515741373045		[learning rate: 2.7531e-05]
		[batch 20/20] avg loss: 0.020133982537950415		[learning rate: 2.7481e-05]
	Learning Rate: 2.74806e-05
	LOSS [training: 0.017084569975840437 | validation: 0.00813995533331864]
	TIME [epoch: 8.24 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012294177653706207		[learning rate: 2.7431e-05]
		[batch 20/20] avg loss: 0.014232620400171874		[learning rate: 2.7381e-05]
	Learning Rate: 2.73809e-05
	LOSS [training: 0.01326339902693904 | validation: 0.010308643142593053]
	TIME [epoch: 8.23 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014593726083865277		[learning rate: 2.7331e-05]
		[batch 20/20] avg loss: 0.012289903989143758		[learning rate: 2.7282e-05]
	Learning Rate: 2.72815e-05
	LOSS [training: 0.013441815036504517 | validation: 0.011356517393293599]
	TIME [epoch: 8.25 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013656006349137367		[learning rate: 2.7232e-05]
		[batch 20/20] avg loss: 0.020524576285772637		[learning rate: 2.7183e-05]
	Learning Rate: 2.71825e-05
	LOSS [training: 0.017090291317455004 | validation: 0.015855419938047177]
	TIME [epoch: 8.24 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019023530800242886		[learning rate: 2.7133e-05]
		[batch 20/20] avg loss: 0.014795811116029997		[learning rate: 2.7084e-05]
	Learning Rate: 2.70839e-05
	LOSS [training: 0.01690967095813644 | validation: 0.010190610675717153]
	TIME [epoch: 8.23 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012056154445333676		[learning rate: 2.7035e-05]
		[batch 20/20] avg loss: 0.012668365454529467		[learning rate: 2.6986e-05]
	Learning Rate: 2.69856e-05
	LOSS [training: 0.01236225994993157 | validation: 0.013436223112437105]
	TIME [epoch: 8.23 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01315568704519389		[learning rate: 2.6937e-05]
		[batch 20/20] avg loss: 0.015672737806655894		[learning rate: 2.6888e-05]
	Learning Rate: 2.68876e-05
	LOSS [training: 0.014414212425924893 | validation: 0.009108203194575254]
	TIME [epoch: 8.23 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01478933546024604		[learning rate: 2.6839e-05]
		[batch 20/20] avg loss: 0.011181772707460371		[learning rate: 2.679e-05]
	Learning Rate: 2.67901e-05
	LOSS [training: 0.012985554083853204 | validation: 0.007835001473388998]
	TIME [epoch: 8.25 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0220244750045341		[learning rate: 2.6741e-05]
		[batch 20/20] avg loss: 0.026876430192026318		[learning rate: 2.6693e-05]
	Learning Rate: 2.66928e-05
	LOSS [training: 0.024450452598280204 | validation: 0.0060994745132169]
	TIME [epoch: 8.22 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015162907631849479		[learning rate: 2.6644e-05]
		[batch 20/20] avg loss: 0.012758801358225175		[learning rate: 2.6596e-05]
	Learning Rate: 2.6596e-05
	LOSS [training: 0.013960854495037325 | validation: 0.0076943456825697525]
	TIME [epoch: 8.23 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019296401805193937		[learning rate: 2.6548e-05]
		[batch 20/20] avg loss: 0.022970994454530415		[learning rate: 2.6499e-05]
	Learning Rate: 2.64994e-05
	LOSS [training: 0.021133698129862176 | validation: 0.007637692570159639]
	TIME [epoch: 8.22 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013550159765497189		[learning rate: 2.6451e-05]
		[batch 20/20] avg loss: 0.01333238963900508		[learning rate: 2.6403e-05]
	Learning Rate: 2.64033e-05
	LOSS [training: 0.013441274702251132 | validation: 0.008788118561621585]
	TIME [epoch: 8.25 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016227520664539707		[learning rate: 2.6355e-05]
		[batch 20/20] avg loss: 0.014229277384817949		[learning rate: 2.6307e-05]
	Learning Rate: 2.63075e-05
	LOSS [training: 0.015228399024678832 | validation: 0.00791806884108293]
	TIME [epoch: 8.23 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010789085159038644		[learning rate: 2.626e-05]
		[batch 20/20] avg loss: 0.011487062128586783		[learning rate: 2.6212e-05]
	Learning Rate: 2.6212e-05
	LOSS [training: 0.011138073643812713 | validation: 0.009726560576451545]
	TIME [epoch: 8.23 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011342238526640265		[learning rate: 2.6164e-05]
		[batch 20/20] avg loss: 0.008224885381283822		[learning rate: 2.6117e-05]
	Learning Rate: 2.61169e-05
	LOSS [training: 0.009783561953962043 | validation: 0.0053927616952310665]
	TIME [epoch: 8.24 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010955162800381493		[learning rate: 2.6069e-05]
		[batch 20/20] avg loss: 0.01579062229170548		[learning rate: 2.6022e-05]
	Learning Rate: 2.60221e-05
	LOSS [training: 0.013372892546043489 | validation: 0.01313279550492733]
	TIME [epoch: 8.25 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01341343286254269		[learning rate: 2.5975e-05]
		[batch 20/20] avg loss: 0.017254431093947045		[learning rate: 2.5928e-05]
	Learning Rate: 2.59277e-05
	LOSS [training: 0.015333931978244869 | validation: 0.010191709266992447]
	TIME [epoch: 8.25 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0128155305405365		[learning rate: 2.5881e-05]
		[batch 20/20] avg loss: 0.013905673446094119		[learning rate: 2.5834e-05]
	Learning Rate: 2.58336e-05
	LOSS [training: 0.013360601993315308 | validation: 0.010765554533292351]
	TIME [epoch: 8.24 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010366322985546716		[learning rate: 2.5787e-05]
		[batch 20/20] avg loss: 0.01233009695871658		[learning rate: 2.574e-05]
	Learning Rate: 2.57398e-05
	LOSS [training: 0.011348209972131647 | validation: 0.00920905354133854]
	TIME [epoch: 8.23 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01986133225306916		[learning rate: 2.5693e-05]
		[batch 20/20] avg loss: 0.01187433099287624		[learning rate: 2.5646e-05]
	Learning Rate: 2.56464e-05
	LOSS [training: 0.015867831622972704 | validation: 0.007670191405537803]
	TIME [epoch: 8.23 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015384289619618827		[learning rate: 2.56e-05]
		[batch 20/20] avg loss: 0.011548185338906645		[learning rate: 2.5553e-05]
	Learning Rate: 2.55533e-05
	LOSS [training: 0.013466237479262738 | validation: 0.003142370915153483]
	TIME [epoch: 8.25 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014234627671590321		[learning rate: 2.5507e-05]
		[batch 20/20] avg loss: 0.020054155388236447		[learning rate: 2.5461e-05]
	Learning Rate: 2.54606e-05
	LOSS [training: 0.01714439152991339 | validation: 0.007642728037629414]
	TIME [epoch: 8.23 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015525384550738115		[learning rate: 2.5414e-05]
		[batch 20/20] avg loss: 0.011585987179100758		[learning rate: 2.5368e-05]
	Learning Rate: 2.53682e-05
	LOSS [training: 0.013555685864919436 | validation: 0.006797120485176224]
	TIME [epoch: 8.23 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021297103931172394		[learning rate: 2.5322e-05]
		[batch 20/20] avg loss: 0.00986294199398919		[learning rate: 2.5276e-05]
	Learning Rate: 2.52761e-05
	LOSS [training: 0.015580022962580792 | validation: 0.0051630114808047045]
	TIME [epoch: 8.23 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01319868983951939		[learning rate: 2.523e-05]
		[batch 20/20] avg loss: 0.015995283469866346		[learning rate: 2.5184e-05]
	Learning Rate: 2.51844e-05
	LOSS [training: 0.014596986654692865 | validation: 0.0063680755403669465]
	TIME [epoch: 8.24 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015023735720908704		[learning rate: 2.5139e-05]
		[batch 20/20] avg loss: 0.018079509174153418		[learning rate: 2.5093e-05]
	Learning Rate: 2.5093e-05
	LOSS [training: 0.016551622447531063 | validation: 0.00759499047990134]
	TIME [epoch: 8.24 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009363995762985685		[learning rate: 2.5047e-05]
		[batch 20/20] avg loss: 0.013227210585438725		[learning rate: 2.5002e-05]
	Learning Rate: 2.50019e-05
	LOSS [training: 0.011295603174212207 | validation: 0.009204859994156817]
	TIME [epoch: 8.23 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010890063353151454		[learning rate: 2.4957e-05]
		[batch 20/20] avg loss: 0.017853532655486046		[learning rate: 2.4911e-05]
	Learning Rate: 2.49112e-05
	LOSS [training: 0.01437179800431875 | validation: 0.0005482017243678243]
	TIME [epoch: 8.23 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011806658293222515		[learning rate: 2.4866e-05]
		[batch 20/20] avg loss: 0.016152675276599297		[learning rate: 2.4821e-05]
	Learning Rate: 2.48208e-05
	LOSS [training: 0.013979666784910907 | validation: 0.006586077059948222]
	TIME [epoch: 8.23 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020158927042962453		[learning rate: 2.4776e-05]
		[batch 20/20] avg loss: 0.01047657241189851		[learning rate: 2.4731e-05]
	Learning Rate: 2.47307e-05
	LOSS [training: 0.015317749727430482 | validation: 0.004804605904813991]
	TIME [epoch: 8.26 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01192682934452523		[learning rate: 2.4686e-05]
		[batch 20/20] avg loss: 0.012989521973468374		[learning rate: 2.4641e-05]
	Learning Rate: 2.4641e-05
	LOSS [training: 0.0124581756589968 | validation: 0.0062206312027863996]
	TIME [epoch: 8.24 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010765333730549342		[learning rate: 2.4596e-05]
		[batch 20/20] avg loss: 0.017509683832645027		[learning rate: 2.4552e-05]
	Learning Rate: 2.45516e-05
	LOSS [training: 0.014137508781597183 | validation: 0.007701515591227063]
	TIME [epoch: 8.24 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012486081079430295		[learning rate: 2.4507e-05]
		[batch 20/20] avg loss: 0.01675461580639129		[learning rate: 2.4462e-05]
	Learning Rate: 2.44625e-05
	LOSS [training: 0.014620348442910791 | validation: 0.007794160162502319]
	TIME [epoch: 8.23 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009247701207201062		[learning rate: 2.4418e-05]
		[batch 20/20] avg loss: 0.010452397033346158		[learning rate: 2.4374e-05]
	Learning Rate: 2.43737e-05
	LOSS [training: 0.00985004912027361 | validation: 0.007299986798572234]
	TIME [epoch: 8.25 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018420010854409835		[learning rate: 2.4329e-05]
		[batch 20/20] avg loss: 0.012899890013811626		[learning rate: 2.4285e-05]
	Learning Rate: 2.42852e-05
	LOSS [training: 0.015659950434110732 | validation: 0.009061926754269254]
	TIME [epoch: 8.23 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01847641402009726		[learning rate: 2.4241e-05]
		[batch 20/20] avg loss: 0.014448942406358184		[learning rate: 2.4197e-05]
	Learning Rate: 2.41971e-05
	LOSS [training: 0.01646267821322772 | validation: 0.007996257042898436]
	TIME [epoch: 8.22 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01084750020452179		[learning rate: 2.4153e-05]
		[batch 20/20] avg loss: 0.01542739212260034		[learning rate: 2.4109e-05]
	Learning Rate: 2.41093e-05
	LOSS [training: 0.013137446163561068 | validation: 0.004731530452109284]
	TIME [epoch: 8.22 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012424574643152244		[learning rate: 2.4065e-05]
		[batch 20/20] avg loss: 0.010259486427303514		[learning rate: 2.4022e-05]
	Learning Rate: 2.40218e-05
	LOSS [training: 0.011342030535227882 | validation: 0.005635599067235028]
	TIME [epoch: 8.24 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015544791133670913		[learning rate: 2.3978e-05]
		[batch 20/20] avg loss: 0.015676891048627465		[learning rate: 2.3935e-05]
	Learning Rate: 2.39346e-05
	LOSS [training: 0.015610841091149191 | validation: 0.00797131184917731]
	TIME [epoch: 8.24 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013692557222779534		[learning rate: 2.3891e-05]
		[batch 20/20] avg loss: 0.009782378742160067		[learning rate: 2.3848e-05]
	Learning Rate: 2.38477e-05
	LOSS [training: 0.011737467982469803 | validation: 0.006948250226021746]
	TIME [epoch: 8.22 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013744576550466356		[learning rate: 2.3804e-05]
		[batch 20/20] avg loss: 0.01937183325505837		[learning rate: 2.3761e-05]
	Learning Rate: 2.37612e-05
	LOSS [training: 0.01655820490276236 | validation: 0.008601006044114252]
	TIME [epoch: 8.23 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016635630968439553		[learning rate: 2.3718e-05]
		[batch 20/20] avg loss: 0.0121530034635131		[learning rate: 2.3675e-05]
	Learning Rate: 2.3675e-05
	LOSS [training: 0.014394317215976327 | validation: 0.007955976698193014]
	TIME [epoch: 8.23 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011932016559139468		[learning rate: 2.3632e-05]
		[batch 20/20] avg loss: 0.011820291270270598		[learning rate: 2.3589e-05]
	Learning Rate: 2.35891e-05
	LOSS [training: 0.011876153914705031 | validation: 0.00380746472556037]
	TIME [epoch: 8.25 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019237984745339148		[learning rate: 2.3546e-05]
		[batch 20/20] avg loss: 0.020293922940451216		[learning rate: 2.3503e-05]
	Learning Rate: 2.35034e-05
	LOSS [training: 0.019765953842895184 | validation: 0.016979769315301707]
	TIME [epoch: 8.23 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01674382768272222		[learning rate: 2.3461e-05]
		[batch 20/20] avg loss: 0.015294277190065453		[learning rate: 2.3418e-05]
	Learning Rate: 2.34182e-05
	LOSS [training: 0.016019052436393837 | validation: 0.006006903305545006]
	TIME [epoch: 8.24 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019390977383850882		[learning rate: 2.3376e-05]
		[batch 20/20] avg loss: 0.014109310862414574		[learning rate: 2.3333e-05]
	Learning Rate: 2.33332e-05
	LOSS [training: 0.016750144123132728 | validation: 0.004847033190154381]
	TIME [epoch: 8.24 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01569151805353796		[learning rate: 2.3291e-05]
		[batch 20/20] avg loss: 0.01771525339765565		[learning rate: 2.3248e-05]
	Learning Rate: 2.32485e-05
	LOSS [training: 0.016703385725596806 | validation: 0.00747366837015765]
	TIME [epoch: 8.26 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014996330455142343		[learning rate: 2.3206e-05]
		[batch 20/20] avg loss: 0.011585514843530731		[learning rate: 2.3164e-05]
	Learning Rate: 2.31641e-05
	LOSS [training: 0.013290922649336534 | validation: 0.007746309715135326]
	TIME [epoch: 8.24 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00860408529698232		[learning rate: 2.3122e-05]
		[batch 20/20] avg loss: 0.017312777995483773		[learning rate: 2.308e-05]
	Learning Rate: 2.30801e-05
	LOSS [training: 0.012958431646233046 | validation: 0.0018985752852777973]
	TIME [epoch: 8.23 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02137090254737021		[learning rate: 2.3038e-05]
		[batch 20/20] avg loss: 0.010884090081823809		[learning rate: 2.2996e-05]
	Learning Rate: 2.29963e-05
	LOSS [training: 0.01612749631459701 | validation: 0.0120373688364474]
	TIME [epoch: 8.24 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007394531652875872		[learning rate: 2.2955e-05]
		[batch 20/20] avg loss: 0.016265520799940775		[learning rate: 2.2913e-05]
	Learning Rate: 2.29128e-05
	LOSS [training: 0.011830026226408323 | validation: 0.008130901003937829]
	TIME [epoch: 8.23 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00887261345325793		[learning rate: 2.2871e-05]
		[batch 20/20] avg loss: 0.01587239420943403		[learning rate: 2.283e-05]
	Learning Rate: 2.28297e-05
	LOSS [training: 0.01237250383134598 | validation: -0.0001235752862803948]
	TIME [epoch: 8.25 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017529321357929385		[learning rate: 2.2788e-05]
		[batch 20/20] avg loss: 0.008139203488660348		[learning rate: 2.2747e-05]
	Learning Rate: 2.27468e-05
	LOSS [training: 0.012834262423294865 | validation: 0.005322491834236947]
	TIME [epoch: 8.24 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015920969770253514		[learning rate: 2.2706e-05]
		[batch 20/20] avg loss: 0.012622192511076113		[learning rate: 2.2664e-05]
	Learning Rate: 2.26643e-05
	LOSS [training: 0.014271581140664813 | validation: 0.0064705538881433345]
	TIME [epoch: 8.23 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011644570307591317		[learning rate: 2.2623e-05]
		[batch 20/20] avg loss: 0.014483262783211279		[learning rate: 2.2582e-05]
	Learning Rate: 2.2582e-05
	LOSS [training: 0.013063916545401297 | validation: 0.008605671841691288]
	TIME [epoch: 8.23 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011759738216926361		[learning rate: 2.2541e-05]
		[batch 20/20] avg loss: 0.010750844412261585		[learning rate: 2.25e-05]
	Learning Rate: 2.25001e-05
	LOSS [training: 0.011255291314593975 | validation: 0.008195680348760117]
	TIME [epoch: 8.26 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011902279934569156		[learning rate: 2.2459e-05]
		[batch 20/20] avg loss: 0.014082498584075672		[learning rate: 2.2418e-05]
	Learning Rate: 2.24184e-05
	LOSS [training: 0.012992389259322413 | validation: 0.012318845822242465]
	TIME [epoch: 8.24 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012279926546982619		[learning rate: 2.2378e-05]
		[batch 20/20] avg loss: 0.015267345526696558		[learning rate: 2.2337e-05]
	Learning Rate: 2.23371e-05
	LOSS [training: 0.01377363603683959 | validation: 0.011291092445494892]
	TIME [epoch: 8.23 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013464444422537612		[learning rate: 2.2297e-05]
		[batch 20/20] avg loss: 0.013932206286073296		[learning rate: 2.2256e-05]
	Learning Rate: 2.2256e-05
	LOSS [training: 0.013698325354305454 | validation: 0.004614439302554344]
	TIME [epoch: 8.24 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017962462261777334		[learning rate: 2.2216e-05]
		[batch 20/20] avg loss: 0.011808191388006003		[learning rate: 2.2175e-05]
	Learning Rate: 2.21753e-05
	LOSS [training: 0.01488532682489167 | validation: 0.008428761066430054]
	TIME [epoch: 8.25 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01654255864923836		[learning rate: 2.2135e-05]
		[batch 20/20] avg loss: 0.01751123669978997		[learning rate: 2.2095e-05]
	Learning Rate: 2.20948e-05
	LOSS [training: 0.01702689767451416 | validation: 0.005562045951595323]
	TIME [epoch: 8.24 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007349798042746702		[learning rate: 2.2055e-05]
		[batch 20/20] avg loss: 0.01761433660354093		[learning rate: 2.2015e-05]
	Learning Rate: 2.20146e-05
	LOSS [training: 0.012482067323143818 | validation: 0.01131788933060839]
	TIME [epoch: 8.24 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01952472058913631		[learning rate: 2.1975e-05]
		[batch 20/20] avg loss: 0.01640766408357474		[learning rate: 2.1935e-05]
	Learning Rate: 2.19347e-05
	LOSS [training: 0.017966192336355526 | validation: 0.0070580226942476565]
	TIME [epoch: 8.23 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02104124595797437		[learning rate: 2.1895e-05]
		[batch 20/20] avg loss: 0.011125811927735058		[learning rate: 2.1855e-05]
	Learning Rate: 2.18551e-05
	LOSS [training: 0.016083528942854718 | validation: 0.007882208915029974]
	TIME [epoch: 8.24 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008151679365938889		[learning rate: 2.1815e-05]
		[batch 20/20] avg loss: 0.013353060154651509		[learning rate: 2.1776e-05]
	Learning Rate: 2.17758e-05
	LOSS [training: 0.010752369760295201 | validation: 0.00471537505448988]
	TIME [epoch: 8.26 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015923016639841935		[learning rate: 2.1736e-05]
		[batch 20/20] avg loss: 0.021850473178297664		[learning rate: 2.1697e-05]
	Learning Rate: 2.16968e-05
	LOSS [training: 0.018886744909069796 | validation: 0.013695208347480906]
	TIME [epoch: 8.24 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015460945558469335		[learning rate: 2.1657e-05]
		[batch 20/20] avg loss: 0.01495536136591038		[learning rate: 2.1618e-05]
	Learning Rate: 2.1618e-05
	LOSS [training: 0.015208153462189858 | validation: 0.007023090640173313]
	TIME [epoch: 8.23 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009886502454734674		[learning rate: 2.1579e-05]
		[batch 20/20] avg loss: 0.013204063131682375		[learning rate: 2.154e-05]
	Learning Rate: 2.15396e-05
	LOSS [training: 0.011545282793208524 | validation: 0.008745172468496144]
	TIME [epoch: 8.24 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018648545247925467		[learning rate: 2.15e-05]
		[batch 20/20] avg loss: 0.009290019438969926		[learning rate: 2.1461e-05]
	Learning Rate: 2.14614e-05
	LOSS [training: 0.013969282343447695 | validation: 0.012189380655509223]
	TIME [epoch: 8.25 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010538451908459003		[learning rate: 2.1422e-05]
		[batch 20/20] avg loss: 0.019812515195919454		[learning rate: 2.1384e-05]
	Learning Rate: 2.13835e-05
	LOSS [training: 0.015175483552189228 | validation: 0.010181615918387227]
	TIME [epoch: 8.24 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015287521941196555		[learning rate: 2.1345e-05]
		[batch 20/20] avg loss: 0.015062049556100313		[learning rate: 2.1306e-05]
	Learning Rate: 2.13059e-05
	LOSS [training: 0.015174785748648435 | validation: 0.0070508051205467725]
	TIME [epoch: 8.23 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01040907410165164		[learning rate: 2.1267e-05]
		[batch 20/20] avg loss: 0.01725167932910123		[learning rate: 2.1229e-05]
	Learning Rate: 2.12286e-05
	LOSS [training: 0.013830376715376434 | validation: 0.008290978602766702]
	TIME [epoch: 8.23 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020979791339912505		[learning rate: 2.119e-05]
		[batch 20/20] avg loss: 0.013129887070930064		[learning rate: 2.1152e-05]
	Learning Rate: 2.11515e-05
	LOSS [training: 0.017054839205421286 | validation: 0.005023820895170679]
	TIME [epoch: 8.24 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01276211796785356		[learning rate: 2.1113e-05]
		[batch 20/20] avg loss: 0.01851135755964985		[learning rate: 2.1075e-05]
	Learning Rate: 2.10748e-05
	LOSS [training: 0.015636737763751702 | validation: 0.005421598283048629]
	TIME [epoch: 8.25 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01240296965261657		[learning rate: 2.1037e-05]
		[batch 20/20] avg loss: 0.011977982686532496		[learning rate: 2.0998e-05]
	Learning Rate: 2.09983e-05
	LOSS [training: 0.012190476169574535 | validation: 0.010371440320682728]
	TIME [epoch: 8.23 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017104178581319057		[learning rate: 2.096e-05]
		[batch 20/20] avg loss: 0.010492159240239602		[learning rate: 2.0922e-05]
	Learning Rate: 2.09221e-05
	LOSS [training: 0.013798168910779332 | validation: 0.011447816275146494]
	TIME [epoch: 8.24 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0156570818801769		[learning rate: 2.0884e-05]
		[batch 20/20] avg loss: 0.00984530178333407		[learning rate: 2.0846e-05]
	Learning Rate: 2.08462e-05
	LOSS [training: 0.012751191831755488 | validation: -0.0004813880869516839]
	TIME [epoch: 8.23 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012386013062933063		[learning rate: 2.0808e-05]
		[batch 20/20] avg loss: 0.01720441603228107		[learning rate: 2.0771e-05]
	Learning Rate: 2.07705e-05
	LOSS [training: 0.014795214547607063 | validation: 0.00395186884474954]
	TIME [epoch: 8.25 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009506723136692919		[learning rate: 2.0733e-05]
		[batch 20/20] avg loss: 0.016166199236579748		[learning rate: 2.0695e-05]
	Learning Rate: 2.06951e-05
	LOSS [training: 0.012836461186636334 | validation: 0.013534959596622353]
	TIME [epoch: 8.24 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013737577153321268		[learning rate: 2.0658e-05]
		[batch 20/20] avg loss: 0.011970344224986183		[learning rate: 2.062e-05]
	Learning Rate: 2.062e-05
	LOSS [training: 0.012853960689153726 | validation: 0.010422358022790517]
	TIME [epoch: 8.23 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012275058476036101		[learning rate: 2.0583e-05]
		[batch 20/20] avg loss: 0.014180097817825085		[learning rate: 2.0545e-05]
	Learning Rate: 2.05452e-05
	LOSS [training: 0.013227578146930593 | validation: 0.00934091167102601]
	TIME [epoch: 8.23 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015813600201319087		[learning rate: 2.0508e-05]
		[batch 20/20] avg loss: 0.011611893639316582		[learning rate: 2.0471e-05]
	Learning Rate: 2.04706e-05
	LOSS [training: 0.013712746920317837 | validation: 0.011112767809528756]
	TIME [epoch: 8.25 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019477057387999108		[learning rate: 2.0433e-05]
		[batch 20/20] avg loss: 0.010186113579550314		[learning rate: 2.0396e-05]
	Learning Rate: 2.03964e-05
	LOSS [training: 0.014831585483774709 | validation: 0.011425483217749517]
	TIME [epoch: 8.23 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014406903252794177		[learning rate: 2.0359e-05]
		[batch 20/20] avg loss: 0.016027360779419166		[learning rate: 2.0322e-05]
	Learning Rate: 2.03223e-05
	LOSS [training: 0.01521713201610667 | validation: 0.006907323199394677]
	TIME [epoch: 8.23 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015716008953330467		[learning rate: 2.0285e-05]
		[batch 20/20] avg loss: 0.013778816973226932		[learning rate: 2.0249e-05]
	Learning Rate: 2.02486e-05
	LOSS [training: 0.0147474129632787 | validation: 0.005873653377121608]
	TIME [epoch: 8.23 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01766497525143387		[learning rate: 2.0212e-05]
		[batch 20/20] avg loss: 0.012359229468599792		[learning rate: 2.0175e-05]
	Learning Rate: 2.01751e-05
	LOSS [training: 0.01501210236001683 | validation: 0.006706828655914259]
	TIME [epoch: 8.23 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012144695091567573		[learning rate: 2.0138e-05]
		[batch 20/20] avg loss: 0.013215419860890578		[learning rate: 2.0102e-05]
	Learning Rate: 2.01019e-05
	LOSS [training: 0.012680057476229079 | validation: 0.010816471676883198]
	TIME [epoch: 8.25 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011011741167489309		[learning rate: 2.0065e-05]
		[batch 20/20] avg loss: 0.01900917898261969		[learning rate: 2.0029e-05]
	Learning Rate: 2.00289e-05
	LOSS [training: 0.015010460075054499 | validation: 0.016255220968382642]
	TIME [epoch: 8.23 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016812121582238628		[learning rate: 1.9993e-05]
		[batch 20/20] avg loss: 0.015082730963990843		[learning rate: 1.9956e-05]
	Learning Rate: 1.99563e-05
	LOSS [training: 0.015947426273114733 | validation: 0.003515034537818086]
	TIME [epoch: 8.26 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014689284908608843		[learning rate: 1.992e-05]
		[batch 20/20] avg loss: 0.013922001292496928		[learning rate: 1.9884e-05]
	Learning Rate: 1.98838e-05
	LOSS [training: 0.014305643100552889 | validation: 0.005666807922988027]
	TIME [epoch: 8.23 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011866493608842987		[learning rate: 1.9848e-05]
		[batch 20/20] avg loss: 0.012728375341778942		[learning rate: 1.9812e-05]
	Learning Rate: 1.98117e-05
	LOSS [training: 0.012297434475310964 | validation: 0.005532980130329991]
	TIME [epoch: 8.24 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014077452119706216		[learning rate: 1.9776e-05]
		[batch 20/20] avg loss: 0.015300741733912434		[learning rate: 1.974e-05]
	Learning Rate: 1.97398e-05
	LOSS [training: 0.014689096926809326 | validation: 0.006439679885955155]
	TIME [epoch: 8.24 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014238339710437461		[learning rate: 1.9704e-05]
		[batch 20/20] avg loss: 0.008377145269849097		[learning rate: 1.9668e-05]
	Learning Rate: 1.96681e-05
	LOSS [training: 0.01130774249014328 | validation: 0.00025249307421134227]
	TIME [epoch: 8.23 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011156139274315458		[learning rate: 1.9632e-05]
		[batch 20/20] avg loss: 0.012933681804042895		[learning rate: 1.9597e-05]
	Learning Rate: 1.95968e-05
	LOSS [training: 0.012044910539179176 | validation: 0.007703277490792612]
	TIME [epoch: 8.23 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009630225718832125		[learning rate: 1.9561e-05]
		[batch 20/20] avg loss: 0.014714676834857287		[learning rate: 1.9526e-05]
	Learning Rate: 1.95256e-05
	LOSS [training: 0.012172451276844706 | validation: 0.013321779302402133]
	TIME [epoch: 8.25 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02098618868589946		[learning rate: 1.949e-05]
		[batch 20/20] avg loss: 0.013884722518604873		[learning rate: 1.9455e-05]
	Learning Rate: 1.94548e-05
	LOSS [training: 0.017435455602252164 | validation: 0.010840570978234812]
	TIME [epoch: 8.25 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015319033007606866		[learning rate: 1.9419e-05]
		[batch 20/20] avg loss: 0.014080505506910656		[learning rate: 1.9384e-05]
	Learning Rate: 1.93842e-05
	LOSS [training: 0.014699769257258763 | validation: 0.003241169709111819]
	TIME [epoch: 8.23 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008605464458377295		[learning rate: 1.9349e-05]
		[batch 20/20] avg loss: 0.01701439655470063		[learning rate: 1.9314e-05]
	Learning Rate: 1.93138e-05
	LOSS [training: 0.012809930506538964 | validation: 0.009880032571200782]
	TIME [epoch: 8.24 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014920421322821379		[learning rate: 1.9279e-05]
		[batch 20/20] avg loss: 0.012087028712557159		[learning rate: 1.9244e-05]
	Learning Rate: 1.92437e-05
	LOSS [training: 0.013503725017689266 | validation: 0.01122738035652462]
	TIME [epoch: 8.24 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012740405029442745		[learning rate: 1.9209e-05]
		[batch 20/20] avg loss: 0.0166528457977056		[learning rate: 1.9174e-05]
	Learning Rate: 1.91739e-05
	LOSS [training: 0.014696625413574171 | validation: 0.007798159493318446]
	TIME [epoch: 8.26 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013201187452697813		[learning rate: 1.9139e-05]
		[batch 20/20] avg loss: 0.014321479416061789		[learning rate: 1.9104e-05]
	Learning Rate: 1.91043e-05
	LOSS [training: 0.0137613334343798 | validation: 0.010430054929324695]
	TIME [epoch: 8.23 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01327874025994168		[learning rate: 1.907e-05]
		[batch 20/20] avg loss: 0.01575399306672322		[learning rate: 1.9035e-05]
	Learning Rate: 1.9035e-05
	LOSS [training: 0.01451636666333245 | validation: -0.0024109172534339867]
	TIME [epoch: 8.24 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006495579267953576		[learning rate: 1.9e-05]
		[batch 20/20] avg loss: 0.014679166594338214		[learning rate: 1.8966e-05]
	Learning Rate: 1.89659e-05
	LOSS [training: 0.010587372931145894 | validation: 0.004919999367907313]
	TIME [epoch: 8.23 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01641043936759927		[learning rate: 1.8931e-05]
		[batch 20/20] avg loss: 0.014962324599592108		[learning rate: 1.8897e-05]
	Learning Rate: 1.88971e-05
	LOSS [training: 0.01568638198359569 | validation: 0.005245066180569888]
	TIME [epoch: 8.26 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013852331935142289		[learning rate: 1.8863e-05]
		[batch 20/20] avg loss: 0.014540481223985468		[learning rate: 1.8829e-05]
	Learning Rate: 1.88285e-05
	LOSS [training: 0.014196406579563878 | validation: 0.007854964650059781]
	TIME [epoch: 8.25 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014037997020022489		[learning rate: 1.8794e-05]
		[batch 20/20] avg loss: 0.01355007183489595		[learning rate: 1.876e-05]
	Learning Rate: 1.87602e-05
	LOSS [training: 0.013794034427459217 | validation: 0.011635955344870705]
	TIME [epoch: 8.25 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016069991158342612		[learning rate: 1.8726e-05]
		[batch 20/20] avg loss: 0.01576475819733876		[learning rate: 1.8692e-05]
	Learning Rate: 1.86921e-05
	LOSS [training: 0.015917374677840688 | validation: 0.01436611272109163]
	TIME [epoch: 8.24 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012622864762617291		[learning rate: 1.8658e-05]
		[batch 20/20] avg loss: 0.015284206784578053		[learning rate: 1.8624e-05]
	Learning Rate: 1.86243e-05
	LOSS [training: 0.01395353577359767 | validation: 0.006090429862685275]
	TIME [epoch: 8.26 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015513839934453233		[learning rate: 1.859e-05]
		[batch 20/20] avg loss: 0.015547394718983806		[learning rate: 1.8557e-05]
	Learning Rate: 1.85567e-05
	LOSS [training: 0.01553061732671852 | validation: 0.011872066494392493]
	TIME [epoch: 8.28 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0063374266428864985		[learning rate: 1.8523e-05]
		[batch 20/20] avg loss: 0.018047913130042437		[learning rate: 1.8489e-05]
	Learning Rate: 1.84893e-05
	LOSS [training: 0.012192669886464468 | validation: 0.00832834144353482]
	TIME [epoch: 8.26 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01490885084230143		[learning rate: 1.8456e-05]
		[batch 20/20] avg loss: 0.015265252851623352		[learning rate: 1.8422e-05]
	Learning Rate: 1.84222e-05
	LOSS [training: 0.015087051846962388 | validation: 0.0103597900732923]
	TIME [epoch: 8.24 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01408153504264128		[learning rate: 1.8389e-05]
		[batch 20/20] avg loss: 0.01074256067209738		[learning rate: 1.8355e-05]
	Learning Rate: 1.83554e-05
	LOSS [training: 0.012412047857369329 | validation: 0.014447217004246359]
	TIME [epoch: 8.25 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006305235656940284		[learning rate: 1.8322e-05]
		[batch 20/20] avg loss: 0.023217171503493776		[learning rate: 1.8289e-05]
	Learning Rate: 1.82888e-05
	LOSS [training: 0.014761203580217028 | validation: 0.012415764223451826]
	TIME [epoch: 8.27 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0126328466473244		[learning rate: 1.8256e-05]
		[batch 20/20] avg loss: 0.013426404182810426		[learning rate: 1.8222e-05]
	Learning Rate: 1.82224e-05
	LOSS [training: 0.01302962541506741 | validation: 0.011745557506754808]
	TIME [epoch: 8.24 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016194460156002476		[learning rate: 1.8189e-05]
		[batch 20/20] avg loss: 0.010328437099384635		[learning rate: 1.8156e-05]
	Learning Rate: 1.81563e-05
	LOSS [training: 0.013261448627693555 | validation: 0.0020302045916832465]
	TIME [epoch: 8.25 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017457483076324564		[learning rate: 1.8123e-05]
		[batch 20/20] avg loss: 0.01576868307405578		[learning rate: 1.809e-05]
	Learning Rate: 1.80904e-05
	LOSS [training: 0.01661308307519017 | validation: 0.010960669912956742]
	TIME [epoch: 8.24 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012949353671143615		[learning rate: 1.8058e-05]
		[batch 20/20] avg loss: 0.013392138886929894		[learning rate: 1.8025e-05]
	Learning Rate: 1.80247e-05
	LOSS [training: 0.013170746279036754 | validation: 0.010106469494667139]
	TIME [epoch: 8.26 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011973060380631353		[learning rate: 1.7992e-05]
		[batch 20/20] avg loss: 0.014941126201517219		[learning rate: 1.7959e-05]
	Learning Rate: 1.79593e-05
	LOSS [training: 0.013457093291074288 | validation: 0.009992366989658616]
	TIME [epoch: 8.26 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01086366312891094		[learning rate: 1.7927e-05]
		[batch 20/20] avg loss: 0.01056197442223582		[learning rate: 1.7894e-05]
	Learning Rate: 1.78941e-05
	LOSS [training: 0.010712818775573379 | validation: 0.007978315460752583]
	TIME [epoch: 8.25 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014299849920101412		[learning rate: 1.7862e-05]
		[batch 20/20] avg loss: 0.015431490573656014		[learning rate: 1.7829e-05]
	Learning Rate: 1.78292e-05
	LOSS [training: 0.014865670246878715 | validation: 0.006604656242051729]
	TIME [epoch: 8.24 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015607662204215587		[learning rate: 1.7797e-05]
		[batch 20/20] avg loss: 0.013289498930612643		[learning rate: 1.7764e-05]
	Learning Rate: 1.77645e-05
	LOSS [training: 0.014448580567414113 | validation: 0.008510049624873336]
	TIME [epoch: 8.24 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01100391262805293		[learning rate: 1.7732e-05]
		[batch 20/20] avg loss: 0.014574044746521445		[learning rate: 1.77e-05]
	Learning Rate: 1.77e-05
	LOSS [training: 0.012788978687287187 | validation: 0.012051310264708306]
	TIME [epoch: 8.26 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009276829333061712		[learning rate: 1.7668e-05]
		[batch 20/20] avg loss: 0.01523410676039474		[learning rate: 1.7636e-05]
	Learning Rate: 1.76358e-05
	LOSS [training: 0.012255468046728224 | validation: 0.0035263614551527605]
	TIME [epoch: 8.24 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010153833319342019		[learning rate: 1.7604e-05]
		[batch 20/20] avg loss: 0.014097207058266947		[learning rate: 1.7572e-05]
	Learning Rate: 1.75718e-05
	LOSS [training: 0.012125520188804483 | validation: 0.014265615026176168]
	TIME [epoch: 8.24 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011226634212316034		[learning rate: 1.754e-05]
		[batch 20/20] avg loss: 0.014392069721883657		[learning rate: 1.7508e-05]
	Learning Rate: 1.7508e-05
	LOSS [training: 0.012809351967099845 | validation: 0.00941674054889244]
	TIME [epoch: 8.24 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014819039632175315		[learning rate: 1.7476e-05]
		[batch 20/20] avg loss: 0.01227089238707425		[learning rate: 1.7444e-05]
	Learning Rate: 1.74445e-05
	LOSS [training: 0.013544966009624784 | validation: 0.0039094577534821056]
	TIME [epoch: 8.27 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01635062625316986		[learning rate: 1.7413e-05]
		[batch 20/20] avg loss: 0.011275073504170717		[learning rate: 1.7381e-05]
	Learning Rate: 1.73812e-05
	LOSS [training: 0.013812849878670288 | validation: 0.012793053090142503]
	TIME [epoch: 8.25 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01758478164312784		[learning rate: 1.735e-05]
		[batch 20/20] avg loss: 0.010903254634944109		[learning rate: 1.7318e-05]
	Learning Rate: 1.73181e-05
	LOSS [training: 0.014244018139035971 | validation: 0.013779429399364534]
	TIME [epoch: 8.24 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01133953366774103		[learning rate: 1.7287e-05]
		[batch 20/20] avg loss: 0.010675911414810276		[learning rate: 1.7255e-05]
	Learning Rate: 1.72552e-05
	LOSS [training: 0.011007722541275649 | validation: 0.007265993318622491]
	TIME [epoch: 8.25 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01257291034210208		[learning rate: 1.7224e-05]
		[batch 20/20] avg loss: 0.015169405601683252		[learning rate: 1.7193e-05]
	Learning Rate: 1.71926e-05
	LOSS [training: 0.013871157971892669 | validation: 0.009868863381571372]
	TIME [epoch: 8.25 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014181097152501928		[learning rate: 1.7161e-05]
		[batch 20/20] avg loss: 0.014027968909864626		[learning rate: 1.713e-05]
	Learning Rate: 1.71302e-05
	LOSS [training: 0.014104533031183277 | validation: 0.002841548161684967]
	TIME [epoch: 8.26 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011868714097088343		[learning rate: 1.7099e-05]
		[batch 20/20] avg loss: 0.012391012034000583		[learning rate: 1.7068e-05]
	Learning Rate: 1.70681e-05
	LOSS [training: 0.01212986306554446 | validation: 0.005774400794906053]
	TIME [epoch: 8.24 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016211596775185556		[learning rate: 1.7037e-05]
		[batch 20/20] avg loss: 0.01728413457698457		[learning rate: 1.7006e-05]
	Learning Rate: 1.70061e-05
	LOSS [training: 0.016747865676085064 | validation: 0.004482368579890874]
	TIME [epoch: 8.25 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010632408697972679		[learning rate: 1.6975e-05]
		[batch 20/20] avg loss: 0.01680314668710477		[learning rate: 1.6944e-05]
	Learning Rate: 1.69444e-05
	LOSS [training: 0.013717777692538721 | validation: 0.0008581015557753251]
	TIME [epoch: 8.24 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0158624933334836		[learning rate: 1.6914e-05]
		[batch 20/20] avg loss: 0.011719062208874982		[learning rate: 1.6883e-05]
	Learning Rate: 1.68829e-05
	LOSS [training: 0.013790777771179291 | validation: 0.009886288390654356]
	TIME [epoch: 8.27 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01085755905896474		[learning rate: 1.6852e-05]
		[batch 20/20] avg loss: 0.014021561363832938		[learning rate: 1.6822e-05]
	Learning Rate: 1.68216e-05
	LOSS [training: 0.012439560211398842 | validation: 0.005974572967262961]
	TIME [epoch: 8.24 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010970505191163622		[learning rate: 1.6791e-05]
		[batch 20/20] avg loss: 0.009333629534671905		[learning rate: 1.6761e-05]
	Learning Rate: 1.67606e-05
	LOSS [training: 0.010152067362917763 | validation: 0.0028666595617424795]
	TIME [epoch: 8.24 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01979269039802862		[learning rate: 1.673e-05]
		[batch 20/20] avg loss: 0.014214182156682336		[learning rate: 1.67e-05]
	Learning Rate: 1.66998e-05
	LOSS [training: 0.01700343627735548 | validation: 0.004726504338952425]
	TIME [epoch: 8.24 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01060037454321566		[learning rate: 1.6669e-05]
		[batch 20/20] avg loss: 0.014612753326481227		[learning rate: 1.6639e-05]
	Learning Rate: 1.66392e-05
	LOSS [training: 0.012606563934848445 | validation: 0.008549501517198452]
	TIME [epoch: 8.25 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011343018991953587		[learning rate: 1.6609e-05]
		[batch 20/20] avg loss: 0.014932811951169267		[learning rate: 1.6579e-05]
	Learning Rate: 1.65788e-05
	LOSS [training: 0.013137915471561429 | validation: 0.006408353600528226]
	TIME [epoch: 8.25 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013971333037099598		[learning rate: 1.6549e-05]
		[batch 20/20] avg loss: 0.007285976915934705		[learning rate: 1.6519e-05]
	Learning Rate: 1.65186e-05
	LOSS [training: 0.010628654976517155 | validation: 0.004143512476992057]
	TIME [epoch: 8.23 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007716402327029281		[learning rate: 1.6489e-05]
		[batch 20/20] avg loss: 0.012275805114319673		[learning rate: 1.6459e-05]
	Learning Rate: 1.64587e-05
	LOSS [training: 0.009996103720674478 | validation: 0.010189656662018598]
	TIME [epoch: 8.24 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012624949103903485		[learning rate: 1.6429e-05]
		[batch 20/20] avg loss: 0.012182911800953837		[learning rate: 1.6399e-05]
	Learning Rate: 1.63989e-05
	LOSS [training: 0.012403930452428661 | validation: 0.009139699754797679]
	TIME [epoch: 8.23 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014964334379244257		[learning rate: 1.6369e-05]
		[batch 20/20] avg loss: 0.009804163095607653		[learning rate: 1.6339e-05]
	Learning Rate: 1.63394e-05
	LOSS [training: 0.012384248737425956 | validation: 0.005982399094919701]
	TIME [epoch: 8.26 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01745121617802226		[learning rate: 1.631e-05]
		[batch 20/20] avg loss: 0.011793261183382985		[learning rate: 1.628e-05]
	Learning Rate: 1.62801e-05
	LOSS [training: 0.014622238680702625 | validation: 0.007738145800505783]
	TIME [epoch: 8.23 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007388519960080666		[learning rate: 1.6251e-05]
		[batch 20/20] avg loss: 0.01267881692769377		[learning rate: 1.6221e-05]
	Learning Rate: 1.62211e-05
	LOSS [training: 0.01003366844388722 | validation: 0.011852133497115827]
	TIME [epoch: 8.25 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014923456563033713		[learning rate: 1.6192e-05]
		[batch 20/20] avg loss: 0.012655485083655337		[learning rate: 1.6162e-05]
	Learning Rate: 1.61622e-05
	LOSS [training: 0.013789470823344526 | validation: 0.006769258062581247]
	TIME [epoch: 8.24 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009066743949622447		[learning rate: 1.6133e-05]
		[batch 20/20] avg loss: 0.01698002239700289		[learning rate: 1.6104e-05]
	Learning Rate: 1.61035e-05
	LOSS [training: 0.01302338317331267 | validation: 0.008043194438589491]
	TIME [epoch: 8.26 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014614205235580376		[learning rate: 1.6074e-05]
		[batch 20/20] avg loss: 0.012991759689382584		[learning rate: 1.6045e-05]
	Learning Rate: 1.60451e-05
	LOSS [training: 0.013802982462481479 | validation: 0.009783476020597013]
	TIME [epoch: 8.23 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011716268283722926		[learning rate: 1.6016e-05]
		[batch 20/20] avg loss: 0.01231885169039756		[learning rate: 1.5987e-05]
	Learning Rate: 1.59869e-05
	LOSS [training: 0.012017559987060245 | validation: 0.0074159165903452135]
	TIME [epoch: 8.23 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007211267268000958		[learning rate: 1.5958e-05]
		[batch 20/20] avg loss: 0.017444545700909864		[learning rate: 1.5929e-05]
	Learning Rate: 1.59288e-05
	LOSS [training: 0.012327906484455411 | validation: 0.01040443206573096]
	TIME [epoch: 8.23 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015181439068927282		[learning rate: 1.59e-05]
		[batch 20/20] avg loss: 0.012521013363211587		[learning rate: 1.5871e-05]
	Learning Rate: 1.5871e-05
	LOSS [training: 0.013851226216069434 | validation: 0.0018985231009266985]
	TIME [epoch: 8.25 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013445506851432823		[learning rate: 1.5842e-05]
		[batch 20/20] avg loss: 0.015754202899152393		[learning rate: 1.5813e-05]
	Learning Rate: 1.58134e-05
	LOSS [training: 0.014599854875292606 | validation: 0.0023437048190084503]
	TIME [epoch: 8.24 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011361309943907117		[learning rate: 1.5785e-05]
		[batch 20/20] avg loss: 0.012212598173686364		[learning rate: 1.5756e-05]
	Learning Rate: 1.57561e-05
	LOSS [training: 0.011786954058796739 | validation: 0.008683176482333806]
	TIME [epoch: 8.23 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017641084818966098		[learning rate: 1.5727e-05]
		[batch 20/20] avg loss: 0.011431636016560668		[learning rate: 1.5699e-05]
	Learning Rate: 1.56989e-05
	LOSS [training: 0.014536360417763383 | validation: 0.009250858734866549]
	TIME [epoch: 8.24 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01203552181639018		[learning rate: 1.567e-05]
		[batch 20/20] avg loss: 0.01337654486543351		[learning rate: 1.5642e-05]
	Learning Rate: 1.56419e-05
	LOSS [training: 0.012706033340911842 | validation: 0.0031911159694719953]
	TIME [epoch: 8.23 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009694596804021042		[learning rate: 1.5613e-05]
		[batch 20/20] avg loss: 0.015150303825792546		[learning rate: 1.5585e-05]
	Learning Rate: 1.55851e-05
	LOSS [training: 0.012422450314906795 | validation: 0.00898804402553591]
	TIME [epoch: 8.25 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012051754010594008		[learning rate: 1.5557e-05]
		[batch 20/20] avg loss: 0.015070514071315846		[learning rate: 1.5529e-05]
	Learning Rate: 1.55286e-05
	LOSS [training: 0.013561134040954928 | validation: 0.009303764664463378]
	TIME [epoch: 8.24 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013419691295469067		[learning rate: 1.55e-05]
		[batch 20/20] avg loss: 0.016035476381676867		[learning rate: 1.5472e-05]
	Learning Rate: 1.54722e-05
	LOSS [training: 0.014727583838572963 | validation: 0.0062502603392993825]
	TIME [epoch: 8.23 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012754034277728185		[learning rate: 1.5444e-05]
		[batch 20/20] avg loss: 0.010512389799021067		[learning rate: 1.5416e-05]
	Learning Rate: 1.54161e-05
	LOSS [training: 0.011633212038374627 | validation: 0.011305385640446227]
	TIME [epoch: 8.23 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007302771131457437		[learning rate: 1.5388e-05]
		[batch 20/20] avg loss: 0.014946530694658322		[learning rate: 1.536e-05]
	Learning Rate: 1.53601e-05
	LOSS [training: 0.011124650913057879 | validation: 0.009833420349240429]
	TIME [epoch: 8.25 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006209092920559182		[learning rate: 1.5332e-05]
		[batch 20/20] avg loss: 0.013447130224641784		[learning rate: 1.5304e-05]
	Learning Rate: 1.53044e-05
	LOSS [training: 0.009828111572600481 | validation: 0.01441873016149486]
	TIME [epoch: 8.23 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009941327142208509		[learning rate: 1.5277e-05]
		[batch 20/20] avg loss: 0.01578905539756809		[learning rate: 1.5249e-05]
	Learning Rate: 1.52488e-05
	LOSS [training: 0.012865191269888296 | validation: 0.00899458203954202]
	TIME [epoch: 8.23 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011051022748847424		[learning rate: 1.5221e-05]
		[batch 20/20] avg loss: 0.016847983686693623		[learning rate: 1.5194e-05]
	Learning Rate: 1.51935e-05
	LOSS [training: 0.013949503217770522 | validation: 0.010971916688693883]
	TIME [epoch: 8.24 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014874079434487356		[learning rate: 1.5166e-05]
		[batch 20/20] avg loss: 0.01668416813596476		[learning rate: 1.5138e-05]
	Learning Rate: 1.51384e-05
	LOSS [training: 0.01577912378522606 | validation: 0.0020452185428572575]
	TIME [epoch: 8.23 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012555085469392237		[learning rate: 1.5111e-05]
		[batch 20/20] avg loss: 0.01560822888229732		[learning rate: 1.5083e-05]
	Learning Rate: 1.50834e-05
	LOSS [training: 0.014081657175844777 | validation: 0.011304158192617065]
	TIME [epoch: 8.26 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012942460685183007		[learning rate: 1.5056e-05]
		[batch 20/20] avg loss: 0.009735985772855001		[learning rate: 1.5029e-05]
	Learning Rate: 1.50287e-05
	LOSS [training: 0.011339223229019008 | validation: 0.008280968142659707]
	TIME [epoch: 8.23 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01537737734234002		[learning rate: 1.5001e-05]
		[batch 20/20] avg loss: 0.009516106774713923		[learning rate: 1.4974e-05]
	Learning Rate: 1.49741e-05
	LOSS [training: 0.012446742058526973 | validation: 0.007306029753070796]
	TIME [epoch: 8.23 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01616822114230649		[learning rate: 1.4947e-05]
		[batch 20/20] avg loss: 0.01647438541345401		[learning rate: 1.492e-05]
	Learning Rate: 1.49198e-05
	LOSS [training: 0.01632130327788025 | validation: 0.0021146122943183983]
	TIME [epoch: 8.23 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01070086321992603		[learning rate: 1.4893e-05]
		[batch 20/20] avg loss: 0.01647557583916854		[learning rate: 1.4866e-05]
	Learning Rate: 1.48657e-05
	LOSS [training: 0.013588219529547285 | validation: 0.0017495991065240176]
	TIME [epoch: 8.25 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013920863133715137		[learning rate: 1.4839e-05]
		[batch 20/20] avg loss: 0.014793518239024115		[learning rate: 1.4812e-05]
	Learning Rate: 1.48117e-05
	LOSS [training: 0.014357190686369625 | validation: 0.006509053735790166]
	TIME [epoch: 8.23 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010352485303738987		[learning rate: 1.4785e-05]
		[batch 20/20] avg loss: 0.012810269600404944		[learning rate: 1.4758e-05]
	Learning Rate: 1.4758e-05
	LOSS [training: 0.011581377452071965 | validation: 0.0075526866328048115]
	TIME [epoch: 8.23 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010679114742844593		[learning rate: 1.4731e-05]
		[batch 20/20] avg loss: 0.011809079619256337		[learning rate: 1.4704e-05]
	Learning Rate: 1.47044e-05
	LOSS [training: 0.011244097181050464 | validation: 0.0010406512784746865]
	TIME [epoch: 8.24 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01340843834973835		[learning rate: 1.4678e-05]
		[batch 20/20] avg loss: 0.006740965147525107		[learning rate: 1.4651e-05]
	Learning Rate: 1.4651e-05
	LOSS [training: 0.01007470174863173 | validation: 0.006801320647838472]
	TIME [epoch: 8.26 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01499269852624765		[learning rate: 1.4624e-05]
		[batch 20/20] avg loss: 0.014876313339612155		[learning rate: 1.4598e-05]
	Learning Rate: 1.45979e-05
	LOSS [training: 0.0149345059329299 | validation: 0.004322415175909161]
	TIME [epoch: 8.24 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01639328264695244		[learning rate: 1.4571e-05]
		[batch 20/20] avg loss: 0.008852287298556211		[learning rate: 1.4545e-05]
	Learning Rate: 1.45449e-05
	LOSS [training: 0.012622784972754328 | validation: 0.007893258164649157]
	TIME [epoch: 8.23 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013341804617913363		[learning rate: 1.4518e-05]
		[batch 20/20] avg loss: 0.012189613499425236		[learning rate: 1.4492e-05]
	Learning Rate: 1.44921e-05
	LOSS [training: 0.0127657090586693 | validation: 0.0059721894708990395]
	TIME [epoch: 8.24 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021609506024617198		[learning rate: 1.4466e-05]
		[batch 20/20] avg loss: 0.008223315351126009		[learning rate: 1.444e-05]
	Learning Rate: 1.44395e-05
	LOSS [training: 0.014916410687871603 | validation: 0.009664033654248807]
	TIME [epoch: 8.23 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015903029283014583		[learning rate: 1.4413e-05]
		[batch 20/20] avg loss: 0.01007607893793983		[learning rate: 1.4387e-05]
	Learning Rate: 1.43871e-05
	LOSS [training: 0.012989554110477208 | validation: 0.010851553825987684]
	TIME [epoch: 8.25 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01344987168004132		[learning rate: 1.4361e-05]
		[batch 20/20] avg loss: 0.017233303461325768		[learning rate: 1.4335e-05]
	Learning Rate: 1.43349e-05
	LOSS [training: 0.015341587570683542 | validation: 0.009449408788282251]
	TIME [epoch: 8.24 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020071970364582276		[learning rate: 1.4309e-05]
		[batch 20/20] avg loss: 0.009215703936833354		[learning rate: 1.4283e-05]
	Learning Rate: 1.42829e-05
	LOSS [training: 0.014643837150707814 | validation: 0.01430120806186609]
	TIME [epoch: 8.23 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01171478763366901		[learning rate: 1.4257e-05]
		[batch 20/20] avg loss: 0.010363617651565585		[learning rate: 1.4231e-05]
	Learning Rate: 1.4231e-05
	LOSS [training: 0.011039202642617298 | validation: 0.00948244542336308]
	TIME [epoch: 8.23 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00969548804306984		[learning rate: 1.4205e-05]
		[batch 20/20] avg loss: 0.014751686022176885		[learning rate: 1.4179e-05]
	Learning Rate: 1.41794e-05
	LOSS [training: 0.012223587032623363 | validation: 0.012281498094069582]
	TIME [epoch: 8.26 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017103669494784347		[learning rate: 1.4154e-05]
		[batch 20/20] avg loss: 0.011554203469653471		[learning rate: 1.4128e-05]
	Learning Rate: 1.41279e-05
	LOSS [training: 0.01432893648221891 | validation: 0.0024633395022854833]
	TIME [epoch: 8.24 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018289719257341013		[learning rate: 1.4102e-05]
		[batch 20/20] avg loss: 0.016116705707713647		[learning rate: 1.4077e-05]
	Learning Rate: 1.40767e-05
	LOSS [training: 0.01720321248252733 | validation: 0.00516187887806802]
	TIME [epoch: 8.23 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009575563441143665		[learning rate: 1.4051e-05]
		[batch 20/20] avg loss: 0.011985463546252736		[learning rate: 1.4026e-05]
	Learning Rate: 1.40256e-05
	LOSS [training: 0.0107805134936982 | validation: 0.006644176974315084]
	TIME [epoch: 8.23 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014100676235854051		[learning rate: 1.4e-05]
		[batch 20/20] avg loss: 0.010770863332806734		[learning rate: 1.3975e-05]
	Learning Rate: 1.39747e-05
	LOSS [training: 0.012435769784330392 | validation: 0.003848489417900343]
	TIME [epoch: 8.25 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015275700242603368		[learning rate: 1.3949e-05]
		[batch 20/20] avg loss: 0.014110820061461799		[learning rate: 1.3924e-05]
	Learning Rate: 1.3924e-05
	LOSS [training: 0.014693260152032584 | validation: 0.00837592883259477]
	TIME [epoch: 8.25 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011803481088994313		[learning rate: 1.3899e-05]
		[batch 20/20] avg loss: 0.012123230471068835		[learning rate: 1.3873e-05]
	Learning Rate: 1.38734e-05
	LOSS [training: 0.011963355780031574 | validation: 0.007116799790440334]
	TIME [epoch: 8.23 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017494756667872532		[learning rate: 1.3848e-05]
		[batch 20/20] avg loss: 0.008986353651763843		[learning rate: 1.3823e-05]
	Learning Rate: 1.38231e-05
	LOSS [training: 0.013240555159818191 | validation: 0.0017620150289129062]
	TIME [epoch: 8.23 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013849354903356892		[learning rate: 1.3798e-05]
		[batch 20/20] avg loss: 0.01299632933960354		[learning rate: 1.3773e-05]
	Learning Rate: 1.37729e-05
	LOSS [training: 0.013422842121480216 | validation: 0.00597575591713704]
	TIME [epoch: 8.24 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01707043183847882		[learning rate: 1.3748e-05]
		[batch 20/20] avg loss: 0.008776899734726796		[learning rate: 1.3723e-05]
	Learning Rate: 1.37229e-05
	LOSS [training: 0.012923665786602808 | validation: 0.00330241842358421]
	TIME [epoch: 8.25 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019961333089720604		[learning rate: 1.3698e-05]
		[batch 20/20] avg loss: 0.01697901036344892		[learning rate: 1.3673e-05]
	Learning Rate: 1.36731e-05
	LOSS [training: 0.01847017172658476 | validation: 0.01124614442533551]
	TIME [epoch: 8.25 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015998117197406674		[learning rate: 1.3648e-05]
		[batch 20/20] avg loss: 0.008460579698199387		[learning rate: 1.3624e-05]
	Learning Rate: 1.36235e-05
	LOSS [training: 0.01222934844780303 | validation: 0.013085095576106128]
	TIME [epoch: 8.25 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011331710024159794		[learning rate: 1.3599e-05]
		[batch 20/20] avg loss: 0.011507026435523291		[learning rate: 1.3574e-05]
	Learning Rate: 1.35741e-05
	LOSS [training: 0.01141936822984154 | validation: 0.007966232620269695]
	TIME [epoch: 8.25 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013610121782349147		[learning rate: 1.3549e-05]
		[batch 20/20] avg loss: 0.014806072870283368		[learning rate: 1.3525e-05]
	Learning Rate: 1.35248e-05
	LOSS [training: 0.014208097326316258 | validation: 0.0036929686164337977]
	TIME [epoch: 8.27 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015061615804156733		[learning rate: 1.35e-05]
		[batch 20/20] avg loss: 0.011312918014702262		[learning rate: 1.3476e-05]
	Learning Rate: 1.34757e-05
	LOSS [training: 0.013187266909429498 | validation: 0.005098889950996635]
	TIME [epoch: 8.24 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01190607903061273		[learning rate: 1.3451e-05]
		[batch 20/20] avg loss: 0.01469261642474318		[learning rate: 1.3427e-05]
	Learning Rate: 1.34268e-05
	LOSS [training: 0.013299347727677954 | validation: 0.006877735807247765]
	TIME [epoch: 8.24 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010507185646030929		[learning rate: 1.3402e-05]
		[batch 20/20] avg loss: 0.016511030564388105		[learning rate: 1.3378e-05]
	Learning Rate: 1.33781e-05
	LOSS [training: 0.013509108105209514 | validation: 0.005409610044603509]
	TIME [epoch: 8.23 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015009151160784861		[learning rate: 1.3354e-05]
		[batch 20/20] avg loss: 0.014789054960073553		[learning rate: 1.333e-05]
	Learning Rate: 1.33296e-05
	LOSS [training: 0.014899103060429204 | validation: 0.010722531362728008]
	TIME [epoch: 8.24 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015612765160668016		[learning rate: 1.3305e-05]
		[batch 20/20] avg loss: 0.011515604544493747		[learning rate: 1.3281e-05]
	Learning Rate: 1.32812e-05
	LOSS [training: 0.013564184852580883 | validation: 0.003556676986023657]
	TIME [epoch: 8.26 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014067380515940664		[learning rate: 1.3257e-05]
		[batch 20/20] avg loss: 0.008465529638143994		[learning rate: 1.3233e-05]
	Learning Rate: 1.3233e-05
	LOSS [training: 0.01126645507704233 | validation: 0.011332842516920152]
	TIME [epoch: 8.24 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01040250149222471		[learning rate: 1.3209e-05]
		[batch 20/20] avg loss: 0.011257782546894841		[learning rate: 1.3185e-05]
	Learning Rate: 1.3185e-05
	LOSS [training: 0.010830142019559777 | validation: 0.00810476882875139]
	TIME [epoch: 8.24 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015747432403019245		[learning rate: 1.3161e-05]
		[batch 20/20] avg loss: 0.00690895016119078		[learning rate: 1.3137e-05]
	Learning Rate: 1.31371e-05
	LOSS [training: 0.011328191282105014 | validation: 0.006272979586074331]
	TIME [epoch: 8.23 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015344334839409205		[learning rate: 1.3113e-05]
		[batch 20/20] avg loss: 0.011979632283192484		[learning rate: 1.3089e-05]
	Learning Rate: 1.30894e-05
	LOSS [training: 0.013661983561300845 | validation: 0.009423650765832604]
	TIME [epoch: 8.27 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011934442285050962		[learning rate: 1.3066e-05]
		[batch 20/20] avg loss: 0.012358902079683489		[learning rate: 1.3042e-05]
	Learning Rate: 1.30419e-05
	LOSS [training: 0.012146672182367224 | validation: 0.012781367080472098]
	TIME [epoch: 8.23 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01301852201260946		[learning rate: 1.3018e-05]
		[batch 20/20] avg loss: 0.014940659607673563		[learning rate: 1.2995e-05]
	Learning Rate: 1.29946e-05
	LOSS [training: 0.01397959081014151 | validation: 0.017320920525938414]
	TIME [epoch: 8.23 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017425634854859307		[learning rate: 1.2971e-05]
		[batch 20/20] avg loss: 0.01266074281196942		[learning rate: 1.2947e-05]
	Learning Rate: 1.29475e-05
	LOSS [training: 0.01504318883341436 | validation: 0.0031901912018720397]
	TIME [epoch: 8.22 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01020430012149045		[learning rate: 1.2924e-05]
		[batch 20/20] avg loss: 0.011834506563034744		[learning rate: 1.29e-05]
	Learning Rate: 1.29005e-05
	LOSS [training: 0.011019403342262599 | validation: 0.0019747329659957815]
	TIME [epoch: 8.24 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014530926234770985		[learning rate: 1.2877e-05]
		[batch 20/20] avg loss: 0.017308932497294458		[learning rate: 1.2854e-05]
	Learning Rate: 1.28536e-05
	LOSS [training: 0.015919929366032724 | validation: 0.007743693456907502]
	TIME [epoch: 8.24 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008847746443471072		[learning rate: 1.283e-05]
		[batch 20/20] avg loss: 0.013844815487493708		[learning rate: 1.2807e-05]
	Learning Rate: 1.2807e-05
	LOSS [training: 0.011346280965482392 | validation: 0.010472605394788878]
	TIME [epoch: 8.24 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01696429149217379		[learning rate: 1.2784e-05]
		[batch 20/20] avg loss: 0.007778366793966938		[learning rate: 1.2761e-05]
	Learning Rate: 1.27605e-05
	LOSS [training: 0.01237132914307036 | validation: 0.007159624524431719]
	TIME [epoch: 8.23 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013873831518944063		[learning rate: 1.2737e-05]
		[batch 20/20] avg loss: 0.010388461570989825		[learning rate: 1.2714e-05]
	Learning Rate: 1.27142e-05
	LOSS [training: 0.012131146544966945 | validation: 0.004192913022951154]
	TIME [epoch: 8.24 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013959922380078307		[learning rate: 1.2691e-05]
		[batch 20/20] avg loss: 0.010639114095087015		[learning rate: 1.2668e-05]
	Learning Rate: 1.26681e-05
	LOSS [training: 0.01229951823758266 | validation: 0.007040321693601999]
	TIME [epoch: 8.26 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0139673611282562		[learning rate: 1.2645e-05]
		[batch 20/20] avg loss: 0.012252736649602		[learning rate: 1.2622e-05]
	Learning Rate: 1.26221e-05
	LOSS [training: 0.013110048888929101 | validation: 0.0012056865326856817]
	TIME [epoch: 8.23 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011404827398841797		[learning rate: 1.2599e-05]
		[batch 20/20] avg loss: 0.017236298930961146		[learning rate: 1.2576e-05]
	Learning Rate: 1.25763e-05
	LOSS [training: 0.014320563164901472 | validation: 0.012813845762871973]
	TIME [epoch: 8.24 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010650689901981671		[learning rate: 1.2553e-05]
		[batch 20/20] avg loss: 0.015909597406542495		[learning rate: 1.2531e-05]
	Learning Rate: 1.25307e-05
	LOSS [training: 0.013280143654262083 | validation: 0.006489585471003261]
	TIME [epoch: 8.24 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014637930448676418		[learning rate: 1.2508e-05]
		[batch 20/20] avg loss: 0.012270575834855751		[learning rate: 1.2485e-05]
	Learning Rate: 1.24852e-05
	LOSS [training: 0.013454253141766084 | validation: 0.011042515180585454]
	TIME [epoch: 8.26 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016409500404315554		[learning rate: 1.2463e-05]
		[batch 20/20] avg loss: 0.012895361940154327		[learning rate: 1.244e-05]
	Learning Rate: 1.24399e-05
	LOSS [training: 0.014652431172234942 | validation: 0.005178536525587285]
	TIME [epoch: 8.24 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014823000837724828		[learning rate: 1.2417e-05]
		[batch 20/20] avg loss: 0.01654678600475331		[learning rate: 1.2395e-05]
	Learning Rate: 1.23947e-05
	LOSS [training: 0.01568489342123907 | validation: 0.009079600552750552]
	TIME [epoch: 8.24 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01178617930074361		[learning rate: 1.2372e-05]
		[batch 20/20] avg loss: 0.013500080972568353		[learning rate: 1.235e-05]
	Learning Rate: 1.23497e-05
	LOSS [training: 0.012643130136655984 | validation: 0.004219123167818981]
	TIME [epoch: 8.24 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016608174116962176		[learning rate: 1.2327e-05]
		[batch 20/20] avg loss: 0.011148517564472793		[learning rate: 1.2305e-05]
	Learning Rate: 1.23049e-05
	LOSS [training: 0.013878345840717485 | validation: 0.005607782213038252]
	TIME [epoch: 8.24 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01620061695090375		[learning rate: 1.2283e-05]
		[batch 20/20] avg loss: 0.011816062808528349		[learning rate: 1.226e-05]
	Learning Rate: 1.22603e-05
	LOSS [training: 0.014008339879716048 | validation: 0.007728764012791175]
	TIME [epoch: 8.25 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012681502841845441		[learning rate: 1.2238e-05]
		[batch 20/20] avg loss: 0.009376416547845757		[learning rate: 1.2216e-05]
	Learning Rate: 1.22158e-05
	LOSS [training: 0.011028959694845599 | validation: -0.0008213180647636909]
	TIME [epoch: 8.24 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014093970065669365		[learning rate: 1.2194e-05]
		[batch 20/20] avg loss: 0.013797628068766787		[learning rate: 1.2171e-05]
	Learning Rate: 1.21714e-05
	LOSS [training: 0.013945799067218077 | validation: 0.0034338402861399664]
	TIME [epoch: 8.23 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008287636879854496		[learning rate: 1.2149e-05]
		[batch 20/20] avg loss: 0.01229590567202799		[learning rate: 1.2127e-05]
	Learning Rate: 1.21273e-05
	LOSS [training: 0.010291771275941244 | validation: 0.004137551871460461]
	TIME [epoch: 8.24 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010310230009059177		[learning rate: 1.2105e-05]
		[batch 20/20] avg loss: 0.00970466441033592		[learning rate: 1.2083e-05]
	Learning Rate: 1.20833e-05
	LOSS [training: 0.01000744720969755 | validation: 0.008175235313251124]
	TIME [epoch: 8.27 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011973541111182661		[learning rate: 1.2061e-05]
		[batch 20/20] avg loss: 0.011956963092269452		[learning rate: 1.2039e-05]
	Learning Rate: 1.20394e-05
	LOSS [training: 0.011965252101726057 | validation: 0.008085008913317747]
	TIME [epoch: 8.24 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008387287522349329		[learning rate: 1.2018e-05]
		[batch 20/20] avg loss: 0.016775661192524025		[learning rate: 1.1996e-05]
	Learning Rate: 1.19957e-05
	LOSS [training: 0.012581474357436678 | validation: 0.00842580064598741]
	TIME [epoch: 8.24 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015746002972878325		[learning rate: 1.1974e-05]
		[batch 20/20] avg loss: 0.01454857577371806		[learning rate: 1.1952e-05]
	Learning Rate: 1.19522e-05
	LOSS [training: 0.015147289373298196 | validation: 0.007035381885015394]
	TIME [epoch: 8.23 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006687500196359622		[learning rate: 1.193e-05]
		[batch 20/20] avg loss: 0.019752463515768714		[learning rate: 1.1909e-05]
	Learning Rate: 1.19088e-05
	LOSS [training: 0.013219981856064166 | validation: 0.01176250394369614]
	TIME [epoch: 8.25 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009960980443214213		[learning rate: 1.1887e-05]
		[batch 20/20] avg loss: 0.016440196357607306		[learning rate: 1.1866e-05]
	Learning Rate: 1.18656e-05
	LOSS [training: 0.01320058840041076 | validation: 0.009545130333154994]
	TIME [epoch: 8.24 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014251787725853166		[learning rate: 1.1844e-05]
		[batch 20/20] avg loss: 0.012846708691833676		[learning rate: 1.1823e-05]
	Learning Rate: 1.18225e-05
	LOSS [training: 0.013549248208843423 | validation: 0.0016035831228252756]
	TIME [epoch: 8.23 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016301666981614324		[learning rate: 1.1801e-05]
		[batch 20/20] avg loss: 0.01000030671332131		[learning rate: 1.178e-05]
	Learning Rate: 1.17796e-05
	LOSS [training: 0.013150986847467816 | validation: 0.0043505806019473]
	TIME [epoch: 8.23 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018056177669953384		[learning rate: 1.1758e-05]
		[batch 20/20] avg loss: 0.009947570470756369		[learning rate: 1.1737e-05]
	Learning Rate: 1.17369e-05
	LOSS [training: 0.014001874070354876 | validation: 0.009247414275900609]
	TIME [epoch: 8.24 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015650232406493547		[learning rate: 1.1716e-05]
		[batch 20/20] avg loss: 0.01192719312313695		[learning rate: 1.1694e-05]
	Learning Rate: 1.16943e-05
	LOSS [training: 0.013788712764815247 | validation: 0.005189512376252087]
	TIME [epoch: 8.26 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013472338897979117		[learning rate: 1.1673e-05]
		[batch 20/20] avg loss: 0.008319596212454611		[learning rate: 1.1652e-05]
	Learning Rate: 1.16518e-05
	LOSS [training: 0.010895967555216863 | validation: 0.010107775466688788]
	TIME [epoch: 8.24 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009109594299638427		[learning rate: 1.1631e-05]
		[batch 20/20] avg loss: 0.01722768659703234		[learning rate: 1.161e-05]
	Learning Rate: 1.16096e-05
	LOSS [training: 0.013168640448335383 | validation: 0.005153862873012792]
	TIME [epoch: 8.23 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007149182901589101		[learning rate: 1.1588e-05]
		[batch 20/20] avg loss: 0.013586526294835612		[learning rate: 1.1567e-05]
	Learning Rate: 1.15674e-05
	LOSS [training: 0.010367854598212356 | validation: 0.010418003414085868]
	TIME [epoch: 8.24 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016338245516247026		[learning rate: 1.1546e-05]
		[batch 20/20] avg loss: 0.008844073643207441		[learning rate: 1.1525e-05]
	Learning Rate: 1.15255e-05
	LOSS [training: 0.012591159579727233 | validation: 0.008261314596846519]
	TIME [epoch: 8.27 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01137754758462234		[learning rate: 1.1505e-05]
		[batch 20/20] avg loss: 0.01380227190607346		[learning rate: 1.1484e-05]
	Learning Rate: 1.14836e-05
	LOSS [training: 0.0125899097453479 | validation: 0.004641588478632388]
	TIME [epoch: 8.24 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014851997425283734		[learning rate: 1.1463e-05]
		[batch 20/20] avg loss: 0.008582466742084096		[learning rate: 1.1442e-05]
	Learning Rate: 1.1442e-05
	LOSS [training: 0.011717232083683913 | validation: 0.005576721098193455]
	TIME [epoch: 8.24 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011334186787976479		[learning rate: 1.1421e-05]
		[batch 20/20] avg loss: 0.017530905864012663		[learning rate: 1.14e-05]
	Learning Rate: 1.14004e-05
	LOSS [training: 0.014432546325994569 | validation: 0.006274257660509642]
	TIME [epoch: 8.24 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016316109496279423		[learning rate: 1.138e-05]
		[batch 20/20] avg loss: 0.013412768233241243		[learning rate: 1.1359e-05]
	Learning Rate: 1.13591e-05
	LOSS [training: 0.014864438864760332 | validation: -0.00034387982288477384]
	TIME [epoch: 8.25 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008424876534800051		[learning rate: 1.1338e-05]
		[batch 20/20] avg loss: 0.016011362783221326		[learning rate: 1.1318e-05]
	Learning Rate: 1.13178e-05
	LOSS [training: 0.01221811965901069 | validation: 0.010471128611947324]
	TIME [epoch: 8.25 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013156105904671		[learning rate: 1.1297e-05]
		[batch 20/20] avg loss: 0.008707535650060192		[learning rate: 1.1277e-05]
	Learning Rate: 1.12768e-05
	LOSS [training: 0.010931820777365594 | validation: 0.0059345449744350786]
	TIME [epoch: 8.24 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013693666807375804		[learning rate: 1.1256e-05]
		[batch 20/20] avg loss: 0.013903146074570402		[learning rate: 1.1236e-05]
	Learning Rate: 1.12358e-05
	LOSS [training: 0.013798406440973101 | validation: 0.005337250297840584]
	TIME [epoch: 8.23 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011483461798961765		[learning rate: 1.1215e-05]
		[batch 20/20] avg loss: 0.010781702907854199		[learning rate: 1.1195e-05]
	Learning Rate: 1.11951e-05
	LOSS [training: 0.011132582353407982 | validation: 0.009999876856347154]
	TIME [epoch: 8.23 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012808895572243231		[learning rate: 1.1175e-05]
		[batch 20/20] avg loss: 0.014756106284078927		[learning rate: 1.1154e-05]
	Learning Rate: 1.11544e-05
	LOSS [training: 0.013782500928161082 | validation: 0.007631013540731329]
	TIME [epoch: 8.26 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01741269854559289		[learning rate: 1.1134e-05]
		[batch 20/20] avg loss: 0.012054614612106513		[learning rate: 1.1114e-05]
	Learning Rate: 1.1114e-05
	LOSS [training: 0.014733656578849699 | validation: 0.010297600933432878]
	TIME [epoch: 8.24 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0100443015525538		[learning rate: 1.1094e-05]
		[batch 20/20] avg loss: 0.013938675284078383		[learning rate: 1.1074e-05]
	Learning Rate: 1.10736e-05
	LOSS [training: 0.011991488418316092 | validation: 0.0036415762193887243]
	TIME [epoch: 8.24 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012074251659514245		[learning rate: 1.1054e-05]
		[batch 20/20] avg loss: 0.011030939092872167		[learning rate: 1.1033e-05]
	Learning Rate: 1.10334e-05
	LOSS [training: 0.011552595376193209 | validation: 0.005102520688990057]
	TIME [epoch: 8.23 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00687151158525884		[learning rate: 1.1013e-05]
		[batch 20/20] avg loss: 0.011484208261101626		[learning rate: 1.0993e-05]
	Learning Rate: 1.09934e-05
	LOSS [training: 0.009177859923180234 | validation: 0.0026450386822519916]
	TIME [epoch: 8.26 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012374147546041582		[learning rate: 1.0973e-05]
		[batch 20/20] avg loss: 0.0092386215592612		[learning rate: 1.0953e-05]
	Learning Rate: 1.09535e-05
	LOSS [training: 0.01080638455265139 | validation: 0.002742707720163141]
	TIME [epoch: 8.24 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011585611455951255		[learning rate: 1.0934e-05]
		[batch 20/20] avg loss: 0.015390318382860155		[learning rate: 1.0914e-05]
	Learning Rate: 1.09137e-05
	LOSS [training: 0.013487964919405706 | validation: 0.010890848136546649]
	TIME [epoch: 8.24 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010617502511243382		[learning rate: 1.0894e-05]
		[batch 20/20] avg loss: 0.0129075965082751		[learning rate: 1.0874e-05]
	Learning Rate: 1.08741e-05
	LOSS [training: 0.011762549509759239 | validation: 0.0026542446437294916]
	TIME [epoch: 8.25 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007359265811349061		[learning rate: 1.0854e-05]
		[batch 20/20] avg loss: 0.013344956410696695		[learning rate: 1.0835e-05]
	Learning Rate: 1.08347e-05
	LOSS [training: 0.010352111111022877 | validation: 0.0020156452762926523]
	TIME [epoch: 8.25 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014763176490211588		[learning rate: 1.0815e-05]
		[batch 20/20] avg loss: 0.01631288047315212		[learning rate: 1.0795e-05]
	Learning Rate: 1.07954e-05
	LOSS [training: 0.015538028481681853 | validation: 0.012723526417826387]
	TIME [epoch: 8.26 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015083827257625495		[learning rate: 1.0776e-05]
		[batch 20/20] avg loss: 0.012195096535941355		[learning rate: 1.0756e-05]
	Learning Rate: 1.07562e-05
	LOSS [training: 0.013639461896783426 | validation: 0.009460173801131224]
	TIME [epoch: 8.24 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010460238019032578		[learning rate: 1.0737e-05]
		[batch 20/20] avg loss: 0.015819102982889528		[learning rate: 1.0717e-05]
	Learning Rate: 1.07171e-05
	LOSS [training: 0.013139670500961053 | validation: 0.0008098792967055723]
	TIME [epoch: 8.25 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01274789067039355		[learning rate: 1.0698e-05]
		[batch 20/20] avg loss: 0.011604531801512975		[learning rate: 1.0678e-05]
	Learning Rate: 1.06782e-05
	LOSS [training: 0.01217621123595326 | validation: 0.006774863079974891]
	TIME [epoch: 8.24 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011243802947935628		[learning rate: 1.0659e-05]
		[batch 20/20] avg loss: 0.014467848122365392		[learning rate: 1.0639e-05]
	Learning Rate: 1.06395e-05
	LOSS [training: 0.012855825535150512 | validation: 0.012651977238330833]
	TIME [epoch: 8.26 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016989881460305688		[learning rate: 1.062e-05]
		[batch 20/20] avg loss: 0.013189123382135848		[learning rate: 1.0601e-05]
	Learning Rate: 1.06009e-05
	LOSS [training: 0.015089502421220768 | validation: 0.007530244861077467]
	TIME [epoch: 8.24 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01562934551595389		[learning rate: 1.0582e-05]
		[batch 20/20] avg loss: 0.006968642964350713		[learning rate: 1.0562e-05]
	Learning Rate: 1.05624e-05
	LOSS [training: 0.011298994240152302 | validation: 0.008554503617148454]
	TIME [epoch: 8.24 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012604681706913376		[learning rate: 1.0543e-05]
		[batch 20/20] avg loss: 0.013825102377256948		[learning rate: 1.0524e-05]
	Learning Rate: 1.05241e-05
	LOSS [training: 0.013214892042085164 | validation: 0.006972808777979925]
	TIME [epoch: 8.23 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009253208844941518		[learning rate: 1.0505e-05]
		[batch 20/20] avg loss: 0.012478694787223148		[learning rate: 1.0486e-05]
	Learning Rate: 1.04859e-05
	LOSS [training: 0.010865951816082334 | validation: 0.003841331117443009]
	TIME [epoch: 8.25 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01351818828402101		[learning rate: 1.0467e-05]
		[batch 20/20] avg loss: 0.012641002180161828		[learning rate: 1.0448e-05]
	Learning Rate: 1.04478e-05
	LOSS [training: 0.013079595232091418 | validation: 0.002353351882314017]
	TIME [epoch: 8.25 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012767288258419347		[learning rate: 1.0429e-05]
		[batch 20/20] avg loss: 0.016016524268282126		[learning rate: 1.041e-05]
	Learning Rate: 1.04099e-05
	LOSS [training: 0.014391906263350737 | validation: 0.0070605712105612254]
	TIME [epoch: 8.26 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013566291046625215		[learning rate: 1.0391e-05]
		[batch 20/20] avg loss: 0.014049649368044892		[learning rate: 1.0372e-05]
	Learning Rate: 1.03721e-05
	LOSS [training: 0.013807970207335051 | validation: 0.005734026059478775]
	TIME [epoch: 8.24 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012717886465239544		[learning rate: 1.0353e-05]
		[batch 20/20] avg loss: 0.014294307739765677		[learning rate: 1.0335e-05]
	Learning Rate: 1.03345e-05
	LOSS [training: 0.013506097102502612 | validation: 0.0063555354268336525]
	TIME [epoch: 8.24 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016066202018314504		[learning rate: 1.0316e-05]
		[batch 20/20] avg loss: 0.011296773841513483		[learning rate: 1.0297e-05]
	Learning Rate: 1.0297e-05
	LOSS [training: 0.013681487929913996 | validation: 0.007854183724389594]
	TIME [epoch: 8.26 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011973314184575291		[learning rate: 1.0278e-05]
		[batch 20/20] avg loss: 0.014763287159561406		[learning rate: 1.026e-05]
	Learning Rate: 1.02596e-05
	LOSS [training: 0.013368300672068345 | validation: 0.010937748675703499]
	TIME [epoch: 8.24 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01161988850962717		[learning rate: 1.0241e-05]
		[batch 20/20] avg loss: 0.0174090052035329		[learning rate: 1.0222e-05]
	Learning Rate: 1.02224e-05
	LOSS [training: 0.014514446856580035 | validation: 0.008448836222152272]
	TIME [epoch: 8.23 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007739391535137636		[learning rate: 1.0204e-05]
		[batch 20/20] avg loss: 0.01449642326739568		[learning rate: 1.0185e-05]
	Learning Rate: 1.01853e-05
	LOSS [training: 0.011117907401266659 | validation: 0.002477497439880269]
	TIME [epoch: 8.23 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019324553858154764		[learning rate: 1.0167e-05]
		[batch 20/20] avg loss: 0.013390586346858514		[learning rate: 1.0148e-05]
	Learning Rate: 1.01483e-05
	LOSS [training: 0.016357570102506637 | validation: 0.009159219274889309]
	TIME [epoch: 8.25 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013383766284417401		[learning rate: 1.013e-05]
		[batch 20/20] avg loss: 0.015030888127245725		[learning rate: 1.0112e-05]
	Learning Rate: 1.01115e-05
	LOSS [training: 0.014207327205831562 | validation: 0.007204072987102272]
	TIME [epoch: 8.25 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00997876063845879		[learning rate: 1.0093e-05]
		[batch 20/20] avg loss: 0.011922380505413117		[learning rate: 1.0075e-05]
	Learning Rate: 1.00748e-05
	LOSS [training: 0.010950570571935953 | validation: 0.0047157316823985135]
	TIME [epoch: 8.25 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008012522702317075		[learning rate: 1.0057e-05]
		[batch 20/20] avg loss: 0.018347666610694296		[learning rate: 1.0038e-05]
	Learning Rate: 1.00382e-05
	LOSS [training: 0.013180094656505683 | validation: 0.009322701529598122]
	TIME [epoch: 8.25 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013804354850191467		[learning rate: 1.002e-05]
		[batch 20/20] avg loss: 0.011559819787228795		[learning rate: 1.0002e-05]
	Learning Rate: 1.00018e-05
	LOSS [training: 0.012682087318710131 | validation: -0.0011476259438634824]
	TIME [epoch: 8.25 sec]
Finished training in 16761.492 seconds.
