Args:
Namespace(name='model_tr_study202', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3', training_data='data/transition_rate_studies/tr_study202/tr_study202_training/r3', validation_data='data/transition_rate_studies/tr_study202/tr_study202_validation/r3', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 344132386

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.706313794414346		[learning rate: 0.01]
		[batch 20/20] avg loss: 9.625877861495722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.166095827955036 | validation: 9.286434268010247]
	TIME [epoch: 77.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.8761881476532		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.111966232672177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.494077190162688 | validation: 8.71776323278447]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.8217698324803475		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.572601137968651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.697185485224499 | validation: 7.147714927116004]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.072244523296602		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.883406014453536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.977825268875068 | validation: 7.845595897969945]
	TIME [epoch: 8.31 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.985236971142221		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.700972709860222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.843104840501221 | validation: 8.012366567864346]
	TIME [epoch: 8.37 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.803489463484505		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.777741662214235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.79061556284937 | validation: 6.633279925372194]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.557961943432815		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.595700442770093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.576831193101453 | validation: 6.463299760901223]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.477214557179916		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.393923502177334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.435569029678625 | validation: 6.931678857749912]
	TIME [epoch: 8.3 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.1783273379788834		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.6223067487231395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.900317043351012 | validation: 4.6886424067636]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.616390493872727		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.488411758141393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5524011260070605 | validation: 4.38836597459098]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.533021971683704		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.379786798033441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.456404384858572 | validation: 4.835120857427121]
	TIME [epoch: 8.31 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.34730653165561		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.308697737339107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.328002134497358 | validation: 4.322267328543618]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.206191584971007		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.055366790565607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.130779187768307 | validation: 4.603685237822004]
	TIME [epoch: 8.36 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.25831098540038		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.954585136746869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.106448061073624 | validation: 4.567515948397127]
	TIME [epoch: 8.33 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.104165134876161		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.064851315888688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.084508225382424 | validation: 4.125783568919935]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.747561026817665		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.141483594991394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9445223109045293 | validation: 4.686880998200756]
	TIME [epoch: 8.34 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.036933379112139		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.7512050321050205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.89406920560858 | validation: 4.374170002239784]
	TIME [epoch: 8.32 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.876852252021668		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8916504280203412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.884251340021006 | validation: 4.16979666120008]
	TIME [epoch: 8.32 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.6836093771027123		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8449341417740697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7642717594383903 | validation: 4.469063418046465]
	TIME [epoch: 8.31 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8407548139465177		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8066471740191226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8237009939828193 | validation: 4.561176644566929]
	TIME [epoch: 8.34 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.657786904423582		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.9854011924378554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.821594048430719 | validation: 4.2397453051374825]
	TIME [epoch: 8.3 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.83475772259004		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.639831218122842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.737294470356441 | validation: 3.7642065765258685]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.6875400500882263		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.6249332039937783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6562366270410025 | validation: 3.998783290689289]
	TIME [epoch: 8.3 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.6863329318152926		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.486171123504275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5862520276597842 | validation: 3.6704442589092534]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.542998982793231		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.583454470610577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5632267267019033 | validation: 3.8892349427278763]
	TIME [epoch: 8.29 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.542381323397163		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.638248469375828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.590314896386495 | validation: 4.043612689353641]
	TIME [epoch: 8.32 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.490231995770275		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.634574055446874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5624030256085746 | validation: 3.8678555895852993]
	TIME [epoch: 8.31 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.442296737905707		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5289066859388547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4856017119222806 | validation: 3.903682183554564]
	TIME [epoch: 8.31 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3876469954230557		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.430284297291828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.408965646357442 | validation: 4.1162526359467755]
	TIME [epoch: 8.28 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.239149063644541		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5052188603676937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3721839620061167 | validation: 4.307455550855143]
	TIME [epoch: 8.3 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3562821061923045		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.3138073442191156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.33504472520571 | validation: 3.9931191191136755]
	TIME [epoch: 8.31 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.468611827760559		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1203417788965977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2944768033285783 | validation: 4.0240993961566165]
	TIME [epoch: 8.31 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.214704686400375		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.375971714379234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2953382003898044 | validation: 3.7063049429394033]
	TIME [epoch: 8.29 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.321793473691187		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.3277470967548766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.324770285223031 | validation: 3.898903467171385]
	TIME [epoch: 8.28 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2473307048371964		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.4327451929742585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.340037948905727 | validation: 3.4796785454545462]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.19825850722011		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.4217659292220297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3100122182210705 | validation: 3.77453626428464]
	TIME [epoch: 8.3 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2109456387122477		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.3534991518768202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2822223952945335 | validation: 3.6195686523661084]
	TIME [epoch: 8.29 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.249260972233678		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1429906961299925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.196125834181835 | validation: 3.5562086480954243]
	TIME [epoch: 8.28 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5822678165179296		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.148896800408772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.365582308463351 | validation: 3.836558102564303]
	TIME [epoch: 8.35 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.282117998506775		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0229633681818315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.152540683344303 | validation: 3.6788068185812373]
	TIME [epoch: 8.29 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2162779463531983		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.06092195716133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1385999517572643 | validation: 3.368378367890178]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.189085276902289		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.177204426120288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.183144851511288 | validation: 3.5796420427881213]
	TIME [epoch: 8.29 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.206554240226788		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.99777882174487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1021665309858295 | validation: 3.4427530884267368]
	TIME [epoch: 8.34 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.177937849754231		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1244986264201042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.151218238087168 | validation: 3.5969455986419034]
	TIME [epoch: 8.3 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.097051095068578		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0210997314196937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0590754132441362 | validation: 3.5341436930080787]
	TIME [epoch: 8.29 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8966302616169575		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0349117964921635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9657710290545602 | validation: 3.327531974789636]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0336723628329283		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.078876727415016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0562745451239723 | validation: 3.9751725972724112]
	TIME [epoch: 8.32 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0533608498457903		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.880940881120478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9671508654831342 | validation: 3.721502250165345]
	TIME [epoch: 8.31 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.048535275911738		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8124047093787654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9304699926452513 | validation: 3.3221789108388]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9563790052103185		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0229193028531123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.989649154031716 | validation: 3.7797942417442782]
	TIME [epoch: 8.3 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.997638883386602		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1127730051064746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0552059442465382 | validation: 3.317096964732807]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8807215687922207		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.103819692679587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.992270630735904 | validation: 3.859244206172987]
	TIME [epoch: 8.32 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0758550292950404		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.931649698764256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0037523640296473 | validation: 3.4547209810409343]
	TIME [epoch: 8.29 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.983619818240503		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9256458136772214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9546328159588624 | validation: 3.4909733088776767]
	TIME [epoch: 8.32 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0505476566382006		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8849011808503158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9677244187442584 | validation: 3.8005718732977884]
	TIME [epoch: 8.3 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8206021083191666		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0230150959841287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9218086021516476 | validation: 3.5354765027890576]
	TIME [epoch: 8.29 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0575241938026516		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.776877085255546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9172006395290992 | validation: 3.2373987917387743]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.86207519031945		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0378302296354827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9499527099774663 | validation: 3.369650361811198]
	TIME [epoch: 8.33 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.15571179045771		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.883529268704144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.019620529580927 | validation: 3.6106095646511664]
	TIME [epoch: 8.29 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8460980250367838		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.3377899450430037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0919439850398933 | validation: 6.281797261153012]
	TIME [epoch: 8.29 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.552333921267401		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9119197243784125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.232126822822907 | validation: 3.681185101549393]
	TIME [epoch: 8.31 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.798840816695164		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0036358047125025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.901238310703833 | validation: 3.9634575238884153]
	TIME [epoch: 8.32 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.893428860615989		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0575755484555827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9755022045357857 | validation: 3.20251445130914]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.037302191709153		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8485415479756453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9429218698423987 | validation: 3.5139017535766994]
	TIME [epoch: 8.29 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8888549279132167		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.922759666955158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.905807297434187 | validation: 3.131223522024704]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.810034058428888		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.713317545294385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7616758018616365 | validation: 3.5447839340740885]
	TIME [epoch: 8.31 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.634875473925925		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8332607985783733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7340681362521497 | validation: 3.2551140789858417]
	TIME [epoch: 8.29 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.73054033060416		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.694531301773395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7125358161887774 | validation: 3.1284101821365224]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.694192644380433		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7707612193487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.732476931864567 | validation: 3.076460589630257]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5353420835037452		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.925105138694589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7302236110991673 | validation: 3.482557467209646]
	TIME [epoch: 8.33 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.635910575859959		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.882661903452388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7592862396561735 | validation: 3.090624717180944]
	TIME [epoch: 8.32 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7953667268201565		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.595457046412066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6954118866161108 | validation: 3.1489791180386595]
	TIME [epoch: 8.31 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6816555839793352		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5469822545647283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6143189192720317 | validation: 3.557808254896042]
	TIME [epoch: 8.34 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.888962563821933		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.592345874462105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.740654219142019 | validation: 3.245372250311198]
	TIME [epoch: 8.34 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.670020785446203		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8146774739313107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.742349129688758 | validation: 3.153658761021626]
	TIME [epoch: 8.31 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.598266524345406		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0545789735465716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8264227489459883 | validation: 3.1693279440621263]
	TIME [epoch: 8.31 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.512092528928739		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.682460185794104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5972763573614217 | validation: 3.0679192780580005]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.513975684894606		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5979344525560393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5559550687253223 | validation: 3.876009324819993]
	TIME [epoch: 8.33 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.546805340200234		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6052104445610746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.576007892380655 | validation: 2.9699502309483794]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.558285777516587		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.576780063522415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5675329205195014 | validation: 2.9190903420182392]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.65436549541274		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5115885014763877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.582976998444564 | validation: 2.8849979072125578]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.477793789941126		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.145363012029123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.311578400985124 | validation: 2.9554630061738947]
	TIME [epoch: 8.32 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9465858151374982		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1359266586925783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0412562369150375 | validation: 2.1121571782772737]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8199238947887886		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6459236675021809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7329237811454845 | validation: 0.6797919563930818]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.192664198805979		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8962932236735929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0444787112397862 | validation: 0.510021623322615]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7451941896035924		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6466235696678929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6959088796357425 | validation: 0.48686847066749694]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.611756373634081		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9172824971918528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7645194354129667 | validation: 0.6963916977384708]
	TIME [epoch: 8.3 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7372578475943765		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.638071397492191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6876646225432836 | validation: 0.6099757510320845]
	TIME [epoch: 8.32 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7153171319594639		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.587229417680644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6512732748200538 | validation: 1.1071479486714089]
	TIME [epoch: 8.29 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7140833003715369		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7198966141882066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7169899572798718 | validation: 0.46499839155922507]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6287381863879993		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8553057369368352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7420219616624172 | validation: 0.6200831312780339]
	TIME [epoch: 8.34 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8520814228805269		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5711504764046815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7116159496426043 | validation: 1.262665472536776]
	TIME [epoch: 8.31 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7042915455724906		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6080379969528555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.656164771262673 | validation: 0.888434583095989]
	TIME [epoch: 8.27 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6762257582779395		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5878385990213383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6320321786496389 | validation: 0.3757146366246066]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0483708844995607		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5833314898931365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8158511871963485 | validation: 0.6835363673242558]
	TIME [epoch: 8.34 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4971152887180499		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6336857562163126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5654005224671812 | validation: 0.2843206493009691]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6583416863690786		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6747142736007293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.666527979984904 | validation: 0.41958371079548495]
	TIME [epoch: 8.32 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5660411025748259		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5809240301134804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.573482566344153 | validation: 0.505668325201875]
	TIME [epoch: 8.32 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6313885314926712		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5397378546900824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5855631930913768 | validation: 0.2548784113968855]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6833118377785654		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6280687357854099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6556902867819877 | validation: 0.30116753672573926]
	TIME [epoch: 8.33 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5816624802926567		[learning rate: 0.0099837]
		[batch 20/20] avg loss: 0.5788223543826015		[learning rate: 0.0099655]
	Learning Rate: 0.00996552
	LOSS [training: 0.5802424173376293 | validation: 0.3514469648259739]
	TIME [epoch: 8.31 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.712988084181759		[learning rate: 0.0099474]
		[batch 20/20] avg loss: 1.234564386625944		[learning rate: 0.0099294]
	Learning Rate: 0.00992935
	LOSS [training: 0.9737762354038514 | validation: 1.3583612381625758]
	TIME [epoch: 8.3 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6736155538075448		[learning rate: 0.0099113]
		[batch 20/20] avg loss: 0.8218937656667695		[learning rate: 0.0098933]
	Learning Rate: 0.00989332
	LOSS [training: 0.7477546597371573 | validation: 0.5369564164734404]
	TIME [epoch: 8.36 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4963203628618843		[learning rate: 0.0098754]
		[batch 20/20] avg loss: 0.5584990726715457		[learning rate: 0.0098574]
	Learning Rate: 0.00985742
	LOSS [training: 0.5274097177667151 | validation: 0.35578752491445326]
	TIME [epoch: 8.32 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7008616355327774		[learning rate: 0.0098395]
		[batch 20/20] avg loss: 0.5771250244833568		[learning rate: 0.0098216]
	Learning Rate: 0.00982164
	LOSS [training: 0.638993330008067 | validation: 0.44579628563506096]
	TIME [epoch: 8.31 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5023561200484377		[learning rate: 0.0098038]
		[batch 20/20] avg loss: 0.5107910151264263		[learning rate: 0.009786]
	Learning Rate: 0.009786
	LOSS [training: 0.506573567587432 | validation: 0.5640724915031878]
	TIME [epoch: 8.31 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2967884807731598		[learning rate: 0.0097682]
		[batch 20/20] avg loss: 0.5437131038682208		[learning rate: 0.0097505]
	Learning Rate: 0.00975049
	LOSS [training: 0.9202507923206902 | validation: 0.4898437462878231]
	TIME [epoch: 8.35 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5038310640762328		[learning rate: 0.0097328]
		[batch 20/20] avg loss: 0.5688559180865568		[learning rate: 0.0097151]
	Learning Rate: 0.0097151
	LOSS [training: 0.5363434910813947 | validation: 1.3275564260961024]
	TIME [epoch: 8.34 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8952881363680024		[learning rate: 0.0096975]
		[batch 20/20] avg loss: 0.5998656515453474		[learning rate: 0.0096798]
	Learning Rate: 0.00967984
	LOSS [training: 0.7475768939566748 | validation: 0.8337000742106953]
	TIME [epoch: 8.32 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.557015174487203		[learning rate: 0.0096623]
		[batch 20/20] avg loss: 0.49496931758961915		[learning rate: 0.0096447]
	Learning Rate: 0.00964472
	LOSS [training: 0.5259922460384111 | validation: 0.2984184616116858]
	TIME [epoch: 8.32 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5774854933982232		[learning rate: 0.0096272]
		[batch 20/20] avg loss: 0.5055035865134587		[learning rate: 0.0096097]
	Learning Rate: 0.00960972
	LOSS [training: 0.5414945399558408 | validation: 0.6400635089111304]
	TIME [epoch: 8.34 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5850436919747303		[learning rate: 0.0095923]
		[batch 20/20] avg loss: 0.5657779295028901		[learning rate: 0.0095748]
	Learning Rate: 0.00957484
	LOSS [training: 0.5754108107388102 | validation: 0.540119144391001]
	TIME [epoch: 8.33 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5915491952002507		[learning rate: 0.0095575]
		[batch 20/20] avg loss: 0.597692442578759		[learning rate: 0.0095401]
	Learning Rate: 0.00954009
	LOSS [training: 0.594620818889505 | validation: 0.28921713226770845]
	TIME [epoch: 8.31 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44542843800850235		[learning rate: 0.0095228]
		[batch 20/20] avg loss: 0.5974479364285588		[learning rate: 0.0095055]
	Learning Rate: 0.00950547
	LOSS [training: 0.5214381872185305 | validation: 1.6585499681808904]
	TIME [epoch: 8.33 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6611564299450523		[learning rate: 0.0094882]
		[batch 20/20] avg loss: 0.8862257089915957		[learning rate: 0.009471]
	Learning Rate: 0.00947098
	LOSS [training: 0.7736910694683239 | validation: 0.27151526885233024]
	TIME [epoch: 8.33 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6425033110737307		[learning rate: 0.0094538]
		[batch 20/20] avg loss: 0.5014102142579219		[learning rate: 0.0094366]
	Learning Rate: 0.0094366
	LOSS [training: 0.5719567626658263 | validation: 0.42655935254919375]
	TIME [epoch: 8.32 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6580599883551694		[learning rate: 0.0094195]
		[batch 20/20] avg loss: 0.5914794334525786		[learning rate: 0.0094024]
	Learning Rate: 0.00940236
	LOSS [training: 0.624769710903874 | validation: 0.5957863514037809]
	TIME [epoch: 8.32 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41662625458181307		[learning rate: 0.0093853]
		[batch 20/20] avg loss: 0.5660713249586573		[learning rate: 0.0093682]
	Learning Rate: 0.00936824
	LOSS [training: 0.4913487897702352 | validation: 0.36089796142467345]
	TIME [epoch: 8.33 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5624145912194394		[learning rate: 0.0093512]
		[batch 20/20] avg loss: 0.8141889345160033		[learning rate: 0.0093342]
	Learning Rate: 0.00933424
	LOSS [training: 0.6883017628677214 | validation: 0.8157159309911549]
	TIME [epoch: 8.32 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5126732376543298		[learning rate: 0.0093173]
		[batch 20/20] avg loss: 0.5228050533284453		[learning rate: 0.0093004]
	Learning Rate: 0.00930036
	LOSS [training: 0.5177391454913876 | validation: 0.9630212140926404]
	TIME [epoch: 8.31 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6292452215190105		[learning rate: 0.0092835]
		[batch 20/20] avg loss: 1.1535746596298249		[learning rate: 0.0092666]
	Learning Rate: 0.00926661
	LOSS [training: 0.8914099405744176 | validation: 1.259878097532229]
	TIME [epoch: 8.34 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7610459287419926		[learning rate: 0.0092498]
		[batch 20/20] avg loss: 0.5708119064280366		[learning rate: 0.009233]
	Learning Rate: 0.00923298
	LOSS [training: 0.6659289175850146 | validation: 0.2535748206346158]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5510220194564849		[learning rate: 0.0092162]
		[batch 20/20] avg loss: 0.6756267277446003		[learning rate: 0.0091995]
	Learning Rate: 0.00919948
	LOSS [training: 0.6133243736005426 | validation: 0.5464802515604396]
	TIME [epoch: 8.32 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4163655346493361		[learning rate: 0.0091828]
		[batch 20/20] avg loss: 0.4670013622318936		[learning rate: 0.0091661]
	Learning Rate: 0.00916609
	LOSS [training: 0.44168344844061486 | validation: 0.3370977577605094]
	TIME [epoch: 8.3 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6013284992352972		[learning rate: 0.0091494]
		[batch 20/20] avg loss: 0.580175088808639		[learning rate: 0.0091328]
	Learning Rate: 0.00913283
	LOSS [training: 0.590751794021968 | validation: 0.3524957037333759]
	TIME [epoch: 8.33 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5721995757150534		[learning rate: 0.0091162]
		[batch 20/20] avg loss: 0.6351436589418323		[learning rate: 0.0090997]
	Learning Rate: 0.00909968
	LOSS [training: 0.6036716173284429 | validation: 0.7650258996450583]
	TIME [epoch: 8.35 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6227130056636371		[learning rate: 0.0090832]
		[batch 20/20] avg loss: 0.5657111740039092		[learning rate: 0.0090667]
	Learning Rate: 0.00906666
	LOSS [training: 0.594212089833773 | validation: 0.525201040865032]
	TIME [epoch: 8.3 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7654600580116253		[learning rate: 0.0090502]
		[batch 20/20] avg loss: 0.5702476477334659		[learning rate: 0.0090338]
	Learning Rate: 0.00903376
	LOSS [training: 0.6678538528725455 | validation: 0.3654427848143324]
	TIME [epoch: 8.31 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6023299476025376		[learning rate: 0.0090174]
		[batch 20/20] avg loss: 0.5602989028567972		[learning rate: 0.009001]
	Learning Rate: 0.00900097
	LOSS [training: 0.5813144252296675 | validation: 0.36063657791025805]
	TIME [epoch: 8.31 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.447147397004247		[learning rate: 0.0089846]
		[batch 20/20] avg loss: 0.7195075870184736		[learning rate: 0.0089683]
	Learning Rate: 0.00896831
	LOSS [training: 0.5833274920113604 | validation: 0.29443162256164346]
	TIME [epoch: 8.36 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41653878323645355		[learning rate: 0.008952]
		[batch 20/20] avg loss: 0.5777076306264755		[learning rate: 0.0089358]
	Learning Rate: 0.00893576
	LOSS [training: 0.49712320693146445 | validation: 0.8308452484491404]
	TIME [epoch: 8.3 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6345824423832344		[learning rate: 0.0089195]
		[batch 20/20] avg loss: 0.6092348361867838		[learning rate: 0.0089033]
	Learning Rate: 0.00890333
	LOSS [training: 0.6219086392850092 | validation: 0.801579586950242]
	TIME [epoch: 8.3 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5824737444865045		[learning rate: 0.0088872]
		[batch 20/20] avg loss: 0.7143099049027065		[learning rate: 0.008871]
	Learning Rate: 0.00887102
	LOSS [training: 0.6483918246946054 | validation: 0.4057850541075052]
	TIME [epoch: 8.32 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5299990330623523		[learning rate: 0.0088549]
		[batch 20/20] avg loss: 0.5624029842087963		[learning rate: 0.0088388]
	Learning Rate: 0.00883883
	LOSS [training: 0.5462010086355743 | validation: 0.3358525519617894]
	TIME [epoch: 8.35 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4501532953648798		[learning rate: 0.0088228]
		[batch 20/20] avg loss: 0.4658959687611174		[learning rate: 0.0088068]
	Learning Rate: 0.00880675
	LOSS [training: 0.45802463206299854 | validation: 0.3810116762723693]
	TIME [epoch: 8.31 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46344383111679965		[learning rate: 0.0087908]
		[batch 20/20] avg loss: 0.4092720221904109		[learning rate: 0.0087748]
	Learning Rate: 0.00877479
	LOSS [training: 0.43635792665360523 | validation: 0.2889306872326568]
	TIME [epoch: 8.31 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4353297462189475		[learning rate: 0.0087589]
		[batch 20/20] avg loss: 0.46411681291742574		[learning rate: 0.0087429]
	Learning Rate: 0.00874295
	LOSS [training: 0.4497232795681866 | validation: 0.3304508037569205]
	TIME [epoch: 8.32 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5677313048325523		[learning rate: 0.0087271]
		[batch 20/20] avg loss: 0.5498113078510836		[learning rate: 0.0087112]
	Learning Rate: 0.00871122
	LOSS [training: 0.558771306341818 | validation: 0.5663436207702356]
	TIME [epoch: 8.35 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5978189005055469		[learning rate: 0.0086954]
		[batch 20/20] avg loss: 0.4097035371769862		[learning rate: 0.0086796]
	Learning Rate: 0.00867961
	LOSS [training: 0.5037612188412666 | validation: 0.21718130282294137]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5329069968347373		[learning rate: 0.0086638]
		[batch 20/20] avg loss: 0.5474761429573192		[learning rate: 0.0086481]
	Learning Rate: 0.00864811
	LOSS [training: 0.5401915698960281 | validation: 1.1495947109407079]
	TIME [epoch: 8.3 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.633138411263072		[learning rate: 0.0086324]
		[batch 20/20] avg loss: 0.5872882479017814		[learning rate: 0.0086167]
	Learning Rate: 0.00861672
	LOSS [training: 0.6102133295824267 | validation: 0.2837150772268984]
	TIME [epoch: 8.32 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.412825269194494		[learning rate: 0.0086011]
		[batch 20/20] avg loss: 0.36311615283109216		[learning rate: 0.0085855]
	Learning Rate: 0.00858545
	LOSS [training: 0.3879707110127931 | validation: 0.3220141422971843]
	TIME [epoch: 8.33 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5405321664841225		[learning rate: 0.0085699]
		[batch 20/20] avg loss: 0.4409338973354139		[learning rate: 0.0085543]
	Learning Rate: 0.00855429
	LOSS [training: 0.49073303190976814 | validation: 0.45356652541080045]
	TIME [epoch: 8.31 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4536166720779085		[learning rate: 0.0085388]
		[batch 20/20] avg loss: 0.44507949099036004		[learning rate: 0.0085232]
	Learning Rate: 0.00852325
	LOSS [training: 0.4493480815341343 | validation: 0.16384698495365715]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48350360017068894		[learning rate: 0.0085078]
		[batch 20/20] avg loss: 0.5446288949806581		[learning rate: 0.0084923]
	Learning Rate: 0.00849232
	LOSS [training: 0.5140662475756734 | validation: 0.6364660273748566]
	TIME [epoch: 8.33 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5643539621551247		[learning rate: 0.0084769]
		[batch 20/20] avg loss: 0.5061072967725286		[learning rate: 0.0084615]
	Learning Rate: 0.0084615
	LOSS [training: 0.5352306294638266 | validation: 0.8237078162723382]
	TIME [epoch: 8.3 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4853216053099635		[learning rate: 0.0084461]
		[batch 20/20] avg loss: 0.5987515140397135		[learning rate: 0.0084308]
	Learning Rate: 0.00843079
	LOSS [training: 0.5420365596748383 | validation: 0.3441113167206413]
	TIME [epoch: 8.32 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4164849089454691		[learning rate: 0.0084155]
		[batch 20/20] avg loss: 0.4884943657074518		[learning rate: 0.0084002]
	Learning Rate: 0.0084002
	LOSS [training: 0.4524896373264604 | validation: 0.45956656197257384]
	TIME [epoch: 8.29 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.557385681998133		[learning rate: 0.0083849]
		[batch 20/20] avg loss: 0.34408339513550257		[learning rate: 0.0083697]
	Learning Rate: 0.00836971
	LOSS [training: 0.4507345385668177 | validation: 0.5815839047413917]
	TIME [epoch: 8.33 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6164943062704163		[learning rate: 0.0083545]
		[batch 20/20] avg loss: 0.4822294382998593		[learning rate: 0.0083393]
	Learning Rate: 0.00833934
	LOSS [training: 0.5493618722851379 | validation: 0.28917943850794836]
	TIME [epoch: 8.29 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6887196798052428		[learning rate: 0.0083242]
		[batch 20/20] avg loss: 0.544439750458356		[learning rate: 0.0083091]
	Learning Rate: 0.00830907
	LOSS [training: 0.6165797151317995 | validation: 0.2148563159518011]
	TIME [epoch: 8.32 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3850956014478025		[learning rate: 0.008294]
		[batch 20/20] avg loss: 0.4164644199369542		[learning rate: 0.0082789]
	Learning Rate: 0.00827892
	LOSS [training: 0.40078001069237834 | validation: 0.2860102458968233]
	TIME [epoch: 8.29 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.596461543812153		[learning rate: 0.0082639]
		[batch 20/20] avg loss: 0.8491980058577449		[learning rate: 0.0082489]
	Learning Rate: 0.00824887
	LOSS [training: 0.7228297748349489 | validation: 1.2084547940419812]
	TIME [epoch: 8.33 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6878107742252042		[learning rate: 0.0082339]
		[batch 20/20] avg loss: 0.6166199085814874		[learning rate: 0.0082189]
	Learning Rate: 0.00821894
	LOSS [training: 0.6522153414033458 | validation: 0.2962265197468042]
	TIME [epoch: 8.29 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38424783105843396		[learning rate: 0.008204]
		[batch 20/20] avg loss: 0.44929144426110434		[learning rate: 0.0081891]
	Learning Rate: 0.00818911
	LOSS [training: 0.4167696376597692 | validation: 0.40453792005245914]
	TIME [epoch: 8.3 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.450527000952914		[learning rate: 0.0081742]
		[batch 20/20] avg loss: 0.32924258436104303		[learning rate: 0.0081594]
	Learning Rate: 0.00815939
	LOSS [training: 0.3898847926569785 | validation: 0.3406469363112132]
	TIME [epoch: 8.34 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6314553999988533		[learning rate: 0.0081446]
		[batch 20/20] avg loss: 0.3727565401891251		[learning rate: 0.0081298]
	Learning Rate: 0.00812978
	LOSS [training: 0.5021059700939893 | validation: 0.36168892607748526]
	TIME [epoch: 8.31 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3877166119267026		[learning rate: 0.008115]
		[batch 20/20] avg loss: 0.44661607445007234		[learning rate: 0.0081003]
	Learning Rate: 0.00810028
	LOSS [training: 0.4171663431883875 | validation: 0.4983096374814423]
	TIME [epoch: 8.3 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7092328934803993		[learning rate: 0.0080856]
		[batch 20/20] avg loss: 0.40360451817946597		[learning rate: 0.0080709]
	Learning Rate: 0.00807088
	LOSS [training: 0.5564187058299327 | validation: 0.2811242554159684]
	TIME [epoch: 8.29 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47723938465161886		[learning rate: 0.0080562]
		[batch 20/20] avg loss: 0.5474340899069363		[learning rate: 0.0080416]
	Learning Rate: 0.00804159
	LOSS [training: 0.5123367372792775 | validation: 0.8045462290838012]
	TIME [epoch: 8.34 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5158247088345139		[learning rate: 0.008027]
		[batch 20/20] avg loss: 0.4920286471666035		[learning rate: 0.0080124]
	Learning Rate: 0.00801241
	LOSS [training: 0.5039266780005588 | validation: 0.5497011891147486]
	TIME [epoch: 8.32 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37106957351673925		[learning rate: 0.0079979]
		[batch 20/20] avg loss: 0.43834736870073215		[learning rate: 0.0079833]
	Learning Rate: 0.00798333
	LOSS [training: 0.4047084711087357 | validation: 0.36738003962920196]
	TIME [epoch: 8.29 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45950475406983715		[learning rate: 0.0079688]
		[batch 20/20] avg loss: 0.35475907810005025		[learning rate: 0.0079544]
	Learning Rate: 0.00795436
	LOSS [training: 0.4071319160849437 | validation: 0.3325647412117575]
	TIME [epoch: 8.29 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3513665920768149		[learning rate: 0.0079399]
		[batch 20/20] avg loss: 0.351323564988347		[learning rate: 0.0079255]
	Learning Rate: 0.00792549
	LOSS [training: 0.351345078532581 | validation: 0.1540436171079954]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5403241905114909		[learning rate: 0.0079111]
		[batch 20/20] avg loss: 0.38425740858150903		[learning rate: 0.0078967]
	Learning Rate: 0.00789673
	LOSS [training: 0.4622907995465001 | validation: 0.22470112166068995]
	TIME [epoch: 8.31 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4556493964186317		[learning rate: 0.0078824]
		[batch 20/20] avg loss: 0.5322717641426883		[learning rate: 0.0078681]
	Learning Rate: 0.00786807
	LOSS [training: 0.4939605802806599 | validation: 0.305616501018991]
	TIME [epoch: 8.29 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5786977393034534		[learning rate: 0.0078538]
		[batch 20/20] avg loss: 0.9572062078884629		[learning rate: 0.0078395]
	Learning Rate: 0.00783952
	LOSS [training: 0.7679519735959582 | validation: 0.24873790728899564]
	TIME [epoch: 8.29 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39367196144890454		[learning rate: 0.0078253]
		[batch 20/20] avg loss: 0.4047697760968358		[learning rate: 0.0078111]
	Learning Rate: 0.00781107
	LOSS [training: 0.3992208687728702 | validation: 0.20559103299894313]
	TIME [epoch: 8.33 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5392595796415283		[learning rate: 0.0077969]
		[batch 20/20] avg loss: 0.4493722947201092		[learning rate: 0.0077827]
	Learning Rate: 0.00778272
	LOSS [training: 0.4943159371808187 | validation: 0.31126079804643636]
	TIME [epoch: 8.32 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46309136559968234		[learning rate: 0.0077686]
		[batch 20/20] avg loss: 0.4622305913103476		[learning rate: 0.0077545]
	Learning Rate: 0.00775448
	LOSS [training: 0.46266097845501497 | validation: 2.0003996528894765]
	TIME [epoch: 8.29 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7735575426110697		[learning rate: 0.0077404]
		[batch 20/20] avg loss: 0.4538501186099002		[learning rate: 0.0077263]
	Learning Rate: 0.00772634
	LOSS [training: 0.613703830610485 | validation: 0.4699622553475437]
	TIME [epoch: 8.28 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5629785995428692		[learning rate: 0.0077123]
		[batch 20/20] avg loss: 0.4965141057562869		[learning rate: 0.0076983]
	Learning Rate: 0.0076983
	LOSS [training: 0.529746352649578 | validation: 0.4068294175112378]
	TIME [epoch: 8.32 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42540934292581556		[learning rate: 0.0076843]
		[batch 20/20] avg loss: 0.4062205069309249		[learning rate: 0.0076704]
	Learning Rate: 0.00767036
	LOSS [training: 0.4158149249283703 | validation: 0.3755237441160306]
	TIME [epoch: 8.32 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39582046100382395		[learning rate: 0.0076564]
		[batch 20/20] avg loss: 0.4955433559227308		[learning rate: 0.0076425]
	Learning Rate: 0.00764252
	LOSS [training: 0.4456819084632774 | validation: 0.2284051330096522]
	TIME [epoch: 8.3 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4567823140532384		[learning rate: 0.0076286]
		[batch 20/20] avg loss: 0.4839867046913831		[learning rate: 0.0076148]
	Learning Rate: 0.00761479
	LOSS [training: 0.4703845093723107 | validation: 0.5103032730003021]
	TIME [epoch: 8.3 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6900266729758272		[learning rate: 0.007601]
		[batch 20/20] avg loss: 0.5886319144238471		[learning rate: 0.0075872]
	Learning Rate: 0.00758715
	LOSS [training: 0.6393292936998372 | validation: 0.25753066510053224]
	TIME [epoch: 8.32 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3944774729849291		[learning rate: 0.0075734]
		[batch 20/20] avg loss: 0.5350023036236271		[learning rate: 0.0075596]
	Learning Rate: 0.00755962
	LOSS [training: 0.46473988830427804 | validation: 0.3844610954825254]
	TIME [epoch: 8.31 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3906654497414249		[learning rate: 0.0075459]
		[batch 20/20] avg loss: 0.38844486650237353		[learning rate: 0.0075322]
	Learning Rate: 0.00753219
	LOSS [training: 0.3895551581218992 | validation: 0.43374871702103457]
	TIME [epoch: 8.3 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3389773655280764		[learning rate: 0.0075185]
		[batch 20/20] avg loss: 0.4272444938651868		[learning rate: 0.0075049]
	Learning Rate: 0.00750485
	LOSS [training: 0.3831109296966316 | validation: 0.7183555655273195]
	TIME [epoch: 8.32 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4788732974522942		[learning rate: 0.0074912]
		[batch 20/20] avg loss: 0.38491672414980754		[learning rate: 0.0074776]
	Learning Rate: 0.00747762
	LOSS [training: 0.4318950108010508 | validation: 0.5274761294749957]
	TIME [epoch: 8.32 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3637792935531879		[learning rate: 0.007464]
		[batch 20/20] avg loss: 0.3724896013781263		[learning rate: 0.0074505]
	Learning Rate: 0.00745048
	LOSS [training: 0.36813444746565704 | validation: 0.4321688151180436]
	TIME [epoch: 8.3 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45692653951851814		[learning rate: 0.0074369]
		[batch 20/20] avg loss: 0.45700133866472237		[learning rate: 0.0074234]
	Learning Rate: 0.00742344
	LOSS [training: 0.4569639390916203 | validation: 0.15220974732204]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45031249538250134		[learning rate: 0.00741]
		[batch 20/20] avg loss: 0.5055643359623047		[learning rate: 0.0073965]
	Learning Rate: 0.0073965
	LOSS [training: 0.47793841567240297 | validation: 0.43055962625265776]
	TIME [epoch: 8.31 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40882801973081245		[learning rate: 0.0073831]
		[batch 20/20] avg loss: 0.37527702997337		[learning rate: 0.0073697]
	Learning Rate: 0.00736966
	LOSS [training: 0.3920525248520912 | validation: 0.2337425753474102]
	TIME [epoch: 8.31 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31132649694341075		[learning rate: 0.0073563]
		[batch 20/20] avg loss: 0.3856590259953741		[learning rate: 0.0073429]
	Learning Rate: 0.00734291
	LOSS [training: 0.3484927614693924 | validation: 0.23412590181879794]
	TIME [epoch: 8.29 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.373048774223114		[learning rate: 0.0073296]
		[batch 20/20] avg loss: 0.3357676509923949		[learning rate: 0.0073163]
	Learning Rate: 0.00731627
	LOSS [training: 0.35440821260775446 | validation: 0.5940814029541567]
	TIME [epoch: 8.32 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41787730292499503		[learning rate: 0.007303]
		[batch 20/20] avg loss: 0.34244529277823443		[learning rate: 0.0072897]
	Learning Rate: 0.00728971
	LOSS [training: 0.38016129785161473 | validation: 0.35632635910214777]
	TIME [epoch: 8.33 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30516643167844537		[learning rate: 0.0072765]
		[batch 20/20] avg loss: 0.2845633235683583		[learning rate: 0.0072633]
	Learning Rate: 0.00726326
	LOSS [training: 0.2948648776234018 | validation: 0.2747696452341522]
	TIME [epoch: 8.29 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3352342299634454		[learning rate: 0.0072501]
		[batch 20/20] avg loss: 0.7824490005061389		[learning rate: 0.0072369]
	Learning Rate: 0.0072369
	LOSS [training: 0.558841615234792 | validation: 0.29591288068899196]
	TIME [epoch: 8.3 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.439135105006503		[learning rate: 0.0072238]
		[batch 20/20] avg loss: 0.3473753772814825		[learning rate: 0.0072106]
	Learning Rate: 0.00721064
	LOSS [training: 0.3932552411439928 | validation: 0.2310815061948854]
	TIME [epoch: 8.31 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34796870897440424		[learning rate: 0.0071975]
		[batch 20/20] avg loss: 0.307275731444525		[learning rate: 0.0071845]
	Learning Rate: 0.00718447
	LOSS [training: 0.32762222020946463 | validation: 1.4968986791132188]
	TIME [epoch: 8.34 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6209144642833779		[learning rate: 0.0071714]
		[batch 20/20] avg loss: 0.4190609744471062		[learning rate: 0.0071584]
	Learning Rate: 0.0071584
	LOSS [training: 0.519987719365242 | validation: 0.43121393562907295]
	TIME [epoch: 8.29 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37439546245560373		[learning rate: 0.0071454]
		[batch 20/20] avg loss: 0.36411201034185003		[learning rate: 0.0071324]
	Learning Rate: 0.00713242
	LOSS [training: 0.3692537363987269 | validation: 0.3076906346321717]
	TIME [epoch: 8.29 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5016745741825243		[learning rate: 0.0071195]
		[batch 20/20] avg loss: 0.3720578011499803		[learning rate: 0.0071065]
	Learning Rate: 0.00710653
	LOSS [training: 0.43686618766625224 | validation: 0.37267607720821905]
	TIME [epoch: 8.29 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3948249498614942		[learning rate: 0.0070936]
		[batch 20/20] avg loss: 0.4350042602503521		[learning rate: 0.0070807]
	Learning Rate: 0.00708074
	LOSS [training: 0.41491460505592304 | validation: 0.4984204590557452]
	TIME [epoch: 8.36 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4084978344792424		[learning rate: 0.0070679]
		[batch 20/20] avg loss: 0.4358355215429473		[learning rate: 0.007055]
	Learning Rate: 0.00705505
	LOSS [training: 0.4221666780110949 | validation: 0.5568830259639402]
	TIME [epoch: 8.29 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41954156163879724		[learning rate: 0.0070422]
		[batch 20/20] avg loss: 0.3774275546458824		[learning rate: 0.0070294]
	Learning Rate: 0.00702945
	LOSS [training: 0.39848455814233985 | validation: 0.47138391741818936]
	TIME [epoch: 8.29 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3863714728026591		[learning rate: 0.0070167]
		[batch 20/20] avg loss: 0.5944163278324094		[learning rate: 0.0070039]
	Learning Rate: 0.00700394
	LOSS [training: 0.4903939003175341 | validation: 0.318104786257261]
	TIME [epoch: 8.3 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4560997939973535		[learning rate: 0.0069912]
		[batch 20/20] avg loss: 0.4522391649054889		[learning rate: 0.0069785]
	Learning Rate: 0.00697852
	LOSS [training: 0.45416947945142117 | validation: 0.5647773352831309]
	TIME [epoch: 8.35 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30828551918255165		[learning rate: 0.0069658]
		[batch 20/20] avg loss: 0.4477670699057641		[learning rate: 0.0069532]
	Learning Rate: 0.00695319
	LOSS [training: 0.3780262945441578 | validation: 0.2065654034548127]
	TIME [epoch: 8.3 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33802203708395095		[learning rate: 0.0069406]
		[batch 20/20] avg loss: 0.4132863432377599		[learning rate: 0.006928]
	Learning Rate: 0.00692796
	LOSS [training: 0.37565419016085544 | validation: 0.15078263002121617]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37343891131997414		[learning rate: 0.0069154]
		[batch 20/20] avg loss: 0.37927421933882816		[learning rate: 0.0069028]
	Learning Rate: 0.00690282
	LOSS [training: 0.37635656532940115 | validation: 0.12976308493846458]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4495907650181442		[learning rate: 0.0068903]
		[batch 20/20] avg loss: 0.4552031357007241		[learning rate: 0.0068778]
	Learning Rate: 0.00687777
	LOSS [training: 0.45239695035943417 | validation: 0.35964335517993756]
	TIME [epoch: 8.32 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.420477906669307		[learning rate: 0.0068653]
		[batch 20/20] avg loss: 0.43244328650731123		[learning rate: 0.0068528]
	Learning Rate: 0.00685281
	LOSS [training: 0.42646059658830915 | validation: 0.25094578159836345]
	TIME [epoch: 8.3 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49819154484477207		[learning rate: 0.0068404]
		[batch 20/20] avg loss: 0.4098208024690123		[learning rate: 0.0068279]
	Learning Rate: 0.00682794
	LOSS [training: 0.4540061736568922 | validation: 0.18684777911210215]
	TIME [epoch: 8.29 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49385426835668866		[learning rate: 0.0068155]
		[batch 20/20] avg loss: 0.7492651981370653		[learning rate: 0.0068032]
	Learning Rate: 0.00680316
	LOSS [training: 0.6215597332468769 | validation: 1.0357030600408985]
	TIME [epoch: 8.32 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38905367548339004		[learning rate: 0.0067908]
		[batch 20/20] avg loss: 0.3820488919991071		[learning rate: 0.0067785]
	Learning Rate: 0.00677847
	LOSS [training: 0.38555128374124853 | validation: 0.26135916135177695]
	TIME [epoch: 8.31 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4672016172934522		[learning rate: 0.0067662]
		[batch 20/20] avg loss: 0.3989982150921237		[learning rate: 0.0067539]
	Learning Rate: 0.00675387
	LOSS [training: 0.4330999161927879 | validation: 0.5439709516705511]
	TIME [epoch: 8.32 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33963803831465245		[learning rate: 0.0067416]
		[batch 20/20] avg loss: 0.4719145547795778		[learning rate: 0.0067294]
	Learning Rate: 0.00672936
	LOSS [training: 0.40577629654711506 | validation: 1.3458644581614865]
	TIME [epoch: 8.29 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4293522955911383		[learning rate: 0.0067171]
		[batch 20/20] avg loss: 0.36066783641124467		[learning rate: 0.0067049]
	Learning Rate: 0.00670494
	LOSS [training: 0.3950100660011916 | validation: 0.16728984258429877]
	TIME [epoch: 8.32 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4957114375496879		[learning rate: 0.0066928]
		[batch 20/20] avg loss: 0.3833237902759869		[learning rate: 0.0066806]
	Learning Rate: 0.0066806
	LOSS [training: 0.4395176139128374 | validation: 0.3790792049911531]
	TIME [epoch: 8.29 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4012459181811125		[learning rate: 0.0066685]
		[batch 20/20] avg loss: 0.4252461781985173		[learning rate: 0.0066564]
	Learning Rate: 0.00665636
	LOSS [training: 0.413246048189815 | validation: 0.7459862577154512]
	TIME [epoch: 8.32 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4241753699001761		[learning rate: 0.0066443]
		[batch 20/20] avg loss: 0.25352163677697953		[learning rate: 0.0066322]
	Learning Rate: 0.0066322
	LOSS [training: 0.33884850333857774 | validation: 0.3345284176209475]
	TIME [epoch: 8.29 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3418799387527681		[learning rate: 0.0066202]
		[batch 20/20] avg loss: 0.3020174166285649		[learning rate: 0.0066081]
	Learning Rate: 0.00660814
	LOSS [training: 0.3219486776906665 | validation: 0.3625458251043938]
	TIME [epoch: 8.33 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42041372187161896		[learning rate: 0.0065961]
		[batch 20/20] avg loss: 0.35277268297062775		[learning rate: 0.0065842]
	Learning Rate: 0.00658415
	LOSS [training: 0.3865932024211234 | validation: 0.513074941157841]
	TIME [epoch: 8.28 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29601242078435963		[learning rate: 0.0065722]
		[batch 20/20] avg loss: 0.6292284162167151		[learning rate: 0.0065603]
	Learning Rate: 0.00656026
	LOSS [training: 0.4626204185005374 | validation: 0.26090405820769225]
	TIME [epoch: 8.3 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3472338584328964		[learning rate: 0.0065483]
		[batch 20/20] avg loss: 0.37552240155236655		[learning rate: 0.0065365]
	Learning Rate: 0.00653645
	LOSS [training: 0.3613781299926314 | validation: 0.20080264181715723]
	TIME [epoch: 8.3 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38890483830810024		[learning rate: 0.0065246]
		[batch 20/20] avg loss: 0.3460784913744353		[learning rate: 0.0065127]
	Learning Rate: 0.00651273
	LOSS [training: 0.3674916648412678 | validation: 0.649434925365662]
	TIME [epoch: 8.33 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3268113441813842		[learning rate: 0.0065009]
		[batch 20/20] avg loss: 0.2752427943997821		[learning rate: 0.0064891]
	Learning Rate: 0.0064891
	LOSS [training: 0.30102706929058315 | validation: 0.15756739780288706]
	TIME [epoch: 8.29 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3579349095407295		[learning rate: 0.0064773]
		[batch 20/20] avg loss: 0.46568392628187744		[learning rate: 0.0064655]
	Learning Rate: 0.00646555
	LOSS [training: 0.4118094179113035 | validation: 0.4364581832531308]
	TIME [epoch: 8.29 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31220408534832994		[learning rate: 0.0064538]
		[batch 20/20] avg loss: 0.39890012605362035		[learning rate: 0.0064421]
	Learning Rate: 0.00644208
	LOSS [training: 0.3555521057009752 | validation: 0.31488828599787744]
	TIME [epoch: 8.34 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4448031497591308		[learning rate: 0.0064304]
		[batch 20/20] avg loss: 0.6085669630984063		[learning rate: 0.0064187]
	Learning Rate: 0.0064187
	LOSS [training: 0.5266850564287686 | validation: 0.3914341194021911]
	TIME [epoch: 8.32 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4573560286484848		[learning rate: 0.006407]
		[batch 20/20] avg loss: 0.29085744243770445		[learning rate: 0.0063954]
	Learning Rate: 0.00639541
	LOSS [training: 0.3741067355430946 | validation: 0.3079700921400232]
	TIME [epoch: 8.29 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28401736818128037		[learning rate: 0.0063838]
		[batch 20/20] avg loss: 0.3089627931796866		[learning rate: 0.0063722]
	Learning Rate: 0.0063722
	LOSS [training: 0.29649008068048344 | validation: 0.3003262999674182]
	TIME [epoch: 8.29 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2865714753840704		[learning rate: 0.0063606]
		[batch 20/20] avg loss: 0.4641766220221627		[learning rate: 0.0063491]
	Learning Rate: 0.00634908
	LOSS [training: 0.3753740487031166 | validation: 0.49565461207621286]
	TIME [epoch: 8.33 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37457503479694526		[learning rate: 0.0063375]
		[batch 20/20] avg loss: 0.32073057637985464		[learning rate: 0.006326]
	Learning Rate: 0.00632603
	LOSS [training: 0.34765280558839995 | validation: 0.3069923692543871]
	TIME [epoch: 8.32 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2894891224970803		[learning rate: 0.0063145]
		[batch 20/20] avg loss: 0.45877122036140106		[learning rate: 0.0063031]
	Learning Rate: 0.00630308
	LOSS [training: 0.3741301714292407 | validation: 0.3488033028904844]
	TIME [epoch: 8.29 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22886639855122876		[learning rate: 0.0062916]
		[batch 20/20] avg loss: 0.3341924773894128		[learning rate: 0.0062802]
	Learning Rate: 0.0062802
	LOSS [training: 0.2815294379703207 | validation: 0.18828899323206627]
	TIME [epoch: 8.3 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3001707371076752		[learning rate: 0.0062688]
		[batch 20/20] avg loss: 0.2396179388339057		[learning rate: 0.0062574]
	Learning Rate: 0.00625741
	LOSS [training: 0.26989433797079043 | validation: 0.3601115722588306]
	TIME [epoch: 8.33 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34554389762828863		[learning rate: 0.006246]
		[batch 20/20] avg loss: 0.31072515363612146		[learning rate: 0.0062347]
	Learning Rate: 0.0062347
	LOSS [training: 0.32813452563220513 | validation: 0.1642353409453592]
	TIME [epoch: 8.33 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5015327439728711		[learning rate: 0.0062234]
		[batch 20/20] avg loss: 0.272946582633827		[learning rate: 0.0062121]
	Learning Rate: 0.00621208
	LOSS [training: 0.3872396633033489 | validation: 0.3716868756849595]
	TIME [epoch: 8.3 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2829733629064315		[learning rate: 0.0062008]
		[batch 20/20] avg loss: 0.32135941978714816		[learning rate: 0.0061895]
	Learning Rate: 0.00618953
	LOSS [training: 0.30216639134678974 | validation: 0.589775304775427]
	TIME [epoch: 8.31 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4159935436894958		[learning rate: 0.0061783]
		[batch 20/20] avg loss: 0.3224425885023218		[learning rate: 0.0061671]
	Learning Rate: 0.00616707
	LOSS [training: 0.36921806609590885 | validation: 0.5453857224222696]
	TIME [epoch: 8.33 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33840876543517007		[learning rate: 0.0061559]
		[batch 20/20] avg loss: 0.33834830181873005		[learning rate: 0.0061447]
	Learning Rate: 0.00614469
	LOSS [training: 0.3383785336269501 | validation: 0.40472189889882565]
	TIME [epoch: 8.33 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4672646280417395		[learning rate: 0.0061335]
		[batch 20/20] avg loss: 0.38959562923765195		[learning rate: 0.0061224]
	Learning Rate: 0.00612239
	LOSS [training: 0.42843012863969576 | validation: 0.2662599520052516]
	TIME [epoch: 8.3 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2619058403002048		[learning rate: 0.0061113]
		[batch 20/20] avg loss: 0.32949779522743317		[learning rate: 0.0061002]
	Learning Rate: 0.00610017
	LOSS [training: 0.295701817763819 | validation: 0.4110744440078212]
	TIME [epoch: 8.3 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3161238517054593		[learning rate: 0.0060891]
		[batch 20/20] avg loss: 0.29625432382613737		[learning rate: 0.006078]
	Learning Rate: 0.00607803
	LOSS [training: 0.30618908776579834 | validation: 0.2765426570779265]
	TIME [epoch: 8.33 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.492897684430852		[learning rate: 0.006067]
		[batch 20/20] avg loss: 0.3702496680402626		[learning rate: 0.006056]
	Learning Rate: 0.00605598
	LOSS [training: 0.43157367623555737 | validation: 0.402506933032279]
	TIME [epoch: 8.32 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3068186149703193		[learning rate: 0.006045]
		[batch 20/20] avg loss: 0.2911846903202173		[learning rate: 0.006034]
	Learning Rate: 0.006034
	LOSS [training: 0.29900165264526835 | validation: 0.16276565516801647]
	TIME [epoch: 8.3 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33150404347915063		[learning rate: 0.006023]
		[batch 20/20] avg loss: 0.3460478281958056		[learning rate: 0.0060121]
	Learning Rate: 0.0060121
	LOSS [training: 0.3387759358374781 | validation: 0.29954572429323306]
	TIME [epoch: 8.3 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2923735397740048		[learning rate: 0.0060012]
		[batch 20/20] avg loss: 0.24706705784756583		[learning rate: 0.0059903]
	Learning Rate: 0.00599028
	LOSS [training: 0.2697202988107853 | validation: 0.456555053956012]
	TIME [epoch: 8.33 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47759519654244553		[learning rate: 0.0059794]
		[batch 20/20] avg loss: 0.29594112322848065		[learning rate: 0.0059685]
	Learning Rate: 0.00596854
	LOSS [training: 0.38676815988546315 | validation: 0.24924835159728137]
	TIME [epoch: 8.3 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.320701780526356		[learning rate: 0.0059577]
		[batch 20/20] avg loss: 0.48719702566460166		[learning rate: 0.0059469]
	Learning Rate: 0.00594688
	LOSS [training: 0.40394940309547883 | validation: 0.18917976854606638]
	TIME [epoch: 8.32 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32904693065440005		[learning rate: 0.0059361]
		[batch 20/20] avg loss: 0.4812993859283997		[learning rate: 0.0059253]
	Learning Rate: 0.0059253
	LOSS [training: 0.4051731582913999 | validation: 0.07596371350019626]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34370208451416334		[learning rate: 0.0059145]
		[batch 20/20] avg loss: 0.4792182197585018		[learning rate: 0.0059038]
	Learning Rate: 0.0059038
	LOSS [training: 0.4114601521363325 | validation: 0.27245427096701236]
	TIME [epoch: 8.32 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4384649955365904		[learning rate: 0.0058931]
		[batch 20/20] avg loss: 0.26246294767703543		[learning rate: 0.0058824]
	Learning Rate: 0.00588237
	LOSS [training: 0.3504639716068129 | validation: 0.2867220473843198]
	TIME [epoch: 8.29 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27745669732990125		[learning rate: 0.0058717]
		[batch 20/20] avg loss: 0.2697487176096852		[learning rate: 0.005861]
	Learning Rate: 0.00586103
	LOSS [training: 0.2736027074697932 | validation: 0.7140051489832988]
	TIME [epoch: 8.31 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3763533034398739		[learning rate: 0.0058504]
		[batch 20/20] avg loss: 0.26236917427855466		[learning rate: 0.0058398]
	Learning Rate: 0.00583976
	LOSS [training: 0.31936123885921425 | validation: 0.09370817460641587]
	TIME [epoch: 8.31 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29218283811581003		[learning rate: 0.0058291]
		[batch 20/20] avg loss: 0.2991716120213285		[learning rate: 0.0058186]
	Learning Rate: 0.00581856
	LOSS [training: 0.29567722506856925 | validation: 0.12442019188661507]
	TIME [epoch: 8.31 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3336803003415926		[learning rate: 0.005808]
		[batch 20/20] avg loss: 0.40388722791331677		[learning rate: 0.0057974]
	Learning Rate: 0.00579745
	LOSS [training: 0.3687837641274546 | validation: 0.344369170497453]
	TIME [epoch: 8.28 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2970054994707656		[learning rate: 0.0057869]
		[batch 20/20] avg loss: 0.31042857015548564		[learning rate: 0.0057764]
	Learning Rate: 0.00577641
	LOSS [training: 0.3037170348131256 | validation: 0.1540270588341323]
	TIME [epoch: 8.3 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22109802832591113		[learning rate: 0.0057659]
		[batch 20/20] avg loss: 0.3920443123221663		[learning rate: 0.0057554]
	Learning Rate: 0.00575545
	LOSS [training: 0.3065711703240388 | validation: 0.1893252223762087]
	TIME [epoch: 8.32 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2896978249439063		[learning rate: 0.005745]
		[batch 20/20] avg loss: 0.3735588838532101		[learning rate: 0.0057346]
	Learning Rate: 0.00573456
	LOSS [training: 0.3316283543985582 | validation: 0.23056074854437783]
	TIME [epoch: 8.31 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3472324612767402		[learning rate: 0.0057241]
		[batch 20/20] avg loss: 0.3470290252762441		[learning rate: 0.0057137]
	Learning Rate: 0.00571375
	LOSS [training: 0.3471307432764921 | validation: 0.27886216753552806]
	TIME [epoch: 8.28 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2575909946529077		[learning rate: 0.0057034]
		[batch 20/20] avg loss: 0.4434804276659775		[learning rate: 0.005693]
	Learning Rate: 0.00569301
	LOSS [training: 0.3505357111594426 | validation: 0.1323551298341073]
	TIME [epoch: 8.28 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23629018511274763		[learning rate: 0.0056827]
		[batch 20/20] avg loss: 0.3002478495260491		[learning rate: 0.0056724]
	Learning Rate: 0.00567235
	LOSS [training: 0.26826901731939834 | validation: 0.2539007631869355]
	TIME [epoch: 8.33 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.314049439255914		[learning rate: 0.005662]
		[batch 20/20] avg loss: 0.3056404395690914		[learning rate: 0.0056518]
	Learning Rate: 0.00565177
	LOSS [training: 0.30984493941250274 | validation: 0.6017231014671555]
	TIME [epoch: 8.3 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35820928304578914		[learning rate: 0.0056415]
		[batch 20/20] avg loss: 0.26051620727662517		[learning rate: 0.0056313]
	Learning Rate: 0.00563126
	LOSS [training: 0.3093627451612072 | validation: 0.16515051655313973]
	TIME [epoch: 8.28 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3690776329581576		[learning rate: 0.005621]
		[batch 20/20] avg loss: 0.34170532415510657		[learning rate: 0.0056108]
	Learning Rate: 0.00561082
	LOSS [training: 0.355391478556632 | validation: 0.39649193967127233]
	TIME [epoch: 8.28 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33795537488291283		[learning rate: 0.0056006]
		[batch 20/20] avg loss: 0.2688899194455683		[learning rate: 0.0055905]
	Learning Rate: 0.00559046
	LOSS [training: 0.3034226471642406 | validation: 0.24306314034953902]
	TIME [epoch: 8.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5040988636733268		[learning rate: 0.0055803]
		[batch 20/20] avg loss: 0.30418394274635824		[learning rate: 0.0055702]
	Learning Rate: 0.00557017
	LOSS [training: 0.4041414032098425 | validation: 0.3251340470353567]
	TIME [epoch: 8.3 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3391607889698563		[learning rate: 0.0055601]
		[batch 20/20] avg loss: 0.3596508249168101		[learning rate: 0.00555]
	Learning Rate: 0.00554996
	LOSS [training: 0.34940580694333323 | validation: 0.24508294920116241]
	TIME [epoch: 8.29 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24892830150490827		[learning rate: 0.0055399]
		[batch 20/20] avg loss: 0.2602160618091124		[learning rate: 0.0055298]
	Learning Rate: 0.00552981
	LOSS [training: 0.25457218165701034 | validation: 0.3310044553301912]
	TIME [epoch: 8.28 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33202494719343406		[learning rate: 0.0055198]
		[batch 20/20] avg loss: 0.34026501406270554		[learning rate: 0.0055097]
	Learning Rate: 0.00550975
	LOSS [training: 0.3361449806280698 | validation: 0.11825067696898042]
	TIME [epoch: 8.34 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23634235447960528		[learning rate: 0.0054997]
		[batch 20/20] avg loss: 0.3276138739940431		[learning rate: 0.0054898]
	Learning Rate: 0.00548975
	LOSS [training: 0.2819781142368242 | validation: 0.09670244845490553]
	TIME [epoch: 8.3 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40161379312875123		[learning rate: 0.0054798]
		[batch 20/20] avg loss: 0.2505497396614285		[learning rate: 0.0054698]
	Learning Rate: 0.00546983
	LOSS [training: 0.3260817663950899 | validation: 0.11240997393823422]
	TIME [epoch: 8.3 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32130367072786414		[learning rate: 0.0054599]
		[batch 20/20] avg loss: 0.2606159620575265		[learning rate: 0.00545]
	Learning Rate: 0.00544998
	LOSS [training: 0.2909598163926953 | validation: 0.36429211887157276]
	TIME [epoch: 8.29 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23670160775063356		[learning rate: 0.0054401]
		[batch 20/20] avg loss: 0.35021425988316024		[learning rate: 0.0054302]
	Learning Rate: 0.0054302
	LOSS [training: 0.2934579338168969 | validation: 0.42506196804089885]
	TIME [epoch: 8.32 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2711342923380342		[learning rate: 0.0054203]
		[batch 20/20] avg loss: 0.30397435937279693		[learning rate: 0.0054105]
	Learning Rate: 0.00541049
	LOSS [training: 0.28755432585541557 | validation: 0.1366460466161466]
	TIME [epoch: 8.32 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2539402304957456		[learning rate: 0.0054007]
		[batch 20/20] avg loss: 0.35922628745228885		[learning rate: 0.0053909]
	Learning Rate: 0.00539086
	LOSS [training: 0.30658325897401717 | validation: 0.29822386436072423]
	TIME [epoch: 8.29 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35978048819080344		[learning rate: 0.0053811]
		[batch 20/20] avg loss: 0.2858183299162799		[learning rate: 0.0053713]
	Learning Rate: 0.00537129
	LOSS [training: 0.32279940905354165 | validation: 0.21969243281603648]
	TIME [epoch: 8.29 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22233486664799013		[learning rate: 0.0053615]
		[batch 20/20] avg loss: 0.27759178172115284		[learning rate: 0.0053518]
	Learning Rate: 0.0053518
	LOSS [training: 0.24996332418457148 | validation: 0.11072382413255667]
	TIME [epoch: 8.33 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4327694635987182		[learning rate: 0.0053421]
		[batch 20/20] avg loss: 0.2979982259909826		[learning rate: 0.0053324]
	Learning Rate: 0.00533238
	LOSS [training: 0.3653838447948504 | validation: 0.23540530207094523]
	TIME [epoch: 8.32 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26127603011825784		[learning rate: 0.0053227]
		[batch 20/20] avg loss: 0.36869354802070753		[learning rate: 0.005313]
	Learning Rate: 0.00531303
	LOSS [training: 0.3149847890694827 | validation: 0.9516326474996506]
	TIME [epoch: 8.29 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.399973562055052		[learning rate: 0.0053034]
		[batch 20/20] avg loss: 0.22584350880339246		[learning rate: 0.0052937]
	Learning Rate: 0.00529375
	LOSS [training: 0.3129085354292222 | validation: 0.09481020266372021]
	TIME [epoch: 8.29 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32929997342467976		[learning rate: 0.0052841]
		[batch 20/20] avg loss: 0.2857497556675651		[learning rate: 0.0052745]
	Learning Rate: 0.00527454
	LOSS [training: 0.30752486454612243 | validation: 0.3162244637018416]
	TIME [epoch: 8.32 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2843153355802282		[learning rate: 0.005265]
		[batch 20/20] avg loss: 0.19419489990973976		[learning rate: 0.0052554]
	Learning Rate: 0.00525539
	LOSS [training: 0.23925511774498398 | validation: 0.22027959889779353]
	TIME [epoch: 8.3 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.195408242060079		[learning rate: 0.0052458]
		[batch 20/20] avg loss: 0.3363187113681342		[learning rate: 0.0052363]
	Learning Rate: 0.00523632
	LOSS [training: 0.26586347671410665 | validation: 0.17470587370753532]
	TIME [epoch: 8.31 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29517038260602574		[learning rate: 0.0052268]
		[batch 20/20] avg loss: 0.22867359807119475		[learning rate: 0.0052173]
	Learning Rate: 0.00521732
	LOSS [training: 0.2619219903386102 | validation: 0.09065542184743165]
	TIME [epoch: 8.31 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36573693116985584		[learning rate: 0.0052078]
		[batch 20/20] avg loss: 0.28354309499523184		[learning rate: 0.0051984]
	Learning Rate: 0.00519839
	LOSS [training: 0.3246400130825438 | validation: 0.3425168965816011]
	TIME [epoch: 8.32 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27137361392374715		[learning rate: 0.0051889]
		[batch 20/20] avg loss: 0.42373880170534345		[learning rate: 0.0051795]
	Learning Rate: 0.00517952
	LOSS [training: 0.3475562078145453 | validation: 0.316293244217972]
	TIME [epoch: 8.3 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2632139626495301		[learning rate: 0.0051701]
		[batch 20/20] avg loss: 0.3524056677722932		[learning rate: 0.0051607]
	Learning Rate: 0.00516072
	LOSS [training: 0.30780981521091166 | validation: 0.3268498482692831]
	TIME [epoch: 8.32 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2881074588544209		[learning rate: 0.0051513]
		[batch 20/20] avg loss: 0.28816674652735774		[learning rate: 0.005142]
	Learning Rate: 0.00514199
	LOSS [training: 0.28813710269088927 | validation: 0.20217800857263624]
	TIME [epoch: 8.32 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2920842445450317		[learning rate: 0.0051327]
		[batch 20/20] avg loss: 0.2683202518270952		[learning rate: 0.0051233]
	Learning Rate: 0.00512333
	LOSS [training: 0.2802022481860635 | validation: 0.3432200757976568]
	TIME [epoch: 8.31 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2371550071482101		[learning rate: 0.005114]
		[batch 20/20] avg loss: 0.3564917890716078		[learning rate: 0.0051047]
	Learning Rate: 0.00510474
	LOSS [training: 0.29682339810990893 | validation: 0.24986912440023665]
	TIME [epoch: 8.29 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17703766781869168		[learning rate: 0.0050955]
		[batch 20/20] avg loss: 0.2051713765665522		[learning rate: 0.0050862]
	Learning Rate: 0.00508622
	LOSS [training: 0.19110452219262192 | validation: 0.216792995610698]
	TIME [epoch: 8.31 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27674137632092527		[learning rate: 0.005077]
		[batch 20/20] avg loss: 0.2939325930181774		[learning rate: 0.0050678]
	Learning Rate: 0.00506776
	LOSS [training: 0.2853369846695514 | validation: 0.20559956614821345]
	TIME [epoch: 8.33 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2196570949862421		[learning rate: 0.0050586]
		[batch 20/20] avg loss: 0.24473791544694593		[learning rate: 0.0050494]
	Learning Rate: 0.00504937
	LOSS [training: 0.23219750521659402 | validation: 0.21736959033673367]
	TIME [epoch: 8.32 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19607191573588403		[learning rate: 0.0050402]
		[batch 20/20] avg loss: 0.25330727064380587		[learning rate: 0.005031]
	Learning Rate: 0.00503104
	LOSS [training: 0.224689593189845 | validation: 0.18691499800795772]
	TIME [epoch: 8.3 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26108715126185156		[learning rate: 0.0050219]
		[batch 20/20] avg loss: 0.261721819092983		[learning rate: 0.0050128]
	Learning Rate: 0.00501278
	LOSS [training: 0.26140448517741727 | validation: 0.32639101585540403]
	TIME [epoch: 8.3 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22055625752709673		[learning rate: 0.0050037]
		[batch 20/20] avg loss: 0.3194216843111215		[learning rate: 0.0049946]
	Learning Rate: 0.00499459
	LOSS [training: 0.2699889709191091 | validation: 0.1869498404125356]
	TIME [epoch: 8.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27864636023319		[learning rate: 0.0049855]
		[batch 20/20] avg loss: 0.17797670358603263		[learning rate: 0.0049765]
	Learning Rate: 0.00497647
	LOSS [training: 0.22831153190961134 | validation: 0.06650815168793262]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23558055794914462		[learning rate: 0.0049674]
		[batch 20/20] avg loss: 0.2953846831557415		[learning rate: 0.0049584]
	Learning Rate: 0.00495841
	LOSS [training: 0.26548262055244304 | validation: 0.06931473148210472]
	TIME [epoch: 8.29 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2457939278411861		[learning rate: 0.0049494]
		[batch 20/20] avg loss: 0.25539182859939014		[learning rate: 0.0049404]
	Learning Rate: 0.00494041
	LOSS [training: 0.25059287822028814 | validation: 0.08454925057170351]
	TIME [epoch: 8.27 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2628290451032308		[learning rate: 0.0049314]
		[batch 20/20] avg loss: 0.2615331091741038		[learning rate: 0.0049225]
	Learning Rate: 0.00492248
	LOSS [training: 0.26218107713866734 | validation: 1.0420784070616127]
	TIME [epoch: 8.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35334063920007325		[learning rate: 0.0049135]
		[batch 20/20] avg loss: 0.27642812216626983		[learning rate: 0.0049046]
	Learning Rate: 0.00490462
	LOSS [training: 0.3148843806831715 | validation: 0.17657062533412177]
	TIME [epoch: 8.29 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2112660080761494		[learning rate: 0.0048957]
		[batch 20/20] avg loss: 0.2266079945970155		[learning rate: 0.0048868]
	Learning Rate: 0.00488682
	LOSS [training: 0.21893700133658248 | validation: 0.1493907652213449]
	TIME [epoch: 8.29 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21553717650818		[learning rate: 0.0048779]
		[batch 20/20] avg loss: 0.3960009283020893		[learning rate: 0.0048691]
	Learning Rate: 0.00486909
	LOSS [training: 0.3057690524051346 | validation: 0.08645268664416467]
	TIME [epoch: 8.28 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.242379853863515		[learning rate: 0.0048602]
		[batch 20/20] avg loss: 0.22131540580248638		[learning rate: 0.0048514]
	Learning Rate: 0.00485141
	LOSS [training: 0.23184762983300072 | validation: 0.0941221594898746]
	TIME [epoch: 8.34 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21721387181929996		[learning rate: 0.0048426]
		[batch 20/20] avg loss: 0.25538310635398104		[learning rate: 0.0048338]
	Learning Rate: 0.00483381
	LOSS [training: 0.2362984890866405 | validation: 0.07259583153316013]
	TIME [epoch: 8.3 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18950814649577802		[learning rate: 0.004825]
		[batch 20/20] avg loss: 0.23353928614294403		[learning rate: 0.0048163]
	Learning Rate: 0.00481627
	LOSS [training: 0.211523716319361 | validation: 0.08751326266908116]
	TIME [epoch: 8.28 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16937532724979154		[learning rate: 0.0048075]
		[batch 20/20] avg loss: 0.24794802962717905		[learning rate: 0.0047988]
	Learning Rate: 0.00479879
	LOSS [training: 0.20866167843848524 | validation: 0.15291044083607228]
	TIME [epoch: 8.28 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2269856436966941		[learning rate: 0.0047901]
		[batch 20/20] avg loss: 0.27877649980638813		[learning rate: 0.0047814]
	Learning Rate: 0.00478137
	LOSS [training: 0.2528810717515412 | validation: 0.1480209980756281]
	TIME [epoch: 8.32 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1888903415750896		[learning rate: 0.0047727]
		[batch 20/20] avg loss: 0.44091098611763513		[learning rate: 0.004764]
	Learning Rate: 0.00476402
	LOSS [training: 0.3149006638463624 | validation: 0.5071341172869505]
	TIME [epoch: 8.31 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31626173897628007		[learning rate: 0.0047554]
		[batch 20/20] avg loss: 0.22622387153930418		[learning rate: 0.0047467]
	Learning Rate: 0.00474673
	LOSS [training: 0.2712428052577921 | validation: 0.1395418766919131]
	TIME [epoch: 8.28 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19209564834926923		[learning rate: 0.0047381]
		[batch 20/20] avg loss: 0.2525104893284263		[learning rate: 0.0047295]
	Learning Rate: 0.00472951
	LOSS [training: 0.22230306883884773 | validation: 0.240142508610248]
	TIME [epoch: 8.3 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2703510687187883		[learning rate: 0.0047209]
		[batch 20/20] avg loss: 0.29497430928972906		[learning rate: 0.0047123]
	Learning Rate: 0.00471234
	LOSS [training: 0.2826626890042586 | validation: 0.09133891395631172]
	TIME [epoch: 8.31 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22572112474398165		[learning rate: 0.0047038]
		[batch 20/20] avg loss: 0.33495251110065133		[learning rate: 0.0046952]
	Learning Rate: 0.00469524
	LOSS [training: 0.2803368179223165 | validation: 0.2623061269407807]
	TIME [epoch: 8.3 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25512686242408616		[learning rate: 0.0046867]
		[batch 20/20] avg loss: 0.21217880717948026		[learning rate: 0.0046782]
	Learning Rate: 0.0046782
	LOSS [training: 0.23365283480178317 | validation: 0.1283050644986178]
	TIME [epoch: 8.29 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26283437550978744		[learning rate: 0.0046697]
		[batch 20/20] avg loss: 0.221687942006152		[learning rate: 0.0046612]
	Learning Rate: 0.00466122
	LOSS [training: 0.24226115875796977 | validation: 0.19940899037294602]
	TIME [epoch: 8.3 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24134401728242633		[learning rate: 0.0046528]
		[batch 20/20] avg loss: 0.26208184258320755		[learning rate: 0.0046443]
	Learning Rate: 0.00464431
	LOSS [training: 0.25171292993281696 | validation: 0.21377186246914687]
	TIME [epoch: 8.31 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2418405528974759		[learning rate: 0.0046359]
		[batch 20/20] avg loss: 0.2778734982254039		[learning rate: 0.0046275]
	Learning Rate: 0.00462745
	LOSS [training: 0.25985702556143997 | validation: 0.12338428704291927]
	TIME [epoch: 8.29 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20050728310692287		[learning rate: 0.004619]
		[batch 20/20] avg loss: 0.20848309131921808		[learning rate: 0.0046107]
	Learning Rate: 0.00461066
	LOSS [training: 0.20449518721307053 | validation: 0.175814280602604]
	TIME [epoch: 8.29 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23766058891997116		[learning rate: 0.0046023]
		[batch 20/20] avg loss: 0.174618179616892		[learning rate: 0.0045939]
	Learning Rate: 0.00459393
	LOSS [training: 0.2061393842684316 | validation: 0.28589159914746215]
	TIME [epoch: 8.3 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2970358780346919		[learning rate: 0.0045856]
		[batch 20/20] avg loss: 0.2240731430130977		[learning rate: 0.0045773]
	Learning Rate: 0.00457726
	LOSS [training: 0.2605545105238948 | validation: 0.2699971445064663]
	TIME [epoch: 8.3 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20999350622144827		[learning rate: 0.0045689]
		[batch 20/20] avg loss: 0.27194331087674684		[learning rate: 0.0045606]
	Learning Rate: 0.00456065
	LOSS [training: 0.24096840854909757 | validation: 0.05698319906838598]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20631867277449709		[learning rate: 0.0045524]
		[batch 20/20] avg loss: 0.19759031476385078		[learning rate: 0.0045441]
	Learning Rate: 0.00454409
	LOSS [training: 0.2019544937691739 | validation: 0.17703127168017532]
	TIME [epoch: 8.34 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21836581402581395		[learning rate: 0.0045358]
		[batch 20/20] avg loss: 0.2399256903164607		[learning rate: 0.0045276]
	Learning Rate: 0.0045276
	LOSS [training: 0.2291457521711373 | validation: 0.37740985741281785]
	TIME [epoch: 8.33 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27730168933811883		[learning rate: 0.0045194]
		[batch 20/20] avg loss: 0.16460456924746633		[learning rate: 0.0045112]
	Learning Rate: 0.00451117
	LOSS [training: 0.2209531292927926 | validation: 0.09979166049352624]
	TIME [epoch: 8.33 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19510982084853729		[learning rate: 0.004503]
		[batch 20/20] avg loss: 0.25968787291357864		[learning rate: 0.0044948]
	Learning Rate: 0.0044948
	LOSS [training: 0.22739884688105808 | validation: 0.05232075461978308]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18419655179227387		[learning rate: 0.0044866]
		[batch 20/20] avg loss: 0.20349193571563434		[learning rate: 0.0044785]
	Learning Rate: 0.00447849
	LOSS [training: 0.19384424375395412 | validation: 0.05221874887420027]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19851891082746545		[learning rate: 0.0044704]
		[batch 20/20] avg loss: 0.1575569172379079		[learning rate: 0.0044622]
	Learning Rate: 0.00446224
	LOSS [training: 0.17803791403268668 | validation: 0.08576801117456732]
	TIME [epoch: 8.35 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2104241482629134		[learning rate: 0.0044541]
		[batch 20/20] avg loss: 0.14596626135589522		[learning rate: 0.004446]
	Learning Rate: 0.00444604
	LOSS [training: 0.17819520480940435 | validation: 0.15407788770372113]
	TIME [epoch: 8.32 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19814016044819494		[learning rate: 0.004438]
		[batch 20/20] avg loss: 0.18900371995362955		[learning rate: 0.0044299]
	Learning Rate: 0.00442991
	LOSS [training: 0.19357194020091226 | validation: 0.06677827247260894]
	TIME [epoch: 8.3 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22057011007109883		[learning rate: 0.0044219]
		[batch 20/20] avg loss: 0.3129244556771913		[learning rate: 0.0044138]
	Learning Rate: 0.00441383
	LOSS [training: 0.2667472828741451 | validation: 0.17376807959329066]
	TIME [epoch: 8.32 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21264358377884168		[learning rate: 0.0044058]
		[batch 20/20] avg loss: 0.20511823693483833		[learning rate: 0.0043978]
	Learning Rate: 0.00439781
	LOSS [training: 0.20888091035683995 | validation: 0.326310716965547]
	TIME [epoch: 8.35 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1658943160007261		[learning rate: 0.0043898]
		[batch 20/20] avg loss: 0.20849696145145233		[learning rate: 0.0043819]
	Learning Rate: 0.00438185
	LOSS [training: 0.1871956387260892 | validation: 0.1132769233430748]
	TIME [epoch: 8.31 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2266380491770792		[learning rate: 0.0043739]
		[batch 20/20] avg loss: 0.1811400854375192		[learning rate: 0.004366]
	Learning Rate: 0.00436595
	LOSS [training: 0.2038890673072992 | validation: 0.21798049410764703]
	TIME [epoch: 8.29 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32410981938320277		[learning rate: 0.004358]
		[batch 20/20] avg loss: 0.23429285452354262		[learning rate: 0.0043501]
	Learning Rate: 0.00435011
	LOSS [training: 0.27920133695337274 | validation: 0.05380834277754741]
	TIME [epoch: 8.31 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21324363428481105		[learning rate: 0.0043422]
		[batch 20/20] avg loss: 0.11713477772808648		[learning rate: 0.0043343]
	Learning Rate: 0.00433432
	LOSS [training: 0.16518920600644876 | validation: 0.14377340365346936]
	TIME [epoch: 8.35 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19433440393158097		[learning rate: 0.0043264]
		[batch 20/20] avg loss: 0.30525901036041486		[learning rate: 0.0043186]
	Learning Rate: 0.00431859
	LOSS [training: 0.2497967071459979 | validation: 0.14221165728531723]
	TIME [epoch: 8.32 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16495158194513954		[learning rate: 0.0043107]
		[batch 20/20] avg loss: 0.20270759147281833		[learning rate: 0.0043029]
	Learning Rate: 0.00430292
	LOSS [training: 0.18382958670897898 | validation: 0.16441724809452934]
	TIME [epoch: 8.3 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2155581357556465		[learning rate: 0.0042951]
		[batch 20/20] avg loss: 0.21558925773263898		[learning rate: 0.0042873]
	Learning Rate: 0.0042873
	LOSS [training: 0.2155736967441427 | validation: 0.2974608984447923]
	TIME [epoch: 8.3 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2147201767670571		[learning rate: 0.0042795]
		[batch 20/20] avg loss: 0.15942882495946495		[learning rate: 0.0042717]
	Learning Rate: 0.00427174
	LOSS [training: 0.187074500863261 | validation: 0.18729517096284648]
	TIME [epoch: 8.35 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1890049476674578		[learning rate: 0.004264]
		[batch 20/20] avg loss: 0.20670023546436891		[learning rate: 0.0042562]
	Learning Rate: 0.00425624
	LOSS [training: 0.19785259156591337 | validation: 0.17089014039317332]
	TIME [epoch: 8.32 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23941106516746222		[learning rate: 0.0042485]
		[batch 20/20] avg loss: 0.2821874301951494		[learning rate: 0.0042408]
	Learning Rate: 0.0042408
	LOSS [training: 0.2607992476813058 | validation: 0.08739663890149871]
	TIME [epoch: 8.29 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21934743117101937		[learning rate: 0.0042331]
		[batch 20/20] avg loss: 0.22607371811923435		[learning rate: 0.0042254]
	Learning Rate: 0.00422541
	LOSS [training: 0.22271057464512686 | validation: 0.41185605708347606]
	TIME [epoch: 8.3 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13898793451793362		[learning rate: 0.0042177]
		[batch 20/20] avg loss: 0.5340102407568266		[learning rate: 0.0042101]
	Learning Rate: 0.00421007
	LOSS [training: 0.3364990876373801 | validation: 0.13691919356778254]
	TIME [epoch: 8.36 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20523688314760385		[learning rate: 0.0042024]
		[batch 20/20] avg loss: 0.1666442971053821		[learning rate: 0.0041948]
	Learning Rate: 0.00419479
	LOSS [training: 0.18594059012649297 | validation: 0.1252314564738241]
	TIME [epoch: 8.32 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.240969765625336		[learning rate: 0.0041872]
		[batch 20/20] avg loss: 0.1890908186453072		[learning rate: 0.0041796]
	Learning Rate: 0.00417957
	LOSS [training: 0.2150302921353216 | validation: 0.06525635375584833]
	TIME [epoch: 8.29 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20353602989972552		[learning rate: 0.004172]
		[batch 20/20] avg loss: 0.15460934756761638		[learning rate: 0.0041644]
	Learning Rate: 0.0041644
	LOSS [training: 0.17907268873367094 | validation: 0.22383852820556904]
	TIME [epoch: 8.3 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13426627622574888		[learning rate: 0.0041568]
		[batch 20/20] avg loss: 0.12526805890339557		[learning rate: 0.0041493]
	Learning Rate: 0.00414929
	LOSS [training: 0.12976716756457224 | validation: 0.4657715610008879]
	TIME [epoch: 8.34 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20742605016182813		[learning rate: 0.0041418]
		[batch 20/20] avg loss: 0.18393466466443237		[learning rate: 0.0041342]
	Learning Rate: 0.00413423
	LOSS [training: 0.19568035741313025 | validation: 0.14060072546708446]
	TIME [epoch: 8.33 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13877823187825783		[learning rate: 0.0041267]
		[batch 20/20] avg loss: 0.2102666640900523		[learning rate: 0.0041192]
	Learning Rate: 0.00411923
	LOSS [training: 0.17452244798415506 | validation: 0.10653088409336214]
	TIME [epoch: 8.29 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23814905721735932		[learning rate: 0.0041117]
		[batch 20/20] avg loss: 0.18569861812722038		[learning rate: 0.0041043]
	Learning Rate: 0.00410428
	LOSS [training: 0.2119238376722899 | validation: 0.13218435688781072]
	TIME [epoch: 8.3 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2803480570929383		[learning rate: 0.0040968]
		[batch 20/20] avg loss: 0.21966457307951442		[learning rate: 0.0040894]
	Learning Rate: 0.00408938
	LOSS [training: 0.25000631508622634 | validation: 0.16156090997474357]
	TIME [epoch: 8.33 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17923363174935966		[learning rate: 0.004082]
		[batch 20/20] avg loss: 0.22235006559099774		[learning rate: 0.0040745]
	Learning Rate: 0.00407454
	LOSS [training: 0.20079184867017869 | validation: 0.21817241338141097]
	TIME [epoch: 8.33 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1668873365542483		[learning rate: 0.0040671]
		[batch 20/20] avg loss: 0.14608474533985671		[learning rate: 0.0040598]
	Learning Rate: 0.00405976
	LOSS [training: 0.1564860409470525 | validation: 0.16711747460882134]
	TIME [epoch: 8.29 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15732772720947175		[learning rate: 0.0040524]
		[batch 20/20] avg loss: 0.21863449326399848		[learning rate: 0.004045]
	Learning Rate: 0.00404502
	LOSS [training: 0.18798111023673508 | validation: 0.13432956962874992]
	TIME [epoch: 8.3 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19639218900545527		[learning rate: 0.0040377]
		[batch 20/20] avg loss: 0.17305008590791388		[learning rate: 0.0040303]
	Learning Rate: 0.00403034
	LOSS [training: 0.18472113745668456 | validation: 0.1721287448986987]
	TIME [epoch: 8.34 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15575230542207036		[learning rate: 0.004023]
		[batch 20/20] avg loss: 0.14945709635284893		[learning rate: 0.0040157]
	Learning Rate: 0.00401572
	LOSS [training: 0.15260470088745964 | validation: 0.18125325996350306]
	TIME [epoch: 8.32 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1936871544992477		[learning rate: 0.0040084]
		[batch 20/20] avg loss: 0.17034092188394223		[learning rate: 0.0040011]
	Learning Rate: 0.00400114
	LOSS [training: 0.18201403819159492 | validation: 0.24711369844153602]
	TIME [epoch: 8.31 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2785609286613441		[learning rate: 0.0039939]
		[batch 20/20] avg loss: 0.144517028412343		[learning rate: 0.0039866]
	Learning Rate: 0.00398662
	LOSS [training: 0.21153897853684356 | validation: 0.16790088102935619]
	TIME [epoch: 8.29 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17371975174734103		[learning rate: 0.0039794]
		[batch 20/20] avg loss: 0.17885373161663493		[learning rate: 0.0039722]
	Learning Rate: 0.00397216
	LOSS [training: 0.176286741681988 | validation: 0.3296843797905581]
	TIME [epoch: 8.34 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18594424613651447		[learning rate: 0.0039649]
		[batch 20/20] avg loss: 0.18619503844272506		[learning rate: 0.0039577]
	Learning Rate: 0.00395774
	LOSS [training: 0.18606964228961975 | validation: 0.05185740368813589]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17891758083234094		[learning rate: 0.0039506]
		[batch 20/20] avg loss: 0.170084936971682		[learning rate: 0.0039434]
	Learning Rate: 0.00394338
	LOSS [training: 0.17450125890201146 | validation: 0.022742806330732578]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09536166832048742		[learning rate: 0.0039362]
		[batch 20/20] avg loss: 0.1873073732035167		[learning rate: 0.0039291]
	Learning Rate: 0.00392907
	LOSS [training: 0.14133452076200206 | validation: 0.20619798103307652]
	TIME [epoch: 8.31 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15723312250263793		[learning rate: 0.0039219]
		[batch 20/20] avg loss: 0.21854768466584534		[learning rate: 0.0039148]
	Learning Rate: 0.00391481
	LOSS [training: 0.18789040358424164 | validation: 0.11337377793160461]
	TIME [epoch: 8.31 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20989337546100417		[learning rate: 0.0039077]
		[batch 20/20] avg loss: 0.17665573613058907		[learning rate: 0.0039006]
	Learning Rate: 0.0039006
	LOSS [training: 0.1932745557957966 | validation: 0.18947126077384485]
	TIME [epoch: 8.29 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14296497851267076		[learning rate: 0.0038935]
		[batch 20/20] avg loss: 0.11954280258237722		[learning rate: 0.0038864]
	Learning Rate: 0.00388645
	LOSS [training: 0.13125389054752395 | validation: 0.15403391325107457]
	TIME [epoch: 8.31 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20574967287074478		[learning rate: 0.0038794]
		[batch 20/20] avg loss: 0.2436248081441843		[learning rate: 0.0038723]
	Learning Rate: 0.00387234
	LOSS [training: 0.22468724050746453 | validation: 0.0402907026774204]
	TIME [epoch: 8.3 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12504053817569044		[learning rate: 0.0038653]
		[batch 20/20] avg loss: 0.09743133207953032		[learning rate: 0.0038583]
	Learning Rate: 0.00385829
	LOSS [training: 0.11123593512761036 | validation: 0.2464542577817994]
	TIME [epoch: 8.3 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15326853087372067		[learning rate: 0.0038513]
		[batch 20/20] avg loss: 0.179816656202482		[learning rate: 0.0038443]
	Learning Rate: 0.00384429
	LOSS [training: 0.16654259353810133 | validation: 0.09528350038981476]
	TIME [epoch: 8.29 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13370278970995764		[learning rate: 0.0038373]
		[batch 20/20] avg loss: 0.10126675963281492		[learning rate: 0.0038303]
	Learning Rate: 0.00383034
	LOSS [training: 0.11748477467138627 | validation: 0.061322846417660545]
	TIME [epoch: 8.3 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12890304659770319		[learning rate: 0.0038234]
		[batch 20/20] avg loss: 0.20222437299097837		[learning rate: 0.0038164]
	Learning Rate: 0.00381644
	LOSS [training: 0.16556370979434076 | validation: 0.08882145363733387]
	TIME [epoch: 8.32 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1571021611629823		[learning rate: 0.0038095]
		[batch 20/20] avg loss: 0.1658887996850059		[learning rate: 0.0038026]
	Learning Rate: 0.00380258
	LOSS [training: 0.16149548042399414 | validation: 0.07045539106213794]
	TIME [epoch: 8.31 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13318833857458395		[learning rate: 0.0037957]
		[batch 20/20] avg loss: 0.20160611447824067		[learning rate: 0.0037888]
	Learning Rate: 0.00378879
	LOSS [training: 0.16739722652641228 | validation: 0.06051716109474906]
	TIME [epoch: 8.28 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15645808936162794		[learning rate: 0.0037819]
		[batch 20/20] avg loss: 0.15286600259009048		[learning rate: 0.003775]
	Learning Rate: 0.00377504
	LOSS [training: 0.15466204597585925 | validation: 0.18366492239784815]
	TIME [epoch: 8.29 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1415559983053931		[learning rate: 0.0037682]
		[batch 20/20] avg loss: 0.17686097183287713		[learning rate: 0.0037613]
	Learning Rate: 0.00376134
	LOSS [training: 0.15920848506913515 | validation: 0.3820460843118278]
	TIME [epoch: 8.33 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23365153719459447		[learning rate: 0.0037545]
		[batch 20/20] avg loss: 0.11222117313737238		[learning rate: 0.0037477]
	Learning Rate: 0.00374769
	LOSS [training: 0.17293635516598344 | validation: 0.14270609198559436]
	TIME [epoch: 8.3 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11803955105082267		[learning rate: 0.0037409]
		[batch 20/20] avg loss: 0.18400028648445094		[learning rate: 0.0037341]
	Learning Rate: 0.00373408
	LOSS [training: 0.15101991876763682 | validation: 0.06992409853271433]
	TIME [epoch: 8.27 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16039201481866694		[learning rate: 0.0037273]
		[batch 20/20] avg loss: 0.17905648468882954		[learning rate: 0.0037205]
	Learning Rate: 0.00372053
	LOSS [training: 0.16972424975374822 | validation: 0.04572093534569899]
	TIME [epoch: 8.28 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1031854842106251		[learning rate: 0.0037138]
		[batch 20/20] avg loss: 0.28185843462512794		[learning rate: 0.003707]
	Learning Rate: 0.00370703
	LOSS [training: 0.1925219594178765 | validation: 0.16337536133384944]
	TIME [epoch: 8.33 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20643984715388938		[learning rate: 0.0037003]
		[batch 20/20] avg loss: 0.10981958261616295		[learning rate: 0.0036936]
	Learning Rate: 0.00369358
	LOSS [training: 0.15812971488502614 | validation: 0.04196495730042681]
	TIME [epoch: 8.31 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14420646139265095		[learning rate: 0.0036869]
		[batch 20/20] avg loss: 0.12126503648073297		[learning rate: 0.0036802]
	Learning Rate: 0.00368017
	LOSS [training: 0.13273574893669196 | validation: 0.4505445203639771]
	TIME [epoch: 8.28 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22912506743779099		[learning rate: 0.0036735]
		[batch 20/20] avg loss: 0.17324099934013254		[learning rate: 0.0036668]
	Learning Rate: 0.00366682
	LOSS [training: 0.2011830333889618 | validation: 0.08833549777594635]
	TIME [epoch: 8.28 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11936317839942197		[learning rate: 0.0036602]
		[batch 20/20] avg loss: 0.16324696065106817		[learning rate: 0.0036535]
	Learning Rate: 0.00365351
	LOSS [training: 0.14130506952524508 | validation: 0.07268606062408155]
	TIME [epoch: 8.32 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10702777040164005		[learning rate: 0.0036469]
		[batch 20/20] avg loss: 0.22853668789119824		[learning rate: 0.0036403]
	Learning Rate: 0.00364025
	LOSS [training: 0.16778222914641913 | validation: 0.045371548590931454]
	TIME [epoch: 8.31 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20807519215967485		[learning rate: 0.0036336]
		[batch 20/20] avg loss: 0.12487125855105205		[learning rate: 0.003627]
	Learning Rate: 0.00362704
	LOSS [training: 0.16647322535536346 | validation: 0.2702899247493135]
	TIME [epoch: 8.28 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2010303434632393		[learning rate: 0.0036205]
		[batch 20/20] avg loss: 0.14498883578390118		[learning rate: 0.0036139]
	Learning Rate: 0.00361388
	LOSS [training: 0.17300958962357027 | validation: 0.08046429797903631]
	TIME [epoch: 8.28 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19448198623680601		[learning rate: 0.0036073]
		[batch 20/20] avg loss: 0.12578251869974783		[learning rate: 0.0036008]
	Learning Rate: 0.00360076
	LOSS [training: 0.16013225246827695 | validation: 0.10954774672651933]
	TIME [epoch: 8.3 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.135011704680081		[learning rate: 0.0035942]
		[batch 20/20] avg loss: 0.22623314364517144		[learning rate: 0.0035877]
	Learning Rate: 0.0035877
	LOSS [training: 0.18062242416262622 | validation: 0.3583345845272322]
	TIME [epoch: 8.33 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18336450503523066		[learning rate: 0.0035812]
		[batch 20/20] avg loss: 0.15439384572679718		[learning rate: 0.0035747]
	Learning Rate: 0.00357468
	LOSS [training: 0.16887917538101394 | validation: 0.19486719564832694]
	TIME [epoch: 8.28 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17822714935228784		[learning rate: 0.0035682]
		[batch 20/20] avg loss: 0.11772893803941234		[learning rate: 0.0035617]
	Learning Rate: 0.0035617
	LOSS [training: 0.14797804369585008 | validation: 0.06145421918478297]
	TIME [epoch: 8.28 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16876214379732524		[learning rate: 0.0035552]
		[batch 20/20] avg loss: 0.14391284292593864		[learning rate: 0.0035488]
	Learning Rate: 0.00354878
	LOSS [training: 0.15633749336163189 | validation: 0.03672095547386196]
	TIME [epoch: 8.3 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1351239751027745		[learning rate: 0.0035423]
		[batch 20/20] avg loss: 0.16296125855308322		[learning rate: 0.0035359]
	Learning Rate: 0.0035359
	LOSS [training: 0.14904261682792885 | validation: 0.1295597852445193]
	TIME [epoch: 8.32 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1722607171883456		[learning rate: 0.0035295]
		[batch 20/20] avg loss: 0.11688006950395606		[learning rate: 0.0035231]
	Learning Rate: 0.00352307
	LOSS [training: 0.1445703933461508 | validation: 0.028740094682332865]
	TIME [epoch: 8.28 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1383956208943452		[learning rate: 0.0035167]
		[batch 20/20] avg loss: 0.1337426022481689		[learning rate: 0.0035103]
	Learning Rate: 0.00351028
	LOSS [training: 0.13606911157125703 | validation: 0.14789645548474892]
	TIME [epoch: 8.29 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16931120906438363		[learning rate: 0.0035039]
		[batch 20/20] avg loss: 0.15848632590892903		[learning rate: 0.0034975]
	Learning Rate: 0.00349754
	LOSS [training: 0.1638987674866563 | validation: 0.03343826203629597]
	TIME [epoch: 8.31 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19222834061225055		[learning rate: 0.0034912]
		[batch 20/20] avg loss: 0.14316195791870073		[learning rate: 0.0034849]
	Learning Rate: 0.00348485
	LOSS [training: 0.1676951492654757 | validation: 0.08376879528829492]
	TIME [epoch: 8.3 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11309450720741272		[learning rate: 0.0034785]
		[batch 20/20] avg loss: 0.2514636309680817		[learning rate: 0.0034722]
	Learning Rate: 0.0034722
	LOSS [training: 0.18227906908774721 | validation: 0.18694575961975335]
	TIME [epoch: 8.29 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13528631107495137		[learning rate: 0.0034659]
		[batch 20/20] avg loss: 0.22653773029833718		[learning rate: 0.0034596]
	Learning Rate: 0.0034596
	LOSS [training: 0.18091202068664422 | validation: 0.2400153892161378]
	TIME [epoch: 8.28 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16317991146864597		[learning rate: 0.0034533]
		[batch 20/20] avg loss: 0.1316564887667254		[learning rate: 0.003447]
	Learning Rate: 0.00344705
	LOSS [training: 0.1474182001176857 | validation: 0.11940341234414253]
	TIME [epoch: 8.31 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15492892573012432		[learning rate: 0.0034408]
		[batch 20/20] avg loss: 0.17830314925721547		[learning rate: 0.0034345]
	Learning Rate: 0.00343454
	LOSS [training: 0.16661603749366988 | validation: 0.08275560370193183]
	TIME [epoch: 8.29 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0980195869343901		[learning rate: 0.0034283]
		[batch 20/20] avg loss: 0.15494877267456345		[learning rate: 0.0034221]
	Learning Rate: 0.00342207
	LOSS [training: 0.12648417980447674 | validation: 0.28844804412836655]
	TIME [epoch: 8.31 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14593554270558284		[learning rate: 0.0034159]
		[batch 20/20] avg loss: 0.12488897369986837		[learning rate: 0.0034097]
	Learning Rate: 0.00340966
	LOSS [training: 0.13541225820272562 | validation: 0.1469688266662429]
	TIME [epoch: 8.29 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19670792850120367		[learning rate: 0.0034035]
		[batch 20/20] avg loss: 0.11933042139873862		[learning rate: 0.0033973]
	Learning Rate: 0.00339728
	LOSS [training: 0.15801917494997114 | validation: 0.16761485882415966]
	TIME [epoch: 8.32 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08466533343236292		[learning rate: 0.0033911]
		[batch 20/20] avg loss: 0.10945986917496364		[learning rate: 0.003385]
	Learning Rate: 0.00338495
	LOSS [training: 0.09706260130366329 | validation: 0.06944055932106694]
	TIME [epoch: 8.28 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1344756796445363		[learning rate: 0.0033788]
		[batch 20/20] avg loss: 0.16037718539363252		[learning rate: 0.0033727]
	Learning Rate: 0.00337267
	LOSS [training: 0.1474264325190844 | validation: 0.10782482414011814]
	TIME [epoch: 8.3 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16703169181446137		[learning rate: 0.0033665]
		[batch 20/20] avg loss: 0.18956429738877761		[learning rate: 0.0033604]
	Learning Rate: 0.00336043
	LOSS [training: 0.1782979946016195 | validation: 0.11145631997150192]
	TIME [epoch: 8.3 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19768090016603682		[learning rate: 0.0033543]
		[batch 20/20] avg loss: 0.12229569792065957		[learning rate: 0.0033482]
	Learning Rate: 0.00334823
	LOSS [training: 0.15998829904334816 | validation: 0.03469117612383453]
	TIME [epoch: 8.32 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12499960498441445		[learning rate: 0.0033422]
		[batch 20/20] avg loss: 0.17640510427000325		[learning rate: 0.0033361]
	Learning Rate: 0.00333608
	LOSS [training: 0.15070235462720888 | validation: 0.14746856420542673]
	TIME [epoch: 8.29 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19042893256611354		[learning rate: 0.00333]
		[batch 20/20] avg loss: 0.09222479655039975		[learning rate: 0.003324]
	Learning Rate: 0.00332398
	LOSS [training: 0.14132686455825666 | validation: 0.2694030227861853]
	TIME [epoch: 8.29 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13503721298923343		[learning rate: 0.0033179]
		[batch 20/20] avg loss: 0.11787277824811986		[learning rate: 0.0033119]
	Learning Rate: 0.00331191
	LOSS [training: 0.12645499561867662 | validation: 0.06665588084709706]
	TIME [epoch: 8.31 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13308298677308344		[learning rate: 0.0033059]
		[batch 20/20] avg loss: 0.18174667901457292		[learning rate: 0.0032999]
	Learning Rate: 0.00329989
	LOSS [training: 0.15741483289382818 | validation: 0.16763995092442963]
	TIME [epoch: 8.32 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13865287781820923		[learning rate: 0.0032939]
		[batch 20/20] avg loss: 0.1443982879224705		[learning rate: 0.0032879]
	Learning Rate: 0.00328792
	LOSS [training: 0.1415255828703399 | validation: 0.1647339292043204]
	TIME [epoch: 8.29 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16439309942914984		[learning rate: 0.0032819]
		[batch 20/20] avg loss: 0.173172001421146		[learning rate: 0.003276]
	Learning Rate: 0.00327599
	LOSS [training: 0.1687825504251479 | validation: 0.10199663185986405]
	TIME [epoch: 8.28 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1289353606081141		[learning rate: 0.00327]
		[batch 20/20] avg loss: 0.16625858807856547		[learning rate: 0.0032641]
	Learning Rate: 0.0032641
	LOSS [training: 0.1475969743433398 | validation: 0.1715649516467543]
	TIME [epoch: 8.31 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1501633230151215		[learning rate: 0.0032582]
		[batch 20/20] avg loss: 0.12201822540725411		[learning rate: 0.0032523]
	Learning Rate: 0.00325225
	LOSS [training: 0.1360907742111878 | validation: 0.13192530581277556]
	TIME [epoch: 8.32 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13287490869457913		[learning rate: 0.0032463]
		[batch 20/20] avg loss: 0.14089068819874795		[learning rate: 0.0032404]
	Learning Rate: 0.00324045
	LOSS [training: 0.13688279844666357 | validation: 0.11362064940774555]
	TIME [epoch: 8.28 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12353088779880098		[learning rate: 0.0032346]
		[batch 20/20] avg loss: 0.23814098910405246		[learning rate: 0.0032287]
	Learning Rate: 0.00322869
	LOSS [training: 0.1808359384514267 | validation: 0.10233681553590124]
	TIME [epoch: 8.28 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1540755994317806		[learning rate: 0.0032228]
		[batch 20/20] avg loss: 0.19599425180632077		[learning rate: 0.003217]
	Learning Rate: 0.00321697
	LOSS [training: 0.17503492561905068 | validation: 0.0390503163462143]
	TIME [epoch: 8.3 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1298601602535924		[learning rate: 0.0032111]
		[batch 20/20] avg loss: 0.11962425096105789		[learning rate: 0.0032053]
	Learning Rate: 0.0032053
	LOSS [training: 0.12474220560732514 | validation: 0.05455402291602882]
	TIME [epoch: 8.34 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13438696402803835		[learning rate: 0.0031995]
		[batch 20/20] avg loss: 0.1678457801169762		[learning rate: 0.0031937]
	Learning Rate: 0.00319367
	LOSS [training: 0.15111637207250728 | validation: 0.17462657359585262]
	TIME [epoch: 8.28 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1199294329827656		[learning rate: 0.0031879]
		[batch 20/20] avg loss: 0.11613760043004347		[learning rate: 0.0031821]
	Learning Rate: 0.00318208
	LOSS [training: 0.11803351670640456 | validation: 0.12606135854759654]
	TIME [epoch: 8.28 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09813295288526531		[learning rate: 0.0031763]
		[batch 20/20] avg loss: 0.09312275038693349		[learning rate: 0.0031705]
	Learning Rate: 0.00317053
	LOSS [training: 0.09562785163609941 | validation: 0.11639726954370606]
	TIME [epoch: 8.28 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14341411994472614		[learning rate: 0.0031648]
		[batch 20/20] avg loss: 0.08685767294050478		[learning rate: 0.003159]
	Learning Rate: 0.00315902
	LOSS [training: 0.11513589644261543 | validation: 0.07134005599643319]
	TIME [epoch: 8.35 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10907433235311421		[learning rate: 0.0031533]
		[batch 20/20] avg loss: 0.08641628579769937		[learning rate: 0.0031476]
	Learning Rate: 0.00314756
	LOSS [training: 0.09774530907540678 | validation: 0.503907858190892]
	TIME [epoch: 8.28 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21868377397999628		[learning rate: 0.0031418]
		[batch 20/20] avg loss: 0.11556800507173734		[learning rate: 0.0031361]
	Learning Rate: 0.00313613
	LOSS [training: 0.16712588952586682 | validation: 0.18982267031092426]
	TIME [epoch: 8.28 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19096298067005502		[learning rate: 0.0031304]
		[batch 20/20] avg loss: 0.14311247744781075		[learning rate: 0.0031248]
	Learning Rate: 0.00312475
	LOSS [training: 0.16703772905893285 | validation: 0.17728632149121923]
	TIME [epoch: 8.29 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13238675474676423		[learning rate: 0.0031191]
		[batch 20/20] avg loss: 0.17003999510513673		[learning rate: 0.0031134]
	Learning Rate: 0.00311341
	LOSS [training: 0.15121337492595047 | validation: 0.3607495056729644]
	TIME [epoch: 8.35 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17327316464713757		[learning rate: 0.0031078]
		[batch 20/20] avg loss: 0.21859550773292788		[learning rate: 0.0031021]
	Learning Rate: 0.00310212
	LOSS [training: 0.1959343361900327 | validation: 0.31274036488936807]
	TIME [epoch: 8.28 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15536051314276558		[learning rate: 0.0030965]
		[batch 20/20] avg loss: 0.11931965410497973		[learning rate: 0.0030909]
	Learning Rate: 0.00309086
	LOSS [training: 0.13734008362387265 | validation: 0.07918675045586725]
	TIME [epoch: 8.28 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1027382768739867		[learning rate: 0.0030852]
		[batch 20/20] avg loss: 0.15996007089830638		[learning rate: 0.0030796]
	Learning Rate: 0.00307964
	LOSS [training: 0.1313491738861466 | validation: 0.07021025170760292]
	TIME [epoch: 8.28 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15440644631705608		[learning rate: 0.003074]
		[batch 20/20] avg loss: 0.09824644129682392		[learning rate: 0.0030685]
	Learning Rate: 0.00306846
	LOSS [training: 0.12632644380693997 | validation: 0.04947431466289679]
	TIME [epoch: 8.32 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08794217590208242		[learning rate: 0.0030629]
		[batch 20/20] avg loss: 0.10016850222780041		[learning rate: 0.0030573]
	Learning Rate: 0.00305733
	LOSS [training: 0.09405533906494143 | validation: 0.06020112682943218]
	TIME [epoch: 8.31 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17174842765141063		[learning rate: 0.0030518]
		[batch 20/20] avg loss: 0.13952685406244122		[learning rate: 0.0030462]
	Learning Rate: 0.00304623
	LOSS [training: 0.15563764085692594 | validation: 0.20707755032368386]
	TIME [epoch: 8.28 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12092689268671757		[learning rate: 0.0030407]
		[batch 20/20] avg loss: 0.10353749960810507		[learning rate: 0.0030352]
	Learning Rate: 0.00303518
	LOSS [training: 0.11223219614741131 | validation: 0.06542700690020897]
	TIME [epoch: 8.3 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17253308254656227		[learning rate: 0.0030297]
		[batch 20/20] avg loss: 0.17429610957308758		[learning rate: 0.0030242]
	Learning Rate: 0.00302416
	LOSS [training: 0.17341459605982493 | validation: 0.08618084863477685]
	TIME [epoch: 8.32 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1444708492280134		[learning rate: 0.0030187]
		[batch 20/20] avg loss: 0.16921217749782333		[learning rate: 0.0030132]
	Learning Rate: 0.00301319
	LOSS [training: 0.15684151336291835 | validation: 0.22143461658417266]
	TIME [epoch: 8.31 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15528928296069838		[learning rate: 0.0030077]
		[batch 20/20] avg loss: 0.14969722285686488		[learning rate: 0.0030023]
	Learning Rate: 0.00300225
	LOSS [training: 0.15249325290878163 | validation: 0.1618862780684742]
	TIME [epoch: 8.29 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18560320310297512		[learning rate: 0.0029968]
		[batch 20/20] avg loss: 0.1875796544144603		[learning rate: 0.0029914]
	Learning Rate: 0.00299136
	LOSS [training: 0.1865914287587177 | validation: 0.22447268315378782]
	TIME [epoch: 8.3 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12957001650346073		[learning rate: 0.0029859]
		[batch 20/20] avg loss: 0.11916950260992734		[learning rate: 0.0029805]
	Learning Rate: 0.0029805
	LOSS [training: 0.12436975955669405 | validation: 0.13379262627688143]
	TIME [epoch: 8.32 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0833393267628298		[learning rate: 0.0029751]
		[batch 20/20] avg loss: 0.14328284992924806		[learning rate: 0.0029697]
	Learning Rate: 0.00296969
	LOSS [training: 0.11331108834603894 | validation: 0.08686808324799923]
	TIME [epoch: 8.3 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15827758468218883		[learning rate: 0.0029643]
		[batch 20/20] avg loss: 0.12752727743079956		[learning rate: 0.0029589]
	Learning Rate: 0.00295891
	LOSS [training: 0.1429024310564942 | validation: 0.09170278975843703]
	TIME [epoch: 8.3 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1269789333433132		[learning rate: 0.0029535]
		[batch 20/20] avg loss: 0.12068591025009268		[learning rate: 0.0029482]
	Learning Rate: 0.00294817
	LOSS [training: 0.12383242179670295 | validation: 0.15947138884734996]
	TIME [epoch: 8.31 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13036909782051115		[learning rate: 0.0029428]
		[batch 20/20] avg loss: 0.10677978643437538		[learning rate: 0.0029375]
	Learning Rate: 0.00293747
	LOSS [training: 0.11857444212744328 | validation: 0.12027034988711954]
	TIME [epoch: 8.31 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13108587426394297		[learning rate: 0.0029321]
		[batch 20/20] avg loss: 0.12526909572146833		[learning rate: 0.0029268]
	Learning Rate: 0.00292681
	LOSS [training: 0.12817748499270562 | validation: 0.0624379556263171]
	TIME [epoch: 8.29 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1330195713105565		[learning rate: 0.0029215]
		[batch 20/20] avg loss: 0.20192715948969125		[learning rate: 0.0029162]
	Learning Rate: 0.00291619
	LOSS [training: 0.1674733654001239 | validation: 0.0741386941369647]
	TIME [epoch: 8.31 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12495673613922202		[learning rate: 0.0029109]
		[batch 20/20] avg loss: 0.14539608341970617		[learning rate: 0.0029056]
	Learning Rate: 0.00290561
	LOSS [training: 0.13517640977946407 | validation: 0.12939599478101121]
	TIME [epoch: 8.31 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13721603656873738		[learning rate: 0.0029003]
		[batch 20/20] avg loss: 0.10695518512158313		[learning rate: 0.0028951]
	Learning Rate: 0.00289506
	LOSS [training: 0.12208561084516026 | validation: 0.2107790482171425]
	TIME [epoch: 8.31 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15945122317026839		[learning rate: 0.0028898]
		[batch 20/20] avg loss: 0.16737878351237395		[learning rate: 0.0028846]
	Learning Rate: 0.00288456
	LOSS [training: 0.1634150033413212 | validation: 0.05384517233440612]
	TIME [epoch: 8.29 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13134304474295874		[learning rate: 0.0028793]
		[batch 20/20] avg loss: 0.10799056107130048		[learning rate: 0.0028741]
	Learning Rate: 0.00287409
	LOSS [training: 0.11966680290712961 | validation: 0.07870269918051387]
	TIME [epoch: 8.32 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14363641473835911		[learning rate: 0.0028689]
		[batch 20/20] avg loss: 0.13376983666755254		[learning rate: 0.0028637]
	Learning Rate: 0.00286366
	LOSS [training: 0.13870312570295587 | validation: 0.19247072962941753]
	TIME [epoch: 8.31 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11046039327930338		[learning rate: 0.0028585]
		[batch 20/20] avg loss: 0.17094521902841922		[learning rate: 0.0028533]
	Learning Rate: 0.00285326
	LOSS [training: 0.14070280615386127 | validation: 0.17938432751630368]
	TIME [epoch: 8.31 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13421470846672823		[learning rate: 0.0028481]
		[batch 20/20] avg loss: 0.08859589023671086		[learning rate: 0.0028429]
	Learning Rate: 0.00284291
	LOSS [training: 0.11140529935171954 | validation: 0.04819055368011384]
	TIME [epoch: 8.29 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1524716577928328		[learning rate: 0.0028377]
		[batch 20/20] avg loss: 0.13652332295335878		[learning rate: 0.0028326]
	Learning Rate: 0.00283259
	LOSS [training: 0.1444974903730958 | validation: 0.14498595095310002]
	TIME [epoch: 8.31 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09896364257990474		[learning rate: 0.0028274]
		[batch 20/20] avg loss: 0.11794826086252597		[learning rate: 0.0028223]
	Learning Rate: 0.00282231
	LOSS [training: 0.10845595172121536 | validation: 0.1865627797712769]
	TIME [epoch: 8.32 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11124264233956538		[learning rate: 0.0028172]
		[batch 20/20] avg loss: 0.12040765769102028		[learning rate: 0.0028121]
	Learning Rate: 0.00281207
	LOSS [training: 0.11582515001529284 | validation: 0.08104992722868691]
	TIME [epoch: 8.31 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11504149568205799		[learning rate: 0.002807]
		[batch 20/20] avg loss: 0.08349242829726021		[learning rate: 0.0028019]
	Learning Rate: 0.00280187
	LOSS [training: 0.09926696198965909 | validation: 0.22429469772927751]
	TIME [epoch: 8.29 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15134016411060786		[learning rate: 0.0027968]
		[batch 20/20] avg loss: 0.12972751003781036		[learning rate: 0.0027917]
	Learning Rate: 0.0027917
	LOSS [training: 0.1405338370742091 | validation: 0.12510657895492983]
	TIME [epoch: 8.29 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18045950053572796		[learning rate: 0.0027866]
		[batch 20/20] avg loss: 0.19045826703035476		[learning rate: 0.0027816]
	Learning Rate: 0.00278157
	LOSS [training: 0.18545888378304137 | validation: 0.10751094282783287]
	TIME [epoch: 8.34 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08742580313014159		[learning rate: 0.0027765]
		[batch 20/20] avg loss: 0.15645993358265903		[learning rate: 0.0027715]
	Learning Rate: 0.00277147
	LOSS [training: 0.1219428683564003 | validation: 0.047096989728107434]
	TIME [epoch: 8.3 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11729529385207746		[learning rate: 0.0027664]
		[batch 20/20] avg loss: 0.09322471408926261		[learning rate: 0.0027614]
	Learning Rate: 0.00276141
	LOSS [training: 0.10526000397067004 | validation: 0.08148481033861399]
	TIME [epoch: 8.29 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1021585018527437		[learning rate: 0.0027564]
		[batch 20/20] avg loss: 0.13350387222907384		[learning rate: 0.0027514]
	Learning Rate: 0.00275139
	LOSS [training: 0.11783118704090874 | validation: 0.03976287690532782]
	TIME [epoch: 8.29 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07271079295213637		[learning rate: 0.0027464]
		[batch 20/20] avg loss: 0.19396204009352408		[learning rate: 0.0027414]
	Learning Rate: 0.00274141
	LOSS [training: 0.13333641652283026 | validation: 0.0744019038401019]
	TIME [epoch: 8.34 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16183119001112747		[learning rate: 0.0027364]
		[batch 20/20] avg loss: 0.11009754903402205		[learning rate: 0.0027315]
	Learning Rate: 0.00273146
	LOSS [training: 0.1359643695225747 | validation: 0.05299121298911127]
	TIME [epoch: 8.3 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12477842735561857		[learning rate: 0.0027265]
		[batch 20/20] avg loss: 0.10095538895346398		[learning rate: 0.0027215]
	Learning Rate: 0.00272155
	LOSS [training: 0.1128669081545413 | validation: 0.07349275965753516]
	TIME [epoch: 8.29 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13520516984173156		[learning rate: 0.0027166]
		[batch 20/20] avg loss: 0.1329255733466004		[learning rate: 0.0027117]
	Learning Rate: 0.00271167
	LOSS [training: 0.134065371594166 | validation: 0.155292114938965]
	TIME [epoch: 8.29 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14765835017061452		[learning rate: 0.0027067]
		[batch 20/20] avg loss: 0.11707734443290436		[learning rate: 0.0027018]
	Learning Rate: 0.00270183
	LOSS [training: 0.13236784730175943 | validation: 0.11354838833543038]
	TIME [epoch: 8.34 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11784934212767044		[learning rate: 0.0026969]
		[batch 20/20] avg loss: 0.11084631648273886		[learning rate: 0.002692]
	Learning Rate: 0.00269202
	LOSS [training: 0.11434782930520467 | validation: 0.04774337848440626]
	TIME [epoch: 8.3 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13124660790074705		[learning rate: 0.0026871]
		[batch 20/20] avg loss: 0.09352014352738357		[learning rate: 0.0026823]
	Learning Rate: 0.00268225
	LOSS [training: 0.11238337571406531 | validation: 0.03186171905886721]
	TIME [epoch: 8.29 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09666141239982856		[learning rate: 0.0026774]
		[batch 20/20] avg loss: 0.1445547256966169		[learning rate: 0.0026725]
	Learning Rate: 0.00267252
	LOSS [training: 0.1206080690482227 | validation: 0.09277773985495787]
	TIME [epoch: 8.29 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08273993633937327		[learning rate: 0.0026677]
		[batch 20/20] avg loss: 0.1352151216914533		[learning rate: 0.0026628]
	Learning Rate: 0.00266282
	LOSS [training: 0.10897752901541327 | validation: 0.33646662901653107]
	TIME [epoch: 8.33 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1714414220204901		[learning rate: 0.002658]
		[batch 20/20] avg loss: 0.1049656817019414		[learning rate: 0.0026532]
	Learning Rate: 0.00265316
	LOSS [training: 0.13820355186121575 | validation: 0.0654833651005089]
	TIME [epoch: 8.32 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1202889232719494		[learning rate: 0.0026483]
		[batch 20/20] avg loss: 0.1242157862483441		[learning rate: 0.0026435]
	Learning Rate: 0.00264353
	LOSS [training: 0.12225235476014676 | validation: 0.0443179566321327]
	TIME [epoch: 8.29 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22257008889911659		[learning rate: 0.0026387]
		[batch 20/20] avg loss: 0.08269285608926143		[learning rate: 0.0026339]
	Learning Rate: 0.00263394
	LOSS [training: 0.152631472494189 | validation: 0.09194941617356492]
	TIME [epoch: 8.29 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10870670277270719		[learning rate: 0.0026292]
		[batch 20/20] avg loss: 0.09049180914799602		[learning rate: 0.0026244]
	Learning Rate: 0.00262438
	LOSS [training: 0.09959925596035162 | validation: 0.06661227599311487]
	TIME [epoch: 8.33 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10185000023572972		[learning rate: 0.0026196]
		[batch 20/20] avg loss: 0.10282662855671695		[learning rate: 0.0026149]
	Learning Rate: 0.00261485
	LOSS [training: 0.10233831439622336 | validation: 0.2372648376918116]
	TIME [epoch: 8.32 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1245254239465586		[learning rate: 0.0026101]
		[batch 20/20] avg loss: 0.1589751879593654		[learning rate: 0.0026054]
	Learning Rate: 0.00260536
	LOSS [training: 0.141750305952962 | validation: 0.1260612452853588]
	TIME [epoch: 8.3 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15526183205996857		[learning rate: 0.0026006]
		[batch 20/20] avg loss: 0.1468109252759165		[learning rate: 0.0025959]
	Learning Rate: 0.00259591
	LOSS [training: 0.15103637866794253 | validation: 0.13717841171106265]
	TIME [epoch: 8.31 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14792850937338622		[learning rate: 0.0025912]
		[batch 20/20] avg loss: 0.18856891079625265		[learning rate: 0.0025865]
	Learning Rate: 0.00258649
	LOSS [training: 0.16824871008481948 | validation: 0.15782344461647396]
	TIME [epoch: 8.32 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1559188815946852		[learning rate: 0.0025818]
		[batch 20/20] avg loss: 0.1089167053370941		[learning rate: 0.0025771]
	Learning Rate: 0.0025771
	LOSS [training: 0.1324177934658897 | validation: 0.1186202105605854]
	TIME [epoch: 8.31 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12727867092927753		[learning rate: 0.0025724]
		[batch 20/20] avg loss: 0.11134089310513753		[learning rate: 0.0025677]
	Learning Rate: 0.00256775
	LOSS [training: 0.11930978201720754 | validation: 0.030656686611780542]
	TIME [epoch: 8.32 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12926786817013639		[learning rate: 0.0025631]
		[batch 20/20] avg loss: 0.09842748537358183		[learning rate: 0.0025584]
	Learning Rate: 0.00255843
	LOSS [training: 0.1138476767718591 | validation: 0.11421519347389245]
	TIME [epoch: 8.32 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.142134423500008		[learning rate: 0.0025538]
		[batch 20/20] avg loss: 0.10402665579858454		[learning rate: 0.0025491]
	Learning Rate: 0.00254915
	LOSS [training: 0.12308053964929629 | validation: 0.07244826850030446]
	TIME [epoch: 8.32 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08931125088850253		[learning rate: 0.0025445]
		[batch 20/20] avg loss: 0.10820043610398493		[learning rate: 0.0025399]
	Learning Rate: 0.0025399
	LOSS [training: 0.09875584349624372 | validation: 0.11957304564164963]
	TIME [epoch: 8.3 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12802933377447298		[learning rate: 0.0025353]
		[batch 20/20] avg loss: 0.13321929954954143		[learning rate: 0.0025307]
	Learning Rate: 0.00253068
	LOSS [training: 0.1306243166620072 | validation: 0.2353826408380035]
	TIME [epoch: 8.32 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13233300098494688		[learning rate: 0.0025261]
		[batch 20/20] avg loss: 0.08459024306276255		[learning rate: 0.0025215]
	Learning Rate: 0.00252149
	LOSS [training: 0.10846162202385472 | validation: 0.13107357324771546]
	TIME [epoch: 8.32 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14224464386772873		[learning rate: 0.0025169]
		[batch 20/20] avg loss: 0.10782699498394177		[learning rate: 0.0025123]
	Learning Rate: 0.00251234
	LOSS [training: 0.12503581942583525 | validation: 0.07585639773250322]
	TIME [epoch: 8.31 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0951113716601782		[learning rate: 0.0025078]
		[batch 20/20] avg loss: 0.0801614216793301		[learning rate: 0.0025032]
	Learning Rate: 0.00250323
	LOSS [training: 0.08763639666975417 | validation: 0.17426710672920256]
	TIME [epoch: 8.3 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14019408332704625		[learning rate: 0.0024987]
		[batch 20/20] avg loss: 0.07180239961461171		[learning rate: 0.0024941]
	Learning Rate: 0.00249414
	LOSS [training: 0.105998241470829 | validation: 0.041109738575885735]
	TIME [epoch: 8.32 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10725878668635289		[learning rate: 0.0024896]
		[batch 20/20] avg loss: 0.12509341879693975		[learning rate: 0.0024851]
	Learning Rate: 0.00248509
	LOSS [training: 0.11617610274164633 | validation: 0.200166296902373]
	TIME [epoch: 8.32 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15655358787685897		[learning rate: 0.0024806]
		[batch 20/20] avg loss: 0.1583635223636075		[learning rate: 0.0024761]
	Learning Rate: 0.00247607
	LOSS [training: 0.15745855512023318 | validation: 0.3011115118615132]
	TIME [epoch: 8.31 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1633400492345476		[learning rate: 0.0024716]
		[batch 20/20] avg loss: 0.08419490584518355		[learning rate: 0.0024671]
	Learning Rate: 0.00246709
	LOSS [training: 0.12376747753986554 | validation: 0.2990619654691321]
	TIME [epoch: 8.3 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17378390368782795		[learning rate: 0.0024626]
		[batch 20/20] avg loss: 0.07248776770150608		[learning rate: 0.0024581]
	Learning Rate: 0.00245813
	LOSS [training: 0.12313583569466702 | validation: 0.11230992373884192]
	TIME [epoch: 8.3 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10519855690022255		[learning rate: 0.0024537]
		[batch 20/20] avg loss: 0.09108573544956913		[learning rate: 0.0024492]
	Learning Rate: 0.00244921
	LOSS [training: 0.09814214617489583 | validation: 0.11656772470544613]
	TIME [epoch: 8.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06933917830720009		[learning rate: 0.0024448]
		[batch 20/20] avg loss: 0.09271635960408273		[learning rate: 0.0024403]
	Learning Rate: 0.00244032
	LOSS [training: 0.08102776895564141 | validation: 0.03949752094907044]
	TIME [epoch: 8.32 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10378034223436572		[learning rate: 0.0024359]
		[batch 20/20] avg loss: 0.13893695858950625		[learning rate: 0.0024315]
	Learning Rate: 0.00243147
	LOSS [training: 0.12135865041193597 | validation: 0.06497609268652697]
	TIME [epoch: 8.3 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07263732518626477		[learning rate: 0.0024271]
		[batch 20/20] avg loss: 0.12218665362200479		[learning rate: 0.0024226]
	Learning Rate: 0.00242264
	LOSS [training: 0.09741198940413476 | validation: 0.1521075331621423]
	TIME [epoch: 8.29 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16537037980479866		[learning rate: 0.0024182]
		[batch 20/20] avg loss: 0.13406357032886568		[learning rate: 0.0024139]
	Learning Rate: 0.00241385
	LOSS [training: 0.14971697506683213 | validation: 0.06288614631711714]
	TIME [epoch: 8.34 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19411715250630873		[learning rate: 0.0024095]
		[batch 20/20] avg loss: 0.23741150942716266		[learning rate: 0.0024051]
	Learning Rate: 0.00240509
	LOSS [training: 0.2157643309667357 | validation: 0.1398427186478533]
	TIME [epoch: 8.32 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15908252112588522		[learning rate: 0.0024007]
		[batch 20/20] avg loss: 0.17174016451565582		[learning rate: 0.0023964]
	Learning Rate: 0.00239636
	LOSS [training: 0.16541134282077052 | validation: 0.06937405141216557]
	TIME [epoch: 8.3 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16178826872129126		[learning rate: 0.002392]
		[batch 20/20] avg loss: 0.11912970555294214		[learning rate: 0.0023877]
	Learning Rate: 0.00238767
	LOSS [training: 0.1404589871371167 | validation: 0.04437437318212452]
	TIME [epoch: 8.29 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1143435926370491		[learning rate: 0.0023833]
		[batch 20/20] avg loss: 0.11638038286152383		[learning rate: 0.002379]
	Learning Rate: 0.002379
	LOSS [training: 0.11536198774928648 | validation: 0.1327726716366788]
	TIME [epoch: 8.34 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10480933873047452		[learning rate: 0.0023747]
		[batch 20/20] avg loss: 0.08766789554089692		[learning rate: 0.0023704]
	Learning Rate: 0.00237037
	LOSS [training: 0.0962386171356857 | validation: 0.06564094773663098]
	TIME [epoch: 8.32 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08562425595524412		[learning rate: 0.0023661]
		[batch 20/20] avg loss: 0.08961758311766993		[learning rate: 0.0023618]
	Learning Rate: 0.00236177
	LOSS [training: 0.08762091953645701 | validation: 0.028955658595903072]
	TIME [epoch: 8.29 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09894558669672401		[learning rate: 0.0023575]
		[batch 20/20] avg loss: 0.09849113989097932		[learning rate: 0.0023532]
	Learning Rate: 0.00235319
	LOSS [training: 0.09871836329385167 | validation: 0.07892161295718111]
	TIME [epoch: 8.29 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08431473156364		[learning rate: 0.0023489]
		[batch 20/20] avg loss: 0.06658107501459631		[learning rate: 0.0023447]
	Learning Rate: 0.00234465
	LOSS [training: 0.07544790328911816 | validation: 0.069585913787482]
	TIME [epoch: 8.32 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11873523216694219		[learning rate: 0.0023404]
		[batch 20/20] avg loss: 0.12061821583455927		[learning rate: 0.0023361]
	Learning Rate: 0.00233615
	LOSS [training: 0.11967672400075073 | validation: 0.014444772819748786]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13723300779439898		[learning rate: 0.0023319]
		[batch 20/20] avg loss: 0.09346119225041549		[learning rate: 0.0023277]
	Learning Rate: 0.00232767
	LOSS [training: 0.11534710002240722 | validation: 0.044595746740518465]
	TIME [epoch: 8.29 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09260428291431451		[learning rate: 0.0023234]
		[batch 20/20] avg loss: 0.17323675464012928		[learning rate: 0.0023192]
	Learning Rate: 0.00231922
	LOSS [training: 0.13292051877722189 | validation: 0.05040953585673129]
	TIME [epoch: 8.28 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059978378378503984		[learning rate: 0.002315]
		[batch 20/20] avg loss: 0.09282278957563853		[learning rate: 0.0023108]
	Learning Rate: 0.0023108
	LOSS [training: 0.07640058397707125 | validation: 0.03868358425925046]
	TIME [epoch: 8.3 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09102053028046868		[learning rate: 0.0023066]
		[batch 20/20] avg loss: 0.2181729299601895		[learning rate: 0.0023024]
	Learning Rate: 0.00230242
	LOSS [training: 0.1545967301203291 | validation: 0.03641430917183854]
	TIME [epoch: 8.31 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10930410308967682		[learning rate: 0.0022982]
		[batch 20/20] avg loss: 0.06518720084514765		[learning rate: 0.0022941]
	Learning Rate: 0.00229406
	LOSS [training: 0.08724565196741224 | validation: 0.0454802742214701]
	TIME [epoch: 8.29 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08381561320961667		[learning rate: 0.0022899]
		[batch 20/20] avg loss: 0.07176834461786512		[learning rate: 0.0022857]
	Learning Rate: 0.00228574
	LOSS [training: 0.0777919789137409 | validation: 0.044005797994348804]
	TIME [epoch: 8.29 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09579311420495396		[learning rate: 0.0022816]
		[batch 20/20] avg loss: 0.07931831566613759		[learning rate: 0.0022774]
	Learning Rate: 0.00227744
	LOSS [training: 0.08755571493554579 | validation: 0.016008018670752126]
	TIME [epoch: 8.32 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20618403079106487		[learning rate: 0.0022733]
		[batch 20/20] avg loss: 0.10453309442345737		[learning rate: 0.0022692]
	Learning Rate: 0.00226918
	LOSS [training: 0.1553585626072611 | validation: 0.03889916445643322]
	TIME [epoch: 8.31 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0954567745057989		[learning rate: 0.0022651]
		[batch 20/20] avg loss: 0.09200735485219014		[learning rate: 0.0022609]
	Learning Rate: 0.00226094
	LOSS [training: 0.0937320646789945 | validation: 0.07750338872667219]
	TIME [epoch: 8.3 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1388161126878474		[learning rate: 0.0022568]
		[batch 20/20] avg loss: 0.09115791577842203		[learning rate: 0.0022527]
	Learning Rate: 0.00225274
	LOSS [training: 0.1149870142331347 | validation: 0.048145509462790434]
	TIME [epoch: 8.28 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.096659787706287		[learning rate: 0.0022486]
		[batch 20/20] avg loss: 0.07885413555888184		[learning rate: 0.0022446]
	Learning Rate: 0.00224456
	LOSS [training: 0.08775696163258442 | validation: 0.038551665588923756]
	TIME [epoch: 8.33 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054241973128654454		[learning rate: 0.0022405]
		[batch 20/20] avg loss: 0.11336728977295966		[learning rate: 0.0022364]
	Learning Rate: 0.00223642
	LOSS [training: 0.08380463145080705 | validation: 0.030596811125598988]
	TIME [epoch: 8.28 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10089433233602256		[learning rate: 0.0022324]
		[batch 20/20] avg loss: 0.12577913690872766		[learning rate: 0.0022283]
	Learning Rate: 0.0022283
	LOSS [training: 0.1133367346223751 | validation: 0.01009511498127524]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_513.pth
	Model improved!!!
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06396024003563257		[learning rate: 0.0022243]
		[batch 20/20] avg loss: 0.09855822664499457		[learning rate: 0.0022202]
	Learning Rate: 0.00222021
	LOSS [training: 0.08125923334031356 | validation: 0.041068075675089626]
	TIME [epoch: 8.3 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08704595612881182		[learning rate: 0.0022162]
		[batch 20/20] avg loss: 0.10035259510986212		[learning rate: 0.0022122]
	Learning Rate: 0.00221216
	LOSS [training: 0.09369927561933696 | validation: 0.06399349775592714]
	TIME [epoch: 8.3 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06811146794812448		[learning rate: 0.0022081]
		[batch 20/20] avg loss: 0.1231358495634316		[learning rate: 0.0022041]
	Learning Rate: 0.00220413
	LOSS [training: 0.09562365875577804 | validation: 0.09149516652555326]
	TIME [epoch: 8.28 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07410054341490736		[learning rate: 0.0022001]
		[batch 20/20] avg loss: 0.08213156599123432		[learning rate: 0.0021961]
	Learning Rate: 0.00219613
	LOSS [training: 0.07811605470307086 | validation: 0.1352859752334166]
	TIME [epoch: 8.3 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1089031507622201		[learning rate: 0.0021921]
		[batch 20/20] avg loss: 0.0805748389136494		[learning rate: 0.0021882]
	Learning Rate: 0.00218816
	LOSS [training: 0.09473899483793476 | validation: 0.24503146196198392]
	TIME [epoch: 8.28 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08286265630537468		[learning rate: 0.0021842]
		[batch 20/20] avg loss: 0.2126460229756221		[learning rate: 0.0021802]
	Learning Rate: 0.00218022
	LOSS [training: 0.1477543396404984 | validation: 0.026789139136910143]
	TIME [epoch: 8.29 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1002579665733205		[learning rate: 0.0021763]
		[batch 20/20] avg loss: 0.13077399585199673		[learning rate: 0.0021723]
	Learning Rate: 0.00217231
	LOSS [training: 0.11551598121265863 | validation: 0.05066400785060432]
	TIME [epoch: 8.27 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11060957554849701		[learning rate: 0.0021684]
		[batch 20/20] avg loss: 0.13521069874011477		[learning rate: 0.0021644]
	Learning Rate: 0.00216442
	LOSS [training: 0.12291013714430588 | validation: 0.07552254186105237]
	TIME [epoch: 8.29 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11624838615509876		[learning rate: 0.0021605]
		[batch 20/20] avg loss: 0.1315557456296087		[learning rate: 0.0021566]
	Learning Rate: 0.00215657
	LOSS [training: 0.12390206589235371 | validation: 0.04368274317349549]
	TIME [epoch: 8.3 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09119291831907168		[learning rate: 0.0021527]
		[batch 20/20] avg loss: 0.13096085217965495		[learning rate: 0.0021487]
	Learning Rate: 0.00214874
	LOSS [training: 0.1110768852493633 | validation: 0.10373463341699977]
	TIME [epoch: 8.29 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20107761497149523		[learning rate: 0.0021448]
		[batch 20/20] avg loss: 0.07098368467262935		[learning rate: 0.0021409]
	Learning Rate: 0.00214094
	LOSS [training: 0.13603064982206223 | validation: 0.04281101565342413]
	TIME [epoch: 8.27 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0935847913447		[learning rate: 0.0021371]
		[batch 20/20] avg loss: 0.08196350185879263		[learning rate: 0.0021332]
	Learning Rate: 0.00213317
	LOSS [training: 0.0877741466017463 | validation: 0.15758667769522788]
	TIME [epoch: 8.28 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1056053840802735		[learning rate: 0.0021293]
		[batch 20/20] avg loss: 0.1331259124674061		[learning rate: 0.0021254]
	Learning Rate: 0.00212543
	LOSS [training: 0.11936564827383982 | validation: 0.033984164131626]
	TIME [epoch: 8.33 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1224070945078329		[learning rate: 0.0021216]
		[batch 20/20] avg loss: 0.10790656852348024		[learning rate: 0.0021177]
	Learning Rate: 0.00211772
	LOSS [training: 0.11515683151565659 | validation: 0.06583658104456122]
	TIME [epoch: 8.29 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09456026162557817		[learning rate: 0.0021139]
		[batch 20/20] avg loss: 0.08875632615088616		[learning rate: 0.00211]
	Learning Rate: 0.00211003
	LOSS [training: 0.09165829388823216 | validation: 0.05436622444738457]
	TIME [epoch: 8.27 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09125492560829018		[learning rate: 0.0021062]
		[batch 20/20] avg loss: 0.07112087090230727		[learning rate: 0.0021024]
	Learning Rate: 0.00210238
	LOSS [training: 0.08118789825529873 | validation: 0.09044349378226361]
	TIME [epoch: 8.26 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10200427169094795		[learning rate: 0.0020986]
		[batch 20/20] avg loss: 0.07058762614734426		[learning rate: 0.0020947]
	Learning Rate: 0.00209475
	LOSS [training: 0.08629594891914612 | validation: 0.04514530763314058]
	TIME [epoch: 8.33 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1425646598264272		[learning rate: 0.0020909]
		[batch 20/20] avg loss: 0.11585089982982588		[learning rate: 0.0020871]
	Learning Rate: 0.00208714
	LOSS [training: 0.12920777982812653 | validation: 0.12831607220850544]
	TIME [epoch: 8.28 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11695417834291696		[learning rate: 0.0020834]
		[batch 20/20] avg loss: 0.07818130039695914		[learning rate: 0.0020796]
	Learning Rate: 0.00207957
	LOSS [training: 0.09756773936993807 | validation: 0.06116876331119643]
	TIME [epoch: 8.28 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08494193725158461		[learning rate: 0.0020758]
		[batch 20/20] avg loss: 0.10967327026152363		[learning rate: 0.002072]
	Learning Rate: 0.00207202
	LOSS [training: 0.09730760375655413 | validation: 0.02231231401290317]
	TIME [epoch: 8.28 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10655517980899296		[learning rate: 0.0020683]
		[batch 20/20] avg loss: 0.0739869164131843		[learning rate: 0.0020645]
	Learning Rate: 0.0020645
	LOSS [training: 0.09027104811108863 | validation: 0.07531117758856941]
	TIME [epoch: 8.31 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10763284905381072		[learning rate: 0.0020608]
		[batch 20/20] avg loss: 0.07984185448877162		[learning rate: 0.002057]
	Learning Rate: 0.00205701
	LOSS [training: 0.0937373517712912 | validation: 0.03683199920438651]
	TIME [epoch: 8.29 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0865487742018566		[learning rate: 0.0020533]
		[batch 20/20] avg loss: 0.1810587660529287		[learning rate: 0.0020495]
	Learning Rate: 0.00204955
	LOSS [training: 0.13380377012739264 | validation: 0.03885347174399646]
	TIME [epoch: 8.28 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09639735273209876		[learning rate: 0.0020458]
		[batch 20/20] avg loss: 0.08393043974913691		[learning rate: 0.0020421]
	Learning Rate: 0.00204211
	LOSS [training: 0.09016389624061782 | validation: 0.057819244739031134]
	TIME [epoch: 8.27 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09295044116447532		[learning rate: 0.0020384]
		[batch 20/20] avg loss: 0.10419999385036469		[learning rate: 0.0020347]
	Learning Rate: 0.0020347
	LOSS [training: 0.09857521750742002 | validation: 0.03749595514449734]
	TIME [epoch: 8.31 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11565130405875543		[learning rate: 0.002031]
		[batch 20/20] avg loss: 0.07874090656195153		[learning rate: 0.0020273]
	Learning Rate: 0.00202731
	LOSS [training: 0.09719610531035347 | validation: 0.04514145539182568]
	TIME [epoch: 8.3 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06759952061092987		[learning rate: 0.0020236]
		[batch 20/20] avg loss: 0.1106898245856586		[learning rate: 0.00202]
	Learning Rate: 0.00201996
	LOSS [training: 0.08914467259829423 | validation: 0.09307809208856566]
	TIME [epoch: 8.28 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06138726288286539		[learning rate: 0.0020163]
		[batch 20/20] avg loss: 0.07445965718340321		[learning rate: 0.0020126]
	Learning Rate: 0.00201263
	LOSS [training: 0.06792346003313429 | validation: 0.11019893392131402]
	TIME [epoch: 8.29 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07420648429844075		[learning rate: 0.002009]
		[batch 20/20] avg loss: 0.1072036891179641		[learning rate: 0.0020053]
	Learning Rate: 0.00200532
	LOSS [training: 0.09070508670820243 | validation: 0.11348379141594658]
	TIME [epoch: 8.31 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06294432322624174		[learning rate: 0.0020017]
		[batch 20/20] avg loss: 0.16268296634179402		[learning rate: 0.001998]
	Learning Rate: 0.00199805
	LOSS [training: 0.11281364478401787 | validation: 0.11961631396332313]
	TIME [epoch: 8.3 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07209369069488258		[learning rate: 0.0019944]
		[batch 20/20] avg loss: 0.08665267995158764		[learning rate: 0.0019908]
	Learning Rate: 0.00199079
	LOSS [training: 0.07937318532323512 | validation: 0.054889160952601684]
	TIME [epoch: 8.29 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06839738292566032		[learning rate: 0.0019872]
		[batch 20/20] avg loss: 0.11396717445433827		[learning rate: 0.0019836]
	Learning Rate: 0.00198357
	LOSS [training: 0.09118227868999927 | validation: 0.07054815983930716]
	TIME [epoch: 8.29 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11185550127740133		[learning rate: 0.00198]
		[batch 20/20] avg loss: 0.10648471836847921		[learning rate: 0.0019764]
	Learning Rate: 0.00197637
	LOSS [training: 0.10917010982294026 | validation: 0.058589296619735234]
	TIME [epoch: 8.3 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07753151275314148		[learning rate: 0.0019728]
		[batch 20/20] avg loss: 0.08349938345061729		[learning rate: 0.0019692]
	Learning Rate: 0.0019692
	LOSS [training: 0.0805154481018794 | validation: 0.11675979514694668]
	TIME [epoch: 8.29 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1301818572977113		[learning rate: 0.0019656]
		[batch 20/20] avg loss: 0.18938533439508265		[learning rate: 0.0019621]
	Learning Rate: 0.00196205
	LOSS [training: 0.15978359584639695 | validation: 0.09039838088840699]
	TIME [epoch: 8.31 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07951077184601532		[learning rate: 0.0019585]
		[batch 20/20] avg loss: 0.0952187916494945		[learning rate: 0.0019549]
	Learning Rate: 0.00195493
	LOSS [training: 0.0873647817477549 | validation: 0.03224070454362834]
	TIME [epoch: 8.3 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06706841902367064		[learning rate: 0.0019514]
		[batch 20/20] avg loss: 0.14820802649086248		[learning rate: 0.0019478]
	Learning Rate: 0.00194784
	LOSS [training: 0.10763822275726656 | validation: 0.04623054579947579]
	TIME [epoch: 8.29 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08001369521234164		[learning rate: 0.0019443]
		[batch 20/20] avg loss: 0.09141149298077356		[learning rate: 0.0019408]
	Learning Rate: 0.00194077
	LOSS [training: 0.0857125940965576 | validation: 0.08370578542690554]
	TIME [epoch: 8.27 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07772547754778014		[learning rate: 0.0019372]
		[batch 20/20] avg loss: 0.03486520413030052		[learning rate: 0.0019337]
	Learning Rate: 0.00193373
	LOSS [training: 0.05629534083904033 | validation: 0.024149372951063244]
	TIME [epoch: 8.31 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07486174011779326		[learning rate: 0.0019302]
		[batch 20/20] avg loss: 0.07474526675769508		[learning rate: 0.0019267]
	Learning Rate: 0.00192671
	LOSS [training: 0.07480350343774417 | validation: 0.02382937170840619]
	TIME [epoch: 8.29 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09583383251929398		[learning rate: 0.0019232]
		[batch 20/20] avg loss: 0.12212234324246674		[learning rate: 0.0019197]
	Learning Rate: 0.00191972
	LOSS [training: 0.10897808788088037 | validation: 0.020711887142160934]
	TIME [epoch: 8.3 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0584553903154222		[learning rate: 0.0019162]
		[batch 20/20] avg loss: 0.0668229170445712		[learning rate: 0.0019127]
	Learning Rate: 0.00191275
	LOSS [training: 0.0626391536799967 | validation: 0.1479818094870528]
	TIME [epoch: 8.27 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10234624923672839		[learning rate: 0.0019093]
		[batch 20/20] avg loss: 0.12963825471838952		[learning rate: 0.0019058]
	Learning Rate: 0.00190581
	LOSS [training: 0.11599225197755896 | validation: 0.20665460714925699]
	TIME [epoch: 8.29 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1356076301078326		[learning rate: 0.0019023]
		[batch 20/20] avg loss: 0.14311618252446023		[learning rate: 0.0018989]
	Learning Rate: 0.00189889
	LOSS [training: 0.13936190631614642 | validation: 0.1418213935218982]
	TIME [epoch: 8.3 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13418850517397493		[learning rate: 0.0018954]
		[batch 20/20] avg loss: 0.1298694457765777		[learning rate: 0.001892]
	Learning Rate: 0.001892
	LOSS [training: 0.1320289754752763 | validation: 0.16594401537138034]
	TIME [epoch: 8.3 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15253149004250174		[learning rate: 0.0018886]
		[batch 20/20] avg loss: 0.2145907958395264		[learning rate: 0.0018851]
	Learning Rate: 0.00188513
	LOSS [training: 0.18356114294101405 | validation: 0.09716056571998331]
	TIME [epoch: 8.27 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1306478370290465		[learning rate: 0.0018817]
		[batch 20/20] avg loss: 0.11173104553461584		[learning rate: 0.0018783]
	Learning Rate: 0.00187829
	LOSS [training: 0.12118944128183118 | validation: 0.05110213906776956]
	TIME [epoch: 8.28 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11473633225043653		[learning rate: 0.0018749]
		[batch 20/20] avg loss: 0.12900229888818918		[learning rate: 0.0018715]
	Learning Rate: 0.00187148
	LOSS [training: 0.12186931556931284 | validation: 0.158594862947617]
	TIME [epoch: 8.33 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11860620712563812		[learning rate: 0.0018681]
		[batch 20/20] avg loss: 0.1298258147072952		[learning rate: 0.0018647]
	Learning Rate: 0.00186468
	LOSS [training: 0.12421601091646668 | validation: 0.06131429633828868]
	TIME [epoch: 8.28 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12760179037617125		[learning rate: 0.0018613]
		[batch 20/20] avg loss: 0.0894150595814505		[learning rate: 0.0018579]
	Learning Rate: 0.00185792
	LOSS [training: 0.10850842497881089 | validation: 0.10579290321175659]
	TIME [epoch: 8.28 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10260794729148807		[learning rate: 0.0018545]
		[batch 20/20] avg loss: 0.10007341582055755		[learning rate: 0.0018512]
	Learning Rate: 0.00185117
	LOSS [training: 0.1013406815560228 | validation: 0.034770875510339215]
	TIME [epoch: 8.27 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08918976567657406		[learning rate: 0.0018478]
		[batch 20/20] avg loss: 0.07941570170487748		[learning rate: 0.0018445]
	Learning Rate: 0.00184446
	LOSS [training: 0.08430273369072577 | validation: 0.05230105310485619]
	TIME [epoch: 8.33 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09071038925657068		[learning rate: 0.0018411]
		[batch 20/20] avg loss: 0.07609029152223323		[learning rate: 0.0018378]
	Learning Rate: 0.00183776
	LOSS [training: 0.08340034038940196 | validation: 0.031172809696227988]
	TIME [epoch: 8.29 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06164950438551538		[learning rate: 0.0018344]
		[batch 20/20] avg loss: 0.09798121902415514		[learning rate: 0.0018311]
	Learning Rate: 0.00183109
	LOSS [training: 0.07981536170483525 | validation: 0.034226689463931204]
	TIME [epoch: 8.29 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05222928851827232		[learning rate: 0.0018278]
		[batch 20/20] avg loss: 0.06337901383395951		[learning rate: 0.0018244]
	Learning Rate: 0.00182445
	LOSS [training: 0.05780415117611591 | validation: 0.09034426600048519]
	TIME [epoch: 8.29 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10209813635560896		[learning rate: 0.0018211]
		[batch 20/20] avg loss: 0.054920086351719875		[learning rate: 0.0018178]
	Learning Rate: 0.00181783
	LOSS [training: 0.07850911135366442 | validation: 0.06414425925587101]
	TIME [epoch: 8.33 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10044701624180581		[learning rate: 0.0018145]
		[batch 20/20] avg loss: 0.10578200112186573		[learning rate: 0.0018112]
	Learning Rate: 0.00181123
	LOSS [training: 0.10311450868183576 | validation: 0.03720242155613523]
	TIME [epoch: 8.3 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05184315011648198		[learning rate: 0.0018079]
		[batch 20/20] avg loss: 0.08790230399962673		[learning rate: 0.0018047]
	Learning Rate: 0.00180466
	LOSS [training: 0.06987272705805433 | validation: 0.1457471992309738]
	TIME [epoch: 8.28 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10642199604040689		[learning rate: 0.0018014]
		[batch 20/20] avg loss: 0.09789568942855657		[learning rate: 0.0017981]
	Learning Rate: 0.00179811
	LOSS [training: 0.10215884273448175 | validation: 0.07164748361484484]
	TIME [epoch: 8.29 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08791685501931738		[learning rate: 0.0017948]
		[batch 20/20] avg loss: 0.09464274757377336		[learning rate: 0.0017916]
	Learning Rate: 0.00179158
	LOSS [training: 0.09127980129654538 | validation: 0.04110114202101991]
	TIME [epoch: 8.31 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057952702030853856		[learning rate: 0.0017883]
		[batch 20/20] avg loss: 0.056591182680862565		[learning rate: 0.0017851]
	Learning Rate: 0.00178508
	LOSS [training: 0.05727194235585821 | validation: 0.06803420615008866]
	TIME [epoch: 8.31 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07957268918289342		[learning rate: 0.0017818]
		[batch 20/20] avg loss: 0.07859758292382726		[learning rate: 0.0017786]
	Learning Rate: 0.0017786
	LOSS [training: 0.07908513605336034 | validation: 0.02737981930239939]
	TIME [epoch: 8.28 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08953713385850004		[learning rate: 0.0017754]
		[batch 20/20] avg loss: 0.10571192703393509		[learning rate: 0.0017721]
	Learning Rate: 0.00177215
	LOSS [training: 0.09762453044621758 | validation: 0.06538067941711721]
	TIME [epoch: 8.29 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11979827745998173		[learning rate: 0.0017689]
		[batch 20/20] avg loss: 0.11058181903612721		[learning rate: 0.0017657]
	Learning Rate: 0.00176572
	LOSS [training: 0.11519004824805446 | validation: 0.15076268227203748]
	TIME [epoch: 8.31 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07618591567701438		[learning rate: 0.0017625]
		[batch 20/20] avg loss: 0.06702832039764704		[learning rate: 0.0017593]
	Learning Rate: 0.00175931
	LOSS [training: 0.07160711803733073 | validation: 0.051561487791697044]
	TIME [epoch: 8.31 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06054728530439708		[learning rate: 0.0017561]
		[batch 20/20] avg loss: 0.0701318116739318		[learning rate: 0.0017529]
	Learning Rate: 0.00175292
	LOSS [training: 0.06533954848916443 | validation: -0.000794576470083571]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_579.pth
	Model improved!!!
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061305760952854504		[learning rate: 0.0017497]
		[batch 20/20] avg loss: 0.10421219637354025		[learning rate: 0.0017466]
	Learning Rate: 0.00174656
	LOSS [training: 0.08275897866319738 | validation: 0.02251234641450541]
	TIME [epoch: 8.27 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06279286264186061		[learning rate: 0.0017434]
		[batch 20/20] avg loss: 0.06492141912341927		[learning rate: 0.0017402]
	Learning Rate: 0.00174022
	LOSS [training: 0.06385714088263995 | validation: 0.04760171357115467]
	TIME [epoch: 8.31 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07391735819211628		[learning rate: 0.0017371]
		[batch 20/20] avg loss: 0.13011514181271033		[learning rate: 0.0017339]
	Learning Rate: 0.00173391
	LOSS [training: 0.1020162500024133 | validation: 0.040114211001917935]
	TIME [epoch: 8.28 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09573124363355369		[learning rate: 0.0017308]
		[batch 20/20] avg loss: 0.1393726128289443		[learning rate: 0.0017276]
	Learning Rate: 0.00172762
	LOSS [training: 0.117551928231249 | validation: 0.05785666596226431]
	TIME [epoch: 8.29 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06339946291641546		[learning rate: 0.0017245]
		[batch 20/20] avg loss: 0.056118702516744765		[learning rate: 0.0017213]
	Learning Rate: 0.00172135
	LOSS [training: 0.05975908271658011 | validation: 0.0778245507538709]
	TIME [epoch: 8.27 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08079493788468647		[learning rate: 0.0017182]
		[batch 20/20] avg loss: 0.07965935335928429		[learning rate: 0.0017151]
	Learning Rate: 0.0017151
	LOSS [training: 0.08022714562198538 | validation: 0.07174768654782362]
	TIME [epoch: 8.31 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07412315395747147		[learning rate: 0.001712]
		[batch 20/20] avg loss: 0.1511957284486946		[learning rate: 0.0017089]
	Learning Rate: 0.00170888
	LOSS [training: 0.11265944120308305 | validation: 0.08049968303692212]
	TIME [epoch: 8.28 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05789742820249115		[learning rate: 0.0017058]
		[batch 20/20] avg loss: 0.04957788533071365		[learning rate: 0.0017027]
	Learning Rate: 0.00170267
	LOSS [training: 0.053737656766602394 | validation: 0.13705973481327427]
	TIME [epoch: 8.3 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08269109156501109		[learning rate: 0.0016996]
		[batch 20/20] avg loss: 0.039681691100053394		[learning rate: 0.0016965]
	Learning Rate: 0.0016965
	LOSS [training: 0.06118639133253225 | validation: 0.04117063401519423]
	TIME [epoch: 8.29 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07479701068661998		[learning rate: 0.0016934]
		[batch 20/20] avg loss: 0.11101700098342493		[learning rate: 0.0016903]
	Learning Rate: 0.00169034
	LOSS [training: 0.09290700583502245 | validation: 0.06295468829234396]
	TIME [epoch: 8.29 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03944931080145777		[learning rate: 0.0016873]
		[batch 20/20] avg loss: 0.03886973545348385		[learning rate: 0.0016842]
	Learning Rate: 0.0016842
	LOSS [training: 0.039159523127470806 | validation: 0.023009212907643573]
	TIME [epoch: 8.28 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06994506999225453		[learning rate: 0.0016811]
		[batch 20/20] avg loss: 0.05311341319594688		[learning rate: 0.0016781]
	Learning Rate: 0.00167809
	LOSS [training: 0.061529241594100705 | validation: 0.020666010269134557]
	TIME [epoch: 8.29 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09687049004172915		[learning rate: 0.001675]
		[batch 20/20] avg loss: 0.10927256622862216		[learning rate: 0.001672]
	Learning Rate: 0.001672
	LOSS [training: 0.10307152813517566 | validation: 0.08957530582674822]
	TIME [epoch: 8.3 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14414899581036483		[learning rate: 0.001669]
		[batch 20/20] avg loss: 0.07097400665050821		[learning rate: 0.0016659]
	Learning Rate: 0.00166593
	LOSS [training: 0.10756150123043651 | validation: 0.04227471454920713]
	TIME [epoch: 8.3 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035476878403536236		[learning rate: 0.0016629]
		[batch 20/20] avg loss: 0.06139469740264995		[learning rate: 0.0016599]
	Learning Rate: 0.00165989
	LOSS [training: 0.048435787903093094 | validation: 0.1376934602553488]
	TIME [epoch: 8.27 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05963546968331061		[learning rate: 0.0016569]
		[batch 20/20] avg loss: 0.06084188006570514		[learning rate: 0.0016539]
	Learning Rate: 0.00165387
	LOSS [training: 0.06023867487450786 | validation: 0.05134232315609615]
	TIME [epoch: 8.28 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06619076130162103		[learning rate: 0.0016509]
		[batch 20/20] avg loss: 0.08063554541469029		[learning rate: 0.0016479]
	Learning Rate: 0.00164786
	LOSS [training: 0.07341315335815567 | validation: 0.02142166303000309]
	TIME [epoch: 8.32 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06765175283038877		[learning rate: 0.0016449]
		[batch 20/20] avg loss: 0.0544667705936452		[learning rate: 0.0016419]
	Learning Rate: 0.00164188
	LOSS [training: 0.06105926171201699 | validation: 0.05195843007591901]
	TIME [epoch: 8.29 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04279089950335424		[learning rate: 0.0016389]
		[batch 20/20] avg loss: 0.06068618329133291		[learning rate: 0.0016359]
	Learning Rate: 0.00163592
	LOSS [training: 0.051738541397343574 | validation: 0.044083530879072026]
	TIME [epoch: 8.27 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04915500548367899		[learning rate: 0.001633]
		[batch 20/20] avg loss: 0.08600824004332316		[learning rate: 0.00163]
	Learning Rate: 0.00162999
	LOSS [training: 0.06758162276350109 | validation: 0.21188271579112503]
	TIME [epoch: 8.27 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10623964520358922		[learning rate: 0.001627]
		[batch 20/20] avg loss: 0.1366319625384098		[learning rate: 0.0016241]
	Learning Rate: 0.00162407
	LOSS [training: 0.1214358038709995 | validation: 0.041604697451840256]
	TIME [epoch: 8.32 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05102094715873864		[learning rate: 0.0016211]
		[batch 20/20] avg loss: 0.10430070406482597		[learning rate: 0.0016182]
	Learning Rate: 0.00161818
	LOSS [training: 0.0776608256117823 | validation: 0.022628691551310094]
	TIME [epoch: 8.3 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06861098912011877		[learning rate: 0.0016152]
		[batch 20/20] avg loss: 0.045181134744363985		[learning rate: 0.0016123]
	Learning Rate: 0.00161231
	LOSS [training: 0.05689606193224137 | validation: 0.042257002314722326]
	TIME [epoch: 8.28 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04861704899917319		[learning rate: 0.0016094]
		[batch 20/20] avg loss: 0.06126966403703578		[learning rate: 0.0016065]
	Learning Rate: 0.00160645
	LOSS [training: 0.05494335651810447 | validation: 0.10912784237348457]
	TIME [epoch: 8.27 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08955966371655519		[learning rate: 0.0016035]
		[batch 20/20] avg loss: 0.07254178998288915		[learning rate: 0.0016006]
	Learning Rate: 0.00160062
	LOSS [training: 0.08105072684972217 | validation: 0.07851627948314363]
	TIME [epoch: 8.31 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09973565868542623		[learning rate: 0.0015977]
		[batch 20/20] avg loss: 0.06498334053421193		[learning rate: 0.0015948]
	Learning Rate: 0.00159482
	LOSS [training: 0.08235949960981909 | validation: 0.04066996087574147]
	TIME [epoch: 8.31 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05757542934906098		[learning rate: 0.0015919]
		[batch 20/20] avg loss: 0.058330360057472365		[learning rate: 0.001589]
	Learning Rate: 0.00158903
	LOSS [training: 0.057952894703266665 | validation: 0.04769619219138797]
	TIME [epoch: 8.27 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07316022417605227		[learning rate: 0.0015861]
		[batch 20/20] avg loss: 0.07193214825649406		[learning rate: 0.0015833]
	Learning Rate: 0.00158326
	LOSS [training: 0.07254618621627319 | validation: 0.053510939624403904]
	TIME [epoch: 8.27 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07188889541440105		[learning rate: 0.0015804]
		[batch 20/20] avg loss: 0.0658123541162867		[learning rate: 0.0015775]
	Learning Rate: 0.00157752
	LOSS [training: 0.0688506247653439 | validation: 0.03341803494149347]
	TIME [epoch: 8.29 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055938672635103034		[learning rate: 0.0015747]
		[batch 20/20] avg loss: 0.07553162343278252		[learning rate: 0.0015718]
	Learning Rate: 0.00157179
	LOSS [training: 0.06573514803394279 | validation: 0.05605264483401419]
	TIME [epoch: 8.32 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061173864466635974		[learning rate: 0.0015689]
		[batch 20/20] avg loss: 0.04904137622971931		[learning rate: 0.0015661]
	Learning Rate: 0.00156609
	LOSS [training: 0.05510762034817764 | validation: 0.03244487149925283]
	TIME [epoch: 8.26 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08312572765776514		[learning rate: 0.0015632]
		[batch 20/20] avg loss: 0.08872130320589877		[learning rate: 0.0015604]
	Learning Rate: 0.0015604
	LOSS [training: 0.08592351543183196 | validation: 0.0586559976103687]
	TIME [epoch: 8.27 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06944980875947912		[learning rate: 0.0015576]
		[batch 20/20] avg loss: 0.045535064791259156		[learning rate: 0.0015547]
	Learning Rate: 0.00155474
	LOSS [training: 0.057492436775369124 | validation: 0.07882928907973266]
	TIME [epoch: 8.29 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044811047708823444		[learning rate: 0.0015519]
		[batch 20/20] avg loss: 0.0704410523713121		[learning rate: 0.0015491]
	Learning Rate: 0.0015491
	LOSS [training: 0.05762605004006777 | validation: 0.11652230265095277]
	TIME [epoch: 8.32 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056383063196137895		[learning rate: 0.0015463]
		[batch 20/20] avg loss: 0.07196991767912804		[learning rate: 0.0015435]
	Learning Rate: 0.00154348
	LOSS [training: 0.06417649043763296 | validation: 0.18690824736408976]
	TIME [epoch: 8.27 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08910246148800031		[learning rate: 0.0015407]
		[batch 20/20] avg loss: 0.0614188117663438		[learning rate: 0.0015379]
	Learning Rate: 0.00153788
	LOSS [training: 0.07526063662717204 | validation: 0.04074660115938887]
	TIME [epoch: 8.27 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05147665915452868		[learning rate: 0.0015351]
		[batch 20/20] avg loss: 0.08099143634608993		[learning rate: 0.0015323]
	Learning Rate: 0.00153229
	LOSS [training: 0.0662340477503093 | validation: 0.05229413050235027]
	TIME [epoch: 8.29 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08516161263926479		[learning rate: 0.0015295]
		[batch 20/20] avg loss: 0.04943247290633656		[learning rate: 0.0015267]
	Learning Rate: 0.00152673
	LOSS [training: 0.06729704277280066 | validation: 0.055467833123247434]
	TIME [epoch: 8.31 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05436894515564702		[learning rate: 0.001524]
		[batch 20/20] avg loss: 0.06644630285992018		[learning rate: 0.0015212]
	Learning Rate: 0.00152119
	LOSS [training: 0.060407624007783614 | validation: 0.02056677689143149]
	TIME [epoch: 8.27 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06667338831458498		[learning rate: 0.0015184]
		[batch 20/20] avg loss: 0.04575071931552568		[learning rate: 0.0015157]
	Learning Rate: 0.00151567
	LOSS [training: 0.05621205381505533 | validation: 0.04942184897589498]
	TIME [epoch: 8.27 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0872610176625213		[learning rate: 0.0015129]
		[batch 20/20] avg loss: 0.07359107603270917		[learning rate: 0.0015102]
	Learning Rate: 0.00151017
	LOSS [training: 0.08042604684761522 | validation: 0.1133937751864751]
	TIME [epoch: 8.29 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09039806529308443		[learning rate: 0.0015074]
		[batch 20/20] avg loss: 0.06824765190151894		[learning rate: 0.0015047]
	Learning Rate: 0.00150469
	LOSS [training: 0.0793228585973017 | validation: 0.04089969384668499]
	TIME [epoch: 8.3 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06441417874508774		[learning rate: 0.001502]
		[batch 20/20] avg loss: 0.08815749284488597		[learning rate: 0.0014992]
	Learning Rate: 0.00149923
	LOSS [training: 0.07628583579498685 | validation: 0.036681288128443]
	TIME [epoch: 8.3 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051797950982248556		[learning rate: 0.0014965]
		[batch 20/20] avg loss: 0.047449785714745354		[learning rate: 0.0014938]
	Learning Rate: 0.00149379
	LOSS [training: 0.049623868348496955 | validation: 0.05994472821046577]
	TIME [epoch: 8.28 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0533900135290852		[learning rate: 0.0014911]
		[batch 20/20] avg loss: 0.04106852829325184		[learning rate: 0.0014884]
	Learning Rate: 0.00148837
	LOSS [training: 0.04722927091116851 | validation: 0.06054680832188948]
	TIME [epoch: 8.29 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058291862587796825		[learning rate: 0.0014857]
		[batch 20/20] avg loss: 0.07326045169791144		[learning rate: 0.001483]
	Learning Rate: 0.00148297
	LOSS [training: 0.06577615714285413 | validation: 0.10596501434965877]
	TIME [epoch: 8.29 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11659321916280689		[learning rate: 0.0014803]
		[batch 20/20] avg loss: 0.04902562805284709		[learning rate: 0.0014776]
	Learning Rate: 0.00147759
	LOSS [training: 0.08280942360782699 | validation: 0.006507753655596019]
	TIME [epoch: 8.3 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03515861709087988		[learning rate: 0.0014749]
		[batch 20/20] avg loss: 0.048261575549784		[learning rate: 0.0014722]
	Learning Rate: 0.00147222
	LOSS [training: 0.04171009632033193 | validation: 0.0563514529852839]
	TIME [epoch: 8.28 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08152041886520872		[learning rate: 0.0014695]
		[batch 20/20] avg loss: 0.05990819613318825		[learning rate: 0.0014669]
	Learning Rate: 0.00146688
	LOSS [training: 0.07071430749919848 | validation: 0.10582683342804936]
	TIME [epoch: 8.29 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06363708783303637		[learning rate: 0.0014642]
		[batch 20/20] avg loss: 0.04799602934301746		[learning rate: 0.0014616]
	Learning Rate: 0.00146156
	LOSS [training: 0.05581655858802692 | validation: 0.08210355275724751]
	TIME [epoch: 8.29 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06199992633958142		[learning rate: 0.0014589]
		[batch 20/20] avg loss: 0.05465989575187334		[learning rate: 0.0014563]
	Learning Rate: 0.00145625
	LOSS [training: 0.0583299110457274 | validation: 0.3296124526845699]
	TIME [epoch: 8.28 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10050319741117278		[learning rate: 0.0014536]
		[batch 20/20] avg loss: 0.072116336720328		[learning rate: 0.001451]
	Learning Rate: 0.00145097
	LOSS [training: 0.08630976706575037 | validation: 0.047385720276832445]
	TIME [epoch: 8.29 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06026076507856778		[learning rate: 0.0014483]
		[batch 20/20] avg loss: 0.056139520411868774		[learning rate: 0.0014457]
	Learning Rate: 0.0014457
	LOSS [training: 0.05820014274521827 | validation: 0.0029012809761930556]
	TIME [epoch: 8.29 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07814438489909963		[learning rate: 0.0014431]
		[batch 20/20] avg loss: 0.05290337614271973		[learning rate: 0.0014405]
	Learning Rate: 0.00144046
	LOSS [training: 0.06552388052090968 | validation: 0.02653798888976474]
	TIME [epoch: 8.3 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10266954966958275		[learning rate: 0.0014378]
		[batch 20/20] avg loss: 0.04452604696050071		[learning rate: 0.0014352]
	Learning Rate: 0.00143523
	LOSS [training: 0.07359779831504173 | validation: 0.030760446246356373]
	TIME [epoch: 8.27 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04081869129361192		[learning rate: 0.0014326]
		[batch 20/20] avg loss: 0.050764577491695925		[learning rate: 0.00143]
	Learning Rate: 0.00143002
	LOSS [training: 0.04579163439265392 | validation: 0.08614608118986229]
	TIME [epoch: 8.3 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07739956561403057		[learning rate: 0.0014274]
		[batch 20/20] avg loss: 0.033187293058394775		[learning rate: 0.0014248]
	Learning Rate: 0.00142483
	LOSS [training: 0.055293429336212674 | validation: 0.24136649171815258]
	TIME [epoch: 8.3 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1735522646321858		[learning rate: 0.0014222]
		[batch 20/20] avg loss: 0.044612826821682036		[learning rate: 0.0014197]
	Learning Rate: 0.00141966
	LOSS [training: 0.10908254572693392 | validation: 0.04920127119342009]
	TIME [epoch: 8.28 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08585508002728164		[learning rate: 0.0014171]
		[batch 20/20] avg loss: 0.06336901313687955		[learning rate: 0.0014145]
	Learning Rate: 0.00141451
	LOSS [training: 0.0746120465820806 | validation: 0.014596117949300033]
	TIME [epoch: 8.28 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08768325580899401		[learning rate: 0.0014119]
		[batch 20/20] avg loss: 0.04126982515165296		[learning rate: 0.0014094]
	Learning Rate: 0.00140937
	LOSS [training: 0.06447654048032347 | validation: 0.06270687742285883]
	TIME [epoch: 8.29 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06064981495021341		[learning rate: 0.0014068]
		[batch 20/20] avg loss: 0.04108226614957827		[learning rate: 0.0014043]
	Learning Rate: 0.00140426
	LOSS [training: 0.05086604054989585 | validation: 0.03952447498599824]
	TIME [epoch: 8.31 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054433513146898725		[learning rate: 0.0014017]
		[batch 20/20] avg loss: 0.066961763490096		[learning rate: 0.0013992]
	Learning Rate: 0.00139916
	LOSS [training: 0.06069763831849737 | validation: 0.046504105744127305]
	TIME [epoch: 8.29 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06965635130613178		[learning rate: 0.0013966]
		[batch 20/20] avg loss: 0.1250252897948711		[learning rate: 0.0013941]
	Learning Rate: 0.00139409
	LOSS [training: 0.09734082055050144 | validation: 0.07055510304314055]
	TIME [epoch: 8.28 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07843367240765639		[learning rate: 0.0013916]
		[batch 20/20] avg loss: 0.10332183314327699		[learning rate: 0.001389]
	Learning Rate: 0.00138903
	LOSS [training: 0.0908777527754667 | validation: 0.07262169004729314]
	TIME [epoch: 8.28 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11120306422171171		[learning rate: 0.0013865]
		[batch 20/20] avg loss: 0.11492182469031945		[learning rate: 0.001384]
	Learning Rate: 0.00138399
	LOSS [training: 0.11306244445601558 | validation: 0.056975155997603145]
	TIME [epoch: 8.34 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11425980890342513		[learning rate: 0.0013815]
		[batch 20/20] avg loss: 0.13277364329548527		[learning rate: 0.001379]
	Learning Rate: 0.00137896
	LOSS [training: 0.12351672609945519 | validation: 0.06420866887758613]
	TIME [epoch: 8.29 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07030892371391133		[learning rate: 0.0013765]
		[batch 20/20] avg loss: 0.06142115571782688		[learning rate: 0.001374]
	Learning Rate: 0.00137396
	LOSS [training: 0.06586503971586911 | validation: 0.04705565103962111]
	TIME [epoch: 8.27 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07661190574834739		[learning rate: 0.0013715]
		[batch 20/20] avg loss: 0.07490100915399388		[learning rate: 0.001369]
	Learning Rate: 0.00136897
	LOSS [training: 0.07575645745117063 | validation: 0.03697630122680995]
	TIME [epoch: 8.27 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1019753177062622		[learning rate: 0.0013665]
		[batch 20/20] avg loss: 0.09662518095313825		[learning rate: 0.001364]
	Learning Rate: 0.001364
	LOSS [training: 0.09930024932970022 | validation: 0.033263162793784745]
	TIME [epoch: 8.34 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07318521737933727		[learning rate: 0.0013615]
		[batch 20/20] avg loss: 0.13987762738100068		[learning rate: 0.0013591]
	Learning Rate: 0.00135905
	LOSS [training: 0.10653142238016897 | validation: 0.04693290065307389]
	TIME [epoch: 8.29 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08022047278512108		[learning rate: 0.0013566]
		[batch 20/20] avg loss: 0.12975689049630273		[learning rate: 0.0013541]
	Learning Rate: 0.00135412
	LOSS [training: 0.10498868164071191 | validation: 0.09394762422539633]
	TIME [epoch: 8.28 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1439015934972134		[learning rate: 0.0013517]
		[batch 20/20] avg loss: 0.10123920686505687		[learning rate: 0.0013492]
	Learning Rate: 0.00134921
	LOSS [training: 0.12257040018113512 | validation: 0.051039753494121376]
	TIME [epoch: 8.28 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08845503515025097		[learning rate: 0.0013468]
		[batch 20/20] avg loss: 0.11110188486907717		[learning rate: 0.0013443]
	Learning Rate: 0.00134431
	LOSS [training: 0.09977846000966407 | validation: 0.08802015840160797]
	TIME [epoch: 8.33 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10294824608814704		[learning rate: 0.0013419]
		[batch 20/20] avg loss: 0.0965319041155466		[learning rate: 0.0013394]
	Learning Rate: 0.00133943
	LOSS [training: 0.09974007510184683 | validation: 0.06479581330500259]
	TIME [epoch: 8.29 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11576694872858462		[learning rate: 0.001337]
		[batch 20/20] avg loss: 0.09663251042949897		[learning rate: 0.0013346]
	Learning Rate: 0.00133457
	LOSS [training: 0.10619972957904175 | validation: 0.032487249887292555]
	TIME [epoch: 8.27 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07619519545159847		[learning rate: 0.0013321]
		[batch 20/20] avg loss: 0.08873018953703436		[learning rate: 0.0013297]
	Learning Rate: 0.00132973
	LOSS [training: 0.08246269249431641 | validation: 0.09933226636991038]
	TIME [epoch: 8.28 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11610549471256286		[learning rate: 0.0013273]
		[batch 20/20] avg loss: 0.0748220001228207		[learning rate: 0.0013249]
	Learning Rate: 0.0013249
	LOSS [training: 0.09546374741769179 | validation: 0.05110424985455775]
	TIME [epoch: 8.31 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07643764972617458		[learning rate: 0.0013225]
		[batch 20/20] avg loss: 0.04763827140493036		[learning rate: 0.0013201]
	Learning Rate: 0.0013201
	LOSS [training: 0.06203796056555248 | validation: 0.028314978051100944]
	TIME [epoch: 8.31 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04042459587479458		[learning rate: 0.0013177]
		[batch 20/20] avg loss: 0.09607592130641593		[learning rate: 0.0013153]
	Learning Rate: 0.0013153
	LOSS [training: 0.06825025859060525 | validation: 0.09544129008954069]
	TIME [epoch: 8.27 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06179497772408593		[learning rate: 0.0013129]
		[batch 20/20] avg loss: 0.05205906219602698		[learning rate: 0.0013105]
	Learning Rate: 0.00131053
	LOSS [training: 0.056927019960056446 | validation: 0.11697760084474157]
	TIME [epoch: 8.28 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07198695544580647		[learning rate: 0.0013082]
		[batch 20/20] avg loss: 0.062141821591719604		[learning rate: 0.0013058]
	Learning Rate: 0.00130578
	LOSS [training: 0.06706438851876305 | validation: 0.07615084454672924]
	TIME [epoch: 8.32 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07071002856399741		[learning rate: 0.0013034]
		[batch 20/20] avg loss: 0.05001327714351683		[learning rate: 0.001301]
	Learning Rate: 0.00130104
	LOSS [training: 0.06036165285375712 | validation: 0.04122896405734564]
	TIME [epoch: 8.31 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06912849392985401		[learning rate: 0.0012987]
		[batch 20/20] avg loss: 0.07202783889049717		[learning rate: 0.0012963]
	Learning Rate: 0.00129631
	LOSS [training: 0.0705781664101756 | validation: 0.031701217104612656]
	TIME [epoch: 8.29 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07662225739437507		[learning rate: 0.001294]
		[batch 20/20] avg loss: 0.03533635904960218		[learning rate: 0.0012916]
	Learning Rate: 0.00129161
	LOSS [training: 0.055979308221988634 | validation: 0.034492549955221045]
	TIME [epoch: 8.28 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039298930365490384		[learning rate: 0.0012893]
		[batch 20/20] avg loss: 0.06840941963216285		[learning rate: 0.0012869]
	Learning Rate: 0.00128692
	LOSS [training: 0.05385417499882661 | validation: 0.03348088745759839]
	TIME [epoch: 8.32 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05003400910949164		[learning rate: 0.0012846]
		[batch 20/20] avg loss: 0.04293929891964897		[learning rate: 0.0012823]
	Learning Rate: 0.00128225
	LOSS [training: 0.0464866540145703 | validation: 0.03951212046655185]
	TIME [epoch: 8.28 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0531570808372462		[learning rate: 0.0012799]
		[batch 20/20] avg loss: 0.07519369168141872		[learning rate: 0.0012776]
	Learning Rate: 0.0012776
	LOSS [training: 0.06417538625933246 | validation: 0.06368922160544299]
	TIME [epoch: 8.3 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05473825401011366		[learning rate: 0.0012753]
		[batch 20/20] avg loss: 0.05536897596186687		[learning rate: 0.001273]
	Learning Rate: 0.00127296
	LOSS [training: 0.055053614985990276 | validation: 0.1043674469528057]
	TIME [epoch: 8.3 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10212299731144688		[learning rate: 0.0012707]
		[batch 20/20] avg loss: 0.040052075129296455		[learning rate: 0.0012683]
	Learning Rate: 0.00126834
	LOSS [training: 0.07108753622037164 | validation: 0.05189403463780947]
	TIME [epoch: 8.31 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0404551293837374		[learning rate: 0.001266]
		[batch 20/20] avg loss: 0.08000207883395632		[learning rate: 0.0012637]
	Learning Rate: 0.00126374
	LOSS [training: 0.06022860410884686 | validation: 0.025241472197129716]
	TIME [epoch: 8.29 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04708703576681072		[learning rate: 0.0012614]
		[batch 20/20] avg loss: 0.06056909826501715		[learning rate: 0.0012592]
	Learning Rate: 0.00125915
	LOSS [training: 0.05382806701591394 | validation: 0.03977602022262566]
	TIME [epoch: 8.3 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05294581470362438		[learning rate: 0.0012569]
		[batch 20/20] avg loss: 0.0351272642632304		[learning rate: 0.0012546]
	Learning Rate: 0.00125458
	LOSS [training: 0.044036539483427395 | validation: 0.025559735852224083]
	TIME [epoch: 8.3 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08288493079414501		[learning rate: 0.0012523]
		[batch 20/20] avg loss: 0.0882447566857486		[learning rate: 0.00125]
	Learning Rate: 0.00125003
	LOSS [training: 0.0855648437399468 | validation: 0.06809817768547323]
	TIME [epoch: 8.3 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026798810828130508		[learning rate: 0.0012478]
		[batch 20/20] avg loss: 0.07170046645625854		[learning rate: 0.0012455]
	Learning Rate: 0.0012455
	LOSS [training: 0.049249638642194525 | validation: 0.04860139601239897]
	TIME [epoch: 8.27 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07457562338221752		[learning rate: 0.0012432]
		[batch 20/20] avg loss: 0.11100951402762523		[learning rate: 0.001241]
	Learning Rate: 0.00124098
	LOSS [training: 0.09279256870492139 | validation: 0.10956081252739742]
	TIME [epoch: 8.3 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0914299810940239		[learning rate: 0.0012387]
		[batch 20/20] avg loss: 0.04153325555964691		[learning rate: 0.0012365]
	Learning Rate: 0.00123647
	LOSS [training: 0.06648161832683538 | validation: 0.017168518982111924]
	TIME [epoch: 8.31 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030533427063708384		[learning rate: 0.0012342]
		[batch 20/20] avg loss: 0.04685405420599349		[learning rate: 0.001232]
	Learning Rate: 0.00123198
	LOSS [training: 0.03869374063485093 | validation: 0.02439789457395898]
	TIME [epoch: 8.3 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08681023384618643		[learning rate: 0.0012297]
		[batch 20/20] avg loss: 0.058934732629875294		[learning rate: 0.0012275]
	Learning Rate: 0.00122751
	LOSS [training: 0.07287248323803087 | validation: 0.034547280389850665]
	TIME [epoch: 8.28 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04773955709503143		[learning rate: 0.0012253]
		[batch 20/20] avg loss: 0.0648806202202604		[learning rate: 0.0012231]
	Learning Rate: 0.00122306
	LOSS [training: 0.056310088657645904 | validation: 0.02171131135838634]
	TIME [epoch: 8.29 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03659139563264529		[learning rate: 0.0012208]
		[batch 20/20] avg loss: 0.039186373410732125		[learning rate: 0.0012186]
	Learning Rate: 0.00121862
	LOSS [training: 0.03788888452168872 | validation: 0.04127302606955506]
	TIME [epoch: 8.31 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03230582632825761		[learning rate: 0.0012164]
		[batch 20/20] avg loss: 0.03805488129548143		[learning rate: 0.0012142]
	Learning Rate: 0.0012142
	LOSS [training: 0.03518035381186953 | validation: 0.017180066492031618]
	TIME [epoch: 8.31 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041238268888690685		[learning rate: 0.001212]
		[batch 20/20] avg loss: 0.04814494192081162		[learning rate: 0.0012098]
	Learning Rate: 0.00120979
	LOSS [training: 0.04469160540475115 | validation: 0.027409529203512202]
	TIME [epoch: 8.27 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021078116891254613		[learning rate: 0.0012076]
		[batch 20/20] avg loss: 0.02572620807706617		[learning rate: 0.0012054]
	Learning Rate: 0.0012054
	LOSS [training: 0.023402162484160393 | validation: 0.07958148519651467]
	TIME [epoch: 8.28 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04924123319917019		[learning rate: 0.0012032]
		[batch 20/20] avg loss: 0.07253370874995177		[learning rate: 0.001201]
	Learning Rate: 0.00120103
	LOSS [training: 0.06088747097456099 | validation: 0.07204873704697189]
	TIME [epoch: 8.32 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05408387430399801		[learning rate: 0.0011988]
		[batch 20/20] avg loss: 0.05076491685656589		[learning rate: 0.0011967]
	Learning Rate: 0.00119667
	LOSS [training: 0.05242439558028196 | validation: 0.04599796430495832]
	TIME [epoch: 8.29 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06764520460998408		[learning rate: 0.0011945]
		[batch 20/20] avg loss: 0.06319317065505775		[learning rate: 0.0011923]
	Learning Rate: 0.00119233
	LOSS [training: 0.06541918763252091 | validation: 0.037868324965423715]
	TIME [epoch: 8.28 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047464630869622645		[learning rate: 0.0011902]
		[batch 20/20] avg loss: 0.08290565991582785		[learning rate: 0.001188]
	Learning Rate: 0.001188
	LOSS [training: 0.06518514539272524 | validation: 0.04417304716325584]
	TIME [epoch: 8.28 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06745259916014243		[learning rate: 0.0011858]
		[batch 20/20] avg loss: 0.12423896448487841		[learning rate: 0.0011837]
	Learning Rate: 0.00118369
	LOSS [training: 0.09584578182251044 | validation: 0.09451001442412205]
	TIME [epoch: 8.33 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10337459461426088		[learning rate: 0.0011815]
		[batch 20/20] avg loss: 0.09151018665349112		[learning rate: 0.0011794]
	Learning Rate: 0.00117939
	LOSS [training: 0.097442390633876 | validation: 0.034930785626824334]
	TIME [epoch: 8.3 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09492195951851216		[learning rate: 0.0011772]
		[batch 20/20] avg loss: 0.07858474091862397		[learning rate: 0.0011751]
	Learning Rate: 0.00117511
	LOSS [training: 0.08675335021856806 | validation: 0.08524930267004244]
	TIME [epoch: 8.29 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07882017288323762		[learning rate: 0.001173]
		[batch 20/20] avg loss: 0.05834943816976723		[learning rate: 0.0011708]
	Learning Rate: 0.00117085
	LOSS [training: 0.06858480552650242 | validation: 0.01479613556091847]
	TIME [epoch: 8.27 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05572227745356242		[learning rate: 0.0011687]
		[batch 20/20] avg loss: 0.09063609096127151		[learning rate: 0.0011666]
	Learning Rate: 0.0011666
	LOSS [training: 0.07317918420741697 | validation: 0.041556292662115975]
	TIME [epoch: 8.31 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038967793129866035		[learning rate: 0.0011645]
		[batch 20/20] avg loss: 0.0347014016181844		[learning rate: 0.0011624]
	Learning Rate: 0.00116236
	LOSS [training: 0.03683459737402523 | validation: 0.01672562183973809]
	TIME [epoch: 8.31 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05950763802353892		[learning rate: 0.0011603]
		[batch 20/20] avg loss: 0.05507711602390612		[learning rate: 0.0011581]
	Learning Rate: 0.00115815
	LOSS [training: 0.057292377023722516 | validation: 0.029121183592774748]
	TIME [epoch: 8.28 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04810682021566056		[learning rate: 0.001156]
		[batch 20/20] avg loss: 0.04240462068517512		[learning rate: 0.0011539]
	Learning Rate: 0.00115394
	LOSS [training: 0.04525572045041785 | validation: 0.021544146812372756]
	TIME [epoch: 8.28 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028195124978557383		[learning rate: 0.0011518]
		[batch 20/20] avg loss: 0.05684403390141986		[learning rate: 0.0011498]
	Learning Rate: 0.00114975
	LOSS [training: 0.04251957943998862 | validation: 0.04378045451028869]
	TIME [epoch: 8.32 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035200193653420184		[learning rate: 0.0011477]
		[batch 20/20] avg loss: 0.05313936787898256		[learning rate: 0.0011456]
	Learning Rate: 0.00114558
	LOSS [training: 0.04416978076620138 | validation: 0.025487842072898885]
	TIME [epoch: 8.31 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06626372787652812		[learning rate: 0.0011435]
		[batch 20/20] avg loss: 0.04039221257856669		[learning rate: 0.0011414]
	Learning Rate: 0.00114142
	LOSS [training: 0.05332797022754741 | validation: 0.011871026953148874]
	TIME [epoch: 8.29 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043389502240176916		[learning rate: 0.0011394]
		[batch 20/20] avg loss: 0.06387446810049477		[learning rate: 0.0011373]
	Learning Rate: 0.00113728
	LOSS [training: 0.05363198517033583 | validation: 0.03327180754813648]
	TIME [epoch: 8.28 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04899488424811538		[learning rate: 0.0011352]
		[batch 20/20] avg loss: 0.08727628139667368		[learning rate: 0.0011332]
	Learning Rate: 0.00113316
	LOSS [training: 0.06813558282239451 | validation: 0.15740929437206475]
	TIME [epoch: 8.32 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06877047441183115		[learning rate: 0.0011311]
		[batch 20/20] avg loss: 0.0354406279746816		[learning rate: 0.001129]
	Learning Rate: 0.00112904
	LOSS [training: 0.05210555119325638 | validation: 0.026475869874141997]
	TIME [epoch: 8.3 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03941458175738137		[learning rate: 0.001127]
		[batch 20/20] avg loss: 0.04459091734567734		[learning rate: 0.0011249]
	Learning Rate: 0.00112495
	LOSS [training: 0.042002749551529356 | validation: 0.026271647126010395]
	TIME [epoch: 8.29 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06435886354788248		[learning rate: 0.0011229]
		[batch 20/20] avg loss: 0.06750073578855473		[learning rate: 0.0011209]
	Learning Rate: 0.00112086
	LOSS [training: 0.0659297996682186 | validation: 0.07157418315173943]
	TIME [epoch: 8.28 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051650707718411404		[learning rate: 0.0011188]
		[batch 20/20] avg loss: 0.026143261050624887		[learning rate: 0.0011168]
	Learning Rate: 0.0011168
	LOSS [training: 0.03889698438451815 | validation: 0.007005105857773917]
	TIME [epoch: 8.32 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05982093221884148		[learning rate: 0.0011148]
		[batch 20/20] avg loss: 0.09344710602769878		[learning rate: 0.0011127]
	Learning Rate: 0.00111274
	LOSS [training: 0.07663401912327014 | validation: 0.04726706118417519]
	TIME [epoch: 8.28 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0738321499177151		[learning rate: 0.0011107]
		[batch 20/20] avg loss: 0.08950695254975857		[learning rate: 0.0011087]
	Learning Rate: 0.0011087
	LOSS [training: 0.08166955123373684 | validation: 0.03139571707920198]
	TIME [epoch: 8.3 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05127894697821116		[learning rate: 0.0011067]
		[batch 20/20] avg loss: 0.02977881755124302		[learning rate: 0.0011047]
	Learning Rate: 0.00110468
	LOSS [training: 0.040528882264727104 | validation: 0.08029405286108071]
	TIME [epoch: 8.28 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06622032136438326		[learning rate: 0.0011027]
		[batch 20/20] avg loss: 0.067683194783736		[learning rate: 0.0011007]
	Learning Rate: 0.00110067
	LOSS [training: 0.06695175807405963 | validation: 0.10780612123079149]
	TIME [epoch: 8.3 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04088419565892894		[learning rate: 0.0010987]
		[batch 20/20] avg loss: 0.04982775189388432		[learning rate: 0.0010967]
	Learning Rate: 0.00109668
	LOSS [training: 0.045355973776406634 | validation: 0.06480668382344026]
	TIME [epoch: 8.27 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.080717645872681		[learning rate: 0.0010947]
		[batch 20/20] avg loss: 0.03638402170446499		[learning rate: 0.0010927]
	Learning Rate: 0.0010927
	LOSS [training: 0.058550833788572995 | validation: 0.032162236729038264]
	TIME [epoch: 8.3 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06397666238078903		[learning rate: 0.0010907]
		[batch 20/20] avg loss: 0.07282525004895471		[learning rate: 0.0010887]
	Learning Rate: 0.00108873
	LOSS [training: 0.06840095621487188 | validation: 0.037858126723520125]
	TIME [epoch: 8.3 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03745746889822939		[learning rate: 0.0010868]
		[batch 20/20] avg loss: 0.033727438033417614		[learning rate: 0.0010848]
	Learning Rate: 0.00108478
	LOSS [training: 0.0355924534658235 | validation: 0.08016524380428243]
	TIME [epoch: 8.3 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04194129642358886		[learning rate: 0.0010828]
		[batch 20/20] avg loss: 0.06230298858949486		[learning rate: 0.0010808]
	Learning Rate: 0.00108084
	LOSS [training: 0.05212214250654187 | validation: 0.05525619698919275]
	TIME [epoch: 8.27 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03234874573326131		[learning rate: 0.0010789]
		[batch 20/20] avg loss: 0.05313510723754776		[learning rate: 0.0010769]
	Learning Rate: 0.00107692
	LOSS [training: 0.042741926485404536 | validation: 0.08830292413627613]
	TIME [epoch: 8.28 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05034778079289861		[learning rate: 0.001075]
		[batch 20/20] avg loss: 0.07345326996641169		[learning rate: 0.001073]
	Learning Rate: 0.00107301
	LOSS [training: 0.06190052537965516 | validation: 0.023989091960654598]
	TIME [epoch: 8.31 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04062842204572894		[learning rate: 0.0010711]
		[batch 20/20] avg loss: 0.0525787206589445		[learning rate: 0.0010691]
	Learning Rate: 0.00106912
	LOSS [training: 0.04660357135233671 | validation: -0.004827961218677304]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_715.pth
	Model improved!!!
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05153737700578394		[learning rate: 0.0010672]
		[batch 20/20] avg loss: 0.03823628078568643		[learning rate: 0.0010652]
	Learning Rate: 0.00106524
	LOSS [training: 0.04488682889573519 | validation: 0.035675047636032506]
	TIME [epoch: 8.27 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043108280846972914		[learning rate: 0.0010633]
		[batch 20/20] avg loss: 0.05617777896112506		[learning rate: 0.0010614]
	Learning Rate: 0.00106137
	LOSS [training: 0.049643029904048994 | validation: 0.041359778398855636]
	TIME [epoch: 8.27 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0316579046478853		[learning rate: 0.0010594]
		[batch 20/20] avg loss: 0.05955247042011523		[learning rate: 0.0010575]
	Learning Rate: 0.00105752
	LOSS [training: 0.04560518753400027 | validation: 0.03647526787807867]
	TIME [epoch: 8.31 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05467923566227271		[learning rate: 0.0010556]
		[batch 20/20] avg loss: 0.03626736068731744		[learning rate: 0.0010537]
	Learning Rate: 0.00105368
	LOSS [training: 0.045473298174795077 | validation: -0.005892478248398444]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_719.pth
	Model improved!!!
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02526077676463308		[learning rate: 0.0010518]
		[batch 20/20] avg loss: 0.025236003022019903		[learning rate: 0.0010499]
	Learning Rate: 0.00104986
	LOSS [training: 0.025248389893326496 | validation: 0.051478808470420905]
	TIME [epoch: 8.28 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041935870514956244		[learning rate: 0.001048]
		[batch 20/20] avg loss: 0.050214674685349035		[learning rate: 0.0010461]
	Learning Rate: 0.00104605
	LOSS [training: 0.04607527260015265 | validation: 0.045371918552470566]
	TIME [epoch: 8.27 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030797978185529467		[learning rate: 0.0010442]
		[batch 20/20] avg loss: 0.018445764121330192		[learning rate: 0.0010423]
	Learning Rate: 0.00104225
	LOSS [training: 0.02462187115342983 | validation: 0.011091389232678881]
	TIME [epoch: 8.32 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03481387760172341		[learning rate: 0.0010404]
		[batch 20/20] avg loss: 0.04599933066291776		[learning rate: 0.0010385]
	Learning Rate: 0.00103847
	LOSS [training: 0.04040660413232059 | validation: 0.02811098310449839]
	TIME [epoch: 8.28 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08226851303346537		[learning rate: 0.0010366]
		[batch 20/20] avg loss: 0.027412932152227416		[learning rate: 0.0010347]
	Learning Rate: 0.0010347
	LOSS [training: 0.054840722592846405 | validation: 0.02457198995746784]
	TIME [epoch: 8.27 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03182724661640443		[learning rate: 0.0010328]
		[batch 20/20] avg loss: 0.05999213149074878		[learning rate: 0.0010309]
	Learning Rate: 0.00103095
	LOSS [training: 0.045909689053576605 | validation: 0.036432289313588356]
	TIME [epoch: 8.27 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04908727772058166		[learning rate: 0.0010291]
		[batch 20/20] avg loss: 0.04501209448289991		[learning rate: 0.0010272]
	Learning Rate: 0.00102721
	LOSS [training: 0.0470496861017408 | validation: 0.02784700155564585]
	TIME [epoch: 8.3 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03619487645709282		[learning rate: 0.0010253]
		[batch 20/20] avg loss: 0.04783692268427508		[learning rate: 0.0010235]
	Learning Rate: 0.00102348
	LOSS [training: 0.04201589957068396 | validation: 0.030730825900574313]
	TIME [epoch: 8.28 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040737235632608323		[learning rate: 0.0010216]
		[batch 20/20] avg loss: 0.048135300280925684		[learning rate: 0.0010198]
	Learning Rate: 0.00101976
	LOSS [training: 0.044436267956767014 | validation: 0.0366977955267419]
	TIME [epoch: 8.26 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08476034700510993		[learning rate: 0.0010179]
		[batch 20/20] avg loss: 0.0844459465271926		[learning rate: 0.0010161]
	Learning Rate: 0.00101606
	LOSS [training: 0.08460314676615127 | validation: 0.050734705067948145]
	TIME [epoch: 8.27 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04600840023723986		[learning rate: 0.0010142]
		[batch 20/20] avg loss: 0.050231314623066924		[learning rate: 0.0010124]
	Learning Rate: 0.00101238
	LOSS [training: 0.04811985743015339 | validation: 0.03347901618477445]
	TIME [epoch: 8.3 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055657521761131236		[learning rate: 0.0010105]
		[batch 20/20] avg loss: 0.04307443207473897		[learning rate: 0.0010087]
	Learning Rate: 0.0010087
	LOSS [training: 0.049365976917935095 | validation: 0.07394907498268777]
	TIME [epoch: 8.29 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04990307542657155		[learning rate: 0.0010069]
		[batch 20/20] avg loss: 0.047521754102085596		[learning rate: 0.001005]
	Learning Rate: 0.00100504
	LOSS [training: 0.04871241476432857 | validation: 0.026453577372430728]
	TIME [epoch: 8.27 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03924428259673524		[learning rate: 0.0010032]
		[batch 20/20] avg loss: 0.038106985022165266		[learning rate: 0.0010014]
	Learning Rate: 0.00100139
	LOSS [training: 0.03867563380945026 | validation: 0.03578006059456931]
	TIME [epoch: 8.27 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0802927435210424		[learning rate: 0.00099958]
		[batch 20/20] avg loss: 0.06248983468427801		[learning rate: 0.00099776]
	Learning Rate: 0.00099776
	LOSS [training: 0.07139128910266021 | validation: 0.051329555910550426]
	TIME [epoch: 8.3 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10417750888828696		[learning rate: 0.00099595]
		[batch 20/20] avg loss: 0.09489236329085068		[learning rate: 0.00099414]
	Learning Rate: 0.00099414
	LOSS [training: 0.09953493608956882 | validation: 0.06130521218032232]
	TIME [epoch: 8.3 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09641671756207708		[learning rate: 0.00099233]
		[batch 20/20] avg loss: 0.08786847990782229		[learning rate: 0.00099053]
	Learning Rate: 0.000990532
	LOSS [training: 0.09214259873494969 | validation: 0.05672991296506876]
	TIME [epoch: 8.29 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09767793952975586		[learning rate: 0.00098873]
		[batch 20/20] avg loss: 0.061882769616903845		[learning rate: 0.00098694]
	Learning Rate: 0.000986937
	LOSS [training: 0.07978035457332985 | validation: 0.011564465481573926]
	TIME [epoch: 8.27 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05461628224276934		[learning rate: 0.00098514]
		[batch 20/20] avg loss: 0.0549857661048475		[learning rate: 0.00098336]
	Learning Rate: 0.000983355
	LOSS [training: 0.05480102417380842 | validation: 0.052911424003461216]
	TIME [epoch: 8.31 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03982074795318584		[learning rate: 0.00098157]
		[batch 20/20] avg loss: 0.059973936193538226		[learning rate: 0.00097979]
	Learning Rate: 0.000979787
	LOSS [training: 0.049897342073362036 | validation: 0.0736643280751502]
	TIME [epoch: 8.28 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07574858424713685		[learning rate: 0.00097801]
		[batch 20/20] avg loss: 0.08897767191836035		[learning rate: 0.00097623]
	Learning Rate: 0.000976231
	LOSS [training: 0.08236312808274862 | validation: 0.036862703601153714]
	TIME [epoch: 8.3 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07402736048259613		[learning rate: 0.00097446]
		[batch 20/20] avg loss: 0.07234089721249841		[learning rate: 0.00097269]
	Learning Rate: 0.000972688
	LOSS [training: 0.07318412884754726 | validation: 0.06136785023426381]
	TIME [epoch: 8.28 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09050099040683125		[learning rate: 0.00097092]
		[batch 20/20] avg loss: 0.051291980138212664		[learning rate: 0.00096916]
	Learning Rate: 0.000969158
	LOSS [training: 0.07089648527252194 | validation: 0.027730509669089035]
	TIME [epoch: 8.31 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0316317388776739		[learning rate: 0.0009674]
		[batch 20/20] avg loss: 0.03413801210826974		[learning rate: 0.00096564]
	Learning Rate: 0.000965641
	LOSS [training: 0.032884875492971824 | validation: 0.023317182549917922]
	TIME [epoch: 8.3 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027875409044931652		[learning rate: 0.00096389]
		[batch 20/20] avg loss: 0.043980634792065765		[learning rate: 0.00096214]
	Learning Rate: 0.000962137
	LOSS [training: 0.03592802191849871 | validation: 0.11408832690532977]
	TIME [epoch: 8.3 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042357668267003164		[learning rate: 0.00096039]
		[batch 20/20] avg loss: 0.02805229352133779		[learning rate: 0.00095865]
	Learning Rate: 0.000958645
	LOSS [training: 0.03520498089417048 | validation: 0.13767208033851064]
	TIME [epoch: 8.27 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06907383454794062		[learning rate: 0.0009569]
		[batch 20/20] avg loss: 0.08079207658218841		[learning rate: 0.00095517]
	Learning Rate: 0.000955166
	LOSS [training: 0.07493295556506452 | validation: 0.04731556608443193]
	TIME [epoch: 8.31 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07059263193187187		[learning rate: 0.00095343]
		[batch 20/20] avg loss: 0.10044283202178901		[learning rate: 0.0009517]
	Learning Rate: 0.0009517
	LOSS [training: 0.08551773197683045 | validation: 0.015311129288764366]
	TIME [epoch: 8.29 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016920839003549203		[learning rate: 0.00094997]
		[batch 20/20] avg loss: 0.08767508859086352		[learning rate: 0.00094825]
	Learning Rate: 0.000948246
	LOSS [training: 0.05229796379720636 | validation: 0.021546814963199906]
	TIME [epoch: 8.29 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04850379433137988		[learning rate: 0.00094652]
		[batch 20/20] avg loss: 0.05847582452046387		[learning rate: 0.0009448]
	Learning Rate: 0.000944805
	LOSS [training: 0.05348980942592187 | validation: 0.03644079828536389]
	TIME [epoch: 8.28 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08265399886986546		[learning rate: 0.00094309]
		[batch 20/20] avg loss: 0.048617651894018385		[learning rate: 0.00094138]
	Learning Rate: 0.000941376
	LOSS [training: 0.06563582538194195 | validation: 0.041290862488986585]
	TIME [epoch: 8.32 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03696751544560059		[learning rate: 0.00093967]
		[batch 20/20] avg loss: 0.04367680863166887		[learning rate: 0.00093796]
	Learning Rate: 0.00093796
	LOSS [training: 0.040322162038634715 | validation: 0.07271928092699151]
	TIME [epoch: 8.27 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030631209773348593		[learning rate: 0.00093626]
		[batch 20/20] avg loss: 0.0525052694379143		[learning rate: 0.00093456]
	Learning Rate: 0.000934556
	LOSS [training: 0.04156823960563145 | validation: 0.06657998653609015]
	TIME [epoch: 8.29 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043070713907418484		[learning rate: 0.00093286]
		[batch 20/20] avg loss: 0.04110434186021504		[learning rate: 0.00093116]
	Learning Rate: 0.000931164
	LOSS [training: 0.04208752788381677 | validation: 0.017234537320426624]
	TIME [epoch: 8.3 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028626554779365106		[learning rate: 0.00092947]
		[batch 20/20] avg loss: 0.03438068736835366		[learning rate: 0.00092779]
	Learning Rate: 0.000927785
	LOSS [training: 0.031503621073859386 | validation: 0.05376726126232719]
	TIME [epoch: 8.32 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07586933337850783		[learning rate: 0.0009261]
		[batch 20/20] avg loss: 0.01828545372689567		[learning rate: 0.00092442]
	Learning Rate: 0.000924418
	LOSS [training: 0.04707739355270175 | validation: 0.01196630068085833]
	TIME [epoch: 8.27 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022707625957141077		[learning rate: 0.00092274]
		[batch 20/20] avg loss: 0.03435288652303644		[learning rate: 0.00092106]
	Learning Rate: 0.000921063
	LOSS [training: 0.02853025624008876 | validation: 0.02238283050926585]
	TIME [epoch: 8.28 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04153494135930798		[learning rate: 0.00091939]
		[batch 20/20] avg loss: 0.025687111402780644		[learning rate: 0.00091772]
	Learning Rate: 0.000917721
	LOSS [training: 0.03361102638104431 | validation: 0.04400945202607326]
	TIME [epoch: 8.31 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05151370447310359		[learning rate: 0.00091605]
		[batch 20/20] avg loss: 0.03495007574523802		[learning rate: 0.00091439]
	Learning Rate: 0.00091439
	LOSS [training: 0.04323189010917081 | validation: 0.009209068510891495]
	TIME [epoch: 8.32 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021631774795776847		[learning rate: 0.00091273]
		[batch 20/20] avg loss: 0.03188644818086759		[learning rate: 0.00091107]
	Learning Rate: 0.000911072
	LOSS [training: 0.026759111488322218 | validation: 0.045833862796973664]
	TIME [epoch: 8.27 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05436222430392379		[learning rate: 0.00090942]
		[batch 20/20] avg loss: 0.01995266110872384		[learning rate: 0.00090777]
	Learning Rate: 0.000907766
	LOSS [training: 0.03715744270632381 | validation: 0.005424223342289078]
	TIME [epoch: 8.27 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047680849223088864		[learning rate: 0.00090612]
		[batch 20/20] avg loss: 0.046319509464206966		[learning rate: 0.00090447]
	Learning Rate: 0.000904471
	LOSS [training: 0.047000179343647926 | validation: 0.03641943022121251]
	TIME [epoch: 8.3 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04128333836651819		[learning rate: 0.00090283]
		[batch 20/20] avg loss: 0.020673440558635316		[learning rate: 0.00090119]
	Learning Rate: 0.000901189
	LOSS [training: 0.030978389462576757 | validation: 0.053274951109797075]
	TIME [epoch: 8.32 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03497955166155688		[learning rate: 0.00089955]
		[batch 20/20] avg loss: 0.03744986730118424		[learning rate: 0.00089792]
	Learning Rate: 0.000897918
	LOSS [training: 0.03621470948137055 | validation: 0.007124686971340584]
	TIME [epoch: 8.27 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10243050114163224		[learning rate: 0.00089629]
		[batch 20/20] avg loss: 0.0382699864142013		[learning rate: 0.00089466]
	Learning Rate: 0.00089466
	LOSS [training: 0.07035024377791677 | validation: 0.07748643136995516]
	TIME [epoch: 8.27 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03382910549206874		[learning rate: 0.00089303]
		[batch 20/20] avg loss: 0.029537321517459764		[learning rate: 0.00089141]
	Learning Rate: 0.000891413
	LOSS [training: 0.031683213504764246 | validation: 0.0006521654094994243]
	TIME [epoch: 8.29 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05096624458686157		[learning rate: 0.00088979]
		[batch 20/20] avg loss: 0.03820288389956641		[learning rate: 0.00088818]
	Learning Rate: 0.000888178
	LOSS [training: 0.044584564243213984 | validation: 0.00675564029762803]
	TIME [epoch: 8.33 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0441840975059611		[learning rate: 0.00088656]
		[batch 20/20] avg loss: 0.03969644197998361		[learning rate: 0.00088495]
	Learning Rate: 0.000884955
	LOSS [training: 0.041940269742972346 | validation: -0.0014675452764413622]
	TIME [epoch: 8.28 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04519459157927466		[learning rate: 0.00088335]
		[batch 20/20] avg loss: 0.03370493285641017		[learning rate: 0.00088174]
	Learning Rate: 0.000881743
	LOSS [training: 0.03944976221784241 | validation: 0.012136108962703125]
	TIME [epoch: 8.28 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02132007574074971		[learning rate: 0.00088014]
		[batch 20/20] avg loss: 0.04248345987093737		[learning rate: 0.00087854]
	Learning Rate: 0.000878543
	LOSS [training: 0.03190176780584354 | validation: 0.027554841250134725]
	TIME [epoch: 8.3 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04981837463043504		[learning rate: 0.00087695]
		[batch 20/20] avg loss: 0.03554970745354498		[learning rate: 0.00087536]
	Learning Rate: 0.000875355
	LOSS [training: 0.04268404104199001 | validation: 0.025677265435377926]
	TIME [epoch: 8.32 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04906643974182325		[learning rate: 0.00087377]
		[batch 20/20] avg loss: 0.036739525595243815		[learning rate: 0.00087218]
	Learning Rate: 0.000872178
	LOSS [training: 0.042902982668533526 | validation: 0.039171101845942594]
	TIME [epoch: 8.29 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032364634274944834		[learning rate: 0.00087059]
		[batch 20/20] avg loss: 0.07918943893122203		[learning rate: 0.00086901]
	Learning Rate: 0.000869013
	LOSS [training: 0.05577703660308344 | validation: 0.04339854049258411]
	TIME [epoch: 8.27 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04266790547141218		[learning rate: 0.00086743]
		[batch 20/20] avg loss: 0.04910108888596036		[learning rate: 0.00086586]
	Learning Rate: 0.000865859
	LOSS [training: 0.04588449717868627 | validation: 0.02110326011284014]
	TIME [epoch: 8.29 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030305665099025714		[learning rate: 0.00086429]
		[batch 20/20] avg loss: 0.026199069280918096		[learning rate: 0.00086272]
	Learning Rate: 0.000862717
	LOSS [training: 0.028252367189971907 | validation: 0.009160449778390806]
	TIME [epoch: 8.31 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029438704578614132		[learning rate: 0.00086115]
		[batch 20/20] avg loss: 0.024423022386237517		[learning rate: 0.00085959]
	Learning Rate: 0.000859586
	LOSS [training: 0.02693086348242582 | validation: 0.005969396910301232]
	TIME [epoch: 8.3 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05083389943320925		[learning rate: 0.00085803]
		[batch 20/20] avg loss: 0.04678548047582603		[learning rate: 0.00085647]
	Learning Rate: 0.000856467
	LOSS [training: 0.04880968995451764 | validation: 0.019817288746819067]
	TIME [epoch: 8.27 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0405632284753008		[learning rate: 0.00085491]
		[batch 20/20] avg loss: 0.015785943960667313		[learning rate: 0.00085336]
	Learning Rate: 0.000853359
	LOSS [training: 0.02817458621798405 | validation: 0.009421699995867451]
	TIME [epoch: 8.28 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03125408076267207		[learning rate: 0.00085181]
		[batch 20/20] avg loss: 0.030143917327424337		[learning rate: 0.00085026]
	Learning Rate: 0.000850262
	LOSS [training: 0.0306989990450482 | validation: 0.02271375829386046]
	TIME [epoch: 8.3 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05841732817111904		[learning rate: 0.00084872]
		[batch 20/20] avg loss: 0.02898587476364899		[learning rate: 0.00084718]
	Learning Rate: 0.000847176
	LOSS [training: 0.04370160146738402 | validation: 0.04975136588553898]
	TIME [epoch: 8.29 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028246406237274424		[learning rate: 0.00084564]
		[batch 20/20] avg loss: 0.021962868648196417		[learning rate: 0.0008441]
	Learning Rate: 0.000844102
	LOSS [training: 0.025104637442735424 | validation: 0.014363131751829624]
	TIME [epoch: 8.27 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03807588815401923		[learning rate: 0.00084257]
		[batch 20/20] avg loss: 0.04000929393182294		[learning rate: 0.00084104]
	Learning Rate: 0.000841038
	LOSS [training: 0.039042591042921085 | validation: 0.019517824819104947]
	TIME [epoch: 8.28 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026019115414231304		[learning rate: 0.00083951]
		[batch 20/20] avg loss: 0.04810121313874619		[learning rate: 0.00083799]
	Learning Rate: 0.000837986
	LOSS [training: 0.03706016427648874 | validation: 0.03523975505007819]
	TIME [epoch: 8.3 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023367143954803972		[learning rate: 0.00083646]
		[batch 20/20] avg loss: 0.02190082488136496		[learning rate: 0.00083495]
	Learning Rate: 0.000834945
	LOSS [training: 0.022633984418084466 | validation: 0.019490081055003244]
	TIME [epoch: 8.29 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01708744690054514		[learning rate: 0.00083343]
		[batch 20/20] avg loss: 0.027290312363695352		[learning rate: 0.00083192]
	Learning Rate: 0.000831915
	LOSS [training: 0.022188879632120247 | validation: 0.0007088758051312477]
	TIME [epoch: 8.27 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0328796833189954		[learning rate: 0.0008304]
		[batch 20/20] avg loss: 0.04153571643803648		[learning rate: 0.0008289]
	Learning Rate: 0.000828896
	LOSS [training: 0.03720769987851594 | validation: 0.040079027020091335]
	TIME [epoch: 8.29 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04356474553628793		[learning rate: 0.00082739]
		[batch 20/20] avg loss: 0.03045182479033821		[learning rate: 0.00082589]
	Learning Rate: 0.000825888
	LOSS [training: 0.037008285163313066 | validation: -0.0011577961053909798]
	TIME [epoch: 8.3 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01576097715410902		[learning rate: 0.00082439]
		[batch 20/20] avg loss: 0.027889957422164036		[learning rate: 0.00082289]
	Learning Rate: 0.000822891
	LOSS [training: 0.021825467288136526 | validation: 0.00281582832983326]
	TIME [epoch: 8.27 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028471611177715034		[learning rate: 0.0008214]
		[batch 20/20] avg loss: 0.03878627343512708		[learning rate: 0.0008199]
	Learning Rate: 0.000819904
	LOSS [training: 0.033628942306421064 | validation: 0.015505168397928806]
	TIME [epoch: 8.3 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013511649198443304		[learning rate: 0.00081842]
		[batch 20/20] avg loss: 0.023855686067768424		[learning rate: 0.00081693]
	Learning Rate: 0.000816929
	LOSS [training: 0.01868366763310586 | validation: 0.035901183329929844]
	TIME [epoch: 8.28 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029325337193215194		[learning rate: 0.00081545]
		[batch 20/20] avg loss: 0.01926428245634467		[learning rate: 0.00081396]
	Learning Rate: 0.000813964
	LOSS [training: 0.024294809824779932 | validation: 0.01825555729515794]
	TIME [epoch: 8.29 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05450917799993037		[learning rate: 0.00081249]
		[batch 20/20] avg loss: 0.03962842190102429		[learning rate: 0.00081101]
	Learning Rate: 0.00081101
	LOSS [training: 0.04706879995047734 | validation: 0.040715278200706195]
	TIME [epoch: 8.27 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03264867589506401		[learning rate: 0.00080954]
		[batch 20/20] avg loss: 0.02478930570186293		[learning rate: 0.00080807]
	Learning Rate: 0.000808067
	LOSS [training: 0.02871899079846347 | validation: 0.0419543261555153]
	TIME [epoch: 8.3 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05215402769159737		[learning rate: 0.0008066]
		[batch 20/20] avg loss: 0.0467891148853682		[learning rate: 0.00080513]
	Learning Rate: 0.000805135
	LOSS [training: 0.04947157128848279 | validation: 0.008565361192834353]
	TIME [epoch: 8.29 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012699581812865865		[learning rate: 0.00080367]
		[batch 20/20] avg loss: 0.019936410296416067		[learning rate: 0.00080221]
	Learning Rate: 0.000802213
	LOSS [training: 0.016317996054640967 | validation: 0.03730019838407815]
	TIME [epoch: 8.29 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03293888597835692		[learning rate: 0.00080076]
		[batch 20/20] avg loss: 0.022886667790560932		[learning rate: 0.0007993]
	Learning Rate: 0.000799301
	LOSS [training: 0.02791277688445893 | validation: 0.027301756842991458]
	TIME [epoch: 8.28 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023946934506116593		[learning rate: 0.00079785]
		[batch 20/20] avg loss: 0.020392535049736045		[learning rate: 0.0007964]
	Learning Rate: 0.000796401
	LOSS [training: 0.022169734777926324 | validation: 0.015872692364181845]
	TIME [epoch: 8.28 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024915132560082345		[learning rate: 0.00079495]
		[batch 20/20] avg loss: 0.04343034455239868		[learning rate: 0.00079351]
	Learning Rate: 0.000793511
	LOSS [training: 0.034172738556240506 | validation: 0.014726473047417393]
	TIME [epoch: 8.31 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041282574051540996		[learning rate: 0.00079207]
		[batch 20/20] avg loss: 0.06714472494438876		[learning rate: 0.00079063]
	Learning Rate: 0.000790631
	LOSS [training: 0.05421364949796488 | validation: 0.03832770986511697]
	TIME [epoch: 8.29 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06276587877534942		[learning rate: 0.00078919]
		[batch 20/20] avg loss: 0.0539849684718083		[learning rate: 0.00078776]
	Learning Rate: 0.000787761
	LOSS [training: 0.05837542362357887 | validation: 0.044373210387132406]
	TIME [epoch: 8.27 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0697336437125948		[learning rate: 0.00078633]
		[batch 20/20] avg loss: 0.05979008092482626		[learning rate: 0.0007849]
	Learning Rate: 0.000784903
	LOSS [training: 0.06476186231871053 | validation: 0.044064680162287456]
	TIME [epoch: 8.26 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06321482683917881		[learning rate: 0.00078348]
		[batch 20/20] avg loss: 0.04956825221320362		[learning rate: 0.00078205]
	Learning Rate: 0.000782054
	LOSS [training: 0.05639153952619121 | validation: 0.01556920399715566]
	TIME [epoch: 8.34 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016255620404792347		[learning rate: 0.00078063]
		[batch 20/20] avg loss: 0.020526903919260157		[learning rate: 0.00077922]
	Learning Rate: 0.000779216
	LOSS [training: 0.018391262162026252 | validation: 0.014799542039809847]
	TIME [epoch: 8.27 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0359834941483649		[learning rate: 0.0007778]
		[batch 20/20] avg loss: 0.022313993605380382		[learning rate: 0.00077639]
	Learning Rate: 0.000776388
	LOSS [training: 0.02914874387687264 | validation: 0.020001764458224915]
	TIME [epoch: 8.27 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045316942366990846		[learning rate: 0.00077498]
		[batch 20/20] avg loss: 0.022190850847728382		[learning rate: 0.00077357]
	Learning Rate: 0.000773571
	LOSS [training: 0.03375389660735961 | validation: 0.03735936339216846]
	TIME [epoch: 8.27 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03212848270090045		[learning rate: 0.00077217]
		[batch 20/20] avg loss: 0.0396394758702842		[learning rate: 0.00077076]
	Learning Rate: 0.000770763
	LOSS [training: 0.035883979285592324 | validation: 0.01245254865475065]
	TIME [epoch: 8.32 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03356614698640507		[learning rate: 0.00076936]
		[batch 20/20] avg loss: 0.049576249182891594		[learning rate: 0.00076797]
	Learning Rate: 0.000767966
	LOSS [training: 0.041571198084648334 | validation: 0.008058496220736157]
	TIME [epoch: 8.29 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025718611124456565		[learning rate: 0.00076657]
		[batch 20/20] avg loss: 0.026723204048833955		[learning rate: 0.00076518]
	Learning Rate: 0.000765179
	LOSS [training: 0.02622090758664526 | validation: -0.0027873470107144653]
	TIME [epoch: 8.26 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015068091505715031		[learning rate: 0.00076379]
		[batch 20/20] avg loss: 0.01902881379814027		[learning rate: 0.0007624]
	Learning Rate: 0.000762402
	LOSS [training: 0.01704845265192765 | validation: 0.014204223612561066]
	TIME [epoch: 8.27 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022081696625526684		[learning rate: 0.00076102]
		[batch 20/20] avg loss: 0.041982599923333676		[learning rate: 0.00075964]
	Learning Rate: 0.000759636
	LOSS [training: 0.03203214827443018 | validation: 0.06696400800870103]
	TIME [epoch: 8.3 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029678238544721326		[learning rate: 0.00075826]
		[batch 20/20] avg loss: 0.03449933185589974		[learning rate: 0.00075688]
	Learning Rate: 0.000756879
	LOSS [training: 0.032088785200310534 | validation: 0.037070432406247696]
	TIME [epoch: 8.3 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03083042907138841		[learning rate: 0.0007555]
		[batch 20/20] avg loss: 0.04055896329512959		[learning rate: 0.00075413]
	Learning Rate: 0.000754132
	LOSS [training: 0.035694696183258996 | validation: 0.0993379588535028]
	TIME [epoch: 8.26 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026580686493378785		[learning rate: 0.00075276]
		[batch 20/20] avg loss: 0.03900766270060881		[learning rate: 0.0007514]
	Learning Rate: 0.000751395
	LOSS [training: 0.032794174596993804 | validation: 0.034931615755244054]
	TIME [epoch: 8.27 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02854817211393686		[learning rate: 0.00075003]
		[batch 20/20] avg loss: 0.029602917808296842		[learning rate: 0.00074867]
	Learning Rate: 0.000748668
	LOSS [training: 0.02907554496111685 | validation: 0.05843799296496164]
	TIME [epoch: 8.3 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029184428341942226		[learning rate: 0.00074731]
		[batch 20/20] avg loss: 0.049266948654707696		[learning rate: 0.00074595]
	Learning Rate: 0.000745951
	LOSS [training: 0.03922568849832496 | validation: 0.04631766285040564]
	TIME [epoch: 8.3 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04445535449346551		[learning rate: 0.0007446]
		[batch 20/20] avg loss: 0.030617591128386207		[learning rate: 0.00074324]
	Learning Rate: 0.000743244
	LOSS [training: 0.03753647281092586 | validation: 0.02456665129826096]
	TIME [epoch: 8.27 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023956295977421597		[learning rate: 0.00074189]
		[batch 20/20] avg loss: 0.047237947472576494		[learning rate: 0.00074055]
	Learning Rate: 0.000740547
	LOSS [training: 0.03559712172499904 | validation: 0.00017021302089553856]
	TIME [epoch: 8.26 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031851720143986774		[learning rate: 0.0007392]
		[batch 20/20] avg loss: 0.029479129542556375		[learning rate: 0.00073786]
	Learning Rate: 0.00073786
	LOSS [training: 0.030665424843271578 | validation: 0.04089118231119429]
	TIME [epoch: 8.31 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028823349880543674		[learning rate: 0.00073652]
		[batch 20/20] avg loss: 0.031843919660475825		[learning rate: 0.00073518]
	Learning Rate: 0.000735182
	LOSS [training: 0.030333634770509744 | validation: 0.020616628038817388]
	TIME [epoch: 8.29 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027315158225913776		[learning rate: 0.00073385]
		[batch 20/20] avg loss: 0.04644250684097266		[learning rate: 0.00073251]
	Learning Rate: 0.000732514
	LOSS [training: 0.03687883253344322 | validation: 0.013435795282982978]
	TIME [epoch: 8.27 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04420478892975948		[learning rate: 0.00073118]
		[batch 20/20] avg loss: 0.02029432512014151		[learning rate: 0.00072986]
	Learning Rate: 0.000729855
	LOSS [training: 0.03224955702495049 | validation: 0.02678400847426688]
	TIME [epoch: 8.26 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01285402163096406		[learning rate: 0.00072853]
		[batch 20/20] avg loss: 0.014503650259885464		[learning rate: 0.00072721]
	Learning Rate: 0.000727207
	LOSS [training: 0.013678835945424763 | validation: 0.026128071110749197]
	TIME [epoch: 8.31 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03337619003058219		[learning rate: 0.00072589]
		[batch 20/20] avg loss: 0.028910215093504076		[learning rate: 0.00072457]
	Learning Rate: 0.000724568
	LOSS [training: 0.031143202562043126 | validation: 0.00804987236087399]
	TIME [epoch: 8.26 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015409822063966572		[learning rate: 0.00072325]
		[batch 20/20] avg loss: 0.013585381255530382		[learning rate: 0.00072194]
	Learning Rate: 0.000721938
	LOSS [training: 0.014497601659748477 | validation: 0.026459957095867355]
	TIME [epoch: 8.3 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034099316663635124		[learning rate: 0.00072063]
		[batch 20/20] avg loss: 0.031595410176278635		[learning rate: 0.00071932]
	Learning Rate: 0.000719318
	LOSS [training: 0.03284736341995688 | validation: 0.05019279692488818]
	TIME [epoch: 8.26 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023084524339713374		[learning rate: 0.00071801]
		[batch 20/20] avg loss: 0.030795931188478315		[learning rate: 0.00071671]
	Learning Rate: 0.000716708
	LOSS [training: 0.026940227764095843 | validation: 0.027625183478266577]
	TIME [epoch: 8.3 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020919645422662125		[learning rate: 0.00071541]
		[batch 20/20] avg loss: 0.025958502123619255		[learning rate: 0.00071411]
	Learning Rate: 0.000714107
	LOSS [training: 0.023439073773140688 | validation: 0.11452806373209498]
	TIME [epoch: 8.27 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05368984830921063		[learning rate: 0.00071281]
		[batch 20/20] avg loss: 0.032966653735825455		[learning rate: 0.00071152]
	Learning Rate: 0.000711515
	LOSS [training: 0.04332825102251804 | validation: 0.029028841783582292]
	TIME [epoch: 8.29 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024144383475194105		[learning rate: 0.00071022]
		[batch 20/20] avg loss: 0.024232534195669116		[learning rate: 0.00070893]
	Learning Rate: 0.000708933
	LOSS [training: 0.02418845883543161 | validation: 0.000617464174760397]
	TIME [epoch: 8.27 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04833318697219287		[learning rate: 0.00070765]
		[batch 20/20] avg loss: 0.05966822975000587		[learning rate: 0.00070636]
	Learning Rate: 0.00070636
	LOSS [training: 0.05400070836109938 | validation: 0.036443724182655055]
	TIME [epoch: 8.3 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0389584538235533		[learning rate: 0.00070508]
		[batch 20/20] avg loss: 0.022294946660906596		[learning rate: 0.0007038]
	Learning Rate: 0.000703797
	LOSS [training: 0.030626700242229955 | validation: 0.015483396316779493]
	TIME [epoch: 8.27 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013567570299796855		[learning rate: 0.00070252]
		[batch 20/20] avg loss: 0.026596061055906912		[learning rate: 0.00070124]
	Learning Rate: 0.000701243
	LOSS [training: 0.02008181567785188 | validation: 0.021506626342489656]
	TIME [epoch: 8.27 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03488643789873282		[learning rate: 0.00069997]
		[batch 20/20] avg loss: 0.01171400858254364		[learning rate: 0.0006987]
	Learning Rate: 0.000698698
	LOSS [training: 0.02330022324063823 | validation: 0.045790101840491954]
	TIME [epoch: 8.29 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022009089196526208		[learning rate: 0.00069743]
		[batch 20/20] avg loss: 0.011308350773971595		[learning rate: 0.00069616]
	Learning Rate: 0.000696162
	LOSS [training: 0.016658719985248902 | validation: 0.035711909830504554]
	TIME [epoch: 8.28 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028338973584614412		[learning rate: 0.0006949]
		[batch 20/20] avg loss: 0.026995657378911304		[learning rate: 0.00069364]
	Learning Rate: 0.000693636
	LOSS [training: 0.02766731548176286 | validation: -0.0045100828910026765]
	TIME [epoch: 8.26 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032380639048936		[learning rate: 0.00069238]
		[batch 20/20] avg loss: 0.0161295104563769		[learning rate: 0.00069112]
	Learning Rate: 0.000691119
	LOSS [training: 0.02425507475265645 | validation: 0.008498059473602623]
	TIME [epoch: 8.26 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05783925260407036		[learning rate: 0.00068986]
		[batch 20/20] avg loss: 0.05664997232527071		[learning rate: 0.00068861]
	Learning Rate: 0.000688611
	LOSS [training: 0.05724461246467053 | validation: 0.021745278322066935]
	TIME [epoch: 8.31 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049797032381467925		[learning rate: 0.00068736]
		[batch 20/20] avg loss: 0.050931855392706414		[learning rate: 0.00068611]
	Learning Rate: 0.000686112
	LOSS [training: 0.05036444388708716 | validation: 0.03885383023053587]
	TIME [epoch: 8.28 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046277168488742935		[learning rate: 0.00068487]
		[batch 20/20] avg loss: 0.023548958716010007		[learning rate: 0.00068362]
	Learning Rate: 0.000683622
	LOSS [training: 0.03491306360237647 | validation: 0.0020927585967101827]
	TIME [epoch: 8.25 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020502047024947907		[learning rate: 0.00068238]
		[batch 20/20] avg loss: 0.02388230513775145		[learning rate: 0.00068114]
	Learning Rate: 0.000681141
	LOSS [training: 0.022192176081349678 | validation: 0.06509386316530755]
	TIME [epoch: 8.26 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03203124843361148		[learning rate: 0.0006799]
		[batch 20/20] avg loss: 0.018053581293374765		[learning rate: 0.00067867]
	Learning Rate: 0.000678669
	LOSS [training: 0.025042414863493116 | validation: 0.042132865698052455]
	TIME [epoch: 8.31 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040748665631951765		[learning rate: 0.00067744]
		[batch 20/20] avg loss: 0.02559801579596298		[learning rate: 0.00067621]
	Learning Rate: 0.000676206
	LOSS [training: 0.033173340713957375 | validation: 0.024495282921790745]
	TIME [epoch: 8.28 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03649388811580462		[learning rate: 0.00067498]
		[batch 20/20] avg loss: 0.01268294666151828		[learning rate: 0.00067375]
	Learning Rate: 0.000673752
	LOSS [training: 0.024588417388661448 | validation: 0.0034555588404103083]
	TIME [epoch: 8.26 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019290321733335905		[learning rate: 0.00067253]
		[batch 20/20] avg loss: 0.021196579795891672		[learning rate: 0.00067131]
	Learning Rate: 0.000671307
	LOSS [training: 0.020243450764613785 | validation: 0.011357043634539144]
	TIME [epoch: 8.26 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02165737282849664		[learning rate: 0.00067009]
		[batch 20/20] avg loss: 0.028093691582682805		[learning rate: 0.00066887]
	Learning Rate: 0.000668871
	LOSS [training: 0.024875532205589725 | validation: 0.039642381572696256]
	TIME [epoch: 8.28 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0336042675516245		[learning rate: 0.00066766]
		[batch 20/20] avg loss: 0.02064905574169481		[learning rate: 0.00066644]
	Learning Rate: 0.000666443
	LOSS [training: 0.027126661646659656 | validation: 0.012957956690150076]
	TIME [epoch: 8.3 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017157938565018856		[learning rate: 0.00066523]
		[batch 20/20] avg loss: 0.008117513970129565		[learning rate: 0.00066402]
	Learning Rate: 0.000664025
	LOSS [training: 0.012637726267574211 | validation: 0.015225982382589253]
	TIME [epoch: 8.25 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05202948985825787		[learning rate: 0.00066282]
		[batch 20/20] avg loss: 0.01988168641917043		[learning rate: 0.00066161]
	Learning Rate: 0.000661615
	LOSS [training: 0.03595558813871415 | validation: 0.008985731853692511]
	TIME [epoch: 8.26 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026246599023552298		[learning rate: 0.00066041]
		[batch 20/20] avg loss: 0.015407699568888978		[learning rate: 0.00065921]
	Learning Rate: 0.000659214
	LOSS [training: 0.020827149296220638 | validation: 0.019702660110533864]
	TIME [epoch: 8.29 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02071564705504201		[learning rate: 0.00065802]
		[batch 20/20] avg loss: 0.01930490195744273		[learning rate: 0.00065682]
	Learning Rate: 0.000656822
	LOSS [training: 0.02001027450624237 | validation: 0.04532930271677217]
	TIME [epoch: 8.3 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038280352269631404		[learning rate: 0.00065563]
		[batch 20/20] avg loss: 0.040485698296388206		[learning rate: 0.00065444]
	Learning Rate: 0.000654438
	LOSS [training: 0.0393830252830098 | validation: 0.014822646645889693]
	TIME [epoch: 8.26 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030202963017157608		[learning rate: 0.00065325]
		[batch 20/20] avg loss: 0.0431871520316358		[learning rate: 0.00065206]
	Learning Rate: 0.000652063
	LOSS [training: 0.03669505752439671 | validation: 0.07444331290630914]
	TIME [epoch: 8.26 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04518001432502235		[learning rate: 0.00065088]
		[batch 20/20] avg loss: 0.030483707552076134		[learning rate: 0.0006497]
	Learning Rate: 0.000649697
	LOSS [training: 0.03783186093854923 | validation: 0.005164726712955168]
	TIME [epoch: 8.29 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0410029224619654		[learning rate: 0.00064852]
		[batch 20/20] avg loss: 0.027956450770068652		[learning rate: 0.00064734]
	Learning Rate: 0.000647339
	LOSS [training: 0.03447968661601703 | validation: 0.026615680688518613]
	TIME [epoch: 8.3 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04514465623770337		[learning rate: 0.00064616]
		[batch 20/20] avg loss: 0.02872987707239803		[learning rate: 0.00064499]
	Learning Rate: 0.00064499
	LOSS [training: 0.0369372666550507 | validation: 0.004070373508925551]
	TIME [epoch: 8.26 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01423676414722089		[learning rate: 0.00064382]
		[batch 20/20] avg loss: 0.024054912741142805		[learning rate: 0.00064265]
	Learning Rate: 0.000642649
	LOSS [training: 0.01914583844418185 | validation: 0.0862419785152742]
	TIME [epoch: 8.25 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04437818185495264		[learning rate: 0.00064148]
		[batch 20/20] avg loss: 0.0019493205401830474		[learning rate: 0.00064032]
	Learning Rate: 0.000640317
	LOSS [training: 0.02316375119756784 | validation: 0.0030303287551028844]
	TIME [epoch: 8.29 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007331962982955589		[learning rate: 0.00063915]
		[batch 20/20] avg loss: 0.016738385127260995		[learning rate: 0.00063799]
	Learning Rate: 0.000637993
	LOSS [training: 0.012035174055108294 | validation: 0.019507972291907406]
	TIME [epoch: 8.27 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02740333340597533		[learning rate: 0.00063683]
		[batch 20/20] avg loss: 0.026725754495399		[learning rate: 0.00063568]
	Learning Rate: 0.000635677
	LOSS [training: 0.02706454395068717 | validation: 0.0006547764338406647]
	TIME [epoch: 8.29 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013693813901879143		[learning rate: 0.00063452]
		[batch 20/20] avg loss: 0.014699211247717609		[learning rate: 0.00063337]
	Learning Rate: 0.000633371
	LOSS [training: 0.014196512574798375 | validation: 0.019106072310475473]
	TIME [epoch: 8.26 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009183929324208062		[learning rate: 0.00063222]
		[batch 20/20] avg loss: 0.019957911903446486		[learning rate: 0.00063107]
	Learning Rate: 0.000631072
	LOSS [training: 0.014570920613827273 | validation: 0.010557156940738518]
	TIME [epoch: 8.29 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030887323961977787		[learning rate: 0.00062993]
		[batch 20/20] avg loss: 0.04164235024475423		[learning rate: 0.00062878]
	Learning Rate: 0.000628782
	LOSS [training: 0.03626483710336601 | validation: 0.040536372768640534]
	TIME [epoch: 8.26 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012447052285609148		[learning rate: 0.00062764]
		[batch 20/20] avg loss: 0.022561865134067365		[learning rate: 0.0006265]
	Learning Rate: 0.0006265
	LOSS [training: 0.017504458709838258 | validation: 0.007444515514214219]
	TIME [epoch: 8.27 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013292156590677909		[learning rate: 0.00062536]
		[batch 20/20] avg loss: 0.019310505189540018		[learning rate: 0.00062423]
	Learning Rate: 0.000624226
	LOSS [training: 0.01630133089010896 | validation: 0.0066943096658634085]
	TIME [epoch: 8.27 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026151947303651585		[learning rate: 0.00062309]
		[batch 20/20] avg loss: 0.01124285749461497		[learning rate: 0.00062196]
	Learning Rate: 0.000621961
	LOSS [training: 0.018697402399133277 | validation: 0.008886553844908338]
	TIME [epoch: 8.31 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02888518627141335		[learning rate: 0.00062083]
		[batch 20/20] avg loss: 0.029553415836986258		[learning rate: 0.0006197]
	Learning Rate: 0.000619704
	LOSS [training: 0.029219301054199804 | validation: -0.0011937080029392378]
	TIME [epoch: 8.27 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007208464367199567		[learning rate: 0.00061858]
		[batch 20/20] avg loss: 0.007936547377461392		[learning rate: 0.00061745]
	Learning Rate: 0.000617455
	LOSS [training: 0.007572505872330481 | validation: 0.030778994078156905]
	TIME [epoch: 8.28 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02598781884953687		[learning rate: 0.00061633]
		[batch 20/20] avg loss: 0.027556723165606874		[learning rate: 0.00061521]
	Learning Rate: 0.000615214
	LOSS [training: 0.026772271007571874 | validation: -0.00168946335847795]
	TIME [epoch: 8.28 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01799217600728801		[learning rate: 0.0006141]
		[batch 20/20] avg loss: 0.016290214456998818		[learning rate: 0.00061298]
	Learning Rate: 0.000612982
	LOSS [training: 0.017141195232143417 | validation: 0.00357501009667117]
	TIME [epoch: 8.3 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0351264552677756		[learning rate: 0.00061187]
		[batch 20/20] avg loss: 0.019797237670263246		[learning rate: 0.00061076]
	Learning Rate: 0.000610757
	LOSS [training: 0.027461846469019426 | validation: 0.014398096280743705]
	TIME [epoch: 8.26 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016273120824255263		[learning rate: 0.00060965]
		[batch 20/20] avg loss: 0.015493794778450833		[learning rate: 0.00060854]
	Learning Rate: 0.00060854
	LOSS [training: 0.015883457801353047 | validation: 0.02129970195335072]
	TIME [epoch: 8.26 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025000179308402464		[learning rate: 0.00060744]
		[batch 20/20] avg loss: 0.012802061393406278		[learning rate: 0.00060633]
	Learning Rate: 0.000606332
	LOSS [training: 0.018901120350904372 | validation: 0.01972080238534096]
	TIME [epoch: 8.31 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020487680167371458		[learning rate: 0.00060523]
		[batch 20/20] avg loss: 0.03434627964379022		[learning rate: 0.00060413]
	Learning Rate: 0.000604132
	LOSS [training: 0.02741697990558084 | validation: -0.0012893561509444242]
	TIME [epoch: 8.3 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019201650431428012		[learning rate: 0.00060303]
		[batch 20/20] avg loss: 0.017892380508234038		[learning rate: 0.00060194]
	Learning Rate: 0.000601939
	LOSS [training: 0.01854701546983102 | validation: 0.010610089753284008]
	TIME [epoch: 8.26 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019932977198855735		[learning rate: 0.00060085]
		[batch 20/20] avg loss: 0.05086023867374505		[learning rate: 0.00059975]
	Learning Rate: 0.000599755
	LOSS [training: 0.035396607936300395 | validation: 0.05345417041348944]
	TIME [epoch: 8.27 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06686482738751895		[learning rate: 0.00059867]
		[batch 20/20] avg loss: 0.05400119953391815		[learning rate: 0.00059758]
	Learning Rate: 0.000597578
	LOSS [training: 0.060433013460718554 | validation: 0.04283204123385197]
	TIME [epoch: 8.3 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07114898465722073		[learning rate: 0.00059649]
		[batch 20/20] avg loss: 0.07427841460656862		[learning rate: 0.00059541]
	Learning Rate: 0.00059541
	LOSS [training: 0.07271369963189467 | validation: 0.04912289019776346]
	TIME [epoch: 8.28 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08464871487009512		[learning rate: 0.00059433]
		[batch 20/20] avg loss: 0.0703046260795377		[learning rate: 0.00059325]
	Learning Rate: 0.000593249
	LOSS [training: 0.0774766704748164 | validation: 0.040321033079263796]
	TIME [epoch: 8.26 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05563179290014593		[learning rate: 0.00059217]
		[batch 20/20] avg loss: 0.03233080892760445		[learning rate: 0.0005911]
	Learning Rate: 0.000591096
	LOSS [training: 0.043981300913875185 | validation: 0.018582936136965985]
	TIME [epoch: 8.25 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019393715912058206		[learning rate: 0.00059002]
		[batch 20/20] avg loss: 0.03323301404661745		[learning rate: 0.00058895]
	Learning Rate: 0.000588951
	LOSS [training: 0.02631336497933783 | validation: 0.03495410165142047]
	TIME [epoch: 8.29 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04032065128613191		[learning rate: 0.00058788]
		[batch 20/20] avg loss: 0.03757445443308864		[learning rate: 0.00058681]
	Learning Rate: 0.000586813
	LOSS [training: 0.03894755285961028 | validation: 0.013735885817172257]
	TIME [epoch: 8.3 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0167426926761273		[learning rate: 0.00058575]
		[batch 20/20] avg loss: 0.061606146967438345		[learning rate: 0.00058468]
	Learning Rate: 0.000584684
	LOSS [training: 0.03917441982178283 | validation: 0.0380975602082176]
	TIME [epoch: 8.27 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01786208113996165		[learning rate: 0.00058362]
		[batch 20/20] avg loss: 0.0190614791931817		[learning rate: 0.00058256]
	Learning Rate: 0.000582562
	LOSS [training: 0.018461780166571674 | validation: 0.026692673794187296]
	TIME [epoch: 8.26 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017400274032316802		[learning rate: 0.0005815]
		[batch 20/20] avg loss: 0.015268207662675188		[learning rate: 0.00058045]
	Learning Rate: 0.000580448
	LOSS [training: 0.016334240847495997 | validation: 0.001614335247800721]
	TIME [epoch: 8.28 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016868478467065646		[learning rate: 0.00057939]
		[batch 20/20] avg loss: 0.010979728760903598		[learning rate: 0.00057834]
	Learning Rate: 0.000578341
	LOSS [training: 0.01392410361398462 | validation: 0.003619667316820482]
	TIME [epoch: 8.32 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025236490485862812		[learning rate: 0.00057729]
		[batch 20/20] avg loss: 0.015694005121494516		[learning rate: 0.00057624]
	Learning Rate: 0.000576243
	LOSS [training: 0.020465247803678657 | validation: 0.03025986459624691]
	TIME [epoch: 8.27 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015501530064695129		[learning rate: 0.0005752]
		[batch 20/20] avg loss: 0.023214353123445976		[learning rate: 0.00057415]
	Learning Rate: 0.000574151
	LOSS [training: 0.019357941594070553 | validation: 0.021686491546767613]
	TIME [epoch: 8.26 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018092023008730203		[learning rate: 0.00057311]
		[batch 20/20] avg loss: 0.04160244045602779		[learning rate: 0.00057207]
	Learning Rate: 0.000572068
	LOSS [training: 0.029847231732378993 | validation: 0.010436448685324586]
	TIME [epoch: 8.29 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02860813072625132		[learning rate: 0.00057103]
		[batch 20/20] avg loss: 0.015886922421723695		[learning rate: 0.00056999]
	Learning Rate: 0.000569992
	LOSS [training: 0.022247526573987507 | validation: 0.006247490271270005]
	TIME [epoch: 8.29 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03156672887667061		[learning rate: 0.00056896]
		[batch 20/20] avg loss: 0.029993413056238094		[learning rate: 0.00056792]
	Learning Rate: 0.000567923
	LOSS [training: 0.030780070966454354 | validation: -0.002589040136866868]
	TIME [epoch: 8.28 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022468606406298112		[learning rate: 0.00056689]
		[batch 20/20] avg loss: 0.02028390079726374		[learning rate: 0.00056586]
	Learning Rate: 0.000565862
	LOSS [training: 0.02137625360178092 | validation: 0.025555019965383157]
	TIME [epoch: 8.27 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02261998043170784		[learning rate: 0.00056483]
		[batch 20/20] avg loss: 0.024213748206484857		[learning rate: 0.00056381]
	Learning Rate: 0.000563808
	LOSS [training: 0.02341686431909635 | validation: 0.041955912116398666]
	TIME [epoch: 8.29 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019862352963438377		[learning rate: 0.00056278]
		[batch 20/20] avg loss: 0.018200471619509176		[learning rate: 0.00056176]
	Learning Rate: 0.000561762
	LOSS [training: 0.01903141229147378 | validation: 0.01124670205396165]
	TIME [epoch: 8.29 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03472991084925087		[learning rate: 0.00056074]
		[batch 20/20] avg loss: 0.01793674208891282		[learning rate: 0.00055972]
	Learning Rate: 0.000559724
	LOSS [training: 0.02633332646908184 | validation: 0.009718846861801276]
	TIME [epoch: 8.29 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02801068258672349		[learning rate: 0.00055871]
		[batch 20/20] avg loss: 0.015408540407912013		[learning rate: 0.00055769]
	Learning Rate: 0.000557692
	LOSS [training: 0.021709611497317753 | validation: 0.0025793949589546393]
	TIME [epoch: 8.26 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008648941119881627		[learning rate: 0.00055668]
		[batch 20/20] avg loss: 0.014324535942634515		[learning rate: 0.00055567]
	Learning Rate: 0.000555669
	LOSS [training: 0.01148673853125807 | validation: 0.005981313747755098]
	TIME [epoch: 8.28 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023421027931785927		[learning rate: 0.00055466]
		[batch 20/20] avg loss: 0.01686716937155694		[learning rate: 0.00055365]
	Learning Rate: 0.000553652
	LOSS [training: 0.02014409865167143 | validation: 0.021058506939173313]
	TIME [epoch: 8.27 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043388579292994875		[learning rate: 0.00055265]
		[batch 20/20] avg loss: 0.022962199096092217		[learning rate: 0.00055164]
	Learning Rate: 0.000551643
	LOSS [training: 0.03317538919454354 | validation: 0.025958481613258384]
	TIME [epoch: 8.27 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016466073984591604		[learning rate: 0.00055064]
		[batch 20/20] avg loss: 0.024907066404460436		[learning rate: 0.00054964]
	Learning Rate: 0.000549641
	LOSS [training: 0.020686570194526015 | validation: 0.03865194452988518]
	TIME [epoch: 8.27 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020351534674357		[learning rate: 0.00054864]
		[batch 20/20] avg loss: 0.02065154410897505		[learning rate: 0.00054765]
	Learning Rate: 0.000547646
	LOSS [training: 0.02050153939166602 | validation: 0.007342719741464227]
	TIME [epoch: 8.29 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03251420709042484		[learning rate: 0.00054665]
		[batch 20/20] avg loss: 0.02450558508446716		[learning rate: 0.00054566]
	Learning Rate: 0.000545659
	LOSS [training: 0.028509896087446 | validation: 0.01187422510443745]
	TIME [epoch: 8.28 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014022511418710332		[learning rate: 0.00054467]
		[batch 20/20] avg loss: 0.03188996767442563		[learning rate: 0.00054368]
	Learning Rate: 0.000543678
	LOSS [training: 0.022956239546567978 | validation: 0.017796600128957205]
	TIME [epoch: 8.28 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011649321951263333		[learning rate: 0.00054269]
		[batch 20/20] avg loss: 0.01633889417152819		[learning rate: 0.00054171]
	Learning Rate: 0.000541705
	LOSS [training: 0.013994108061395763 | validation: 0.0028308908873841314]
	TIME [epoch: 8.27 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01804088581802337		[learning rate: 0.00054072]
		[batch 20/20] avg loss: 0.01857252435957811		[learning rate: 0.00053974]
	Learning Rate: 0.00053974
	LOSS [training: 0.01830670508880074 | validation: 0.0009239195663338976]
	TIME [epoch: 8.31 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007234987245508617		[learning rate: 0.00053876]
		[batch 20/20] avg loss: 0.01986977201357125		[learning rate: 0.00053778]
	Learning Rate: 0.000537781
	LOSS [training: 0.013552379629539929 | validation: -0.00520247843935508]
	TIME [epoch: 8.28 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017273513734045075		[learning rate: 0.0005368]
		[batch 20/20] avg loss: 0.02280797227076762		[learning rate: 0.00053583]
	Learning Rate: 0.000535829
	LOSS [training: 0.020040743002406346 | validation: 0.023992806025065656]
	TIME [epoch: 8.25 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01810421518202711		[learning rate: 0.00053486]
		[batch 20/20] avg loss: 0.013457571048075009		[learning rate: 0.00053388]
	Learning Rate: 0.000533885
	LOSS [training: 0.01578089311505106 | validation: 0.011952821792297529]
	TIME [epoch: 8.28 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017934094394805648		[learning rate: 0.00053291]
		[batch 20/20] avg loss: 0.01854336540856726		[learning rate: 0.00053195]
	Learning Rate: 0.000531947
	LOSS [training: 0.018238729901686453 | validation: 0.015496229294158483]
	TIME [epoch: 8.29 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013034041233059332		[learning rate: 0.00053098]
		[batch 20/20] avg loss: 0.011899316024039685		[learning rate: 0.00053002]
	Learning Rate: 0.000530017
	LOSS [training: 0.01246667862854951 | validation: 0.006170365477205919]
	TIME [epoch: 8.27 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020200036059165323		[learning rate: 0.00052905]
		[batch 20/20] avg loss: 0.010092047772096378		[learning rate: 0.00052809]
	Learning Rate: 0.000528093
	LOSS [training: 0.015146041915630851 | validation: 0.039477438514125426]
	TIME [epoch: 8.26 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029883497835222428		[learning rate: 0.00052713]
		[batch 20/20] avg loss: 0.03231847977200427		[learning rate: 0.00052618]
	Learning Rate: 0.000526177
	LOSS [training: 0.03110098880361335 | validation: 0.02014405484052402]
	TIME [epoch: 8.29 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021274764064883288		[learning rate: 0.00052522]
		[batch 20/20] avg loss: 0.03310856257349648		[learning rate: 0.00052427]
	Learning Rate: 0.000524267
	LOSS [training: 0.027191663319189884 | validation: 0.015696090061030635]
	TIME [epoch: 8.31 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022138216826571515		[learning rate: 0.00052332]
		[batch 20/20] avg loss: 0.02508167651464749		[learning rate: 0.00052236]
	Learning Rate: 0.000522365
	LOSS [training: 0.023609946670609498 | validation: 0.00665404794957423]
	TIME [epoch: 8.26 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01729757450746187		[learning rate: 0.00052142]
		[batch 20/20] avg loss: 0.012388523366722307		[learning rate: 0.00052047]
	Learning Rate: 0.000520469
	LOSS [training: 0.01484304893709209 | validation: -0.0016051861168319957]
	TIME [epoch: 8.27 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022470273301891268		[learning rate: 0.00051952]
		[batch 20/20] avg loss: 0.012487909959045203		[learning rate: 0.00051858]
	Learning Rate: 0.00051858
	LOSS [training: 0.01747909163046823 | validation: 0.016961007651515835]
	TIME [epoch: 8.27 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02475678614044258		[learning rate: 0.00051764]
		[batch 20/20] avg loss: 0.025752005329728996		[learning rate: 0.0005167]
	Learning Rate: 0.000516698
	LOSS [training: 0.02525439573508579 | validation: 0.012482280994345053]
	TIME [epoch: 8.33 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010983290227950897		[learning rate: 0.00051576]
		[batch 20/20] avg loss: 0.026562834604101672		[learning rate: 0.00051482]
	Learning Rate: 0.000514823
	LOSS [training: 0.018773062416026283 | validation: 0.006442982782082809]
	TIME [epoch: 8.26 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010836267714173845		[learning rate: 0.00051389]
		[batch 20/20] avg loss: 0.02792851255370426		[learning rate: 0.00051295]
	Learning Rate: 0.000512955
	LOSS [training: 0.01938239013393905 | validation: 0.004370607277864693]
	TIME [epoch: 8.28 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01623019154019226		[learning rate: 0.00051202]
		[batch 20/20] avg loss: 0.02081764581345678		[learning rate: 0.00051109]
	Learning Rate: 0.000511093
	LOSS [training: 0.018523918676824517 | validation: 0.027850666159148645]
	TIME [epoch: 8.27 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014012492643359398		[learning rate: 0.00051016]
		[batch 20/20] avg loss: 0.02559509273874676		[learning rate: 0.00050924]
	Learning Rate: 0.000509238
	LOSS [training: 0.01980379269105308 | validation: 0.013286249375740133]
	TIME [epoch: 8.32 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006300709341871101		[learning rate: 0.00050831]
		[batch 20/20] avg loss: 0.018121602648409917		[learning rate: 0.00050739]
	Learning Rate: 0.00050739
	LOSS [training: 0.012211155995140508 | validation: -0.007161018949251589]
	TIME [epoch: 8.27 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_920.pth
	Model improved!!!
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011492044369270635		[learning rate: 0.00050647]
		[batch 20/20] avg loss: 0.003862147413275796		[learning rate: 0.00050555]
	Learning Rate: 0.000505549
	LOSS [training: 0.007677095891273215 | validation: 0.000183873299544118]
	TIME [epoch: 8.25 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014148107598345061		[learning rate: 0.00050463]
		[batch 20/20] avg loss: 0.006649816047646839		[learning rate: 0.00050371]
	Learning Rate: 0.000503714
	LOSS [training: 0.01039896182299595 | validation: -0.004704649114824498]
	TIME [epoch: 8.27 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00562551194621648		[learning rate: 0.0005028]
		[batch 20/20] avg loss: 0.01509577688841981		[learning rate: 0.00050189]
	Learning Rate: 0.000501886
	LOSS [training: 0.010360644417318144 | validation: 0.0021333611193192154]
	TIME [epoch: 8.29 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012445153488710513		[learning rate: 0.00050097]
		[batch 20/20] avg loss: 0.011493429917954816		[learning rate: 0.00050006]
	Learning Rate: 0.000500065
	LOSS [training: 0.011969291703332665 | validation: 0.012463268463552266]
	TIME [epoch: 8.26 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03120595024014488		[learning rate: 0.00049916]
		[batch 20/20] avg loss: 0.018060209855998384		[learning rate: 0.00049825]
	Learning Rate: 0.00049825
	LOSS [training: 0.024633080048071635 | validation: 0.0033075376384292267]
	TIME [epoch: 8.25 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006045581204661167		[learning rate: 0.00049735]
		[batch 20/20] avg loss: 0.02046125029570097		[learning rate: 0.00049644]
	Learning Rate: 0.000496442
	LOSS [training: 0.01325341575018107 | validation: -0.006047350313830534]
	TIME [epoch: 8.29 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01831490321531541		[learning rate: 0.00049554]
		[batch 20/20] avg loss: 0.02728435848253229		[learning rate: 0.00049464]
	Learning Rate: 0.00049464
	LOSS [training: 0.02279963084892385 | validation: 0.007706314414778525]
	TIME [epoch: 8.27 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017905660385880595		[learning rate: 0.00049374]
		[batch 20/20] avg loss: 0.0035455129794942377		[learning rate: 0.00049285]
	Learning Rate: 0.000492845
	LOSS [training: 0.010725586682687416 | validation: 0.004885197137534899]
	TIME [epoch: 8.26 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009963659537748717		[learning rate: 0.00049195]
		[batch 20/20] avg loss: 0.02302652639296502		[learning rate: 0.00049106]
	Learning Rate: 0.000491057
	LOSS [training: 0.016495092965356874 | validation: 0.028715431525667]
	TIME [epoch: 8.25 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04352186909936123		[learning rate: 0.00049016]
		[batch 20/20] avg loss: 0.01730917967791492		[learning rate: 0.00048927]
	Learning Rate: 0.000489275
	LOSS [training: 0.030415524388638072 | validation: 0.00920504289154512]
	TIME [epoch: 8.29 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013762616899790431		[learning rate: 0.00048839]
		[batch 20/20] avg loss: 0.019830235358623178		[learning rate: 0.0004875]
	Learning Rate: 0.000487499
	LOSS [training: 0.016796426129206798 | validation: 0.023397930780909654]
	TIME [epoch: 8.26 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01679779599603936		[learning rate: 0.00048661]
		[batch 20/20] avg loss: 0.008996826221197034		[learning rate: 0.00048573]
	Learning Rate: 0.00048573
	LOSS [training: 0.012897311108618196 | validation: 0.0019030711058141926]
	TIME [epoch: 8.27 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007379077609882145		[learning rate: 0.00048485]
		[batch 20/20] avg loss: 0.0016328059980326528		[learning rate: 0.00048397]
	Learning Rate: 0.000483967
	LOSS [training: 0.004505941803957399 | validation: -0.004083361929637628]
	TIME [epoch: 8.26 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017646684663802762		[learning rate: 0.00048309]
		[batch 20/20] avg loss: 0.024269886480472177		[learning rate: 0.00048221]
	Learning Rate: 0.000482211
	LOSS [training: 0.02095828557213747 | validation: 0.007763512250055367]
	TIME [epoch: 8.31 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00688939523636718		[learning rate: 0.00048133]
		[batch 20/20] avg loss: 0.019634187235579467		[learning rate: 0.00048046]
	Learning Rate: 0.000480461
	LOSS [training: 0.013261791235973324 | validation: 0.025566706538257662]
	TIME [epoch: 8.26 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01518115438519511		[learning rate: 0.00047959]
		[batch 20/20] avg loss: 0.016175697326526694		[learning rate: 0.00047872]
	Learning Rate: 0.000478717
	LOSS [training: 0.015678425855860902 | validation: 0.011340841571863805]
	TIME [epoch: 8.27 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026545509867814183		[learning rate: 0.00047785]
		[batch 20/20] avg loss: 0.0015526040039582395		[learning rate: 0.00047698]
	Learning Rate: 0.00047698
	LOSS [training: 0.01404905693588621 | validation: 0.006553367631660747]
	TIME [epoch: 8.27 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015704324270378663		[learning rate: 0.00047611]
		[batch 20/20] avg loss: 0.029001711700069828		[learning rate: 0.00047525]
	Learning Rate: 0.000475249
	LOSS [training: 0.02235301798522424 | validation: 0.01858538088322763]
	TIME [epoch: 8.3 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018695790227783483		[learning rate: 0.00047439]
		[batch 20/20] avg loss: 0.001064111104197347		[learning rate: 0.00047352]
	Learning Rate: 0.000473524
	LOSS [training: 0.009879950665990416 | validation: 0.009527747835787692]
	TIME [epoch: 8.26 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014028413457178198		[learning rate: 0.00047266]
		[batch 20/20] avg loss: 0.0017347815651042142		[learning rate: 0.00047181]
	Learning Rate: 0.000471806
	LOSS [training: 0.007881597511141206 | validation: 0.007805102901026522]
	TIME [epoch: 8.25 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007729624033792561		[learning rate: 0.00047095]
		[batch 20/20] avg loss: 0.005554789869777638		[learning rate: 0.00047009]
	Learning Rate: 0.000470093
	LOSS [training: 0.0066422069517851 | validation: -0.002585362135602875]
	TIME [epoch: 8.29 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009757401972760697		[learning rate: 0.00046924]
		[batch 20/20] avg loss: 0.006644688786507594		[learning rate: 0.00046839]
	Learning Rate: 0.000468388
	LOSS [training: 0.008201045379634144 | validation: 0.0026574224702596155]
	TIME [epoch: 8.28 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003610272621850883		[learning rate: 0.00046754]
		[batch 20/20] avg loss: 0.027596681780471893		[learning rate: 0.00046669]
	Learning Rate: 0.000466688
	LOSS [training: 0.015603477201161389 | validation: -0.0010803426419243474]
	TIME [epoch: 8.24 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027428309163299164		[learning rate: 0.00046584]
		[batch 20/20] avg loss: 0.010484199924310461		[learning rate: 0.00046499]
	Learning Rate: 0.000464994
	LOSS [training: 0.018956254543804812 | validation: 0.010445085113180803]
	TIME [epoch: 8.25 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012147857245970915		[learning rate: 0.00046415]
		[batch 20/20] avg loss: 0.007001530088028585		[learning rate: 0.00046331]
	Learning Rate: 0.000463307
	LOSS [training: 0.00957469366699975 | validation: 0.006192409724713908]
	TIME [epoch: 8.3 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032096844994411455		[learning rate: 0.00046247]
		[batch 20/20] avg loss: 0.025569603402971635		[learning rate: 0.00046163]
	Learning Rate: 0.000461625
	LOSS [training: 0.028833224198691543 | validation: 0.007837109891685328]
	TIME [epoch: 8.28 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027342983032492296		[learning rate: 0.00046079]
		[batch 20/20] avg loss: -5.919235620450019e-05		[learning rate: 0.00045995]
	Learning Rate: 0.00045995
	LOSS [training: 0.0013375529735223646 | validation: 0.0019362195926534717]
	TIME [epoch: 8.25 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004329198546714368		[learning rate: 0.00045911]
		[batch 20/20] avg loss: 0.008063349003493587		[learning rate: 0.00045828]
	Learning Rate: 0.000458281
	LOSS [training: 0.006196273775103978 | validation: 0.03378225140776677]
	TIME [epoch: 8.26 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015882008126176163		[learning rate: 0.00045745]
		[batch 20/20] avg loss: 0.01028191176939714		[learning rate: 0.00045662]
	Learning Rate: 0.000456618
	LOSS [training: 0.01308195994778665 | validation: -0.011721460892208112]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_949.pth
	Model improved!!!
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006182195908014438		[learning rate: 0.00045579]
		[batch 20/20] avg loss: 0.013480517401696951		[learning rate: 0.00045496]
	Learning Rate: 0.000454961
	LOSS [training: 0.009831356654855693 | validation: 0.0021192691590553258]
	TIME [epoch: 8.33 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006786751151077677		[learning rate: 0.00045413]
		[batch 20/20] avg loss: 0.006919604790971175		[learning rate: 0.00045331]
	Learning Rate: 0.000453309
	LOSS [training: 0.006853177971024427 | validation: -0.0063632534394984005]
	TIME [epoch: 8.28 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013229102035675993		[learning rate: 0.00045249]
		[batch 20/20] avg loss: 0.007955458826779883		[learning rate: 0.00045166]
	Learning Rate: 0.000451664
	LOSS [training: 0.01059228043122794 | validation: 0.03291656587681849]
	TIME [epoch: 8.29 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017395406815278184		[learning rate: 0.00045084]
		[batch 20/20] avg loss: 0.014269702958138744		[learning rate: 0.00045003]
	Learning Rate: 0.000450025
	LOSS [training: 0.01583255488670847 | validation: 0.016330066091221618]
	TIME [epoch: 8.32 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012554267848002015		[learning rate: 0.00044921]
		[batch 20/20] avg loss: 0.004339624956344436		[learning rate: 0.00044839]
	Learning Rate: 0.000448392
	LOSS [training: 0.008446946402173224 | validation: 0.008018732272536515]
	TIME [epoch: 8.32 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015320959021531844		[learning rate: 0.00044758]
		[batch 20/20] avg loss: 0.010464443432023112		[learning rate: 0.00044676]
	Learning Rate: 0.000446765
	LOSS [training: 0.012892701226777475 | validation: -0.003977244574140984]
	TIME [epoch: 8.28 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.411437084698282e-05		[learning rate: 0.00044595]
		[batch 20/20] avg loss: 0.0030140061465101175		[learning rate: 0.00044514]
	Learning Rate: 0.000445143
	LOSS [training: 0.0015390602586785504 | validation: -0.008150853724721014]
	TIME [epoch: 8.28 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01514709194030787		[learning rate: 0.00044434]
		[batch 20/20] avg loss: 0.014865966702313433		[learning rate: 0.00044353]
	Learning Rate: 0.000443528
	LOSS [training: 0.015006529321310655 | validation: 0.016597228001793814]
	TIME [epoch: 8.33 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007853840721039835		[learning rate: 0.00044272]
		[batch 20/20] avg loss: 0.0014104049580981087		[learning rate: 0.00044192]
	Learning Rate: 0.000441918
	LOSS [training: 0.004632122839568971 | validation: 0.0008898839604351977]
	TIME [epoch: 8.31 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02016970808103008		[learning rate: 0.00044112]
		[batch 20/20] avg loss: 0.015690979828201643		[learning rate: 0.00044031]
	Learning Rate: 0.000440315
	LOSS [training: 0.01793034395461586 | validation: 0.006430220485434352]
	TIME [epoch: 8.29 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016472500684640467		[learning rate: 0.00043952]
		[batch 20/20] avg loss: 0.04007026179434618		[learning rate: 0.00043872]
	Learning Rate: 0.000438717
	LOSS [training: 0.028271381239493326 | validation: 0.016097218337907074]
	TIME [epoch: 8.28 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02632835759938814		[learning rate: 0.00043792]
		[batch 20/20] avg loss: 0.021776545082521536		[learning rate: 0.00043712]
	Learning Rate: 0.000437125
	LOSS [training: 0.02405245134095484 | validation: 0.0035688610093536468]
	TIME [epoch: 8.33 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02031368580898261		[learning rate: 0.00043633]
		[batch 20/20] avg loss: 0.02085560406350683		[learning rate: 0.00043554]
	Learning Rate: 0.000435538
	LOSS [training: 0.020584644936244727 | validation: -0.0018356457395408304]
	TIME [epoch: 8.29 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012330802919877752		[learning rate: 0.00043475]
		[batch 20/20] avg loss: 0.0014339396550588643		[learning rate: 0.00043396]
	Learning Rate: 0.000433958
	LOSS [training: 0.006882371287468306 | validation: 0.006024955128664063]
	TIME [epoch: 8.31 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002739007855214105		[learning rate: 0.00043317]
		[batch 20/20] avg loss: 0.014072860589449291		[learning rate: 0.00043238]
	Learning Rate: 0.000432383
	LOSS [training: 0.008405934222331697 | validation: 0.02777603153403105]
	TIME [epoch: 8.29 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01881905197437258		[learning rate: 0.0004316]
		[batch 20/20] avg loss: 0.020719086746415254		[learning rate: 0.00043081]
	Learning Rate: 0.000430814
	LOSS [training: 0.019769069360393916 | validation: -0.008533921987037549]
	TIME [epoch: 8.33 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021065648598127967		[learning rate: 0.00043003]
		[batch 20/20] avg loss: 0.014841692894517195		[learning rate: 0.00042925]
	Learning Rate: 0.00042925
	LOSS [training: 0.017953670746322585 | validation: -0.00034801677305377094]
	TIME [epoch: 8.28 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0039275540342116255		[learning rate: 0.00042847]
		[batch 20/20] avg loss: 0.012706138132305944		[learning rate: 0.00042769]
	Learning Rate: 0.000427692
	LOSS [training: 0.00438929204904716 | validation: -0.007473112804780886]
	TIME [epoch: 8.31 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008773613415972968		[learning rate: 0.00042692]
		[batch 20/20] avg loss: 0.006251877388747622		[learning rate: 0.00042614]
	Learning Rate: 0.00042614
	LOSS [training: 0.003564619365172459 | validation: 0.004073038247139187]
	TIME [epoch: 8.3 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.36964454592344e-05		[learning rate: 0.00042537]
		[batch 20/20] avg loss: 0.01310091836641917		[learning rate: 0.00042459]
	Learning Rate: 0.000424594
	LOSS [training: 0.0065973074059392026 | validation: 0.0326975048873414]
	TIME [epoch: 8.31 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013743668010983609		[learning rate: 0.00042382]
		[batch 20/20] avg loss: 0.01996223488280973		[learning rate: 0.00042305]
	Learning Rate: 0.000423053
	LOSS [training: 0.016852951446896674 | validation: 0.021985000378644582]
	TIME [epoch: 8.28 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023453412194552905		[learning rate: 0.00042228]
		[batch 20/20] avg loss: 0.005432685159654267		[learning rate: 0.00042152]
	Learning Rate: 0.000421518
	LOSS [training: 0.014443048677103587 | validation: 0.016611697329424104]
	TIME [epoch: 8.31 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04304565138987793		[learning rate: 0.00042075]
		[batch 20/20] avg loss: 0.01362561058132119		[learning rate: 0.00041999]
	Learning Rate: 0.000419988
	LOSS [training: 0.028335630985599564 | validation: 0.013278556339799689]
	TIME [epoch: 8.31 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02129534410653087		[learning rate: 0.00041923]
		[batch 20/20] avg loss: 0.013150308758612833		[learning rate: 0.00041846]
	Learning Rate: 0.000418464
	LOSS [training: 0.01722282643257185 | validation: 0.003843704747667527]
	TIME [epoch: 8.31 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004639234100672962		[learning rate: 0.0004177]
		[batch 20/20] avg loss: 0.0058898039933779605		[learning rate: 0.00041695]
	Learning Rate: 0.000416945
	LOSS [training: 0.005264519047025462 | validation: -0.004576098874379274]
	TIME [epoch: 8.28 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007767679279815735		[learning rate: 0.00041619]
		[batch 20/20] avg loss: 0.004708252105039232		[learning rate: 0.00041543]
	Learning Rate: 0.000415432
	LOSS [training: 0.006237965692427484 | validation: -0.005639051818846771]
	TIME [epoch: 8.29 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009405619446552415		[learning rate: 0.00041468]
		[batch 20/20] avg loss: 0.01491661496528435		[learning rate: 0.00041392]
	Learning Rate: 0.000413924
	LOSS [training: 0.012161117205918382 | validation: 0.006289281611469797]
	TIME [epoch: 8.34 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006828279736291796		[learning rate: 0.00041317]
		[batch 20/20] avg loss: 0.013958336944045201		[learning rate: 0.00041242]
	Learning Rate: 0.000412422
	LOSS [training: 0.010393308340168498 | validation: 0.006518421148290515]
	TIME [epoch: 8.3 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02058893228394473		[learning rate: 0.00041167]
		[batch 20/20] avg loss: 0.019568092033672217		[learning rate: 0.00041093]
	Learning Rate: 0.000410926
	LOSS [training: 0.02007851215880847 | validation: 0.008467216006736672]
	TIME [epoch: 8.29 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004747559786811543		[learning rate: 0.00041018]
		[batch 20/20] avg loss: 0.0181763177045337		[learning rate: 0.00040943]
	Learning Rate: 0.000409434
	LOSS [training: 0.01146193874567262 | validation: 0.02360443643343448]
	TIME [epoch: 8.28 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012031154362035721		[learning rate: 0.00040869]
		[batch 20/20] avg loss: 0.0007539679567088314		[learning rate: 0.00040795]
	Learning Rate: 0.000407948
	LOSS [training: 0.0063925611593722755 | validation: 0.003046033001961111]
	TIME [epoch: 8.34 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001060817940373435		[learning rate: 0.00040721]
		[batch 20/20] avg loss: 0.0029364587737735386		[learning rate: 0.00040647]
	Learning Rate: 0.000406468
	LOSS [training: 0.0009378204167000519 | validation: -0.006496296280488733]
	TIME [epoch: 8.29 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007028985498534364		[learning rate: 0.00040573]
		[batch 20/20] avg loss: 0.009360790817445255		[learning rate: 0.00040499]
	Learning Rate: 0.000404993
	LOSS [training: 0.00819488815798981 | validation: -0.007472562104139828]
	TIME [epoch: 8.29 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00035272491662378047		[learning rate: 0.00040426]
		[batch 20/20] avg loss: 0.0029444125001807436		[learning rate: 0.00040352]
	Learning Rate: 0.000403523
	LOSS [training: 0.0016485687084022622 | validation: 0.006899326554115298]
	TIME [epoch: 8.28 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010190790079760025		[learning rate: 0.00040279]
		[batch 20/20] avg loss: -6.374820223275374e-05		[learning rate: 0.00040206]
	Learning Rate: 0.000402059
	LOSS [training: 0.005063520938763636 | validation: -0.0014275443664422906]
	TIME [epoch: 8.33 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00038860225741997806		[learning rate: 0.00040133]
		[batch 20/20] avg loss: 0.008561478400378571		[learning rate: 0.0004006]
	Learning Rate: 0.0004006
	LOSS [training: 0.004086438071479298 | validation: -0.0023678959396401566]
	TIME [epoch: 8.3 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010039172882088733		[learning rate: 0.00039987]
		[batch 20/20] avg loss: 0.0010301384421523817		[learning rate: 0.00039915]
	Learning Rate: 0.000399146
	LOSS [training: 0.005534655662120557 | validation: 0.02275366272653691]
	TIME [epoch: 8.28 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025173664015261988		[learning rate: 0.00039842]
		[batch 20/20] avg loss: 0.0058104273202239775		[learning rate: 0.0003977]
	Learning Rate: 0.000397697
	LOSS [training: 0.015492045667742978 | validation: -0.0004142397259754746]
	TIME [epoch: 8.29 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011365415836005574		[learning rate: 0.00039698]
		[batch 20/20] avg loss: 0.001145268523202728		[learning rate: 0.00039625]
	Learning Rate: 0.000396254
	LOSS [training: 0.006255342179604151 | validation: 0.007673051408044676]
	TIME [epoch: 8.33 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004124390368039266		[learning rate: 0.00039553]
		[batch 20/20] avg loss: -0.004620149791828564		[learning rate: 0.00039482]
	Learning Rate: 0.000394816
	LOSS [training: -0.00024787971189464903 | validation: -0.005379461745593419]
	TIME [epoch: 8.31 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021889373984075116		[learning rate: 0.0003941]
		[batch 20/20] avg loss: 0.009816486156714273		[learning rate: 0.00039338]
	Learning Rate: 0.000393383
	LOSS [training: 0.006002711777560892 | validation: 0.020653863653458393]
	TIME [epoch: 8.29 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010396043784638107		[learning rate: 0.00039267]
		[batch 20/20] avg loss: 0.016351721684843737		[learning rate: 0.00039196]
	Learning Rate: 0.000391956
	LOSS [training: 0.013373882734740921 | validation: -0.0022668217797181425]
	TIME [epoch: 8.3 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005875904934485194		[learning rate: 0.00039124]
		[batch 20/20] avg loss: 0.006204822983388201		[learning rate: 0.00039053]
	Learning Rate: 0.000390533
	LOSS [training: 0.0060403639589366965 | validation: -0.0034116229503718186]
	TIME [epoch: 8.32 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014619012880999743		[learning rate: 0.00038982]
		[batch 20/20] avg loss: 0.007160807667312331		[learning rate: 0.00038912]
	Learning Rate: 0.000389116
	LOSS [training: 0.010889910274156037 | validation: -6.859184470311817e-05]
	TIME [epoch: 8.3 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008312319990815803		[learning rate: 0.00038841]
		[batch 20/20] avg loss: 0.013136144851011467		[learning rate: 0.0003877]
	Learning Rate: 0.000387704
	LOSS [training: 0.010724232420913634 | validation: -0.0056532109902846205]
	TIME [epoch: 8.29 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003791707436701918		[learning rate: 0.000387]
		[batch 20/20] avg loss: 0.001085019871257056		[learning rate: 0.0003863]
	Learning Rate: 0.000386297
	LOSS [training: 0.0024383636539794867 | validation: -0.003378368106070598]
	TIME [epoch: 8.3 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012153241735275695		[learning rate: 0.0003856]
		[batch 20/20] avg loss: -0.007062605183215519		[learning rate: 0.00038489]
	Learning Rate: 0.000384895
	LOSS [training: 0.0025453182760300876 | validation: -0.004297294285071656]
	TIME [epoch: 8.3 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01192129675233208		[learning rate: 0.0003842]
		[batch 20/20] avg loss: 0.006014464580710179		[learning rate: 0.0003835]
	Learning Rate: 0.000383498
	LOSS [training: 0.008967880666521127 | validation: 0.003668522514255518]
	TIME [epoch: 8.3 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010734496739956988		[learning rate: 0.0003828]
		[batch 20/20] avg loss: 0.003792282328036029		[learning rate: 0.00038211]
	Learning Rate: 0.000382106
	LOSS [training: 0.007263389533996507 | validation: -0.010341246737076185]
	TIME [epoch: 8.29 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0044203904911571685		[learning rate: 0.00038141]
		[batch 20/20] avg loss: 0.01811549099413303		[learning rate: 0.00038072]
	Learning Rate: 0.00038072
	LOSS [training: 0.011267940742645099 | validation: 0.010073326519647026]
	TIME [epoch: 8.31 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0202468445698425		[learning rate: 0.00038003]
		[batch 20/20] avg loss: 0.007374443036003833		[learning rate: 0.00037934]
	Learning Rate: 0.000379338
	LOSS [training: 0.013810643802923166 | validation: 0.002957406761014322]
	TIME [epoch: 8.31 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013173058598163196		[learning rate: 0.00037865]
		[batch 20/20] avg loss: -0.0017394251280666885		[learning rate: 0.00037796]
	Learning Rate: 0.000377961
	LOSS [training: 0.005716816735048254 | validation: 0.012797279865565819]
	TIME [epoch: 8.28 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009820802176400364		[learning rate: 0.00037727]
		[batch 20/20] avg loss: 0.025958050958116503		[learning rate: 0.00037659]
	Learning Rate: 0.00037659
	LOSS [training: 0.013470065587878268 | validation: 0.0026579244065495205]
	TIME [epoch: 8.31 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009538099623614585		[learning rate: 0.00037591]
		[batch 20/20] avg loss: 0.0033817928216106435		[learning rate: 0.00037522]
	Learning Rate: 0.000375223
	LOSS [training: 0.006459946222612614 | validation: 0.0019318099139005915]
	TIME [epoch: 8.3 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007487638936006303		[learning rate: 0.00037454]
		[batch 20/20] avg loss: 0.012714923187967436		[learning rate: 0.00037386]
	Learning Rate: 0.000373861
	LOSS [training: 0.01010128106198687 | validation: 0.0032822999568049354]
	TIME [epoch: 8.31 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008361301913873367		[learning rate: 0.00037318]
		[batch 20/20] avg loss: 0.007087613917484567		[learning rate: 0.0003725]
	Learning Rate: 0.000372505
	LOSS [training: 0.007724457915678967 | validation: 0.005251110364000187]
	TIME [epoch: 8.28 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023175141383632282		[learning rate: 0.00037183]
		[batch 20/20] avg loss: 0.007578647233019094		[learning rate: 0.00037115]
	Learning Rate: 0.000371153
	LOSS [training: 0.01537689430832569 | validation: -0.0036308437126698688]
	TIME [epoch: 8.3 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006356622000257055		[learning rate: 0.00037048]
		[batch 20/20] avg loss: 0.01490754434041451		[learning rate: 0.00036981]
	Learning Rate: 0.000369806
	LOSS [training: 0.010632083170335782 | validation: 0.03012341806183888]
	TIME [epoch: 8.32 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012842733663181274		[learning rate: 0.00036913]
		[batch 20/20] avg loss: -0.0031006738389332605		[learning rate: 0.00036846]
	Learning Rate: 0.000368464
	LOSS [training: 0.004871029912124008 | validation: 0.0032809391028078795]
	TIME [epoch: 8.29 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010375932155204616		[learning rate: 0.00036779]
		[batch 20/20] avg loss: 0.01017194247036761		[learning rate: 0.00036713]
	Learning Rate: 0.000367127
	LOSS [training: 0.004567174627423573 | validation: 0.051214218767832845]
	TIME [epoch: 8.29 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05127515463619362		[learning rate: 0.00036646]
		[batch 20/20] avg loss: 0.004495618861188713		[learning rate: 0.00036579]
	Learning Rate: 0.000365794
	LOSS [training: 0.02788538674869117 | validation: -0.005692600154481798]
	TIME [epoch: 8.29 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003185211958810321		[learning rate: 0.00036513]
		[batch 20/20] avg loss: 0.0062791651395076534		[learning rate: 0.00036447]
	Learning Rate: 0.000364467
	LOSS [training: 0.0015469765903486656 | validation: 0.005561115799531339]
	TIME [epoch: 8.33 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001507601100183402		[learning rate: 0.0003638]
		[batch 20/20] avg loss: 0.00327900339990263		[learning rate: 0.00036314]
	Learning Rate: 0.000363144
	LOSS [training: 0.002393302250043016 | validation: -0.0024572499928574485]
	TIME [epoch: 8.29 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036587373412192035		[learning rate: 0.00036248]
		[batch 20/20] avg loss: 0.01720083558297037		[learning rate: 0.00036183]
	Learning Rate: 0.000361826
	LOSS [training: 0.010429786462094788 | validation: 0.004797345996139824]
	TIME [epoch: 8.28 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027716825170258585		[learning rate: 0.00036117]
		[batch 20/20] avg loss: 0.0012137264535208505		[learning rate: 0.00036051]
	Learning Rate: 0.000360513
	LOSS [training: 0.0019927044852733533 | validation: -0.010403592104078875]
	TIME [epoch: 8.28 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006746600667506894		[learning rate: 0.00035986]
		[batch 20/20] avg loss: -0.0016305423466054666		[learning rate: 0.0003592]
	Learning Rate: 0.000359205
	LOSS [training: -0.0041885715070561795 | validation: -0.008223796567884006]
	TIME [epoch: 8.35 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010471448179485279		[learning rate: 0.00035855]
		[batch 20/20] avg loss: 0.0063821589409044244		[learning rate: 0.0003579]
	Learning Rate: 0.000357901
	LOSS [training: 0.008426803560194852 | validation: 0.0018189544058269337]
	TIME [epoch: 8.28 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012625244334916371		[learning rate: 0.00035725]
		[batch 20/20] avg loss: 0.018161353438549716		[learning rate: 0.0003566]
	Learning Rate: 0.000356602
	LOSS [training: 0.015393298886733048 | validation: 0.029812199389056656]
	TIME [epoch: 8.28 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014149521518399691		[learning rate: 0.00035595]
		[batch 20/20] avg loss: 0.009692263009150204		[learning rate: 0.00035531]
	Learning Rate: 0.000355308
	LOSS [training: 0.011920892263774949 | validation: 0.0010818316918679693]
	TIME [epoch: 8.29 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006812726793482843		[learning rate: 0.00035466]
		[batch 20/20] avg loss: 0.011046244037451773		[learning rate: 0.00035402]
	Learning Rate: 0.000354019
	LOSS [training: 0.00892948541546731 | validation: -0.00179446679045989]
	TIME [epoch: 8.35 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0047396160598421125		[learning rate: 0.00035338]
		[batch 20/20] avg loss: 0.004961941436576925		[learning rate: 0.00035273]
	Learning Rate: 0.000352734
	LOSS [training: 0.004850778748209519 | validation: 0.0017035841844705024]
	TIME [epoch: 8.28 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03237742059994466		[learning rate: 0.00035209]
		[batch 20/20] avg loss: 0.006021427167833951		[learning rate: 0.00035145]
	Learning Rate: 0.000351454
	LOSS [training: 0.01919942388388931 | validation: 0.011829741259901556]
	TIME [epoch: 8.28 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001047674655745537		[learning rate: 0.00035082]
		[batch 20/20] avg loss: 0.012381419041756775		[learning rate: 0.00035018]
	Learning Rate: 0.000350179
	LOSS [training: 0.005666872193005619 | validation: 0.004651055513206454]
	TIME [epoch: 8.3 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007896318413031976		[learning rate: 0.00034954]
		[batch 20/20] avg loss: 0.011141809960894833		[learning rate: 0.00034891]
	Learning Rate: 0.000348908
	LOSS [training: 0.005176089059795818 | validation: 0.018641177617752903]
	TIME [epoch: 8.31 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021882331732980317		[learning rate: 0.00034827]
		[batch 20/20] avg loss: 0.03004585768664798		[learning rate: 0.00034764]
	Learning Rate: 0.000347641
	LOSS [training: 0.025964094709814152 | validation: 0.011397237604383909]
	TIME [epoch: 8.29 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016785228461138897		[learning rate: 0.00034701]
		[batch 20/20] avg loss: 0.015311596910961681		[learning rate: 0.00034638]
	Learning Rate: 0.00034638
	LOSS [training: 0.01604841268605029 | validation: -0.010038019467085506]
	TIME [epoch: 8.28 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011983697550177499		[learning rate: 0.00034575]
		[batch 20/20] avg loss: 0.003767598977829784		[learning rate: 0.00034512]
	Learning Rate: 0.000345123
	LOSS [training: 0.00787564826400364 | validation: 0.005023071057133507]
	TIME [epoch: 8.3 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005799991501170942		[learning rate: 0.0003445]
		[batch 20/20] avg loss: 0.0046607024808186575		[learning rate: 0.00034387]
	Learning Rate: 0.00034387
	LOSS [training: 0.002620350815467876 | validation: 0.003219655669855082]
	TIME [epoch: 8.31 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021177300360991698		[learning rate: 0.00034325]
		[batch 20/20] avg loss: 0.003940082045958304		[learning rate: 0.00034262]
	Learning Rate: 0.000342622
	LOSS [training: 0.012558691203474998 | validation: -0.010333042041544362]
	TIME [epoch: 8.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011180561850005977		[learning rate: 0.000342]
		[batch 20/20] avg loss: 0.009942106608636599		[learning rate: 0.00034138]
	Learning Rate: 0.000341379
	LOSS [training: 0.01056133422932129 | validation: 0.010682932972113776]
	TIME [epoch: 8.28 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018462486570832027		[learning rate: 0.00034076]
		[batch 20/20] avg loss: 0.002980624197200097		[learning rate: 0.00034014]
	Learning Rate: 0.00034014
	LOSS [training: 0.00241343642714165 | validation: -0.0006572052504394117]
	TIME [epoch: 8.3 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00044434228189863136		[learning rate: 0.00033952]
		[batch 20/20] avg loss: 0.007189657086547814		[learning rate: 0.00033891]
	Learning Rate: 0.000338906
	LOSS [training: 0.0033726574023245914 | validation: 0.01149585310618881]
	TIME [epoch: 8.31 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007880543448734715		[learning rate: 0.00033829]
		[batch 20/20] avg loss: -0.0005109748799260183		[learning rate: 0.00033768]
	Learning Rate: 0.000337676
	LOSS [training: 0.0036847842844043473 | validation: 0.0059578463783581635]
	TIME [epoch: 8.3 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034345145772434896		[learning rate: 0.00033706]
		[batch 20/20] avg loss: 0.001234438496680512		[learning rate: 0.00033645]
	Learning Rate: 0.00033645
	LOSS [training: 0.0023344765369620007 | validation: 0.004977646993964689]
	TIME [epoch: 8.29 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02006708844362393		[learning rate: 0.00033584]
		[batch 20/20] avg loss: 0.017676094926185094		[learning rate: 0.00033523]
	Learning Rate: 0.000335229
	LOSS [training: 0.018871591684904508 | validation: 0.001763413998216442]
	TIME [epoch: 8.31 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006952593644291655		[learning rate: 0.00033462]
		[batch 20/20] avg loss: 0.010911802774593563		[learning rate: 0.00033401]
	Learning Rate: 0.000334013
	LOSS [training: 0.00893219820944261 | validation: -0.003580022549946668]
	TIME [epoch: 8.3 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006720311769448114		[learning rate: 0.00033341]
		[batch 20/20] avg loss: 0.009923347781053526		[learning rate: 0.0003328]
	Learning Rate: 0.000332801
	LOSS [training: 0.008321829775250818 | validation: 0.005147319067388605]
	TIME [epoch: 8.29 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014930843594447102		[learning rate: 0.0003322]
		[batch 20/20] avg loss: 0.0012008007849263111		[learning rate: 0.00033159]
	Learning Rate: 0.000331593
	LOSS [training: 0.008065822189686708 | validation: 0.015796664855724687]
	TIME [epoch: 8.31 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018528921592003374		[learning rate: 0.00033099]
		[batch 20/20] avg loss: 0.010989902992801545		[learning rate: 0.00033039]
	Learning Rate: 0.00033039
	LOSS [training: 0.01475941229240246 | validation: 0.014953922493367824]
	TIME [epoch: 8.32 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0054765862743426045		[learning rate: 0.00032979]
		[batch 20/20] avg loss: 0.020010186225435458		[learning rate: 0.00032919]
	Learning Rate: 0.000329191
	LOSS [training: 0.012743386249889032 | validation: 0.021712808086075126]
	TIME [epoch: 8.3 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006037779816130929		[learning rate: 0.00032859]
		[batch 20/20] avg loss: 0.004331873723412683		[learning rate: 0.000328]
	Learning Rate: 0.000327996
	LOSS [training: 0.005184826769771806 | validation: 0.004418842034558487]
	TIME [epoch: 8.28 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009340621284418515		[learning rate: 0.0003274]
		[batch 20/20] avg loss: 0.01087440795410789		[learning rate: 0.00032681]
	Learning Rate: 0.000326806
	LOSS [training: 0.010107514619263203 | validation: -0.0030806338078016213]
	TIME [epoch: 8.31 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003666653142642591		[learning rate: 0.00032621]
		[batch 20/20] avg loss: 0.007216311326842815		[learning rate: 0.00032562]
	Learning Rate: 0.00032562
	LOSS [training: 0.005441482234742704 | validation: 0.00019063631095180694]
	TIME [epoch: 8.32 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006441197258100942		[learning rate: 0.00032503]
		[batch 20/20] avg loss: -0.00012378108956686393		[learning rate: 0.00032444]
	Learning Rate: 0.000324438
	LOSS [training: -0.0003839504076884789 | validation: -0.009229032084138082]
	TIME [epoch: 8.29 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02243837110504477		[learning rate: 0.00032385]
		[batch 20/20] avg loss: 0.0023121299553557654		[learning rate: 0.00032326]
	Learning Rate: 0.00032326
	LOSS [training: 0.012375250530200268 | validation: 0.006208670305793227]
	TIME [epoch: 8.28 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032938129167392805		[learning rate: 0.00032267]
		[batch 20/20] avg loss: 0.0035930958472551754		[learning rate: 0.00032209]
	Learning Rate: 0.000322087
	LOSS [training: 0.0034434543819972276 | validation: -0.0007067367627984145]
	TIME [epoch: 8.3 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00195553048981796		[learning rate: 0.0003215]
		[batch 20/20] avg loss: 0.008889016337904067		[learning rate: 0.00032092]
	Learning Rate: 0.000320918
	LOSS [training: 0.005422273413861014 | validation: -0.002634278974001053]
	TIME [epoch: 8.34 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00014054446664913916		[learning rate: 0.00032034]
		[batch 20/20] avg loss: 0.0005613395413344987		[learning rate: 0.00031975]
	Learning Rate: 0.000319754
	LOSS [training: 0.0003509420039918188 | validation: 0.0027746151172512888]
	TIME [epoch: 8.28 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003917400773797119		[learning rate: 0.00031917]
		[batch 20/20] avg loss: 0.007915463742855351		[learning rate: 0.00031859]
	Learning Rate: 0.000318593
	LOSS [training: 0.0019990314845291157 | validation: -0.00469896625828407]
	TIME [epoch: 8.28 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004056558467047446		[learning rate: 0.00031801]
		[batch 20/20] avg loss: 0.013336236680986591		[learning rate: 0.00031744]
	Learning Rate: 0.000317437
	LOSS [training: 0.00869639757401702 | validation: 0.04204307585529944]
	TIME [epoch: 8.29 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025710143935548657		[learning rate: 0.00031686]
		[batch 20/20] avg loss: 0.004364916459281524		[learning rate: 0.00031629]
	Learning Rate: 0.000316285
	LOSS [training: 0.015037530197415095 | validation: -0.002726986350345328]
	TIME [epoch: 8.35 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032681521378003947		[learning rate: 0.00031571]
		[batch 20/20] avg loss: 0.010319465744679499		[learning rate: 0.00031514]
	Learning Rate: 0.000315137
	LOSS [training: 0.006793808941239947 | validation: -0.014179392365008427]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_1051.pth
	Model improved!!!
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016612995981719475		[learning rate: 0.00031457]
		[batch 20/20] avg loss: 0.0008148248099592428		[learning rate: 0.00031399]
	Learning Rate: 0.000313994
	LOSS [training: -0.00042323739410635224 | validation: 0.009574152757671966]
	TIME [epoch: 8.27 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01478582883785765		[learning rate: 0.00031342]
		[batch 20/20] avg loss: -0.0057155904369002464		[learning rate: 0.00031285]
	Learning Rate: 0.000312854
	LOSS [training: 0.004535119200478702 | validation: 0.0020083063448652684]
	TIME [epoch: 8.28 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004717149651650691		[learning rate: 0.00031229]
		[batch 20/20] avg loss: 0.0028584947989924612		[learning rate: 0.00031172]
	Learning Rate: 0.000311719
	LOSS [training: 0.0037878222253215762 | validation: -0.0002551501447379736]
	TIME [epoch: 8.32 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008519943517458373		[learning rate: 0.00031115]
		[batch 20/20] avg loss: 0.004288273084217344		[learning rate: 0.00031059]
	Learning Rate: 0.000310588
	LOSS [training: 0.006404108300837856 | validation: -0.002066899940919664]
	TIME [epoch: 8.27 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005150318562103051		[learning rate: 0.00031002]
		[batch 20/20] avg loss: -0.00023331854428684795		[learning rate: 0.00030946]
	Learning Rate: 0.000309461
	LOSS [training: 0.0001408566559617287 | validation: -0.0069998048355889905]
	TIME [epoch: 8.27 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0077241643324649206		[learning rate: 0.0003089]
		[batch 20/20] avg loss: 0.021182949179072288		[learning rate: 0.00030834]
	Learning Rate: 0.000308338
	LOSS [training: 0.014453556755768603 | validation: 0.013677433411710224]
	TIME [epoch: 8.29 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0107534789078963		[learning rate: 0.00030778]
		[batch 20/20] avg loss: 0.004891090013765241		[learning rate: 0.00030722]
	Learning Rate: 0.000307219
	LOSS [training: 0.007822284460830772 | validation: 0.0025205698135256868]
	TIME [epoch: 8.31 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012480316847297586		[learning rate: 0.00030666]
		[batch 20/20] avg loss: 0.0030125313693608073		[learning rate: 0.0003061]
	Learning Rate: 0.000306104
	LOSS [training: 0.007746424108329199 | validation: 0.002345989896382338]
	TIME [epoch: 8.28 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038322856963120154		[learning rate: 0.00030555]
		[batch 20/20] avg loss: 0.010233282375359207		[learning rate: 0.00030499]
	Learning Rate: 0.000304993
	LOSS [training: 0.007032784035835611 | validation: -0.01029419808015159]
	TIME [epoch: 8.26 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001471298391938673		[learning rate: 0.00030444]
		[batch 20/20] avg loss: 0.010696377049086367		[learning rate: 0.00030389]
	Learning Rate: 0.000303886
	LOSS [training: 0.00608383772051252 | validation: 0.018699350970251206]
	TIME [epoch: 8.31 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008725709844225533		[learning rate: 0.00030333]
		[batch 20/20] avg loss: 0.006220871851728381		[learning rate: 0.00030278]
	Learning Rate: 0.000302783
	LOSS [training: 0.007473290847976957 | validation: -0.00024424180184715494]
	TIME [epoch: 8.29 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014981736969698168		[learning rate: 0.00030223]
		[batch 20/20] avg loss: 0.014133285784397742		[learning rate: 0.00030168]
	Learning Rate: 0.000301684
	LOSS [training: 0.014557511377047954 | validation: -0.0007855795360915698]
	TIME [epoch: 8.3 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023143176905031718		[learning rate: 0.00030114]
		[batch 20/20] avg loss: 0.009931373749938731		[learning rate: 0.00030059]
	Learning Rate: 0.000300589
	LOSS [training: 0.0038085280297177797 | validation: -0.000591652877508753]
	TIME [epoch: 8.27 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.212998112817095e-05		[learning rate: 0.00030004]
		[batch 20/20] avg loss: 0.0335395743633634		[learning rate: 0.0002995]
	Learning Rate: 0.000299499
	LOSS [training: 0.016785852172245784 | validation: 0.021530308459583706]
	TIME [epoch: 8.31 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0138356616458852		[learning rate: 0.00029895]
		[batch 20/20] avg loss: 0.0002454175480458773		[learning rate: 0.00029841]
	Learning Rate: 0.000298412
	LOSS [training: 0.007040539596965538 | validation: -0.0034303196986797437]
	TIME [epoch: 8.27 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005864858994306741		[learning rate: 0.00029787]
		[batch 20/20] avg loss: 0.004146544529103123		[learning rate: 0.00029733]
	Learning Rate: 0.000297329
	LOSS [training: -0.0008591572326018093 | validation: -0.0015337051709665619]
	TIME [epoch: 8.29 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037468487737230367		[learning rate: 0.00029679]
		[batch 20/20] avg loss: 0.010279667542033605		[learning rate: 0.00029625]
	Learning Rate: 0.00029625
	LOSS [training: 0.003266409384155283 | validation: 0.01677316656305388]
	TIME [epoch: 8.27 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0265854060885178		[learning rate: 0.00029571]
		[batch 20/20] avg loss: 0.021274329021820342		[learning rate: 0.00029517]
	Learning Rate: 0.000295175
	LOSS [training: 0.023929867555169074 | validation: 0.009591650483285703]
	TIME [epoch: 8.31 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005287650137150867		[learning rate: 0.00029464]
		[batch 20/20] avg loss: 0.009764884464988495		[learning rate: 0.0002941]
	Learning Rate: 0.000294103
	LOSS [training: 0.007526267301069679 | validation: 0.020085140399284548]
	TIME [epoch: 8.28 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015800188307821324		[learning rate: 0.00029357]
		[batch 20/20] avg loss: 0.010029913822071263		[learning rate: 0.00029304]
	Learning Rate: 0.000293036
	LOSS [training: 0.012915051064946295 | validation: 0.025539982840787373]
	TIME [epoch: 8.29 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007637099371387557		[learning rate: 0.0002925]
		[batch 20/20] avg loss: -0.000920485910318786		[learning rate: 0.00029197]
	Learning Rate: 0.000291973
	LOSS [training: 0.003358306730534386 | validation: 0.0018502735930510425]
	TIME [epoch: 8.29 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005388692293684337		[learning rate: 0.00029144]
		[batch 20/20] avg loss: 0.010937260925043182		[learning rate: 0.00029091]
	Learning Rate: 0.000290913
	LOSS [training: 0.00816297660936376 | validation: -0.004760920485615917]
	TIME [epoch: 8.32 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004180343793121971		[learning rate: 0.00029038]
		[batch 20/20] avg loss: 0.0009410969800676559		[learning rate: 0.00028986]
	Learning Rate: 0.000289857
	LOSS [training: 0.0025607203865948133 | validation: -0.0011781619603182965]
	TIME [epoch: 8.27 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008840993243897959		[learning rate: 0.00028933]
		[batch 20/20] avg loss: 0.004531423669027239		[learning rate: 0.00028881]
	Learning Rate: 0.000288805
	LOSS [training: 0.002707761496708517 | validation: -0.013946711667887742]
	TIME [epoch: 8.27 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005335564909521831		[learning rate: 0.00028828]
		[batch 20/20] avg loss: 0.010671161978119261		[learning rate: 0.00028776]
	Learning Rate: 0.000287757
	LOSS [training: 0.008003363443820546 | validation: 0.0024410148901417225]
	TIME [epoch: 8.32 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034734010401296566		[learning rate: 0.00028723]
		[batch 20/20] avg loss: 0.013084963238014266		[learning rate: 0.00028671]
	Learning Rate: 0.000286713
	LOSS [training: 0.008279182139071962 | validation: 0.005349968660363528]
	TIME [epoch: 8.3 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008351651145367235		[learning rate: 0.00028619]
		[batch 20/20] avg loss: 0.01780889740366463		[learning rate: 0.00028567]
	Learning Rate: 0.000285672
	LOSS [training: 0.013080274274515935 | validation: -0.004970776752482848]
	TIME [epoch: 8.27 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004158822029775727		[learning rate: 0.00028515]
		[batch 20/20] avg loss: 0.011037882638729037		[learning rate: 0.00028464]
	Learning Rate: 0.000284636
	LOSS [training: 0.007598352334252384 | validation: -0.001116782666594509]
	TIME [epoch: 8.28 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006578165218967244		[learning rate: 0.00028412]
		[batch 20/20] avg loss: 0.0021772777348037222		[learning rate: 0.0002836]
	Learning Rate: 0.000283603
	LOSS [training: 0.004377721476885484 | validation: -0.006012586366446989]
	TIME [epoch: 8.32 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037133986199218245		[learning rate: 0.00028309]
		[batch 20/20] avg loss: 0.002655763406222248		[learning rate: 0.00028257]
	Learning Rate: 0.000282574
	LOSS [training: -0.0005288176068497885 | validation: -0.0009303874785887352]
	TIME [epoch: 8.3 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009674918768735167		[learning rate: 0.00028206]
		[batch 20/20] avg loss: 0.006402204883460975		[learning rate: 0.00028155]
	Learning Rate: 0.000281548
	LOSS [training: 0.008038561826098073 | validation: 0.011043155309217526]
	TIME [epoch: 8.27 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00016935895776576921		[learning rate: 0.00028104]
		[batch 20/20] avg loss: 0.005888758430007076		[learning rate: 0.00028053]
	Learning Rate: 0.000280526
	LOSS [training: 0.0030290586938864225 | validation: 0.007401451420531079]
	TIME [epoch: 8.27 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0040457781363627005		[learning rate: 0.00028002]
		[batch 20/20] avg loss: 0.009666994196905025		[learning rate: 0.00027951]
	Learning Rate: 0.000279508
	LOSS [training: 0.006856386166633864 | validation: 0.002029618367735803]
	TIME [epoch: 8.3 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007077622209147698		[learning rate: 0.000279]
		[batch 20/20] avg loss: -0.0013948790737742442		[learning rate: 0.00027849]
	Learning Rate: 0.000278494
	LOSS [training: 0.002841371567686727 | validation: -0.010264229225184978]
	TIME [epoch: 8.32 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005411442792205942		[learning rate: 0.00027799]
		[batch 20/20] avg loss: 0.006090696537424377		[learning rate: 0.00027748]
	Learning Rate: 0.000277483
	LOSS [training: 0.0003396268726092169 | validation: 0.007512662580835784]
	TIME [epoch: 8.27 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004777095740481846		[learning rate: 0.00027698]
		[batch 20/20] avg loss: 0.005339034807762837		[learning rate: 0.00027648]
	Learning Rate: 0.000276476
	LOSS [training: 0.0002809695336404955 | validation: -0.004242297483755582]
	TIME [epoch: 8.27 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008168055735446527		[learning rate: 0.00027597]
		[batch 20/20] avg loss: 0.006277618639402943		[learning rate: 0.00027547]
	Learning Rate: 0.000275473
	LOSS [training: 0.0027304065329291447 | validation: -0.0077915186471517375]
	TIME [epoch: 8.3 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014419782765103164		[learning rate: 0.00027497]
		[batch 20/20] avg loss: 0.023774374186229238		[learning rate: 0.00027447]
	Learning Rate: 0.000274473
	LOSS [training: 0.019097078475666197 | validation: 0.00016797198088672064]
	TIME [epoch: 8.32 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014873724627655386		[learning rate: 0.00027397]
		[batch 20/20] avg loss: 0.008909577540047687		[learning rate: 0.00027348]
	Learning Rate: 0.000273477
	LOSS [training: 0.011891651083851536 | validation: -0.0023073984655420537]
	TIME [epoch: 8.28 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016455425926701454		[learning rate: 0.00027298]
		[batch 20/20] avg loss: 0.0046831062228271676		[learning rate: 0.00027248]
	Learning Rate: 0.000272485
	LOSS [training: 0.010569266074764312 | validation: -0.0008104811325247623]
	TIME [epoch: 8.29 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01709938059747295		[learning rate: 0.00027199]
		[batch 20/20] avg loss: 0.013550569014061226		[learning rate: 0.0002715]
	Learning Rate: 0.000271496
	LOSS [training: 0.015324974805767092 | validation: 0.0034045138162356672]
	TIME [epoch: 8.3 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020833255869430526		[learning rate: 0.000271]
		[batch 20/20] avg loss: 0.019657234031460732		[learning rate: 0.00027051]
	Learning Rate: 0.000270511
	LOSS [training: 0.020245244950445627 | validation: 0.0015067414161958857]
	TIME [epoch: 8.32 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006557582427761807		[learning rate: 0.00027002]
		[batch 20/20] avg loss: -0.0007910699814852625		[learning rate: 0.00026953]
	Learning Rate: 0.000269529
	LOSS [training: 0.002883256223138272 | validation: 0.004626650209536053]
	TIME [epoch: 8.29 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0096451154558017		[learning rate: 0.00026904]
		[batch 20/20] avg loss: 0.0072707009682285465		[learning rate: 0.00026855]
	Learning Rate: 0.000268551
	LOSS [training: 0.008457908212015124 | validation: 0.009046570413816105]
	TIME [epoch: 8.28 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01251524696184173		[learning rate: 0.00026806]
		[batch 20/20] avg loss: 0.004131971616888037		[learning rate: 0.00026758]
	Learning Rate: 0.000267576
	LOSS [training: 0.008323609289364885 | validation: 0.0011531210507405704]
	TIME [epoch: 8.31 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00906846780602549		[learning rate: 0.00026709]
		[batch 20/20] avg loss: -0.004095563635400587		[learning rate: 0.00026661]
	Learning Rate: 0.000266605
	LOSS [training: 0.00248645208531245 | validation: -0.00027645822132951097]
	TIME [epoch: 8.29 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015949618917770949		[learning rate: 0.00026612]
		[batch 20/20] avg loss: 0.005604961451475614		[learning rate: 0.00026564]
	Learning Rate: 0.000265638
	LOSS [training: 0.003599961671626355 | validation: 0.007327628793015565]
	TIME [epoch: 8.3 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010829119430235136		[learning rate: 0.00026516]
		[batch 20/20] avg loss: -0.0016784019214333076		[learning rate: 0.00026467]
	Learning Rate: 0.000264674
	LOSS [training: 0.004575358754400914 | validation: -0.0047472904050095775]
	TIME [epoch: 8.28 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00010724156179441657		[learning rate: 0.00026419]
		[batch 20/20] avg loss: 0.007554192648760416		[learning rate: 0.00026371]
	Learning Rate: 0.000263713
	LOSS [training: 0.0037234755434829995 | validation: -0.0034964796977742544]
	TIME [epoch: 8.32 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00037365855648658285		[learning rate: 0.00026323]
		[batch 20/20] avg loss: 0.0013804371320272888		[learning rate: 0.00026276]
	Learning Rate: 0.000262756
	LOSS [training: 0.0008770478442569358 | validation: -0.007709929189809676]
	TIME [epoch: 8.28 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00795632727630231		[learning rate: 0.00026228]
		[batch 20/20] avg loss: 0.0011602059833160923		[learning rate: 0.0002618]
	Learning Rate: 0.000261802
	LOSS [training: -0.003398060646493109 | validation: -0.01110327924022812]
	TIME [epoch: 8.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011519064891545678		[learning rate: 0.00026133]
		[batch 20/20] avg loss: 0.004457740576852829		[learning rate: 0.00026085]
	Learning Rate: 0.000260852
	LOSS [training: 0.0028048235330036977 | validation: -0.008668597342330544]
	TIME [epoch: 8.28 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026060246783758635		[learning rate: 0.00026038]
		[batch 20/20] avg loss: 0.02909691763734066		[learning rate: 0.00025991]
	Learning Rate: 0.000259906
	LOSS [training: 0.01585147115785826 | validation: 0.028029551890380353]
	TIME [epoch: 8.32 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014195998500647445		[learning rate: 0.00025943]
		[batch 20/20] avg loss: 0.018747539520336365		[learning rate: 0.00025896]
	Learning Rate: 0.000258962
	LOSS [training: 0.016471769010491903 | validation: 0.008704046541846996]
	TIME [epoch: 8.27 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018077603261764975		[learning rate: 0.00025849]
		[batch 20/20] avg loss: 0.00692825658167312		[learning rate: 0.00025802]
	Learning Rate: 0.000258023
	LOSS [training: 0.012502929921719048 | validation: -0.0021410754959398388]
	TIME [epoch: 8.3 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009467730183710558		[learning rate: 0.00025755]
		[batch 20/20] avg loss: -8.480939186325511e-05		[learning rate: 0.00025709]
	Learning Rate: 0.000257086
	LOSS [training: 0.004691460395923651 | validation: -0.011234949176224324]
	TIME [epoch: 8.3 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001324423860661364		[learning rate: 0.00025662]
		[batch 20/20] avg loss: -0.007373458612219471		[learning rate: 0.00025615]
	Learning Rate: 0.000256153
	LOSS [training: -0.0030245173757790537 | validation: -0.011207545905581954]
	TIME [epoch: 8.31 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006897866702199627		[learning rate: 0.00025569]
		[batch 20/20] avg loss: 0.0044312329425716486		[learning rate: 0.00025522]
	Learning Rate: 0.000255224
	LOSS [training: 0.0018707231361758427 | validation: 0.012910193452779859]
	TIME [epoch: 8.27 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0053851751618741155		[learning rate: 0.00025476]
		[batch 20/20] avg loss: -0.0030804786451961636		[learning rate: 0.0002543]
	Learning Rate: 0.000254298
	LOSS [training: 0.0011523482583389768 | validation: -0.011071607123260151]
	TIME [epoch: 8.28 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013119567728670412		[learning rate: 0.00025384]
		[batch 20/20] avg loss: 0.003218392131393805		[learning rate: 0.00025337]
	Learning Rate: 0.000253375
	LOSS [training: 0.000953217679263382 | validation: -0.011056622809596931]
	TIME [epoch: 8.32 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019384959891787818		[learning rate: 0.00025291]
		[batch 20/20] avg loss: 0.003981171143853095		[learning rate: 0.00025246]
	Learning Rate: 0.000252455
	LOSS [training: 0.001021337577337156 | validation: 0.004496190072403892]
	TIME [epoch: 8.3 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007051157680897241		[learning rate: 0.000252]
		[batch 20/20] avg loss: -0.004056201446668639		[learning rate: 0.00025154]
	Learning Rate: 0.000251539
	LOSS [training: 0.001497478117114301 | validation: -0.014073171066997486]
	TIME [epoch: 8.28 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008311634671725479		[learning rate: 0.00025108]
		[batch 20/20] avg loss: 0.0018467341051492479		[learning rate: 0.00025063]
	Learning Rate: 0.000250626
	LOSS [training: -0.0032324502832881156 | validation: -0.0008032027736062756]
	TIME [epoch: 8.28 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016517054414688956		[learning rate: 0.00025017]
		[batch 20/20] avg loss: -0.004048384649116265		[learning rate: 0.00024972]
	Learning Rate: 0.000249717
	LOSS [training: -0.0028500450452925803 | validation: -0.0061461899873572045]
	TIME [epoch: 8.33 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007479934698659629		[learning rate: 0.00024926]
		[batch 20/20] avg loss: -0.0014284902985616924		[learning rate: 0.00024881]
	Learning Rate: 0.00024881
	LOSS [training: -0.00034024841434786453 | validation: -0.012103987148425022]
	TIME [epoch: 8.3 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007200502722625392		[learning rate: 0.00024836]
		[batch 20/20] avg loss: 0.0018774829286935704		[learning rate: 0.00024791]
	Learning Rate: 0.000247907
	LOSS [training: -0.0026615098969659107 | validation: -0.00916359794910948]
	TIME [epoch: 8.28 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023097821207706524		[learning rate: 0.00024746]
		[batch 20/20] avg loss: 0.0026175474915745007		[learning rate: 0.00024701]
	Learning Rate: 0.000247008
	LOSS [training: 0.002463664806172577 | validation: -0.00015912653020357624]
	TIME [epoch: 8.28 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004865725084511082		[learning rate: 0.00024656]
		[batch 20/20] avg loss: -0.004564499111280246		[learning rate: 0.00024611]
	Learning Rate: 0.000246111
	LOSS [training: -0.004715112097895663 | validation: -0.008886894901332735]
	TIME [epoch: 8.32 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007049143509176648		[learning rate: 0.00024566]
		[batch 20/20] avg loss: 0.0035508012184709433		[learning rate: 0.00024522]
	Learning Rate: 0.000245218
	LOSS [training: 0.0014229434337766393 | validation: -0.004511669988957025]
	TIME [epoch: 8.3 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026172323199409285		[learning rate: 0.00024477]
		[batch 20/20] avg loss: 0.005552829501869079		[learning rate: 0.00024433]
	Learning Rate: 0.000244328
	LOSS [training: 0.001467798590964076 | validation: -0.005631175480707871]
	TIME [epoch: 8.27 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011771128472484706		[learning rate: 0.00024388]
		[batch 20/20] avg loss: -0.0027615580792339613		[learning rate: 0.00024344]
	Learning Rate: 0.000243442
	LOSS [training: 0.004504785196625373 | validation: -0.007133348509301515]
	TIME [epoch: 8.28 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016739369036990741		[learning rate: 0.000243]
		[batch 20/20] avg loss: 0.004926162461836661		[learning rate: 0.00024256]
	Learning Rate: 0.000242558
	LOSS [training: 0.003300049682767868 | validation: -0.005460811365009598]
	TIME [epoch: 8.32 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008232140884985084		[learning rate: 0.00024212]
		[batch 20/20] avg loss: 0.002238130959158817		[learning rate: 0.00024168]
	Learning Rate: 0.000241678
	LOSS [training: 0.00523513592207195 | validation: -0.004868043883362966]
	TIME [epoch: 8.3 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027684526909165915		[learning rate: 0.00024124]
		[batch 20/20] avg loss: 0.0015867693849722561		[learning rate: 0.0002408]
	Learning Rate: 0.000240801
	LOSS [training: 0.002177611037944424 | validation: 0.0012024039944297856]
	TIME [epoch: 8.28 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011845489642001352		[learning rate: 0.00024036]
		[batch 20/20] avg loss: 0.002714860757140989		[learning rate: 0.00023993]
	Learning Rate: 0.000239927
	LOSS [training: 0.001949704860670562 | validation: 0.00961552455260417]
	TIME [epoch: 8.28 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011989063828956927		[learning rate: 0.00023949]
		[batch 20/20] avg loss: 0.00209055798866152		[learning rate: 0.00023906]
	Learning Rate: 0.000239056
	LOSS [training: 0.007039810908809223 | validation: -0.003349851432198322]
	TIME [epoch: 8.32 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.986279405999877e-05		[learning rate: 0.00023862]
		[batch 20/20] avg loss: -0.0005495051528829687		[learning rate: 0.00023819]
	Learning Rate: 0.000238189
	LOSS [training: -0.0002448211794114851 | validation: -0.012477526152424477]
	TIME [epoch: 8.29 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00668118343706677		[learning rate: 0.00023776]
		[batch 20/20] avg loss: 0.007994133806780605		[learning rate: 0.00023732]
	Learning Rate: 0.000237324
	LOSS [training: 0.007337658621923687 | validation: 0.02624394210194526]
	TIME [epoch: 8.28 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008076352296406486		[learning rate: 0.00023689]
		[batch 20/20] avg loss: 0.0018507261034049903		[learning rate: 0.00023646]
	Learning Rate: 0.000236463
	LOSS [training: 0.004963539199905739 | validation: -0.0037905420740455267]
	TIME [epoch: 8.28 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034707769259830464		[learning rate: 0.00023603]
		[batch 20/20] avg loss: -0.0020872134726337066		[learning rate: 0.0002356]
	Learning Rate: 0.000235605
	LOSS [training: -0.0027789951993083765 | validation: -0.006283150890694032]
	TIME [epoch: 8.31 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0061655552573424525		[learning rate: 0.00023518]
		[batch 20/20] avg loss: -0.00030205348049200647		[learning rate: 0.00023475]
	Learning Rate: 0.00023475
	LOSS [training: 0.0029317508884252226 | validation: 0.017782253634595027]
	TIME [epoch: 8.27 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016831944821296664		[learning rate: 0.00023432]
		[batch 20/20] avg loss: 0.005773537450117383		[learning rate: 0.0002339]
	Learning Rate: 0.000233898
	LOSS [training: 0.011302741135707024 | validation: -0.0051255116927401865]
	TIME [epoch: 8.3 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016412845889631448		[learning rate: 0.00023347]
		[batch 20/20] avg loss: 0.003275615786489469		[learning rate: 0.00023305]
	Learning Rate: 0.000233049
	LOSS [training: 0.0008171655987631624 | validation: 0.0017772785436756128]
	TIME [epoch: 8.29 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035486047724695956		[learning rate: 0.00023263]
		[batch 20/20] avg loss: 0.0027839124890446784		[learning rate: 0.0002322]
	Learning Rate: 0.000232203
	LOSS [training: 0.0031662586307571365 | validation: -0.014109707287175637]
	TIME [epoch: 8.3 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005196317348743914		[learning rate: 0.00023178]
		[batch 20/20] avg loss: 0.0026898241888118173		[learning rate: 0.00023136]
	Learning Rate: 0.000231361
	LOSS [training: 0.001085096226968713 | validation: -0.007415859370041869]
	TIME [epoch: 8.27 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004241074391261158		[learning rate: 0.00023094]
		[batch 20/20] avg loss: -0.0005251929132516549		[learning rate: 0.00023052]
	Learning Rate: 0.000230521
	LOSS [training: 0.0018579407390047517 | validation: 0.01409101320963333]
	TIME [epoch: 8.3 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013477694023456674		[learning rate: 0.0002301]
		[batch 20/20] avg loss: -0.0020063946659727543		[learning rate: 0.00022968]
	Learning Rate: 0.000229685
	LOSS [training: 0.005735649678741961 | validation: 0.00728803051096276]
	TIME [epoch: 8.3 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00547795477106876		[learning rate: 0.00022927]
		[batch 20/20] avg loss: 0.007715800549468146		[learning rate: 0.00022885]
	Learning Rate: 0.000228851
	LOSS [training: 0.006596877660268454 | validation: 0.00865838544042185]
	TIME [epoch: 8.3 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00874247021426885		[learning rate: 0.00022844]
		[batch 20/20] avg loss: -0.005623819735118831		[learning rate: 0.00022802]
	Learning Rate: 0.00022802
	LOSS [training: 0.001559325239575009 | validation: -0.00422526296784337]
	TIME [epoch: 8.27 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005737927762287883		[learning rate: 0.00022761]
		[batch 20/20] avg loss: -5.595433489265049e-05		[learning rate: 0.00022719]
	Learning Rate: 0.000227193
	LOSS [training: 0.00025891922066806887 | validation: -0.0005095589725693584]
	TIME [epoch: 8.29 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011971895056793243		[learning rate: 0.00022678]
		[batch 20/20] avg loss: 0.04259113284736508		[learning rate: 0.00022637]
	Learning Rate: 0.000226368
	LOSS [training: 0.021894161176522202 | validation: 0.00723809617567522]
	TIME [epoch: 8.31 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014562678548436991		[learning rate: 0.00022596]
		[batch 20/20] avg loss: 0.02188356944558434		[learning rate: 0.00022555]
	Learning Rate: 0.000225547
	LOSS [training: 0.018223123997010666 | validation: 0.0068230244554170275]
	TIME [epoch: 8.3 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008070796680867218		[learning rate: 0.00022514]
		[batch 20/20] avg loss: -0.004166691337933662		[learning rate: 0.00022473]
	Learning Rate: 0.000224728
	LOSS [training: 0.0019520526714667784 | validation: -0.009694945011539682]
	TIME [epoch: 8.27 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021287100003622335		[learning rate: 0.00022432]
		[batch 20/20] avg loss: -0.005964062242447676		[learning rate: 0.00022391]
	Learning Rate: 0.000223913
	LOSS [training: -0.0040463861214049554 | validation: -0.0008754832597068982]
	TIME [epoch: 8.27 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028501705293781333		[learning rate: 0.00022351]
		[batch 20/20] avg loss: -7.111148011449859e-05		[learning rate: 0.0002231]
	Learning Rate: 0.0002231
	LOSS [training: -0.0014606410047463161 | validation: 0.009049924695218602]
	TIME [epoch: 8.33 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017598628616546715		[learning rate: 0.0002227]
		[batch 20/20] avg loss: 0.015338288938176268		[learning rate: 0.00022229]
	Learning Rate: 0.000222291
	LOSS [training: 0.016468458777361493 | validation: -0.0006990196381041298]
	TIME [epoch: 8.29 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00046839117672136255		[learning rate: 0.00022189]
		[batch 20/20] avg loss: 0.009503648523911432		[learning rate: 0.00022148]
	Learning Rate: 0.000221484
	LOSS [training: 0.004517628673595035 | validation: 0.01845187058554283]
	TIME [epoch: 8.27 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002745963827342368		[learning rate: 0.00022108]
		[batch 20/20] avg loss: 0.006184705410646125		[learning rate: 0.00022068]
	Learning Rate: 0.00022068
	LOSS [training: 0.004465334618994246 | validation: 0.013176294622633072]
	TIME [epoch: 8.27 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021599624594844456		[learning rate: 0.00022028]
		[batch 20/20] avg loss: -2.927876271365316e-05		[learning rate: 0.00021988]
	Learning Rate: 0.000219879
	LOSS [training: 0.0107851729160654 | validation: -0.00835509590226653]
	TIME [epoch: 8.33 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014823314133317104		[learning rate: 0.00021948]
		[batch 20/20] avg loss: 5.3279638980359946e-05		[learning rate: 0.00021908]
	Learning Rate: 0.000219081
	LOSS [training: -0.0007145258871756751 | validation: 0.003217233246401464]
	TIME [epoch: 8.28 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005512814277525038		[learning rate: 0.00021868]
		[batch 20/20] avg loss: 0.00046850339057363		[learning rate: 0.00021829]
	Learning Rate: 0.000218286
	LOSS [training: 0.0029906588340493333 | validation: -0.012151920753163218]
	TIME [epoch: 8.27 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005941623567090985		[learning rate: 0.00021789]
		[batch 20/20] avg loss: 0.0004828631965856327		[learning rate: 0.00021749]
	Learning Rate: 0.000217494
	LOSS [training: 0.0005385127766473656 | validation: -0.009044334629805124]
	TIME [epoch: 8.27 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003096235330621745		[learning rate: 0.0002171]
		[batch 20/20] avg loss: 0.003964834456805975		[learning rate: 0.0002167]
	Learning Rate: 0.000216705
	LOSS [training: 0.00043429956309211577 | validation: -0.012241659428312585]
	TIME [epoch: 8.33 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005205589196658066		[learning rate: 0.00021631]
		[batch 20/20] avg loss: 0.004613112883031715		[learning rate: 0.00021592]
	Learning Rate: 0.000215918
	LOSS [training: 0.004909351039844891 | validation: -7.568228253097744e-06]
	TIME [epoch: 8.29 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006501827813780808		[learning rate: 0.00021553]
		[batch 20/20] avg loss: -0.00489251533771057		[learning rate: 0.00021513]
	Learning Rate: 0.000215135
	LOSS [training: -0.00569717157574569 | validation: -0.005361506933778528]
	TIME [epoch: 8.27 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006514517849391293		[learning rate: 0.00021474]
		[batch 20/20] avg loss: 0.00355280352071169		[learning rate: 0.00021435]
	Learning Rate: 0.000214354
	LOSS [training: -0.0014808571643398012 | validation: -0.009628722129539628]
	TIME [epoch: 8.27 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005937942364231563		[learning rate: 0.00021396]
		[batch 20/20] avg loss: 0.0006634270387570283		[learning rate: 0.00021358]
	Learning Rate: 0.000213576
	LOSS [training: 3.4816401166936056e-05 | validation: -0.014099761559481425]
	TIME [epoch: 8.31 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012333325698834334		[learning rate: 0.00021319]
		[batch 20/20] avg loss: -0.0021534038503936742		[learning rate: 0.0002128]
	Learning Rate: 0.000212801
	LOSS [training: -0.0016933682101385535 | validation: -0.0025008431326944943]
	TIME [epoch: 8.3 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006982613765897803		[learning rate: 0.00021241]
		[batch 20/20] avg loss: -0.004608922591078021		[learning rate: 0.00021203]
	Learning Rate: 0.000212029
	LOSS [training: -0.002653591983833901 | validation: -0.004207342845890901]
	TIME [epoch: 8.27 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002111339059847518		[learning rate: 0.00021164]
		[batch 20/20] avg loss: 0.0007622031837700822		[learning rate: 0.00021126]
	Learning Rate: 0.000211259
	LOSS [training: 0.0014367711218087997 | validation: -0.006590379226756242]
	TIME [epoch: 8.27 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002629485494257817		[learning rate: 0.00021088]
		[batch 20/20] avg loss: 0.016733080972005685		[learning rate: 0.00021049]
	Learning Rate: 0.000210493
	LOSS [training: 0.009681283233131752 | validation: 0.006430269755919344]
	TIME [epoch: 8.32 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017505958175635775		[learning rate: 0.00021011]
		[batch 20/20] avg loss: 0.007269373837971218		[learning rate: 0.00020973]
	Learning Rate: 0.000209729
	LOSS [training: 0.012387666006803497 | validation: -0.008531048238936844]
	TIME [epoch: 8.29 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014618598073166613		[learning rate: 0.00020935]
		[batch 20/20] avg loss: 0.00983615610347398		[learning rate: 0.00020897]
	Learning Rate: 0.000208968
	LOSS [training: 0.012227377088320297 | validation: -0.0038150384514196714]
	TIME [epoch: 8.27 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008164589939677759		[learning rate: 0.00020859]
		[batch 20/20] avg loss: 0.01631367288894499		[learning rate: 0.00020821]
	Learning Rate: 0.000208209
	LOSS [training: 0.007748606947488607 | validation: 0.0034818680526034996]
	TIME [epoch: 8.29 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004767222075772461		[learning rate: 0.00020783]
		[batch 20/20] avg loss: 0.013931960298942619		[learning rate: 0.00020745]
	Learning Rate: 0.000207454
	LOSS [training: 0.00934959118735754 | validation: 0.008687174788266656]
	TIME [epoch: 8.29 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005344573219626339		[learning rate: 0.00020708]
		[batch 20/20] avg loss: 0.0077821461147892425		[learning rate: 0.0002067]
	Learning Rate: 0.000206701
	LOSS [training: 0.006563359667207791 | validation: 0.005417449542199049]
	TIME [epoch: 8.28 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007831425881124152		[learning rate: 0.00020633]
		[batch 20/20] avg loss: 0.006486375139164409		[learning rate: 0.00020595]
	Learning Rate: 0.000205951
	LOSS [training: 0.007158900510144282 | validation: -0.006549892120563378]
	TIME [epoch: 8.29 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005757474491808458		[learning rate: 0.00020558]
		[batch 20/20] avg loss: 0.01449330478912863		[learning rate: 0.0002052]
	Learning Rate: 0.000205203
	LOSS [training: 0.010125389640468541 | validation: 0.018353576520245975]
	TIME [epoch: 8.31 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023621709929950722		[learning rate: 0.00020483]
		[batch 20/20] avg loss: 0.00280462990690572		[learning rate: 0.00020446]
	Learning Rate: 0.000204459
	LOSS [training: 0.01321316991842822 | validation: -0.007943082460478559]
	TIME [epoch: 8.29 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00705830870979052		[learning rate: 0.00020409]
		[batch 20/20] avg loss: 0.021929883539435705		[learning rate: 0.00020372]
	Learning Rate: 0.000203717
	LOSS [training: 0.014494096124613113 | validation: 0.009296298926034388]
	TIME [epoch: 8.27 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007597408015718531		[learning rate: 0.00020335]
		[batch 20/20] avg loss: 0.013978862331609382		[learning rate: 0.00020298]
	Learning Rate: 0.000202977
	LOSS [training: 0.010788135173663956 | validation: -0.008025185861218356]
	TIME [epoch: 8.29 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003750097180595261		[learning rate: 0.00020261]
		[batch 20/20] avg loss: 0.00345810679809498		[learning rate: 0.00020224]
	Learning Rate: 0.000202241
	LOSS [training: 0.0015415485400177272 | validation: -0.004316480616214236]
	TIME [epoch: 8.31 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004911557215312		[learning rate: 0.00020187]
		[batch 20/20] avg loss: -0.0006836345060294273		[learning rate: 0.00020151]
	Learning Rate: 0.000201507
	LOSS [training: -0.002797595860670714 | validation: 0.002608980838265616]
	TIME [epoch: 8.29 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00605166197927096		[learning rate: 0.00020114]
		[batch 20/20] avg loss: 0.011117851666954389		[learning rate: 0.00020078]
	Learning Rate: 0.000200775
	LOSS [training: 0.002533094843841715 | validation: 0.0023195938606868473]
	TIME [epoch: 8.26 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003892689043677045		[learning rate: 0.00020041]
		[batch 20/20] avg loss: -0.0026137870414715747		[learning rate: 0.00020005]
	Learning Rate: 0.000200047
	LOSS [training: 0.0006394510011027355 | validation: -0.0074941617768103565]
	TIME [epoch: 8.29 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002436287754361239		[learning rate: 0.00019968]
		[batch 20/20] avg loss: 0.0038004549487319016		[learning rate: 0.00019932]
	Learning Rate: 0.000199321
	LOSS [training: 0.0006820835971853313 | validation: -0.0021047445051397434]
	TIME [epoch: 8.32 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007865515535581537		[learning rate: 0.00019896]
		[batch 20/20] avg loss: 0.007468489796639784		[learning rate: 0.0001986]
	Learning Rate: 0.000198597
	LOSS [training: -0.00019851286947087662 | validation: -0.008316037871932426]
	TIME [epoch: 8.28 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005811321619005815		[learning rate: 0.00019824]
		[batch 20/20] avg loss: 0.0009622724321181041		[learning rate: 0.00019788]
	Learning Rate: 0.000197877
	LOSS [training: -0.002424524593443855 | validation: 0.010224346551549475]
	TIME [epoch: 8.27 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002740845130272514		[learning rate: 0.00019752]
		[batch 20/20] avg loss: 4.8960225822212674e-05		[learning rate: 0.00019716]
	Learning Rate: 0.000197159
	LOSS [training: 0.0013949026780473634 | validation: -0.008217794419503929]
	TIME [epoch: 8.28 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034256359206172315		[learning rate: 0.0001968]
		[batch 20/20] avg loss: -7.107682737982484e-05		[learning rate: 0.00019644]
	Learning Rate: 0.000196443
	LOSS [training: -0.0017483563739985283 | validation: -0.006396688196769806]
	TIME [epoch: 8.33 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037507871223981548		[learning rate: 0.00019609]
		[batch 20/20] avg loss: -0.004489854807144916		[learning rate: 0.00019573]
	Learning Rate: 0.00019573
	LOSS [training: -0.004120320964771535 | validation: -0.010351183024524781]
	TIME [epoch: 8.28 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002107339453466771		[learning rate: 0.00019537]
		[batch 20/20] avg loss: 0.004205198482868763		[learning rate: 0.00019502]
	Learning Rate: 0.00019502
	LOSS [training: 0.003156268968167766 | validation: -0.007199409766591943]
	TIME [epoch: 8.27 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0004187430872348149		[learning rate: 0.00019467]
		[batch 20/20] avg loss: 0.002873021335575778		[learning rate: 0.00019431]
	Learning Rate: 0.000194312
	LOSS [training: 0.0012271391241704813 | validation: -0.007894495213667755]
	TIME [epoch: 8.27 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0003989828523700731		[learning rate: 0.00019396]
		[batch 20/20] avg loss: 0.0042448675638486625		[learning rate: 0.00019361]
	Learning Rate: 0.000193607
	LOSS [training: 0.001922942355739295 | validation: -0.0030709790097991213]
	TIME [epoch: 8.34 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00048525161313214794		[learning rate: 0.00019326]
		[batch 20/20] avg loss: 0.005859665513413728		[learning rate: 0.0001929]
	Learning Rate: 0.000192904
	LOSS [training: 0.00268720695014079 | validation: -0.0050864987837597395]
	TIME [epoch: 8.27 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00150799466502238		[learning rate: 0.00019255]
		[batch 20/20] avg loss: -0.00034642851620267297		[learning rate: 0.0001922]
	Learning Rate: 0.000192204
	LOSS [training: 0.0005807830744098533 | validation: -0.005889785765672285]
	TIME [epoch: 8.27 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0063306360827542095		[learning rate: 0.00019186]
		[batch 20/20] avg loss: 0.004704045013301636		[learning rate: 0.00019151]
	Learning Rate: 0.000191507
	LOSS [training: 0.0055173405480279224 | validation: -0.007879737000481214]
	TIME [epoch: 8.27 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022704514613748525		[learning rate: 0.00019116]
		[batch 20/20] avg loss: 0.005435075838855908		[learning rate: 0.00019081]
	Learning Rate: 0.000190812
	LOSS [training: 0.0038527636501153803 | validation: -0.0064240636775763395]
	TIME [epoch: 8.33 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025265143787582115		[learning rate: 0.00019047]
		[batch 20/20] avg loss: 0.0012435253308570192		[learning rate: 0.00019012]
	Learning Rate: 0.000190119
	LOSS [training: -0.0006414945239505961 | validation: -0.01131443615268337]
	TIME [epoch: 8.28 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0053605736574538895		[learning rate: 0.00018977]
		[batch 20/20] avg loss: 0.006253112087315089		[learning rate: 0.00018943]
	Learning Rate: 0.000189429
	LOSS [training: 0.005806842872384489 | validation: -0.009800670682848116]
	TIME [epoch: 8.27 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006554746500008157		[learning rate: 0.00018909]
		[batch 20/20] avg loss: 0.008808199263966532		[learning rate: 0.00018874]
	Learning Rate: 0.000188742
	LOSS [training: 0.004731836956983673 | validation: 0.006086394413469982]
	TIME [epoch: 8.29 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003494063293885883		[learning rate: 0.0001884]
		[batch 20/20] avg loss: 0.005488586206267252		[learning rate: 0.00018806]
	Learning Rate: 0.000188057
	LOSS [training: 0.004491324750076567 | validation: -0.00801485901211064]
	TIME [epoch: 8.3 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00021184646614101944		[learning rate: 0.00018772]
		[batch 20/20] avg loss: 0.0068540957489501095		[learning rate: 0.00018737]
	Learning Rate: 0.000187375
	LOSS [training: 0.003321124641404545 | validation: -0.008223573232102202]
	TIME [epoch: 8.3 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004189189546115259		[learning rate: 0.00018703]
		[batch 20/20] avg loss: -0.002660024979343792		[learning rate: 0.00018669]
	Learning Rate: 0.000186695
	LOSS [training: -0.0034246072627295253 | validation: -0.0010484011368238457]
	TIME [epoch: 8.27 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004855879619619918		[learning rate: 0.00018636]
		[batch 20/20] avg loss: -0.006889787029685432		[learning rate: 0.00018602]
	Learning Rate: 0.000186017
	LOSS [training: -0.0058728333246526754 | validation: -0.0070568669748182905]
	TIME [epoch: 8.29 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003830387994394213		[learning rate: 0.00018568]
		[batch 20/20] avg loss: 0.004856943388169497		[learning rate: 0.00018534]
	Learning Rate: 0.000185342
	LOSS [training: 0.0005132776968876423 | validation: 0.000725986200419398]
	TIME [epoch: 8.29 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018359737018591536		[learning rate: 0.00018501]
		[batch 20/20] avg loss: -0.002462767382613599		[learning rate: 0.00018467]
	Learning Rate: 0.000184669
	LOSS [training: -0.0021493705422363763 | validation: -0.005463549086539982]
	TIME [epoch: 8.3 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003779484009172475		[learning rate: 0.00018433]
		[batch 20/20] avg loss: -0.00500543037938324		[learning rate: 0.000184]
	Learning Rate: 0.000183999
	LOSS [training: -0.0043924571942778575 | validation: -0.0013323180632169376]
	TIME [epoch: 8.27 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002827616409209654		[learning rate: 0.00018367]
		[batch 20/20] avg loss: 0.0034597204973879784		[learning rate: 0.00018333]
	Learning Rate: 0.000183331
	LOSS [training: 0.0031436684532988165 | validation: -0.011517899247793307]
	TIME [epoch: 8.29 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003788445005289444		[learning rate: 0.000183]
		[batch 20/20] avg loss: -0.0013819700699910335		[learning rate: 0.00018267]
	Learning Rate: 0.000182666
	LOSS [training: -0.002585207537640239 | validation: -0.0073659442669136155]
	TIME [epoch: 8.31 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007044288792172314		[learning rate: 0.00018233]
		[batch 20/20] avg loss: 4.545880514147654e-05		[learning rate: 0.000182]
	Learning Rate: 0.000182003
	LOSS [training: -0.0034994149935154185 | validation: -0.0036829019588978983]
	TIME [epoch: 8.29 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007181957904014822		[learning rate: 0.00018167]
		[batch 20/20] avg loss: -0.003191778039936853		[learning rate: 0.00018134]
	Learning Rate: 0.000181343
	LOSS [training: -0.005186867971975837 | validation: -0.006376864729400757]
	TIME [epoch: 8.28 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004197642635035994		[learning rate: 0.00018101]
		[batch 20/20] avg loss: -0.0026872868410776087		[learning rate: 0.00018068]
	Learning Rate: 0.000180685
	LOSS [training: -0.003442464738056801 | validation: -0.0013896871975930662]
	TIME [epoch: 8.3 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009067137521288727		[learning rate: 0.00018036]
		[batch 20/20] avg loss: 0.004002581467661801		[learning rate: 0.00018003]
	Learning Rate: 0.000180029
	LOSS [training: 0.002454647609895337 | validation: -0.0032431107469983365]
	TIME [epoch: 8.28 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011680866185085086		[learning rate: 0.0001797]
		[batch 20/20] avg loss: 0.0030503247630835964		[learning rate: 0.00017938]
	Learning Rate: 0.000179376
	LOSS [training: 0.0009411190722875444 | validation: -0.0022905792369572785]
	TIME [epoch: 8.27 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008737214609246798		[learning rate: 0.00017905]
		[batch 20/20] avg loss: 0.006136315087785682		[learning rate: 0.00017872]
	Learning Rate: 0.000178725
	LOSS [training: 0.0035050182743551804 | validation: -0.007924704685286476]
	TIME [epoch: 8.29 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001176432118485244		[learning rate: 0.0001784]
		[batch 20/20] avg loss: 0.003621958848023971		[learning rate: 0.00017808]
	Learning Rate: 0.000178076
	LOSS [training: 0.0012227633647693636 | validation: -0.0036562238137688783]
	TIME [epoch: 8.3 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005599954310309401		[learning rate: 0.00017775]
		[batch 20/20] avg loss: -0.004094159351673189		[learning rate: 0.00017743]
	Learning Rate: 0.00017743
	LOSS [training: -0.0017670819603211238 | validation: 0.0007361516439172208]
	TIME [epoch: 8.28 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00058245625692688		[learning rate: 0.00017711]
		[batch 20/20] avg loss: 0.007075430807793319		[learning rate: 0.00017679]
	Learning Rate: 0.000176786
	LOSS [training: 0.003828943532360099 | validation: 0.0020532418643966098]
	TIME [epoch: 8.27 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011516266171480525		[learning rate: 0.00017646]
		[batch 20/20] avg loss: 0.0007485835789813819		[learning rate: 0.00017614]
	Learning Rate: 0.000176144
	LOSS [training: 0.000950105098064717 | validation: -0.0015489276746190545]
	TIME [epoch: 8.29 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027712451798480035		[learning rate: 0.00017582]
		[batch 20/20] avg loss: -0.00411439470272683		[learning rate: 0.0001755]
	Learning Rate: 0.000175505
	LOSS [training: -0.0034428199412874167 | validation: -0.004445453106960587]
	TIME [epoch: 8.31 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010498853051590699		[learning rate: 0.00017519]
		[batch 20/20] avg loss: -0.003408145562109345		[learning rate: 0.00017487]
	Learning Rate: 0.000174868
	LOSS [training: -0.002229015433634207 | validation: 0.0011999461446498276]
	TIME [epoch: 8.28 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036949662360713947		[learning rate: 0.00017455]
		[batch 20/20] avg loss: 0.008485417457606457		[learning rate: 0.00017423]
	Learning Rate: 0.000174233
	LOSS [training: 0.006090191846838925 | validation: -0.0070069482672248835]
	TIME [epoch: 8.27 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01260010235552747		[learning rate: 0.00017392]
		[batch 20/20] avg loss: 0.0005315383751655502		[learning rate: 0.0001736]
	Learning Rate: 0.000173601
	LOSS [training: 0.006565820365346512 | validation: -0.0026819570262617046]
	TIME [epoch: 8.28 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014083223924112134		[learning rate: 0.00017329]
		[batch 20/20] avg loss: 0.00655659455082844		[learning rate: 0.00017297]
	Learning Rate: 0.000172971
	LOSS [training: 0.003982458471619827 | validation: -0.0050013689151861005]
	TIME [epoch: 8.33 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033162124769862953		[learning rate: 0.00017266]
		[batch 20/20] avg loss: -0.0007087098938698387		[learning rate: 0.00017234]
	Learning Rate: 0.000172343
	LOSS [training: 0.0013037512915582282 | validation: -0.009642042975696881]
	TIME [epoch: 8.27 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029577799154432423		[learning rate: 0.00017203]
		[batch 20/20] avg loss: 0.008689411722604982		[learning rate: 0.00017172]
	Learning Rate: 0.000171718
	LOSS [training: 0.0028658159035808697 | validation: -0.00704696026017348]
	TIME [epoch: 8.27 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00040669836128344634		[learning rate: 0.00017141]
		[batch 20/20] avg loss: 0.003434519440746818		[learning rate: 0.00017109]
	Learning Rate: 0.000171095
	LOSS [training: 0.001920608901015132 | validation: -0.002542499566563494]
	TIME [epoch: 8.27 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036998113338350576		[learning rate: 0.00017078]
		[batch 20/20] avg loss: 0.0017139466132070759		[learning rate: 0.00017047]
	Learning Rate: 0.000170474
	LOSS [training: 0.0027068789735210665 | validation: -0.007274730525952764]
	TIME [epoch: 8.34 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003945958593628885		[learning rate: 0.00017016]
		[batch 20/20] avg loss: -0.0001380152319324207		[learning rate: 0.00016986]
	Learning Rate: 0.000169855
	LOSS [training: -0.0020419869127806523 | validation: 0.004185880077918207]
	TIME [epoch: 8.27 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0054813242424712905		[learning rate: 0.00016955]
		[batch 20/20] avg loss: -0.003716885150781615		[learning rate: 0.00016924]
	Learning Rate: 0.000169239
	LOSS [training: -0.004599104696626452 | validation: 0.0015408700915595618]
	TIME [epoch: 8.26 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000674899053302596		[learning rate: 0.00016893]
		[batch 20/20] avg loss: -0.0037477254333876803		[learning rate: 0.00016862]
	Learning Rate: 0.000168625
	LOSS [training: -0.0015364131900425426 | validation: -0.0044346240480384195]
	TIME [epoch: 8.28 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00443494389629203		[learning rate: 0.00016832]
		[batch 20/20] avg loss: -0.0004076694220985609		[learning rate: 0.00016801]
	Learning Rate: 0.000168013
	LOSS [training: -0.002421306659195296 | validation: -0.0037742889543269224]
	TIME [epoch: 8.32 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0001255030434560704		[learning rate: 0.00016771]
		[batch 20/20] avg loss: 0.0004757104083130025		[learning rate: 0.0001674]
	Learning Rate: 0.000167403
	LOSS [training: 0.0003006067258845365 | validation: -0.0072884208035005]
	TIME [epoch: 8.28 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015643389921396842		[learning rate: 0.0001671]
		[batch 20/20] avg loss: -0.005611908190860739		[learning rate: 0.0001668]
	Learning Rate: 0.000166795
	LOSS [training: -0.003588123591500212 | validation: -0.004756734002022983]
	TIME [epoch: 8.26 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.468945702096046e-05		[learning rate: 0.00016649]
		[batch 20/20] avg loss: -0.008224219466983386		[learning rate: 0.00016619]
	Learning Rate: 0.00016619
	LOSS [training: -0.004094765004981213 | validation: -0.006060225054869349]
	TIME [epoch: 8.29 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007287185056381251		[learning rate: 0.00016589]
		[batch 20/20] avg loss: -0.004722318864298252		[learning rate: 0.00016559]
	Learning Rate: 0.000165587
	LOSS [training: -0.006004751960339752 | validation: -0.0038684577238516215]
	TIME [epoch: 8.29 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0053878798327667825		[learning rate: 0.00016529]
		[batch 20/20] avg loss: -0.00527479543684844		[learning rate: 0.00016499]
	Learning Rate: 0.000164986
	LOSS [training: -0.005331337634807612 | validation: -0.004763990211109435]
	TIME [epoch: 8.3 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006465146934511452		[learning rate: 0.00016469]
		[batch 20/20] avg loss: -0.0007393389633388184		[learning rate: 0.00016439]
	Learning Rate: 0.000164387
	LOSS [training: -0.003602242948925134 | validation: -0.006179724108118579]
	TIME [epoch: 8.27 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003727190586453774		[learning rate: 0.00016409]
		[batch 20/20] avg loss: -0.011421436818408608		[learning rate: 0.00016379]
	Learning Rate: 0.000163791
	LOSS [training: -0.007574313702431191 | validation: -0.009066981128488041]
	TIME [epoch: 8.28 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026027889445723366		[learning rate: 0.00016349]
		[batch 20/20] avg loss: 0.0016166230005334836		[learning rate: 0.0001632]
	Learning Rate: 0.000163196
	LOSS [training: -0.0004930829720194263 | validation: -0.008839495058478985]
	TIME [epoch: 8.3 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007742000783392311		[learning rate: 0.0001629]
		[batch 20/20] avg loss: 0.0006992501203958413		[learning rate: 0.0001626]
	Learning Rate: 0.000162604
	LOSS [training: 0.0042206254518940755 | validation: -0.0019125995901925312]
	TIME [epoch: 8.29 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009139553628770327		[learning rate: 0.00016231]
		[batch 20/20] avg loss: 0.0022370647783026437		[learning rate: 0.00016201]
	Learning Rate: 0.000162014
	LOSS [training: 0.0015755100705898387 | validation: -0.010870776027740744]
	TIME [epoch: 8.27 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023515775769666104		[learning rate: 0.00016172]
		[batch 20/20] avg loss: -0.002412223297224568		[learning rate: 0.00016143]
	Learning Rate: 0.000161426
	LOSS [training: -0.002381900437095589 | validation: -0.010705496049535398]
	TIME [epoch: 8.29 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038736480009487163		[learning rate: 0.00016113]
		[batch 20/20] avg loss: 0.007418204327130143		[learning rate: 0.00016084]
	Learning Rate: 0.00016084
	LOSS [training: 0.0017722781630907132 | validation: 0.001523886785467098]
	TIME [epoch: 8.29 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004939077137339217		[learning rate: 0.00016055]
		[batch 20/20] avg loss: 0.002068327746972098		[learning rate: 0.00016026]
	Learning Rate: 0.000160257
	LOSS [training: 0.0012811177303530099 | validation: -0.009781973772104987]
	TIME [epoch: 8.28 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006163622914851052		[learning rate: 0.00015997]
		[batch 20/20] avg loss: 0.011821928477689852		[learning rate: 0.00015967]
	Learning Rate: 0.000159675
	LOSS [training: 0.00899277569627045 | validation: -0.0022424756900399894]
	TIME [epoch: 8.27 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002014598508988881		[learning rate: 0.00015938]
		[batch 20/20] avg loss: 0.004967579231465654		[learning rate: 0.0001591]
	Learning Rate: 0.000159096
	LOSS [training: 0.0034910888702272664 | validation: -0.00524696299168545]
	TIME [epoch: 8.28 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018480847975767066		[learning rate: 0.00015881]
		[batch 20/20] avg loss: 0.004283314034815009		[learning rate: 0.00015852]
	Learning Rate: 0.000158518
	LOSS [training: 0.0012176146186191515 | validation: -0.0030038196554260164]
	TIME [epoch: 8.28 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008197463908689178		[learning rate: 0.00015823]
		[batch 20/20] avg loss: -0.00024776876710423527		[learning rate: 0.00015794]
	Learning Rate: 0.000157943
	LOSS [training: -0.004222616337896707 | validation: -0.00758205722974813]
	TIME [epoch: 8.26 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0051642195970375004		[learning rate: 0.00015766]
		[batch 20/20] avg loss: -0.0019647836839334666		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: -0.0035645016404854844 | validation: -0.0012338801990532107]
	TIME [epoch: 8.29 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002230593215983753		[learning rate: 0.00015708]
		[batch 20/20] avg loss: -0.004441904897887976		[learning rate: 0.0001568]
	Learning Rate: 0.000156799
	LOSS [training: -0.002332482109743176 | validation: -0.013392243157002976]
	TIME [epoch: 8.3 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003202891141313274		[learning rate: 0.00015651]
		[batch 20/20] avg loss: -0.0030621732395406244		[learning rate: 0.00015623]
	Learning Rate: 0.00015623
	LOSS [training: -0.0031325321904269498 | validation: -0.009471375889006393]
	TIME [epoch: 8.28 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011697787617387851		[learning rate: 0.00015595]
		[batch 20/20] avg loss: -0.0005338239619175887		[learning rate: 0.00015566]
	Learning Rate: 0.000155663
	LOSS [training: -0.0008518013618281871 | validation: -0.005164681496295084]
	TIME [epoch: 8.26 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0059612411039888045		[learning rate: 0.00015538]
		[batch 20/20] avg loss: -0.0007514594211489334		[learning rate: 0.0001551]
	Learning Rate: 0.000155098
	LOSS [training: 0.0026048908414199354 | validation: -0.007161002350051923]
	TIME [epoch: 8.29 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007750958485550384		[learning rate: 0.00015482]
		[batch 20/20] avg loss: -0.0012522370819095722		[learning rate: 0.00015453]
	Learning Rate: 0.000154535
	LOSS [training: -0.0010136664652323052 | validation: -0.00709881895149894]
	TIME [epoch: 8.31 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006289245258314853		[learning rate: 0.00015425]
		[batch 20/20] avg loss: -0.004527086120037984		[learning rate: 0.00015397]
	Learning Rate: 0.000153974
	LOSS [training: -0.005408165689176419 | validation: -0.009827535177042924]
	TIME [epoch: 8.27 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000364616009062272		[learning rate: 0.00015369]
		[batch 20/20] avg loss: 0.0010546386860977581		[learning rate: 0.00015342]
	Learning Rate: 0.000153415
	LOSS [training: 0.000345011338517743 | validation: -0.0038196558124381002]
	TIME [epoch: 8.26 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008042026948278677		[learning rate: 0.00015314]
		[batch 20/20] avg loss: 0.001224455644902059		[learning rate: 0.00015286]
	Learning Rate: 0.000152858
	LOSS [training: -0.0034087856516883085 | validation: -0.0016806265046953404]
	TIME [epoch: 8.27 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001777389053120499		[learning rate: 0.00015258]
		[batch 20/20] avg loss: -0.0013228315396181646		[learning rate: 0.0001523]
	Learning Rate: 0.000152304
	LOSS [training: 0.00022727875675116724 | validation: -0.006789646861746522]
	TIME [epoch: 8.32 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.455024134562877e-05		[learning rate: 0.00015203]
		[batch 20/20] avg loss: -0.0010125218640771425		[learning rate: 0.00015175]
	Learning Rate: 0.000151751
	LOSS [training: -0.00045898581136575667 | validation: -0.00539675478953241]
	TIME [epoch: 8.26 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005207616408425302		[learning rate: 0.00015148]
		[batch 20/20] avg loss: -0.0013716335119887224		[learning rate: 0.0001512]
	Learning Rate: 0.0001512
	LOSS [training: -0.0009461975764156264 | validation: 0.0035831857490423676]
	TIME [epoch: 8.26 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007097606903379075		[learning rate: 0.00015093]
		[batch 20/20] avg loss: -0.006481889735136061		[learning rate: 0.00015065]
	Learning Rate: 0.000150652
	LOSS [training: -0.0035958252127369847 | validation: -0.008896545627419173]
	TIME [epoch: 8.28 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036855762288109917		[learning rate: 0.00015038]
		[batch 20/20] avg loss: 0.003861379652133823		[learning rate: 0.0001501]
	Learning Rate: 0.000150105
	LOSS [training: 0.0037734779404724078 | validation: -0.006489440417756242]
	TIME [epoch: 8.31 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0045791918423605665		[learning rate: 0.00014983]
		[batch 20/20] avg loss: -0.005652894387455466		[learning rate: 0.00014956]
	Learning Rate: 0.00014956
	LOSS [training: -0.0005368512725474495 | validation: -0.009646873729982392]
	TIME [epoch: 8.27 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00028267513344833867		[learning rate: 0.00014929]
		[batch 20/20] avg loss: 0.0017874529090104898		[learning rate: 0.00014902]
	Learning Rate: 0.000149017
	LOSS [training: 0.0007523888877810755 | validation: 0.00033455078063433894]
	TIME [epoch: 8.26 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002022657678483318		[learning rate: 0.00014875]
		[batch 20/20] avg loss: -0.004453316299017818		[learning rate: 0.00014848]
	Learning Rate: 0.000148477
	LOSS [training: -0.003237986988750567 | validation: -0.0021294062701549313]
	TIME [epoch: 8.28 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001402386959537023		[learning rate: 0.00014821]
		[batch 20/20] avg loss: -0.003278816290793554		[learning rate: 0.00014794]
	Learning Rate: 0.000147938
	LOSS [training: -0.0023406016251652888 | validation: -0.005637425037339913]
	TIME [epoch: 8.31 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021455852293207687		[learning rate: 0.00014767]
		[batch 20/20] avg loss: 0.004854770081622536		[learning rate: 0.0001474]
	Learning Rate: 0.000147401
	LOSS [training: 0.001354592426150884 | validation: -0.004992130573786752]
	TIME [epoch: 8.28 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001977766370870877		[learning rate: 0.00014713]
		[batch 20/20] avg loss: -0.004759261024257076		[learning rate: 0.00014687]
	Learning Rate: 0.000146866
	LOSS [training: -0.0033685136975639767 | validation: -0.003720180993341527]
	TIME [epoch: 8.27 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021088583002993647		[learning rate: 0.0001466]
		[batch 20/20] avg loss: -0.003150280749360572		[learning rate: 0.00014633]
	Learning Rate: 0.000146333
	LOSS [training: -0.0026295695248299686 | validation: -0.006161746349148229]
	TIME [epoch: 8.29 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009605331420002143		[learning rate: 0.00014607]
		[batch 20/20] avg loss: -0.0010342406663433447		[learning rate: 0.0001458]
	Learning Rate: 0.000145802
	LOSS [training: -0.0009973869041717794 | validation: 0.003876694334168393]
	TIME [epoch: 8.29 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002357803819713332		[learning rate: 0.00014554]
		[batch 20/20] avg loss: -0.001958530266442874		[learning rate: 0.00014527]
	Learning Rate: 0.000145273
	LOSS [training: 0.00019963677663522957 | validation: -0.011620966873021641]
	TIME [epoch: 8.29 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009919718462063687		[learning rate: 0.00014501]
		[batch 20/20] avg loss: -0.0018612404076921263		[learning rate: 0.00014475]
	Learning Rate: 0.000144746
	LOSS [training: -0.005890479434877906 | validation: -0.0036354606651084176]
	TIME [epoch: 8.27 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00042378832118962354		[learning rate: 0.00014448]
		[batch 20/20] avg loss: 0.001516459252140429		[learning rate: 0.00014422]
	Learning Rate: 0.00014422
	LOSS [training: 0.0005463354654754027 | validation: -0.007077523692536301]
	TIME [epoch: 8.3 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006465395035282871		[learning rate: 0.00014396]
		[batch 20/20] avg loss: -0.00690394916315452		[learning rate: 0.0001437]
	Learning Rate: 0.000143697
	LOSS [training: -0.006684672099218697 | validation: -0.009225436411119082]
	TIME [epoch: 8.29 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006396861233991445		[learning rate: 0.00014344]
		[batch 20/20] avg loss: -0.0015603053962169478		[learning rate: 0.00014318]
	Learning Rate: 0.000143175
	LOSS [training: -0.003978583315104198 | validation: -0.006864042882712926]
	TIME [epoch: 8.3 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0057471740308005		[learning rate: 0.00014292]
		[batch 20/20] avg loss: -0.002688941585530221		[learning rate: 0.00014266]
	Learning Rate: 0.000142656
	LOSS [training: -0.004218057808165359 | validation: -0.004191448435871544]
	TIME [epoch: 8.27 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001650352070967275		[learning rate: 0.0001424]
		[batch 20/20] avg loss: -0.0011120334784869123		[learning rate: 0.00014214]
	Learning Rate: 0.000142138
	LOSS [training: -0.0013811927747270935 | validation: -0.0091813729139123]
	TIME [epoch: 8.3 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00395693395092722		[learning rate: 0.00014188]
		[batch 20/20] avg loss: -0.0042602108061522165		[learning rate: 0.00014162]
	Learning Rate: 0.000141622
	LOSS [training: -0.00015163842761249847 | validation: -0.004190380774068539]
	TIME [epoch: 8.29 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004041074296816517		[learning rate: 0.00014137]
		[batch 20/20] avg loss: 0.0004780566589755703		[learning rate: 0.00014111]
	Learning Rate: 0.000141108
	LOSS [training: -0.0017815088189204735 | validation: -0.010013196510058752]
	TIME [epoch: 8.29 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022760865772164054		[learning rate: 0.00014085]
		[batch 20/20] avg loss: -0.002093261623974596		[learning rate: 0.0001406]
	Learning Rate: 0.000140596
	LOSS [training: -0.002184674100595501 | validation: -0.004974260856541838]
	TIME [epoch: 8.27 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005912834196004687		[learning rate: 0.00014034]
		[batch 20/20] avg loss: -0.003734883207176408		[learning rate: 0.00014009]
	Learning Rate: 0.000140086
	LOSS [training: -0.004823858701590547 | validation: -0.0006685624354334035]
	TIME [epoch: 8.31 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00618625080155524		[learning rate: 0.00013983]
		[batch 20/20] avg loss: -0.0032104633080747777		[learning rate: 0.00013958]
	Learning Rate: 0.000139578
	LOSS [training: -0.004698357054815009 | validation: -0.009889265255669388]
	TIME [epoch: 8.28 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004771573320083138		[learning rate: 0.00013932]
		[batch 20/20] avg loss: -0.005459491395013949		[learning rate: 0.00013907]
	Learning Rate: 0.000139071
	LOSS [training: -0.005115532357548544 | validation: -0.011560865956028394]
	TIME [epoch: 8.26 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004039413572307111		[learning rate: 0.00013882]
		[batch 20/20] avg loss: 0.004573698908251817		[learning rate: 0.00013857]
	Learning Rate: 0.000138566
	LOSS [training: 0.0002671426679723528 | validation: -0.0032198752167095423]
	TIME [epoch: 8.3 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00432689431135151		[learning rate: 0.00013831]
		[batch 20/20] avg loss: -0.00412310591697866		[learning rate: 0.00013806]
	Learning Rate: 0.000138064
	LOSS [training: 0.0001018941971864253 | validation: -0.002594757291605517]
	TIME [epoch: 8.31 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013149327939285169		[learning rate: 0.00013781]
		[batch 20/20] avg loss: -0.007242111346980422		[learning rate: 0.00013756]
	Learning Rate: 0.000137562
	LOSS [training: -0.004278522070454469 | validation: -0.002522813903560492]
	TIME [epoch: 8.27 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006971317837995755		[learning rate: 0.00013731]
		[batch 20/20] avg loss: -0.0019962004000512344		[learning rate: 0.00013706]
	Learning Rate: 0.000137063
	LOSS [training: -0.001346666091925405 | validation: -0.006162995835885216]
	TIME [epoch: 8.26 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.716622713369343e-07		[learning rate: 0.00013681]
		[batch 20/20] avg loss: 0.0003891367643751099		[learning rate: 0.00013657]
	Learning Rate: 0.000136566
	LOSS [training: 0.00019500421332322343 | validation: -0.006188760324806256]
	TIME [epoch: 8.3 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031838787062953475		[learning rate: 0.00013632]
		[batch 20/20] avg loss: -0.005538917287921692		[learning rate: 0.00013607]
	Learning Rate: 0.00013607
	LOSS [training: -0.004361397997108519 | validation: -0.013395706550455024]
	TIME [epoch: 8.31 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00355239062123702		[learning rate: 0.00013582]
		[batch 20/20] avg loss: -0.0022620745903528623		[learning rate: 0.00013558]
	Learning Rate: 0.000135576
	LOSS [training: -0.0029072326057949407 | validation: -0.005255858648527615]
	TIME [epoch: 8.27 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004477880865034282		[learning rate: 0.00013533]
		[batch 20/20] avg loss: 0.00700226947272922		[learning rate: 0.00013508]
	Learning Rate: 0.000135084
	LOSS [training: 0.0012621943038474681 | validation: -0.0059739617453964475]
	TIME [epoch: 8.27 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014251159801069008		[learning rate: 0.00013484]
		[batch 20/20] avg loss: -0.0006610392602134385		[learning rate: 0.00013459]
	Learning Rate: 0.000134594
	LOSS [training: -0.0010430776201601696 | validation: -0.007392870532471534]
	TIME [epoch: 8.29 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004321741016466348		[learning rate: 0.00013435]
		[batch 20/20] avg loss: -0.0015590656823954202		[learning rate: 0.00013411]
	Learning Rate: 0.000134106
	LOSS [training: -0.0029404033494308847 | validation: -0.0077556584736377335]
	TIME [epoch: 8.33 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005071691476693912		[learning rate: 0.00013386]
		[batch 20/20] avg loss: 0.0016900091581475835		[learning rate: 0.00013362]
	Learning Rate: 0.000133619
	LOSS [training: -0.0016908411592731639 | validation: -0.008277903408253838]
	TIME [epoch: 8.26 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009182263860548065		[learning rate: 0.00013338]
		[batch 20/20] avg loss: 0.002342264937195641		[learning rate: 0.00013313]
	Learning Rate: 0.000133134
	LOSS [training: -0.003419999461676212 | validation: -0.0068355561709012224]
	TIME [epoch: 8.27 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004864963619632122		[learning rate: 0.00013289]
		[batch 20/20] avg loss: -0.0016624558301000848		[learning rate: 0.00013265]
	Learning Rate: 0.000132651
	LOSS [training: -0.0032637097248661034 | validation: -0.008230992796570183]
	TIME [epoch: 8.27 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004927340979391124		[learning rate: 0.00013241]
		[batch 20/20] avg loss: -0.0026100514059139517		[learning rate: 0.00013217]
	Learning Rate: 0.00013217
	LOSS [training: -0.0037686961926525382 | validation: -0.0022217627527342182]
	TIME [epoch: 8.35 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006847371167097875		[learning rate: 0.00013193]
		[batch 20/20] avg loss: -0.0018695934341779103		[learning rate: 0.00013169]
	Learning Rate: 0.00013169
	LOSS [training: -0.0043584823006378925 | validation: -0.0031492477662152063]
	TIME [epoch: 8.27 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001029773002525643		[learning rate: 0.00013145]
		[batch 20/20] avg loss: -0.005778680350455127		[learning rate: 0.00013121]
	Learning Rate: 0.000131212
	LOSS [training: -0.0034042266764903856 | validation: -0.013362125199115706]
	TIME [epoch: 8.27 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005547469666091457		[learning rate: 0.00013097]
		[batch 20/20] avg loss: 0.0045337958649942865		[learning rate: 0.00013074]
	Learning Rate: 0.000130736
	LOSS [training: -0.0005068369005485851 | validation: -0.008497302001259904]
	TIME [epoch: 8.28 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008377027873290753		[learning rate: 0.0001305]
		[batch 20/20] avg loss: -0.0018868040954370287		[learning rate: 0.00013026]
	Learning Rate: 0.000130261
	LOSS [training: -0.0051319159843638915 | validation: -0.009443521128365192]
	TIME [epoch: 8.33 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014430894893518848		[learning rate: 0.00013002]
		[batch 20/20] avg loss: -0.0053927479512134386		[learning rate: 0.00012979]
	Learning Rate: 0.000129789
	LOSS [training: -0.003417918720282661 | validation: -0.007930607454098676]
	TIME [epoch: 8.27 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007240140146390894		[learning rate: 0.00012955]
		[batch 20/20] avg loss: -0.006365679910907458		[learning rate: 0.00012932]
	Learning Rate: 0.000129318
	LOSS [training: -0.006802910028649176 | validation: -0.007112320836001182]
	TIME [epoch: 8.27 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006151738243954289		[learning rate: 0.00012908]
		[batch 20/20] avg loss: -0.0003481568868712396		[learning rate: 0.00012885]
	Learning Rate: 0.000128848
	LOSS [training: -0.0032499475654127648 | validation: -0.009825606929552762]
	TIME [epoch: 8.29 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002528174034939432		[learning rate: 0.00012861]
		[batch 20/20] avg loss: -0.0009074372801353566		[learning rate: 0.00012838]
	Learning Rate: 0.000128381
	LOSS [training: 0.0008103683774020378 | validation: 0.0017423178023398083]
	TIME [epoch: 8.31 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011811143865873007		[learning rate: 0.00012815]
		[batch 20/20] avg loss: 8.81283499064248e-05		[learning rate: 0.00012791]
	Learning Rate: 0.000127915
	LOSS [training: -0.005861507757983291 | validation: -0.003137463848507215]
	TIME [epoch: 8.28 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024798137419243535		[learning rate: 0.00012768]
		[batch 20/20] avg loss: -0.005253365888523507		[learning rate: 0.00012745]
	Learning Rate: 0.000127451
	LOSS [training: -0.003866589815223931 | validation: -0.006307652755981272]
	TIME [epoch: 8.26 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0053130576794677255		[learning rate: 0.00012722]
		[batch 20/20] avg loss: -0.00821287650833157		[learning rate: 0.00012699]
	Learning Rate: 0.000126988
	LOSS [training: -0.006762967093899647 | validation: -0.00265792470624813]
	TIME [epoch: 8.29 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004227640075583207		[learning rate: 0.00012676]
		[batch 20/20] avg loss: -0.004639597448871211		[learning rate: 0.00012653]
	Learning Rate: 0.000126527
	LOSS [training: -0.004433618762227209 | validation: -0.014856881310058752]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_1302.pth
	Model improved!!!
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0069089309792793475		[learning rate: 0.0001263]
		[batch 20/20] avg loss: -0.006025339819907798		[learning rate: 0.00012607]
	Learning Rate: 0.000126068
	LOSS [training: -0.006467135399593572 | validation: -0.0032559171594725837]
	TIME [epoch: 8.29 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007630908885101149		[learning rate: 0.00012584]
		[batch 20/20] avg loss: -0.003600141747318448		[learning rate: 0.00012561]
	Learning Rate: 0.000125611
	LOSS [training: -0.005615525316209799 | validation: -0.0019288760586820726]
	TIME [epoch: 8.26 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0062049051085295865		[learning rate: 0.00012538]
		[batch 20/20] avg loss: 0.008603902949881587		[learning rate: 0.00012515]
	Learning Rate: 0.000125155
	LOSS [training: 0.001199498920676 | validation: -0.008412386663567816]
	TIME [epoch: 8.29 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0004868654754606268		[learning rate: 0.00012493]
		[batch 20/20] avg loss: -0.010796013958032745		[learning rate: 0.0001247]
	Learning Rate: 0.000124701
	LOSS [training: -0.005641439716746686 | validation: -0.006678288117615507]
	TIME [epoch: 8.27 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0061873810226965595		[learning rate: 0.00012447]
		[batch 20/20] avg loss: -0.002518663565746369		[learning rate: 0.00012425]
	Learning Rate: 0.000124248
	LOSS [training: -0.004353022294221464 | validation: -0.007299171679153383]
	TIME [epoch: 8.28 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005804001678028415		[learning rate: 0.00012402]
		[batch 20/20] avg loss: -6.505999657669481e-05		[learning rate: 0.0001238]
	Learning Rate: 0.000123797
	LOSS [training: -0.0029345308373025544 | validation: -0.006959987317973616]
	TIME [epoch: 8.26 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009487807053771481		[learning rate: 0.00012357]
		[batch 20/20] avg loss: -0.001711541589385808		[learning rate: 0.00012335]
	Learning Rate: 0.000123348
	LOSS [training: -0.005599674321578645 | validation: -0.0062771107445318275]
	TIME [epoch: 8.35 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038809055545575817		[learning rate: 0.00012312]
		[batch 20/20] avg loss: -0.0077700998508975685		[learning rate: 0.0001229]
	Learning Rate: 0.0001229
	LOSS [training: -0.0058255027027275755 | validation: -0.010605976496822149]
	TIME [epoch: 8.26 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0027216314357304985		[learning rate: 0.00012268]
		[batch 20/20] avg loss: -0.008046485012212513		[learning rate: 0.00012245]
	Learning Rate: 0.000122454
	LOSS [training: -0.005384058223971505 | validation: -0.014163674782701502]
	TIME [epoch: 8.27 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004456667715586364		[learning rate: 0.00012223]
		[batch 20/20] avg loss: -0.004513392885827515		[learning rate: 0.00012201]
	Learning Rate: 0.00012201
	LOSS [training: -0.00448503030070694 | validation: -0.0077433270739237815]
	TIME [epoch: 8.27 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005004879716642475		[learning rate: 0.00012179]
		[batch 20/20] avg loss: -0.004587165528736965		[learning rate: 0.00012157]
	Learning Rate: 0.000121567
	LOSS [training: -0.00479602262268972 | validation: -0.011517140515436676]
	TIME [epoch: 8.3 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007175570770407262		[learning rate: 0.00012135]
		[batch 20/20] avg loss: -0.001903966585089589		[learning rate: 0.00012113]
	Learning Rate: 0.000121126
	LOSS [training: -0.004539768677748426 | validation: -0.009037291163720908]
	TIME [epoch: 8.26 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005422126305504088		[learning rate: 0.00012091]
		[batch 20/20] avg loss: -0.005064041808141028		[learning rate: 0.00012069]
	Learning Rate: 0.000120686
	LOSS [training: -0.005243084056822559 | validation: -0.005934534088070933]
	TIME [epoch: 8.26 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00800030277276644		[learning rate: 0.00012047]
		[batch 20/20] avg loss: -0.0010760960831746233		[learning rate: 0.00012025]
	Learning Rate: 0.000120248
	LOSS [training: -0.004538199427970534 | validation: -0.007716911698643223]
	TIME [epoch: 8.28 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0068582472652185785		[learning rate: 0.00012003]
		[batch 20/20] avg loss: -0.008355533593434583		[learning rate: 0.00011981]
	Learning Rate: 0.000119812
	LOSS [training: -0.007606890429326579 | validation: -0.010489521656029963]
	TIME [epoch: 8.31 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009028751635334765		[learning rate: 0.00011959]
		[batch 20/20] avg loss: 0.0027884040426364035		[learning rate: 0.00011938]
	Learning Rate: 0.000119377
	LOSS [training: -0.0031201737963491814 | validation: 0.0041477005121277905]
	TIME [epoch: 8.26 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005580154015459804		[learning rate: 0.00011916]
		[batch 20/20] avg loss: 0.0018469557416470495		[learning rate: 0.00011894]
	Learning Rate: 0.000118944
	LOSS [training: -0.0018665991369063773 | validation: -0.006516649976072176]
	TIME [epoch: 8.26 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008473882930700064		[learning rate: 0.00011873]
		[batch 20/20] avg loss: -0.0007286181438380696		[learning rate: 0.00011851]
	Learning Rate: 0.000118512
	LOSS [training: 5.938507461596826e-05 | validation: -0.007318429526067904]
	TIME [epoch: 8.27 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004270924884231879		[learning rate: 0.0001183]
		[batch 20/20] avg loss: -0.0037215294024378906		[learning rate: 0.00011808]
	Learning Rate: 0.000118082
	LOSS [training: -0.0039962271433348845 | validation: -0.01065330144305826]
	TIME [epoch: 8.31 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002532922682203061		[learning rate: 0.00011787]
		[batch 20/20] avg loss: -0.003633385379110262		[learning rate: 0.00011765]
	Learning Rate: 0.000117654
	LOSS [training: -0.0016900465554449777 | validation: -0.012206017756420823]
	TIME [epoch: 8.26 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008724194023957708		[learning rate: 0.00011744]
		[batch 20/20] avg loss: -0.0011856994733923719		[learning rate: 0.00011723]
	Learning Rate: 0.000117227
	LOSS [training: -0.0049549467486750395 | validation: -0.004184006978785771]
	TIME [epoch: 8.26 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014424559053525345		[learning rate: 0.00011701]
		[batch 20/20] avg loss: -0.004888584141832705		[learning rate: 0.0001168]
	Learning Rate: 0.000116801
	LOSS [training: -0.0031655200235926203 | validation: -0.007493243994126597]
	TIME [epoch: 8.27 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005924132534458203		[learning rate: 0.00011659]
		[batch 20/20] avg loss: 0.007038277109783084		[learning rate: 0.00011638]
	Learning Rate: 0.000116377
	LOSS [training: 0.006481204822120642 | validation: 0.0031618937854357757]
	TIME [epoch: 8.32 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001538629696027027		[learning rate: 0.00011617]
		[batch 20/20] avg loss: -0.0004380427906910102		[learning rate: 0.00011595]
	Learning Rate: 0.000115955
	LOSS [training: -0.000988336243359018 | validation: -0.008339194931107779]
	TIME [epoch: 8.26 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0025817574336899646		[learning rate: 0.00011574]
		[batch 20/20] avg loss: 0.005171958136045698		[learning rate: 0.00011553]
	Learning Rate: 0.000115534
	LOSS [training: 0.0012951003511778672 | validation: -0.0035794965465291164]
	TIME [epoch: 8.25 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00735702760305245		[learning rate: 0.00011532]
		[batch 20/20] avg loss: -0.005580573921238353		[learning rate: 0.00011511]
	Learning Rate: 0.000115115
	LOSS [training: -0.006468800762145402 | validation: -0.010869748843947293]
	TIME [epoch: 8.28 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01029907652916715		[learning rate: 0.00011491]
		[batch 20/20] avg loss: -0.002350919718407579		[learning rate: 0.0001147]
	Learning Rate: 0.000114697
	LOSS [training: -0.006324998123787365 | validation: -0.015754454554675976]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_1329.pth
	Model improved!!!
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0041196569910956216		[learning rate: 0.00011449]
		[batch 20/20] avg loss: -0.008347722992542097		[learning rate: 0.00011428]
	Learning Rate: 0.000114281
	LOSS [training: -0.006233689991818859 | validation: -0.0063602437596858635]
	TIME [epoch: 8.25 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033037236744090427		[learning rate: 0.00011407]
		[batch 20/20] avg loss: -0.0027180980693332205		[learning rate: 0.00011387]
	Learning Rate: 0.000113866
	LOSS [training: -0.0030109108718711316 | validation: -0.008249651088068995]
	TIME [epoch: 8.25 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032025237252641774		[learning rate: 0.00011366]
		[batch 20/20] avg loss: -0.0028649047660670368		[learning rate: 0.00011345]
	Learning Rate: 0.000113453
	LOSS [training: -0.003033714245665607 | validation: -0.010118097216594532]
	TIME [epoch: 8.28 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005991777200544444		[learning rate: 0.00011325]
		[batch 20/20] avg loss: -0.004279713298986218		[learning rate: 0.00011304]
	Learning Rate: 0.000113041
	LOSS [training: -0.005135745249765331 | validation: -0.010865033125575177]
	TIME [epoch: 8.29 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004203942161047883		[learning rate: 0.00011284]
		[batch 20/20] avg loss: -0.00022951069169593495		[learning rate: 0.00011263]
	Learning Rate: 0.000112631
	LOSS [training: -0.002216726426371909 | validation: -0.005921217819069579]
	TIME [epoch: 8.26 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00690769556081605		[learning rate: 0.00011243]
		[batch 20/20] avg loss: -0.0022943890433641963		[learning rate: 0.00011222]
	Learning Rate: 0.000112222
	LOSS [training: -0.004601042302090123 | validation: -0.009311024165144295]
	TIME [epoch: 8.25 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0051940534376012995		[learning rate: 0.00011202]
		[batch 20/20] avg loss: -0.00489131933941261		[learning rate: 0.00011181]
	Learning Rate: 0.000111815
	LOSS [training: -0.005042686388506954 | validation: -0.002518484593746503]
	TIME [epoch: 8.27 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006055638732041272		[learning rate: 0.00011161]
		[batch 20/20] avg loss: -0.005307698900239513		[learning rate: 0.00011141]
	Learning Rate: 0.000111409
	LOSS [training: -0.005681668816140393 | validation: -0.008058646162255812]
	TIME [epoch: 8.28 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016147397631183051		[learning rate: 0.00011121]
		[batch 20/20] avg loss: -0.0020649893116045323		[learning rate: 0.000111]
	Learning Rate: 0.000111005
	LOSS [training: -0.0018398645373614182 | validation: -0.00472684894555046]
	TIME [epoch: 8.29 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007152914172141672		[learning rate: 0.0001108]
		[batch 20/20] avg loss: -0.0033454813691431474		[learning rate: 0.0001106]
	Learning Rate: 0.000110602
	LOSS [training: -0.005249197770642411 | validation: -0.008142917071700478]
	TIME [epoch: 8.25 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013647693326622958		[learning rate: 0.0001104]
		[batch 20/20] avg loss: -0.002385385376804871		[learning rate: 0.0001102]
	Learning Rate: 0.000110201
	LOSS [training: -0.0018750773547335838 | validation: -0.008591792572809115]
	TIME [epoch: 8.27 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008137703186422185		[learning rate: 0.00011]
		[batch 20/20] avg loss: -0.0027117577959708938		[learning rate: 0.0001098]
	Learning Rate: 0.000109801
	LOSS [training: -0.005424730491196539 | validation: -0.01371984573807859]
	TIME [epoch: 8.28 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.394803103136115e-05		[learning rate: 0.0001096]
		[batch 20/20] avg loss: -0.0059864529476570515		[learning rate: 0.0001094]
	Learning Rate: 0.000109402
	LOSS [training: -0.0029712524583128458 | validation: -0.016790081486836236]
	TIME [epoch: 8.27 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_1342.pth
	Model improved!!!
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009403290637820673		[learning rate: 0.0001092]
		[batch 20/20] avg loss: -0.0025979465325728095		[learning rate: 0.00010901]
	Learning Rate: 0.000109005
	LOSS [training: -0.006000618585196742 | validation: -0.010080548362035924]
	TIME [epoch: 8.26 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013278071285806217		[learning rate: 0.00010881]
		[batch 20/20] avg loss: -0.0010013026830775296		[learning rate: 0.00010861]
	Learning Rate: 0.00010861
	LOSS [training: -0.0011645549058290755 | validation: -0.006188935861726288]
	TIME [epoch: 8.28 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004433250607195146		[learning rate: 0.00010841]
		[batch 20/20] avg loss: -0.009354965739448133		[learning rate: 0.00010822]
	Learning Rate: 0.000108215
	LOSS [training: -0.00689410817332164 | validation: -0.005550928993855032]
	TIME [epoch: 8.26 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0067175344816229315		[learning rate: 0.00010802]
		[batch 20/20] avg loss: -0.0031568914589358483		[learning rate: 0.00010782]
	Learning Rate: 0.000107823
	LOSS [training: -0.0049372129702793905 | validation: -0.01379035229289613]
	TIME [epoch: 8.26 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003906115839638797		[learning rate: 0.00010763]
		[batch 20/20] avg loss: -0.004768070736386154		[learning rate: 0.00010743]
	Learning Rate: 0.000107432
	LOSS [training: -0.004337093288012475 | validation: -0.0035465678126018418]
	TIME [epoch: 8.26 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008680928184698986		[learning rate: 0.00010724]
		[batch 20/20] avg loss: -0.008487565759814245		[learning rate: 0.00010704]
	Learning Rate: 0.000107042
	LOSS [training: -0.008584246972256616 | validation: -0.007458659467529334]
	TIME [epoch: 8.28 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005380411798590544		[learning rate: 0.00010685]
		[batch 20/20] avg loss: -0.011199072496312308		[learning rate: 0.00010665]
	Learning Rate: 0.000106653
	LOSS [training: -0.008289742147451425 | validation: -0.00740421788820078]
	TIME [epoch: 8.27 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005461551858255772		[learning rate: 0.00010646]
		[batch 20/20] avg loss: -0.00612363907362962		[learning rate: 0.00010627]
	Learning Rate: 0.000106266
	LOSS [training: -0.005792595465942697 | validation: -0.010257024864413386]
	TIME [epoch: 8.25 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004753697328114729		[learning rate: 0.00010607]
		[batch 20/20] avg loss: -0.008201551641902456		[learning rate: 0.00010588]
	Learning Rate: 0.00010588
	LOSS [training: -0.006477624485008593 | validation: -0.010932437629969469]
	TIME [epoch: 8.28 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002539073463614298		[learning rate: 0.00010569]
		[batch 20/20] avg loss: -0.010014242784586062		[learning rate: 0.0001055]
	Learning Rate: 0.000105496
	LOSS [training: -0.006276658124100182 | validation: -0.007711463908647409]
	TIME [epoch: 8.28 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005722345047910589		[learning rate: 0.0001053]
		[batch 20/20] avg loss: -0.00344947064602954		[learning rate: 0.00010511]
	Learning Rate: 0.000105113
	LOSS [training: -0.0045859078469700654 | validation: -0.0043086073289454185]
	TIME [epoch: 8.26 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007586348096903932		[learning rate: 0.00010492]
		[batch 20/20] avg loss: -0.006899272731053775		[learning rate: 0.00010473]
	Learning Rate: 0.000104732
	LOSS [training: -0.007242810413978855 | validation: 5.854817677596947e-05]
	TIME [epoch: 8.25 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008440247294822584		[learning rate: 0.00010454]
		[batch 20/20] avg loss: -0.00784989800093103		[learning rate: 0.00010435]
	Learning Rate: 0.000104352
	LOSS [training: -0.0035029366357243867 | validation: -0.01136439781755896]
	TIME [epoch: 8.27 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012869090841649892		[learning rate: 0.00010416]
		[batch 20/20] avg loss: -0.003552445973385229		[learning rate: 0.00010397]
	Learning Rate: 0.000103973
	LOSS [training: -0.00821076840751756 | validation: -0.01014941589469108]
	TIME [epoch: 8.29 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006930013414475593		[learning rate: 0.00010378]
		[batch 20/20] avg loss: -0.007775568863786687		[learning rate: 0.0001036]
	Learning Rate: 0.000103596
	LOSS [training: -0.007352791139131139 | validation: 0.0015594796096426566]
	TIME [epoch: 8.26 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003972759538187491		[learning rate: 0.00010341]
		[batch 20/20] avg loss: 0.0002167810273136131		[learning rate: 0.00010322]
	Learning Rate: 0.00010322
	LOSS [training: -0.0018779892554369394 | validation: -0.002051935578706599]
	TIME [epoch: 8.25 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032590407141607075		[learning rate: 0.00010303]
		[batch 20/20] avg loss: -0.0056921177109054335		[learning rate: 0.00010285]
	Learning Rate: 0.000102845
	LOSS [training: -0.0044755792125330705 | validation: -0.009754370527069817]
	TIME [epoch: 8.25 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006052376974024094		[learning rate: 0.00010266]
		[batch 20/20] avg loss: -0.007355348336273254		[learning rate: 0.00010247]
	Learning Rate: 0.000102472
	LOSS [training: -0.0067038626551486746 | validation: -0.01175816700231536]
	TIME [epoch: 8.31 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007485194807090478		[learning rate: 0.00010229]
		[batch 20/20] avg loss: -0.007942203456625047		[learning rate: 0.0001021]
	Learning Rate: 0.0001021
	LOSS [training: -0.007713699131857761 | validation: -0.009638167545218063]
	TIME [epoch: 8.25 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014630990765100183		[learning rate: 0.00010191]
		[batch 20/20] avg loss: -0.0058927756890239		[learning rate: 0.00010173]
	Learning Rate: 0.00010173
	LOSS [training: -0.010261883227062042 | validation: -0.01064010612329252]
	TIME [epoch: 8.25 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0026695302328284915		[learning rate: 0.00010154]
		[batch 20/20] avg loss: -0.005157736607975099		[learning rate: 0.00010136]
	Learning Rate: 0.00010136
	LOSS [training: -0.003913633420401795 | validation: -0.01123929903299539]
	TIME [epoch: 8.25 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010990816860450318		[learning rate: 0.00010118]
		[batch 20/20] avg loss: -0.006628511751853699		[learning rate: 0.00010099]
	Learning Rate: 0.000100993
	LOSS [training: -0.003863796718949366 | validation: -0.007198534832705615]
	TIME [epoch: 8.31 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011864191647536174		[learning rate: 0.00010081]
		[batch 20/20] avg loss: -0.005722068693290782		[learning rate: 0.00010063]
	Learning Rate: 0.000100626
	LOSS [training: -0.008793130170413478 | validation: -0.0046268807453212]
	TIME [epoch: 8.25 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0044810069080725885		[learning rate: 0.00010044]
		[batch 20/20] avg loss: -0.004861030636930913		[learning rate: 0.00010026]
	Learning Rate: 0.000100261
	LOSS [training: -0.00467101877250175 | validation: -0.007997688071320098]
	TIME [epoch: 8.25 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0035571406279381614		[learning rate: 0.00010008]
		[batch 20/20] avg loss: -0.0057372469048634		[learning rate: 9.9897e-05]
	Learning Rate: 9.98971e-05
	LOSS [training: -0.0046471937664007805 | validation: -0.010341260952527145]
	TIME [epoch: 8.26 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005229178018279076		[learning rate: 9.9716e-05]
		[batch 20/20] avg loss: -0.006072359542607253		[learning rate: 9.9535e-05]
	Learning Rate: 9.95345e-05
	LOSS [training: -0.005650768780443166 | validation: -0.00430110143525419]
	TIME [epoch: 8.3 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0056254688497016205		[learning rate: 9.9354e-05]
		[batch 20/20] avg loss: 0.0010069560324136966		[learning rate: 9.9173e-05]
	Learning Rate: 9.91733e-05
	LOSS [training: -0.0023092564086439624 | validation: -0.012502357309054472]
	TIME [epoch: 8.26 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020524141961209296		[learning rate: 9.8993e-05]
		[batch 20/20] avg loss: -0.005226487430267006		[learning rate: 9.8813e-05]
	Learning Rate: 9.88134e-05
	LOSS [training: -0.0036394508131939673 | validation: -0.009597101186814843]
	TIME [epoch: 8.25 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00032700963069323485		[learning rate: 9.8634e-05]
		[batch 20/20] avg loss: -0.0012582076023301557		[learning rate: 9.8455e-05]
	Learning Rate: 9.84548e-05
	LOSS [training: -0.0007926086165116952 | validation: -0.008244460794735975]
	TIME [epoch: 8.26 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009163960203928035		[learning rate: 9.8276e-05]
		[batch 20/20] avg loss: -0.0008112245098216694		[learning rate: 9.8098e-05]
	Learning Rate: 9.80975e-05
	LOSS [training: -0.0049875923568748525 | validation: -0.008750662298610426]
	TIME [epoch: 8.28 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004983098427806056		[learning rate: 9.7919e-05]
		[batch 20/20] avg loss: -0.0062623699742656675		[learning rate: 9.7742e-05]
	Learning Rate: 9.77415e-05
	LOSS [training: -0.005622734201035861 | validation: -0.004594298269759616]
	TIME [epoch: 8.29 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008096113604156446		[learning rate: 9.7564e-05]
		[batch 20/20] avg loss: -0.004210691102598721		[learning rate: 9.7387e-05]
	Learning Rate: 9.73868e-05
	LOSS [training: -0.006153402353377583 | validation: -0.007348587193975287]
	TIME [epoch: 8.25 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011108978926583602		[learning rate: 9.721e-05]
		[batch 20/20] avg loss: -0.006071842090062185		[learning rate: 9.7033e-05]
	Learning Rate: 9.70334e-05
	LOSS [training: -0.008590410508322894 | validation: -0.0029732501564042396]
	TIME [epoch: 8.27 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002552764683768044		[learning rate: 9.6857e-05]
		[batch 20/20] avg loss: 0.0022431419777314146		[learning rate: 9.6681e-05]
	Learning Rate: 9.66812e-05
	LOSS [training: -0.00015481135301831465 | validation: -0.008150524510672837]
	TIME [epoch: 8.28 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0060973647423093625		[learning rate: 9.6506e-05]
		[batch 20/20] avg loss: -0.0008198966181297222		[learning rate: 9.633e-05]
	Learning Rate: 9.63304e-05
	LOSS [training: -0.003458630680219542 | validation: -0.007425402764478989]
	TIME [epoch: 8.27 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016980045310662853		[learning rate: 9.6155e-05]
		[batch 20/20] avg loss: -0.0019949618549003094		[learning rate: 9.5981e-05]
	Learning Rate: 9.59808e-05
	LOSS [training: -0.00014847866191701188 | validation: -0.005059658988715572]
	TIME [epoch: 8.25 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007676049480482329		[learning rate: 9.5806e-05]
		[batch 20/20] avg loss: -0.001353652580296496		[learning rate: 9.5632e-05]
	Learning Rate: 9.56324e-05
	LOSS [training: -0.0002930238161241316 | validation: -0.004755575604522932]
	TIME [epoch: 8.27 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.805274669112543e-05		[learning rate: 9.5459e-05]
		[batch 20/20] avg loss: 0.0010689051664212758		[learning rate: 9.5285e-05]
	Learning Rate: 9.52854e-05
	LOSS [training: 0.0005734789565562008 | validation: -0.013102479433982772]
	TIME [epoch: 8.27 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002379388979674712		[learning rate: 9.5112e-05]
		[batch 20/20] avg loss: -0.009879204092601982		[learning rate: 9.494e-05]
	Learning Rate: 9.49396e-05
	LOSS [training: -0.0061292965361383475 | validation: -0.010374500121857439]
	TIME [epoch: 8.26 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008174774209836793		[learning rate: 9.4767e-05]
		[batch 20/20] avg loss: -0.0006339061684781284		[learning rate: 9.4595e-05]
	Learning Rate: 9.45951e-05
	LOSS [training: -0.004404340189157461 | validation: -0.005799559674815396]
	TIME [epoch: 8.26 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011732467279350667		[learning rate: 9.4423e-05]
		[batch 20/20] avg loss: 0.00661630930127504		[learning rate: 9.4252e-05]
	Learning Rate: 9.42518e-05
	LOSS [training: -0.0025580789890378127 | validation: -0.007542437200122409]
	TIME [epoch: 8.27 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004425158451250578		[learning rate: 9.4081e-05]
		[batch 20/20] avg loss: -0.002080597615034167		[learning rate: 9.391e-05]
	Learning Rate: 9.39097e-05
	LOSS [training: -0.0032528780331423724 | validation: -0.000715814362368084]
	TIME [epoch: 8.27 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007563109205153733		[learning rate: 9.3739e-05]
		[batch 20/20] avg loss: -0.0039661415947001444		[learning rate: 9.3569e-05]
	Learning Rate: 9.35689e-05
	LOSS [training: -0.005764625399926939 | validation: -0.011653424147682546]
	TIME [epoch: 8.25 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014235698487112852		[learning rate: 9.3399e-05]
		[batch 20/20] avg loss: -0.0027095263698431395		[learning rate: 9.3229e-05]
	Learning Rate: 9.32294e-05
	LOSS [training: -0.008472612428477997 | validation: -0.012615527900030805]
	TIME [epoch: 8.27 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007181049079110417		[learning rate: 9.306e-05]
		[batch 20/20] avg loss: -0.007128385154428189		[learning rate: 9.2891e-05]
	Learning Rate: 9.2891e-05
	LOSS [training: -0.007154717116769303 | validation: -0.014117959681033309]
	TIME [epoch: 8.27 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005185648760568774		[learning rate: 9.2722e-05]
		[batch 20/20] avg loss: -0.0032173933099694926		[learning rate: 9.2554e-05]
	Learning Rate: 9.25539e-05
	LOSS [training: -0.004201521035269134 | validation: -0.007958560374311882]
	TIME [epoch: 8.27 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005304789088541491		[learning rate: 9.2386e-05]
		[batch 20/20] avg loss: -0.009874072085601975		[learning rate: 9.2218e-05]
	Learning Rate: 9.2218e-05
	LOSS [training: -0.007589430587071733 | validation: -0.003532424867287062]
	TIME [epoch: 8.25 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001873758397122559		[learning rate: 9.2051e-05]
		[batch 20/20] avg loss: -0.0027289995291155155		[learning rate: 9.1883e-05]
	Learning Rate: 9.18834e-05
	LOSS [training: -0.0023013789631190365 | validation: -0.005829566374478555]
	TIME [epoch: 8.27 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028411848942487233		[learning rate: 9.1716e-05]
		[batch 20/20] avg loss: -0.006349791887787987		[learning rate: 9.155e-05]
	Learning Rate: 9.15499e-05
	LOSS [training: -0.004595488391018355 | validation: -0.006594689583908593]
	TIME [epoch: 8.27 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008409808559014817		[learning rate: 9.1384e-05]
		[batch 20/20] avg loss: -0.0003503319919546374		[learning rate: 9.1218e-05]
	Learning Rate: 9.12177e-05
	LOSS [training: -0.004380070275484728 | validation: -0.009001616149608404]
	TIME [epoch: 8.27 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024142658044121815		[learning rate: 9.1052e-05]
		[batch 20/20] avg loss: 2.791279828803384e-05		[learning rate: 9.0887e-05]
	Learning Rate: 9.08866e-05
	LOSS [training: 0.0012210893013501073 | validation: 0.0012916884829191765]
	TIME [epoch: 8.25 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0037698184192007126		[learning rate: 9.0722e-05]
		[batch 20/20] avg loss: -0.005839954095542159		[learning rate: 9.0557e-05]
	Learning Rate: 9.05568e-05
	LOSS [training: -0.004804886257371435 | validation: -0.006439159582724626]
	TIME [epoch: 8.25 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00461801882585		[learning rate: 9.0392e-05]
		[batch 20/20] avg loss: -0.0054221872211803485		[learning rate: 9.0228e-05]
	Learning Rate: 9.02281e-05
	LOSS [training: -0.005020103023515174 | validation: -0.003223954899847355]
	TIME [epoch: 8.29 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016662670623687787		[learning rate: 9.0064e-05]
		[batch 20/20] avg loss: -0.0010981879924600274		[learning rate: 8.9901e-05]
	Learning Rate: 8.99007e-05
	LOSS [training: -0.0013822275274144034 | validation: 0.0020824061772026256]
	TIME [epoch: 8.27 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031755598119851806		[learning rate: 8.9737e-05]
		[batch 20/20] avg loss: 0.0008056855688605419		[learning rate: 8.9574e-05]
	Learning Rate: 8.95745e-05
	LOSS [training: 0.0019906226904228613 | validation: -0.005178853178945541]
	TIME [epoch: 8.25 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006662444674280861		[learning rate: 8.9412e-05]
		[batch 20/20] avg loss: -0.0031547855100993705		[learning rate: 8.9249e-05]
	Learning Rate: 8.92494e-05
	LOSS [training: -0.0049086150921901165 | validation: -0.01123970727295167]
	TIME [epoch: 8.25 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003895166636917404		[learning rate: 8.9087e-05]
		[batch 20/20] avg loss: -0.006081833854887622		[learning rate: 8.8926e-05]
	Learning Rate: 8.89255e-05
	LOSS [training: -0.004988500245902512 | validation: -0.008792410843858869]
	TIME [epoch: 8.3 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006288669190828072		[learning rate: 8.8764e-05]
		[batch 20/20] avg loss: -0.0047299279046263114		[learning rate: 8.8603e-05]
	Learning Rate: 8.86028e-05
	LOSS [training: -0.005509298547727192 | validation: -0.01033716632546197]
	TIME [epoch: 8.27 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028578605672766946		[learning rate: 8.8442e-05]
		[batch 20/20] avg loss: -0.0028497052514688042		[learning rate: 8.8281e-05]
	Learning Rate: 8.82813e-05
	LOSS [training: -0.002853782909372749 | validation: -0.00970666178086976]
	TIME [epoch: 8.24 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003221253115092427		[learning rate: 8.8121e-05]
		[batch 20/20] avg loss: -0.012924302987869596		[learning rate: 8.7961e-05]
	Learning Rate: 8.79609e-05
	LOSS [training: -0.008072778051481011 | validation: -0.013616875180573676]
	TIME [epoch: 8.25 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028019619593535198		[learning rate: 8.7801e-05]
		[batch 20/20] avg loss: -0.006958107228677829		[learning rate: 8.7642e-05]
	Learning Rate: 8.76417e-05
	LOSS [training: -0.0048800345940156736 | validation: -0.008107113861686333]
	TIME [epoch: 8.29 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008253245636792432		[learning rate: 8.7482e-05]
		[batch 20/20] avg loss: -0.016026914710734152		[learning rate: 8.7324e-05]
	Learning Rate: 8.73236e-05
	LOSS [training: -0.012140080173763293 | validation: -0.003791587473662976]
	TIME [epoch: 8.28 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004297367957958717		[learning rate: 8.7165e-05]
		[batch 20/20] avg loss: -0.0064318828234105705		[learning rate: 8.7007e-05]
	Learning Rate: 8.70067e-05
	LOSS [training: -0.005364625390684644 | validation: -0.008403462905324127]
	TIME [epoch: 8.25 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018822969714301946		[learning rate: 8.6849e-05]
		[batch 20/20] avg loss: -0.010703671087463045		[learning rate: 8.6691e-05]
	Learning Rate: 8.66909e-05
	LOSS [training: -0.00629298402944662 | validation: -0.009063967571582787]
	TIME [epoch: 8.25 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002780628105888584		[learning rate: 8.6533e-05]
		[batch 20/20] avg loss: -0.0024462063237062105		[learning rate: 8.6376e-05]
	Learning Rate: 8.63763e-05
	LOSS [training: -0.0026134172147973976 | validation: -0.007005215384644573]
	TIME [epoch: 8.28 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009668193975688061		[learning rate: 8.6219e-05]
		[batch 20/20] avg loss: -0.0038232729230128007		[learning rate: 8.6063e-05]
	Learning Rate: 8.60629e-05
	LOSS [training: -0.002395046160290803 | validation: -0.0074991326439841285]
	TIME [epoch: 8.28 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004520887228760187		[learning rate: 8.5907e-05]
		[batch 20/20] avg loss: -0.0034858155395250277		[learning rate: 8.5751e-05]
	Learning Rate: 8.57505e-05
	LOSS [training: -0.004003351384142607 | validation: -0.008684500756368722]
	TIME [epoch: 8.25 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006982307862700268		[learning rate: 8.5595e-05]
		[batch 20/20] avg loss: -0.007306331516755646		[learning rate: 8.5439e-05]
	Learning Rate: 8.54394e-05
	LOSS [training: -0.007144319689727956 | validation: -0.013698420925284054]
	TIME [epoch: 8.25 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006466983651296011		[learning rate: 8.5284e-05]
		[batch 20/20] avg loss: 0.0014662007805349968		[learning rate: 8.5129e-05]
	Learning Rate: 8.51293e-05
	LOSS [training: -0.002500391435380508 | validation: -0.00771148719707951]
	TIME [epoch: 8.29 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005780067433586766		[learning rate: 8.4975e-05]
		[batch 20/20] avg loss: -0.004816259586465356		[learning rate: 8.482e-05]
	Learning Rate: 8.48204e-05
	LOSS [training: -0.005298163510026061 | validation: -0.013298165151000227]
	TIME [epoch: 8.28 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002683933545261379		[learning rate: 8.4666e-05]
		[batch 20/20] avg loss: -0.003955983770760053		[learning rate: 8.4513e-05]
	Learning Rate: 8.45125e-05
	LOSS [training: -0.003319958658010716 | validation: -0.012296644724725353]
	TIME [epoch: 8.25 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008350274843456995		[learning rate: 8.4359e-05]
		[batch 20/20] avg loss: -0.009956575179536763		[learning rate: 8.4206e-05]
	Learning Rate: 8.42058e-05
	LOSS [training: -0.009153425011496878 | validation: -0.00785414462051411]
	TIME [epoch: 8.25 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002056464965041289		[learning rate: 8.4053e-05]
		[batch 20/20] avg loss: -0.012258343442890172		[learning rate: 8.39e-05]
	Learning Rate: 8.39002e-05
	LOSS [training: -0.007157404203965731 | validation: -0.00846278780629705]
	TIME [epoch: 8.29 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011472828563531587		[learning rate: 8.3748e-05]
		[batch 20/20] avg loss: 6.63280834088547e-05		[learning rate: 8.3596e-05]
	Learning Rate: 8.35958e-05
	LOSS [training: -0.005703250240061365 | validation: -0.017071472070951763]
	TIME [epoch: 8.26 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_1416.pth
	Model improved!!!
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008747366912890086		[learning rate: 8.3444e-05]
		[batch 20/20] avg loss: -0.008770780113113752		[learning rate: 8.3292e-05]
	Learning Rate: 8.32924e-05
	LOSS [training: -0.008759073513001918 | validation: -0.005219643952287002]
	TIME [epoch: 8.26 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: -5.882072824962802e-05		[learning rate: 8.3141e-05]
		[batch 20/20] avg loss: -0.0072561031988501706		[learning rate: 8.299e-05]
	Learning Rate: 8.29901e-05
	LOSS [training: -0.0036574619635498995 | validation: -0.01439348072816236]
	TIME [epoch: 8.25 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006136416639831529		[learning rate: 8.2839e-05]
		[batch 20/20] avg loss: -0.005369800947774472		[learning rate: 8.2689e-05]
	Learning Rate: 8.26889e-05
	LOSS [training: -0.005753108793803002 | validation: -0.007015287508263558]
	TIME [epoch: 8.29 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00519346743988358		[learning rate: 8.2539e-05]
		[batch 20/20] avg loss: -0.007930442263756289		[learning rate: 8.2389e-05]
	Learning Rate: 8.23889e-05
	LOSS [training: -0.006561954851819934 | validation: -0.007393487454998424]
	TIME [epoch: 8.25 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006433751495923986		[learning rate: 8.2239e-05]
		[batch 20/20] avg loss: -0.008199212459298997		[learning rate: 8.209e-05]
	Learning Rate: 8.20899e-05
	LOSS [training: -0.0073164819776114914 | validation: -0.011137133039287537]
	TIME [epoch: 8.28 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006200182399739947		[learning rate: 8.1941e-05]
		[batch 20/20] avg loss: -0.004481331604292772		[learning rate: 8.1792e-05]
	Learning Rate: 8.17919e-05
	LOSS [training: -0.005340757002016359 | validation: -0.006179342389829563]
	TIME [epoch: 8.26 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006723954392069524		[learning rate: 8.1643e-05]
		[batch 20/20] avg loss: -0.008246304200741416		[learning rate: 8.1495e-05]
	Learning Rate: 8.14951e-05
	LOSS [training: -0.0074851292964054714 | validation: -0.006435715739672034]
	TIME [epoch: 8.29 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001034884739941023		[learning rate: 8.1347e-05]
		[batch 20/20] avg loss: -0.01150889036152582		[learning rate: 8.1199e-05]
	Learning Rate: 8.11994e-05
	LOSS [training: -0.006271887550733421 | validation: -0.006834011270555883]
	TIME [epoch: 8.25 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007629809989342885		[learning rate: 8.1052e-05]
		[batch 20/20] avg loss: -0.008477262202577496		[learning rate: 8.0905e-05]
	Learning Rate: 8.09047e-05
	LOSS [training: -0.008053536095960191 | validation: -0.007497358441325215]
	TIME [epoch: 8.27 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007102519181509341		[learning rate: 8.0758e-05]
		[batch 20/20] avg loss: -0.00659092354726849		[learning rate: 8.0611e-05]
	Learning Rate: 8.06111e-05
	LOSS [training: -0.006846721364388917 | validation: -0.007870708440274437]
	TIME [epoch: 8.27 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006075132663069668		[learning rate: 8.0465e-05]
		[batch 20/20] avg loss: 0.0028709667283623895		[learning rate: 8.0319e-05]
	Learning Rate: 8.03186e-05
	LOSS [training: -0.001602082967353639 | validation: -0.011109413163440286]
	TIME [epoch: 8.28 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0045308925710744845		[learning rate: 8.0173e-05]
		[batch 20/20] avg loss: -0.011077533787320841		[learning rate: 8.0027e-05]
	Learning Rate: 8.00271e-05
	LOSS [training: -0.007804213179197663 | validation: -0.009603113701647898]
	TIME [epoch: 8.25 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007836763510443764		[learning rate: 7.9882e-05]
		[batch 20/20] avg loss: -0.012559217870034773		[learning rate: 7.9737e-05]
	Learning Rate: 7.97367e-05
	LOSS [training: -0.010197990690239269 | validation: -0.014823177574360068]
	TIME [epoch: 8.27 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0039007631246063485		[learning rate: 7.9592e-05]
		[batch 20/20] avg loss: -0.0028329120923317576		[learning rate: 7.9447e-05]
	Learning Rate: 7.94473e-05
	LOSS [training: -0.0033668376084690526 | validation: -0.006799503108075623]
	TIME [epoch: 8.28 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010156671289319187		[learning rate: 7.9303e-05]
		[batch 20/20] avg loss: -0.008548983589265827		[learning rate: 7.9159e-05]
	Learning Rate: 7.91589e-05
	LOSS [training: -0.009352827439292506 | validation: -0.006650944046456164]
	TIME [epoch: 8.28 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009337010472756877		[learning rate: 7.9015e-05]
		[batch 20/20] avg loss: 0.0022745925189256075		[learning rate: 7.8872e-05]
	Learning Rate: 7.88717e-05
	LOSS [training: -0.003531208976915635 | validation: -0.0007274576873217611]
	TIME [epoch: 8.25 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028718971066001484		[learning rate: 7.8728e-05]
		[batch 20/20] avg loss: -0.002220016217349254		[learning rate: 7.8585e-05]
	Learning Rate: 7.85854e-05
	LOSS [training: -0.002545956661974701 | validation: -0.007344556365487106]
	TIME [epoch: 8.25 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008810749445551665		[learning rate: 7.8443e-05]
		[batch 20/20] avg loss: -0.0010136164844816455		[learning rate: 7.83e-05]
	Learning Rate: 7.83003e-05
	LOSS [training: -0.004912182965016655 | validation: -0.001442207342737299]
	TIME [epoch: 8.3 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006237750446863784		[learning rate: 7.8158e-05]
		[batch 20/20] avg loss: -0.004981163709857236		[learning rate: 7.8016e-05]
	Learning Rate: 7.80161e-05
	LOSS [training: -0.002802469377271807 | validation: -0.003023226209220022]
	TIME [epoch: 8.28 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021305960666117064		[learning rate: 7.7874e-05]
		[batch 20/20] avg loss: 0.001845285191584026		[learning rate: 7.7733e-05]
	Learning Rate: 7.7733e-05
	LOSS [training: -0.00014265543751383988 | validation: 0.0070492768066656355]
	TIME [epoch: 8.25 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011766039784696872		[learning rate: 7.7592e-05]
		[batch 20/20] avg loss: 0.0026945869370817396		[learning rate: 7.7451e-05]
	Learning Rate: 7.74509e-05
	LOSS [training: 0.0007589914793060263 | validation: -0.00343547592313782]
	TIME [epoch: 8.24 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009602102762087427		[learning rate: 7.731e-05]
		[batch 20/20] avg loss: 0.00540164384845721		[learning rate: 7.717e-05]
	Learning Rate: 7.71698e-05
	LOSS [training: 0.007501873305272319 | validation: -0.0042537080686755975]
	TIME [epoch: 8.29 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005551512888110313		[learning rate: 7.703e-05]
		[batch 20/20] avg loss: 0.004104266900315815		[learning rate: 7.689e-05]
	Learning Rate: 7.68897e-05
	LOSS [training: 0.0017745578057523914 | validation: -0.009337748855504435]
	TIME [epoch: 8.28 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004943231486155246		[learning rate: 7.675e-05]
		[batch 20/20] avg loss: 0.0034711860462964842		[learning rate: 7.6611e-05]
	Learning Rate: 7.66107e-05
	LOSS [training: -0.0007360227199293806 | validation: -0.0011019398931353802]
	TIME [epoch: 8.26 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006076950243573845		[learning rate: 7.6472e-05]
		[batch 20/20] avg loss: -0.0014280928617299929		[learning rate: 7.6333e-05]
	Learning Rate: 7.63327e-05
	LOSS [training: 0.002324428690921925 | validation: -0.008859052354267247]
	TIME [epoch: 8.25 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036097311787534614		[learning rate: 7.6194e-05]
		[batch 20/20] avg loss: -0.003947545121163151		[learning rate: 7.6056e-05]
	Learning Rate: 7.60557e-05
	LOSS [training: -0.0037786381499583057 | validation: -0.01236399249554844]
	TIME [epoch: 8.27 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014972320507324471		[learning rate: 7.5918e-05]
		[batch 20/20] avg loss: -0.0006088349354863117		[learning rate: 7.578e-05]
	Learning Rate: 7.57797e-05
	LOSS [training: -0.0010530334931093796 | validation: -0.014530476202730201]
	TIME [epoch: 8.3 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0044677254687945835		[learning rate: 7.5642e-05]
		[batch 20/20] avg loss: -0.003820142563505684		[learning rate: 7.5505e-05]
	Learning Rate: 7.55047e-05
	LOSS [training: -0.0041439340161501335 | validation: -0.004441146796866845]
	TIME [epoch: 8.26 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006094237486667019		[learning rate: 7.5368e-05]
		[batch 20/20] avg loss: -0.00494352637162214		[learning rate: 7.5231e-05]
	Learning Rate: 7.52307e-05
	LOSS [training: -0.005518881929144579 | validation: -0.010626931222902898]
	TIME [epoch: 8.25 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0043494703849426535		[learning rate: 7.5094e-05]
		[batch 20/20] avg loss: 0.004853392255844901		[learning rate: 7.4958e-05]
	Learning Rate: 7.49576e-05
	LOSS [training: 0.0002519609354511241 | validation: -0.0017879420656071567]
	TIME [epoch: 8.27 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013451221452859023		[learning rate: 7.4822e-05]
		[batch 20/20] avg loss: -0.0004482129913171283		[learning rate: 7.4686e-05]
	Learning Rate: 7.46856e-05
	LOSS [training: -0.0008966675683015152 | validation: -0.010612064905700736]
	TIME [epoch: 8.29 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0001327926123728599		[learning rate: 7.455e-05]
		[batch 20/20] avg loss: -0.0049600179963501774		[learning rate: 7.4415e-05]
	Learning Rate: 7.44146e-05
	LOSS [training: -0.002546405304361518 | validation: -0.006078356797392758]
	TIME [epoch: 8.25 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002693168511796874		[learning rate: 7.4279e-05]
		[batch 20/20] avg loss: -0.006657305190916863		[learning rate: 7.4144e-05]
	Learning Rate: 7.41445e-05
	LOSS [training: -0.004675236851356868 | validation: -0.01646262619221611]
	TIME [epoch: 8.25 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005863660015197962		[learning rate: 7.401e-05]
		[batch 20/20] avg loss: -0.0018054404846081379		[learning rate: 7.3875e-05]
	Learning Rate: 7.38754e-05
	LOSS [training: -0.0038345502499030496 | validation: -0.013567240606193335]
	TIME [epoch: 8.28 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0053779299295251045		[learning rate: 7.3741e-05]
		[batch 20/20] avg loss: -0.007590400128349474		[learning rate: 7.3607e-05]
	Learning Rate: 7.36073e-05
	LOSS [training: -0.00648416502893729 | validation: -0.0118482784725691]
	TIME [epoch: 8.28 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00810789624073235		[learning rate: 7.3474e-05]
		[batch 20/20] avg loss: -0.00024215165122688513		[learning rate: 7.334e-05]
	Learning Rate: 7.33402e-05
	LOSS [training: -0.004175023945979618 | validation: -0.004474072322295169]
	TIME [epoch: 8.26 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003591249304893968		[learning rate: 7.3207e-05]
		[batch 20/20] avg loss: -0.004229647016729887		[learning rate: 7.3074e-05]
	Learning Rate: 7.30741e-05
	LOSS [training: -0.003910448160811928 | validation: -0.006787334481016332]
	TIME [epoch: 8.25 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007172796672280002		[learning rate: 7.2941e-05]
		[batch 20/20] avg loss: -0.010621461108285234		[learning rate: 7.2809e-05]
	Learning Rate: 7.28089e-05
	LOSS [training: -0.00889712889028262 | validation: -0.011155672200203498]
	TIME [epoch: 8.27 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009055833954958327		[learning rate: 7.2677e-05]
		[batch 20/20] avg loss: -0.007088335164257245		[learning rate: 7.2545e-05]
	Learning Rate: 7.25447e-05
	LOSS [training: -0.008072084559607784 | validation: -0.00541426284823598]
	TIME [epoch: 8.27 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008421984818335768		[learning rate: 7.2413e-05]
		[batch 20/20] avg loss: -0.007011211998150911		[learning rate: 7.2281e-05]
	Learning Rate: 7.22814e-05
	LOSS [training: -0.007716598408243338 | validation: -0.007504699703304748]
	TIME [epoch: 8.28 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001283181417870854		[learning rate: 7.215e-05]
		[batch 20/20] avg loss: -0.0006586719926769928		[learning rate: 7.2019e-05]
	Learning Rate: 7.2019e-05
	LOSS [training: -0.0009709267052739233 | validation: -0.012404027872468196]
	TIME [epoch: 8.25 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006834730968956128		[learning rate: 7.1888e-05]
		[batch 20/20] avg loss: -0.0033346638123415194		[learning rate: 7.1758e-05]
	Learning Rate: 7.17577e-05
	LOSS [training: -0.005084697390648824 | validation: -0.01012870530396446]
	TIME [epoch: 8.29 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007345327154800317		[learning rate: 7.1627e-05]
		[batch 20/20] avg loss: -0.0020455180997347986		[learning rate: 7.1497e-05]
	Learning Rate: 7.14973e-05
	LOSS [training: -0.004695422627267558 | validation: -0.00650640423740285]
	TIME [epoch: 8.26 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005054661971213041		[learning rate: 7.1367e-05]
		[batch 20/20] avg loss: 0.0022521329749896003		[learning rate: 7.1238e-05]
	Learning Rate: 7.12378e-05
	LOSS [training: -0.00140126449811172 | validation: -0.00699888655427112]
	TIME [epoch: 8.28 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002387772831010275		[learning rate: 7.1108e-05]
		[batch 20/20] avg loss: -0.006652219772502093		[learning rate: 7.0979e-05]
	Learning Rate: 7.09793e-05
	LOSS [training: -0.004519996301756184 | validation: -0.007876785992242487]
	TIME [epoch: 8.26 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004335483274189948		[learning rate: 7.085e-05]
		[batch 20/20] avg loss: 0.0007431947758173151		[learning rate: 7.0722e-05]
	Learning Rate: 7.07217e-05
	LOSS [training: -0.001796144249186317 | validation: -0.01421068498464441]
	TIME [epoch: 8.3 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020016670729754876		[learning rate: 7.0593e-05]
		[batch 20/20] avg loss: -0.008811548022522986		[learning rate: 7.0465e-05]
	Learning Rate: 7.0465e-05
	LOSS [training: -0.005406607547749237 | validation: -0.007412857903378928]
	TIME [epoch: 8.25 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00901194744856246		[learning rate: 7.0337e-05]
		[batch 20/20] avg loss: -0.006182230834226166		[learning rate: 7.0209e-05]
	Learning Rate: 7.02093e-05
	LOSS [training: -0.007597089141394314 | validation: -0.008165442602726662]
	TIME [epoch: 8.27 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007394400311595178		[learning rate: 7.0082e-05]
		[batch 20/20] avg loss: -0.011829490873187468		[learning rate: 6.9955e-05]
	Learning Rate: 6.99545e-05
	LOSS [training: -0.009611945592391324 | validation: -0.011658845417183499]
	TIME [epoch: 8.26 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0059500887867588795		[learning rate: 6.9827e-05]
		[batch 20/20] avg loss: 0.0005757925574372547		[learning rate: 6.9701e-05]
	Learning Rate: 6.97006e-05
	LOSS [training: -0.002687148114660812 | validation: -0.006286167743196087]
	TIME [epoch: 8.3 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006481217174155845		[learning rate: 6.9574e-05]
		[batch 20/20] avg loss: -0.005713406870667815		[learning rate: 6.9448e-05]
	Learning Rate: 6.94477e-05
	LOSS [training: -0.00609731202241183 | validation: -0.007697189663101247]
	TIME [epoch: 8.26 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010653797896890429		[learning rate: 6.9322e-05]
		[batch 20/20] avg loss: -0.00402656328404178		[learning rate: 6.9196e-05]
	Learning Rate: 6.91957e-05
	LOSS [training: -0.0025459715368654117 | validation: -0.007839541769646336]
	TIME [epoch: 8.26 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004900438122631659		[learning rate: 6.907e-05]
		[batch 20/20] avg loss: -0.007247996177489885		[learning rate: 6.8945e-05]
	Learning Rate: 6.89446e-05
	LOSS [training: -0.006074217150060773 | validation: -0.008952735543523212]
	TIME [epoch: 8.3 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015843405384330697		[learning rate: 6.8819e-05]
		[batch 20/20] avg loss: -0.0013589282571795079		[learning rate: 6.8694e-05]
	Learning Rate: 6.86944e-05
	LOSS [training: 0.00011270614062678105 | validation: -0.005404230761525302]
	TIME [epoch: 8.27 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003769092890952426		[learning rate: 6.857e-05]
		[batch 20/20] avg loss: -0.004984851515886502		[learning rate: 6.8445e-05]
	Learning Rate: 6.84451e-05
	LOSS [training: -0.004376972203419464 | validation: -0.008418798136977476]
	TIME [epoch: 8.25 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006355357156393284		[learning rate: 6.8321e-05]
		[batch 20/20] avg loss: -0.007978996014454568		[learning rate: 6.8197e-05]
	Learning Rate: 6.81967e-05
	LOSS [training: -0.007167176585423926 | validation: -0.01291666977652233]
	TIME [epoch: 8.25 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006114653627566352		[learning rate: 6.8073e-05]
		[batch 20/20] avg loss: -0.00917895058014349		[learning rate: 6.7949e-05]
	Learning Rate: 6.79492e-05
	LOSS [training: -0.0076468021038549215 | validation: -0.010389166665448363]
	TIME [epoch: 8.3 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007088774508962762		[learning rate: 6.7826e-05]
		[batch 20/20] avg loss: -0.008494802366964623		[learning rate: 6.7703e-05]
	Learning Rate: 6.77026e-05
	LOSS [training: -0.007791788437963692 | validation: -0.0096066541496399]
	TIME [epoch: 8.28 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005176627312996334		[learning rate: 6.758e-05]
		[batch 20/20] avg loss: -0.004197281770542367		[learning rate: 6.7457e-05]
	Learning Rate: 6.74569e-05
	LOSS [training: -0.004686954541769351 | validation: -0.01102336872902514]
	TIME [epoch: 8.25 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006642979843031342		[learning rate: 6.7334e-05]
		[batch 20/20] avg loss: -0.007444430283437919		[learning rate: 6.7212e-05]
	Learning Rate: 6.72121e-05
	LOSS [training: -0.0033900661495673925 | validation: -0.004982831996650932]
	TIME [epoch: 8.26 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004724668568856889		[learning rate: 6.709e-05]
		[batch 20/20] avg loss: -0.009504225639359081		[learning rate: 6.6968e-05]
	Learning Rate: 6.69682e-05
	LOSS [training: -0.007114447104107985 | validation: -0.00953774679042122]
	TIME [epoch: 8.28 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00971361157605359		[learning rate: 6.6847e-05]
		[batch 20/20] avg loss: -0.006759793016325363		[learning rate: 6.6725e-05]
	Learning Rate: 6.67251e-05
	LOSS [training: -0.008236702296189478 | validation: -0.009533162379089176]
	TIME [epoch: 8.3 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0070350258545592015		[learning rate: 6.6604e-05]
		[batch 20/20] avg loss: -0.00518267937763299		[learning rate: 6.6483e-05]
	Learning Rate: 6.6483e-05
	LOSS [training: -0.006108852616096096 | validation: -0.003962609034782797]
	TIME [epoch: 8.27 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007452884061973727		[learning rate: 6.6362e-05]
		[batch 20/20] avg loss: -0.004009593996416051		[learning rate: 6.6242e-05]
	Learning Rate: 6.62417e-05
	LOSS [training: -0.00573123902919489 | validation: -0.003942707157807485]
	TIME [epoch: 8.26 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008194857685838313		[learning rate: 6.6121e-05]
		[batch 20/20] avg loss: -0.005978710683594199		[learning rate: 6.6001e-05]
	Learning Rate: 6.60013e-05
	LOSS [training: -0.0070867841847162575 | validation: -0.009258186384971511]
	TIME [epoch: 8.29 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014736447598060273		[learning rate: 6.5881e-05]
		[batch 20/20] avg loss: -0.0056755694896017065		[learning rate: 6.5762e-05]
	Learning Rate: 6.57618e-05
	LOSS [training: -0.010206008543830991 | validation: -0.014510375812988132]
	TIME [epoch: 8.29 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005749548611969619		[learning rate: 6.5642e-05]
		[batch 20/20] avg loss: -0.007510501815293611		[learning rate: 6.5523e-05]
	Learning Rate: 6.55232e-05
	LOSS [training: -0.0066300252136316145 | validation: -0.0175876014535318]
	TIME [epoch: 8.26 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_1483.pth
	Model improved!!!
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009299528404985833		[learning rate: 6.5404e-05]
		[batch 20/20] avg loss: -0.006693995832009761		[learning rate: 6.5285e-05]
	Learning Rate: 6.52854e-05
	LOSS [training: -0.007996762118497798 | validation: -0.009490191048753852]
	TIME [epoch: 8.26 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0062542602936796615		[learning rate: 6.5167e-05]
		[batch 20/20] avg loss: -0.0044809411121407286		[learning rate: 6.5048e-05]
	Learning Rate: 6.50484e-05
	LOSS [training: -0.0053676007029101955 | validation: -0.017244115095681262]
	TIME [epoch: 8.29 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002435046686045038		[learning rate: 6.493e-05]
		[batch 20/20] avg loss: -0.00689450454327358		[learning rate: 6.4812e-05]
	Learning Rate: 6.48124e-05
	LOSS [training: -0.00466477561465931 | validation: -0.0021464738438668595]
	TIME [epoch: 8.28 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0042793534461868675		[learning rate: 6.4695e-05]
		[batch 20/20] avg loss: -0.009464069949792884		[learning rate: 6.4577e-05]
	Learning Rate: 6.45772e-05
	LOSS [training: -0.006871711697989875 | validation: -0.00889561296884886]
	TIME [epoch: 8.27 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: -2.2829898620672094e-05		[learning rate: 6.446e-05]
		[batch 20/20] avg loss: -0.003229540491561776		[learning rate: 6.4343e-05]
	Learning Rate: 6.43428e-05
	LOSS [training: -0.0016261851950912238 | validation: -0.01494439265375565]
	TIME [epoch: 8.25 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010732694262946959		[learning rate: 6.4226e-05]
		[batch 20/20] avg loss: -0.00657025304788574		[learning rate: 6.4109e-05]
	Learning Rate: 6.41093e-05
	LOSS [training: -0.00865147365541635 | validation: -0.00739805995463091]
	TIME [epoch: 8.29 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0067815384405726375		[learning rate: 6.3993e-05]
		[batch 20/20] avg loss: -0.0044320848893286005		[learning rate: 6.3877e-05]
	Learning Rate: 6.38767e-05
	LOSS [training: -0.005606811664950619 | validation: -0.014071336494439913]
	TIME [epoch: 8.25 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004658318228377832		[learning rate: 6.3761e-05]
		[batch 20/20] avg loss: -0.008575132481872572		[learning rate: 6.3645e-05]
	Learning Rate: 6.36449e-05
	LOSS [training: -0.0066167253551252 | validation: -0.015113244875901766]
	TIME [epoch: 8.29 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007106536136468044		[learning rate: 6.3529e-05]
		[batch 20/20] avg loss: -0.002886536718477601		[learning rate: 6.3414e-05]
	Learning Rate: 6.34139e-05
	LOSS [training: -0.004996536427472824 | validation: 0.0017324555980062688]
	TIME [epoch: 8.26 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022199618657351896		[learning rate: 6.3299e-05]
		[batch 20/20] avg loss: -0.012203206659253474		[learning rate: 6.3184e-05]
	Learning Rate: 6.31837e-05
	LOSS [training: -0.004991622396759142 | validation: -0.0038393448456613428]
	TIME [epoch: 8.3 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010266930069199272		[learning rate: 6.3069e-05]
		[batch 20/20] avg loss: -0.003971315891886389		[learning rate: 6.2954e-05]
	Learning Rate: 6.29544e-05
	LOSS [training: -0.007119122980542831 | validation: -0.004068804001912702]
	TIME [epoch: 8.26 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008691136136103749		[learning rate: 6.284e-05]
		[batch 20/20] avg loss: -0.0034826475703409925		[learning rate: 6.2726e-05]
	Learning Rate: 6.2726e-05
	LOSS [training: -0.0021758805919756844 | validation: -0.006232982103378455]
	TIME [epoch: 8.28 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007539082353354186		[learning rate: 6.2612e-05]
		[batch 20/20] avg loss: -0.01049690725754159		[learning rate: 6.2498e-05]
	Learning Rate: 6.24983e-05
	LOSS [training: -0.009017994805447887 | validation: -0.01165634126236205]
	TIME [epoch: 8.27 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011440550140644707		[learning rate: 6.2385e-05]
		[batch 20/20] avg loss: -0.004179863757319214		[learning rate: 6.2272e-05]
	Learning Rate: 6.22715e-05
	LOSS [training: -0.007810206948981958 | validation: -0.007081845005216827]
	TIME [epoch: 8.29 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003952476642258138		[learning rate: 6.2158e-05]
		[batch 20/20] avg loss: -0.008439982468678298		[learning rate: 6.2046e-05]
	Learning Rate: 6.20455e-05
	LOSS [training: -0.0061962295554682184 | validation: -0.01222372086643611]
	TIME [epoch: 8.25 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007106220948828108		[learning rate: 6.1933e-05]
		[batch 20/20] avg loss: 0.0038394729842949234		[learning rate: 6.182e-05]
	Learning Rate: 6.18204e-05
	LOSS [training: -0.0016333739822665919 | validation: 0.0035034728255274464]
	TIME [epoch: 8.27 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020785424265179217		[learning rate: 6.1708e-05]
		[batch 20/20] avg loss: -0.006686875170332128		[learning rate: 6.1596e-05]
	Learning Rate: 6.1596e-05
	LOSS [training: -0.002304166371907103 | validation: -0.016012436475459114]
	TIME [epoch: 8.28 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004336420594496343		[learning rate: 6.1484e-05]
		[batch 20/20] avg loss: -0.010153815065943064		[learning rate: 6.1372e-05]
	Learning Rate: 6.13725e-05
	LOSS [training: -0.0072451178302197035 | validation: -0.0097739421962897]
	TIME [epoch: 8.28 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005230360892161149		[learning rate: 6.1261e-05]
		[batch 20/20] avg loss: -0.006542738525716553		[learning rate: 6.115e-05]
	Learning Rate: 6.11498e-05
	LOSS [training: -0.005886549708938852 | validation: -0.014404315648046256]
	TIME [epoch: 8.26 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00867039895819713		[learning rate: 6.1039e-05]
		[batch 20/20] avg loss: -0.003920293009923538		[learning rate: 6.0928e-05]
	Learning Rate: 6.09278e-05
	LOSS [training: -0.006295345984060336 | validation: -0.01140130813246381]
	TIME [epoch: 8.26 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010645160182999403		[learning rate: 6.0817e-05]
		[batch 20/20] avg loss: -0.006527163258834534		[learning rate: 6.0707e-05]
	Learning Rate: 6.07067e-05
	LOSS [training: -0.00858616172091697 | validation: -0.012300075081899453]
	TIME [epoch: 8.3 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007137112559595182		[learning rate: 6.0596e-05]
		[batch 20/20] avg loss: -0.010499797374964858		[learning rate: 6.0486e-05]
	Learning Rate: 6.04864e-05
	LOSS [training: -0.00881845496728002 | validation: -0.00802444659953564]
	TIME [epoch: 8.29 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009698667969905341		[learning rate: 6.0377e-05]
		[batch 20/20] avg loss: -9.6233845706244e-06		[learning rate: 6.0267e-05]
	Learning Rate: 6.02669e-05
	LOSS [training: -0.0048541456772379825 | validation: -0.007560721927313876]
	TIME [epoch: 8.26 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009455126288769112		[learning rate: 6.0157e-05]
		[batch 20/20] avg loss: -0.005046849363266432		[learning rate: 6.0048e-05]
	Learning Rate: 6.00482e-05
	LOSS [training: -0.007250987826017771 | validation: -0.008358814909957175]
	TIME [epoch: 8.25 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006331794221719223		[learning rate: 5.9939e-05]
		[batch 20/20] avg loss: -0.009664921599925012		[learning rate: 5.983e-05]
	Learning Rate: 5.98303e-05
	LOSS [training: -0.007998357910822117 | validation: -0.011933457319761627]
	TIME [epoch: 8.31 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010357975449159833		[learning rate: 5.9722e-05]
		[batch 20/20] avg loss: -0.007876543324358597		[learning rate: 5.9613e-05]
	Learning Rate: 5.96132e-05
	LOSS [training: -0.009117259386759214 | validation: -0.004044431756559067]
	TIME [epoch: 8.29 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008233822708801708		[learning rate: 5.9505e-05]
		[batch 20/20] avg loss: -0.008089444016766588		[learning rate: 5.9397e-05]
	Learning Rate: 5.93968e-05
	LOSS [training: -0.008161633362784148 | validation: -0.005392757547812103]
	TIME [epoch: 8.25 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0040419547228618775		[learning rate: 5.9289e-05]
		[batch 20/20] avg loss: -0.006427125350446139		[learning rate: 5.9181e-05]
	Learning Rate: 5.91813e-05
	LOSS [training: -0.005234540036654008 | validation: -0.006181413796582203]
	TIME [epoch: 8.24 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025347897968056913		[learning rate: 5.9074e-05]
		[batch 20/20] avg loss: -0.00946930429607146		[learning rate: 5.8966e-05]
	Learning Rate: 5.89665e-05
	LOSS [training: -0.003467257249632885 | validation: -0.011179807420717277]
	TIME [epoch: 8.3 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005507011448053346		[learning rate: 5.8859e-05]
		[batch 20/20] avg loss: -0.004963049994241993		[learning rate: 5.8752e-05]
	Learning Rate: 5.87525e-05
	LOSS [training: -0.005235030721147669 | validation: -0.009945673167297255]
	TIME [epoch: 8.28 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007227164824177458		[learning rate: 5.8646e-05]
		[batch 20/20] avg loss: -0.011605298439250473		[learning rate: 5.8539e-05]
	Learning Rate: 5.85393e-05
	LOSS [training: -0.009416231631713966 | validation: -0.02074011804704346]
	TIME [epoch: 8.26 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_1514.pth
	Model improved!!!
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006619023397795283		[learning rate: 5.8433e-05]
		[batch 20/20] avg loss: -0.003079562641233198		[learning rate: 5.8327e-05]
	Learning Rate: 5.83268e-05
	LOSS [training: -0.00484929301951424 | validation: -0.005956641695744661]
	TIME [epoch: 8.25 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007312217789483648		[learning rate: 5.8221e-05]
		[batch 20/20] avg loss: -0.008690188132883747		[learning rate: 5.8115e-05]
	Learning Rate: 5.81152e-05
	LOSS [training: -0.008001202961183698 | validation: -0.013166582371904283]
	TIME [epoch: 8.28 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00915395120252228		[learning rate: 5.801e-05]
		[batch 20/20] avg loss: -0.00023409086083075727		[learning rate: 5.7904e-05]
	Learning Rate: 5.79043e-05
	LOSS [training: -0.004694021031676518 | validation: -0.01056350644579184]
	TIME [epoch: 8.29 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0048229903633164204		[learning rate: 5.7799e-05]
		[batch 20/20] avg loss: -0.0055602566882966865		[learning rate: 5.7694e-05]
	Learning Rate: 5.76941e-05
	LOSS [training: -0.005191623525806553 | validation: -0.010367832216739259]
	TIME [epoch: 8.25 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00029077507528230806		[learning rate: 5.7589e-05]
		[batch 20/20] avg loss: -0.009346041830389045		[learning rate: 5.7485e-05]
	Learning Rate: 5.74847e-05
	LOSS [training: -0.004527633377553368 | validation: -0.007694566822236955]
	TIME [epoch: 8.25 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012211910172255363		[learning rate: 5.738e-05]
		[batch 20/20] avg loss: -0.007364062013885267		[learning rate: 5.7276e-05]
	Learning Rate: 5.72761e-05
	LOSS [training: -0.003071435498329865 | validation: -0.013266618683347057]
	TIME [epoch: 8.28 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0040936256851603695		[learning rate: 5.7172e-05]
		[batch 20/20] avg loss: -0.006085497249387426		[learning rate: 5.7068e-05]
	Learning Rate: 5.70683e-05
	LOSS [training: -0.005089561467273898 | validation: -0.007774940223954186]
	TIME [epoch: 8.28 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004092497461480469		[learning rate: 5.6965e-05]
		[batch 20/20] avg loss: -0.00772928331549376		[learning rate: 5.6861e-05]
	Learning Rate: 5.68612e-05
	LOSS [training: -0.005910890388487114 | validation: -0.016024601114017885]
	TIME [epoch: 8.26 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008226808693155934		[learning rate: 5.6758e-05]
		[batch 20/20] avg loss: -0.005228493868377362		[learning rate: 5.6655e-05]
	Learning Rate: 5.66548e-05
	LOSS [training: -0.006727651280766649 | validation: -0.012928725692919496]
	TIME [epoch: 8.25 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005538374840894181		[learning rate: 5.6552e-05]
		[batch 20/20] avg loss: -0.007218227496036934		[learning rate: 5.6449e-05]
	Learning Rate: 5.64492e-05
	LOSS [training: -0.006378301168465559 | validation: -0.01826156717629334]
	TIME [epoch: 8.29 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009786674755258526		[learning rate: 5.6347e-05]
		[batch 20/20] avg loss: -0.005899485635329864		[learning rate: 5.6244e-05]
	Learning Rate: 5.62444e-05
	LOSS [training: -0.007843080195294196 | validation: -0.011008217785290571]
	TIME [epoch: 8.26 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003732325152109891		[learning rate: 5.6142e-05]
		[batch 20/20] avg loss: -0.010667333374051912		[learning rate: 5.604e-05]
	Learning Rate: 5.60403e-05
	LOSS [training: -0.0071998292630809 | validation: -0.00687497808504403]
	TIME [epoch: 8.27 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003937160694067257		[learning rate: 5.5938e-05]
		[batch 20/20] avg loss: -0.007076750184213542		[learning rate: 5.5837e-05]
	Learning Rate: 5.58369e-05
	LOSS [training: -0.0055069554391404 | validation: -0.010135241490093292]
	TIME [epoch: 8.25 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004823074135994519		[learning rate: 5.5735e-05]
		[batch 20/20] avg loss: -0.010545933183731019		[learning rate: 5.5634e-05]
	Learning Rate: 5.56342e-05
	LOSS [training: -0.007684503659862769 | validation: -0.013328529760578543]
	TIME [epoch: 8.29 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008377336328973225		[learning rate: 5.5533e-05]
		[batch 20/20] avg loss: -0.007067362210661232		[learning rate: 5.5432e-05]
	Learning Rate: 5.54323e-05
	LOSS [training: -0.0077223492698172275 | validation: -0.007544037700091924]
	TIME [epoch: 8.25 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008827210523370625		[learning rate: 5.5332e-05]
		[batch 20/20] avg loss: -0.006098335730947245		[learning rate: 5.5231e-05]
	Learning Rate: 5.52312e-05
	LOSS [training: -0.007462773127158933 | validation: -0.007963946152864065]
	TIME [epoch: 8.28 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006367403902105826		[learning rate: 5.5131e-05]
		[batch 20/20] avg loss: -0.012463533202666036		[learning rate: 5.5031e-05]
	Learning Rate: 5.50307e-05
	LOSS [training: -0.00941546855238593 | validation: -0.005505836055298707]
	TIME [epoch: 8.26 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013430175918305667		[learning rate: 5.4931e-05]
		[batch 20/20] avg loss: -0.007623098664001746		[learning rate: 5.4831e-05]
	Learning Rate: 5.4831e-05
	LOSS [training: -0.010526637291153707 | validation: -0.0011363312904378593]
	TIME [epoch: 8.3 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0033847596572368467		[learning rate: 5.4731e-05]
		[batch 20/20] avg loss: -0.005029620600164995		[learning rate: 5.4632e-05]
	Learning Rate: 5.4632e-05
	LOSS [training: -0.004207190128700921 | validation: -0.009934389840372888]
	TIME [epoch: 8.26 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010076971457607229		[learning rate: 5.4533e-05]
		[batch 20/20] avg loss: -0.006653367691554675		[learning rate: 5.4434e-05]
	Learning Rate: 5.44338e-05
	LOSS [training: -0.008365169574580951 | validation: -0.01696203548090845]
	TIME [epoch: 8.26 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005656091178308825		[learning rate: 5.4335e-05]
		[batch 20/20] avg loss: -0.009806565985711425		[learning rate: 5.4236e-05]
	Learning Rate: 5.42362e-05
	LOSS [training: -0.007731328582010126 | validation: -0.01252477559783602]
	TIME [epoch: 8.27 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01117656292060321		[learning rate: 5.4138e-05]
		[batch 20/20] avg loss: -0.002693346442397869		[learning rate: 5.4039e-05]
	Learning Rate: 5.40394e-05
	LOSS [training: -0.006934954681500538 | validation: -0.011934201356654964]
	TIME [epoch: 8.3 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012118514399964359		[learning rate: 5.3941e-05]
		[batch 20/20] avg loss: -0.007082108639569533		[learning rate: 5.3843e-05]
	Learning Rate: 5.38433e-05
	LOSS [training: -0.009600311519766945 | validation: -0.007034638803793265]
	TIME [epoch: 8.25 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008890586275861626		[learning rate: 5.3746e-05]
		[batch 20/20] avg loss: -0.007891032405719839		[learning rate: 5.3648e-05]
	Learning Rate: 5.36479e-05
	LOSS [training: -0.008390809340790732 | validation: -0.01087602462077764]
	TIME [epoch: 8.26 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008311220628950938		[learning rate: 5.355e-05]
		[batch 20/20] avg loss: -0.009121042119057184		[learning rate: 5.3453e-05]
	Learning Rate: 5.34532e-05
	LOSS [training: -0.008716131374004062 | validation: -0.006227622229569633]
	TIME [epoch: 8.29 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002739702573814142		[learning rate: 5.3356e-05]
		[batch 20/20] avg loss: -0.009296360591897906		[learning rate: 5.3259e-05]
	Learning Rate: 5.32592e-05
	LOSS [training: -0.006018031582856026 | validation: -0.010325491281575372]
	TIME [epoch: 8.29 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005203083223884665		[learning rate: 5.3162e-05]
		[batch 20/20] avg loss: -0.007651380095702194		[learning rate: 5.3066e-05]
	Learning Rate: 5.30659e-05
	LOSS [training: -0.006427231659793429 | validation: -0.011030028284931213]
	TIME [epoch: 8.26 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004732294495986092		[learning rate: 5.297e-05]
		[batch 20/20] avg loss: -0.010590206389768469		[learning rate: 5.2873e-05]
	Learning Rate: 5.28734e-05
	LOSS [training: -0.0076612504428772805 | validation: -0.012907553803323916]
	TIME [epoch: 8.25 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007683589990828176		[learning rate: 5.2777e-05]
		[batch 20/20] avg loss: -0.01116635597034315		[learning rate: 5.2681e-05]
	Learning Rate: 5.26815e-05
	LOSS [training: -0.009424972980585661 | validation: -0.01339050055881321]
	TIME [epoch: 8.28 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011411105875711824		[learning rate: 5.2586e-05]
		[batch 20/20] avg loss: -0.00956257255457179		[learning rate: 5.249e-05]
	Learning Rate: 5.24903e-05
	LOSS [training: -0.010486839215141807 | validation: -0.014453742994263406]
	TIME [epoch: 8.29 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013174801101269354		[learning rate: 5.2395e-05]
		[batch 20/20] avg loss: -0.0066487625789073326		[learning rate: 5.23e-05]
	Learning Rate: 5.22998e-05
	LOSS [training: -0.009911781840088344 | validation: -0.010010402443692895]
	TIME [epoch: 8.25 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010881626765682939		[learning rate: 5.2205e-05]
		[batch 20/20] avg loss: -0.005860823408667251		[learning rate: 5.211e-05]
	Learning Rate: 5.211e-05
	LOSS [training: -0.008371225087175095 | validation: -0.012640273188341688]
	TIME [epoch: 8.26 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009150152449487196		[learning rate: 5.2015e-05]
		[batch 20/20] avg loss: -5.302191009648235e-05		[learning rate: 5.1921e-05]
	Learning Rate: 5.19209e-05
	LOSS [training: -0.004601587179791838 | validation: -0.008698526523737697]
	TIME [epoch: 8.29 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0075445329193692765		[learning rate: 5.1827e-05]
		[batch 20/20] avg loss: -0.0066216520049453345		[learning rate: 5.1732e-05]
	Learning Rate: 5.17325e-05
	LOSS [training: -0.007083092462157306 | validation: -0.012536303807217238]
	TIME [epoch: 8.29 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007774814759492428		[learning rate: 5.1639e-05]
		[batch 20/20] avg loss: -0.0037286999930130687		[learning rate: 5.1545e-05]
	Learning Rate: 5.15447e-05
	LOSS [training: -0.005751757376252747 | validation: -0.0076920487792512034]
	TIME [epoch: 8.25 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01137322073165777		[learning rate: 5.1451e-05]
		[batch 20/20] avg loss: -0.007788915875446306		[learning rate: 5.1358e-05]
	Learning Rate: 5.13577e-05
	LOSS [training: -0.00958106830355204 | validation: -0.001682994625476347]
	TIME [epoch: 8.25 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0042495350408740225		[learning rate: 5.1264e-05]
		[batch 20/20] avg loss: -0.007464809244975765		[learning rate: 5.1171e-05]
	Learning Rate: 5.11713e-05
	LOSS [training: -0.005857172142924893 | validation: -0.010748475730940957]
	TIME [epoch: 8.27 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008058113262367023		[learning rate: 5.1078e-05]
		[batch 20/20] avg loss: 0.000685968985640217		[learning rate: 5.0986e-05]
	Learning Rate: 5.09856e-05
	LOSS [training: -0.0036860721383634026 | validation: -0.005117093862038629]
	TIME [epoch: 8.31 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006560311144423205		[learning rate: 5.0893e-05]
		[batch 20/20] avg loss: 0.0008029793934847916		[learning rate: 5.0801e-05]
	Learning Rate: 5.08006e-05
	LOSS [training: 7.34741395212353e-05 | validation: -0.006675214011826502]
	TIME [epoch: 8.25 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021269916582216716		[learning rate: 5.0708e-05]
		[batch 20/20] avg loss: -0.0006845502402381691		[learning rate: 5.0616e-05]
	Learning Rate: 5.06162e-05
	LOSS [training: -0.0014057709492299206 | validation: -0.0079291707702411]
	TIME [epoch: 8.25 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005541021006679623		[learning rate: 5.0524e-05]
		[batch 20/20] avg loss: -0.0026680806145073224		[learning rate: 5.0433e-05]
	Learning Rate: 5.04325e-05
	LOSS [training: -0.004104550810593474 | validation: -0.004538122422665276]
	TIME [epoch: 8.28 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004528971745374659		[learning rate: 5.0341e-05]
		[batch 20/20] avg loss: -0.010616734246842516		[learning rate: 5.0249e-05]
	Learning Rate: 5.02495e-05
	LOSS [training: -0.007572852996108588 | validation: -0.016949723454083486]
	TIME [epoch: 8.3 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005529007398884534		[learning rate: 5.0158e-05]
		[batch 20/20] avg loss: -0.004368556595946357		[learning rate: 5.0067e-05]
	Learning Rate: 5.00671e-05
	LOSS [training: -0.004948781997415446 | validation: -0.0077975071551342895]
	TIME [epoch: 8.26 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003956453148349899		[learning rate: 4.9976e-05]
		[batch 20/20] avg loss: -0.011152029950964246		[learning rate: 4.9885e-05]
	Learning Rate: 4.98854e-05
	LOSS [training: -0.007554241549657073 | validation: -0.00663455312279559]
	TIME [epoch: 8.25 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006424096471205015		[learning rate: 4.9795e-05]
		[batch 20/20] avg loss: -0.008886939650312285		[learning rate: 4.9704e-05]
	Learning Rate: 4.97044e-05
	LOSS [training: -0.007655518060758651 | validation: -0.017097489498530325]
	TIME [epoch: 8.28 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008874359368315387		[learning rate: 4.9614e-05]
		[batch 20/20] avg loss: -0.0037219069966114766		[learning rate: 4.9524e-05]
	Learning Rate: 4.9524e-05
	LOSS [training: -0.006298133182463432 | validation: -0.006427633288784419]
	TIME [epoch: 8.29 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005468434674163277		[learning rate: 4.9434e-05]
		[batch 20/20] avg loss: -0.010604996371373281		[learning rate: 4.9344e-05]
	Learning Rate: 4.93443e-05
	LOSS [training: -0.00803671552276828 | validation: -0.017624507339594236]
	TIME [epoch: 8.26 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007077559968887112		[learning rate: 4.9255e-05]
		[batch 20/20] avg loss: -0.010046278169481181		[learning rate: 4.9165e-05]
	Learning Rate: 4.91652e-05
	LOSS [training: -0.008561919069184147 | validation: -0.013296928820500091]
	TIME [epoch: 8.26 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006449690322706249		[learning rate: 4.9076e-05]
		[batch 20/20] avg loss: -0.007233925572432957		[learning rate: 4.8987e-05]
	Learning Rate: 4.89868e-05
	LOSS [training: -0.006841807947569602 | validation: -0.011511392926741557]
	TIME [epoch: 8.3 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028288495175421606		[learning rate: 4.8898e-05]
		[batch 20/20] avg loss: -0.003560105598230407		[learning rate: 4.8809e-05]
	Learning Rate: 4.8809e-05
	LOSS [training: -0.0031944775578862843 | validation: -0.009449633605327964]
	TIME [epoch: 8.27 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003897342845191865		[learning rate: 4.872e-05]
		[batch 20/20] avg loss: -0.006754267841985032		[learning rate: 4.8632e-05]
	Learning Rate: 4.86319e-05
	LOSS [training: -0.005325805343588449 | validation: -0.012776550099180576]
	TIME [epoch: 8.28 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004969264440755637		[learning rate: 4.8544e-05]
		[batch 20/20] avg loss: -0.003990246632790555		[learning rate: 4.8455e-05]
	Learning Rate: 4.84554e-05
	LOSS [training: -0.004479755536773097 | validation: -0.010003230149324329]
	TIME [epoch: 8.26 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007832417108676673		[learning rate: 4.8367e-05]
		[batch 20/20] avg loss: -0.0019370794530835967		[learning rate: 4.828e-05]
	Learning Rate: 4.82795e-05
	LOSS [training: -0.004884748280880135 | validation: -0.01100529621831602]
	TIME [epoch: 8.29 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005629421185965086		[learning rate: 4.8192e-05]
		[batch 20/20] avg loss: -0.013207696543137507		[learning rate: 4.8104e-05]
	Learning Rate: 4.81043e-05
	LOSS [training: -0.009418558864551296 | validation: -0.007822703313043643]
	TIME [epoch: 8.25 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034573823501625445		[learning rate: 4.8017e-05]
		[batch 20/20] avg loss: -0.006477174024391913		[learning rate: 4.793e-05]
	Learning Rate: 4.79298e-05
	LOSS [training: -0.004967278187277229 | validation: -0.012840372507929012]
	TIME [epoch: 8.27 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0042522171911966045		[learning rate: 4.7843e-05]
		[batch 20/20] avg loss: -0.011682864321060096		[learning rate: 4.7756e-05]
	Learning Rate: 4.77558e-05
	LOSS [training: -0.007967540756128353 | validation: -0.008193416936627443]
	TIME [epoch: 8.27 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004927189523072761		[learning rate: 4.7669e-05]
		[batch 20/20] avg loss: -0.009083776430174877		[learning rate: 4.7583e-05]
	Learning Rate: 4.75825e-05
	LOSS [training: -0.007005482976623819 | validation: -0.008424616787247608]
	TIME [epoch: 8.3 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011302730361217506		[learning rate: 4.7496e-05]
		[batch 20/20] avg loss: -0.008241326752063589		[learning rate: 4.741e-05]
	Learning Rate: 4.74098e-05
	LOSS [training: -0.009772028556640545 | validation: -0.008051405288121528]
	TIME [epoch: 8.26 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004388852408788507		[learning rate: 4.7324e-05]
		[batch 20/20] avg loss: -0.002830092894603318		[learning rate: 4.7238e-05]
	Learning Rate: 4.72378e-05
	LOSS [training: -0.0036094726516959124 | validation: -0.014395721053345547]
	TIME [epoch: 8.26 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009962701701483662		[learning rate: 4.7152e-05]
		[batch 20/20] avg loss: -0.00802746543783658		[learning rate: 4.7066e-05]
	Learning Rate: 4.70664e-05
	LOSS [training: -0.008995083569660122 | validation: -0.008585926645149936]
	TIME [epoch: 8.28 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006791329941028964		[learning rate: 4.6981e-05]
		[batch 20/20] avg loss: -0.008103579674327661		[learning rate: 4.6896e-05]
	Learning Rate: 4.68955e-05
	LOSS [training: -0.007447454807678312 | validation: -0.010032439059441234]
	TIME [epoch: 8.3 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002602178936836824		[learning rate: 4.681e-05]
		[batch 20/20] avg loss: -0.008760503799428453		[learning rate: 4.6725e-05]
	Learning Rate: 4.67254e-05
	LOSS [training: -0.005681341368132638 | validation: -0.008008267871002154]
	TIME [epoch: 8.26 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012657465776538614		[learning rate: 4.6641e-05]
		[batch 20/20] avg loss: -0.008566980589447897		[learning rate: 4.6556e-05]
	Learning Rate: 4.65558e-05
	LOSS [training: -0.010612223182993255 | validation: -0.012747466152664458]
	TIME [epoch: 8.25 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010186267065514823		[learning rate: 4.6471e-05]
		[batch 20/20] avg loss: -0.008067214488854628		[learning rate: 4.6387e-05]
	Learning Rate: 4.63868e-05
	LOSS [training: -0.009126740777184726 | validation: -0.008333395169762893]
	TIME [epoch: 8.27 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005267237520473809		[learning rate: 4.6303e-05]
		[batch 20/20] avg loss: -0.008068095058273259		[learning rate: 4.6219e-05]
	Learning Rate: 4.62185e-05
	LOSS [training: -0.006667666289373533 | validation: -0.008141453195072826]
	TIME [epoch: 8.3 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006966784742605256		[learning rate: 4.6135e-05]
		[batch 20/20] avg loss: -0.0075133477966377316		[learning rate: 4.6051e-05]
	Learning Rate: 4.60508e-05
	LOSS [training: -0.007240066269621495 | validation: -0.007030879974539273]
	TIME [epoch: 8.25 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007746036908555906		[learning rate: 4.5967e-05]
		[batch 20/20] avg loss: -0.005158552761712089		[learning rate: 4.5884e-05]
	Learning Rate: 4.58836e-05
	LOSS [training: -0.006452294835133998 | validation: -0.01050832859423687]
	TIME [epoch: 8.25 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.015223921506224296		[learning rate: 4.58e-05]
		[batch 20/20] avg loss: -0.00678857401480442		[learning rate: 4.5717e-05]
	Learning Rate: 4.57171e-05
	LOSS [training: -0.011006247760514355 | validation: -0.012776153661526544]
	TIME [epoch: 8.28 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008097692820916071		[learning rate: 4.5634e-05]
		[batch 20/20] avg loss: -0.008514412634864162		[learning rate: 4.5551e-05]
	Learning Rate: 4.55512e-05
	LOSS [training: -0.008306052727890114 | validation: -0.013644370396505241]
	TIME [epoch: 8.29 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00535202234952484		[learning rate: 4.5468e-05]
		[batch 20/20] avg loss: -0.010470725057997237		[learning rate: 4.5386e-05]
	Learning Rate: 4.53859e-05
	LOSS [training: -0.00791137370376104 | validation: -0.01102178394579281]
	TIME [epoch: 8.26 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006634450595374599		[learning rate: 4.5303e-05]
		[batch 20/20] avg loss: -0.00773772849963013		[learning rate: 4.5221e-05]
	Learning Rate: 4.52212e-05
	LOSS [training: -0.007186089547502365 | validation: -0.009512009652208527]
	TIME [epoch: 8.24 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007249882342759709		[learning rate: 4.5139e-05]
		[batch 20/20] avg loss: -0.009923241579221576		[learning rate: 4.5057e-05]
	Learning Rate: 4.50571e-05
	LOSS [training: -0.00858656196099064 | validation: -0.0071206201062410405]
	TIME [epoch: 8.27 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011265755197445139		[learning rate: 4.4975e-05]
		[batch 20/20] avg loss: -0.010897947836835495		[learning rate: 4.4894e-05]
	Learning Rate: 4.48936e-05
	LOSS [training: -0.011081851517140317 | validation: -0.012408842656283968]
	TIME [epoch: 8.31 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013289938434870158		[learning rate: 4.4812e-05]
		[batch 20/20] avg loss: -0.00888817115431376		[learning rate: 4.4731e-05]
	Learning Rate: 4.47307e-05
	LOSS [training: -0.01108905479459196 | validation: -0.005617556350846039]
	TIME [epoch: 8.25 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012123563280397497		[learning rate: 4.4649e-05]
		[batch 20/20] avg loss: -0.006645309590231368		[learning rate: 4.4568e-05]
	Learning Rate: 4.45683e-05
	LOSS [training: -0.009384436435314437 | validation: -0.009621829755752799]
	TIME [epoch: 8.25 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006367023756821846		[learning rate: 4.4487e-05]
		[batch 20/20] avg loss: -0.007393488671698029		[learning rate: 4.4407e-05]
	Learning Rate: 4.44066e-05
	LOSS [training: -0.0068802562142599374 | validation: -0.010059689998976891]
	TIME [epoch: 8.28 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007234786654717178		[learning rate: 4.4326e-05]
		[batch 20/20] avg loss: -0.010162653546128187		[learning rate: 4.4245e-05]
	Learning Rate: 4.42454e-05
	LOSS [training: -0.008698720100422682 | validation: -0.01354704831153988]
	TIME [epoch: 8.31 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010595796506614352		[learning rate: 4.4165e-05]
		[batch 20/20] avg loss: -4.416951658099376e-05		[learning rate: 4.4085e-05]
	Learning Rate: 4.40849e-05
	LOSS [training: -0.005319983011597673 | validation: -0.00818335809724853]
	TIME [epoch: 8.25 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01400865413336464		[learning rate: 4.4005e-05]
		[batch 20/20] avg loss: -0.004579852732710608		[learning rate: 4.3925e-05]
	Learning Rate: 4.39249e-05
	LOSS [training: -0.009294253433037624 | validation: -0.01047368496129578]
	TIME [epoch: 8.26 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0064396999298986304		[learning rate: 4.3845e-05]
		[batch 20/20] avg loss: -0.010115059100008247		[learning rate: 4.3765e-05]
	Learning Rate: 4.37655e-05
	LOSS [training: -0.008277379514953439 | validation: -0.007396428639167769]
	TIME [epoch: 8.27 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009309925194784505		[learning rate: 4.3686e-05]
		[batch 20/20] avg loss: -0.006965896690085191		[learning rate: 4.3607e-05]
	Learning Rate: 4.36067e-05
	LOSS [training: -0.008137910942434847 | validation: -0.006302265150140255]
	TIME [epoch: 8.3 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008638694846889583		[learning rate: 4.3527e-05]
		[batch 20/20] avg loss: -0.006690641480928702		[learning rate: 4.3448e-05]
	Learning Rate: 4.34484e-05
	LOSS [training: -0.007664668163909143 | validation: -0.01164200165234128]
	TIME [epoch: 8.25 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009654452919025853		[learning rate: 4.3369e-05]
		[batch 20/20] avg loss: -0.00849623013522553		[learning rate: 4.3291e-05]
	Learning Rate: 4.32907e-05
	LOSS [training: -0.009075341527125688 | validation: -0.005567769904321507]
	TIME [epoch: 8.25 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005731309958956664		[learning rate: 4.3212e-05]
		[batch 20/20] avg loss: -0.011000473620298195		[learning rate: 4.3134e-05]
	Learning Rate: 4.31336e-05
	LOSS [training: -0.00836589178962743 | validation: -0.0072328860540828615]
	TIME [epoch: 8.27 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00654045268586786		[learning rate: 4.3055e-05]
		[batch 20/20] avg loss: -0.01190789256488091		[learning rate: 4.2977e-05]
	Learning Rate: 4.29771e-05
	LOSS [training: -0.009224172625374385 | validation: -0.010488678269924615]
	TIME [epoch: 8.27 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01028463691323257		[learning rate: 4.2899e-05]
		[batch 20/20] avg loss: -0.004570929780527645		[learning rate: 4.2821e-05]
	Learning Rate: 4.28211e-05
	LOSS [training: -0.0074277833468801065 | validation: -0.012032391822712384]
	TIME [epoch: 8.29 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009094710016919078		[learning rate: 4.2743e-05]
		[batch 20/20] avg loss: -0.00020049099920319973		[learning rate: 4.2666e-05]
	Learning Rate: 4.26657e-05
	LOSS [training: -0.004647600508061138 | validation: -0.013417399679707266]
	TIME [epoch: 8.26 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0032237434903067434		[learning rate: 4.2588e-05]
		[batch 20/20] avg loss: -0.00608547567371742		[learning rate: 4.2511e-05]
	Learning Rate: 4.25109e-05
	LOSS [training: -0.0046546095820120805 | validation: -0.0126977023154124]
	TIME [epoch: 8.3 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013412766202172582		[learning rate: 4.2434e-05]
		[batch 20/20] avg loss: -0.0055824205196056634		[learning rate: 4.2357e-05]
	Learning Rate: 4.23566e-05
	LOSS [training: -0.009497593360889126 | validation: -0.016096683937236272]
	TIME [epoch: 8.27 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009162126209348555		[learning rate: 4.228e-05]
		[batch 20/20] avg loss: -0.0022396492495169364		[learning rate: 4.2203e-05]
	Learning Rate: 4.22029e-05
	LOSS [training: -0.005700887729432747 | validation: -0.010344134203924962]
	TIME [epoch: 8.28 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01079982031248573		[learning rate: 4.2126e-05]
		[batch 20/20] avg loss: -0.00791084818347099		[learning rate: 4.205e-05]
	Learning Rate: 4.20497e-05
	LOSS [training: -0.009355334247978358 | validation: -0.014548273594256145]
	TIME [epoch: 8.26 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005833751153896699		[learning rate: 4.1973e-05]
		[batch 20/20] avg loss: -0.010574472529799732		[learning rate: 4.1897e-05]
	Learning Rate: 4.18971e-05
	LOSS [training: -0.008204111841848216 | validation: -0.006860490427371267]
	TIME [epoch: 8.3 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008763271261238223		[learning rate: 4.1821e-05]
		[batch 20/20] avg loss: -0.006855924411149773		[learning rate: 4.1745e-05]
	Learning Rate: 4.17451e-05
	LOSS [training: -0.007809597836193999 | validation: -0.009067110079835699]
	TIME [epoch: 8.27 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011695517985009651		[learning rate: 4.1669e-05]
		[batch 20/20] avg loss: -0.0072027122614726725		[learning rate: 4.1594e-05]
	Learning Rate: 4.15936e-05
	LOSS [training: -0.009449115123241161 | validation: -0.017002471147506828]
	TIME [epoch: 8.26 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0053650572600291		[learning rate: 4.1518e-05]
		[batch 20/20] avg loss: -0.007028480205227084		[learning rate: 4.1443e-05]
	Learning Rate: 4.14426e-05
	LOSS [training: -0.006196768732628092 | validation: -0.010147121209493933]
	TIME [epoch: 8.27 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007426835645057235		[learning rate: 4.1367e-05]
		[batch 20/20] avg loss: -0.001580584722353734		[learning rate: 4.1292e-05]
	Learning Rate: 4.12922e-05
	LOSS [training: -0.004503710183705484 | validation: -0.008794635810707517]
	TIME [epoch: 8.3 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009385977826820604		[learning rate: 4.1217e-05]
		[batch 20/20] avg loss: -0.008786048349993362		[learning rate: 4.1142e-05]
	Learning Rate: 4.11424e-05
	LOSS [training: -0.009086013088406984 | validation: -0.011585817187206469]
	TIME [epoch: 8.26 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006040475296053487		[learning rate: 4.1068e-05]
		[batch 20/20] avg loss: -0.006512610704759271		[learning rate: 4.0993e-05]
	Learning Rate: 4.09931e-05
	LOSS [training: -0.0062765430004063805 | validation: -0.008077988784576585]
	TIME [epoch: 8.26 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009551549159219984		[learning rate: 4.0919e-05]
		[batch 20/20] avg loss: -0.008432654068639937		[learning rate: 4.0844e-05]
	Learning Rate: 4.08443e-05
	LOSS [training: -0.008992101613929961 | validation: -0.01447961818013209]
	TIME [epoch: 8.29 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00945641598860793		[learning rate: 4.077e-05]
		[batch 20/20] avg loss: -0.005845463763679651		[learning rate: 4.0696e-05]
	Learning Rate: 4.06961e-05
	LOSS [training: -0.00765093987614379 | validation: -0.014831940296597478]
	TIME [epoch: 8.3 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00819895502051288		[learning rate: 4.0622e-05]
		[batch 20/20] avg loss: -0.01010726708753763		[learning rate: 4.0548e-05]
	Learning Rate: 4.05484e-05
	LOSS [training: -0.009153111054025253 | validation: -0.012980848085156947]
	TIME [epoch: 8.26 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010508894946597139		[learning rate: 4.0475e-05]
		[batch 20/20] avg loss: -0.01060770613206596		[learning rate: 4.0401e-05]
	Learning Rate: 4.04012e-05
	LOSS [training: -0.010558300539331548 | validation: -0.0099155705947447]
	TIME [epoch: 8.26 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011570268332555908		[learning rate: 4.0328e-05]
		[batch 20/20] avg loss: -0.013488165853087106		[learning rate: 4.0255e-05]
	Learning Rate: 4.02546e-05
	LOSS [training: -0.012529217092821506 | validation: -0.01010243540574842]
	TIME [epoch: 8.29 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006141208511258234		[learning rate: 4.0182e-05]
		[batch 20/20] avg loss: -0.008637268123688068		[learning rate: 4.0109e-05]
	Learning Rate: 4.01085e-05
	LOSS [training: -0.007389238317473153 | validation: -0.0022525684115211166]
	TIME [epoch: 8.29 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007692785821393815		[learning rate: 4.0036e-05]
		[batch 20/20] avg loss: -0.008479505467836924		[learning rate: 3.9963e-05]
	Learning Rate: 3.9963e-05
	LOSS [training: -0.00808614564461537 | validation: -0.004839118170248655]
	TIME [epoch: 8.26 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007831922825257945		[learning rate: 3.989e-05]
		[batch 20/20] avg loss: -0.006914881294469771		[learning rate: 3.9818e-05]
	Learning Rate: 3.9818e-05
	LOSS [training: -0.00737340205986386 | validation: -0.006046711176675117]
	TIME [epoch: 8.26 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010789947508403602		[learning rate: 3.9746e-05]
		[batch 20/20] avg loss: -0.014320341574888772		[learning rate: 3.9673e-05]
	Learning Rate: 3.96735e-05
	LOSS [training: -0.012555144541646187 | validation: -0.011133855593127706]
	TIME [epoch: 8.27 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008317131590885245		[learning rate: 3.9601e-05]
		[batch 20/20] avg loss: -0.00914002878876508		[learning rate: 3.9529e-05]
	Learning Rate: 3.95295e-05
	LOSS [training: -0.008728580189825164 | validation: -0.01636887534329726]
	TIME [epoch: 8.31 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002296803143043611		[learning rate: 3.9458e-05]
		[batch 20/20] avg loss: -0.01215151708983744		[learning rate: 3.9386e-05]
	Learning Rate: 3.9386e-05
	LOSS [training: -0.007224160116440524 | validation: -0.01173457122697952]
	TIME [epoch: 8.25 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0031658891914751034		[learning rate: 3.9315e-05]
		[batch 20/20] avg loss: -0.007206470797585454		[learning rate: 3.9243e-05]
	Learning Rate: 3.92431e-05
	LOSS [training: -0.005186179994530278 | validation: -0.0078822298022563]
	TIME [epoch: 8.26 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007556755105961479		[learning rate: 3.9172e-05]
		[batch 20/20] avg loss: -0.007236087778760047		[learning rate: 3.9101e-05]
	Learning Rate: 3.91007e-05
	LOSS [training: -0.007396421442360765 | validation: -0.00973317477386997]
	TIME [epoch: 8.29 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008247766311309362		[learning rate: 3.903e-05]
		[batch 20/20] avg loss: -0.009265317631332455		[learning rate: 3.8959e-05]
	Learning Rate: 3.89588e-05
	LOSS [training: -0.008756541971320907 | validation: -0.010068798272167391]
	TIME [epoch: 8.3 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00957938246659472		[learning rate: 3.8888e-05]
		[batch 20/20] avg loss: -0.0034563577174834157		[learning rate: 3.8817e-05]
	Learning Rate: 3.88174e-05
	LOSS [training: -0.006517870092039068 | validation: -0.012807341772980136]
	TIME [epoch: 8.26 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008749293699610708		[learning rate: 3.8747e-05]
		[batch 20/20] avg loss: -0.008908159093613425		[learning rate: 3.8677e-05]
	Learning Rate: 3.86765e-05
	LOSS [training: -0.008828726396612068 | validation: -0.01064881918914678]
	TIME [epoch: 8.26 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003593836564073103		[learning rate: 3.8606e-05]
		[batch 20/20] avg loss: -0.008002514904245677		[learning rate: 3.8536e-05]
	Learning Rate: 3.85362e-05
	LOSS [training: -0.00579817573415939 | validation: -0.014968507700570174]
	TIME [epoch: 8.29 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005548174507693198		[learning rate: 3.8466e-05]
		[batch 20/20] avg loss: -0.012833840455669848		[learning rate: 3.8396e-05]
	Learning Rate: 3.83963e-05
	LOSS [training: -0.009191007481681524 | validation: -0.010176551320916499]
	TIME [epoch: 8.29 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005672339483824113		[learning rate: 3.8327e-05]
		[batch 20/20] avg loss: -0.009216600470474887		[learning rate: 3.8257e-05]
	Learning Rate: 3.8257e-05
	LOSS [training: -0.007444469977149501 | validation: -0.005711311187997033]
	TIME [epoch: 8.27 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007212305260933587		[learning rate: 3.8187e-05]
		[batch 20/20] avg loss: -0.013425161759796514		[learning rate: 3.8118e-05]
	Learning Rate: 3.81181e-05
	LOSS [training: -0.01031873351036505 | validation: -0.02199362020207652]
	TIME [epoch: 8.26 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r3_20240219_233648/states/model_tr_study202_1632.pth
	Model improved!!!
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004327114241704635		[learning rate: 3.8049e-05]
		[batch 20/20] avg loss: -0.012798405933435614		[learning rate: 3.798e-05]
	Learning Rate: 3.79798e-05
	LOSS [training: -0.008562760087570124 | validation: -0.017955028038821964]
	TIME [epoch: 8.3 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010183043283322767		[learning rate: 3.7911e-05]
		[batch 20/20] avg loss: -0.012568196603434344		[learning rate: 3.7842e-05]
	Learning Rate: 3.7842e-05
	LOSS [training: -0.011375619943378554 | validation: -0.01354985044549165]
	TIME [epoch: 8.27 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0098405974355419		[learning rate: 3.7773e-05]
		[batch 20/20] avg loss: -0.006468011263477675		[learning rate: 3.7705e-05]
	Learning Rate: 3.77046e-05
	LOSS [training: -0.008154304349509789 | validation: -0.013093643572344273]
	TIME [epoch: 8.28 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010034627593539767		[learning rate: 3.7636e-05]
		[batch 20/20] avg loss: -0.00694308677785378		[learning rate: 3.7568e-05]
	Learning Rate: 3.75678e-05
	LOSS [training: -0.008488857185696773 | validation: -0.01734178246901639]
	TIME [epoch: 8.26 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006476468015960468		[learning rate: 3.75e-05]
		[batch 20/20] avg loss: -0.008114392577568016		[learning rate: 3.7431e-05]
	Learning Rate: 3.74315e-05
	LOSS [training: -0.007295430296764242 | validation: -0.014611610725959146]
	TIME [epoch: 8.3 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006149371378726395		[learning rate: 3.7363e-05]
		[batch 20/20] avg loss: -0.013447002122185725		[learning rate: 3.7296e-05]
	Learning Rate: 3.72956e-05
	LOSS [training: -0.009798186750456057 | validation: -0.005942920274965697]
	TIME [epoch: 8.27 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009734868738726987		[learning rate: 3.7228e-05]
		[batch 20/20] avg loss: -0.014483474240569713		[learning rate: 3.716e-05]
	Learning Rate: 3.71603e-05
	LOSS [training: -0.01210917148964835 | validation: -0.01325906126975304]
	TIME [epoch: 8.28 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007607475524641976		[learning rate: 3.7093e-05]
		[batch 20/20] avg loss: -0.00737785921138053		[learning rate: 3.7025e-05]
	Learning Rate: 3.70254e-05
	LOSS [training: -0.007492667368011253 | validation: -0.014517220990458359]
	TIME [epoch: 8.26 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007564125016295544		[learning rate: 3.6958e-05]
		[batch 20/20] avg loss: -0.006773086644282243		[learning rate: 3.6891e-05]
	Learning Rate: 3.68911e-05
	LOSS [training: -0.007168605830288895 | validation: -0.005387198818499726]
	TIME [epoch: 8.31 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007246690969567477		[learning rate: 3.6824e-05]
		[batch 20/20] avg loss: -0.010493823844201612		[learning rate: 3.6757e-05]
	Learning Rate: 3.67572e-05
	LOSS [training: -0.008870257406884545 | validation: -0.009059534521677298]
	TIME [epoch: 8.26 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00825756652710059		[learning rate: 3.669e-05]
		[batch 20/20] avg loss: -0.008342650061968		[learning rate: 3.6624e-05]
	Learning Rate: 3.66238e-05
	LOSS [training: -0.008300108294534297 | validation: -0.0035381552219235337]
	TIME [epoch: 8.27 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019264065736917077		[learning rate: 3.6557e-05]
		[batch 20/20] avg loss: -0.01004018251285221		[learning rate: 3.6491e-05]
	Learning Rate: 3.64909e-05
	LOSS [training: -0.005983294543271959 | validation: -0.008734110753867402]
	TIME [epoch: 8.27 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019595925999471423		[learning rate: 3.6425e-05]
		[batch 20/20] avg loss: -0.008507110833618018		[learning rate: 3.6358e-05]
	Learning Rate: 3.63584e-05
	LOSS [training: -0.005233351716782581 | validation: -0.011599094300557061]
	TIME [epoch: 8.3 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0069382894226026285		[learning rate: 3.6292e-05]
		[batch 20/20] avg loss: -0.006109766974992835		[learning rate: 3.6226e-05]
	Learning Rate: 3.62265e-05
	LOSS [training: -0.006524028198797731 | validation: -0.014060377938773534]
	TIME [epoch: 8.25 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010271928529558955		[learning rate: 3.6161e-05]
		[batch 20/20] avg loss: -0.0036777469612257795		[learning rate: 3.6095e-05]
	Learning Rate: 3.6095e-05
	LOSS [training: -0.006974837745392368 | validation: -0.009882284591773555]
	TIME [epoch: 8.25 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007400258858542365		[learning rate: 3.6029e-05]
		[batch 20/20] avg loss: -0.005313507994417368		[learning rate: 3.5964e-05]
	Learning Rate: 3.5964e-05
	LOSS [training: -0.006356883426479864 | validation: -0.009521334832410331]
	TIME [epoch: 8.29 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00582905006322505		[learning rate: 3.5899e-05]
		[batch 20/20] avg loss: -0.013567084348782671		[learning rate: 3.5834e-05]
	Learning Rate: 3.58335e-05
	LOSS [training: -0.009698067206003861 | validation: -0.009101469170182107]
	TIME [epoch: 8.28 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009985310242803812		[learning rate: 3.5768e-05]
		[batch 20/20] avg loss: -0.008263439273220442		[learning rate: 3.5703e-05]
	Learning Rate: 3.57035e-05
	LOSS [training: -0.009124374758012128 | validation: -0.014031046590920925]
	TIME [epoch: 8.25 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006422585750062708		[learning rate: 3.5639e-05]
		[batch 20/20] avg loss: -0.014042139580171299		[learning rate: 3.5574e-05]
	Learning Rate: 3.55739e-05
	LOSS [training: -0.010232362665117003 | validation: -0.009162200498365611]
	TIME [epoch: 8.25 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011973811305259173		[learning rate: 3.5509e-05]
		[batch 20/20] avg loss: -0.007496485042736504		[learning rate: 3.5445e-05]
	Learning Rate: 3.54448e-05
	LOSS [training: -0.009735148173997838 | validation: -0.010453032666473834]
	TIME [epoch: 8.3 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01133679832058362		[learning rate: 3.538e-05]
		[batch 20/20] avg loss: -0.01148231606134742		[learning rate: 3.5316e-05]
	Learning Rate: 3.53162e-05
	LOSS [training: -0.01140955719096552 | validation: -0.015899669849954622]
	TIME [epoch: 8.29 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007012016624756841		[learning rate: 3.5252e-05]
		[batch 20/20] avg loss: -0.009523351204410304		[learning rate: 3.5188e-05]
	Learning Rate: 3.5188e-05
	LOSS [training: -0.008267683914583573 | validation: -0.012080806105220627]
	TIME [epoch: 8.26 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0099223792446917		[learning rate: 3.5124e-05]
		[batch 20/20] avg loss: -0.0028283187946977423		[learning rate: 3.506e-05]
	Learning Rate: 3.50603e-05
	LOSS [training: -0.006375349019694722 | validation: -0.015301816678699133]
	TIME [epoch: 8.26 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011435844982579345		[learning rate: 3.4997e-05]
		[batch 20/20] avg loss: -0.010844693006664519		[learning rate: 3.4933e-05]
	Learning Rate: 3.49331e-05
	LOSS [training: -0.011140268994621932 | validation: -0.0090747334618852]
	TIME [epoch: 8.29 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007515268200410506		[learning rate: 3.487e-05]
		[batch 20/20] avg loss: -0.010331546016816106		[learning rate: 3.4806e-05]
	Learning Rate: 3.48063e-05
	LOSS [training: -0.008923407108613307 | validation: -0.010793218062225912]
	TIME [epoch: 8.3 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008430075914941219		[learning rate: 3.4743e-05]
		[batch 20/20] avg loss: -0.007442350146930921		[learning rate: 3.468e-05]
	Learning Rate: 3.468e-05
	LOSS [training: -0.007936213030936069 | validation: -0.00822316868180291]
	TIME [epoch: 8.26 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009682145764625729		[learning rate: 3.4617e-05]
		[batch 20/20] avg loss: -0.008445560755734436		[learning rate: 3.4554e-05]
	Learning Rate: 3.45541e-05
	LOSS [training: -0.009063853260180086 | validation: -0.008383693531356123]
	TIME [epoch: 8.24 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009494350006121397		[learning rate: 3.4491e-05]
		[batch 20/20] avg loss: -0.006451219057225535		[learning rate: 3.4429e-05]
	Learning Rate: 3.44287e-05
	LOSS [training: -0.00797278453167347 | validation: -0.01204299444619235]
	TIME [epoch: 8.28 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010242832307097388		[learning rate: 3.4366e-05]
		[batch 20/20] avg loss: -0.0083665727243496		[learning rate: 3.4304e-05]
	Learning Rate: 3.43038e-05
	LOSS [training: -0.009304702515723493 | validation: -0.013811061452208735]
	TIME [epoch: 8.3 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010598920949205927		[learning rate: 3.4241e-05]
		[batch 20/20] avg loss: -0.006157930961012028		[learning rate: 3.4179e-05]
	Learning Rate: 3.41793e-05
	LOSS [training: -0.008378425955108978 | validation: -0.007250757179644244]
	TIME [epoch: 8.26 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005771884177755252		[learning rate: 3.4117e-05]
		[batch 20/20] avg loss: -0.009046992323853245		[learning rate: 3.4055e-05]
	Learning Rate: 3.40553e-05
	LOSS [training: -0.007409438250804248 | validation: -0.011881909169780094]
	TIME [epoch: 8.25 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007354200380164169		[learning rate: 3.3993e-05]
		[batch 20/20] avg loss: -0.011397491198740659		[learning rate: 3.3932e-05]
	Learning Rate: 3.39317e-05
	LOSS [training: -0.009375845789452416 | validation: -0.00842941030413036]
	TIME [epoch: 8.28 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036886774067421205		[learning rate: 3.387e-05]
		[batch 20/20] avg loss: -0.008616659718146073		[learning rate: 3.3809e-05]
	Learning Rate: 3.38085e-05
	LOSS [training: -0.0061526685624440965 | validation: -0.007014155344848754]
	TIME [epoch: 8.28 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009158492348263991		[learning rate: 3.3747e-05]
		[batch 20/20] avg loss: -0.00894953822616817		[learning rate: 3.3686e-05]
	Learning Rate: 3.36858e-05
	LOSS [training: -0.00905401528721608 | validation: -0.0036824859351934835]
	TIME [epoch: 8.26 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013352842676393816		[learning rate: 3.3625e-05]
		[batch 20/20] avg loss: -0.00887078389783866		[learning rate: 3.3564e-05]
	Learning Rate: 3.35636e-05
	LOSS [training: -0.01111181328711624 | validation: -0.007504291032617469]
	TIME [epoch: 8.25 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013938321304987178		[learning rate: 3.3503e-05]
		[batch 20/20] avg loss: -0.002204210553718676		[learning rate: 3.3442e-05]
	Learning Rate: 3.34418e-05
	LOSS [training: -0.008071265929352926 | validation: -0.007940891740225537]
	TIME [epoch: 8.29 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009148394619802021		[learning rate: 3.3381e-05]
		[batch 20/20] avg loss: -0.0057582765108404755		[learning rate: 3.332e-05]
	Learning Rate: 3.33204e-05
	LOSS [training: -0.007453335565321249 | validation: -0.007796254793896779]
	TIME [epoch: 8.27 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007993110450134584		[learning rate: 3.326e-05]
		[batch 20/20] avg loss: -0.005641956802374336		[learning rate: 3.32e-05]
	Learning Rate: 3.31995e-05
	LOSS [training: -0.006817533626254463 | validation: -0.013257808004882335]
	TIME [epoch: 8.28 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004702870305757773		[learning rate: 3.3139e-05]
		[batch 20/20] avg loss: -0.008619841091502626		[learning rate: 3.3079e-05]
	Learning Rate: 3.3079e-05
	LOSS [training: -0.0066613556986302004 | validation: -0.001724597370846678]
	TIME [epoch: 8.25 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003440397485231013		[learning rate: 3.3019e-05]
		[batch 20/20] avg loss: -0.010698890615886558		[learning rate: 3.2959e-05]
	Learning Rate: 3.2959e-05
	LOSS [training: -0.007069644050558786 | validation: -0.015342482033307049]
	TIME [epoch: 8.29 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007703659081503782		[learning rate: 3.2899e-05]
		[batch 20/20] avg loss: -0.007696238595754376		[learning rate: 3.2839e-05]
	Learning Rate: 3.28394e-05
	LOSS [training: -0.007699948838629079 | validation: -0.012005708335289971]
	TIME [epoch: 8.26 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006611678251884035		[learning rate: 3.278e-05]
		[batch 20/20] avg loss: -0.009447745066046228		[learning rate: 3.272e-05]
	Learning Rate: 3.27202e-05
	LOSS [training: -0.008029711658965133 | validation: -0.011421744454677104]
	TIME [epoch: 8.28 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011715407913717005		[learning rate: 3.2661e-05]
		[batch 20/20] avg loss: -0.008654889485043471		[learning rate: 3.2601e-05]
	Learning Rate: 3.26014e-05
	LOSS [training: -0.010185148699380238 | validation: -0.012251315126539479]
	TIME [epoch: 8.26 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012370806044087811		[learning rate: 3.2542e-05]
		[batch 20/20] avg loss: -0.0035700311010323027		[learning rate: 3.2483e-05]
	Learning Rate: 3.24831e-05
	LOSS [training: -0.007970418572560057 | validation: -0.007003354282781124]
	TIME [epoch: 8.3 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012353849263162643		[learning rate: 3.2424e-05]
		[batch 20/20] avg loss: -0.004054000473113694		[learning rate: 3.2365e-05]
	Learning Rate: 3.23652e-05
	LOSS [training: -0.008203924868138169 | validation: -0.012961527220284003]
	TIME [epoch: 8.26 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013060787745014808		[learning rate: 3.2306e-05]
		[batch 20/20] avg loss: -0.008234349637124296		[learning rate: 3.2248e-05]
	Learning Rate: 3.22478e-05
	LOSS [training: -0.010647568691069554 | validation: -0.012339140194767572]
	TIME [epoch: 8.27 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006633593125146662		[learning rate: 3.2189e-05]
		[batch 20/20] avg loss: -0.009373310331194703		[learning rate: 3.2131e-05]
	Learning Rate: 3.21308e-05
	LOSS [training: -0.008003451728170682 | validation: -0.012275505341190556]
	TIME [epoch: 8.26 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008095356829920719		[learning rate: 3.2072e-05]
		[batch 20/20] avg loss: -0.00938205625445403		[learning rate: 3.2014e-05]
	Learning Rate: 3.20142e-05
	LOSS [training: -0.008738706542187374 | validation: -0.015344125600611807]
	TIME [epoch: 8.29 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00877681096502581		[learning rate: 3.1956e-05]
		[batch 20/20] avg loss: -0.008549808048980983		[learning rate: 3.1898e-05]
	Learning Rate: 3.1898e-05
	LOSS [training: -0.008663309507003397 | validation: -0.0088472441279334]
	TIME [epoch: 8.25 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008779162315700917		[learning rate: 3.184e-05]
		[batch 20/20] avg loss: -0.013431950316552222		[learning rate: 3.1782e-05]
	Learning Rate: 3.17822e-05
	LOSS [training: -0.011105556316126567 | validation: -0.015334866347189537]
	TIME [epoch: 8.25 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006075174007560638		[learning rate: 3.1725e-05]
		[batch 20/20] avg loss: -0.01057399984138197		[learning rate: 3.1667e-05]
	Learning Rate: 3.16669e-05
	LOSS [training: -0.008324586924471303 | validation: -0.0046570257561259895]
	TIME [epoch: 8.29 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009912894626287617		[learning rate: 3.1609e-05]
		[batch 20/20] avg loss: -0.006020523642851239		[learning rate: 3.1552e-05]
	Learning Rate: 3.1552e-05
	LOSS [training: -0.007966709134569427 | validation: -0.013921837525653558]
	TIME [epoch: 8.29 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0060030252019010934		[learning rate: 3.1495e-05]
		[batch 20/20] avg loss: -0.007734085934141576		[learning rate: 3.1437e-05]
	Learning Rate: 3.14375e-05
	LOSS [training: -0.006868555568021334 | validation: -0.01240708792478569]
	TIME [epoch: 8.25 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010017372471648267		[learning rate: 3.138e-05]
		[batch 20/20] avg loss: -0.013096052598918181		[learning rate: 3.1323e-05]
	Learning Rate: 3.13234e-05
	LOSS [training: -0.011556712535283224 | validation: -0.012479211267077106]
	TIME [epoch: 8.24 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010072162145561826		[learning rate: 3.1266e-05]
		[batch 20/20] avg loss: -0.0077306603167995565		[learning rate: 3.121e-05]
	Learning Rate: 3.12097e-05
	LOSS [training: -0.00890141123118069 | validation: -0.012993135075222567]
	TIME [epoch: 8.27 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00845020763522809		[learning rate: 3.1153e-05]
		[batch 20/20] avg loss: -0.005994837431204108		[learning rate: 3.1096e-05]
	Learning Rate: 3.10964e-05
	LOSS [training: -0.007222522533216098 | validation: -0.009343543158211573]
	TIME [epoch: 8.29 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00822690808730208		[learning rate: 3.104e-05]
		[batch 20/20] avg loss: -0.007896547756559548		[learning rate: 3.0984e-05]
	Learning Rate: 3.09836e-05
	LOSS [training: -0.008061727921930816 | validation: -0.013023289684528145]
	TIME [epoch: 8.27 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009847966351992365		[learning rate: 3.0927e-05]
		[batch 20/20] avg loss: -0.007520991826980639		[learning rate: 3.0871e-05]
	Learning Rate: 3.08711e-05
	LOSS [training: -0.008684479089486502 | validation: -0.013937038271799942]
	TIME [epoch: 8.26 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007604854824995964		[learning rate: 3.0815e-05]
		[batch 20/20] avg loss: -0.002048778356733977		[learning rate: 3.0759e-05]
	Learning Rate: 3.07591e-05
	LOSS [training: -0.004826816590864971 | validation: -0.014889426593180393]
	TIME [epoch: 8.25 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008490583568745496		[learning rate: 3.0703e-05]
		[batch 20/20] avg loss: -0.007095705019044871		[learning rate: 3.0647e-05]
	Learning Rate: 3.06475e-05
	LOSS [training: -0.007793144293895185 | validation: -0.014999380763276343]
	TIME [epoch: 8.31 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009173580917761751		[learning rate: 3.0592e-05]
		[batch 20/20] avg loss: -0.012210091948665885		[learning rate: 3.0536e-05]
	Learning Rate: 3.05363e-05
	LOSS [training: -0.010691836433213819 | validation: -0.011708636025975905]
	TIME [epoch: 8.25 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009066919139517906		[learning rate: 3.0481e-05]
		[batch 20/20] avg loss: -0.00962727562280414		[learning rate: 3.0425e-05]
	Learning Rate: 3.04254e-05
	LOSS [training: -0.009347097381161022 | validation: -0.016862527083054864]
	TIME [epoch: 8.24 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010146716252663831		[learning rate: 3.037e-05]
		[batch 20/20] avg loss: -0.010687948230490008		[learning rate: 3.0315e-05]
	Learning Rate: 3.0315e-05
	LOSS [training: -0.010417332241576918 | validation: -0.017973906759886665]
	TIME [epoch: 8.25 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009868014983253664		[learning rate: 3.026e-05]
		[batch 20/20] avg loss: -0.003099763979213483		[learning rate: 3.0205e-05]
	Learning Rate: 3.0205e-05
	LOSS [training: -0.0064838894812335734 | validation: -0.010235036158196114]
	TIME [epoch: 8.32 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008747744936297406		[learning rate: 3.015e-05]
		[batch 20/20] avg loss: -0.008744819527408281		[learning rate: 3.0095e-05]
	Learning Rate: 3.00954e-05
	LOSS [training: -0.008746282231852842 | validation: -0.0057397661233561745]
	TIME [epoch: 8.25 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0073751874466629185		[learning rate: 3.0041e-05]
		[batch 20/20] avg loss: -0.011181800637982922		[learning rate: 2.9986e-05]
	Learning Rate: 2.99862e-05
	LOSS [training: -0.00927849404232292 | validation: -0.008325651010174526]
	TIME [epoch: 8.25 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01064621762030263		[learning rate: 2.9932e-05]
		[batch 20/20] avg loss: -0.0054199317541845345		[learning rate: 2.9877e-05]
	Learning Rate: 2.98774e-05
	LOSS [training: -0.008033074687243583 | validation: -0.009081706961512854]
	TIME [epoch: 8.26 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009708877856883343		[learning rate: 2.9823e-05]
		[batch 20/20] avg loss: -0.011743097277680876		[learning rate: 2.9769e-05]
	Learning Rate: 2.97689e-05
	LOSS [training: -0.010725987567282111 | validation: -0.01166050811248984]
	TIME [epoch: 8.31 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038886143353737725		[learning rate: 2.9715e-05]
		[batch 20/20] avg loss: -0.009682071110607992		[learning rate: 2.9661e-05]
	Learning Rate: 2.96609e-05
	LOSS [training: -0.006785342722990884 | validation: -0.012369989051125435]
	TIME [epoch: 8.26 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012817119906689304		[learning rate: 2.9607e-05]
		[batch 20/20] avg loss: -0.0066004693221817625		[learning rate: 2.9553e-05]
	Learning Rate: 2.95533e-05
	LOSS [training: -0.009708794614435532 | validation: -0.009201676758280018]
	TIME [epoch: 8.24 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01024820560650341		[learning rate: 2.95e-05]
		[batch 20/20] avg loss: -0.005385952698542274		[learning rate: 2.9446e-05]
	Learning Rate: 2.9446e-05
	LOSS [training: -0.007817079152522843 | validation: -0.010343401895029261]
	TIME [epoch: 8.27 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007451059781767108		[learning rate: 2.9393e-05]
		[batch 20/20] avg loss: -0.006659868845851029		[learning rate: 2.9339e-05]
	Learning Rate: 2.93391e-05
	LOSS [training: -0.007055464313809068 | validation: -0.015577681169404894]
	TIME [epoch: 8.28 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00775399490119962		[learning rate: 2.9286e-05]
		[batch 20/20] avg loss: -0.010772678217372449		[learning rate: 2.9233e-05]
	Learning Rate: 2.92327e-05
	LOSS [training: -0.009263336559286036 | validation: -0.010769342609249569]
	TIME [epoch: 8.28 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.015086791991684218		[learning rate: 2.918e-05]
		[batch 20/20] avg loss: -0.007776745544740224		[learning rate: 2.9127e-05]
	Learning Rate: 2.91266e-05
	LOSS [training: -0.01143176876821222 | validation: -0.01722142345215818]
	TIME [epoch: 8.25 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0018821753368502962		[learning rate: 2.9074e-05]
		[batch 20/20] avg loss: -0.01086995502628456		[learning rate: 2.9021e-05]
	Learning Rate: 2.90209e-05
	LOSS [training: -0.006376065181567428 | validation: -0.011884816365088792]
	TIME [epoch: 8.27 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01358611278739329		[learning rate: 2.8968e-05]
		[batch 20/20] avg loss: -0.0070443544696637715		[learning rate: 2.8916e-05]
	Learning Rate: 2.89156e-05
	LOSS [training: -0.01031523362852853 | validation: -0.01787981774553945]
	TIME [epoch: 8.27 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0040913934579947546		[learning rate: 2.8863e-05]
		[batch 20/20] avg loss: -0.012099660236635614		[learning rate: 2.8811e-05]
	Learning Rate: 2.88106e-05
	LOSS [training: -0.008095526847315184 | validation: -0.009133076411175597]
	TIME [epoch: 8.28 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006701658135371177		[learning rate: 2.8758e-05]
		[batch 20/20] avg loss: -0.012545442339498312		[learning rate: 2.8706e-05]
	Learning Rate: 2.87061e-05
	LOSS [training: -0.009623550237434745 | validation: -0.011497220147502773]
	TIME [epoch: 8.26 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004338132322375644		[learning rate: 2.8654e-05]
		[batch 20/20] avg loss: -0.010983076931772282		[learning rate: 2.8602e-05]
	Learning Rate: 2.86019e-05
	LOSS [training: -0.007660604627073962 | validation: -0.011092418038686052]
	TIME [epoch: 8.27 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006797501285976777		[learning rate: 2.855e-05]
		[batch 20/20] avg loss: -0.012053134910824883		[learning rate: 2.8498e-05]
	Learning Rate: 2.84981e-05
	LOSS [training: -0.009425318098400828 | validation: -0.008875554939599937]
	TIME [epoch: 8.28 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012564696582699052		[learning rate: 2.8446e-05]
		[batch 20/20] avg loss: -0.01159883916724346		[learning rate: 2.8395e-05]
	Learning Rate: 2.83947e-05
	LOSS [training: -0.012081767874971257 | validation: -0.013284820949002315]
	TIME [epoch: 8.26 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010480054286462885		[learning rate: 2.8343e-05]
		[batch 20/20] avg loss: -0.007103933852645719		[learning rate: 2.8292e-05]
	Learning Rate: 2.82916e-05
	LOSS [training: -0.0087919940695543 | validation: -0.006707242077626295]
	TIME [epoch: 8.26 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008853419222140417		[learning rate: 2.824e-05]
		[batch 20/20] avg loss: -0.006725690793466074		[learning rate: 2.8189e-05]
	Learning Rate: 2.8189e-05
	LOSS [training: -0.007789555007803246 | validation: -0.012099881301554979]
	TIME [epoch: 8.28 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007693540654044609		[learning rate: 2.8138e-05]
		[batch 20/20] avg loss: -0.010630765227826161		[learning rate: 2.8087e-05]
	Learning Rate: 2.80867e-05
	LOSS [training: -0.009162152940935387 | validation: -0.01401670045305083]
	TIME [epoch: 8.26 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007793903347965278		[learning rate: 2.8036e-05]
		[batch 20/20] avg loss: -0.006857014187476853		[learning rate: 2.7985e-05]
	Learning Rate: 2.79847e-05
	LOSS [training: -0.0073254587677210655 | validation: -0.01725866048169284]
	TIME [epoch: 8.25 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004542290216062731		[learning rate: 2.7934e-05]
		[batch 20/20] avg loss: -0.00775184816083641		[learning rate: 2.7883e-05]
	Learning Rate: 2.78832e-05
	LOSS [training: -0.00614706918844957 | validation: -0.016127270871848415]
	TIME [epoch: 8.29 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007384128177939921		[learning rate: 2.7833e-05]
		[batch 20/20] avg loss: -0.009212613356457722		[learning rate: 2.7782e-05]
	Learning Rate: 2.7782e-05
	LOSS [training: -0.008298370767198821 | validation: -0.01075838936243173]
	TIME [epoch: 8.29 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011335808730011334		[learning rate: 2.7732e-05]
		[batch 20/20] avg loss: -0.0030629747990041215		[learning rate: 2.7681e-05]
	Learning Rate: 2.76812e-05
	LOSS [training: -0.007199391764507727 | validation: -0.01640604344652717]
	TIME [epoch: 8.26 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006334750919128525		[learning rate: 2.7631e-05]
		[batch 20/20] avg loss: -0.009376191264407764		[learning rate: 2.7581e-05]
	Learning Rate: 2.75807e-05
	LOSS [training: -0.007855471091768144 | validation: -0.015405371240369923]
	TIME [epoch: 8.25 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004056051304161905		[learning rate: 2.7531e-05]
		[batch 20/20] avg loss: -0.012108146560560355		[learning rate: 2.7481e-05]
	Learning Rate: 2.74806e-05
	LOSS [training: -0.008082098932361129 | validation: -0.014548401758113077]
	TIME [epoch: 8.27 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010497982035585851		[learning rate: 2.7431e-05]
		[batch 20/20] avg loss: -0.006064835580826941		[learning rate: 2.7381e-05]
	Learning Rate: 2.73809e-05
	LOSS [training: -0.008281408808206395 | validation: -0.012131289911312716]
	TIME [epoch: 8.3 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00792394718488588		[learning rate: 2.7331e-05]
		[batch 20/20] avg loss: -0.008183901466481814		[learning rate: 2.7282e-05]
	Learning Rate: 2.72815e-05
	LOSS [training: -0.008053924325683847 | validation: -0.01268793124521685]
	TIME [epoch: 8.27 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004885841473463124		[learning rate: 2.7232e-05]
		[batch 20/20] avg loss: -0.008952629149004706		[learning rate: 2.7183e-05]
	Learning Rate: 2.71825e-05
	LOSS [training: -0.006919235311233915 | validation: -0.009087965182767561]
	TIME [epoch: 8.26 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0107805498845558		[learning rate: 2.7133e-05]
		[batch 20/20] avg loss: -0.011308069298634208		[learning rate: 2.7084e-05]
	Learning Rate: 2.70839e-05
	LOSS [training: -0.011044309591595003 | validation: -0.010624721558687662]
	TIME [epoch: 8.27 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008489480091984549		[learning rate: 2.7035e-05]
		[batch 20/20] avg loss: -0.006944771426619126		[learning rate: 2.6986e-05]
	Learning Rate: 2.69856e-05
	LOSS [training: -0.007717125759301838 | validation: -0.010990311638063122]
	TIME [epoch: 8.31 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010480984587312085		[learning rate: 2.6937e-05]
		[batch 20/20] avg loss: -0.00798455016235615		[learning rate: 2.6888e-05]
	Learning Rate: 2.68876e-05
	LOSS [training: -0.00923276737483412 | validation: -0.015234472399541694]
	TIME [epoch: 8.26 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010177172240858755		[learning rate: 2.6839e-05]
		[batch 20/20] avg loss: -0.00890837956843959		[learning rate: 2.679e-05]
	Learning Rate: 2.67901e-05
	LOSS [training: -0.009542775904649171 | validation: -0.007434032933016681]
	TIME [epoch: 8.26 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00741036118341572		[learning rate: 2.6741e-05]
		[batch 20/20] avg loss: -0.00810771792449554		[learning rate: 2.6693e-05]
	Learning Rate: 2.66928e-05
	LOSS [training: -0.007759039553955629 | validation: -0.007954588415962227]
	TIME [epoch: 8.26 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010335406435704287		[learning rate: 2.6644e-05]
		[batch 20/20] avg loss: -0.008113638126887326		[learning rate: 2.6596e-05]
	Learning Rate: 2.6596e-05
	LOSS [training: -0.009224522281295807 | validation: -0.009450991249329015]
	TIME [epoch: 8.32 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008818496031616588		[learning rate: 2.6548e-05]
		[batch 20/20] avg loss: -0.012484223324212099		[learning rate: 2.6499e-05]
	Learning Rate: 2.64994e-05
	LOSS [training: -0.01065135967791434 | validation: -0.011631021089330132]
	TIME [epoch: 8.26 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01113843269754524		[learning rate: 2.6451e-05]
		[batch 20/20] avg loss: -0.00868890013621845		[learning rate: 2.6403e-05]
	Learning Rate: 2.64033e-05
	LOSS [training: -0.009913666416881847 | validation: -0.009743289762936433]
	TIME [epoch: 8.24 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010989207895602596		[learning rate: 2.6355e-05]
		[batch 20/20] avg loss: -0.0067128725849358185		[learning rate: 2.6307e-05]
	Learning Rate: 2.63075e-05
	LOSS [training: -0.008851040240269208 | validation: -0.01145590172717993]
	TIME [epoch: 8.26 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008035685401323489		[learning rate: 2.626e-05]
		[batch 20/20] avg loss: -0.007423535862441199		[learning rate: 2.6212e-05]
	Learning Rate: 2.6212e-05
	LOSS [training: -0.007729610631882345 | validation: -0.016039545980105117]
	TIME [epoch: 8.33 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007536477935951184		[learning rate: 2.6164e-05]
		[batch 20/20] avg loss: -0.007683010249311473		[learning rate: 2.6117e-05]
	Learning Rate: 2.61169e-05
	LOSS [training: -0.0076097440926313304 | validation: -0.011400342477744102]
	TIME [epoch: 8.27 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006807285351087075		[learning rate: 2.6069e-05]
		[batch 20/20] avg loss: -0.006830093345536731		[learning rate: 2.6022e-05]
	Learning Rate: 2.60221e-05
	LOSS [training: -0.006818689348311905 | validation: -0.00932474489443564]
	TIME [epoch: 8.26 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005921409253070048		[learning rate: 2.5975e-05]
		[batch 20/20] avg loss: -0.006897906223604285		[learning rate: 2.5928e-05]
	Learning Rate: 2.59277e-05
	LOSS [training: -0.006409657738337167 | validation: -0.012377031293894732]
	TIME [epoch: 8.26 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011784903947373308		[learning rate: 2.5881e-05]
		[batch 20/20] avg loss: -0.00822359879213257		[learning rate: 2.5834e-05]
	Learning Rate: 2.58336e-05
	LOSS [training: -0.010004251369752939 | validation: -0.008740481643550803]
	TIME [epoch: 8.3 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00845681699378396		[learning rate: 2.5787e-05]
		[batch 20/20] avg loss: -0.00558876985540366		[learning rate: 2.574e-05]
	Learning Rate: 2.57398e-05
	LOSS [training: -0.007022793424593811 | validation: -0.013442039962558043]
	TIME [epoch: 8.26 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006611088738704194		[learning rate: 2.5693e-05]
		[batch 20/20] avg loss: -0.009519712400128989		[learning rate: 2.5646e-05]
	Learning Rate: 2.56464e-05
	LOSS [training: -0.00806540056941659 | validation: -0.016813831931107352]
	TIME [epoch: 8.25 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010783438985390003		[learning rate: 2.56e-05]
		[batch 20/20] avg loss: -0.009381233887189597		[learning rate: 2.5553e-05]
	Learning Rate: 2.55533e-05
	LOSS [training: -0.0100823364362898 | validation: -0.008475515133193895]
	TIME [epoch: 8.27 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00978457868381621		[learning rate: 2.5507e-05]
		[batch 20/20] avg loss: -0.012516160136018594		[learning rate: 2.5461e-05]
	Learning Rate: 2.54606e-05
	LOSS [training: -0.011150369409917402 | validation: -0.014463191737017617]
	TIME [epoch: 8.28 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005743783985710893		[learning rate: 2.5414e-05]
		[batch 20/20] avg loss: -0.01086507898115371		[learning rate: 2.5368e-05]
	Learning Rate: 2.53682e-05
	LOSS [training: -0.008304431483432302 | validation: -0.014425149158122312]
	TIME [epoch: 8.29 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.015264202326486967		[learning rate: 2.5322e-05]
		[batch 20/20] avg loss: -0.009464739423486444		[learning rate: 2.5276e-05]
	Learning Rate: 2.52761e-05
	LOSS [training: -0.012364470874986704 | validation: -0.013416558379537525]
	TIME [epoch: 8.26 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004094820339677861		[learning rate: 2.523e-05]
		[batch 20/20] avg loss: -0.010531343265171519		[learning rate: 2.5184e-05]
	Learning Rate: 2.51844e-05
	LOSS [training: -0.00731308180242469 | validation: -0.018548780989995538]
	TIME [epoch: 8.27 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008684853871171905		[learning rate: 2.5139e-05]
		[batch 20/20] avg loss: -0.007137423790290996		[learning rate: 2.5093e-05]
	Learning Rate: 2.5093e-05
	LOSS [training: -0.00791113883073145 | validation: -0.017406273980352924]
	TIME [epoch: 8.29 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00629360208166405		[learning rate: 2.5047e-05]
		[batch 20/20] avg loss: -0.008502187434629634		[learning rate: 2.5002e-05]
	Learning Rate: 2.50019e-05
	LOSS [training: -0.007397894758146842 | validation: -0.009942082351515292]
	TIME [epoch: 8.28 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007934083521022164		[learning rate: 2.4957e-05]
		[batch 20/20] avg loss: -0.010510975398340155		[learning rate: 2.4911e-05]
	Learning Rate: 2.49112e-05
	LOSS [training: -0.009222529459681161 | validation: -0.010759421498291516]
	TIME [epoch: 8.27 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006597623348606227		[learning rate: 2.4866e-05]
		[batch 20/20] avg loss: -0.01304894500329765		[learning rate: 2.4821e-05]
	Learning Rate: 2.48208e-05
	LOSS [training: -0.009823284175951939 | validation: -0.011233163532751363]
	TIME [epoch: 8.29 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009799117214271843		[learning rate: 2.4776e-05]
		[batch 20/20] avg loss: -0.009110355020116655		[learning rate: 2.4731e-05]
	Learning Rate: 2.47307e-05
	LOSS [training: -0.009454736117194251 | validation: -0.011347853679096704]
	TIME [epoch: 8.28 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01168652741093405		[learning rate: 2.4686e-05]
		[batch 20/20] avg loss: -0.002995798601037474		[learning rate: 2.4641e-05]
	Learning Rate: 2.4641e-05
	LOSS [training: -0.007341163005985761 | validation: -0.010627313691940192]
	TIME [epoch: 8.27 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006864839662298411		[learning rate: 2.4596e-05]
		[batch 20/20] avg loss: -0.010533488718797017		[learning rate: 2.4552e-05]
	Learning Rate: 2.45516e-05
	LOSS [training: -0.008699164190547715 | validation: -0.011304902123351992]
	TIME [epoch: 8.3 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013020555802228998		[learning rate: 2.4507e-05]
		[batch 20/20] avg loss: -0.008406057078674709		[learning rate: 2.4462e-05]
	Learning Rate: 2.44625e-05
	LOSS [training: -0.010713306440451853 | validation: -0.004020859595270556]
	TIME [epoch: 8.29 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007736053425296603		[learning rate: 2.4418e-05]
		[batch 20/20] avg loss: -0.007745925749565183		[learning rate: 2.4374e-05]
	Learning Rate: 2.43737e-05
	LOSS [training: -0.007740989587430892 | validation: -0.016995845434377307]
	TIME [epoch: 8.29 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0070605042820713875		[learning rate: 2.4329e-05]
		[batch 20/20] avg loss: -0.011579761397709798		[learning rate: 2.4285e-05]
	Learning Rate: 2.42852e-05
	LOSS [training: -0.009320132839890593 | validation: -0.011041080683824577]
	TIME [epoch: 8.25 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00975921650013072		[learning rate: 2.4241e-05]
		[batch 20/20] avg loss: -0.008380747845568906		[learning rate: 2.4197e-05]
	Learning Rate: 2.41971e-05
	LOSS [training: -0.009069982172849812 | validation: -0.010523441156282206]
	TIME [epoch: 8.29 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012046983432777561		[learning rate: 2.4153e-05]
		[batch 20/20] avg loss: -0.007591749396434928		[learning rate: 2.4109e-05]
	Learning Rate: 2.41093e-05
	LOSS [training: -0.009819366414606243 | validation: -0.010292418463714998]
	TIME [epoch: 8.3 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006536166198802276		[learning rate: 2.4065e-05]
		[batch 20/20] avg loss: -0.013884610479963139		[learning rate: 2.4022e-05]
	Learning Rate: 2.40218e-05
	LOSS [training: -0.010210388339382708 | validation: -0.011830726357296236]
	TIME [epoch: 8.28 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011968154627025971		[learning rate: 2.3978e-05]
		[batch 20/20] avg loss: -0.004874343596624033		[learning rate: 2.3935e-05]
	Learning Rate: 2.39346e-05
	LOSS [training: -0.008421249111825003 | validation: -0.01284064578370659]
	TIME [epoch: 8.26 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007474727714012594		[learning rate: 2.3891e-05]
		[batch 20/20] avg loss: -0.009529364162143433		[learning rate: 2.3848e-05]
	Learning Rate: 2.38477e-05
	LOSS [training: -0.008502045938078013 | validation: -0.011527099532523908]
	TIME [epoch: 8.28 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01161636362025427		[learning rate: 2.3804e-05]
		[batch 20/20] avg loss: -0.008820413967244891		[learning rate: 2.3761e-05]
	Learning Rate: 2.37612e-05
	LOSS [training: -0.01021838879374958 | validation: -0.012961183853986761]
	TIME [epoch: 8.31 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00870205536495459		[learning rate: 2.3718e-05]
		[batch 20/20] avg loss: -0.012395311866402442		[learning rate: 2.3675e-05]
	Learning Rate: 2.3675e-05
	LOSS [training: -0.010548683615678515 | validation: -0.014142703735592097]
	TIME [epoch: 8.27 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010482481231215748		[learning rate: 2.3632e-05]
		[batch 20/20] avg loss: -0.007702396385581578		[learning rate: 2.3589e-05]
	Learning Rate: 2.35891e-05
	LOSS [training: -0.009092438808398665 | validation: -0.014527688780282363]
	TIME [epoch: 8.26 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007933783568157866		[learning rate: 2.3546e-05]
		[batch 20/20] avg loss: -0.010627152133824928		[learning rate: 2.3503e-05]
	Learning Rate: 2.35034e-05
	LOSS [training: -0.009280467850991398 | validation: -0.012452886744022567]
	TIME [epoch: 8.26 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010615602448500083		[learning rate: 2.3461e-05]
		[batch 20/20] avg loss: -0.011354762358077275		[learning rate: 2.3418e-05]
	Learning Rate: 2.34182e-05
	LOSS [training: -0.01098518240328868 | validation: -0.010724276830878498]
	TIME [epoch: 8.34 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011251138711501147		[learning rate: 2.3376e-05]
		[batch 20/20] avg loss: -0.010066716672825517		[learning rate: 2.3333e-05]
	Learning Rate: 2.33332e-05
	LOSS [training: -0.010658927692163332 | validation: -0.0137126909925995]
	TIME [epoch: 8.26 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010677175353274268		[learning rate: 2.3291e-05]
		[batch 20/20] avg loss: -0.013491276441175773		[learning rate: 2.3248e-05]
	Learning Rate: 2.32485e-05
	LOSS [training: -0.01208422589722502 | validation: -0.011458290600400033]
	TIME [epoch: 8.26 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00848762551573671		[learning rate: 2.3206e-05]
		[batch 20/20] avg loss: -0.008846929552816303		[learning rate: 2.3164e-05]
	Learning Rate: 2.31641e-05
	LOSS [training: -0.008667277534276504 | validation: -0.016811596567758404]
	TIME [epoch: 8.26 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012405850113242006		[learning rate: 2.3122e-05]
		[batch 20/20] avg loss: -0.0036812711669055006		[learning rate: 2.308e-05]
	Learning Rate: 2.30801e-05
	LOSS [training: -0.008043560640073753 | validation: -0.011122804919125914]
	TIME [epoch: 8.33 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010613899544599802		[learning rate: 2.3038e-05]
		[batch 20/20] avg loss: -0.007022246479585025		[learning rate: 2.2996e-05]
	Learning Rate: 2.29963e-05
	LOSS [training: -0.008818073012092415 | validation: -0.010932489560919089]
	TIME [epoch: 8.27 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009487517732936771		[learning rate: 2.2955e-05]
		[batch 20/20] avg loss: -0.0042626661278210385		[learning rate: 2.2913e-05]
	Learning Rate: 2.29128e-05
	LOSS [training: -0.006875091930378904 | validation: -0.013508946802031963]
	TIME [epoch: 8.26 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0073802137239924485		[learning rate: 2.2871e-05]
		[batch 20/20] avg loss: -0.00846387259169344		[learning rate: 2.283e-05]
	Learning Rate: 2.28297e-05
	LOSS [training: -0.007922043157842945 | validation: -0.012329325894858405]
	TIME [epoch: 8.28 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008120698762652267		[learning rate: 2.2788e-05]
		[batch 20/20] avg loss: -0.010417454665092163		[learning rate: 2.2747e-05]
	Learning Rate: 2.27468e-05
	LOSS [training: -0.009269076713872216 | validation: -0.009721397559309146]
	TIME [epoch: 8.31 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011234152666741427		[learning rate: 2.2706e-05]
		[batch 20/20] avg loss: -0.007752646438896839		[learning rate: 2.2664e-05]
	Learning Rate: 2.26643e-05
	LOSS [training: -0.00949339955281913 | validation: -0.016251492479621953]
	TIME [epoch: 8.27 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012544969635805725		[learning rate: 2.2623e-05]
		[batch 20/20] avg loss: -0.0053055120525206815		[learning rate: 2.2582e-05]
	Learning Rate: 2.2582e-05
	LOSS [training: -0.008925240844163206 | validation: -0.008781221325204107]
	TIME [epoch: 8.26 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011252009081521292		[learning rate: 2.2541e-05]
		[batch 20/20] avg loss: -0.009451626590320982		[learning rate: 2.25e-05]
	Learning Rate: 2.25001e-05
	LOSS [training: -0.010351817835921135 | validation: -0.011907289589159226]
	TIME [epoch: 8.29 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0068919166362708395		[learning rate: 2.2459e-05]
		[batch 20/20] avg loss: -0.009817408653017725		[learning rate: 2.2418e-05]
	Learning Rate: 2.24184e-05
	LOSS [training: -0.008354662644644283 | validation: -0.008467934197131415]
	TIME [epoch: 8.29 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009781489621358061		[learning rate: 2.2378e-05]
		[batch 20/20] avg loss: -0.010841492338061632		[learning rate: 2.2337e-05]
	Learning Rate: 2.23371e-05
	LOSS [training: -0.010311490979709848 | validation: -0.008201089291067658]
	TIME [epoch: 8.29 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010313439682411956		[learning rate: 2.2297e-05]
		[batch 20/20] avg loss: -0.01050166164291783		[learning rate: 2.2256e-05]
	Learning Rate: 2.2256e-05
	LOSS [training: -0.010407550662664893 | validation: -0.019679931788601522]
	TIME [epoch: 8.26 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008619414067550998		[learning rate: 2.2216e-05]
		[batch 20/20] avg loss: -0.006676021445341162		[learning rate: 2.2175e-05]
	Learning Rate: 2.21753e-05
	LOSS [training: -0.00764771775644608 | validation: -0.007506316365577323]
	TIME [epoch: 8.29 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01292699790021585		[learning rate: 2.2135e-05]
		[batch 20/20] avg loss: -0.011289190723823912		[learning rate: 2.2095e-05]
	Learning Rate: 2.20948e-05
	LOSS [training: -0.01210809431201988 | validation: -0.013505347288876009]
	TIME [epoch: 8.28 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006600318672280706		[learning rate: 2.2055e-05]
		[batch 20/20] avg loss: -0.009251934765056273		[learning rate: 2.2015e-05]
	Learning Rate: 2.20146e-05
	LOSS [training: -0.007926126718668488 | validation: -0.015459150250810282]
	TIME [epoch: 8.29 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012123225469243496		[learning rate: 2.1975e-05]
		[batch 20/20] avg loss: -0.007926958872524962		[learning rate: 2.1935e-05]
	Learning Rate: 2.19347e-05
	LOSS [training: -0.010025092170884226 | validation: -0.01847328614432988]
	TIME [epoch: 8.26 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009749739800064766		[learning rate: 2.1895e-05]
		[batch 20/20] avg loss: -0.00757879578812726		[learning rate: 2.1855e-05]
	Learning Rate: 2.18551e-05
	LOSS [training: -0.008664267794096014 | validation: -0.01767873908983569]
	TIME [epoch: 8.3 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010855991740700491		[learning rate: 2.1815e-05]
		[batch 20/20] avg loss: -0.009299594733646572		[learning rate: 2.1776e-05]
	Learning Rate: 2.17758e-05
	LOSS [training: -0.01007779323717353 | validation: -0.011133526814286402]
	TIME [epoch: 8.27 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012554724358129413		[learning rate: 2.1736e-05]
		[batch 20/20] avg loss: -0.010222670671437645		[learning rate: 2.1697e-05]
	Learning Rate: 2.16968e-05
	LOSS [training: -0.01138869751478353 | validation: -0.016583637396473644]
	TIME [epoch: 8.26 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00959531272195748		[learning rate: 2.1657e-05]
		[batch 20/20] avg loss: -0.010517715706893608		[learning rate: 2.1618e-05]
	Learning Rate: 2.1618e-05
	LOSS [training: -0.010056514214425544 | validation: -0.011368597432772149]
	TIME [epoch: 8.28 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01762317694432747		[learning rate: 2.1579e-05]
		[batch 20/20] avg loss: -0.009181177214804572		[learning rate: 2.154e-05]
	Learning Rate: 2.15396e-05
	LOSS [training: -0.013402177079566022 | validation: -0.008326345328602009]
	TIME [epoch: 8.3 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006244669466049437		[learning rate: 2.15e-05]
		[batch 20/20] avg loss: -0.009496895897768063		[learning rate: 2.1461e-05]
	Learning Rate: 2.14614e-05
	LOSS [training: -0.007870782681908748 | validation: -0.01007540110774511]
	TIME [epoch: 8.26 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008154748837490679		[learning rate: 2.1422e-05]
		[batch 20/20] avg loss: -0.011483168912793228		[learning rate: 2.1384e-05]
	Learning Rate: 2.13835e-05
	LOSS [training: -0.009818958875141955 | validation: -0.01558593365776275]
	TIME [epoch: 8.26 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005059066273797248		[learning rate: 2.1345e-05]
		[batch 20/20] avg loss: -0.011797351251964166		[learning rate: 2.1306e-05]
	Learning Rate: 2.13059e-05
	LOSS [training: -0.00842820876288071 | validation: -0.01138726011070187]
	TIME [epoch: 8.29 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006350885870026161		[learning rate: 2.1267e-05]
		[batch 20/20] avg loss: -0.00678697852147124		[learning rate: 2.1229e-05]
	Learning Rate: 2.12286e-05
	LOSS [training: -0.0065689321957487 | validation: -0.010643127986716709]
	TIME [epoch: 8.3 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00981759829791073		[learning rate: 2.119e-05]
		[batch 20/20] avg loss: -0.01008586900048032		[learning rate: 2.1152e-05]
	Learning Rate: 2.11515e-05
	LOSS [training: -0.009951733649195525 | validation: -0.014720638245457607]
	TIME [epoch: 8.26 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012306753851565681		[learning rate: 2.1113e-05]
		[batch 20/20] avg loss: -0.00620761063855116		[learning rate: 2.1075e-05]
	Learning Rate: 2.10748e-05
	LOSS [training: -0.00925718224505842 | validation: -0.008281053962502442]
	TIME [epoch: 8.26 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014326863878270433		[learning rate: 2.1037e-05]
		[batch 20/20] avg loss: -0.006962571861526984		[learning rate: 2.0998e-05]
	Learning Rate: 2.09983e-05
	LOSS [training: -0.01064471786989871 | validation: -0.009630255799824137]
	TIME [epoch: 8.28 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008280100661600436		[learning rate: 2.096e-05]
		[batch 20/20] avg loss: -0.009762013002184553		[learning rate: 2.0922e-05]
	Learning Rate: 2.09221e-05
	LOSS [training: -0.009021056831892494 | validation: -0.01798276948066374]
	TIME [epoch: 8.31 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005641164051740278		[learning rate: 2.0884e-05]
		[batch 20/20] avg loss: -0.011241028210381384		[learning rate: 2.0846e-05]
	Learning Rate: 2.08462e-05
	LOSS [training: -0.00844109613106083 | validation: -0.012500507254928588]
	TIME [epoch: 8.26 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006036204445436963		[learning rate: 2.0808e-05]
		[batch 20/20] avg loss: -0.008318191898479037		[learning rate: 2.0771e-05]
	Learning Rate: 2.07705e-05
	LOSS [training: -0.007177198171957999 | validation: -0.01882076491422809]
	TIME [epoch: 8.27 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009519821467091297		[learning rate: 2.0733e-05]
		[batch 20/20] avg loss: -0.009770005790246462		[learning rate: 2.0695e-05]
	Learning Rate: 2.06951e-05
	LOSS [training: -0.00964491362866888 | validation: -0.013105181061502533]
	TIME [epoch: 8.28 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004906743716624454		[learning rate: 2.0658e-05]
		[batch 20/20] avg loss: -0.010372651197233988		[learning rate: 2.062e-05]
	Learning Rate: 2.062e-05
	LOSS [training: -0.00763969745692922 | validation: -0.019853924217115914]
	TIME [epoch: 8.31 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009552414189496436		[learning rate: 2.0583e-05]
		[batch 20/20] avg loss: -0.009988511321671716		[learning rate: 2.0545e-05]
	Learning Rate: 2.05452e-05
	LOSS [training: -0.009770462755584075 | validation: -0.012264592564547587]
	TIME [epoch: 8.26 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014952359608586322		[learning rate: 2.0508e-05]
		[batch 20/20] avg loss: -0.009216830311732625		[learning rate: 2.0471e-05]
	Learning Rate: 2.04706e-05
	LOSS [training: -0.012084594960159472 | validation: -0.007303159686216001]
	TIME [epoch: 8.25 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00810016120961705		[learning rate: 2.0433e-05]
		[batch 20/20] avg loss: -0.011807007413793568		[learning rate: 2.0396e-05]
	Learning Rate: 2.03964e-05
	LOSS [training: -0.009953584311705309 | validation: -0.00869986778796705]
	TIME [epoch: 8.29 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009867194982607056		[learning rate: 2.0359e-05]
		[batch 20/20] avg loss: -0.0043604122639778475		[learning rate: 2.0322e-05]
	Learning Rate: 2.03223e-05
	LOSS [training: -0.0071138036232924525 | validation: -0.013566423037051651]
	TIME [epoch: 8.3 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011505093687600427		[learning rate: 2.0285e-05]
		[batch 20/20] avg loss: -0.008466610001429531		[learning rate: 2.0249e-05]
	Learning Rate: 2.02486e-05
	LOSS [training: -0.00998585184451498 | validation: -0.012831248598920722]
	TIME [epoch: 8.26 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00881908008815269		[learning rate: 2.0212e-05]
		[batch 20/20] avg loss: -0.011086899609969201		[learning rate: 2.0175e-05]
	Learning Rate: 2.01751e-05
	LOSS [training: -0.009952989849060944 | validation: -0.012779175915553808]
	TIME [epoch: 8.27 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004463270354554902		[learning rate: 2.0138e-05]
		[batch 20/20] avg loss: -0.009284393943739627		[learning rate: 2.0102e-05]
	Learning Rate: 2.01019e-05
	LOSS [training: -0.006873832149147265 | validation: -0.012588715126512062]
	TIME [epoch: 8.29 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005384688423396237		[learning rate: 2.0065e-05]
		[batch 20/20] avg loss: -0.007664653752558204		[learning rate: 2.0029e-05]
	Learning Rate: 2.00289e-05
	LOSS [training: -0.00652467108797722 | validation: -0.015753482103031152]
	TIME [epoch: 8.28 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029793781153133582		[learning rate: 1.9993e-05]
		[batch 20/20] avg loss: -0.01075842466851008		[learning rate: 1.9956e-05]
	Learning Rate: 1.99563e-05
	LOSS [training: -0.006868901391911719 | validation: -0.010248768715211406]
	TIME [epoch: 8.26 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009407130099713137		[learning rate: 1.992e-05]
		[batch 20/20] avg loss: -0.005853208138762667		[learning rate: 1.9884e-05]
	Learning Rate: 1.98838e-05
	LOSS [training: -0.007630169119237903 | validation: -0.019555000749270045]
	TIME [epoch: 8.26 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013100166889005446		[learning rate: 1.9848e-05]
		[batch 20/20] avg loss: -0.007597259320887381		[learning rate: 1.9812e-05]
	Learning Rate: 1.98117e-05
	LOSS [training: -0.010348713104946412 | validation: -0.012544137246964444]
	TIME [epoch: 8.29 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00923053324927511		[learning rate: 1.9776e-05]
		[batch 20/20] avg loss: -0.0125952677975771		[learning rate: 1.974e-05]
	Learning Rate: 1.97398e-05
	LOSS [training: -0.010912900523426106 | validation: -0.013367274476978655]
	TIME [epoch: 8.27 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01038440675058926		[learning rate: 1.9704e-05]
		[batch 20/20] avg loss: -0.011446252838877891		[learning rate: 1.9668e-05]
	Learning Rate: 1.96681e-05
	LOSS [training: -0.010915329794733575 | validation: -0.010003925133214957]
	TIME [epoch: 8.29 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011854187591804266		[learning rate: 1.9632e-05]
		[batch 20/20] avg loss: -0.006951260147035403		[learning rate: 1.9597e-05]
	Learning Rate: 1.95968e-05
	LOSS [training: -0.009402723869419836 | validation: -0.013104987346855073]
	TIME [epoch: 8.26 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008436476578249866		[learning rate: 1.9561e-05]
		[batch 20/20] avg loss: -0.010398492611467216		[learning rate: 1.9526e-05]
	Learning Rate: 1.95256e-05
	LOSS [training: -0.009417484594858541 | validation: -0.01604617667417378]
	TIME [epoch: 8.5 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008457731303244547		[learning rate: 1.949e-05]
		[batch 20/20] avg loss: -0.012792785634921588		[learning rate: 1.9455e-05]
	Learning Rate: 1.94548e-05
	LOSS [training: -0.010625258469083066 | validation: -0.015708238001052994]
	TIME [epoch: 8.27 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007557740724265948		[learning rate: 1.9419e-05]
		[batch 20/20] avg loss: -0.008456814426641968		[learning rate: 1.9384e-05]
	Learning Rate: 1.93842e-05
	LOSS [training: -0.008007277575453959 | validation: -0.012682409617733172]
	TIME [epoch: 8.29 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013072149921388523		[learning rate: 1.9349e-05]
		[batch 20/20] avg loss: -0.008497463603273518		[learning rate: 1.9314e-05]
	Learning Rate: 1.93138e-05
	LOSS [training: -0.010784806762331022 | validation: -0.017451126284538262]
	TIME [epoch: 8.28 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008579686004661299		[learning rate: 1.9279e-05]
		[batch 20/20] avg loss: -0.008188109182669656		[learning rate: 1.9244e-05]
	Learning Rate: 1.92437e-05
	LOSS [training: -0.008383897593665477 | validation: -0.014604119627272743]
	TIME [epoch: 8.29 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011649055732335711		[learning rate: 1.9209e-05]
		[batch 20/20] avg loss: -0.008964587526267415		[learning rate: 1.9174e-05]
	Learning Rate: 1.91739e-05
	LOSS [training: -0.010306821629301565 | validation: -0.013890931626984165]
	TIME [epoch: 8.26 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010068189078441727		[learning rate: 1.9139e-05]
		[batch 20/20] avg loss: -0.004307588590520798		[learning rate: 1.9104e-05]
	Learning Rate: 1.91043e-05
	LOSS [training: -0.007187888834481263 | validation: -0.013205208923526588]
	TIME [epoch: 8.26 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010857149919326917		[learning rate: 1.907e-05]
		[batch 20/20] avg loss: -0.011077951378742599		[learning rate: 1.9035e-05]
	Learning Rate: 1.9035e-05
	LOSS [training: -0.01096755064903476 | validation: -0.008287716685878565]
	TIME [epoch: 8.31 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010732074414971374		[learning rate: 1.9e-05]
		[batch 20/20] avg loss: -0.0070149040674940336		[learning rate: 1.8966e-05]
	Learning Rate: 1.89659e-05
	LOSS [training: -0.008873489241232704 | validation: -0.008812616399152517]
	TIME [epoch: 8.29 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005109828755215365		[learning rate: 1.8931e-05]
		[batch 20/20] avg loss: -0.00827819230211721		[learning rate: 1.8897e-05]
	Learning Rate: 1.88971e-05
	LOSS [training: -0.006694010528666287 | validation: -0.015281047279734004]
	TIME [epoch: 8.27 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008179285190058106		[learning rate: 1.8863e-05]
		[batch 20/20] avg loss: -0.01434343860251969		[learning rate: 1.8829e-05]
	Learning Rate: 1.88285e-05
	LOSS [training: -0.011261361896288898 | validation: -0.010796318662571084]
	TIME [epoch: 8.27 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009495372544029957		[learning rate: 1.8794e-05]
		[batch 20/20] avg loss: -0.011500361782112336		[learning rate: 1.876e-05]
	Learning Rate: 1.87602e-05
	LOSS [training: -0.010497867163071146 | validation: -0.015061374638020193]
	TIME [epoch: 8.3 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012854633307434925		[learning rate: 1.8726e-05]
		[batch 20/20] avg loss: -0.010970501302435747		[learning rate: 1.8692e-05]
	Learning Rate: 1.86921e-05
	LOSS [training: -0.011912567304935336 | validation: -0.010660773203958586]
	TIME [epoch: 8.29 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009048281493919187		[learning rate: 1.8658e-05]
		[batch 20/20] avg loss: -0.007701336468342675		[learning rate: 1.8624e-05]
	Learning Rate: 1.86243e-05
	LOSS [training: -0.008374808981130931 | validation: -0.01244602460703723]
	TIME [epoch: 8.27 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007647895134699269		[learning rate: 1.859e-05]
		[batch 20/20] avg loss: -0.006244189827776722		[learning rate: 1.8557e-05]
	Learning Rate: 1.85567e-05
	LOSS [training: -0.006946042481237996 | validation: -0.012620128437248246]
	TIME [epoch: 8.27 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01413241543302749		[learning rate: 1.8523e-05]
		[batch 20/20] avg loss: -0.009960099034524253		[learning rate: 1.8489e-05]
	Learning Rate: 1.84893e-05
	LOSS [training: -0.012046257233775871 | validation: -0.006592912587424406]
	TIME [epoch: 8.31 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010926469787477135		[learning rate: 1.8456e-05]
		[batch 20/20] avg loss: -0.008583105810091075		[learning rate: 1.8422e-05]
	Learning Rate: 1.84222e-05
	LOSS [training: -0.009754787798784106 | validation: -0.00996021258449101]
	TIME [epoch: 8.29 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01251689796541004		[learning rate: 1.8389e-05]
		[batch 20/20] avg loss: -0.00504809274102687		[learning rate: 1.8355e-05]
	Learning Rate: 1.83554e-05
	LOSS [training: -0.008782495353218452 | validation: -0.007742299609022306]
	TIME [epoch: 8.26 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009642692580199074		[learning rate: 1.8322e-05]
		[batch 20/20] avg loss: -0.009817397939098273		[learning rate: 1.8289e-05]
	Learning Rate: 1.82888e-05
	LOSS [training: -0.009730045259648674 | validation: -0.013672163591063019]
	TIME [epoch: 8.26 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014710882525665322		[learning rate: 1.8256e-05]
		[batch 20/20] avg loss: -0.010354543120769878		[learning rate: 1.8222e-05]
	Learning Rate: 1.82224e-05
	LOSS [training: -0.012532712823217601 | validation: -0.00912630215463279]
	TIME [epoch: 8.3 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0076752563286505635		[learning rate: 1.8189e-05]
		[batch 20/20] avg loss: -0.007739232443639178		[learning rate: 1.8156e-05]
	Learning Rate: 1.81563e-05
	LOSS [training: -0.007707244386144872 | validation: -0.017192014204801074]
	TIME [epoch: 8.3 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006516013501239195		[learning rate: 1.8123e-05]
		[batch 20/20] avg loss: -0.011875020251590722		[learning rate: 1.809e-05]
	Learning Rate: 1.80904e-05
	LOSS [training: -0.009195516876414959 | validation: -0.011895412066539709]
	TIME [epoch: 8.26 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00666604329567429		[learning rate: 1.8058e-05]
		[batch 20/20] avg loss: -0.014362199160556066		[learning rate: 1.8025e-05]
	Learning Rate: 1.80247e-05
	LOSS [training: -0.010514121228115179 | validation: -0.012909077436350277]
	TIME [epoch: 8.26 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005288592044887627		[learning rate: 1.7992e-05]
		[batch 20/20] avg loss: -0.006690604426852935		[learning rate: 1.7959e-05]
	Learning Rate: 1.79593e-05
	LOSS [training: -0.005989598235870281 | validation: -0.012113685652527097]
	TIME [epoch: 8.3 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00958548546524123		[learning rate: 1.7927e-05]
		[batch 20/20] avg loss: -0.009406258832971868		[learning rate: 1.7894e-05]
	Learning Rate: 1.78941e-05
	LOSS [training: -0.00949587214910655 | validation: -0.01998042128614816]
	TIME [epoch: 8.29 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002809390443772899		[learning rate: 1.7862e-05]
		[batch 20/20] avg loss: -0.011985417907901292		[learning rate: 1.7829e-05]
	Learning Rate: 1.78292e-05
	LOSS [training: -0.0073974041758370955 | validation: -0.012194359016986105]
	TIME [epoch: 8.26 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008948460923144743		[learning rate: 1.7797e-05]
		[batch 20/20] avg loss: -0.008695921078445207		[learning rate: 1.7764e-05]
	Learning Rate: 1.77645e-05
	LOSS [training: -0.008822191000794974 | validation: -0.012029088505899515]
	TIME [epoch: 8.26 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011517695171717404		[learning rate: 1.7732e-05]
		[batch 20/20] avg loss: -0.005277868361536373		[learning rate: 1.77e-05]
	Learning Rate: 1.77e-05
	LOSS [training: -0.008397781766626889 | validation: -0.008817786370629988]
	TIME [epoch: 8.3 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009009269186601904		[learning rate: 1.7668e-05]
		[batch 20/20] avg loss: -0.005625920516784189		[learning rate: 1.7636e-05]
	Learning Rate: 1.76358e-05
	LOSS [training: -0.007317594851693046 | validation: -0.007551251676922588]
	TIME [epoch: 8.28 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013533046304726303		[learning rate: 1.7604e-05]
		[batch 20/20] avg loss: -0.005697449609520346		[learning rate: 1.7572e-05]
	Learning Rate: 1.75718e-05
	LOSS [training: -0.009615247957123327 | validation: -0.01032280906371412]
	TIME [epoch: 8.27 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007788506909486003		[learning rate: 1.754e-05]
		[batch 20/20] avg loss: -0.010440434067850677		[learning rate: 1.7508e-05]
	Learning Rate: 1.7508e-05
	LOSS [training: -0.009114470488668341 | validation: -0.012077107927014738]
	TIME [epoch: 8.27 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007968424324346305		[learning rate: 1.7476e-05]
		[batch 20/20] avg loss: -0.009428128839469098		[learning rate: 1.7444e-05]
	Learning Rate: 1.74445e-05
	LOSS [training: -0.008698276581907702 | validation: -0.012862187275904765]
	TIME [epoch: 8.3 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006842095747022436		[learning rate: 1.7413e-05]
		[batch 20/20] avg loss: -0.00760951511693578		[learning rate: 1.7381e-05]
	Learning Rate: 1.73812e-05
	LOSS [training: -0.007225805431979109 | validation: -0.008246155629245537]
	TIME [epoch: 8.26 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014549272270498254		[learning rate: 1.735e-05]
		[batch 20/20] avg loss: -0.007333566328337547		[learning rate: 1.7318e-05]
	Learning Rate: 1.73181e-05
	LOSS [training: -0.010941419299417901 | validation: -0.011393684120128316]
	TIME [epoch: 8.29 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012793878432000999		[learning rate: 1.7287e-05]
		[batch 20/20] avg loss: -0.00828239439248141		[learning rate: 1.7255e-05]
	Learning Rate: 1.72552e-05
	LOSS [training: -0.010538136412241203 | validation: -0.009647488222483451]
	TIME [epoch: 8.28 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005505868962546862		[learning rate: 1.7224e-05]
		[batch 20/20] avg loss: -0.010868399930502667		[learning rate: 1.7193e-05]
	Learning Rate: 1.71926e-05
	LOSS [training: -0.008187134446524764 | validation: -0.010866305443963512]
	TIME [epoch: 8.28 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012160755795306736		[learning rate: 1.7161e-05]
		[batch 20/20] avg loss: -0.009657542337049841		[learning rate: 1.713e-05]
	Learning Rate: 1.71302e-05
	LOSS [training: -0.010909149066178288 | validation: -0.01310921050714204]
	TIME [epoch: 8.26 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010933319892603498		[learning rate: 1.7099e-05]
		[batch 20/20] avg loss: -0.007886795770071535		[learning rate: 1.7068e-05]
	Learning Rate: 1.70681e-05
	LOSS [training: -0.009410057831337514 | validation: -0.01494869162923709]
	TIME [epoch: 8.28 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010645331756822746		[learning rate: 1.7037e-05]
		[batch 20/20] avg loss: -0.012875281130525454		[learning rate: 1.7006e-05]
	Learning Rate: 1.70061e-05
	LOSS [training: -0.011760306443674099 | validation: -0.00659968985871711]
	TIME [epoch: 8.28 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007660224629326766		[learning rate: 1.6975e-05]
		[batch 20/20] avg loss: -0.006915694436842116		[learning rate: 1.6944e-05]
	Learning Rate: 1.69444e-05
	LOSS [training: -0.007287959533084441 | validation: -0.011885057245903392]
	TIME [epoch: 8.29 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003028566333285947		[learning rate: 1.6914e-05]
		[batch 20/20] avg loss: -0.009320312235908644		[learning rate: 1.6883e-05]
	Learning Rate: 1.68829e-05
	LOSS [training: -0.006174439284597296 | validation: -0.01168276901265398]
	TIME [epoch: 8.27 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004573864730610058		[learning rate: 1.6852e-05]
		[batch 20/20] avg loss: -0.01489595431790014		[learning rate: 1.6822e-05]
	Learning Rate: 1.68216e-05
	LOSS [training: -0.009734909524255096 | validation: -0.01680054259670035]
	TIME [epoch: 8.28 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010112599021453398		[learning rate: 1.6791e-05]
		[batch 20/20] avg loss: -0.013373660100362455		[learning rate: 1.6761e-05]
	Learning Rate: 1.67606e-05
	LOSS [training: -0.011743129560907926 | validation: -0.014838867339438958]
	TIME [epoch: 8.3 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00803386675908185		[learning rate: 1.673e-05]
		[batch 20/20] avg loss: -0.010483492064149696		[learning rate: 1.67e-05]
	Learning Rate: 1.66998e-05
	LOSS [training: -0.009258679411615772 | validation: -0.010769869856412295]
	TIME [epoch: 8.27 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007397193129047146		[learning rate: 1.6669e-05]
		[batch 20/20] avg loss: -0.010575537880142058		[learning rate: 1.6639e-05]
	Learning Rate: 1.66392e-05
	LOSS [training: -0.008986365504594602 | validation: -0.009019747384268922]
	TIME [epoch: 8.26 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011933577361845124		[learning rate: 1.6609e-05]
		[batch 20/20] avg loss: -0.009729648028213397		[learning rate: 1.6579e-05]
	Learning Rate: 1.65788e-05
	LOSS [training: -0.010831612695029262 | validation: -0.015197516924939777]
	TIME [epoch: 8.27 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011695399666539626		[learning rate: 1.6549e-05]
		[batch 20/20] avg loss: -0.0077548603711956625		[learning rate: 1.6519e-05]
	Learning Rate: 1.65186e-05
	LOSS [training: -0.009725130018867644 | validation: -0.01366195312922945]
	TIME [epoch: 8.32 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011374940697122197		[learning rate: 1.6489e-05]
		[batch 20/20] avg loss: -0.007576529678317745		[learning rate: 1.6459e-05]
	Learning Rate: 1.64587e-05
	LOSS [training: -0.00947573518771997 | validation: -0.010876284727145677]
	TIME [epoch: 8.29 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012208864243742757		[learning rate: 1.6429e-05]
		[batch 20/20] avg loss: -0.006816241387358814		[learning rate: 1.6399e-05]
	Learning Rate: 1.63989e-05
	LOSS [training: -0.009512552815550784 | validation: -0.008271237675047467]
	TIME [epoch: 8.26 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008644206433766845		[learning rate: 1.6369e-05]
		[batch 20/20] avg loss: -0.013171234818332819		[learning rate: 1.6339e-05]
	Learning Rate: 1.63394e-05
	LOSS [training: -0.010907720626049831 | validation: -0.012790830663507075]
	TIME [epoch: 8.26 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.015229334292333762		[learning rate: 1.631e-05]
		[batch 20/20] avg loss: -0.007267089870498518		[learning rate: 1.628e-05]
	Learning Rate: 1.62801e-05
	LOSS [training: -0.011248212081416142 | validation: -0.014594721114610095]
	TIME [epoch: 8.3 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011684583329648945		[learning rate: 1.6251e-05]
		[batch 20/20] avg loss: -0.009003199544931898		[learning rate: 1.6221e-05]
	Learning Rate: 1.62211e-05
	LOSS [training: -0.010343891437290422 | validation: -0.009976473880746252]
	TIME [epoch: 8.28 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00685546344410135		[learning rate: 1.6192e-05]
		[batch 20/20] avg loss: -0.007707363653717355		[learning rate: 1.6162e-05]
	Learning Rate: 1.61622e-05
	LOSS [training: -0.007281413548909352 | validation: -0.014088429546295625]
	TIME [epoch: 8.26 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012073194920340442		[learning rate: 1.6133e-05]
		[batch 20/20] avg loss: -0.009817901441729057		[learning rate: 1.6104e-05]
	Learning Rate: 1.61035e-05
	LOSS [training: -0.01094554818103475 | validation: -0.012024128406967746]
	TIME [epoch: 8.26 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011338022220374255		[learning rate: 1.6074e-05]
		[batch 20/20] avg loss: -0.007449697631126411		[learning rate: 1.6045e-05]
	Learning Rate: 1.60451e-05
	LOSS [training: -0.009393859925750333 | validation: -0.011339650115435775]
	TIME [epoch: 8.29 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014256855460802395		[learning rate: 1.6016e-05]
		[batch 20/20] avg loss: -0.008581206126420977		[learning rate: 1.5987e-05]
	Learning Rate: 1.59869e-05
	LOSS [training: -0.011419030793611687 | validation: -0.007626067871497492]
	TIME [epoch: 8.3 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006958522779507452		[learning rate: 1.5958e-05]
		[batch 20/20] avg loss: -0.012672958423267424		[learning rate: 1.5929e-05]
	Learning Rate: 1.59288e-05
	LOSS [training: -0.00981574060138744 | validation: -0.0025267619462440534]
	TIME [epoch: 8.25 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01201446416470604		[learning rate: 1.59e-05]
		[batch 20/20] avg loss: -0.012303370970161416		[learning rate: 1.5871e-05]
	Learning Rate: 1.5871e-05
	LOSS [training: -0.012158917567433728 | validation: -0.00858764182276778]
	TIME [epoch: 8.25 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.015138350863881798		[learning rate: 1.5842e-05]
		[batch 20/20] avg loss: -0.006294270235169143		[learning rate: 1.5813e-05]
	Learning Rate: 1.58134e-05
	LOSS [training: -0.010716310549525473 | validation: -0.01414227108286016]
	TIME [epoch: 8.29 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011293771087257633		[learning rate: 1.5785e-05]
		[batch 20/20] avg loss: -0.008382924269343903		[learning rate: 1.5756e-05]
	Learning Rate: 1.57561e-05
	LOSS [training: -0.009838347678300769 | validation: -0.008481408504925136]
	TIME [epoch: 8.28 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009542370879007535		[learning rate: 1.5727e-05]
		[batch 20/20] avg loss: -0.011273538077270778		[learning rate: 1.5699e-05]
	Learning Rate: 1.56989e-05
	LOSS [training: -0.010407954478139157 | validation: -0.010480559126699072]
	TIME [epoch: 8.26 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007383268239961332		[learning rate: 1.567e-05]
		[batch 20/20] avg loss: -0.010679220083381277		[learning rate: 1.5642e-05]
	Learning Rate: 1.56419e-05
	LOSS [training: -0.009031244161671306 | validation: -0.010537248908811426]
	TIME [epoch: 8.25 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012268072745659485		[learning rate: 1.5613e-05]
		[batch 20/20] avg loss: -0.009592788386847993		[learning rate: 1.5585e-05]
	Learning Rate: 1.55851e-05
	LOSS [training: -0.010930430566253741 | validation: -0.016016903343883768]
	TIME [epoch: 8.3 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008976298582392146		[learning rate: 1.5557e-05]
		[batch 20/20] avg loss: -0.010350588067451224		[learning rate: 1.5529e-05]
	Learning Rate: 1.55286e-05
	LOSS [training: -0.009663443324921684 | validation: -0.01376066336689533]
	TIME [epoch: 8.29 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006266432719862653		[learning rate: 1.55e-05]
		[batch 20/20] avg loss: -0.011485097791830613		[learning rate: 1.5472e-05]
	Learning Rate: 1.54722e-05
	LOSS [training: -0.008875765255846633 | validation: -0.0092881772315087]
	TIME [epoch: 8.27 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010660448554074869		[learning rate: 1.5444e-05]
		[batch 20/20] avg loss: -0.010510940845901366		[learning rate: 1.5416e-05]
	Learning Rate: 1.54161e-05
	LOSS [training: -0.01058569469998812 | validation: -0.010671022530741613]
	TIME [epoch: 8.26 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0036575980027359805		[learning rate: 1.5388e-05]
		[batch 20/20] avg loss: -0.008994463399361097		[learning rate: 1.536e-05]
	Learning Rate: 1.53601e-05
	LOSS [training: -0.006326030701048539 | validation: -0.01111289047214171]
	TIME [epoch: 8.3 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009161677407786286		[learning rate: 1.5332e-05]
		[batch 20/20] avg loss: -0.006490368330389541		[learning rate: 1.5304e-05]
	Learning Rate: 1.53044e-05
	LOSS [training: -0.007826022869087911 | validation: -0.012482356935290307]
	TIME [epoch: 8.27 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013903864230614566		[learning rate: 1.5277e-05]
		[batch 20/20] avg loss: -0.011457365456522877		[learning rate: 1.5249e-05]
	Learning Rate: 1.52488e-05
	LOSS [training: -0.012680614843568722 | validation: -0.01590150632655095]
	TIME [epoch: 8.28 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010610238463093944		[learning rate: 1.5221e-05]
		[batch 20/20] avg loss: -0.007891409592825017		[learning rate: 1.5194e-05]
	Learning Rate: 1.51935e-05
	LOSS [training: -0.009250824027959483 | validation: -0.01612395196417101]
	TIME [epoch: 8.25 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007809678846337815		[learning rate: 1.5166e-05]
		[batch 20/20] avg loss: -0.010652410115312786		[learning rate: 1.5138e-05]
	Learning Rate: 1.51384e-05
	LOSS [training: -0.009231044480825302 | validation: -0.01666827347698449]
	TIME [epoch: 8.3 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0059272194016592285		[learning rate: 1.5111e-05]
		[batch 20/20] avg loss: -0.010837023602504785		[learning rate: 1.5083e-05]
	Learning Rate: 1.50834e-05
	LOSS [training: -0.008382121502082007 | validation: -0.013192065792234637]
	TIME [epoch: 8.26 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008903661071743153		[learning rate: 1.5056e-05]
		[batch 20/20] avg loss: -0.011177267256387027		[learning rate: 1.5029e-05]
	Learning Rate: 1.50287e-05
	LOSS [training: -0.010040464164065092 | validation: -0.014722281349529175]
	TIME [epoch: 8.27 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016510885777437923		[learning rate: 1.5001e-05]
		[batch 20/20] avg loss: -0.012590961763272943		[learning rate: 1.4974e-05]
	Learning Rate: 1.49741e-05
	LOSS [training: -0.007121025170508367 | validation: -0.007421294172325817]
	TIME [epoch: 8.26 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007020321271309489		[learning rate: 1.4947e-05]
		[batch 20/20] avg loss: -0.006762564274591142		[learning rate: 1.492e-05]
	Learning Rate: 1.49198e-05
	LOSS [training: -0.006891442772950317 | validation: -0.008856512717492888]
	TIME [epoch: 8.3 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013661865796762976		[learning rate: 1.4893e-05]
		[batch 20/20] avg loss: -0.0062816152726656626		[learning rate: 1.4866e-05]
	Learning Rate: 1.48657e-05
	LOSS [training: -0.009971740534714318 | validation: -0.013219246929825918]
	TIME [epoch: 8.26 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012331961702591036		[learning rate: 1.4839e-05]
		[batch 20/20] avg loss: -0.008668046888513898		[learning rate: 1.4812e-05]
	Learning Rate: 1.48117e-05
	LOSS [training: -0.010500004295552466 | validation: -0.010532464742636131]
	TIME [epoch: 8.27 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01373453870563202		[learning rate: 1.4785e-05]
		[batch 20/20] avg loss: -0.007229872219053199		[learning rate: 1.4758e-05]
	Learning Rate: 1.4758e-05
	LOSS [training: -0.010482205462342608 | validation: -0.00862266316121377]
	TIME [epoch: 8.27 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013718809620403439		[learning rate: 1.4731e-05]
		[batch 20/20] avg loss: -0.00931298568101861		[learning rate: 1.4704e-05]
	Learning Rate: 1.47044e-05
	LOSS [training: -0.011515897650711024 | validation: -0.015204589110638088]
	TIME [epoch: 8.3 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008253799077416172		[learning rate: 1.4678e-05]
		[batch 20/20] avg loss: -0.009592697265573005		[learning rate: 1.4651e-05]
	Learning Rate: 1.4651e-05
	LOSS [training: -0.008923248171494588 | validation: -0.012084238090555095]
	TIME [epoch: 8.26 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007587139231374035		[learning rate: 1.4624e-05]
		[batch 20/20] avg loss: -0.008406002938365409		[learning rate: 1.4598e-05]
	Learning Rate: 1.45979e-05
	LOSS [training: -0.007996571084869722 | validation: -0.011116413018624764]
	TIME [epoch: 8.26 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.015903532244453657		[learning rate: 1.4571e-05]
		[batch 20/20] avg loss: -0.006784461328330852		[learning rate: 1.4545e-05]
	Learning Rate: 1.45449e-05
	LOSS [training: -0.011343996786392254 | validation: -0.00700321091289111]
	TIME [epoch: 8.3 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010375185523420444		[learning rate: 1.4518e-05]
		[batch 20/20] avg loss: -0.012616254693868012		[learning rate: 1.4492e-05]
	Learning Rate: 1.44921e-05
	LOSS [training: -0.011495720108644228 | validation: -0.010748539872957767]
	TIME [epoch: 8.29 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004176003507130899		[learning rate: 1.4466e-05]
		[batch 20/20] avg loss: -0.01009527400874997		[learning rate: 1.444e-05]
	Learning Rate: 1.44395e-05
	LOSS [training: -0.007135638757940435 | validation: -0.007456541534095578]
	TIME [epoch: 8.25 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009690958471603521		[learning rate: 1.4413e-05]
		[batch 20/20] avg loss: -0.005283074847356653		[learning rate: 1.4387e-05]
	Learning Rate: 1.43871e-05
	LOSS [training: -0.007487016659480086 | validation: -0.015367553745119427]
	TIME [epoch: 8.26 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007750609983067476		[learning rate: 1.4361e-05]
		[batch 20/20] avg loss: -0.008212327787280998		[learning rate: 1.4335e-05]
	Learning Rate: 1.43349e-05
	LOSS [training: -0.007981468885174235 | validation: -0.0092473178949009]
	TIME [epoch: 8.3 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013862136368372715		[learning rate: 1.4309e-05]
		[batch 20/20] avg loss: -0.005389446964532228		[learning rate: 1.4283e-05]
	Learning Rate: 1.42829e-05
	LOSS [training: -0.009625791666452471 | validation: -0.010066353716668714]
	TIME [epoch: 8.29 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010605942912578787		[learning rate: 1.4257e-05]
		[batch 20/20] avg loss: -0.009758008890588614		[learning rate: 1.4231e-05]
	Learning Rate: 1.4231e-05
	LOSS [training: -0.010181975901583701 | validation: -0.012798733204537487]
	TIME [epoch: 8.25 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0038438724582113303		[learning rate: 1.4205e-05]
		[batch 20/20] avg loss: -0.012054452808023619		[learning rate: 1.4179e-05]
	Learning Rate: 1.41794e-05
	LOSS [training: -0.007949162633117474 | validation: -0.010922092640111025]
	TIME [epoch: 8.26 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009333186603769947		[learning rate: 1.4154e-05]
		[batch 20/20] avg loss: -0.010586139645388274		[learning rate: 1.4128e-05]
	Learning Rate: 1.41279e-05
	LOSS [training: -0.009959663124579108 | validation: -0.018973509825113363]
	TIME [epoch: 8.3 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009864939374655128		[learning rate: 1.4102e-05]
		[batch 20/20] avg loss: -0.009060506638314564		[learning rate: 1.4077e-05]
	Learning Rate: 1.40767e-05
	LOSS [training: -0.009462723006484845 | validation: -0.017326554573260093]
	TIME [epoch: 8.27 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008839330653214302		[learning rate: 1.4051e-05]
		[batch 20/20] avg loss: -0.005790287916500436		[learning rate: 1.4026e-05]
	Learning Rate: 1.40256e-05
	LOSS [training: -0.00731480928485737 | validation: -0.012417389833014972]
	TIME [epoch: 8.26 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011759382953115672		[learning rate: 1.4e-05]
		[batch 20/20] avg loss: -0.006389913657611999		[learning rate: 1.3975e-05]
	Learning Rate: 1.39747e-05
	LOSS [training: -0.009074648305363838 | validation: -0.01876359544977238]
	TIME [epoch: 8.26 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008059861097865387		[learning rate: 1.3949e-05]
		[batch 20/20] avg loss: -0.012711923466267294		[learning rate: 1.3924e-05]
	Learning Rate: 1.3924e-05
	LOSS [training: -0.01038589228206634 | validation: -0.009486556182732656]
	TIME [epoch: 8.3 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011121602560605806		[learning rate: 1.3899e-05]
		[batch 20/20] avg loss: -0.00906641577416574		[learning rate: 1.3873e-05]
	Learning Rate: 1.38734e-05
	LOSS [training: -0.010094009167385773 | validation: -0.00830262273892556]
	TIME [epoch: 8.31 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004814183269911596		[learning rate: 1.3848e-05]
		[batch 20/20] avg loss: -0.008089166084606013		[learning rate: 1.3823e-05]
	Learning Rate: 1.38231e-05
	LOSS [training: -0.006451674677258804 | validation: -0.014366865352534106]
	TIME [epoch: 8.26 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011542775204509576		[learning rate: 1.3798e-05]
		[batch 20/20] avg loss: -0.00793526502296739		[learning rate: 1.3773e-05]
	Learning Rate: 1.37729e-05
	LOSS [training: -0.009739020113738483 | validation: -0.011721878421961113]
	TIME [epoch: 8.26 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008742654498872912		[learning rate: 1.3748e-05]
		[batch 20/20] avg loss: -0.008835024347817978		[learning rate: 1.3723e-05]
	Learning Rate: 1.37229e-05
	LOSS [training: -0.008788839423345447 | validation: -0.01380547205125253]
	TIME [epoch: 8.31 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0060545955025456625		[learning rate: 1.3698e-05]
		[batch 20/20] avg loss: -0.010257097152156225		[learning rate: 1.3673e-05]
	Learning Rate: 1.36731e-05
	LOSS [training: -0.008155846327350943 | validation: -0.015389357026533344]
	TIME [epoch: 8.29 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008579009249162044		[learning rate: 1.3648e-05]
		[batch 20/20] avg loss: -0.007811192803977862		[learning rate: 1.3624e-05]
	Learning Rate: 1.36235e-05
	LOSS [training: -0.008195101026569953 | validation: -0.012308927467702738]
	TIME [epoch: 8.27 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011067530004217603		[learning rate: 1.3599e-05]
		[batch 20/20] avg loss: -0.00859638073934436		[learning rate: 1.3574e-05]
	Learning Rate: 1.35741e-05
	LOSS [training: -0.00983195537178098 | validation: -0.004274276051363489]
	TIME [epoch: 8.27 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008264118235826876		[learning rate: 1.3549e-05]
		[batch 20/20] avg loss: -0.009529883781835495		[learning rate: 1.3525e-05]
	Learning Rate: 1.35248e-05
	LOSS [training: -0.008897001008831186 | validation: -0.011211389132140653]
	TIME [epoch: 8.3 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009525799458977751		[learning rate: 1.35e-05]
		[batch 20/20] avg loss: -0.008650605046147968		[learning rate: 1.3476e-05]
	Learning Rate: 1.34757e-05
	LOSS [training: -0.009088202252562861 | validation: -0.014190255965747012]
	TIME [epoch: 8.27 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00751353598068451		[learning rate: 1.3451e-05]
		[batch 20/20] avg loss: -0.01132631388604021		[learning rate: 1.3427e-05]
	Learning Rate: 1.34268e-05
	LOSS [training: -0.009419924933362361 | validation: -0.01607526930053453]
	TIME [epoch: 8.27 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009109087611435914		[learning rate: 1.3402e-05]
		[batch 20/20] avg loss: -0.008856642234037452		[learning rate: 1.3378e-05]
	Learning Rate: 1.33781e-05
	LOSS [training: -0.008982864922736682 | validation: -0.012724815681199552]
	TIME [epoch: 8.25 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011242368612631618		[learning rate: 1.3354e-05]
		[batch 20/20] avg loss: -0.006683826520761153		[learning rate: 1.333e-05]
	Learning Rate: 1.33296e-05
	LOSS [training: -0.008963097566696384 | validation: -0.011877745375407568]
	TIME [epoch: 8.3 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008950547638118925		[learning rate: 1.3305e-05]
		[batch 20/20] avg loss: -0.010849642668793468		[learning rate: 1.3281e-05]
	Learning Rate: 1.32812e-05
	LOSS [training: -0.0099000951534562 | validation: -0.012612307084503314]
	TIME [epoch: 8.26 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009532725765574481		[learning rate: 1.3257e-05]
		[batch 20/20] avg loss: -0.006849679974639122		[learning rate: 1.3233e-05]
	Learning Rate: 1.3233e-05
	LOSS [training: -0.008191202870106799 | validation: -0.012694652607093907]
	TIME [epoch: 8.28 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00604984908346523		[learning rate: 1.3209e-05]
		[batch 20/20] avg loss: -0.011731925881593707		[learning rate: 1.3185e-05]
	Learning Rate: 1.3185e-05
	LOSS [training: -0.00889088748252947 | validation: -0.01458522319535613]
	TIME [epoch: 8.27 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009197345517962214		[learning rate: 1.3161e-05]
		[batch 20/20] avg loss: -0.012170220934856079		[learning rate: 1.3137e-05]
	Learning Rate: 1.31371e-05
	LOSS [training: -0.010683783226409145 | validation: -0.014107012736606642]
	TIME [epoch: 8.29 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014106115021760245		[learning rate: 1.3113e-05]
		[batch 20/20] avg loss: -0.009908146631643366		[learning rate: 1.3089e-05]
	Learning Rate: 1.30894e-05
	LOSS [training: -0.012007130826701804 | validation: -0.011261824610279757]
	TIME [epoch: 8.26 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009763184310861539		[learning rate: 1.3066e-05]
		[batch 20/20] avg loss: -0.008565783944052408		[learning rate: 1.3042e-05]
	Learning Rate: 1.30419e-05
	LOSS [training: -0.009164484127456973 | validation: -0.009914103790376983]
	TIME [epoch: 8.27 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007083672934406711		[learning rate: 1.3018e-05]
		[batch 20/20] avg loss: -0.015472242246059508		[learning rate: 1.2995e-05]
	Learning Rate: 1.29946e-05
	LOSS [training: -0.011277957590233109 | validation: -0.007295197430509193]
	TIME [epoch: 8.29 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008915007343138107		[learning rate: 1.2971e-05]
		[batch 20/20] avg loss: -0.009744650307766039		[learning rate: 1.2947e-05]
	Learning Rate: 1.29475e-05
	LOSS [training: -0.009329828825452073 | validation: -0.012246669473834508]
	TIME [epoch: 8.28 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011359959416022839		[learning rate: 1.2924e-05]
		[batch 20/20] avg loss: -0.011229455468525582		[learning rate: 1.29e-05]
	Learning Rate: 1.29005e-05
	LOSS [training: -0.01129470744227421 | validation: -0.008609114076758455]
	TIME [epoch: 8.26 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010608724122274246		[learning rate: 1.2877e-05]
		[batch 20/20] avg loss: -0.009549323982314993		[learning rate: 1.2854e-05]
	Learning Rate: 1.28536e-05
	LOSS [training: -0.01007902405229462 | validation: -0.012506583076751681]
	TIME [epoch: 8.26 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0129633413769907		[learning rate: 1.283e-05]
		[batch 20/20] avg loss: -0.011368982343870298		[learning rate: 1.2807e-05]
	Learning Rate: 1.2807e-05
	LOSS [training: -0.012166161860430501 | validation: -0.01434121577203273]
	TIME [epoch: 8.31 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01290720943556115		[learning rate: 1.2784e-05]
		[batch 20/20] avg loss: -0.007595698166491876		[learning rate: 1.2761e-05]
	Learning Rate: 1.27605e-05
	LOSS [training: -0.010251453801026513 | validation: -0.011592232450039095]
	TIME [epoch: 8.27 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008792350214442224		[learning rate: 1.2737e-05]
		[batch 20/20] avg loss: -0.008370827446130387		[learning rate: 1.2714e-05]
	Learning Rate: 1.27142e-05
	LOSS [training: -0.008581588830286304 | validation: -0.011841218138455956]
	TIME [epoch: 8.26 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007774377362818154		[learning rate: 1.2691e-05]
		[batch 20/20] avg loss: -0.01054660907428176		[learning rate: 1.2668e-05]
	Learning Rate: 1.26681e-05
	LOSS [training: -0.009160493218549955 | validation: -0.011797189039763714]
	TIME [epoch: 8.26 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014528164743445945		[learning rate: 1.2645e-05]
		[batch 20/20] avg loss: -0.011426043163339939		[learning rate: 1.2622e-05]
	Learning Rate: 1.26221e-05
	LOSS [training: -0.012977103953392946 | validation: -0.016269998350150496]
	TIME [epoch: 8.31 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012642256929995146		[learning rate: 1.2599e-05]
		[batch 20/20] avg loss: -0.009381101371102511		[learning rate: 1.2576e-05]
	Learning Rate: 1.25763e-05
	LOSS [training: -0.01101167915054883 | validation: -0.01271527911271765]
	TIME [epoch: 8.29 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008291158201171569		[learning rate: 1.2553e-05]
		[batch 20/20] avg loss: -0.012977295100617386		[learning rate: 1.2531e-05]
	Learning Rate: 1.25307e-05
	LOSS [training: -0.010634226650894477 | validation: -0.009552288301152193]
	TIME [epoch: 8.27 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010800773658916294		[learning rate: 1.2508e-05]
		[batch 20/20] avg loss: -0.008138356646689432		[learning rate: 1.2485e-05]
	Learning Rate: 1.24852e-05
	LOSS [training: -0.009469565152802865 | validation: -0.013948918644829309]
	TIME [epoch: 8.26 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010435206034182121		[learning rate: 1.2463e-05]
		[batch 20/20] avg loss: -0.013201622722491594		[learning rate: 1.244e-05]
	Learning Rate: 1.24399e-05
	LOSS [training: -0.011818414378336856 | validation: -0.012999149695830933]
	TIME [epoch: 8.3 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0069215404242209235		[learning rate: 1.2417e-05]
		[batch 20/20] avg loss: -0.010335216240177211		[learning rate: 1.2395e-05]
	Learning Rate: 1.23947e-05
	LOSS [training: -0.008628378332199068 | validation: -0.016447285641441736]
	TIME [epoch: 8.28 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011984627049833602		[learning rate: 1.2372e-05]
		[batch 20/20] avg loss: -0.006699552676321543		[learning rate: 1.235e-05]
	Learning Rate: 1.23497e-05
	LOSS [training: -0.009342089863077569 | validation: -0.016962211328035156]
	TIME [epoch: 8.25 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007355567681329042		[learning rate: 1.2327e-05]
		[batch 20/20] avg loss: -0.005932185177262708		[learning rate: 1.2305e-05]
	Learning Rate: 1.23049e-05
	LOSS [training: -0.006643876429295875 | validation: -0.020230658096891903]
	TIME [epoch: 8.26 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008577073863556361		[learning rate: 1.2283e-05]
		[batch 20/20] avg loss: -0.009427785834921619		[learning rate: 1.226e-05]
	Learning Rate: 1.22603e-05
	LOSS [training: -0.009002429849238993 | validation: -0.01748535241346589]
	TIME [epoch: 8.3 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010112569206441687		[learning rate: 1.2238e-05]
		[batch 20/20] avg loss: -0.011036979252803166		[learning rate: 1.2216e-05]
	Learning Rate: 1.22158e-05
	LOSS [training: -0.010574774229622426 | validation: -0.014992769719120634]
	TIME [epoch: 8.29 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005489741475333203		[learning rate: 1.2194e-05]
		[batch 20/20] avg loss: -0.008811262978563878		[learning rate: 1.2171e-05]
	Learning Rate: 1.21714e-05
	LOSS [training: -0.007150502226948541 | validation: -0.013198847026470242]
	TIME [epoch: 8.27 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008486267530485732		[learning rate: 1.2149e-05]
		[batch 20/20] avg loss: -0.010908239096962516		[learning rate: 1.2127e-05]
	Learning Rate: 1.21273e-05
	LOSS [training: -0.009697253313724124 | validation: -0.013944913116532223]
	TIME [epoch: 8.26 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011501012315077733		[learning rate: 1.2105e-05]
		[batch 20/20] avg loss: -0.003904805773687381		[learning rate: 1.2083e-05]
	Learning Rate: 1.20833e-05
	LOSS [training: -0.007702909044382558 | validation: -0.01280538509046833]
	TIME [epoch: 8.31 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01103083623135193		[learning rate: 1.2061e-05]
		[batch 20/20] avg loss: -0.008181225063586173		[learning rate: 1.2039e-05]
	Learning Rate: 1.20394e-05
	LOSS [training: -0.009606030647469052 | validation: -0.010387480579908247]
	TIME [epoch: 8.3 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006973731678631132		[learning rate: 1.2018e-05]
		[batch 20/20] avg loss: -0.00908151557855209		[learning rate: 1.1996e-05]
	Learning Rate: 1.19957e-05
	LOSS [training: -0.00802762362859161 | validation: -0.006693511097347544]
	TIME [epoch: 8.27 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008016524204625362		[learning rate: 1.1974e-05]
		[batch 20/20] avg loss: -0.0069313817296351695		[learning rate: 1.1952e-05]
	Learning Rate: 1.19522e-05
	LOSS [training: -0.007473952967130266 | validation: -0.018414609900772677]
	TIME [epoch: 8.26 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010609616342261558		[learning rate: 1.193e-05]
		[batch 20/20] avg loss: -0.014286321443837091		[learning rate: 1.1909e-05]
	Learning Rate: 1.19088e-05
	LOSS [training: -0.007673641539031625 | validation: -0.014528164753061604]
	TIME [epoch: 8.3 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009396611206841729		[learning rate: 1.1887e-05]
		[batch 20/20] avg loss: -0.009122026402328152		[learning rate: 1.1866e-05]
	Learning Rate: 1.18656e-05
	LOSS [training: -0.009259318804584938 | validation: -0.010772518123956337]
	TIME [epoch: 8.28 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009349874428270606		[learning rate: 1.1844e-05]
		[batch 20/20] avg loss: -0.014743846858405948		[learning rate: 1.1823e-05]
	Learning Rate: 1.18225e-05
	LOSS [training: -0.012046860643338278 | validation: -0.013954116629869283]
	TIME [epoch: 8.27 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009436451925365425		[learning rate: 1.1801e-05]
		[batch 20/20] avg loss: -0.011447224235664194		[learning rate: 1.178e-05]
	Learning Rate: 1.17796e-05
	LOSS [training: -0.010441838080514809 | validation: -0.012183037949225388]
	TIME [epoch: 8.27 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008043426435904725		[learning rate: 1.1758e-05]
		[batch 20/20] avg loss: -0.008907149678069597		[learning rate: 1.1737e-05]
	Learning Rate: 1.17369e-05
	LOSS [training: -0.008475288056987159 | validation: -0.012582135238209561]
	TIME [epoch: 8.31 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008707608318863549		[learning rate: 1.1716e-05]
		[batch 20/20] avg loss: -0.006675326832669301		[learning rate: 1.1694e-05]
	Learning Rate: 1.16943e-05
	LOSS [training: -0.007691467575766426 | validation: -0.014949443301782266]
	TIME [epoch: 8.27 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006817881303763211		[learning rate: 1.1673e-05]
		[batch 20/20] avg loss: -0.007926848950068887		[learning rate: 1.1652e-05]
	Learning Rate: 1.16518e-05
	LOSS [training: -0.00737236512691605 | validation: -0.016079145488389963]
	TIME [epoch: 8.29 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.013733253176142763		[learning rate: 1.1631e-05]
		[batch 20/20] avg loss: -0.004589670777552861		[learning rate: 1.161e-05]
	Learning Rate: 1.16096e-05
	LOSS [training: -0.00916146197684781 | validation: -0.009383488782691922]
	TIME [epoch: 8.27 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009802579679970455		[learning rate: 1.1588e-05]
		[batch 20/20] avg loss: -0.014717095252657835		[learning rate: 1.1567e-05]
	Learning Rate: 1.15674e-05
	LOSS [training: -0.012259837466314146 | validation: -0.017560168706038607]
	TIME [epoch: 8.3 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007157379465899528		[learning rate: 1.1546e-05]
		[batch 20/20] avg loss: -0.010176716054564678		[learning rate: 1.1525e-05]
	Learning Rate: 1.15255e-05
	LOSS [training: -0.0086670477602321 | validation: -0.010426648071103106]
	TIME [epoch: 8.26 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0068514182380039115		[learning rate: 1.1505e-05]
		[batch 20/20] avg loss: -0.008938734734152732		[learning rate: 1.1484e-05]
	Learning Rate: 1.14836e-05
	LOSS [training: -0.007895076486078323 | validation: -0.015094844117813986]
	TIME [epoch: 8.29 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.006236360038583018		[learning rate: 1.1463e-05]
		[batch 20/20] avg loss: -0.013749235857980574		[learning rate: 1.1442e-05]
	Learning Rate: 1.1442e-05
	LOSS [training: -0.009992797948281796 | validation: -0.008803188273584285]
	TIME [epoch: 8.28 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009570010588835989		[learning rate: 1.1421e-05]
		[batch 20/20] avg loss: -0.009611504791715691		[learning rate: 1.14e-05]
	Learning Rate: 1.14004e-05
	LOSS [training: -0.00959075769027584 | validation: -0.01631549056452704]
	TIME [epoch: 8.28 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010125655541877858		[learning rate: 1.138e-05]
		[batch 20/20] avg loss: -0.009996223542142121		[learning rate: 1.1359e-05]
	Learning Rate: 1.13591e-05
	LOSS [training: -0.010060939542009987 | validation: -0.011973892812935576]
	TIME [epoch: 8.26 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01586749805095689		[learning rate: 1.1338e-05]
		[batch 20/20] avg loss: -0.008646947702370142		[learning rate: 1.1318e-05]
	Learning Rate: 1.13178e-05
	LOSS [training: -0.012257222876663514 | validation: -0.013640243190127604]
	TIME [epoch: 8.27 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008239673248700336		[learning rate: 1.1297e-05]
		[batch 20/20] avg loss: -0.00849797140354411		[learning rate: 1.1277e-05]
	Learning Rate: 1.12768e-05
	LOSS [training: -0.008368822326122224 | validation: -0.011723131536642305]
	TIME [epoch: 8.3 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008965231929466513		[learning rate: 1.1256e-05]
		[batch 20/20] avg loss: -0.009653856187631722		[learning rate: 1.1236e-05]
	Learning Rate: 1.12358e-05
	LOSS [training: -0.009309544058549118 | validation: -0.014696164252698304]
	TIME [epoch: 8.29 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.005585423270990804		[learning rate: 1.1215e-05]
		[batch 20/20] avg loss: -0.009183679377549882		[learning rate: 1.1195e-05]
	Learning Rate: 1.11951e-05
	LOSS [training: -0.007384551324270343 | validation: -0.016295631789728465]
	TIME [epoch: 8.26 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014175043836404246		[learning rate: 1.1175e-05]
		[batch 20/20] avg loss: -0.007773106933094602		[learning rate: 1.1154e-05]
	Learning Rate: 1.11544e-05
	LOSS [training: -0.010974075384749425 | validation: -0.012449403716300789]
	TIME [epoch: 8.25 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.003954250132198422		[learning rate: 1.1134e-05]
		[batch 20/20] avg loss: -0.011580494388953637		[learning rate: 1.1114e-05]
	Learning Rate: 1.1114e-05
	LOSS [training: -0.007767372260576032 | validation: -0.016263462435751972]
	TIME [epoch: 8.32 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009433986087263266		[learning rate: 1.1094e-05]
		[batch 20/20] avg loss: -0.006209314576076123		[learning rate: 1.1074e-05]
	Learning Rate: 1.10736e-05
	LOSS [training: -0.007821650331669693 | validation: -0.013785400150043165]
	TIME [epoch: 8.28 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010450632119631167		[learning rate: 1.1054e-05]
		[batch 20/20] avg loss: -0.008019743514394622		[learning rate: 1.1033e-05]
	Learning Rate: 1.10334e-05
	LOSS [training: -0.009235187817012895 | validation: -0.01126875514432886]
	TIME [epoch: 8.25 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0024150744440891796		[learning rate: 1.1013e-05]
		[batch 20/20] avg loss: -0.010963332474494112		[learning rate: 1.0993e-05]
	Learning Rate: 1.09934e-05
	LOSS [training: -0.006689203459291647 | validation: -0.013309762659261617]
	TIME [epoch: 8.25 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010198368221779703		[learning rate: 1.0973e-05]
		[batch 20/20] avg loss: -0.0071093463042620885		[learning rate: 1.0953e-05]
	Learning Rate: 1.09535e-05
	LOSS [training: -0.008653857263020898 | validation: -0.01685497250217162]
	TIME [epoch: 8.3 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011156880358676489		[learning rate: 1.0934e-05]
		[batch 20/20] avg loss: -0.009549083466391746		[learning rate: 1.0914e-05]
	Learning Rate: 1.09137e-05
	LOSS [training: -0.010352981912534118 | validation: -0.010944936153250609]
	TIME [epoch: 8.28 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.007778525419813378		[learning rate: 1.0894e-05]
		[batch 20/20] avg loss: -0.010500433019771209		[learning rate: 1.0874e-05]
	Learning Rate: 1.08741e-05
	LOSS [training: -0.009139479219792294 | validation: -0.01446106859661779]
	TIME [epoch: 8.24 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011115094319677107		[learning rate: 1.0854e-05]
		[batch 20/20] avg loss: -0.013783084681699925		[learning rate: 1.0835e-05]
	Learning Rate: 1.08347e-05
	LOSS [training: -0.012449089500688518 | validation: -0.014447822011036662]
	TIME [epoch: 8.24 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012504261040859558		[learning rate: 1.0815e-05]
		[batch 20/20] avg loss: -0.0070121509814758815		[learning rate: 1.0795e-05]
	Learning Rate: 1.07954e-05
	LOSS [training: -0.00975820601116772 | validation: -0.014716868247644295]
	TIME [epoch: 8.28 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.015039048689072071		[learning rate: 1.0776e-05]
		[batch 20/20] avg loss: -0.002584204479108203		[learning rate: 1.0756e-05]
	Learning Rate: 1.07562e-05
	LOSS [training: -0.008811626584090137 | validation: -0.012772091078863481]
	TIME [epoch: 8.28 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0059666046313278445		[learning rate: 1.0737e-05]
		[batch 20/20] avg loss: -0.011064571405962356		[learning rate: 1.0717e-05]
	Learning Rate: 1.07171e-05
	LOSS [training: -0.0085155880186451 | validation: -0.01782050810798073]
	TIME [epoch: 8.23 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008586351391400241		[learning rate: 1.0698e-05]
		[batch 20/20] avg loss: -0.010861895075208013		[learning rate: 1.0678e-05]
	Learning Rate: 1.06782e-05
	LOSS [training: -0.009724123233304125 | validation: -0.018980852585573967]
	TIME [epoch: 8.24 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010180503013476307		[learning rate: 1.0659e-05]
		[batch 20/20] avg loss: -0.004849846722205153		[learning rate: 1.0639e-05]
	Learning Rate: 1.06395e-05
	LOSS [training: -0.007515174867840729 | validation: -0.015161575103191412]
	TIME [epoch: 8.28 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.008651365340332734		[learning rate: 1.062e-05]
		[batch 20/20] avg loss: -0.010649827064149419		[learning rate: 1.0601e-05]
	Learning Rate: 1.06009e-05
	LOSS [training: -0.009650596202241079 | validation: -0.01139136973921099]
	TIME [epoch: 8.27 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01507287804220458		[learning rate: 1.0582e-05]
		[batch 20/20] avg loss: -0.012086115989821702		[learning rate: 1.0562e-05]
	Learning Rate: 1.05624e-05
	LOSS [training: -0.01357949701601314 | validation: -0.010128064463835223]
	TIME [epoch: 8.24 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.012051083586243478		[learning rate: 1.0543e-05]
		[batch 20/20] avg loss: -0.010195295240406691		[learning rate: 1.0524e-05]
	Learning Rate: 1.05241e-05
	LOSS [training: -0.011123189413325088 | validation: -0.008970524601951845]
	TIME [epoch: 8.24 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01207889129464186		[learning rate: 1.0505e-05]
		[batch 20/20] avg loss: -0.0066882921575698185		[learning rate: 1.0486e-05]
	Learning Rate: 1.04859e-05
	LOSS [training: -0.009383591726105842 | validation: -0.01151260570188915]
	TIME [epoch: 8.29 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011368876317836677		[learning rate: 1.0467e-05]
		[batch 20/20] avg loss: -0.0043387122508965325		[learning rate: 1.0448e-05]
	Learning Rate: 1.04478e-05
	LOSS [training: -0.007853794284366606 | validation: -0.011520914262288094]
	TIME [epoch: 8.27 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01314050041793261		[learning rate: 1.0429e-05]
		[batch 20/20] avg loss: -0.008143884270195864		[learning rate: 1.041e-05]
	Learning Rate: 1.04099e-05
	LOSS [training: -0.010642192344064239 | validation: -0.013499591370130973]
	TIME [epoch: 8.24 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0034355362276272867		[learning rate: 1.0391e-05]
		[batch 20/20] avg loss: -0.01079593189381342		[learning rate: 1.0372e-05]
	Learning Rate: 1.03721e-05
	LOSS [training: -0.0071157340607203535 | validation: -0.014180125278668023]
	TIME [epoch: 8.24 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01103106379382525		[learning rate: 1.0353e-05]
		[batch 20/20] avg loss: -0.007797238970749349		[learning rate: 1.0335e-05]
	Learning Rate: 1.03345e-05
	LOSS [training: -0.009414151382287298 | validation: -0.015190411220081825]
	TIME [epoch: 8.29 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00873589755241775		[learning rate: 1.0316e-05]
		[batch 20/20] avg loss: -0.008706653096750554		[learning rate: 1.0297e-05]
	Learning Rate: 1.0297e-05
	LOSS [training: -0.008721275324584152 | validation: -0.013357656735343613]
	TIME [epoch: 8.25 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014633716621447601		[learning rate: 1.0278e-05]
		[batch 20/20] avg loss: -0.0069674080084059665		[learning rate: 1.026e-05]
	Learning Rate: 1.02596e-05
	LOSS [training: -0.010800562314926782 | validation: -0.014165218354450856]
	TIME [epoch: 8.27 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.009832498440915508		[learning rate: 1.0241e-05]
		[batch 20/20] avg loss: -0.008477410589921816		[learning rate: 1.0222e-05]
	Learning Rate: 1.02224e-05
	LOSS [training: -0.009154954515418662 | validation: -0.010771415813614056]
	TIME [epoch: 8.24 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.010581447433929362		[learning rate: 1.0204e-05]
		[batch 20/20] avg loss: -0.008699028188651842		[learning rate: 1.0185e-05]
	Learning Rate: 1.01853e-05
	LOSS [training: -0.009640237811290604 | validation: -0.007136294609504603]
	TIME [epoch: 8.28 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0076822471131427345		[learning rate: 1.0167e-05]
		[batch 20/20] avg loss: -0.0071260851798086635		[learning rate: 1.0148e-05]
	Learning Rate: 1.01483e-05
	LOSS [training: -0.007404166146475699 | validation: -0.011688068126608319]
	TIME [epoch: 8.25 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.014485274325670108		[learning rate: 1.013e-05]
		[batch 20/20] avg loss: -0.0042252715977043375		[learning rate: 1.0112e-05]
	Learning Rate: 1.01115e-05
	LOSS [training: -0.009355272961687224 | validation: -0.009646331072812831]
	TIME [epoch: 8.26 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01019310076293201		[learning rate: 1.0093e-05]
		[batch 20/20] avg loss: -0.010065269142199887		[learning rate: 1.0075e-05]
	Learning Rate: 1.00748e-05
	LOSS [training: -0.010129184952565946 | validation: -0.010922514415553412]
	TIME [epoch: 8.24 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.011493892939390649		[learning rate: 1.0057e-05]
		[batch 20/20] avg loss: -0.0033644520056965166		[learning rate: 1.0038e-05]
	Learning Rate: 1.00382e-05
	LOSS [training: -0.007429172472543583 | validation: -0.019160266363839315]
	TIME [epoch: 8.3 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.01267555823266021		[learning rate: 1.002e-05]
		[batch 20/20] avg loss: -0.011025967637898924		[learning rate: 1.0002e-05]
	Learning Rate: 1.00018e-05
	LOSS [training: -0.011850762935279568 | validation: -0.01026235130429944]
	TIME [epoch: 8.24 sec]
Finished training in 16734.041 seconds.
