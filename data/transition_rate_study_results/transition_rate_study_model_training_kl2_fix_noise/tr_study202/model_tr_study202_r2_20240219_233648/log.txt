Args:
Namespace(name='model_tr_study202', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2', training_data='data/transition_rate_studies/tr_study202/tr_study202_training/r2', validation_data='data/transition_rate_studies/tr_study202/tr_study202_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3471630228

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.394343419451392		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.572003780918191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.483173600184791 | validation: 9.04099250153244]
	TIME [epoch: 78.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.415412592328688		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.0572189669064755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.236315779617581 | validation: 7.818455012407184]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.452013390244379		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.355865663562616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.403939526903498 | validation: 7.1855621381193036]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.440885341588327		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.260421665341615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.350653503464971 | validation: 6.5195628912610415]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.069317983306037		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.046916610935947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.05811729712099 | validation: 6.598253032146937]
	TIME [epoch: 8.24 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.094923650886476		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.835675160868754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.965299405877614 | validation: 6.3644514007390764]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.806611059414899		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.167671712024434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.987141385719667 | validation: 6.2905639814162235]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.8021957034858005		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.955732448769111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.878964076127456 | validation: 6.344929304526197]
	TIME [epoch: 8.2 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.776595460786207		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.763875456903215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.770235458844711 | validation: 6.043477772721037]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.665148035998336		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.910382423294198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.787765229646267 | validation: 6.058275529680759]
	TIME [epoch: 8.19 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.621578803971159		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.802894874337882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.712236839154522 | validation: 6.246383721647054]
	TIME [epoch: 8.47 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.657447041951588		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.5131830387132315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.585315040332412 | validation: 6.099981653539937]
	TIME [epoch: 8.22 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.457321595215344		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.695624231769613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.576472913492478 | validation: 5.960404585916612]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.482243569322882		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.538086324556365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.510164946939623 | validation: 5.931756736953927]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.365194461735738		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.499198507981251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.432196484858494 | validation: 5.99114838712481]
	TIME [epoch: 8.19 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.31223377191678		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.452254468750492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.382244120333636 | validation: 5.59249306127074]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.2892896833447995		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.358047450165186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.323668566754993 | validation: 5.741296596312122]
	TIME [epoch: 8.21 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.424621386236863		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.120754023667475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.27268770495217 | validation: 5.441329579506849]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.376637852001542		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.3203243134002065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.348481082700873 | validation: 5.662027075077759]
	TIME [epoch: 8.18 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.071194982349757		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.20304620913766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.137120595743709 | validation: 5.744683359190111]
	TIME [epoch: 8.24 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.091079744384768		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.15231058067795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.121695162531358 | validation: 5.028029217989286]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.038523056834081		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.111045833928438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.07478444538126 | validation: 4.882819123028187]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.2491036840219625		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.109369026158512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.179236355090238 | validation: 5.366636652240949]
	TIME [epoch: 8.18 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.170561571898331		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.965827460897251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.06819451639779 | validation: 5.2293038968347645]
	TIME [epoch: 8.23 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.93868645364313		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.9059945322098066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9223404929264674 | validation: 5.40781834021988]
	TIME [epoch: 8.19 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.9528087383480868		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.875656573903526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.914232656125806 | validation: 4.5693826493903815]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8811324671254552		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.802563345117675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.841847906121565 | validation: 4.606648574972841]
	TIME [epoch: 8.18 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.636753691512516		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.840521345826237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.738637518669376 | validation: 4.6726215045705395]
	TIME [epoch: 8.24 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.237810809357829		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.7965150690582568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.017162939208044 | validation: 4.695023366439648]
	TIME [epoch: 8.18 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.6585850580248027		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.6375562916988207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.648070674861812 | validation: 4.586786229605558]
	TIME [epoch: 8.18 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5758165473060046		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.628236660517262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.602026603911633 | validation: 4.621957953609347]
	TIME [epoch: 8.17 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.517833670942315		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.6532899464734867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5855618087079018 | validation: 5.5744561498905085]
	TIME [epoch: 8.24 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.549920266571405		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.38039625679901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4651582616852075 | validation: 3.1609116012874425]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.034617926347805		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.827066679991694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9308423031697495 | validation: 3.538730533732806]
	TIME [epoch: 8.19 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0291726755397783		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1288396641676526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0790061698537157 | validation: 2.950100361305549]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.136573711422069		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7905868034003247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.963580257411197 | validation: 3.5524545182539917]
	TIME [epoch: 8.24 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.983854172611357		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.868282036065873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9260681043386145 | validation: 3.0487700655377377]
	TIME [epoch: 8.18 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.236627297676617		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.100211967334624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1684196325056204 | validation: 3.6921277000619215]
	TIME [epoch: 8.16 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.785504090921033		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.913037209695099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.849270650308066 | validation: 3.911455871042007]
	TIME [epoch: 8.19 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.777136041104765		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9292784740705886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.853207257587677 | validation: 4.203245320225102]
	TIME [epoch: 8.22 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.777525093725244		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.864670061245715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8210975774854794 | validation: 3.37065945236155]
	TIME [epoch: 8.19 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.697282162717039		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7185254836753208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7079038231961805 | validation: 3.5547212836773823]
	TIME [epoch: 8.18 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6870633082162385		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0615454497119923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.874304378964116 | validation: 3.1344538359713887]
	TIME [epoch: 8.2 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9805798157273933		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1168786461362186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0487292309318055 | validation: 3.078038971232931]
	TIME [epoch: 8.22 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8741274945306032		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6097037258270253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7419156101788142 | validation: 3.5869570072925816]
	TIME [epoch: 8.2 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.50393615626223		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9193604731581124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7116483147101715 | validation: 3.082242996174992]
	TIME [epoch: 8.17 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8404468847089257		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.677323073291837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.758884979000382 | validation: 3.7323383400614505]
	TIME [epoch: 8.19 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8056360024074527		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7915215905734865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.79857879649047 | validation: 3.183882426237066]
	TIME [epoch: 8.22 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7018801900746934		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6653774388167273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6836288144457106 | validation: 2.972574242172362]
	TIME [epoch: 8.18 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.604246394689983		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7386800560538687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.671463225371926 | validation: 3.2906249762605246]
	TIME [epoch: 8.18 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.646150535659978		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7386986845066095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6924246100832936 | validation: 3.7999748957340866]
	TIME [epoch: 8.2 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6949005820487755		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.010234638959289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8525676105040327 | validation: 4.647344591215337]
	TIME [epoch: 8.22 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8204950107427846		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5720613418616693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6962781763022265 | validation: 4.499994305752217]
	TIME [epoch: 8.18 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1708691295461486		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.1436151201180245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.157242124832086 | validation: 3.292557711402062]
	TIME [epoch: 8.18 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8078968048034794		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.974840394751264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8913685997773717 | validation: 3.355796268532096]
	TIME [epoch: 8.2 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.669590595467244		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5605233826909384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6150569890790907 | validation: 2.938418526179598]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5434685487478053		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6751023242567156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6092854365022604 | validation: 3.154857492118931]
	TIME [epoch: 8.19 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6113895850404205		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4705702705950943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5409799278177574 | validation: 2.9364602899667496]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6830877561796167		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3370094011673954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5100485786735063 | validation: 2.703986771913948]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5004948153825777		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.554595195775709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5275450055791433 | validation: 2.8811290076866793]
	TIME [epoch: 8.19 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4291997515327406		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.506139908249407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4676698298910735 | validation: 3.3563527601425163]
	TIME [epoch: 8.19 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.533810391466481		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.41407291309291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4739416522796955 | validation: 2.7624728665161506]
	TIME [epoch: 8.17 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.494375554492617		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.577991134064344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.536183344278481 | validation: 3.4123021916985645]
	TIME [epoch: 8.21 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5603576506571835		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.495336673292407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5278471619747953 | validation: 3.294028521960072]
	TIME [epoch: 8.19 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5061626201578986		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.457575158190736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4818688891743172 | validation: 3.1848703818940414]
	TIME [epoch: 8.19 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5159498056528977		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4476039137612156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4817768597070566 | validation: 3.0603132191436897]
	TIME [epoch: 8.16 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.483053740979042		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3265170148351526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.404785377907097 | validation: 2.9186156565278685]
	TIME [epoch: 8.21 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3287408659225077		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5227538567331176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.425747361327813 | validation: 2.51615300045136]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2893518291685786		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.843358527394124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.566355178281351 | validation: 2.879887348661694]
	TIME [epoch: 8.21 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.442629316077562		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.441574197754893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4421017569162276 | validation: 2.6125991822402383]
	TIME [epoch: 8.22 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.380290090760348		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.488587799477524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4344389451189365 | validation: 3.041694646581875]
	TIME [epoch: 8.22 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.699620441748529		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5091584943061314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.60438946802733 | validation: 2.653384424149924]
	TIME [epoch: 8.22 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.358812794530779		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.557469959677966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4581413771043725 | validation: 2.527148052899194]
	TIME [epoch: 8.21 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.343599981487378		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.520039692736172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4318198371117745 | validation: 2.546962292324876]
	TIME [epoch: 8.22 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7269075603532844		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4807006515742693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.603804105963777 | validation: 2.5254384902150604]
	TIME [epoch: 8.21 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2492061644899257		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.392324934184148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.320765549337037 | validation: 2.8678761367810477]
	TIME [epoch: 8.22 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.37915579354955		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3063562035333254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.342755998541438 | validation: 2.6853047814851774]
	TIME [epoch: 8.21 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.627076309513625		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2840631200224744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.45556971476805 | validation: 2.5201879215291334]
	TIME [epoch: 8.23 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4329910904130534		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.282244128841946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3576176096274994 | validation: 2.832381480988808]
	TIME [epoch: 8.21 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.306424835874025		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.26313356461909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.284779200246557 | validation: 3.8081578247419436]
	TIME [epoch: 8.21 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2930203849704		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3316021081122407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.31231124654132 | validation: 2.525475223569784]
	TIME [epoch: 8.21 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3233131791149098		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3227783025622326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3230457408385705 | validation: 2.987701798492732]
	TIME [epoch: 8.23 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1998024681721815		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3405616342862365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.270182051229209 | validation: 2.519769491166349]
	TIME [epoch: 8.21 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2090074602917964		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.482350721163431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3456790907276135 | validation: 3.245401694361588]
	TIME [epoch: 8.2 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.309872656679652		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2030151684243577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.256443912552004 | validation: 2.4414591719897096]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4438581903337626		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2299165611957634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3368873757647637 | validation: 2.649901781592412]
	TIME [epoch: 8.23 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1580711965511146		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3671085968826704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.262589896716892 | validation: 2.5434344537086244]
	TIME [epoch: 8.19 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.308459003443166		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3317972509726075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.320128127207887 | validation: 2.450543809713275]
	TIME [epoch: 8.2 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.143840939180534		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2662169067699707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2050289229752527 | validation: 2.6431263597034387]
	TIME [epoch: 8.21 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3425965699887468		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3029965426081844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.322796556298465 | validation: 2.8257479581172777]
	TIME [epoch: 8.23 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.263129533837846		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3207927363607146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.29196113509928 | validation: 2.71964490027895]
	TIME [epoch: 8.19 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3788340561356067		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1773955680656614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.278114812100634 | validation: 2.722551601006785]
	TIME [epoch: 8.2 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2085603268808436		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6035323143503089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.906046320615576 | validation: 1.5926074764693003]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4657346606893795		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.469351955351525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4675433080204525 | validation: 0.8947889788911849]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1603830225128007		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.029672661624968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0950278420688844 | validation: 0.9991534763991339]
	TIME [epoch: 8.19 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9472650394995805		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9975822571434012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9724236483214911 | validation: 0.8071413061863231]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2434711996924208		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0323765454885874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1379238725905039 | validation: 0.6138246407006679]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9431599910321815		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8329022668776249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8880311289549032 | validation: 1.8813523266996033]
	TIME [epoch: 8.2 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9344352038430911		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9887324251369227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9615838144900071 | validation: 0.46633296064774743]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8828207350931343		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7705758116049337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8266982733490341 | validation: 2.1300854004603695]
	TIME [epoch: 8.2 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0506339178140116		[learning rate: 0.0099837]
		[batch 20/20] avg loss: 1.0941398731176697		[learning rate: 0.0099655]
	Learning Rate: 0.00996552
	LOSS [training: 1.0723868954658409 | validation: 1.3632695813668398]
	TIME [epoch: 8.22 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0199265589359003		[learning rate: 0.0099474]
		[batch 20/20] avg loss: 0.7795049051929471		[learning rate: 0.0099294]
	Learning Rate: 0.00992935
	LOSS [training: 0.8997157320644238 | validation: 1.1205729230710781]
	TIME [epoch: 8.19 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7770038518726141		[learning rate: 0.0099113]
		[batch 20/20] avg loss: 0.7730809768572029		[learning rate: 0.0098933]
	Learning Rate: 0.00989332
	LOSS [training: 0.7750424143649084 | validation: 0.7901569729677937]
	TIME [epoch: 8.18 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8060266286724665		[learning rate: 0.0098754]
		[batch 20/20] avg loss: 1.0572933125606336		[learning rate: 0.0098574]
	Learning Rate: 0.00985742
	LOSS [training: 0.9316599706165501 | validation: 0.8961831232825977]
	TIME [epoch: 8.19 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7584370763586166		[learning rate: 0.0098395]
		[batch 20/20] avg loss: 0.9530711006820554		[learning rate: 0.0098216]
	Learning Rate: 0.00982164
	LOSS [training: 0.8557540885203359 | validation: 0.6559209487965786]
	TIME [epoch: 8.23 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7273381818436808		[learning rate: 0.0098038]
		[batch 20/20] avg loss: 0.7594753745344488		[learning rate: 0.009786]
	Learning Rate: 0.009786
	LOSS [training: 0.7434067781890649 | validation: 0.630166902561117]
	TIME [epoch: 8.18 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6487419933901482		[learning rate: 0.0097682]
		[batch 20/20] avg loss: 0.6548169180220658		[learning rate: 0.0097505]
	Learning Rate: 0.00975049
	LOSS [training: 0.651779455706107 | validation: 0.6142608460709896]
	TIME [epoch: 8.17 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7093094544079392		[learning rate: 0.0097328]
		[batch 20/20] avg loss: 0.7393844942953001		[learning rate: 0.0097151]
	Learning Rate: 0.0097151
	LOSS [training: 0.7243469743516195 | validation: 0.4130748216762179]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.719987532397018		[learning rate: 0.0096975]
		[batch 20/20] avg loss: 0.7345665764340038		[learning rate: 0.0096798]
	Learning Rate: 0.00967984
	LOSS [training: 0.7272770544155109 | validation: 0.41260750472766383]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6272827172249622		[learning rate: 0.0096623]
		[batch 20/20] avg loss: 0.7531191287399436		[learning rate: 0.0096447]
	Learning Rate: 0.00964472
	LOSS [training: 0.6902009229824531 | validation: 0.3679937860643878]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6480296318046872		[learning rate: 0.0096272]
		[batch 20/20] avg loss: 0.611044089237083		[learning rate: 0.0096097]
	Learning Rate: 0.00960972
	LOSS [training: 0.6295368605208852 | validation: 0.9197675437134235]
	TIME [epoch: 8.17 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8039906970669032		[learning rate: 0.0095923]
		[batch 20/20] avg loss: 0.6023282752986363		[learning rate: 0.0095748]
	Learning Rate: 0.00957484
	LOSS [training: 0.7031594861827698 | validation: 0.7092250668371861]
	TIME [epoch: 8.19 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.618518776865184		[learning rate: 0.0095575]
		[batch 20/20] avg loss: 0.6203160442058285		[learning rate: 0.0095401]
	Learning Rate: 0.00954009
	LOSS [training: 0.6194174105355061 | validation: 0.9907460979616947]
	TIME [epoch: 8.23 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6567058163690181		[learning rate: 0.0095228]
		[batch 20/20] avg loss: 0.6274925486096414		[learning rate: 0.0095055]
	Learning Rate: 0.00950547
	LOSS [training: 0.6420991824893297 | validation: 0.6496838905972477]
	TIME [epoch: 8.18 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6314482294307807		[learning rate: 0.0094882]
		[batch 20/20] avg loss: 0.7366958672044712		[learning rate: 0.009471]
	Learning Rate: 0.00947098
	LOSS [training: 0.6840720483176261 | validation: 1.016229572876975]
	TIME [epoch: 8.18 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9409659330231049		[learning rate: 0.0094538]
		[batch 20/20] avg loss: 0.6296116668636326		[learning rate: 0.0094366]
	Learning Rate: 0.0094366
	LOSS [training: 0.7852887999433686 | validation: 0.7573418923728562]
	TIME [epoch: 8.21 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5973146244009813		[learning rate: 0.0094195]
		[batch 20/20] avg loss: 0.5506736348614036		[learning rate: 0.0094024]
	Learning Rate: 0.00940236
	LOSS [training: 0.5739941296311925 | validation: 0.792501426949317]
	TIME [epoch: 8.21 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5650118139779472		[learning rate: 0.0093853]
		[batch 20/20] avg loss: 0.7148693696898178		[learning rate: 0.0093682]
	Learning Rate: 0.00936824
	LOSS [training: 0.6399405918338824 | validation: 0.6768657052786624]
	TIME [epoch: 8.18 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6135622076923111		[learning rate: 0.0093512]
		[batch 20/20] avg loss: 0.587007812558212		[learning rate: 0.0093342]
	Learning Rate: 0.00933424
	LOSS [training: 0.6002850101252615 | validation: 0.43348411652523416]
	TIME [epoch: 8.17 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6255350574539634		[learning rate: 0.0093173]
		[batch 20/20] avg loss: 0.7382450197909323		[learning rate: 0.0093004]
	Learning Rate: 0.00930036
	LOSS [training: 0.6818900386224478 | validation: 0.7596603507745338]
	TIME [epoch: 8.19 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5101941057002612		[learning rate: 0.0092835]
		[batch 20/20] avg loss: 0.4816971930487372		[learning rate: 0.0092666]
	Learning Rate: 0.00926661
	LOSS [training: 0.49594564937449925 | validation: 0.37418245954812357]
	TIME [epoch: 8.21 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6718293386366009		[learning rate: 0.0092498]
		[batch 20/20] avg loss: 0.5559471072975477		[learning rate: 0.009233]
	Learning Rate: 0.00923298
	LOSS [training: 0.6138882229670743 | validation: 0.293474353328889]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5110955494660252		[learning rate: 0.0092162]
		[batch 20/20] avg loss: 0.7339760807218172		[learning rate: 0.0091995]
	Learning Rate: 0.00919948
	LOSS [training: 0.6225358150939212 | validation: 0.9982741489747451]
	TIME [epoch: 8.19 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5574177801808738		[learning rate: 0.0091828]
		[batch 20/20] avg loss: 0.5390491523209346		[learning rate: 0.0091661]
	Learning Rate: 0.00916609
	LOSS [training: 0.5482334662509043 | validation: 0.47356928069692444]
	TIME [epoch: 8.21 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5095952100970348		[learning rate: 0.0091494]
		[batch 20/20] avg loss: 0.56088577747608		[learning rate: 0.0091328]
	Learning Rate: 0.00913283
	LOSS [training: 0.5352404937865574 | validation: 0.45586507559515876]
	TIME [epoch: 8.23 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7594217197828078		[learning rate: 0.0091162]
		[batch 20/20] avg loss: 0.4953494791657279		[learning rate: 0.0090997]
	Learning Rate: 0.00909968
	LOSS [training: 0.627385599474268 | validation: 0.49874015330374827]
	TIME [epoch: 8.18 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5714558163982202		[learning rate: 0.0090832]
		[batch 20/20] avg loss: 0.5382220719752578		[learning rate: 0.0090667]
	Learning Rate: 0.00906666
	LOSS [training: 0.554838944186739 | validation: 0.44391806747214746]
	TIME [epoch: 8.18 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6589169323702728		[learning rate: 0.0090502]
		[batch 20/20] avg loss: 0.5196853193906532		[learning rate: 0.0090338]
	Learning Rate: 0.00903376
	LOSS [training: 0.589301125880463 | validation: 0.6432594381227943]
	TIME [epoch: 8.2 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.628008856401233		[learning rate: 0.0090174]
		[batch 20/20] avg loss: 0.6453383544303695		[learning rate: 0.009001]
	Learning Rate: 0.00900097
	LOSS [training: 0.6366736054158013 | validation: 0.6060214737761783]
	TIME [epoch: 8.21 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8054090454980619		[learning rate: 0.0089846]
		[batch 20/20] avg loss: 0.6313133983162201		[learning rate: 0.0089683]
	Learning Rate: 0.00896831
	LOSS [training: 0.7183612219071408 | validation: 0.6626983554115491]
	TIME [epoch: 8.18 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5986952230551278		[learning rate: 0.008952]
		[batch 20/20] avg loss: 0.4892082957486414		[learning rate: 0.0089358]
	Learning Rate: 0.00893576
	LOSS [training: 0.5439517594018846 | validation: 0.44300404486569256]
	TIME [epoch: 8.18 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49200971809192995		[learning rate: 0.0089195]
		[batch 20/20] avg loss: 0.6398216120238386		[learning rate: 0.0089033]
	Learning Rate: 0.00890333
	LOSS [training: 0.5659156650578844 | validation: 0.27787520084687517]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6025552147677992		[learning rate: 0.0088872]
		[batch 20/20] avg loss: 0.5737623461746999		[learning rate: 0.008871]
	Learning Rate: 0.00887102
	LOSS [training: 0.5881587804712496 | validation: 0.6502230160331344]
	TIME [epoch: 8.22 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5534972472060726		[learning rate: 0.0088549]
		[batch 20/20] avg loss: 0.5499957367053679		[learning rate: 0.0088388]
	Learning Rate: 0.00883883
	LOSS [training: 0.5517464919557203 | validation: 0.3458770208944617]
	TIME [epoch: 8.18 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5520711344015327		[learning rate: 0.0088228]
		[batch 20/20] avg loss: 0.46765517966294723		[learning rate: 0.0088068]
	Learning Rate: 0.00880675
	LOSS [training: 0.50986315703224 | validation: 0.3274461008718599]
	TIME [epoch: 8.17 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5519400775353456		[learning rate: 0.0087908]
		[batch 20/20] avg loss: 0.6965396512354196		[learning rate: 0.0087748]
	Learning Rate: 0.00877479
	LOSS [training: 0.6242398643853827 | validation: 0.6509171224752315]
	TIME [epoch: 8.2 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.839920556421178		[learning rate: 0.0087589]
		[batch 20/20] avg loss: 0.5203816815669832		[learning rate: 0.0087429]
	Learning Rate: 0.00874295
	LOSS [training: 0.6801511189940805 | validation: 0.24425121219710588]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41918783441544305		[learning rate: 0.0087271]
		[batch 20/20] avg loss: 0.5499279404724375		[learning rate: 0.0087112]
	Learning Rate: 0.00871122
	LOSS [training: 0.48455788744394035 | validation: 0.4430633433687624]
	TIME [epoch: 8.51 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48727577671807465		[learning rate: 0.0086954]
		[batch 20/20] avg loss: 0.7442009782993472		[learning rate: 0.0086796]
	Learning Rate: 0.00867961
	LOSS [training: 0.6157383775087109 | validation: 0.23702938777740873]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5382062336534322		[learning rate: 0.0086638]
		[batch 20/20] avg loss: 0.44393356574276926		[learning rate: 0.0086481]
	Learning Rate: 0.00864811
	LOSS [training: 0.4910698996981008 | validation: 0.440657331187429]
	TIME [epoch: 8.25 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5796348546716152		[learning rate: 0.0086324]
		[batch 20/20] avg loss: 0.4866991410874834		[learning rate: 0.0086167]
	Learning Rate: 0.00861672
	LOSS [training: 0.5331669978795492 | validation: 0.4699016961800502]
	TIME [epoch: 8.22 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4084210998606272		[learning rate: 0.0086011]
		[batch 20/20] avg loss: 0.47401335794234123		[learning rate: 0.0085855]
	Learning Rate: 0.00858545
	LOSS [training: 0.4412172289014843 | validation: 1.5377964800879504]
	TIME [epoch: 8.21 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5058891136855327		[learning rate: 0.0085699]
		[batch 20/20] avg loss: 0.40462553399644374		[learning rate: 0.0085543]
	Learning Rate: 0.00855429
	LOSS [training: 0.45525732384098827 | validation: 0.450386586259681]
	TIME [epoch: 8.2 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.402224588687106		[learning rate: 0.0085388]
		[batch 20/20] avg loss: 0.42763968245528927		[learning rate: 0.0085232]
	Learning Rate: 0.00852325
	LOSS [training: 0.4149321355711977 | validation: 0.359739783830306]
	TIME [epoch: 8.25 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4018815176097644		[learning rate: 0.0085078]
		[batch 20/20] avg loss: 0.5479928953252007		[learning rate: 0.0084923]
	Learning Rate: 0.00849232
	LOSS [training: 0.47493720646748255 | validation: 0.37518165511330254]
	TIME [epoch: 8.22 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5198601741605697		[learning rate: 0.0084769]
		[batch 20/20] avg loss: 0.49732405034600147		[learning rate: 0.0084615]
	Learning Rate: 0.0084615
	LOSS [training: 0.5085921122532855 | validation: 0.4718609975427932]
	TIME [epoch: 8.2 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.447187107942909		[learning rate: 0.0084461]
		[batch 20/20] avg loss: 0.4453194652056801		[learning rate: 0.0084308]
	Learning Rate: 0.00843079
	LOSS [training: 0.4462532865742945 | validation: 0.7206512378903409]
	TIME [epoch: 8.21 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4928037451060022		[learning rate: 0.0084155]
		[batch 20/20] avg loss: 0.3780868118953116		[learning rate: 0.0084002]
	Learning Rate: 0.0084002
	LOSS [training: 0.43544527850065695 | validation: 0.342504201952474]
	TIME [epoch: 8.22 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33983112564559625		[learning rate: 0.0083849]
		[batch 20/20] avg loss: 0.39583333719674346		[learning rate: 0.0083697]
	Learning Rate: 0.00836971
	LOSS [training: 0.36783223142116983 | validation: 0.23931119979842555]
	TIME [epoch: 8.23 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3933262322616849		[learning rate: 0.0083545]
		[batch 20/20] avg loss: 0.42806534100965266		[learning rate: 0.0083393]
	Learning Rate: 0.00833934
	LOSS [training: 0.4106957866356688 | validation: 0.28283968768390755]
	TIME [epoch: 8.21 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42851202956394135		[learning rate: 0.0083242]
		[batch 20/20] avg loss: 0.6668900455733391		[learning rate: 0.0083091]
	Learning Rate: 0.00830907
	LOSS [training: 0.5477010375686402 | validation: 3.214188060887804]
	TIME [epoch: 8.22 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0837021570211467		[learning rate: 0.008294]
		[batch 20/20] avg loss: 0.49586013905921805		[learning rate: 0.0082789]
	Learning Rate: 0.00827892
	LOSS [training: 0.7897811480401824 | validation: 0.5131706222952566]
	TIME [epoch: 8.23 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34112499392256923		[learning rate: 0.0082639]
		[batch 20/20] avg loss: 0.4502768206933826		[learning rate: 0.0082489]
	Learning Rate: 0.00824887
	LOSS [training: 0.39570090730797597 | validation: 0.2797482526497283]
	TIME [epoch: 8.23 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4777564387220936		[learning rate: 0.0082339]
		[batch 20/20] avg loss: 0.5286774355682093		[learning rate: 0.0082189]
	Learning Rate: 0.00821894
	LOSS [training: 0.5032169371451514 | validation: 0.7805587506427292]
	TIME [epoch: 8.2 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.472411135858987		[learning rate: 0.008204]
		[batch 20/20] avg loss: 0.4131724952620984		[learning rate: 0.0081891]
	Learning Rate: 0.00818911
	LOSS [training: 0.44279181556054265 | validation: 0.5493454293776568]
	TIME [epoch: 8.22 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33546229437965003		[learning rate: 0.0081742]
		[batch 20/20] avg loss: 0.4735667318661386		[learning rate: 0.0081594]
	Learning Rate: 0.00815939
	LOSS [training: 0.4045145131228944 | validation: 0.3526807722180233]
	TIME [epoch: 8.22 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.652773482291696		[learning rate: 0.0081446]
		[batch 20/20] avg loss: 0.6130996316973502		[learning rate: 0.0081298]
	Learning Rate: 0.00812978
	LOSS [training: 0.632936556994523 | validation: 0.4819528236367824]
	TIME [epoch: 8.24 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3792222841093807		[learning rate: 0.008115]
		[batch 20/20] avg loss: 0.3372985478237943		[learning rate: 0.0081003]
	Learning Rate: 0.00810028
	LOSS [training: 0.35826041596658753 | validation: 0.9798577135474731]
	TIME [epoch: 8.2 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4915864310921242		[learning rate: 0.0080856]
		[batch 20/20] avg loss: 0.4544759112854061		[learning rate: 0.0080709]
	Learning Rate: 0.00807088
	LOSS [training: 0.47303117118876503 | validation: 0.2887663110750364]
	TIME [epoch: 8.23 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37059521684565205		[learning rate: 0.0080562]
		[batch 20/20] avg loss: 0.3939253822441154		[learning rate: 0.0080416]
	Learning Rate: 0.00804159
	LOSS [training: 0.3822602995448837 | validation: 0.5297635914645256]
	TIME [epoch: 8.21 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48484021528256804		[learning rate: 0.008027]
		[batch 20/20] avg loss: 0.558957168612969		[learning rate: 0.0080124]
	Learning Rate: 0.00801241
	LOSS [training: 0.5218986919477684 | validation: 0.44936017607582723]
	TIME [epoch: 8.23 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.363586896925771		[learning rate: 0.0079979]
		[batch 20/20] avg loss: 0.5050454682354627		[learning rate: 0.0079833]
	Learning Rate: 0.00798333
	LOSS [training: 0.4343161825806169 | validation: 0.2911400778618388]
	TIME [epoch: 8.2 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41359254176183224		[learning rate: 0.0079688]
		[batch 20/20] avg loss: 0.5004869305646772		[learning rate: 0.0079544]
	Learning Rate: 0.00795436
	LOSS [training: 0.4570397361632548 | validation: 1.298783956509785]
	TIME [epoch: 8.23 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5373631599776524		[learning rate: 0.0079399]
		[batch 20/20] avg loss: 0.46972165294361057		[learning rate: 0.0079255]
	Learning Rate: 0.00792549
	LOSS [training: 0.5035424064606315 | validation: 0.23843910341402746]
	TIME [epoch: 8.21 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39929143459402644		[learning rate: 0.0079111]
		[batch 20/20] avg loss: 0.3675222028849268		[learning rate: 0.0078967]
	Learning Rate: 0.00789673
	LOSS [training: 0.3834068187394767 | validation: 0.7096444507403104]
	TIME [epoch: 8.23 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47992824526142097		[learning rate: 0.0078824]
		[batch 20/20] avg loss: 0.339319995162958		[learning rate: 0.0078681]
	Learning Rate: 0.00786807
	LOSS [training: 0.4096241202121894 | validation: 0.6088589066629281]
	TIME [epoch: 8.2 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4159510855708498		[learning rate: 0.0078538]
		[batch 20/20] avg loss: 0.39588766972272893		[learning rate: 0.0078395]
	Learning Rate: 0.00783952
	LOSS [training: 0.4059193776467893 | validation: 0.4772015154217901]
	TIME [epoch: 8.24 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40024460539568424		[learning rate: 0.0078253]
		[batch 20/20] avg loss: 0.5170315239387437		[learning rate: 0.0078111]
	Learning Rate: 0.00781107
	LOSS [training: 0.4586380646672141 | validation: 0.23866959704299856]
	TIME [epoch: 8.21 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4870186686861285		[learning rate: 0.0077969]
		[batch 20/20] avg loss: 0.36079947890525765		[learning rate: 0.0077827]
	Learning Rate: 0.00778272
	LOSS [training: 0.4239090737956929 | validation: 0.5221605303702925]
	TIME [epoch: 8.23 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5324592802076188		[learning rate: 0.0077686]
		[batch 20/20] avg loss: 0.6172295945429436		[learning rate: 0.0077545]
	Learning Rate: 0.00775448
	LOSS [training: 0.5748444373752812 | validation: 0.37912549165323883]
	TIME [epoch: 8.2 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3755434808894786		[learning rate: 0.0077404]
		[batch 20/20] avg loss: 0.5272838347909004		[learning rate: 0.0077263]
	Learning Rate: 0.00772634
	LOSS [training: 0.4514136578401895 | validation: 0.36214766789244973]
	TIME [epoch: 8.24 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3103092027476672		[learning rate: 0.0077123]
		[batch 20/20] avg loss: 0.3498095070212808		[learning rate: 0.0076983]
	Learning Rate: 0.0076983
	LOSS [training: 0.330059354884474 | validation: 0.644360124759856]
	TIME [epoch: 8.2 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3961113726057483		[learning rate: 0.0076843]
		[batch 20/20] avg loss: 0.44154896318285247		[learning rate: 0.0076704]
	Learning Rate: 0.00767036
	LOSS [training: 0.4188301678943004 | validation: 0.3957637635871064]
	TIME [epoch: 8.23 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40223076901621857		[learning rate: 0.0076564]
		[batch 20/20] avg loss: 0.384647217288154		[learning rate: 0.0076425]
	Learning Rate: 0.00764252
	LOSS [training: 0.3934389931521863 | validation: 0.6430512279005912]
	TIME [epoch: 8.21 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3791904699576315		[learning rate: 0.0076286]
		[batch 20/20] avg loss: 0.5304437841985365		[learning rate: 0.0076148]
	Learning Rate: 0.00761479
	LOSS [training: 0.454817127078084 | validation: 0.23123640763194522]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30834248715624657		[learning rate: 0.007601]
		[batch 20/20] avg loss: 0.39678582847061056		[learning rate: 0.0075872]
	Learning Rate: 0.00758715
	LOSS [training: 0.35256415781342854 | validation: 0.6079751978821655]
	TIME [epoch: 8.2 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3480911570625469		[learning rate: 0.0075734]
		[batch 20/20] avg loss: 0.5415831803594771		[learning rate: 0.0075596]
	Learning Rate: 0.00755962
	LOSS [training: 0.444837168711012 | validation: 0.33638377030115046]
	TIME [epoch: 8.23 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29943478469304935		[learning rate: 0.0075459]
		[batch 20/20] avg loss: 0.4134551244250737		[learning rate: 0.0075322]
	Learning Rate: 0.00753219
	LOSS [training: 0.35644495455906156 | validation: 0.23548220368557796]
	TIME [epoch: 8.22 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31545916387891976		[learning rate: 0.0075185]
		[batch 20/20] avg loss: 0.37237020797594556		[learning rate: 0.0075049]
	Learning Rate: 0.00750485
	LOSS [training: 0.34391468592743263 | validation: 0.2340128776225417]
	TIME [epoch: 8.22 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3191036381032655		[learning rate: 0.0074912]
		[batch 20/20] avg loss: 0.4037799969304525		[learning rate: 0.0074776]
	Learning Rate: 0.00747762
	LOSS [training: 0.361441817516859 | validation: 0.32755890751285555]
	TIME [epoch: 8.19 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3677049612004394		[learning rate: 0.007464]
		[batch 20/20] avg loss: 0.3746009397113244		[learning rate: 0.0074505]
	Learning Rate: 0.00745048
	LOSS [training: 0.37115295045588187 | validation: 0.7082647793880128]
	TIME [epoch: 8.23 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3620430904030714		[learning rate: 0.0074369]
		[batch 20/20] avg loss: 0.3289621354766146		[learning rate: 0.0074234]
	Learning Rate: 0.00742344
	LOSS [training: 0.345502612939843 | validation: 0.19668903070811447]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3307027626568101		[learning rate: 0.00741]
		[batch 20/20] avg loss: 0.3079132266470824		[learning rate: 0.0073965]
	Learning Rate: 0.0073965
	LOSS [training: 0.31930799465194626 | validation: 0.18731282169830837]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5661436244410447		[learning rate: 0.0073831]
		[batch 20/20] avg loss: 0.39746994313983947		[learning rate: 0.0073697]
	Learning Rate: 0.00736966
	LOSS [training: 0.4818067837904422 | validation: 0.141229580633772]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2995790638583126		[learning rate: 0.0073563]
		[batch 20/20] avg loss: 0.41226035568146785		[learning rate: 0.0073429]
	Learning Rate: 0.00734291
	LOSS [training: 0.3559197097698903 | validation: 0.4219960930905361]
	TIME [epoch: 8.21 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39007103256964315		[learning rate: 0.0073296]
		[batch 20/20] avg loss: 0.2817347205095431		[learning rate: 0.0073163]
	Learning Rate: 0.00731627
	LOSS [training: 0.3359028765395931 | validation: 0.18343299894373094]
	TIME [epoch: 8.21 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28989998241580295		[learning rate: 0.007303]
		[batch 20/20] avg loss: 0.4703414362484793		[learning rate: 0.0072897]
	Learning Rate: 0.00728971
	LOSS [training: 0.38012070933214115 | validation: 0.2284752107736687]
	TIME [epoch: 8.18 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3882231582824173		[learning rate: 0.0072765]
		[batch 20/20] avg loss: 0.37045935575834416		[learning rate: 0.0072633]
	Learning Rate: 0.00726326
	LOSS [training: 0.37934125702038074 | validation: 0.8125791887138325]
	TIME [epoch: 8.18 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35805962211016495		[learning rate: 0.0072501]
		[batch 20/20] avg loss: 0.3169751483338706		[learning rate: 0.0072369]
	Learning Rate: 0.0072369
	LOSS [training: 0.33751738522201774 | validation: 0.33224181376807327]
	TIME [epoch: 8.21 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26498956041728466		[learning rate: 0.0072238]
		[batch 20/20] avg loss: 0.5412128391729347		[learning rate: 0.0072106]
	Learning Rate: 0.00721064
	LOSS [training: 0.4031011997951097 | validation: 0.5286877737213361]
	TIME [epoch: 8.22 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29770137534899177		[learning rate: 0.0071975]
		[batch 20/20] avg loss: 0.37253372266192164		[learning rate: 0.0071845]
	Learning Rate: 0.00718447
	LOSS [training: 0.33511754900545665 | validation: 0.5059014475863958]
	TIME [epoch: 8.18 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3484392430236884		[learning rate: 0.0071714]
		[batch 20/20] avg loss: 0.4385586913994278		[learning rate: 0.0071584]
	Learning Rate: 0.0071584
	LOSS [training: 0.39349896721155814 | validation: 0.33967210706472917]
	TIME [epoch: 8.18 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3718146320852253		[learning rate: 0.0071454]
		[batch 20/20] avg loss: 0.307982375418185		[learning rate: 0.0071324]
	Learning Rate: 0.00713242
	LOSS [training: 0.33989850375170516 | validation: 0.8141036668155953]
	TIME [epoch: 8.21 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5648728649141129		[learning rate: 0.0071195]
		[batch 20/20] avg loss: 0.44769206875488987		[learning rate: 0.0071065]
	Learning Rate: 0.00710653
	LOSS [training: 0.5062824668345015 | validation: 0.713256117432388]
	TIME [epoch: 8.22 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4777296743384193		[learning rate: 0.0070936]
		[batch 20/20] avg loss: 0.39024320965823883		[learning rate: 0.0070807]
	Learning Rate: 0.00708074
	LOSS [training: 0.43398644199832914 | validation: 0.6816094387533105]
	TIME [epoch: 8.18 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4123283071336141		[learning rate: 0.0070679]
		[batch 20/20] avg loss: 0.31959958915949505		[learning rate: 0.007055]
	Learning Rate: 0.00705505
	LOSS [training: 0.36596394814655453 | validation: 0.3574557875945064]
	TIME [epoch: 8.18 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26700583669626654		[learning rate: 0.0070422]
		[batch 20/20] avg loss: 0.531038279670658		[learning rate: 0.0070294]
	Learning Rate: 0.00702945
	LOSS [training: 0.39902205818346226 | validation: 0.2790431986530732]
	TIME [epoch: 8.21 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3266814776759648		[learning rate: 0.0070167]
		[batch 20/20] avg loss: 0.3407052642321171		[learning rate: 0.0070039]
	Learning Rate: 0.00700394
	LOSS [training: 0.33369337095404095 | validation: 0.3883909683111398]
	TIME [epoch: 8.22 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33805775113053094		[learning rate: 0.0069912]
		[batch 20/20] avg loss: 0.3115177341991579		[learning rate: 0.0069785]
	Learning Rate: 0.00697852
	LOSS [training: 0.32478774266484445 | validation: 0.5714143942425268]
	TIME [epoch: 8.18 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3378529524446118		[learning rate: 0.0069658]
		[batch 20/20] avg loss: 0.3860396794892152		[learning rate: 0.0069532]
	Learning Rate: 0.00695319
	LOSS [training: 0.3619463159669135 | validation: 0.2871106325412043]
	TIME [epoch: 8.18 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32899284029316667		[learning rate: 0.0069406]
		[batch 20/20] avg loss: 0.36559655658599044		[learning rate: 0.006928]
	Learning Rate: 0.00692796
	LOSS [training: 0.34729469843957855 | validation: 0.28242493728695284]
	TIME [epoch: 8.21 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2975797692611578		[learning rate: 0.0069154]
		[batch 20/20] avg loss: 0.3905408193556017		[learning rate: 0.0069028]
	Learning Rate: 0.00690282
	LOSS [training: 0.34406029430837975 | validation: 0.2876751490799173]
	TIME [epoch: 8.22 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3989783590346387		[learning rate: 0.0068903]
		[batch 20/20] avg loss: 0.2640574965908596		[learning rate: 0.0068778]
	Learning Rate: 0.00687777
	LOSS [training: 0.3315179278127492 | validation: 0.39256534062126475]
	TIME [epoch: 8.18 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27536684116420657		[learning rate: 0.0068653]
		[batch 20/20] avg loss: 0.34373662134756827		[learning rate: 0.0068528]
	Learning Rate: 0.00685281
	LOSS [training: 0.3095517312558874 | validation: 0.15328848399191503]
	TIME [epoch: 8.19 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3779327795959636		[learning rate: 0.0068404]
		[batch 20/20] avg loss: 0.28390681043273147		[learning rate: 0.0068279]
	Learning Rate: 0.00682794
	LOSS [training: 0.33091979501434754 | validation: 0.3726586003479616]
	TIME [epoch: 8.22 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30641195454189607		[learning rate: 0.0068155]
		[batch 20/20] avg loss: 0.4075429295190801		[learning rate: 0.0068032]
	Learning Rate: 0.00680316
	LOSS [training: 0.35697744203048815 | validation: 0.41892674928462326]
	TIME [epoch: 8.22 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3288768179050712		[learning rate: 0.0067908]
		[batch 20/20] avg loss: 0.33257067139262075		[learning rate: 0.0067785]
	Learning Rate: 0.00677847
	LOSS [training: 0.33072374464884596 | validation: 0.2636397282205005]
	TIME [epoch: 8.18 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29174668991074904		[learning rate: 0.0067662]
		[batch 20/20] avg loss: 0.3009103935901666		[learning rate: 0.0067539]
	Learning Rate: 0.00675387
	LOSS [training: 0.29632854175045786 | validation: 0.31868895880394726]
	TIME [epoch: 8.18 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2951434326596288		[learning rate: 0.0067416]
		[batch 20/20] avg loss: 0.40589017319010684		[learning rate: 0.0067294]
	Learning Rate: 0.00672936
	LOSS [training: 0.3505168029248678 | validation: 0.2658530454482976]
	TIME [epoch: 8.23 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26999232535946277		[learning rate: 0.0067171]
		[batch 20/20] avg loss: 0.3444630564089778		[learning rate: 0.0067049]
	Learning Rate: 0.00670494
	LOSS [training: 0.30722769088422025 | validation: 0.24757223620744923]
	TIME [epoch: 8.21 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29141087712234265		[learning rate: 0.0066928]
		[batch 20/20] avg loss: 0.29466299007887176		[learning rate: 0.0066806]
	Learning Rate: 0.0066806
	LOSS [training: 0.29303693360060723 | validation: 0.45940129441766325]
	TIME [epoch: 8.18 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3677703486773108		[learning rate: 0.0066685]
		[batch 20/20] avg loss: 0.3926621130579865		[learning rate: 0.0066564]
	Learning Rate: 0.00665636
	LOSS [training: 0.3802162308676486 | validation: 0.21867398436666252]
	TIME [epoch: 8.18 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28246941657361524		[learning rate: 0.0066443]
		[batch 20/20] avg loss: 0.37733074589993343		[learning rate: 0.0066322]
	Learning Rate: 0.0066322
	LOSS [training: 0.32990008123677433 | validation: 0.48152159133068156]
	TIME [epoch: 8.22 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35652443147167057		[learning rate: 0.0066202]
		[batch 20/20] avg loss: 0.3051076048380915		[learning rate: 0.0066081]
	Learning Rate: 0.00660814
	LOSS [training: 0.33081601815488104 | validation: 0.1786227836104109]
	TIME [epoch: 8.2 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2607917003808827		[learning rate: 0.0065961]
		[batch 20/20] avg loss: 0.3536535058895274		[learning rate: 0.0065842]
	Learning Rate: 0.00658415
	LOSS [training: 0.307222603135205 | validation: 0.5010639741499054]
	TIME [epoch: 8.18 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38969957085255336		[learning rate: 0.0065722]
		[batch 20/20] avg loss: 0.39511422577059996		[learning rate: 0.0065603]
	Learning Rate: 0.00656026
	LOSS [training: 0.39240689831157666 | validation: 0.22876756114412528]
	TIME [epoch: 8.18 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29143952637848825		[learning rate: 0.0065483]
		[batch 20/20] avg loss: 0.3494837745059193		[learning rate: 0.0065365]
	Learning Rate: 0.00653645
	LOSS [training: 0.3204616504422038 | validation: 0.27144733076029093]
	TIME [epoch: 8.22 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3780412478461112		[learning rate: 0.0065246]
		[batch 20/20] avg loss: 0.28365669932600507		[learning rate: 0.0065127]
	Learning Rate: 0.00651273
	LOSS [training: 0.33084897358605814 | validation: 0.26772827850265224]
	TIME [epoch: 8.22 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2609458711693084		[learning rate: 0.0065009]
		[batch 20/20] avg loss: 0.32308569604471693		[learning rate: 0.0064891]
	Learning Rate: 0.0064891
	LOSS [training: 0.29201578360701264 | validation: 0.3173182361605835]
	TIME [epoch: 8.18 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3524705894400356		[learning rate: 0.0064773]
		[batch 20/20] avg loss: 0.33084804745470253		[learning rate: 0.0064655]
	Learning Rate: 0.00646555
	LOSS [training: 0.34165931844736896 | validation: 0.20802372756088583]
	TIME [epoch: 8.18 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26153824039249207		[learning rate: 0.0064538]
		[batch 20/20] avg loss: 0.43935866787623323		[learning rate: 0.0064421]
	Learning Rate: 0.00644208
	LOSS [training: 0.3504484541343627 | validation: 0.5486970753921669]
	TIME [epoch: 8.23 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42493243733835745		[learning rate: 0.0064304]
		[batch 20/20] avg loss: 0.45141909500115435		[learning rate: 0.0064187]
	Learning Rate: 0.0064187
	LOSS [training: 0.43817576616975584 | validation: 0.2822927834385515]
	TIME [epoch: 8.21 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21491917544586842		[learning rate: 0.006407]
		[batch 20/20] avg loss: 0.44090296725719325		[learning rate: 0.0063954]
	Learning Rate: 0.00639541
	LOSS [training: 0.32791107135153086 | validation: 0.36354012664893615]
	TIME [epoch: 8.18 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2498742849830053		[learning rate: 0.0063838]
		[batch 20/20] avg loss: 0.2875985753999021		[learning rate: 0.0063722]
	Learning Rate: 0.0063722
	LOSS [training: 0.26873643019145366 | validation: 0.13700435444840456]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3268850803162061		[learning rate: 0.0063606]
		[batch 20/20] avg loss: 0.2676090631679532		[learning rate: 0.0063491]
	Learning Rate: 0.00634908
	LOSS [training: 0.29724707174207965 | validation: 0.21620462352721853]
	TIME [epoch: 8.23 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32361577749981374		[learning rate: 0.0063375]
		[batch 20/20] avg loss: 0.315985886480325		[learning rate: 0.006326]
	Learning Rate: 0.00632603
	LOSS [training: 0.31980083199006937 | validation: 0.4066978789056992]
	TIME [epoch: 8.2 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38191589620129196		[learning rate: 0.0063145]
		[batch 20/20] avg loss: 0.29664794757025204		[learning rate: 0.0063031]
	Learning Rate: 0.00630308
	LOSS [training: 0.3392819218857721 | validation: 0.13598618634670825]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21545040751969138		[learning rate: 0.0062916]
		[batch 20/20] avg loss: 0.2437569969220839		[learning rate: 0.0062802]
	Learning Rate: 0.0062802
	LOSS [training: 0.2296037022208876 | validation: 0.6716834772803757]
	TIME [epoch: 8.19 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3902326165457081		[learning rate: 0.0062688]
		[batch 20/20] avg loss: 0.2365852090650261		[learning rate: 0.0062574]
	Learning Rate: 0.00625741
	LOSS [training: 0.3134089128053671 | validation: 0.4186533287379839]
	TIME [epoch: 8.23 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2921439475591981		[learning rate: 0.006246]
		[batch 20/20] avg loss: 0.19013859879700773		[learning rate: 0.0062347]
	Learning Rate: 0.0062347
	LOSS [training: 0.2411412731781029 | validation: 0.39574344655551297]
	TIME [epoch: 8.19 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31888659049002743		[learning rate: 0.0062234]
		[batch 20/20] avg loss: 0.3484296672731202		[learning rate: 0.0062121]
	Learning Rate: 0.00621208
	LOSS [training: 0.3336581288815738 | validation: 0.15800088474011104]
	TIME [epoch: 8.18 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.301953161636922		[learning rate: 0.0062008]
		[batch 20/20] avg loss: 0.28162765027194564		[learning rate: 0.0061895]
	Learning Rate: 0.00618953
	LOSS [training: 0.29179040595443373 | validation: 0.4503567440286471]
	TIME [epoch: 8.18 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31301043597148126		[learning rate: 0.0061783]
		[batch 20/20] avg loss: 0.30755619095974585		[learning rate: 0.0061671]
	Learning Rate: 0.00616707
	LOSS [training: 0.3102833134656135 | validation: 0.18785338404182678]
	TIME [epoch: 8.24 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4668778963546004		[learning rate: 0.0061559]
		[batch 20/20] avg loss: 0.296795224045043		[learning rate: 0.0061447]
	Learning Rate: 0.00614469
	LOSS [training: 0.3818365601998217 | validation: 0.4001421067848512]
	TIME [epoch: 8.19 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3149492833538923		[learning rate: 0.0061335]
		[batch 20/20] avg loss: 0.37570891288899766		[learning rate: 0.0061224]
	Learning Rate: 0.00612239
	LOSS [training: 0.3453290981214451 | validation: 0.21367784506476872]
	TIME [epoch: 8.18 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3640343209296383		[learning rate: 0.0061113]
		[batch 20/20] avg loss: 0.3057529048742886		[learning rate: 0.0061002]
	Learning Rate: 0.00610017
	LOSS [training: 0.33489361290196346 | validation: 0.8798922484685674]
	TIME [epoch: 8.2 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3479842447362681		[learning rate: 0.0060891]
		[batch 20/20] avg loss: 0.2667301452988762		[learning rate: 0.006078]
	Learning Rate: 0.00607803
	LOSS [training: 0.30735719501757214 | validation: 0.2369521126432963]
	TIME [epoch: 8.22 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3545673832460835		[learning rate: 0.006067]
		[batch 20/20] avg loss: 0.3153203303096398		[learning rate: 0.006056]
	Learning Rate: 0.00605598
	LOSS [training: 0.33494385677786165 | validation: 0.3441513057408473]
	TIME [epoch: 8.2 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3115620762613773		[learning rate: 0.006045]
		[batch 20/20] avg loss: 0.3161013586104202		[learning rate: 0.006034]
	Learning Rate: 0.006034
	LOSS [training: 0.3138317174358987 | validation: 0.2515361882555889]
	TIME [epoch: 8.18 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.470656150880882		[learning rate: 0.006023]
		[batch 20/20] avg loss: 0.338209372576305		[learning rate: 0.0060121]
	Learning Rate: 0.0060121
	LOSS [training: 0.40443276172859355 | validation: 0.18539482401986093]
	TIME [epoch: 8.2 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23281427676626132		[learning rate: 0.0060012]
		[batch 20/20] avg loss: 0.30367316864147953		[learning rate: 0.0059903]
	Learning Rate: 0.00599028
	LOSS [training: 0.2682437227038704 | validation: 0.2470811887059473]
	TIME [epoch: 8.23 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2774351519961692		[learning rate: 0.0059794]
		[batch 20/20] avg loss: 0.4383446852914189		[learning rate: 0.0059685]
	Learning Rate: 0.00596854
	LOSS [training: 0.35788991864379405 | validation: 0.4542836891773869]
	TIME [epoch: 8.21 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2652326839344482		[learning rate: 0.0059577]
		[batch 20/20] avg loss: 0.2655367103172526		[learning rate: 0.0059469]
	Learning Rate: 0.00594688
	LOSS [training: 0.26538469712585044 | validation: 0.4580443349809401]
	TIME [epoch: 8.18 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26753634618767325		[learning rate: 0.0059361]
		[batch 20/20] avg loss: 0.3088689902068059		[learning rate: 0.0059253]
	Learning Rate: 0.0059253
	LOSS [training: 0.2882026681972395 | validation: 0.28391157687246904]
	TIME [epoch: 8.2 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20876190036517875		[learning rate: 0.0059145]
		[batch 20/20] avg loss: 0.32168790793916274		[learning rate: 0.0059038]
	Learning Rate: 0.0059038
	LOSS [training: 0.26522490415217076 | validation: 0.17617022632268595]
	TIME [epoch: 8.22 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3408677394323329		[learning rate: 0.0058931]
		[batch 20/20] avg loss: 0.28161665089352494		[learning rate: 0.0058824]
	Learning Rate: 0.00588237
	LOSS [training: 0.3112421951629289 | validation: 0.6962480087566094]
	TIME [epoch: 8.2 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3249609738043494		[learning rate: 0.0058717]
		[batch 20/20] avg loss: 0.25064575911952686		[learning rate: 0.005861]
	Learning Rate: 0.00586103
	LOSS [training: 0.2878033664619381 | validation: 0.3305337161140234]
	TIME [epoch: 8.19 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.242436058645773		[learning rate: 0.0058504]
		[batch 20/20] avg loss: 0.29909367025422184		[learning rate: 0.0058398]
	Learning Rate: 0.00583976
	LOSS [training: 0.2707648644499975 | validation: 0.19307380650193873]
	TIME [epoch: 8.21 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2446725550164862		[learning rate: 0.0058291]
		[batch 20/20] avg loss: 0.2795413785347979		[learning rate: 0.0058186]
	Learning Rate: 0.00581856
	LOSS [training: 0.26210696677564205 | validation: 0.47262310304842003]
	TIME [epoch: 8.21 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23050824961267674		[learning rate: 0.005808]
		[batch 20/20] avg loss: 0.24287963925191663		[learning rate: 0.0057974]
	Learning Rate: 0.00579745
	LOSS [training: 0.23669394443229658 | validation: 0.304158716908793]
	TIME [epoch: 8.21 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3493035595693734		[learning rate: 0.0057869]
		[batch 20/20] avg loss: 0.22093528546533858		[learning rate: 0.0057764]
	Learning Rate: 0.00577641
	LOSS [training: 0.285119422517356 | validation: 0.3465977922617941]
	TIME [epoch: 8.18 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32491256235782434		[learning rate: 0.0057659]
		[batch 20/20] avg loss: 0.33124467489096115		[learning rate: 0.0057554]
	Learning Rate: 0.00575545
	LOSS [training: 0.32807861862439275 | validation: 0.14541730829121183]
	TIME [epoch: 8.2 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3226395757816921		[learning rate: 0.005745]
		[batch 20/20] avg loss: 0.2986543545781867		[learning rate: 0.0057346]
	Learning Rate: 0.00573456
	LOSS [training: 0.3106469651799394 | validation: 0.34762254893253397]
	TIME [epoch: 8.21 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2727764331324309		[learning rate: 0.0057241]
		[batch 20/20] avg loss: 0.24615905658678852		[learning rate: 0.0057137]
	Learning Rate: 0.00571375
	LOSS [training: 0.2594677448596097 | validation: 0.229297703083913]
	TIME [epoch: 8.21 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23513205527134265		[learning rate: 0.0057034]
		[batch 20/20] avg loss: 0.26204349970059204		[learning rate: 0.005693]
	Learning Rate: 0.00569301
	LOSS [training: 0.24858777748596733 | validation: 0.1570082813562516]
	TIME [epoch: 8.18 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37302122216548467		[learning rate: 0.0056827]
		[batch 20/20] avg loss: 0.2519302147364143		[learning rate: 0.0056724]
	Learning Rate: 0.00567235
	LOSS [training: 0.31247571845094946 | validation: 0.32899044676063316]
	TIME [epoch: 8.21 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29643811639021767		[learning rate: 0.005662]
		[batch 20/20] avg loss: 0.3252473527668909		[learning rate: 0.0056518]
	Learning Rate: 0.00565177
	LOSS [training: 0.31084273457855427 | validation: 0.3111569711009785]
	TIME [epoch: 8.2 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26218116227292254		[learning rate: 0.0056415]
		[batch 20/20] avg loss: 0.26189256218392254		[learning rate: 0.0056313]
	Learning Rate: 0.00563126
	LOSS [training: 0.2620368622284225 | validation: 0.25611668739996246]
	TIME [epoch: 8.2 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2553944847032379		[learning rate: 0.005621]
		[batch 20/20] avg loss: 0.25993418858988304		[learning rate: 0.0056108]
	Learning Rate: 0.00561082
	LOSS [training: 0.2576643366465604 | validation: 0.286261879031825]
	TIME [epoch: 8.19 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3426027192134661		[learning rate: 0.0056006]
		[batch 20/20] avg loss: 0.22368610558822555		[learning rate: 0.0055905]
	Learning Rate: 0.00559046
	LOSS [training: 0.2831444124008458 | validation: 0.1676586305748394]
	TIME [epoch: 8.22 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24926660366202355		[learning rate: 0.0055803]
		[batch 20/20] avg loss: 0.3509750314317473		[learning rate: 0.0055702]
	Learning Rate: 0.00557017
	LOSS [training: 0.3001208175468854 | validation: 0.3751582120267461]
	TIME [epoch: 8.19 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2882330008377787		[learning rate: 0.0055601]
		[batch 20/20] avg loss: 0.2535561098142164		[learning rate: 0.00555]
	Learning Rate: 0.00554996
	LOSS [training: 0.27089455532599754 | validation: 0.4764155089149571]
	TIME [epoch: 8.21 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2673063080709237		[learning rate: 0.0055399]
		[batch 20/20] avg loss: 0.2567183326155957		[learning rate: 0.0055298]
	Learning Rate: 0.00552981
	LOSS [training: 0.2620123203432597 | validation: 0.2543618894136085]
	TIME [epoch: 8.19 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37544634983432046		[learning rate: 0.0055198]
		[batch 20/20] avg loss: 0.3194224678242302		[learning rate: 0.0055097]
	Learning Rate: 0.00550975
	LOSS [training: 0.34743440882927534 | validation: 0.2648864802960258]
	TIME [epoch: 8.2 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22751282880989634		[learning rate: 0.0054997]
		[batch 20/20] avg loss: 0.26395268712293374		[learning rate: 0.0054898]
	Learning Rate: 0.00548975
	LOSS [training: 0.24573275796641503 | validation: 0.22988391559449145]
	TIME [epoch: 8.19 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28447322441101075		[learning rate: 0.0054798]
		[batch 20/20] avg loss: 0.22422211631838024		[learning rate: 0.0054698]
	Learning Rate: 0.00546983
	LOSS [training: 0.2543476703646954 | validation: 0.18890057194083712]
	TIME [epoch: 8.2 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24335108580451817		[learning rate: 0.0054599]
		[batch 20/20] avg loss: 0.20184282150944108		[learning rate: 0.00545]
	Learning Rate: 0.00544998
	LOSS [training: 0.22259695365697968 | validation: 0.2101787510650517]
	TIME [epoch: 8.18 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2242569071902661		[learning rate: 0.0054401]
		[batch 20/20] avg loss: 0.2295018975590491		[learning rate: 0.0054302]
	Learning Rate: 0.0054302
	LOSS [training: 0.2268794023746576 | validation: 0.37963136518217344]
	TIME [epoch: 8.23 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3702807557500306		[learning rate: 0.0054203]
		[batch 20/20] avg loss: 0.30020905132261233		[learning rate: 0.0054105]
	Learning Rate: 0.00541049
	LOSS [training: 0.3352449035363215 | validation: 0.13474789487918293]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23977916907940244		[learning rate: 0.0054007]
		[batch 20/20] avg loss: 0.33171039526523577		[learning rate: 0.0053909]
	Learning Rate: 0.00539086
	LOSS [training: 0.2857447821723191 | validation: 0.3691153479433932]
	TIME [epoch: 8.2 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4069068490206275		[learning rate: 0.0053811]
		[batch 20/20] avg loss: 0.23886932560272234		[learning rate: 0.0053713]
	Learning Rate: 0.00537129
	LOSS [training: 0.32288808731167495 | validation: 0.3145093179697012]
	TIME [epoch: 8.17 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22919826174765054		[learning rate: 0.0053615]
		[batch 20/20] avg loss: 0.29475323117608754		[learning rate: 0.0053518]
	Learning Rate: 0.0053518
	LOSS [training: 0.26197574646186905 | validation: 0.3461225700013352]
	TIME [epoch: 8.21 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2741978214351929		[learning rate: 0.0053421]
		[batch 20/20] avg loss: 0.3293543852054044		[learning rate: 0.0053324]
	Learning Rate: 0.00533238
	LOSS [training: 0.30177610332029864 | validation: 0.31075903801452576]
	TIME [epoch: 8.18 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2784062915510828		[learning rate: 0.0053227]
		[batch 20/20] avg loss: 0.33810938819062397		[learning rate: 0.005313]
	Learning Rate: 0.00531303
	LOSS [training: 0.3082578398708534 | validation: 0.271039220752606]
	TIME [epoch: 8.2 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24854177464879826		[learning rate: 0.0053034]
		[batch 20/20] avg loss: 0.3692824937356735		[learning rate: 0.0052937]
	Learning Rate: 0.00529375
	LOSS [training: 0.30891213419223584 | validation: 0.4872256727938321]
	TIME [epoch: 8.18 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24361456748049562		[learning rate: 0.0052841]
		[batch 20/20] avg loss: 0.25075028006340877		[learning rate: 0.0052745]
	Learning Rate: 0.00527454
	LOSS [training: 0.24718242377195215 | validation: 0.45007461886650485]
	TIME [epoch: 8.22 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.263870566084058		[learning rate: 0.005265]
		[batch 20/20] avg loss: 0.2137829559148622		[learning rate: 0.0052554]
	Learning Rate: 0.00525539
	LOSS [training: 0.2388267609994601 | validation: 0.2262100388806515]
	TIME [epoch: 8.18 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29583453552994576		[learning rate: 0.0052458]
		[batch 20/20] avg loss: 0.2479783180603762		[learning rate: 0.0052363]
	Learning Rate: 0.00523632
	LOSS [training: 0.2719064267951609 | validation: 0.20234792439448807]
	TIME [epoch: 8.2 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1907552686408563		[learning rate: 0.0052268]
		[batch 20/20] avg loss: 0.26297435480902115		[learning rate: 0.0052173]
	Learning Rate: 0.00521732
	LOSS [training: 0.22686481172493864 | validation: 0.12549869017784854]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2741594251951155		[learning rate: 0.0052078]
		[batch 20/20] avg loss: 0.26918603495190896		[learning rate: 0.0051984]
	Learning Rate: 0.00519839
	LOSS [training: 0.27167273007351217 | validation: 0.2577339527408355]
	TIME [epoch: 8.21 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18081112339095032		[learning rate: 0.0051889]
		[batch 20/20] avg loss: 0.19884601213647413		[learning rate: 0.0051795]
	Learning Rate: 0.00517952
	LOSS [training: 0.1898285677637122 | validation: 0.20959675852349094]
	TIME [epoch: 8.17 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31856092778626377		[learning rate: 0.0051701]
		[batch 20/20] avg loss: 0.30559208948234096		[learning rate: 0.0051607]
	Learning Rate: 0.00516072
	LOSS [training: 0.31207650863430236 | validation: 0.2249043727954435]
	TIME [epoch: 8.2 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2288047877239116		[learning rate: 0.0051513]
		[batch 20/20] avg loss: 0.23546782034647099		[learning rate: 0.005142]
	Learning Rate: 0.00514199
	LOSS [training: 0.2321363040351913 | validation: 0.3776961444804847]
	TIME [epoch: 8.18 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2505889467923056		[learning rate: 0.0051327]
		[batch 20/20] avg loss: 0.2786033949184631		[learning rate: 0.0051233]
	Learning Rate: 0.00512333
	LOSS [training: 0.26459617085538434 | validation: 0.2489584627481734]
	TIME [epoch: 8.2 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22644368659283062		[learning rate: 0.005114]
		[batch 20/20] avg loss: 0.2353675382015183		[learning rate: 0.0051047]
	Learning Rate: 0.00510474
	LOSS [training: 0.2309056123971745 | validation: 0.26460721119190256]
	TIME [epoch: 8.18 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24481864370994844		[learning rate: 0.0050955]
		[batch 20/20] avg loss: 0.2167907384488029		[learning rate: 0.0050862]
	Learning Rate: 0.00508622
	LOSS [training: 0.23080469107937565 | validation: 0.15762342774086158]
	TIME [epoch: 8.2 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27234911618709917		[learning rate: 0.005077]
		[batch 20/20] avg loss: 0.1915209858289076		[learning rate: 0.0050678]
	Learning Rate: 0.00506776
	LOSS [training: 0.23193505100800343 | validation: 0.616384577945527]
	TIME [epoch: 8.18 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3257487516173821		[learning rate: 0.0050586]
		[batch 20/20] avg loss: 0.19897214328389062		[learning rate: 0.0050494]
	Learning Rate: 0.00504937
	LOSS [training: 0.26236044745063636 | validation: 0.40107193360288224]
	TIME [epoch: 8.18 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27107226123381845		[learning rate: 0.0050402]
		[batch 20/20] avg loss: 0.21469390012564676		[learning rate: 0.005031]
	Learning Rate: 0.00503104
	LOSS [training: 0.2428830806797327 | validation: 0.17648880222613855]
	TIME [epoch: 8.17 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.239525332883803		[learning rate: 0.0050219]
		[batch 20/20] avg loss: 0.3432435454465374		[learning rate: 0.0050128]
	Learning Rate: 0.00501278
	LOSS [training: 0.2913844391651702 | validation: 0.28124535306186604]
	TIME [epoch: 8.19 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2499428889560955		[learning rate: 0.0050037]
		[batch 20/20] avg loss: 0.20163348998727484		[learning rate: 0.0049946]
	Learning Rate: 0.00499459
	LOSS [training: 0.2257881894716852 | validation: 0.07606025645572558]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2264430319653033		[learning rate: 0.0049855]
		[batch 20/20] avg loss: 0.33011621926259915		[learning rate: 0.0049765]
	Learning Rate: 0.00497647
	LOSS [training: 0.27827962561395125 | validation: 0.15014405545608925]
	TIME [epoch: 8.19 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20184665534793797		[learning rate: 0.0049674]
		[batch 20/20] avg loss: 0.2885798548116867		[learning rate: 0.0049584]
	Learning Rate: 0.00495841
	LOSS [training: 0.24521325507981234 | validation: 0.27556727381165164]
	TIME [epoch: 8.16 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21795373345713887		[learning rate: 0.0049494]
		[batch 20/20] avg loss: 0.17511727621538767		[learning rate: 0.0049404]
	Learning Rate: 0.00494041
	LOSS [training: 0.19653550483626325 | validation: 0.17789392655524855]
	TIME [epoch: 8.24 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46970702810963605		[learning rate: 0.0049314]
		[batch 20/20] avg loss: 0.21296626987603476		[learning rate: 0.0049225]
	Learning Rate: 0.00492248
	LOSS [training: 0.34133664899283545 | validation: 0.14613061458723153]
	TIME [epoch: 8.2 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17494963153537665		[learning rate: 0.0049135]
		[batch 20/20] avg loss: 0.21313249527002767		[learning rate: 0.0049046]
	Learning Rate: 0.00490462
	LOSS [training: 0.19404106340270216 | validation: 0.4063715875357355]
	TIME [epoch: 8.19 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24096908132413578		[learning rate: 0.0048957]
		[batch 20/20] avg loss: 0.3232359950927048		[learning rate: 0.0048868]
	Learning Rate: 0.00488682
	LOSS [training: 0.2821025382084203 | validation: 0.2920243728715411]
	TIME [epoch: 8.17 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22540873263687417		[learning rate: 0.0048779]
		[batch 20/20] avg loss: 0.25876963755679894		[learning rate: 0.0048691]
	Learning Rate: 0.00486909
	LOSS [training: 0.24208918509683652 | validation: 0.14751567389147638]
	TIME [epoch: 8.19 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19343307297962203		[learning rate: 0.0048602]
		[batch 20/20] avg loss: 0.428620006393464		[learning rate: 0.0048514]
	Learning Rate: 0.00485141
	LOSS [training: 0.3110265396865431 | validation: 0.10567265603477646]
	TIME [epoch: 8.2 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22199354782693242		[learning rate: 0.0048426]
		[batch 20/20] avg loss: 0.16675738306733065		[learning rate: 0.0048338]
	Learning Rate: 0.00483381
	LOSS [training: 0.19437546544713158 | validation: 0.2688030990356667]
	TIME [epoch: 8.19 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29284055612290405		[learning rate: 0.004825]
		[batch 20/20] avg loss: 0.2824014010825924		[learning rate: 0.0048163]
	Learning Rate: 0.00481627
	LOSS [training: 0.2876209786027482 | validation: 0.2402950316889042]
	TIME [epoch: 8.18 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2798961786480078		[learning rate: 0.0048075]
		[batch 20/20] avg loss: 0.21440150960066337		[learning rate: 0.0047988]
	Learning Rate: 0.00479879
	LOSS [training: 0.24714884412433555 | validation: 0.20487424309971136]
	TIME [epoch: 8.19 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19909102276521193		[learning rate: 0.0047901]
		[batch 20/20] avg loss: 0.20353250436058085		[learning rate: 0.0047814]
	Learning Rate: 0.00478137
	LOSS [training: 0.2013117635628964 | validation: 0.2659911091421891]
	TIME [epoch: 8.21 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21429771404815462		[learning rate: 0.0047727]
		[batch 20/20] avg loss: 0.16784053667823656		[learning rate: 0.004764]
	Learning Rate: 0.00476402
	LOSS [training: 0.1910691253631956 | validation: 0.18804897192795253]
	TIME [epoch: 8.18 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19457990603942293		[learning rate: 0.0047554]
		[batch 20/20] avg loss: 0.18311945487361142		[learning rate: 0.0047467]
	Learning Rate: 0.00474673
	LOSS [training: 0.1888496804565172 | validation: 0.39767739665385166]
	TIME [epoch: 8.18 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22146577169901954		[learning rate: 0.0047381]
		[batch 20/20] avg loss: 0.2107592036406055		[learning rate: 0.0047295]
	Learning Rate: 0.00472951
	LOSS [training: 0.21611248766981248 | validation: 0.4155946239629238]
	TIME [epoch: 8.2 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21568760462734735		[learning rate: 0.0047209]
		[batch 20/20] avg loss: 0.263300269944748		[learning rate: 0.0047123]
	Learning Rate: 0.00471234
	LOSS [training: 0.23949393728604768 | validation: 0.17901432120578017]
	TIME [epoch: 8.21 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20138951274913913		[learning rate: 0.0047038]
		[batch 20/20] avg loss: 0.20096849831358213		[learning rate: 0.0046952]
	Learning Rate: 0.00469524
	LOSS [training: 0.20117900553136064 | validation: 0.20948037317743276]
	TIME [epoch: 8.18 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2667063622064394		[learning rate: 0.0046867]
		[batch 20/20] avg loss: 0.24186429778226004		[learning rate: 0.0046782]
	Learning Rate: 0.0046782
	LOSS [training: 0.2542853299943497 | validation: 0.1467350014165649]
	TIME [epoch: 8.17 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2645973365018202		[learning rate: 0.0046697]
		[batch 20/20] avg loss: 0.2208459014577422		[learning rate: 0.0046612]
	Learning Rate: 0.00466122
	LOSS [training: 0.24272161897978117 | validation: 0.42464468420130785]
	TIME [epoch: 8.19 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16126482533805012		[learning rate: 0.0046528]
		[batch 20/20] avg loss: 0.21336647181183035		[learning rate: 0.0046443]
	Learning Rate: 0.00464431
	LOSS [training: 0.18731564857494024 | validation: 0.14831083192755842]
	TIME [epoch: 8.22 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23096393508059415		[learning rate: 0.0046359]
		[batch 20/20] avg loss: 0.19413766949137334		[learning rate: 0.0046275]
	Learning Rate: 0.00462745
	LOSS [training: 0.21255080228598375 | validation: 0.17728112523314532]
	TIME [epoch: 8.18 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17975728754289738		[learning rate: 0.004619]
		[batch 20/20] avg loss: 0.2229457461580986		[learning rate: 0.0046107]
	Learning Rate: 0.00461066
	LOSS [training: 0.201351516850498 | validation: 0.26841729819298193]
	TIME [epoch: 8.17 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16172288957203476		[learning rate: 0.0046023]
		[batch 20/20] avg loss: 0.152765973812735		[learning rate: 0.0045939]
	Learning Rate: 0.00459393
	LOSS [training: 0.15724443169238486 | validation: 0.14927029476861883]
	TIME [epoch: 8.19 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21286130142591347		[learning rate: 0.0045856]
		[batch 20/20] avg loss: 0.2284271883030951		[learning rate: 0.0045773]
	Learning Rate: 0.00457726
	LOSS [training: 0.2206442448645043 | validation: 0.1664243380703422]
	TIME [epoch: 8.21 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33915771861701544		[learning rate: 0.0045689]
		[batch 20/20] avg loss: 0.18735043462628811		[learning rate: 0.0045606]
	Learning Rate: 0.00456065
	LOSS [training: 0.26325407662165184 | validation: 0.33745999608092897]
	TIME [epoch: 8.18 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18351092844586553		[learning rate: 0.0045524]
		[batch 20/20] avg loss: 0.3196438128435316		[learning rate: 0.0045441]
	Learning Rate: 0.00454409
	LOSS [training: 0.25157737064469854 | validation: 0.1981486044305274]
	TIME [epoch: 8.17 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18066210444430672		[learning rate: 0.0045358]
		[batch 20/20] avg loss: 0.22358756876144753		[learning rate: 0.0045276]
	Learning Rate: 0.0045276
	LOSS [training: 0.2021248366028771 | validation: 0.2139302674454074]
	TIME [epoch: 8.18 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19291506302041675		[learning rate: 0.0045194]
		[batch 20/20] avg loss: 0.23657714321957846		[learning rate: 0.0045112]
	Learning Rate: 0.00451117
	LOSS [training: 0.21474610311999762 | validation: 0.15904900395956645]
	TIME [epoch: 8.22 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22067399274630778		[learning rate: 0.004503]
		[batch 20/20] avg loss: 0.2351682356402875		[learning rate: 0.0044948]
	Learning Rate: 0.0044948
	LOSS [training: 0.22792111419329766 | validation: 0.1557533623384713]
	TIME [epoch: 8.17 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19134590930025575		[learning rate: 0.0044866]
		[batch 20/20] avg loss: 0.16239630759188847		[learning rate: 0.0044785]
	Learning Rate: 0.00447849
	LOSS [training: 0.1768711084460721 | validation: 0.1403530887037565]
	TIME [epoch: 8.18 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22480455234867622		[learning rate: 0.0044704]
		[batch 20/20] avg loss: 0.2148514700717171		[learning rate: 0.0044622]
	Learning Rate: 0.00446224
	LOSS [training: 0.21982801121019663 | validation: 0.16737848478086934]
	TIME [epoch: 8.18 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21932958466191826		[learning rate: 0.0044541]
		[batch 20/20] avg loss: 0.18964579589126115		[learning rate: 0.004446]
	Learning Rate: 0.00444604
	LOSS [training: 0.20448769027658967 | validation: 0.31059923181576365]
	TIME [epoch: 8.23 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.229659495763485		[learning rate: 0.004438]
		[batch 20/20] avg loss: 0.22855228518716691		[learning rate: 0.0044299]
	Learning Rate: 0.00442991
	LOSS [training: 0.22910589047532595 | validation: 0.09482287781522163]
	TIME [epoch: 8.18 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20572372628325786		[learning rate: 0.0044219]
		[batch 20/20] avg loss: 0.16109915701421904		[learning rate: 0.0044138]
	Learning Rate: 0.00441383
	LOSS [training: 0.18341144164873843 | validation: 0.16977407627048266]
	TIME [epoch: 8.17 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2075169460343686		[learning rate: 0.0044058]
		[batch 20/20] avg loss: 0.2476469182859231		[learning rate: 0.0043978]
	Learning Rate: 0.00439781
	LOSS [training: 0.22758193216014583 | validation: 0.3572302566266583]
	TIME [epoch: 8.19 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2450584331199454		[learning rate: 0.0043898]
		[batch 20/20] avg loss: 0.22820878086107657		[learning rate: 0.0043819]
	Learning Rate: 0.00438185
	LOSS [training: 0.23663360699051098 | validation: 0.1147465230519841]
	TIME [epoch: 8.21 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16931242239455221		[learning rate: 0.0043739]
		[batch 20/20] avg loss: 0.17717509902045223		[learning rate: 0.004366]
	Learning Rate: 0.00436595
	LOSS [training: 0.17324376070750222 | validation: 0.11567546515573203]
	TIME [epoch: 8.18 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1766031796766449		[learning rate: 0.004358]
		[batch 20/20] avg loss: 0.1797478001254629		[learning rate: 0.0043501]
	Learning Rate: 0.00435011
	LOSS [training: 0.17817548990105386 | validation: 0.33695884561425693]
	TIME [epoch: 8.17 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19760600527499475		[learning rate: 0.0043422]
		[batch 20/20] avg loss: 0.16988057416667568		[learning rate: 0.0043343]
	Learning Rate: 0.00433432
	LOSS [training: 0.1837432897208352 | validation: 0.9287121173888067]
	TIME [epoch: 8.18 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3231731419550983		[learning rate: 0.0043264]
		[batch 20/20] avg loss: 0.1570256365401557		[learning rate: 0.0043186]
	Learning Rate: 0.00431859
	LOSS [training: 0.24009938924762694 | validation: 0.10214636107746117]
	TIME [epoch: 8.23 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24570464511815496		[learning rate: 0.0043107]
		[batch 20/20] avg loss: 0.18434473129153456		[learning rate: 0.0043029]
	Learning Rate: 0.00430292
	LOSS [training: 0.21502468820484477 | validation: 0.29583373733290746]
	TIME [epoch: 8.18 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21591490157548274		[learning rate: 0.0042951]
		[batch 20/20] avg loss: 0.19242991840764193		[learning rate: 0.0042873]
	Learning Rate: 0.0042873
	LOSS [training: 0.20417240999156233 | validation: 0.1916083148237491]
	TIME [epoch: 8.16 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14891472571492984		[learning rate: 0.0042795]
		[batch 20/20] avg loss: 0.18055643612440694		[learning rate: 0.0042717]
	Learning Rate: 0.00427174
	LOSS [training: 0.1647355809196684 | validation: 0.11935417530486087]
	TIME [epoch: 8.18 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2180244531212307		[learning rate: 0.004264]
		[batch 20/20] avg loss: 0.21462417620751867		[learning rate: 0.0042562]
	Learning Rate: 0.00425624
	LOSS [training: 0.21632431466437474 | validation: 0.19114461271457303]
	TIME [epoch: 8.22 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14622581779001304		[learning rate: 0.0042485]
		[batch 20/20] avg loss: 0.15262371621600135		[learning rate: 0.0042408]
	Learning Rate: 0.0042408
	LOSS [training: 0.14942476700300716 | validation: 0.2928008666011799]
	TIME [epoch: 8.2 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2772052775158841		[learning rate: 0.0042331]
		[batch 20/20] avg loss: 0.15758932810907567		[learning rate: 0.0042254]
	Learning Rate: 0.00422541
	LOSS [training: 0.21739730281247988 | validation: 0.0826293369490311]
	TIME [epoch: 8.18 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.190407590558603		[learning rate: 0.0042177]
		[batch 20/20] avg loss: 0.2274701808622667		[learning rate: 0.0042101]
	Learning Rate: 0.00421007
	LOSS [training: 0.2089388857104349 | validation: 0.24724851713648038]
	TIME [epoch: 8.17 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15291797617499053		[learning rate: 0.0042024]
		[batch 20/20] avg loss: 0.17053843878304625		[learning rate: 0.0041948]
	Learning Rate: 0.00419479
	LOSS [training: 0.16172820747901837 | validation: 0.3796887990484858]
	TIME [epoch: 8.22 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18798584510568672		[learning rate: 0.0041872]
		[batch 20/20] avg loss: 0.1805566172960143		[learning rate: 0.0041796]
	Learning Rate: 0.00417957
	LOSS [training: 0.18427123120085048 | validation: 0.21959792584612886]
	TIME [epoch: 8.2 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18818478759175097		[learning rate: 0.004172]
		[batch 20/20] avg loss: 0.1991483607265414		[learning rate: 0.0041644]
	Learning Rate: 0.0041644
	LOSS [training: 0.1936665741591462 | validation: 0.17964734703550855]
	TIME [epoch: 8.16 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17778231498931926		[learning rate: 0.0041568]
		[batch 20/20] avg loss: 0.23447827143584324		[learning rate: 0.0041493]
	Learning Rate: 0.00414929
	LOSS [training: 0.2061302932125813 | validation: 0.18970134494669288]
	TIME [epoch: 8.18 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14888348206709628		[learning rate: 0.0041418]
		[batch 20/20] avg loss: 0.3440046685594922		[learning rate: 0.0041342]
	Learning Rate: 0.00413423
	LOSS [training: 0.24644407531329424 | validation: 0.537149467714757]
	TIME [epoch: 8.23 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23830342572281332		[learning rate: 0.0041267]
		[batch 20/20] avg loss: 0.18824054740641885		[learning rate: 0.0041192]
	Learning Rate: 0.00411923
	LOSS [training: 0.21327198656461607 | validation: 0.22061861398535726]
	TIME [epoch: 8.18 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21123675647786996		[learning rate: 0.0041117]
		[batch 20/20] avg loss: 0.18184351864179377		[learning rate: 0.0041043]
	Learning Rate: 0.00410428
	LOSS [training: 0.19654013755983185 | validation: 0.08555715408459745]
	TIME [epoch: 8.18 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16389312807643658		[learning rate: 0.0040968]
		[batch 20/20] avg loss: 0.17304382856735726		[learning rate: 0.0040894]
	Learning Rate: 0.00408938
	LOSS [training: 0.16846847832189688 | validation: 0.10968536109453178]
	TIME [epoch: 8.17 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16835812448267326		[learning rate: 0.004082]
		[batch 20/20] avg loss: 0.18111072049325486		[learning rate: 0.0040745]
	Learning Rate: 0.00407454
	LOSS [training: 0.1747344224879641 | validation: 0.1196611859135059]
	TIME [epoch: 8.24 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22574639717541115		[learning rate: 0.0040671]
		[batch 20/20] avg loss: 0.16581585106072766		[learning rate: 0.0040598]
	Learning Rate: 0.00405976
	LOSS [training: 0.19578112411806942 | validation: 0.2824123609936306]
	TIME [epoch: 8.19 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13179391628837483		[learning rate: 0.0040524]
		[batch 20/20] avg loss: 0.21402875901023427		[learning rate: 0.004045]
	Learning Rate: 0.00404502
	LOSS [training: 0.17291133764930455 | validation: 0.09328246516343655]
	TIME [epoch: 8.18 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24110154152919		[learning rate: 0.0040377]
		[batch 20/20] avg loss: 0.18352509498727457		[learning rate: 0.0040303]
	Learning Rate: 0.00403034
	LOSS [training: 0.21231331825823227 | validation: 0.1341387324174347]
	TIME [epoch: 8.18 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12783380950552165		[learning rate: 0.004023]
		[batch 20/20] avg loss: 0.1846647709051053		[learning rate: 0.0040157]
	Learning Rate: 0.00401572
	LOSS [training: 0.15624929020531342 | validation: 0.29375076957737156]
	TIME [epoch: 8.24 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2619263486952271		[learning rate: 0.0040084]
		[batch 20/20] avg loss: 0.19432743569413066		[learning rate: 0.0040011]
	Learning Rate: 0.00400114
	LOSS [training: 0.2281268921946788 | validation: 0.12391697840350888]
	TIME [epoch: 8.19 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16676774723670346		[learning rate: 0.0039939]
		[batch 20/20] avg loss: 0.1712040270816856		[learning rate: 0.0039866]
	Learning Rate: 0.00398662
	LOSS [training: 0.16898588715919455 | validation: 0.14247361427317934]
	TIME [epoch: 8.18 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18016170456510844		[learning rate: 0.0039794]
		[batch 20/20] avg loss: 0.24942805457373513		[learning rate: 0.0039722]
	Learning Rate: 0.00397216
	LOSS [training: 0.21479487956942175 | validation: 0.5977922920056907]
	TIME [epoch: 8.17 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21711520407988055		[learning rate: 0.0039649]
		[batch 20/20] avg loss: 0.15254225964228035		[learning rate: 0.0039577]
	Learning Rate: 0.00395774
	LOSS [training: 0.18482873186108045 | validation: 0.1506892470207941]
	TIME [epoch: 8.24 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18243131629752096		[learning rate: 0.0039506]
		[batch 20/20] avg loss: 0.17927009445582007		[learning rate: 0.0039434]
	Learning Rate: 0.00394338
	LOSS [training: 0.1808507053766705 | validation: 0.4067970076918636]
	TIME [epoch: 8.19 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2210539048909644		[learning rate: 0.0039362]
		[batch 20/20] avg loss: 0.18400995506473822		[learning rate: 0.0039291]
	Learning Rate: 0.00392907
	LOSS [training: 0.20253192997785133 | validation: 0.09046833483961507]
	TIME [epoch: 8.17 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17914129148443664		[learning rate: 0.0039219]
		[batch 20/20] avg loss: 0.1711682822374852		[learning rate: 0.0039148]
	Learning Rate: 0.00391481
	LOSS [training: 0.17515478686096086 | validation: 0.1056418226072175]
	TIME [epoch: 8.18 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15781316184526123		[learning rate: 0.0039077]
		[batch 20/20] avg loss: 0.14391426258393938		[learning rate: 0.0039006]
	Learning Rate: 0.0039006
	LOSS [training: 0.15086371221460032 | validation: 0.1611799456642445]
	TIME [epoch: 8.24 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1884336892264597		[learning rate: 0.0038935]
		[batch 20/20] avg loss: 0.17171877008548125		[learning rate: 0.0038864]
	Learning Rate: 0.00388645
	LOSS [training: 0.18007622965597045 | validation: 0.14749221059526738]
	TIME [epoch: 8.2 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16164432146226865		[learning rate: 0.0038794]
		[batch 20/20] avg loss: 0.20637172665253495		[learning rate: 0.0038723]
	Learning Rate: 0.00387234
	LOSS [training: 0.1840080240574018 | validation: 0.19031611299339823]
	TIME [epoch: 8.18 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1336442049246571		[learning rate: 0.0038653]
		[batch 20/20] avg loss: 0.13183125985842709		[learning rate: 0.0038583]
	Learning Rate: 0.00385829
	LOSS [training: 0.13273773239154207 | validation: 0.13446532271086098]
	TIME [epoch: 8.17 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14224840339221836		[learning rate: 0.0038513]
		[batch 20/20] avg loss: 0.17241098237548821		[learning rate: 0.0038443]
	Learning Rate: 0.00384429
	LOSS [training: 0.1573296928838533 | validation: 0.28529422008913835]
	TIME [epoch: 8.23 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17635876828227975		[learning rate: 0.0038373]
		[batch 20/20] avg loss: 0.15604762972952968		[learning rate: 0.0038303]
	Learning Rate: 0.00383034
	LOSS [training: 0.16620319900590472 | validation: 0.11949116760361374]
	TIME [epoch: 8.19 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.124911041729677		[learning rate: 0.0038234]
		[batch 20/20] avg loss: 0.16487681708180937		[learning rate: 0.0038164]
	Learning Rate: 0.00381644
	LOSS [training: 0.14489392940574317 | validation: 0.1655344838714564]
	TIME [epoch: 8.19 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14118626303816353		[learning rate: 0.0038095]
		[batch 20/20] avg loss: 0.15506880551778604		[learning rate: 0.0038026]
	Learning Rate: 0.00380258
	LOSS [training: 0.14812753427797481 | validation: 0.17818797338100723]
	TIME [epoch: 8.18 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15632584451342937		[learning rate: 0.0037957]
		[batch 20/20] avg loss: 0.17034589904556055		[learning rate: 0.0037888]
	Learning Rate: 0.00378879
	LOSS [training: 0.16333587177949493 | validation: 0.10488486406760336]
	TIME [epoch: 8.24 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16218858550822085		[learning rate: 0.0037819]
		[batch 20/20] avg loss: 0.20069012250775983		[learning rate: 0.003775]
	Learning Rate: 0.00377504
	LOSS [training: 0.18143935400799036 | validation: 0.20658356501183428]
	TIME [epoch: 8.18 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21036582191632647		[learning rate: 0.0037682]
		[batch 20/20] avg loss: 0.2068358010550871		[learning rate: 0.0037613]
	Learning Rate: 0.00376134
	LOSS [training: 0.2086008114857068 | validation: 0.24577765442773936]
	TIME [epoch: 8.17 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18339666446006736		[learning rate: 0.0037545]
		[batch 20/20] avg loss: 0.1812665694791157		[learning rate: 0.0037477]
	Learning Rate: 0.00374769
	LOSS [training: 0.18233161696959155 | validation: 0.4311026098079848]
	TIME [epoch: 8.18 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2051049042182918		[learning rate: 0.0037409]
		[batch 20/20] avg loss: 0.17440150141880642		[learning rate: 0.0037341]
	Learning Rate: 0.00373408
	LOSS [training: 0.18975320281854918 | validation: 0.11856121696793631]
	TIME [epoch: 8.23 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12140849250300001		[learning rate: 0.0037273]
		[batch 20/20] avg loss: 0.19395251910164552		[learning rate: 0.0037205]
	Learning Rate: 0.00372053
	LOSS [training: 0.15768050580232276 | validation: 0.22413289694168909]
	TIME [epoch: 8.21 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12477377828779765		[learning rate: 0.0037138]
		[batch 20/20] avg loss: 0.17717911876558157		[learning rate: 0.003707]
	Learning Rate: 0.00370703
	LOSS [training: 0.15097644852668962 | validation: 0.18254596076125995]
	TIME [epoch: 8.17 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16407663068028772		[learning rate: 0.0037003]
		[batch 20/20] avg loss: 0.15976145563310623		[learning rate: 0.0036936]
	Learning Rate: 0.00369358
	LOSS [training: 0.16191904315669697 | validation: 0.1242838678268046]
	TIME [epoch: 8.17 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20668866821899062		[learning rate: 0.0036869]
		[batch 20/20] avg loss: 0.18770554139260137		[learning rate: 0.0036802]
	Learning Rate: 0.00368017
	LOSS [training: 0.197197104805796 | validation: 0.0873193564596261]
	TIME [epoch: 8.22 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18255701785575046		[learning rate: 0.0036735]
		[batch 20/20] avg loss: 0.14845884823580852		[learning rate: 0.0036668]
	Learning Rate: 0.00366682
	LOSS [training: 0.1655079330457795 | validation: 0.08984927755849965]
	TIME [epoch: 8.19 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12715793872756798		[learning rate: 0.0036602]
		[batch 20/20] avg loss: 0.15924371268534696		[learning rate: 0.0036535]
	Learning Rate: 0.00365351
	LOSS [training: 0.14320082570645745 | validation: 0.27750275727936735]
	TIME [epoch: 8.16 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1740673532349428		[learning rate: 0.0036469]
		[batch 20/20] avg loss: 0.18418913162600453		[learning rate: 0.0036403]
	Learning Rate: 0.00364025
	LOSS [training: 0.17912824243047368 | validation: 0.14612701581708448]
	TIME [epoch: 8.16 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12142653137145562		[learning rate: 0.0036336]
		[batch 20/20] avg loss: 0.19159012905735442		[learning rate: 0.003627]
	Learning Rate: 0.00362704
	LOSS [training: 0.156508330214405 | validation: 0.2864082480230114]
	TIME [epoch: 8.22 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20208166953785348		[learning rate: 0.0036205]
		[batch 20/20] avg loss: 0.14109883645238566		[learning rate: 0.0036139]
	Learning Rate: 0.00361388
	LOSS [training: 0.1715902529951196 | validation: 0.2101606028561372]
	TIME [epoch: 8.19 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18087262170973065		[learning rate: 0.0036073]
		[batch 20/20] avg loss: 0.16138171517741592		[learning rate: 0.0036008]
	Learning Rate: 0.00360076
	LOSS [training: 0.17112716844357329 | validation: 0.22596604227071576]
	TIME [epoch: 8.17 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13853016330449722		[learning rate: 0.0035942]
		[batch 20/20] avg loss: 0.16702860168112596		[learning rate: 0.0035877]
	Learning Rate: 0.0035877
	LOSS [training: 0.15277938249281162 | validation: 0.2335164119736111]
	TIME [epoch: 8.17 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22483207248303763		[learning rate: 0.0035812]
		[batch 20/20] avg loss: 0.2338467936639045		[learning rate: 0.0035747]
	Learning Rate: 0.00357468
	LOSS [training: 0.2293394330734711 | validation: 0.10657005176054689]
	TIME [epoch: 8.23 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15201850959259636		[learning rate: 0.0035682]
		[batch 20/20] avg loss: 0.15754073686397446		[learning rate: 0.0035617]
	Learning Rate: 0.0035617
	LOSS [training: 0.15477962322828542 | validation: 0.30738255574452633]
	TIME [epoch: 8.2 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15749186160574932		[learning rate: 0.0035552]
		[batch 20/20] avg loss: 0.17554286961694204		[learning rate: 0.0035488]
	Learning Rate: 0.00354878
	LOSS [training: 0.16651736561134567 | validation: 0.13611236140215383]
	TIME [epoch: 8.18 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17009843163946023		[learning rate: 0.0035423]
		[batch 20/20] avg loss: 0.16099524768716184		[learning rate: 0.0035359]
	Learning Rate: 0.0035359
	LOSS [training: 0.16554683966331102 | validation: 0.18397588483258961]
	TIME [epoch: 8.17 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20804870624536714		[learning rate: 0.0035295]
		[batch 20/20] avg loss: 0.15063478365851754		[learning rate: 0.0035231]
	Learning Rate: 0.00352307
	LOSS [training: 0.17934174495194233 | validation: 0.1919709633875478]
	TIME [epoch: 8.22 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19381931319325604		[learning rate: 0.0035167]
		[batch 20/20] avg loss: 0.1445016911248475		[learning rate: 0.0035103]
	Learning Rate: 0.00351028
	LOSS [training: 0.1691605021590518 | validation: 0.09366168545391906]
	TIME [epoch: 8.2 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12827452341140183		[learning rate: 0.0035039]
		[batch 20/20] avg loss: 0.14875545061543816		[learning rate: 0.0034975]
	Learning Rate: 0.00349754
	LOSS [training: 0.13851498701341997 | validation: 0.19673672415616988]
	TIME [epoch: 8.18 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2021672828365626		[learning rate: 0.0034912]
		[batch 20/20] avg loss: 0.12787243569097884		[learning rate: 0.0034849]
	Learning Rate: 0.00348485
	LOSS [training: 0.16501985926377072 | validation: 0.06950286435033601]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16298004624368007		[learning rate: 0.0034785]
		[batch 20/20] avg loss: 0.11635738434770482		[learning rate: 0.0034722]
	Learning Rate: 0.0034722
	LOSS [training: 0.13966871529569244 | validation: 0.1272085478032375]
	TIME [epoch: 8.22 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13041188736312		[learning rate: 0.0034659]
		[batch 20/20] avg loss: 0.2163901478941673		[learning rate: 0.0034596]
	Learning Rate: 0.0034596
	LOSS [training: 0.17340101762864366 | validation: 0.12468735101916073]
	TIME [epoch: 8.18 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19612710406656841		[learning rate: 0.0034533]
		[batch 20/20] avg loss: 0.11761560584245798		[learning rate: 0.003447]
	Learning Rate: 0.00344705
	LOSS [training: 0.15687135495451315 | validation: 0.06606699787084229]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13753762943360384		[learning rate: 0.0034408]
		[batch 20/20] avg loss: 0.2529652469084266		[learning rate: 0.0034345]
	Learning Rate: 0.00343454
	LOSS [training: 0.19525143817101523 | validation: 0.18602153602406857]
	TIME [epoch: 8.19 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17260692596324548		[learning rate: 0.0034283]
		[batch 20/20] avg loss: 0.15910089175335818		[learning rate: 0.0034221]
	Learning Rate: 0.00342207
	LOSS [training: 0.16585390885830184 | validation: 0.13492999766823346]
	TIME [epoch: 8.26 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12497414806015919		[learning rate: 0.0034159]
		[batch 20/20] avg loss: 0.19505640537968563		[learning rate: 0.0034097]
	Learning Rate: 0.00340966
	LOSS [training: 0.16001527671992238 | validation: 0.1709005370743937]
	TIME [epoch: 8.22 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17403241917924858		[learning rate: 0.0034035]
		[batch 20/20] avg loss: 0.13971955474129746		[learning rate: 0.0033973]
	Learning Rate: 0.00339728
	LOSS [training: 0.156875986960273 | validation: 0.17764980478286901]
	TIME [epoch: 8.19 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16693330084241836		[learning rate: 0.0033911]
		[batch 20/20] avg loss: 0.18923990272315905		[learning rate: 0.003385]
	Learning Rate: 0.00338495
	LOSS [training: 0.1780866017827887 | validation: 0.10430779185205706]
	TIME [epoch: 8.19 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14778204600589606		[learning rate: 0.0033788]
		[batch 20/20] avg loss: 0.18071342397267603		[learning rate: 0.0033727]
	Learning Rate: 0.00337267
	LOSS [training: 0.16424773498928602 | validation: 0.12171269564667449]
	TIME [epoch: 8.25 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13369561572658473		[learning rate: 0.0033665]
		[batch 20/20] avg loss: 0.14366122278268018		[learning rate: 0.0033604]
	Learning Rate: 0.00336043
	LOSS [training: 0.13867841925463245 | validation: 0.1082808413199512]
	TIME [epoch: 8.21 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16886206214024518		[learning rate: 0.0033543]
		[batch 20/20] avg loss: 0.10844035197376392		[learning rate: 0.0033482]
	Learning Rate: 0.00334823
	LOSS [training: 0.13865120705700457 | validation: 0.12604136339158378]
	TIME [epoch: 8.2 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15959880959289582		[learning rate: 0.0033422]
		[batch 20/20] avg loss: 0.14167982255558825		[learning rate: 0.0033361]
	Learning Rate: 0.00333608
	LOSS [training: 0.15063931607424202 | validation: 0.14063606493491]
	TIME [epoch: 8.19 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16455670986123863		[learning rate: 0.00333]
		[batch 20/20] avg loss: 0.2452204976771203		[learning rate: 0.003324]
	Learning Rate: 0.00332398
	LOSS [training: 0.20488860376917944 | validation: 0.21991700301945888]
	TIME [epoch: 8.25 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1338621988652937		[learning rate: 0.0033179]
		[batch 20/20] avg loss: 0.1454604012201676		[learning rate: 0.0033119]
	Learning Rate: 0.00331191
	LOSS [training: 0.13966130004273067 | validation: 0.16984662065142417]
	TIME [epoch: 8.21 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1450507782949798		[learning rate: 0.0033059]
		[batch 20/20] avg loss: 0.1658512992484913		[learning rate: 0.0032999]
	Learning Rate: 0.00329989
	LOSS [training: 0.1554510387717356 | validation: 0.18171426245200845]
	TIME [epoch: 8.19 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14103638099556368		[learning rate: 0.0032939]
		[batch 20/20] avg loss: 0.192007980358813		[learning rate: 0.0032879]
	Learning Rate: 0.00328792
	LOSS [training: 0.16652218067718835 | validation: 0.23205596369283055]
	TIME [epoch: 8.19 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15892732782629396		[learning rate: 0.0032819]
		[batch 20/20] avg loss: 0.12166037745353579		[learning rate: 0.003276]
	Learning Rate: 0.00327599
	LOSS [training: 0.14029385263991487 | validation: 0.11228606462592927]
	TIME [epoch: 8.25 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14044238216792462		[learning rate: 0.00327]
		[batch 20/20] avg loss: 0.13240357523189253		[learning rate: 0.0032641]
	Learning Rate: 0.0032641
	LOSS [training: 0.13642297869990858 | validation: 0.0860619252772197]
	TIME [epoch: 8.22 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1283701710439518		[learning rate: 0.0032582]
		[batch 20/20] avg loss: 0.17174965453124832		[learning rate: 0.0032523]
	Learning Rate: 0.00325225
	LOSS [training: 0.15005991278760006 | validation: 0.40160418776914253]
	TIME [epoch: 8.2 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1627075245601625		[learning rate: 0.0032463]
		[batch 20/20] avg loss: 0.11186544319826382		[learning rate: 0.0032404]
	Learning Rate: 0.00324045
	LOSS [training: 0.13728648387921313 | validation: 0.14516707539752116]
	TIME [epoch: 8.19 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1104330279752663		[learning rate: 0.0032346]
		[batch 20/20] avg loss: 0.1874750537608171		[learning rate: 0.0032287]
	Learning Rate: 0.00322869
	LOSS [training: 0.1489540408680417 | validation: 0.08968719013333601]
	TIME [epoch: 8.25 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12549804159122827		[learning rate: 0.0032228]
		[batch 20/20] avg loss: 0.15461431306362397		[learning rate: 0.003217]
	Learning Rate: 0.00321697
	LOSS [training: 0.1400561773274261 | validation: 0.2581965034614165]
	TIME [epoch: 8.21 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16327138183798834		[learning rate: 0.0032111]
		[batch 20/20] avg loss: 0.1411962372835811		[learning rate: 0.0032053]
	Learning Rate: 0.0032053
	LOSS [training: 0.15223380956078472 | validation: 0.20627475011542493]
	TIME [epoch: 8.19 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20294289071757285		[learning rate: 0.0031995]
		[batch 20/20] avg loss: 0.20320340551423968		[learning rate: 0.0031937]
	Learning Rate: 0.00319367
	LOSS [training: 0.20307314811590632 | validation: 0.08259080829258408]
	TIME [epoch: 8.19 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13457518958854897		[learning rate: 0.0031879]
		[batch 20/20] avg loss: 0.16470300397778476		[learning rate: 0.0031821]
	Learning Rate: 0.00318208
	LOSS [training: 0.1496390967831669 | validation: 0.18932547930192428]
	TIME [epoch: 8.25 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13846968582189303		[learning rate: 0.0031763]
		[batch 20/20] avg loss: 0.13787612910142794		[learning rate: 0.0031705]
	Learning Rate: 0.00317053
	LOSS [training: 0.13817290746166047 | validation: 0.09373239628364348]
	TIME [epoch: 8.22 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16339256942888408		[learning rate: 0.0031648]
		[batch 20/20] avg loss: 0.1381758130667731		[learning rate: 0.003159]
	Learning Rate: 0.00315902
	LOSS [training: 0.15078419124782858 | validation: 0.272259000407255]
	TIME [epoch: 8.19 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13962824357098808		[learning rate: 0.0031533]
		[batch 20/20] avg loss: 0.14941819364095302		[learning rate: 0.0031476]
	Learning Rate: 0.00314756
	LOSS [training: 0.14452321860597056 | validation: 0.21553208147669578]
	TIME [epoch: 8.19 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16196740607127574		[learning rate: 0.0031418]
		[batch 20/20] avg loss: 0.14064069010732966		[learning rate: 0.0031361]
	Learning Rate: 0.00313613
	LOSS [training: 0.1513040480893027 | validation: 0.10475880300255198]
	TIME [epoch: 8.24 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13384652391494625		[learning rate: 0.0031304]
		[batch 20/20] avg loss: 0.11093089506979988		[learning rate: 0.0031248]
	Learning Rate: 0.00312475
	LOSS [training: 0.12238870949237309 | validation: 0.15009357602931003]
	TIME [epoch: 8.22 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1573902262175985		[learning rate: 0.0031191]
		[batch 20/20] avg loss: 0.13580382641327363		[learning rate: 0.0031134]
	Learning Rate: 0.00311341
	LOSS [training: 0.14659702631543603 | validation: 0.10343936598377607]
	TIME [epoch: 8.2 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11017383113667609		[learning rate: 0.0031078]
		[batch 20/20] avg loss: 0.20250653399889748		[learning rate: 0.0031021]
	Learning Rate: 0.00310212
	LOSS [training: 0.15634018256778684 | validation: 0.17537020457783228]
	TIME [epoch: 8.2 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1278753948900375		[learning rate: 0.0030965]
		[batch 20/20] avg loss: 0.14974942257046459		[learning rate: 0.0030909]
	Learning Rate: 0.00309086
	LOSS [training: 0.13881240873025102 | validation: 0.21730993470722615]
	TIME [epoch: 8.24 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15687701009896318		[learning rate: 0.0030852]
		[batch 20/20] avg loss: 0.13773882476867297		[learning rate: 0.0030796]
	Learning Rate: 0.00307964
	LOSS [training: 0.14730791743381805 | validation: 0.09057415172832879]
	TIME [epoch: 8.22 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1370013938513448		[learning rate: 0.003074]
		[batch 20/20] avg loss: 0.1656687576978742		[learning rate: 0.0030685]
	Learning Rate: 0.00306846
	LOSS [training: 0.15133507577460947 | validation: 0.09148731893693977]
	TIME [epoch: 8.19 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12079304930654441		[learning rate: 0.0030629]
		[batch 20/20] avg loss: 0.10002104619007073		[learning rate: 0.0030573]
	Learning Rate: 0.00305733
	LOSS [training: 0.1104070477483076 | validation: 0.0811927768888751]
	TIME [epoch: 8.19 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1446383955430021		[learning rate: 0.0030518]
		[batch 20/20] avg loss: 0.1844584871255249		[learning rate: 0.0030462]
	Learning Rate: 0.00304623
	LOSS [training: 0.16454844133426355 | validation: 0.13969612549327237]
	TIME [epoch: 8.24 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14553416638041555		[learning rate: 0.0030407]
		[batch 20/20] avg loss: 0.15648047788307753		[learning rate: 0.0030352]
	Learning Rate: 0.00303518
	LOSS [training: 0.15100732213174653 | validation: 0.18431351216604852]
	TIME [epoch: 8.23 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23018927590192648		[learning rate: 0.0030297]
		[batch 20/20] avg loss: 0.13278765334640857		[learning rate: 0.0030242]
	Learning Rate: 0.00302416
	LOSS [training: 0.1814884646241675 | validation: 0.1360597912926817]
	TIME [epoch: 8.2 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20690236622376562		[learning rate: 0.0030187]
		[batch 20/20] avg loss: 0.13527421611160734		[learning rate: 0.0030132]
	Learning Rate: 0.00301319
	LOSS [training: 0.17108829116768648 | validation: 0.0804715499741326]
	TIME [epoch: 8.19 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15974413569886475		[learning rate: 0.0030077]
		[batch 20/20] avg loss: 0.18063807492497735		[learning rate: 0.0030023]
	Learning Rate: 0.00300225
	LOSS [training: 0.17019110531192108 | validation: 0.08738970338271541]
	TIME [epoch: 8.23 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1199794407176876		[learning rate: 0.0029968]
		[batch 20/20] avg loss: 0.10137629583940048		[learning rate: 0.0029914]
	Learning Rate: 0.00299136
	LOSS [training: 0.11067786827854406 | validation: 0.16091184368137337]
	TIME [epoch: 8.23 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10212181080749487		[learning rate: 0.0029859]
		[batch 20/20] avg loss: 0.1767099463777192		[learning rate: 0.0029805]
	Learning Rate: 0.0029805
	LOSS [training: 0.13941587859260707 | validation: 0.1601349488311704]
	TIME [epoch: 8.2 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13922432969329598		[learning rate: 0.0029751]
		[batch 20/20] avg loss: 0.13966356627963455		[learning rate: 0.0029697]
	Learning Rate: 0.00296969
	LOSS [training: 0.13944394798646528 | validation: 0.16972385309244029]
	TIME [epoch: 8.19 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10896731746767734		[learning rate: 0.0029643]
		[batch 20/20] avg loss: 0.12059244493858465		[learning rate: 0.0029589]
	Learning Rate: 0.00295891
	LOSS [training: 0.11477988120313101 | validation: 0.1528856420648689]
	TIME [epoch: 8.24 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12104547424649748		[learning rate: 0.0029535]
		[batch 20/20] avg loss: 0.11037780941216044		[learning rate: 0.0029482]
	Learning Rate: 0.00294817
	LOSS [training: 0.11571164182932896 | validation: 0.08734038220207924]
	TIME [epoch: 8.23 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14080374007066657		[learning rate: 0.0029428]
		[batch 20/20] avg loss: 0.14046846970188615		[learning rate: 0.0029375]
	Learning Rate: 0.00293747
	LOSS [training: 0.14063610488627631 | validation: 0.2153086826601778]
	TIME [epoch: 8.2 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13738090438337908		[learning rate: 0.0029321]
		[batch 20/20] avg loss: 0.1617500561531347		[learning rate: 0.0029268]
	Learning Rate: 0.00292681
	LOSS [training: 0.1495654802682569 | validation: 0.19340372282945925]
	TIME [epoch: 8.19 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14426165857891177		[learning rate: 0.0029215]
		[batch 20/20] avg loss: 0.12988556479132854		[learning rate: 0.0029162]
	Learning Rate: 0.00291619
	LOSS [training: 0.13707361168512017 | validation: 0.23390907472201075]
	TIME [epoch: 8.24 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13400588065749622		[learning rate: 0.0029109]
		[batch 20/20] avg loss: 0.10154943974192945		[learning rate: 0.0029056]
	Learning Rate: 0.00290561
	LOSS [training: 0.11777766019971285 | validation: 0.1132499880406167]
	TIME [epoch: 8.22 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12132841211567587		[learning rate: 0.0029003]
		[batch 20/20] avg loss: 0.13386462239864766		[learning rate: 0.0028951]
	Learning Rate: 0.00289506
	LOSS [training: 0.12759651725716176 | validation: 0.10641551953484556]
	TIME [epoch: 8.2 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14026929671228633		[learning rate: 0.0028898]
		[batch 20/20] avg loss: 0.1095910025198118		[learning rate: 0.0028846]
	Learning Rate: 0.00288456
	LOSS [training: 0.12493014961604905 | validation: 0.110839989384673]
	TIME [epoch: 8.19 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16257202471701382		[learning rate: 0.0028793]
		[batch 20/20] avg loss: 0.15391525093714228		[learning rate: 0.0028741]
	Learning Rate: 0.00287409
	LOSS [training: 0.15824363782707804 | validation: 0.3014831517012039]
	TIME [epoch: 8.23 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11326706036758745		[learning rate: 0.0028689]
		[batch 20/20] avg loss: 0.17310410995804312		[learning rate: 0.0028637]
	Learning Rate: 0.00286366
	LOSS [training: 0.14318558516281527 | validation: 0.07940597283663348]
	TIME [epoch: 8.22 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10745876196046049		[learning rate: 0.0028585]
		[batch 20/20] avg loss: 0.12462233509844396		[learning rate: 0.0028533]
	Learning Rate: 0.00285326
	LOSS [training: 0.11604054852945223 | validation: 0.22158703156596657]
	TIME [epoch: 8.2 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14897659636632077		[learning rate: 0.0028481]
		[batch 20/20] avg loss: 0.13605542706548016		[learning rate: 0.0028429]
	Learning Rate: 0.00284291
	LOSS [training: 0.1425160117159005 | validation: 0.22646632863418154]
	TIME [epoch: 8.19 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15682795516775053		[learning rate: 0.0028377]
		[batch 20/20] avg loss: 0.17210559228436415		[learning rate: 0.0028326]
	Learning Rate: 0.00283259
	LOSS [training: 0.1644667737260573 | validation: 0.17400329602743728]
	TIME [epoch: 8.24 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11736022030496293		[learning rate: 0.0028274]
		[batch 20/20] avg loss: 0.13974544347404433		[learning rate: 0.0028223]
	Learning Rate: 0.00282231
	LOSS [training: 0.12855283188950362 | validation: 0.17238690309198657]
	TIME [epoch: 8.21 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11480356690338447		[learning rate: 0.0028172]
		[batch 20/20] avg loss: 0.116721144178721		[learning rate: 0.0028121]
	Learning Rate: 0.00281207
	LOSS [training: 0.11576235554105271 | validation: 0.13051744133007065]
	TIME [epoch: 8.19 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13028351981149378		[learning rate: 0.002807]
		[batch 20/20] avg loss: 0.1584622074989312		[learning rate: 0.0028019]
	Learning Rate: 0.00280187
	LOSS [training: 0.1443728636552125 | validation: 0.1016165209826283]
	TIME [epoch: 8.2 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11897359824300317		[learning rate: 0.0027968]
		[batch 20/20] avg loss: 0.3293482719392959		[learning rate: 0.0027917]
	Learning Rate: 0.0027917
	LOSS [training: 0.2241609350911496 | validation: 0.06961769788925855]
	TIME [epoch: 8.24 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0963723886161904		[learning rate: 0.0027866]
		[batch 20/20] avg loss: 0.11939761244749132		[learning rate: 0.0027816]
	Learning Rate: 0.00278157
	LOSS [training: 0.10788500053184087 | validation: 0.11047696134891105]
	TIME [epoch: 8.22 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10050314256005925		[learning rate: 0.0027765]
		[batch 20/20] avg loss: 0.11480062864046654		[learning rate: 0.0027715]
	Learning Rate: 0.00277147
	LOSS [training: 0.10765188560026291 | validation: 0.06568382995752384]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11791030644713547		[learning rate: 0.0027664]
		[batch 20/20] avg loss: 0.15892695166618465		[learning rate: 0.0027614]
	Learning Rate: 0.00276141
	LOSS [training: 0.13841862905666008 | validation: 0.09278954951690191]
	TIME [epoch: 8.19 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11880765139273283		[learning rate: 0.0027564]
		[batch 20/20] avg loss: 0.13317581298865072		[learning rate: 0.0027514]
	Learning Rate: 0.00275139
	LOSS [training: 0.1259917321906918 | validation: 0.10102107082639565]
	TIME [epoch: 8.22 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11626948435404783		[learning rate: 0.0027464]
		[batch 20/20] avg loss: 0.13719872166682145		[learning rate: 0.0027414]
	Learning Rate: 0.00274141
	LOSS [training: 0.12673410301043467 | validation: 0.3703105490976185]
	TIME [epoch: 8.21 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11585770408237295		[learning rate: 0.0027364]
		[batch 20/20] avg loss: 0.14177441545060693		[learning rate: 0.0027315]
	Learning Rate: 0.00273146
	LOSS [training: 0.1288160597664899 | validation: 0.08897241666392072]
	TIME [epoch: 8.19 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12189434652487838		[learning rate: 0.0027265]
		[batch 20/20] avg loss: 0.10686425496811132		[learning rate: 0.0027215]
	Learning Rate: 0.00272155
	LOSS [training: 0.11437930074649486 | validation: 0.12920162888690323]
	TIME [epoch: 8.2 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10738768319394201		[learning rate: 0.0027166]
		[batch 20/20] avg loss: 0.17186750360543893		[learning rate: 0.0027117]
	Learning Rate: 0.00271167
	LOSS [training: 0.1396275933996905 | validation: 0.07558942709027923]
	TIME [epoch: 8.21 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16328154676721557		[learning rate: 0.0027067]
		[batch 20/20] avg loss: 0.11085705770222762		[learning rate: 0.0027018]
	Learning Rate: 0.00270183
	LOSS [training: 0.13706930223472163 | validation: 0.062294989218198274]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_460.pth
	Model improved!!!
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17974787144564242		[learning rate: 0.0026969]
		[batch 20/20] avg loss: 0.1700718841127541		[learning rate: 0.002692]
	Learning Rate: 0.00269202
	LOSS [training: 0.17490987777919825 | validation: 0.1255067541730966]
	TIME [epoch: 8.19 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09545841726223744		[learning rate: 0.0026871]
		[batch 20/20] avg loss: 0.09181352488849656		[learning rate: 0.0026823]
	Learning Rate: 0.00268225
	LOSS [training: 0.093635971075367 | validation: 0.06262164923847226]
	TIME [epoch: 8.2 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1206794059101762		[learning rate: 0.0026774]
		[batch 20/20] avg loss: 0.10309506961259977		[learning rate: 0.0026725]
	Learning Rate: 0.00267252
	LOSS [training: 0.11188723776138801 | validation: 0.11399502998619014]
	TIME [epoch: 8.2 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11686926321865772		[learning rate: 0.0026677]
		[batch 20/20] avg loss: 0.11050056063324769		[learning rate: 0.0026628]
	Learning Rate: 0.00266282
	LOSS [training: 0.11368491192595273 | validation: 0.10175772427486454]
	TIME [epoch: 8.21 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1524040077842543		[learning rate: 0.002658]
		[batch 20/20] avg loss: 0.10870926655540933		[learning rate: 0.0026532]
	Learning Rate: 0.00265316
	LOSS [training: 0.13055663716983182 | validation: 0.15343343084658073]
	TIME [epoch: 8.19 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11070288659602945		[learning rate: 0.0026483]
		[batch 20/20] avg loss: 0.12639798651542256		[learning rate: 0.0026435]
	Learning Rate: 0.00264353
	LOSS [training: 0.11855043655572599 | validation: 0.4218356078887336]
	TIME [epoch: 8.2 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1436738200861993		[learning rate: 0.0026387]
		[batch 20/20] avg loss: 0.18262886595911315		[learning rate: 0.0026339]
	Learning Rate: 0.00263394
	LOSS [training: 0.16315134302265621 | validation: 0.14587248767747193]
	TIME [epoch: 8.2 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0979279792753376		[learning rate: 0.0026292]
		[batch 20/20] avg loss: 0.10790548972174192		[learning rate: 0.0026244]
	Learning Rate: 0.00262438
	LOSS [training: 0.10291673449853975 | validation: 0.14775752945307793]
	TIME [epoch: 8.21 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1268642791462505		[learning rate: 0.0026196]
		[batch 20/20] avg loss: 0.1098584867323799		[learning rate: 0.0026149]
	Learning Rate: 0.00261485
	LOSS [training: 0.11836138293931522 | validation: 0.07864617701453869]
	TIME [epoch: 8.18 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08992793879955216		[learning rate: 0.0026101]
		[batch 20/20] avg loss: 0.11180025121333563		[learning rate: 0.0026054]
	Learning Rate: 0.00260536
	LOSS [training: 0.10086409500644389 | validation: 0.1433122563398862]
	TIME [epoch: 8.2 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15983456597362591		[learning rate: 0.0026006]
		[batch 20/20] avg loss: 0.10494491878077002		[learning rate: 0.0025959]
	Learning Rate: 0.00259591
	LOSS [training: 0.13238974237719797 | validation: 0.09024334952606608]
	TIME [epoch: 8.2 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19552063091486577		[learning rate: 0.0025912]
		[batch 20/20] avg loss: 0.1382516807957762		[learning rate: 0.0025865]
	Learning Rate: 0.00258649
	LOSS [training: 0.16688615585532102 | validation: 0.126761774096682]
	TIME [epoch: 8.21 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11268752059273761		[learning rate: 0.0025818]
		[batch 20/20] avg loss: 0.10677588741226866		[learning rate: 0.0025771]
	Learning Rate: 0.0025771
	LOSS [training: 0.10973170400250314 | validation: 0.10649567906080709]
	TIME [epoch: 8.19 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17967274491563875		[learning rate: 0.0025724]
		[batch 20/20] avg loss: 0.0908877036493754		[learning rate: 0.0025677]
	Learning Rate: 0.00256775
	LOSS [training: 0.13528022428250713 | validation: 0.07069595413980578]
	TIME [epoch: 8.2 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11197597594753506		[learning rate: 0.0025631]
		[batch 20/20] avg loss: 0.15350715192948847		[learning rate: 0.0025584]
	Learning Rate: 0.00255843
	LOSS [training: 0.13274156393851178 | validation: 0.09216367967424831]
	TIME [epoch: 8.2 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10324815211507983		[learning rate: 0.0025538]
		[batch 20/20] avg loss: 0.1345197375049459		[learning rate: 0.0025491]
	Learning Rate: 0.00254915
	LOSS [training: 0.11888394481001281 | validation: 0.11449002641214784]
	TIME [epoch: 8.22 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15461482832676327		[learning rate: 0.0025445]
		[batch 20/20] avg loss: 0.1835252299273157		[learning rate: 0.0025399]
	Learning Rate: 0.0025399
	LOSS [training: 0.1690700291270395 | validation: 0.0912743229692657]
	TIME [epoch: 8.18 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10589199701022342		[learning rate: 0.0025353]
		[batch 20/20] avg loss: 0.1436932616920726		[learning rate: 0.0025307]
	Learning Rate: 0.00253068
	LOSS [training: 0.12479262935114803 | validation: 0.07518897946592387]
	TIME [epoch: 8.21 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1264101994115913		[learning rate: 0.0025261]
		[batch 20/20] avg loss: 0.11822811163758422		[learning rate: 0.0025215]
	Learning Rate: 0.00252149
	LOSS [training: 0.12231915552458775 | validation: 0.18131042155102634]
	TIME [epoch: 8.2 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10380797870861802		[learning rate: 0.0025169]
		[batch 20/20] avg loss: 0.1902750209452382		[learning rate: 0.0025123]
	Learning Rate: 0.00251234
	LOSS [training: 0.1470414998269281 | validation: 0.12782699709188805]
	TIME [epoch: 8.21 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09549346595665256		[learning rate: 0.0025078]
		[batch 20/20] avg loss: 0.10979945828134068		[learning rate: 0.0025032]
	Learning Rate: 0.00250323
	LOSS [training: 0.10264646211899661 | validation: 0.0770191058645574]
	TIME [epoch: 8.19 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10176106028243086		[learning rate: 0.0024987]
		[batch 20/20] avg loss: 0.11566421812754377		[learning rate: 0.0024941]
	Learning Rate: 0.00249414
	LOSS [training: 0.1087126392049873 | validation: 0.13550754880674915]
	TIME [epoch: 8.21 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13464490806503904		[learning rate: 0.0024896]
		[batch 20/20] avg loss: 0.1579627035422556		[learning rate: 0.0024851]
	Learning Rate: 0.00248509
	LOSS [training: 0.14630380580364732 | validation: 0.1618728972194945]
	TIME [epoch: 8.2 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09721900139538597		[learning rate: 0.0024806]
		[batch 20/20] avg loss: 0.09828364950420186		[learning rate: 0.0024761]
	Learning Rate: 0.00247607
	LOSS [training: 0.09775132544979391 | validation: 0.10179536467910365]
	TIME [epoch: 8.21 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10666794741837635		[learning rate: 0.0024716]
		[batch 20/20] avg loss: 0.10158098728883595		[learning rate: 0.0024671]
	Learning Rate: 0.00246709
	LOSS [training: 0.10412446735360617 | validation: 0.08747384732697523]
	TIME [epoch: 8.19 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09363571948783088		[learning rate: 0.0024626]
		[batch 20/20] avg loss: 0.10709129916018752		[learning rate: 0.0024581]
	Learning Rate: 0.00245813
	LOSS [training: 0.1003635093240092 | validation: 0.10151273068721825]
	TIME [epoch: 8.21 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11026826732679389		[learning rate: 0.0024537]
		[batch 20/20] avg loss: 0.14355990997074178		[learning rate: 0.0024492]
	Learning Rate: 0.00244921
	LOSS [training: 0.1269140886487678 | validation: 0.22387964977542973]
	TIME [epoch: 8.2 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10603096957624858		[learning rate: 0.0024448]
		[batch 20/20] avg loss: 0.13125416120620315		[learning rate: 0.0024403]
	Learning Rate: 0.00244032
	LOSS [training: 0.11864256539122583 | validation: 0.106925649963806]
	TIME [epoch: 8.2 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09600627221813338		[learning rate: 0.0024359]
		[batch 20/20] avg loss: 0.10082268658638968		[learning rate: 0.0024315]
	Learning Rate: 0.00243147
	LOSS [training: 0.09841447940226153 | validation: 0.2097270379881281]
	TIME [epoch: 8.19 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11597743735411381		[learning rate: 0.0024271]
		[batch 20/20] avg loss: 0.08422645641538674		[learning rate: 0.0024226]
	Learning Rate: 0.00242264
	LOSS [training: 0.10010194688475027 | validation: 0.10525654656269842]
	TIME [epoch: 8.22 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08913316052203502		[learning rate: 0.0024182]
		[batch 20/20] avg loss: 0.12630710745676899		[learning rate: 0.0024139]
	Learning Rate: 0.00241385
	LOSS [training: 0.10772013398940199 | validation: 0.34988625466300455]
	TIME [epoch: 8.19 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22810699158655257		[learning rate: 0.0024095]
		[batch 20/20] avg loss: 0.12981183263623938		[learning rate: 0.0024051]
	Learning Rate: 0.00240509
	LOSS [training: 0.178959412111396 | validation: 0.08685127018283101]
	TIME [epoch: 8.2 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0821850371450304		[learning rate: 0.0024007]
		[batch 20/20] avg loss: 0.1123239865983475		[learning rate: 0.0023964]
	Learning Rate: 0.00239636
	LOSS [training: 0.09725451187168895 | validation: 0.08457893235501814]
	TIME [epoch: 8.19 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0919457805049674		[learning rate: 0.002392]
		[batch 20/20] avg loss: 0.1056256281447621		[learning rate: 0.0023877]
	Learning Rate: 0.00238767
	LOSS [training: 0.09878570432486475 | validation: 0.14757778256056212]
	TIME [epoch: 8.22 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10696736722612266		[learning rate: 0.0023833]
		[batch 20/20] avg loss: 0.10344826613962936		[learning rate: 0.002379]
	Learning Rate: 0.002379
	LOSS [training: 0.10520781668287602 | validation: 0.07642288224767728]
	TIME [epoch: 8.2 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16384180769057377		[learning rate: 0.0023747]
		[batch 20/20] avg loss: 0.12289879507726875		[learning rate: 0.0023704]
	Learning Rate: 0.00237037
	LOSS [training: 0.14337030138392126 | validation: 0.08336070975013361]
	TIME [epoch: 8.2 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09953027387398251		[learning rate: 0.0023661]
		[batch 20/20] avg loss: 0.09546621976301059		[learning rate: 0.0023618]
	Learning Rate: 0.00236177
	LOSS [training: 0.09749824681849656 | validation: 0.08077553793635221]
	TIME [epoch: 8.2 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1351177149757083		[learning rate: 0.0023575]
		[batch 20/20] avg loss: 0.08750984601869112		[learning rate: 0.0023532]
	Learning Rate: 0.00235319
	LOSS [training: 0.11131378049719971 | validation: 0.16772873011512082]
	TIME [epoch: 8.21 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17933462694460056		[learning rate: 0.0023489]
		[batch 20/20] avg loss: 0.08139850068367997		[learning rate: 0.0023447]
	Learning Rate: 0.00234465
	LOSS [training: 0.1303665638141403 | validation: 0.09106258835468195]
	TIME [epoch: 8.2 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0839073193894223		[learning rate: 0.0023404]
		[batch 20/20] avg loss: 0.10760390515433907		[learning rate: 0.0023361]
	Learning Rate: 0.00233615
	LOSS [training: 0.09575561227188067 | validation: 0.08663003463818247]
	TIME [epoch: 8.2 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08727978036855497		[learning rate: 0.0023319]
		[batch 20/20] avg loss: 0.09785289497095477		[learning rate: 0.0023277]
	Learning Rate: 0.00232767
	LOSS [training: 0.09256633766975486 | validation: 0.16033473075173799]
	TIME [epoch: 8.2 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09946319457938001		[learning rate: 0.0023234]
		[batch 20/20] avg loss: 0.09863666971104743		[learning rate: 0.0023192]
	Learning Rate: 0.00231922
	LOSS [training: 0.09904993214521371 | validation: 0.08338408528644456]
	TIME [epoch: 8.22 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07995577486493108		[learning rate: 0.002315]
		[batch 20/20] avg loss: 0.0950070681316246		[learning rate: 0.0023108]
	Learning Rate: 0.0023108
	LOSS [training: 0.08748142149827784 | validation: 0.061709010954606514]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_503.pth
	Model improved!!!
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1026046686181531		[learning rate: 0.0023066]
		[batch 20/20] avg loss: 0.08895088328527356		[learning rate: 0.0023024]
	Learning Rate: 0.00230242
	LOSS [training: 0.09577777595171336 | validation: 0.12234944857515158]
	TIME [epoch: 8.19 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11529933616851258		[learning rate: 0.0022982]
		[batch 20/20] avg loss: 0.08975298528953793		[learning rate: 0.0022941]
	Learning Rate: 0.00229406
	LOSS [training: 0.10252616072902525 | validation: 0.06344193660815406]
	TIME [epoch: 8.19 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0889151787397903		[learning rate: 0.0022899]
		[batch 20/20] avg loss: 0.06854783040609083		[learning rate: 0.0022857]
	Learning Rate: 0.00228574
	LOSS [training: 0.07873150457294056 | validation: 0.1265898152597959]
	TIME [epoch: 8.2 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08201445791599313		[learning rate: 0.0022816]
		[batch 20/20] avg loss: 0.11417266843136845		[learning rate: 0.0022774]
	Learning Rate: 0.00227744
	LOSS [training: 0.09809356317368079 | validation: 0.16973480244280736]
	TIME [epoch: 8.19 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10291268093816801		[learning rate: 0.0022733]
		[batch 20/20] avg loss: 0.12087615231126758		[learning rate: 0.0022692]
	Learning Rate: 0.00226918
	LOSS [training: 0.1118944166247178 | validation: 0.1076921554477053]
	TIME [epoch: 8.18 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12036770459919988		[learning rate: 0.0022651]
		[batch 20/20] avg loss: 0.1043590402876183		[learning rate: 0.0022609]
	Learning Rate: 0.00226094
	LOSS [training: 0.1123633724434091 | validation: 0.09548915442341865]
	TIME [epoch: 8.19 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10956906510591684		[learning rate: 0.0022568]
		[batch 20/20] avg loss: 0.11552634961620567		[learning rate: 0.0022527]
	Learning Rate: 0.00225274
	LOSS [training: 0.11254770736106126 | validation: 0.052377723251541324]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07765825977855931		[learning rate: 0.0022486]
		[batch 20/20] avg loss: 0.14339775131488902		[learning rate: 0.0022446]
	Learning Rate: 0.00224456
	LOSS [training: 0.11052800554672418 | validation: 0.16458423926995666]
	TIME [epoch: 8.18 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11285080377646337		[learning rate: 0.0022405]
		[batch 20/20] avg loss: 0.10833053798206342		[learning rate: 0.0022364]
	Learning Rate: 0.00223642
	LOSS [training: 0.1105906708792634 | validation: 0.078339944039527]
	TIME [epoch: 8.19 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09544117844037321		[learning rate: 0.0022324]
		[batch 20/20] avg loss: 0.0930838222659914		[learning rate: 0.0022283]
	Learning Rate: 0.0022283
	LOSS [training: 0.09426250035318232 | validation: 0.0797802105009907]
	TIME [epoch: 8.18 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0934102695526399		[learning rate: 0.0022243]
		[batch 20/20] avg loss: 0.11761339944024043		[learning rate: 0.0022202]
	Learning Rate: 0.00222021
	LOSS [training: 0.10551183449644015 | validation: 0.09575447285368875]
	TIME [epoch: 8.21 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.110300914275779		[learning rate: 0.0022162]
		[batch 20/20] avg loss: 0.12378430985341515		[learning rate: 0.0022122]
	Learning Rate: 0.00221216
	LOSS [training: 0.1170426120645971 | validation: 0.08668368392556469]
	TIME [epoch: 8.17 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11345025727631335		[learning rate: 0.0022081]
		[batch 20/20] avg loss: 0.09376679493290088		[learning rate: 0.0022041]
	Learning Rate: 0.00220413
	LOSS [training: 0.10360852610460711 | validation: 0.24943665581899777]
	TIME [epoch: 8.18 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1201178089822837		[learning rate: 0.0022001]
		[batch 20/20] avg loss: 0.07183724172645894		[learning rate: 0.0021961]
	Learning Rate: 0.00219613
	LOSS [training: 0.09597752535437135 | validation: 0.15273144410116107]
	TIME [epoch: 8.19 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13310696612441658		[learning rate: 0.0021921]
		[batch 20/20] avg loss: 0.0670371478320386		[learning rate: 0.0021882]
	Learning Rate: 0.00218816
	LOSS [training: 0.10007205697822756 | validation: 0.07758707198700393]
	TIME [epoch: 8.21 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1296644100281643		[learning rate: 0.0021842]
		[batch 20/20] avg loss: 0.0933593414968383		[learning rate: 0.0021802]
	Learning Rate: 0.00218022
	LOSS [training: 0.1115118757625013 | validation: 0.19493836937544548]
	TIME [epoch: 8.18 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10966165839424027		[learning rate: 0.0021763]
		[batch 20/20] avg loss: 0.11860393163732999		[learning rate: 0.0021723]
	Learning Rate: 0.00217231
	LOSS [training: 0.11413279501578513 | validation: 0.13842065243469484]
	TIME [epoch: 8.18 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11255322736018569		[learning rate: 0.0021684]
		[batch 20/20] avg loss: 0.08112385061440491		[learning rate: 0.0021644]
	Learning Rate: 0.00216442
	LOSS [training: 0.0968385389872953 | validation: 0.09240938113159508]
	TIME [epoch: 8.21 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10642885529560653		[learning rate: 0.0021605]
		[batch 20/20] avg loss: 0.11734436353885525		[learning rate: 0.0021566]
	Learning Rate: 0.00215657
	LOSS [training: 0.11188660941723087 | validation: 0.4810227202093572]
	TIME [epoch: 8.2 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20632899646505617		[learning rate: 0.0021527]
		[batch 20/20] avg loss: 0.07882468667418033		[learning rate: 0.0021487]
	Learning Rate: 0.00214874
	LOSS [training: 0.14257684156961828 | validation: 0.11077873087842968]
	TIME [epoch: 8.18 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0995236503535728		[learning rate: 0.0021448]
		[batch 20/20] avg loss: 0.13770700501986016		[learning rate: 0.0021409]
	Learning Rate: 0.00214094
	LOSS [training: 0.1186153276867165 | validation: 0.07035267108371383]
	TIME [epoch: 8.18 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11591181723715431		[learning rate: 0.0021371]
		[batch 20/20] avg loss: 0.10294677535037086		[learning rate: 0.0021332]
	Learning Rate: 0.00213317
	LOSS [training: 0.10942929629376257 | validation: 0.12041234825490627]
	TIME [epoch: 8.21 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.095995186796289		[learning rate: 0.0021293]
		[batch 20/20] avg loss: 0.14797888064343573		[learning rate: 0.0021254]
	Learning Rate: 0.00212543
	LOSS [training: 0.12198703371986239 | validation: 0.09714262074535829]
	TIME [epoch: 8.21 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0902489374353856		[learning rate: 0.0021216]
		[batch 20/20] avg loss: 0.14752610064187252		[learning rate: 0.0021177]
	Learning Rate: 0.00211772
	LOSS [training: 0.11888751903862907 | validation: 0.08397471721155045]
	TIME [epoch: 8.17 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11448113120201223		[learning rate: 0.0021139]
		[batch 20/20] avg loss: 0.1448565870566209		[learning rate: 0.00211]
	Learning Rate: 0.00211003
	LOSS [training: 0.12966885912931653 | validation: 0.12940253510281083]
	TIME [epoch: 8.17 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12052036617679049		[learning rate: 0.0021062]
		[batch 20/20] avg loss: 0.12959925266200206		[learning rate: 0.0021024]
	Learning Rate: 0.00210238
	LOSS [training: 0.1250598094193963 | validation: 0.05180559176926732]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_529.pth
	Model improved!!!
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07686348445944471		[learning rate: 0.0020986]
		[batch 20/20] avg loss: 0.09023120036392254		[learning rate: 0.0020947]
	Learning Rate: 0.00209475
	LOSS [training: 0.08354734241168361 | validation: 0.0928971743850438]
	TIME [epoch: 8.2 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0957333108418807		[learning rate: 0.0020909]
		[batch 20/20] avg loss: 0.10413282221619344		[learning rate: 0.0020871]
	Learning Rate: 0.00208714
	LOSS [training: 0.09993306652903708 | validation: 0.09416352868972493]
	TIME [epoch: 8.17 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12477601420954057		[learning rate: 0.0020834]
		[batch 20/20] avg loss: 0.09679664209205313		[learning rate: 0.0020796]
	Learning Rate: 0.00207957
	LOSS [training: 0.11078632815079685 | validation: 0.08516171394216296]
	TIME [epoch: 8.17 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12035350730214436		[learning rate: 0.0020758]
		[batch 20/20] avg loss: 0.10871470805568766		[learning rate: 0.002072]
	Learning Rate: 0.00207202
	LOSS [training: 0.114534107678916 | validation: 0.13039644514608248]
	TIME [epoch: 8.22 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10597197481583467		[learning rate: 0.0020683]
		[batch 20/20] avg loss: 0.09415702115941982		[learning rate: 0.0020645]
	Learning Rate: 0.0020645
	LOSS [training: 0.10006449798762725 | validation: 0.15733791441776535]
	TIME [epoch: 8.19 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12286847720673084		[learning rate: 0.0020608]
		[batch 20/20] avg loss: 0.119478626469541		[learning rate: 0.002057]
	Learning Rate: 0.00205701
	LOSS [training: 0.12117355183813591 | validation: 0.09062450299816688]
	TIME [epoch: 8.17 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11872637296620496		[learning rate: 0.0020533]
		[batch 20/20] avg loss: 0.1341490247142531		[learning rate: 0.0020495]
	Learning Rate: 0.00204955
	LOSS [training: 0.12643769884022904 | validation: 0.07592287514988956]
	TIME [epoch: 8.17 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08014151723598498		[learning rate: 0.0020458]
		[batch 20/20] avg loss: 0.1094923104865819		[learning rate: 0.0020421]
	Learning Rate: 0.00204211
	LOSS [training: 0.09481691386128341 | validation: 0.10206578417712084]
	TIME [epoch: 8.22 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12120229582950563		[learning rate: 0.0020384]
		[batch 20/20] avg loss: 0.1113657163525944		[learning rate: 0.0020347]
	Learning Rate: 0.0020347
	LOSS [training: 0.11628400609105001 | validation: 0.053137927253079795]
	TIME [epoch: 8.19 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09314350429737789		[learning rate: 0.002031]
		[batch 20/20] avg loss: 0.14344030769342037		[learning rate: 0.0020273]
	Learning Rate: 0.00202731
	LOSS [training: 0.11829190599539911 | validation: 0.03787707283561667]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_539.pth
	Model improved!!!
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06982058499517294		[learning rate: 0.0020236]
		[batch 20/20] avg loss: 0.0918951432614074		[learning rate: 0.00202]
	Learning Rate: 0.00201996
	LOSS [training: 0.08085786412829017 | validation: 0.16331957318915322]
	TIME [epoch: 8.18 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12238355052793967		[learning rate: 0.0020163]
		[batch 20/20] avg loss: 0.07599383503053381		[learning rate: 0.0020126]
	Learning Rate: 0.00201263
	LOSS [training: 0.09918869277923673 | validation: 0.13634478934329522]
	TIME [epoch: 8.22 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.123349936710925		[learning rate: 0.002009]
		[batch 20/20] avg loss: 0.12359419335452418		[learning rate: 0.0020053]
	Learning Rate: 0.00200532
	LOSS [training: 0.12347206503272459 | validation: 0.23993233259436894]
	TIME [epoch: 8.18 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16905755025560715		[learning rate: 0.0020017]
		[batch 20/20] avg loss: 0.09148611153127621		[learning rate: 0.001998]
	Learning Rate: 0.00199805
	LOSS [training: 0.1302718308934417 | validation: 0.13062652336891087]
	TIME [epoch: 8.16 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08043092914429766		[learning rate: 0.0019944]
		[batch 20/20] avg loss: 0.08092366618694187		[learning rate: 0.0019908]
	Learning Rate: 0.00199079
	LOSS [training: 0.08067729766561976 | validation: 0.40375267041942303]
	TIME [epoch: 8.17 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1427746227839422		[learning rate: 0.0019872]
		[batch 20/20] avg loss: 0.10608166664288507		[learning rate: 0.0019836]
	Learning Rate: 0.00198357
	LOSS [training: 0.12442814471341361 | validation: 0.12703813954266788]
	TIME [epoch: 8.23 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09381453346629424		[learning rate: 0.00198]
		[batch 20/20] avg loss: 0.1690003673300252		[learning rate: 0.0019764]
	Learning Rate: 0.00197637
	LOSS [training: 0.1314074503981597 | validation: 0.04629392971747083]
	TIME [epoch: 8.18 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06505500027463838		[learning rate: 0.0019728]
		[batch 20/20] avg loss: 0.07218229857006991		[learning rate: 0.0019692]
	Learning Rate: 0.0019692
	LOSS [training: 0.06861864942235416 | validation: 0.2482542791407314]
	TIME [epoch: 8.18 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.151865712212012		[learning rate: 0.0019656]
		[batch 20/20] avg loss: 0.09884910324527488		[learning rate: 0.0019621]
	Learning Rate: 0.00196205
	LOSS [training: 0.12535740772864343 | validation: 0.09937049415814474]
	TIME [epoch: 8.17 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1076128233188542		[learning rate: 0.0019585]
		[batch 20/20] avg loss: 0.12976902784579697		[learning rate: 0.0019549]
	Learning Rate: 0.00195493
	LOSS [training: 0.11869092558232555 | validation: 0.14963045432914662]
	TIME [epoch: 8.23 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10650621505611746		[learning rate: 0.0019514]
		[batch 20/20] avg loss: 0.11464310638839106		[learning rate: 0.0019478]
	Learning Rate: 0.00194784
	LOSS [training: 0.11057466072225428 | validation: 0.13490432279168033]
	TIME [epoch: 8.17 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11159273351675651		[learning rate: 0.0019443]
		[batch 20/20] avg loss: 0.13094915126587875		[learning rate: 0.0019408]
	Learning Rate: 0.00194077
	LOSS [training: 0.12127094239131764 | validation: 0.16211453468937514]
	TIME [epoch: 8.16 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08860085033839174		[learning rate: 0.0019372]
		[batch 20/20] avg loss: 0.09165590614347371		[learning rate: 0.0019337]
	Learning Rate: 0.00193373
	LOSS [training: 0.09012837824093273 | validation: 0.08868469819566879]
	TIME [epoch: 8.17 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08333640170179032		[learning rate: 0.0019302]
		[batch 20/20] avg loss: 0.11589612817763595		[learning rate: 0.0019267]
	Learning Rate: 0.00192671
	LOSS [training: 0.09961626493971314 | validation: 0.0912270507725053]
	TIME [epoch: 8.23 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08595669710568389		[learning rate: 0.0019232]
		[batch 20/20] avg loss: 0.08255488220062504		[learning rate: 0.0019197]
	Learning Rate: 0.00191972
	LOSS [training: 0.08425578965315446 | validation: 0.08393175543974213]
	TIME [epoch: 8.19 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07342880722129612		[learning rate: 0.0019162]
		[batch 20/20] avg loss: 0.06789825053033204		[learning rate: 0.0019127]
	Learning Rate: 0.00191275
	LOSS [training: 0.07066352887581409 | validation: 0.13077225258257635]
	TIME [epoch: 8.17 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0751371834891763		[learning rate: 0.0019093]
		[batch 20/20] avg loss: 0.08796151763169971		[learning rate: 0.0019058]
	Learning Rate: 0.00190581
	LOSS [training: 0.081549350560438 | validation: 0.04389602641517798]
	TIME [epoch: 8.17 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09652901597122267		[learning rate: 0.0019023]
		[batch 20/20] avg loss: 0.09316359453790617		[learning rate: 0.0018989]
	Learning Rate: 0.00189889
	LOSS [training: 0.09484630525456442 | validation: 0.21889825874534352]
	TIME [epoch: 8.23 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10579476533070262		[learning rate: 0.0018954]
		[batch 20/20] avg loss: 0.0862401767928034		[learning rate: 0.001892]
	Learning Rate: 0.001892
	LOSS [training: 0.09601747106175298 | validation: 0.10468850424188551]
	TIME [epoch: 8.18 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1006652823379027		[learning rate: 0.0018886]
		[batch 20/20] avg loss: 0.08374942489773654		[learning rate: 0.0018851]
	Learning Rate: 0.00188513
	LOSS [training: 0.09220735361781965 | validation: 0.07307972564123517]
	TIME [epoch: 8.17 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07021591050694752		[learning rate: 0.0018817]
		[batch 20/20] avg loss: 0.07850389859241669		[learning rate: 0.0018783]
	Learning Rate: 0.00187829
	LOSS [training: 0.07435990454968208 | validation: 0.06369817793509511]
	TIME [epoch: 8.17 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09784586173607002		[learning rate: 0.0018749]
		[batch 20/20] avg loss: 0.09642412523796554		[learning rate: 0.0018715]
	Learning Rate: 0.00187148
	LOSS [training: 0.09713499348701778 | validation: 0.1771158896454517]
	TIME [epoch: 8.23 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11658002436840227		[learning rate: 0.0018681]
		[batch 20/20] avg loss: 0.09026806237202624		[learning rate: 0.0018647]
	Learning Rate: 0.00186468
	LOSS [training: 0.10342404337021424 | validation: 0.15920472400659602]
	TIME [epoch: 8.18 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10840076795441937		[learning rate: 0.0018613]
		[batch 20/20] avg loss: 0.08898065201568031		[learning rate: 0.0018579]
	Learning Rate: 0.00185792
	LOSS [training: 0.09869070998504983 | validation: 0.10114970701663215]
	TIME [epoch: 8.16 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06707649188157785		[learning rate: 0.0018545]
		[batch 20/20] avg loss: 0.12071678522693689		[learning rate: 0.0018512]
	Learning Rate: 0.00185117
	LOSS [training: 0.09389663855425737 | validation: 0.07371300647580926]
	TIME [epoch: 8.16 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09862586395559222		[learning rate: 0.0018478]
		[batch 20/20] avg loss: 0.09974436188909772		[learning rate: 0.0018445]
	Learning Rate: 0.00184446
	LOSS [training: 0.09918511292234498 | validation: 0.14046150071450253]
	TIME [epoch: 8.23 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10078254829883274		[learning rate: 0.0018411]
		[batch 20/20] avg loss: 0.07787409552389445		[learning rate: 0.0018378]
	Learning Rate: 0.00183776
	LOSS [training: 0.08932832191136357 | validation: 0.07943299149645537]
	TIME [epoch: 8.18 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08376846921661125		[learning rate: 0.0018344]
		[batch 20/20] avg loss: 0.07868585071668778		[learning rate: 0.0018311]
	Learning Rate: 0.00183109
	LOSS [training: 0.0812271599666495 | validation: 0.08887027026475447]
	TIME [epoch: 8.17 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12280449004007363		[learning rate: 0.0018278]
		[batch 20/20] avg loss: 0.07162240479590647		[learning rate: 0.0018244]
	Learning Rate: 0.00182445
	LOSS [training: 0.09721344741799007 | validation: 0.06200037443588198]
	TIME [epoch: 8.17 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0523862102228219		[learning rate: 0.0018211]
		[batch 20/20] avg loss: 0.08659217048482258		[learning rate: 0.0018178]
	Learning Rate: 0.00181783
	LOSS [training: 0.06948919035382226 | validation: 0.039711452803709656]
	TIME [epoch: 8.23 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1203985604768287		[learning rate: 0.0018145]
		[batch 20/20] avg loss: 0.07127803265321783		[learning rate: 0.0018112]
	Learning Rate: 0.00181123
	LOSS [training: 0.09583829656502327 | validation: 0.12517922043641336]
	TIME [epoch: 8.18 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0630082028412923		[learning rate: 0.0018079]
		[batch 20/20] avg loss: 0.0865195078967381		[learning rate: 0.0018047]
	Learning Rate: 0.00180466
	LOSS [training: 0.0747638553690152 | validation: 0.1506101142573729]
	TIME [epoch: 8.18 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08353334875711782		[learning rate: 0.0018014]
		[batch 20/20] avg loss: 0.10061771892686863		[learning rate: 0.0017981]
	Learning Rate: 0.00179811
	LOSS [training: 0.09207553384199323 | validation: 0.10507049188596869]
	TIME [epoch: 8.17 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06900960777582428		[learning rate: 0.0017948]
		[batch 20/20] avg loss: 0.08251171317092827		[learning rate: 0.0017916]
	Learning Rate: 0.00179158
	LOSS [training: 0.07576066047337629 | validation: 0.04569156377774339]
	TIME [epoch: 8.23 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09311755606397201		[learning rate: 0.0017883]
		[batch 20/20] avg loss: 0.0799517304268601		[learning rate: 0.0017851]
	Learning Rate: 0.00178508
	LOSS [training: 0.08653464324541606 | validation: 0.11274221090098582]
	TIME [epoch: 8.18 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10694822490769992		[learning rate: 0.0017818]
		[batch 20/20] avg loss: 0.09810412925778451		[learning rate: 0.0017786]
	Learning Rate: 0.0017786
	LOSS [training: 0.10252617708274223 | validation: 0.036113745242648536]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_575.pth
	Model improved!!!
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06458462358173542		[learning rate: 0.0017754]
		[batch 20/20] avg loss: 0.09330169970566447		[learning rate: 0.0017721]
	Learning Rate: 0.00177215
	LOSS [training: 0.07894316164369997 | validation: 0.09753567627497874]
	TIME [epoch: 8.18 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0793102539307122		[learning rate: 0.0017689]
		[batch 20/20] avg loss: 0.08857901276400396		[learning rate: 0.0017657]
	Learning Rate: 0.00176572
	LOSS [training: 0.08394463334735808 | validation: 0.04729178390845862]
	TIME [epoch: 8.23 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10523710113653498		[learning rate: 0.0017625]
		[batch 20/20] avg loss: 0.0864717251876167		[learning rate: 0.0017593]
	Learning Rate: 0.00175931
	LOSS [training: 0.09585441316207584 | validation: 0.08872160725552411]
	TIME [epoch: 8.17 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08528851308045929		[learning rate: 0.0017561]
		[batch 20/20] avg loss: 0.07354809204347627		[learning rate: 0.0017529]
	Learning Rate: 0.00175292
	LOSS [training: 0.07941830256196776 | validation: 0.04665158531342295]
	TIME [epoch: 8.17 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10964938589350195		[learning rate: 0.0017497]
		[batch 20/20] avg loss: 0.06529433814291805		[learning rate: 0.0017466]
	Learning Rate: 0.00174656
	LOSS [training: 0.08747186201821003 | validation: 0.03833350128726407]
	TIME [epoch: 8.18 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05839480579336988		[learning rate: 0.0017434]
		[batch 20/20] avg loss: 0.1335441666091537		[learning rate: 0.0017402]
	Learning Rate: 0.00174022
	LOSS [training: 0.09596948620126178 | validation: 0.08895396002945222]
	TIME [epoch: 8.22 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08904675483618585		[learning rate: 0.0017371]
		[batch 20/20] avg loss: 0.07926794737319685		[learning rate: 0.0017339]
	Learning Rate: 0.00173391
	LOSS [training: 0.08415735110469136 | validation: 0.12394092963970968]
	TIME [epoch: 8.18 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11023637711610017		[learning rate: 0.0017308]
		[batch 20/20] avg loss: 0.11778334086397887		[learning rate: 0.0017276]
	Learning Rate: 0.00172762
	LOSS [training: 0.1140098589900395 | validation: 0.1902472705760402]
	TIME [epoch: 8.17 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09115375238337213		[learning rate: 0.0017245]
		[batch 20/20] avg loss: 0.09901809479526187		[learning rate: 0.0017213]
	Learning Rate: 0.00172135
	LOSS [training: 0.09508592358931701 | validation: 0.16547712763134154]
	TIME [epoch: 8.18 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09631830759596904		[learning rate: 0.0017182]
		[batch 20/20] avg loss: 0.0786170614025541		[learning rate: 0.0017151]
	Learning Rate: 0.0017151
	LOSS [training: 0.08746768449926157 | validation: 0.0676703663290967]
	TIME [epoch: 8.22 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08516114006498102		[learning rate: 0.001712]
		[batch 20/20] avg loss: 0.11239140078095536		[learning rate: 0.0017089]
	Learning Rate: 0.00170888
	LOSS [training: 0.09877627042296819 | validation: 0.12139525872669508]
	TIME [epoch: 8.18 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0940211975049328		[learning rate: 0.0017058]
		[batch 20/20] avg loss: 0.08696565283438025		[learning rate: 0.0017027]
	Learning Rate: 0.00170267
	LOSS [training: 0.09049342516965651 | validation: 0.07813210317006118]
	TIME [epoch: 8.17 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08948101456830297		[learning rate: 0.0016996]
		[batch 20/20] avg loss: 0.08128162710514363		[learning rate: 0.0016965]
	Learning Rate: 0.0016965
	LOSS [training: 0.0853813208367233 | validation: 0.1345089037480514]
	TIME [epoch: 8.17 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0991001963059633		[learning rate: 0.0016934]
		[batch 20/20] avg loss: 0.10086970805314166		[learning rate: 0.0016903]
	Learning Rate: 0.00169034
	LOSS [training: 0.09998495217955247 | validation: 0.05104162767213952]
	TIME [epoch: 8.23 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13298723729080847		[learning rate: 0.0016873]
		[batch 20/20] avg loss: 0.09424829684968702		[learning rate: 0.0016842]
	Learning Rate: 0.0016842
	LOSS [training: 0.11361776707024776 | validation: 0.06444753662051603]
	TIME [epoch: 8.18 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09255672375891076		[learning rate: 0.0016811]
		[batch 20/20] avg loss: 0.09890369616462281		[learning rate: 0.0016781]
	Learning Rate: 0.00167809
	LOSS [training: 0.09573020996176677 | validation: 0.07305692836089396]
	TIME [epoch: 8.17 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09610574812483659		[learning rate: 0.001675]
		[batch 20/20] avg loss: 0.0722737417557394		[learning rate: 0.001672]
	Learning Rate: 0.001672
	LOSS [training: 0.08418974494028801 | validation: 0.15190637399964144]
	TIME [epoch: 8.18 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07755117495094661		[learning rate: 0.001669]
		[batch 20/20] avg loss: 0.09689705961059356		[learning rate: 0.0016659]
	Learning Rate: 0.00166593
	LOSS [training: 0.08722411728077009 | validation: 0.12717194128406503]
	TIME [epoch: 8.22 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09477143994134142		[learning rate: 0.0016629]
		[batch 20/20] avg loss: 0.08270214577731026		[learning rate: 0.0016599]
	Learning Rate: 0.00165989
	LOSS [training: 0.08873679285932584 | validation: 0.13012172489892346]
	TIME [epoch: 8.18 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09912106761660845		[learning rate: 0.0016569]
		[batch 20/20] avg loss: 0.08397828877635552		[learning rate: 0.0016539]
	Learning Rate: 0.00165387
	LOSS [training: 0.09154967819648198 | validation: 0.06965154721269437]
	TIME [epoch: 8.17 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06963096458941413		[learning rate: 0.0016509]
		[batch 20/20] avg loss: 0.06887490323557414		[learning rate: 0.0016479]
	Learning Rate: 0.00164786
	LOSS [training: 0.06925293391249412 | validation: 0.07421034641155798]
	TIME [epoch: 8.18 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09612859350600958		[learning rate: 0.0016449]
		[batch 20/20] avg loss: 0.06961797338780473		[learning rate: 0.0016419]
	Learning Rate: 0.00164188
	LOSS [training: 0.08287328344690716 | validation: 0.0801998300064603]
	TIME [epoch: 8.22 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08052795137451488		[learning rate: 0.0016389]
		[batch 20/20] avg loss: 0.11286948565229318		[learning rate: 0.0016359]
	Learning Rate: 0.00163592
	LOSS [training: 0.09669871851340402 | validation: 0.1431585173997897]
	TIME [epoch: 8.18 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09757912654269447		[learning rate: 0.001633]
		[batch 20/20] avg loss: 0.08113797200447162		[learning rate: 0.00163]
	Learning Rate: 0.00162999
	LOSS [training: 0.08935854927358304 | validation: 0.07316150099300497]
	TIME [epoch: 8.17 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09082987260168972		[learning rate: 0.001627]
		[batch 20/20] avg loss: 0.07148657628882847		[learning rate: 0.0016241]
	Learning Rate: 0.00162407
	LOSS [training: 0.08115822444525911 | validation: 0.0810932876988341]
	TIME [epoch: 8.18 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11733839994670961		[learning rate: 0.0016211]
		[batch 20/20] avg loss: 0.08284274860770871		[learning rate: 0.0016182]
	Learning Rate: 0.00161818
	LOSS [training: 0.10009057427720916 | validation: 0.06907335754168217]
	TIME [epoch: 8.21 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09573231433753193		[learning rate: 0.0016152]
		[batch 20/20] avg loss: 0.0772718678740325		[learning rate: 0.0016123]
	Learning Rate: 0.00161231
	LOSS [training: 0.0865020911057822 | validation: 0.07225197940188731]
	TIME [epoch: 8.19 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10030913512793113		[learning rate: 0.0016094]
		[batch 20/20] avg loss: 0.07119345532624585		[learning rate: 0.0016065]
	Learning Rate: 0.00160645
	LOSS [training: 0.0857512952270885 | validation: 0.04776471226522289]
	TIME [epoch: 8.17 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06262037744179706		[learning rate: 0.0016035]
		[batch 20/20] avg loss: 0.04959092262998634		[learning rate: 0.0016006]
	Learning Rate: 0.00160062
	LOSS [training: 0.056105650035891706 | validation: 0.0607559685431511]
	TIME [epoch: 8.18 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07719363382626518		[learning rate: 0.0015977]
		[batch 20/20] avg loss: 0.07321467501437273		[learning rate: 0.0015948]
	Learning Rate: 0.00159482
	LOSS [training: 0.07520415442031897 | validation: 0.16865457412107884]
	TIME [epoch: 8.21 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11595118254708085		[learning rate: 0.0015919]
		[batch 20/20] avg loss: 0.06478078373078976		[learning rate: 0.001589]
	Learning Rate: 0.00158903
	LOSS [training: 0.0903659831389353 | validation: 0.11390774005759409]
	TIME [epoch: 8.19 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09421502401031509		[learning rate: 0.0015861]
		[batch 20/20] avg loss: 0.06292851701883968		[learning rate: 0.0015833]
	Learning Rate: 0.00158326
	LOSS [training: 0.0785717705145774 | validation: 0.0755753339735106]
	TIME [epoch: 8.17 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1008619817662699		[learning rate: 0.0015804]
		[batch 20/20] avg loss: 0.0883165808920256		[learning rate: 0.0015775]
	Learning Rate: 0.00157752
	LOSS [training: 0.09458928132914773 | validation: 0.1115096905480487]
	TIME [epoch: 8.18 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07121463702013452		[learning rate: 0.0015747]
		[batch 20/20] avg loss: 0.10181175004979078		[learning rate: 0.0015718]
	Learning Rate: 0.00157179
	LOSS [training: 0.08651319353496263 | validation: 0.11157083778460009]
	TIME [epoch: 8.2 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0726534336728906		[learning rate: 0.0015689]
		[batch 20/20] avg loss: 0.14824414312189846		[learning rate: 0.0015661]
	Learning Rate: 0.00156609
	LOSS [training: 0.11044878839739454 | validation: 0.058244478750890116]
	TIME [epoch: 8.19 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07325397611698732		[learning rate: 0.0015632]
		[batch 20/20] avg loss: 0.08257364696688421		[learning rate: 0.0015604]
	Learning Rate: 0.0015604
	LOSS [training: 0.07791381154193576 | validation: 0.11420457245779185]
	TIME [epoch: 8.18 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053788986142772624		[learning rate: 0.0015576]
		[batch 20/20] avg loss: 0.07816055094780217		[learning rate: 0.0015547]
	Learning Rate: 0.00155474
	LOSS [training: 0.06597476854528739 | validation: 0.07069725749419821]
	TIME [epoch: 8.18 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06463546238742421		[learning rate: 0.0015519]
		[batch 20/20] avg loss: 0.0655321930660188		[learning rate: 0.0015491]
	Learning Rate: 0.0015491
	LOSS [training: 0.06508382772672151 | validation: 0.0834835225432227]
	TIME [epoch: 8.21 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07789681361959531		[learning rate: 0.0015463]
		[batch 20/20] avg loss: 0.0809499234945487		[learning rate: 0.0015435]
	Learning Rate: 0.00154348
	LOSS [training: 0.07942336855707202 | validation: 0.0787883733567219]
	TIME [epoch: 8.2 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0652356449831746		[learning rate: 0.0015407]
		[batch 20/20] avg loss: 0.07942040079960368		[learning rate: 0.0015379]
	Learning Rate: 0.00153788
	LOSS [training: 0.07232802289138913 | validation: 0.08611394036703757]
	TIME [epoch: 8.17 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06570418728213787		[learning rate: 0.0015351]
		[batch 20/20] avg loss: 0.1001475198049043		[learning rate: 0.0015323]
	Learning Rate: 0.00153229
	LOSS [training: 0.08292585354352108 | validation: 0.2252804251320666]
	TIME [epoch: 8.18 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09357771000691452		[learning rate: 0.0015295]
		[batch 20/20] avg loss: 0.0763924311167147		[learning rate: 0.0015267]
	Learning Rate: 0.00152673
	LOSS [training: 0.08498507056181463 | validation: 0.05678583595391857]
	TIME [epoch: 8.2 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0627163329392138		[learning rate: 0.001524]
		[batch 20/20] avg loss: 0.06328914281222472		[learning rate: 0.0015212]
	Learning Rate: 0.00152119
	LOSS [training: 0.06300273787571926 | validation: 0.04179119203204624]
	TIME [epoch: 8.2 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08116910979602902		[learning rate: 0.0015184]
		[batch 20/20] avg loss: 0.09097181890815423		[learning rate: 0.0015157]
	Learning Rate: 0.00151567
	LOSS [training: 0.08607046435209162 | validation: 0.0793102371353904]
	TIME [epoch: 8.17 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061448619810692294		[learning rate: 0.0015129]
		[batch 20/20] avg loss: 0.12214696459313683		[learning rate: 0.0015102]
	Learning Rate: 0.00151017
	LOSS [training: 0.09179779220191457 | validation: 0.2828980229128739]
	TIME [epoch: 8.18 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11136386409706225		[learning rate: 0.0015074]
		[batch 20/20] avg loss: 0.07285758284363512		[learning rate: 0.0015047]
	Learning Rate: 0.00150469
	LOSS [training: 0.09211072347034868 | validation: 0.07351913600478446]
	TIME [epoch: 8.2 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07881509011467532		[learning rate: 0.001502]
		[batch 20/20] avg loss: 0.05664562518575363		[learning rate: 0.0014992]
	Learning Rate: 0.00149923
	LOSS [training: 0.06773035765021448 | validation: 0.0828911435028955]
	TIME [epoch: 8.2 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07886802444404413		[learning rate: 0.0014965]
		[batch 20/20] avg loss: 0.07840898976947623		[learning rate: 0.0014938]
	Learning Rate: 0.00149379
	LOSS [training: 0.0786385071067602 | validation: 0.09725752938418807]
	TIME [epoch: 8.17 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07585642323462206		[learning rate: 0.0014911]
		[batch 20/20] avg loss: 0.07522064550055631		[learning rate: 0.0014884]
	Learning Rate: 0.00148837
	LOSS [training: 0.07553853436758917 | validation: 0.04647562147532749]
	TIME [epoch: 8.18 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07810743436902642		[learning rate: 0.0014857]
		[batch 20/20] avg loss: 0.046271660896808196		[learning rate: 0.001483]
	Learning Rate: 0.00148297
	LOSS [training: 0.06218954763291733 | validation: 0.0732491315772802]
	TIME [epoch: 8.21 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062458469366710054		[learning rate: 0.0014803]
		[batch 20/20] avg loss: 0.050400158907123536		[learning rate: 0.0014776]
	Learning Rate: 0.00147759
	LOSS [training: 0.056429314136916785 | validation: 0.04184588702614252]
	TIME [epoch: 8.21 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052595693529338315		[learning rate: 0.0014749]
		[batch 20/20] avg loss: 0.05982217263256393		[learning rate: 0.0014722]
	Learning Rate: 0.00147222
	LOSS [training: 0.056208933080951116 | validation: 0.09649785412588126]
	TIME [epoch: 8.18 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08342121018315307		[learning rate: 0.0014695]
		[batch 20/20] avg loss: 0.06514106071330023		[learning rate: 0.0014669]
	Learning Rate: 0.00146688
	LOSS [training: 0.07428113544822665 | validation: 0.0619057549784044]
	TIME [epoch: 8.18 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07006254563006786		[learning rate: 0.0014642]
		[batch 20/20] avg loss: 0.06560933182222353		[learning rate: 0.0014616]
	Learning Rate: 0.00146156
	LOSS [training: 0.06783593872614568 | validation: 0.062406513625550136]
	TIME [epoch: 8.21 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04879781506016154		[learning rate: 0.0014589]
		[batch 20/20] avg loss: 0.07222269582425898		[learning rate: 0.0014563]
	Learning Rate: 0.00145625
	LOSS [training: 0.06051025544221026 | validation: 0.09385516875082393]
	TIME [epoch: 8.2 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07194218140477823		[learning rate: 0.0014536]
		[batch 20/20] avg loss: 0.061760596419873734		[learning rate: 0.001451]
	Learning Rate: 0.00145097
	LOSS [training: 0.06685138891232598 | validation: 0.04334880801926287]
	TIME [epoch: 8.17 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06854218428085186		[learning rate: 0.0014483]
		[batch 20/20] avg loss: 0.07448848975190853		[learning rate: 0.0014457]
	Learning Rate: 0.0014457
	LOSS [training: 0.07151533701638019 | validation: 0.06388565110212627]
	TIME [epoch: 8.17 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.086862435328626		[learning rate: 0.0014431]
		[batch 20/20] avg loss: 0.06359130467401074		[learning rate: 0.0014405]
	Learning Rate: 0.00144046
	LOSS [training: 0.07522687000131838 | validation: 0.04555755830767218]
	TIME [epoch: 8.22 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07620691011530567		[learning rate: 0.0014378]
		[batch 20/20] avg loss: 0.061345601162250143		[learning rate: 0.0014352]
	Learning Rate: 0.00143523
	LOSS [training: 0.0687762556387779 | validation: 0.06738517017703508]
	TIME [epoch: 8.2 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06049994337867478		[learning rate: 0.0014326]
		[batch 20/20] avg loss: 0.06395981746830033		[learning rate: 0.00143]
	Learning Rate: 0.00143002
	LOSS [training: 0.06222988042348755 | validation: 0.04188015317545741]
	TIME [epoch: 8.17 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058826055060564965		[learning rate: 0.0014274]
		[batch 20/20] avg loss: 0.07574644299583973		[learning rate: 0.0014248]
	Learning Rate: 0.00142483
	LOSS [training: 0.06728624902820234 | validation: 0.07212383991155867]
	TIME [epoch: 8.17 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11086963670717773		[learning rate: 0.0014222]
		[batch 20/20] avg loss: 0.057339124490815685		[learning rate: 0.0014197]
	Learning Rate: 0.00141966
	LOSS [training: 0.0841043805989967 | validation: 0.057064470403061146]
	TIME [epoch: 8.2 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057957957207690515		[learning rate: 0.0014171]
		[batch 20/20] avg loss: 0.06916886404378436		[learning rate: 0.0014145]
	Learning Rate: 0.00141451
	LOSS [training: 0.06356341062573742 | validation: 0.11423724716689945]
	TIME [epoch: 8.2 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06734703735087685		[learning rate: 0.0014119]
		[batch 20/20] avg loss: 0.0627032724135004		[learning rate: 0.0014094]
	Learning Rate: 0.00140937
	LOSS [training: 0.06502515488218862 | validation: 0.07888133589161703]
	TIME [epoch: 8.17 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08652849995933067		[learning rate: 0.0014068]
		[batch 20/20] avg loss: 0.07824255933833599		[learning rate: 0.0014043]
	Learning Rate: 0.00140426
	LOSS [training: 0.08238552964883335 | validation: 0.12396125965838961]
	TIME [epoch: 8.18 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05559783074888708		[learning rate: 0.0014017]
		[batch 20/20] avg loss: 0.0722440839295396		[learning rate: 0.0013992]
	Learning Rate: 0.00139916
	LOSS [training: 0.06392095733921335 | validation: 0.0554189777408185]
	TIME [epoch: 8.21 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11149321811625801		[learning rate: 0.0013966]
		[batch 20/20] avg loss: 0.08013155523221475		[learning rate: 0.0013941]
	Learning Rate: 0.00139409
	LOSS [training: 0.09581238667423639 | validation: 0.055942919318714]
	TIME [epoch: 8.21 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06028560014852711		[learning rate: 0.0013916]
		[batch 20/20] avg loss: 0.06929440859173491		[learning rate: 0.001389]
	Learning Rate: 0.00138903
	LOSS [training: 0.06479000437013102 | validation: 0.090302522973226]
	TIME [epoch: 8.18 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05147242416017902		[learning rate: 0.0013865]
		[batch 20/20] avg loss: 0.060526935659201274		[learning rate: 0.001384]
	Learning Rate: 0.00138399
	LOSS [training: 0.055999679909690146 | validation: 0.06158402447332893]
	TIME [epoch: 8.17 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05725036941638183		[learning rate: 0.0013815]
		[batch 20/20] avg loss: 0.06804333842774349		[learning rate: 0.001379]
	Learning Rate: 0.00137896
	LOSS [training: 0.06264685392206266 | validation: 0.08194862353612367]
	TIME [epoch: 8.21 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07005803817487857		[learning rate: 0.0013765]
		[batch 20/20] avg loss: 0.0573463392301952		[learning rate: 0.001374]
	Learning Rate: 0.00137396
	LOSS [training: 0.06370218870253688 | validation: 0.06275006322781308]
	TIME [epoch: 8.2 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06530832234024452		[learning rate: 0.0013715]
		[batch 20/20] avg loss: 0.06532044992007245		[learning rate: 0.001369]
	Learning Rate: 0.00136897
	LOSS [training: 0.0653143861301585 | validation: 0.04650532932256367]
	TIME [epoch: 8.18 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08110879784433579		[learning rate: 0.0013665]
		[batch 20/20] avg loss: 0.09745448719563604		[learning rate: 0.001364]
	Learning Rate: 0.001364
	LOSS [training: 0.08928164251998591 | validation: 0.04452536188636266]
	TIME [epoch: 8.18 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056792358697460424		[learning rate: 0.0013615]
		[batch 20/20] avg loss: 0.05879370168596917		[learning rate: 0.0013591]
	Learning Rate: 0.00135905
	LOSS [training: 0.05779303019171479 | validation: 0.08811821501941156]
	TIME [epoch: 8.21 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0970774154239559		[learning rate: 0.0013566]
		[batch 20/20] avg loss: 0.0799493457506825		[learning rate: 0.0013541]
	Learning Rate: 0.00135412
	LOSS [training: 0.08851338058731918 | validation: 0.14148599265683687]
	TIME [epoch: 8.2 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07792916311522213		[learning rate: 0.0013517]
		[batch 20/20] avg loss: 0.05132676275018668		[learning rate: 0.0013492]
	Learning Rate: 0.00134921
	LOSS [training: 0.06462796293270442 | validation: 0.08532263501173251]
	TIME [epoch: 8.18 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05187114884001152		[learning rate: 0.0013468]
		[batch 20/20] avg loss: 0.05713995216233658		[learning rate: 0.0013443]
	Learning Rate: 0.00134431
	LOSS [training: 0.05450555050117404 | validation: 0.1307782957051723]
	TIME [epoch: 8.17 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06697104935628215		[learning rate: 0.0013419]
		[batch 20/20] avg loss: 0.08516724156558926		[learning rate: 0.0013394]
	Learning Rate: 0.00133943
	LOSS [training: 0.07606914546093571 | validation: 0.04894965247536141]
	TIME [epoch: 8.21 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057282885416532016		[learning rate: 0.001337]
		[batch 20/20] avg loss: 0.07067999467580906		[learning rate: 0.0013346]
	Learning Rate: 0.00133457
	LOSS [training: 0.06398144004617054 | validation: 0.0514004340412926]
	TIME [epoch: 8.19 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09637178477761565		[learning rate: 0.0013321]
		[batch 20/20] avg loss: 0.08967161393869164		[learning rate: 0.0013297]
	Learning Rate: 0.00132973
	LOSS [training: 0.09302169935815363 | validation: 0.09190523305719822]
	TIME [epoch: 8.19 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04644761010370195		[learning rate: 0.0013273]
		[batch 20/20] avg loss: 0.07280512152314512		[learning rate: 0.0013249]
	Learning Rate: 0.0013249
	LOSS [training: 0.059626365813423544 | validation: 0.04392098887957109]
	TIME [epoch: 8.18 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05735865956266213		[learning rate: 0.0013225]
		[batch 20/20] avg loss: 0.06873453716351302		[learning rate: 0.0013201]
	Learning Rate: 0.0013201
	LOSS [training: 0.06304659836308758 | validation: 0.1227103145025368]
	TIME [epoch: 8.21 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10795865731376071		[learning rate: 0.0013177]
		[batch 20/20] avg loss: 0.05964520977859587		[learning rate: 0.0013153]
	Learning Rate: 0.0013153
	LOSS [training: 0.08380193354617829 | validation: 0.06459612840759893]
	TIME [epoch: 8.19 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049106290908037316		[learning rate: 0.0013129]
		[batch 20/20] avg loss: 0.04944094899904102		[learning rate: 0.0013105]
	Learning Rate: 0.00131053
	LOSS [training: 0.04927361995353917 | validation: 0.07961581660665173]
	TIME [epoch: 8.18 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06794634447898615		[learning rate: 0.0013082]
		[batch 20/20] avg loss: 0.07289394667944737		[learning rate: 0.0013058]
	Learning Rate: 0.00130578
	LOSS [training: 0.07042014557921676 | validation: 0.12171369176862083]
	TIME [epoch: 8.19 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06394997371246965		[learning rate: 0.0013034]
		[batch 20/20] avg loss: 0.0546371445351583		[learning rate: 0.001301]
	Learning Rate: 0.00130104
	LOSS [training: 0.05929355912381399 | validation: 0.08423426115241434]
	TIME [epoch: 8.2 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0674250473904189		[learning rate: 0.0012987]
		[batch 20/20] avg loss: 0.06267640165699238		[learning rate: 0.0012963]
	Learning Rate: 0.00129631
	LOSS [training: 0.06505072452370564 | validation: 0.06717719480821148]
	TIME [epoch: 8.19 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06894000980690403		[learning rate: 0.001294]
		[batch 20/20] avg loss: 0.05916117017908775		[learning rate: 0.0012916]
	Learning Rate: 0.00129161
	LOSS [training: 0.06405058999299591 | validation: 0.052029079714566884]
	TIME [epoch: 8.18 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04638440673214376		[learning rate: 0.0012893]
		[batch 20/20] avg loss: 0.07919755109996593		[learning rate: 0.0012869]
	Learning Rate: 0.00128692
	LOSS [training: 0.06279097891605484 | validation: 0.06788115785980421]
	TIME [epoch: 8.17 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04904472166793726		[learning rate: 0.0012846]
		[batch 20/20] avg loss: 0.0692528818914274		[learning rate: 0.0012823]
	Learning Rate: 0.00128225
	LOSS [training: 0.059148801779682326 | validation: 0.05615815991704607]
	TIME [epoch: 8.21 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04948468454399514		[learning rate: 0.0012799]
		[batch 20/20] avg loss: 0.06713269662019009		[learning rate: 0.0012776]
	Learning Rate: 0.0012776
	LOSS [training: 0.05830869058209263 | validation: 0.04993238654609862]
	TIME [epoch: 8.19 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04876486826570689		[learning rate: 0.0012753]
		[batch 20/20] avg loss: 0.0415746593858271		[learning rate: 0.001273]
	Learning Rate: 0.00127296
	LOSS [training: 0.045169763825767 | validation: 0.034623638349179975]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_667.pth
	Model improved!!!
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04991216718051433		[learning rate: 0.0012707]
		[batch 20/20] avg loss: 0.07551622273406974		[learning rate: 0.0012683]
	Learning Rate: 0.00126834
	LOSS [training: 0.06271419495729204 | validation: 0.07664707029212872]
	TIME [epoch: 8.2 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062927187079199		[learning rate: 0.001266]
		[batch 20/20] avg loss: 0.05708984075190482		[learning rate: 0.0012637]
	Learning Rate: 0.00126374
	LOSS [training: 0.06000851391555192 | validation: 0.04660557026079296]
	TIME [epoch: 8.2 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06790455538986793		[learning rate: 0.0012614]
		[batch 20/20] avg loss: 0.06330150384480156		[learning rate: 0.0012592]
	Learning Rate: 0.00125915
	LOSS [training: 0.06560302961733473 | validation: 0.03718316285682856]
	TIME [epoch: 8.19 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06199861282885		[learning rate: 0.0012569]
		[batch 20/20] avg loss: 0.05623554286119179		[learning rate: 0.0012546]
	Learning Rate: 0.00125458
	LOSS [training: 0.059117077845020884 | validation: 0.09125341926177963]
	TIME [epoch: 8.19 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0643068250686383		[learning rate: 0.0012523]
		[batch 20/20] avg loss: 0.08026121082020375		[learning rate: 0.00125]
	Learning Rate: 0.00125003
	LOSS [training: 0.07228401794442102 | validation: 0.09952889517767749]
	TIME [epoch: 8.19 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06593649145117401		[learning rate: 0.0012478]
		[batch 20/20] avg loss: 0.06096683729520502		[learning rate: 0.0012455]
	Learning Rate: 0.0012455
	LOSS [training: 0.06345166437318951 | validation: 0.059203854634302035]
	TIME [epoch: 8.19 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05291309529970758		[learning rate: 0.0012432]
		[batch 20/20] avg loss: 0.0837550971053544		[learning rate: 0.001241]
	Learning Rate: 0.00124098
	LOSS [training: 0.06833409620253098 | validation: 0.06781105211977956]
	TIME [epoch: 8.19 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06465679270350452		[learning rate: 0.0012387]
		[batch 20/20] avg loss: 0.055967747874181836		[learning rate: 0.0012365]
	Learning Rate: 0.00123647
	LOSS [training: 0.06031227028884317 | validation: 0.07241178872264953]
	TIME [epoch: 8.19 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05567526391824135		[learning rate: 0.0012342]
		[batch 20/20] avg loss: 0.0500318752638357		[learning rate: 0.001232]
	Learning Rate: 0.00123198
	LOSS [training: 0.05285356959103852 | validation: 0.05969270447036014]
	TIME [epoch: 8.19 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06673187716479728		[learning rate: 0.0012297]
		[batch 20/20] avg loss: 0.04948344914383929		[learning rate: 0.0012275]
	Learning Rate: 0.00122751
	LOSS [training: 0.05810766315431829 | validation: 0.035334254205728645]
	TIME [epoch: 8.2 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03876338645917192		[learning rate: 0.0012253]
		[batch 20/20] avg loss: 0.06650448826276835		[learning rate: 0.0012231]
	Learning Rate: 0.00122306
	LOSS [training: 0.05263393736097013 | validation: 0.06716354577070224]
	TIME [epoch: 8.18 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08415703063691918		[learning rate: 0.0012208]
		[batch 20/20] avg loss: 0.06416705360179101		[learning rate: 0.0012186]
	Learning Rate: 0.00121862
	LOSS [training: 0.07416204211935509 | validation: 0.05744963872594617]
	TIME [epoch: 8.19 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051723799897356273		[learning rate: 0.0012164]
		[batch 20/20] avg loss: 0.07742975309701175		[learning rate: 0.0012142]
	Learning Rate: 0.0012142
	LOSS [training: 0.06457677649718402 | validation: 0.08772735802425258]
	TIME [epoch: 8.2 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04786512696738164		[learning rate: 0.001212]
		[batch 20/20] avg loss: 0.039192191318544		[learning rate: 0.0012098]
	Learning Rate: 0.00120979
	LOSS [training: 0.043528659142962826 | validation: 0.049822223433533055]
	TIME [epoch: 8.18 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06072878614811171		[learning rate: 0.0012076]
		[batch 20/20] avg loss: 0.038031363413558346		[learning rate: 0.0012054]
	Learning Rate: 0.0012054
	LOSS [training: 0.04938007478083502 | validation: 0.08303760580822662]
	TIME [epoch: 8.18 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05014400611694998		[learning rate: 0.0012032]
		[batch 20/20] avg loss: 0.06709585666026911		[learning rate: 0.001201]
	Learning Rate: 0.00120103
	LOSS [training: 0.058619931388609534 | validation: 0.05848842295863461]
	TIME [epoch: 8.19 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05117332100716602		[learning rate: 0.0011988]
		[batch 20/20] avg loss: 0.08487473359423896		[learning rate: 0.0011967]
	Learning Rate: 0.00119667
	LOSS [training: 0.06802402730070248 | validation: 0.034807749248541346]
	TIME [epoch: 8.19 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05216748169164874		[learning rate: 0.0011945]
		[batch 20/20] avg loss: 0.07362094140979303		[learning rate: 0.0011923]
	Learning Rate: 0.00119233
	LOSS [training: 0.06289421155072089 | validation: 0.15477948191859808]
	TIME [epoch: 8.2 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10013499731289237		[learning rate: 0.0011902]
		[batch 20/20] avg loss: 0.087570450855741		[learning rate: 0.001188]
	Learning Rate: 0.001188
	LOSS [training: 0.09385272408431666 | validation: 0.05944744834426313]
	TIME [epoch: 8.17 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06541103531325246		[learning rate: 0.0011858]
		[batch 20/20] avg loss: 0.06416240703205628		[learning rate: 0.0011837]
	Learning Rate: 0.00118369
	LOSS [training: 0.06478672117265435 | validation: 0.05098969767840968]
	TIME [epoch: 8.19 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08367562018650274		[learning rate: 0.0011815]
		[batch 20/20] avg loss: 0.09927853786123417		[learning rate: 0.0011794]
	Learning Rate: 0.00117939
	LOSS [training: 0.09147707902386844 | validation: 0.13684899885617846]
	TIME [epoch: 8.19 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09453440243082492		[learning rate: 0.0011772]
		[batch 20/20] avg loss: 0.06270783242937045		[learning rate: 0.0011751]
	Learning Rate: 0.00117511
	LOSS [training: 0.07862111743009767 | validation: 0.06347582434205268]
	TIME [epoch: 8.19 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07814935967161041		[learning rate: 0.001173]
		[batch 20/20] avg loss: 0.05346696525019688		[learning rate: 0.0011708]
	Learning Rate: 0.00117085
	LOSS [training: 0.06580816246090365 | validation: 0.13848264987648518]
	TIME [epoch: 8.17 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08499789996597376		[learning rate: 0.0011687]
		[batch 20/20] avg loss: 0.052867581663455676		[learning rate: 0.0011666]
	Learning Rate: 0.0011666
	LOSS [training: 0.0689327408147147 | validation: 0.08551609504628778]
	TIME [epoch: 8.2 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07831020007043998		[learning rate: 0.0011645]
		[batch 20/20] avg loss: 0.044041106863513285		[learning rate: 0.0011624]
	Learning Rate: 0.00116236
	LOSS [training: 0.061175653466976634 | validation: 0.04422537304807456]
	TIME [epoch: 8.19 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05563827812777309		[learning rate: 0.0011603]
		[batch 20/20] avg loss: 0.05391379997309116		[learning rate: 0.0011581]
	Learning Rate: 0.00115815
	LOSS [training: 0.05477603905043212 | validation: 0.038044572145921385]
	TIME [epoch: 8.19 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06843307390283421		[learning rate: 0.001156]
		[batch 20/20] avg loss: 0.03707080029566555		[learning rate: 0.0011539]
	Learning Rate: 0.00115394
	LOSS [training: 0.05275193709924987 | validation: 0.043964360337174914]
	TIME [epoch: 8.18 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0829520426267409		[learning rate: 0.0011518]
		[batch 20/20] avg loss: 0.040068904956091105		[learning rate: 0.0011498]
	Learning Rate: 0.00114975
	LOSS [training: 0.061510473791416 | validation: 0.05586026482950586]
	TIME [epoch: 8.2 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06367413211364967		[learning rate: 0.0011477]
		[batch 20/20] avg loss: 0.05319348548481332		[learning rate: 0.0011456]
	Learning Rate: 0.00114558
	LOSS [training: 0.05843380879923151 | validation: 0.061871223901518835]
	TIME [epoch: 8.19 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044076692031182245		[learning rate: 0.0011435]
		[batch 20/20] avg loss: 0.06556159124933218		[learning rate: 0.0011414]
	Learning Rate: 0.00114142
	LOSS [training: 0.05481914164025721 | validation: 0.05568593463552672]
	TIME [epoch: 8.18 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04164331506234709		[learning rate: 0.0011394]
		[batch 20/20] avg loss: 0.055768682106322146		[learning rate: 0.0011373]
	Learning Rate: 0.00113728
	LOSS [training: 0.048705998584334616 | validation: 0.10817894586067314]
	TIME [epoch: 8.17 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050715367581746704		[learning rate: 0.0011352]
		[batch 20/20] avg loss: 0.04172885024080021		[learning rate: 0.0011332]
	Learning Rate: 0.00113316
	LOSS [training: 0.04622210891127346 | validation: 0.18175407578765265]
	TIME [epoch: 8.2 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06810401748455448		[learning rate: 0.0011311]
		[batch 20/20] avg loss: 0.048837895115216884		[learning rate: 0.001129]
	Learning Rate: 0.00112904
	LOSS [training: 0.05847095629988569 | validation: 0.026168918534890985]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_700.pth
	Model improved!!!
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04067620127067391		[learning rate: 0.001127]
		[batch 20/20] avg loss: 0.05325783369221164		[learning rate: 0.0011249]
	Learning Rate: 0.00112495
	LOSS [training: 0.04696701748144278 | validation: 0.05715128515380894]
	TIME [epoch: 8.19 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04149454887501024		[learning rate: 0.0011229]
		[batch 20/20] avg loss: 0.052857677823558824		[learning rate: 0.0011209]
	Learning Rate: 0.00112086
	LOSS [training: 0.04717611334928454 | validation: 0.042415279733811824]
	TIME [epoch: 8.18 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0337892703998679		[learning rate: 0.0011188]
		[batch 20/20] avg loss: 0.0788011677255885		[learning rate: 0.0011168]
	Learning Rate: 0.0011168
	LOSS [training: 0.056295219062728205 | validation: 0.08143524297365508]
	TIME [epoch: 8.19 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09734068679020527		[learning rate: 0.0011148]
		[batch 20/20] avg loss: 0.06521599127972781		[learning rate: 0.0011127]
	Learning Rate: 0.00111274
	LOSS [training: 0.08127833903496653 | validation: 0.020410924201582335]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_704.pth
	Model improved!!!
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06277338915713251		[learning rate: 0.0011107]
		[batch 20/20] avg loss: 0.04703051912104499		[learning rate: 0.0011087]
	Learning Rate: 0.0011087
	LOSS [training: 0.05490195413908875 | validation: 0.047236719115474224]
	TIME [epoch: 8.19 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05074312670046449		[learning rate: 0.0011067]
		[batch 20/20] avg loss: 0.06923159457808395		[learning rate: 0.0011047]
	Learning Rate: 0.00110468
	LOSS [training: 0.05998736063927422 | validation: 0.09296631402237468]
	TIME [epoch: 8.18 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04528130948392785		[learning rate: 0.0011027]
		[batch 20/20] avg loss: 0.0678480547495914		[learning rate: 0.0011007]
	Learning Rate: 0.00110067
	LOSS [training: 0.05656468211675962 | validation: 0.07724297679847633]
	TIME [epoch: 8.21 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06633397023136585		[learning rate: 0.0010987]
		[batch 20/20] avg loss: 0.03128885604434217		[learning rate: 0.0010967]
	Learning Rate: 0.00109668
	LOSS [training: 0.04881141313785401 | validation: 0.05828524884309251]
	TIME [epoch: 8.22 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060650914873870664		[learning rate: 0.0010947]
		[batch 20/20] avg loss: 0.04443456961051116		[learning rate: 0.0010927]
	Learning Rate: 0.0010927
	LOSS [training: 0.05254274224219091 | validation: 0.059674843201953126]
	TIME [epoch: 8.17 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06323080240204379		[learning rate: 0.0010907]
		[batch 20/20] avg loss: 0.06086643676025153		[learning rate: 0.0010887]
	Learning Rate: 0.00108873
	LOSS [training: 0.06204861958114766 | validation: 0.09160970182057823]
	TIME [epoch: 8.17 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05119369556461588		[learning rate: 0.0010868]
		[batch 20/20] avg loss: 0.06678474448778328		[learning rate: 0.0010848]
	Learning Rate: 0.00108478
	LOSS [training: 0.058989220026199586 | validation: 0.03976703931966028]
	TIME [epoch: 8.21 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.064814023477187		[learning rate: 0.0010828]
		[batch 20/20] avg loss: 0.0775266564888925		[learning rate: 0.0010808]
	Learning Rate: 0.00108084
	LOSS [training: 0.07117033998303976 | validation: 0.04161638761507786]
	TIME [epoch: 8.21 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043293086799184075		[learning rate: 0.0010789]
		[batch 20/20] avg loss: 0.05743416042523759		[learning rate: 0.0010769]
	Learning Rate: 0.00107692
	LOSS [training: 0.05036362361221084 | validation: 0.053856297079037614]
	TIME [epoch: 8.19 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04064800708685744		[learning rate: 0.001075]
		[batch 20/20] avg loss: 0.08910358963155887		[learning rate: 0.001073]
	Learning Rate: 0.00107301
	LOSS [training: 0.06487579835920816 | validation: 0.05746265267451425]
	TIME [epoch: 8.18 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03063697597495884		[learning rate: 0.0010711]
		[batch 20/20] avg loss: 0.08464231717939948		[learning rate: 0.0010691]
	Learning Rate: 0.00106912
	LOSS [training: 0.05763964657717916 | validation: 0.11811105697295571]
	TIME [epoch: 8.21 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05538253408624838		[learning rate: 0.0010672]
		[batch 20/20] avg loss: 0.06681914552530943		[learning rate: 0.0010652]
	Learning Rate: 0.00106524
	LOSS [training: 0.06110083980577892 | validation: 0.033858467145109664]
	TIME [epoch: 8.22 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03485577777145592		[learning rate: 0.0010633]
		[batch 20/20] avg loss: 0.08683521385153535		[learning rate: 0.0010614]
	Learning Rate: 0.00106137
	LOSS [training: 0.06084549581149562 | validation: 0.04146457563966564]
	TIME [epoch: 8.18 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03839017239168457		[learning rate: 0.0010594]
		[batch 20/20] avg loss: 0.05514293790199887		[learning rate: 0.0010575]
	Learning Rate: 0.00105752
	LOSS [training: 0.04676655514684173 | validation: 0.03047653009649569]
	TIME [epoch: 8.18 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04864085747864792		[learning rate: 0.0010556]
		[batch 20/20] avg loss: 0.044591632032967676		[learning rate: 0.0010537]
	Learning Rate: 0.00105368
	LOSS [training: 0.0466162447558078 | validation: 0.03005214605207957]
	TIME [epoch: 8.2 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0442096867282891		[learning rate: 0.0010518]
		[batch 20/20] avg loss: 0.07376208804415238		[learning rate: 0.0010499]
	Learning Rate: 0.00104986
	LOSS [training: 0.05898588738622074 | validation: 0.04999964291550196]
	TIME [epoch: 8.22 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04577175943084562		[learning rate: 0.001048]
		[batch 20/20] avg loss: 0.03907015639946777		[learning rate: 0.0010461]
	Learning Rate: 0.00104605
	LOSS [training: 0.04242095791515669 | validation: 0.0566760331728344]
	TIME [epoch: 8.19 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05873249119484778		[learning rate: 0.0010442]
		[batch 20/20] avg loss: 0.044106024337341544		[learning rate: 0.0010423]
	Learning Rate: 0.00104225
	LOSS [training: 0.05141925776609466 | validation: 0.03741622955800982]
	TIME [epoch: 8.18 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05609776397363174		[learning rate: 0.0010404]
		[batch 20/20] avg loss: 0.06446841292873037		[learning rate: 0.0010385]
	Learning Rate: 0.00103847
	LOSS [training: 0.06028308845118104 | validation: 0.025064702370937553]
	TIME [epoch: 8.21 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03825934170200097		[learning rate: 0.0010366]
		[batch 20/20] avg loss: 0.05664445604934515		[learning rate: 0.0010347]
	Learning Rate: 0.0010347
	LOSS [training: 0.04745189887567306 | validation: 0.04527496474824728]
	TIME [epoch: 8.23 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045880260767203106		[learning rate: 0.0010328]
		[batch 20/20] avg loss: 0.08283813898800996		[learning rate: 0.0010309]
	Learning Rate: 0.00103095
	LOSS [training: 0.06435919987760652 | validation: 0.07906205670188571]
	TIME [epoch: 8.18 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052764886993998686		[learning rate: 0.0010291]
		[batch 20/20] avg loss: 0.053411892481147805		[learning rate: 0.0010272]
	Learning Rate: 0.00102721
	LOSS [training: 0.05308838973757324 | validation: 0.05278359217200514]
	TIME [epoch: 8.18 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04020146357739276		[learning rate: 0.0010253]
		[batch 20/20] avg loss: 0.05715219955239703		[learning rate: 0.0010235]
	Learning Rate: 0.00102348
	LOSS [training: 0.0486768315648949 | validation: 0.06006733190129008]
	TIME [epoch: 8.2 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047219421142475626		[learning rate: 0.0010216]
		[batch 20/20] avg loss: 0.043481836742182575		[learning rate: 0.0010198]
	Learning Rate: 0.00101976
	LOSS [training: 0.04535062894232911 | validation: 0.06079978409324344]
	TIME [epoch: 8.22 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07661888544972743		[learning rate: 0.0010179]
		[batch 20/20] avg loss: 0.06296652854340051		[learning rate: 0.0010161]
	Learning Rate: 0.00101606
	LOSS [training: 0.06979270699656397 | validation: 0.030677758395247064]
	TIME [epoch: 8.18 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04804835962923046		[learning rate: 0.0010142]
		[batch 20/20] avg loss: 0.06679552516819258		[learning rate: 0.0010124]
	Learning Rate: 0.00101238
	LOSS [training: 0.057421942398711526 | validation: 0.054740120246386484]
	TIME [epoch: 8.18 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06638266724040062		[learning rate: 0.0010105]
		[batch 20/20] avg loss: 0.04746303137328232		[learning rate: 0.0010087]
	Learning Rate: 0.0010087
	LOSS [training: 0.05692284930684147 | validation: 0.03267898469371951]
	TIME [epoch: 8.21 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03026541696948091		[learning rate: 0.0010069]
		[batch 20/20] avg loss: 0.10420455155759675		[learning rate: 0.001005]
	Learning Rate: 0.00100504
	LOSS [training: 0.06723498426353883 | validation: 0.03390810808117935]
	TIME [epoch: 8.22 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056308377214319805		[learning rate: 0.0010032]
		[batch 20/20] avg loss: 0.0577602909149854		[learning rate: 0.0010014]
	Learning Rate: 0.00100139
	LOSS [training: 0.057034334064652595 | validation: 0.06017200419334558]
	TIME [epoch: 8.18 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06541166679426992		[learning rate: 0.00099958]
		[batch 20/20] avg loss: 0.0436001208538675		[learning rate: 0.00099776]
	Learning Rate: 0.00099776
	LOSS [training: 0.05450589382406871 | validation: 0.06665049170105158]
	TIME [epoch: 8.17 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07111543408802232		[learning rate: 0.00099595]
		[batch 20/20] avg loss: 0.05544750100825857		[learning rate: 0.00099414]
	Learning Rate: 0.00099414
	LOSS [training: 0.06328146754814043 | validation: 0.10899120375437824]
	TIME [epoch: 8.22 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051964390121789404		[learning rate: 0.00099233]
		[batch 20/20] avg loss: 0.028460714853621127		[learning rate: 0.00099053]
	Learning Rate: 0.000990532
	LOSS [training: 0.040212552487705264 | validation: 0.03482696274808003]
	TIME [epoch: 8.22 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03595612943392227		[learning rate: 0.00098873]
		[batch 20/20] avg loss: 0.08394695519471138		[learning rate: 0.00098694]
	Learning Rate: 0.000986937
	LOSS [training: 0.059951542314316805 | validation: 0.03938793703112835]
	TIME [epoch: 8.19 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06352968714944227		[learning rate: 0.00098514]
		[batch 20/20] avg loss: 0.040921621771794384		[learning rate: 0.00098336]
	Learning Rate: 0.000983355
	LOSS [training: 0.05222565446061833 | validation: 0.04052956173129976]
	TIME [epoch: 8.18 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06048440245666533		[learning rate: 0.00098157]
		[batch 20/20] avg loss: 0.06396506389758208		[learning rate: 0.00097979]
	Learning Rate: 0.000979787
	LOSS [training: 0.062224733177123695 | validation: 0.05793893983474155]
	TIME [epoch: 8.23 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03933777236461307		[learning rate: 0.00097801]
		[batch 20/20] avg loss: 0.05548872383446225		[learning rate: 0.00097623]
	Learning Rate: 0.000976231
	LOSS [training: 0.04741324809953766 | validation: 0.02639652784784367]
	TIME [epoch: 8.21 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03881923303532295		[learning rate: 0.00097446]
		[batch 20/20] avg loss: 0.04728891398451939		[learning rate: 0.00097269]
	Learning Rate: 0.000972688
	LOSS [training: 0.043054073509921166 | validation: 0.03219431607989102]
	TIME [epoch: 8.18 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053316426935059225		[learning rate: 0.00097092]
		[batch 20/20] avg loss: 0.07385354603145328		[learning rate: 0.00096916]
	Learning Rate: 0.000969158
	LOSS [training: 0.06358498648325625 | validation: 0.03638279616266536]
	TIME [epoch: 8.18 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04212214654839134		[learning rate: 0.0009674]
		[batch 20/20] avg loss: 0.026684597462531066		[learning rate: 0.00096564]
	Learning Rate: 0.000965641
	LOSS [training: 0.03440337200546121 | validation: 0.03263298087256976]
	TIME [epoch: 8.22 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030147105562240824		[learning rate: 0.00096389]
		[batch 20/20] avg loss: 0.041114741495281215		[learning rate: 0.00096214]
	Learning Rate: 0.000962137
	LOSS [training: 0.03563092352876102 | validation: 0.035631875611247935]
	TIME [epoch: 8.21 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044510597499864316		[learning rate: 0.00096039]
		[batch 20/20] avg loss: 0.03302326267912668		[learning rate: 0.00095865]
	Learning Rate: 0.000958645
	LOSS [training: 0.0387669300894955 | validation: 0.049418247623773316]
	TIME [epoch: 8.17 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04302366791664579		[learning rate: 0.0009569]
		[batch 20/20] avg loss: 0.04607559639838607		[learning rate: 0.00095517]
	Learning Rate: 0.000955166
	LOSS [training: 0.044549632157515937 | validation: 0.0789940604837879]
	TIME [epoch: 8.18 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04953431689030573		[learning rate: 0.00095343]
		[batch 20/20] avg loss: 0.032253898812745374		[learning rate: 0.0009517]
	Learning Rate: 0.0009517
	LOSS [training: 0.04089410785152555 | validation: 0.03714739678715467]
	TIME [epoch: 8.19 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040913392059042146		[learning rate: 0.00094997]
		[batch 20/20] avg loss: 0.04018242437655192		[learning rate: 0.00094825]
	Learning Rate: 0.000948246
	LOSS [training: 0.04054790821779704 | validation: 0.05395297263910502]
	TIME [epoch: 8.23 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050630899698087		[learning rate: 0.00094652]
		[batch 20/20] avg loss: 0.047846724153226855		[learning rate: 0.0009448]
	Learning Rate: 0.000944805
	LOSS [training: 0.04923881192565692 | validation: 0.01753370601644197]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_749.pth
	Model improved!!!
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032812098138101764		[learning rate: 0.00094309]
		[batch 20/20] avg loss: 0.03715586421065009		[learning rate: 0.00094138]
	Learning Rate: 0.000941376
	LOSS [training: 0.03498398117437592 | validation: 0.07781137270366156]
	TIME [epoch: 8.2 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059045933795065286		[learning rate: 0.00093967]
		[batch 20/20] avg loss: 0.043581234868638866		[learning rate: 0.00093796]
	Learning Rate: 0.00093796
	LOSS [training: 0.05131358433185208 | validation: 0.04523712115402246]
	TIME [epoch: 8.25 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03876419144622341		[learning rate: 0.00093626]
		[batch 20/20] avg loss: 0.05971795713538691		[learning rate: 0.00093456]
	Learning Rate: 0.000934556
	LOSS [training: 0.04924107429080516 | validation: 0.04848747302603721]
	TIME [epoch: 8.23 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04237416414579057		[learning rate: 0.00093286]
		[batch 20/20] avg loss: 0.055146137789820096		[learning rate: 0.00093116]
	Learning Rate: 0.000931164
	LOSS [training: 0.04876015096780533 | validation: 0.06093422095418895]
	TIME [epoch: 8.21 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04609750006313016		[learning rate: 0.00092947]
		[batch 20/20] avg loss: 0.041667506135182275		[learning rate: 0.00092779]
	Learning Rate: 0.000927785
	LOSS [training: 0.043882503099156214 | validation: 0.06357270549590693]
	TIME [epoch: 8.19 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04231979257165826		[learning rate: 0.0009261]
		[batch 20/20] avg loss: 0.05746177591062625		[learning rate: 0.00092442]
	Learning Rate: 0.000924418
	LOSS [training: 0.04989078424114225 | validation: 0.0482537848519172]
	TIME [epoch: 8.23 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03963503557906475		[learning rate: 0.00092274]
		[batch 20/20] avg loss: 0.0615794132595465		[learning rate: 0.00092106]
	Learning Rate: 0.000921063
	LOSS [training: 0.05060722441930562 | validation: 0.04400702912950527]
	TIME [epoch: 8.23 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06292417544464128		[learning rate: 0.00091939]
		[batch 20/20] avg loss: 0.04110354247270255		[learning rate: 0.00091772]
	Learning Rate: 0.000917721
	LOSS [training: 0.05201385895867191 | validation: 0.06503791975114491]
	TIME [epoch: 8.19 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05302597815282323		[learning rate: 0.00091605]
		[batch 20/20] avg loss: 0.029854658959485154		[learning rate: 0.00091439]
	Learning Rate: 0.00091439
	LOSS [training: 0.041440318556154196 | validation: 0.08762402952440623]
	TIME [epoch: 8.19 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048714083331946455		[learning rate: 0.00091273]
		[batch 20/20] avg loss: 0.02664282371946315		[learning rate: 0.00091107]
	Learning Rate: 0.000911072
	LOSS [training: 0.03767845352570479 | validation: 0.036677770893907544]
	TIME [epoch: 8.21 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06232971070262331		[learning rate: 0.00090942]
		[batch 20/20] avg loss: 0.043178414742510766		[learning rate: 0.00090777]
	Learning Rate: 0.000907766
	LOSS [training: 0.05275406272256704 | validation: 0.04375314752416572]
	TIME [epoch: 8.24 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07024264778721467		[learning rate: 0.00090612]
		[batch 20/20] avg loss: 0.047092078492755665		[learning rate: 0.00090447]
	Learning Rate: 0.000904471
	LOSS [training: 0.058667363139985176 | validation: 0.033487735384619674]
	TIME [epoch: 8.18 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040214177823210344		[learning rate: 0.00090283]
		[batch 20/20] avg loss: 0.03486063086508963		[learning rate: 0.00090119]
	Learning Rate: 0.000901189
	LOSS [training: 0.03753740434414998 | validation: 0.04509320405449374]
	TIME [epoch: 8.18 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03295813370543017		[learning rate: 0.00089955]
		[batch 20/20] avg loss: 0.036145012639395124		[learning rate: 0.00089792]
	Learning Rate: 0.000897918
	LOSS [training: 0.034551573172412656 | validation: 0.03151071653767483]
	TIME [epoch: 8.22 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03189971096692432		[learning rate: 0.00089629]
		[batch 20/20] avg loss: 0.030152942418060387		[learning rate: 0.00089466]
	Learning Rate: 0.00089466
	LOSS [training: 0.03102632669249235 | validation: 0.032328080290357344]
	TIME [epoch: 8.23 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029591792283022444		[learning rate: 0.00089303]
		[batch 20/20] avg loss: 0.07158374227394002		[learning rate: 0.00089141]
	Learning Rate: 0.000891413
	LOSS [training: 0.05058776727848123 | validation: 0.049714238572044836]
	TIME [epoch: 8.18 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05044864437450425		[learning rate: 0.00088979]
		[batch 20/20] avg loss: 0.04266325406626109		[learning rate: 0.00088818]
	Learning Rate: 0.000888178
	LOSS [training: 0.04655594922038268 | validation: 0.045954022208498736]
	TIME [epoch: 8.19 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050352862786081955		[learning rate: 0.00088656]
		[batch 20/20] avg loss: 0.04455377870008985		[learning rate: 0.00088495]
	Learning Rate: 0.000884955
	LOSS [training: 0.04745332074308589 | validation: 0.04206257580021695]
	TIME [epoch: 8.22 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051732683742408005		[learning rate: 0.00088335]
		[batch 20/20] avg loss: 0.027118076884478537		[learning rate: 0.00088174]
	Learning Rate: 0.000881743
	LOSS [training: 0.03942538031344327 | validation: 0.05243490002361227]
	TIME [epoch: 8.24 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04782055580083223		[learning rate: 0.00088014]
		[batch 20/20] avg loss: 0.04056307978921055		[learning rate: 0.00087854]
	Learning Rate: 0.000878543
	LOSS [training: 0.04419181779502139 | validation: 0.034858562178939775]
	TIME [epoch: 8.18 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02662877153328534		[learning rate: 0.00087695]
		[batch 20/20] avg loss: 0.03089429727919644		[learning rate: 0.00087536]
	Learning Rate: 0.000875355
	LOSS [training: 0.02876153440624089 | validation: 0.049264450406513866]
	TIME [epoch: 8.19 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045582473188089265		[learning rate: 0.00087377]
		[batch 20/20] avg loss: 0.052391388325374856		[learning rate: 0.00087218]
	Learning Rate: 0.000872178
	LOSS [training: 0.048986930756732064 | validation: 0.025658004091493724]
	TIME [epoch: 8.23 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03574993485539679		[learning rate: 0.00087059]
		[batch 20/20] avg loss: 0.0329147308142121		[learning rate: 0.00086901]
	Learning Rate: 0.000869013
	LOSS [training: 0.034332332834804455 | validation: 0.06533730549136145]
	TIME [epoch: 8.23 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03908139214810645		[learning rate: 0.00086743]
		[batch 20/20] avg loss: 0.03527609998121345		[learning rate: 0.00086586]
	Learning Rate: 0.000865859
	LOSS [training: 0.03717874606465994 | validation: 0.057226216832843776]
	TIME [epoch: 8.2 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0439161904040571		[learning rate: 0.00086429]
		[batch 20/20] avg loss: 0.039920640612596266		[learning rate: 0.00086272]
	Learning Rate: 0.000862717
	LOSS [training: 0.04191841550832668 | validation: 0.03863250254391013]
	TIME [epoch: 8.19 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04703977661854057		[learning rate: 0.00086115]
		[batch 20/20] avg loss: 0.047205147792540765		[learning rate: 0.00085959]
	Learning Rate: 0.000859586
	LOSS [training: 0.04712246220554066 | validation: 0.05677621457014183]
	TIME [epoch: 8.23 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04130705668017406		[learning rate: 0.00085803]
		[batch 20/20] avg loss: 0.03769327369831405		[learning rate: 0.00085647]
	Learning Rate: 0.000856467
	LOSS [training: 0.039500165189244055 | validation: 0.08066441281891093]
	TIME [epoch: 8.23 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04058605152620668		[learning rate: 0.00085491]
		[batch 20/20] avg loss: 0.04097442399422034		[learning rate: 0.00085336]
	Learning Rate: 0.000853359
	LOSS [training: 0.040780237760213514 | validation: 0.03456624712375873]
	TIME [epoch: 8.18 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04121655111341402		[learning rate: 0.00085181]
		[batch 20/20] avg loss: 0.03414707381596045		[learning rate: 0.00085026]
	Learning Rate: 0.000850262
	LOSS [training: 0.037681812464687235 | validation: 0.07136334521118474]
	TIME [epoch: 8.18 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03423029182803404		[learning rate: 0.00084872]
		[batch 20/20] avg loss: 0.033118558896974425		[learning rate: 0.00084718]
	Learning Rate: 0.000847176
	LOSS [training: 0.03367442536250424 | validation: 0.03813337973973786]
	TIME [epoch: 8.22 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04797225233944246		[learning rate: 0.00084564]
		[batch 20/20] avg loss: 0.04108119088235662		[learning rate: 0.0008441]
	Learning Rate: 0.000844102
	LOSS [training: 0.04452672161089953 | validation: 0.07997802155225753]
	TIME [epoch: 8.24 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034587351673763686		[learning rate: 0.00084257]
		[batch 20/20] avg loss: 0.03602863402523426		[learning rate: 0.00084104]
	Learning Rate: 0.000841038
	LOSS [training: 0.03530799284949897 | validation: 0.07040554015217423]
	TIME [epoch: 8.18 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0410434995406725		[learning rate: 0.00083951]
		[batch 20/20] avg loss: 0.03476773222860537		[learning rate: 0.00083799]
	Learning Rate: 0.000837986
	LOSS [training: 0.037905615884638935 | validation: 0.04911060757063129]
	TIME [epoch: 8.19 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035665454329928646		[learning rate: 0.00083646]
		[batch 20/20] avg loss: 0.045491438840524825		[learning rate: 0.00083495]
	Learning Rate: 0.000834945
	LOSS [training: 0.04057844658522674 | validation: 0.06080983673695694]
	TIME [epoch: 8.21 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03911588639815512		[learning rate: 0.00083343]
		[batch 20/20] avg loss: 0.03544482826173195		[learning rate: 0.00083192]
	Learning Rate: 0.000831915
	LOSS [training: 0.03728035732994354 | validation: 0.03856133345442637]
	TIME [epoch: 8.25 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04261184799021476		[learning rate: 0.0008304]
		[batch 20/20] avg loss: 0.043803106501987765		[learning rate: 0.0008289]
	Learning Rate: 0.000828896
	LOSS [training: 0.04320747724610126 | validation: 0.029391029277347883]
	TIME [epoch: 8.18 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03884668030537837		[learning rate: 0.00082739]
		[batch 20/20] avg loss: 0.04037478560431771		[learning rate: 0.00082589]
	Learning Rate: 0.000825888
	LOSS [training: 0.03961073295484804 | validation: 0.09007888524102389]
	TIME [epoch: 8.19 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0375201874002893		[learning rate: 0.00082439]
		[batch 20/20] avg loss: 0.038814097333019125		[learning rate: 0.00082289]
	Learning Rate: 0.000822891
	LOSS [training: 0.03816714236665421 | validation: 0.02269409999503771]
	TIME [epoch: 8.19 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04278989803976669		[learning rate: 0.0008214]
		[batch 20/20] avg loss: 0.03214090266178924		[learning rate: 0.0008199]
	Learning Rate: 0.000819904
	LOSS [training: 0.03746540035077796 | validation: 0.0329639166332407]
	TIME [epoch: 8.25 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045521907400416825		[learning rate: 0.00081842]
		[batch 20/20] avg loss: 0.02662686829493207		[learning rate: 0.00081693]
	Learning Rate: 0.000816929
	LOSS [training: 0.03607438784767445 | validation: 0.022396048495841238]
	TIME [epoch: 8.18 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041602738627898164		[learning rate: 0.00081545]
		[batch 20/20] avg loss: 0.03617434408593834		[learning rate: 0.00081396]
	Learning Rate: 0.000813964
	LOSS [training: 0.038888541356918244 | validation: 0.03214986682930173]
	TIME [epoch: 8.19 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03274453638180244		[learning rate: 0.00081249]
		[batch 20/20] avg loss: 0.034354872935657275		[learning rate: 0.00081101]
	Learning Rate: 0.00081101
	LOSS [training: 0.03354970465872985 | validation: 0.03408531987751132]
	TIME [epoch: 8.19 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0462772802106274		[learning rate: 0.00080954]
		[batch 20/20] avg loss: 0.03872906236675995		[learning rate: 0.00080807]
	Learning Rate: 0.000808067
	LOSS [training: 0.04250317128869368 | validation: 0.022539883906941807]
	TIME [epoch: 8.26 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024645920250696625		[learning rate: 0.0008066]
		[batch 20/20] avg loss: 0.04991033952460129		[learning rate: 0.00080513]
	Learning Rate: 0.000805135
	LOSS [training: 0.03727812988764896 | validation: 0.056978518912348536]
	TIME [epoch: 8.18 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05272779534630121		[learning rate: 0.00080367]
		[batch 20/20] avg loss: 0.026060478790242196		[learning rate: 0.00080221]
	Learning Rate: 0.000802213
	LOSS [training: 0.039394137068271703 | validation: 0.030552085714574136]
	TIME [epoch: 8.18 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057350891745396725		[learning rate: 0.00080076]
		[batch 20/20] avg loss: 0.026336588732670602		[learning rate: 0.0007993]
	Learning Rate: 0.000799301
	LOSS [training: 0.04184374023903366 | validation: 0.044138726756337836]
	TIME [epoch: 8.19 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049770239597242534		[learning rate: 0.00079785]
		[batch 20/20] avg loss: 0.03037010921649707		[learning rate: 0.0007964]
	Learning Rate: 0.000796401
	LOSS [training: 0.0400701744068698 | validation: 0.023223534331270092]
	TIME [epoch: 8.25 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039689275607280694		[learning rate: 0.00079495]
		[batch 20/20] avg loss: 0.03184170094877234		[learning rate: 0.00079351]
	Learning Rate: 0.000793511
	LOSS [training: 0.03576548827802652 | validation: 0.025535984394684325]
	TIME [epoch: 8.18 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030593132198927665		[learning rate: 0.00079207]
		[batch 20/20] avg loss: 0.035998920770798144		[learning rate: 0.00079063]
	Learning Rate: 0.000790631
	LOSS [training: 0.033296026484862906 | validation: 0.020755563816587108]
	TIME [epoch: 8.18 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03021718586985191		[learning rate: 0.00078919]
		[batch 20/20] avg loss: 0.030382018374527954		[learning rate: 0.00078776]
	Learning Rate: 0.000787761
	LOSS [training: 0.030299602122189927 | validation: 0.06271076954865443]
	TIME [epoch: 8.2 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03996338754277362		[learning rate: 0.00078633]
		[batch 20/20] avg loss: 0.034203590497570394		[learning rate: 0.0007849]
	Learning Rate: 0.000784903
	LOSS [training: 0.037083489020172 | validation: 0.021555835631613854]
	TIME [epoch: 8.24 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030400651419654208		[learning rate: 0.00078348]
		[batch 20/20] avg loss: 0.04196716232828504		[learning rate: 0.00078205]
	Learning Rate: 0.000782054
	LOSS [training: 0.03618390687396962 | validation: 0.046783856994158504]
	TIME [epoch: 8.18 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032603710048338175		[learning rate: 0.00078063]
		[batch 20/20] avg loss: 0.03471077283807452		[learning rate: 0.00077922]
	Learning Rate: 0.000779216
	LOSS [training: 0.03365724144320635 | validation: 0.020172851917625273]
	TIME [epoch: 8.18 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03416817625606221		[learning rate: 0.0007778]
		[batch 20/20] avg loss: 0.028196672207338604		[learning rate: 0.00077639]
	Learning Rate: 0.000776388
	LOSS [training: 0.031182424231700406 | validation: 0.022722388986690482]
	TIME [epoch: 8.2 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047719116921702956		[learning rate: 0.00077498]
		[batch 20/20] avg loss: 0.028806836729733283		[learning rate: 0.00077357]
	Learning Rate: 0.000773571
	LOSS [training: 0.03826297682571812 | validation: 0.02326796632496791]
	TIME [epoch: 8.24 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03888021023076656		[learning rate: 0.00077217]
		[batch 20/20] avg loss: 0.03916775341841079		[learning rate: 0.00077076]
	Learning Rate: 0.000770763
	LOSS [training: 0.039023981824588674 | validation: 0.041886939122344584]
	TIME [epoch: 8.18 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024916381786437826		[learning rate: 0.00076936]
		[batch 20/20] avg loss: 0.02578432617164033		[learning rate: 0.00076797]
	Learning Rate: 0.000767966
	LOSS [training: 0.025350353979039075 | validation: 0.05287000586503016]
	TIME [epoch: 8.18 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052014533031591405		[learning rate: 0.00076657]
		[batch 20/20] avg loss: 0.02833407188314795		[learning rate: 0.00076518]
	Learning Rate: 0.000765179
	LOSS [training: 0.04017430245736968 | validation: 0.0249565050494818]
	TIME [epoch: 8.2 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035350107078842744		[learning rate: 0.00076379]
		[batch 20/20] avg loss: 0.025852796749679535		[learning rate: 0.0007624]
	Learning Rate: 0.000762402
	LOSS [training: 0.03060145191426114 | validation: 0.055524660065569886]
	TIME [epoch: 8.22 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047629913628649996		[learning rate: 0.00076102]
		[batch 20/20] avg loss: 0.03424428512073009		[learning rate: 0.00075964]
	Learning Rate: 0.000759636
	LOSS [training: 0.04093709937469005 | validation: 0.036992953314358645]
	TIME [epoch: 8.18 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02901039241064455		[learning rate: 0.00075826]
		[batch 20/20] avg loss: 0.04127825418425922		[learning rate: 0.00075688]
	Learning Rate: 0.000756879
	LOSS [training: 0.03514432329745188 | validation: 0.02235147689745706]
	TIME [epoch: 8.17 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04301160453236643		[learning rate: 0.0007555]
		[batch 20/20] avg loss: 0.025152059122397003		[learning rate: 0.00075413]
	Learning Rate: 0.000754132
	LOSS [training: 0.03408183182738172 | validation: 0.024686994791544032]
	TIME [epoch: 8.21 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0396254546017419		[learning rate: 0.00075276]
		[batch 20/20] avg loss: 0.042918157333606585		[learning rate: 0.0007514]
	Learning Rate: 0.000751395
	LOSS [training: 0.041271805967674234 | validation: 0.03691228263219815]
	TIME [epoch: 8.23 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025734844192977613		[learning rate: 0.00075003]
		[batch 20/20] avg loss: 0.06320724344684353		[learning rate: 0.00074867]
	Learning Rate: 0.000748668
	LOSS [training: 0.044471043819910565 | validation: 0.02162935118820626]
	TIME [epoch: 8.18 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04666321618023349		[learning rate: 0.00074731]
		[batch 20/20] avg loss: 0.022487872387869177		[learning rate: 0.00074595]
	Learning Rate: 0.000745951
	LOSS [training: 0.03457554428405134 | validation: 0.057292041036770394]
	TIME [epoch: 8.17 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04741352895729022		[learning rate: 0.0007446]
		[batch 20/20] avg loss: 0.028838312214403528		[learning rate: 0.00074324]
	Learning Rate: 0.000743244
	LOSS [training: 0.038125920585846865 | validation: 0.04060218844411949]
	TIME [epoch: 8.2 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03371080654151563		[learning rate: 0.00074189]
		[batch 20/20] avg loss: 0.032165997916670755		[learning rate: 0.00074055]
	Learning Rate: 0.000740547
	LOSS [training: 0.03293840222909318 | validation: 0.0421997075614059]
	TIME [epoch: 8.24 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043574291030125556		[learning rate: 0.0007392]
		[batch 20/20] avg loss: 0.026941805874976653		[learning rate: 0.00073786]
	Learning Rate: 0.00073786
	LOSS [training: 0.03525804845255111 | validation: 0.030336880706854495]
	TIME [epoch: 8.19 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04477195335314464		[learning rate: 0.00073652]
		[batch 20/20] avg loss: 0.026316770511909948		[learning rate: 0.00073518]
	Learning Rate: 0.000735182
	LOSS [training: 0.035544361932527295 | validation: 0.035696289138441]
	TIME [epoch: 8.18 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03448269566749047		[learning rate: 0.00073385]
		[batch 20/20] avg loss: 0.031754261168240656		[learning rate: 0.00073251]
	Learning Rate: 0.000732514
	LOSS [training: 0.03311847841786557 | validation: 0.03434859486181505]
	TIME [epoch: 8.2 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04666495194692084		[learning rate: 0.00073118]
		[batch 20/20] avg loss: 0.02506583414157647		[learning rate: 0.00072986]
	Learning Rate: 0.000729855
	LOSS [training: 0.03586539304424865 | validation: 0.04316148566913624]
	TIME [epoch: 8.25 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04179698845918828		[learning rate: 0.00072853]
		[batch 20/20] avg loss: 0.035039210003207956		[learning rate: 0.00072721]
	Learning Rate: 0.000727207
	LOSS [training: 0.03841809923119811 | validation: 0.028935164981394826]
	TIME [epoch: 8.19 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03340238660065826		[learning rate: 0.00072589]
		[batch 20/20] avg loss: 0.041421642714491905		[learning rate: 0.00072457]
	Learning Rate: 0.000724568
	LOSS [training: 0.03741201465757508 | validation: 0.023129817802370535]
	TIME [epoch: 8.18 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044796637448281995		[learning rate: 0.00072325]
		[batch 20/20] avg loss: 0.043967950753800236		[learning rate: 0.00072194]
	Learning Rate: 0.000721938
	LOSS [training: 0.04438229410104112 | validation: 0.04010537346369816]
	TIME [epoch: 8.21 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04189627182158887		[learning rate: 0.00072063]
		[batch 20/20] avg loss: 0.030135445012334765		[learning rate: 0.00071932]
	Learning Rate: 0.000719318
	LOSS [training: 0.03601585841696182 | validation: 0.06608422055058828]
	TIME [epoch: 8.24 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03601890739633388		[learning rate: 0.00071801]
		[batch 20/20] avg loss: 0.034244683783257124		[learning rate: 0.00071671]
	Learning Rate: 0.000716708
	LOSS [training: 0.0351317955897955 | validation: 0.03900379464280344]
	TIME [epoch: 8.19 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039264673413234516		[learning rate: 0.00071541]
		[batch 20/20] avg loss: 0.024749634479693687		[learning rate: 0.00071411]
	Learning Rate: 0.000714107
	LOSS [training: 0.0320071539464641 | validation: 0.038579433435460456]
	TIME [epoch: 8.18 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034947691529575484		[learning rate: 0.00071281]
		[batch 20/20] avg loss: 0.03282097462825154		[learning rate: 0.00071152]
	Learning Rate: 0.000711515
	LOSS [training: 0.03388433307891352 | validation: 0.018493526840195028]
	TIME [epoch: 8.21 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031673042168981105		[learning rate: 0.00071022]
		[batch 20/20] avg loss: 0.03625248055120558		[learning rate: 0.00070893]
	Learning Rate: 0.000708933
	LOSS [training: 0.03396276136009333 | validation: 0.03796691539105551]
	TIME [epoch: 8.24 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03755309224206459		[learning rate: 0.00070765]
		[batch 20/20] avg loss: 0.027454879134188655		[learning rate: 0.00070636]
	Learning Rate: 0.00070636
	LOSS [training: 0.03250398568812663 | validation: 0.04263920521842859]
	TIME [epoch: 8.19 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043448518728600956		[learning rate: 0.00070508]
		[batch 20/20] avg loss: 0.04083123303826515		[learning rate: 0.0007038]
	Learning Rate: 0.000703797
	LOSS [training: 0.042139875883433056 | validation: 0.049631660249325193]
	TIME [epoch: 8.18 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03900857567934243		[learning rate: 0.00070252]
		[batch 20/20] avg loss: 0.0574833718827471		[learning rate: 0.00070124]
	Learning Rate: 0.000701243
	LOSS [training: 0.04824597378104476 | validation: 0.047189621834540264]
	TIME [epoch: 8.21 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02372978908977641		[learning rate: 0.00069997]
		[batch 20/20] avg loss: 0.021120011003175156		[learning rate: 0.0006987]
	Learning Rate: 0.000698698
	LOSS [training: 0.022424900046475783 | validation: 0.03744822669082958]
	TIME [epoch: 8.23 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027374555530140197		[learning rate: 0.00069743]
		[batch 20/20] avg loss: 0.02231037418030094		[learning rate: 0.00069616]
	Learning Rate: 0.000696162
	LOSS [training: 0.02484246485522057 | validation: 0.028326277492934735]
	TIME [epoch: 8.19 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02705632265728471		[learning rate: 0.0006949]
		[batch 20/20] avg loss: 0.032273416188915896		[learning rate: 0.00069364]
	Learning Rate: 0.000693636
	LOSS [training: 0.029664869423100298 | validation: 0.06616312835540493]
	TIME [epoch: 8.18 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044695385987267376		[learning rate: 0.00069238]
		[batch 20/20] avg loss: 0.03411486822451651		[learning rate: 0.00069112]
	Learning Rate: 0.000691119
	LOSS [training: 0.039405127105891946 | validation: 0.03275534057843922]
	TIME [epoch: 8.2 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027224616657158102		[learning rate: 0.00068986]
		[batch 20/20] avg loss: 0.024003891462878468		[learning rate: 0.00068861]
	Learning Rate: 0.000688611
	LOSS [training: 0.02561425406001828 | validation: 0.029123622296883325]
	TIME [epoch: 8.23 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040676088880836334		[learning rate: 0.00068736]
		[batch 20/20] avg loss: 0.02699671407952219		[learning rate: 0.00068611]
	Learning Rate: 0.000686112
	LOSS [training: 0.03383640148017926 | validation: 0.01708109937154558]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_837.pth
	Model improved!!!
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02183513303651334		[learning rate: 0.00068487]
		[batch 20/20] avg loss: 0.02478460816592119		[learning rate: 0.00068362]
	Learning Rate: 0.000683622
	LOSS [training: 0.023309870601217263 | validation: 0.02425373329232723]
	TIME [epoch: 8.18 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029795591121906224		[learning rate: 0.00068238]
		[batch 20/20] avg loss: 0.02873653283196213		[learning rate: 0.00068114]
	Learning Rate: 0.000681141
	LOSS [training: 0.029266061976934175 | validation: 0.029556963419088353]
	TIME [epoch: 8.21 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04366924256361214		[learning rate: 0.0006799]
		[batch 20/20] avg loss: 0.035713149164894334		[learning rate: 0.00067867]
	Learning Rate: 0.000678669
	LOSS [training: 0.03969119586425323 | validation: 0.0234518998894912]
	TIME [epoch: 8.21 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03369110844264176		[learning rate: 0.00067744]
		[batch 20/20] avg loss: 0.02866890457204831		[learning rate: 0.00067621]
	Learning Rate: 0.000676206
	LOSS [training: 0.031180006507345037 | validation: 0.03082133232287171]
	TIME [epoch: 8.19 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02569479806456032		[learning rate: 0.00067498]
		[batch 20/20] avg loss: 0.029524216386522934		[learning rate: 0.00067375]
	Learning Rate: 0.000673752
	LOSS [training: 0.02760950722554163 | validation: 0.025357789123993023]
	TIME [epoch: 8.17 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022854739289095767		[learning rate: 0.00067253]
		[batch 20/20] avg loss: 0.03632162370377806		[learning rate: 0.00067131]
	Learning Rate: 0.000671307
	LOSS [training: 0.029588181496436917 | validation: 0.04376279515393748]
	TIME [epoch: 8.21 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03756704602259007		[learning rate: 0.00067009]
		[batch 20/20] avg loss: 0.0324928672006925		[learning rate: 0.00066887]
	Learning Rate: 0.000668871
	LOSS [training: 0.03502995661164128 | validation: 0.02375629991694273]
	TIME [epoch: 8.21 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034833448758891244		[learning rate: 0.00066766]
		[batch 20/20] avg loss: 0.02447670934180061		[learning rate: 0.00066644]
	Learning Rate: 0.000666443
	LOSS [training: 0.02965507905034593 | validation: 0.05426377839960277]
	TIME [epoch: 8.19 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031209955151609847		[learning rate: 0.00066523]
		[batch 20/20] avg loss: 0.021967886096876058		[learning rate: 0.00066402]
	Learning Rate: 0.000664025
	LOSS [training: 0.02658892062424295 | validation: 0.03625035690307128]
	TIME [epoch: 8.19 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03862380541620075		[learning rate: 0.00066282]
		[batch 20/20] avg loss: 0.031704513059697646		[learning rate: 0.00066161]
	Learning Rate: 0.000661615
	LOSS [training: 0.0351641592379492 | validation: 0.0271320222946664]
	TIME [epoch: 8.22 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026005922891125705		[learning rate: 0.00066041]
		[batch 20/20] avg loss: 0.028411602790438428		[learning rate: 0.00065921]
	Learning Rate: 0.000659214
	LOSS [training: 0.027208762840782063 | validation: 0.04454269951836692]
	TIME [epoch: 8.21 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023575646693805172		[learning rate: 0.00065802]
		[batch 20/20] avg loss: 0.029580053850798184		[learning rate: 0.00065682]
	Learning Rate: 0.000656822
	LOSS [training: 0.026577850272301674 | validation: 0.032671830864914535]
	TIME [epoch: 8.19 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038840833439211554		[learning rate: 0.00065563]
		[batch 20/20] avg loss: 0.03379528518068182		[learning rate: 0.00065444]
	Learning Rate: 0.000654438
	LOSS [training: 0.03631805930994668 | validation: 0.015155678204023738]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_850.pth
	Model improved!!!
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027243761582822455		[learning rate: 0.00065325]
		[batch 20/20] avg loss: 0.0337189482267415		[learning rate: 0.00065206]
	Learning Rate: 0.000652063
	LOSS [training: 0.030481354904781977 | validation: 0.014518826151302637]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_851.pth
	Model improved!!!
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03208400206011616		[learning rate: 0.00065088]
		[batch 20/20] avg loss: 0.037355933504957514		[learning rate: 0.0006497]
	Learning Rate: 0.000649697
	LOSS [training: 0.03471996778253684 | validation: 0.03650473999482244]
	TIME [epoch: 8.18 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02656620047665754		[learning rate: 0.00064852]
		[batch 20/20] avg loss: 0.03801437733862737		[learning rate: 0.00064734]
	Learning Rate: 0.000647339
	LOSS [training: 0.032290288907642455 | validation: 0.03887047128372315]
	TIME [epoch: 8.18 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046576533995697764		[learning rate: 0.00064616]
		[batch 20/20] avg loss: 0.02816886898672577		[learning rate: 0.00064499]
	Learning Rate: 0.00064499
	LOSS [training: 0.037372701491211774 | validation: 0.03603681263019097]
	TIME [epoch: 8.15 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031745738940459615		[learning rate: 0.00064382]
		[batch 20/20] avg loss: 0.04802868374865864		[learning rate: 0.00064265]
	Learning Rate: 0.000642649
	LOSS [training: 0.03988721134455912 | validation: 0.10583034260500232]
	TIME [epoch: 8.21 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04193286540061597		[learning rate: 0.00064148]
		[batch 20/20] avg loss: 0.02436273529273931		[learning rate: 0.00064032]
	Learning Rate: 0.000640317
	LOSS [training: 0.033147800346677636 | validation: 0.019696849664493726]
	TIME [epoch: 8.18 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03765152259471067		[learning rate: 0.00063915]
		[batch 20/20] avg loss: 0.034391890839010085		[learning rate: 0.00063799]
	Learning Rate: 0.000637993
	LOSS [training: 0.03602170671686038 | validation: 0.029458448956567993]
	TIME [epoch: 8.17 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027618399307221653		[learning rate: 0.00063683]
		[batch 20/20] avg loss: 0.030209282580795516		[learning rate: 0.00063568]
	Learning Rate: 0.000635677
	LOSS [training: 0.028913840944008583 | validation: 0.02805917889751586]
	TIME [epoch: 8.18 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04073375386329403		[learning rate: 0.00063452]
		[batch 20/20] avg loss: 0.027932168258592273		[learning rate: 0.00063337]
	Learning Rate: 0.000633371
	LOSS [training: 0.034332961060943155 | validation: 0.017772723061066434]
	TIME [epoch: 8.2 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027436059692178022		[learning rate: 0.00063222]
		[batch 20/20] avg loss: 0.030608355342869764		[learning rate: 0.00063107]
	Learning Rate: 0.000631072
	LOSS [training: 0.029022207517523897 | validation: 0.02566451947661002]
	TIME [epoch: 8.19 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028138451251680074		[learning rate: 0.00062993]
		[batch 20/20] avg loss: 0.03183724923859418		[learning rate: 0.00062878]
	Learning Rate: 0.000628782
	LOSS [training: 0.029987850245137126 | validation: 0.041890323160984795]
	TIME [epoch: 8.18 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022862937367236623		[learning rate: 0.00062764]
		[batch 20/20] avg loss: 0.02924724153680692		[learning rate: 0.0006265]
	Learning Rate: 0.0006265
	LOSS [training: 0.026055089452021767 | validation: 0.0373693584823499]
	TIME [epoch: 8.18 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02770773344628205		[learning rate: 0.00062536]
		[batch 20/20] avg loss: 0.022555036689918876		[learning rate: 0.00062423]
	Learning Rate: 0.000624226
	LOSS [training: 0.025131385068100465 | validation: 0.01905352085096121]
	TIME [epoch: 8.19 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022663842558464993		[learning rate: 0.00062309]
		[batch 20/20] avg loss: 0.04668938051142479		[learning rate: 0.00062196]
	Learning Rate: 0.000621961
	LOSS [training: 0.0346766115349449 | validation: 0.04443129809182447]
	TIME [epoch: 8.19 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04182383740594729		[learning rate: 0.00062083]
		[batch 20/20] avg loss: 0.024386496585895097		[learning rate: 0.0006197]
	Learning Rate: 0.000619704
	LOSS [training: 0.0331051669959212 | validation: 0.0669979973858083]
	TIME [epoch: 8.17 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029265882741187976		[learning rate: 0.00061858]
		[batch 20/20] avg loss: 0.02605472929484079		[learning rate: 0.00061745]
	Learning Rate: 0.000617455
	LOSS [training: 0.02766030601801438 | validation: 0.015812081125858263]
	TIME [epoch: 8.17 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023252938302295967		[learning rate: 0.00061633]
		[batch 20/20] avg loss: 0.021889683254694575		[learning rate: 0.00061521]
	Learning Rate: 0.000615214
	LOSS [training: 0.022571310778495272 | validation: 0.017115130479960907]
	TIME [epoch: 8.21 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03278845291748833		[learning rate: 0.0006141]
		[batch 20/20] avg loss: 0.05148312622068785		[learning rate: 0.00061298]
	Learning Rate: 0.000612982
	LOSS [training: 0.04213578956908808 | validation: 0.04051001479446532]
	TIME [epoch: 8.19 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04891380798653206		[learning rate: 0.00061187]
		[batch 20/20] avg loss: 0.02493644601159404		[learning rate: 0.00061076]
	Learning Rate: 0.000610757
	LOSS [training: 0.03692512699906305 | validation: 0.014305319548225219]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_869.pth
	Model improved!!!
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026858554024376768		[learning rate: 0.00060965]
		[batch 20/20] avg loss: 0.04150346606920856		[learning rate: 0.00060854]
	Learning Rate: 0.00060854
	LOSS [training: 0.034181010046792665 | validation: 0.07189121712326532]
	TIME [epoch: 8.18 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04327720323205638		[learning rate: 0.00060744]
		[batch 20/20] avg loss: 0.036121314126046924		[learning rate: 0.00060633]
	Learning Rate: 0.000606332
	LOSS [training: 0.03969925867905165 | validation: 0.031085818277692752]
	TIME [epoch: 8.18 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055830925121628		[learning rate: 0.00060523]
		[batch 20/20] avg loss: 0.02645076011303451		[learning rate: 0.00060413]
	Learning Rate: 0.000604132
	LOSS [training: 0.041140842617331266 | validation: 0.09582532700945268]
	TIME [epoch: 8.18 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0578339317122033		[learning rate: 0.00060303]
		[batch 20/20] avg loss: 0.04899519723314375		[learning rate: 0.00060194]
	Learning Rate: 0.000601939
	LOSS [training: 0.05341456447267352 | validation: 0.038754058971962504]
	TIME [epoch: 8.17 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03434658199335243		[learning rate: 0.00060085]
		[batch 20/20] avg loss: 0.045895751855962806		[learning rate: 0.00059975]
	Learning Rate: 0.000599755
	LOSS [training: 0.04012116692465762 | validation: 0.03851699597591514]
	TIME [epoch: 8.19 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03370240827989153		[learning rate: 0.00059867]
		[batch 20/20] avg loss: 0.032452332901157004		[learning rate: 0.00059758]
	Learning Rate: 0.000597578
	LOSS [training: 0.033077370590524256 | validation: 0.016644638559791477]
	TIME [epoch: 8.19 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02670356265710692		[learning rate: 0.00059649]
		[batch 20/20] avg loss: 0.03678730679073804		[learning rate: 0.00059541]
	Learning Rate: 0.00059541
	LOSS [training: 0.03174543472392248 | validation: 0.029292586825128843]
	TIME [epoch: 8.17 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02339952070388927		[learning rate: 0.00059433]
		[batch 20/20] avg loss: 0.030983458427174277		[learning rate: 0.00059325]
	Learning Rate: 0.000593249
	LOSS [training: 0.02719148956553178 | validation: 0.012803641174499279]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_877.pth
	Model improved!!!
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05611887706762172		[learning rate: 0.00059217]
		[batch 20/20] avg loss: 0.025979216505322165		[learning rate: 0.0005911]
	Learning Rate: 0.000591096
	LOSS [training: 0.04104904678647195 | validation: 0.02554301351770113]
	TIME [epoch: 8.19 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021317430260974476		[learning rate: 0.00059002]
		[batch 20/20] avg loss: 0.029893231974056002		[learning rate: 0.00058895]
	Learning Rate: 0.000588951
	LOSS [training: 0.02560533111751524 | validation: 0.025358729633613814]
	TIME [epoch: 8.19 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03429496423918441		[learning rate: 0.00058788]
		[batch 20/20] avg loss: 0.024137738187374524		[learning rate: 0.00058681]
	Learning Rate: 0.000586813
	LOSS [training: 0.02921635121327946 | validation: 0.024593870460429903]
	TIME [epoch: 8.18 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040719334541024714		[learning rate: 0.00058575]
		[batch 20/20] avg loss: 0.023040440396533333		[learning rate: 0.00058468]
	Learning Rate: 0.000584684
	LOSS [training: 0.031879887468779025 | validation: 0.028631067646834134]
	TIME [epoch: 8.17 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034235157853051285		[learning rate: 0.00058362]
		[batch 20/20] avg loss: 0.03858080609237682		[learning rate: 0.00058256]
	Learning Rate: 0.000582562
	LOSS [training: 0.03640798197271405 | validation: 0.05171373017369686]
	TIME [epoch: 8.19 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033718384124608515		[learning rate: 0.0005815]
		[batch 20/20] avg loss: 0.03290138989577916		[learning rate: 0.00058045]
	Learning Rate: 0.000580448
	LOSS [training: 0.03330988701019384 | validation: 0.02649228205015051]
	TIME [epoch: 8.18 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024406221614657424		[learning rate: 0.00057939]
		[batch 20/20] avg loss: 0.031915196153890546		[learning rate: 0.00057834]
	Learning Rate: 0.000578341
	LOSS [training: 0.028160708884273983 | validation: 0.030759307006953]
	TIME [epoch: 8.16 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021878480394449906		[learning rate: 0.00057729]
		[batch 20/20] avg loss: 0.026480693361686798		[learning rate: 0.00057624]
	Learning Rate: 0.000576243
	LOSS [training: 0.02417958687806835 | validation: 0.01441368722501885]
	TIME [epoch: 8.17 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03521875268001125		[learning rate: 0.0005752]
		[batch 20/20] avg loss: 0.03863591511712667		[learning rate: 0.00057415]
	Learning Rate: 0.000574151
	LOSS [training: 0.036927333898568955 | validation: 0.020399200264020697]
	TIME [epoch: 8.18 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02089826270899018		[learning rate: 0.00057311]
		[batch 20/20] avg loss: 0.029216220385671128		[learning rate: 0.00057207]
	Learning Rate: 0.000572068
	LOSS [training: 0.02505724154733065 | validation: 0.032308226073317375]
	TIME [epoch: 8.17 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025131700727138228		[learning rate: 0.00057103]
		[batch 20/20] avg loss: 0.041327259813360516		[learning rate: 0.00056999]
	Learning Rate: 0.000569992
	LOSS [training: 0.03322948027024937 | validation: 0.026678261226952046]
	TIME [epoch: 8.17 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025945597046199524		[learning rate: 0.00056896]
		[batch 20/20] avg loss: 0.03196705399689552		[learning rate: 0.00056792]
	Learning Rate: 0.000567923
	LOSS [training: 0.028956325521547522 | validation: 0.02171415523899715]
	TIME [epoch: 8.19 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02205438270097592		[learning rate: 0.00056689]
		[batch 20/20] avg loss: 0.03172952248251128		[learning rate: 0.00056586]
	Learning Rate: 0.000565862
	LOSS [training: 0.026891952591743595 | validation: 0.02536094479942849]
	TIME [epoch: 8.2 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022302530169554614		[learning rate: 0.00056483]
		[batch 20/20] avg loss: 0.041452975993113726		[learning rate: 0.00056381]
	Learning Rate: 0.000563808
	LOSS [training: 0.03187775308133418 | validation: 0.025793325833580828]
	TIME [epoch: 8.18 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03279404059741279		[learning rate: 0.00056278]
		[batch 20/20] avg loss: 0.022088815748619987		[learning rate: 0.00056176]
	Learning Rate: 0.000561762
	LOSS [training: 0.027441428173016386 | validation: 0.021314329655524766]
	TIME [epoch: 8.16 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017389075604392155		[learning rate: 0.00056074]
		[batch 20/20] avg loss: 0.030357025482180493		[learning rate: 0.00055972]
	Learning Rate: 0.000559724
	LOSS [training: 0.023873050543286322 | validation: 0.018864557983354926]
	TIME [epoch: 8.18 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02215198595038171		[learning rate: 0.00055871]
		[batch 20/20] avg loss: 0.023705244782779935		[learning rate: 0.00055769]
	Learning Rate: 0.000557692
	LOSS [training: 0.022928615366580817 | validation: 0.03210046575668464]
	TIME [epoch: 8.19 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038347411398882814		[learning rate: 0.00055668]
		[batch 20/20] avg loss: 0.02093739731370827		[learning rate: 0.00055567]
	Learning Rate: 0.000555669
	LOSS [training: 0.029642404356295542 | validation: 0.020988642767011876]
	TIME [epoch: 8.17 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033839494095628406		[learning rate: 0.00055466]
		[batch 20/20] avg loss: 0.03201994860048653		[learning rate: 0.00055365]
	Learning Rate: 0.000553652
	LOSS [training: 0.03292972134805747 | validation: 0.02444008770849621]
	TIME [epoch: 8.15 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03188820930532145		[learning rate: 0.00055265]
		[batch 20/20] avg loss: 0.027787230085043912		[learning rate: 0.00055164]
	Learning Rate: 0.000551643
	LOSS [training: 0.02983771969518268 | validation: 0.022280734907527924]
	TIME [epoch: 8.18 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022459383703359082		[learning rate: 0.00055064]
		[batch 20/20] avg loss: 0.04407551089095023		[learning rate: 0.00054964]
	Learning Rate: 0.000549641
	LOSS [training: 0.033267447297154654 | validation: 0.021308511698352017]
	TIME [epoch: 8.18 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018576570865364784		[learning rate: 0.00054864]
		[batch 20/20] avg loss: 0.018355020628629645		[learning rate: 0.00054765]
	Learning Rate: 0.000547646
	LOSS [training: 0.018465795746997214 | validation: 0.02618121323285024]
	TIME [epoch: 8.17 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0263979747552667		[learning rate: 0.00054665]
		[batch 20/20] avg loss: 0.04180059337548826		[learning rate: 0.00054566]
	Learning Rate: 0.000545659
	LOSS [training: 0.03409928406537748 | validation: 0.0362775310253524]
	TIME [epoch: 8.16 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02504204100897898		[learning rate: 0.00054467]
		[batch 20/20] avg loss: 0.03356620347067265		[learning rate: 0.00054368]
	Learning Rate: 0.000543678
	LOSS [training: 0.029304122239825814 | validation: 0.030035952837349174]
	TIME [epoch: 8.18 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027273438684430246		[learning rate: 0.00054269]
		[batch 20/20] avg loss: 0.03542242081230091		[learning rate: 0.00054171]
	Learning Rate: 0.000541705
	LOSS [training: 0.031347929748365574 | validation: 0.03054710597517743]
	TIME [epoch: 8.18 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026451528049044958		[learning rate: 0.00054072]
		[batch 20/20] avg loss: 0.0346942174068314		[learning rate: 0.00053974]
	Learning Rate: 0.00053974
	LOSS [training: 0.030572872727938176 | validation: 0.03039412784371815]
	TIME [epoch: 8.19 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028118959122995925		[learning rate: 0.00053876]
		[batch 20/20] avg loss: 0.017448616268873816		[learning rate: 0.00053778]
	Learning Rate: 0.000537781
	LOSS [training: 0.02278378769593487 | validation: 0.03952350444741923]
	TIME [epoch: 8.16 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025296179805388667		[learning rate: 0.0005368]
		[batch 20/20] avg loss: 0.026523772788709793		[learning rate: 0.00053583]
	Learning Rate: 0.000535829
	LOSS [training: 0.02590997629704923 | validation: 0.012853679246942242]
	TIME [epoch: 8.18 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026391890769279856		[learning rate: 0.00053486]
		[batch 20/20] avg loss: 0.017058870662095722		[learning rate: 0.00053388]
	Learning Rate: 0.000533885
	LOSS [training: 0.021725380715687793 | validation: 0.01816624083769958]
	TIME [epoch: 8.2 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0192337599302325		[learning rate: 0.00053291]
		[batch 20/20] avg loss: 0.027755635064662963		[learning rate: 0.00053195]
	Learning Rate: 0.000531947
	LOSS [training: 0.02349469749744773 | validation: 0.030572748358329505]
	TIME [epoch: 8.17 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018157245213270154		[learning rate: 0.00053098]
		[batch 20/20] avg loss: 0.03146867000858354		[learning rate: 0.00053002]
	Learning Rate: 0.000530017
	LOSS [training: 0.024812957610926844 | validation: 0.047319890351343145]
	TIME [epoch: 8.16 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021476136859854673		[learning rate: 0.00052905]
		[batch 20/20] avg loss: 0.014471416564897227		[learning rate: 0.00052809]
	Learning Rate: 0.000528093
	LOSS [training: 0.017973776712375947 | validation: 0.020235378746087437]
	TIME [epoch: 8.18 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03247081619316387		[learning rate: 0.00052713]
		[batch 20/20] avg loss: 0.05080839650144423		[learning rate: 0.00052618]
	Learning Rate: 0.000526177
	LOSS [training: 0.04163960634730405 | validation: 0.01881107689649442]
	TIME [epoch: 8.2 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03087329306876168		[learning rate: 0.00052522]
		[batch 20/20] avg loss: 0.029630630476663578		[learning rate: 0.00052427]
	Learning Rate: 0.000524267
	LOSS [training: 0.030251961772712627 | validation: 0.02756918497685094]
	TIME [epoch: 8.17 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024785745655791083		[learning rate: 0.00052332]
		[batch 20/20] avg loss: 0.04191574020864861		[learning rate: 0.00052236]
	Learning Rate: 0.000522365
	LOSS [training: 0.033350742932219846 | validation: 0.04899029147908806]
	TIME [epoch: 8.16 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026695237241249857		[learning rate: 0.00052142]
		[batch 20/20] avg loss: 0.029640553050315776		[learning rate: 0.00052047]
	Learning Rate: 0.000520469
	LOSS [training: 0.028167895145782818 | validation: 0.03794540163095145]
	TIME [epoch: 8.17 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0404956598977006		[learning rate: 0.00051952]
		[batch 20/20] avg loss: 0.019408340338181455		[learning rate: 0.00051858]
	Learning Rate: 0.00051858
	LOSS [training: 0.029952000117941018 | validation: 0.02450208232308206]
	TIME [epoch: 8.21 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021843424583816114		[learning rate: 0.00051764]
		[batch 20/20] avg loss: 0.01948072145272261		[learning rate: 0.0005167]
	Learning Rate: 0.000516698
	LOSS [training: 0.02066207301826936 | validation: 0.047617776500367456]
	TIME [epoch: 8.18 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03177905450879863		[learning rate: 0.00051576]
		[batch 20/20] avg loss: 0.021533384249750535		[learning rate: 0.00051482]
	Learning Rate: 0.000514823
	LOSS [training: 0.026656219379274575 | validation: 0.013359168599273158]
	TIME [epoch: 8.16 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018432671801542497		[learning rate: 0.00051389]
		[batch 20/20] avg loss: 0.0301386330818106		[learning rate: 0.00051295]
	Learning Rate: 0.000512955
	LOSS [training: 0.024285652441676547 | validation: 0.050846150845030594]
	TIME [epoch: 8.2 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04319149425188392		[learning rate: 0.00051202]
		[batch 20/20] avg loss: 0.027391182128015584		[learning rate: 0.00051109]
	Learning Rate: 0.000511093
	LOSS [training: 0.03529133818994974 | validation: 0.041354934219758166]
	TIME [epoch: 8.22 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025437732274554842		[learning rate: 0.00051016]
		[batch 20/20] avg loss: 0.030635387031479437		[learning rate: 0.00050924]
	Learning Rate: 0.000509238
	LOSS [training: 0.02803655965301714 | validation: 0.024166870766397293]
	TIME [epoch: 8.18 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01826253465859618		[learning rate: 0.00050831]
		[batch 20/20] avg loss: 0.015117555245871703		[learning rate: 0.00050739]
	Learning Rate: 0.00050739
	LOSS [training: 0.01669004495223394 | validation: 0.03201764057835162]
	TIME [epoch: 8.18 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025646757553485743		[learning rate: 0.00050647]
		[batch 20/20] avg loss: 0.01760372621440604		[learning rate: 0.00050555]
	Learning Rate: 0.000505549
	LOSS [training: 0.0216252418839459 | validation: 0.019468590634058588]
	TIME [epoch: 8.19 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029002968777809425		[learning rate: 0.00050463]
		[batch 20/20] avg loss: 0.030848413730701862		[learning rate: 0.00050371]
	Learning Rate: 0.000503714
	LOSS [training: 0.02992569125425564 | validation: 0.07896096605695502]
	TIME [epoch: 8.21 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030600643962776498		[learning rate: 0.0005028]
		[batch 20/20] avg loss: 0.022692606714734782		[learning rate: 0.00050189]
	Learning Rate: 0.000501886
	LOSS [training: 0.026646625338755635 | validation: 0.04927492360612566]
	TIME [epoch: 8.17 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03986835048475358		[learning rate: 0.00050097]
		[batch 20/20] avg loss: 0.024915358112573126		[learning rate: 0.00050006]
	Learning Rate: 0.000500065
	LOSS [training: 0.032391854298663356 | validation: 0.01847354981426742]
	TIME [epoch: 8.17 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019591921796507556		[learning rate: 0.00049916]
		[batch 20/20] avg loss: 0.02014724394378967		[learning rate: 0.00049825]
	Learning Rate: 0.00049825
	LOSS [training: 0.01986958287014861 | validation: 0.02459580953438339]
	TIME [epoch: 8.18 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019705045393299907		[learning rate: 0.00049735]
		[batch 20/20] avg loss: 0.040877287152279736		[learning rate: 0.00049644]
	Learning Rate: 0.000496442
	LOSS [training: 0.03029116627278982 | validation: 0.04046075843629483]
	TIME [epoch: 8.22 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02220698471688775		[learning rate: 0.00049554]
		[batch 20/20] avg loss: 0.026551313332053628		[learning rate: 0.00049464]
	Learning Rate: 0.00049464
	LOSS [training: 0.024379149024470685 | validation: 0.011093231005847536]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_927.pth
	Model improved!!!
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028754064780781935		[learning rate: 0.00049374]
		[batch 20/20] avg loss: 0.031508241346759204		[learning rate: 0.00049285]
	Learning Rate: 0.000492845
	LOSS [training: 0.030131153063770566 | validation: 0.0346338990923776]
	TIME [epoch: 8.16 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02449958866529895		[learning rate: 0.00049195]
		[batch 20/20] avg loss: 0.02721022880448982		[learning rate: 0.00049106]
	Learning Rate: 0.000491057
	LOSS [training: 0.025854908734894388 | validation: 0.02367968653073077]
	TIME [epoch: 8.19 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031009607366919822		[learning rate: 0.00049016]
		[batch 20/20] avg loss: 0.0182748797172435		[learning rate: 0.00048927]
	Learning Rate: 0.000489275
	LOSS [training: 0.02464224354208166 | validation: 0.023272308079642853]
	TIME [epoch: 8.21 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036119128423966695		[learning rate: 0.00048839]
		[batch 20/20] avg loss: 0.026462134653634607		[learning rate: 0.0004875]
	Learning Rate: 0.000487499
	LOSS [training: 0.031290631538800656 | validation: 0.026470745070415944]
	TIME [epoch: 8.15 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028098458818132575		[learning rate: 0.00048661]
		[batch 20/20] avg loss: 0.03746359857785033		[learning rate: 0.00048573]
	Learning Rate: 0.00048573
	LOSS [training: 0.03278102869799145 | validation: 0.02201950418203733]
	TIME [epoch: 8.16 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02092092786775133		[learning rate: 0.00048485]
		[batch 20/20] avg loss: 0.019071437417290636		[learning rate: 0.00048397]
	Learning Rate: 0.000483967
	LOSS [training: 0.019996182642520982 | validation: 0.03149071055854931]
	TIME [epoch: 8.2 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02665367384161168		[learning rate: 0.00048309]
		[batch 20/20] avg loss: 0.02083262634090461		[learning rate: 0.00048221]
	Learning Rate: 0.000482211
	LOSS [training: 0.02374315009125814 | validation: 0.03521451749430903]
	TIME [epoch: 8.2 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009807336687564155		[learning rate: 0.00048133]
		[batch 20/20] avg loss: 0.025987701210902676		[learning rate: 0.00048046]
	Learning Rate: 0.000480461
	LOSS [training: 0.017897518949233415 | validation: 0.012068903373178985]
	TIME [epoch: 8.16 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031442040344097984		[learning rate: 0.00047959]
		[batch 20/20] avg loss: 0.02369653033577306		[learning rate: 0.00047872]
	Learning Rate: 0.000478717
	LOSS [training: 0.027569285339935523 | validation: 0.02522929669330231]
	TIME [epoch: 8.16 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0339641874354051		[learning rate: 0.00047785]
		[batch 20/20] avg loss: 0.02176916890259416		[learning rate: 0.00047698]
	Learning Rate: 0.00047698
	LOSS [training: 0.027866678168999633 | validation: 0.024002876163955515]
	TIME [epoch: 8.19 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03876746492031925		[learning rate: 0.00047611]
		[batch 20/20] avg loss: 0.016544477856755793		[learning rate: 0.00047525]
	Learning Rate: 0.000475249
	LOSS [training: 0.027655971388537526 | validation: 0.018648147815840566]
	TIME [epoch: 8.2 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020611213622757536		[learning rate: 0.00047439]
		[batch 20/20] avg loss: 0.01982919933126411		[learning rate: 0.00047352]
	Learning Rate: 0.000473524
	LOSS [training: 0.020220206477010824 | validation: 0.02511301291171418]
	TIME [epoch: 8.16 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028185776365812536		[learning rate: 0.00047266]
		[batch 20/20] avg loss: 0.02592860894712773		[learning rate: 0.00047181]
	Learning Rate: 0.000471806
	LOSS [training: 0.02705719265647013 | validation: 0.014649779054853844]
	TIME [epoch: 8.16 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03259200920490733		[learning rate: 0.00047095]
		[batch 20/20] avg loss: 0.045937008477414956		[learning rate: 0.00047009]
	Learning Rate: 0.000470093
	LOSS [training: 0.039264508841161144 | validation: 0.04055756172161986]
	TIME [epoch: 8.2 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028259780052740054		[learning rate: 0.00046924]
		[batch 20/20] avg loss: 0.021803837365120544		[learning rate: 0.00046839]
	Learning Rate: 0.000468388
	LOSS [training: 0.0250318087089303 | validation: 0.021498574994504787]
	TIME [epoch: 8.2 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031565725201676435		[learning rate: 0.00046754]
		[batch 20/20] avg loss: 0.020351439874505068		[learning rate: 0.00046669]
	Learning Rate: 0.000466688
	LOSS [training: 0.025958582538090757 | validation: 0.025695762697991075]
	TIME [epoch: 8.16 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026730626062200103		[learning rate: 0.00046584]
		[batch 20/20] avg loss: 0.029072224983504563		[learning rate: 0.00046499]
	Learning Rate: 0.000464994
	LOSS [training: 0.02790142552285233 | validation: 0.020962925571908302]
	TIME [epoch: 8.16 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02248690432081765		[learning rate: 0.00046415]
		[batch 20/20] avg loss: 0.013213748651727267		[learning rate: 0.00046331]
	Learning Rate: 0.000463307
	LOSS [training: 0.01785032648627246 | validation: 0.015495442813082638]
	TIME [epoch: 8.19 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018479316976949073		[learning rate: 0.00046247]
		[batch 20/20] avg loss: 0.028488597536321675		[learning rate: 0.00046163]
	Learning Rate: 0.000461625
	LOSS [training: 0.023483957256635376 | validation: 0.014327877781261798]
	TIME [epoch: 8.2 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016417203935507753		[learning rate: 0.00046079]
		[batch 20/20] avg loss: 0.018592015825004894		[learning rate: 0.00045995]
	Learning Rate: 0.00045995
	LOSS [training: 0.017504609880256324 | validation: 0.026741683817008925]
	TIME [epoch: 8.15 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03361060041196594		[learning rate: 0.00045911]
		[batch 20/20] avg loss: 0.015275590378104942		[learning rate: 0.00045828]
	Learning Rate: 0.000458281
	LOSS [training: 0.02444309539503544 | validation: 0.01714731790390861]
	TIME [epoch: 8.17 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0218760041524097		[learning rate: 0.00045745]
		[batch 20/20] avg loss: 0.038098125688658536		[learning rate: 0.00045662]
	Learning Rate: 0.000456618
	LOSS [training: 0.029987064920534122 | validation: 0.008290495129197118]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_949.pth
	Model improved!!!
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025261188799166966		[learning rate: 0.00045579]
		[batch 20/20] avg loss: 0.013922239183668588		[learning rate: 0.00045496]
	Learning Rate: 0.000454961
	LOSS [training: 0.019591713991417778 | validation: 0.025074494820424717]
	TIME [epoch: 8.19 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025068099360113488		[learning rate: 0.00045413]
		[batch 20/20] avg loss: 0.025764177263041626		[learning rate: 0.00045331]
	Learning Rate: 0.000453309
	LOSS [training: 0.025416138311577557 | validation: 0.02420144237052004]
	TIME [epoch: 8.16 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026793634287537522		[learning rate: 0.00045249]
		[batch 20/20] avg loss: 0.01755796790169276		[learning rate: 0.00045166]
	Learning Rate: 0.000451664
	LOSS [training: 0.02217580109461514 | validation: 0.021471074742300502]
	TIME [epoch: 8.16 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02518976885837628		[learning rate: 0.00045084]
		[batch 20/20] avg loss: 0.014575099010015882		[learning rate: 0.00045003]
	Learning Rate: 0.000450025
	LOSS [training: 0.01988243393419608 | validation: 0.036788406381428676]
	TIME [epoch: 8.21 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02253532992393712		[learning rate: 0.00044921]
		[batch 20/20] avg loss: 0.020550981010408916		[learning rate: 0.00044839]
	Learning Rate: 0.000448392
	LOSS [training: 0.021543155467173024 | validation: 0.05804332384418489]
	TIME [epoch: 8.19 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06149990601878448		[learning rate: 0.00044758]
		[batch 20/20] avg loss: 0.020486259986510897		[learning rate: 0.00044676]
	Learning Rate: 0.000446765
	LOSS [training: 0.04099308300264769 | validation: 0.02011462644695709]
	TIME [epoch: 8.15 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029855634992720974		[learning rate: 0.00044595]
		[batch 20/20] avg loss: 0.029906618075588608		[learning rate: 0.00044514]
	Learning Rate: 0.000445143
	LOSS [training: 0.0298811265341548 | validation: 0.020923151837416446]
	TIME [epoch: 8.16 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016780726646026378		[learning rate: 0.00044434]
		[batch 20/20] avg loss: 0.032385255336293675		[learning rate: 0.00044353]
	Learning Rate: 0.000443528
	LOSS [training: 0.024582990991160025 | validation: 0.09531792582849731]
	TIME [epoch: 8.2 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04842235008899911		[learning rate: 0.00044272]
		[batch 20/20] avg loss: 0.011217786560875517		[learning rate: 0.00044192]
	Learning Rate: 0.000441918
	LOSS [training: 0.02982006832493731 | validation: 0.028513714170916537]
	TIME [epoch: 8.18 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027988601344533865		[learning rate: 0.00044112]
		[batch 20/20] avg loss: 0.022364625797631425		[learning rate: 0.00044031]
	Learning Rate: 0.000440315
	LOSS [training: 0.025176613571082647 | validation: 0.06271124222381172]
	TIME [epoch: 8.15 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029348877656785642		[learning rate: 0.00043952]
		[batch 20/20] avg loss: 0.04177888250327352		[learning rate: 0.00043872]
	Learning Rate: 0.000438717
	LOSS [training: 0.035563880080029586 | validation: 0.04473296015230968]
	TIME [epoch: 8.15 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02274274753797456		[learning rate: 0.00043792]
		[batch 20/20] avg loss: 0.026714258389422263		[learning rate: 0.00043712]
	Learning Rate: 0.000437125
	LOSS [training: 0.024728502963698412 | validation: 0.025777813936879782]
	TIME [epoch: 8.22 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03240540858913562		[learning rate: 0.00043633]
		[batch 20/20] avg loss: 0.020900320755128196		[learning rate: 0.00043554]
	Learning Rate: 0.000435538
	LOSS [training: 0.026652864672131904 | validation: 0.014054925558413613]
	TIME [epoch: 8.18 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05690211737245954		[learning rate: 0.00043475]
		[batch 20/20] avg loss: 0.025261355219628457		[learning rate: 0.00043396]
	Learning Rate: 0.000433958
	LOSS [training: 0.041081736296043996 | validation: 0.032743652268058566]
	TIME [epoch: 8.15 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028077071665470705		[learning rate: 0.00043317]
		[batch 20/20] avg loss: 0.017074086334503507		[learning rate: 0.00043238]
	Learning Rate: 0.000432383
	LOSS [training: 0.0225755789999871 | validation: 0.013233363371986181]
	TIME [epoch: 8.16 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019886529069928237		[learning rate: 0.0004316]
		[batch 20/20] avg loss: 0.01406542259951527		[learning rate: 0.00043081]
	Learning Rate: 0.000430814
	LOSS [training: 0.016975975834721754 | validation: 0.018727202520776882]
	TIME [epoch: 8.2 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02642877263060605		[learning rate: 0.00043003]
		[batch 20/20] avg loss: 0.03227389509154928		[learning rate: 0.00042925]
	Learning Rate: 0.00042925
	LOSS [training: 0.029351333861077668 | validation: 0.0378187926883464]
	TIME [epoch: 8.18 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012385525595219538		[learning rate: 0.00042847]
		[batch 20/20] avg loss: 0.017504600756947417		[learning rate: 0.00042769]
	Learning Rate: 0.000427692
	LOSS [training: 0.014945063176083481 | validation: 0.008665662549040928]
	TIME [epoch: 8.14 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018036882116007416		[learning rate: 0.00042692]
		[batch 20/20] avg loss: 0.03590930372712446		[learning rate: 0.00042614]
	Learning Rate: 0.00042614
	LOSS [training: 0.026973092921565946 | validation: 0.01998295425240358]
	TIME [epoch: 8.16 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014795894729840326		[learning rate: 0.00042537]
		[batch 20/20] avg loss: 0.0189097558277676		[learning rate: 0.00042459]
	Learning Rate: 0.000424594
	LOSS [training: 0.01685282527880396 | validation: 0.03112524361479285]
	TIME [epoch: 8.19 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03161232948784167		[learning rate: 0.00042382]
		[batch 20/20] avg loss: 0.04580284960867068		[learning rate: 0.00042305]
	Learning Rate: 0.000423053
	LOSS [training: 0.03870758954825617 | validation: 0.031281808411612794]
	TIME [epoch: 8.18 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023409128276724808		[learning rate: 0.00042228]
		[batch 20/20] avg loss: 0.01758144967237221		[learning rate: 0.00042152]
	Learning Rate: 0.000421518
	LOSS [training: 0.0204952889745485 | validation: 0.01908561700801038]
	TIME [epoch: 8.15 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015384857442983822		[learning rate: 0.00042075]
		[batch 20/20] avg loss: 0.020651099417376757		[learning rate: 0.00041999]
	Learning Rate: 0.000419988
	LOSS [training: 0.01801797843018029 | validation: 0.0349506671808611]
	TIME [epoch: 8.17 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016299693597630158		[learning rate: 0.00041923]
		[batch 20/20] avg loss: 0.01832699530142187		[learning rate: 0.00041846]
	Learning Rate: 0.000418464
	LOSS [training: 0.017313344449526015 | validation: 0.02528162125263868]
	TIME [epoch: 8.18 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023586555008168607		[learning rate: 0.0004177]
		[batch 20/20] avg loss: 0.022883769723455096		[learning rate: 0.00041695]
	Learning Rate: 0.000416945
	LOSS [training: 0.02323516236581185 | validation: 0.023194156413606518]
	TIME [epoch: 8.18 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017352257126249885		[learning rate: 0.00041619]
		[batch 20/20] avg loss: 0.02354883559143908		[learning rate: 0.00041543]
	Learning Rate: 0.000415432
	LOSS [training: 0.020450546358844482 | validation: 0.021267459270461017]
	TIME [epoch: 8.15 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016296728696181637		[learning rate: 0.00041468]
		[batch 20/20] avg loss: 0.023896316779332488		[learning rate: 0.00041392]
	Learning Rate: 0.000413924
	LOSS [training: 0.02009652273775706 | validation: 0.010324554770281859]
	TIME [epoch: 8.18 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020247285583145057		[learning rate: 0.00041317]
		[batch 20/20] avg loss: 0.023488562642746662		[learning rate: 0.00041242]
	Learning Rate: 0.000412422
	LOSS [training: 0.021867924112945854 | validation: 0.033334520307475766]
	TIME [epoch: 8.18 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02005461710481535		[learning rate: 0.00041167]
		[batch 20/20] avg loss: 0.019266034375281925		[learning rate: 0.00041093]
	Learning Rate: 0.000410926
	LOSS [training: 0.019660325740048636 | validation: 0.01777390160263589]
	TIME [epoch: 8.18 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012875594358841868		[learning rate: 0.00041018]
		[batch 20/20] avg loss: 0.02007908761137439		[learning rate: 0.00040943]
	Learning Rate: 0.000409434
	LOSS [training: 0.01647734098510813 | validation: 0.052227914123040765]
	TIME [epoch: 8.16 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03106657645413039		[learning rate: 0.00040869]
		[batch 20/20] avg loss: 0.012453522375163438		[learning rate: 0.00040795]
	Learning Rate: 0.000407948
	LOSS [training: 0.021760049414646914 | validation: 0.013837788189540744]
	TIME [epoch: 8.17 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010640732770063076		[learning rate: 0.00040721]
		[batch 20/20] avg loss: 0.022761012666479924		[learning rate: 0.00040647]
	Learning Rate: 0.000406468
	LOSS [training: 0.016700872718271503 | validation: 0.023415501150490636]
	TIME [epoch: 8.18 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028731872060363435		[learning rate: 0.00040573]
		[batch 20/20] avg loss: 0.01421033667931241		[learning rate: 0.00040499]
	Learning Rate: 0.000404993
	LOSS [training: 0.021471104369837922 | validation: 0.0362840016450367]
	TIME [epoch: 8.18 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02074402571042781		[learning rate: 0.00040426]
		[batch 20/20] avg loss: 0.014418784302769513		[learning rate: 0.00040352]
	Learning Rate: 0.000403523
	LOSS [training: 0.017581405006598664 | validation: 0.02473343212442018]
	TIME [epoch: 8.17 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010430977267737968		[learning rate: 0.00040279]
		[batch 20/20] avg loss: 0.03865204511547114		[learning rate: 0.00040206]
	Learning Rate: 0.000402059
	LOSS [training: 0.02454151119160455 | validation: 0.015451224644645468]
	TIME [epoch: 8.18 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008491707375780146		[learning rate: 0.00040133]
		[batch 20/20] avg loss: 0.019302443382325896		[learning rate: 0.0004006]
	Learning Rate: 0.0004006
	LOSS [training: 0.013897075379053023 | validation: 0.034954673837359396]
	TIME [epoch: 8.18 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017199088794602607		[learning rate: 0.00039987]
		[batch 20/20] avg loss: 0.03576735917902936		[learning rate: 0.00039915]
	Learning Rate: 0.000399146
	LOSS [training: 0.02648322398681598 | validation: 0.04014925372028554]
	TIME [epoch: 8.17 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01756024971444271		[learning rate: 0.00039842]
		[batch 20/20] avg loss: 0.022969565116617872		[learning rate: 0.0003977]
	Learning Rate: 0.000397697
	LOSS [training: 0.020264907415530293 | validation: 0.037005124890322566]
	TIME [epoch: 8.16 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02103154383916349		[learning rate: 0.00039698]
		[batch 20/20] avg loss: 0.017053470157219627		[learning rate: 0.00039625]
	Learning Rate: 0.000396254
	LOSS [training: 0.019042506998191558 | validation: 0.04119694471413794]
	TIME [epoch: 8.17 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016499954746402697		[learning rate: 0.00039553]
		[batch 20/20] avg loss: 0.014557493923514412		[learning rate: 0.00039482]
	Learning Rate: 0.000394816
	LOSS [training: 0.015528724334958558 | validation: 0.017835608869637566]
	TIME [epoch: 8.18 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0230077764776403		[learning rate: 0.0003941]
		[batch 20/20] avg loss: 0.018523426371261115		[learning rate: 0.00039338]
	Learning Rate: 0.000393383
	LOSS [training: 0.020765601424450705 | validation: 0.035957923388941805]
	TIME [epoch: 8.17 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01735550119495879		[learning rate: 0.00039267]
		[batch 20/20] avg loss: 0.020361645133695143		[learning rate: 0.00039196]
	Learning Rate: 0.000391956
	LOSS [training: 0.018858573164326965 | validation: 0.049532615764047425]
	TIME [epoch: 8.16 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014352595953448899		[learning rate: 0.00039124]
		[batch 20/20] avg loss: 0.02894162608094219		[learning rate: 0.00039053]
	Learning Rate: 0.000390533
	LOSS [training: 0.021647111017195547 | validation: 0.04760799911113046]
	TIME [epoch: 8.19 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01682830621122041		[learning rate: 0.00038982]
		[batch 20/20] avg loss: 0.027754265050726822		[learning rate: 0.00038912]
	Learning Rate: 0.000389116
	LOSS [training: 0.022291285630973618 | validation: 0.02977604915162685]
	TIME [epoch: 8.18 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02410717923691468		[learning rate: 0.00038841]
		[batch 20/20] avg loss: 0.02574828482150044		[learning rate: 0.0003877]
	Learning Rate: 0.000387704
	LOSS [training: 0.02492773202920756 | validation: 0.027850679237069402]
	TIME [epoch: 8.19 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03034639430927773		[learning rate: 0.000387]
		[batch 20/20] avg loss: 0.02079059783140825		[learning rate: 0.0003863]
	Learning Rate: 0.000386297
	LOSS [training: 0.025568496070342988 | validation: 0.01692055993144727]
	TIME [epoch: 8.15 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022856755188113108		[learning rate: 0.0003856]
		[batch 20/20] avg loss: 0.025263981238406796		[learning rate: 0.00038489]
	Learning Rate: 0.000384895
	LOSS [training: 0.02406036821325995 | validation: 0.014106639149633948]
	TIME [epoch: 8.19 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012524082738216947		[learning rate: 0.0003842]
		[batch 20/20] avg loss: 0.024471646681941094		[learning rate: 0.0003835]
	Learning Rate: 0.000383498
	LOSS [training: 0.01849786471007902 | validation: 0.026632914133453377]
	TIME [epoch: 8.17 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01757720907732105		[learning rate: 0.0003828]
		[batch 20/20] avg loss: 0.013588453491563074		[learning rate: 0.00038211]
	Learning Rate: 0.000382106
	LOSS [training: 0.015582831284442062 | validation: 0.04200976445801]
	TIME [epoch: 8.18 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0307217588811586		[learning rate: 0.00038141]
		[batch 20/20] avg loss: 0.03107048268536654		[learning rate: 0.00038072]
	Learning Rate: 0.00038072
	LOSS [training: 0.030896120783262565 | validation: 0.011590830705902744]
	TIME [epoch: 8.16 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019090728595499463		[learning rate: 0.00038003]
		[batch 20/20] avg loss: 0.015777831932213562		[learning rate: 0.00037934]
	Learning Rate: 0.000379338
	LOSS [training: 0.017434280263856513 | validation: 0.01837670869992975]
	TIME [epoch: 8.19 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02973385147708809		[learning rate: 0.00037865]
		[batch 20/20] avg loss: 0.0357385293077941		[learning rate: 0.00037796]
	Learning Rate: 0.000377961
	LOSS [training: 0.0327361903924411 | validation: 0.014972681198325592]
	TIME [epoch: 8.16 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011650833639276107		[learning rate: 0.00037727]
		[batch 20/20] avg loss: 0.02758075532207895		[learning rate: 0.00037659]
	Learning Rate: 0.00037659
	LOSS [training: 0.019615794480677528 | validation: 0.03178355998332305]
	TIME [epoch: 8.17 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02715870676102188		[learning rate: 0.00037591]
		[batch 20/20] avg loss: 0.008788488102001949		[learning rate: 0.00037522]
	Learning Rate: 0.000375223
	LOSS [training: 0.01797359743151191 | validation: 0.021830586835234398]
	TIME [epoch: 8.16 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024796633157153823		[learning rate: 0.00037454]
		[batch 20/20] avg loss: 0.02015082720482622		[learning rate: 0.00037386]
	Learning Rate: 0.000373861
	LOSS [training: 0.02247373018099002 | validation: 0.014759211648655336]
	TIME [epoch: 8.19 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01718269919228239		[learning rate: 0.00037318]
		[batch 20/20] avg loss: 0.017048540775380053		[learning rate: 0.0003725]
	Learning Rate: 0.000372505
	LOSS [training: 0.017115619983831223 | validation: 0.02633799922468574]
	TIME [epoch: 8.17 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034388735196258916		[learning rate: 0.00037183]
		[batch 20/20] avg loss: 0.024463789447132275		[learning rate: 0.00037115]
	Learning Rate: 0.000371153
	LOSS [training: 0.029426262321695585 | validation: 0.018273607511285817]
	TIME [epoch: 8.18 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021036549073471245		[learning rate: 0.00037048]
		[batch 20/20] avg loss: 0.019429426749303723		[learning rate: 0.00036981]
	Learning Rate: 0.000369806
	LOSS [training: 0.020232987911387484 | validation: 0.050181134509691416]
	TIME [epoch: 8.18 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024252921071199675		[learning rate: 0.00036913]
		[batch 20/20] avg loss: 0.020012455429898206		[learning rate: 0.00036846]
	Learning Rate: 0.000368464
	LOSS [training: 0.02213268825054894 | validation: 0.02248817132098099]
	TIME [epoch: 8.2 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023841396476225095		[learning rate: 0.00036779]
		[batch 20/20] avg loss: 0.017853184547209223		[learning rate: 0.00036713]
	Learning Rate: 0.000367127
	LOSS [training: 0.02084729051171716 | validation: 0.018747057708507004]
	TIME [epoch: 8.17 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021399503155173145		[learning rate: 0.00036646]
		[batch 20/20] avg loss: 0.014588042291315639		[learning rate: 0.00036579]
	Learning Rate: 0.000365794
	LOSS [training: 0.017993772723244393 | validation: 0.025059155040840334]
	TIME [epoch: 8.18 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014852137990804884		[learning rate: 0.00036513]
		[batch 20/20] avg loss: 0.012766930958847424		[learning rate: 0.00036447]
	Learning Rate: 0.000364467
	LOSS [training: 0.013809534474826152 | validation: 0.01352621358219207]
	TIME [epoch: 8.17 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013031213484099352		[learning rate: 0.0003638]
		[batch 20/20] avg loss: 0.010543786694378013		[learning rate: 0.00036314]
	Learning Rate: 0.000363144
	LOSS [training: 0.011787500089238682 | validation: 0.01816293893838445]
	TIME [epoch: 8.21 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0166654207601035		[learning rate: 0.00036248]
		[batch 20/20] avg loss: 0.0293249921269615		[learning rate: 0.00036183]
	Learning Rate: 0.000361826
	LOSS [training: 0.0229952064435325 | validation: 0.02081552984934047]
	TIME [epoch: 8.17 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018583300493014203		[learning rate: 0.00036117]
		[batch 20/20] avg loss: 0.02682030149456175		[learning rate: 0.00036051]
	Learning Rate: 0.000360513
	LOSS [training: 0.022701800993787973 | validation: 0.11213887562913609]
	TIME [epoch: 8.17 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035734093824911405		[learning rate: 0.00035986]
		[batch 20/20] avg loss: 0.03129940432963378		[learning rate: 0.0003592]
	Learning Rate: 0.000359205
	LOSS [training: 0.0335167490772726 | validation: 0.011532617384318815]
	TIME [epoch: 8.17 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018854087485295136		[learning rate: 0.00035855]
		[batch 20/20] avg loss: 0.01144162121996888		[learning rate: 0.0003579]
	Learning Rate: 0.000357901
	LOSS [training: 0.015147854352632014 | validation: 0.020936538451779667]
	TIME [epoch: 8.21 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012667208481827974		[learning rate: 0.00035725]
		[batch 20/20] avg loss: 0.01521169066556451		[learning rate: 0.0003566]
	Learning Rate: 0.000356602
	LOSS [training: 0.013939449573696241 | validation: 0.027073857501236417]
	TIME [epoch: 8.16 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020610459920921226		[learning rate: 0.00035595]
		[batch 20/20] avg loss: 0.020928653034190624		[learning rate: 0.00035531]
	Learning Rate: 0.000355308
	LOSS [training: 0.020769556477555923 | validation: 0.015311628604943492]
	TIME [epoch: 8.18 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017610450305018323		[learning rate: 0.00035466]
		[batch 20/20] avg loss: 0.009341079672160881		[learning rate: 0.00035402]
	Learning Rate: 0.000354019
	LOSS [training: 0.0134757649885896 | validation: 0.021093755796472703]
	TIME [epoch: 8.18 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015415491208533933		[learning rate: 0.00035338]
		[batch 20/20] avg loss: 0.021715842902272044		[learning rate: 0.00035273]
	Learning Rate: 0.000352734
	LOSS [training: 0.01856566705540299 | validation: 0.01696412388836866]
	TIME [epoch: 8.2 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02065262720144536		[learning rate: 0.00035209]
		[batch 20/20] avg loss: 0.020658889121044362		[learning rate: 0.00035145]
	Learning Rate: 0.000351454
	LOSS [training: 0.020655758161244868 | validation: 0.02310557935291239]
	TIME [epoch: 8.16 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018142391490594958		[learning rate: 0.00035082]
		[batch 20/20] avg loss: 0.017831308204236458		[learning rate: 0.00035018]
	Learning Rate: 0.000350179
	LOSS [training: 0.017986849847415706 | validation: 0.013482543916911924]
	TIME [epoch: 8.18 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012711563866899566		[learning rate: 0.00034954]
		[batch 20/20] avg loss: 0.01628809077126627		[learning rate: 0.00034891]
	Learning Rate: 0.000348908
	LOSS [training: 0.01449982731908292 | validation: 0.02651317672110329]
	TIME [epoch: 8.18 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016244901110793134		[learning rate: 0.00034827]
		[batch 20/20] avg loss: 0.01526327823449819		[learning rate: 0.00034764]
	Learning Rate: 0.000347641
	LOSS [training: 0.015754089672645667 | validation: 0.007371528552097897]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_1024.pth
	Model improved!!!
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021034552149139612		[learning rate: 0.00034701]
		[batch 20/20] avg loss: 0.010664511040255774		[learning rate: 0.00034638]
	Learning Rate: 0.00034638
	LOSS [training: 0.015849531594697697 | validation: 0.018002462167821257]
	TIME [epoch: 8.15 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01958274108485293		[learning rate: 0.00034575]
		[batch 20/20] avg loss: 0.015548404688579187		[learning rate: 0.00034512]
	Learning Rate: 0.000345123
	LOSS [training: 0.017565572886716056 | validation: 0.019557260366234938]
	TIME [epoch: 8.16 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012950585387858943		[learning rate: 0.0003445]
		[batch 20/20] avg loss: 0.02637040935405307		[learning rate: 0.00034387]
	Learning Rate: 0.00034387
	LOSS [training: 0.019660497370956008 | validation: 0.03892486780817404]
	TIME [epoch: 8.19 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011544498808861828		[learning rate: 0.00034325]
		[batch 20/20] avg loss: 0.02012465083393509		[learning rate: 0.00034262]
	Learning Rate: 0.000342622
	LOSS [training: 0.015834574821398457 | validation: 0.018075171867456608]
	TIME [epoch: 8.19 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010626309864482587		[learning rate: 0.000342]
		[batch 20/20] avg loss: 0.021874513343034585		[learning rate: 0.00034138]
	Learning Rate: 0.000341379
	LOSS [training: 0.016250411603758588 | validation: 0.017067288026156577]
	TIME [epoch: 8.16 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01445207658995222		[learning rate: 0.00034076]
		[batch 20/20] avg loss: 0.011908515592556368		[learning rate: 0.00034014]
	Learning Rate: 0.00034014
	LOSS [training: 0.013180296091254295 | validation: 0.020439660143763496]
	TIME [epoch: 8.17 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017116513349377183		[learning rate: 0.00033952]
		[batch 20/20] avg loss: 0.02083180882954959		[learning rate: 0.00033891]
	Learning Rate: 0.000338906
	LOSS [training: 0.01897416108946339 | validation: 0.030136651366823475]
	TIME [epoch: 8.18 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021772089104626427		[learning rate: 0.00033829]
		[batch 20/20] avg loss: 0.016798675171702023		[learning rate: 0.00033768]
	Learning Rate: 0.000337676
	LOSS [training: 0.019285382138164223 | validation: 0.022175533998669335]
	TIME [epoch: 8.18 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013554421951850738		[learning rate: 0.00033706]
		[batch 20/20] avg loss: 0.01780286870899919		[learning rate: 0.00033645]
	Learning Rate: 0.00033645
	LOSS [training: 0.015678645330424963 | validation: 0.013315282112115781]
	TIME [epoch: 8.15 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017131875325090272		[learning rate: 0.00033584]
		[batch 20/20] avg loss: 0.01358030620562999		[learning rate: 0.00033523]
	Learning Rate: 0.000335229
	LOSS [training: 0.015356090765360134 | validation: 0.03251842444498058]
	TIME [epoch: 8.17 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03176688327422912		[learning rate: 0.00033462]
		[batch 20/20] avg loss: 0.010299290114796586		[learning rate: 0.00033401]
	Learning Rate: 0.000334013
	LOSS [training: 0.021033086694512853 | validation: 0.02164229805662003]
	TIME [epoch: 8.2 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014718231910172982		[learning rate: 0.00033341]
		[batch 20/20] avg loss: 0.02614943218760545		[learning rate: 0.0003328]
	Learning Rate: 0.000332801
	LOSS [training: 0.020433832048889215 | validation: 0.018157529107043418]
	TIME [epoch: 8.18 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015601035738632257		[learning rate: 0.0003322]
		[batch 20/20] avg loss: 0.0306884623503585		[learning rate: 0.00033159]
	Learning Rate: 0.000331593
	LOSS [training: 0.023144749044495375 | validation: 0.03566110702938649]
	TIME [epoch: 8.16 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029130375246583802		[learning rate: 0.00033099]
		[batch 20/20] avg loss: 0.014312424845016403		[learning rate: 0.00033039]
	Learning Rate: 0.00033039
	LOSS [training: 0.021721400045800105 | validation: 0.026878987531007845]
	TIME [epoch: 8.16 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01500501484905939		[learning rate: 0.00032979]
		[batch 20/20] avg loss: 0.020288138395285038		[learning rate: 0.00032919]
	Learning Rate: 0.000329191
	LOSS [training: 0.017646576622172214 | validation: 0.033404450113908954]
	TIME [epoch: 8.22 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03050063002940004		[learning rate: 0.00032859]
		[batch 20/20] avg loss: 0.03154687603855648		[learning rate: 0.000328]
	Learning Rate: 0.000327996
	LOSS [training: 0.031023753033978262 | validation: 0.02605808966875267]
	TIME [epoch: 8.18 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014263243618168008		[learning rate: 0.0003274]
		[batch 20/20] avg loss: 0.016670729989809875		[learning rate: 0.00032681]
	Learning Rate: 0.000326806
	LOSS [training: 0.015466986803988943 | validation: 0.01694826094991694]
	TIME [epoch: 8.16 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016786587642002297		[learning rate: 0.00032621]
		[batch 20/20] avg loss: 0.019295053857242665		[learning rate: 0.00032562]
	Learning Rate: 0.00032562
	LOSS [training: 0.01804082074962248 | validation: 0.02848484511246886]
	TIME [epoch: 8.16 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01788989870068338		[learning rate: 0.00032503]
		[batch 20/20] avg loss: 0.01883335471545158		[learning rate: 0.00032444]
	Learning Rate: 0.000324438
	LOSS [training: 0.01836162670806748 | validation: 0.01752099241021401]
	TIME [epoch: 8.2 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018909287447976614		[learning rate: 0.00032385]
		[batch 20/20] avg loss: 0.012239957873387285		[learning rate: 0.00032326]
	Learning Rate: 0.00032326
	LOSS [training: 0.015574622660681946 | validation: 0.014750368655401462]
	TIME [epoch: 8.18 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006225851400772709		[learning rate: 0.00032267]
		[batch 20/20] avg loss: 0.02471276329365085		[learning rate: 0.00032209]
	Learning Rate: 0.000322087
	LOSS [training: 0.015469307347211778 | validation: 0.026109873581981413]
	TIME [epoch: 8.15 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03055907671223402		[learning rate: 0.0003215]
		[batch 20/20] avg loss: 0.0238767451373749		[learning rate: 0.00032092]
	Learning Rate: 0.000320918
	LOSS [training: 0.027217910924804457 | validation: 0.010251794447935654]
	TIME [epoch: 8.15 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014083969503901098		[learning rate: 0.00032034]
		[batch 20/20] avg loss: 0.010071015979598277		[learning rate: 0.00031975]
	Learning Rate: 0.000319754
	LOSS [training: 0.012077492741749686 | validation: 0.014399970235409612]
	TIME [epoch: 8.21 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006097725406720755		[learning rate: 0.00031917]
		[batch 20/20] avg loss: 0.02348172738129184		[learning rate: 0.00031859]
	Learning Rate: 0.000318593
	LOSS [training: 0.014789726394006298 | validation: 0.03243082010989853]
	TIME [epoch: 8.17 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02559186621495126		[learning rate: 0.00031801]
		[batch 20/20] avg loss: 0.018398332995348994		[learning rate: 0.00031744]
	Learning Rate: 0.000317437
	LOSS [training: 0.021995099605150125 | validation: 0.021958324970662753]
	TIME [epoch: 8.15 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02080508254202689		[learning rate: 0.00031686]
		[batch 20/20] avg loss: 0.03986196326026704		[learning rate: 0.00031629]
	Learning Rate: 0.000316285
	LOSS [training: 0.030333522901146964 | validation: 0.03839830431052895]
	TIME [epoch: 8.15 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01788800531930961		[learning rate: 0.00031571]
		[batch 20/20] avg loss: 0.014182887476021814		[learning rate: 0.00031514]
	Learning Rate: 0.000315137
	LOSS [training: 0.016035446397665708 | validation: 0.009354670009044825]
	TIME [epoch: 8.23 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022099809279404328		[learning rate: 0.00031457]
		[batch 20/20] avg loss: 0.008061126260501709		[learning rate: 0.00031399]
	Learning Rate: 0.000313994
	LOSS [training: 0.015080467769953018 | validation: 0.01604506832420872]
	TIME [epoch: 8.17 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014681677898527895		[learning rate: 0.00031342]
		[batch 20/20] avg loss: 0.012320817029232739		[learning rate: 0.00031285]
	Learning Rate: 0.000312854
	LOSS [training: 0.013501247463880317 | validation: 0.02358577630308808]
	TIME [epoch: 8.15 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01700190214091678		[learning rate: 0.00031229]
		[batch 20/20] avg loss: 0.014549489390305603		[learning rate: 0.00031172]
	Learning Rate: 0.000311719
	LOSS [training: 0.01577569576561119 | validation: 0.01519470661286601]
	TIME [epoch: 8.16 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018006996916737104		[learning rate: 0.00031115]
		[batch 20/20] avg loss: 0.009827267139849164		[learning rate: 0.00031059]
	Learning Rate: 0.000310588
	LOSS [training: 0.013917132028293136 | validation: 0.021677859029724404]
	TIME [epoch: 8.22 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01460159763704908		[learning rate: 0.00031002]
		[batch 20/20] avg loss: 0.02440225666845424		[learning rate: 0.00030946]
	Learning Rate: 0.000309461
	LOSS [training: 0.019501927152751664 | validation: 0.03775906985216154]
	TIME [epoch: 8.16 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015416970384877032		[learning rate: 0.0003089]
		[batch 20/20] avg loss: 0.018558871827918457		[learning rate: 0.00030834]
	Learning Rate: 0.000308338
	LOSS [training: 0.01698792110639774 | validation: 0.021725840216524003]
	TIME [epoch: 8.16 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025301247986638426		[learning rate: 0.00030778]
		[batch 20/20] avg loss: 0.013156575148769667		[learning rate: 0.00030722]
	Learning Rate: 0.000307219
	LOSS [training: 0.01922891156770405 | validation: 0.018442774875425387]
	TIME [epoch: 8.16 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018323251886054805		[learning rate: 0.00030666]
		[batch 20/20] avg loss: 0.01830520194516475		[learning rate: 0.0003061]
	Learning Rate: 0.000306104
	LOSS [training: 0.018314226915609782 | validation: 0.007148056153033737]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_1059.pth
	Model improved!!!
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020211648182034532		[learning rate: 0.00030555]
		[batch 20/20] avg loss: 0.008864318557604634		[learning rate: 0.00030499]
	Learning Rate: 0.000304993
	LOSS [training: 0.014537983369819586 | validation: 0.027192220084112484]
	TIME [epoch: 8.16 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011589699447021238		[learning rate: 0.00030444]
		[batch 20/20] avg loss: 0.0144387576565173		[learning rate: 0.00030389]
	Learning Rate: 0.000303886
	LOSS [training: 0.013014228551769272 | validation: 0.0173200593696654]
	TIME [epoch: 8.16 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010137542434075924		[learning rate: 0.00030333]
		[batch 20/20] avg loss: 0.015074725794797017		[learning rate: 0.00030278]
	Learning Rate: 0.000302783
	LOSS [training: 0.012606134114436469 | validation: 0.017086571787168475]
	TIME [epoch: 8.17 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011217704040098789		[learning rate: 0.00030223]
		[batch 20/20] avg loss: 0.00868371782092164		[learning rate: 0.00030168]
	Learning Rate: 0.000301684
	LOSS [training: 0.009950710930510214 | validation: 0.011802315177932217]
	TIME [epoch: 8.23 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024538406825439486		[learning rate: 0.00030114]
		[batch 20/20] avg loss: 0.017964310487692137		[learning rate: 0.00030059]
	Learning Rate: 0.000300589
	LOSS [training: 0.021251358656565814 | validation: 0.024739806908163672]
	TIME [epoch: 8.16 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010399038132612493		[learning rate: 0.00030004]
		[batch 20/20] avg loss: 0.02942278981853048		[learning rate: 0.0002995]
	Learning Rate: 0.000299499
	LOSS [training: 0.01991091397557149 | validation: 0.07140178455355455]
	TIME [epoch: 8.16 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03433641027242007		[learning rate: 0.00029895]
		[batch 20/20] avg loss: 0.03408747041268153		[learning rate: 0.00029841]
	Learning Rate: 0.000298412
	LOSS [training: 0.0342119403425508 | validation: 0.03348097495288586]
	TIME [epoch: 8.18 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014037535126780196		[learning rate: 0.00029787]
		[batch 20/20] avg loss: 0.018223054115862287		[learning rate: 0.00029733]
	Learning Rate: 0.000297329
	LOSS [training: 0.01613029462132124 | validation: 0.01076951567122292]
	TIME [epoch: 8.2 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01410341759801816		[learning rate: 0.00029679]
		[batch 20/20] avg loss: 0.011515439222092772		[learning rate: 0.00029625]
	Learning Rate: 0.00029625
	LOSS [training: 0.012809428410055465 | validation: 0.022988744202299016]
	TIME [epoch: 8.16 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016673202078788175		[learning rate: 0.00029571]
		[batch 20/20] avg loss: 0.01318947263142502		[learning rate: 0.00029517]
	Learning Rate: 0.000295175
	LOSS [training: 0.014931337355106595 | validation: 0.024975597179243456]
	TIME [epoch: 8.16 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009496959526869134		[learning rate: 0.00029464]
		[batch 20/20] avg loss: 0.012270127140017406		[learning rate: 0.0002941]
	Learning Rate: 0.000294103
	LOSS [training: 0.010883543333443272 | validation: 0.017311764269213707]
	TIME [epoch: 8.18 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014440897200526099		[learning rate: 0.00029357]
		[batch 20/20] avg loss: 0.013360410863593058		[learning rate: 0.00029304]
	Learning Rate: 0.000293036
	LOSS [training: 0.013900654032059579 | validation: 0.014196932383136136]
	TIME [epoch: 8.21 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0232482622664691		[learning rate: 0.0002925]
		[batch 20/20] avg loss: 0.015586563371927264		[learning rate: 0.00029197]
	Learning Rate: 0.000291973
	LOSS [training: 0.019417412819198183 | validation: 0.013010691590241878]
	TIME [epoch: 8.16 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013796488145307498		[learning rate: 0.00029144]
		[batch 20/20] avg loss: 0.0054941837886674245		[learning rate: 0.00029091]
	Learning Rate: 0.000290913
	LOSS [training: 0.009645335966987462 | validation: 0.024701336039182033]
	TIME [epoch: 8.15 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01627244671930258		[learning rate: 0.00029038]
		[batch 20/20] avg loss: 0.017058706737974043		[learning rate: 0.00028986]
	Learning Rate: 0.000289857
	LOSS [training: 0.01666557672863831 | validation: 0.013771620520432478]
	TIME [epoch: 8.18 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01642305016180837		[learning rate: 0.00028933]
		[batch 20/20] avg loss: 0.014508692623382294		[learning rate: 0.00028881]
	Learning Rate: 0.000288805
	LOSS [training: 0.015465871392595334 | validation: 0.022337408636087082]
	TIME [epoch: 8.2 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0109635823453996		[learning rate: 0.00028828]
		[batch 20/20] avg loss: 0.011325419146745059		[learning rate: 0.00028776]
	Learning Rate: 0.000287757
	LOSS [training: 0.011144500746072327 | validation: 0.018328157106153715]
	TIME [epoch: 8.17 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017188198707813095		[learning rate: 0.00028723]
		[batch 20/20] avg loss: 0.007759370706034106		[learning rate: 0.00028671]
	Learning Rate: 0.000286713
	LOSS [training: 0.012473784706923601 | validation: 0.02158705868585173]
	TIME [epoch: 8.16 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024241634949940652		[learning rate: 0.00028619]
		[batch 20/20] avg loss: 0.008354463668396973		[learning rate: 0.00028567]
	Learning Rate: 0.000285672
	LOSS [training: 0.01629804930916881 | validation: 0.019243621672901837]
	TIME [epoch: 8.19 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012422687693810832		[learning rate: 0.00028515]
		[batch 20/20] avg loss: 0.020691083631216592		[learning rate: 0.00028464]
	Learning Rate: 0.000284636
	LOSS [training: 0.01655688566251371 | validation: 0.044919980590479656]
	TIME [epoch: 8.2 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031658084282225815		[learning rate: 0.00028412]
		[batch 20/20] avg loss: 0.008078678962047624		[learning rate: 0.0002836]
	Learning Rate: 0.000283603
	LOSS [training: 0.019868381622136715 | validation: 0.021080448205895563]
	TIME [epoch: 8.16 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02039399438712137		[learning rate: 0.00028309]
		[batch 20/20] avg loss: 0.00928083061635172		[learning rate: 0.00028257]
	Learning Rate: 0.000282574
	LOSS [training: 0.01483741250173654 | validation: 0.019106031612013713]
	TIME [epoch: 8.16 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00736556738761774		[learning rate: 0.00028206]
		[batch 20/20] avg loss: 0.012047806727798639		[learning rate: 0.00028155]
	Learning Rate: 0.000281548
	LOSS [training: 0.009706687057708187 | validation: 0.012883102659986856]
	TIME [epoch: 8.19 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018286479676408368		[learning rate: 0.00028104]
		[batch 20/20] avg loss: 0.010073057770392137		[learning rate: 0.00028053]
	Learning Rate: 0.000280526
	LOSS [training: 0.014179768723400252 | validation: 0.0169917036276991]
	TIME [epoch: 8.2 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019256790122635255		[learning rate: 0.00028002]
		[batch 20/20] avg loss: 0.022635068143528437		[learning rate: 0.00027951]
	Learning Rate: 0.000279508
	LOSS [training: 0.020945929133081846 | validation: 0.015774474004449975]
	TIME [epoch: 8.15 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016614502301840486		[learning rate: 0.000279]
		[batch 20/20] avg loss: 0.02136157838842452		[learning rate: 0.00027849]
	Learning Rate: 0.000278494
	LOSS [training: 0.018988040345132497 | validation: 0.013428973037524869]
	TIME [epoch: 8.16 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018314542812426327		[learning rate: 0.00027799]
		[batch 20/20] avg loss: 0.015999088518559936		[learning rate: 0.00027748]
	Learning Rate: 0.000277483
	LOSS [training: 0.01715681566549313 | validation: 0.023755078257403757]
	TIME [epoch: 8.18 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01658200604946116		[learning rate: 0.00027698]
		[batch 20/20] avg loss: 0.022673397807334864		[learning rate: 0.00027648]
	Learning Rate: 0.000276476
	LOSS [training: 0.019627701928398012 | validation: 0.02028077618709408]
	TIME [epoch: 8.18 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01372995536002603		[learning rate: 0.00027597]
		[batch 20/20] avg loss: 0.0200604053391575		[learning rate: 0.00027547]
	Learning Rate: 0.000275473
	LOSS [training: 0.016895180349591764 | validation: 0.019390405454422464]
	TIME [epoch: 8.16 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01866217180998459		[learning rate: 0.00027497]
		[batch 20/20] avg loss: 0.019534546834957686		[learning rate: 0.00027447]
	Learning Rate: 0.000274473
	LOSS [training: 0.01909835932247114 | validation: 0.01210988391955907]
	TIME [epoch: 8.15 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014070483645833045		[learning rate: 0.00027397]
		[batch 20/20] avg loss: 0.016796414012449197		[learning rate: 0.00027348]
	Learning Rate: 0.000273477
	LOSS [training: 0.01543344882914112 | validation: 0.009712716895559605]
	TIME [epoch: 8.19 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0160068437747264		[learning rate: 0.00027298]
		[batch 20/20] avg loss: 0.01087483910697557		[learning rate: 0.00027248]
	Learning Rate: 0.000272485
	LOSS [training: 0.013440841440850984 | validation: 0.006264738611972594]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_1091.pth
	Model improved!!!
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014397521059449808		[learning rate: 0.00027199]
		[batch 20/20] avg loss: 0.016699693685074315		[learning rate: 0.0002715]
	Learning Rate: 0.000271496
	LOSS [training: 0.01554860737226206 | validation: 0.01654710407509659]
	TIME [epoch: 8.16 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017414248522569564		[learning rate: 0.000271]
		[batch 20/20] avg loss: 0.01584037859730413		[learning rate: 0.00027051]
	Learning Rate: 0.000270511
	LOSS [training: 0.016627313559936845 | validation: 0.01237915163761987]
	TIME [epoch: 8.15 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007576379341890152		[learning rate: 0.00027002]
		[batch 20/20] avg loss: 0.013449258961988744		[learning rate: 0.00026953]
	Learning Rate: 0.000269529
	LOSS [training: 0.01051281915193945 | validation: 0.014467426662328649]
	TIME [epoch: 8.18 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013213826449461636		[learning rate: 0.00026904]
		[batch 20/20] avg loss: 0.014000114110414524		[learning rate: 0.00026855]
	Learning Rate: 0.000268551
	LOSS [training: 0.01360697027993808 | validation: 0.011698854834430991]
	TIME [epoch: 8.16 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011545529425443575		[learning rate: 0.00026806]
		[batch 20/20] avg loss: 0.012880249730871828		[learning rate: 0.00026758]
	Learning Rate: 0.000267576
	LOSS [training: 0.012212889578157703 | validation: 0.017221173733625764]
	TIME [epoch: 8.15 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01583616229410756		[learning rate: 0.00026709]
		[batch 20/20] avg loss: 0.012678478002761073		[learning rate: 0.00026661]
	Learning Rate: 0.000266605
	LOSS [training: 0.014257320148434319 | validation: 0.01623149299846399]
	TIME [epoch: 8.14 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011897511365635336		[learning rate: 0.00026612]
		[batch 20/20] avg loss: 0.00629212952916672		[learning rate: 0.00026564]
	Learning Rate: 0.000265638
	LOSS [training: 0.009094820447401026 | validation: 0.019999981209409008]
	TIME [epoch: 8.18 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007883078787570894		[learning rate: 0.00026516]
		[batch 20/20] avg loss: 0.03048098805663731		[learning rate: 0.00026467]
	Learning Rate: 0.000264674
	LOSS [training: 0.019182033422104103 | validation: 0.022220028881537013]
	TIME [epoch: 8.16 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01742465251028576		[learning rate: 0.00026419]
		[batch 20/20] avg loss: 0.019212211785548904		[learning rate: 0.00026371]
	Learning Rate: 0.000263713
	LOSS [training: 0.01831843214791733 | validation: 0.026547445449112552]
	TIME [epoch: 8.16 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021380402484620274		[learning rate: 0.00026323]
		[batch 20/20] avg loss: 0.016400003950808862		[learning rate: 0.00026276]
	Learning Rate: 0.000262756
	LOSS [training: 0.01889020321771457 | validation: 0.011771755162987209]
	TIME [epoch: 8.15 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017684835243939677		[learning rate: 0.00026228]
		[batch 20/20] avg loss: 0.014966334848089102		[learning rate: 0.0002618]
	Learning Rate: 0.000261802
	LOSS [training: 0.016325585046014387 | validation: 0.024314211821491617]
	TIME [epoch: 8.19 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010325518297581885		[learning rate: 0.00026133]
		[batch 20/20] avg loss: 0.014564062714020599		[learning rate: 0.00026085]
	Learning Rate: 0.000260852
	LOSS [training: 0.01244479050580124 | validation: 0.017915636238756596]
	TIME [epoch: 8.16 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016085812611946273		[learning rate: 0.00026038]
		[batch 20/20] avg loss: 0.019423707813519455		[learning rate: 0.00025991]
	Learning Rate: 0.000259906
	LOSS [training: 0.01775476021273286 | validation: 0.005547780137690289]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_1104.pth
	Model improved!!!
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015771705787161163		[learning rate: 0.00025943]
		[batch 20/20] avg loss: 0.014998829848086878		[learning rate: 0.00025896]
	Learning Rate: 0.000258962
	LOSS [training: 0.015385267817624022 | validation: 0.018633097158736293]
	TIME [epoch: 8.18 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009406157290210573		[learning rate: 0.00025849]
		[batch 20/20] avg loss: 0.010180434021356257		[learning rate: 0.00025802]
	Learning Rate: 0.000258023
	LOSS [training: 0.009793295655783412 | validation: 0.01877726143974021]
	TIME [epoch: 8.22 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02289797913368729		[learning rate: 0.00025755]
		[batch 20/20] avg loss: 0.013457169467415032		[learning rate: 0.00025709]
	Learning Rate: 0.000257086
	LOSS [training: 0.018177574300551162 | validation: 0.018299555342796182]
	TIME [epoch: 8.2 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013893843373955442		[learning rate: 0.00025662]
		[batch 20/20] avg loss: 0.011937907698373246		[learning rate: 0.00025615]
	Learning Rate: 0.000256153
	LOSS [training: 0.012915875536164342 | validation: 0.010364706585931681]
	TIME [epoch: 8.18 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007396749179119122		[learning rate: 0.00025569]
		[batch 20/20] avg loss: 0.0202194984590818		[learning rate: 0.00025522]
	Learning Rate: 0.000255224
	LOSS [training: 0.01380812381910046 | validation: 0.010558551319885779]
	TIME [epoch: 8.18 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008410517604278888		[learning rate: 0.00025476]
		[batch 20/20] avg loss: 0.016879293791010698		[learning rate: 0.0002543]
	Learning Rate: 0.000254298
	LOSS [training: 0.012644905697644792 | validation: 0.014914702631343825]
	TIME [epoch: 8.2 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011255073045365577		[learning rate: 0.00025384]
		[batch 20/20] avg loss: 0.012338539182687033		[learning rate: 0.00025337]
	Learning Rate: 0.000253375
	LOSS [training: 0.011796806114026306 | validation: 0.0029807037239987927]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_1111.pth
	Model improved!!!
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007805077660162178		[learning rate: 0.00025291]
		[batch 20/20] avg loss: 0.015898063354913467		[learning rate: 0.00025246]
	Learning Rate: 0.000252455
	LOSS [training: 0.011851570507537823 | validation: 0.03388937061511149]
	TIME [epoch: 8.18 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03216966600235359		[learning rate: 0.000252]
		[batch 20/20] avg loss: 0.01104756242444101		[learning rate: 0.00025154]
	Learning Rate: 0.000251539
	LOSS [training: 0.021608614213397298 | validation: 0.005598706754324733]
	TIME [epoch: 8.19 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00864580198285839		[learning rate: 0.00025108]
		[batch 20/20] avg loss: 0.011145188979815064		[learning rate: 0.00025063]
	Learning Rate: 0.000250626
	LOSS [training: 0.009895495481336728 | validation: 0.0181771316009166]
	TIME [epoch: 8.2 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005852403322699271		[learning rate: 0.00025017]
		[batch 20/20] avg loss: 0.01795523048276417		[learning rate: 0.00024972]
	Learning Rate: 0.000249717
	LOSS [training: 0.011903816902731722 | validation: 0.013052708378421584]
	TIME [epoch: 8.19 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013221685487121307		[learning rate: 0.00024926]
		[batch 20/20] avg loss: 0.011462024450824006		[learning rate: 0.00024881]
	Learning Rate: 0.00024881
	LOSS [training: 0.012341854968972658 | validation: 0.01654201981596513]
	TIME [epoch: 8.17 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018629924961065518		[learning rate: 0.00024836]
		[batch 20/20] avg loss: 0.01204977524641018		[learning rate: 0.00024791]
	Learning Rate: 0.000247907
	LOSS [training: 0.01533985010373785 | validation: 0.01025181955123022]
	TIME [epoch: 8.18 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012191384160962345		[learning rate: 0.00024746]
		[batch 20/20] avg loss: 0.017694058477552697		[learning rate: 0.00024701]
	Learning Rate: 0.000247008
	LOSS [training: 0.014942721319257524 | validation: 0.01554976418870435]
	TIME [epoch: 8.19 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01766109314481489		[learning rate: 0.00024656]
		[batch 20/20] avg loss: 0.011579110520538382		[learning rate: 0.00024611]
	Learning Rate: 0.000246111
	LOSS [training: 0.014620101832676629 | validation: 0.012150961788282913]
	TIME [epoch: 8.17 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01717154857307705		[learning rate: 0.00024566]
		[batch 20/20] avg loss: 0.011925770825274726		[learning rate: 0.00024522]
	Learning Rate: 0.000245218
	LOSS [training: 0.01454865969917589 | validation: 0.014267570274616496]
	TIME [epoch: 8.17 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015838513827496216		[learning rate: 0.00024477]
		[batch 20/20] avg loss: 0.010945644398182488		[learning rate: 0.00024433]
	Learning Rate: 0.000244328
	LOSS [training: 0.01339207911283935 | validation: 0.023048119286241828]
	TIME [epoch: 8.19 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01400151208866435		[learning rate: 0.00024388]
		[batch 20/20] avg loss: 0.011210560864746495		[learning rate: 0.00024344]
	Learning Rate: 0.000243442
	LOSS [training: 0.012606036476705421 | validation: 0.011617430092421898]
	TIME [epoch: 8.19 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016090279669872344		[learning rate: 0.000243]
		[batch 20/20] avg loss: 0.013630250963044923		[learning rate: 0.00024256]
	Learning Rate: 0.000242558
	LOSS [training: 0.014860265316458631 | validation: 0.007388638188442871]
	TIME [epoch: 8.18 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009523208418064304		[learning rate: 0.00024212]
		[batch 20/20] avg loss: 0.00809484291047192		[learning rate: 0.00024168]
	Learning Rate: 0.000241678
	LOSS [training: 0.00880902566426811 | validation: 0.013389109092883975]
	TIME [epoch: 8.18 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009616504235763813		[learning rate: 0.00024124]
		[batch 20/20] avg loss: 0.014661999386993113		[learning rate: 0.0002408]
	Learning Rate: 0.000240801
	LOSS [training: 0.012139251811378463 | validation: 0.012712590600386384]
	TIME [epoch: 8.2 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011323821929063407		[learning rate: 0.00024036]
		[batch 20/20] avg loss: 0.010505180108348522		[learning rate: 0.00023993]
	Learning Rate: 0.000239927
	LOSS [training: 0.010914501018705964 | validation: 0.01592838674037763]
	TIME [epoch: 8.18 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010801296958967394		[learning rate: 0.00023949]
		[batch 20/20] avg loss: 0.01577871543935792		[learning rate: 0.00023906]
	Learning Rate: 0.000239056
	LOSS [training: 0.01329000619916266 | validation: 0.006563219658062557]
	TIME [epoch: 8.17 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014468368887426239		[learning rate: 0.00023862]
		[batch 20/20] avg loss: 0.0161206154292781		[learning rate: 0.00023819]
	Learning Rate: 0.000238189
	LOSS [training: 0.015294492158352168 | validation: 0.02234449285464326]
	TIME [epoch: 8.18 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01162412424910948		[learning rate: 0.00023776]
		[batch 20/20] avg loss: 0.00941942959418111		[learning rate: 0.00023732]
	Learning Rate: 0.000237324
	LOSS [training: 0.010521776921645293 | validation: 0.008196899521091736]
	TIME [epoch: 8.2 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014655399336280859		[learning rate: 0.00023689]
		[batch 20/20] avg loss: 0.016876803706816434		[learning rate: 0.00023646]
	Learning Rate: 0.000236463
	LOSS [training: 0.01576610152154865 | validation: 0.01708892628585294]
	TIME [epoch: 8.17 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015477942444606374		[learning rate: 0.00023603]
		[batch 20/20] avg loss: 0.015458751053147137		[learning rate: 0.0002356]
	Learning Rate: 0.000235605
	LOSS [training: 0.015468346748876757 | validation: 0.02283257293584353]
	TIME [epoch: 8.16 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013645162208983727		[learning rate: 0.00023518]
		[batch 20/20] avg loss: 0.007163772367763355		[learning rate: 0.00023475]
	Learning Rate: 0.00023475
	LOSS [training: 0.010404467288373541 | validation: 0.008149480009158085]
	TIME [epoch: 8.19 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023915001186152664		[learning rate: 0.00023432]
		[batch 20/20] avg loss: 0.010593939192667988		[learning rate: 0.0002339]
	Learning Rate: 0.000233898
	LOSS [training: 0.017254470189410325 | validation: 0.01555333336916474]
	TIME [epoch: 8.2 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008018033538737018		[learning rate: 0.00023347]
		[batch 20/20] avg loss: 0.013981095088957373		[learning rate: 0.00023305]
	Learning Rate: 0.000233049
	LOSS [training: 0.010999564313847196 | validation: 0.009005371346529088]
	TIME [epoch: 8.17 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013781527964148785		[learning rate: 0.00023263]
		[batch 20/20] avg loss: 0.008435963111085402		[learning rate: 0.0002322]
	Learning Rate: 0.000232203
	LOSS [training: 0.011108745537617093 | validation: 0.009809543620863746]
	TIME [epoch: 8.16 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013354358719572537		[learning rate: 0.00023178]
		[batch 20/20] avg loss: 0.009560537687322086		[learning rate: 0.00023136]
	Learning Rate: 0.000231361
	LOSS [training: 0.011457448203447311 | validation: 0.017620126241073987]
	TIME [epoch: 8.2 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012579055405087991		[learning rate: 0.00023094]
		[batch 20/20] avg loss: 0.012149260553965199		[learning rate: 0.00023052]
	Learning Rate: 0.000230521
	LOSS [training: 0.012364157979526592 | validation: 0.018646151151802768]
	TIME [epoch: 8.21 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009520767804675015		[learning rate: 0.0002301]
		[batch 20/20] avg loss: 0.01056302240277171		[learning rate: 0.00022968]
	Learning Rate: 0.000229685
	LOSS [training: 0.010041895103723364 | validation: 0.01045735205583559]
	TIME [epoch: 8.17 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008438749407322888		[learning rate: 0.00022927]
		[batch 20/20] avg loss: 0.011624236323337771		[learning rate: 0.00022885]
	Learning Rate: 0.000228851
	LOSS [training: 0.01003149286533033 | validation: 0.017664242226055442]
	TIME [epoch: 8.17 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016209608215516876		[learning rate: 0.00022844]
		[batch 20/20] avg loss: 0.015199925983359563		[learning rate: 0.00022802]
	Learning Rate: 0.00022802
	LOSS [training: 0.01570476709943822 | validation: 0.026725902468615337]
	TIME [epoch: 8.19 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012508787363984678		[learning rate: 0.00022761]
		[batch 20/20] avg loss: 0.01183428681204334		[learning rate: 0.00022719]
	Learning Rate: 0.000227193
	LOSS [training: 0.012171537088014012 | validation: 0.007523937216619234]
	TIME [epoch: 8.21 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01335363039215388		[learning rate: 0.00022678]
		[batch 20/20] avg loss: 0.008395214017637905		[learning rate: 0.00022637]
	Learning Rate: 0.000226368
	LOSS [training: 0.010874422204895892 | validation: 0.014651728008611226]
	TIME [epoch: 8.16 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02016932957184189		[learning rate: 0.00022596]
		[batch 20/20] avg loss: 0.017586246838091363		[learning rate: 0.00022555]
	Learning Rate: 0.000225547
	LOSS [training: 0.018877788204966627 | validation: 0.007450042652433462]
	TIME [epoch: 8.16 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01239956343382736		[learning rate: 0.00022514]
		[batch 20/20] avg loss: 0.020765902818262585		[learning rate: 0.00022473]
	Learning Rate: 0.000224728
	LOSS [training: 0.016582733126044972 | validation: 0.0172275720506999]
	TIME [epoch: 8.2 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016974104474359476		[learning rate: 0.00022432]
		[batch 20/20] avg loss: 0.008073872002523835		[learning rate: 0.00022391]
	Learning Rate: 0.000223913
	LOSS [training: 0.012523988238441655 | validation: 0.04435507850547822]
	TIME [epoch: 8.2 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009170796398829643		[learning rate: 0.00022351]
		[batch 20/20] avg loss: 0.005568644218590291		[learning rate: 0.0002231]
	Learning Rate: 0.0002231
	LOSS [training: 0.007369720308709966 | validation: 0.005702123489654973]
	TIME [epoch: 8.16 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0057560378680409415		[learning rate: 0.0002227]
		[batch 20/20] avg loss: 0.015225417672533801		[learning rate: 0.00022229]
	Learning Rate: 0.000222291
	LOSS [training: 0.01049072777028737 | validation: 0.006898768877871233]
	TIME [epoch: 8.16 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011519546421499837		[learning rate: 0.00022189]
		[batch 20/20] avg loss: 0.008935141452425393		[learning rate: 0.00022148]
	Learning Rate: 0.000221484
	LOSS [training: 0.010227343936962615 | validation: 0.019323824081515333]
	TIME [epoch: 8.2 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009229177993355299		[learning rate: 0.00022108]
		[batch 20/20] avg loss: 0.014137371328444883		[learning rate: 0.00022068]
	Learning Rate: 0.00022068
	LOSS [training: 0.01168327466090009 | validation: 0.006185711154624647]
	TIME [epoch: 8.2 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031683582132965805		[learning rate: 0.00022028]
		[batch 20/20] avg loss: 0.016972205224261818		[learning rate: 0.00021988]
	Learning Rate: 0.000219879
	LOSS [training: 0.0100702817187792 | validation: 0.002454417621561469]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_1150.pth
	Model improved!!!
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017874272585971514		[learning rate: 0.00021948]
		[batch 20/20] avg loss: 0.006754966848102444		[learning rate: 0.00021908]
	Learning Rate: 0.000219081
	LOSS [training: 0.012314619717036977 | validation: 0.008423700630948282]
	TIME [epoch: 8.18 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013216182566853257		[learning rate: 0.00021868]
		[batch 20/20] avg loss: 0.01272569513517555		[learning rate: 0.00021829]
	Learning Rate: 0.000218286
	LOSS [training: 0.012970938851014408 | validation: 0.015435971242505023]
	TIME [epoch: 8.22 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012018133578051527		[learning rate: 0.00021789]
		[batch 20/20] avg loss: 0.018133537604130456		[learning rate: 0.00021749]
	Learning Rate: 0.000217494
	LOSS [training: 0.015075835591090991 | validation: 0.01189250518416769]
	TIME [epoch: 8.19 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011376276385369035		[learning rate: 0.0002171]
		[batch 20/20] avg loss: 0.011549148202521686		[learning rate: 0.0002167]
	Learning Rate: 0.000216705
	LOSS [training: 0.01146271229394536 | validation: 0.013654629355761666]
	TIME [epoch: 8.18 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010705341309106076		[learning rate: 0.00021631]
		[batch 20/20] avg loss: 0.006885314002525292		[learning rate: 0.00021592]
	Learning Rate: 0.000215918
	LOSS [training: 0.008795327655815685 | validation: 0.006180686529944761]
	TIME [epoch: 8.17 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026706193664492585		[learning rate: 0.00021553]
		[batch 20/20] avg loss: 0.012214270147025886		[learning rate: 0.00021513]
	Learning Rate: 0.000215135
	LOSS [training: 0.007442444756737572 | validation: 0.009216027048897103]
	TIME [epoch: 8.21 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019214022559972763		[learning rate: 0.00021474]
		[batch 20/20] avg loss: 0.011611746402520046		[learning rate: 0.00021435]
	Learning Rate: 0.000214354
	LOSS [training: 0.015412884481246406 | validation: 0.019443100539936285]
	TIME [epoch: 8.2 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02397455858100849		[learning rate: 0.00021396]
		[batch 20/20] avg loss: 0.019340003681856316		[learning rate: 0.00021358]
	Learning Rate: 0.000213576
	LOSS [training: 0.021657281131432407 | validation: 0.010605147712271942]
	TIME [epoch: 8.17 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006377791863454443		[learning rate: 0.00021319]
		[batch 20/20] avg loss: 0.017214920329577332		[learning rate: 0.0002128]
	Learning Rate: 0.000212801
	LOSS [training: 0.011796356096515886 | validation: 0.006033460875851879]
	TIME [epoch: 8.17 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010139887350901194		[learning rate: 0.00021241]
		[batch 20/20] avg loss: 0.015443434407331345		[learning rate: 0.00021203]
	Learning Rate: 0.000212029
	LOSS [training: 0.012791660879116267 | validation: 0.015759346596055927]
	TIME [epoch: 8.21 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009640888275399249		[learning rate: 0.00021164]
		[batch 20/20] avg loss: 0.007822903750035448		[learning rate: 0.00021126]
	Learning Rate: 0.000211259
	LOSS [training: 0.00873189601271735 | validation: 0.011460373247042112]
	TIME [epoch: 8.19 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008639206081575696		[learning rate: 0.00021088]
		[batch 20/20] avg loss: 0.013869094917433539		[learning rate: 0.00021049]
	Learning Rate: 0.000210493
	LOSS [training: 0.011254150499504617 | validation: 0.005907489904466481]
	TIME [epoch: 8.16 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004853936914576237		[learning rate: 0.00021011]
		[batch 20/20] avg loss: 0.014137283293045005		[learning rate: 0.00020973]
	Learning Rate: 0.000209729
	LOSS [training: 0.007311338492251314 | validation: 0.013939448461141361]
	TIME [epoch: 8.17 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014682921276097646		[learning rate: 0.00020935]
		[batch 20/20] avg loss: 0.01614822482536238		[learning rate: 0.00020897]
	Learning Rate: 0.000208968
	LOSS [training: 0.01541557305073001 | validation: 0.014607531607303853]
	TIME [epoch: 8.22 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020836131497194054		[learning rate: 0.00020859]
		[batch 20/20] avg loss: 0.005408178542570019		[learning rate: 0.00020821]
	Learning Rate: 0.000208209
	LOSS [training: 0.013122155019882037 | validation: 0.0061518011833281774]
	TIME [epoch: 8.19 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007973811442660681		[learning rate: 0.00020783]
		[batch 20/20] avg loss: 0.014902639304525215		[learning rate: 0.00020745]
	Learning Rate: 0.000207454
	LOSS [training: 0.011438225373592948 | validation: 0.020850118247566156]
	TIME [epoch: 8.17 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012255516850874771		[learning rate: 0.00020708]
		[batch 20/20] avg loss: 0.01108301927400843		[learning rate: 0.0002067]
	Learning Rate: 0.000206701
	LOSS [training: 0.0116692680624416 | validation: 0.00912693118708266]
	TIME [epoch: 8.17 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0062636256635945865		[learning rate: 0.00020633]
		[batch 20/20] avg loss: 0.021420828172222562		[learning rate: 0.00020595]
	Learning Rate: 0.000205951
	LOSS [training: 0.013842226917908573 | validation: 0.03404107058134358]
	TIME [epoch: 8.23 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016923840390826287		[learning rate: 0.00020558]
		[batch 20/20] avg loss: 0.02599717044330181		[learning rate: 0.0002052]
	Learning Rate: 0.000205203
	LOSS [training: 0.021460505417064048 | validation: 0.014199332788107677]
	TIME [epoch: 8.19 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009301324450804976		[learning rate: 0.00020483]
		[batch 20/20] avg loss: 0.015771655557246363		[learning rate: 0.00020446]
	Learning Rate: 0.000204459
	LOSS [training: 0.01253649000402567 | validation: 0.023752390074997694]
	TIME [epoch: 8.16 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008902694674180853		[learning rate: 0.00020409]
		[batch 20/20] avg loss: 0.009490068908402979		[learning rate: 0.00020372]
	Learning Rate: 0.000203717
	LOSS [training: 0.009196381791291917 | validation: 0.008770052699909004]
	TIME [epoch: 8.17 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013011766973766253		[learning rate: 0.00020335]
		[batch 20/20] avg loss: 0.009041380720245445		[learning rate: 0.00020298]
	Learning Rate: 0.000202977
	LOSS [training: 0.01102657384700585 | validation: 0.009296838084507692]
	TIME [epoch: 8.22 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010072728863023744		[learning rate: 0.00020261]
		[batch 20/20] avg loss: 0.010624316135323598		[learning rate: 0.00020224]
	Learning Rate: 0.000202241
	LOSS [training: 0.01034852249917367 | validation: 0.007574252500932997]
	TIME [epoch: 8.19 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019680354325917998		[learning rate: 0.00020187]
		[batch 20/20] avg loss: 0.009111592662224232		[learning rate: 0.00020151]
	Learning Rate: 0.000201507
	LOSS [training: 0.014395973494071113 | validation: 0.007660736594263887]
	TIME [epoch: 8.16 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014397044013169788		[learning rate: 0.00020114]
		[batch 20/20] avg loss: 0.015167752601012396		[learning rate: 0.00020078]
	Learning Rate: 0.000200775
	LOSS [training: 0.014782398307091094 | validation: 0.014611906132597066]
	TIME [epoch: 8.17 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010511741403494668		[learning rate: 0.00020041]
		[batch 20/20] avg loss: 0.010945529405702339		[learning rate: 0.00020005]
	Learning Rate: 0.000200047
	LOSS [training: 0.010728635404598504 | validation: 0.007558222711983412]
	TIME [epoch: 8.23 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004647336652462409		[learning rate: 0.00019968]
		[batch 20/20] avg loss: 0.012890065896801453		[learning rate: 0.00019932]
	Learning Rate: 0.000199321
	LOSS [training: 0.00876870127463193 | validation: 0.012677963781450322]
	TIME [epoch: 8.18 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01068595691904016		[learning rate: 0.00019896]
		[batch 20/20] avg loss: 0.007202506982524691		[learning rate: 0.0001986]
	Learning Rate: 0.000198597
	LOSS [training: 0.008944231950782425 | validation: 0.017295056410497847]
	TIME [epoch: 8.17 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006581652156495911		[learning rate: 0.00019824]
		[batch 20/20] avg loss: 0.008658868561815075		[learning rate: 0.00019788]
	Learning Rate: 0.000197877
	LOSS [training: 0.007620260359155494 | validation: 0.004519441101836322]
	TIME [epoch: 8.16 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008061014618445692		[learning rate: 0.00019752]
		[batch 20/20] avg loss: 0.009866953689764445		[learning rate: 0.00019716]
	Learning Rate: 0.000197159
	LOSS [training: 0.008963984154105068 | validation: 0.00589798243700355]
	TIME [epoch: 8.23 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011547533024240332		[learning rate: 0.0001968]
		[batch 20/20] avg loss: 0.009759252372195031		[learning rate: 0.00019644]
	Learning Rate: 0.000196443
	LOSS [training: 0.010653392698217681 | validation: 0.010290064909630896]
	TIME [epoch: 8.17 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006709958278322875		[learning rate: 0.00019609]
		[batch 20/20] avg loss: 0.009426366109317203		[learning rate: 0.00019573]
	Learning Rate: 0.00019573
	LOSS [training: 0.00806816219382004 | validation: 0.023069827310246302]
	TIME [epoch: 8.17 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006379579402841918		[learning rate: 0.00019537]
		[batch 20/20] avg loss: 0.01041080425391003		[learning rate: 0.00019502]
	Learning Rate: 0.00019502
	LOSS [training: 0.008395191828375972 | validation: 0.007298163606425049]
	TIME [epoch: 8.16 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011528507854732003		[learning rate: 0.00019467]
		[batch 20/20] avg loss: 0.006459393295754333		[learning rate: 0.00019431]
	Learning Rate: 0.000194312
	LOSS [training: 0.00899395057524317 | validation: 0.007466175578491059]
	TIME [epoch: 8.23 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00710747938925022		[learning rate: 0.00019396]
		[batch 20/20] avg loss: 0.010294111015923464		[learning rate: 0.00019361]
	Learning Rate: 0.000193607
	LOSS [training: 0.008700795202586844 | validation: 0.01532448025522808]
	TIME [epoch: 8.17 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010450173530034735		[learning rate: 0.00019326]
		[batch 20/20] avg loss: 0.004217440159584926		[learning rate: 0.0001929]
	Learning Rate: 0.000192904
	LOSS [training: 0.0073338068448098315 | validation: 0.011084846339701061]
	TIME [epoch: 8.16 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013618304559487513		[learning rate: 0.00019255]
		[batch 20/20] avg loss: 0.013807918375577821		[learning rate: 0.0001922]
	Learning Rate: 0.000192204
	LOSS [training: 0.007584874415763288 | validation: 0.015143207061520308]
	TIME [epoch: 8.17 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01639416192212287		[learning rate: 0.00019186]
		[batch 20/20] avg loss: 0.011009298874943367		[learning rate: 0.00019151]
	Learning Rate: 0.000191507
	LOSS [training: 0.013701730398533115 | validation: 0.011784270312877298]
	TIME [epoch: 8.23 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005321786411128575		[learning rate: 0.00019116]
		[batch 20/20] avg loss: 0.010165141064110355		[learning rate: 0.00019081]
	Learning Rate: 0.000190812
	LOSS [training: 0.007743463737619466 | validation: 0.007226221939596081]
	TIME [epoch: 8.19 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006227549591520503		[learning rate: 0.00019047]
		[batch 20/20] avg loss: 0.014628319338594226		[learning rate: 0.00019012]
	Learning Rate: 0.000190119
	LOSS [training: 0.010427934465057364 | validation: 0.012430356597824993]
	TIME [epoch: 8.16 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005514489346825499		[learning rate: 0.00018977]
		[batch 20/20] avg loss: 0.014783461447092023		[learning rate: 0.00018943]
	Learning Rate: 0.000189429
	LOSS [training: 0.010148975396958762 | validation: 0.0083047209618437]
	TIME [epoch: 8.18 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01402519235255445		[learning rate: 0.00018909]
		[batch 20/20] avg loss: 0.011625482725162116		[learning rate: 0.00018874]
	Learning Rate: 0.000188742
	LOSS [training: 0.012825337538858284 | validation: 0.008203722646806577]
	TIME [epoch: 8.21 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009604801358740976		[learning rate: 0.0001884]
		[batch 20/20] avg loss: 0.012655392865551668		[learning rate: 0.00018806]
	Learning Rate: 0.000188057
	LOSS [training: 0.011130097112146322 | validation: 0.018467035447296356]
	TIME [epoch: 8.18 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0053282554560445715		[learning rate: 0.00018772]
		[batch 20/20] avg loss: 0.01074375552139989		[learning rate: 0.00018737]
	Learning Rate: 0.000187375
	LOSS [training: 0.008036005488722232 | validation: 0.006711654985250577]
	TIME [epoch: 8.16 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025225095096517193		[learning rate: 0.00018703]
		[batch 20/20] avg loss: 0.012969580277319273		[learning rate: 0.00018669]
	Learning Rate: 0.000186695
	LOSS [training: 0.007746044893485496 | validation: -0.006322278950814159]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_1195.pth
	Model improved!!!
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00954079515091986		[learning rate: 0.00018636]
		[batch 20/20] avg loss: 0.011492151294065165		[learning rate: 0.00018602]
	Learning Rate: 0.000186017
	LOSS [training: 0.010516473222492512 | validation: 0.011664482571215226]
	TIME [epoch: 8.2 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011714922086482615		[learning rate: 0.00018568]
		[batch 20/20] avg loss: 0.009486400355908595		[learning rate: 0.00018534]
	Learning Rate: 0.000185342
	LOSS [training: 0.010600661221195604 | validation: 0.016942070263106134]
	TIME [epoch: 8.16 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005067275063373556		[learning rate: 0.00018501]
		[batch 20/20] avg loss: 0.014478164721449522		[learning rate: 0.00018467]
	Learning Rate: 0.000184669
	LOSS [training: 0.009772719892411538 | validation: 0.027087952666918276]
	TIME [epoch: 8.15 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011763652101427984		[learning rate: 0.00018433]
		[batch 20/20] avg loss: 0.015858848908870703		[learning rate: 0.000184]
	Learning Rate: 0.000183999
	LOSS [training: 0.013811250505149345 | validation: 0.022718952474047222]
	TIME [epoch: 8.17 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008748370131129727		[learning rate: 0.00018367]
		[batch 20/20] avg loss: 0.015959456589860803		[learning rate: 0.00018333]
	Learning Rate: 0.000183331
	LOSS [training: 0.012353913360495263 | validation: 0.0028479817657850477]
	TIME [epoch: 8.19 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015951381838398355		[learning rate: 0.000183]
		[batch 20/20] avg loss: 0.013777958526157142		[learning rate: 0.00018267]
	Learning Rate: 0.000182666
	LOSS [training: 0.014864670182277748 | validation: 0.026339718750169244]
	TIME [epoch: 8.17 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01674263399102193		[learning rate: 0.00018233]
		[batch 20/20] avg loss: 0.010249480899512324		[learning rate: 0.000182]
	Learning Rate: 0.000182003
	LOSS [training: 0.01349605744526713 | validation: 0.008232875826766746]
	TIME [epoch: 8.15 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00748412738620235		[learning rate: 0.00018167]
		[batch 20/20] avg loss: 0.016427190938745298		[learning rate: 0.00018134]
	Learning Rate: 0.000181343
	LOSS [training: 0.011955659162473823 | validation: 0.01646077713643519]
	TIME [epoch: 8.18 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014597384583754583		[learning rate: 0.00018101]
		[batch 20/20] avg loss: 0.009298252596704432		[learning rate: 0.00018068]
	Learning Rate: 0.000180685
	LOSS [training: 0.011947818590229507 | validation: 0.015639933489912577]
	TIME [epoch: 8.19 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01357337857745849		[learning rate: 0.00018036]
		[batch 20/20] avg loss: 0.0035171165226489318		[learning rate: 0.00018003]
	Learning Rate: 0.000180029
	LOSS [training: 0.008545247550053712 | validation: 0.00766613226268608]
	TIME [epoch: 8.17 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008082387070062518		[learning rate: 0.0001797]
		[batch 20/20] avg loss: 0.012331003264131358		[learning rate: 0.00017938]
	Learning Rate: 0.000179376
	LOSS [training: 0.010206695167096937 | validation: 0.011154682660893804]
	TIME [epoch: 8.15 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004452264351518692		[learning rate: 0.00017905]
		[batch 20/20] avg loss: 0.015381027905923175		[learning rate: 0.00017872]
	Learning Rate: 0.000178725
	LOSS [training: 0.009916646128720934 | validation: 0.015586131558421045]
	TIME [epoch: 8.19 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007762449826803687		[learning rate: 0.0001784]
		[batch 20/20] avg loss: 0.011128624301877487		[learning rate: 0.00017808]
	Learning Rate: 0.000178076
	LOSS [training: 0.009445537064340587 | validation: 0.010728407915985852]
	TIME [epoch: 8.17 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013073006059871276		[learning rate: 0.00017775]
		[batch 20/20] avg loss: 0.013077511588500823		[learning rate: 0.00017743]
	Learning Rate: 0.00017743
	LOSS [training: 0.01307525882418605 | validation: 0.006704850277084666]
	TIME [epoch: 8.18 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011663160205104047		[learning rate: 0.00017711]
		[batch 20/20] avg loss: 0.01058588386826824		[learning rate: 0.00017679]
	Learning Rate: 0.000176786
	LOSS [training: 0.011124522036686143 | validation: 0.004907509622130059]
	TIME [epoch: 8.16 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0174646557667425		[learning rate: 0.00017646]
		[batch 20/20] avg loss: 0.02476526021604606		[learning rate: 0.00017614]
	Learning Rate: 0.000176144
	LOSS [training: 0.021114957991394283 | validation: 0.022479030007638883]
	TIME [epoch: 8.19 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00963595083641891		[learning rate: 0.00017582]
		[batch 20/20] avg loss: 0.007069296441385514		[learning rate: 0.0001755]
	Learning Rate: 0.000175505
	LOSS [training: 0.008352623638902214 | validation: 0.009691148293621697]
	TIME [epoch: 8.17 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013760630993418838		[learning rate: 0.00017519]
		[batch 20/20] avg loss: 0.008788203797812486		[learning rate: 0.00017487]
	Learning Rate: 0.000174868
	LOSS [training: 0.011274417395615658 | validation: 0.009249556045794735]
	TIME [epoch: 8.18 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011286989291150695		[learning rate: 0.00017455]
		[batch 20/20] avg loss: 0.010373694891846114		[learning rate: 0.00017423]
	Learning Rate: 0.000174233
	LOSS [training: 0.010830342091498404 | validation: 0.013118893817841029]
	TIME [epoch: 8.15 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005780304823550794		[learning rate: 0.00017392]
		[batch 20/20] avg loss: 0.010574935448513803		[learning rate: 0.0001736]
	Learning Rate: 0.000173601
	LOSS [training: 0.008177620136032297 | validation: 0.006291731450821567]
	TIME [epoch: 8.19 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0124889709885152		[learning rate: 0.00017329]
		[batch 20/20] avg loss: 0.012672572370662851		[learning rate: 0.00017297]
	Learning Rate: 0.000172971
	LOSS [training: 0.012580771679589025 | validation: 0.006627502841509198]
	TIME [epoch: 8.16 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008591084525034232		[learning rate: 0.00017266]
		[batch 20/20] avg loss: 0.0079312987550472		[learning rate: 0.00017234]
	Learning Rate: 0.000172343
	LOSS [training: 0.008261191640040714 | validation: 0.031935223083053604]
	TIME [epoch: 8.18 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013849911861863805		[learning rate: 0.00017203]
		[batch 20/20] avg loss: 0.00841320725111459		[learning rate: 0.00017172]
	Learning Rate: 0.000171718
	LOSS [training: 0.011131559556489201 | validation: 0.023313657038145523]
	TIME [epoch: 8.15 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012834809458448615		[learning rate: 0.00017141]
		[batch 20/20] avg loss: 0.009073130029727604		[learning rate: 0.00017109]
	Learning Rate: 0.000171095
	LOSS [training: 0.01095396974408811 | validation: 0.01263596669451864]
	TIME [epoch: 8.18 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005886710528808295		[learning rate: 0.00017078]
		[batch 20/20] avg loss: 0.0098799630611069		[learning rate: 0.00017047]
	Learning Rate: 0.000170474
	LOSS [training: 0.0078833367949576 | validation: 0.019494006831081063]
	TIME [epoch: 8.17 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008008864716244216		[learning rate: 0.00017016]
		[batch 20/20] avg loss: 0.011252635705830377		[learning rate: 0.00016986]
	Learning Rate: 0.000169855
	LOSS [training: 0.009630750211037296 | validation: 0.013678194053634136]
	TIME [epoch: 8.18 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016481003358210622		[learning rate: 0.00016955]
		[batch 20/20] avg loss: 0.012138288179890499		[learning rate: 0.00016924]
	Learning Rate: 0.000169239
	LOSS [training: 0.01430964576905056 | validation: 0.019542219286433526]
	TIME [epoch: 8.15 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01933486510376932		[learning rate: 0.00016893]
		[batch 20/20] avg loss: 0.011972945556780563		[learning rate: 0.00016862]
	Learning Rate: 0.000168625
	LOSS [training: 0.015653905330274942 | validation: 0.005895859716798552]
	TIME [epoch: 8.19 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009753666980753384		[learning rate: 0.00016832]
		[batch 20/20] avg loss: 0.011424444976783701		[learning rate: 0.00016801]
	Learning Rate: 0.000168013
	LOSS [training: 0.010589055978768542 | validation: 0.019979421995526134]
	TIME [epoch: 8.17 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020916415951847754		[learning rate: 0.00016771]
		[batch 20/20] avg loss: 0.007789274793765721		[learning rate: 0.0001674]
	Learning Rate: 0.000167403
	LOSS [training: 0.014352845372806736 | validation: 0.006490455917115985]
	TIME [epoch: 8.18 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005952399414253679		[learning rate: 0.0001671]
		[batch 20/20] avg loss: 0.005119082715416152		[learning rate: 0.0001668]
	Learning Rate: 0.000166795
	LOSS [training: 0.0055357410648349155 | validation: 0.02539100042223954]
	TIME [epoch: 8.15 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02196758696927608		[learning rate: 0.00016649]
		[batch 20/20] avg loss: 0.011625509603396833		[learning rate: 0.00016619]
	Learning Rate: 0.00016619
	LOSS [training: 0.016796548286336456 | validation: 0.009648313955196967]
	TIME [epoch: 8.19 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006857423225810834		[learning rate: 0.00016589]
		[batch 20/20] avg loss: 0.015500688321021878		[learning rate: 0.00016559]
	Learning Rate: 0.000165587
	LOSS [training: 0.011179055773416357 | validation: 0.019131819898397275]
	TIME [epoch: 8.16 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0191012116125631		[learning rate: 0.00016529]
		[batch 20/20] avg loss: 0.007981728080662002		[learning rate: 0.00016499]
	Learning Rate: 0.000164986
	LOSS [training: 0.01354146984661255 | validation: 0.007618096494401937]
	TIME [epoch: 8.18 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004324917916540172		[learning rate: 0.00016469]
		[batch 20/20] avg loss: 0.017307167222783852		[learning rate: 0.00016439]
	Learning Rate: 0.000164387
	LOSS [training: 0.010816042569662015 | validation: 0.010789997987502008]
	TIME [epoch: 8.15 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013995158885461647		[learning rate: 0.00016409]
		[batch 20/20] avg loss: 0.008106444296070018		[learning rate: 0.00016379]
	Learning Rate: 0.000163791
	LOSS [training: 0.011050801590765833 | validation: 0.011230351659169961]
	TIME [epoch: 8.2 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012487774824260434		[learning rate: 0.00016349]
		[batch 20/20] avg loss: 0.014742304452978744		[learning rate: 0.0001632]
	Learning Rate: 0.000163196
	LOSS [training: 0.013615039638619592 | validation: 0.012618130398555425]
	TIME [epoch: 8.15 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01108930543448228		[learning rate: 0.0001629]
		[batch 20/20] avg loss: 0.011458219168958195		[learning rate: 0.0001626]
	Learning Rate: 0.000162604
	LOSS [training: 0.011273762301720237 | validation: 0.0016274516671554622]
	TIME [epoch: 8.18 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006478674586719302		[learning rate: 0.00016231]
		[batch 20/20] avg loss: 0.0070799902718477565		[learning rate: 0.00016201]
	Learning Rate: 0.000162014
	LOSS [training: 0.00677933242928353 | validation: 0.009623653016881121]
	TIME [epoch: 8.16 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009160349250397912		[learning rate: 0.00016172]
		[batch 20/20] avg loss: 0.009689295933712297		[learning rate: 0.00016143]
	Learning Rate: 0.000161426
	LOSS [training: 0.009424822592055102 | validation: 0.006061360976165558]
	TIME [epoch: 8.2 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01378422410634834		[learning rate: 0.00016113]
		[batch 20/20] avg loss: 0.00721237479446536		[learning rate: 0.00016084]
	Learning Rate: 0.00016084
	LOSS [training: 0.01049829945040685 | validation: 0.01001976822635426]
	TIME [epoch: 8.16 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008148580468802803		[learning rate: 0.00016055]
		[batch 20/20] avg loss: 0.016234253188892038		[learning rate: 0.00016026]
	Learning Rate: 0.000160257
	LOSS [training: 0.012191416828847421 | validation: 0.008473165854475781]
	TIME [epoch: 8.17 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007270460289963523		[learning rate: 0.00015997]
		[batch 20/20] avg loss: 0.018504315999340548		[learning rate: 0.00015967]
	Learning Rate: 0.000159675
	LOSS [training: 0.012887388144652036 | validation: 0.0199217162505569]
	TIME [epoch: 8.17 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010582333231013397		[learning rate: 0.00015938]
		[batch 20/20] avg loss: 0.011717151598548702		[learning rate: 0.0001591]
	Learning Rate: 0.000159096
	LOSS [training: 0.011149742414781047 | validation: 0.016279624069327415]
	TIME [epoch: 8.2 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015648488454392644		[learning rate: 0.00015881]
		[batch 20/20] avg loss: 0.007436812398460488		[learning rate: 0.00015852]
	Learning Rate: 0.000158518
	LOSS [training: 0.011542650426426567 | validation: 0.007677192676414178]
	TIME [epoch: 8.16 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007630933654322575		[learning rate: 0.00015823]
		[batch 20/20] avg loss: 0.008999649409888077		[learning rate: 0.00015794]
	Learning Rate: 0.000157943
	LOSS [training: 0.008315291532105326 | validation: 0.009789551453177003]
	TIME [epoch: 8.18 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014789125822674127		[learning rate: 0.00015766]
		[batch 20/20] avg loss: 0.007987251173199865		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.011388188497936994 | validation: 0.015604224947812892]
	TIME [epoch: 8.17 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015231730011318024		[learning rate: 0.00015708]
		[batch 20/20] avg loss: 0.010667781424292895		[learning rate: 0.0001568]
	Learning Rate: 0.000156799
	LOSS [training: 0.012949755717805458 | validation: 0.01337619076353149]
	TIME [epoch: 8.19 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007862732459484436		[learning rate: 0.00015651]
		[batch 20/20] avg loss: 0.007001998024541181		[learning rate: 0.00015623]
	Learning Rate: 0.00015623
	LOSS [training: 0.007432365242012807 | validation: 0.0023825531801817063]
	TIME [epoch: 8.16 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009269712824637016		[learning rate: 0.00015595]
		[batch 20/20] avg loss: 0.012155089554719046		[learning rate: 0.00015566]
	Learning Rate: 0.000155663
	LOSS [training: 0.01071240118967803 | validation: 0.0034856825940162055]
	TIME [epoch: 8.18 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011357240709895067		[learning rate: 0.00015538]
		[batch 20/20] avg loss: 0.0036829402033520956		[learning rate: 0.0001551]
	Learning Rate: 0.000155098
	LOSS [training: 0.007520090456623582 | validation: 0.011204076258423836]
	TIME [epoch: 8.18 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00992592822540358		[learning rate: 0.00015482]
		[batch 20/20] avg loss: 0.007089788061596336		[learning rate: 0.00015453]
	Learning Rate: 0.000154535
	LOSS [training: 0.008507858143499959 | validation: 0.009353233497243076]
	TIME [epoch: 8.19 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009211297585823508		[learning rate: 0.00015425]
		[batch 20/20] avg loss: 0.006562249733724766		[learning rate: 0.00015397]
	Learning Rate: 0.000153974
	LOSS [training: 0.007886773659774134 | validation: 0.007465224727939939]
	TIME [epoch: 8.16 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002178809153716651		[learning rate: 0.00015369]
		[batch 20/20] avg loss: 0.017690444434195877		[learning rate: 0.00015342]
	Learning Rate: 0.000153415
	LOSS [training: 0.009934626793956264 | validation: 0.016806565246233442]
	TIME [epoch: 8.18 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01010258547564809		[learning rate: 0.00015314]
		[batch 20/20] avg loss: 0.017658749246044075		[learning rate: 0.00015286]
	Learning Rate: 0.000152858
	LOSS [training: 0.013880667360846085 | validation: 0.014224870842330333]
	TIME [epoch: 8.17 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014338568693056424		[learning rate: 0.00015258]
		[batch 20/20] avg loss: 0.00926671555568		[learning rate: 0.0001523]
	Learning Rate: 0.000152304
	LOSS [training: 0.011802642124368214 | validation: 0.014521853796733488]
	TIME [epoch: 8.18 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007381957117051643		[learning rate: 0.00015203]
		[batch 20/20] avg loss: 0.012519708795029885		[learning rate: 0.00015175]
	Learning Rate: 0.000151751
	LOSS [training: 0.009950832956040765 | validation: 0.010196114084913699]
	TIME [epoch: 8.16 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009375355903555701		[learning rate: 0.00015148]
		[batch 20/20] avg loss: 0.0029297232200914216		[learning rate: 0.0001512]
	Learning Rate: 0.0001512
	LOSS [training: 0.006152539561823561 | validation: 0.005296186583259793]
	TIME [epoch: 8.18 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004365332753596932		[learning rate: 0.00015093]
		[batch 20/20] avg loss: 0.0019830933755348155		[learning rate: 0.00015065]
	Learning Rate: 0.000150652
	LOSS [training: 0.0031742130645658743 | validation: 0.005342103287824093]
	TIME [epoch: 8.18 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00966685446848308		[learning rate: 0.00015038]
		[batch 20/20] avg loss: 0.0125738703491081		[learning rate: 0.0001501]
	Learning Rate: 0.000150105
	LOSS [training: 0.011120362408795588 | validation: 0.016566636750576756]
	TIME [epoch: 8.18 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007722855852512113		[learning rate: 0.00014983]
		[batch 20/20] avg loss: 0.0051018338082401225		[learning rate: 0.00014956]
	Learning Rate: 0.00014956
	LOSS [training: 0.006412344830376118 | validation: 0.009294505037707247]
	TIME [epoch: 8.15 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00837159084213946		[learning rate: 0.00014929]
		[batch 20/20] avg loss: 0.007526182744943044		[learning rate: 0.00014902]
	Learning Rate: 0.000149017
	LOSS [training: 0.007948886793541252 | validation: 0.007530291665839183]
	TIME [epoch: 8.17 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010940503138338532		[learning rate: 0.00014875]
		[batch 20/20] avg loss: 0.008376239501824177		[learning rate: 0.00014848]
	Learning Rate: 0.000148477
	LOSS [training: 0.009658371320081356 | validation: 0.012548043840791038]
	TIME [epoch: 8.19 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011453766768792771		[learning rate: 0.00014821]
		[batch 20/20] avg loss: 0.012849908296186851		[learning rate: 0.00014794]
	Learning Rate: 0.000147938
	LOSS [training: 0.012151837532489813 | validation: 0.010241482488170816]
	TIME [epoch: 8.17 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010811208017936226		[learning rate: 0.00014767]
		[batch 20/20] avg loss: 0.004950071646393331		[learning rate: 0.0001474]
	Learning Rate: 0.000147401
	LOSS [training: 0.00788063983216478 | validation: 0.009491747787404123]
	TIME [epoch: 8.16 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013845654234814186		[learning rate: 0.00014713]
		[batch 20/20] avg loss: 0.004657430506859553		[learning rate: 0.00014687]
	Learning Rate: 0.000146866
	LOSS [training: 0.009251542370836873 | validation: 0.006471546978296947]
	TIME [epoch: 8.17 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009152711375316201		[learning rate: 0.0001466]
		[batch 20/20] avg loss: 0.009142565340808203		[learning rate: 0.00014633]
	Learning Rate: 0.000146333
	LOSS [training: 0.009147638358062201 | validation: 0.01030916173630474]
	TIME [epoch: 8.18 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01133137842985792		[learning rate: 0.00014607]
		[batch 20/20] avg loss: 0.03137233994754211		[learning rate: 0.0001458]
	Learning Rate: 0.000145802
	LOSS [training: 0.02135185918870002 | validation: 0.029184239757123018]
	TIME [epoch: 8.17 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009702524669257024		[learning rate: 0.00014554]
		[batch 20/20] avg loss: 0.010671285098764471		[learning rate: 0.00014527]
	Learning Rate: 0.000145273
	LOSS [training: 0.010186904884010748 | validation: 0.013139812127622275]
	TIME [epoch: 8.16 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038561342739627893		[learning rate: 0.00014501]
		[batch 20/20] avg loss: 0.007342718254413076		[learning rate: 0.00014475]
	Learning Rate: 0.000144746
	LOSS [training: 0.005599426264187933 | validation: 0.00881988417850778]
	TIME [epoch: 8.17 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01222898444224007		[learning rate: 0.00014448]
		[batch 20/20] avg loss: 0.012253432061516425		[learning rate: 0.00014422]
	Learning Rate: 0.00014422
	LOSS [training: 0.012241208251878249 | validation: 0.020814569921762074]
	TIME [epoch: 8.2 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01111121119278818		[learning rate: 0.00014396]
		[batch 20/20] avg loss: 0.004587316030934362		[learning rate: 0.0001437]
	Learning Rate: 0.000143697
	LOSS [training: 0.00784926361186127 | validation: 0.009207179538503888]
	TIME [epoch: 8.17 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0065160168035490375		[learning rate: 0.00014344]
		[batch 20/20] avg loss: 0.015721875792135645		[learning rate: 0.00014318]
	Learning Rate: 0.000143175
	LOSS [training: 0.011118946297842342 | validation: 0.010018361807488543]
	TIME [epoch: 8.16 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005409942069830534		[learning rate: 0.00014292]
		[batch 20/20] avg loss: 0.006652774113497161		[learning rate: 0.00014266]
	Learning Rate: 0.000142656
	LOSS [training: 0.006031358091663846 | validation: 0.010852457263734969]
	TIME [epoch: 8.17 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01053299217400114		[learning rate: 0.0001424]
		[batch 20/20] avg loss: 0.005097137828780902		[learning rate: 0.00014214]
	Learning Rate: 0.000142138
	LOSS [training: 0.00781506500139102 | validation: 0.013199124663099485]
	TIME [epoch: 8.21 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007794000736449491		[learning rate: 0.00014188]
		[batch 20/20] avg loss: 0.015565190403494394		[learning rate: 0.00014162]
	Learning Rate: 0.000141622
	LOSS [training: 0.011679595569971942 | validation: 0.007992818024740379]
	TIME [epoch: 8.17 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009923100399751509		[learning rate: 0.00014137]
		[batch 20/20] avg loss: 0.0034203874761646845		[learning rate: 0.00014111]
	Learning Rate: 0.000141108
	LOSS [training: 0.006671743937958096 | validation: 0.016124585856377337]
	TIME [epoch: 8.15 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033126482496500355		[learning rate: 0.00014085]
		[batch 20/20] avg loss: 0.007295996945338297		[learning rate: 0.0001406]
	Learning Rate: 0.000140596
	LOSS [training: 0.005304322597494166 | validation: 0.007504416636238296]
	TIME [epoch: 8.17 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005899521237481725		[learning rate: 0.00014034]
		[batch 20/20] avg loss: 0.010348831745305542		[learning rate: 0.00014009]
	Learning Rate: 0.000140086
	LOSS [training: 0.008124176491393634 | validation: 0.0054796054412838]
	TIME [epoch: 8.2 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011004711776090739		[learning rate: 0.00013983]
		[batch 20/20] avg loss: 0.002552307216281822		[learning rate: 0.00013958]
	Learning Rate: 0.000139578
	LOSS [training: 0.006778509496186283 | validation: 0.013114178877849061]
	TIME [epoch: 8.16 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009555793619364318		[learning rate: 0.00013932]
		[batch 20/20] avg loss: 0.013528438673312474		[learning rate: 0.00013907]
	Learning Rate: 0.000139071
	LOSS [training: 0.011542116146338394 | validation: 0.007754979237340566]
	TIME [epoch: 8.15 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012814405550392502		[learning rate: 0.00013882]
		[batch 20/20] avg loss: 0.012029117466992683		[learning rate: 0.00013857]
	Learning Rate: 0.000138566
	LOSS [training: 0.006655279011015964 | validation: 0.006907135647894078]
	TIME [epoch: 8.17 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004949190558966182		[learning rate: 0.00013831]
		[batch 20/20] avg loss: 0.01240820135160341		[learning rate: 0.00013806]
	Learning Rate: 0.000138064
	LOSS [training: 0.008678695955284798 | validation: 0.004974829175782964]
	TIME [epoch: 8.21 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00458947965058974		[learning rate: 0.00013781]
		[batch 20/20] avg loss: 0.00509004628594583		[learning rate: 0.00013756]
	Learning Rate: 0.000137562
	LOSS [training: 0.004839762968267785 | validation: 0.012712799565353658]
	TIME [epoch: 8.16 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006657013165966211		[learning rate: 0.00013731]
		[batch 20/20] avg loss: 0.010528537144119485		[learning rate: 0.00013706]
	Learning Rate: 0.000137063
	LOSS [training: 0.008592775155042847 | validation: 0.008000533711662894]
	TIME [epoch: 8.15 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011176987645732047		[learning rate: 0.00013681]
		[batch 20/20] avg loss: 0.007240652701112471		[learning rate: 0.00013657]
	Learning Rate: 0.000136566
	LOSS [training: 0.009208820173422259 | validation: 0.006157472052964822]
	TIME [epoch: 8.15 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005804714675486381		[learning rate: 0.00013632]
		[batch 20/20] avg loss: 0.015404249834037117		[learning rate: 0.00013607]
	Learning Rate: 0.00013607
	LOSS [training: 0.007992360650792878 | validation: 0.019126122404340005]
	TIME [epoch: 8.22 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008229089479711538		[learning rate: 0.00013582]
		[batch 20/20] avg loss: 0.009178110764041762		[learning rate: 0.00013558]
	Learning Rate: 0.000135576
	LOSS [training: 0.00870360012187665 | validation: 0.0058690824683800295]
	TIME [epoch: 8.16 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005263241520412956		[learning rate: 0.00013533]
		[batch 20/20] avg loss: 0.012375736459291702		[learning rate: 0.00013508]
	Learning Rate: 0.000135084
	LOSS [training: 0.008819488989852329 | validation: 0.01407565243989658]
	TIME [epoch: 8.16 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013599001624258145		[learning rate: 0.00013484]
		[batch 20/20] avg loss: 0.008655671473330977		[learning rate: 0.00013459]
	Learning Rate: 0.000134594
	LOSS [training: 0.01112733654879456 | validation: 0.0056882022342270785]
	TIME [epoch: 8.15 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012105168395416942		[learning rate: 0.00013435]
		[batch 20/20] avg loss: -0.0016544037170484513		[learning rate: 0.00013411]
	Learning Rate: 0.000134106
	LOSS [training: 0.005225382339184245 | validation: 0.008400973959758137]
	TIME [epoch: 8.21 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007511795915389964		[learning rate: 0.00013386]
		[batch 20/20] avg loss: 0.005257943541758675		[learning rate: 0.00013362]
	Learning Rate: 0.000133619
	LOSS [training: 0.0063848697285743185 | validation: 0.011057851066644916]
	TIME [epoch: 8.15 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01718282842695061		[learning rate: 0.00013338]
		[batch 20/20] avg loss: 0.0013881812792177091		[learning rate: 0.00013313]
	Learning Rate: 0.000133134
	LOSS [training: 0.009285504853084162 | validation: 0.006277125240674071]
	TIME [epoch: 8.14 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009219449786132464		[learning rate: 0.00013289]
		[batch 20/20] avg loss: 0.007924977967788047		[learning rate: 0.00013265]
	Learning Rate: 0.000132651
	LOSS [training: 0.008572213876960255 | validation: 0.009127129370876228]
	TIME [epoch: 8.15 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008250997520733864		[learning rate: 0.00013241]
		[batch 20/20] avg loss: 0.006399512696735608		[learning rate: 0.00013217]
	Learning Rate: 0.00013217
	LOSS [training: 0.007325255108734737 | validation: 0.005055509209448412]
	TIME [epoch: 8.22 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009855092669121917		[learning rate: 0.00013193]
		[batch 20/20] avg loss: 0.005562751349007719		[learning rate: 0.00013169]
	Learning Rate: 0.00013169
	LOSS [training: 0.007708922009064818 | validation: 0.006142558473060904]
	TIME [epoch: 8.15 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016344110590192196		[learning rate: 0.00013145]
		[batch 20/20] avg loss: 0.0050400533396086885		[learning rate: 0.00013121]
	Learning Rate: 0.000131212
	LOSS [training: 0.010692081964900443 | validation: 0.01592350955543507]
	TIME [epoch: 8.15 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008965444507392801		[learning rate: 0.00013097]
		[batch 20/20] avg loss: 0.009521281987943334		[learning rate: 0.00013074]
	Learning Rate: 0.000130736
	LOSS [training: 0.00924336324766807 | validation: 0.011871354480038019]
	TIME [epoch: 8.15 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008986449280802241		[learning rate: 0.0001305]
		[batch 20/20] avg loss: 0.005587954553657229		[learning rate: 0.00013026]
	Learning Rate: 0.000130261
	LOSS [training: 0.007287201917229735 | validation: 0.00871337379211197]
	TIME [epoch: 8.21 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008229197902182071		[learning rate: 0.00013002]
		[batch 20/20] avg loss: 0.007965548438112		[learning rate: 0.00012979]
	Learning Rate: 0.000129789
	LOSS [training: 0.008097373170147035 | validation: 0.008016545097589107]
	TIME [epoch: 8.15 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0072162407723061995		[learning rate: 0.00012955]
		[batch 20/20] avg loss: 0.010421814738154627		[learning rate: 0.00012932]
	Learning Rate: 0.000129318
	LOSS [training: 0.008819027755230415 | validation: 0.007012816498458235]
	TIME [epoch: 8.15 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005402808872142922		[learning rate: 0.00012908]
		[batch 20/20] avg loss: 0.009170716438022228		[learning rate: 0.00012885]
	Learning Rate: 0.000128848
	LOSS [training: 0.0072867626550825754 | validation: 0.007659555653028803]
	TIME [epoch: 8.16 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007328641797949838		[learning rate: 0.00012861]
		[batch 20/20] avg loss: 0.008785030637434134		[learning rate: 0.00012838]
	Learning Rate: 0.000128381
	LOSS [training: 0.008056836217691985 | validation: 0.007482746331480935]
	TIME [epoch: 8.22 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012917767999181456		[learning rate: 0.00012815]
		[batch 20/20] avg loss: 0.006494311828259806		[learning rate: 0.00012791]
	Learning Rate: 0.000127915
	LOSS [training: 0.009706039913720632 | validation: 0.005952750572975945]
	TIME [epoch: 8.15 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007827963504155416		[learning rate: 0.00012768]
		[batch 20/20] avg loss: 0.004858592970301333		[learning rate: 0.00012745]
	Learning Rate: 0.000127451
	LOSS [training: 0.006343278237228377 | validation: 0.0077358105994191064]
	TIME [epoch: 8.14 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007745718958912201		[learning rate: 0.00012722]
		[batch 20/20] avg loss: 0.010158478563771136		[learning rate: 0.00012699]
	Learning Rate: 0.000126988
	LOSS [training: 0.008952098761341665 | validation: 0.013781094171077406]
	TIME [epoch: 8.15 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00941039734471532		[learning rate: 0.00012676]
		[batch 20/20] avg loss: 0.009006210211615382		[learning rate: 0.00012653]
	Learning Rate: 0.000126527
	LOSS [training: 0.009208303778165351 | validation: 0.009742335426009448]
	TIME [epoch: 8.22 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00724911860869099		[learning rate: 0.0001263]
		[batch 20/20] avg loss: 0.010642572601183032		[learning rate: 0.00012607]
	Learning Rate: 0.000126068
	LOSS [training: 0.008945845604937011 | validation: 0.012387410215241163]
	TIME [epoch: 8.16 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009504409532636145		[learning rate: 0.00012584]
		[batch 20/20] avg loss: 0.0030892720871076645		[learning rate: 0.00012561]
	Learning Rate: 0.000125611
	LOSS [training: 0.006296840809871905 | validation: 0.004420983568504054]
	TIME [epoch: 8.16 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010281726750435597		[learning rate: 0.00012538]
		[batch 20/20] avg loss: 0.013249465211892169		[learning rate: 0.00012515]
	Learning Rate: 0.000125155
	LOSS [training: 0.011765595981163883 | validation: 0.014283404716742687]
	TIME [epoch: 8.16 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009252883431613537		[learning rate: 0.00012493]
		[batch 20/20] avg loss: 0.0107083829860718		[learning rate: 0.0001247]
	Learning Rate: 0.000124701
	LOSS [training: 0.00998063320884267 | validation: 0.010499425377358793]
	TIME [epoch: 8.22 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013705180619914844		[learning rate: 0.00012447]
		[batch 20/20] avg loss: 0.006650871032279971		[learning rate: 0.00012425]
	Learning Rate: 0.000124248
	LOSS [training: 0.01017802582609741 | validation: 0.004989610762786081]
	TIME [epoch: 8.15 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006374693926680294		[learning rate: 0.00012402]
		[batch 20/20] avg loss: 0.00409539867212771		[learning rate: 0.0001238]
	Learning Rate: 0.000123797
	LOSS [training: 0.005235046299404002 | validation: 0.009551994664169505]
	TIME [epoch: 8.15 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007716326637473137		[learning rate: 0.00012357]
		[batch 20/20] avg loss: 0.003891879015287344		[learning rate: 0.00012335]
	Learning Rate: 0.000123348
	LOSS [training: 0.00580410282638024 | validation: -0.003337092453395589]
	TIME [epoch: 8.15 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008344055419737368		[learning rate: 0.00012312]
		[batch 20/20] avg loss: 0.0038833086756126594		[learning rate: 0.0001229]
	Learning Rate: 0.0001229
	LOSS [training: 0.006113682047675014 | validation: 0.010671387214028485]
	TIME [epoch: 8.21 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006358352921835647		[learning rate: 0.00012268]
		[batch 20/20] avg loss: 0.00925863055091918		[learning rate: 0.00012245]
	Learning Rate: 0.000122454
	LOSS [training: 0.0078084917363774155 | validation: 0.008562623082400654]
	TIME [epoch: 8.16 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009888967579456369		[learning rate: 0.00012223]
		[batch 20/20] avg loss: 0.004287688424628498		[learning rate: 0.00012201]
	Learning Rate: 0.00012201
	LOSS [training: 0.007088328002042435 | validation: 0.02172076177079882]
	TIME [epoch: 8.16 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01635850767692435		[learning rate: 0.00012179]
		[batch 20/20] avg loss: 0.0006758311585715993		[learning rate: 0.00012157]
	Learning Rate: 0.000121567
	LOSS [training: 0.008517169417747973 | validation: 0.01904245966383105]
	TIME [epoch: 8.18 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005217057663616455		[learning rate: 0.00012135]
		[batch 20/20] avg loss: 0.011647560598890005		[learning rate: 0.00012113]
	Learning Rate: 0.000121126
	LOSS [training: 0.00843230913125323 | validation: 0.015361712287230789]
	TIME [epoch: 8.2 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011143930029132519		[learning rate: 0.00012091]
		[batch 20/20] avg loss: 0.0034581692025504054		[learning rate: 0.00012069]
	Learning Rate: 0.000120686
	LOSS [training: 0.007301049615841461 | validation: 0.014356506591378158]
	TIME [epoch: 8.15 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0048931498628016		[learning rate: 0.00012047]
		[batch 20/20] avg loss: 0.009067901090621689		[learning rate: 0.00012025]
	Learning Rate: 0.000120248
	LOSS [training: 0.006980525476711644 | validation: 0.0007392333603384234]
	TIME [epoch: 8.15 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002817117884959279		[learning rate: 0.00012003]
		[batch 20/20] avg loss: 0.006548459181378997		[learning rate: 0.00011981]
	Learning Rate: 0.000119812
	LOSS [training: 0.004682788533169138 | validation: 0.013032694242053796]
	TIME [epoch: 8.18 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006194366649197704		[learning rate: 0.00011959]
		[batch 20/20] avg loss: 0.0020980329616853955		[learning rate: 0.00011938]
	Learning Rate: 0.000119377
	LOSS [training: 0.004146199805441549 | validation: 0.009189965666529538]
	TIME [epoch: 8.2 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004093199110475389		[learning rate: 0.00011916]
		[batch 20/20] avg loss: 0.008911330814809507		[learning rate: 0.00011894]
	Learning Rate: 0.000118944
	LOSS [training: 0.00650226496264245 | validation: 0.012248460307307939]
	TIME [epoch: 8.16 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007892774609773506		[learning rate: 0.00011873]
		[batch 20/20] avg loss: 0.013631683264130317		[learning rate: 0.00011851]
	Learning Rate: 0.000118512
	LOSS [training: 0.01076222893695191 | validation: 0.022645012788771676]
	TIME [epoch: 8.15 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005474456022151935		[learning rate: 0.0001183]
		[batch 20/20] avg loss: 0.007671352896521079		[learning rate: 0.00011808]
	Learning Rate: 0.000118082
	LOSS [training: 0.0065729044593365055 | validation: 0.0029369742398600456]
	TIME [epoch: 8.18 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0069336455389600385		[learning rate: 0.00011787]
		[batch 20/20] avg loss: 0.01327035194693113		[learning rate: 0.00011765]
	Learning Rate: 0.000117654
	LOSS [training: 0.010101998742945586 | validation: 0.010395573998452525]
	TIME [epoch: 8.21 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010879136266457787		[learning rate: 0.00011744]
		[batch 20/20] avg loss: 0.01014864467906776		[learning rate: 0.00011723]
	Learning Rate: 0.000117227
	LOSS [training: 0.010513890472762775 | validation: 0.01725594227902193]
	TIME [epoch: 8.16 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0085921254341769		[learning rate: 0.00011701]
		[batch 20/20] avg loss: 0.004451607319454812		[learning rate: 0.0001168]
	Learning Rate: 0.000116801
	LOSS [training: 0.0065218663768158575 | validation: 0.006322713419016869]
	TIME [epoch: 8.16 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000674298783011207		[learning rate: 0.00011659]
		[batch 20/20] avg loss: 0.00894452026636508		[learning rate: 0.00011638]
	Learning Rate: 0.000116377
	LOSS [training: 0.004809409524688143 | validation: 0.00742869004494746]
	TIME [epoch: 8.17 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00651004248702817		[learning rate: 0.00011617]
		[batch 20/20] avg loss: 0.009130247258498805		[learning rate: 0.00011595]
	Learning Rate: 0.000115955
	LOSS [training: 0.007820144872763486 | validation: 0.007718739942396016]
	TIME [epoch: 8.19 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011784084354546295		[learning rate: 0.00011574]
		[batch 20/20] avg loss: 0.0060876665825767935		[learning rate: 0.00011553]
	Learning Rate: 0.000115534
	LOSS [training: 0.008935875468561545 | validation: -0.0012001025453730657]
	TIME [epoch: 8.17 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002659038841259516		[learning rate: 0.00011532]
		[batch 20/20] avg loss: 0.010940136340594984		[learning rate: 0.00011511]
	Learning Rate: 0.000115115
	LOSS [training: 0.00679958759092725 | validation: 0.020195820987699693]
	TIME [epoch: 8.16 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009863626289042202		[learning rate: 0.00011491]
		[batch 20/20] avg loss: 0.008356650527378736		[learning rate: 0.0001147]
	Learning Rate: 0.000114697
	LOSS [training: 0.009110138408210473 | validation: 0.008132485114324567]
	TIME [epoch: 8.18 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005731259544222427		[learning rate: 0.00011449]
		[batch 20/20] avg loss: 0.006269967476337503		[learning rate: 0.00011428]
	Learning Rate: 0.000114281
	LOSS [training: 0.006000613510279965 | validation: 0.006085394188596129]
	TIME [epoch: 8.19 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003081209454408296		[learning rate: 0.00011407]
		[batch 20/20] avg loss: 0.003523753261888881		[learning rate: 0.00011387]
	Learning Rate: 0.000113866
	LOSS [training: 0.003302481358148588 | validation: 0.006694678965018706]
	TIME [epoch: 8.17 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011645175634562872		[learning rate: 0.00011366]
		[batch 20/20] avg loss: 0.002558427248324859		[learning rate: 0.00011345]
	Learning Rate: 0.000113453
	LOSS [training: 0.0071018014414438654 | validation: 0.010256446268919372]
	TIME [epoch: 8.15 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007444988684756998		[learning rate: 0.00011325]
		[batch 20/20] avg loss: 0.008896730441334894		[learning rate: 0.00011304]
	Learning Rate: 0.000113041
	LOSS [training: 0.008170859563045947 | validation: 0.014168470134746758]
	TIME [epoch: 8.17 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010249002576605435		[learning rate: 0.00011284]
		[batch 20/20] avg loss: 0.0028912268405075184		[learning rate: 0.00011263]
	Learning Rate: 0.000112631
	LOSS [training: 0.0065701147085564775 | validation: 0.008836439389671227]
	TIME [epoch: 8.19 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035683656642104536		[learning rate: 0.00011243]
		[batch 20/20] avg loss: 0.00878964478960374		[learning rate: 0.00011222]
	Learning Rate: 0.000112222
	LOSS [training: 0.006179005226907097 | validation: 0.008728600076637937]
	TIME [epoch: 8.16 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0052609333748312535		[learning rate: 0.00011202]
		[batch 20/20] avg loss: 0.014489980806181043		[learning rate: 0.00011181]
	Learning Rate: 0.000111815
	LOSS [training: 0.009875457090506148 | validation: 0.010015381111167941]
	TIME [epoch: 8.15 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010958587115030307		[learning rate: 0.00011161]
		[batch 20/20] avg loss: 0.00210474578250558		[learning rate: 0.00011141]
	Learning Rate: 0.000111409
	LOSS [training: 0.006531666448767943 | validation: 0.008086125446364953]
	TIME [epoch: 8.18 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007107910675264504		[learning rate: 0.00011121]
		[batch 20/20] avg loss: 0.00865403288252491		[learning rate: 0.000111]
	Learning Rate: 0.000111005
	LOSS [training: 0.007880971778894706 | validation: 0.00479202237132095]
	TIME [epoch: 8.18 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023128584606331823		[learning rate: 0.0001108]
		[batch 20/20] avg loss: 0.006921007651559154		[learning rate: 0.0001106]
	Learning Rate: 0.000110602
	LOSS [training: 0.004616933056096168 | validation: 0.008951547741220901]
	TIME [epoch: 8.16 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005328538623703578		[learning rate: 0.0001104]
		[batch 20/20] avg loss: 0.004521231259127092		[learning rate: 0.0001102]
	Learning Rate: 0.000110201
	LOSS [training: 0.004924884941415336 | validation: 0.007818862418086481]
	TIME [epoch: 8.15 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006212364403527014		[learning rate: 0.00011]
		[batch 20/20] avg loss: 0.01033044376192117		[learning rate: 0.0001098]
	Learning Rate: 0.000109801
	LOSS [training: 0.008271404082724091 | validation: 0.01264786329345874]
	TIME [epoch: 8.19 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005080604422083327		[learning rate: 0.0001096]
		[batch 20/20] avg loss: 0.007913965331811466		[learning rate: 0.0001094]
	Learning Rate: 0.000109402
	LOSS [training: 0.006497284876947396 | validation: 0.00946649981927495]
	TIME [epoch: 8.19 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010833465610677682		[learning rate: 0.0001092]
		[batch 20/20] avg loss: 0.019534504968281293		[learning rate: 0.00010901]
	Learning Rate: 0.000109005
	LOSS [training: 0.01518398528947949 | validation: 0.011342037815732795]
	TIME [epoch: 8.16 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017105236409803333		[learning rate: 0.00010881]
		[batch 20/20] avg loss: 0.0055830308767901154		[learning rate: 0.00010861]
	Learning Rate: 0.00010861
	LOSS [training: 0.011344133643296724 | validation: 0.012061538787410541]
	TIME [epoch: 8.15 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008876855895065434		[learning rate: 0.00010841]
		[batch 20/20] avg loss: 0.004963268497744839		[learning rate: 0.00010822]
	Learning Rate: 0.000108215
	LOSS [training: 0.0069200621964051355 | validation: -5.067038925002376e-05]
	TIME [epoch: 8.19 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009637146022332049		[learning rate: 0.00010802]
		[batch 20/20] avg loss: 0.005978224307414787		[learning rate: 0.00010782]
	Learning Rate: 0.000107823
	LOSS [training: 0.007807685164873417 | validation: 0.01212987270913222]
	TIME [epoch: 8.18 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006791236299764221		[learning rate: 0.00010763]
		[batch 20/20] avg loss: 0.008196318620581016		[learning rate: 0.00010743]
	Learning Rate: 0.000107432
	LOSS [training: 0.007493777460172617 | validation: 0.00835514360514541]
	TIME [epoch: 8.17 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012402027362451126		[learning rate: 0.00010724]
		[batch 20/20] avg loss: 0.006775176014046822		[learning rate: 0.00010704]
	Learning Rate: 0.000107042
	LOSS [training: 0.009588601688248974 | validation: 0.0016636761638324235]
	TIME [epoch: 8.15 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005732282289347455		[learning rate: 0.00010685]
		[batch 20/20] avg loss: 0.004817287433402777		[learning rate: 0.00010665]
	Learning Rate: 0.000106653
	LOSS [training: 0.0052747848613751164 | validation: 0.004169606062796946]
	TIME [epoch: 8.18 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005716757016701534		[learning rate: 0.00010646]
		[batch 20/20] avg loss: 0.009392129345729888		[learning rate: 0.00010627]
	Learning Rate: 0.000106266
	LOSS [training: 0.007554443181215712 | validation: 0.01515367796520808]
	TIME [epoch: 8.16 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008656788097453917		[learning rate: 0.00010607]
		[batch 20/20] avg loss: 0.00718377677388293		[learning rate: 0.00010588]
	Learning Rate: 0.00010588
	LOSS [training: 0.007920282435668422 | validation: 0.01644514083019425]
	TIME [epoch: 8.17 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009055833034312143		[learning rate: 0.00010569]
		[batch 20/20] avg loss: 0.008415092389958653		[learning rate: 0.0001055]
	Learning Rate: 0.000105496
	LOSS [training: 0.008735462712135395 | validation: 0.0070846605056766356]
	TIME [epoch: 8.16 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004088689507086803		[learning rate: 0.0001053]
		[batch 20/20] avg loss: 0.011948584066272739		[learning rate: 0.00010511]
	Learning Rate: 0.000105113
	LOSS [training: 0.008018636786679773 | validation: 0.012204654589805797]
	TIME [epoch: 8.18 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010870543849626399		[learning rate: 0.00010492]
		[batch 20/20] avg loss: 0.00621316323900411		[learning rate: 0.00010473]
	Learning Rate: 0.000104732
	LOSS [training: 0.008541853544315253 | validation: 0.0031340204893510384]
	TIME [epoch: 8.17 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007930163371387954		[learning rate: 0.00010454]
		[batch 20/20] avg loss: 0.0011035677079359167		[learning rate: 0.00010435]
	Learning Rate: 0.000104352
	LOSS [training: 0.004516865539661936 | validation: 0.01120291122466723]
	TIME [epoch: 8.17 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005965175380576263		[learning rate: 0.00010416]
		[batch 20/20] avg loss: 0.006076113847281137		[learning rate: 0.00010397]
	Learning Rate: 0.000103973
	LOSS [training: 0.0060206446139287 | validation: 0.0065468384353077735]
	TIME [epoch: 8.15 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006328323172348408		[learning rate: 0.00010378]
		[batch 20/20] avg loss: 0.008664017800407866		[learning rate: 0.0001036]
	Learning Rate: 0.000103596
	LOSS [training: 0.007496170486378138 | validation: 0.006641273701342186]
	TIME [epoch: 8.19 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0065889028449442845		[learning rate: 0.00010341]
		[batch 20/20] avg loss: 0.003820877269510486		[learning rate: 0.00010322]
	Learning Rate: 0.00010322
	LOSS [training: 0.005204890057227385 | validation: 0.004157881534847275]
	TIME [epoch: 8.16 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006391289968551256		[learning rate: 0.00010303]
		[batch 20/20] avg loss: 0.009209578084235035		[learning rate: 0.00010285]
	Learning Rate: 0.000102845
	LOSS [training: 0.0078004340263931465 | validation: 0.013024005105985642]
	TIME [epoch: 8.18 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010425751324909277		[learning rate: 0.00010266]
		[batch 20/20] avg loss: 0.00412665691178114		[learning rate: 0.00010247]
	Learning Rate: 0.000102472
	LOSS [training: 0.007276204118345208 | validation: 0.007490134203780492]
	TIME [epoch: 8.15 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010794818597947323		[learning rate: 0.00010229]
		[batch 20/20] avg loss: 0.0034916193500609607		[learning rate: 0.0001021]
	Learning Rate: 0.0001021
	LOSS [training: 0.007143218974004139 | validation: 0.011133645676078266]
	TIME [epoch: 8.18 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009268870929904392		[learning rate: 0.00010191]
		[batch 20/20] avg loss: 0.01052614387490398		[learning rate: 0.00010173]
	Learning Rate: 0.00010173
	LOSS [training: 0.009897507402404184 | validation: 0.013314898630120808]
	TIME [epoch: 8.16 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006998892491679755		[learning rate: 0.00010154]
		[batch 20/20] avg loss: 0.006174805012514773		[learning rate: 0.00010136]
	Learning Rate: 0.00010136
	LOSS [training: 0.006586848752097264 | validation: 0.006482386878635798]
	TIME [epoch: 8.18 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012676434526587458		[learning rate: 0.00010118]
		[batch 20/20] avg loss: 0.006140885858213589		[learning rate: 0.00010099]
	Learning Rate: 0.000100993
	LOSS [training: 0.009408660192400524 | validation: 0.006897694958779895]
	TIME [epoch: 8.15 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011601583950446286		[learning rate: 0.00010081]
		[batch 20/20] avg loss: 0.0032541245016131605		[learning rate: 0.00010063]
	Learning Rate: 0.000100626
	LOSS [training: 0.007427854226029725 | validation: 0.002864173319837217]
	TIME [epoch: 8.18 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007115495227865849		[learning rate: 0.00010044]
		[batch 20/20] avg loss: 0.0021490079589309664		[learning rate: 0.00010026]
	Learning Rate: 0.000100261
	LOSS [training: 0.004632251593398408 | validation: 0.015268698051704369]
	TIME [epoch: 8.17 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001570980068413001		[learning rate: 0.00010008]
		[batch 20/20] avg loss: 0.010596605789936651		[learning rate: 9.9897e-05]
	Learning Rate: 9.98971e-05
	LOSS [training: 0.006083792929174827 | validation: 0.010679468037594163]
	TIME [epoch: 8.17 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0058452692969447415		[learning rate: 9.9716e-05]
		[batch 20/20] avg loss: 0.004726888804724725		[learning rate: 9.9535e-05]
	Learning Rate: 9.95345e-05
	LOSS [training: 0.005286079050834732 | validation: 0.02178509433415421]
	TIME [epoch: 8.15 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005108109473211423		[learning rate: 9.9354e-05]
		[batch 20/20] avg loss: 0.009589632075873404		[learning rate: 9.9173e-05]
	Learning Rate: 9.91733e-05
	LOSS [training: 0.005050221511597273 | validation: 0.010507330931742947]
	TIME [epoch: 8.19 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0047241372816669595		[learning rate: 9.8993e-05]
		[batch 20/20] avg loss: 0.004972705830615931		[learning rate: 9.8813e-05]
	Learning Rate: 9.88134e-05
	LOSS [training: 0.004848421556141446 | validation: 0.01311816281479212]
	TIME [epoch: 8.16 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007762058115760674		[learning rate: 9.8634e-05]
		[batch 20/20] avg loss: 0.0076748009898048464		[learning rate: 9.8455e-05]
	Learning Rate: 9.84548e-05
	LOSS [training: 0.00771842955278276 | validation: 0.010047883380252871]
	TIME [epoch: 8.18 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023760585988582923		[learning rate: 9.8276e-05]
		[batch 20/20] avg loss: 0.010931760973682652		[learning rate: 9.8098e-05]
	Learning Rate: 9.80975e-05
	LOSS [training: 0.006653909786270472 | validation: 0.011843164512235688]
	TIME [epoch: 8.15 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014353605094313859		[learning rate: 9.7919e-05]
		[batch 20/20] avg loss: 0.0012922994072275702		[learning rate: 9.7742e-05]
	Learning Rate: 9.77415e-05
	LOSS [training: 0.007822952250770714 | validation: 0.010127490953588834]
	TIME [epoch: 8.19 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0042574823868946465		[learning rate: 9.7564e-05]
		[batch 20/20] avg loss: 0.0009186931867235903		[learning rate: 9.7387e-05]
	Learning Rate: 9.73868e-05
	LOSS [training: 0.0025880877868091185 | validation: 0.0066625471063426365]
	TIME [epoch: 8.16 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008729430566118523		[learning rate: 9.721e-05]
		[batch 20/20] avg loss: 0.014476334834112287		[learning rate: 9.7033e-05]
	Learning Rate: 9.70334e-05
	LOSS [training: 0.011602882700115405 | validation: 0.013468261205253173]
	TIME [epoch: 8.17 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006714988194811733		[learning rate: 9.6857e-05]
		[batch 20/20] avg loss: 0.004038380807664279		[learning rate: 9.6681e-05]
	Learning Rate: 9.66812e-05
	LOSS [training: 0.005376684501238006 | validation: 0.0023593888249297435]
	TIME [epoch: 8.15 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011108004480327085		[learning rate: 9.6506e-05]
		[batch 20/20] avg loss: -0.001312615056773662		[learning rate: 9.633e-05]
	Learning Rate: 9.63304e-05
	LOSS [training: 0.0048976947117767115 | validation: 0.006328955710166843]
	TIME [epoch: 8.19 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008341329092671247		[learning rate: 9.6155e-05]
		[batch 20/20] avg loss: 0.007275568955180502		[learning rate: 9.5981e-05]
	Learning Rate: 9.59808e-05
	LOSS [training: 0.0078084490239258755 | validation: 0.011349259140656653]
	TIME [epoch: 8.15 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012200047812679379		[learning rate: 9.5806e-05]
		[batch 20/20] avg loss: 0.013204163278967136		[learning rate: 9.5632e-05]
	Learning Rate: 9.56324e-05
	LOSS [training: 0.012702105545823258 | validation: 0.014763972069059581]
	TIME [epoch: 8.18 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010017536344901112		[learning rate: 9.5459e-05]
		[batch 20/20] avg loss: 0.009085162145754373		[learning rate: 9.5285e-05]
	Learning Rate: 9.52854e-05
	LOSS [training: 0.009551349245327741 | validation: 0.008352108864724074]
	TIME [epoch: 8.15 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004690254556986645		[learning rate: 9.5112e-05]
		[batch 20/20] avg loss: 0.0021011875027762885		[learning rate: 9.494e-05]
	Learning Rate: 9.49396e-05
	LOSS [training: 0.003395721029881466 | validation: 0.006525863204090119]
	TIME [epoch: 8.19 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0060492632098151575		[learning rate: 9.4767e-05]
		[batch 20/20] avg loss: 0.006540069286391113		[learning rate: 9.4595e-05]
	Learning Rate: 9.45951e-05
	LOSS [training: 0.006294666248103135 | validation: 0.008838431037235797]
	TIME [epoch: 8.15 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006532124158419916		[learning rate: 9.4423e-05]
		[batch 20/20] avg loss: 0.00797511967254087		[learning rate: 9.4252e-05]
	Learning Rate: 9.42518e-05
	LOSS [training: 0.007253621915480393 | validation: 0.002720149991257013]
	TIME [epoch: 8.18 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006869401470622158		[learning rate: 9.4081e-05]
		[batch 20/20] avg loss: 0.004699474269021549		[learning rate: 9.391e-05]
	Learning Rate: 9.39097e-05
	LOSS [training: 0.005784437869821854 | validation: 0.008320762232488066]
	TIME [epoch: 8.16 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013184603779512052		[learning rate: 9.3739e-05]
		[batch 20/20] avg loss: 0.0015114066081414544		[learning rate: 9.3569e-05]
	Learning Rate: 9.35689e-05
	LOSS [training: 0.007348005193826754 | validation: 0.006054707346573225]
	TIME [epoch: 8.19 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004710857439191389		[learning rate: 9.3399e-05]
		[batch 20/20] avg loss: 0.009472741903171678		[learning rate: 9.3229e-05]
	Learning Rate: 9.32294e-05
	LOSS [training: 0.007091799671181535 | validation: 0.00914205856022436]
	TIME [epoch: 8.15 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007571245276587597		[learning rate: 9.306e-05]
		[batch 20/20] avg loss: 0.0077950295222228546		[learning rate: 9.2891e-05]
	Learning Rate: 9.2891e-05
	LOSS [training: 0.007683137399405226 | validation: 0.006723728257608283]
	TIME [epoch: 8.17 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008560764767559414		[learning rate: 9.2722e-05]
		[batch 20/20] avg loss: 0.009256095754332972		[learning rate: 9.2554e-05]
	Learning Rate: 9.25539e-05
	LOSS [training: 0.008908430260946192 | validation: 0.009660143377570615]
	TIME [epoch: 8.15 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009133704999033224		[learning rate: 9.2386e-05]
		[batch 20/20] avg loss: 0.007839606181777732		[learning rate: 9.2218e-05]
	Learning Rate: 9.2218e-05
	LOSS [training: 0.008486655590405477 | validation: 0.004086598502462053]
	TIME [epoch: 8.19 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0043895559921673785		[learning rate: 9.2051e-05]
		[batch 20/20] avg loss: 0.012874528391110151		[learning rate: 9.1883e-05]
	Learning Rate: 9.18834e-05
	LOSS [training: 0.008632042191638767 | validation: 0.01368339138640129]
	TIME [epoch: 8.15 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006190612434999554		[learning rate: 9.1716e-05]
		[batch 20/20] avg loss: 0.006350387380285083		[learning rate: 9.155e-05]
	Learning Rate: 9.15499e-05
	LOSS [training: 0.0062704999076423195 | validation: 0.008307829187202659]
	TIME [epoch: 8.17 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013599574479049876		[learning rate: 9.1384e-05]
		[batch 20/20] avg loss: 0.010135561682771113		[learning rate: 9.1218e-05]
	Learning Rate: 9.12177e-05
	LOSS [training: 0.0057477595653380505 | validation: 0.01109054614051138]
	TIME [epoch: 8.15 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0078968532733866		[learning rate: 9.1052e-05]
		[batch 20/20] avg loss: 0.0007460309098399953		[learning rate: 9.0887e-05]
	Learning Rate: 9.08866e-05
	LOSS [training: 0.004321442091613297 | validation: 0.006267158053189422]
	TIME [epoch: 8.19 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003043901958990399		[learning rate: 9.0722e-05]
		[batch 20/20] avg loss: 0.005811065837891453		[learning rate: 9.0557e-05]
	Learning Rate: 9.05568e-05
	LOSS [training: 0.004427483898440926 | validation: 0.004931318361875252]
	TIME [epoch: 8.15 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005659990895311339		[learning rate: 9.0392e-05]
		[batch 20/20] avg loss: 0.008674621064222832		[learning rate: 9.0228e-05]
	Learning Rate: 9.02281e-05
	LOSS [training: 0.007167305979767084 | validation: 0.0025137536216344953]
	TIME [epoch: 8.16 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004979565886510816		[learning rate: 9.0064e-05]
		[batch 20/20] avg loss: 0.005751974983699974		[learning rate: 8.9901e-05]
	Learning Rate: 8.99007e-05
	LOSS [training: 0.005365770435105394 | validation: 0.01474556343516493]
	TIME [epoch: 8.16 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011449961553120763		[learning rate: 8.9737e-05]
		[batch 20/20] avg loss: 0.010782746243082673		[learning rate: 8.9574e-05]
	Learning Rate: 8.95745e-05
	LOSS [training: 0.011116353898101715 | validation: 0.0075661752922808615]
	TIME [epoch: 8.2 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011913057575419677		[learning rate: 8.9412e-05]
		[batch 20/20] avg loss: 0.004733238943416292		[learning rate: 8.9249e-05]
	Learning Rate: 8.92494e-05
	LOSS [training: 0.008323148259417986 | validation: 0.007814049251313442]
	TIME [epoch: 8.15 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002942439192121664		[learning rate: 8.9087e-05]
		[batch 20/20] avg loss: 0.011503511349383358		[learning rate: 8.8926e-05]
	Learning Rate: 8.89255e-05
	LOSS [training: 0.00722297527075251 | validation: 0.00964340825347262]
	TIME [epoch: 8.17 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006991721574341097		[learning rate: 8.8764e-05]
		[batch 20/20] avg loss: 0.004613977323863196		[learning rate: 8.8603e-05]
	Learning Rate: 8.86028e-05
	LOSS [training: 0.005802849449102146 | validation: 0.006055955222207614]
	TIME [epoch: 8.16 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007576990409849912		[learning rate: 8.8442e-05]
		[batch 20/20] avg loss: 0.00429829330792386		[learning rate: 8.8281e-05]
	Learning Rate: 8.82813e-05
	LOSS [training: 0.0059376418588868865 | validation: 0.012341468345585593]
	TIME [epoch: 8.19 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002921182786044369		[learning rate: 8.8121e-05]
		[batch 20/20] avg loss: 0.008575655105148027		[learning rate: 8.7961e-05]
	Learning Rate: 8.79609e-05
	LOSS [training: 0.005748418945596198 | validation: 0.0058389401467883045]
	TIME [epoch: 8.15 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006504552178708741		[learning rate: 8.7801e-05]
		[batch 20/20] avg loss: 0.006545809792980117		[learning rate: 8.7642e-05]
	Learning Rate: 8.76417e-05
	LOSS [training: 0.00652518098584443 | validation: 0.006653409238618701]
	TIME [epoch: 8.17 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003425900623268971		[learning rate: 8.7482e-05]
		[batch 20/20] avg loss: 0.006507471206407556		[learning rate: 8.7324e-05]
	Learning Rate: 8.73236e-05
	LOSS [training: 0.0049666859148382635 | validation: 0.003063690555007617]
	TIME [epoch: 8.16 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014565071566817983		[learning rate: 8.7165e-05]
		[batch 20/20] avg loss: 0.0069529386643120625		[learning rate: 8.7007e-05]
	Learning Rate: 8.70067e-05
	LOSS [training: 0.004204722910496931 | validation: 0.004625356720212196]
	TIME [epoch: 8.19 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038304458443689402		[learning rate: 8.6849e-05]
		[batch 20/20] avg loss: 0.008900191647138138		[learning rate: 8.6691e-05]
	Learning Rate: 8.66909e-05
	LOSS [training: 0.00636531874575354 | validation: 0.014484166096284305]
	TIME [epoch: 8.15 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009972256632726638		[learning rate: 8.6533e-05]
		[batch 20/20] avg loss: 0.006998978830188005		[learning rate: 8.6376e-05]
	Learning Rate: 8.63763e-05
	LOSS [training: 0.003998102246730334 | validation: 0.006555678336344788]
	TIME [epoch: 8.17 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005239921888493951		[learning rate: 8.6219e-05]
		[batch 20/20] avg loss: 0.00870040065002297		[learning rate: 8.6063e-05]
	Learning Rate: 8.60629e-05
	LOSS [training: 0.00697016126925846 | validation: 0.010017724899331048]
	TIME [epoch: 8.16 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016679321705780307		[learning rate: 8.5907e-05]
		[batch 20/20] avg loss: 0.009912917519080957		[learning rate: 8.5751e-05]
	Learning Rate: 8.57505e-05
	LOSS [training: 0.0057904248448294934 | validation: 0.008045192189846482]
	TIME [epoch: 8.19 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011787297893554715		[learning rate: 8.5595e-05]
		[batch 20/20] avg loss: 0.004663327565843597		[learning rate: 8.5439e-05]
	Learning Rate: 8.54394e-05
	LOSS [training: 0.008225312729699157 | validation: 0.009972707420031545]
	TIME [epoch: 8.16 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002537330221660157		[learning rate: 8.5284e-05]
		[batch 20/20] avg loss: 0.0067587857719410725		[learning rate: 8.5129e-05]
	Learning Rate: 8.51293e-05
	LOSS [training: 0.004648057996800614 | validation: 0.0035173509762407426]
	TIME [epoch: 8.16 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004355131822824451		[learning rate: 8.4975e-05]
		[batch 20/20] avg loss: 0.0031322292467303877		[learning rate: 8.482e-05]
	Learning Rate: 8.48204e-05
	LOSS [training: 0.0037436805347774195 | validation: 0.008194661913003255]
	TIME [epoch: 8.15 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009293052586142794		[learning rate: 8.4666e-05]
		[batch 20/20] avg loss: 0.005094262469343153		[learning rate: 8.4513e-05]
	Learning Rate: 8.45125e-05
	LOSS [training: 0.007193657527742972 | validation: 0.0060662755902775025]
	TIME [epoch: 8.19 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004097570746992121		[learning rate: 8.4359e-05]
		[batch 20/20] avg loss: 0.006859161127708685		[learning rate: 8.4206e-05]
	Learning Rate: 8.42058e-05
	LOSS [training: 0.005478365937350403 | validation: 0.002143712609845058]
	TIME [epoch: 8.15 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009168202531551145		[learning rate: 8.4053e-05]
		[batch 20/20] avg loss: 0.004615951574847538		[learning rate: 8.39e-05]
	Learning Rate: 8.39002e-05
	LOSS [training: 0.006892077053199342 | validation: 0.0027015962515373982]
	TIME [epoch: 8.16 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022353641709335595		[learning rate: 8.3748e-05]
		[batch 20/20] avg loss: 0.006055014308996291		[learning rate: 8.3596e-05]
	Learning Rate: 8.35958e-05
	LOSS [training: 0.004145189239964926 | validation: 0.006589677963851646]
	TIME [epoch: 8.17 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038263393588110423		[learning rate: 8.3444e-05]
		[batch 20/20] avg loss: 0.004075341486582025		[learning rate: 8.3292e-05]
	Learning Rate: 8.32924e-05
	LOSS [training: 0.003950840422696534 | validation: 0.006947813984889548]
	TIME [epoch: 8.19 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008045536682467398		[learning rate: 8.3141e-05]
		[batch 20/20] avg loss: 0.00540737306641295		[learning rate: 8.299e-05]
	Learning Rate: 8.29901e-05
	LOSS [training: 0.0067264548744401746 | validation: 0.003246659135625345]
	TIME [epoch: 8.14 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01041534843023358		[learning rate: 8.2839e-05]
		[batch 20/20] avg loss: 0.006548833203120963		[learning rate: 8.2689e-05]
	Learning Rate: 8.26889e-05
	LOSS [training: 0.00848209081667727 | validation: 0.009474016704320921]
	TIME [epoch: 8.14 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005285392588383547		[learning rate: 8.2539e-05]
		[batch 20/20] avg loss: 0.007839406107379042		[learning rate: 8.2389e-05]
	Learning Rate: 8.23889e-05
	LOSS [training: 0.006562399347881294 | validation: 0.005550115017568657]
	TIME [epoch: 8.16 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004198294818569608		[learning rate: 8.2239e-05]
		[batch 20/20] avg loss: 0.006357484577344747		[learning rate: 8.209e-05]
	Learning Rate: 8.20899e-05
	LOSS [training: 0.005277889697957176 | validation: 0.0023129941216784187]
	TIME [epoch: 8.19 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002751929860434579		[learning rate: 8.1941e-05]
		[batch 20/20] avg loss: 0.014097816794865676		[learning rate: 8.1792e-05]
	Learning Rate: 8.17919e-05
	LOSS [training: 0.008424873327650125 | validation: 0.005009135921499968]
	TIME [epoch: 8.15 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002998578844140196		[learning rate: 8.1643e-05]
		[batch 20/20] avg loss: 0.012248813019163958		[learning rate: 8.1495e-05]
	Learning Rate: 8.14951e-05
	LOSS [training: 0.007623695931652077 | validation: 0.012003600945175831]
	TIME [epoch: 8.15 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008982922488419453		[learning rate: 8.1347e-05]
		[batch 20/20] avg loss: 0.0036670949223519224		[learning rate: 8.1199e-05]
	Learning Rate: 8.11994e-05
	LOSS [training: 0.006325008705385689 | validation: 0.008629144649485426]
	TIME [epoch: 8.17 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004024527866677266		[learning rate: 8.1052e-05]
		[batch 20/20] avg loss: 0.009698406516806144		[learning rate: 8.0905e-05]
	Learning Rate: 8.09047e-05
	LOSS [training: 0.006861467191741706 | validation: 0.010576458187472833]
	TIME [epoch: 8.2 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01013079270919824		[learning rate: 8.0758e-05]
		[batch 20/20] avg loss: 0.0012413095129075076		[learning rate: 8.0611e-05]
	Learning Rate: 8.06111e-05
	LOSS [training: 0.0056860511110528745 | validation: 0.01136205617776714]
	TIME [epoch: 8.15 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007277590163675042		[learning rate: 8.0465e-05]
		[batch 20/20] avg loss: 0.011950173905951095		[learning rate: 8.0319e-05]
	Learning Rate: 8.03186e-05
	LOSS [training: 0.009613882034813068 | validation: 0.0018018988292043832]
	TIME [epoch: 8.16 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025914171408526684		[learning rate: 8.0173e-05]
		[batch 20/20] avg loss: 0.009175442525101458		[learning rate: 8.0027e-05]
	Learning Rate: 8.00271e-05
	LOSS [training: 0.005883429832977062 | validation: 0.008676129506936798]
	TIME [epoch: 8.19 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005931781959537899		[learning rate: 7.9882e-05]
		[batch 20/20] avg loss: 0.007029542310113315		[learning rate: 7.9737e-05]
	Learning Rate: 7.97367e-05
	LOSS [training: 0.006480662134825607 | validation: 0.009600854726228906]
	TIME [epoch: 8.2 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035281016539227593		[learning rate: 7.9592e-05]
		[batch 20/20] avg loss: 0.015254442001550475		[learning rate: 7.9447e-05]
	Learning Rate: 7.94473e-05
	LOSS [training: 0.009391271827736617 | validation: 0.008769116552664024]
	TIME [epoch: 8.15 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00423147210722276		[learning rate: 7.9303e-05]
		[batch 20/20] avg loss: 0.010237465849549692		[learning rate: 7.9159e-05]
	Learning Rate: 7.91589e-05
	LOSS [training: 0.007234468978386227 | validation: 0.007336790785024823]
	TIME [epoch: 8.16 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006547479491187723		[learning rate: 7.9015e-05]
		[batch 20/20] avg loss: 0.005620489301143308		[learning rate: 7.8872e-05]
	Learning Rate: 7.88717e-05
	LOSS [training: 0.006083984396165516 | validation: 0.00408235899513464]
	TIME [epoch: 8.19 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032031799762969374		[learning rate: 7.8728e-05]
		[batch 20/20] avg loss: 0.003886509219624798		[learning rate: 7.8585e-05]
	Learning Rate: 7.85854e-05
	LOSS [training: 0.0035448445979608676 | validation: 0.006971689015525629]
	TIME [epoch: 8.19 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0058136551649657555		[learning rate: 7.8443e-05]
		[batch 20/20] avg loss: 0.00322354299986422		[learning rate: 7.83e-05]
	Learning Rate: 7.83003e-05
	LOSS [training: 0.004518599082414988 | validation: 0.007119948114962964]
	TIME [epoch: 8.15 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00430343391026024		[learning rate: 7.8158e-05]
		[batch 20/20] avg loss: 0.0036626293093384075		[learning rate: 7.8016e-05]
	Learning Rate: 7.80161e-05
	LOSS [training: 0.003983031609799324 | validation: 0.008532690303053385]
	TIME [epoch: 8.15 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009662320468725857		[learning rate: 7.7874e-05]
		[batch 20/20] avg loss: 0.0024579836078456275		[learning rate: 7.7733e-05]
	Learning Rate: 7.7733e-05
	LOSS [training: 0.0017121078273591064 | validation: 0.011310193961057993]
	TIME [epoch: 8.18 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013303142332335174		[learning rate: 7.7592e-05]
		[batch 20/20] avg loss: 0.008073325775810915		[learning rate: 7.7451e-05]
	Learning Rate: 7.74509e-05
	LOSS [training: 0.010688234054073043 | validation: 0.006372469589531918]
	TIME [epoch: 8.19 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008136756997525552		[learning rate: 7.731e-05]
		[batch 20/20] avg loss: 0.004552667488608319		[learning rate: 7.717e-05]
	Learning Rate: 7.71698e-05
	LOSS [training: 0.006344712243066935 | validation: 0.0006635682345349969]
	TIME [epoch: 8.15 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003931286619773341		[learning rate: 7.703e-05]
		[batch 20/20] avg loss: 0.006400292844666726		[learning rate: 7.689e-05]
	Learning Rate: 7.68897e-05
	LOSS [training: 0.0051657897322200325 | validation: 0.012524755162949656]
	TIME [epoch: 8.15 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008032657903654882		[learning rate: 7.675e-05]
		[batch 20/20] avg loss: 0.0003586479595566334		[learning rate: 7.6611e-05]
	Learning Rate: 7.66107e-05
	LOSS [training: 0.004195652931605759 | validation: 0.007242464200869507]
	TIME [epoch: 8.18 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006466301990613085		[learning rate: 7.6472e-05]
		[batch 20/20] avg loss: 0.0054365703199990845		[learning rate: 7.6333e-05]
	Learning Rate: 7.63327e-05
	LOSS [training: 0.005951436155306085 | validation: 0.0019168508848660878]
	TIME [epoch: 8.19 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005071329681596241		[learning rate: 7.6194e-05]
		[batch 20/20] avg loss: 0.004494432182638003		[learning rate: 7.6056e-05]
	Learning Rate: 7.60557e-05
	LOSS [training: 0.004782880932117123 | validation: 0.007269995905394778]
	TIME [epoch: 8.15 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00899359511912586		[learning rate: 7.5918e-05]
		[batch 20/20] avg loss: 0.01672311034593245		[learning rate: 7.578e-05]
	Learning Rate: 7.57797e-05
	LOSS [training: 0.012858352732529155 | validation: 0.016864696580632114]
	TIME [epoch: 8.16 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01373861725266369		[learning rate: 7.5642e-05]
		[batch 20/20] avg loss: 0.007238342744240715		[learning rate: 7.5505e-05]
	Learning Rate: 7.55047e-05
	LOSS [training: 0.010488479998452203 | validation: 0.015045810261590164]
	TIME [epoch: 8.19 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006187424590690932		[learning rate: 7.5368e-05]
		[batch 20/20] avg loss: 0.01079948689616126		[learning rate: 7.5231e-05]
	Learning Rate: 7.52307e-05
	LOSS [training: 0.008493455743426097 | validation: 0.014297516765363972]
	TIME [epoch: 8.18 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017451358167285047		[learning rate: 7.5094e-05]
		[batch 20/20] avg loss: 0.00966272565588923		[learning rate: 7.4958e-05]
	Learning Rate: 7.49576e-05
	LOSS [training: 0.01355704191158714 | validation: 0.01665324203325247]
	TIME [epoch: 8.15 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004780205559374804		[learning rate: 7.4822e-05]
		[batch 20/20] avg loss: 0.006234324799512083		[learning rate: 7.4686e-05]
	Learning Rate: 7.46856e-05
	LOSS [training: 0.0055072651794434435 | validation: 0.009405669846511798]
	TIME [epoch: 8.15 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023232919045014125		[learning rate: 7.455e-05]
		[batch 20/20] avg loss: 0.004698290520534937		[learning rate: 7.4415e-05]
	Learning Rate: 7.44146e-05
	LOSS [training: 0.0035107912125181746 | validation: 0.004092130148946745]
	TIME [epoch: 8.19 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00671870469306744		[learning rate: 7.4279e-05]
		[batch 20/20] avg loss: 0.005116499207706405		[learning rate: 7.4144e-05]
	Learning Rate: 7.41445e-05
	LOSS [training: 0.005917601950386923 | validation: 0.004815021337611549]
	TIME [epoch: 8.18 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002960499588060159		[learning rate: 7.401e-05]
		[batch 20/20] avg loss: 0.0035608884711779793		[learning rate: 7.3875e-05]
	Learning Rate: 7.38754e-05
	LOSS [training: 0.003260694029619069 | validation: 0.004185888198146264]
	TIME [epoch: 8.15 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011113693744244132		[learning rate: 7.3741e-05]
		[batch 20/20] avg loss: -0.0005204506874513617		[learning rate: 7.3607e-05]
	Learning Rate: 7.36073e-05
	LOSS [training: 0.005296621528396386 | validation: 0.012070542988579097]
	TIME [epoch: 8.15 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003030483309670832		[learning rate: 7.3474e-05]
		[batch 20/20] avg loss: 0.00619045542114908		[learning rate: 7.334e-05]
	Learning Rate: 7.33402e-05
	LOSS [training: 0.0046104693654099565 | validation: 0.010915692916722546]
	TIME [epoch: 8.19 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001176779880229253		[learning rate: 7.3207e-05]
		[batch 20/20] avg loss: 0.0011172869124944566		[learning rate: 7.3074e-05]
	Learning Rate: 7.30741e-05
	LOSS [training: -2.974648386739817e-05 | validation: 0.004093367997813727]
	TIME [epoch: 8.18 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: -1.563396610171179e-05		[learning rate: 7.2941e-05]
		[batch 20/20] avg loss: 0.006782053708119842		[learning rate: 7.2809e-05]
	Learning Rate: 7.28089e-05
	LOSS [training: 0.003383209871009065 | validation: 0.011256848599548093]
	TIME [epoch: 8.16 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0055572379448141935		[learning rate: 7.2677e-05]
		[batch 20/20] avg loss: 0.004093181276946608		[learning rate: 7.2545e-05]
	Learning Rate: 7.25447e-05
	LOSS [training: 0.004825209610880402 | validation: 0.006963904764118507]
	TIME [epoch: 8.15 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004441120899663547		[learning rate: 7.2413e-05]
		[batch 20/20] avg loss: 0.007808913950622086		[learning rate: 7.2281e-05]
	Learning Rate: 7.22814e-05
	LOSS [training: 0.006125017425142816 | validation: 0.00915550376514469]
	TIME [epoch: 8.2 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030336823076568724		[learning rate: 7.215e-05]
		[batch 20/20] avg loss: 0.011491831243862086		[learning rate: 7.2019e-05]
	Learning Rate: 7.2019e-05
	LOSS [training: 0.007262756775759481 | validation: 0.022075139191848135]
	TIME [epoch: 8.17 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008549405832168006		[learning rate: 7.1888e-05]
		[batch 20/20] avg loss: 0.007647721513391356		[learning rate: 7.1758e-05]
	Learning Rate: 7.17577e-05
	LOSS [training: 0.00809856367277968 | validation: 0.006321024132152961]
	TIME [epoch: 8.15 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005845659924022271		[learning rate: 7.1627e-05]
		[batch 20/20] avg loss: 0.0031807057140756127		[learning rate: 7.1497e-05]
	Learning Rate: 7.14973e-05
	LOSS [training: 0.0045131828190489425 | validation: 0.003780055889176068]
	TIME [epoch: 8.15 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00981221295473269		[learning rate: 7.1367e-05]
		[batch 20/20] avg loss: 0.0047429958025219775		[learning rate: 7.1238e-05]
	Learning Rate: 7.12378e-05
	LOSS [training: 0.007277604378627333 | validation: 0.011393114844574224]
	TIME [epoch: 8.17 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002556287805108526		[learning rate: 7.1108e-05]
		[batch 20/20] avg loss: 0.0037649056321823284		[learning rate: 7.0979e-05]
	Learning Rate: 7.09793e-05
	LOSS [training: 0.003160596718645427 | validation: 0.005475465324477503]
	TIME [epoch: 8.2 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033593493821600763		[learning rate: 7.085e-05]
		[batch 20/20] avg loss: 0.0017850257383586123		[learning rate: 7.0722e-05]
	Learning Rate: 7.07217e-05
	LOSS [training: 0.0025721875602593446 | validation: 0.002454326271301007]
	TIME [epoch: 8.15 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004655493758822951		[learning rate: 7.0593e-05]
		[batch 20/20] avg loss: 0.0043572740305972005		[learning rate: 7.0465e-05]
	Learning Rate: 7.0465e-05
	LOSS [training: 0.004506383894710075 | validation: 0.007198438513078708]
	TIME [epoch: 8.15 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0055563775019588065		[learning rate: 7.0337e-05]
		[batch 20/20] avg loss: 0.008551471984569318		[learning rate: 7.0209e-05]
	Learning Rate: 7.02093e-05
	LOSS [training: 0.007053924743264063 | validation: 0.00613530426695623]
	TIME [epoch: 8.17 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015035850449337109		[learning rate: 7.0082e-05]
		[batch 20/20] avg loss: 0.004015778566231918		[learning rate: 6.9955e-05]
	Learning Rate: 6.99545e-05
	LOSS [training: 0.009525814507784515 | validation: 0.009205296999998905]
	TIME [epoch: 8.2 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004742615940878488		[learning rate: 6.9827e-05]
		[batch 20/20] avg loss: -0.003356370206310436		[learning rate: 6.9701e-05]
	Learning Rate: 6.97006e-05
	LOSS [training: 0.0006931228672840254 | validation: 0.002288114388654459]
	TIME [epoch: 8.15 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002202696474794944		[learning rate: 6.9574e-05]
		[batch 20/20] avg loss: 0.006322694051181153		[learning rate: 6.9448e-05]
	Learning Rate: 6.94477e-05
	LOSS [training: 0.004262695262988048 | validation: 0.0057714954516687065]
	TIME [epoch: 8.16 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009503542552234822		[learning rate: 6.9322e-05]
		[batch 20/20] avg loss: 0.004036154040815851		[learning rate: 6.9196e-05]
	Learning Rate: 6.91957e-05
	LOSS [training: 0.006769848296525337 | validation: 0.010271105233356741]
	TIME [epoch: 8.19 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00765270426018127		[learning rate: 6.907e-05]
		[batch 20/20] avg loss: 0.0063292165774187		[learning rate: 6.8945e-05]
	Learning Rate: 6.89446e-05
	LOSS [training: 0.006990960418799987 | validation: 0.006049417158153362]
	TIME [epoch: 8.19 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0074024863556714105		[learning rate: 6.8819e-05]
		[batch 20/20] avg loss: 0.003539740375239192		[learning rate: 6.8694e-05]
	Learning Rate: 6.86944e-05
	LOSS [training: 0.0054711133654553 | validation: 0.0058683877723731005]
	TIME [epoch: 8.15 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004671233096371066		[learning rate: 6.857e-05]
		[batch 20/20] avg loss: 0.005868615739080383		[learning rate: 6.8445e-05]
	Learning Rate: 6.84451e-05
	LOSS [training: 0.005269924417725723 | validation: 0.008910331932969877]
	TIME [epoch: 8.14 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014027701822602336		[learning rate: 6.8321e-05]
		[batch 20/20] avg loss: 0.006506926483694017		[learning rate: 6.8197e-05]
	Learning Rate: 6.81967e-05
	LOSS [training: 0.003954848332977125 | validation: 0.014089442955653527]
	TIME [epoch: 8.19 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008834538750162116		[learning rate: 6.8073e-05]
		[batch 20/20] avg loss: 0.003334189595372366		[learning rate: 6.7949e-05]
	Learning Rate: 6.79492e-05
	LOSS [training: 0.0060843641727672424 | validation: 0.005816249692858653]
	TIME [epoch: 8.19 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006175378871315463		[learning rate: 6.7826e-05]
		[batch 20/20] avg loss: 0.007764340584729223		[learning rate: 6.7703e-05]
	Learning Rate: 6.77026e-05
	LOSS [training: 0.006969859728022343 | validation: 0.019063806372811347]
	TIME [epoch: 8.15 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00034321056239808816		[learning rate: 6.758e-05]
		[batch 20/20] avg loss: 0.008252035380309617		[learning rate: 6.7457e-05]
	Learning Rate: 6.74569e-05
	LOSS [training: 0.0042976229713538525 | validation: 0.010873096079998975]
	TIME [epoch: 8.15 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006686122514634092		[learning rate: 6.7334e-05]
		[batch 20/20] avg loss: 0.009637463866252322		[learning rate: 6.7212e-05]
	Learning Rate: 6.72121e-05
	LOSS [training: 0.005153038058857866 | validation: 0.00973341450011017]
	TIME [epoch: 8.19 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006084685524003717		[learning rate: 6.709e-05]
		[batch 20/20] avg loss: 0.0025844458230551955		[learning rate: 6.6968e-05]
	Learning Rate: 6.69682e-05
	LOSS [training: 0.004334565673529457 | validation: 0.01014331774203684]
	TIME [epoch: 8.18 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006206567819124278		[learning rate: 6.6847e-05]
		[batch 20/20] avg loss: 0.004744897745959329		[learning rate: 6.6725e-05]
	Learning Rate: 6.67251e-05
	LOSS [training: 0.005475732782541803 | validation: 0.004132755935133055]
	TIME [epoch: 8.15 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0055870627418160455		[learning rate: 6.6604e-05]
		[batch 20/20] avg loss: 0.0012809079089387064		[learning rate: 6.6483e-05]
	Learning Rate: 6.6483e-05
	LOSS [training: 0.003433985325377376 | validation: 0.005616053479455329]
	TIME [epoch: 8.15 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006316276269744195		[learning rate: 6.6362e-05]
		[batch 20/20] avg loss: 0.005421422526030709		[learning rate: 6.6242e-05]
	Learning Rate: 6.62417e-05
	LOSS [training: 0.005868849397887453 | validation: 0.008145452031260773]
	TIME [epoch: 8.19 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038123204053999633		[learning rate: 6.6121e-05]
		[batch 20/20] avg loss: 0.006430796454193345		[learning rate: 6.6001e-05]
	Learning Rate: 6.60013e-05
	LOSS [training: 0.005121558429796654 | validation: 0.010453944050189295]
	TIME [epoch: 8.19 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00148609153424069		[learning rate: 6.5881e-05]
		[batch 20/20] avg loss: 0.011710625156587202		[learning rate: 6.5762e-05]
	Learning Rate: 6.57618e-05
	LOSS [training: 0.006598358345413946 | validation: 0.007200299756541582]
	TIME [epoch: 8.15 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011831465853512633		[learning rate: 6.5642e-05]
		[batch 20/20] avg loss: 0.012161161850059115		[learning rate: 6.5523e-05]
	Learning Rate: 6.55232e-05
	LOSS [training: 0.006672154217705189 | validation: 0.0034557701535303633]
	TIME [epoch: 8.15 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006403499280896077		[learning rate: 6.5404e-05]
		[batch 20/20] avg loss: 0.008074506833019829		[learning rate: 6.5285e-05]
	Learning Rate: 6.52854e-05
	LOSS [training: 0.007239003056957955 | validation: -0.001930888904052521]
	TIME [epoch: 8.18 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001400841590073399		[learning rate: 6.5167e-05]
		[batch 20/20] avg loss: 0.009514294132303624		[learning rate: 6.5048e-05]
	Learning Rate: 6.50484e-05
	LOSS [training: 0.005457567861188511 | validation: 0.012129364803369048]
	TIME [epoch: 8.19 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011517085227348617		[learning rate: 6.493e-05]
		[batch 20/20] avg loss: 0.00451759464785835		[learning rate: 6.4812e-05]
	Learning Rate: 6.48124e-05
	LOSS [training: 0.008017339937603484 | validation: 0.0016375267933886922]
	TIME [epoch: 8.16 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001033826905118593		[learning rate: 6.4695e-05]
		[batch 20/20] avg loss: 0.012584286770217005		[learning rate: 6.4577e-05]
	Learning Rate: 6.45772e-05
	LOSS [training: 0.006809056837667799 | validation: 0.008482001664595607]
	TIME [epoch: 8.16 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00638561887653363		[learning rate: 6.446e-05]
		[batch 20/20] avg loss: 0.007715783027848215		[learning rate: 6.4343e-05]
	Learning Rate: 6.43428e-05
	LOSS [training: 0.007050700952190922 | validation: 0.01146543847200587]
	TIME [epoch: 8.2 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008224647534832022		[learning rate: 6.4226e-05]
		[batch 20/20] avg loss: 0.0028544808227753114		[learning rate: 6.4109e-05]
	Learning Rate: 6.41093e-05
	LOSS [training: 0.005539564178803667 | validation: 0.011013633377916726]
	TIME [epoch: 8.17 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007018132578235773		[learning rate: 6.3993e-05]
		[batch 20/20] avg loss: 0.005898093178865303		[learning rate: 6.3877e-05]
	Learning Rate: 6.38767e-05
	LOSS [training: 0.006458112878550538 | validation: 0.00863716110635394]
	TIME [epoch: 8.15 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009012081644038968		[learning rate: 6.3761e-05]
		[batch 20/20] avg loss: 0.003427742676053078		[learning rate: 6.3645e-05]
	Learning Rate: 6.36449e-05
	LOSS [training: 0.0021644754202284873 | validation: 0.009507922672732916]
	TIME [epoch: 8.15 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00846888988977354		[learning rate: 6.3529e-05]
		[batch 20/20] avg loss: 0.0019811031802425276		[learning rate: 6.3414e-05]
	Learning Rate: 6.34139e-05
	LOSS [training: 0.0052249965350080335 | validation: 0.003294561956919264]
	TIME [epoch: 8.19 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004565212054637467		[learning rate: 6.3299e-05]
		[batch 20/20] avg loss: 0.0009955017385897785		[learning rate: 6.3184e-05]
	Learning Rate: 6.31837e-05
	LOSS [training: 0.0027803568966136226 | validation: 0.011763160908928476]
	TIME [epoch: 8.18 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004434472666096727		[learning rate: 6.3069e-05]
		[batch 20/20] avg loss: 0.0023880865374482857		[learning rate: 6.2954e-05]
	Learning Rate: 6.29544e-05
	LOSS [training: 0.0034112796017725065 | validation: 0.0038128961293804053]
	TIME [epoch: 8.15 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009162071360097605		[learning rate: 6.284e-05]
		[batch 20/20] avg loss: 0.0049704890811843126		[learning rate: 6.2726e-05]
	Learning Rate: 6.2726e-05
	LOSS [training: 0.00706628022064096 | validation: 0.014608494129121585]
	TIME [epoch: 8.15 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01044513758523829		[learning rate: 6.2612e-05]
		[batch 20/20] avg loss: 0.007797746140655318		[learning rate: 6.2498e-05]
	Learning Rate: 6.24983e-05
	LOSS [training: 0.009121441862946802 | validation: 0.00923564979960284]
	TIME [epoch: 8.2 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007978635249580528		[learning rate: 6.2385e-05]
		[batch 20/20] avg loss: 0.004894144352337542		[learning rate: 6.2272e-05]
	Learning Rate: 6.22715e-05
	LOSS [training: 0.006436389800959034 | validation: 0.011531513117952048]
	TIME [epoch: 8.18 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037486926660663597		[learning rate: 6.2158e-05]
		[batch 20/20] avg loss: 0.003894743519051487		[learning rate: 6.2046e-05]
	Learning Rate: 6.20455e-05
	LOSS [training: 0.003821718092558923 | validation: 0.010254177388309504]
	TIME [epoch: 8.16 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0041571706195577774		[learning rate: 6.1933e-05]
		[batch 20/20] avg loss: 0.005257058644222946		[learning rate: 6.182e-05]
	Learning Rate: 6.18204e-05
	LOSS [training: 0.004707114631890361 | validation: 0.011043940707582576]
	TIME [epoch: 8.15 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005692948690312653		[learning rate: 6.1708e-05]
		[batch 20/20] avg loss: 0.003987843846063672		[learning rate: 6.1596e-05]
	Learning Rate: 6.1596e-05
	LOSS [training: 0.004840396268188164 | validation: 0.003617363774864159]
	TIME [epoch: 8.2 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00983734424448029		[learning rate: 6.1484e-05]
		[batch 20/20] avg loss: 0.006998351302358456		[learning rate: 6.1372e-05]
	Learning Rate: 6.13725e-05
	LOSS [training: 0.008417847773419373 | validation: 0.0056546610561459985]
	TIME [epoch: 8.18 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005192166594637545		[learning rate: 6.1261e-05]
		[batch 20/20] avg loss: 0.008813994520010021		[learning rate: 6.115e-05]
	Learning Rate: 6.11498e-05
	LOSS [training: 0.0070030805573237835 | validation: 0.007570347194365119]
	TIME [epoch: 8.16 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008485974000872264		[learning rate: 6.1039e-05]
		[batch 20/20] avg loss: 0.004878870223025935		[learning rate: 6.0928e-05]
	Learning Rate: 6.09278e-05
	LOSS [training: 0.0066824221119491005 | validation: 0.008324670023586346]
	TIME [epoch: 8.15 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014330142099726394		[learning rate: 6.0817e-05]
		[batch 20/20] avg loss: 0.012011661700108457		[learning rate: 6.0707e-05]
	Learning Rate: 6.07067e-05
	LOSS [training: 0.013170901899917422 | validation: 0.010990033629577518]
	TIME [epoch: 8.2 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011172780381917624		[learning rate: 6.0596e-05]
		[batch 20/20] avg loss: 0.005559692545287391		[learning rate: 6.0486e-05]
	Learning Rate: 6.04864e-05
	LOSS [training: 0.008366236463602508 | validation: 0.004372541899961748]
	TIME [epoch: 8.18 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009426616649729094		[learning rate: 6.0377e-05]
		[batch 20/20] avg loss: 0.0033123517450717316		[learning rate: 6.0267e-05]
	Learning Rate: 6.02669e-05
	LOSS [training: 0.006369484197400413 | validation: 0.010388680519104472]
	TIME [epoch: 8.15 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005043773226725999		[learning rate: 6.0157e-05]
		[batch 20/20] avg loss: 0.003124367475047807		[learning rate: 6.0048e-05]
	Learning Rate: 6.00482e-05
	LOSS [training: 0.004084070350886903 | validation: 0.005620382387206268]
	TIME [epoch: 8.15 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009629730826012347		[learning rate: 5.9939e-05]
		[batch 20/20] avg loss: 0.005411053381038115		[learning rate: 5.983e-05]
	Learning Rate: 5.98303e-05
	LOSS [training: 0.007520392103525231 | validation: 0.005867159973506263]
	TIME [epoch: 8.2 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0039044275696132904		[learning rate: 5.9722e-05]
		[batch 20/20] avg loss: 0.009910986972867946		[learning rate: 5.9613e-05]
	Learning Rate: 5.96132e-05
	LOSS [training: 0.006907707271240617 | validation: 0.012447205050458768]
	TIME [epoch: 8.18 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038539982558882255		[learning rate: 5.9505e-05]
		[batch 20/20] avg loss: 0.003947610061612899		[learning rate: 5.9397e-05]
	Learning Rate: 5.93968e-05
	LOSS [training: 0.003900804158750562 | validation: 0.004015239698671966]
	TIME [epoch: 8.15 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031618376822311446		[learning rate: 5.9289e-05]
		[batch 20/20] avg loss: 0.006699350367575563		[learning rate: 5.9181e-05]
	Learning Rate: 5.91813e-05
	LOSS [training: 0.004930594024903354 | validation: 0.009638716527389474]
	TIME [epoch: 8.16 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005860740453668752		[learning rate: 5.9074e-05]
		[batch 20/20] avg loss: 0.0015953974861604835		[learning rate: 5.8966e-05]
	Learning Rate: 5.89665e-05
	LOSS [training: 0.0037280689699146184 | validation: 0.01003704738319246]
	TIME [epoch: 8.19 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004652148055130334		[learning rate: 5.8859e-05]
		[batch 20/20] avg loss: 0.002272788596629671		[learning rate: 5.8752e-05]
	Learning Rate: 5.87525e-05
	LOSS [training: 0.0034624683258800025 | validation: 0.009015421893631687]
	TIME [epoch: 8.18 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016387204979644346		[learning rate: 5.8646e-05]
		[batch 20/20] avg loss: 0.010171692879752364		[learning rate: 5.8539e-05]
	Learning Rate: 5.85393e-05
	LOSS [training: 0.005905206688858399 | validation: 0.004421272534599363]
	TIME [epoch: 8.15 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002799817755031552		[learning rate: 5.8433e-05]
		[batch 20/20] avg loss: 0.005625349000654876		[learning rate: 5.8327e-05]
	Learning Rate: 5.83268e-05
	LOSS [training: 0.004212583377843215 | validation: 0.0034412950707104367]
	TIME [epoch: 8.16 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0041972010080767175		[learning rate: 5.8221e-05]
		[batch 20/20] avg loss: 0.004522786940022269		[learning rate: 5.8115e-05]
	Learning Rate: 5.81152e-05
	LOSS [training: 0.004359993974049492 | validation: 0.00543112822827418]
	TIME [epoch: 8.18 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003615313267364439		[learning rate: 5.801e-05]
		[batch 20/20] avg loss: 0.002156189196141156		[learning rate: 5.7904e-05]
	Learning Rate: 5.79043e-05
	LOSS [training: 0.0028857512317527973 | validation: 0.01274269504703204]
	TIME [epoch: 8.18 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009435035176159592		[learning rate: 5.7799e-05]
		[batch 20/20] avg loss: 0.007477233019123812		[learning rate: 5.7694e-05]
	Learning Rate: 5.76941e-05
	LOSS [training: 0.0084561340976417 | validation: 0.0013865270627467573]
	TIME [epoch: 8.16 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002149974673685639		[learning rate: 5.7589e-05]
		[batch 20/20] avg loss: 0.004921397917002989		[learning rate: 5.7485e-05]
	Learning Rate: 5.74847e-05
	LOSS [training: 0.003535686295344314 | validation: 0.002106597460684461]
	TIME [epoch: 8.17 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0075742436103505475		[learning rate: 5.738e-05]
		[batch 20/20] avg loss: 0.008633542619195408		[learning rate: 5.7276e-05]
	Learning Rate: 5.72761e-05
	LOSS [training: 0.008103893114772977 | validation: 0.011594030802054898]
	TIME [epoch: 8.18 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015595520038025486		[learning rate: 5.7172e-05]
		[batch 20/20] avg loss: 0.005313377016241551		[learning rate: 5.7068e-05]
	Learning Rate: 5.70683e-05
	LOSS [training: 0.003436464510022049 | validation: 0.0007581954327915939]
	TIME [epoch: 8.17 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004417562938303521		[learning rate: 5.6965e-05]
		[batch 20/20] avg loss: 0.0063829559866904724		[learning rate: 5.6861e-05]
	Learning Rate: 5.68612e-05
	LOSS [training: 0.005400259462496997 | validation: 0.0088298486834518]
	TIME [epoch: 8.15 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005947781119049968		[learning rate: 5.6758e-05]
		[batch 20/20] avg loss: 0.007610775819787913		[learning rate: 5.6655e-05]
	Learning Rate: 5.66548e-05
	LOSS [training: 0.006779278469418938 | validation: 0.0094805572511984]
	TIME [epoch: 8.17 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018771288502016114		[learning rate: 5.6552e-05]
		[batch 20/20] avg loss: 0.006489165134602832		[learning rate: 5.6449e-05]
	Learning Rate: 5.64492e-05
	LOSS [training: 0.0041831469924022225 | validation: 0.006695483209083684]
	TIME [epoch: 8.19 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004617853688302089		[learning rate: 5.6347e-05]
		[batch 20/20] avg loss: -0.001379275282642076		[learning rate: 5.6244e-05]
	Learning Rate: 5.62444e-05
	LOSS [training: 0.0016192892028300064 | validation: 0.0026720460290332064]
	TIME [epoch: 8.17 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010613324102077102		[learning rate: 5.6142e-05]
		[batch 20/20] avg loss: 0.002660213503587568		[learning rate: 5.604e-05]
	Learning Rate: 5.60403e-05
	LOSS [training: 0.0066367688028323366 | validation: -0.0009573006047010346]
	TIME [epoch: 8.15 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006346839327224835		[learning rate: 5.5938e-05]
		[batch 20/20] avg loss: -0.0036799061427893786		[learning rate: 5.5837e-05]
	Learning Rate: 5.58369e-05
	LOSS [training: 0.0013334665922177281 | validation: 0.01128276442127095]
	TIME [epoch: 8.17 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007671991011573472		[learning rate: 5.5735e-05]
		[batch 20/20] avg loss: 0.0022591459073954797		[learning rate: 5.5634e-05]
	Learning Rate: 5.56342e-05
	LOSS [training: 0.004965568459484476 | validation: 0.012642743281875866]
	TIME [epoch: 8.18 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010286613663988206		[learning rate: 5.5533e-05]
		[batch 20/20] avg loss: -0.0001796591320943653		[learning rate: 5.5432e-05]
	Learning Rate: 5.54323e-05
	LOSS [training: 0.005053477265946921 | validation: 0.008154476980054204]
	TIME [epoch: 8.17 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018149357844593837		[learning rate: 5.5332e-05]
		[batch 20/20] avg loss: 0.00876674890757968		[learning rate: 5.5231e-05]
	Learning Rate: 5.52312e-05
	LOSS [training: 0.005290842346019531 | validation: 0.004863060741514002]
	TIME [epoch: 8.15 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0047686962519689876		[learning rate: 5.5131e-05]
		[batch 20/20] avg loss: 0.012090249207919342		[learning rate: 5.5031e-05]
	Learning Rate: 5.50307e-05
	LOSS [training: 0.008429472729944164 | validation: 0.003573027527536373]
	TIME [epoch: 8.17 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003577318052413323		[learning rate: 5.4931e-05]
		[batch 20/20] avg loss: 0.0032847953224875034		[learning rate: 5.4831e-05]
	Learning Rate: 5.4831e-05
	LOSS [training: 0.0034310566874504135 | validation: 0.0051137932340296075]
	TIME [epoch: 8.18 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003701790265363472		[learning rate: 5.4731e-05]
		[batch 20/20] avg loss: 0.00431139688023198		[learning rate: 5.4632e-05]
	Learning Rate: 5.4632e-05
	LOSS [training: 0.004006593572797726 | validation: 0.0066284544686914]
	TIME [epoch: 8.17 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00024731237597959334		[learning rate: 5.4533e-05]
		[batch 20/20] avg loss: 0.005465269330213749		[learning rate: 5.4434e-05]
	Learning Rate: 5.44338e-05
	LOSS [training: 0.002856290853096671 | validation: 0.010475244013005248]
	TIME [epoch: 8.15 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01056333754419996		[learning rate: 5.4335e-05]
		[batch 20/20] avg loss: 0.005310207609353894		[learning rate: 5.4236e-05]
	Learning Rate: 5.42362e-05
	LOSS [training: 0.007936772576776926 | validation: 0.009703630947964658]
	TIME [epoch: 8.17 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007211636170533367		[learning rate: 5.4138e-05]
		[batch 20/20] avg loss: 0.0005775943715439517		[learning rate: 5.4039e-05]
	Learning Rate: 5.40394e-05
	LOSS [training: 0.0038946152710386595 | validation: 0.006039525781809928]
	TIME [epoch: 8.18 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007178292982490721		[learning rate: 5.3941e-05]
		[batch 20/20] avg loss: 0.0038907638959725403		[learning rate: 5.3843e-05]
	Learning Rate: 5.38433e-05
	LOSS [training: 0.0055345284392316315 | validation: 0.009428226014531303]
	TIME [epoch: 8.16 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010139237686918861		[learning rate: 5.3746e-05]
		[batch 20/20] avg loss: 0.005260759917068281		[learning rate: 5.3648e-05]
	Learning Rate: 5.36479e-05
	LOSS [training: 0.007699998801993571 | validation: 0.009634196400578042]
	TIME [epoch: 8.16 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009365162861286894		[learning rate: 5.355e-05]
		[batch 20/20] avg loss: 0.0066721587159226745		[learning rate: 5.3453e-05]
	Learning Rate: 5.34532e-05
	LOSS [training: 0.008018660788604784 | validation: 0.00787559514777839]
	TIME [epoch: 8.17 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002838720170047435		[learning rate: 5.3356e-05]
		[batch 20/20] avg loss: 0.006135403028119259		[learning rate: 5.3259e-05]
	Learning Rate: 5.32592e-05
	LOSS [training: 0.004487061599083348 | validation: 0.0026517778458635174]
	TIME [epoch: 8.17 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005224766309213802		[learning rate: 5.3162e-05]
		[batch 20/20] avg loss: 0.010696016580037812		[learning rate: 5.3066e-05]
	Learning Rate: 5.30659e-05
	LOSS [training: 0.007960391444625804 | validation: 0.011558402367158058]
	TIME [epoch: 8.16 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023586703958895176		[learning rate: 5.297e-05]
		[batch 20/20] avg loss: 0.008440284499203646		[learning rate: 5.2873e-05]
	Learning Rate: 5.28734e-05
	LOSS [training: 0.005399477447546582 | validation: 0.0069483390762115835]
	TIME [epoch: 8.16 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002390490518156276		[learning rate: 5.2777e-05]
		[batch 20/20] avg loss: 0.007621601672342372		[learning rate: 5.2681e-05]
	Learning Rate: 5.26815e-05
	LOSS [training: 0.005006046095249324 | validation: 0.008569219808450683]
	TIME [epoch: 8.17 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005042572257476119		[learning rate: 5.2586e-05]
		[batch 20/20] avg loss: 0.006082531239157006		[learning rate: 5.249e-05]
	Learning Rate: 5.24903e-05
	LOSS [training: 0.005562551748316562 | validation: 0.010487939533329308]
	TIME [epoch: 8.17 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037080915449833316		[learning rate: 5.2395e-05]
		[batch 20/20] avg loss: 0.002154505034719631		[learning rate: 5.23e-05]
	Learning Rate: 5.22998e-05
	LOSS [training: 0.0029312982898514815 | validation: 0.003089620480778379]
	TIME [epoch: 8.17 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006488315216836479		[learning rate: 5.2205e-05]
		[batch 20/20] avg loss: 0.006341590993855015		[learning rate: 5.211e-05]
	Learning Rate: 5.211e-05
	LOSS [training: 0.006414953105345745 | validation: 0.006167298062143611]
	TIME [epoch: 8.16 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035037952901117095		[learning rate: 5.2015e-05]
		[batch 20/20] avg loss: 0.008974943889951893		[learning rate: 5.1921e-05]
	Learning Rate: 5.19209e-05
	LOSS [training: 0.006239369590031801 | validation: 0.0115421931671381]
	TIME [epoch: 8.17 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029128730353311524		[learning rate: 5.1827e-05]
		[batch 20/20] avg loss: -0.000629119960669954		[learning rate: 5.1732e-05]
	Learning Rate: 5.17325e-05
	LOSS [training: 0.0011418765373305993 | validation: 0.007552056352572882]
	TIME [epoch: 8.17 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004943261375665713		[learning rate: 5.1639e-05]
		[batch 20/20] avg loss: 0.005642393725891244		[learning rate: 5.1545e-05]
	Learning Rate: 5.15447e-05
	LOSS [training: 0.0052928275507784775 | validation: 0.010978285932693059]
	TIME [epoch: 8.16 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009624393776488908		[learning rate: 5.1451e-05]
		[batch 20/20] avg loss: 0.006578335920551667		[learning rate: 5.1358e-05]
	Learning Rate: 5.13577e-05
	LOSS [training: 0.008101364848520289 | validation: 0.002036210783526389]
	TIME [epoch: 8.15 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0039044871420179385		[learning rate: 5.1264e-05]
		[batch 20/20] avg loss: 0.006681200306620436		[learning rate: 5.1171e-05]
	Learning Rate: 5.11713e-05
	LOSS [training: 0.005292843724319188 | validation: 0.016287081642172168]
	TIME [epoch: 8.17 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006427783169349924		[learning rate: 5.1078e-05]
		[batch 20/20] avg loss: 0.0052675784787171		[learning rate: 5.0986e-05]
	Learning Rate: 5.09856e-05
	LOSS [training: 0.0058476808240335125 | validation: 0.0037674596267583413]
	TIME [epoch: 8.17 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007945260538191742		[learning rate: 5.0893e-05]
		[batch 20/20] avg loss: 0.002102480177563271		[learning rate: 5.0801e-05]
	Learning Rate: 5.08006e-05
	LOSS [training: 0.005023870357877508 | validation: 0.0013647302653594907]
	TIME [epoch: 8.15 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037563195921500843		[learning rate: 5.0708e-05]
		[batch 20/20] avg loss: 0.004696353690769378		[learning rate: 5.0616e-05]
	Learning Rate: 5.06162e-05
	LOSS [training: 0.0042263366414597315 | validation: 0.006104605390318445]
	TIME [epoch: 8.16 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004193522745528341		[learning rate: 5.0524e-05]
		[batch 20/20] avg loss: 0.0027151238333368655		[learning rate: 5.0433e-05]
	Learning Rate: 5.04325e-05
	LOSS [training: 0.0034543232894326038 | validation: 0.0021847522975581898]
	TIME [epoch: 8.18 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004764126565859523		[learning rate: 5.0341e-05]
		[batch 20/20] avg loss: 0.0036118603711428644		[learning rate: 5.0249e-05]
	Learning Rate: 5.02495e-05
	LOSS [training: 0.004187993468501193 | validation: 0.0050604608800178166]
	TIME [epoch: 8.17 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003674491669686039		[learning rate: 5.0158e-05]
		[batch 20/20] avg loss: 0.009399578717838555		[learning rate: 5.0067e-05]
	Learning Rate: 5.00671e-05
	LOSS [training: 0.006537035193762297 | validation: 0.012990778278485526]
	TIME [epoch: 8.16 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003999719225833763		[learning rate: 4.9976e-05]
		[batch 20/20] avg loss: 0.00880541134072146		[learning rate: 4.9885e-05]
	Learning Rate: 4.98854e-05
	LOSS [training: 0.006402565283277613 | validation: 0.00710424839515261]
	TIME [epoch: 8.17 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0041450891274043234		[learning rate: 4.9795e-05]
		[batch 20/20] avg loss: 0.008719403225863124		[learning rate: 4.9704e-05]
	Learning Rate: 4.97044e-05
	LOSS [training: 0.006432246176633724 | validation: 0.011437314460517404]
	TIME [epoch: 8.16 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007021617192011016		[learning rate: 4.9614e-05]
		[batch 20/20] avg loss: 0.008259507356488383		[learning rate: 4.9524e-05]
	Learning Rate: 4.9524e-05
	LOSS [training: 0.0076405622742497 | validation: 0.013348243269237238]
	TIME [epoch: 8.17 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007870175606269246		[learning rate: 4.9434e-05]
		[batch 20/20] avg loss: 0.008648007047536928		[learning rate: 4.9344e-05]
	Learning Rate: 4.93443e-05
	LOSS [training: 0.004717512304081926 | validation: 0.00849947857614344]
	TIME [epoch: 8.15 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0040657584035296975		[learning rate: 4.9255e-05]
		[batch 20/20] avg loss: 0.006274106724564904		[learning rate: 4.9165e-05]
	Learning Rate: 4.91652e-05
	LOSS [training: 0.0051699325640473005 | validation: 0.0012302429439937292]
	TIME [epoch: 8.18 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007412517424686191		[learning rate: 4.9076e-05]
		[batch 20/20] avg loss: 0.0029993539261636572		[learning rate: 4.8987e-05]
	Learning Rate: 4.89868e-05
	LOSS [training: 0.005205935675424924 | validation: 0.001987615008721876]
	TIME [epoch: 8.18 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024079227007300034		[learning rate: 4.8898e-05]
		[batch 20/20] avg loss: 0.006845916755501792		[learning rate: 4.8809e-05]
	Learning Rate: 4.8809e-05
	LOSS [training: 0.0046269197281158985 | validation: 0.0035924999244624717]
	TIME [epoch: 8.18 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006061577329455128		[learning rate: 4.872e-05]
		[batch 20/20] avg loss: 0.00010698634121736432		[learning rate: 4.8632e-05]
	Learning Rate: 4.86319e-05
	LOSS [training: 0.0030842818353362467 | validation: -0.0005110176281331005]
	TIME [epoch: 8.15 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0065843283800724114		[learning rate: 4.8544e-05]
		[batch 20/20] avg loss: -0.0015837048928659554		[learning rate: 4.8455e-05]
	Learning Rate: 4.84554e-05
	LOSS [training: 0.002500311743603228 | validation: 0.00263539245597246]
	TIME [epoch: 8.17 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002672113957067284		[learning rate: 4.8367e-05]
		[batch 20/20] avg loss: 0.0078809595025762		[learning rate: 4.828e-05]
	Learning Rate: 4.82795e-05
	LOSS [training: 0.005276536729821741 | validation: 0.007152808710450596]
	TIME [epoch: 8.17 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019450581699324474		[learning rate: 4.8192e-05]
		[batch 20/20] avg loss: 0.004525718021535537		[learning rate: 4.8104e-05]
	Learning Rate: 4.81043e-05
	LOSS [training: 0.0032353880957339933 | validation: 0.004466367552313039]
	TIME [epoch: 8.16 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005994123929515748		[learning rate: 4.8017e-05]
		[batch 20/20] avg loss: -0.0004870916715230207		[learning rate: 4.793e-05]
	Learning Rate: 4.79298e-05
	LOSS [training: 0.0027535161289963636 | validation: 0.0049312320428905425]
	TIME [epoch: 8.14 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00797685487473072		[learning rate: 4.7843e-05]
		[batch 20/20] avg loss: 0.0037708604764483456		[learning rate: 4.7756e-05]
	Learning Rate: 4.77558e-05
	LOSS [training: 0.005873857675589532 | validation: 0.012191679487406782]
	TIME [epoch: 8.17 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015776149984755523		[learning rate: 4.7669e-05]
		[batch 20/20] avg loss: 0.0016607700175816239		[learning rate: 4.7583e-05]
	Learning Rate: 4.75825e-05
	LOSS [training: 0.008718460001168573 | validation: -0.0004328654386015903]
	TIME [epoch: 8.18 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025654889647536297		[learning rate: 4.7496e-05]
		[batch 20/20] avg loss: 0.006892023815161302		[learning rate: 4.741e-05]
	Learning Rate: 4.74098e-05
	LOSS [training: 0.004728756389957465 | validation: 0.0002326541280497788]
	TIME [epoch: 8.15 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007087403373869689		[learning rate: 4.7324e-05]
		[batch 20/20] avg loss: 0.0005824099597274633		[learning rate: 4.7238e-05]
	Learning Rate: 4.72378e-05
	LOSS [training: 0.0038349066667985763 | validation: 0.0017671540296342022]
	TIME [epoch: 8.14 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005160724147527889		[learning rate: 4.7152e-05]
		[batch 20/20] avg loss: 0.005691625267617047		[learning rate: 4.7066e-05]
	Learning Rate: 4.70664e-05
	LOSS [training: 0.005426174707572469 | validation: 0.0066629585982041235]
	TIME [epoch: 8.17 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006096768186113909		[learning rate: 4.6981e-05]
		[batch 20/20] avg loss: 0.0033309161332141088		[learning rate: 4.6896e-05]
	Learning Rate: 4.68955e-05
	LOSS [training: 0.004713842159664009 | validation: -0.0013155598127165594]
	TIME [epoch: 8.19 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004147572155050723		[learning rate: 4.681e-05]
		[batch 20/20] avg loss: 0.003786724673356829		[learning rate: 4.6725e-05]
	Learning Rate: 4.67254e-05
	LOSS [training: 0.003967148414203775 | validation: 0.006604475905731402]
	TIME [epoch: 8.16 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004753286267475059		[learning rate: 4.6641e-05]
		[batch 20/20] avg loss: 0.010630258444771936		[learning rate: 4.6556e-05]
	Learning Rate: 4.65558e-05
	LOSS [training: 0.007691772356123498 | validation: 0.007468608468480486]
	TIME [epoch: 8.15 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007702657826695149		[learning rate: 4.6471e-05]
		[batch 20/20] avg loss: 0.009179655473400706		[learning rate: 4.6387e-05]
	Learning Rate: 4.63868e-05
	LOSS [training: 0.008441156650047929 | validation: 0.004460966403510142]
	TIME [epoch: 8.16 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005193306259883088		[learning rate: 4.6303e-05]
		[batch 20/20] avg loss: 0.002798264150600427		[learning rate: 4.6219e-05]
	Learning Rate: 4.62185e-05
	LOSS [training: 0.003995785205241758 | validation: 0.0008763848897660612]
	TIME [epoch: 8.18 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001976486739190816		[learning rate: 4.6135e-05]
		[batch 20/20] avg loss: 0.002200763044767566		[learning rate: 4.6051e-05]
	Learning Rate: 4.60508e-05
	LOSS [training: 0.00011213815278837492 | validation: 0.002384593192173149]
	TIME [epoch: 8.15 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007742752467804475		[learning rate: 4.5967e-05]
		[batch 20/20] avg loss: -0.001457892674036818		[learning rate: 4.5884e-05]
	Learning Rate: 4.58836e-05
	LOSS [training: -0.00034180871362818533 | validation: 0.007901707406031902]
	TIME [epoch: 8.13 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002253824799061827		[learning rate: 4.58e-05]
		[batch 20/20] avg loss: 0.009435592241011708		[learning rate: 4.5717e-05]
	Learning Rate: 4.57171e-05
	LOSS [training: 0.0058447085200367685 | validation: 0.00033393236401650607]
	TIME [epoch: 8.16 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035844720852760787		[learning rate: 4.5634e-05]
		[batch 20/20] avg loss: 0.005724339129567055		[learning rate: 4.5551e-05]
	Learning Rate: 4.55512e-05
	LOSS [training: 0.004654405607421567 | validation: 0.007936736751545987]
	TIME [epoch: 8.19 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017604696045264642		[learning rate: 4.5468e-05]
		[batch 20/20] avg loss: 0.008364922960255785		[learning rate: 4.5386e-05]
	Learning Rate: 4.53859e-05
	LOSS [training: 0.0033022266778646586 | validation: 0.00390881315870064]
	TIME [epoch: 8.15 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003175980163793711		[learning rate: 4.5303e-05]
		[batch 20/20] avg loss: 0.003033831573713835		[learning rate: 4.5221e-05]
	Learning Rate: 4.52212e-05
	LOSS [training: 0.0031049058687537736 | validation: 0.01285011955636967]
	TIME [epoch: 8.15 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005690103761140849		[learning rate: 4.5139e-05]
		[batch 20/20] avg loss: -0.0003045824410894132		[learning rate: 4.5057e-05]
	Learning Rate: 4.50571e-05
	LOSS [training: 0.0026927606600257183 | validation: 0.010913191476095782]
	TIME [epoch: 8.17 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007702340312210287		[learning rate: 4.4975e-05]
		[batch 20/20] avg loss: 0.0035419819934599042		[learning rate: 4.4894e-05]
	Learning Rate: 4.48936e-05
	LOSS [training: 0.005622161152835096 | validation: 0.006111820812098829]
	TIME [epoch: 8.18 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032245188016140638		[learning rate: 4.4812e-05]
		[batch 20/20] avg loss: 0.004206757629231253		[learning rate: 4.4731e-05]
	Learning Rate: 4.47307e-05
	LOSS [training: 0.0037156382154226583 | validation: 0.006804946755973351]
	TIME [epoch: 8.15 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001283834001188717		[learning rate: 4.4649e-05]
		[batch 20/20] avg loss: 0.0013121299592279054		[learning rate: 4.4568e-05]
	Learning Rate: 4.45683e-05
	LOSS [training: 0.0012979819802083111 | validation: 0.0030775521588031324]
	TIME [epoch: 8.14 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033759958769991063		[learning rate: 4.4487e-05]
		[batch 20/20] avg loss: 0.0018436739912693428		[learning rate: 4.4407e-05]
	Learning Rate: 4.44066e-05
	LOSS [training: 0.0026098349341342245 | validation: 0.002749760689202305]
	TIME [epoch: 8.17 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003511565953153819		[learning rate: 4.4326e-05]
		[batch 20/20] avg loss: 0.004645493360360071		[learning rate: 4.4245e-05]
	Learning Rate: 4.42454e-05
	LOSS [training: 0.004078529656756945 | validation: 0.012372278742963896]
	TIME [epoch: 8.19 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000674841341735985		[learning rate: 4.4165e-05]
		[batch 20/20] avg loss: 0.00681893850383142		[learning rate: 4.4085e-05]
	Learning Rate: 4.40849e-05
	LOSS [training: 0.0037468899227837025 | validation: 0.007597617179293251]
	TIME [epoch: 8.15 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005342704061193327		[learning rate: 4.4005e-05]
		[batch 20/20] avg loss: 0.005164131816390814		[learning rate: 4.3925e-05]
	Learning Rate: 4.39249e-05
	LOSS [training: 0.00525341793879207 | validation: 0.01032829818187972]
	TIME [epoch: 8.15 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0050791642569224076		[learning rate: 4.3845e-05]
		[batch 20/20] avg loss: 0.002099531421654388		[learning rate: 4.3765e-05]
	Learning Rate: 4.37655e-05
	LOSS [training: 0.003589347839288399 | validation: 0.005195112376220209]
	TIME [epoch: 8.17 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007519209829984545		[learning rate: 4.3686e-05]
		[batch 20/20] avg loss: 0.004964387903259915		[learning rate: 4.3607e-05]
	Learning Rate: 4.36067e-05
	LOSS [training: 0.00624179886662223 | validation: 0.006177318684067735]
	TIME [epoch: 8.19 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007310912246720791		[learning rate: 4.3527e-05]
		[batch 20/20] avg loss: 0.0011311252766931438		[learning rate: 4.3448e-05]
	Learning Rate: 4.34484e-05
	LOSS [training: 0.004221018761706967 | validation: 0.002783355447180079]
	TIME [epoch: 8.15 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003873327991639819		[learning rate: 4.3369e-05]
		[batch 20/20] avg loss: 0.0047933476379128604		[learning rate: 4.3291e-05]
	Learning Rate: 4.32907e-05
	LOSS [training: 0.004333337814776338 | validation: 0.006477746871048599]
	TIME [epoch: 8.15 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003218097116562073		[learning rate: 4.3212e-05]
		[batch 20/20] avg loss: 0.00626873957256841		[learning rate: 4.3134e-05]
	Learning Rate: 4.31336e-05
	LOSS [training: 0.004743418344565242 | validation: 0.00874851190557139]
	TIME [epoch: 8.16 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0062816178167434		[learning rate: 4.3055e-05]
		[batch 20/20] avg loss: 0.002935933852864214		[learning rate: 4.2977e-05]
	Learning Rate: 4.29771e-05
	LOSS [training: 0.004608775834803807 | validation: 0.010984287560470646]
	TIME [epoch: 8.19 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006435992065680933		[learning rate: 4.2899e-05]
		[batch 20/20] avg loss: 0.0051607513067142554		[learning rate: 4.2821e-05]
	Learning Rate: 4.28211e-05
	LOSS [training: 0.005798371686197595 | validation: 0.005000212950454667]
	TIME [epoch: 8.15 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013643903402256394		[learning rate: 4.2743e-05]
		[batch 20/20] avg loss: 0.002843088973026416		[learning rate: 4.2666e-05]
	Learning Rate: 4.26657e-05
	LOSS [training: 0.0021037396566260273 | validation: 0.0015494070246191067]
	TIME [epoch: 8.15 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004402569925200481		[learning rate: 4.2588e-05]
		[batch 20/20] avg loss: 0.0024350560039832375		[learning rate: 4.2511e-05]
	Learning Rate: 4.25109e-05
	LOSS [training: 0.003418812964591859 | validation: 0.006649226044626699]
	TIME [epoch: 8.17 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003522205277567962		[learning rate: 4.2434e-05]
		[batch 20/20] avg loss: 0.007688587604736912		[learning rate: 4.2357e-05]
	Learning Rate: 4.23566e-05
	LOSS [training: 0.005605396441152438 | validation: 0.007679626974615286]
	TIME [epoch: 8.19 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004003937741324204		[learning rate: 4.228e-05]
		[batch 20/20] avg loss: -0.0006501198932452674		[learning rate: 4.2203e-05]
	Learning Rate: 4.22029e-05
	LOSS [training: 0.0016769089240394682 | validation: 0.00391845434021187]
	TIME [epoch: 8.15 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031431743887006232		[learning rate: 4.2126e-05]
		[batch 20/20] avg loss: 0.00513010317739309		[learning rate: 4.205e-05]
	Learning Rate: 4.20497e-05
	LOSS [training: 0.004136638783046856 | validation: 0.004292788675846608]
	TIME [epoch: 8.15 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004147162004746215		[learning rate: 4.1973e-05]
		[batch 20/20] avg loss: 0.0002797173435617249		[learning rate: 4.1897e-05]
	Learning Rate: 4.18971e-05
	LOSS [training: 0.0022134396741539694 | validation: 0.0016803977390452633]
	TIME [epoch: 8.17 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031753072439964945		[learning rate: 4.1821e-05]
		[batch 20/20] avg loss: 0.004604094117957797		[learning rate: 4.1745e-05]
	Learning Rate: 4.17451e-05
	LOSS [training: 0.0038897006809771456 | validation: 0.0100736178587553]
	TIME [epoch: 8.2 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0056969650431744605		[learning rate: 4.1669e-05]
		[batch 20/20] avg loss: 0.003821728742143369		[learning rate: 4.1594e-05]
	Learning Rate: 4.15936e-05
	LOSS [training: 0.004759346892658915 | validation: 0.006010410687333613]
	TIME [epoch: 8.15 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00618131877916272		[learning rate: 4.1518e-05]
		[batch 20/20] avg loss: 0.0003968215559400336		[learning rate: 4.1443e-05]
	Learning Rate: 4.14426e-05
	LOSS [training: 0.003289070167551378 | validation: 0.00261691032631465]
	TIME [epoch: 8.14 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00817229947771479		[learning rate: 4.1367e-05]
		[batch 20/20] avg loss: 0.0018070847847893039		[learning rate: 4.1292e-05]
	Learning Rate: 4.12922e-05
	LOSS [training: 0.004989692131252048 | validation: 0.010167033108841636]
	TIME [epoch: 8.17 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006782846926706386		[learning rate: 4.1217e-05]
		[batch 20/20] avg loss: 0.005567993133089944		[learning rate: 4.1142e-05]
	Learning Rate: 4.11424e-05
	LOSS [training: 0.006175420029898165 | validation: 0.007136931699086061]
	TIME [epoch: 8.19 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004865661295362587		[learning rate: 4.1068e-05]
		[batch 20/20] avg loss: 5.68340288252837e-06		[learning rate: 4.0993e-05]
	Learning Rate: 4.09931e-05
	LOSS [training: 0.0024356723491225575 | validation: -0.00043461067950067295]
	TIME [epoch: 8.14 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020976186124535266		[learning rate: 4.0919e-05]
		[batch 20/20] avg loss: 0.0021228247799085095		[learning rate: 4.0844e-05]
	Learning Rate: 4.08443e-05
	LOSS [training: 0.002110221696181018 | validation: 0.0040875659935384525]
	TIME [epoch: 8.15 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0002174368513019755		[learning rate: 4.077e-05]
		[batch 20/20] avg loss: 0.005777129435388378		[learning rate: 4.0696e-05]
	Learning Rate: 4.06961e-05
	LOSS [training: 0.002779846292043201 | validation: 0.0031187796498647747]
	TIME [epoch: 8.17 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00545812978462241		[learning rate: 4.0622e-05]
		[batch 20/20] avg loss: -0.0017117187760221236		[learning rate: 4.0548e-05]
	Learning Rate: 4.05484e-05
	LOSS [training: 0.0018732055043001437 | validation: 0.008040436504197482]
	TIME [epoch: 8.2 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00011587567754061966		[learning rate: 4.0475e-05]
		[batch 20/20] avg loss: 0.007007971922772543		[learning rate: 4.0401e-05]
	Learning Rate: 4.04012e-05
	LOSS [training: 0.0034460481226159614 | validation: 0.009947967953678786]
	TIME [epoch: 8.14 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014934829638579023		[learning rate: 4.0328e-05]
		[batch 20/20] avg loss: 0.005193203167655142		[learning rate: 4.0255e-05]
	Learning Rate: 4.02546e-05
	LOSS [training: 0.0033433430657565225 | validation: 0.0019780820403607477]
	TIME [epoch: 8.15 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001440782076680697		[learning rate: 4.0182e-05]
		[batch 20/20] avg loss: 0.003266197943913735		[learning rate: 4.0109e-05]
	Learning Rate: 4.01085e-05
	LOSS [training: 0.0023534900102972164 | validation: 0.0022543422740768848]
	TIME [epoch: 8.18 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007926157724558121		[learning rate: 4.0036e-05]
		[batch 20/20] avg loss: -0.0016449883717112933		[learning rate: 3.9963e-05]
	Learning Rate: 3.9963e-05
	LOSS [training: 0.0031405846764234143 | validation: 0.0019728197318863343]
	TIME [epoch: 8.19 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006341709385115873		[learning rate: 3.989e-05]
		[batch 20/20] avg loss: 0.005765473640525666		[learning rate: 3.9818e-05]
	Learning Rate: 3.9818e-05
	LOSS [training: 0.006053591512820771 | validation: -0.003027051168529392]
	TIME [epoch: 8.14 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00032491067962857437		[learning rate: 3.9746e-05]
		[batch 20/20] avg loss: 0.0074599570549046396		[learning rate: 3.9673e-05]
	Learning Rate: 3.96735e-05
	LOSS [training: 0.003567523187638033 | validation: 0.007155963531450714]
	TIME [epoch: 8.14 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006812626715032725		[learning rate: 3.9601e-05]
		[batch 20/20] avg loss: 0.00208714539696254		[learning rate: 3.9529e-05]
	Learning Rate: 3.95295e-05
	LOSS [training: 0.004449886055997633 | validation: 0.010285832351573602]
	TIME [epoch: 8.16 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0041364099532043225		[learning rate: 3.9458e-05]
		[batch 20/20] avg loss: 0.011400654069321583		[learning rate: 3.9386e-05]
	Learning Rate: 3.9386e-05
	LOSS [training: 0.00363212205805863 | validation: 0.003177348832951594]
	TIME [epoch: 8.18 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004980526431031213		[learning rate: 3.9315e-05]
		[batch 20/20] avg loss: 0.0022183393525863388		[learning rate: 3.9243e-05]
	Learning Rate: 3.92431e-05
	LOSS [training: 0.0035994328918087766 | validation: 0.009377558632795184]
	TIME [epoch: 8.14 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002689855609795907		[learning rate: 3.9172e-05]
		[batch 20/20] avg loss: 0.0038211153485682493		[learning rate: 3.9101e-05]
	Learning Rate: 3.91007e-05
	LOSS [training: 0.0032554854791820775 | validation: 0.0028633248159546684]
	TIME [epoch: 8.14 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003969348885577912		[learning rate: 3.903e-05]
		[batch 20/20] avg loss: 0.004862680327496997		[learning rate: 3.8959e-05]
	Learning Rate: 3.89588e-05
	LOSS [training: 0.004416014606537454 | validation: 0.008378750793967056]
	TIME [epoch: 8.17 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012409118160945298		[learning rate: 3.8888e-05]
		[batch 20/20] avg loss: 0.0076860401933008		[learning rate: 3.8817e-05]
	Learning Rate: 3.88174e-05
	LOSS [training: 0.004463476004697665 | validation: 0.0021416747069576882]
	TIME [epoch: 8.19 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027294769766555214		[learning rate: 3.8747e-05]
		[batch 20/20] avg loss: 0.0023529809946747715		[learning rate: 3.8677e-05]
	Learning Rate: 3.86765e-05
	LOSS [training: 0.0025412289856651467 | validation: 0.008392030848182182]
	TIME [epoch: 8.15 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012764402243072187		[learning rate: 3.8606e-05]
		[batch 20/20] avg loss: -0.0011945170532333105		[learning rate: 3.8536e-05]
	Learning Rate: 3.85362e-05
	LOSS [training: 0.005784942594919438 | validation: 0.010972365711903671]
	TIME [epoch: 8.14 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002189104985442053		[learning rate: 3.8466e-05]
		[batch 20/20] avg loss: 0.005505026273027723		[learning rate: 3.8396e-05]
	Learning Rate: 3.83963e-05
	LOSS [training: 0.003847065629234888 | validation: 0.00815587364877332]
	TIME [epoch: 8.18 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005583960599942169		[learning rate: 3.8327e-05]
		[batch 20/20] avg loss: 0.004969959265801115		[learning rate: 3.8257e-05]
	Learning Rate: 3.8257e-05
	LOSS [training: 0.005276959932871642 | validation: 0.0012098970729273973]
	TIME [epoch: 8.2 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031612529684422394		[learning rate: 3.8187e-05]
		[batch 20/20] avg loss: 0.002881980852340959		[learning rate: 3.8118e-05]
	Learning Rate: 3.81181e-05
	LOSS [training: 0.0030216169103915985 | validation: 0.005372849699150196]
	TIME [epoch: 8.15 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006984854121382724		[learning rate: 3.8049e-05]
		[batch 20/20] avg loss: 0.004546786682026285		[learning rate: 3.798e-05]
	Learning Rate: 3.79798e-05
	LOSS [training: 0.0019241506349440066 | validation: 0.004959753597953143]
	TIME [epoch: 8.15 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00024878907092930585		[learning rate: 3.7911e-05]
		[batch 20/20] avg loss: 0.0025196519854036935		[learning rate: 3.7842e-05]
	Learning Rate: 3.7842e-05
	LOSS [training: 0.0011354314572371938 | validation: 0.00795060531414495]
	TIME [epoch: 8.18 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007155004079213418		[learning rate: 3.7773e-05]
		[batch 20/20] avg loss: -0.0009866079095090766		[learning rate: 3.7705e-05]
	Learning Rate: 3.77046e-05
	LOSS [training: 0.0030841980848521702 | validation: 7.309743586825625e-05]
	TIME [epoch: 8.18 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0078803389505011		[learning rate: 3.7636e-05]
		[batch 20/20] avg loss: 0.0029223915443852347		[learning rate: 3.7568e-05]
	Learning Rate: 3.75678e-05
	LOSS [training: 0.005401365247443169 | validation: 0.00886456411824146]
	TIME [epoch: 8.15 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008585792140107986		[learning rate: 3.75e-05]
		[batch 20/20] avg loss: 0.000489370697903797		[learning rate: 3.7431e-05]
	Learning Rate: 3.74315e-05
	LOSS [training: 0.004537581419005892 | validation: 0.0005642344684767412]
	TIME [epoch: 8.15 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00594307908038949		[learning rate: 3.7363e-05]
		[batch 20/20] avg loss: 0.005151601475597879		[learning rate: 3.7296e-05]
	Learning Rate: 3.72956e-05
	LOSS [training: 0.005547340277993684 | validation: -0.00045515957111412204]
	TIME [epoch: 8.17 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003187887825012539		[learning rate: 3.7228e-05]
		[batch 20/20] avg loss: 0.00569172986579948		[learning rate: 3.716e-05]
	Learning Rate: 3.71603e-05
	LOSS [training: 0.00443980884540601 | validation: 0.004333935458839658]
	TIME [epoch: 8.18 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008761307098089874		[learning rate: 3.7093e-05]
		[batch 20/20] avg loss: 0.003330011710339474		[learning rate: 3.7025e-05]
	Learning Rate: 3.70254e-05
	LOSS [training: 0.0012269405002652434 | validation: 0.006096332136167264]
	TIME [epoch: 8.14 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0001845717815527871		[learning rate: 3.6958e-05]
		[batch 20/20] avg loss: 0.0038858031583755247		[learning rate: 3.6891e-05]
	Learning Rate: 3.68911e-05
	LOSS [training: 0.002035187469964156 | validation: 0.008128575748992426]
	TIME [epoch: 8.15 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003154259474706901		[learning rate: 3.6824e-05]
		[batch 20/20] avg loss: 0.0014292292962684253		[learning rate: 3.6757e-05]
	Learning Rate: 3.67572e-05
	LOSS [training: 0.002291744385487663 | validation: 0.003657065154475465]
	TIME [epoch: 8.17 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002010007882309089		[learning rate: 3.669e-05]
		[batch 20/20] avg loss: 0.0037326689072253094		[learning rate: 3.6624e-05]
	Learning Rate: 3.66238e-05
	LOSS [training: 0.002871338394767199 | validation: 0.00584295563360968]
	TIME [epoch: 8.18 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025916410666499074		[learning rate: 3.6557e-05]
		[batch 20/20] avg loss: 0.005836685455628665		[learning rate: 3.6491e-05]
	Learning Rate: 3.64909e-05
	LOSS [training: 0.004214163261139286 | validation: 0.00664259545376783]
	TIME [epoch: 8.14 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00033229064859510097		[learning rate: 3.6425e-05]
		[batch 20/20] avg loss: 0.0037627217820303065		[learning rate: 3.6358e-05]
	Learning Rate: 3.63584e-05
	LOSS [training: 0.002047506215312704 | validation: 0.005065494798222007]
	TIME [epoch: 8.15 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025021691708572795		[learning rate: 3.6292e-05]
		[batch 20/20] avg loss: 0.0020002592905478374		[learning rate: 3.6226e-05]
	Learning Rate: 3.62265e-05
	LOSS [training: 0.002251214230702558 | validation: 0.0016871079311076961]
	TIME [epoch: 8.18 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017746397954645893		[learning rate: 3.6161e-05]
		[batch 20/20] avg loss: 0.0047456561229657		[learning rate: 3.6095e-05]
	Learning Rate: 3.6095e-05
	LOSS [training: 0.0032601479592151454 | validation: 0.004104669591989126]
	TIME [epoch: 8.17 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011558277649490836		[learning rate: 3.6029e-05]
		[batch 20/20] avg loss: 0.005444057465331269		[learning rate: 3.5964e-05]
	Learning Rate: 3.5964e-05
	LOSS [training: 0.0032999426151401764 | validation: 0.0016681458338903977]
	TIME [epoch: 8.15 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004548686929470686		[learning rate: 3.5899e-05]
		[batch 20/20] avg loss: 0.0034730107255810084		[learning rate: 3.5834e-05]
	Learning Rate: 3.58335e-05
	LOSS [training: 0.004010848827525847 | validation: 0.009888038232324425]
	TIME [epoch: 8.15 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006841535338663132		[learning rate: 3.5768e-05]
		[batch 20/20] avg loss: 0.0035979938824546283		[learning rate: 3.5703e-05]
	Learning Rate: 3.57035e-05
	LOSS [training: 0.00521976461055888 | validation: 0.008530635611486216]
	TIME [epoch: 8.19 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011511208957446927		[learning rate: 3.5639e-05]
		[batch 20/20] avg loss: 0.007758478433889289		[learning rate: 3.5574e-05]
	Learning Rate: 3.55739e-05
	LOSS [training: 0.00445479966481699 | validation: 0.005140876481428383]
	TIME [epoch: 8.17 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009225787530150738		[learning rate: 3.5509e-05]
		[batch 20/20] avg loss: 0.0015353750146563103		[learning rate: 3.5445e-05]
	Learning Rate: 3.54448e-05
	LOSS [training: 0.005380581272403525 | validation: 0.010166119804277613]
	TIME [epoch: 8.16 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0007165471469445458		[learning rate: 3.538e-05]
		[batch 20/20] avg loss: 0.009726125714503205		[learning rate: 3.5316e-05]
	Learning Rate: 3.53162e-05
	LOSS [training: 0.0045047892837793285 | validation: 0.0022603282602791487]
	TIME [epoch: 8.14 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004132220361642233		[learning rate: 3.5252e-05]
		[batch 20/20] avg loss: 0.004236929184148892		[learning rate: 3.5188e-05]
	Learning Rate: 3.5188e-05
	LOSS [training: 0.004184574772895562 | validation: 0.002274348062896788]
	TIME [epoch: 8.18 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0050164854280707865		[learning rate: 3.5124e-05]
		[batch 20/20] avg loss: 0.0025357630662516772		[learning rate: 3.506e-05]
	Learning Rate: 3.50603e-05
	LOSS [training: 0.003776124247161232 | validation: 0.004885004913386578]
	TIME [epoch: 8.18 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030942367702199297		[learning rate: 3.4997e-05]
		[batch 20/20] avg loss: 0.002995068142294887		[learning rate: 3.4933e-05]
	Learning Rate: 3.49331e-05
	LOSS [training: 0.003044652456257408 | validation: 0.0055860010038827]
	TIME [epoch: 8.15 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00011415981388098618		[learning rate: 3.487e-05]
		[batch 20/20] avg loss: -0.00043184408077144836		[learning rate: 3.4806e-05]
	Learning Rate: 3.48063e-05
	LOSS [training: -0.0002730019473262172 | validation: 0.0034614819567301454]
	TIME [epoch: 8.14 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004321212341500833		[learning rate: 3.4743e-05]
		[batch 20/20] avg loss: 0.000968688844804768		[learning rate: 3.468e-05]
	Learning Rate: 3.468e-05
	LOSS [training: 0.0026449505931528004 | validation: 0.01476549695346198]
	TIME [epoch: 8.19 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00433987430957852		[learning rate: 3.4617e-05]
		[batch 20/20] avg loss: 0.007801316158407913		[learning rate: 3.4554e-05]
	Learning Rate: 3.45541e-05
	LOSS [training: 0.006070595233993217 | validation: -0.0005622868356205586]
	TIME [epoch: 8.17 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028079464069045634		[learning rate: 3.4491e-05]
		[batch 20/20] avg loss: 0.0044390120184208005		[learning rate: 3.4429e-05]
	Learning Rate: 3.44287e-05
	LOSS [training: 0.0036234792126626824 | validation: 0.010658161369563261]
	TIME [epoch: 8.15 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021439302278388876		[learning rate: 3.4366e-05]
		[batch 20/20] avg loss: 0.0013541152461888028		[learning rate: 3.4304e-05]
	Learning Rate: 3.43038e-05
	LOSS [training: 0.0017490227370138458 | validation: 0.006854269944924491]
	TIME [epoch: 8.14 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010691637604357622		[learning rate: 3.4241e-05]
		[batch 20/20] avg loss: -0.0016689681268778195		[learning rate: 3.4179e-05]
	Learning Rate: 3.41793e-05
	LOSS [training: -0.0002999021832210287 | validation: 0.004135241628505293]
	TIME [epoch: 8.19 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0001975422207818123		[learning rate: 3.4117e-05]
		[batch 20/20] avg loss: 0.010619322864277324		[learning rate: 3.4055e-05]
	Learning Rate: 3.40553e-05
	LOSS [training: 0.0054084325425295685 | validation: -0.0011827990305927037]
	TIME [epoch: 8.17 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005387819513271014		[learning rate: 3.3993e-05]
		[batch 20/20] avg loss: 0.0059353037340409195		[learning rate: 3.3932e-05]
	Learning Rate: 3.39317e-05
	LOSS [training: 0.0032370428426840105 | validation: 0.005860354840960087]
	TIME [epoch: 8.15 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036957608972748935		[learning rate: 3.387e-05]
		[batch 20/20] avg loss: 0.005182674288273885		[learning rate: 3.3809e-05]
	Learning Rate: 3.38085e-05
	LOSS [training: 0.00443921759277439 | validation: 0.0012682503288822993]
	TIME [epoch: 8.15 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002734068652878829		[learning rate: 3.3747e-05]
		[batch 20/20] avg loss: 0.001837568078157865		[learning rate: 3.3686e-05]
	Learning Rate: 3.36858e-05
	LOSS [training: 0.002285818365518347 | validation: 0.005794728367298083]
	TIME [epoch: 8.2 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010017322131810863		[learning rate: 3.3625e-05]
		[batch 20/20] avg loss: 0.0068456115637237774		[learning rate: 3.3564e-05]
	Learning Rate: 3.35636e-05
	LOSS [training: 0.008431466847767322 | validation: 0.00304768562127273]
	TIME [epoch: 8.17 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007712386046366386		[learning rate: 3.3503e-05]
		[batch 20/20] avg loss: 0.006775271347088096		[learning rate: 3.3442e-05]
	Learning Rate: 3.34418e-05
	LOSS [training: 0.007243828696727241 | validation: 0.001817728593689143]
	TIME [epoch: 8.15 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0059463419576105256		[learning rate: 3.3381e-05]
		[batch 20/20] avg loss: 0.002660500120010926		[learning rate: 3.332e-05]
	Learning Rate: 3.33204e-05
	LOSS [training: 0.0043034210388107254 | validation: 0.005676156809125301]
	TIME [epoch: 8.16 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011088474997961377		[learning rate: 3.326e-05]
		[batch 20/20] avg loss: 0.009981115599052105		[learning rate: 3.32e-05]
	Learning Rate: 3.31995e-05
	LOSS [training: 0.005544981549424121 | validation: 0.007649633658985653]
	TIME [epoch: 8.19 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006817400867316819		[learning rate: 3.3139e-05]
		[batch 20/20] avg loss: 0.006800720714221997		[learning rate: 3.3079e-05]
	Learning Rate: 3.3079e-05
	LOSS [training: 0.006809060790769409 | validation: 0.004917709675571699]
	TIME [epoch: 8.17 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00020987731700749783		[learning rate: 3.3019e-05]
		[batch 20/20] avg loss: 0.0020204573441970014		[learning rate: 3.2959e-05]
	Learning Rate: 3.2959e-05
	LOSS [training: 0.000905290013594752 | validation: 0.00952390573058115]
	TIME [epoch: 8.16 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009824669626326731		[learning rate: 3.2899e-05]
		[batch 20/20] avg loss: -0.0020150806310333335		[learning rate: 3.2839e-05]
	Learning Rate: 3.28394e-05
	LOSS [training: 0.0039047944976466986 | validation: 0.003069210313424176]
	TIME [epoch: 8.16 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0063007042052290576		[learning rate: 3.278e-05]
		[batch 20/20] avg loss: 0.003635128631943491		[learning rate: 3.272e-05]
	Learning Rate: 3.27202e-05
	LOSS [training: 0.004967916418586274 | validation: -0.00333730158272938]
	TIME [epoch: 8.18 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005050768354192282		[learning rate: 3.2661e-05]
		[batch 20/20] avg loss: 0.002394311577558387		[learning rate: 3.2601e-05]
	Learning Rate: 3.26014e-05
	LOSS [training: 0.003722539965875334 | validation: 0.006400596655996225]
	TIME [epoch: 8.16 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001884259554407592		[learning rate: 3.2542e-05]
		[batch 20/20] avg loss: 0.0065604373141209815		[learning rate: 3.2483e-05]
	Learning Rate: 3.24831e-05
	LOSS [training: 0.004222348434264287 | validation: 0.003777635712608405]
	TIME [epoch: 8.15 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008256278929069012		[learning rate: 3.2424e-05]
		[batch 20/20] avg loss: 0.0023090064845751315		[learning rate: 3.2365e-05]
	Learning Rate: 3.23652e-05
	LOSS [training: 0.005282642706822072 | validation: 0.0038231937955582975]
	TIME [epoch: 8.17 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00432033600874835		[learning rate: 3.2306e-05]
		[batch 20/20] avg loss: 0.003243375265514351		[learning rate: 3.2248e-05]
	Learning Rate: 3.22478e-05
	LOSS [training: 0.003781855637131351 | validation: 0.0017949217277958376]
	TIME [epoch: 8.17 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004849268308284638		[learning rate: 3.2189e-05]
		[batch 20/20] avg loss: 0.006371016506733369		[learning rate: 3.2131e-05]
	Learning Rate: 3.21308e-05
	LOSS [training: 0.0056101424075090025 | validation: 8.025704846415307e-05]
	TIME [epoch: 8.16 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005355675644003401		[learning rate: 3.2072e-05]
		[batch 20/20] avg loss: 0.0038142714388135768		[learning rate: 3.2014e-05]
	Learning Rate: 3.20142e-05
	LOSS [training: 0.004584973541408488 | validation: -0.00045569083815877705]
	TIME [epoch: 8.15 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038861376269379655		[learning rate: 3.1956e-05]
		[batch 20/20] avg loss: 0.007970887935420192		[learning rate: 3.1898e-05]
	Learning Rate: 3.1898e-05
	LOSS [training: 0.005928512781179078 | validation: -0.0011196777714987186]
	TIME [epoch: 8.17 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025378663556000275		[learning rate: 3.184e-05]
		[batch 20/20] avg loss: -0.0011363676032983675		[learning rate: 3.1782e-05]
	Learning Rate: 3.17822e-05
	LOSS [training: 0.0007007493761508303 | validation: 0.0036718690477914365]
	TIME [epoch: 8.18 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013727265776216143		[learning rate: 3.1725e-05]
		[batch 20/20] avg loss: 0.0055045402612058855		[learning rate: 3.1667e-05]
	Learning Rate: 3.16669e-05
	LOSS [training: 0.0020659068417921353 | validation: 0.005799399960660766]
	TIME [epoch: 8.17 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021347521375190753		[learning rate: 3.1609e-05]
		[batch 20/20] avg loss: 0.006287225382242255		[learning rate: 3.1552e-05]
	Learning Rate: 3.1552e-05
	LOSS [training: 0.004210988759880665 | validation: 0.0036675028688553575]
	TIME [epoch: 8.15 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000588777115367241		[learning rate: 3.1495e-05]
		[batch 20/20] avg loss: 0.0051936627451612104		[learning rate: 3.1437e-05]
	Learning Rate: 3.14375e-05
	LOSS [training: 0.0028912199302642263 | validation: 0.0024156655392027888]
	TIME [epoch: 8.17 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004543026317250927		[learning rate: 3.138e-05]
		[batch 20/20] avg loss: 9.278133329702642e-05		[learning rate: 3.1323e-05]
	Learning Rate: 3.13234e-05
	LOSS [training: 0.002317903825273977 | validation: 0.005746132473806164]
	TIME [epoch: 8.18 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026349874250491006		[learning rate: 3.1266e-05]
		[batch 20/20] avg loss: 0.007795237156683887		[learning rate: 3.121e-05]
	Learning Rate: 3.12097e-05
	LOSS [training: 0.005215112290866493 | validation: -0.001152561114775857]
	TIME [epoch: 8.15 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007611039984983595		[learning rate: 3.1153e-05]
		[batch 20/20] avg loss: 0.00996662484743127		[learning rate: 3.1096e-05]
	Learning Rate: 3.10964e-05
	LOSS [training: 0.008788832416207431 | validation: 0.007072409608165273]
	TIME [epoch: 8.15 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011085782131590474		[learning rate: 3.104e-05]
		[batch 20/20] avg loss: 0.0030942995984457582		[learning rate: 3.0984e-05]
	Learning Rate: 3.09836e-05
	LOSS [training: 0.007090040865018117 | validation: 0.002969127441201823]
	TIME [epoch: 8.17 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002991409150350161		[learning rate: 3.0927e-05]
		[batch 20/20] avg loss: 0.0020358263555493284		[learning rate: 3.0871e-05]
	Learning Rate: 3.08711e-05
	LOSS [training: 0.0025136177529497448 | validation: 0.007434715488645563]
	TIME [epoch: 8.17 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016058409021992696		[learning rate: 3.0815e-05]
		[batch 20/20] avg loss: 0.006325750648682067		[learning rate: 3.0759e-05]
	Learning Rate: 3.07591e-05
	LOSS [training: 0.0039657957754406685 | validation: 0.009249266783742886]
	TIME [epoch: 8.15 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006381379039522668		[learning rate: 3.0703e-05]
		[batch 20/20] avg loss: 0.006039323687670611		[learning rate: 3.0647e-05]
	Learning Rate: 3.06475e-05
	LOSS [training: 0.006210351363596639 | validation: 0.0040465740480168915]
	TIME [epoch: 8.17 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002872734985806463		[learning rate: 3.0592e-05]
		[batch 20/20] avg loss: 0.0038874065741274035		[learning rate: 3.0536e-05]
	Learning Rate: 3.05363e-05
	LOSS [training: 0.003380070779966933 | validation: 0.008634644695345]
	TIME [epoch: 8.17 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036939589023124524		[learning rate: 3.0481e-05]
		[batch 20/20] avg loss: 0.004909357874056068		[learning rate: 3.0425e-05]
	Learning Rate: 3.04254e-05
	LOSS [training: 0.004301658388184261 | validation: 0.0016722795547633048]
	TIME [epoch: 8.17 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0003187663316380054		[learning rate: 3.037e-05]
		[batch 20/20] avg loss: 0.007054937914283241		[learning rate: 3.0315e-05]
	Learning Rate: 3.0315e-05
	LOSS [training: 0.003686852122960623 | validation: 0.005350862476108126]
	TIME [epoch: 8.15 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004553231612436336		[learning rate: 3.026e-05]
		[batch 20/20] avg loss: 0.0064233496774325355		[learning rate: 3.0205e-05]
	Learning Rate: 3.0205e-05
	LOSS [training: 0.005488290644934435 | validation: 0.004832194841083315]
	TIME [epoch: 8.17 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004094882140776271		[learning rate: 3.015e-05]
		[batch 20/20] avg loss: 0.0034657934217885615		[learning rate: 3.0095e-05]
	Learning Rate: 3.00954e-05
	LOSS [training: 0.0037803377812824163 | validation: 0.003193550485622389]
	TIME [epoch: 8.16 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028736606883320494		[learning rate: 3.0041e-05]
		[batch 20/20] avg loss: -0.0008803422890671731		[learning rate: 2.9986e-05]
	Learning Rate: 2.99862e-05
	LOSS [training: 0.0009966591996324382 | validation: 0.007144757230295295]
	TIME [epoch: 8.23 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005041204013010718		[learning rate: 2.9932e-05]
		[batch 20/20] avg loss: 0.007241524417571002		[learning rate: 2.9877e-05]
	Learning Rate: 2.98774e-05
	LOSS [training: 0.0038728224094360373 | validation: 0.0017531383608037207]
	TIME [epoch: 8.16 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005833206629757231		[learning rate: 2.9823e-05]
		[batch 20/20] avg loss: 0.002205675393670192		[learning rate: 2.9769e-05]
	Learning Rate: 2.97689e-05
	LOSS [training: 0.004019441011713712 | validation: 0.004376863618054651]
	TIME [epoch: 8.17 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0055994473364794056		[learning rate: 2.9715e-05]
		[batch 20/20] avg loss: 0.005801160835884808		[learning rate: 2.9661e-05]
	Learning Rate: 2.96609e-05
	LOSS [training: 0.005700304086182107 | validation: -0.00032389977337683555]
	TIME [epoch: 8.17 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006692986554282571		[learning rate: 2.9607e-05]
		[batch 20/20] avg loss: 0.00010776354689773479		[learning rate: 2.9553e-05]
	Learning Rate: 2.95533e-05
	LOSS [training: 0.003400375050590153 | validation: 0.003207976445991847]
	TIME [epoch: 8.16 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005722651409251526		[learning rate: 2.95e-05]
		[batch 20/20] avg loss: 0.0025145484169094325		[learning rate: 2.9446e-05]
	Learning Rate: 2.9446e-05
	LOSS [training: 0.00411859991308048 | validation: 0.0003499927746306031]
	TIME [epoch: 8.14 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013187484774419835		[learning rate: 2.9393e-05]
		[batch 20/20] avg loss: 0.007978599841018508		[learning rate: 2.9339e-05]
	Learning Rate: 2.93391e-05
	LOSS [training: 0.0046486741592302455 | validation: 0.013460088750125585]
	TIME [epoch: 8.16 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00852104697969274		[learning rate: 2.9286e-05]
		[batch 20/20] avg loss: 0.0027952019868898344		[learning rate: 2.9233e-05]
	Learning Rate: 2.92327e-05
	LOSS [training: 0.005658124483291287 | validation: 0.007438881478096977]
	TIME [epoch: 8.17 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0070702859292422465		[learning rate: 2.918e-05]
		[batch 20/20] avg loss: 0.003795225470911138		[learning rate: 2.9127e-05]
	Learning Rate: 2.91266e-05
	LOSS [training: 0.005432755700076693 | validation: 0.0015494082215186259]
	TIME [epoch: 8.16 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004879405091764188		[learning rate: 2.9074e-05]
		[batch 20/20] avg loss: 0.004415089111978091		[learning rate: 2.9021e-05]
	Learning Rate: 2.90209e-05
	LOSS [training: 0.004647247101871139 | validation: 0.005519267285323986]
	TIME [epoch: 8.15 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004673986470233226		[learning rate: 2.8968e-05]
		[batch 20/20] avg loss: 0.008377152702269835		[learning rate: 2.8916e-05]
	Learning Rate: 2.89156e-05
	LOSS [training: 0.006525569586251531 | validation: 0.011181253515480546]
	TIME [epoch: 8.17 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005242720949649115		[learning rate: 2.8863e-05]
		[batch 20/20] avg loss: 0.00199742946357873		[learning rate: 2.8811e-05]
	Learning Rate: 2.88106e-05
	LOSS [training: 0.003620075206613923 | validation: 0.0024029705879905715]
	TIME [epoch: 8.17 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00844386671699131		[learning rate: 2.8758e-05]
		[batch 20/20] avg loss: 0.0014849924549191172		[learning rate: 2.8706e-05]
	Learning Rate: 2.87061e-05
	LOSS [training: 0.004964429585955213 | validation: 0.005000325538051277]
	TIME [epoch: 8.18 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007029328829924274		[learning rate: 2.8654e-05]
		[batch 20/20] avg loss: 0.003175215733928787		[learning rate: 2.8602e-05]
	Learning Rate: 2.86019e-05
	LOSS [training: 0.005102272281926531 | validation: 0.008504687519875355]
	TIME [epoch: 8.15 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007920247691849649		[learning rate: 2.855e-05]
		[batch 20/20] avg loss: 0.0034698999877265033		[learning rate: 2.8498e-05]
	Learning Rate: 2.84981e-05
	LOSS [training: 0.005695073839788076 | validation: 0.004544293866367535]
	TIME [epoch: 8.17 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030534690041128095		[learning rate: 2.8446e-05]
		[batch 20/20] avg loss: 0.008157809994623311		[learning rate: 2.8395e-05]
	Learning Rate: 2.83947e-05
	LOSS [training: 0.00560563949936806 | validation: 0.0011452852020332788]
	TIME [epoch: 8.16 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005791641625559392		[learning rate: 2.8343e-05]
		[batch 20/20] avg loss: 0.001411010382344945		[learning rate: 2.8292e-05]
	Learning Rate: 2.82916e-05
	LOSS [training: 0.003601326003952168 | validation: 0.002034630156056685]
	TIME [epoch: 8.17 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016237689425663989		[learning rate: 2.824e-05]
		[batch 20/20] avg loss: 0.008253421719324583		[learning rate: 2.8189e-05]
	Learning Rate: 2.8189e-05
	LOSS [training: 0.004938595330945492 | validation: -0.0004443066864816492]
	TIME [epoch: 8.14 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00322241889029291		[learning rate: 2.8138e-05]
		[batch 20/20] avg loss: 0.0011894189075950818		[learning rate: 2.8087e-05]
	Learning Rate: 2.80867e-05
	LOSS [training: 0.002205918898943996 | validation: 0.00959283063750406]
	TIME [epoch: 8.16 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0073222656287061675		[learning rate: 2.8036e-05]
		[batch 20/20] avg loss: 0.0017259458660025904		[learning rate: 2.7985e-05]
	Learning Rate: 2.79847e-05
	LOSS [training: 0.004524105747354379 | validation: 0.0034128931993926566]
	TIME [epoch: 8.16 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035627437565070993		[learning rate: 2.7934e-05]
		[batch 20/20] avg loss: 0.006206420302805433		[learning rate: 2.7883e-05]
	Learning Rate: 2.78832e-05
	LOSS [training: 0.0048845820296562655 | validation: 0.009224631938315641]
	TIME [epoch: 8.16 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010648550361360757		[learning rate: 2.7833e-05]
		[batch 20/20] avg loss: 0.005488726658558667		[learning rate: 2.7782e-05]
	Learning Rate: 2.7782e-05
	LOSS [training: 0.0022119358112112956 | validation: -0.0027073075106428856]
	TIME [epoch: 8.15 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004748363523170743		[learning rate: 2.7732e-05]
		[batch 20/20] avg loss: -0.0027795977337437268		[learning rate: 2.7681e-05]
	Learning Rate: 2.76812e-05
	LOSS [training: 0.0009843828947135084 | validation: 0.010647713732620072]
	TIME [epoch: 8.17 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008126446464249456		[learning rate: 2.7631e-05]
		[batch 20/20] avg loss: 0.0037767004615132755		[learning rate: 2.7581e-05]
	Learning Rate: 2.75807e-05
	LOSS [training: 0.005951573462881365 | validation: 0.0008540422720577504]
	TIME [epoch: 8.18 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006533729198494149		[learning rate: 2.7531e-05]
		[batch 20/20] avg loss: 0.00832635293916279		[learning rate: 2.7481e-05]
	Learning Rate: 2.74806e-05
	LOSS [training: 0.007430041068828472 | validation: 0.004198928281699652]
	TIME [epoch: 8.17 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008297784483318589		[learning rate: 2.7431e-05]
		[batch 20/20] avg loss: -0.0005356876692947291		[learning rate: 2.7381e-05]
	Learning Rate: 2.73809e-05
	LOSS [training: 0.0038810484070119305 | validation: 0.007189036103360686]
	TIME [epoch: 8.14 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037961492838923735		[learning rate: 2.7331e-05]
		[batch 20/20] avg loss: 0.003207407023390173		[learning rate: 2.7282e-05]
	Learning Rate: 2.72815e-05
	LOSS [training: 0.003501778153641273 | validation: 0.0030534963552790884]
	TIME [epoch: 8.17 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004689823652224348		[learning rate: 2.7232e-05]
		[batch 20/20] avg loss: -0.0010548749559834373		[learning rate: 2.7183e-05]
	Learning Rate: 2.71825e-05
	LOSS [training: 0.0018174743481204551 | validation: -0.00371934588034046]
	TIME [epoch: 8.18 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00365305563964947		[learning rate: 2.7133e-05]
		[batch 20/20] avg loss: 0.0033091405557409406		[learning rate: 2.7084e-05]
	Learning Rate: 2.70839e-05
	LOSS [training: 0.0034810980976952054 | validation: -0.003098209678680134]
	TIME [epoch: 8.18 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002903954954340452		[learning rate: 2.7035e-05]
		[batch 20/20] avg loss: 0.0002465144363408207		[learning rate: 2.6986e-05]
	Learning Rate: 2.69856e-05
	LOSS [training: 0.0015752346953406362 | validation: 0.0011990108682334725]
	TIME [epoch: 8.15 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0040038327873255794		[learning rate: 2.6937e-05]
		[batch 20/20] avg loss: 0.004107794869795498		[learning rate: 2.6888e-05]
	Learning Rate: 2.68876e-05
	LOSS [training: 0.004055813828560538 | validation: -0.00042983686947614913]
	TIME [epoch: 8.17 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026395420086616703		[learning rate: 2.6839e-05]
		[batch 20/20] avg loss: 0.0031232193933347823		[learning rate: 2.679e-05]
	Learning Rate: 2.67901e-05
	LOSS [training: 0.0028813807009982263 | validation: 0.005950845707225611]
	TIME [epoch: 8.18 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009848923284168695		[learning rate: 2.6741e-05]
		[batch 20/20] avg loss: 0.003228428422328949		[learning rate: 2.6693e-05]
	Learning Rate: 2.66928e-05
	LOSS [training: 0.0021066603753729088 | validation: 0.004861428960695785]
	TIME [epoch: 8.16 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015946275184330672		[learning rate: 2.6644e-05]
		[batch 20/20] avg loss: -0.0007063069890296521		[learning rate: 2.6596e-05]
	Learning Rate: 2.6596e-05
	LOSS [training: 0.00044416026470170763 | validation: 0.0045386843047944625]
	TIME [epoch: 8.15 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007686574572384759		[learning rate: 2.6548e-05]
		[batch 20/20] avg loss: 0.003889855091463395		[learning rate: 2.6499e-05]
	Learning Rate: 2.64994e-05
	LOSS [training: 0.005788214831924078 | validation: -0.0015588838612416696]
	TIME [epoch: 8.17 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025553145934277288		[learning rate: 2.6451e-05]
		[batch 20/20] avg loss: 0.002620824548719869		[learning rate: 2.6403e-05]
	Learning Rate: 2.64033e-05
	LOSS [training: 0.0025880695710737986 | validation: 0.0011959715722955028]
	TIME [epoch: 8.18 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003748250657337527		[learning rate: 2.6355e-05]
		[batch 20/20] avg loss: 5.062049378934527e-05		[learning rate: 2.6307e-05]
	Learning Rate: 2.63075e-05
	LOSS [training: 0.0018994355755634362 | validation: 0.008463450479779424]
	TIME [epoch: 8.15 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005449844511493359		[learning rate: 2.626e-05]
		[batch 20/20] avg loss: 0.004927421336554766		[learning rate: 2.6212e-05]
	Learning Rate: 2.6212e-05
	LOSS [training: 0.0051886329240240624 | validation: -0.003376084719292032]
	TIME [epoch: 8.15 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002705652907069281		[learning rate: 2.6164e-05]
		[batch 20/20] avg loss: 0.001160504852563542		[learning rate: 2.6117e-05]
	Learning Rate: 2.61169e-05
	LOSS [training: 0.0019330788798164115 | validation: 0.005983389015152878]
	TIME [epoch: 8.17 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00052772871064612		[learning rate: 2.6069e-05]
		[batch 20/20] avg loss: 0.006208495629969694		[learning rate: 2.6022e-05]
	Learning Rate: 2.60221e-05
	LOSS [training: 0.0033681121703079075 | validation: 0.009887416052202069]
	TIME [epoch: 8.18 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016468842002428017		[learning rate: 2.5975e-05]
		[batch 20/20] avg loss: 0.0033844512291423268		[learning rate: 2.5928e-05]
	Learning Rate: 2.59277e-05
	LOSS [training: 0.0025156677146925635 | validation: 0.0006343492133789683]
	TIME [epoch: 8.15 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008038653233898577		[learning rate: 2.5881e-05]
		[batch 20/20] avg loss: 0.003230988742860349		[learning rate: 2.5834e-05]
	Learning Rate: 2.58336e-05
	LOSS [training: 0.005634820988379463 | validation: 0.00812767146255029]
	TIME [epoch: 8.15 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004455769381900334		[learning rate: 2.5787e-05]
		[batch 20/20] avg loss: 0.00207637632207481		[learning rate: 2.574e-05]
	Learning Rate: 2.57398e-05
	LOSS [training: 0.0032660728519875726 | validation: 0.004296525539082585]
	TIME [epoch: 8.17 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004039894917083806		[learning rate: 2.5693e-05]
		[batch 20/20] avg loss: 0.0023526956792961117		[learning rate: 2.5646e-05]
	Learning Rate: 2.56464e-05
	LOSS [training: 0.0031962952981899584 | validation: -0.0005851648971521009]
	TIME [epoch: 8.2 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010134944758861926		[learning rate: 2.56e-05]
		[batch 20/20] avg loss: -0.004744339764664973		[learning rate: 2.5553e-05]
	Learning Rate: 2.55533e-05
	LOSS [training: 0.002695302497098476 | validation: 0.004459053958539874]
	TIME [epoch: 8.15 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0097386544249596		[learning rate: 2.5507e-05]
		[batch 20/20] avg loss: 0.00105196288389639		[learning rate: 2.5461e-05]
	Learning Rate: 2.54606e-05
	LOSS [training: 0.005395308654427995 | validation: 0.002492783742030508]
	TIME [epoch: 8.16 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006128614836477466		[learning rate: 2.5414e-05]
		[batch 20/20] avg loss: -0.0036269836992179616		[learning rate: 2.5368e-05]
	Learning Rate: 2.53682e-05
	LOSS [training: 0.0012508155686297528 | validation: 0.0025416310094302846]
	TIME [epoch: 8.17 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009615324352493406		[learning rate: 2.5322e-05]
		[batch 20/20] avg loss: 0.003989157983082605		[learning rate: 2.5276e-05]
	Learning Rate: 2.52761e-05
	LOSS [training: 0.0015138127739166324 | validation: 0.002014012401096649]
	TIME [epoch: 8.19 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004320249320267743		[learning rate: 2.523e-05]
		[batch 20/20] avg loss: 0.0034347565914783704		[learning rate: 2.5184e-05]
	Learning Rate: 2.51844e-05
	LOSS [training: 0.0038775029558730557 | validation: -0.0028556697392794405]
	TIME [epoch: 8.16 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002670341037582698		[learning rate: 2.5139e-05]
		[batch 20/20] avg loss: 0.0030045374458120195		[learning rate: 2.5093e-05]
	Learning Rate: 2.5093e-05
	LOSS [training: 0.0028374392416973585 | validation: 0.0056741472799604915]
	TIME [epoch: 8.15 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032151260621930723		[learning rate: 2.5047e-05]
		[batch 20/20] avg loss: 0.0053649107852185894		[learning rate: 2.5002e-05]
	Learning Rate: 2.50019e-05
	LOSS [training: 0.004290018423705831 | validation: 0.010399090997525425]
	TIME [epoch: 8.16 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00902774116769989		[learning rate: 2.4957e-05]
		[batch 20/20] avg loss: 1.4188792380193452e-05		[learning rate: 2.4911e-05]
	Learning Rate: 2.49112e-05
	LOSS [training: 0.004520964980040042 | validation: 0.003171911488935279]
	TIME [epoch: 8.2 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006102917352149741		[learning rate: 2.4866e-05]
		[batch 20/20] avg loss: 0.006488750707876653		[learning rate: 2.4821e-05]
	Learning Rate: 2.48208e-05
	LOSS [training: 0.006295834030013198 | validation: 0.00979529263593819]
	TIME [epoch: 8.16 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030549435041575075		[learning rate: 2.4776e-05]
		[batch 20/20] avg loss: 0.003902019686614382		[learning rate: 2.4731e-05]
	Learning Rate: 2.47307e-05
	LOSS [training: 0.0034784815953859453 | validation: 0.005072415038253231]
	TIME [epoch: 8.15 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033118667199629566		[learning rate: 2.4686e-05]
		[batch 20/20] avg loss: 0.0035278666133030747		[learning rate: 2.4641e-05]
	Learning Rate: 2.4641e-05
	LOSS [training: 0.003419866666633014 | validation: 0.002116685638308891]
	TIME [epoch: 8.18 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005796127544991901		[learning rate: 2.4596e-05]
		[batch 20/20] avg loss: 0.008057357333124158		[learning rate: 2.4552e-05]
	Learning Rate: 2.45516e-05
	LOSS [training: 0.004318485043811673 | validation: 0.0065807656427166435]
	TIME [epoch: 8.2 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014447427564242992		[learning rate: 2.4507e-05]
		[batch 20/20] avg loss: 0.008784550155286126		[learning rate: 2.4462e-05]
	Learning Rate: 2.44625e-05
	LOSS [training: 0.005114646455855213 | validation: 0.0022526006378232903]
	TIME [epoch: 8.15 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028893234493410115		[learning rate: 2.4418e-05]
		[batch 20/20] avg loss: -0.00043106517176548975		[learning rate: 2.4374e-05]
	Learning Rate: 2.43737e-05
	LOSS [training: 0.0012291291387877608 | validation: 0.0035496116959598954]
	TIME [epoch: 8.14 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004997781696868151		[learning rate: 2.4329e-05]
		[batch 20/20] avg loss: 0.003922708199152287		[learning rate: 2.4285e-05]
	Learning Rate: 2.42852e-05
	LOSS [training: 0.004460244948010219 | validation: 0.008350187902400637]
	TIME [epoch: 8.16 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031745796512370112		[learning rate: 2.4241e-05]
		[batch 20/20] avg loss: 0.0037532355263764014		[learning rate: 2.4197e-05]
	Learning Rate: 2.41971e-05
	LOSS [training: 0.0034639075888067057 | validation: 0.006416035374058132]
	TIME [epoch: 8.2 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010388812710746475		[learning rate: 2.4153e-05]
		[batch 20/20] avg loss: 0.00240564694645948		[learning rate: 2.4109e-05]
	Learning Rate: 2.41093e-05
	LOSS [training: 0.0017222641087670635 | validation: 0.00714769976910839]
	TIME [epoch: 8.16 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003259316845231752		[learning rate: 2.4065e-05]
		[batch 20/20] avg loss: 0.0031994282622952357		[learning rate: 2.4022e-05]
	Learning Rate: 2.40218e-05
	LOSS [training: 0.0032293725537634946 | validation: 0.005670981764321572]
	TIME [epoch: 8.14 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022882140128551364		[learning rate: 2.3978e-05]
		[batch 20/20] avg loss: 0.004374961506666867		[learning rate: 2.3935e-05]
	Learning Rate: 2.39346e-05
	LOSS [training: 0.003331587759761002 | validation: 0.006734331718712875]
	TIME [epoch: 8.15 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011698540378738136		[learning rate: 2.3891e-05]
		[batch 20/20] avg loss: 0.005867282904850141		[learning rate: 2.3848e-05]
	Learning Rate: 2.38477e-05
	LOSS [training: 0.003518568471361978 | validation: 0.0061597786991649285]
	TIME [epoch: 8.19 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001440903666695858		[learning rate: 2.3804e-05]
		[batch 20/20] avg loss: 0.0015752915642670256		[learning rate: 2.3761e-05]
	Learning Rate: 2.37612e-05
	LOSS [training: 0.0015080976154814422 | validation: 0.008261503791400731]
	TIME [epoch: 8.15 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.006868890001034e-05		[learning rate: 2.3718e-05]
		[batch 20/20] avg loss: 0.005584147301095892		[learning rate: 2.3675e-05]
	Learning Rate: 2.3675e-05
	LOSS [training: 0.0028271079949979514 | validation: 0.006558585838341663]
	TIME [epoch: 8.14 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011013351179457067		[learning rate: 2.3632e-05]
		[batch 20/20] avg loss: 0.007268116599890363		[learning rate: 2.3589e-05]
	Learning Rate: 2.35891e-05
	LOSS [training: 0.0030833907409723283 | validation: 0.010075333761988142]
	TIME [epoch: 8.15 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029092048369964815		[learning rate: 2.3546e-05]
		[batch 20/20] avg loss: 0.0033018396747124547		[learning rate: 2.3503e-05]
	Learning Rate: 2.35034e-05
	LOSS [training: 0.0031055222558544686 | validation: 0.0025773287128275295]
	TIME [epoch: 8.21 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004683130913954848		[learning rate: 2.3461e-05]
		[batch 20/20] avg loss: 0.0011972401180085808		[learning rate: 2.3418e-05]
	Learning Rate: 2.34182e-05
	LOSS [training: 0.002940185515981714 | validation: 0.011001833396168193]
	TIME [epoch: 8.15 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002501294222196104		[learning rate: 2.3376e-05]
		[batch 20/20] avg loss: 0.009171332581415632		[learning rate: 2.3333e-05]
	Learning Rate: 2.33332e-05
	LOSS [training: 0.005836313401805867 | validation: 0.0009491918230951911]
	TIME [epoch: 8.15 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006456383317347172		[learning rate: 2.3291e-05]
		[batch 20/20] avg loss: 0.00825501947560012		[learning rate: 2.3248e-05]
	Learning Rate: 2.32485e-05
	LOSS [training: 0.0044503289036674195 | validation: 0.005846563845164399]
	TIME [epoch: 8.15 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00018337689391136232		[learning rate: 2.3206e-05]
		[batch 20/20] avg loss: 0.005151890616760404		[learning rate: 2.3164e-05]
	Learning Rate: 2.31641e-05
	LOSS [training: 0.0026676337553358837 | validation: 0.006781293774193678]
	TIME [epoch: 8.21 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0014506234502665405		[learning rate: 2.3122e-05]
		[batch 20/20] avg loss: 0.0036484692262694976		[learning rate: 2.308e-05]
	Learning Rate: 2.30801e-05
	LOSS [training: 0.0010989228880014786 | validation: 0.009657519209695695]
	TIME [epoch: 8.15 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005817826689059851		[learning rate: 2.3038e-05]
		[batch 20/20] avg loss: 0.0029944390502567473		[learning rate: 2.2996e-05]
	Learning Rate: 2.29963e-05
	LOSS [training: 0.004406132869658299 | validation: 0.0038366923584782237]
	TIME [epoch: 8.15 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009542377547489652		[learning rate: 2.2955e-05]
		[batch 20/20] avg loss: 0.004510356207666948		[learning rate: 2.2913e-05]
	Learning Rate: 2.29128e-05
	LOSS [training: 0.0017780592264589918 | validation: 0.007414998066895346]
	TIME [epoch: 8.16 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003634320080673402		[learning rate: 2.2871e-05]
		[batch 20/20] avg loss: 0.0024938506141812576		[learning rate: 2.283e-05]
	Learning Rate: 2.28297e-05
	LOSS [training: 0.0030640853474273298 | validation: 0.009882686260601096]
	TIME [epoch: 8.22 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002288642193025581		[learning rate: 2.2788e-05]
		[batch 20/20] avg loss: 0.0035021303887610216		[learning rate: 2.2747e-05]
	Learning Rate: 2.27468e-05
	LOSS [training: 0.0028953862908933014 | validation: 0.007303965023013036]
	TIME [epoch: 8.15 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016190227207164894		[learning rate: 2.2706e-05]
		[batch 20/20] avg loss: 0.005980204870825902		[learning rate: 2.2664e-05]
	Learning Rate: 2.26643e-05
	LOSS [training: 0.003799613795771195 | validation: 0.005522029562750109]
	TIME [epoch: 8.15 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007564073262423733		[learning rate: 2.2623e-05]
		[batch 20/20] avg loss: 0.005239681800675517		[learning rate: 2.2582e-05]
	Learning Rate: 2.2582e-05
	LOSS [training: 0.006401877531549626 | validation: 0.006766308306335381]
	TIME [epoch: 8.18 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002662368372032135		[learning rate: 2.2541e-05]
		[batch 20/20] avg loss: 0.004111408328341101		[learning rate: 2.25e-05]
	Learning Rate: 2.25001e-05
	LOSS [training: 0.003386888350186618 | validation: 0.000973983579762197]
	TIME [epoch: 8.19 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006716951090289662		[learning rate: 2.2459e-05]
		[batch 20/20] avg loss: 0.005673168401451088		[learning rate: 2.2418e-05]
	Learning Rate: 2.24184e-05
	LOSS [training: 0.006195059745870375 | validation: 0.006720458773719134]
	TIME [epoch: 8.15 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012750301124683646		[learning rate: 2.2378e-05]
		[batch 20/20] avg loss: 0.0034099798378915427		[learning rate: 2.2337e-05]
	Learning Rate: 2.23371e-05
	LOSS [training: 0.0023425049751799537 | validation: 0.008278391430124614]
	TIME [epoch: 8.15 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024286038190246347		[learning rate: 2.2297e-05]
		[batch 20/20] avg loss: 0.0016138093886682314		[learning rate: 2.2256e-05]
	Learning Rate: 2.2256e-05
	LOSS [training: 0.0020212066038464327 | validation: 0.012117076181591352]
	TIME [epoch: 8.18 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006880164660404856		[learning rate: 2.2216e-05]
		[batch 20/20] avg loss: 0.0029021224103109957		[learning rate: 2.2175e-05]
	Learning Rate: 2.21753e-05
	LOSS [training: 0.001795069438175741 | validation: 0.005591621486339556]
	TIME [epoch: 8.21 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035185055194059877		[learning rate: 2.2135e-05]
		[batch 20/20] avg loss: 0.0034852512162056835		[learning rate: 2.2095e-05]
	Learning Rate: 2.20948e-05
	LOSS [training: 0.0035018783678058356 | validation: 0.00045302842715934247]
	TIME [epoch: 8.15 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005218059387576855		[learning rate: 2.2055e-05]
		[batch 20/20] avg loss: 0.004557043211717999		[learning rate: 2.2015e-05]
	Learning Rate: 2.20146e-05
	LOSS [training: 0.004887551299647426 | validation: 0.001856837153066914]
	TIME [epoch: 8.15 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007795804179505714		[learning rate: 2.1975e-05]
		[batch 20/20] avg loss: -0.0013729804740338935		[learning rate: 2.1935e-05]
	Learning Rate: 2.19347e-05
	LOSS [training: 0.0032114118527359114 | validation: 9.626150851755992e-06]
	TIME [epoch: 8.17 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003289456417980078		[learning rate: 2.1895e-05]
		[batch 20/20] avg loss: 0.004907650765309273		[learning rate: 2.1855e-05]
	Learning Rate: 2.18551e-05
	LOSS [training: 0.0040985535916446755 | validation: 0.010754189636287206]
	TIME [epoch: 8.2 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003151643330164859		[learning rate: 2.1815e-05]
		[batch 20/20] avg loss: 0.004702402077144025		[learning rate: 2.1776e-05]
	Learning Rate: 2.17758e-05
	LOSS [training: 0.003927022703654442 | validation: 0.0038056539888303292]
	TIME [epoch: 8.15 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010336074422849328		[learning rate: 2.1736e-05]
		[batch 20/20] avg loss: 0.0017842539664219188		[learning rate: 2.1697e-05]
	Learning Rate: 2.16968e-05
	LOSS [training: 0.0014089307043534257 | validation: 0.00028206663516362187]
	TIME [epoch: 8.15 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.066861166601108e-05		[learning rate: 2.1657e-05]
		[batch 20/20] avg loss: 0.0058804939035396325		[learning rate: 2.1618e-05]
	Learning Rate: 2.1618e-05
	LOSS [training: 0.002970581257602822 | validation: 0.0020476748059132317]
	TIME [epoch: 8.17 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00019933772911876185		[learning rate: 2.1579e-05]
		[batch 20/20] avg loss: 0.0068404649855750245		[learning rate: 2.154e-05]
	Learning Rate: 2.15396e-05
	LOSS [training: 0.003320563628228132 | validation: 0.004131181575281935]
	TIME [epoch: 8.29 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001285369000365769		[learning rate: 2.15e-05]
		[batch 20/20] avg loss: 0.005550544436032315		[learning rate: 2.1461e-05]
	Learning Rate: 2.14614e-05
	LOSS [training: 0.002132587717833273 | validation: 0.000915033889669892]
	TIME [epoch: 8.15 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021461244674212112		[learning rate: 2.1422e-05]
		[batch 20/20] avg loss: 0.0054747816877986615		[learning rate: 2.1384e-05]
	Learning Rate: 2.13835e-05
	LOSS [training: 0.0016643286101887245 | validation: 0.00815066304535149]
	TIME [epoch: 8.15 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007872797639519149		[learning rate: 2.1345e-05]
		[batch 20/20] avg loss: -0.003203795981846915		[learning rate: 2.1306e-05]
	Learning Rate: 2.13059e-05
	LOSS [training: 0.0023345008288361176 | validation: -0.004021374988844257]
	TIME [epoch: 8.16 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00604549118570214		[learning rate: 2.1267e-05]
		[batch 20/20] avg loss: 0.0033019908276261723		[learning rate: 2.1229e-05]
	Learning Rate: 2.12286e-05
	LOSS [training: 0.0046737410066641566 | validation: 0.003059509851806178]
	TIME [epoch: 8.19 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004252051803036509		[learning rate: 2.119e-05]
		[batch 20/20] avg loss: -0.0003299822242036784		[learning rate: 2.1152e-05]
	Learning Rate: 2.11515e-05
	LOSS [training: 0.001961034789416415 | validation: 0.001051550668608341]
	TIME [epoch: 8.16 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026894285484354487		[learning rate: 2.1113e-05]
		[batch 20/20] avg loss: 0.00508662806895444		[learning rate: 2.1075e-05]
	Learning Rate: 2.10748e-05
	LOSS [training: 0.003888028308694945 | validation: 0.011738988612742083]
	TIME [epoch: 8.15 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005127240161753893		[learning rate: 2.1037e-05]
		[batch 20/20] avg loss: -8.038309253436855e-05		[learning rate: 2.0998e-05]
	Learning Rate: 2.09983e-05
	LOSS [training: 0.0025234285346097626 | validation: 0.0030330131859821552]
	TIME [epoch: 8.18 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006593723181396521		[learning rate: 2.096e-05]
		[batch 20/20] avg loss: 0.006525635415684072		[learning rate: 2.0922e-05]
	Learning Rate: 2.09221e-05
	LOSS [training: 0.0065596792985402955 | validation: 0.0057907275521772925]
	TIME [epoch: 8.19 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0063920196401673225		[learning rate: 2.0884e-05]
		[batch 20/20] avg loss: 0.0004593500166629949		[learning rate: 2.0846e-05]
	Learning Rate: 2.08462e-05
	LOSS [training: 0.0034256848284151585 | validation: 0.006372375813638382]
	TIME [epoch: 8.15 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004006262571816668		[learning rate: 2.0808e-05]
		[batch 20/20] avg loss: 0.0023794982542648963		[learning rate: 2.0771e-05]
	Learning Rate: 2.07705e-05
	LOSS [training: 0.003192880413040782 | validation: 0.011593128447432046]
	TIME [epoch: 8.14 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017895968984816258		[learning rate: 2.0733e-05]
		[batch 20/20] avg loss: 0.003929731567723468		[learning rate: 2.0695e-05]
	Learning Rate: 2.06951e-05
	LOSS [training: 0.0028596642331025464 | validation: 0.0009580514331159547]
	TIME [epoch: 8.21 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002883004825315756		[learning rate: 2.0658e-05]
		[batch 20/20] avg loss: 0.001964901823366763		[learning rate: 2.062e-05]
	Learning Rate: 2.062e-05
	LOSS [training: 0.0024239533243412595 | validation: 0.004439772068535197]
	TIME [epoch: 8.18 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023319064109765695		[learning rate: 2.0583e-05]
		[batch 20/20] avg loss: 0.00623473890887333		[learning rate: 2.0545e-05]
	Learning Rate: 2.05452e-05
	LOSS [training: 0.00428332265992495 | validation: 0.003077639395399155]
	TIME [epoch: 8.16 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0054134618191565625		[learning rate: 2.0508e-05]
		[batch 20/20] avg loss: 0.004623870745225664		[learning rate: 2.0471e-05]
	Learning Rate: 2.04706e-05
	LOSS [training: 0.005018666282191114 | validation: 0.008258789792618525]
	TIME [epoch: 8.15 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029396480939097285		[learning rate: 2.0433e-05]
		[batch 20/20] avg loss: -0.0006966300734811755		[learning rate: 2.0396e-05]
	Learning Rate: 2.03964e-05
	LOSS [training: 0.0011215090102142766 | validation: 0.006790588469086948]
	TIME [epoch: 8.19 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00573297374042148		[learning rate: 2.0359e-05]
		[batch 20/20] avg loss: 0.0026649290265840946		[learning rate: 2.0322e-05]
	Learning Rate: 2.03223e-05
	LOSS [training: 0.004198951383502788 | validation: 0.002310189980130272]
	TIME [epoch: 8.18 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007763249545113775		[learning rate: 2.0285e-05]
		[batch 20/20] avg loss: 0.0012879653878892343		[learning rate: 2.0249e-05]
	Learning Rate: 2.02486e-05
	LOSS [training: 0.0010321451712003058 | validation: 0.006374959529209538]
	TIME [epoch: 8.15 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004335026752542722		[learning rate: 2.0212e-05]
		[batch 20/20] avg loss: 0.0068772232298492065		[learning rate: 2.0175e-05]
	Learning Rate: 2.01751e-05
	LOSS [training: 0.0056061249911959635 | validation: 0.011006611270139043]
	TIME [epoch: 8.15 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006706689038372619		[learning rate: 2.0138e-05]
		[batch 20/20] avg loss: 0.004973168818594587		[learning rate: 2.0102e-05]
	Learning Rate: 2.01019e-05
	LOSS [training: 0.0028219188612159246 | validation: 0.00233931440390178]
	TIME [epoch: 8.19 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013995567638814235		[learning rate: 2.0065e-05]
		[batch 20/20] avg loss: 0.005247152321762587		[learning rate: 2.0029e-05]
	Learning Rate: 2.00289e-05
	LOSS [training: 0.0033233545428220045 | validation: 0.002239109298638242]
	TIME [epoch: 8.18 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007584768093537131		[learning rate: 1.9993e-05]
		[batch 20/20] avg loss: -0.00032144652455115216		[learning rate: 1.9956e-05]
	Learning Rate: 1.99563e-05
	LOSS [training: 0.0036316607844929885 | validation: 0.0037052754070508167]
	TIME [epoch: 8.15 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019414465858411313		[learning rate: 1.992e-05]
		[batch 20/20] avg loss: 0.004044091677000535		[learning rate: 1.9884e-05]
	Learning Rate: 1.98838e-05
	LOSS [training: 0.0029927691314208336 | validation: 0.0032486496768912056]
	TIME [epoch: 8.16 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030268311491460217		[learning rate: 1.9848e-05]
		[batch 20/20] avg loss: -0.0017541673551472639		[learning rate: 1.9812e-05]
	Learning Rate: 1.98117e-05
	LOSS [training: 0.0006363318969993791 | validation: 0.010925882481780957]
	TIME [epoch: 8.19 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005680785645205083		[learning rate: 1.9776e-05]
		[batch 20/20] avg loss: -0.002317426675233035		[learning rate: 1.974e-05]
	Learning Rate: 1.97398e-05
	LOSS [training: 0.0016816794849860242 | validation: 0.00403184086715291]
	TIME [epoch: 8.17 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000623443426061842		[learning rate: 1.9704e-05]
		[batch 20/20] avg loss: 0.006081015700059014		[learning rate: 1.9668e-05]
	Learning Rate: 1.96681e-05
	LOSS [training: 0.0033522295630604277 | validation: 0.004783127476848507]
	TIME [epoch: 8.16 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007817958874401494		[learning rate: 1.9632e-05]
		[batch 20/20] avg loss: 0.0006453625957856954		[learning rate: 1.9597e-05]
	Learning Rate: 1.95968e-05
	LOSS [training: 0.004231660735093596 | validation: 0.002746775899566485]
	TIME [epoch: 8.15 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004918151741171062		[learning rate: 1.9561e-05]
		[batch 20/20] avg loss: 0.000803528260584924		[learning rate: 1.9526e-05]
	Learning Rate: 1.95256e-05
	LOSS [training: 0.002860840000877993 | validation: 0.003546043749575378]
	TIME [epoch: 8.18 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031856076873533366		[learning rate: 1.949e-05]
		[batch 20/20] avg loss: 0.004992415130321369		[learning rate: 1.9455e-05]
	Learning Rate: 1.94548e-05
	LOSS [training: 0.004089011408837353 | validation: 0.007451115089703829]
	TIME [epoch: 8.16 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0016702890366870897		[learning rate: 1.9419e-05]
		[batch 20/20] avg loss: 0.004028864997566053		[learning rate: 1.9384e-05]
	Learning Rate: 1.93842e-05
	LOSS [training: 0.0011792879804394816 | validation: 0.003972772003305628]
	TIME [epoch: 8.16 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013846609775687293		[learning rate: 1.9349e-05]
		[batch 20/20] avg loss: 0.00034588693975852344		[learning rate: 1.9314e-05]
	Learning Rate: 1.93138e-05
	LOSS [training: 0.0008652739586636263 | validation: 0.005257159948386442]
	TIME [epoch: 8.15 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004954527233486106		[learning rate: 1.9279e-05]
		[batch 20/20] avg loss: 4.871375812539017e-05		[learning rate: 1.9244e-05]
	Learning Rate: 1.92437e-05
	LOSS [training: 0.0025016204958057476 | validation: 0.009070115738679881]
	TIME [epoch: 8.19 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018231481207702897		[learning rate: 1.9209e-05]
		[batch 20/20] avg loss: 0.007708798696947456		[learning rate: 1.9174e-05]
	Learning Rate: 1.91739e-05
	LOSS [training: 0.004765973408858873 | validation: 0.0033898047252070855]
	TIME [epoch: 8.16 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008758144170794978		[learning rate: 1.9139e-05]
		[batch 20/20] avg loss: 0.006101800147521738		[learning rate: 1.9104e-05]
	Learning Rate: 1.91043e-05
	LOSS [training: 0.003488807282300618 | validation: -0.001839196226226237]
	TIME [epoch: 8.16 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007533360574366747		[learning rate: 1.907e-05]
		[batch 20/20] avg loss: 0.004700981339068749		[learning rate: 1.9035e-05]
	Learning Rate: 1.9035e-05
	LOSS [training: 0.006117170956717748 | validation: -0.0002893996598669216]
	TIME [epoch: 8.17 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000578954443132399		[learning rate: 1.9e-05]
		[batch 20/20] avg loss: 0.000902311120051533		[learning rate: 1.8966e-05]
	Learning Rate: 1.89659e-05
	LOSS [training: 0.0007406327815919659 | validation: 0.005674963794230037]
	TIME [epoch: 8.18 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023027661152265413		[learning rate: 1.8931e-05]
		[batch 20/20] avg loss: 0.006708998190176801		[learning rate: 1.8897e-05]
	Learning Rate: 1.88971e-05
	LOSS [training: 0.004505882152701672 | validation: 0.005088916311495571]
	TIME [epoch: 8.16 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00103168593199721		[learning rate: 1.8863e-05]
		[batch 20/20] avg loss: 0.004821178343397552		[learning rate: 1.8829e-05]
	Learning Rate: 1.88285e-05
	LOSS [training: 0.0029264321376973814 | validation: 0.006951259932283363]
	TIME [epoch: 8.15 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.000263966707329748		[learning rate: 1.8794e-05]
		[batch 20/20] avg loss: 0.005469794688317804		[learning rate: 1.876e-05]
	Learning Rate: 1.87602e-05
	LOSS [training: 0.002866880697823776 | validation: 0.012199064033993958]
	TIME [epoch: 8.17 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009680695173029179		[learning rate: 1.8726e-05]
		[batch 20/20] avg loss: -0.0031491734140795943		[learning rate: 1.8692e-05]
	Learning Rate: 1.86921e-05
	LOSS [training: 0.003265760879474792 | validation: 0.007511611556879847]
	TIME [epoch: 8.17 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005493496773599267		[learning rate: 1.8658e-05]
		[batch 20/20] avg loss: 0.0029119125527408392		[learning rate: 1.8624e-05]
	Learning Rate: 1.86243e-05
	LOSS [training: 0.004202704663170053 | validation: 0.0011837680483671189]
	TIME [epoch: 8.16 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005015532928047438		[learning rate: 1.859e-05]
		[batch 20/20] avg loss: 0.005773640067179034		[learning rate: 1.8557e-05]
	Learning Rate: 1.85567e-05
	LOSS [training: 0.005394586497613235 | validation: 0.003158201425609507]
	TIME [epoch: 8.16 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004187014915928593		[learning rate: 1.8523e-05]
		[batch 20/20] avg loss: 0.0009749451697874698		[learning rate: 1.8489e-05]
	Learning Rate: 1.84893e-05
	LOSS [training: 0.0025809800428580306 | validation: 0.005870949916515426]
	TIME [epoch: 8.16 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014124084809527806		[learning rate: 1.8456e-05]
		[batch 20/20] avg loss: 5.681867338987283e-05		[learning rate: 1.8422e-05]
	Learning Rate: 1.84222e-05
	LOSS [training: 0.0007346135771713266 | validation: 0.009227504596273847]
	TIME [epoch: 8.16 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008019706382814745		[learning rate: 1.8389e-05]
		[batch 20/20] avg loss: 0.0037224723516089157		[learning rate: 1.8355e-05]
	Learning Rate: 1.83554e-05
	LOSS [training: 0.002262221494945195 | validation: 0.0023523552917867163]
	TIME [epoch: 8.16 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010302489684838097		[learning rate: 1.8322e-05]
		[batch 20/20] avg loss: -0.0005494828942102371		[learning rate: 1.8289e-05]
	Learning Rate: 1.82888e-05
	LOSS [training: 0.00024038303713678631 | validation: 0.003902884217015007]
	TIME [epoch: 8.16 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005286957582272879		[learning rate: 1.8256e-05]
		[batch 20/20] avg loss: 0.0024418384585738174		[learning rate: 1.8222e-05]
	Learning Rate: 1.82224e-05
	LOSS [training: 0.0038643980204233486 | validation: 0.0023266239294017724]
	TIME [epoch: 8.18 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005667225366256313		[learning rate: 1.8189e-05]
		[batch 20/20] avg loss: 0.0015009656395932947		[learning rate: 1.8156e-05]
	Learning Rate: 1.81563e-05
	LOSS [training: 0.0035840955029248027 | validation: -0.0013704768590269488]
	TIME [epoch: 8.16 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005420837642623732		[learning rate: 1.8123e-05]
		[batch 20/20] avg loss: 0.004180642070827509		[learning rate: 1.809e-05]
	Learning Rate: 1.80904e-05
	LOSS [training: 0.004800739856725621 | validation: 0.0004801075259727634]
	TIME [epoch: 8.16 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029716719902556405		[learning rate: 1.8058e-05]
		[batch 20/20] avg loss: -0.0019164417825572437		[learning rate: 1.8025e-05]
	Learning Rate: 1.80247e-05
	LOSS [training: 0.0005276151038491981 | validation: 0.0027112068442607315]
	TIME [epoch: 8.16 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0011540200727575152		[learning rate: 1.7992e-05]
		[batch 20/20] avg loss: 0.002713348571692615		[learning rate: 1.7959e-05]
	Learning Rate: 1.79593e-05
	LOSS [training: 0.0007796642494675504 | validation: 0.003678858945798925]
	TIME [epoch: 8.19 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005479475422893261		[learning rate: 1.7927e-05]
		[batch 20/20] avg loss: 0.0056903315023132586		[learning rate: 1.7894e-05]
	Learning Rate: 1.78941e-05
	LOSS [training: 0.005584903462603261 | validation: 0.010303223190973244]
	TIME [epoch: 8.16 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024250539515251884		[learning rate: 1.7862e-05]
		[batch 20/20] avg loss: 0.005004418619410953		[learning rate: 1.7829e-05]
	Learning Rate: 1.78292e-05
	LOSS [training: 0.0037147362854680712 | validation: 0.005567057258361675]
	TIME [epoch: 8.14 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004718286279169152		[learning rate: 1.7797e-05]
		[batch 20/20] avg loss: 0.007740251099872315		[learning rate: 1.7764e-05]
	Learning Rate: 1.77645e-05
	LOSS [training: 0.0062292686895207335 | validation: 0.0020476776571803504]
	TIME [epoch: 8.17 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034889114746017484		[learning rate: 1.7732e-05]
		[batch 20/20] avg loss: 0.0017671528110747982		[learning rate: 1.77e-05]
	Learning Rate: 1.77e-05
	LOSS [training: 0.0026280321428382727 | validation: 0.0037241728896304036]
	TIME [epoch: 8.18 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006976645546410159		[learning rate: 1.7668e-05]
		[batch 20/20] avg loss: -0.0007760842122689251		[learning rate: 1.7636e-05]
	Learning Rate: 1.76358e-05
	LOSS [training: 0.003100280667070617 | validation: 0.0038598852295193345]
	TIME [epoch: 8.14 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009091632320143952		[learning rate: 1.7604e-05]
		[batch 20/20] avg loss: 0.004612527499968511		[learning rate: 1.7572e-05]
	Learning Rate: 1.75718e-05
	LOSS [training: 0.0068520799100562305 | validation: 0.009068987759097133]
	TIME [epoch: 8.14 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005339506198087168		[learning rate: 1.754e-05]
		[batch 20/20] avg loss: 0.002124628729902573		[learning rate: 1.7508e-05]
	Learning Rate: 1.7508e-05
	LOSS [training: 0.0037320674639948702 | validation: 0.0015328199137556958]
	TIME [epoch: 8.17 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030938379787810656		[learning rate: 1.7476e-05]
		[batch 20/20] avg loss: 0.0034374667735943263		[learning rate: 1.7444e-05]
	Learning Rate: 1.74445e-05
	LOSS [training: 0.003265652376187695 | validation: 0.0010232930660161502]
	TIME [epoch: 8.18 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005323384110730393		[learning rate: 1.7413e-05]
		[batch 20/20] avg loss: -0.0012224445939045768		[learning rate: 1.7381e-05]
	Learning Rate: 1.73812e-05
	LOSS [training: 0.002050469758412908 | validation: -0.001122396165317483]
	TIME [epoch: 8.15 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0077967126700034025		[learning rate: 1.735e-05]
		[batch 20/20] avg loss: -0.0006791288927653261		[learning rate: 1.7318e-05]
	Learning Rate: 1.73181e-05
	LOSS [training: 0.003558791888619039 | validation: -0.0008250341734224315]
	TIME [epoch: 8.15 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005079118655346791		[learning rate: 1.7287e-05]
		[batch 20/20] avg loss: 0.0030949428411241866		[learning rate: 1.7255e-05]
	Learning Rate: 1.72552e-05
	LOSS [training: 0.0040870307482354885 | validation: 0.003971423264257434]
	TIME [epoch: 8.17 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0056485106200531475		[learning rate: 1.7224e-05]
		[batch 20/20] avg loss: 0.0007563812648873925		[learning rate: 1.7193e-05]
	Learning Rate: 1.71926e-05
	LOSS [training: 0.0032024459424702693 | validation: 0.003008936814307043]
	TIME [epoch: 8.19 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0021220098789953095		[learning rate: 1.7161e-05]
		[batch 20/20] avg loss: 0.0070672518934765905		[learning rate: 1.713e-05]
	Learning Rate: 1.71302e-05
	LOSS [training: 0.0024726210072406403 | validation: 0.0024955713205197304]
	TIME [epoch: 8.14 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0017379305596223512		[learning rate: 1.7099e-05]
		[batch 20/20] avg loss: 0.007164693450129411		[learning rate: 1.7068e-05]
	Learning Rate: 1.70681e-05
	LOSS [training: 0.00271338144525353 | validation: 0.007053357406081193]
	TIME [epoch: 8.15 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027119657156866		[learning rate: 1.7037e-05]
		[batch 20/20] avg loss: 0.004657452163355684		[learning rate: 1.7006e-05]
	Learning Rate: 1.70061e-05
	LOSS [training: 0.0036847089395211424 | validation: -0.002517172038742844]
	TIME [epoch: 8.17 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030827712554317823		[learning rate: 1.6975e-05]
		[batch 20/20] avg loss: 0.004324951918826465		[learning rate: 1.6944e-05]
	Learning Rate: 1.69444e-05
	LOSS [training: 0.0037038615871291245 | validation: 0.005034580015365767]
	TIME [epoch: 8.19 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00029216248277441094		[learning rate: 1.6914e-05]
		[batch 20/20] avg loss: 0.004172251006073967		[learning rate: 1.6883e-05]
	Learning Rate: 1.68829e-05
	LOSS [training: 0.0022322067444241886 | validation: 0.003349525109471026]
	TIME [epoch: 8.14 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007698572336587864		[learning rate: 1.6852e-05]
		[batch 20/20] avg loss: 0.0071745329685894465		[learning rate: 1.6822e-05]
	Learning Rate: 1.68216e-05
	LOSS [training: 0.007436552652588656 | validation: 0.007733242963118705]
	TIME [epoch: 8.14 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007238093764216522		[learning rate: 1.6791e-05]
		[batch 20/20] avg loss: 0.00465114552612886		[learning rate: 1.6761e-05]
	Learning Rate: 1.67606e-05
	LOSS [training: 0.005944619645172691 | validation: 0.0012276084088988749]
	TIME [epoch: 8.17 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009032358989201584		[learning rate: 1.673e-05]
		[batch 20/20] avg loss: 0.0009574865968421768		[learning rate: 1.67e-05]
	Learning Rate: 1.66998e-05
	LOSS [training: 0.00499492279302188 | validation: 0.006185442112551467]
	TIME [epoch: 8.2 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032444610124607266		[learning rate: 1.6669e-05]
		[batch 20/20] avg loss: 0.00331068624002872		[learning rate: 1.6639e-05]
	Learning Rate: 1.66392e-05
	LOSS [training: 0.003277573626244723 | validation: 0.009638832681697391]
	TIME [epoch: 8.14 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005951896036218929		[learning rate: 1.6609e-05]
		[batch 20/20] avg loss: 0.002714152163510054		[learning rate: 1.6579e-05]
	Learning Rate: 1.65788e-05
	LOSS [training: 0.004333024099864491 | validation: 0.001658757240650643]
	TIME [epoch: 8.15 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004576857034748927		[learning rate: 1.6549e-05]
		[batch 20/20] avg loss: 0.0025335599266417975		[learning rate: 1.6519e-05]
	Learning Rate: 1.65186e-05
	LOSS [training: 0.0035552084806953627 | validation: 0.003480063731270785]
	TIME [epoch: 8.19 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008047704825476625		[learning rate: 1.6489e-05]
		[batch 20/20] avg loss: 0.0017867836625597793		[learning rate: 1.6459e-05]
	Learning Rate: 1.64587e-05
	LOSS [training: 0.001295777072553721 | validation: 0.0021760003002623644]
	TIME [epoch: 8.18 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002149867559728639		[learning rate: 1.6429e-05]
		[batch 20/20] avg loss: 0.0056552598084258525		[learning rate: 1.6399e-05]
	Learning Rate: 1.63989e-05
	LOSS [training: 0.003902563684077246 | validation: 0.0065248659878662345]
	TIME [epoch: 8.15 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0022982047581014096		[learning rate: 1.6369e-05]
		[batch 20/20] avg loss: 0.006736927103268566		[learning rate: 1.6339e-05]
	Learning Rate: 1.63394e-05
	LOSS [training: 0.002219361172583578 | validation: 0.007862490990202286]
	TIME [epoch: 8.14 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011632189957158467		[learning rate: 1.631e-05]
		[batch 20/20] avg loss: 0.0034366893344936534		[learning rate: 1.628e-05]
	Learning Rate: 1.62801e-05
	LOSS [training: 0.00229995416510475 | validation: 0.0023822420860302217]
	TIME [epoch: 8.18 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003142125039144896		[learning rate: 1.6251e-05]
		[batch 20/20] avg loss: 0.004210080881736711		[learning rate: 1.6221e-05]
	Learning Rate: 1.62211e-05
	LOSS [training: 0.0036761029604408047 | validation: 0.0015903205874631251]
	TIME [epoch: 8.18 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028276436780597454		[learning rate: 1.6192e-05]
		[batch 20/20] avg loss: 0.004286154560766658		[learning rate: 1.6162e-05]
	Learning Rate: 1.61622e-05
	LOSS [training: 0.0035568991194132016 | validation: 0.009170644420169814]
	TIME [epoch: 8.15 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0072860110256844305		[learning rate: 1.6133e-05]
		[batch 20/20] avg loss: 0.003089269045849728		[learning rate: 1.6104e-05]
	Learning Rate: 1.61035e-05
	LOSS [training: 0.00518764003576708 | validation: -0.0003409632194228411]
	TIME [epoch: 8.17 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.001054978263325829		[learning rate: 1.6074e-05]
		[batch 20/20] avg loss: 0.004464555802407001		[learning rate: 1.6045e-05]
	Learning Rate: 1.60451e-05
	LOSS [training: 0.0017047887695405863 | validation: 0.01065989725328473]
	TIME [epoch: 8.21 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007434282975541674		[learning rate: 1.6016e-05]
		[batch 20/20] avg loss: 0.0035805224817739194		[learning rate: 1.5987e-05]
	Learning Rate: 1.59869e-05
	LOSS [training: 0.005507402728657797 | validation: 0.0029313627399283897]
	TIME [epoch: 8.17 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031668375053449513		[learning rate: 1.5958e-05]
		[batch 20/20] avg loss: 0.0046813164374006015		[learning rate: 1.5929e-05]
	Learning Rate: 1.59288e-05
	LOSS [training: 0.003924076971372775 | validation: -0.000601605284231897]
	TIME [epoch: 8.14 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005711087491319228		[learning rate: 1.59e-05]
		[batch 20/20] avg loss: 0.002906746100874205		[learning rate: 1.5871e-05]
	Learning Rate: 1.5871e-05
	LOSS [training: 0.004308916796096716 | validation: 0.0058341096633462335]
	TIME [epoch: 8.14 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007845181398066486		[learning rate: 1.5842e-05]
		[batch 20/20] avg loss: -0.00015316457915409985		[learning rate: 1.5813e-05]
	Learning Rate: 1.58134e-05
	LOSS [training: 0.0038460084094561935 | validation: 0.0035292776744563466]
	TIME [epoch: 8.2 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008392149525075603		[learning rate: 1.5785e-05]
		[batch 20/20] avg loss: 0.004998469963994063		[learning rate: 1.5756e-05]
	Learning Rate: 1.57561e-05
	LOSS [training: 0.0029188424582508115 | validation: 0.0019259762272009584]
	TIME [epoch: 8.16 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034832872727303807		[learning rate: 1.5727e-05]
		[batch 20/20] avg loss: -0.0016419830942165203		[learning rate: 1.5699e-05]
	Learning Rate: 1.56989e-05
	LOSS [training: 0.0009206520892569301 | validation: 0.006175997682500246]
	TIME [epoch: 8.15 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004812207127616225		[learning rate: 1.567e-05]
		[batch 20/20] avg loss: 0.0034769949281997143		[learning rate: 1.5642e-05]
	Learning Rate: 1.56419e-05
	LOSS [training: 0.00414460102790797 | validation: 0.004550877843696819]
	TIME [epoch: 8.15 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003397213132243173		[learning rate: 1.5613e-05]
		[batch 20/20] avg loss: 0.005631666737370856		[learning rate: 1.5585e-05]
	Learning Rate: 1.55851e-05
	LOSS [training: 0.004514439934807014 | validation: 0.003625200920843252]
	TIME [epoch: 8.21 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015835954216620516		[learning rate: 1.5557e-05]
		[batch 20/20] avg loss: 0.003697823477551599		[learning rate: 1.5529e-05]
	Learning Rate: 1.55286e-05
	LOSS [training: 0.002640709449606825 | validation: 0.00520877034610315]
	TIME [epoch: 8.17 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004052135517130269		[learning rate: 1.55e-05]
		[batch 20/20] avg loss: 0.0038639755855745363		[learning rate: 1.5472e-05]
	Learning Rate: 1.54722e-05
	LOSS [training: 0.003958055551352402 | validation: 0.0033075833917156593]
	TIME [epoch: 8.15 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008089612487839855		[learning rate: 1.5444e-05]
		[batch 20/20] avg loss: 0.0061768570788888425		[learning rate: 1.5416e-05]
	Learning Rate: 1.54161e-05
	LOSS [training: 0.0071332347833643495 | validation: 0.004669521317806438]
	TIME [epoch: 8.16 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008607450645130457		[learning rate: 1.5388e-05]
		[batch 20/20] avg loss: 0.0011521014174454482		[learning rate: 1.536e-05]
	Learning Rate: 1.53601e-05
	LOSS [training: 0.004879776031287951 | validation: 0.006913569035081743]
	TIME [epoch: 8.21 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012911806153388123		[learning rate: 1.5332e-05]
		[batch 20/20] avg loss: 0.0038895127624030286		[learning rate: 1.5304e-05]
	Learning Rate: 1.53044e-05
	LOSS [training: 0.00259034668887092 | validation: 0.005567850736637155]
	TIME [epoch: 8.17 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001338630203084753		[learning rate: 1.5277e-05]
		[batch 20/20] avg loss: 0.006452619270921842		[learning rate: 1.5249e-05]
	Learning Rate: 1.52488e-05
	LOSS [training: 0.003895624737003297 | validation: 0.005006533381484771]
	TIME [epoch: 8.16 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004428982743844948		[learning rate: 1.5221e-05]
		[batch 20/20] avg loss: 0.007421893522116631		[learning rate: 1.5194e-05]
	Learning Rate: 1.51935e-05
	LOSS [training: 0.0059254381329807895 | validation: 0.0066101111593066504]
	TIME [epoch: 8.15 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004535188038077676		[learning rate: 1.5166e-05]
		[batch 20/20] avg loss: 0.002296799898969026		[learning rate: 1.5138e-05]
	Learning Rate: 1.51384e-05
	LOSS [training: 0.003415993968523351 | validation: 0.005415112092451933]
	TIME [epoch: 8.21 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019317555695136472		[learning rate: 1.5111e-05]
		[batch 20/20] avg loss: 0.007733915354688969		[learning rate: 1.5083e-05]
	Learning Rate: 1.50834e-05
	LOSS [training: 0.004832835462101309 | validation: 0.004923969907764415]
	TIME [epoch: 8.17 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007780913208154644		[learning rate: 1.5056e-05]
		[batch 20/20] avg loss: 2.6848878410038044e-05		[learning rate: 1.5029e-05]
	Learning Rate: 1.50287e-05
	LOSS [training: 0.0039038810432823422 | validation: 0.0008573678373288934]
	TIME [epoch: 8.14 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002804166837415369		[learning rate: 1.5001e-05]
		[batch 20/20] avg loss: -0.004345755646719742		[learning rate: 1.4974e-05]
	Learning Rate: 1.49741e-05
	LOSS [training: -0.0007707944046521868 | validation: 0.002901537120674703]
	TIME [epoch: 8.15 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00275458632674455		[learning rate: 1.4947e-05]
		[batch 20/20] avg loss: 0.004654145402267895		[learning rate: 1.492e-05]
	Learning Rate: 1.49198e-05
	LOSS [training: 0.0037043658645062225 | validation: 0.00838048463711199]
	TIME [epoch: 8.2 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005395115702360754		[learning rate: 1.4893e-05]
		[batch 20/20] avg loss: 0.0027949614119249927		[learning rate: 1.4866e-05]
	Learning Rate: 1.48657e-05
	LOSS [training: 0.004095038557142874 | validation: 0.003704855950459127]
	TIME [epoch: 8.17 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007243394080858892		[learning rate: 1.4839e-05]
		[batch 20/20] avg loss: -0.00020262450776855988		[learning rate: 1.4812e-05]
	Learning Rate: 1.48117e-05
	LOSS [training: 0.003520384786545166 | validation: 0.005548688473447225]
	TIME [epoch: 8.14 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007608341877755892		[learning rate: 1.4785e-05]
		[batch 20/20] avg loss: 0.0027833406789307394		[learning rate: 1.4758e-05]
	Learning Rate: 1.4758e-05
	LOSS [training: 0.005195841278343317 | validation: 0.005228016646224357]
	TIME [epoch: 8.14 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003503830595701372		[learning rate: 1.4731e-05]
		[batch 20/20] avg loss: 0.0027075017177507744		[learning rate: 1.4704e-05]
	Learning Rate: 1.47044e-05
	LOSS [training: 0.003105666156726073 | validation: 0.0026885623595229386]
	TIME [epoch: 8.19 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006038898449787788		[learning rate: 1.4678e-05]
		[batch 20/20] avg loss: 0.001618136803772096		[learning rate: 1.4651e-05]
	Learning Rate: 1.4651e-05
	LOSS [training: 0.0038285176267799418 | validation: 0.010023895527240964]
	TIME [epoch: 8.16 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009112638960528634		[learning rate: 1.4624e-05]
		[batch 20/20] avg loss: 0.0018617028807309866		[learning rate: 1.4598e-05]
	Learning Rate: 1.45979e-05
	LOSS [training: 0.0013864833883919247 | validation: 0.0035194036154507016]
	TIME [epoch: 8.15 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032631843965848497		[learning rate: 1.4571e-05]
		[batch 20/20] avg loss: 0.007246938085690896		[learning rate: 1.4545e-05]
	Learning Rate: 1.45449e-05
	LOSS [training: 0.005255061241137873 | validation: 0.007041814020857297]
	TIME [epoch: 8.15 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019688165150704663		[learning rate: 1.4518e-05]
		[batch 20/20] avg loss: 0.003886301766034745		[learning rate: 1.4492e-05]
	Learning Rate: 1.44921e-05
	LOSS [training: 0.0029275591405526057 | validation: 0.000764643699720022]
	TIME [epoch: 8.2 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00455582143628871		[learning rate: 1.4466e-05]
		[batch 20/20] avg loss: 0.006305793071978314		[learning rate: 1.444e-05]
	Learning Rate: 1.44395e-05
	LOSS [training: 0.005430807254133512 | validation: 0.003121882125838034]
	TIME [epoch: 8.16 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036358657111656895		[learning rate: 1.4413e-05]
		[batch 20/20] avg loss: 0.005248720995364741		[learning rate: 1.4387e-05]
	Learning Rate: 1.43871e-05
	LOSS [training: 0.004442293353265215 | validation: 0.00023990694362838487]
	TIME [epoch: 8.15 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026599274923652535		[learning rate: 1.4361e-05]
		[batch 20/20] avg loss: 0.0023981159833421214		[learning rate: 1.4335e-05]
	Learning Rate: 1.43349e-05
	LOSS [training: 0.002529021737853688 | validation: 0.004681937674744251]
	TIME [epoch: 8.15 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016921791498670485		[learning rate: 1.4309e-05]
		[batch 20/20] avg loss: 0.005654000744931091		[learning rate: 1.4283e-05]
	Learning Rate: 1.42829e-05
	LOSS [training: 0.0036730899473990707 | validation: 0.0025765950882978114]
	TIME [epoch: 8.2 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006559876051280495		[learning rate: 1.4257e-05]
		[batch 20/20] avg loss: 0.0009489662401986834		[learning rate: 1.4231e-05]
	Learning Rate: 1.4231e-05
	LOSS [training: 0.00375442114573959 | validation: 0.0007785448153733705]
	TIME [epoch: 8.16 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004163555857847731		[learning rate: 1.4205e-05]
		[batch 20/20] avg loss: 0.0028509567573193227		[learning rate: 1.4179e-05]
	Learning Rate: 1.41794e-05
	LOSS [training: 0.0035072563075835275 | validation: -0.002054322913538021]
	TIME [epoch: 8.14 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003633968447315029		[learning rate: 1.4154e-05]
		[batch 20/20] avg loss: 0.004088824246206713		[learning rate: 1.4128e-05]
	Learning Rate: 1.41279e-05
	LOSS [training: 0.0038613963467608705 | validation: 0.005035046881825236]
	TIME [epoch: 8.15 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004428334464096682		[learning rate: 1.4102e-05]
		[batch 20/20] avg loss: -0.0010503850767992209		[learning rate: 1.4077e-05]
	Learning Rate: 1.40767e-05
	LOSS [training: 0.0016889746936487302 | validation: 0.0006174192323123344]
	TIME [epoch: 8.19 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0056158003605278175		[learning rate: 1.4051e-05]
		[batch 20/20] avg loss: -0.0005674615480820204		[learning rate: 1.4026e-05]
	Learning Rate: 1.40256e-05
	LOSS [training: 0.002524169406222898 | validation: 0.001110051351180182]
	TIME [epoch: 8.16 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0049141977453055225		[learning rate: 1.4e-05]
		[batch 20/20] avg loss: -0.0011447142259498795		[learning rate: 1.3975e-05]
	Learning Rate: 1.39747e-05
	LOSS [training: 0.0018847417596778212 | validation: 0.0069936288213502795]
	TIME [epoch: 8.14 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032826728226996024		[learning rate: 1.3949e-05]
		[batch 20/20] avg loss: 0.003283810457152911		[learning rate: 1.3924e-05]
	Learning Rate: 1.3924e-05
	LOSS [training: 0.003283241639926257 | validation: 0.002824438544274389]
	TIME [epoch: 8.15 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019465468856890496		[learning rate: 1.3899e-05]
		[batch 20/20] avg loss: 0.0033695845465297448		[learning rate: 1.3873e-05]
	Learning Rate: 1.38734e-05
	LOSS [training: 0.0026580657161093973 | validation: -0.003097315809477699]
	TIME [epoch: 8.2 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017220234561907423		[learning rate: 1.3848e-05]
		[batch 20/20] avg loss: -0.0008939866693081343		[learning rate: 1.3823e-05]
	Learning Rate: 1.38231e-05
	LOSS [training: 0.00041401839344130405 | validation: 0.005549916108118161]
	TIME [epoch: 8.16 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008280314435525644		[learning rate: 1.3798e-05]
		[batch 20/20] avg loss: 0.003932978372456193		[learning rate: 1.3773e-05]
	Learning Rate: 1.37729e-05
	LOSS [training: 0.006106646403990919 | validation: -0.0019964316678468593]
	TIME [epoch: 8.14 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002674451376192956		[learning rate: 1.3748e-05]
		[batch 20/20] avg loss: 0.005878192925413548		[learning rate: 1.3723e-05]
	Learning Rate: 1.37229e-05
	LOSS [training: 0.004276322150803252 | validation: 0.005428152983487554]
	TIME [epoch: 8.14 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009099225107398957		[learning rate: 1.3698e-05]
		[batch 20/20] avg loss: 0.0021275144542484636		[learning rate: 1.3673e-05]
	Learning Rate: 1.36731e-05
	LOSS [training: 0.0056133697808237095 | validation: 0.001517046379563508]
	TIME [epoch: 8.19 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022231877188701264		[learning rate: 1.3648e-05]
		[batch 20/20] avg loss: 0.003981327126179461		[learning rate: 1.3624e-05]
	Learning Rate: 1.36235e-05
	LOSS [training: 0.003102257422524793 | validation: 0.0064296648194360284]
	TIME [epoch: 8.17 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013362307445616427		[learning rate: 1.3599e-05]
		[batch 20/20] avg loss: 0.007323477984142707		[learning rate: 1.3574e-05]
	Learning Rate: 1.35741e-05
	LOSS [training: 0.004329854364352175 | validation: 0.00723261690951936]
	TIME [epoch: 8.14 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003711617014090006		[learning rate: 1.3549e-05]
		[batch 20/20] avg loss: 0.0048068016862870735		[learning rate: 1.3525e-05]
	Learning Rate: 1.35248e-05
	LOSS [training: 0.00425920935018854 | validation: 0.00550282703276124]
	TIME [epoch: 8.16 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016828601910928915		[learning rate: 1.35e-05]
		[batch 20/20] avg loss: 0.004614437621743025		[learning rate: 1.3476e-05]
	Learning Rate: 1.34757e-05
	LOSS [training: 0.0031486489064179576 | validation: -9.25747573481215e-05]
	TIME [epoch: 8.19 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037379025279016924		[learning rate: 1.3451e-05]
		[batch 20/20] avg loss: 8.494123292579637e-05		[learning rate: 1.3427e-05]
	Learning Rate: 1.34268e-05
	LOSS [training: 0.001911421880413744 | validation: 0.00454746079142999]
	TIME [epoch: 8.17 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015643542569154672		[learning rate: 1.3402e-05]
		[batch 20/20] avg loss: 0.004468026999409217		[learning rate: 1.3378e-05]
	Learning Rate: 1.33781e-05
	LOSS [training: 0.0030161906281623426 | validation: 0.0027697071581511087]
	TIME [epoch: 8.15 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002604331773920477		[learning rate: 1.3354e-05]
		[batch 20/20] avg loss: 0.0005266233891822226		[learning rate: 1.333e-05]
	Learning Rate: 1.33296e-05
	LOSS [training: 0.0015654775815513496 | validation: 0.0049689845071356105]
	TIME [epoch: 8.16 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014817947973210985		[learning rate: 1.3305e-05]
		[batch 20/20] avg loss: 0.008092031894284354		[learning rate: 1.3281e-05]
	Learning Rate: 1.32812e-05
	LOSS [training: 0.004786913345802726 | validation: 0.005991862829244158]
	TIME [epoch: 8.17 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002006285267842362		[learning rate: 1.3257e-05]
		[batch 20/20] avg loss: 0.005057429938003106		[learning rate: 1.3233e-05]
	Learning Rate: 1.3233e-05
	LOSS [training: 0.003531857602922734 | validation: 8.020687965714333e-05]
	TIME [epoch: 8.16 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004825586823343239		[learning rate: 1.3209e-05]
		[batch 20/20] avg loss: -0.0006994010542963353		[learning rate: 1.3185e-05]
	Learning Rate: 1.3185e-05
	LOSS [training: 0.0020630928845234526 | validation: 0.005422701198443218]
	TIME [epoch: 8.15 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004821583762432505		[learning rate: 1.3161e-05]
		[batch 20/20] avg loss: -0.0023540588043581825		[learning rate: 1.3137e-05]
	Learning Rate: 1.31371e-05
	LOSS [training: 0.0012337624790371613 | validation: 0.0041985050420227585]
	TIME [epoch: 8.17 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00445398303757516		[learning rate: 1.3113e-05]
		[batch 20/20] avg loss: -1.3635500216548427e-05		[learning rate: 1.3089e-05]
	Learning Rate: 1.30894e-05
	LOSS [training: 0.0022201737686793057 | validation: 0.007063983456431366]
	TIME [epoch: 8.17 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019505715061395486		[learning rate: 1.3066e-05]
		[batch 20/20] avg loss: 0.006194452899757392		[learning rate: 1.3042e-05]
	Learning Rate: 1.30419e-05
	LOSS [training: 0.0021219406968089223 | validation: 0.009463789678143066]
	TIME [epoch: 8.16 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037591360466697047		[learning rate: 1.3018e-05]
		[batch 20/20] avg loss: 0.0079668484347851		[learning rate: 1.2995e-05]
	Learning Rate: 1.29946e-05
	LOSS [training: 0.0058629922407274025 | validation: 0.011305101896761195]
	TIME [epoch: 8.14 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005126156253610454		[learning rate: 1.2971e-05]
		[batch 20/20] avg loss: 0.004283699214586903		[learning rate: 1.2947e-05]
	Learning Rate: 1.29475e-05
	LOSS [training: 0.004704927734098678 | validation: 0.0021773355000371535]
	TIME [epoch: 8.16 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005261299146630824		[learning rate: 1.2924e-05]
		[batch 20/20] avg loss: -0.0019495232267594579		[learning rate: 1.29e-05]
	Learning Rate: 1.29005e-05
	LOSS [training: 0.0016558879599356834 | validation: 0.003954733624393048]
	TIME [epoch: 8.17 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005274260566983246		[learning rate: 1.2877e-05]
		[batch 20/20] avg loss: 0.005258332913646454		[learning rate: 1.2854e-05]
	Learning Rate: 1.28536e-05
	LOSS [training: 0.005266296740314851 | validation: -0.006362760824773366]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r2_20240219_233648/states/model_tr_study202_1931.pth
	Model improved!!!
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003199544081046661		[learning rate: 1.283e-05]
		[batch 20/20] avg loss: -0.0011787723173823637		[learning rate: 1.2807e-05]
	Learning Rate: 1.2807e-05
	LOSS [training: 0.0010103858818321485 | validation: 0.0018099291831492992]
	TIME [epoch: 8.15 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036528780154980512		[learning rate: 1.2784e-05]
		[batch 20/20] avg loss: 0.0033031039661574045		[learning rate: 1.2761e-05]
	Learning Rate: 1.27605e-05
	LOSS [training: 0.003477990990827728 | validation: -5.145813162498814e-05]
	TIME [epoch: 8.18 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015433418173430398		[learning rate: 1.2737e-05]
		[batch 20/20] avg loss: 0.0055554436883178465		[learning rate: 1.2714e-05]
	Learning Rate: 1.27142e-05
	LOSS [training: 0.003549392752830443 | validation: 0.005445544997750529]
	TIME [epoch: 8.18 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007187885783672641		[learning rate: 1.2691e-05]
		[batch 20/20] avg loss: 0.0028741864479200343		[learning rate: 1.2668e-05]
	Learning Rate: 1.26681e-05
	LOSS [training: 0.0017964875131436491 | validation: 0.0038350375733558514]
	TIME [epoch: 8.18 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004060024082254106		[learning rate: 1.2645e-05]
		[batch 20/20] avg loss: 0.00023395226269112254		[learning rate: 1.2622e-05]
	Learning Rate: 1.26221e-05
	LOSS [training: 0.002146988172472614 | validation: 0.0019423878354173185]
	TIME [epoch: 8.15 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008699668911014268		[learning rate: 1.2599e-05]
		[batch 20/20] avg loss: 0.007921971548431321		[learning rate: 1.2576e-05]
	Learning Rate: 1.25763e-05
	LOSS [training: 0.004395969219766374 | validation: 0.003058389433759887]
	TIME [epoch: 8.18 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00042040089631365586		[learning rate: 1.2553e-05]
		[batch 20/20] avg loss: 0.004214861269987587		[learning rate: 1.2531e-05]
	Learning Rate: 1.25307e-05
	LOSS [training: 0.002317631083150622 | validation: 0.0025437528065585294]
	TIME [epoch: 8.16 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004777798546752106		[learning rate: 1.2508e-05]
		[batch 20/20] avg loss: 0.00217739833312285		[learning rate: 1.2485e-05]
	Learning Rate: 1.24852e-05
	LOSS [training: 0.003477598439937478 | validation: 0.008301926135621168]
	TIME [epoch: 8.17 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032057487283475928		[learning rate: 1.2463e-05]
		[batch 20/20] avg loss: -0.00030450055994473736		[learning rate: 1.244e-05]
	Learning Rate: 1.24399e-05
	LOSS [training: 0.0014506240842014275 | validation: 0.006064500397833134]
	TIME [epoch: 8.16 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033183800554852224		[learning rate: 1.2417e-05]
		[batch 20/20] avg loss: 0.0025133271485142726		[learning rate: 1.2395e-05]
	Learning Rate: 1.23947e-05
	LOSS [training: 0.002915853601999747 | validation: 0.009004374762985632]
	TIME [epoch: 8.2 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019415998487194524		[learning rate: 1.2372e-05]
		[batch 20/20] avg loss: 0.0014705738801914343		[learning rate: 1.235e-05]
	Learning Rate: 1.23497e-05
	LOSS [training: 0.001706086864455443 | validation: 0.0048880759088580986]
	TIME [epoch: 8.17 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002125767475238074		[learning rate: 1.2327e-05]
		[batch 20/20] avg loss: 0.00403477584680648		[learning rate: 1.2305e-05]
	Learning Rate: 1.23049e-05
	LOSS [training: 0.003080271661022277 | validation: 0.007772935719261945]
	TIME [epoch: 8.17 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019454567289933055		[learning rate: 1.2283e-05]
		[batch 20/20] avg loss: 0.003601075459559247		[learning rate: 1.226e-05]
	Learning Rate: 1.22603e-05
	LOSS [training: 0.002773266094276277 | validation: 0.0026227897431902432]
	TIME [epoch: 8.16 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003950554527272581		[learning rate: 1.2238e-05]
		[batch 20/20] avg loss: 0.002058307021954085		[learning rate: 1.2216e-05]
	Learning Rate: 1.22158e-05
	LOSS [training: 0.0030044307746133333 | validation: 0.010119798121375255]
	TIME [epoch: 8.18 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003464601200372424		[learning rate: 1.2194e-05]
		[batch 20/20] avg loss: 0.006091755591312247		[learning rate: 1.2171e-05]
	Learning Rate: 1.21714e-05
	LOSS [training: 0.004778178395842336 | validation: -0.00030109833394826766]
	TIME [epoch: 8.15 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004948237363039953		[learning rate: 1.2149e-05]
		[batch 20/20] avg loss: 0.001553316027641752		[learning rate: 1.2127e-05]
	Learning Rate: 1.21273e-05
	LOSS [training: 0.0032507766953408534 | validation: -0.0019457001682984172]
	TIME [epoch: 8.17 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006279088753235197		[learning rate: 1.2105e-05]
		[batch 20/20] avg loss: 0.00501667338097907		[learning rate: 1.2083e-05]
	Learning Rate: 1.20833e-05
	LOSS [training: 0.002822291128151295 | validation: 0.000930338755652797]
	TIME [epoch: 8.15 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004931206136994305		[learning rate: 1.2061e-05]
		[batch 20/20] avg loss: -0.003939149360471368		[learning rate: 1.2039e-05]
	Learning Rate: 1.20394e-05
	LOSS [training: 0.0004960283882614685 | validation: 0.0044486491956259415]
	TIME [epoch: 8.19 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003465256956230219		[learning rate: 1.2018e-05]
		[batch 20/20] avg loss: 0.0012648073498125998		[learning rate: 1.1996e-05]
	Learning Rate: 1.19957e-05
	LOSS [training: 0.0023650321530214098 | validation: 0.006797868978967376]
	TIME [epoch: 8.15 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026086407598226507		[learning rate: 1.1974e-05]
		[batch 20/20] avg loss: 0.0015696614971953758		[learning rate: 1.1952e-05]
	Learning Rate: 1.19522e-05
	LOSS [training: 0.0020891511285090134 | validation: 0.005103769812165183]
	TIME [epoch: 8.17 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003911750030722862		[learning rate: 1.193e-05]
		[batch 20/20] avg loss: 0.0028412198382940677		[learning rate: 1.1909e-05]
	Learning Rate: 1.19088e-05
	LOSS [training: 0.003376484934508465 | validation: 0.0051826362305483825]
	TIME [epoch: 8.14 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002138868214787873		[learning rate: 1.1887e-05]
		[batch 20/20] avg loss: 0.005612382978220424		[learning rate: 1.1866e-05]
	Learning Rate: 1.18656e-05
	LOSS [training: 0.0038756255965041488 | validation: -0.0019302940354621514]
	TIME [epoch: 8.18 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002179940151212932		[learning rate: 1.1844e-05]
		[batch 20/20] avg loss: 0.006413226062882263		[learning rate: 1.1823e-05]
	Learning Rate: 1.18225e-05
	LOSS [training: 0.004296583107047598 | validation: 0.006548843624249728]
	TIME [epoch: 8.15 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011489565155843333		[learning rate: 1.1801e-05]
		[batch 20/20] avg loss: 0.006357776401268479		[learning rate: 1.178e-05]
	Learning Rate: 1.17796e-05
	LOSS [training: 0.0037533664584264064 | validation: 0.006188876604200128]
	TIME [epoch: 8.17 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004706680138688874		[learning rate: 1.1758e-05]
		[batch 20/20] avg loss: 0.004560667924928873		[learning rate: 1.1737e-05]
	Learning Rate: 1.17369e-05
	LOSS [training: 0.004633674031808874 | validation: -0.0010221129501848945]
	TIME [epoch: 8.15 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034617147412731738		[learning rate: 1.1716e-05]
		[batch 20/20] avg loss: 0.0074020619390026505		[learning rate: 1.1694e-05]
	Learning Rate: 1.16943e-05
	LOSS [training: 0.005431888340137914 | validation: 0.009623397700500785]
	TIME [epoch: 8.18 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0023144661824900307		[learning rate: 1.1673e-05]
		[batch 20/20] avg loss: 0.0014901058573945501		[learning rate: 1.1652e-05]
	Learning Rate: 1.16518e-05
	LOSS [training: 0.0019022860199422898 | validation: 0.0007079974801530928]
	TIME [epoch: 8.15 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004758211341950783		[learning rate: 1.1631e-05]
		[batch 20/20] avg loss: -0.0006928471869124571		[learning rate: 1.161e-05]
	Learning Rate: 1.16096e-05
	LOSS [training: 0.002032682077519163 | validation: 0.0037844384682374616]
	TIME [epoch: 8.17 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016278403325630454		[learning rate: 1.1588e-05]
		[batch 20/20] avg loss: 0.011644101727256368		[learning rate: 1.1567e-05]
	Learning Rate: 1.15674e-05
	LOSS [training: 0.006635971029909706 | validation: 0.01109577856655357]
	TIME [epoch: 8.16 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002359081666764742		[learning rate: 1.1546e-05]
		[batch 20/20] avg loss: 0.0045560046950032535		[learning rate: 1.1525e-05]
	Learning Rate: 1.15255e-05
	LOSS [training: 0.003457543180883998 | validation: 0.0017352833502647723]
	TIME [epoch: 8.21 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001969910396266189		[learning rate: 1.1505e-05]
		[batch 20/20] avg loss: 0.0026076098272724405		[learning rate: 1.1484e-05]
	Learning Rate: 1.14836e-05
	LOSS [training: 0.0022887601117693147 | validation: 0.0037245515749679435]
	TIME [epoch: 8.16 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00033686745511787776		[learning rate: 1.1463e-05]
		[batch 20/20] avg loss: 0.0016934026657992828		[learning rate: 1.1442e-05]
	Learning Rate: 1.1442e-05
	LOSS [training: 0.0006782676053407025 | validation: 0.00627160279968013]
	TIME [epoch: 8.21 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001276762392008516		[learning rate: 1.1421e-05]
		[batch 20/20] avg loss: 0.008154517551870882		[learning rate: 1.14e-05]
	Learning Rate: 1.14004e-05
	LOSS [training: 0.004715639971939699 | validation: 0.0029547487554484123]
	TIME [epoch: 8.16 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026263244026713665		[learning rate: 1.138e-05]
		[batch 20/20] avg loss: 0.006065463724568216		[learning rate: 1.1359e-05]
	Learning Rate: 1.13591e-05
	LOSS [training: 0.004345894063619791 | validation: 0.0011584569192551468]
	TIME [epoch: 8.2 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006247938226248436		[learning rate: 1.1338e-05]
		[batch 20/20] avg loss: 0.0023828215201091166		[learning rate: 1.1318e-05]
	Learning Rate: 1.13178e-05
	LOSS [training: 0.004315379873178777 | validation: 0.0045669139422231456]
	TIME [epoch: 8.16 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002613181382561979		[learning rate: 1.1297e-05]
		[batch 20/20] avg loss: 0.0018243725126569216		[learning rate: 1.1277e-05]
	Learning Rate: 1.12768e-05
	LOSS [training: 0.00221877694760945 | validation: 0.002216058995514555]
	TIME [epoch: 8.19 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0052799175510012345		[learning rate: 1.1256e-05]
		[batch 20/20] avg loss: 0.0004490978773931963		[learning rate: 1.1236e-05]
	Learning Rate: 1.12358e-05
	LOSS [training: 0.0028645077141972153 | validation: 0.006045760097330674]
	TIME [epoch: 8.18 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028172171606687356		[learning rate: 1.1215e-05]
		[batch 20/20] avg loss: 0.0003646958478874698		[learning rate: 1.1195e-05]
	Learning Rate: 1.11951e-05
	LOSS [training: 0.001590956504278103 | validation: 0.006193109743832853]
	TIME [epoch: 8.2 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014857244283514436		[learning rate: 1.1175e-05]
		[batch 20/20] avg loss: 0.004117410626571377		[learning rate: 1.1154e-05]
	Learning Rate: 1.11544e-05
	LOSS [training: 0.00280156752746141 | validation: 0.008772613009970856]
	TIME [epoch: 8.17 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034250986588652286		[learning rate: 1.1134e-05]
		[batch 20/20] avg loss: 0.0041488415523030595		[learning rate: 1.1114e-05]
	Learning Rate: 1.1114e-05
	LOSS [training: 0.003786970105584144 | validation: 0.0057509848527377595]
	TIME [epoch: 8.19 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004890909677971419		[learning rate: 1.1094e-05]
		[batch 20/20] avg loss: 0.003104987613600366		[learning rate: 1.1074e-05]
	Learning Rate: 1.10736e-05
	LOSS [training: 0.003997948645785892 | validation: 0.006410218734372936]
	TIME [epoch: 8.19 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019090925839558772		[learning rate: 1.1054e-05]
		[batch 20/20] avg loss: 0.0038808657842407434		[learning rate: 1.1033e-05]
	Learning Rate: 1.10334e-05
	LOSS [training: 0.0009858866001424336 | validation: 0.0023681560634035226]
	TIME [epoch: 8.2 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010779631223737377		[learning rate: 1.1013e-05]
		[batch 20/20] avg loss: 0.0027259109902463357		[learning rate: 1.0993e-05]
	Learning Rate: 1.09934e-05
	LOSS [training: 0.0067527711069918575 | validation: 0.006991486818027691]
	TIME [epoch: 8.15 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015156615474892676		[learning rate: 1.0973e-05]
		[batch 20/20] avg loss: 0.006701836883459973		[learning rate: 1.0953e-05]
	Learning Rate: 1.09535e-05
	LOSS [training: 0.002593087667985353 | validation: 0.001791500743471283]
	TIME [epoch: 8.18 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005190158647383515		[learning rate: 1.0934e-05]
		[batch 20/20] avg loss: 0.005901580559572151		[learning rate: 1.0914e-05]
	Learning Rate: 1.09137e-05
	LOSS [training: 0.005545869603477833 | validation: 0.0032666593232783697]
	TIME [epoch: 8.18 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035168946125706526		[learning rate: 1.0894e-05]
		[batch 20/20] avg loss: 0.00413777066396747		[learning rate: 1.0874e-05]
	Learning Rate: 1.08741e-05
	LOSS [training: 0.003827332638269061 | validation: 0.0013418738124887216]
	TIME [epoch: 8.19 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004511348265163569		[learning rate: 1.0854e-05]
		[batch 20/20] avg loss: 0.003897753154144224		[learning rate: 1.0835e-05]
	Learning Rate: 1.08347e-05
	LOSS [training: 0.004204550709653897 | validation: 0.006811997856563867]
	TIME [epoch: 8.16 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004342820074809729		[learning rate: 1.0815e-05]
		[batch 20/20] avg loss: 0.0066856099193076555		[learning rate: 1.0795e-05]
	Learning Rate: 1.07954e-05
	LOSS [training: 0.0055142149970586925 | validation: 0.0012764022036327888]
	TIME [epoch: 8.16 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018513472601531216		[learning rate: 1.0776e-05]
		[batch 20/20] avg loss: 0.005609416019590812		[learning rate: 1.0756e-05]
	Learning Rate: 1.07562e-05
	LOSS [training: 0.0037303816398719667 | validation: 0.0013504550373991536]
	TIME [epoch: 8.18 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005542933765815871		[learning rate: 1.0737e-05]
		[batch 20/20] avg loss: 0.0021072328490294168		[learning rate: 1.0717e-05]
	Learning Rate: 1.07171e-05
	LOSS [training: 0.0038250833074226437 | validation: -0.0006246608337639142]
	TIME [epoch: 8.19 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006373771904767578		[learning rate: 1.0698e-05]
		[batch 20/20] avg loss: 0.006174716184362277		[learning rate: 1.0678e-05]
	Learning Rate: 1.06782e-05
	LOSS [training: 0.006274244044564928 | validation: 0.004867344123935836]
	TIME [epoch: 8.16 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010869814974943785		[learning rate: 1.0659e-05]
		[batch 20/20] avg loss: 0.0012970191661237332		[learning rate: 1.0639e-05]
	Learning Rate: 1.06395e-05
	LOSS [training: 0.0011920003318090554 | validation: 0.009793562821938738]
	TIME [epoch: 8.18 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002713142218969658		[learning rate: 1.062e-05]
		[batch 20/20] avg loss: 0.005446122683055792		[learning rate: 1.0601e-05]
	Learning Rate: 1.06009e-05
	LOSS [training: 0.0028587184524763794 | validation: -0.0019387715588364233]
	TIME [epoch: 8.19 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018713885024970029		[learning rate: 1.0582e-05]
		[batch 20/20] avg loss: 0.0040828053107024665		[learning rate: 1.0562e-05]
	Learning Rate: 1.05624e-05
	LOSS [training: 0.0029770969065997343 | validation: 0.0023134529709282984]
	TIME [epoch: 8.2 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004610320790551727		[learning rate: 1.0543e-05]
		[batch 20/20] avg loss: 0.002865288859822835		[learning rate: 1.0524e-05]
	Learning Rate: 1.05241e-05
	LOSS [training: 0.0037378048251872804 | validation: 0.0009310737139581391]
	TIME [epoch: 8.16 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030867998305885666		[learning rate: 1.0505e-05]
		[batch 20/20] avg loss: 0.0036543490252845265		[learning rate: 1.0486e-05]
	Learning Rate: 1.04859e-05
	LOSS [training: 0.0033705744279365463 | validation: 0.0058184106199749164]
	TIME [epoch: 8.18 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011877683091239288		[learning rate: 1.0467e-05]
		[batch 20/20] avg loss: 0.003631999772537288		[learning rate: 1.0448e-05]
	Learning Rate: 1.04478e-05
	LOSS [training: 0.0024098840408306086 | validation: -0.0013869778574772886]
	TIME [epoch: 8.18 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027001419568377693		[learning rate: 1.0429e-05]
		[batch 20/20] avg loss: 0.007004110317115246		[learning rate: 1.041e-05]
	Learning Rate: 1.04099e-05
	LOSS [training: 0.0048521261369765065 | validation: 0.0030234756433100566]
	TIME [epoch: 8.18 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035582615401824357		[learning rate: 1.0391e-05]
		[batch 20/20] avg loss: 0.0011059476073830767		[learning rate: 1.0372e-05]
	Learning Rate: 1.03721e-05
	LOSS [training: 0.0023321045737827563 | validation: 0.008165322667540874]
	TIME [epoch: 8.15 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014682483877189003		[learning rate: 1.0353e-05]
		[batch 20/20] avg loss: 0.0048800104855119915		[learning rate: 1.0335e-05]
	Learning Rate: 1.03345e-05
	LOSS [training: 0.0031741294366154455 | validation: 0.003090277036983352]
	TIME [epoch: 8.16 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004203971794838413		[learning rate: 1.0316e-05]
		[batch 20/20] avg loss: 0.005153875657168752		[learning rate: 1.0297e-05]
	Learning Rate: 1.0297e-05
	LOSS [training: 0.004678923726003583 | validation: 0.00661046210477995]
	TIME [epoch: 8.19 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026054537021459553		[learning rate: 1.0278e-05]
		[batch 20/20] avg loss: 0.0075944259690806		[learning rate: 1.026e-05]
	Learning Rate: 1.02596e-05
	LOSS [training: 0.005099939835613278 | validation: 0.006405262611092243]
	TIME [epoch: 8.18 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037313482003354464		[learning rate: 1.0241e-05]
		[batch 20/20] avg loss: 0.0039048278853735527		[learning rate: 1.0222e-05]
	Learning Rate: 1.02224e-05
	LOSS [training: 0.0038180880428545006 | validation: 0.0007873349597910431]
	TIME [epoch: 8.15 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00473752030669185		[learning rate: 1.0204e-05]
		[batch 20/20] avg loss: 0.008332947272805893		[learning rate: 1.0185e-05]
	Learning Rate: 1.01853e-05
	LOSS [training: 0.00653523378974887 | validation: 0.008821548716645837]
	TIME [epoch: 8.17 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005929796594689186		[learning rate: 1.0167e-05]
		[batch 20/20] avg loss: -0.0013075306199132792		[learning rate: 1.0148e-05]
	Learning Rate: 1.01483e-05
	LOSS [training: 0.0023111329873879535 | validation: 0.0009115411794972289]
	TIME [epoch: 8.2 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023558035862045585		[learning rate: 1.013e-05]
		[batch 20/20] avg loss: 0.0026411720340387868		[learning rate: 1.0112e-05]
	Learning Rate: 1.01115e-05
	LOSS [training: 0.0001426842239171143 | validation: 0.002290802901594985]
	TIME [epoch: 8.18 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005082262323151327		[learning rate: 1.0093e-05]
		[batch 20/20] avg loss: 0.0014405179011430585		[learning rate: 1.0075e-05]
	Learning Rate: 1.00748e-05
	LOSS [training: 0.0032613901121471924 | validation: -0.0015532732158974188]
	TIME [epoch: 8.16 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009559565753641064		[learning rate: 1.0057e-05]
		[batch 20/20] avg loss: -0.00017415388982490592		[learning rate: 1.0038e-05]
	Learning Rate: 1.00382e-05
	LOSS [training: 0.00039090134276959995 | validation: 0.0001949566976520256]
	TIME [epoch: 8.15 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005850575295320222		[learning rate: 1.002e-05]
		[batch 20/20] avg loss: 0.0032216315111416676		[learning rate: 1.0002e-05]
	Learning Rate: 1.00018e-05
	LOSS [training: 0.004536103403230945 | validation: -0.003210159871256547]
	TIME [epoch: 8.2 sec]
Finished training in 16528.218 seconds.
