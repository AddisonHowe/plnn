Args:
Namespace(name='model_tr_study202', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5', training_data='data/transition_rate_studies/tr_study202/tr_study202_training/r5', validation_data='data/transition_rate_studies/tr_study202/tr_study202_validation/r5', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4043781604

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.072752530130835		[learning rate: 0.01]
		[batch 20/20] avg loss: 10.49135837062563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.782055450378232 | validation: 10.850934998021884]
	TIME [epoch: 79.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.062168647529209		[learning rate: 0.01]
		[batch 20/20] avg loss: 9.124617589623332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.593393118576271 | validation: 10.260994214067587]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.23302984764914		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.5772957915972174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.905162819623179 | validation: 6.176312424644144]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.539084584557275		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.280616512009534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.409850548283404 | validation: 4.141430277140893]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.972590180511998		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.535896187236481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.754243183874239 | validation: 4.192066374704131]
	TIME [epoch: 8.18 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.255659369812723		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.779009045658031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.517334207735376 | validation: 4.115916390698649]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.5027461668364035		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.2676497039362635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.385197935386333 | validation: 4.598126145327097]
	TIME [epoch: 8.13 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.3976371007082715		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.322649941737622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3601435212229465 | validation: 4.208048439021843]
	TIME [epoch: 8.12 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.4352493604621355		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.120500701550506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.277875031006322 | validation: 3.84992317873993]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.312118641061074		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.089624879238067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.20087176014957 | validation: 4.397543910433594]
	TIME [epoch: 8.14 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.205351595596778		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.1434669418409475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.174409268718863 | validation: 3.8826616784822647]
	TIME [epoch: 8.12 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.230239766645794		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.999213770525312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1147267685855535 | validation: 3.9748221057037005]
	TIME [epoch: 8.12 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.038749631465076		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.142870161917393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.090809896691233 | validation: 3.9696427530876006]
	TIME [epoch: 8.14 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.9339411114720706		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.111339702176323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0226404068241965 | validation: 4.010540335898688]
	TIME [epoch: 8.14 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.1918455337526215		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.0892174021706165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.14053146796162 | validation: 3.187543074495066]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.944184332719343		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.902202060788094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9231931967537177 | validation: 4.362367472262398]
	TIME [epoch: 8.15 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.9504419068226064		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.9684518073390302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.959446857080819 | validation: 3.15316131558251]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.9308524523545287		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.865989014297071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8984207333258 | validation: 3.488608793166505]
	TIME [epoch: 8.19 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.538661957806228		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.974542420843364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7566021893247963 | validation: 3.6633435275657416]
	TIME [epoch: 8.15 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.848696659159951		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.494841273642752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.671768966401352 | validation: 2.270937333390379]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0271790959933256		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.2873548176534433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1572669568233844 | validation: 3.120319981517158]
	TIME [epoch: 8.15 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.211869340327805		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.032212888746764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.122041114537285 | validation: 2.2699236182243565]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0968506659807296		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.990003873778967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0434272698798486 | validation: 2.7465051447192943]
	TIME [epoch: 8.13 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0732235976590014		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.958599731077152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0159116643680766 | validation: 1.8863086692147288]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.063871583378785		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.250186297013247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.157028940196016 | validation: 4.378218494467811]
	TIME [epoch: 8.12 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.408127867847169		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9190149504768375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.163571409162004 | validation: 2.5158997261070484]
	TIME [epoch: 8.14 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0106954761723133		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.796060139110515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9033778076414145 | validation: 2.261684400910023]
	TIME [epoch: 8.14 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9380608562521036		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8938879583134507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.915974407282777 | validation: 2.0612759690045896]
	TIME [epoch: 8.12 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.835699278296503		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.938322485697234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8870108819968685 | validation: 1.769611305334017]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.005255288917746		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6150613511707617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.810158320044254 | validation: 2.1278705147278005]
	TIME [epoch: 8.13 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.895639664973262		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.740470682121238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8180551735472497 | validation: 2.431314399345611]
	TIME [epoch: 8.15 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7954556765695022		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.968983970446156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.882219823507829 | validation: 1.9459975864380628]
	TIME [epoch: 8.12 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.18619872881698		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8187742251939256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0024864770054527 | validation: 1.9052431148160922]
	TIME [epoch: 8.12 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8592429933447012		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6556220891158353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7574325412302683 | validation: 2.0225623406536766]
	TIME [epoch: 8.13 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9377924983028163		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6326151038110255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7852038010569204 | validation: 2.2838756790997965]
	TIME [epoch: 8.14 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7029251005850594		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6095888890179726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.656256994801516 | validation: 1.9225136404270196]
	TIME [epoch: 8.14 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6430945178445353		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.218809696538132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9309521071913336 | validation: 1.8512618132867766]
	TIME [epoch: 8.13 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0715739092647274		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.754381370830738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9129776400477327 | validation: 1.7754543526260693]
	TIME [epoch: 8.12 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7599638988833215		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6912077383197213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.725585818601522 | validation: 1.7796884101415644]
	TIME [epoch: 8.13 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4705047574581256		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7120170171182947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.59126088728821 | validation: 2.213447771874825]
	TIME [epoch: 8.15 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5154810473811144		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8307244960210847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.673102771701099 | validation: 2.3930829397971474]
	TIME [epoch: 8.12 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.732770392397151		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4398296739345855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5863000331658683 | validation: 1.8609745276619618]
	TIME [epoch: 8.12 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6033257915154424		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4818006541796436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5425632228475434 | validation: 1.9365223907263314]
	TIME [epoch: 8.12 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.59700848848111		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4287266814607262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.512867584970918 | validation: 1.7126629775206568]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.484682095287896		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4616652898486047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.473173692568251 | validation: 1.733984444670568]
	TIME [epoch: 8.13 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.644728686728274		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.360352756961351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.502540721844812 | validation: 1.7228194194547022]
	TIME [epoch: 8.12 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4915015507086697		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5324568263999985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.511979188554334 | validation: 1.6577374843067667]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.476556117709442		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5853437289299293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5309499233196853 | validation: 1.6118671976504604]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.462592987720262		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0161464560654623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7393697218928623 | validation: 1.6189578042843493]
	TIME [epoch: 8.14 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7544848471904997		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.290627158496898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.522556002843699 | validation: 1.6724414743440141]
	TIME [epoch: 8.11 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4593991644859297		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5046796091366326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.482039386811281 | validation: 1.6679219961317227]
	TIME [epoch: 8.12 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.602634328863222		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5367760668466994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.56970519785496 | validation: 1.6229233530669738]
	TIME [epoch: 8.11 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.523039022313392		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.399696409655343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.461367715984367 | validation: 2.133818084147868]
	TIME [epoch: 8.15 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.735350891427525		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.232789014819415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4840699531234702 | validation: 2.4282782212539384]
	TIME [epoch: 8.11 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.392484761764725		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5357263164184056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.464105539091565 | validation: 1.6093146378284362]
	TIME [epoch: 8.11 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9784410692530376		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1418140087239794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5601275389885088 | validation: 0.9357609830850323]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.077712671131618		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0054996624519523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.041606166791785 | validation: 0.6712273999226508]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9760384839010252		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8818295363870774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9289340101440512 | validation: 0.8457561157996111]
	TIME [epoch: 8.13 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9265617649424718		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8430195953101325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.884790680126302 | validation: 0.8558364867745714]
	TIME [epoch: 8.11 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8649807501926322		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9598085154149208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9123946328037764 | validation: 0.6946502793887882]
	TIME [epoch: 8.1 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.854813679671825		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7828600522540532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.818836865962939 | validation: 0.9459931298254987]
	TIME [epoch: 8.11 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7441214568172432		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7570656784179278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7505935676175856 | validation: 0.49832234440685735]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7748759737352009		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9775841812181308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8762300774766659 | validation: 1.3963000957230176]
	TIME [epoch: 8.15 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8249936582701475		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7802763663760627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8026350123231051 | validation: 0.5215028057980129]
	TIME [epoch: 8.15 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7777715006066412		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7118880893305511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7448297949685962 | validation: 1.6202228852338758]
	TIME [epoch: 8.14 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8655218533771383		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7116714312039638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7885966422905508 | validation: 0.45320401376107794]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6702135701763178		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6363322767907486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6532729234835332 | validation: 0.6575838126001219]
	TIME [epoch: 8.15 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7856963806027408		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8361834806613387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8109399306320396 | validation: 0.6270178791121793]
	TIME [epoch: 8.14 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8110241685026045		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7051403883369756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.75808227841979 | validation: 0.49433090923885353]
	TIME [epoch: 8.14 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7485821362144941		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6853777887171931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7169799624658437 | validation: 0.8784742166285342]
	TIME [epoch: 8.15 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7743526158955711		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7953793059043177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7848659608999445 | validation: 0.6547507506999631]
	TIME [epoch: 8.16 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6066422601103052		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8290262128624905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7178342364863978 | validation: 0.41928293666774197]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.667647433915137		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7111764291373307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6894119315262339 | validation: 1.6801505507488588]
	TIME [epoch: 8.14 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7664173651699013		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8892629425147482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8278401538423248 | validation: 0.8520144820809346]
	TIME [epoch: 8.13 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8026015578448883		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6783228350088905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7404621964268894 | validation: 0.8057885736297867]
	TIME [epoch: 8.16 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6932629742783176		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6864049364567538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6898339553675357 | validation: 0.8122659634738473]
	TIME [epoch: 8.14 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7002682442447067		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7372969037432527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7187825739939797 | validation: 0.5119644290696836]
	TIME [epoch: 8.14 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8268668841756319		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7127876515482294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7698272678619307 | validation: 0.6194185439584919]
	TIME [epoch: 8.13 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6834741805401275		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6244810680308664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6539776242854969 | validation: 0.9437371298910389]
	TIME [epoch: 8.15 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5901778375345279		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6426992057632399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.616438521648884 | validation: 1.014918011042063]
	TIME [epoch: 8.15 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7411613657265824		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7735144033472938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7573378845369383 | validation: 0.6683816964828011]
	TIME [epoch: 8.13 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6020881472183657		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.680724914826804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6414065310225847 | validation: 0.7279214278109436]
	TIME [epoch: 8.13 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5692497284572269		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6721489349871403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6206993317221835 | validation: 0.4708206340612652]
	TIME [epoch: 8.13 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6576007416505474		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6705801928072807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6640904672289138 | validation: 1.1560649531081837]
	TIME [epoch: 8.16 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7238166508391269		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7451799152533849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7344982830462561 | validation: 1.2919207564334418]
	TIME [epoch: 8.13 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2403867979935235		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8219941868340328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0311904924137782 | validation: 1.8746031882453047]
	TIME [epoch: 8.13 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9498330307594258		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6347770775272022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7923050541433141 | validation: 1.549315289713066]
	TIME [epoch: 8.14 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9592471268045705		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6933593769151224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8263032518598464 | validation: 0.36742979198067544]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5828211575583339		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6852442048999089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6340326812291215 | validation: 0.45476543063200947]
	TIME [epoch: 8.14 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7210769494569278		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6534172552074384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.687247102332183 | validation: 0.5811930516186625]
	TIME [epoch: 8.13 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.590084179488118		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6142990477821619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.60219161363514 | validation: 0.568135617664956]
	TIME [epoch: 8.13 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5969866355911374		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5836890655160364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5903378505535868 | validation: 0.6568564993114381]
	TIME [epoch: 8.13 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6243012792871825		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7873251378689714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.705813208578077 | validation: 0.4535161138334778]
	TIME [epoch: 8.16 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.727789612443425		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0076099183632317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8676997654033283 | validation: 1.3653574476217136]
	TIME [epoch: 8.13 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7777027058406054		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6827160346956277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7302093702681165 | validation: 0.6549702647654]
	TIME [epoch: 8.13 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7248416241470924		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7251548279003106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7249982260237015 | validation: 0.7546739724264457]
	TIME [epoch: 8.13 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7145831216995185		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6588314010047944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6867072613521564 | validation: 0.5826649885261735]
	TIME [epoch: 8.16 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0999845330317353		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3006987688093965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2003416509205658 | validation: 1.0434116511371068]
	TIME [epoch: 8.14 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7678192595312371		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8831571759815648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8254882177564011 | validation: 0.5452171600701661]
	TIME [epoch: 8.14 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6538162304679512		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0796162225039354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8667162264859435 | validation: 0.5513445689877936]
	TIME [epoch: 8.13 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.705265994424721		[learning rate: 0.0099837]
		[batch 20/20] avg loss: 0.7463338678807796		[learning rate: 0.0099655]
	Learning Rate: 0.00996552
	LOSS [training: 0.7257999311527503 | validation: 0.7129677136595065]
	TIME [epoch: 8.13 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6954328996565022		[learning rate: 0.0099474]
		[batch 20/20] avg loss: 0.6323401449700781		[learning rate: 0.0099294]
	Learning Rate: 0.00992935
	LOSS [training: 0.66388652231329 | validation: 0.5090360828167578]
	TIME [epoch: 8.16 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6575815720577608		[learning rate: 0.0099113]
		[batch 20/20] avg loss: 1.1032591904579057		[learning rate: 0.0098933]
	Learning Rate: 0.00989332
	LOSS [training: 0.8804203812578333 | validation: 0.6226835473297943]
	TIME [epoch: 8.13 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.015682708761935		[learning rate: 0.0098754]
		[batch 20/20] avg loss: 0.7553760843132221		[learning rate: 0.0098574]
	Learning Rate: 0.00985742
	LOSS [training: 0.8855293965375785 | validation: 1.0505271876440796]
	TIME [epoch: 8.13 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8309885934726328		[learning rate: 0.0098395]
		[batch 20/20] avg loss: 0.698540986979537		[learning rate: 0.0098216]
	Learning Rate: 0.00982164
	LOSS [training: 0.7647647902260848 | validation: 0.5272972856291567]
	TIME [epoch: 8.14 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6812451005228849		[learning rate: 0.0098038]
		[batch 20/20] avg loss: 0.6241756566545504		[learning rate: 0.009786]
	Learning Rate: 0.009786
	LOSS [training: 0.6527103785887176 | validation: 0.4424308319331686]
	TIME [epoch: 8.16 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.593567566485137		[learning rate: 0.0097682]
		[batch 20/20] avg loss: 0.6074371470148712		[learning rate: 0.0097505]
	Learning Rate: 0.00975049
	LOSS [training: 0.6005023567500041 | validation: 0.3979583195059908]
	TIME [epoch: 8.13 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6427303531132283		[learning rate: 0.0097328]
		[batch 20/20] avg loss: 0.590404524418908		[learning rate: 0.0097151]
	Learning Rate: 0.0097151
	LOSS [training: 0.6165674387660681 | validation: 1.1520312647501574]
	TIME [epoch: 8.13 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6991862779258688		[learning rate: 0.0096975]
		[batch 20/20] avg loss: 0.66915398691196		[learning rate: 0.0096798]
	Learning Rate: 0.00967984
	LOSS [training: 0.6841701324189146 | validation: 0.4539424966311325]
	TIME [epoch: 8.13 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1086521312166848		[learning rate: 0.0096623]
		[batch 20/20] avg loss: 0.8133468632295416		[learning rate: 0.0096447]
	Learning Rate: 0.00964472
	LOSS [training: 0.9609994972231132 | validation: 0.6753728671966519]
	TIME [epoch: 8.14 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0766475042238115		[learning rate: 0.0096272]
		[batch 20/20] avg loss: 0.9205596922572669		[learning rate: 0.0096097]
	Learning Rate: 0.00960972
	LOSS [training: 0.9986035982405392 | validation: 0.9765512109161734]
	TIME [epoch: 8.15 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6070926479623854		[learning rate: 0.0095923]
		[batch 20/20] avg loss: 0.5917781589482072		[learning rate: 0.0095748]
	Learning Rate: 0.00957484
	LOSS [training: 0.5994354034552962 | validation: 0.6983262871385831]
	TIME [epoch: 8.13 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5474706233280713		[learning rate: 0.0095575]
		[batch 20/20] avg loss: 0.86635911223128		[learning rate: 0.0095401]
	Learning Rate: 0.00954009
	LOSS [training: 0.7069148677796757 | validation: 0.3520004990648036]
	TIME [epoch: 8.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9088989843100397		[learning rate: 0.0095228]
		[batch 20/20] avg loss: 0.6514802306451913		[learning rate: 0.0095055]
	Learning Rate: 0.00950547
	LOSS [training: 0.7801896074776156 | validation: 0.6016176054478795]
	TIME [epoch: 8.13 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5780996432840735		[learning rate: 0.0094882]
		[batch 20/20] avg loss: 0.5846053619967495		[learning rate: 0.009471]
	Learning Rate: 0.00947098
	LOSS [training: 0.5813525026404114 | validation: 0.7297255894488818]
	TIME [epoch: 8.14 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6693723793037224		[learning rate: 0.0094538]
		[batch 20/20] avg loss: 0.9021575698760771		[learning rate: 0.0094366]
	Learning Rate: 0.0094366
	LOSS [training: 0.7857649745898997 | validation: 0.6474348884767359]
	TIME [epoch: 8.12 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6017229723429595		[learning rate: 0.0094195]
		[batch 20/20] avg loss: 0.9638187041646239		[learning rate: 0.0094024]
	Learning Rate: 0.00940236
	LOSS [training: 0.7827708382537916 | validation: 0.5400871504495875]
	TIME [epoch: 8.12 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6921654281543471		[learning rate: 0.0093853]
		[batch 20/20] avg loss: 0.7893219128660778		[learning rate: 0.0093682]
	Learning Rate: 0.00936824
	LOSS [training: 0.7407436705102124 | validation: 0.6364869194278332]
	TIME [epoch: 8.12 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6879755686837458		[learning rate: 0.0093512]
		[batch 20/20] avg loss: 0.6468174723612633		[learning rate: 0.0093342]
	Learning Rate: 0.00933424
	LOSS [training: 0.6673965205225045 | validation: 0.5154762351852855]
	TIME [epoch: 8.14 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6555210749867662		[learning rate: 0.0093173]
		[batch 20/20] avg loss: 0.5944105158152135		[learning rate: 0.0093004]
	Learning Rate: 0.00930036
	LOSS [training: 0.62496579540099 | validation: 0.4349108495936326]
	TIME [epoch: 8.13 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5927675035018185		[learning rate: 0.0092835]
		[batch 20/20] avg loss: 0.5339612862958758		[learning rate: 0.0092666]
	Learning Rate: 0.00926661
	LOSS [training: 0.563364394898847 | validation: 0.41338692727795223]
	TIME [epoch: 8.11 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4831511204325887		[learning rate: 0.0092498]
		[batch 20/20] avg loss: 0.5595833395336902		[learning rate: 0.009233]
	Learning Rate: 0.00923298
	LOSS [training: 0.5213672299831394 | validation: 0.4207190927649642]
	TIME [epoch: 8.12 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49982190324730985		[learning rate: 0.0092162]
		[batch 20/20] avg loss: 0.5395719643368956		[learning rate: 0.0091995]
	Learning Rate: 0.00919948
	LOSS [training: 0.5196969337921027 | validation: 0.6648864737401077]
	TIME [epoch: 8.12 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.601063393749265		[learning rate: 0.0091828]
		[batch 20/20] avg loss: 0.7457534424140935		[learning rate: 0.0091661]
	Learning Rate: 0.00916609
	LOSS [training: 0.6734084180816794 | validation: 0.849576356840829]
	TIME [epoch: 8.14 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6357734257829237		[learning rate: 0.0091494]
		[batch 20/20] avg loss: 0.7172480757301645		[learning rate: 0.0091328]
	Learning Rate: 0.00913283
	LOSS [training: 0.6765107507565443 | validation: 0.6127917537385726]
	TIME [epoch: 8.12 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5022378980105181		[learning rate: 0.0091162]
		[batch 20/20] avg loss: 0.739374047024134		[learning rate: 0.0090997]
	Learning Rate: 0.00909968
	LOSS [training: 0.6208059725173262 | validation: 1.4893604879968074]
	TIME [epoch: 8.11 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6965549086670908		[learning rate: 0.0090832]
		[batch 20/20] avg loss: 0.5928035112650596		[learning rate: 0.0090667]
	Learning Rate: 0.00906666
	LOSS [training: 0.6446792099660751 | validation: 0.40682400492148596]
	TIME [epoch: 8.12 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5023169240547987		[learning rate: 0.0090502]
		[batch 20/20] avg loss: 0.7103561645387867		[learning rate: 0.0090338]
	Learning Rate: 0.00903376
	LOSS [training: 0.6063365442967927 | validation: 0.9365277607250939]
	TIME [epoch: 8.13 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5406488291243464		[learning rate: 0.0090174]
		[batch 20/20] avg loss: 0.5415495713317148		[learning rate: 0.009001]
	Learning Rate: 0.00900097
	LOSS [training: 0.5410992002280306 | validation: 0.5691977800487835]
	TIME [epoch: 8.13 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7203996043167276		[learning rate: 0.0089846]
		[batch 20/20] avg loss: 0.5393764732496418		[learning rate: 0.0089683]
	Learning Rate: 0.00896831
	LOSS [training: 0.6298880387831847 | validation: 0.47881318013895185]
	TIME [epoch: 8.12 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5390732475986797		[learning rate: 0.008952]
		[batch 20/20] avg loss: 0.604809172077796		[learning rate: 0.0089358]
	Learning Rate: 0.00893576
	LOSS [training: 0.5719412098382379 | validation: 0.3820667296046357]
	TIME [epoch: 8.11 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6859935987906666		[learning rate: 0.0089195]
		[batch 20/20] avg loss: 0.5298427895739424		[learning rate: 0.0089033]
	Learning Rate: 0.00890333
	LOSS [training: 0.6079181941823044 | validation: 0.5189668803553842]
	TIME [epoch: 8.12 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5157897666886719		[learning rate: 0.0088872]
		[batch 20/20] avg loss: 0.5530083196710457		[learning rate: 0.008871]
	Learning Rate: 0.00887102
	LOSS [training: 0.5343990431798588 | validation: 0.32822548474549074]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4594805669688351		[learning rate: 0.0088549]
		[batch 20/20] avg loss: 0.6091963600740447		[learning rate: 0.0088388]
	Learning Rate: 0.00883883
	LOSS [training: 0.5343384635214401 | validation: 0.5708825924551846]
	TIME [epoch: 8.12 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4657695891312243		[learning rate: 0.0088228]
		[batch 20/20] avg loss: 0.4701129286236867		[learning rate: 0.0088068]
	Learning Rate: 0.00880675
	LOSS [training: 0.4679412588774555 | validation: 0.4604190532595897]
	TIME [epoch: 8.12 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5130797400876964		[learning rate: 0.0087908]
		[batch 20/20] avg loss: 0.4745619920716534		[learning rate: 0.0087748]
	Learning Rate: 0.00877479
	LOSS [training: 0.4938208660796749 | validation: 0.3523414239364584]
	TIME [epoch: 8.12 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6471520974270482		[learning rate: 0.0087589]
		[batch 20/20] avg loss: 0.47125391824673335		[learning rate: 0.0087429]
	Learning Rate: 0.00874295
	LOSS [training: 0.5592030078368907 | validation: 0.32368411488592347]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48765149903198673		[learning rate: 0.0087271]
		[batch 20/20] avg loss: 0.5718153739421818		[learning rate: 0.0087112]
	Learning Rate: 0.00871122
	LOSS [training: 0.5297334364870843 | validation: 0.5839948456379591]
	TIME [epoch: 8.13 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5368971841024214		[learning rate: 0.0086954]
		[batch 20/20] avg loss: 0.5397593146225506		[learning rate: 0.0086796]
	Learning Rate: 0.00867961
	LOSS [training: 0.5383282493624861 | validation: 0.4519192651993411]
	TIME [epoch: 8.12 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5753267882225244		[learning rate: 0.0086638]
		[batch 20/20] avg loss: 0.6187067433639215		[learning rate: 0.0086481]
	Learning Rate: 0.00864811
	LOSS [training: 0.597016765793223 | validation: 0.5481054898876984]
	TIME [epoch: 8.12 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5782095370974007		[learning rate: 0.0086324]
		[batch 20/20] avg loss: 0.49771551259090685		[learning rate: 0.0086167]
	Learning Rate: 0.00861672
	LOSS [training: 0.5379625248441539 | validation: 0.6771030054466598]
	TIME [epoch: 8.12 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5142812860088718		[learning rate: 0.0086011]
		[batch 20/20] avg loss: 0.5812159190592003		[learning rate: 0.0085855]
	Learning Rate: 0.00858545
	LOSS [training: 0.5477486025340361 | validation: 0.3146872100684169]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4970838806491451		[learning rate: 0.0085699]
		[batch 20/20] avg loss: 0.5542769413287285		[learning rate: 0.0085543]
	Learning Rate: 0.00855429
	LOSS [training: 0.5256804109889368 | validation: 0.45429837762544656]
	TIME [epoch: 8.13 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4915878180601812		[learning rate: 0.0085388]
		[batch 20/20] avg loss: 0.48328287111425466		[learning rate: 0.0085232]
	Learning Rate: 0.00852325
	LOSS [training: 0.48743534458721793 | validation: 0.3652059516927365]
	TIME [epoch: 8.11 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5357920887794367		[learning rate: 0.0085078]
		[batch 20/20] avg loss: 0.4330109429410845		[learning rate: 0.0084923]
	Learning Rate: 0.00849232
	LOSS [training: 0.48440151586026065 | validation: 0.5795626818750097]
	TIME [epoch: 8.12 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5861158253707045		[learning rate: 0.0084769]
		[batch 20/20] avg loss: 0.36719259564198214		[learning rate: 0.0084615]
	Learning Rate: 0.0084615
	LOSS [training: 0.47665421050634327 | validation: 0.2198525077182371]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4659100557452594		[learning rate: 0.0084461]
		[batch 20/20] avg loss: 0.6385495397437045		[learning rate: 0.0084308]
	Learning Rate: 0.00843079
	LOSS [training: 0.5522297977444819 | validation: 0.29978286010374555]
	TIME [epoch: 8.12 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.451082035105786		[learning rate: 0.0084155]
		[batch 20/20] avg loss: 0.4511958118295589		[learning rate: 0.0084002]
	Learning Rate: 0.0084002
	LOSS [training: 0.45113892346767237 | validation: 1.1036025485496446]
	TIME [epoch: 8.11 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6134292852840032		[learning rate: 0.0083849]
		[batch 20/20] avg loss: 0.37257984357354		[learning rate: 0.0083697]
	Learning Rate: 0.00836971
	LOSS [training: 0.49300456442877144 | validation: 0.31795023617783424]
	TIME [epoch: 8.11 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4658642250969141		[learning rate: 0.0083545]
		[batch 20/20] avg loss: 0.36275910511145076		[learning rate: 0.0083393]
	Learning Rate: 0.00833934
	LOSS [training: 0.4143116651041824 | validation: 0.3518453178979371]
	TIME [epoch: 8.14 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4136128319471795		[learning rate: 0.0083242]
		[batch 20/20] avg loss: 0.5337689378608692		[learning rate: 0.0083091]
	Learning Rate: 0.00830907
	LOSS [training: 0.4736908849040244 | validation: 0.3169626053708922]
	TIME [epoch: 8.12 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35221714418098327		[learning rate: 0.008294]
		[batch 20/20] avg loss: 0.4900909695716078		[learning rate: 0.0082789]
	Learning Rate: 0.00827892
	LOSS [training: 0.42115405687629553 | validation: 0.2127785533577595]
	TIME [epoch: 8.11 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45441529539055364		[learning rate: 0.0082639]
		[batch 20/20] avg loss: 0.47038441560277916		[learning rate: 0.0082489]
	Learning Rate: 0.00824887
	LOSS [training: 0.4623998554966664 | validation: 0.3865004790142549]
	TIME [epoch: 8.11 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43661648078558546		[learning rate: 0.0082339]
		[batch 20/20] avg loss: 0.5118009158523026		[learning rate: 0.0082189]
	Learning Rate: 0.00821894
	LOSS [training: 0.47420869831894413 | validation: 0.4774582807714969]
	TIME [epoch: 8.11 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5788993649758366		[learning rate: 0.008204]
		[batch 20/20] avg loss: 0.45005327720748217		[learning rate: 0.0081891]
	Learning Rate: 0.00818911
	LOSS [training: 0.5144763210916593 | validation: 0.3975456097710397]
	TIME [epoch: 8.13 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4876963500521444		[learning rate: 0.0081742]
		[batch 20/20] avg loss: 0.38029879212462336		[learning rate: 0.0081594]
	Learning Rate: 0.00815939
	LOSS [training: 0.43399757108838377 | validation: 0.4943498191687512]
	TIME [epoch: 8.12 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5750202541530888		[learning rate: 0.0081446]
		[batch 20/20] avg loss: 0.5014139426462673		[learning rate: 0.0081298]
	Learning Rate: 0.00812978
	LOSS [training: 0.5382170983996778 | validation: 0.5850080807599518]
	TIME [epoch: 8.12 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.874713475877987		[learning rate: 0.008115]
		[batch 20/20] avg loss: 0.3650054652621991		[learning rate: 0.0081003]
	Learning Rate: 0.00810028
	LOSS [training: 0.619859470570093 | validation: 0.4314439735930146]
	TIME [epoch: 8.11 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4774842323248546		[learning rate: 0.0080856]
		[batch 20/20] avg loss: 0.45625600541551936		[learning rate: 0.0080709]
	Learning Rate: 0.00807088
	LOSS [training: 0.46687011887018687 | validation: 0.24974723890830908]
	TIME [epoch: 8.13 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39017310447122033		[learning rate: 0.0080562]
		[batch 20/20] avg loss: 0.44343994768612377		[learning rate: 0.0080416]
	Learning Rate: 0.00804159
	LOSS [training: 0.41680652607867197 | validation: 0.3698029721524194]
	TIME [epoch: 8.13 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5154575530217638		[learning rate: 0.008027]
		[batch 20/20] avg loss: 0.5383104669634651		[learning rate: 0.0080124]
	Learning Rate: 0.00801241
	LOSS [training: 0.5268840099926145 | validation: 0.31031520766829773]
	TIME [epoch: 8.11 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39617187049948466		[learning rate: 0.0079979]
		[batch 20/20] avg loss: 0.4694469909020871		[learning rate: 0.0079833]
	Learning Rate: 0.00798333
	LOSS [training: 0.4328094307007858 | validation: 0.35818584753207916]
	TIME [epoch: 8.11 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5162799382787276		[learning rate: 0.0079688]
		[batch 20/20] avg loss: 0.37063147025280785		[learning rate: 0.0079544]
	Learning Rate: 0.00795436
	LOSS [training: 0.44345570426576775 | validation: 0.2290404622241534]
	TIME [epoch: 8.11 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3652413950711314		[learning rate: 0.0079399]
		[batch 20/20] avg loss: 0.455435333566096		[learning rate: 0.0079255]
	Learning Rate: 0.00792549
	LOSS [training: 0.4103383643186137 | validation: 0.3996670752619509]
	TIME [epoch: 8.16 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5478130694032223		[learning rate: 0.0079111]
		[batch 20/20] avg loss: 0.45961829816366206		[learning rate: 0.0078967]
	Learning Rate: 0.00789673
	LOSS [training: 0.503715683783442 | validation: 0.497567681458837]
	TIME [epoch: 8.12 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4463718961899561		[learning rate: 0.0078824]
		[batch 20/20] avg loss: 0.446474475125465		[learning rate: 0.0078681]
	Learning Rate: 0.00786807
	LOSS [training: 0.4464231856577105 | validation: 0.5913642957094848]
	TIME [epoch: 8.11 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4085930520523699		[learning rate: 0.0078538]
		[batch 20/20] avg loss: 0.5230482340022978		[learning rate: 0.0078395]
	Learning Rate: 0.00783952
	LOSS [training: 0.46582064302733384 | validation: 0.4544050930007681]
	TIME [epoch: 8.11 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6250851346742273		[learning rate: 0.0078253]
		[batch 20/20] avg loss: 0.45290461953001715		[learning rate: 0.0078111]
	Learning Rate: 0.00781107
	LOSS [training: 0.5389948771021221 | validation: 0.2299637917561702]
	TIME [epoch: 8.13 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4441459756205629		[learning rate: 0.0077969]
		[batch 20/20] avg loss: 0.4264056245062264		[learning rate: 0.0077827]
	Learning Rate: 0.00778272
	LOSS [training: 0.4352758000633946 | validation: 0.5438180633190046]
	TIME [epoch: 8.12 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39571214403279165		[learning rate: 0.0077686]
		[batch 20/20] avg loss: 0.47592666111059484		[learning rate: 0.0077545]
	Learning Rate: 0.00775448
	LOSS [training: 0.43581940257169327 | validation: 0.4862436844081004]
	TIME [epoch: 8.11 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42705250087155944		[learning rate: 0.0077404]
		[batch 20/20] avg loss: 0.3784585925343804		[learning rate: 0.0077263]
	Learning Rate: 0.00772634
	LOSS [training: 0.40275554670297 | validation: 0.4312218120411632]
	TIME [epoch: 8.12 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3919054321373243		[learning rate: 0.0077123]
		[batch 20/20] avg loss: 0.44121056006505627		[learning rate: 0.0076983]
	Learning Rate: 0.0076983
	LOSS [training: 0.4165579961011903 | validation: 0.39975991031698743]
	TIME [epoch: 8.11 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4405194288682216		[learning rate: 0.0076843]
		[batch 20/20] avg loss: 0.46724462712684306		[learning rate: 0.0076704]
	Learning Rate: 0.00767036
	LOSS [training: 0.4538820279975323 | validation: 0.37493773655459184]
	TIME [epoch: 8.14 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47462823588302633		[learning rate: 0.0076564]
		[batch 20/20] avg loss: 0.3876637565748145		[learning rate: 0.0076425]
	Learning Rate: 0.00764252
	LOSS [training: 0.4311459962289204 | validation: 0.5201307336380182]
	TIME [epoch: 8.11 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5419479220134529		[learning rate: 0.0076286]
		[batch 20/20] avg loss: 0.49920558571071183		[learning rate: 0.0076148]
	Learning Rate: 0.00761479
	LOSS [training: 0.5205767538620822 | validation: 0.40968782785113916]
	TIME [epoch: 8.12 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3799597666064512		[learning rate: 0.007601]
		[batch 20/20] avg loss: 0.4609743410880758		[learning rate: 0.0075872]
	Learning Rate: 0.00758715
	LOSS [training: 0.42046705384726357 | validation: 0.254951715449562]
	TIME [epoch: 8.12 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33591980447026365		[learning rate: 0.0075734]
		[batch 20/20] avg loss: 0.4558511684370604		[learning rate: 0.0075596]
	Learning Rate: 0.00755962
	LOSS [training: 0.39588548645366206 | validation: 0.6051944604326029]
	TIME [epoch: 8.13 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39289591422589987		[learning rate: 0.0075459]
		[batch 20/20] avg loss: 0.4503549635320564		[learning rate: 0.0075322]
	Learning Rate: 0.00753219
	LOSS [training: 0.4216254388789782 | validation: 0.47772667630438587]
	TIME [epoch: 8.13 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49807073493198917		[learning rate: 0.0075185]
		[batch 20/20] avg loss: 0.3927565633009759		[learning rate: 0.0075049]
	Learning Rate: 0.00750485
	LOSS [training: 0.4454136491164826 | validation: 0.27024957702091745]
	TIME [epoch: 8.11 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4436984349426414		[learning rate: 0.0074912]
		[batch 20/20] avg loss: 0.5277195096956151		[learning rate: 0.0074776]
	Learning Rate: 0.00747762
	LOSS [training: 0.48570897231912824 | validation: 0.2725381436263476]
	TIME [epoch: 8.12 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4463697188848528		[learning rate: 0.007464]
		[batch 20/20] avg loss: 0.3699214017046272		[learning rate: 0.0074505]
	Learning Rate: 0.00745048
	LOSS [training: 0.40814556029474003 | validation: 0.2972759530189643]
	TIME [epoch: 8.12 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4560999228670104		[learning rate: 0.0074369]
		[batch 20/20] avg loss: 0.345030809944951		[learning rate: 0.0074234]
	Learning Rate: 0.00742344
	LOSS [training: 0.4005653664059808 | validation: 0.3751250022330746]
	TIME [epoch: 8.15 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43325293160227474		[learning rate: 0.00741]
		[batch 20/20] avg loss: 0.4585354251265513		[learning rate: 0.0073965]
	Learning Rate: 0.0073965
	LOSS [training: 0.44589417836441303 | validation: 0.5390285617196183]
	TIME [epoch: 8.11 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33082822904181153		[learning rate: 0.0073831]
		[batch 20/20] avg loss: 0.4527071387763251		[learning rate: 0.0073697]
	Learning Rate: 0.00736966
	LOSS [training: 0.39176768390906835 | validation: 0.4623929412668626]
	TIME [epoch: 8.12 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4788252080740706		[learning rate: 0.0073563]
		[batch 20/20] avg loss: 0.26639145422503474		[learning rate: 0.0073429]
	Learning Rate: 0.00734291
	LOSS [training: 0.3726083311495526 | validation: 0.5650490442352758]
	TIME [epoch: 8.12 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4726038026318074		[learning rate: 0.0073296]
		[batch 20/20] avg loss: 0.4426061486522662		[learning rate: 0.0073163]
	Learning Rate: 0.00731627
	LOSS [training: 0.4576049756420367 | validation: 0.2823617540601987]
	TIME [epoch: 8.14 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3366490689698244		[learning rate: 0.007303]
		[batch 20/20] avg loss: 0.45264052378317243		[learning rate: 0.0072897]
	Learning Rate: 0.00728971
	LOSS [training: 0.3946447963764984 | validation: 0.39147055976784406]
	TIME [epoch: 8.12 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3513395716334955		[learning rate: 0.0072765]
		[batch 20/20] avg loss: 0.4551343258168261		[learning rate: 0.0072633]
	Learning Rate: 0.00726326
	LOSS [training: 0.4032369487251609 | validation: 0.41749938205389125]
	TIME [epoch: 8.11 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3182114042223011		[learning rate: 0.0072501]
		[batch 20/20] avg loss: 0.40384591282820603		[learning rate: 0.0072369]
	Learning Rate: 0.0072369
	LOSS [training: 0.3610286585252537 | validation: 0.24390275251600568]
	TIME [epoch: 8.12 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32842792507693447		[learning rate: 0.0072238]
		[batch 20/20] avg loss: 0.3412319147788867		[learning rate: 0.0072106]
	Learning Rate: 0.00721064
	LOSS [training: 0.33482991992791056 | validation: 0.3060727856220196]
	TIME [epoch: 8.11 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34291694599873274		[learning rate: 0.0071975]
		[batch 20/20] avg loss: 0.43820175340464695		[learning rate: 0.0071845]
	Learning Rate: 0.00718447
	LOSS [training: 0.3905593497016898 | validation: 0.25593616266877667]
	TIME [epoch: 8.14 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35125913864584307		[learning rate: 0.0071714]
		[batch 20/20] avg loss: 0.4791311070987521		[learning rate: 0.0071584]
	Learning Rate: 0.0071584
	LOSS [training: 0.41519512287229754 | validation: 0.22112199123503787]
	TIME [epoch: 8.11 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35394975060423783		[learning rate: 0.0071454]
		[batch 20/20] avg loss: 0.3257484639080461		[learning rate: 0.0071324]
	Learning Rate: 0.00713242
	LOSS [training: 0.339849107256142 | validation: 0.2236530572867609]
	TIME [epoch: 8.12 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38325090356462177		[learning rate: 0.0071195]
		[batch 20/20] avg loss: 0.3601972433329349		[learning rate: 0.0071065]
	Learning Rate: 0.00710653
	LOSS [training: 0.3717240734487782 | validation: 0.3158561740400998]
	TIME [epoch: 8.11 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.300836685498867		[learning rate: 0.0070936]
		[batch 20/20] avg loss: 0.4667887456232652		[learning rate: 0.0070807]
	Learning Rate: 0.00708074
	LOSS [training: 0.3838127155610661 | validation: 0.26777210248797645]
	TIME [epoch: 8.14 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31042908427156546		[learning rate: 0.0070679]
		[batch 20/20] avg loss: 0.35381551051651083		[learning rate: 0.007055]
	Learning Rate: 0.00705505
	LOSS [training: 0.3321222973940381 | validation: 0.26566874982374117]
	TIME [epoch: 8.12 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3357569543599087		[learning rate: 0.0070422]
		[batch 20/20] avg loss: 0.3219683482025304		[learning rate: 0.0070294]
	Learning Rate: 0.00702945
	LOSS [training: 0.3288626512812195 | validation: 0.21395182486036773]
	TIME [epoch: 8.11 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3332158391940257		[learning rate: 0.0070167]
		[batch 20/20] avg loss: 0.5572866564112869		[learning rate: 0.0070039]
	Learning Rate: 0.00700394
	LOSS [training: 0.4452512478026563 | validation: 0.30696295671124535]
	TIME [epoch: 8.11 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3148866020191715		[learning rate: 0.0069912]
		[batch 20/20] avg loss: 0.33873097169870725		[learning rate: 0.0069785]
	Learning Rate: 0.00697852
	LOSS [training: 0.3268087868589394 | validation: 0.24800877657934517]
	TIME [epoch: 8.12 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26719239106112636		[learning rate: 0.0069658]
		[batch 20/20] avg loss: 0.4122353488058705		[learning rate: 0.0069532]
	Learning Rate: 0.00695319
	LOSS [training: 0.3397138699334984 | validation: 0.2891984712788803]
	TIME [epoch: 8.15 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2923686763714406		[learning rate: 0.0069406]
		[batch 20/20] avg loss: 0.3759125654668726		[learning rate: 0.006928]
	Learning Rate: 0.00692796
	LOSS [training: 0.33414062091915653 | validation: 0.2778833812836897]
	TIME [epoch: 8.12 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30713814585519184		[learning rate: 0.0069154]
		[batch 20/20] avg loss: 0.3267290179160044		[learning rate: 0.0069028]
	Learning Rate: 0.00690282
	LOSS [training: 0.31693358188559806 | validation: 0.23541877465509783]
	TIME [epoch: 8.12 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33074123209832756		[learning rate: 0.0068903]
		[batch 20/20] avg loss: 0.477511456020509		[learning rate: 0.0068778]
	Learning Rate: 0.00687777
	LOSS [training: 0.40412634405941833 | validation: 0.253014613074828]
	TIME [epoch: 8.11 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3304839078577511		[learning rate: 0.0068653]
		[batch 20/20] avg loss: 0.2596070610061208		[learning rate: 0.0068528]
	Learning Rate: 0.00685281
	LOSS [training: 0.29504548443193596 | validation: 0.4715679320839943]
	TIME [epoch: 8.15 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4096839311333003		[learning rate: 0.0068404]
		[batch 20/20] avg loss: 0.39954901391067865		[learning rate: 0.0068279]
	Learning Rate: 0.00682794
	LOSS [training: 0.4046164725219895 | validation: 0.2036829112705729]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2635488524887185		[learning rate: 0.0068155]
		[batch 20/20] avg loss: 0.26961055611299894		[learning rate: 0.0068032]
	Learning Rate: 0.00680316
	LOSS [training: 0.26657970430085876 | validation: 0.22189758572287793]
	TIME [epoch: 8.11 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4316256940988126		[learning rate: 0.0067908]
		[batch 20/20] avg loss: 0.3052873993598468		[learning rate: 0.0067785]
	Learning Rate: 0.00677847
	LOSS [training: 0.36845654672932965 | validation: 0.6931129215361422]
	TIME [epoch: 8.11 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5400241394521043		[learning rate: 0.0067662]
		[batch 20/20] avg loss: 0.36960990761735035		[learning rate: 0.0067539]
	Learning Rate: 0.00675387
	LOSS [training: 0.4548170235347272 | validation: 0.2712463623573776]
	TIME [epoch: 8.12 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38256000459858364		[learning rate: 0.0067416]
		[batch 20/20] avg loss: 0.33220381967532625		[learning rate: 0.0067294]
	Learning Rate: 0.00672936
	LOSS [training: 0.3573819121369549 | validation: 0.1520330222009423]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3677090972397099		[learning rate: 0.0067171]
		[batch 20/20] avg loss: 0.26362101274372896		[learning rate: 0.0067049]
	Learning Rate: 0.00670494
	LOSS [training: 0.31566505499171943 | validation: 0.3100907881681027]
	TIME [epoch: 8.12 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28422065905352134		[learning rate: 0.0066928]
		[batch 20/20] avg loss: 0.3159745611986428		[learning rate: 0.0066806]
	Learning Rate: 0.0066806
	LOSS [training: 0.3000976101260821 | validation: 0.2553801052009094]
	TIME [epoch: 8.13 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3397532387503358		[learning rate: 0.0066685]
		[batch 20/20] avg loss: 0.5233585584191635		[learning rate: 0.0066564]
	Learning Rate: 0.00665636
	LOSS [training: 0.43155589858474963 | validation: 0.6770946351194836]
	TIME [epoch: 8.12 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34149806352621587		[learning rate: 0.0066443]
		[batch 20/20] avg loss: 0.29984727206477746		[learning rate: 0.0066322]
	Learning Rate: 0.0066322
	LOSS [training: 0.32067266779549675 | validation: 0.22955027868750044]
	TIME [epoch: 8.14 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34252085562784595		[learning rate: 0.0066202]
		[batch 20/20] avg loss: 0.3227850746467227		[learning rate: 0.0066081]
	Learning Rate: 0.00660814
	LOSS [training: 0.3326529651372844 | validation: 0.30277898878641857]
	TIME [epoch: 8.12 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.523178223068321		[learning rate: 0.0065961]
		[batch 20/20] avg loss: 0.3634428072984673		[learning rate: 0.0065842]
	Learning Rate: 0.00658415
	LOSS [training: 0.44331051518339404 | validation: 0.21371032370294374]
	TIME [epoch: 8.12 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2899201690260017		[learning rate: 0.0065722]
		[batch 20/20] avg loss: 0.3486198359017167		[learning rate: 0.0065603]
	Learning Rate: 0.00656026
	LOSS [training: 0.31927000246385917 | validation: 0.43161649378432493]
	TIME [epoch: 8.11 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35716594641697397		[learning rate: 0.0065483]
		[batch 20/20] avg loss: 0.2854321297099939		[learning rate: 0.0065365]
	Learning Rate: 0.00653645
	LOSS [training: 0.3212990380634839 | validation: 0.5968796672873967]
	TIME [epoch: 8.14 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30171440506091446		[learning rate: 0.0065246]
		[batch 20/20] avg loss: 0.3059948848385542		[learning rate: 0.0065127]
	Learning Rate: 0.00651273
	LOSS [training: 0.3038546449497343 | validation: 0.2627177476319588]
	TIME [epoch: 8.13 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3098795468431527		[learning rate: 0.0065009]
		[batch 20/20] avg loss: 0.2693576734869652		[learning rate: 0.0064891]
	Learning Rate: 0.0064891
	LOSS [training: 0.2896186101650589 | validation: 0.3718989541146084]
	TIME [epoch: 8.12 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3305422482119338		[learning rate: 0.0064773]
		[batch 20/20] avg loss: 0.3290761847535485		[learning rate: 0.0064655]
	Learning Rate: 0.00646555
	LOSS [training: 0.3298092164827411 | validation: 0.30730107452319183]
	TIME [epoch: 8.13 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3279583765905804		[learning rate: 0.0064538]
		[batch 20/20] avg loss: 0.266910985572852		[learning rate: 0.0064421]
	Learning Rate: 0.00644208
	LOSS [training: 0.29743468108171617 | validation: 0.39772260862952336]
	TIME [epoch: 8.12 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28637270657319525		[learning rate: 0.0064304]
		[batch 20/20] avg loss: 0.27973593267924346		[learning rate: 0.0064187]
	Learning Rate: 0.0064187
	LOSS [training: 0.2830543196262193 | validation: 0.19214934542132694]
	TIME [epoch: 8.15 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31869213346837266		[learning rate: 0.006407]
		[batch 20/20] avg loss: 0.3487544303882991		[learning rate: 0.0063954]
	Learning Rate: 0.00639541
	LOSS [training: 0.3337232819283359 | validation: 0.7958067747369179]
	TIME [epoch: 8.12 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39462929126522056		[learning rate: 0.0063838]
		[batch 20/20] avg loss: 0.2879733403688157		[learning rate: 0.0063722]
	Learning Rate: 0.0063722
	LOSS [training: 0.3413013158170181 | validation: 0.15198612747970985]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30262582265758986		[learning rate: 0.0063606]
		[batch 20/20] avg loss: 0.3218620036380687		[learning rate: 0.0063491]
	Learning Rate: 0.00634908
	LOSS [training: 0.3122439131478293 | validation: 0.2655167070565632]
	TIME [epoch: 8.14 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3011303931546395		[learning rate: 0.0063375]
		[batch 20/20] avg loss: 0.3353591763731287		[learning rate: 0.006326]
	Learning Rate: 0.00632603
	LOSS [training: 0.31824478476388407 | validation: 0.23712320258314518]
	TIME [epoch: 8.16 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29317409335318506		[learning rate: 0.0063145]
		[batch 20/20] avg loss: 0.3626829685360511		[learning rate: 0.0063031]
	Learning Rate: 0.00630308
	LOSS [training: 0.32792853094461805 | validation: 0.22435747943319356]
	TIME [epoch: 8.14 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35185594442616985		[learning rate: 0.0062916]
		[batch 20/20] avg loss: 0.37071139314353924		[learning rate: 0.0062802]
	Learning Rate: 0.0062802
	LOSS [training: 0.3612836687848545 | validation: 0.5199585766951966]
	TIME [epoch: 8.13 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29216632220282973		[learning rate: 0.0062688]
		[batch 20/20] avg loss: 0.36791554582056596		[learning rate: 0.0062574]
	Learning Rate: 0.00625741
	LOSS [training: 0.33004093401169776 | validation: 0.2591938933579904]
	TIME [epoch: 8.13 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2537611627282222		[learning rate: 0.006246]
		[batch 20/20] avg loss: 0.36394727230425683		[learning rate: 0.0062347]
	Learning Rate: 0.0062347
	LOSS [training: 0.30885421751623954 | validation: 0.3540386506890413]
	TIME [epoch: 8.14 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3166830044877334		[learning rate: 0.0062234]
		[batch 20/20] avg loss: 0.31258589525232455		[learning rate: 0.0062121]
	Learning Rate: 0.00621208
	LOSS [training: 0.31463444987002903 | validation: 0.2930750986641425]
	TIME [epoch: 8.16 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22717200747625296		[learning rate: 0.0062008]
		[batch 20/20] avg loss: 0.2874073045033103		[learning rate: 0.0061895]
	Learning Rate: 0.00618953
	LOSS [training: 0.25728965598978165 | validation: 0.3019560054285742]
	TIME [epoch: 8.13 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31350327619001817		[learning rate: 0.0061783]
		[batch 20/20] avg loss: 0.37069900750479773		[learning rate: 0.0061671]
	Learning Rate: 0.00616707
	LOSS [training: 0.3421011418474079 | validation: 0.3935191397114496]
	TIME [epoch: 8.14 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3379456261885518		[learning rate: 0.0061559]
		[batch 20/20] avg loss: 0.26406170777851967		[learning rate: 0.0061447]
	Learning Rate: 0.00614469
	LOSS [training: 0.30100366698353576 | validation: 0.23538552988323824]
	TIME [epoch: 8.13 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2584797008381111		[learning rate: 0.0061335]
		[batch 20/20] avg loss: 0.2888101757850504		[learning rate: 0.0061224]
	Learning Rate: 0.00612239
	LOSS [training: 0.2736449383115807 | validation: 0.14672108425793914]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2830659689045293		[learning rate: 0.0061113]
		[batch 20/20] avg loss: 0.34031329890840684		[learning rate: 0.0061002]
	Learning Rate: 0.00610017
	LOSS [training: 0.31168963390646814 | validation: 0.16103298396205376]
	TIME [epoch: 8.15 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35147646881115185		[learning rate: 0.0060891]
		[batch 20/20] avg loss: 0.31722027942986913		[learning rate: 0.006078]
	Learning Rate: 0.00607803
	LOSS [training: 0.33434837412051055 | validation: 0.14915565917137616]
	TIME [epoch: 8.14 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2605189768837038		[learning rate: 0.006067]
		[batch 20/20] avg loss: 0.26177209888582864		[learning rate: 0.006056]
	Learning Rate: 0.00605598
	LOSS [training: 0.2611455378847662 | validation: 0.44128922584515695]
	TIME [epoch: 8.14 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3206812850724208		[learning rate: 0.006045]
		[batch 20/20] avg loss: 0.34321639069900706		[learning rate: 0.006034]
	Learning Rate: 0.006034
	LOSS [training: 0.33194883788571394 | validation: 0.30216971509541596]
	TIME [epoch: 8.14 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35175442734944995		[learning rate: 0.006023]
		[batch 20/20] avg loss: 0.30299680910650345		[learning rate: 0.0060121]
	Learning Rate: 0.0060121
	LOSS [training: 0.3273756182279767 | validation: 0.28169879055296554]
	TIME [epoch: 8.15 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26625029718022564		[learning rate: 0.0060012]
		[batch 20/20] avg loss: 0.26830757159780055		[learning rate: 0.0059903]
	Learning Rate: 0.00599028
	LOSS [training: 0.2672789343890132 | validation: 0.15447141758367425]
	TIME [epoch: 8.14 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2719083525489193		[learning rate: 0.0059794]
		[batch 20/20] avg loss: 0.29598885289853405		[learning rate: 0.0059685]
	Learning Rate: 0.00596854
	LOSS [training: 0.28394860272372663 | validation: 0.3060053388352498]
	TIME [epoch: 8.13 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36661924036537663		[learning rate: 0.0059577]
		[batch 20/20] avg loss: 0.28300103960559453		[learning rate: 0.0059469]
	Learning Rate: 0.00594688
	LOSS [training: 0.3248101399854856 | validation: 0.3323071152610417]
	TIME [epoch: 8.13 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26078040799602736		[learning rate: 0.0059361]
		[batch 20/20] avg loss: 0.2401477350964026		[learning rate: 0.0059253]
	Learning Rate: 0.0059253
	LOSS [training: 0.25046407154621503 | validation: 0.2592011739808827]
	TIME [epoch: 8.16 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26762453427547717		[learning rate: 0.0059145]
		[batch 20/20] avg loss: 0.4106200724823975		[learning rate: 0.0059038]
	Learning Rate: 0.0059038
	LOSS [training: 0.33912230337893734 | validation: 0.26577177460116586]
	TIME [epoch: 8.14 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27387668651179947		[learning rate: 0.0058931]
		[batch 20/20] avg loss: 0.32183193813293026		[learning rate: 0.0058824]
	Learning Rate: 0.00588237
	LOSS [training: 0.2978543123223648 | validation: 0.31792449515453525]
	TIME [epoch: 8.14 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35562918627339746		[learning rate: 0.0058717]
		[batch 20/20] avg loss: 0.2249444367094744		[learning rate: 0.005861]
	Learning Rate: 0.00586103
	LOSS [training: 0.29028681149143587 | validation: 0.1591857988311245]
	TIME [epoch: 8.13 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33265124630162374		[learning rate: 0.0058504]
		[batch 20/20] avg loss: 0.2939839570654025		[learning rate: 0.0058398]
	Learning Rate: 0.00583976
	LOSS [training: 0.3133176016835131 | validation: 0.16429020253990684]
	TIME [epoch: 8.16 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2448864675353767		[learning rate: 0.0058291]
		[batch 20/20] avg loss: 0.24669442427828203		[learning rate: 0.0058186]
	Learning Rate: 0.00581856
	LOSS [training: 0.2457904459068294 | validation: 0.2978786436075502]
	TIME [epoch: 8.15 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3024394932743011		[learning rate: 0.005808]
		[batch 20/20] avg loss: 0.42122797254479716		[learning rate: 0.0057974]
	Learning Rate: 0.00579745
	LOSS [training: 0.3618337329095491 | validation: 0.13761232427253295]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20932583484390302		[learning rate: 0.0057869]
		[batch 20/20] avg loss: 0.4365534802508394		[learning rate: 0.0057764]
	Learning Rate: 0.00577641
	LOSS [training: 0.3229396575473712 | validation: 0.1568198590708769]
	TIME [epoch: 8.13 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2812462877623214		[learning rate: 0.0057659]
		[batch 20/20] avg loss: 0.2536024351287292		[learning rate: 0.0057554]
	Learning Rate: 0.00575545
	LOSS [training: 0.2674243614455254 | validation: 0.1826139539006808]
	TIME [epoch: 8.13 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30097972610983803		[learning rate: 0.005745]
		[batch 20/20] avg loss: 0.2666082089843367		[learning rate: 0.0057346]
	Learning Rate: 0.00573456
	LOSS [training: 0.2837939675470873 | validation: 0.3123290014959972]
	TIME [epoch: 8.16 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24625506521270596		[learning rate: 0.0057241]
		[batch 20/20] avg loss: 0.2826714108518132		[learning rate: 0.0057137]
	Learning Rate: 0.00571375
	LOSS [training: 0.26446323803225963 | validation: 0.1503705874274924]
	TIME [epoch: 8.13 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3674076746373294		[learning rate: 0.0057034]
		[batch 20/20] avg loss: 0.2523759735879925		[learning rate: 0.005693]
	Learning Rate: 0.00569301
	LOSS [training: 0.3098918241126609 | validation: 0.36590432686779306]
	TIME [epoch: 8.13 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2789564513289621		[learning rate: 0.0056827]
		[batch 20/20] avg loss: 0.2720727825953535		[learning rate: 0.0056724]
	Learning Rate: 0.00567235
	LOSS [training: 0.2755146169621578 | validation: 0.749822156843834]
	TIME [epoch: 8.13 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3025001797284027		[learning rate: 0.005662]
		[batch 20/20] avg loss: 0.2509474505809063		[learning rate: 0.0056518]
	Learning Rate: 0.00565177
	LOSS [training: 0.27672381515465455 | validation: 0.2584574991078712]
	TIME [epoch: 8.15 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26939107585405575		[learning rate: 0.0056415]
		[batch 20/20] avg loss: 0.22888474634600878		[learning rate: 0.0056313]
	Learning Rate: 0.00563126
	LOSS [training: 0.24913791110003225 | validation: 0.14006894944697837]
	TIME [epoch: 8.14 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23736084924148457		[learning rate: 0.005621]
		[batch 20/20] avg loss: 0.22885381132629737		[learning rate: 0.0056108]
	Learning Rate: 0.00561082
	LOSS [training: 0.23310733028389094 | validation: 0.1593223110622737]
	TIME [epoch: 8.13 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22747745430551927		[learning rate: 0.0056006]
		[batch 20/20] avg loss: 0.2094149013409813		[learning rate: 0.0055905]
	Learning Rate: 0.00559046
	LOSS [training: 0.21844617782325026 | validation: 0.19556799122243906]
	TIME [epoch: 8.13 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20928347747914583		[learning rate: 0.0055803]
		[batch 20/20] avg loss: 0.3078814164157554		[learning rate: 0.0055702]
	Learning Rate: 0.00557017
	LOSS [training: 0.25858244694745064 | validation: 0.4665284775533943]
	TIME [epoch: 8.14 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25897141479463304		[learning rate: 0.0055601]
		[batch 20/20] avg loss: 0.19811558314056166		[learning rate: 0.00555]
	Learning Rate: 0.00554996
	LOSS [training: 0.22854349896759735 | validation: 0.18179971202944656]
	TIME [epoch: 8.16 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26453291601291495		[learning rate: 0.0055399]
		[batch 20/20] avg loss: 0.3430537717708869		[learning rate: 0.0055298]
	Learning Rate: 0.00552981
	LOSS [training: 0.3037933438919009 | validation: 0.10575474392045997]
	TIME [epoch: 8.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24346792461319494		[learning rate: 0.0055198]
		[batch 20/20] avg loss: 0.18981410300383555		[learning rate: 0.0055097]
	Learning Rate: 0.00550975
	LOSS [training: 0.21664101380851525 | validation: 0.3228946328236133]
	TIME [epoch: 8.12 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23706121292626733		[learning rate: 0.0054997]
		[batch 20/20] avg loss: 0.23203819030109335		[learning rate: 0.0054898]
	Learning Rate: 0.00548975
	LOSS [training: 0.23454970161368038 | validation: 0.20089991592125708]
	TIME [epoch: 8.12 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20701585065816283		[learning rate: 0.0054798]
		[batch 20/20] avg loss: 0.19868222063378405		[learning rate: 0.0054698]
	Learning Rate: 0.00546983
	LOSS [training: 0.2028490356459734 | validation: 0.14394347138561492]
	TIME [epoch: 8.15 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19354976533931917		[learning rate: 0.0054599]
		[batch 20/20] avg loss: 0.2595741519299458		[learning rate: 0.00545]
	Learning Rate: 0.00544998
	LOSS [training: 0.22656195863463244 | validation: 0.17689234789297764]
	TIME [epoch: 8.12 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19806739651237415		[learning rate: 0.0054401]
		[batch 20/20] avg loss: 0.2660892521287365		[learning rate: 0.0054302]
	Learning Rate: 0.0054302
	LOSS [training: 0.2320783243205553 | validation: 0.43937223778871587]
	TIME [epoch: 8.12 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3052188885880843		[learning rate: 0.0054203]
		[batch 20/20] avg loss: 0.30188180290692557		[learning rate: 0.0054105]
	Learning Rate: 0.00541049
	LOSS [training: 0.30355034574750495 | validation: 0.18247204572248793]
	TIME [epoch: 8.11 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.334604809434193		[learning rate: 0.0054007]
		[batch 20/20] avg loss: 0.262930789123795		[learning rate: 0.0053909]
	Learning Rate: 0.00539086
	LOSS [training: 0.298767799278994 | validation: 0.2926811041128536]
	TIME [epoch: 8.12 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2719418357042367		[learning rate: 0.0053811]
		[batch 20/20] avg loss: 0.2471483406924174		[learning rate: 0.0053713]
	Learning Rate: 0.00537129
	LOSS [training: 0.2595450881983271 | validation: 0.15345388000776072]
	TIME [epoch: 8.15 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2766762431068926		[learning rate: 0.0053615]
		[batch 20/20] avg loss: 0.1805559496702061		[learning rate: 0.0053518]
	Learning Rate: 0.0053518
	LOSS [training: 0.2286160963885493 | validation: 0.134800880028156]
	TIME [epoch: 8.12 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2480472892370802		[learning rate: 0.0053421]
		[batch 20/20] avg loss: 0.6763220441041932		[learning rate: 0.0053324]
	Learning Rate: 0.00533238
	LOSS [training: 0.4621846666706368 | validation: 0.21647749897364488]
	TIME [epoch: 8.11 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25607355072156424		[learning rate: 0.0053227]
		[batch 20/20] avg loss: 0.24474766477357313		[learning rate: 0.005313]
	Learning Rate: 0.00531303
	LOSS [training: 0.2504106077475686 | validation: 0.1644994674005103]
	TIME [epoch: 8.12 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24329169531270917		[learning rate: 0.0053034]
		[batch 20/20] avg loss: 0.23432336122144765		[learning rate: 0.0052937]
	Learning Rate: 0.00529375
	LOSS [training: 0.23880752826707843 | validation: 0.15083428950827105]
	TIME [epoch: 8.15 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2115291456210981		[learning rate: 0.0052841]
		[batch 20/20] avg loss: 0.19105274042635342		[learning rate: 0.0052745]
	Learning Rate: 0.00527454
	LOSS [training: 0.2012909430237258 | validation: 0.15288414625260027]
	TIME [epoch: 8.12 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21780409511623175		[learning rate: 0.005265]
		[batch 20/20] avg loss: 0.2919128051819086		[learning rate: 0.0052554]
	Learning Rate: 0.00525539
	LOSS [training: 0.25485845014907016 | validation: 0.15111942636408626]
	TIME [epoch: 8.12 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22692543061356735		[learning rate: 0.0052458]
		[batch 20/20] avg loss: 0.2654851836843743		[learning rate: 0.0052363]
	Learning Rate: 0.00523632
	LOSS [training: 0.24620530714897085 | validation: 0.19539705510750913]
	TIME [epoch: 8.12 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17348114992905625		[learning rate: 0.0052268]
		[batch 20/20] avg loss: 0.2941564623307322		[learning rate: 0.0052173]
	Learning Rate: 0.00521732
	LOSS [training: 0.23381880612989425 | validation: 0.1653144604825282]
	TIME [epoch: 8.13 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6284229075603376		[learning rate: 0.0052078]
		[batch 20/20] avg loss: 0.20283417274962953		[learning rate: 0.0051984]
	Learning Rate: 0.00519839
	LOSS [training: 0.4156285401549836 | validation: 0.08511929892044033]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2604424066249055		[learning rate: 0.0051889]
		[batch 20/20] avg loss: 0.22712555808091048		[learning rate: 0.0051795]
	Learning Rate: 0.00517952
	LOSS [training: 0.243783982352908 | validation: 0.2363837539702209]
	TIME [epoch: 8.11 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16406211925371098		[learning rate: 0.0051701]
		[batch 20/20] avg loss: 0.18811504724843156		[learning rate: 0.0051607]
	Learning Rate: 0.00516072
	LOSS [training: 0.1760885832510712 | validation: 0.20624890293752027]
	TIME [epoch: 8.11 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21515738218711578		[learning rate: 0.0051513]
		[batch 20/20] avg loss: 0.20453883455370817		[learning rate: 0.005142]
	Learning Rate: 0.00514199
	LOSS [training: 0.209848108370412 | validation: 0.07815164566655636]
	TIME [epoch: 8.11 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20698578393953646		[learning rate: 0.0051327]
		[batch 20/20] avg loss: 0.27254499858156217		[learning rate: 0.0051233]
	Learning Rate: 0.00512333
	LOSS [training: 0.2397653912605493 | validation: 0.5187027683961163]
	TIME [epoch: 8.15 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.279603354641799		[learning rate: 0.005114]
		[batch 20/20] avg loss: 0.2269151115529379		[learning rate: 0.0051047]
	Learning Rate: 0.00510474
	LOSS [training: 0.2532592330973684 | validation: 0.3566390600408227]
	TIME [epoch: 8.12 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17643913072093081		[learning rate: 0.0050955]
		[batch 20/20] avg loss: 0.23720777348995875		[learning rate: 0.0050862]
	Learning Rate: 0.00508622
	LOSS [training: 0.20682345210544478 | validation: 0.12472535189273673]
	TIME [epoch: 8.11 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2217104407454805		[learning rate: 0.005077]
		[batch 20/20] avg loss: 0.21708141609190754		[learning rate: 0.0050678]
	Learning Rate: 0.00506776
	LOSS [training: 0.21939592841869401 | validation: 0.31824616461894084]
	TIME [epoch: 8.11 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27283501574765956		[learning rate: 0.0050586]
		[batch 20/20] avg loss: 0.2038389247109246		[learning rate: 0.0050494]
	Learning Rate: 0.00504937
	LOSS [training: 0.23833697022929207 | validation: 0.18206634141506922]
	TIME [epoch: 8.13 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17321322641608128		[learning rate: 0.0050402]
		[batch 20/20] avg loss: 0.20025860514766344		[learning rate: 0.005031]
	Learning Rate: 0.00503104
	LOSS [training: 0.18673591578187237 | validation: 0.19825644619125693]
	TIME [epoch: 8.12 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32792832889339185		[learning rate: 0.0050219]
		[batch 20/20] avg loss: 0.18170113447127562		[learning rate: 0.0050128]
	Learning Rate: 0.00501278
	LOSS [training: 0.2548147316823337 | validation: 0.11660012108021664]
	TIME [epoch: 8.12 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16021157967064856		[learning rate: 0.0050037]
		[batch 20/20] avg loss: 0.21061466979211904		[learning rate: 0.0049946]
	Learning Rate: 0.00499459
	LOSS [training: 0.18541312473138383 | validation: 0.19667447168869687]
	TIME [epoch: 8.11 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20887560071045538		[learning rate: 0.0049855]
		[batch 20/20] avg loss: 0.26418695386129676		[learning rate: 0.0049765]
	Learning Rate: 0.00497647
	LOSS [training: 0.23653127728587608 | validation: 0.06115036297111498]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19946244719327683		[learning rate: 0.0049674]
		[batch 20/20] avg loss: 0.19040053619483888		[learning rate: 0.0049584]
	Learning Rate: 0.00495841
	LOSS [training: 0.19493149169405788 | validation: 0.0821301512618045]
	TIME [epoch: 8.14 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17705273201918428		[learning rate: 0.0049494]
		[batch 20/20] avg loss: 0.23722159391188882		[learning rate: 0.0049404]
	Learning Rate: 0.00494041
	LOSS [training: 0.20713716296553658 | validation: 0.21074677714595316]
	TIME [epoch: 8.11 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19927800861564882		[learning rate: 0.0049314]
		[batch 20/20] avg loss: 0.5523823245490729		[learning rate: 0.0049225]
	Learning Rate: 0.00492248
	LOSS [training: 0.3758301665823608 | validation: 0.1861758538754254]
	TIME [epoch: 8.11 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15870036911437327		[learning rate: 0.0049135]
		[batch 20/20] avg loss: 0.2302655879661369		[learning rate: 0.0049046]
	Learning Rate: 0.00490462
	LOSS [training: 0.1944829785402551 | validation: 0.18438716484061923]
	TIME [epoch: 8.11 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16603192582198723		[learning rate: 0.0048957]
		[batch 20/20] avg loss: 0.23944495023629142		[learning rate: 0.0048868]
	Learning Rate: 0.00488682
	LOSS [training: 0.20273843802913932 | validation: 0.23608422162111986]
	TIME [epoch: 8.13 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21956798149025758		[learning rate: 0.0048779]
		[batch 20/20] avg loss: 0.17529722838555256		[learning rate: 0.0048691]
	Learning Rate: 0.00486909
	LOSS [training: 0.19743260493790504 | validation: 0.4354859211314346]
	TIME [epoch: 8.12 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18216664642307973		[learning rate: 0.0048602]
		[batch 20/20] avg loss: 0.23075564653260386		[learning rate: 0.0048514]
	Learning Rate: 0.00485141
	LOSS [training: 0.20646114647784178 | validation: 0.1417738207736707]
	TIME [epoch: 8.12 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2886186792421438		[learning rate: 0.0048426]
		[batch 20/20] avg loss: 0.1580209382665546		[learning rate: 0.0048338]
	Learning Rate: 0.00483381
	LOSS [training: 0.22331980875434923 | validation: 0.2757673393280385]
	TIME [epoch: 8.11 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24022857639424156		[learning rate: 0.004825]
		[batch 20/20] avg loss: 0.24164717422428827		[learning rate: 0.0048163]
	Learning Rate: 0.00481627
	LOSS [training: 0.24093787530926494 | validation: 0.13391463230642808]
	TIME [epoch: 8.12 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1882234289362971		[learning rate: 0.0048075]
		[batch 20/20] avg loss: 0.3240773304925715		[learning rate: 0.0047988]
	Learning Rate: 0.00479879
	LOSS [training: 0.25615037971443433 | validation: 0.14488598136666533]
	TIME [epoch: 8.15 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20937470408070324		[learning rate: 0.0047901]
		[batch 20/20] avg loss: 0.1734676092372836		[learning rate: 0.0047814]
	Learning Rate: 0.00478137
	LOSS [training: 0.19142115665899345 | validation: 0.22580184780884116]
	TIME [epoch: 8.12 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19192648301117515		[learning rate: 0.0047727]
		[batch 20/20] avg loss: 0.41392073078818814		[learning rate: 0.004764]
	Learning Rate: 0.00476402
	LOSS [training: 0.3029236068996817 | validation: 0.3924557700000103]
	TIME [epoch: 8.11 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2718925351625645		[learning rate: 0.0047554]
		[batch 20/20] avg loss: 0.16259031825929823		[learning rate: 0.0047467]
	Learning Rate: 0.00474673
	LOSS [training: 0.21724142671093136 | validation: 0.4811846474470013]
	TIME [epoch: 8.11 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16694721548414437		[learning rate: 0.0047381]
		[batch 20/20] avg loss: 0.2780728089954505		[learning rate: 0.0047295]
	Learning Rate: 0.00472951
	LOSS [training: 0.22251001223979744 | validation: 0.3522276832869276]
	TIME [epoch: 8.14 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27724304413962125		[learning rate: 0.0047209]
		[batch 20/20] avg loss: 0.22712993929689135		[learning rate: 0.0047123]
	Learning Rate: 0.00471234
	LOSS [training: 0.25218649171825624 | validation: 0.2456797470620923]
	TIME [epoch: 8.12 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3209345340807525		[learning rate: 0.0047038]
		[batch 20/20] avg loss: 0.20575301571629406		[learning rate: 0.0046952]
	Learning Rate: 0.00469524
	LOSS [training: 0.2633437748985232 | validation: 0.16745277880358753]
	TIME [epoch: 8.13 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1487145937676433		[learning rate: 0.0046867]
		[batch 20/20] avg loss: 0.2041321804372201		[learning rate: 0.0046782]
	Learning Rate: 0.0046782
	LOSS [training: 0.17642338710243163 | validation: 0.12782652008520098]
	TIME [epoch: 8.12 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15941053358586119		[learning rate: 0.0046697]
		[batch 20/20] avg loss: 0.1556016745620032		[learning rate: 0.0046612]
	Learning Rate: 0.00466122
	LOSS [training: 0.1575061040739322 | validation: 0.23281151355107615]
	TIME [epoch: 8.12 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25376361793763513		[learning rate: 0.0046528]
		[batch 20/20] avg loss: 0.19041357333795855		[learning rate: 0.0046443]
	Learning Rate: 0.00464431
	LOSS [training: 0.22208859563779684 | validation: 0.13655922454374567]
	TIME [epoch: 8.14 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1694898721111084		[learning rate: 0.0046359]
		[batch 20/20] avg loss: 0.22337335110773301		[learning rate: 0.0046275]
	Learning Rate: 0.00462745
	LOSS [training: 0.19643161160942071 | validation: 0.1749301560277713]
	TIME [epoch: 8.12 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22633411853836427		[learning rate: 0.004619]
		[batch 20/20] avg loss: 0.22236064911034967		[learning rate: 0.0046107]
	Learning Rate: 0.00461066
	LOSS [training: 0.22434738382435696 | validation: 0.31021276149652466]
	TIME [epoch: 8.11 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19667405189847856		[learning rate: 0.0046023]
		[batch 20/20] avg loss: 0.14242028322697736		[learning rate: 0.0045939]
	Learning Rate: 0.00459393
	LOSS [training: 0.16954716756272795 | validation: 0.3714221349503897]
	TIME [epoch: 8.11 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28540762174088047		[learning rate: 0.0045856]
		[batch 20/20] avg loss: 0.2766718624177716		[learning rate: 0.0045773]
	Learning Rate: 0.00457726
	LOSS [training: 0.281039742079326 | validation: 0.20391632698232953]
	TIME [epoch: 8.14 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1619932458141105		[learning rate: 0.0045689]
		[batch 20/20] avg loss: 0.17759354652053877		[learning rate: 0.0045606]
	Learning Rate: 0.00456065
	LOSS [training: 0.16979339616732464 | validation: 0.12975989531423907]
	TIME [epoch: 8.12 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1780815121336983		[learning rate: 0.0045524]
		[batch 20/20] avg loss: 0.23357570825151072		[learning rate: 0.0045441]
	Learning Rate: 0.00454409
	LOSS [training: 0.20582861019260448 | validation: 0.2782066725101463]
	TIME [epoch: 8.12 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23265130749395765		[learning rate: 0.0045358]
		[batch 20/20] avg loss: 0.18313185647948263		[learning rate: 0.0045276]
	Learning Rate: 0.0045276
	LOSS [training: 0.20789158198672011 | validation: 0.16503423049585286]
	TIME [epoch: 8.12 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20357587867127544		[learning rate: 0.0045194]
		[batch 20/20] avg loss: 0.22912445000895962		[learning rate: 0.0045112]
	Learning Rate: 0.00451117
	LOSS [training: 0.21635016434011747 | validation: 0.3664687784994375]
	TIME [epoch: 8.12 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19072069297042232		[learning rate: 0.004503]
		[batch 20/20] avg loss: 0.2108036180540723		[learning rate: 0.0044948]
	Learning Rate: 0.0044948
	LOSS [training: 0.20076215551224727 | validation: 0.21596864358195106]
	TIME [epoch: 8.15 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3144069204756802		[learning rate: 0.0044866]
		[batch 20/20] avg loss: 0.16653935947692472		[learning rate: 0.0044785]
	Learning Rate: 0.00447849
	LOSS [training: 0.24047313997630254 | validation: 0.14848928022888497]
	TIME [epoch: 8.12 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24234168419108576		[learning rate: 0.0044704]
		[batch 20/20] avg loss: 0.23442507596983314		[learning rate: 0.0044622]
	Learning Rate: 0.00446224
	LOSS [training: 0.23838338008045948 | validation: 0.0952707898002225]
	TIME [epoch: 8.13 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2955764983434862		[learning rate: 0.0044541]
		[batch 20/20] avg loss: 0.19848731545167467		[learning rate: 0.004446]
	Learning Rate: 0.00444604
	LOSS [training: 0.24703190689758045 | validation: 0.13507620415346605]
	TIME [epoch: 8.12 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17229816148204674		[learning rate: 0.004438]
		[batch 20/20] avg loss: 0.20245128002790547		[learning rate: 0.0044299]
	Learning Rate: 0.00442991
	LOSS [training: 0.18737472075497613 | validation: 0.10821937781451194]
	TIME [epoch: 8.14 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1392803707148044		[learning rate: 0.0044219]
		[batch 20/20] avg loss: 0.1691437790341294		[learning rate: 0.0044138]
	Learning Rate: 0.00441383
	LOSS [training: 0.15421207487446686 | validation: 0.18605795445422252]
	TIME [epoch: 8.13 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2226089785346849		[learning rate: 0.0044058]
		[batch 20/20] avg loss: 0.2360821475693245		[learning rate: 0.0043978]
	Learning Rate: 0.00439781
	LOSS [training: 0.22934556305200465 | validation: 0.10242360631487393]
	TIME [epoch: 8.13 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17466958196843504		[learning rate: 0.0043898]
		[batch 20/20] avg loss: 0.20128358458156623		[learning rate: 0.0043819]
	Learning Rate: 0.00438185
	LOSS [training: 0.18797658327500066 | validation: 0.2588785409107717]
	TIME [epoch: 8.12 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20092684988026255		[learning rate: 0.0043739]
		[batch 20/20] avg loss: 0.21742913866683736		[learning rate: 0.004366]
	Learning Rate: 0.00436595
	LOSS [training: 0.20917799427354988 | validation: 0.11504196464541291]
	TIME [epoch: 8.14 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19822954548831895		[learning rate: 0.004358]
		[batch 20/20] avg loss: 0.1907159864499486		[learning rate: 0.0043501]
	Learning Rate: 0.00435011
	LOSS [training: 0.19447276596913377 | validation: 0.2301668115254299]
	TIME [epoch: 8.15 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23792758279774434		[learning rate: 0.0043422]
		[batch 20/20] avg loss: 0.2610950024835831		[learning rate: 0.0043343]
	Learning Rate: 0.00433432
	LOSS [training: 0.24951129264066368 | validation: 0.1132737611429242]
	TIME [epoch: 8.12 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14911185999277357		[learning rate: 0.0043264]
		[batch 20/20] avg loss: 0.21748660561187266		[learning rate: 0.0043186]
	Learning Rate: 0.00431859
	LOSS [training: 0.18329923280232313 | validation: 0.09057323377653559]
	TIME [epoch: 8.12 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25383435299034934		[learning rate: 0.0043107]
		[batch 20/20] avg loss: 0.19911247933808296		[learning rate: 0.0043029]
	Learning Rate: 0.00430292
	LOSS [training: 0.22647341616421618 | validation: 0.14520380403485805]
	TIME [epoch: 8.12 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.164668538782985		[learning rate: 0.0042951]
		[batch 20/20] avg loss: 0.21864080505277408		[learning rate: 0.0042873]
	Learning Rate: 0.0042873
	LOSS [training: 0.19165467191787955 | validation: 0.20183721238501268]
	TIME [epoch: 8.15 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1991643586560519		[learning rate: 0.0042795]
		[batch 20/20] avg loss: 0.2133762794608085		[learning rate: 0.0042717]
	Learning Rate: 0.00427174
	LOSS [training: 0.2062703190584302 | validation: 0.14189915500144534]
	TIME [epoch: 8.12 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18819073989036522		[learning rate: 0.004264]
		[batch 20/20] avg loss: 0.208873120758593		[learning rate: 0.0042562]
	Learning Rate: 0.00425624
	LOSS [training: 0.19853193032447908 | validation: 0.19351086866663586]
	TIME [epoch: 8.12 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20342868084301044		[learning rate: 0.0042485]
		[batch 20/20] avg loss: 0.22603877987769794		[learning rate: 0.0042408]
	Learning Rate: 0.0042408
	LOSS [training: 0.2147337303603542 | validation: 0.22047749380246007]
	TIME [epoch: 8.12 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1906564832979413		[learning rate: 0.0042331]
		[batch 20/20] avg loss: 0.1891710833482741		[learning rate: 0.0042254]
	Learning Rate: 0.00422541
	LOSS [training: 0.18991378332310765 | validation: 0.4553216981810637]
	TIME [epoch: 8.14 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28964947119730855		[learning rate: 0.0042177]
		[batch 20/20] avg loss: 0.28765738126839224		[learning rate: 0.0042101]
	Learning Rate: 0.00421007
	LOSS [training: 0.28865342623285034 | validation: 0.3347297766048839]
	TIME [epoch: 8.13 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16851977751091815		[learning rate: 0.0042024]
		[batch 20/20] avg loss: 0.20975762319895272		[learning rate: 0.0041948]
	Learning Rate: 0.00419479
	LOSS [training: 0.18913870035493544 | validation: 0.42262587202063406]
	TIME [epoch: 8.12 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21657553369774654		[learning rate: 0.0041872]
		[batch 20/20] avg loss: 0.21061775129847513		[learning rate: 0.0041796]
	Learning Rate: 0.00417957
	LOSS [training: 0.21359664249811083 | validation: 0.10570838174680897]
	TIME [epoch: 8.12 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26918976328995514		[learning rate: 0.004172]
		[batch 20/20] avg loss: 0.19553604477676725		[learning rate: 0.0041644]
	Learning Rate: 0.0041644
	LOSS [training: 0.23236290403336116 | validation: 0.08104818488555542]
	TIME [epoch: 8.13 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1774597066156181		[learning rate: 0.0041568]
		[batch 20/20] avg loss: 0.18228558363305009		[learning rate: 0.0041493]
	Learning Rate: 0.00414929
	LOSS [training: 0.17987264512433407 | validation: 0.18672587921336498]
	TIME [epoch: 8.14 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24467698910587626		[learning rate: 0.0041418]
		[batch 20/20] avg loss: 0.22965850387787495		[learning rate: 0.0041342]
	Learning Rate: 0.00413423
	LOSS [training: 0.23716774649187564 | validation: 0.1441563502960053]
	TIME [epoch: 8.12 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21957076537469247		[learning rate: 0.0041267]
		[batch 20/20] avg loss: 0.17848999500304924		[learning rate: 0.0041192]
	Learning Rate: 0.00411923
	LOSS [training: 0.19903038018887087 | validation: 0.3526772656806615]
	TIME [epoch: 8.12 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2272534934127748		[learning rate: 0.0041117]
		[batch 20/20] avg loss: 0.18533183494289118		[learning rate: 0.0041043]
	Learning Rate: 0.00410428
	LOSS [training: 0.206292664177833 | validation: 0.22878494848856362]
	TIME [epoch: 8.12 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22880994083895936		[learning rate: 0.0040968]
		[batch 20/20] avg loss: 0.34827861886146017		[learning rate: 0.0040894]
	Learning Rate: 0.00408938
	LOSS [training: 0.2885442798502098 | validation: 0.1517133769060554]
	TIME [epoch: 8.13 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17759256400771292		[learning rate: 0.004082]
		[batch 20/20] avg loss: 0.14965175611214834		[learning rate: 0.0040745]
	Learning Rate: 0.00407454
	LOSS [training: 0.16362216005993058 | validation: 0.22247238895894145]
	TIME [epoch: 8.13 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2350406501034418		[learning rate: 0.0040671]
		[batch 20/20] avg loss: 0.16356040647836034		[learning rate: 0.0040598]
	Learning Rate: 0.00405976
	LOSS [training: 0.19930052829090109 | validation: 0.1426493659085036]
	TIME [epoch: 8.12 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20737430847825794		[learning rate: 0.0040524]
		[batch 20/20] avg loss: 0.1752608571967759		[learning rate: 0.004045]
	Learning Rate: 0.00404502
	LOSS [training: 0.1913175828375169 | validation: 0.3421240576804898]
	TIME [epoch: 8.12 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20642748102160186		[learning rate: 0.0040377]
		[batch 20/20] avg loss: 0.1414121209450979		[learning rate: 0.0040303]
	Learning Rate: 0.00403034
	LOSS [training: 0.17391980098334986 | validation: 0.16804460447486272]
	TIME [epoch: 8.12 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1892201644695211		[learning rate: 0.004023]
		[batch 20/20] avg loss: 0.21294752016377413		[learning rate: 0.0040157]
	Learning Rate: 0.00401572
	LOSS [training: 0.20108384231664758 | validation: 0.14499149045072127]
	TIME [epoch: 8.15 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13542114421520585		[learning rate: 0.0040084]
		[batch 20/20] avg loss: 0.2248527241078945		[learning rate: 0.0040011]
	Learning Rate: 0.00400114
	LOSS [training: 0.1801369341615502 | validation: 0.19654158005576183]
	TIME [epoch: 8.12 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22010447119904689		[learning rate: 0.0039939]
		[batch 20/20] avg loss: 0.23647131887312173		[learning rate: 0.0039866]
	Learning Rate: 0.00398662
	LOSS [training: 0.22828789503608432 | validation: 0.1312646907174686]
	TIME [epoch: 8.12 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2288311277493534		[learning rate: 0.0039794]
		[batch 20/20] avg loss: 0.23740845461823298		[learning rate: 0.0039722]
	Learning Rate: 0.00397216
	LOSS [training: 0.23311979118379322 | validation: 0.23917598840120272]
	TIME [epoch: 8.12 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18263032578529617		[learning rate: 0.0039649]
		[batch 20/20] avg loss: 0.2347947250779287		[learning rate: 0.0039577]
	Learning Rate: 0.00395774
	LOSS [training: 0.20871252543161245 | validation: 0.15558273091938896]
	TIME [epoch: 8.13 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22034228882632684		[learning rate: 0.0039506]
		[batch 20/20] avg loss: 0.16995356079894125		[learning rate: 0.0039434]
	Learning Rate: 0.00394338
	LOSS [training: 0.19514792481263404 | validation: 0.2313192473067306]
	TIME [epoch: 8.13 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21295031431288317		[learning rate: 0.0039362]
		[batch 20/20] avg loss: 0.22190728506443474		[learning rate: 0.0039291]
	Learning Rate: 0.00392907
	LOSS [training: 0.21742879968865897 | validation: 0.1864023966326055]
	TIME [epoch: 8.12 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28241718700151525		[learning rate: 0.0039219]
		[batch 20/20] avg loss: 0.22900447451866562		[learning rate: 0.0039148]
	Learning Rate: 0.00391481
	LOSS [training: 0.25571083076009044 | validation: 0.21358706495939675]
	TIME [epoch: 8.12 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26119284619781097		[learning rate: 0.0039077]
		[batch 20/20] avg loss: 0.2641501970156866		[learning rate: 0.0039006]
	Learning Rate: 0.0039006
	LOSS [training: 0.2626715216067487 | validation: 1.1387150218820687]
	TIME [epoch: 8.12 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37077149320720176		[learning rate: 0.0038935]
		[batch 20/20] avg loss: 0.2565906186723429		[learning rate: 0.0038864]
	Learning Rate: 0.00388645
	LOSS [training: 0.3136810559397723 | validation: 0.3876199833119589]
	TIME [epoch: 8.15 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22535528171933944		[learning rate: 0.0038794]
		[batch 20/20] avg loss: 0.2522517762617337		[learning rate: 0.0038723]
	Learning Rate: 0.00387234
	LOSS [training: 0.23880352899053658 | validation: 0.1998024313509344]
	TIME [epoch: 8.11 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1273076949969329		[learning rate: 0.0038653]
		[batch 20/20] avg loss: 0.18436031499477065		[learning rate: 0.0038583]
	Learning Rate: 0.00385829
	LOSS [training: 0.1558340049958518 | validation: 0.1550109391766487]
	TIME [epoch: 8.12 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25225363883144425		[learning rate: 0.0038513]
		[batch 20/20] avg loss: 0.1706830016014881		[learning rate: 0.0038443]
	Learning Rate: 0.00384429
	LOSS [training: 0.21146832021646617 | validation: 0.6090409609409948]
	TIME [epoch: 8.12 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22556947046731324		[learning rate: 0.0038373]
		[batch 20/20] avg loss: 0.19665848034722747		[learning rate: 0.0038303]
	Learning Rate: 0.00383034
	LOSS [training: 0.21111397540727034 | validation: 0.14878527974951386]
	TIME [epoch: 8.14 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24071977283166843		[learning rate: 0.0038234]
		[batch 20/20] avg loss: 0.19061562229205878		[learning rate: 0.0038164]
	Learning Rate: 0.00381644
	LOSS [training: 0.21566769756186366 | validation: 0.09192485278892727]
	TIME [epoch: 8.13 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2444896037170837		[learning rate: 0.0038095]
		[batch 20/20] avg loss: 0.1870247887915541		[learning rate: 0.0038026]
	Learning Rate: 0.00380258
	LOSS [training: 0.21575719625431886 | validation: 0.13664524577768472]
	TIME [epoch: 8.12 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23102567762339848		[learning rate: 0.0037957]
		[batch 20/20] avg loss: 0.21391105780687844		[learning rate: 0.0037888]
	Learning Rate: 0.00378879
	LOSS [training: 0.2224683677151385 | validation: 0.12139427992404753]
	TIME [epoch: 8.12 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2668572549044482		[learning rate: 0.0037819]
		[batch 20/20] avg loss: 0.20967576016605233		[learning rate: 0.003775]
	Learning Rate: 0.00377504
	LOSS [training: 0.23826650753525028 | validation: 0.10064368427665517]
	TIME [epoch: 8.12 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2516862097993736		[learning rate: 0.0037682]
		[batch 20/20] avg loss: 0.19538602724301346		[learning rate: 0.0037613]
	Learning Rate: 0.00376134
	LOSS [training: 0.22353611852119354 | validation: 0.16357415321288407]
	TIME [epoch: 8.15 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17819495087189177		[learning rate: 0.0037545]
		[batch 20/20] avg loss: 0.27006418296841517		[learning rate: 0.0037477]
	Learning Rate: 0.00374769
	LOSS [training: 0.22412956692015346 | validation: 0.206688741619131]
	TIME [epoch: 8.12 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16077350181514613		[learning rate: 0.0037409]
		[batch 20/20] avg loss: 0.20785060091410848		[learning rate: 0.0037341]
	Learning Rate: 0.00373408
	LOSS [training: 0.1843120513646273 | validation: 0.09993040878481851]
	TIME [epoch: 8.12 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15079221046159663		[learning rate: 0.0037273]
		[batch 20/20] avg loss: 0.16047018720676798		[learning rate: 0.0037205]
	Learning Rate: 0.00372053
	LOSS [training: 0.15563119883418228 | validation: 0.15111249972915858]
	TIME [epoch: 8.12 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1915573016527565		[learning rate: 0.0037138]
		[batch 20/20] avg loss: 0.213724432690554		[learning rate: 0.003707]
	Learning Rate: 0.00370703
	LOSS [training: 0.20264086717165525 | validation: 0.24524087035780734]
	TIME [epoch: 8.14 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2920676758327154		[learning rate: 0.0037003]
		[batch 20/20] avg loss: 0.18676604847812756		[learning rate: 0.0036936]
	Learning Rate: 0.00369358
	LOSS [training: 0.2394168621554215 | validation: 0.12540305859696474]
	TIME [epoch: 8.12 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16165556865661607		[learning rate: 0.0036869]
		[batch 20/20] avg loss: 0.2329050659468343		[learning rate: 0.0036802]
	Learning Rate: 0.00368017
	LOSS [training: 0.19728031730172518 | validation: 0.1435032666263668]
	TIME [epoch: 8.12 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2489295594140862		[learning rate: 0.0036735]
		[batch 20/20] avg loss: 0.1802006569986302		[learning rate: 0.0036668]
	Learning Rate: 0.00366682
	LOSS [training: 0.21456510820635816 | validation: 0.1842663178695704]
	TIME [epoch: 8.11 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18587084598212883		[learning rate: 0.0036602]
		[batch 20/20] avg loss: 0.21857539631399742		[learning rate: 0.0036535]
	Learning Rate: 0.00365351
	LOSS [training: 0.20222312114806312 | validation: 0.10304586218937546]
	TIME [epoch: 8.12 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3539341892629788		[learning rate: 0.0036469]
		[batch 20/20] avg loss: 0.2760356976361236		[learning rate: 0.0036403]
	Learning Rate: 0.00364025
	LOSS [training: 0.3149849434495512 | validation: 0.14464147447477166]
	TIME [epoch: 8.15 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24316147944768582		[learning rate: 0.0036336]
		[batch 20/20] avg loss: 0.19704804909073664		[learning rate: 0.003627]
	Learning Rate: 0.00362704
	LOSS [training: 0.22010476426921124 | validation: 0.10868057890800581]
	TIME [epoch: 8.12 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13996020798610617		[learning rate: 0.0036205]
		[batch 20/20] avg loss: 0.23159299686150972		[learning rate: 0.0036139]
	Learning Rate: 0.00361388
	LOSS [training: 0.18577660242380792 | validation: 0.13011795912978597]
	TIME [epoch: 8.12 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22296184599983476		[learning rate: 0.0036073]
		[batch 20/20] avg loss: 0.20917401288826065		[learning rate: 0.0036008]
	Learning Rate: 0.00360076
	LOSS [training: 0.21606792944404773 | validation: 0.27725940889823447]
	TIME [epoch: 8.12 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3070159622795759		[learning rate: 0.0035942]
		[batch 20/20] avg loss: 0.2913094099108956		[learning rate: 0.0035877]
	Learning Rate: 0.0035877
	LOSS [training: 0.2991626860952358 | validation: 0.529897665109799]
	TIME [epoch: 8.14 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28779840342774715		[learning rate: 0.0035812]
		[batch 20/20] avg loss: 0.3084828630884542		[learning rate: 0.0035747]
	Learning Rate: 0.00357468
	LOSS [training: 0.2981406332581007 | validation: 0.22238878187515504]
	TIME [epoch: 8.12 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24276074752989976		[learning rate: 0.0035682]
		[batch 20/20] avg loss: 0.27048144043736505		[learning rate: 0.0035617]
	Learning Rate: 0.0035617
	LOSS [training: 0.2566210939836323 | validation: 0.13025631491688078]
	TIME [epoch: 8.11 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30025427668205695		[learning rate: 0.0035552]
		[batch 20/20] avg loss: 0.19050248216388738		[learning rate: 0.0035488]
	Learning Rate: 0.00354878
	LOSS [training: 0.24537837942297216 | validation: 0.19839022125716385]
	TIME [epoch: 8.11 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17474570289373553		[learning rate: 0.0035423]
		[batch 20/20] avg loss: 0.19775672246545734		[learning rate: 0.0035359]
	Learning Rate: 0.0035359
	LOSS [training: 0.18625121267959638 | validation: 0.1137509397539572]
	TIME [epoch: 8.11 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30808569649270046		[learning rate: 0.0035295]
		[batch 20/20] avg loss: 0.2166722762631413		[learning rate: 0.0035231]
	Learning Rate: 0.00352307
	LOSS [training: 0.26237898637792084 | validation: 0.28989153824460473]
	TIME [epoch: 8.14 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2801055792692842		[learning rate: 0.0035167]
		[batch 20/20] avg loss: 0.17561231005616912		[learning rate: 0.0035103]
	Learning Rate: 0.00351028
	LOSS [training: 0.2278589446627267 | validation: 0.21698463464116954]
	TIME [epoch: 8.12 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21776971010008897		[learning rate: 0.0035039]
		[batch 20/20] avg loss: 0.3950985151166397		[learning rate: 0.0034975]
	Learning Rate: 0.00349754
	LOSS [training: 0.3064341126083643 | validation: 0.1570563870568924]
	TIME [epoch: 8.11 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1786282682643745		[learning rate: 0.0034912]
		[batch 20/20] avg loss: 0.16446730659540337		[learning rate: 0.0034849]
	Learning Rate: 0.00348485
	LOSS [training: 0.17154778742988894 | validation: 0.20560550742887804]
	TIME [epoch: 8.11 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.361221291360765		[learning rate: 0.0034785]
		[batch 20/20] avg loss: 0.2040785330419684		[learning rate: 0.0034722]
	Learning Rate: 0.0034722
	LOSS [training: 0.2826499122013667 | validation: 0.15979701141128883]
	TIME [epoch: 8.14 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2650659119904052		[learning rate: 0.0034659]
		[batch 20/20] avg loss: 0.26464861838460385		[learning rate: 0.0034596]
	Learning Rate: 0.0034596
	LOSS [training: 0.26485726518750446 | validation: 0.22984451363584668]
	TIME [epoch: 8.11 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.288449215112224		[learning rate: 0.0034533]
		[batch 20/20] avg loss: 0.28854763582245974		[learning rate: 0.003447]
	Learning Rate: 0.00344705
	LOSS [training: 0.28849842546734183 | validation: 0.24328046722732832]
	TIME [epoch: 8.11 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2049199002374414		[learning rate: 0.0034408]
		[batch 20/20] avg loss: 0.2443761593485505		[learning rate: 0.0034345]
	Learning Rate: 0.00343454
	LOSS [training: 0.22464802979299595 | validation: 0.35020290592352266]
	TIME [epoch: 8.11 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1947638335210617		[learning rate: 0.0034283]
		[batch 20/20] avg loss: 0.1777126544908269		[learning rate: 0.0034221]
	Learning Rate: 0.00342207
	LOSS [training: 0.1862382440059443 | validation: 0.14837303960039433]
	TIME [epoch: 8.13 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15727469839214903		[learning rate: 0.0034159]
		[batch 20/20] avg loss: 0.23625475514977592		[learning rate: 0.0034097]
	Learning Rate: 0.00340966
	LOSS [training: 0.19676472677096246 | validation: 0.2519841209682137]
	TIME [epoch: 8.14 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19794270391777102		[learning rate: 0.0034035]
		[batch 20/20] avg loss: 0.20357818920731133		[learning rate: 0.0033973]
	Learning Rate: 0.00339728
	LOSS [training: 0.20076044656254116 | validation: 0.09027656142135382]
	TIME [epoch: 8.11 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10660388734369133		[learning rate: 0.0033911]
		[batch 20/20] avg loss: 0.22284289122657927		[learning rate: 0.003385]
	Learning Rate: 0.00338495
	LOSS [training: 0.16472338928513527 | validation: 0.09194764722244146]
	TIME [epoch: 8.11 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12981494732832613		[learning rate: 0.0033788]
		[batch 20/20] avg loss: 0.22268457320162388		[learning rate: 0.0033727]
	Learning Rate: 0.00337267
	LOSS [training: 0.17624976026497502 | validation: 0.11242187674911638]
	TIME [epoch: 8.11 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23416628551103727		[learning rate: 0.0033665]
		[batch 20/20] avg loss: 0.18421018933804495		[learning rate: 0.0033604]
	Learning Rate: 0.00336043
	LOSS [training: 0.20918823742454107 | validation: 0.3065049604707662]
	TIME [epoch: 8.13 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2274256461462179		[learning rate: 0.0033543]
		[batch 20/20] avg loss: 0.14609176582563455		[learning rate: 0.0033482]
	Learning Rate: 0.00334823
	LOSS [training: 0.18675870598592625 | validation: 0.1488621572040875]
	TIME [epoch: 8.11 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15978219161629087		[learning rate: 0.0033422]
		[batch 20/20] avg loss: 0.15886207002468564		[learning rate: 0.0033361]
	Learning Rate: 0.00333608
	LOSS [training: 0.15932213082048827 | validation: 0.09493288533449987]
	TIME [epoch: 8.11 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22527443798444188		[learning rate: 0.00333]
		[batch 20/20] avg loss: 0.21596059215613878		[learning rate: 0.003324]
	Learning Rate: 0.00332398
	LOSS [training: 0.22061751507029034 | validation: 0.13377104998200526]
	TIME [epoch: 8.11 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23732500946031293		[learning rate: 0.0033179]
		[batch 20/20] avg loss: 0.20350036762434023		[learning rate: 0.0033119]
	Learning Rate: 0.00331191
	LOSS [training: 0.22041268854232662 | validation: 0.12023380941752312]
	TIME [epoch: 8.12 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12996197714494537		[learning rate: 0.0033059]
		[batch 20/20] avg loss: 0.1851488318636896		[learning rate: 0.0032999]
	Learning Rate: 0.00329989
	LOSS [training: 0.15755540450431746 | validation: 0.1950220698547821]
	TIME [epoch: 8.12 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18126423780063078		[learning rate: 0.0032939]
		[batch 20/20] avg loss: 0.15860963180446025		[learning rate: 0.0032879]
	Learning Rate: 0.00328792
	LOSS [training: 0.16993693480254554 | validation: 0.15063735847971088]
	TIME [epoch: 8.11 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1779630869863315		[learning rate: 0.0032819]
		[batch 20/20] avg loss: 0.17996091438847478		[learning rate: 0.003276]
	Learning Rate: 0.00327599
	LOSS [training: 0.17896200068740314 | validation: 0.8003549094235086]
	TIME [epoch: 8.1 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33023559939517744		[learning rate: 0.00327]
		[batch 20/20] avg loss: 0.26674606316233607		[learning rate: 0.0032641]
	Learning Rate: 0.0032641
	LOSS [training: 0.2984908312787567 | validation: 0.18423610983224364]
	TIME [epoch: 8.11 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23293177131004086		[learning rate: 0.0032582]
		[batch 20/20] avg loss: 0.1903032834214924		[learning rate: 0.0032523]
	Learning Rate: 0.00325225
	LOSS [training: 0.2116175273657666 | validation: 0.1825330618672644]
	TIME [epoch: 8.13 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18546663970898797		[learning rate: 0.0032463]
		[batch 20/20] avg loss: 0.1780720943255798		[learning rate: 0.0032404]
	Learning Rate: 0.00324045
	LOSS [training: 0.1817693670172839 | validation: 0.16830129849283137]
	TIME [epoch: 8.11 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1798438631687454		[learning rate: 0.0032346]
		[batch 20/20] avg loss: 0.17940082063878277		[learning rate: 0.0032287]
	Learning Rate: 0.00322869
	LOSS [training: 0.17962234190376414 | validation: 0.11456303282090308]
	TIME [epoch: 8.1 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12111025098837687		[learning rate: 0.0032228]
		[batch 20/20] avg loss: 0.13402894040130037		[learning rate: 0.003217]
	Learning Rate: 0.00321697
	LOSS [training: 0.1275695956948386 | validation: 0.1548167530188657]
	TIME [epoch: 8.1 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2198014564077641		[learning rate: 0.0032111]
		[batch 20/20] avg loss: 0.17766956932753528		[learning rate: 0.0032053]
	Learning Rate: 0.0032053
	LOSS [training: 0.19873551286764973 | validation: 0.2374077107283478]
	TIME [epoch: 8.12 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2054822065203982		[learning rate: 0.0031995]
		[batch 20/20] avg loss: 0.21767313976133976		[learning rate: 0.0031937]
	Learning Rate: 0.00319367
	LOSS [training: 0.211577673140869 | validation: 0.14405184259179143]
	TIME [epoch: 8.12 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21353000939935335		[learning rate: 0.0031879]
		[batch 20/20] avg loss: 0.23148849650059863		[learning rate: 0.0031821]
	Learning Rate: 0.00318208
	LOSS [training: 0.22250925294997598 | validation: 0.09759203718911545]
	TIME [epoch: 8.1 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.131403217244599		[learning rate: 0.0031763]
		[batch 20/20] avg loss: 0.1293728018551043		[learning rate: 0.0031705]
	Learning Rate: 0.00317053
	LOSS [training: 0.13038800954985166 | validation: 0.10013751409284402]
	TIME [epoch: 8.1 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2107670519531505		[learning rate: 0.0031648]
		[batch 20/20] avg loss: 0.16864456734423824		[learning rate: 0.003159]
	Learning Rate: 0.00315902
	LOSS [training: 0.18970580964869438 | validation: 0.12015084743918417]
	TIME [epoch: 8.11 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15361747566377731		[learning rate: 0.0031533]
		[batch 20/20] avg loss: 0.17102785391341666		[learning rate: 0.0031476]
	Learning Rate: 0.00314756
	LOSS [training: 0.16232266478859697 | validation: 0.10362467816174958]
	TIME [epoch: 8.13 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16703412845025836		[learning rate: 0.0031418]
		[batch 20/20] avg loss: 0.2084955538978744		[learning rate: 0.0031361]
	Learning Rate: 0.00313613
	LOSS [training: 0.1877648411740664 | validation: 0.18638596433153926]
	TIME [epoch: 8.1 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2029474597447943		[learning rate: 0.0031304]
		[batch 20/20] avg loss: 0.36788402042214746		[learning rate: 0.0031248]
	Learning Rate: 0.00312475
	LOSS [training: 0.2854157400834709 | validation: 0.17526854820738813]
	TIME [epoch: 8.1 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22279778432531244		[learning rate: 0.0031191]
		[batch 20/20] avg loss: 0.15672614135433038		[learning rate: 0.0031134]
	Learning Rate: 0.00311341
	LOSS [training: 0.18976196283982139 | validation: 0.301487160876342]
	TIME [epoch: 8.1 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1677875041618214		[learning rate: 0.0031078]
		[batch 20/20] avg loss: 0.18121754104372875		[learning rate: 0.0031021]
	Learning Rate: 0.00310212
	LOSS [training: 0.1745025226027751 | validation: 0.1641590098935775]
	TIME [epoch: 8.12 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16793996441563425		[learning rate: 0.0030965]
		[batch 20/20] avg loss: 0.13356847560352508		[learning rate: 0.0030909]
	Learning Rate: 0.00309086
	LOSS [training: 0.15075422000957966 | validation: 0.20298969309820994]
	TIME [epoch: 8.11 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2436259020417913		[learning rate: 0.0030852]
		[batch 20/20] avg loss: 0.15246790178332564		[learning rate: 0.0030796]
	Learning Rate: 0.00307964
	LOSS [training: 0.19804690191255847 | validation: 0.0670146947674469]
	TIME [epoch: 8.1 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16484424305312353		[learning rate: 0.003074]
		[batch 20/20] avg loss: 0.13452203182488076		[learning rate: 0.0030685]
	Learning Rate: 0.00306846
	LOSS [training: 0.14968313743900213 | validation: 0.15356613119848977]
	TIME [epoch: 8.1 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17649377897919047		[learning rate: 0.0030629]
		[batch 20/20] avg loss: 0.16440824150181704		[learning rate: 0.0030573]
	Learning Rate: 0.00305733
	LOSS [training: 0.17045101024050374 | validation: 0.0991605922719031]
	TIME [epoch: 8.1 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14448003294027592		[learning rate: 0.0030518]
		[batch 20/20] avg loss: 0.18003905217141353		[learning rate: 0.0030462]
	Learning Rate: 0.00304623
	LOSS [training: 0.16225954255584468 | validation: 0.1321942884515243]
	TIME [epoch: 8.13 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18575528275180164		[learning rate: 0.0030407]
		[batch 20/20] avg loss: 0.2574939843221513		[learning rate: 0.0030352]
	Learning Rate: 0.00303518
	LOSS [training: 0.22162463353697648 | validation: 0.23879419290244358]
	TIME [epoch: 8.1 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2374094895103179		[learning rate: 0.0030297]
		[batch 20/20] avg loss: 0.20647393949394663		[learning rate: 0.0030242]
	Learning Rate: 0.00302416
	LOSS [training: 0.22194171450213224 | validation: 0.11980185283586353]
	TIME [epoch: 8.1 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1993751893030347		[learning rate: 0.0030187]
		[batch 20/20] avg loss: 0.19682115627198793		[learning rate: 0.0030132]
	Learning Rate: 0.00301319
	LOSS [training: 0.1980981727875113 | validation: 0.24921248656265646]
	TIME [epoch: 8.1 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18831712473858433		[learning rate: 0.0030077]
		[batch 20/20] avg loss: 0.20775338625236284		[learning rate: 0.0030023]
	Learning Rate: 0.00300225
	LOSS [training: 0.19803525549547357 | validation: 0.274030917114269]
	TIME [epoch: 8.12 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20666701038205093		[learning rate: 0.0029968]
		[batch 20/20] avg loss: 0.21349034452658744		[learning rate: 0.0029914]
	Learning Rate: 0.00299136
	LOSS [training: 0.21007867745431916 | validation: 0.12601328317849905]
	TIME [epoch: 8.11 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28264849327651836		[learning rate: 0.0029859]
		[batch 20/20] avg loss: 0.18751016321199515		[learning rate: 0.0029805]
	Learning Rate: 0.0029805
	LOSS [training: 0.23507932824425676 | validation: 0.1124595375313355]
	TIME [epoch: 8.1 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13598272688617608		[learning rate: 0.0029751]
		[batch 20/20] avg loss: 0.18833983687825656		[learning rate: 0.0029697]
	Learning Rate: 0.00296969
	LOSS [training: 0.16216128188221632 | validation: 0.22039401256187485]
	TIME [epoch: 8.1 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16714972625534025		[learning rate: 0.0029643]
		[batch 20/20] avg loss: 0.17521569242745638		[learning rate: 0.0029589]
	Learning Rate: 0.00295891
	LOSS [training: 0.17118270934139831 | validation: 0.13391124622516345]
	TIME [epoch: 8.1 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.166820072855226		[learning rate: 0.0029535]
		[batch 20/20] avg loss: 0.1681988323105677		[learning rate: 0.0029482]
	Learning Rate: 0.00294817
	LOSS [training: 0.16750945258289685 | validation: 0.09923432837388921]
	TIME [epoch: 8.12 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22237325578014805		[learning rate: 0.0029428]
		[batch 20/20] avg loss: 0.20366933211982752		[learning rate: 0.0029375]
	Learning Rate: 0.00293747
	LOSS [training: 0.21302129394998776 | validation: 0.15621480459908071]
	TIME [epoch: 8.1 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15354265841989623		[learning rate: 0.0029321]
		[batch 20/20] avg loss: 0.1314524520677382		[learning rate: 0.0029268]
	Learning Rate: 0.00292681
	LOSS [training: 0.1424975552438172 | validation: 0.08158362980318946]
	TIME [epoch: 8.09 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15431070565023572		[learning rate: 0.0029215]
		[batch 20/20] avg loss: 0.13849628265437508		[learning rate: 0.0029162]
	Learning Rate: 0.00291619
	LOSS [training: 0.1464034941523054 | validation: 0.3419106492274753]
	TIME [epoch: 8.1 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17498641182028918		[learning rate: 0.0029109]
		[batch 20/20] avg loss: 0.19369012078586606		[learning rate: 0.0029056]
	Learning Rate: 0.00290561
	LOSS [training: 0.18433826630307762 | validation: 0.1304689536658308]
	TIME [epoch: 8.12 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1404584938988082		[learning rate: 0.0029003]
		[batch 20/20] avg loss: 0.19931463541829725		[learning rate: 0.0028951]
	Learning Rate: 0.00289506
	LOSS [training: 0.16988656465855273 | validation: 0.2558380778699549]
	TIME [epoch: 8.11 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16366471025341037		[learning rate: 0.0028898]
		[batch 20/20] avg loss: 0.1253112659274878		[learning rate: 0.0028846]
	Learning Rate: 0.00288456
	LOSS [training: 0.14448798809044908 | validation: 0.13755236161387255]
	TIME [epoch: 8.11 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1571689957375268		[learning rate: 0.0028793]
		[batch 20/20] avg loss: 0.11374667155963165		[learning rate: 0.0028741]
	Learning Rate: 0.00287409
	LOSS [training: 0.1354578336485792 | validation: 0.17970821024643996]
	TIME [epoch: 8.11 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14026871742109326		[learning rate: 0.0028689]
		[batch 20/20] avg loss: 0.1499090801415161		[learning rate: 0.0028637]
	Learning Rate: 0.00286366
	LOSS [training: 0.14508889878130465 | validation: 0.09956190710231516]
	TIME [epoch: 8.11 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2207839682852982		[learning rate: 0.0028585]
		[batch 20/20] avg loss: 0.13219721622979352		[learning rate: 0.0028533]
	Learning Rate: 0.00285326
	LOSS [training: 0.17649059225754588 | validation: 0.18158232323198667]
	TIME [epoch: 8.13 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07859907938273689		[learning rate: 0.0028481]
		[batch 20/20] avg loss: 0.16056358009710797		[learning rate: 0.0028429]
	Learning Rate: 0.00284291
	LOSS [training: 0.11958132973992242 | validation: 0.093732347037868]
	TIME [epoch: 8.1 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12974909575697297		[learning rate: 0.0028377]
		[batch 20/20] avg loss: 0.16783051983608485		[learning rate: 0.0028326]
	Learning Rate: 0.00283259
	LOSS [training: 0.1487898077965289 | validation: 0.21488660553406325]
	TIME [epoch: 8.11 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12333583158301147		[learning rate: 0.0028274]
		[batch 20/20] avg loss: 0.13142335935295793		[learning rate: 0.0028223]
	Learning Rate: 0.00282231
	LOSS [training: 0.12737959546798466 | validation: 0.14727362799800875]
	TIME [epoch: 8.11 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13863246291436176		[learning rate: 0.0028172]
		[batch 20/20] avg loss: 0.12575116042384166		[learning rate: 0.0028121]
	Learning Rate: 0.00281207
	LOSS [training: 0.1321918116691017 | validation: 0.053474258568852684]
	TIME [epoch: 8.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14564153327587648		[learning rate: 0.002807]
		[batch 20/20] avg loss: 0.14728786238040092		[learning rate: 0.0028019]
	Learning Rate: 0.00280187
	LOSS [training: 0.1464646978281387 | validation: 0.23399266585629225]
	TIME [epoch: 8.12 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1642476933252888		[learning rate: 0.0027968]
		[batch 20/20] avg loss: 0.17183460299078612		[learning rate: 0.0027917]
	Learning Rate: 0.0027917
	LOSS [training: 0.16804114815803747 | validation: 0.26300539549468016]
	TIME [epoch: 8.1 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18330496173616836		[learning rate: 0.0027866]
		[batch 20/20] avg loss: 0.1491009301955635		[learning rate: 0.0027816]
	Learning Rate: 0.00278157
	LOSS [training: 0.16620294596586596 | validation: 0.13005509620602837]
	TIME [epoch: 8.11 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13118336635172884		[learning rate: 0.0027765]
		[batch 20/20] avg loss: 0.14301438682544743		[learning rate: 0.0027715]
	Learning Rate: 0.00277147
	LOSS [training: 0.13709887658858813 | validation: 0.14572110027815954]
	TIME [epoch: 8.11 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13349221874062603		[learning rate: 0.0027664]
		[batch 20/20] avg loss: 0.17357140067603338		[learning rate: 0.0027614]
	Learning Rate: 0.00276141
	LOSS [training: 0.1535318097083297 | validation: 0.15986317229715147]
	TIME [epoch: 8.13 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16822756945552614		[learning rate: 0.0027564]
		[batch 20/20] avg loss: 0.11760880374290379		[learning rate: 0.0027514]
	Learning Rate: 0.00275139
	LOSS [training: 0.14291818659921496 | validation: 0.12327318796479061]
	TIME [epoch: 8.1 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17530737770138766		[learning rate: 0.0027464]
		[batch 20/20] avg loss: 0.16598294101736394		[learning rate: 0.0027414]
	Learning Rate: 0.00274141
	LOSS [training: 0.1706451593593758 | validation: 0.09487926696860424]
	TIME [epoch: 8.11 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15031798212093667		[learning rate: 0.0027364]
		[batch 20/20] avg loss: 0.1378441622740097		[learning rate: 0.0027315]
	Learning Rate: 0.00273146
	LOSS [training: 0.14408107219747318 | validation: 0.05956174257257775]
	TIME [epoch: 8.1 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12238816805368055		[learning rate: 0.0027265]
		[batch 20/20] avg loss: 0.19091223811095093		[learning rate: 0.0027215]
	Learning Rate: 0.00272155
	LOSS [training: 0.15665020308231575 | validation: 0.06190708508165161]
	TIME [epoch: 8.13 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17578062885681262		[learning rate: 0.0027166]
		[batch 20/20] avg loss: 0.10094589937645629		[learning rate: 0.0027117]
	Learning Rate: 0.00271167
	LOSS [training: 0.1383632641166344 | validation: 0.1348309366648171]
	TIME [epoch: 8.12 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17430532300593657		[learning rate: 0.0027067]
		[batch 20/20] avg loss: 0.13417089620828426		[learning rate: 0.0027018]
	Learning Rate: 0.00270183
	LOSS [training: 0.15423810960711046 | validation: 0.12779238036482138]
	TIME [epoch: 8.11 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19754385196515722		[learning rate: 0.0026969]
		[batch 20/20] avg loss: 0.11828943287264412		[learning rate: 0.002692]
	Learning Rate: 0.00269202
	LOSS [training: 0.15791664241890072 | validation: 0.1422249025304003]
	TIME [epoch: 8.11 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19686142500420445		[learning rate: 0.0026871]
		[batch 20/20] avg loss: 0.1722165652734613		[learning rate: 0.0026823]
	Learning Rate: 0.00268225
	LOSS [training: 0.18453899513883287 | validation: 0.2303559660197164]
	TIME [epoch: 8.11 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20729220939327084		[learning rate: 0.0026774]
		[batch 20/20] avg loss: 0.13194576351151227		[learning rate: 0.0026725]
	Learning Rate: 0.00267252
	LOSS [training: 0.1696189864523916 | validation: 0.2022750756531781]
	TIME [epoch: 8.13 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17515722998905744		[learning rate: 0.0026677]
		[batch 20/20] avg loss: 0.21028479274231074		[learning rate: 0.0026628]
	Learning Rate: 0.00266282
	LOSS [training: 0.19272101136568406 | validation: 0.3848317457955034]
	TIME [epoch: 8.1 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19645323593838526		[learning rate: 0.002658]
		[batch 20/20] avg loss: 0.1792338963398277		[learning rate: 0.0026532]
	Learning Rate: 0.00265316
	LOSS [training: 0.1878435661391065 | validation: 0.10109482350597829]
	TIME [epoch: 8.11 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16645281842285053		[learning rate: 0.0026483]
		[batch 20/20] avg loss: 0.13762228534993565		[learning rate: 0.0026435]
	Learning Rate: 0.00264353
	LOSS [training: 0.15203755188639306 | validation: 0.06263645303634995]
	TIME [epoch: 8.1 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2529371829272899		[learning rate: 0.0026387]
		[batch 20/20] avg loss: 0.1237871828005845		[learning rate: 0.0026339]
	Learning Rate: 0.00263394
	LOSS [training: 0.1883621828639372 | validation: 0.1963471326789361]
	TIME [epoch: 8.13 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15934689753914336		[learning rate: 0.0026292]
		[batch 20/20] avg loss: 0.17035651474689079		[learning rate: 0.0026244]
	Learning Rate: 0.00262438
	LOSS [training: 0.16485170614301708 | validation: 0.18861323166037572]
	TIME [epoch: 8.11 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15586228844915032		[learning rate: 0.0026196]
		[batch 20/20] avg loss: 0.1728645656375391		[learning rate: 0.0026149]
	Learning Rate: 0.00261485
	LOSS [training: 0.16436342704334475 | validation: 0.1690978853092015]
	TIME [epoch: 8.11 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1506385953098154		[learning rate: 0.0026101]
		[batch 20/20] avg loss: 0.16863709207588307		[learning rate: 0.0026054]
	Learning Rate: 0.00260536
	LOSS [training: 0.15963784369284922 | validation: 0.13838694059012446]
	TIME [epoch: 8.1 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20252995331207707		[learning rate: 0.0026006]
		[batch 20/20] avg loss: 0.12551604580585693		[learning rate: 0.0025959]
	Learning Rate: 0.00259591
	LOSS [training: 0.164022999558967 | validation: 0.14933665334042354]
	TIME [epoch: 8.11 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24744858804876008		[learning rate: 0.0025912]
		[batch 20/20] avg loss: 0.17563623561263575		[learning rate: 0.0025865]
	Learning Rate: 0.00258649
	LOSS [training: 0.21154241183069794 | validation: 0.23156403227439104]
	TIME [epoch: 8.13 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2012220275977313		[learning rate: 0.0025818]
		[batch 20/20] avg loss: 0.12650681315545603		[learning rate: 0.0025771]
	Learning Rate: 0.0025771
	LOSS [training: 0.1638644203765937 | validation: 0.08897823168158267]
	TIME [epoch: 8.11 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22107233844813984		[learning rate: 0.0025724]
		[batch 20/20] avg loss: 0.14643297258685342		[learning rate: 0.0025677]
	Learning Rate: 0.00256775
	LOSS [training: 0.18375265551749664 | validation: 0.0809981412207012]
	TIME [epoch: 8.1 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12262922230428848		[learning rate: 0.0025631]
		[batch 20/20] avg loss: 0.11398681028237859		[learning rate: 0.0025584]
	Learning Rate: 0.00255843
	LOSS [training: 0.11830801629333357 | validation: 0.11459559180236364]
	TIME [epoch: 8.1 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15758450584193895		[learning rate: 0.0025538]
		[batch 20/20] avg loss: 0.15871406923725892		[learning rate: 0.0025491]
	Learning Rate: 0.00254915
	LOSS [training: 0.15814928753959892 | validation: 0.15303120480173232]
	TIME [epoch: 8.13 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16754266660557032		[learning rate: 0.0025445]
		[batch 20/20] avg loss: 0.15795485580092977		[learning rate: 0.0025399]
	Learning Rate: 0.0025399
	LOSS [training: 0.16274876120325005 | validation: 0.19272650911600425]
	TIME [epoch: 8.1 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15491201418686376		[learning rate: 0.0025353]
		[batch 20/20] avg loss: 0.17274245509692074		[learning rate: 0.0025307]
	Learning Rate: 0.00253068
	LOSS [training: 0.16382723464189225 | validation: 0.19332490631464297]
	TIME [epoch: 8.1 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19732534370470958		[learning rate: 0.0025261]
		[batch 20/20] avg loss: 0.14225147270697158		[learning rate: 0.0025215]
	Learning Rate: 0.00252149
	LOSS [training: 0.16978840820584057 | validation: 0.1803268002500785]
	TIME [epoch: 8.1 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14373835892569958		[learning rate: 0.0025169]
		[batch 20/20] avg loss: 0.13377648614279408		[learning rate: 0.0025123]
	Learning Rate: 0.00251234
	LOSS [training: 0.13875742253424683 | validation: 0.08900769837570445]
	TIME [epoch: 8.11 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29753741596017375		[learning rate: 0.0025078]
		[batch 20/20] avg loss: 0.1653686966665499		[learning rate: 0.0025032]
	Learning Rate: 0.00250323
	LOSS [training: 0.23145305631336188 | validation: 0.1707154619335587]
	TIME [epoch: 8.12 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1300839129023139		[learning rate: 0.0024987]
		[batch 20/20] avg loss: 0.12144287778141245		[learning rate: 0.0024941]
	Learning Rate: 0.00249414
	LOSS [training: 0.12576339534186318 | validation: 0.13131060808717746]
	TIME [epoch: 8.1 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13433030637738028		[learning rate: 0.0024896]
		[batch 20/20] avg loss: 0.12371509346025042		[learning rate: 0.0024851]
	Learning Rate: 0.00248509
	LOSS [training: 0.12902269991881535 | validation: 0.12402236104059818]
	TIME [epoch: 8.11 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12499522400220456		[learning rate: 0.0024806]
		[batch 20/20] avg loss: 0.19095088098431426		[learning rate: 0.0024761]
	Learning Rate: 0.00247607
	LOSS [training: 0.1579730524932594 | validation: 0.13580321067054874]
	TIME [epoch: 8.1 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12629922573242586		[learning rate: 0.0024716]
		[batch 20/20] avg loss: 0.21905079984443399		[learning rate: 0.0024671]
	Learning Rate: 0.00246709
	LOSS [training: 0.17267501278842995 | validation: 0.14215675143546733]
	TIME [epoch: 8.13 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11847028527935595		[learning rate: 0.0024626]
		[batch 20/20] avg loss: 0.16906353455274503		[learning rate: 0.0024581]
	Learning Rate: 0.00245813
	LOSS [training: 0.14376690991605046 | validation: 0.14881258100104636]
	TIME [epoch: 8.11 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1225028942481711		[learning rate: 0.0024537]
		[batch 20/20] avg loss: 0.1112484874996208		[learning rate: 0.0024492]
	Learning Rate: 0.00244921
	LOSS [training: 0.11687569087389596 | validation: 0.25323433479918955]
	TIME [epoch: 8.11 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11864526524586701		[learning rate: 0.0024448]
		[batch 20/20] avg loss: 0.1493792722292372		[learning rate: 0.0024403]
	Learning Rate: 0.00244032
	LOSS [training: 0.13401226873755206 | validation: 0.1325730736563166]
	TIME [epoch: 8.1 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13740886270117963		[learning rate: 0.0024359]
		[batch 20/20] avg loss: 0.11154706976711133		[learning rate: 0.0024315]
	Learning Rate: 0.00243147
	LOSS [training: 0.1244779662341455 | validation: 0.22834653811991773]
	TIME [epoch: 8.11 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14505304349671685		[learning rate: 0.0024271]
		[batch 20/20] avg loss: 0.2011486851671065		[learning rate: 0.0024226]
	Learning Rate: 0.00242264
	LOSS [training: 0.17310086433191169 | validation: 0.2205587101730106]
	TIME [epoch: 8.12 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10271488121529851		[learning rate: 0.0024182]
		[batch 20/20] avg loss: 0.14228410070197844		[learning rate: 0.0024139]
	Learning Rate: 0.00241385
	LOSS [training: 0.12249949095863844 | validation: 0.11383365020318825]
	TIME [epoch: 8.11 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13687663650171428		[learning rate: 0.0024095]
		[batch 20/20] avg loss: 0.11533706170554583		[learning rate: 0.0024051]
	Learning Rate: 0.00240509
	LOSS [training: 0.12610684910363007 | validation: 0.08328766845073343]
	TIME [epoch: 8.1 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1448525819335651		[learning rate: 0.0024007]
		[batch 20/20] avg loss: 0.13158452201952162		[learning rate: 0.0023964]
	Learning Rate: 0.00239636
	LOSS [training: 0.13821855197654337 | validation: 0.12344531597152841]
	TIME [epoch: 8.1 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12404851311667575		[learning rate: 0.002392]
		[batch 20/20] avg loss: 0.09919903724947574		[learning rate: 0.0023877]
	Learning Rate: 0.00238767
	LOSS [training: 0.11162377518307573 | validation: 0.08706017386088966]
	TIME [epoch: 8.13 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15023400241459856		[learning rate: 0.0023833]
		[batch 20/20] avg loss: 0.18882084229596746		[learning rate: 0.002379]
	Learning Rate: 0.002379
	LOSS [training: 0.169527422355283 | validation: 0.21404553418456984]
	TIME [epoch: 8.11 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1031881295772882		[learning rate: 0.0023747]
		[batch 20/20] avg loss: 0.19014514087616702		[learning rate: 0.0023704]
	Learning Rate: 0.00237037
	LOSS [training: 0.1466666352267276 | validation: 0.2519047519707615]
	TIME [epoch: 8.1 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17195444760203238		[learning rate: 0.0023661]
		[batch 20/20] avg loss: 0.17388188130630616		[learning rate: 0.0023618]
	Learning Rate: 0.00236177
	LOSS [training: 0.17291816445416927 | validation: 0.19904469031261968]
	TIME [epoch: 8.11 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13201438795549125		[learning rate: 0.0023575]
		[batch 20/20] avg loss: 0.1195736715110721		[learning rate: 0.0023532]
	Learning Rate: 0.00235319
	LOSS [training: 0.1257940297332817 | validation: 0.0755809312661655]
	TIME [epoch: 8.13 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19720776602359633		[learning rate: 0.0023489]
		[batch 20/20] avg loss: 0.14108312341808732		[learning rate: 0.0023447]
	Learning Rate: 0.00234465
	LOSS [training: 0.16914544472084178 | validation: 0.09708991561606432]
	TIME [epoch: 8.12 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1192174735501151		[learning rate: 0.0023404]
		[batch 20/20] avg loss: 0.15546332224122067		[learning rate: 0.0023361]
	Learning Rate: 0.00233615
	LOSS [training: 0.13734039789566785 | validation: 0.1026924405090549]
	TIME [epoch: 8.11 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13794642940578775		[learning rate: 0.0023319]
		[batch 20/20] avg loss: 0.2207403750777896		[learning rate: 0.0023277]
	Learning Rate: 0.00232767
	LOSS [training: 0.17934340224178866 | validation: 0.15585529475895157]
	TIME [epoch: 8.1 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18758715834874648		[learning rate: 0.0023234]
		[batch 20/20] avg loss: 0.128387987756444		[learning rate: 0.0023192]
	Learning Rate: 0.00231922
	LOSS [training: 0.1579875730525952 | validation: 0.19505049862429005]
	TIME [epoch: 8.11 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1403473863427279		[learning rate: 0.002315]
		[batch 20/20] avg loss: 0.14914367067904039		[learning rate: 0.0023108]
	Learning Rate: 0.0023108
	LOSS [training: 0.14474552851088418 | validation: 0.0756374560259472]
	TIME [epoch: 8.13 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13445508837385567		[learning rate: 0.0023066]
		[batch 20/20] avg loss: 0.12442859040529049		[learning rate: 0.0023024]
	Learning Rate: 0.00230242
	LOSS [training: 0.12944183938957307 | validation: 0.07197358993491612]
	TIME [epoch: 8.11 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19633064189059107		[learning rate: 0.0022982]
		[batch 20/20] avg loss: 0.13116890877903153		[learning rate: 0.0022941]
	Learning Rate: 0.00229406
	LOSS [training: 0.16374977533481128 | validation: 0.147377868200753]
	TIME [epoch: 8.11 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19760478898723582		[learning rate: 0.0022899]
		[batch 20/20] avg loss: 0.2124540841011806		[learning rate: 0.0022857]
	Learning Rate: 0.00228574
	LOSS [training: 0.2050294365442082 | validation: 0.13491881475972128]
	TIME [epoch: 8.11 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1813785515598439		[learning rate: 0.0022816]
		[batch 20/20] avg loss: 0.201638136741708		[learning rate: 0.0022774]
	Learning Rate: 0.00227744
	LOSS [training: 0.19150834415077594 | validation: 0.19234228029637185]
	TIME [epoch: 8.13 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.241239420886789		[learning rate: 0.0022733]
		[batch 20/20] avg loss: 0.21034489088734803		[learning rate: 0.0022692]
	Learning Rate: 0.00226918
	LOSS [training: 0.22579215588706852 | validation: 0.12051741615402436]
	TIME [epoch: 8.12 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18322633381985323		[learning rate: 0.0022651]
		[batch 20/20] avg loss: 0.1400386007966633		[learning rate: 0.0022609]
	Learning Rate: 0.00226094
	LOSS [training: 0.16163246730825823 | validation: 0.10376639831747844]
	TIME [epoch: 8.1 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11500166132394307		[learning rate: 0.0022568]
		[batch 20/20] avg loss: 0.1937275261593289		[learning rate: 0.0022527]
	Learning Rate: 0.00225274
	LOSS [training: 0.15436459374163602 | validation: 0.13165627329188803]
	TIME [epoch: 8.11 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13000166875836455		[learning rate: 0.0022486]
		[batch 20/20] avg loss: 0.11359331470743891		[learning rate: 0.0022446]
	Learning Rate: 0.00224456
	LOSS [training: 0.12179749173290173 | validation: 0.1074136958592247]
	TIME [epoch: 8.11 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15404042032040804		[learning rate: 0.0022405]
		[batch 20/20] avg loss: 0.17307038491759852		[learning rate: 0.0022364]
	Learning Rate: 0.00223642
	LOSS [training: 0.16355540261900325 | validation: 0.15227401875715735]
	TIME [epoch: 8.14 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17512513318290995		[learning rate: 0.0022324]
		[batch 20/20] avg loss: 0.16692259798482972		[learning rate: 0.0022283]
	Learning Rate: 0.0022283
	LOSS [training: 0.17102386558386987 | validation: 0.19524831937013482]
	TIME [epoch: 8.11 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15427135137714393		[learning rate: 0.0022243]
		[batch 20/20] avg loss: 0.18967261584851788		[learning rate: 0.0022202]
	Learning Rate: 0.00222021
	LOSS [training: 0.1719719836128309 | validation: 0.10188132801342387]
	TIME [epoch: 8.11 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12977065307057772		[learning rate: 0.0022162]
		[batch 20/20] avg loss: 0.1250590807325916		[learning rate: 0.0022122]
	Learning Rate: 0.00221216
	LOSS [training: 0.12741486690158466 | validation: 0.24044848570755048]
	TIME [epoch: 8.11 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15720181977192807		[learning rate: 0.0022081]
		[batch 20/20] avg loss: 0.1380437271850719		[learning rate: 0.0022041]
	Learning Rate: 0.00220413
	LOSS [training: 0.14762277347849997 | validation: 0.21906929084826635]
	TIME [epoch: 8.13 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15043953395311765		[learning rate: 0.0022001]
		[batch 20/20] avg loss: 0.14343647724724728		[learning rate: 0.0021961]
	Learning Rate: 0.00219613
	LOSS [training: 0.1469380056001825 | validation: 0.16802500071317528]
	TIME [epoch: 8.12 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13909018896440045		[learning rate: 0.0021921]
		[batch 20/20] avg loss: 0.14726740336104918		[learning rate: 0.0021882]
	Learning Rate: 0.00218816
	LOSS [training: 0.14317879616272483 | validation: 0.14359123495134338]
	TIME [epoch: 8.11 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1549350331983557		[learning rate: 0.0021842]
		[batch 20/20] avg loss: 0.13256663876238284		[learning rate: 0.0021802]
	Learning Rate: 0.00218022
	LOSS [training: 0.14375083598036925 | validation: 0.14949876630854206]
	TIME [epoch: 8.1 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14352001680495047		[learning rate: 0.0021763]
		[batch 20/20] avg loss: 0.10211791718661922		[learning rate: 0.0021723]
	Learning Rate: 0.00217231
	LOSS [training: 0.12281896699578485 | validation: 0.14729660813706277]
	TIME [epoch: 8.11 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11691634100222525		[learning rate: 0.0021684]
		[batch 20/20] avg loss: 0.12806570603520412		[learning rate: 0.0021644]
	Learning Rate: 0.00216442
	LOSS [training: 0.12249102351871469 | validation: 0.1375862422564964]
	TIME [epoch: 8.14 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15623881551935803		[learning rate: 0.0021605]
		[batch 20/20] avg loss: 0.19588729362885274		[learning rate: 0.0021566]
	Learning Rate: 0.00215657
	LOSS [training: 0.17606305457410537 | validation: 0.07950039565629696]
	TIME [epoch: 8.11 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14883589251747747		[learning rate: 0.0021527]
		[batch 20/20] avg loss: 0.11852835358940499		[learning rate: 0.0021487]
	Learning Rate: 0.00214874
	LOSS [training: 0.13368212305344124 | validation: 0.1436156439678662]
	TIME [epoch: 8.11 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14941866458059277		[learning rate: 0.0021448]
		[batch 20/20] avg loss: 0.19516862815107633		[learning rate: 0.0021409]
	Learning Rate: 0.00214094
	LOSS [training: 0.1722936463658345 | validation: 0.10369169455222309]
	TIME [epoch: 8.12 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15646947487725477		[learning rate: 0.0021371]
		[batch 20/20] avg loss: 0.1654118701451659		[learning rate: 0.0021332]
	Learning Rate: 0.00213317
	LOSS [training: 0.16094067251121033 | validation: 0.17175223436497605]
	TIME [epoch: 8.13 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1294412686140262		[learning rate: 0.0021293]
		[batch 20/20] avg loss: 0.1773862779327287		[learning rate: 0.0021254]
	Learning Rate: 0.00212543
	LOSS [training: 0.15341377327337744 | validation: 0.08483164835628204]
	TIME [epoch: 8.12 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10556280671413924		[learning rate: 0.0021216]
		[batch 20/20] avg loss: 0.15411792870684066		[learning rate: 0.0021177]
	Learning Rate: 0.00211772
	LOSS [training: 0.12984036771048996 | validation: 0.10804558309356341]
	TIME [epoch: 8.11 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1709926962294475		[learning rate: 0.0021139]
		[batch 20/20] avg loss: 0.13203072643543295		[learning rate: 0.00211]
	Learning Rate: 0.00211003
	LOSS [training: 0.15151171133244024 | validation: 0.09942895730908774]
	TIME [epoch: 8.12 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2053242247540034		[learning rate: 0.0021062]
		[batch 20/20] avg loss: 0.16541792730572685		[learning rate: 0.0021024]
	Learning Rate: 0.00210238
	LOSS [training: 0.18537107602986513 | validation: 0.10163179831459519]
	TIME [epoch: 8.11 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12872255392701423		[learning rate: 0.0020986]
		[batch 20/20] avg loss: 0.14104410616162294		[learning rate: 0.0020947]
	Learning Rate: 0.00209475
	LOSS [training: 0.13488333004431854 | validation: 0.14750138741640845]
	TIME [epoch: 8.14 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1580121880145499		[learning rate: 0.0020909]
		[batch 20/20] avg loss: 0.11696847093387967		[learning rate: 0.0020871]
	Learning Rate: 0.00208714
	LOSS [training: 0.1374903294742148 | validation: 0.08666347372142025]
	TIME [epoch: 8.11 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1429798626341902		[learning rate: 0.0020834]
		[batch 20/20] avg loss: 0.23329941269511698		[learning rate: 0.0020796]
	Learning Rate: 0.00207957
	LOSS [training: 0.18813963766465364 | validation: 0.5038783692317251]
	TIME [epoch: 8.12 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25804180411161504		[learning rate: 0.0020758]
		[batch 20/20] avg loss: 0.1087874756499027		[learning rate: 0.002072]
	Learning Rate: 0.00207202
	LOSS [training: 0.18341463988075885 | validation: 0.10434982676523052]
	TIME [epoch: 8.13 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16467474019990175		[learning rate: 0.0020683]
		[batch 20/20] avg loss: 0.1764308264406899		[learning rate: 0.0020645]
	Learning Rate: 0.0020645
	LOSS [training: 0.17055278332029583 | validation: 0.16229794396605243]
	TIME [epoch: 8.13 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18249179512056846		[learning rate: 0.0020608]
		[batch 20/20] avg loss: 0.20134206533140722		[learning rate: 0.002057]
	Learning Rate: 0.00205701
	LOSS [training: 0.19191693022598785 | validation: 0.23557093111656252]
	TIME [epoch: 8.12 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11943818144577023		[learning rate: 0.0020533]
		[batch 20/20] avg loss: 0.12618709462198704		[learning rate: 0.0020495]
	Learning Rate: 0.00204955
	LOSS [training: 0.12281263803387862 | validation: 0.14865546311745378]
	TIME [epoch: 8.11 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38963443183950186		[learning rate: 0.0020458]
		[batch 20/20] avg loss: 0.16497035852581088		[learning rate: 0.0020421]
	Learning Rate: 0.00204211
	LOSS [training: 0.2773023951826564 | validation: 0.11576126758858794]
	TIME [epoch: 8.11 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1567052863907239		[learning rate: 0.0020384]
		[batch 20/20] avg loss: 0.13468886979520575		[learning rate: 0.0020347]
	Learning Rate: 0.0020347
	LOSS [training: 0.1456970780929648 | validation: 0.18644363682119164]
	TIME [epoch: 8.12 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12024291690735489		[learning rate: 0.002031]
		[batch 20/20] avg loss: 0.1277009441348404		[learning rate: 0.0020273]
	Learning Rate: 0.00202731
	LOSS [training: 0.12397193052109765 | validation: 0.10722182979298933]
	TIME [epoch: 8.14 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09077690948236533		[learning rate: 0.0020236]
		[batch 20/20] avg loss: 0.10527775641914691		[learning rate: 0.00202]
	Learning Rate: 0.00201996
	LOSS [training: 0.09802733295075614 | validation: 0.12167800312305375]
	TIME [epoch: 8.12 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12953936162403834		[learning rate: 0.0020163]
		[batch 20/20] avg loss: 0.1744165576761366		[learning rate: 0.0020126]
	Learning Rate: 0.00201263
	LOSS [training: 0.15197795965008742 | validation: 0.11554959457797809]
	TIME [epoch: 8.11 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1548554793005773		[learning rate: 0.002009]
		[batch 20/20] avg loss: 0.12247764795850984		[learning rate: 0.0020053]
	Learning Rate: 0.00200532
	LOSS [training: 0.1386665636295436 | validation: 0.17255696350374705]
	TIME [epoch: 8.12 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1257841321985727		[learning rate: 0.0020017]
		[batch 20/20] avg loss: 0.10752065982935488		[learning rate: 0.001998]
	Learning Rate: 0.00199805
	LOSS [training: 0.11665239601396378 | validation: 0.08045567371946277]
	TIME [epoch: 8.14 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10795045146049491		[learning rate: 0.0019944]
		[batch 20/20] avg loss: 0.15044757273559378		[learning rate: 0.0019908]
	Learning Rate: 0.00199079
	LOSS [training: 0.12919901209804435 | validation: 0.11196629430072678]
	TIME [epoch: 8.12 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19432385406330438		[learning rate: 0.0019872]
		[batch 20/20] avg loss: 0.2251408495488092		[learning rate: 0.0019836]
	Learning Rate: 0.00198357
	LOSS [training: 0.2097323518060567 | validation: 0.2437239128517177]
	TIME [epoch: 8.11 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1584126189511575		[learning rate: 0.00198]
		[batch 20/20] avg loss: 0.16298059854213717		[learning rate: 0.0019764]
	Learning Rate: 0.00197637
	LOSS [training: 0.16069660874664732 | validation: 0.11394458632071675]
	TIME [epoch: 8.12 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15381720174501567		[learning rate: 0.0019728]
		[batch 20/20] avg loss: 0.1489454176629702		[learning rate: 0.0019692]
	Learning Rate: 0.0019692
	LOSS [training: 0.15138130970399294 | validation: 0.11822533403290797]
	TIME [epoch: 8.11 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19146757091829134		[learning rate: 0.0019656]
		[batch 20/20] avg loss: 0.1547789191802107		[learning rate: 0.0019621]
	Learning Rate: 0.00196205
	LOSS [training: 0.17312324504925106 | validation: 0.1518066799190286]
	TIME [epoch: 8.15 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1778437653832841		[learning rate: 0.0019585]
		[batch 20/20] avg loss: 0.20740848803716444		[learning rate: 0.0019549]
	Learning Rate: 0.00195493
	LOSS [training: 0.19262612671022428 | validation: 0.18834170587030552]
	TIME [epoch: 8.12 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15733036923066424		[learning rate: 0.0019514]
		[batch 20/20] avg loss: 0.15755338864968702		[learning rate: 0.0019478]
	Learning Rate: 0.00194784
	LOSS [training: 0.15744187894017564 | validation: 0.110928651027605]
	TIME [epoch: 8.11 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16320931950261025		[learning rate: 0.0019443]
		[batch 20/20] avg loss: 0.12861591867016905		[learning rate: 0.0019408]
	Learning Rate: 0.00194077
	LOSS [training: 0.1459126190863897 | validation: 0.18892638485436256]
	TIME [epoch: 8.12 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11309570093884813		[learning rate: 0.0019372]
		[batch 20/20] avg loss: 0.12939875990691935		[learning rate: 0.0019337]
	Learning Rate: 0.00193373
	LOSS [training: 0.12124723042288377 | validation: 0.06144306010451246]
	TIME [epoch: 8.14 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11429923358190106		[learning rate: 0.0019302]
		[batch 20/20] avg loss: 0.13280954511907128		[learning rate: 0.0019267]
	Learning Rate: 0.00192671
	LOSS [training: 0.12355438935048615 | validation: 0.13052878313218105]
	TIME [epoch: 8.12 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15047567954039925		[learning rate: 0.0019232]
		[batch 20/20] avg loss: 0.13238788569556165		[learning rate: 0.0019197]
	Learning Rate: 0.00191972
	LOSS [training: 0.14143178261798045 | validation: 0.13911615035656355]
	TIME [epoch: 8.11 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12507901770519533		[learning rate: 0.0019162]
		[batch 20/20] avg loss: 0.1098174966550695		[learning rate: 0.0019127]
	Learning Rate: 0.00191275
	LOSS [training: 0.11744825718013241 | validation: 0.14011103870179029]
	TIME [epoch: 8.11 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11320322132965661		[learning rate: 0.0019093]
		[batch 20/20] avg loss: 0.10892646737885973		[learning rate: 0.0019058]
	Learning Rate: 0.00190581
	LOSS [training: 0.11106484435425815 | validation: 0.09065219794758309]
	TIME [epoch: 8.12 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10032285767581735		[learning rate: 0.0019023]
		[batch 20/20] avg loss: 0.08008822043401814		[learning rate: 0.0018989]
	Learning Rate: 0.00189889
	LOSS [training: 0.09020553905491774 | validation: 0.08740435306145715]
	TIME [epoch: 8.14 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11557173621142278		[learning rate: 0.0018954]
		[batch 20/20] avg loss: 0.09244602375549121		[learning rate: 0.001892]
	Learning Rate: 0.001892
	LOSS [training: 0.10400887998345701 | validation: 0.06451132700778678]
	TIME [epoch: 8.11 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09338579625766796		[learning rate: 0.0018886]
		[batch 20/20] avg loss: 0.15063309757574828		[learning rate: 0.0018851]
	Learning Rate: 0.00188513
	LOSS [training: 0.12200944691670812 | validation: 0.09382908279518493]
	TIME [epoch: 8.12 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11431066726248766		[learning rate: 0.0018817]
		[batch 20/20] avg loss: 0.1254462569318483		[learning rate: 0.0018783]
	Learning Rate: 0.00187829
	LOSS [training: 0.11987846209716799 | validation: 0.07856855503607871]
	TIME [epoch: 8.11 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0950285241895211		[learning rate: 0.0018749]
		[batch 20/20] avg loss: 0.11278719271456827		[learning rate: 0.0018715]
	Learning Rate: 0.00187148
	LOSS [training: 0.10390785845204469 | validation: 0.09792959935630421]
	TIME [epoch: 8.14 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09064652424168426		[learning rate: 0.0018681]
		[batch 20/20] avg loss: 0.08598824888311593		[learning rate: 0.0018647]
	Learning Rate: 0.00186468
	LOSS [training: 0.0883173865624001 | validation: 0.08098070381328162]
	TIME [epoch: 8.12 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0717163561617922		[learning rate: 0.0018613]
		[batch 20/20] avg loss: 0.17259174018200182		[learning rate: 0.0018579]
	Learning Rate: 0.00185792
	LOSS [training: 0.12215404817189701 | validation: 0.11638315770851618]
	TIME [epoch: 8.11 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1218900979544171		[learning rate: 0.0018545]
		[batch 20/20] avg loss: 0.11317094559666505		[learning rate: 0.0018512]
	Learning Rate: 0.00185117
	LOSS [training: 0.11753052177554106 | validation: 0.09156355059733445]
	TIME [epoch: 8.12 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1354674896464976		[learning rate: 0.0018478]
		[batch 20/20] avg loss: 0.08092236956182891		[learning rate: 0.0018445]
	Learning Rate: 0.00184446
	LOSS [training: 0.10819492960416326 | validation: 0.05363697073531698]
	TIME [epoch: 8.12 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08842092098851331		[learning rate: 0.0018411]
		[batch 20/20] avg loss: 0.14324333320642		[learning rate: 0.0018378]
	Learning Rate: 0.00183776
	LOSS [training: 0.11583212709746663 | validation: 0.08301311303511945]
	TIME [epoch: 8.13 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11032476809345355		[learning rate: 0.0018344]
		[batch 20/20] avg loss: 0.10197450946476645		[learning rate: 0.0018311]
	Learning Rate: 0.00183109
	LOSS [training: 0.10614963877911002 | validation: 0.08214769091185603]
	TIME [epoch: 8.11 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.109376246663212		[learning rate: 0.0018278]
		[batch 20/20] avg loss: 0.08170578588638219		[learning rate: 0.0018244]
	Learning Rate: 0.00182445
	LOSS [training: 0.09554101627479712 | validation: 0.13921844419511192]
	TIME [epoch: 8.11 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14989359628818827		[learning rate: 0.0018211]
		[batch 20/20] avg loss: 0.1467884774170153		[learning rate: 0.0018178]
	Learning Rate: 0.00181783
	LOSS [training: 0.14834103685260178 | validation: 0.10448817606275557]
	TIME [epoch: 8.11 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11200139022616817		[learning rate: 0.0018145]
		[batch 20/20] avg loss: 0.13216476182087297		[learning rate: 0.0018112]
	Learning Rate: 0.00181123
	LOSS [training: 0.1220830760235206 | validation: 0.11826362890013396]
	TIME [epoch: 8.15 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12457084380635397		[learning rate: 0.0018079]
		[batch 20/20] avg loss: 0.13549841861046552		[learning rate: 0.0018047]
	Learning Rate: 0.00180466
	LOSS [training: 0.13003463120840975 | validation: 0.08458207973121296]
	TIME [epoch: 8.12 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13090549386303607		[learning rate: 0.0018014]
		[batch 20/20] avg loss: 0.10409154890566938		[learning rate: 0.0017981]
	Learning Rate: 0.00179811
	LOSS [training: 0.11749852138435272 | validation: 0.06995712943094054]
	TIME [epoch: 8.12 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1024191721197802		[learning rate: 0.0017948]
		[batch 20/20] avg loss: 0.10107884010943177		[learning rate: 0.0017916]
	Learning Rate: 0.00179158
	LOSS [training: 0.101749006114606 | validation: 0.37926243202990195]
	TIME [epoch: 8.11 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17339121283024045		[learning rate: 0.0017883]
		[batch 20/20] avg loss: 0.09194470981494894		[learning rate: 0.0017851]
	Learning Rate: 0.00178508
	LOSS [training: 0.13266796132259467 | validation: 0.06678593196225385]
	TIME [epoch: 8.13 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12207211335240134		[learning rate: 0.0017818]
		[batch 20/20] avg loss: 0.10194366583375729		[learning rate: 0.0017786]
	Learning Rate: 0.0017786
	LOSS [training: 0.11200788959307931 | validation: 0.07259032781572694]
	TIME [epoch: 8.13 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09268684884335615		[learning rate: 0.0017754]
		[batch 20/20] avg loss: 0.12101952780732608		[learning rate: 0.0017721]
	Learning Rate: 0.00177215
	LOSS [training: 0.1068531883253411 | validation: 0.10320504260243507]
	TIME [epoch: 8.12 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15603959336561682		[learning rate: 0.0017689]
		[batch 20/20] avg loss: 0.10571828456521379		[learning rate: 0.0017657]
	Learning Rate: 0.00176572
	LOSS [training: 0.1308789389654153 | validation: 0.10833566959509466]
	TIME [epoch: 8.12 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1250658382789758		[learning rate: 0.0017625]
		[batch 20/20] avg loss: 0.10116716549619376		[learning rate: 0.0017593]
	Learning Rate: 0.00175931
	LOSS [training: 0.11311650188758478 | validation: 0.11790652508478214]
	TIME [epoch: 8.12 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10681555067347963		[learning rate: 0.0017561]
		[batch 20/20] avg loss: 0.144209801799669		[learning rate: 0.0017529]
	Learning Rate: 0.00175292
	LOSS [training: 0.1255126762365743 | validation: 0.09149444310990162]
	TIME [epoch: 8.14 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10366296371643613		[learning rate: 0.0017497]
		[batch 20/20] avg loss: 0.08541039531789599		[learning rate: 0.0017466]
	Learning Rate: 0.00174656
	LOSS [training: 0.09453667951716606 | validation: 0.05099521453285638]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_580.pth
	Model improved!!!
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08576351336678473		[learning rate: 0.0017434]
		[batch 20/20] avg loss: 0.10869718190866132		[learning rate: 0.0017402]
	Learning Rate: 0.00174022
	LOSS [training: 0.09723034763772301 | validation: 0.11975997690853188]
	TIME [epoch: 8.11 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11070355616813968		[learning rate: 0.0017371]
		[batch 20/20] avg loss: 0.13704832320358684		[learning rate: 0.0017339]
	Learning Rate: 0.00173391
	LOSS [training: 0.12387593968586326 | validation: 0.20773974047866905]
	TIME [epoch: 8.09 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1053843207473237		[learning rate: 0.0017308]
		[batch 20/20] avg loss: 0.12920525918642892		[learning rate: 0.0017276]
	Learning Rate: 0.00172762
	LOSS [training: 0.11729478996687633 | validation: 0.08710588142441383]
	TIME [epoch: 8.13 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11431691130438641		[learning rate: 0.0017245]
		[batch 20/20] avg loss: 0.14788582217928153		[learning rate: 0.0017213]
	Learning Rate: 0.00172135
	LOSS [training: 0.13110136674183392 | validation: 0.10968830428203863]
	TIME [epoch: 8.11 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19922641052903806		[learning rate: 0.0017182]
		[batch 20/20] avg loss: 0.23737036206857937		[learning rate: 0.0017151]
	Learning Rate: 0.0017151
	LOSS [training: 0.21829838629880868 | validation: 0.1227854423632774]
	TIME [epoch: 8.1 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13571740027414708		[learning rate: 0.001712]
		[batch 20/20] avg loss: 0.18382592629766142		[learning rate: 0.0017089]
	Learning Rate: 0.00170888
	LOSS [training: 0.15977166328590425 | validation: 0.1309052639916252]
	TIME [epoch: 8.11 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17564178346603804		[learning rate: 0.0017058]
		[batch 20/20] avg loss: 0.1392464203595711		[learning rate: 0.0017027]
	Learning Rate: 0.00170267
	LOSS [training: 0.15744410191280456 | validation: 0.1452634188592985]
	TIME [epoch: 8.11 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1348817303804251		[learning rate: 0.0016996]
		[batch 20/20] avg loss: 0.12103889456759312		[learning rate: 0.0016965]
	Learning Rate: 0.0016965
	LOSS [training: 0.12796031247400913 | validation: 0.07453033236996834]
	TIME [epoch: 8.14 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1325558156848847		[learning rate: 0.0016934]
		[batch 20/20] avg loss: 0.1293276493002766		[learning rate: 0.0016903]
	Learning Rate: 0.00169034
	LOSS [training: 0.13094173249258065 | validation: 0.11268180918180903]
	TIME [epoch: 8.1 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09804633692663148		[learning rate: 0.0016873]
		[batch 20/20] avg loss: 0.102985700391974		[learning rate: 0.0016842]
	Learning Rate: 0.0016842
	LOSS [training: 0.10051601865930274 | validation: 0.06425291563317936]
	TIME [epoch: 8.11 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07941802975084866		[learning rate: 0.0016811]
		[batch 20/20] avg loss: 0.10087585589769402		[learning rate: 0.0016781]
	Learning Rate: 0.00167809
	LOSS [training: 0.09014694282427134 | validation: 0.11173163525860946]
	TIME [epoch: 8.1 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13047402292026863		[learning rate: 0.001675]
		[batch 20/20] avg loss: 0.1375576218838043		[learning rate: 0.001672]
	Learning Rate: 0.001672
	LOSS [training: 0.13401582240203647 | validation: 0.2992034971508251]
	TIME [epoch: 8.12 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1491244060964314		[learning rate: 0.001669]
		[batch 20/20] avg loss: 0.07941207595212497		[learning rate: 0.0016659]
	Learning Rate: 0.00166593
	LOSS [training: 0.1142682410242782 | validation: 0.055427326020604566]
	TIME [epoch: 8.12 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0753160020284058		[learning rate: 0.0016629]
		[batch 20/20] avg loss: 0.12981504773890445		[learning rate: 0.0016599]
	Learning Rate: 0.00165989
	LOSS [training: 0.10256552488365514 | validation: 0.08392917589276035]
	TIME [epoch: 8.11 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09433655370463659		[learning rate: 0.0016569]
		[batch 20/20] avg loss: 0.08384163863500983		[learning rate: 0.0016539]
	Learning Rate: 0.00165387
	LOSS [training: 0.08908909616982322 | validation: 0.06956272040251353]
	TIME [epoch: 8.1 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08963687197990695		[learning rate: 0.0016509]
		[batch 20/20] avg loss: 0.09597237247826072		[learning rate: 0.0016479]
	Learning Rate: 0.00164786
	LOSS [training: 0.09280462222908384 | validation: 0.07489316872014146]
	TIME [epoch: 8.1 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09992393870489448		[learning rate: 0.0016449]
		[batch 20/20] avg loss: 0.11241559449629909		[learning rate: 0.0016419]
	Learning Rate: 0.00164188
	LOSS [training: 0.10616976660059678 | validation: 0.07862853764211676]
	TIME [epoch: 8.13 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09118620029583216		[learning rate: 0.0016389]
		[batch 20/20] avg loss: 0.11174059906830516		[learning rate: 0.0016359]
	Learning Rate: 0.00163592
	LOSS [training: 0.10146339968206866 | validation: 0.1111257396664437]
	TIME [epoch: 8.11 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09899537077325485		[learning rate: 0.001633]
		[batch 20/20] avg loss: 0.09265004839830522		[learning rate: 0.00163]
	Learning Rate: 0.00162999
	LOSS [training: 0.09582270958578003 | validation: 0.05825071290907216]
	TIME [epoch: 8.11 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07283287916590339		[learning rate: 0.001627]
		[batch 20/20] avg loss: 0.13537200154893336		[learning rate: 0.0016241]
	Learning Rate: 0.00162407
	LOSS [training: 0.10410244035741838 | validation: 0.07084016009685376]
	TIME [epoch: 8.1 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08512009050393082		[learning rate: 0.0016211]
		[batch 20/20] avg loss: 0.08899547202238393		[learning rate: 0.0016182]
	Learning Rate: 0.00161818
	LOSS [training: 0.08705778126315736 | validation: 0.12553923865401556]
	TIME [epoch: 8.12 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10355674569130631		[learning rate: 0.0016152]
		[batch 20/20] avg loss: 0.07013694122342837		[learning rate: 0.0016123]
	Learning Rate: 0.00161231
	LOSS [training: 0.08684684345736732 | validation: 0.07888183798206258]
	TIME [epoch: 8.12 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09966321700454417		[learning rate: 0.0016094]
		[batch 20/20] avg loss: 0.10068330351860266		[learning rate: 0.0016065]
	Learning Rate: 0.00160645
	LOSS [training: 0.1001732602615734 | validation: 0.0670938516541064]
	TIME [epoch: 8.1 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10034961361306498		[learning rate: 0.0016035]
		[batch 20/20] avg loss: 0.12542683243984615		[learning rate: 0.0016006]
	Learning Rate: 0.00160062
	LOSS [training: 0.11288822302645556 | validation: 0.13432588751089886]
	TIME [epoch: 8.11 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0916208532997668		[learning rate: 0.0015977]
		[batch 20/20] avg loss: 0.08759036079929486		[learning rate: 0.0015948]
	Learning Rate: 0.00159482
	LOSS [training: 0.08960560704953084 | validation: 0.06485877376608842]
	TIME [epoch: 8.11 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08462186973354735		[learning rate: 0.0015919]
		[batch 20/20] avg loss: 0.1256496482470974		[learning rate: 0.001589]
	Learning Rate: 0.00158903
	LOSS [training: 0.10513575899032238 | validation: 0.23939424871703463]
	TIME [epoch: 8.14 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11659392860083964		[learning rate: 0.0015861]
		[batch 20/20] avg loss: 0.2762423868887296		[learning rate: 0.0015833]
	Learning Rate: 0.00158326
	LOSS [training: 0.1964181577447846 | validation: 0.12503643564153727]
	TIME [epoch: 8.11 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11381212040346349		[learning rate: 0.0015804]
		[batch 20/20] avg loss: 0.07647181169563047		[learning rate: 0.0015775]
	Learning Rate: 0.00157752
	LOSS [training: 0.09514196604954697 | validation: 0.0920677841378946]
	TIME [epoch: 8.11 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08011196183786223		[learning rate: 0.0015747]
		[batch 20/20] avg loss: 0.09292671862417531		[learning rate: 0.0015718]
	Learning Rate: 0.00157179
	LOSS [training: 0.08651934023101877 | validation: 0.09187015886900435]
	TIME [epoch: 8.11 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09802511935964572		[learning rate: 0.0015689]
		[batch 20/20] avg loss: 0.08935408030338855		[learning rate: 0.0015661]
	Learning Rate: 0.00156609
	LOSS [training: 0.09368959983151712 | validation: 0.07898709028566123]
	TIME [epoch: 8.13 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11569576107674721		[learning rate: 0.0015632]
		[batch 20/20] avg loss: 0.12499537914389995		[learning rate: 0.0015604]
	Learning Rate: 0.0015604
	LOSS [training: 0.12034557011032358 | validation: 0.1063714617641886]
	TIME [epoch: 8.11 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11085503659891087		[learning rate: 0.0015576]
		[batch 20/20] avg loss: 0.12277544471891247		[learning rate: 0.0015547]
	Learning Rate: 0.00155474
	LOSS [training: 0.11681524065891166 | validation: 0.09305632519082871]
	TIME [epoch: 8.11 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09355957855403858		[learning rate: 0.0015519]
		[batch 20/20] avg loss: 0.08599388316082719		[learning rate: 0.0015491]
	Learning Rate: 0.0015491
	LOSS [training: 0.08977673085743287 | validation: 0.06644204390774329]
	TIME [epoch: 8.11 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07136679808531271		[learning rate: 0.0015463]
		[batch 20/20] avg loss: 0.0880897682595871		[learning rate: 0.0015435]
	Learning Rate: 0.00154348
	LOSS [training: 0.0797282831724499 | validation: 0.08304752718673042]
	TIME [epoch: 8.11 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07330425800237715		[learning rate: 0.0015407]
		[batch 20/20] avg loss: 0.08855057354316269		[learning rate: 0.0015379]
	Learning Rate: 0.00153788
	LOSS [training: 0.0809274157727699 | validation: 0.12816381351832284]
	TIME [epoch: 8.13 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14280329007396572		[learning rate: 0.0015351]
		[batch 20/20] avg loss: 0.13434165038175938		[learning rate: 0.0015323]
	Learning Rate: 0.00153229
	LOSS [training: 0.13857247022786257 | validation: 0.1043224530076554]
	TIME [epoch: 8.11 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13500510842569943		[learning rate: 0.0015295]
		[batch 20/20] avg loss: 0.1380462480012294		[learning rate: 0.0015267]
	Learning Rate: 0.00152673
	LOSS [training: 0.1365256782134644 | validation: 0.12002669910146901]
	TIME [epoch: 8.11 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09243957761507549		[learning rate: 0.001524]
		[batch 20/20] avg loss: 0.07142602615774216		[learning rate: 0.0015212]
	Learning Rate: 0.00152119
	LOSS [training: 0.08193280188640881 | validation: 0.06578130803584634]
	TIME [epoch: 8.11 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11405819397921131		[learning rate: 0.0015184]
		[batch 20/20] avg loss: 0.10816022427561034		[learning rate: 0.0015157]
	Learning Rate: 0.00151567
	LOSS [training: 0.11110920912741082 | validation: 0.08913264313489017]
	TIME [epoch: 8.14 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06651558127271437		[learning rate: 0.0015129]
		[batch 20/20] avg loss: 0.07873695521054314		[learning rate: 0.0015102]
	Learning Rate: 0.00151017
	LOSS [training: 0.07262626824162874 | validation: 0.19362565051789998]
	TIME [epoch: 8.11 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09543846758728297		[learning rate: 0.0015074]
		[batch 20/20] avg loss: 0.07577220094145443		[learning rate: 0.0015047]
	Learning Rate: 0.00150469
	LOSS [training: 0.08560533426436871 | validation: 0.0699991424943928]
	TIME [epoch: 8.11 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06437790617880113		[learning rate: 0.001502]
		[batch 20/20] avg loss: 0.05799506859362961		[learning rate: 0.0014992]
	Learning Rate: 0.00149923
	LOSS [training: 0.061186487386215384 | validation: 0.11194225788512238]
	TIME [epoch: 8.1 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06866301529098906		[learning rate: 0.0014965]
		[batch 20/20] avg loss: 0.07971376138272943		[learning rate: 0.0014938]
	Learning Rate: 0.00149379
	LOSS [training: 0.07418838833685923 | validation: 0.08744387635835224]
	TIME [epoch: 8.11 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07669539404791856		[learning rate: 0.0014911]
		[batch 20/20] avg loss: 0.08369231155518227		[learning rate: 0.0014884]
	Learning Rate: 0.00148837
	LOSS [training: 0.08019385280155039 | validation: 0.07653411212412302]
	TIME [epoch: 8.13 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06200887321923371		[learning rate: 0.0014857]
		[batch 20/20] avg loss: 0.10123817402092508		[learning rate: 0.001483]
	Learning Rate: 0.00148297
	LOSS [training: 0.08162352362007938 | validation: 0.05315377365424854]
	TIME [epoch: 8.1 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10438799569879338		[learning rate: 0.0014803]
		[batch 20/20] avg loss: 0.1091767810799319		[learning rate: 0.0014776]
	Learning Rate: 0.00147759
	LOSS [training: 0.10678238838936265 | validation: 0.17100688217355056]
	TIME [epoch: 8.1 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09913373633154803		[learning rate: 0.0014749]
		[batch 20/20] avg loss: 0.11655080663224912		[learning rate: 0.0014722]
	Learning Rate: 0.00147222
	LOSS [training: 0.10784227148189858 | validation: 0.06961720038005531]
	TIME [epoch: 8.11 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0926892257594177		[learning rate: 0.0014695]
		[batch 20/20] avg loss: 0.064205183538928		[learning rate: 0.0014669]
	Learning Rate: 0.00146688
	LOSS [training: 0.07844720464917285 | validation: 0.06316194426306428]
	TIME [epoch: 8.14 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11293772084127333		[learning rate: 0.0014642]
		[batch 20/20] avg loss: 0.21548269597278394		[learning rate: 0.0014616]
	Learning Rate: 0.00146156
	LOSS [training: 0.16421020840702863 | validation: 0.08906636971428247]
	TIME [epoch: 8.11 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10354246034072405		[learning rate: 0.0014589]
		[batch 20/20] avg loss: 0.06097913537756326		[learning rate: 0.0014563]
	Learning Rate: 0.00145625
	LOSS [training: 0.08226079785914367 | validation: 0.06572466525643064]
	TIME [epoch: 8.1 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0708597568819464		[learning rate: 0.0014536]
		[batch 20/20] avg loss: 0.08853403671582616		[learning rate: 0.001451]
	Learning Rate: 0.00145097
	LOSS [training: 0.07969689679888628 | validation: 0.11950985466871318]
	TIME [epoch: 8.11 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.080974629252657		[learning rate: 0.0014483]
		[batch 20/20] avg loss: 0.16281988236397477		[learning rate: 0.0014457]
	Learning Rate: 0.0014457
	LOSS [training: 0.12189725580831587 | validation: 0.10598049009894865]
	TIME [epoch: 8.12 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11101277478436214		[learning rate: 0.0014431]
		[batch 20/20] avg loss: 0.0749113266417146		[learning rate: 0.0014405]
	Learning Rate: 0.00144046
	LOSS [training: 0.09296205071303835 | validation: 0.08488677738937177]
	TIME [epoch: 8.12 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09384093487559043		[learning rate: 0.0014378]
		[batch 20/20] avg loss: 0.07755073591843331		[learning rate: 0.0014352]
	Learning Rate: 0.00143523
	LOSS [training: 0.08569583539701187 | validation: 0.09488770924900183]
	TIME [epoch: 8.1 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10218311593229508		[learning rate: 0.0014326]
		[batch 20/20] avg loss: 0.10853015731724887		[learning rate: 0.00143]
	Learning Rate: 0.00143002
	LOSS [training: 0.10535663662477197 | validation: 0.11241225314061112]
	TIME [epoch: 8.1 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0899099617135852		[learning rate: 0.0014274]
		[batch 20/20] avg loss: 0.11936537721400305		[learning rate: 0.0014248]
	Learning Rate: 0.00142483
	LOSS [training: 0.10463766946379413 | validation: 0.11142652528259638]
	TIME [epoch: 8.11 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07019754036738156		[learning rate: 0.0014222]
		[batch 20/20] avg loss: 0.05298524622487011		[learning rate: 0.0014197]
	Learning Rate: 0.00141966
	LOSS [training: 0.06159139329612584 | validation: 0.0688548645576419]
	TIME [epoch: 8.14 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07399753375899501		[learning rate: 0.0014171]
		[batch 20/20] avg loss: 0.12008952476488441		[learning rate: 0.0014145]
	Learning Rate: 0.00141451
	LOSS [training: 0.09704352926193968 | validation: 0.07546686318031323]
	TIME [epoch: 8.11 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08932562512256687		[learning rate: 0.0014119]
		[batch 20/20] avg loss: 0.10311179460867952		[learning rate: 0.0014094]
	Learning Rate: 0.00140937
	LOSS [training: 0.0962187098656232 | validation: 0.17913958648205752]
	TIME [epoch: 8.1 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09910274215160635		[learning rate: 0.0014068]
		[batch 20/20] avg loss: 0.10348704941458498		[learning rate: 0.0014043]
	Learning Rate: 0.00140426
	LOSS [training: 0.10129489578309567 | validation: 0.06120333775215677]
	TIME [epoch: 8.1 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09938061245481995		[learning rate: 0.0014017]
		[batch 20/20] avg loss: 0.08107642770302051		[learning rate: 0.0013992]
	Learning Rate: 0.00139916
	LOSS [training: 0.09022852007892024 | validation: 0.12351229325296942]
	TIME [epoch: 8.13 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08390618884158521		[learning rate: 0.0013966]
		[batch 20/20] avg loss: 0.09055262785671045		[learning rate: 0.0013941]
	Learning Rate: 0.00139409
	LOSS [training: 0.08722940834914782 | validation: 0.04896644091962189]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_642.pth
	Model improved!!!
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0705470061281572		[learning rate: 0.0013916]
		[batch 20/20] avg loss: 0.09131914454581112		[learning rate: 0.001389]
	Learning Rate: 0.00138903
	LOSS [training: 0.08093307533698416 | validation: 0.08494674471028013]
	TIME [epoch: 8.11 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07320788828606402		[learning rate: 0.0013865]
		[batch 20/20] avg loss: 0.07019322077469514		[learning rate: 0.001384]
	Learning Rate: 0.00138399
	LOSS [training: 0.07170055453037957 | validation: 0.3022566361395785]
	TIME [epoch: 8.11 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.103813817964626		[learning rate: 0.0013815]
		[batch 20/20] avg loss: 0.06000332578384625		[learning rate: 0.001379]
	Learning Rate: 0.00137896
	LOSS [training: 0.08190857187423613 | validation: 0.08683847298277608]
	TIME [epoch: 8.11 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09377670233347847		[learning rate: 0.0013765]
		[batch 20/20] avg loss: 0.10246415543110118		[learning rate: 0.001374]
	Learning Rate: 0.00137396
	LOSS [training: 0.09812042888228983 | validation: 0.06366556236749932]
	TIME [epoch: 8.14 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08895695085268587		[learning rate: 0.0013715]
		[batch 20/20] avg loss: 0.0804228705870713		[learning rate: 0.001369]
	Learning Rate: 0.00136897
	LOSS [training: 0.08468991071987858 | validation: 0.07636479027557475]
	TIME [epoch: 8.11 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07923282730927758		[learning rate: 0.0013665]
		[batch 20/20] avg loss: 0.11690735474368845		[learning rate: 0.001364]
	Learning Rate: 0.001364
	LOSS [training: 0.09807009102648302 | validation: 0.12766410509516385]
	TIME [epoch: 8.11 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08237852891800009		[learning rate: 0.0013615]
		[batch 20/20] avg loss: 0.09861835900223498		[learning rate: 0.0013591]
	Learning Rate: 0.00135905
	LOSS [training: 0.09049844396011751 | validation: 0.08437449879918715]
	TIME [epoch: 8.11 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051098285137467184		[learning rate: 0.0013566]
		[batch 20/20] avg loss: 0.07775002859749096		[learning rate: 0.0013541]
	Learning Rate: 0.00135412
	LOSS [training: 0.06442415686747907 | validation: 0.09242040969421032]
	TIME [epoch: 8.13 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08507099165733073		[learning rate: 0.0013517]
		[batch 20/20] avg loss: 0.1047146778274218		[learning rate: 0.0013492]
	Learning Rate: 0.00134921
	LOSS [training: 0.09489283474237625 | validation: 0.07987583914665881]
	TIME [epoch: 8.12 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06713435179202804		[learning rate: 0.0013468]
		[batch 20/20] avg loss: 0.08592472657857218		[learning rate: 0.0013443]
	Learning Rate: 0.00134431
	LOSS [training: 0.07652953918530012 | validation: 0.09236770523913143]
	TIME [epoch: 8.11 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11959351587892335		[learning rate: 0.0013419]
		[batch 20/20] avg loss: 0.06334293029762937		[learning rate: 0.0013394]
	Learning Rate: 0.00133943
	LOSS [training: 0.09146822308827636 | validation: 0.05276082996446998]
	TIME [epoch: 8.11 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07560271715661143		[learning rate: 0.001337]
		[batch 20/20] avg loss: 0.11086209666240505		[learning rate: 0.0013346]
	Learning Rate: 0.00133457
	LOSS [training: 0.09323240690950824 | validation: 0.06748847532858479]
	TIME [epoch: 8.11 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07592279246265234		[learning rate: 0.0013321]
		[batch 20/20] avg loss: 0.08914608583451533		[learning rate: 0.0013297]
	Learning Rate: 0.00132973
	LOSS [training: 0.08253443914858383 | validation: 0.0869493890688531]
	TIME [epoch: 8.14 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07074306059272875		[learning rate: 0.0013273]
		[batch 20/20] avg loss: 0.08110941013946363		[learning rate: 0.0013249]
	Learning Rate: 0.0013249
	LOSS [training: 0.07592623536609619 | validation: 0.051459520670755404]
	TIME [epoch: 8.11 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07885915337322545		[learning rate: 0.0013225]
		[batch 20/20] avg loss: 0.09244120947945542		[learning rate: 0.0013201]
	Learning Rate: 0.0013201
	LOSS [training: 0.08565018142634043 | validation: 0.1408035991306953]
	TIME [epoch: 8.11 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07549679999480088		[learning rate: 0.0013177]
		[batch 20/20] avg loss: 0.08110371044290479		[learning rate: 0.0013153]
	Learning Rate: 0.0013153
	LOSS [training: 0.07830025521885284 | validation: 0.06431642977120922]
	TIME [epoch: 8.11 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07679295542507615		[learning rate: 0.0013129]
		[batch 20/20] avg loss: 0.08495394406944978		[learning rate: 0.0013105]
	Learning Rate: 0.00131053
	LOSS [training: 0.08087344974726295 | validation: 0.0794525851982064]
	TIME [epoch: 8.12 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08692922834843801		[learning rate: 0.0013082]
		[batch 20/20] avg loss: 0.09749495460751942		[learning rate: 0.0013058]
	Learning Rate: 0.00130578
	LOSS [training: 0.09221209147797872 | validation: 0.10793145786554216]
	TIME [epoch: 8.12 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07231283397977907		[learning rate: 0.0013034]
		[batch 20/20] avg loss: 0.07450569469906287		[learning rate: 0.001301]
	Learning Rate: 0.00130104
	LOSS [training: 0.07340926433942098 | validation: 0.06956523487179604]
	TIME [epoch: 8.11 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1078381919012644		[learning rate: 0.0012987]
		[batch 20/20] avg loss: 0.07830615922554199		[learning rate: 0.0012963]
	Learning Rate: 0.00129631
	LOSS [training: 0.09307217556340319 | validation: 0.0995500388334413]
	TIME [epoch: 8.1 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06436875105882855		[learning rate: 0.001294]
		[batch 20/20] avg loss: 0.10626369293582283		[learning rate: 0.0012916]
	Learning Rate: 0.00129161
	LOSS [training: 0.08531622199732568 | validation: 0.12591920954047986]
	TIME [epoch: 8.11 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08487929222839044		[learning rate: 0.0012893]
		[batch 20/20] avg loss: 0.07880412898166442		[learning rate: 0.0012869]
	Learning Rate: 0.00128692
	LOSS [training: 0.08184171060502742 | validation: 0.16495917430660045]
	TIME [epoch: 8.14 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12039739989183718		[learning rate: 0.0012846]
		[batch 20/20] avg loss: 0.0679206513129904		[learning rate: 0.0012823]
	Learning Rate: 0.00128225
	LOSS [training: 0.09415902560241379 | validation: 0.0775967292610634]
	TIME [epoch: 8.11 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11882087839802284		[learning rate: 0.0012799]
		[batch 20/20] avg loss: 0.12419922134547035		[learning rate: 0.0012776]
	Learning Rate: 0.0012776
	LOSS [training: 0.1215100498717466 | validation: 0.05155513167028118]
	TIME [epoch: 8.11 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2316172033753443		[learning rate: 0.0012753]
		[batch 20/20] avg loss: 0.12423178474974975		[learning rate: 0.001273]
	Learning Rate: 0.00127296
	LOSS [training: 0.177924494062547 | validation: 0.04973882750057934]
	TIME [epoch: 8.1 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09996396226433317		[learning rate: 0.0012707]
		[batch 20/20] avg loss: 0.09102009753375234		[learning rate: 0.0012683]
	Learning Rate: 0.00126834
	LOSS [training: 0.09549202989904276 | validation: 0.05216049836554798]
	TIME [epoch: 8.13 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0720372928571284		[learning rate: 0.001266]
		[batch 20/20] avg loss: 0.07105208361339424		[learning rate: 0.0012637]
	Learning Rate: 0.00126374
	LOSS [training: 0.0715446882352613 | validation: 0.08877542646417802]
	TIME [epoch: 8.12 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09403591932569319		[learning rate: 0.0012614]
		[batch 20/20] avg loss: 0.0727943160384896		[learning rate: 0.0012592]
	Learning Rate: 0.00125915
	LOSS [training: 0.08341511768209141 | validation: 0.09995056401829659]
	TIME [epoch: 8.11 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10634635873402021		[learning rate: 0.0012569]
		[batch 20/20] avg loss: 0.08495806872689256		[learning rate: 0.0012546]
	Learning Rate: 0.00125458
	LOSS [training: 0.09565221373045639 | validation: 0.08074829826071489]
	TIME [epoch: 8.11 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07112278225227657		[learning rate: 0.0012523]
		[batch 20/20] avg loss: 0.10445745738853156		[learning rate: 0.00125]
	Learning Rate: 0.00125003
	LOSS [training: 0.08779011982040405 | validation: 0.1383812093407008]
	TIME [epoch: 8.11 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07171439015064082		[learning rate: 0.0012478]
		[batch 20/20] avg loss: 0.06990477556584665		[learning rate: 0.0012455]
	Learning Rate: 0.0012455
	LOSS [training: 0.07080958285824374 | validation: 0.07191730879644423]
	TIME [epoch: 8.14 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05986723699494291		[learning rate: 0.0012432]
		[batch 20/20] avg loss: 0.06940345220765157		[learning rate: 0.001241]
	Learning Rate: 0.00124098
	LOSS [training: 0.06463534460129722 | validation: 0.14924145675524303]
	TIME [epoch: 8.11 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07272541898048104		[learning rate: 0.0012387]
		[batch 20/20] avg loss: 0.06406331295124626		[learning rate: 0.0012365]
	Learning Rate: 0.00123647
	LOSS [training: 0.06839436596586365 | validation: 0.06442864211817775]
	TIME [epoch: 8.11 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07437393021795961		[learning rate: 0.0012342]
		[batch 20/20] avg loss: 0.056470282015432346		[learning rate: 0.001232]
	Learning Rate: 0.00123198
	LOSS [training: 0.065422106116696 | validation: 0.10412596069449498]
	TIME [epoch: 8.11 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10420286049473529		[learning rate: 0.0012297]
		[batch 20/20] avg loss: 0.06638740401799118		[learning rate: 0.0012275]
	Learning Rate: 0.00122751
	LOSS [training: 0.08529513225636323 | validation: 0.13575486241832013]
	TIME [epoch: 8.13 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09390699373660058		[learning rate: 0.0012253]
		[batch 20/20] avg loss: 0.07745474778164199		[learning rate: 0.0012231]
	Learning Rate: 0.00122306
	LOSS [training: 0.0856808707591213 | validation: 0.043528257890171944]
	TIME [epoch: 8.11 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_678.pth
	Model improved!!!
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06973489099276646		[learning rate: 0.0012208]
		[batch 20/20] avg loss: 0.06797989910141652		[learning rate: 0.0012186]
	Learning Rate: 0.00121862
	LOSS [training: 0.0688573950470915 | validation: 0.038729930188004226]
	TIME [epoch: 8.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_679.pth
	Model improved!!!
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09042671072266115		[learning rate: 0.0012164]
		[batch 20/20] avg loss: 0.0889801915023627		[learning rate: 0.0012142]
	Learning Rate: 0.0012142
	LOSS [training: 0.0897034511125119 | validation: 0.07513176630866078]
	TIME [epoch: 8.14 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11650119561547831		[learning rate: 0.001212]
		[batch 20/20] avg loss: 0.07538348098044191		[learning rate: 0.0012098]
	Learning Rate: 0.00120979
	LOSS [training: 0.09594233829796009 | validation: 0.049769524016269794]
	TIME [epoch: 8.15 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06156518893873479		[learning rate: 0.0012076]
		[batch 20/20] avg loss: 0.06948507949763048		[learning rate: 0.0012054]
	Learning Rate: 0.0012054
	LOSS [training: 0.06552513421818262 | validation: 0.14409003762760958]
	TIME [epoch: 8.14 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07395338847301994		[learning rate: 0.0012032]
		[batch 20/20] avg loss: 0.07211990878570315		[learning rate: 0.001201]
	Learning Rate: 0.00120103
	LOSS [training: 0.07303664862936154 | validation: 0.18287392221998788]
	TIME [epoch: 8.13 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07020475442456955		[learning rate: 0.0011988]
		[batch 20/20] avg loss: 0.04992117318759107		[learning rate: 0.0011967]
	Learning Rate: 0.00119667
	LOSS [training: 0.060062963806080304 | validation: 0.06832918161639966]
	TIME [epoch: 8.13 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07308167586653785		[learning rate: 0.0011945]
		[batch 20/20] avg loss: 0.0954432284916509		[learning rate: 0.0011923]
	Learning Rate: 0.00119233
	LOSS [training: 0.08426245217909437 | validation: 0.16908653893162515]
	TIME [epoch: 8.13 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10254347233876027		[learning rate: 0.0011902]
		[batch 20/20] avg loss: 0.06569085937949587		[learning rate: 0.001188]
	Learning Rate: 0.001188
	LOSS [training: 0.08411716585912807 | validation: 0.07873051153589815]
	TIME [epoch: 8.16 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08781750955784412		[learning rate: 0.0011858]
		[batch 20/20] avg loss: 0.08341462681529206		[learning rate: 0.0011837]
	Learning Rate: 0.00118369
	LOSS [training: 0.0856160681865681 | validation: 0.09027356328351609]
	TIME [epoch: 8.14 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05257057600287672		[learning rate: 0.0011815]
		[batch 20/20] avg loss: 0.09131033340288389		[learning rate: 0.0011794]
	Learning Rate: 0.00117939
	LOSS [training: 0.07194045470288031 | validation: 0.07897389163867322]
	TIME [epoch: 8.13 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09497958997087595		[learning rate: 0.0011772]
		[batch 20/20] avg loss: 0.056823498661936325		[learning rate: 0.0011751]
	Learning Rate: 0.00117511
	LOSS [training: 0.07590154431640614 | validation: 0.05130213434698845]
	TIME [epoch: 8.13 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06508634517861157		[learning rate: 0.001173]
		[batch 20/20] avg loss: 0.1321847516992254		[learning rate: 0.0011708]
	Learning Rate: 0.00117085
	LOSS [training: 0.09863554843891849 | validation: 0.05254799817509858]
	TIME [epoch: 8.15 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06612692964001085		[learning rate: 0.0011687]
		[batch 20/20] avg loss: 0.06806158291984568		[learning rate: 0.0011666]
	Learning Rate: 0.0011666
	LOSS [training: 0.06709425627992827 | validation: 0.044770959232595414]
	TIME [epoch: 8.15 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06159526587572667		[learning rate: 0.0011645]
		[batch 20/20] avg loss: 0.0873780997063328		[learning rate: 0.0011624]
	Learning Rate: 0.00116236
	LOSS [training: 0.07448668279102974 | validation: 0.0836157097132822]
	TIME [epoch: 8.14 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08788579408322324		[learning rate: 0.0011603]
		[batch 20/20] avg loss: 0.07736281785787824		[learning rate: 0.0011581]
	Learning Rate: 0.00115815
	LOSS [training: 0.08262430597055076 | validation: 0.15482881466175533]
	TIME [epoch: 8.13 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0970350962070475		[learning rate: 0.001156]
		[batch 20/20] avg loss: 0.10985568304564282		[learning rate: 0.0011539]
	Learning Rate: 0.00115394
	LOSS [training: 0.10344538962634513 | validation: 0.1260339650554593]
	TIME [epoch: 8.13 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15770199581085714		[learning rate: 0.0011518]
		[batch 20/20] avg loss: 0.081664829692579		[learning rate: 0.0011498]
	Learning Rate: 0.00114975
	LOSS [training: 0.11968341275171808 | validation: 0.12421215173663716]
	TIME [epoch: 8.16 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08382722411185102		[learning rate: 0.0011477]
		[batch 20/20] avg loss: 0.10070095106860646		[learning rate: 0.0011456]
	Learning Rate: 0.00114558
	LOSS [training: 0.09226408759022876 | validation: 0.1070403036274809]
	TIME [epoch: 8.13 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09629210607161345		[learning rate: 0.0011435]
		[batch 20/20] avg loss: 0.04660164614939523		[learning rate: 0.0011414]
	Learning Rate: 0.00114142
	LOSS [training: 0.07144687611050435 | validation: 0.05315005872154681]
	TIME [epoch: 8.13 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09354926179187426		[learning rate: 0.0011394]
		[batch 20/20] avg loss: 0.11310037839504761		[learning rate: 0.0011373]
	Learning Rate: 0.00113728
	LOSS [training: 0.10332482009346096 | validation: 0.07712544948473296]
	TIME [epoch: 8.13 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06256267809937545		[learning rate: 0.0011352]
		[batch 20/20] avg loss: 0.07423327632702148		[learning rate: 0.0011332]
	Learning Rate: 0.00113316
	LOSS [training: 0.06839797721319847 | validation: 0.07947523085637277]
	TIME [epoch: 8.15 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05071605191621377		[learning rate: 0.0011311]
		[batch 20/20] avg loss: 0.06611645025863058		[learning rate: 0.001129]
	Learning Rate: 0.00112904
	LOSS [training: 0.05841625108742217 | validation: 0.054670190052023185]
	TIME [epoch: 8.15 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08685256364905489		[learning rate: 0.001127]
		[batch 20/20] avg loss: 0.08346680288903495		[learning rate: 0.0011249]
	Learning Rate: 0.00112495
	LOSS [training: 0.08515968326904491 | validation: 0.05893881574282982]
	TIME [epoch: 8.14 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08608486842366991		[learning rate: 0.0011229]
		[batch 20/20] avg loss: 0.06985046654517724		[learning rate: 0.0011209]
	Learning Rate: 0.00112086
	LOSS [training: 0.07796766748442356 | validation: 0.1025223879671893]
	TIME [epoch: 8.13 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10745269188005975		[learning rate: 0.0011188]
		[batch 20/20] avg loss: 0.07401495991670917		[learning rate: 0.0011168]
	Learning Rate: 0.0011168
	LOSS [training: 0.09073382589838444 | validation: 0.06088965001202497]
	TIME [epoch: 8.14 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05502446306740877		[learning rate: 0.0011148]
		[batch 20/20] avg loss: 0.05054661537335274		[learning rate: 0.0011127]
	Learning Rate: 0.00111274
	LOSS [training: 0.05278553922038076 | validation: 0.09131969694590424]
	TIME [epoch: 8.16 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059805454470817955		[learning rate: 0.0011107]
		[batch 20/20] avg loss: 0.09438634950035504		[learning rate: 0.0011087]
	Learning Rate: 0.0011087
	LOSS [training: 0.0770959019855865 | validation: 0.12997453317412305]
	TIME [epoch: 8.13 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1454072683070516		[learning rate: 0.0011067]
		[batch 20/20] avg loss: 0.06488027405683244		[learning rate: 0.0011047]
	Learning Rate: 0.00110468
	LOSS [training: 0.10514377118194203 | validation: 0.0710756027195114]
	TIME [epoch: 8.13 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08909791880619398		[learning rate: 0.0011027]
		[batch 20/20] avg loss: 0.0945319379973794		[learning rate: 0.0011007]
	Learning Rate: 0.00110067
	LOSS [training: 0.0918149284017867 | validation: 0.060213882478450914]
	TIME [epoch: 8.13 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05958685364809957		[learning rate: 0.0010987]
		[batch 20/20] avg loss: 0.08339672980729486		[learning rate: 0.0010967]
	Learning Rate: 0.00109668
	LOSS [training: 0.07149179172769722 | validation: 0.06812860164699894]
	TIME [epoch: 8.15 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10844514926615412		[learning rate: 0.0010947]
		[batch 20/20] avg loss: 0.1284658960042439		[learning rate: 0.0010927]
	Learning Rate: 0.0010927
	LOSS [training: 0.11845552263519898 | validation: 0.1801420058546862]
	TIME [epoch: 8.13 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09086030669216824		[learning rate: 0.0010907]
		[batch 20/20] avg loss: 0.1101122489017156		[learning rate: 0.0010887]
	Learning Rate: 0.00108873
	LOSS [training: 0.10048627779694194 | validation: 0.17477776311418802]
	TIME [epoch: 8.13 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11888178562937837		[learning rate: 0.0010868]
		[batch 20/20] avg loss: 0.07209817700332467		[learning rate: 0.0010848]
	Learning Rate: 0.00108478
	LOSS [training: 0.09548998131635153 | validation: 0.08065460540047727]
	TIME [epoch: 8.13 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06327173237491696		[learning rate: 0.0010828]
		[batch 20/20] avg loss: 0.07245456450517848		[learning rate: 0.0010808]
	Learning Rate: 0.00108084
	LOSS [training: 0.0678631484400477 | validation: 0.14528721982328022]
	TIME [epoch: 8.13 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08882924498417405		[learning rate: 0.0010789]
		[batch 20/20] avg loss: 0.08095534128473718		[learning rate: 0.0010769]
	Learning Rate: 0.00107692
	LOSS [training: 0.08489229313445561 | validation: 0.06758823649587206]
	TIME [epoch: 8.16 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09075333594287213		[learning rate: 0.001075]
		[batch 20/20] avg loss: 0.10235767585129182		[learning rate: 0.001073]
	Learning Rate: 0.00107301
	LOSS [training: 0.09655550589708198 | validation: 0.06233106655606961]
	TIME [epoch: 8.13 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12234753561438542		[learning rate: 0.0010711]
		[batch 20/20] avg loss: 0.06895055603456432		[learning rate: 0.0010691]
	Learning Rate: 0.00106912
	LOSS [training: 0.0956490458244749 | validation: 0.05911365469366134]
	TIME [epoch: 8.13 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05910767857901522		[learning rate: 0.0010672]
		[batch 20/20] avg loss: 0.06436028886961723		[learning rate: 0.0010652]
	Learning Rate: 0.00106524
	LOSS [training: 0.061733983724316245 | validation: 0.052666106166891796]
	TIME [epoch: 8.13 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08205058884798502		[learning rate: 0.0010633]
		[batch 20/20] avg loss: 0.08866531655852995		[learning rate: 0.0010614]
	Learning Rate: 0.00106137
	LOSS [training: 0.0853579527032575 | validation: 0.06750041268338486]
	TIME [epoch: 8.16 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05984520586164497		[learning rate: 0.0010594]
		[batch 20/20] avg loss: 0.0594038738880557		[learning rate: 0.0010575]
	Learning Rate: 0.00105752
	LOSS [training: 0.059624539874850335 | validation: 0.07982011286771348]
	TIME [epoch: 8.13 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08056276703168447		[learning rate: 0.0010556]
		[batch 20/20] avg loss: 0.07837414833677483		[learning rate: 0.0010537]
	Learning Rate: 0.00105368
	LOSS [training: 0.07946845768422965 | validation: 0.12039722046890591]
	TIME [epoch: 8.13 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06723613322813449		[learning rate: 0.0010518]
		[batch 20/20] avg loss: 0.0833763932797893		[learning rate: 0.0010499]
	Learning Rate: 0.00104986
	LOSS [training: 0.07530626325396188 | validation: 0.09005512460946011]
	TIME [epoch: 8.13 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.142380788171854		[learning rate: 0.001048]
		[batch 20/20] avg loss: 0.0863255962229478		[learning rate: 0.0010461]
	Learning Rate: 0.00104605
	LOSS [training: 0.11435319219740092 | validation: 0.05567737454540551]
	TIME [epoch: 8.14 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057314751765610725		[learning rate: 0.0010442]
		[batch 20/20] avg loss: 0.0689823350862682		[learning rate: 0.0010423]
	Learning Rate: 0.00104225
	LOSS [training: 0.06314854342593948 | validation: 0.07292524175110054]
	TIME [epoch: 8.15 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04702378220907789		[learning rate: 0.0010404]
		[batch 20/20] avg loss: 0.0626578121741606		[learning rate: 0.0010385]
	Learning Rate: 0.00103847
	LOSS [training: 0.05484079719161924 | validation: 0.05265184099170744]
	TIME [epoch: 8.13 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06898530726261248		[learning rate: 0.0010366]
		[batch 20/20] avg loss: 0.056561811941644724		[learning rate: 0.0010347]
	Learning Rate: 0.0010347
	LOSS [training: 0.0627735596021286 | validation: 0.0466804787568356]
	TIME [epoch: 8.13 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11221969974559569		[learning rate: 0.0010328]
		[batch 20/20] avg loss: 0.09445670179345585		[learning rate: 0.0010309]
	Learning Rate: 0.00103095
	LOSS [training: 0.10333820076952578 | validation: 0.07400227154574568]
	TIME [epoch: 8.13 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07671196924434255		[learning rate: 0.0010291]
		[batch 20/20] avg loss: 0.05350262603821132		[learning rate: 0.0010272]
	Learning Rate: 0.00102721
	LOSS [training: 0.06510729764127694 | validation: 0.08654292793029354]
	TIME [epoch: 8.16 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05699225966294572		[learning rate: 0.0010253]
		[batch 20/20] avg loss: 0.0777414098772242		[learning rate: 0.0010235]
	Learning Rate: 0.00102348
	LOSS [training: 0.06736683477008495 | validation: 0.05031722523222469]
	TIME [epoch: 8.13 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08132224988139468		[learning rate: 0.0010216]
		[batch 20/20] avg loss: 0.09743703627879666		[learning rate: 0.0010198]
	Learning Rate: 0.00101976
	LOSS [training: 0.08937964308009566 | validation: 0.03843299644360429]
	TIME [epoch: 8.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_728.pth
	Model improved!!!
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07372522475662788		[learning rate: 0.0010179]
		[batch 20/20] avg loss: 0.06476043873022055		[learning rate: 0.0010161]
	Learning Rate: 0.00101606
	LOSS [training: 0.06924283174342423 | validation: 0.06776502600051237]
	TIME [epoch: 8.12 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05385977485276655		[learning rate: 0.0010142]
		[batch 20/20] avg loss: 0.08351603530966424		[learning rate: 0.0010124]
	Learning Rate: 0.00101238
	LOSS [training: 0.06868790508121539 | validation: 0.05815359202122093]
	TIME [epoch: 8.14 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07021229556095306		[learning rate: 0.0010105]
		[batch 20/20] avg loss: 0.06252352420126145		[learning rate: 0.0010087]
	Learning Rate: 0.0010087
	LOSS [training: 0.06636790988110725 | validation: 0.051216560462327684]
	TIME [epoch: 8.13 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05713368693331997		[learning rate: 0.0010069]
		[batch 20/20] avg loss: 0.05863485584900879		[learning rate: 0.001005]
	Learning Rate: 0.00100504
	LOSS [training: 0.05788427139116439 | validation: 0.0891501526726175]
	TIME [epoch: 8.12 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06378245959831451		[learning rate: 0.0010032]
		[batch 20/20] avg loss: 0.0543627781008539		[learning rate: 0.0010014]
	Learning Rate: 0.00100139
	LOSS [training: 0.0590726188495842 | validation: 0.04461168751942202]
	TIME [epoch: 8.12 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048765836687495887		[learning rate: 0.00099958]
		[batch 20/20] avg loss: 0.0790843288108761		[learning rate: 0.00099776]
	Learning Rate: 0.00099776
	LOSS [training: 0.06392508274918599 | validation: 0.16310223817618624]
	TIME [epoch: 8.12 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06617146929268962		[learning rate: 0.00099595]
		[batch 20/20] avg loss: 0.1386847243395403		[learning rate: 0.00099414]
	Learning Rate: 0.00099414
	LOSS [training: 0.10242809681611495 | validation: 0.052748671324630195]
	TIME [epoch: 8.15 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06527461087408627		[learning rate: 0.00099233]
		[batch 20/20] avg loss: 0.04615211075664545		[learning rate: 0.00099053]
	Learning Rate: 0.000990532
	LOSS [training: 0.055713360815365855 | validation: 0.07688666467201946]
	TIME [epoch: 8.12 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06373565895928583		[learning rate: 0.00098873]
		[batch 20/20] avg loss: 0.048356113923967506		[learning rate: 0.00098694]
	Learning Rate: 0.000986937
	LOSS [training: 0.056045886441626655 | validation: 0.06552291266310603]
	TIME [epoch: 8.12 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07971364259165593		[learning rate: 0.00098514]
		[batch 20/20] avg loss: 0.05551378160162844		[learning rate: 0.00098336]
	Learning Rate: 0.000983355
	LOSS [training: 0.06761371209664217 | validation: 0.053118435816744095]
	TIME [epoch: 8.12 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19885809101881288		[learning rate: 0.00098157]
		[batch 20/20] avg loss: 0.06046377252750825		[learning rate: 0.00097979]
	Learning Rate: 0.000979787
	LOSS [training: 0.12966093177316057 | validation: 0.06575998543270781]
	TIME [epoch: 8.14 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041943936933119305		[learning rate: 0.00097801]
		[batch 20/20] avg loss: 0.064812194841672		[learning rate: 0.00097623]
	Learning Rate: 0.000976231
	LOSS [training: 0.05337806588739565 | validation: 0.05564291074463351]
	TIME [epoch: 8.13 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053780583838559104		[learning rate: 0.00097446]
		[batch 20/20] avg loss: 0.09356987359085092		[learning rate: 0.00097269]
	Learning Rate: 0.000972688
	LOSS [training: 0.07367522871470503 | validation: 0.12242135623457412]
	TIME [epoch: 8.12 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06494734565328417		[learning rate: 0.00097092]
		[batch 20/20] avg loss: 0.08532196848404944		[learning rate: 0.00096916]
	Learning Rate: 0.000969158
	LOSS [training: 0.0751346570686668 | validation: 0.044289850033422215]
	TIME [epoch: 8.12 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058927954219695114		[learning rate: 0.0009674]
		[batch 20/20] avg loss: 0.06447846856540082		[learning rate: 0.00096564]
	Learning Rate: 0.000965641
	LOSS [training: 0.061703211392547974 | validation: 0.11997085612943786]
	TIME [epoch: 8.12 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07863642291430345		[learning rate: 0.00096389]
		[batch 20/20] avg loss: 0.06668066787524327		[learning rate: 0.00096214]
	Learning Rate: 0.000962137
	LOSS [training: 0.07265854539477336 | validation: 0.05383316408250679]
	TIME [epoch: 8.15 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04695013668813163		[learning rate: 0.00096039]
		[batch 20/20] avg loss: 0.07776051993194902		[learning rate: 0.00095865]
	Learning Rate: 0.000958645
	LOSS [training: 0.06235532831004033 | validation: 0.06239242264232139]
	TIME [epoch: 8.12 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04783424479026255		[learning rate: 0.0009569]
		[batch 20/20] avg loss: 0.05792616686889379		[learning rate: 0.00095517]
	Learning Rate: 0.000955166
	LOSS [training: 0.05288020582957816 | validation: 0.04078065809469541]
	TIME [epoch: 8.12 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13312288088198734		[learning rate: 0.00095343]
		[batch 20/20] avg loss: 0.05528196605149751		[learning rate: 0.0009517]
	Learning Rate: 0.0009517
	LOSS [training: 0.09420242346674242 | validation: 0.032371671592531126]
	TIME [epoch: 8.11 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_747.pth
	Model improved!!!
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04974865093150515		[learning rate: 0.00094997]
		[batch 20/20] avg loss: 0.0401084463505054		[learning rate: 0.00094825]
	Learning Rate: 0.000948246
	LOSS [training: 0.044928548641005275 | validation: 0.043068299724347135]
	TIME [epoch: 8.14 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051999214678373096		[learning rate: 0.00094652]
		[batch 20/20] avg loss: 0.058662684730718374		[learning rate: 0.0009448]
	Learning Rate: 0.000944805
	LOSS [training: 0.05533094970454574 | validation: 0.040694330857204475]
	TIME [epoch: 8.13 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07140013114900486		[learning rate: 0.00094309]
		[batch 20/20] avg loss: 0.0381943702593114		[learning rate: 0.00094138]
	Learning Rate: 0.000941376
	LOSS [training: 0.054797250704158126 | validation: 0.1798444948035832]
	TIME [epoch: 8.12 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08154082501692159		[learning rate: 0.00093967]
		[batch 20/20] avg loss: 0.04420012061004598		[learning rate: 0.00093796]
	Learning Rate: 0.00093796
	LOSS [training: 0.06287047281348379 | validation: 0.06702570011765435]
	TIME [epoch: 8.12 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07317602625275407		[learning rate: 0.00093626]
		[batch 20/20] avg loss: 0.040628416560445337		[learning rate: 0.00093456]
	Learning Rate: 0.000934556
	LOSS [training: 0.05690222140659971 | validation: 0.033321796145408006]
	TIME [epoch: 8.12 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05938714543092599		[learning rate: 0.00093286]
		[batch 20/20] avg loss: 0.06625047221995324		[learning rate: 0.00093116]
	Learning Rate: 0.000931164
	LOSS [training: 0.06281880882543961 | validation: 0.06911746585399561]
	TIME [epoch: 8.15 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08288112077378824		[learning rate: 0.00092947]
		[batch 20/20] avg loss: 0.11029468231927449		[learning rate: 0.00092779]
	Learning Rate: 0.000927785
	LOSS [training: 0.09658790154653137 | validation: 0.07187527216656918]
	TIME [epoch: 8.12 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11914436290007754		[learning rate: 0.0009261]
		[batch 20/20] avg loss: 0.08201090117404458		[learning rate: 0.00092442]
	Learning Rate: 0.000924418
	LOSS [training: 0.10057763203706108 | validation: 0.04947741496613722]
	TIME [epoch: 8.12 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04187364984186238		[learning rate: 0.00092274]
		[batch 20/20] avg loss: 0.056302564812086876		[learning rate: 0.00092106]
	Learning Rate: 0.000921063
	LOSS [training: 0.04908810732697462 | validation: 0.031048478116838408]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_756.pth
	Model improved!!!
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09146987681232902		[learning rate: 0.00091939]
		[batch 20/20] avg loss: 0.07388669093367191		[learning rate: 0.00091772]
	Learning Rate: 0.000917721
	LOSS [training: 0.08267828387300047 | validation: 0.052124027836136344]
	TIME [epoch: 8.14 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07295387308846592		[learning rate: 0.00091605]
		[batch 20/20] avg loss: 0.07329046789059952		[learning rate: 0.00091439]
	Learning Rate: 0.00091439
	LOSS [training: 0.0731221704895327 | validation: 0.05144995580945389]
	TIME [epoch: 8.12 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06221235893417539		[learning rate: 0.00091273]
		[batch 20/20] avg loss: 0.0692239389602769		[learning rate: 0.00091107]
	Learning Rate: 0.000911072
	LOSS [training: 0.06571814894722613 | validation: 0.08925083479372446]
	TIME [epoch: 8.11 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08726286288732396		[learning rate: 0.00090942]
		[batch 20/20] avg loss: 0.06871194172582624		[learning rate: 0.00090777]
	Learning Rate: 0.000907766
	LOSS [training: 0.07798740230657511 | validation: 0.04007047378073335]
	TIME [epoch: 8.11 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05220386161016166		[learning rate: 0.00090612]
		[batch 20/20] avg loss: 0.06705734557163764		[learning rate: 0.00090447]
	Learning Rate: 0.000904471
	LOSS [training: 0.059630603590899646 | validation: 0.05814889657492521]
	TIME [epoch: 8.12 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05183323834778014		[learning rate: 0.00090283]
		[batch 20/20] avg loss: 0.049196206359848046		[learning rate: 0.00090119]
	Learning Rate: 0.000901189
	LOSS [training: 0.05051472235381409 | validation: 0.07033495405685515]
	TIME [epoch: 8.13 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0648118208221681		[learning rate: 0.00089955]
		[batch 20/20] avg loss: 0.04483961955113201		[learning rate: 0.00089792]
	Learning Rate: 0.000897918
	LOSS [training: 0.054825720186650065 | validation: 0.06661870633974416]
	TIME [epoch: 8.11 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07371161015294306		[learning rate: 0.00089629]
		[batch 20/20] avg loss: 0.0873185659447007		[learning rate: 0.00089466]
	Learning Rate: 0.00089466
	LOSS [training: 0.08051508804882188 | validation: 0.16900026638983073]
	TIME [epoch: 8.11 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09103960076914294		[learning rate: 0.00089303]
		[batch 20/20] avg loss: 0.10627330426639763		[learning rate: 0.00089141]
	Learning Rate: 0.000891413
	LOSS [training: 0.09865645251777028 | validation: 0.05093294374941891]
	TIME [epoch: 8.11 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060483735129062535		[learning rate: 0.00088979]
		[batch 20/20] avg loss: 0.061684235117691485		[learning rate: 0.00088818]
	Learning Rate: 0.000888178
	LOSS [training: 0.061083985123377 | validation: 0.062016865540355445]
	TIME [epoch: 8.14 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04159461590053884		[learning rate: 0.00088656]
		[batch 20/20] avg loss: 0.06758047966161607		[learning rate: 0.00088495]
	Learning Rate: 0.000884955
	LOSS [training: 0.05458754778107745 | validation: 0.08526643046211838]
	TIME [epoch: 8.11 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04949785037812977		[learning rate: 0.00088335]
		[batch 20/20] avg loss: 0.046287087638556636		[learning rate: 0.00088174]
	Learning Rate: 0.000881743
	LOSS [training: 0.0478924690083432 | validation: 0.04427842420435163]
	TIME [epoch: 8.11 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06246462274263733		[learning rate: 0.00088014]
		[batch 20/20] avg loss: 0.07872218056782487		[learning rate: 0.00087854]
	Learning Rate: 0.000878543
	LOSS [training: 0.07059340165523112 | validation: 0.07679216752150433]
	TIME [epoch: 8.11 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0760794144387904		[learning rate: 0.00087695]
		[batch 20/20] avg loss: 0.057007743194902816		[learning rate: 0.00087536]
	Learning Rate: 0.000875355
	LOSS [training: 0.06654357881684661 | validation: 0.0577505180853816]
	TIME [epoch: 8.13 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07390312421048387		[learning rate: 0.00087377]
		[batch 20/20] avg loss: 0.06011866141107405		[learning rate: 0.00087218]
	Learning Rate: 0.000872178
	LOSS [training: 0.06701089281077896 | validation: 0.06494236764211084]
	TIME [epoch: 8.12 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07324488710142688		[learning rate: 0.00087059]
		[batch 20/20] avg loss: 0.05347427638144821		[learning rate: 0.00086901]
	Learning Rate: 0.000869013
	LOSS [training: 0.06335958174143755 | validation: 0.07660664125773055]
	TIME [epoch: 8.11 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06401930232791628		[learning rate: 0.00086743]
		[batch 20/20] avg loss: 0.07097882465420889		[learning rate: 0.00086586]
	Learning Rate: 0.000865859
	LOSS [training: 0.06749906349106258 | validation: 0.10712496348164846]
	TIME [epoch: 8.1 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09684246056915655		[learning rate: 0.00086429]
		[batch 20/20] avg loss: 0.06761363803500292		[learning rate: 0.00086272]
	Learning Rate: 0.000862717
	LOSS [training: 0.08222804930207973 | validation: 0.05192738507829542]
	TIME [epoch: 8.11 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1038813142552976		[learning rate: 0.00086115]
		[batch 20/20] avg loss: 0.05118635797975803		[learning rate: 0.00085959]
	Learning Rate: 0.000859586
	LOSS [training: 0.07753383611752783 | validation: 0.036533801603214136]
	TIME [epoch: 8.13 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059092994779882745		[learning rate: 0.00085803]
		[batch 20/20] avg loss: 0.05920053940187388		[learning rate: 0.00085647]
	Learning Rate: 0.000856467
	LOSS [training: 0.05914676709087831 | validation: 0.08659527474869905]
	TIME [epoch: 8.11 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05038169626647482		[learning rate: 0.00085491]
		[batch 20/20] avg loss: 0.04742204283294192		[learning rate: 0.00085336]
	Learning Rate: 0.000853359
	LOSS [training: 0.04890186954970836 | validation: 0.04603277253000205]
	TIME [epoch: 8.1 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050874287379457615		[learning rate: 0.00085181]
		[batch 20/20] avg loss: 0.049748453335764165		[learning rate: 0.00085026]
	Learning Rate: 0.000850262
	LOSS [training: 0.05031137035761089 | validation: 0.04106135567341949]
	TIME [epoch: 8.11 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05039316292326167		[learning rate: 0.00084872]
		[batch 20/20] avg loss: 0.0565971732667669		[learning rate: 0.00084718]
	Learning Rate: 0.000847176
	LOSS [training: 0.05349516809501428 | validation: 0.06515437745360554]
	TIME [epoch: 8.13 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07189056844091567		[learning rate: 0.00084564]
		[batch 20/20] avg loss: 0.05317756993292476		[learning rate: 0.0008441]
	Learning Rate: 0.000844102
	LOSS [training: 0.0625340691869202 | validation: 0.06608842118914483]
	TIME [epoch: 8.11 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05320805787820204		[learning rate: 0.00084257]
		[batch 20/20] avg loss: 0.06710310759035407		[learning rate: 0.00084104]
	Learning Rate: 0.000841038
	LOSS [training: 0.06015558273427805 | validation: 0.07451192170836991]
	TIME [epoch: 8.11 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058393685961346455		[learning rate: 0.00083951]
		[batch 20/20] avg loss: 0.05775215919447825		[learning rate: 0.00083799]
	Learning Rate: 0.000837986
	LOSS [training: 0.05807292257791234 | validation: 0.047185451209341006]
	TIME [epoch: 8.1 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06764804737300914		[learning rate: 0.00083646]
		[batch 20/20] avg loss: 0.055917513762418415		[learning rate: 0.00083495]
	Learning Rate: 0.000834945
	LOSS [training: 0.06178278056771379 | validation: 0.04833923582439084]
	TIME [epoch: 8.11 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03560096589010604		[learning rate: 0.00083343]
		[batch 20/20] avg loss: 0.04285157661922227		[learning rate: 0.00083192]
	Learning Rate: 0.000831915
	LOSS [training: 0.03922627125466416 | validation: 0.059638488351779866]
	TIME [epoch: 8.13 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08522739158003464		[learning rate: 0.0008304]
		[batch 20/20] avg loss: 0.06420430068334079		[learning rate: 0.0008289]
	Learning Rate: 0.000828896
	LOSS [training: 0.07471584613168772 | validation: 0.09668232333586496]
	TIME [epoch: 8.11 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07067185370645382		[learning rate: 0.00082739]
		[batch 20/20] avg loss: 0.04808375870008274		[learning rate: 0.00082589]
	Learning Rate: 0.000825888
	LOSS [training: 0.05937780620326828 | validation: 0.05606068873648136]
	TIME [epoch: 8.1 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049035053928207575		[learning rate: 0.00082439]
		[batch 20/20] avg loss: 0.04410886283092241		[learning rate: 0.00082289]
	Learning Rate: 0.000822891
	LOSS [training: 0.046571958379564994 | validation: 0.045758044483849035]
	TIME [epoch: 8.1 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0398218027293533		[learning rate: 0.0008214]
		[batch 20/20] avg loss: 0.1742761078103646		[learning rate: 0.0008199]
	Learning Rate: 0.000819904
	LOSS [training: 0.10704895526985896 | validation: 0.0929102100782298]
	TIME [epoch: 8.12 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049755191685483646		[learning rate: 0.00081842]
		[batch 20/20] avg loss: 0.04418612275839212		[learning rate: 0.00081693]
	Learning Rate: 0.000816929
	LOSS [training: 0.04697065722193789 | validation: 0.08761626513303038]
	TIME [epoch: 8.12 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04328235004841863		[learning rate: 0.00081545]
		[batch 20/20] avg loss: 0.049381315907491986		[learning rate: 0.00081396]
	Learning Rate: 0.000813964
	LOSS [training: 0.04633183297795532 | validation: 0.053514006911460216]
	TIME [epoch: 8.11 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051478220963200726		[learning rate: 0.00081249]
		[batch 20/20] avg loss: 0.06986080197999447		[learning rate: 0.00081101]
	Learning Rate: 0.00081101
	LOSS [training: 0.060669511471597604 | validation: 0.0348983179521992]
	TIME [epoch: 8.1 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04631239456430864		[learning rate: 0.00080954]
		[batch 20/20] avg loss: 0.052659862985728154		[learning rate: 0.00080807]
	Learning Rate: 0.000808067
	LOSS [training: 0.04948612877501841 | validation: 0.033041807016942176]
	TIME [epoch: 8.11 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03093033413152437		[learning rate: 0.0008066]
		[batch 20/20] avg loss: 0.04278014374824998		[learning rate: 0.00080513]
	Learning Rate: 0.000805135
	LOSS [training: 0.036855238939887174 | validation: 0.05345769503029457]
	TIME [epoch: 8.14 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06306827012113576		[learning rate: 0.00080367]
		[batch 20/20] avg loss: 0.043635323477981915		[learning rate: 0.00080221]
	Learning Rate: 0.000802213
	LOSS [training: 0.053351796799558836 | validation: 0.07149714070799751]
	TIME [epoch: 8.1 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058085365267398104		[learning rate: 0.00080076]
		[batch 20/20] avg loss: 0.053097164613268945		[learning rate: 0.0007993]
	Learning Rate: 0.000799301
	LOSS [training: 0.05559126494033352 | validation: 0.05332588328488701]
	TIME [epoch: 8.1 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05110917873979457		[learning rate: 0.00079785]
		[batch 20/20] avg loss: 0.06014613440024089		[learning rate: 0.0007964]
	Learning Rate: 0.000796401
	LOSS [training: 0.05562765657001774 | validation: 0.0612395184287494]
	TIME [epoch: 8.1 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05822897285628666		[learning rate: 0.00079495]
		[batch 20/20] avg loss: 0.03807074938380829		[learning rate: 0.00079351]
	Learning Rate: 0.000793511
	LOSS [training: 0.048149861120047474 | validation: 0.025361210460539726]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_797.pth
	Model improved!!!
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0386272034102035		[learning rate: 0.00079207]
		[batch 20/20] avg loss: 0.02567655314311565		[learning rate: 0.00079063]
	Learning Rate: 0.000790631
	LOSS [training: 0.03215187827665958 | validation: 0.09790592308612407]
	TIME [epoch: 8.12 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05804281691302236		[learning rate: 0.00078919]
		[batch 20/20] avg loss: 0.037371559186894186		[learning rate: 0.00078776]
	Learning Rate: 0.000787761
	LOSS [training: 0.04770718804995828 | validation: 0.056320680944703806]
	TIME [epoch: 8.1 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057578637434873556		[learning rate: 0.00078633]
		[batch 20/20] avg loss: 0.04371962360720739		[learning rate: 0.0007849]
	Learning Rate: 0.000784903
	LOSS [training: 0.050649130521040475 | validation: 0.03762335937177051]
	TIME [epoch: 8.1 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06953002102985271		[learning rate: 0.00078348]
		[batch 20/20] avg loss: 0.057942147110757326		[learning rate: 0.00078205]
	Learning Rate: 0.000782054
	LOSS [training: 0.06373608407030502 | validation: 0.021280307745020578]
	TIME [epoch: 8.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_801.pth
	Model improved!!!
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05514358186364786		[learning rate: 0.00078063]
		[batch 20/20] avg loss: 0.06357998605258089		[learning rate: 0.00077922]
	Learning Rate: 0.000779216
	LOSS [training: 0.05936178395811437 | validation: 0.06502145556181106]
	TIME [epoch: 8.13 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06408013333797845		[learning rate: 0.0007778]
		[batch 20/20] avg loss: 0.0708303830421262		[learning rate: 0.00077639]
	Learning Rate: 0.000776388
	LOSS [training: 0.06745525819005231 | validation: 0.07483471945334935]
	TIME [epoch: 8.1 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05503571470161881		[learning rate: 0.00077498]
		[batch 20/20] avg loss: 0.03683738777305088		[learning rate: 0.00077357]
	Learning Rate: 0.000773571
	LOSS [training: 0.045936551237334845 | validation: 0.03186676427061527]
	TIME [epoch: 8.1 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037321576497269277		[learning rate: 0.00077217]
		[batch 20/20] avg loss: 0.03424384019031111		[learning rate: 0.00077076]
	Learning Rate: 0.000770763
	LOSS [training: 0.03578270834379019 | validation: 0.03201132455114438]
	TIME [epoch: 8.1 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05485714704948952		[learning rate: 0.00076936]
		[batch 20/20] avg loss: 0.04011458187274926		[learning rate: 0.00076797]
	Learning Rate: 0.000767966
	LOSS [training: 0.047485864461119395 | validation: 0.04528107283258972]
	TIME [epoch: 8.12 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03907648519650167		[learning rate: 0.00076657]
		[batch 20/20] avg loss: 0.031638766101912706		[learning rate: 0.00076518]
	Learning Rate: 0.000765179
	LOSS [training: 0.03535762564920719 | validation: 0.03359358216982432]
	TIME [epoch: 8.1 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03746455237652074		[learning rate: 0.00076379]
		[batch 20/20] avg loss: 0.034715480833274685		[learning rate: 0.0007624]
	Learning Rate: 0.000762402
	LOSS [training: 0.03609001660489772 | validation: 0.03454684969596085]
	TIME [epoch: 8.1 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03231260172780659		[learning rate: 0.00076102]
		[batch 20/20] avg loss: 0.04724491174356934		[learning rate: 0.00075964]
	Learning Rate: 0.000759636
	LOSS [training: 0.03977875673568796 | validation: 0.04290757298980867]
	TIME [epoch: 8.11 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043237346720426686		[learning rate: 0.00075826]
		[batch 20/20] avg loss: 0.05054900746488228		[learning rate: 0.00075688]
	Learning Rate: 0.000756879
	LOSS [training: 0.04689317709265447 | validation: 0.07156775279750809]
	TIME [epoch: 8.1 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044479562340223856		[learning rate: 0.0007555]
		[batch 20/20] avg loss: 0.052026617405619605		[learning rate: 0.00075413]
	Learning Rate: 0.000754132
	LOSS [training: 0.04825308987292173 | validation: 0.07757882411769312]
	TIME [epoch: 8.13 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05310075328537365		[learning rate: 0.00075276]
		[batch 20/20] avg loss: 0.03982794799870669		[learning rate: 0.0007514]
	Learning Rate: 0.000751395
	LOSS [training: 0.046464350642040166 | validation: 0.03975447870548552]
	TIME [epoch: 8.1 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03470108533135663		[learning rate: 0.00075003]
		[batch 20/20] avg loss: 0.05642120517591829		[learning rate: 0.00074867]
	Learning Rate: 0.000748668
	LOSS [training: 0.045561145253637456 | validation: 0.036627552875147615]
	TIME [epoch: 8.1 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07645130016603009		[learning rate: 0.00074731]
		[batch 20/20] avg loss: 0.04203265181139511		[learning rate: 0.00074595]
	Learning Rate: 0.000745951
	LOSS [training: 0.05924197598871259 | validation: 0.0704189373975767]
	TIME [epoch: 8.1 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03253455613076028		[learning rate: 0.0007446]
		[batch 20/20] avg loss: 0.0451126125636455		[learning rate: 0.00074324]
	Learning Rate: 0.000743244
	LOSS [training: 0.0388235843472029 | validation: 0.04858165466436782]
	TIME [epoch: 8.13 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029071369487858906		[learning rate: 0.00074189]
		[batch 20/20] avg loss: 0.03958978090353489		[learning rate: 0.00074055]
	Learning Rate: 0.000740547
	LOSS [training: 0.03433057519569689 | validation: 0.045420774345624415]
	TIME [epoch: 8.11 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036112289770146924		[learning rate: 0.0007392]
		[batch 20/20] avg loss: 0.03464418529271791		[learning rate: 0.00073786]
	Learning Rate: 0.00073786
	LOSS [training: 0.03537823753143242 | validation: 0.027536764866406327]
	TIME [epoch: 8.1 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05708917668193257		[learning rate: 0.00073652]
		[batch 20/20] avg loss: 0.04729963681765591		[learning rate: 0.00073518]
	Learning Rate: 0.000735182
	LOSS [training: 0.05219440674979425 | validation: 0.03933834352897316]
	TIME [epoch: 8.1 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04636178278421117		[learning rate: 0.00073385]
		[batch 20/20] avg loss: 0.0331712212744236		[learning rate: 0.00073251]
	Learning Rate: 0.000732514
	LOSS [training: 0.039766502029317384 | validation: 0.05524401104563899]
	TIME [epoch: 8.1 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0605894201753534		[learning rate: 0.00073118]
		[batch 20/20] avg loss: 0.03837760745224651		[learning rate: 0.00072986]
	Learning Rate: 0.000729855
	LOSS [training: 0.04948351381379995 | validation: 0.0795513087921812]
	TIME [epoch: 8.12 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05499741884618627		[learning rate: 0.00072853]
		[batch 20/20] avg loss: 0.046048835131147846		[learning rate: 0.00072721]
	Learning Rate: 0.000727207
	LOSS [training: 0.050523126988667066 | validation: 0.05159089931721102]
	TIME [epoch: 8.09 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05643391442850535		[learning rate: 0.00072589]
		[batch 20/20] avg loss: 0.04322993514515293		[learning rate: 0.00072457]
	Learning Rate: 0.000724568
	LOSS [training: 0.04983192478682914 | validation: 0.060956608315597294]
	TIME [epoch: 8.1 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042633320957640344		[learning rate: 0.00072325]
		[batch 20/20] avg loss: 0.05581878777704981		[learning rate: 0.00072194]
	Learning Rate: 0.000721938
	LOSS [training: 0.049226054367345076 | validation: 0.06170847066802548]
	TIME [epoch: 8.11 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04152859148548864		[learning rate: 0.00072063]
		[batch 20/20] avg loss: 0.04428582360582849		[learning rate: 0.00071932]
	Learning Rate: 0.000719318
	LOSS [training: 0.04290720754565856 | validation: 0.15642544183399476]
	TIME [epoch: 8.12 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06534026565969966		[learning rate: 0.00071801]
		[batch 20/20] avg loss: 0.022672257409511037		[learning rate: 0.00071671]
	Learning Rate: 0.000716708
	LOSS [training: 0.04400626153460536 | validation: 0.018249405660882116]
	TIME [epoch: 8.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_825.pth
	Model improved!!!
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04716345835577518		[learning rate: 0.00071541]
		[batch 20/20] avg loss: 0.057269551005098084		[learning rate: 0.00071411]
	Learning Rate: 0.000714107
	LOSS [training: 0.052216504680436635 | validation: 0.02687401332407868]
	TIME [epoch: 8.11 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026989132641565834		[learning rate: 0.00071281]
		[batch 20/20] avg loss: 0.04086531695436995		[learning rate: 0.00071152]
	Learning Rate: 0.000711515
	LOSS [training: 0.03392722479796789 | validation: 0.04768285409122274]
	TIME [epoch: 8.1 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04706472765970744		[learning rate: 0.00071022]
		[batch 20/20] avg loss: 0.04144410306332892		[learning rate: 0.00070893]
	Learning Rate: 0.000708933
	LOSS [training: 0.04425441536151818 | validation: 0.0441266278027686]
	TIME [epoch: 8.12 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04734221214725044		[learning rate: 0.00070765]
		[batch 20/20] avg loss: 0.028085786007140644		[learning rate: 0.00070636]
	Learning Rate: 0.00070636
	LOSS [training: 0.03771399907719554 | validation: 0.0503739233444059]
	TIME [epoch: 8.12 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03991092066700375		[learning rate: 0.00070508]
		[batch 20/20] avg loss: 0.05518448017716591		[learning rate: 0.0007038]
	Learning Rate: 0.000703797
	LOSS [training: 0.04754770042208482 | validation: 0.10850047539052945]
	TIME [epoch: 8.1 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04640516787519658		[learning rate: 0.00070252]
		[batch 20/20] avg loss: 0.04166346518302231		[learning rate: 0.00070124]
	Learning Rate: 0.000701243
	LOSS [training: 0.04403431652910944 | validation: 0.04635792902032687]
	TIME [epoch: 8.1 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03694954750534014		[learning rate: 0.00069997]
		[batch 20/20] avg loss: 0.03117203698565022		[learning rate: 0.0006987]
	Learning Rate: 0.000698698
	LOSS [training: 0.03406079224549518 | validation: 0.020182310987244413]
	TIME [epoch: 8.11 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03152813851913037		[learning rate: 0.00069743]
		[batch 20/20] avg loss: 0.057383308574187476		[learning rate: 0.00069616]
	Learning Rate: 0.000696162
	LOSS [training: 0.04445572354665891 | validation: 0.03866687159251732]
	TIME [epoch: 8.13 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04777291096916643		[learning rate: 0.0006949]
		[batch 20/20] avg loss: 0.04304451408299228		[learning rate: 0.00069364]
	Learning Rate: 0.000693636
	LOSS [training: 0.04540871252607936 | validation: 0.09388868131186923]
	TIME [epoch: 8.11 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04350129932521013		[learning rate: 0.00069238]
		[batch 20/20] avg loss: 0.03630386508841472		[learning rate: 0.00069112]
	Learning Rate: 0.000691119
	LOSS [training: 0.03990258220681242 | validation: 0.0391220198830458]
	TIME [epoch: 8.1 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033573012274070994		[learning rate: 0.00068986]
		[batch 20/20] avg loss: 0.05381667264166738		[learning rate: 0.00068861]
	Learning Rate: 0.000688611
	LOSS [training: 0.043694842457869185 | validation: 0.09296516043357814]
	TIME [epoch: 8.1 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047958730325193315		[learning rate: 0.00068736]
		[batch 20/20] avg loss: 0.042074861811522514		[learning rate: 0.00068611]
	Learning Rate: 0.000686112
	LOSS [training: 0.045016796068357914 | validation: 0.05081587812388579]
	TIME [epoch: 8.13 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02811596319262711		[learning rate: 0.00068487]
		[batch 20/20] avg loss: 0.04894361704084714		[learning rate: 0.00068362]
	Learning Rate: 0.000683622
	LOSS [training: 0.038529790116737114 | validation: 0.020878640960535932]
	TIME [epoch: 8.12 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03901202197657608		[learning rate: 0.00068238]
		[batch 20/20] avg loss: 0.03681756949650415		[learning rate: 0.00068114]
	Learning Rate: 0.000681141
	LOSS [training: 0.03791479573654011 | validation: 0.03513509488920814]
	TIME [epoch: 8.1 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05929543983770782		[learning rate: 0.0006799]
		[batch 20/20] avg loss: 0.02999215221305488		[learning rate: 0.00067867]
	Learning Rate: 0.000678669
	LOSS [training: 0.04464379602538134 | validation: 0.029147977705385755]
	TIME [epoch: 8.1 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033075190786451304		[learning rate: 0.00067744]
		[batch 20/20] avg loss: 0.04112838486452146		[learning rate: 0.00067621]
	Learning Rate: 0.000676206
	LOSS [training: 0.03710178782548637 | validation: 0.056986659259783426]
	TIME [epoch: 8.1 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04897935088934161		[learning rate: 0.00067498]
		[batch 20/20] avg loss: 0.03708650136160816		[learning rate: 0.00067375]
	Learning Rate: 0.000673752
	LOSS [training: 0.04303292612547489 | validation: 0.047796194289787576]
	TIME [epoch: 8.13 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02523859755879604		[learning rate: 0.00067253]
		[batch 20/20] avg loss: 0.037967870939794764		[learning rate: 0.00067131]
	Learning Rate: 0.000671307
	LOSS [training: 0.0316032342492954 | validation: 0.049595407429209745]
	TIME [epoch: 8.1 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029148586353858124		[learning rate: 0.00067009]
		[batch 20/20] avg loss: 0.05024950455519238		[learning rate: 0.00066887]
	Learning Rate: 0.000668871
	LOSS [training: 0.03969904545452525 | validation: 0.03759097374349568]
	TIME [epoch: 8.1 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03813610334308762		[learning rate: 0.00066766]
		[batch 20/20] avg loss: 0.03334891388134421		[learning rate: 0.00066644]
	Learning Rate: 0.000666443
	LOSS [training: 0.03574250861221592 | validation: 0.04787650636594031]
	TIME [epoch: 8.1 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0489631048879895		[learning rate: 0.00066523]
		[batch 20/20] avg loss: 0.03615108893598556		[learning rate: 0.00066402]
	Learning Rate: 0.000664025
	LOSS [training: 0.04255709691198753 | validation: 0.027269394764997387]
	TIME [epoch: 8.11 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03753562785973532		[learning rate: 0.00066282]
		[batch 20/20] avg loss: 0.03636700486896171		[learning rate: 0.00066161]
	Learning Rate: 0.000661615
	LOSS [training: 0.03695131636434852 | validation: 0.055853091762660796]
	TIME [epoch: 8.11 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04813906694505275		[learning rate: 0.00066041]
		[batch 20/20] avg loss: 0.04950423301307643		[learning rate: 0.00065921]
	Learning Rate: 0.000659214
	LOSS [training: 0.048821649979064595 | validation: 0.028841833835703726]
	TIME [epoch: 8.1 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040812519429262915		[learning rate: 0.00065802]
		[batch 20/20] avg loss: 0.04165731062625115		[learning rate: 0.00065682]
	Learning Rate: 0.000656822
	LOSS [training: 0.041234915027757044 | validation: 0.059636735091485135]
	TIME [epoch: 8.1 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044564190314681465		[learning rate: 0.00065563]
		[batch 20/20] avg loss: 0.05964394762058246		[learning rate: 0.00065444]
	Learning Rate: 0.000654438
	LOSS [training: 0.052104068967631964 | validation: 0.073694630082563]
	TIME [epoch: 8.1 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04356386249253351		[learning rate: 0.00065325]
		[batch 20/20] avg loss: 0.04316229292171556		[learning rate: 0.00065206]
	Learning Rate: 0.000652063
	LOSS [training: 0.04336307770712453 | validation: 0.049968113391186245]
	TIME [epoch: 8.13 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034041738213717476		[learning rate: 0.00065088]
		[batch 20/20] avg loss: 0.04012474862911034		[learning rate: 0.0006497]
	Learning Rate: 0.000649697
	LOSS [training: 0.037083243421413906 | validation: 0.06244900824054174]
	TIME [epoch: 8.1 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04963619126720328		[learning rate: 0.00064852]
		[batch 20/20] avg loss: 0.03507189489051636		[learning rate: 0.00064734]
	Learning Rate: 0.000647339
	LOSS [training: 0.04235404307885983 | validation: 0.05420675441449683]
	TIME [epoch: 8.1 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04098533461329198		[learning rate: 0.00064616]
		[batch 20/20] avg loss: 0.036617295627700795		[learning rate: 0.00064499]
	Learning Rate: 0.00064499
	LOSS [training: 0.03880131512049639 | validation: 0.06668983484556301]
	TIME [epoch: 8.1 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03610446875161663		[learning rate: 0.00064382]
		[batch 20/20] avg loss: 0.05359910457371851		[learning rate: 0.00064265]
	Learning Rate: 0.000642649
	LOSS [training: 0.04485178666266758 | validation: 0.06048442152040556]
	TIME [epoch: 8.12 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06404899097632293		[learning rate: 0.00064148]
		[batch 20/20] avg loss: 0.03639314800199233		[learning rate: 0.00064032]
	Learning Rate: 0.000640317
	LOSS [training: 0.05022106948915763 | validation: 0.034722536540478105]
	TIME [epoch: 8.11 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03905209262867528		[learning rate: 0.00063915]
		[batch 20/20] avg loss: 0.031565353577526906		[learning rate: 0.00063799]
	Learning Rate: 0.000637993
	LOSS [training: 0.03530872310310108 | validation: 0.022542365637931362]
	TIME [epoch: 8.1 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02543413867554752		[learning rate: 0.00063683]
		[batch 20/20] avg loss: 0.058251945039291056		[learning rate: 0.00063568]
	Learning Rate: 0.000635677
	LOSS [training: 0.041843041857419286 | validation: 0.027455774217713867]
	TIME [epoch: 8.1 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02635863707326046		[learning rate: 0.00063452]
		[batch 20/20] avg loss: 0.04169213335170234		[learning rate: 0.00063337]
	Learning Rate: 0.000633371
	LOSS [training: 0.03402538521248139 | validation: 0.022767095212501875]
	TIME [epoch: 8.11 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02795140349706916		[learning rate: 0.00063222]
		[batch 20/20] avg loss: 0.03203396084059658		[learning rate: 0.00063107]
	Learning Rate: 0.000631072
	LOSS [training: 0.029992682168832874 | validation: 0.0240431349901318]
	TIME [epoch: 8.13 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031838319125090775		[learning rate: 0.00062993]
		[batch 20/20] avg loss: 0.06896690899037082		[learning rate: 0.00062878]
	Learning Rate: 0.000628782
	LOSS [training: 0.0504026140577308 | validation: 0.02549706591415629]
	TIME [epoch: 8.1 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029149188197004204		[learning rate: 0.00062764]
		[batch 20/20] avg loss: 0.03867840396802871		[learning rate: 0.0006265]
	Learning Rate: 0.0006265
	LOSS [training: 0.03391379608251645 | validation: 0.055404564621939184]
	TIME [epoch: 8.1 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04934423572424278		[learning rate: 0.00062536]
		[batch 20/20] avg loss: 0.04564452261237738		[learning rate: 0.00062423]
	Learning Rate: 0.000624226
	LOSS [training: 0.04749437916831008 | validation: 0.030050542060311217]
	TIME [epoch: 8.1 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03490766920571256		[learning rate: 0.00062309]
		[batch 20/20] avg loss: 0.04203643710398588		[learning rate: 0.00062196]
	Learning Rate: 0.000621961
	LOSS [training: 0.038472053154849216 | validation: 0.028310875453593815]
	TIME [epoch: 8.13 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029446678928145294		[learning rate: 0.00062083]
		[batch 20/20] avg loss: 0.034278388967345905		[learning rate: 0.0006197]
	Learning Rate: 0.000619704
	LOSS [training: 0.031862533947745596 | validation: 0.031983261403551524]
	TIME [epoch: 8.11 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06941996120126208		[learning rate: 0.00061858]
		[batch 20/20] avg loss: 0.08578657814013178		[learning rate: 0.00061745]
	Learning Rate: 0.000617455
	LOSS [training: 0.07760326967069694 | validation: 0.07109573856306875]
	TIME [epoch: 8.11 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0588276884273176		[learning rate: 0.00061633]
		[batch 20/20] avg loss: 0.07479161910052891		[learning rate: 0.00061521]
	Learning Rate: 0.000615214
	LOSS [training: 0.06680965376392325 | validation: 0.04332517192179128]
	TIME [epoch: 8.11 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0468692949702421		[learning rate: 0.0006141]
		[batch 20/20] avg loss: 0.04258737648730349		[learning rate: 0.00061298]
	Learning Rate: 0.000612982
	LOSS [training: 0.04472833572877281 | validation: 0.04323080858925049]
	TIME [epoch: 8.11 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025112008503269993		[learning rate: 0.00061187]
		[batch 20/20] avg loss: 0.036659099722925456		[learning rate: 0.00061076]
	Learning Rate: 0.000610757
	LOSS [training: 0.030885554113097725 | validation: 0.01909287651889971]
	TIME [epoch: 8.13 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04683460480742478		[learning rate: 0.00060965]
		[batch 20/20] avg loss: 0.03784381301036639		[learning rate: 0.00060854]
	Learning Rate: 0.00060854
	LOSS [training: 0.042339208908895594 | validation: 0.032795736197460776]
	TIME [epoch: 8.1 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021632872727538565		[learning rate: 0.00060744]
		[batch 20/20] avg loss: 0.037044281584188606		[learning rate: 0.00060633]
	Learning Rate: 0.000606332
	LOSS [training: 0.029338577155863587 | validation: 0.043508268758174116]
	TIME [epoch: 8.1 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05424890133651685		[learning rate: 0.00060523]
		[batch 20/20] avg loss: 0.03379203967226222		[learning rate: 0.00060413]
	Learning Rate: 0.000604132
	LOSS [training: 0.04402047050438954 | validation: 0.05806589656541193]
	TIME [epoch: 8.1 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06800336538858766		[learning rate: 0.00060303]
		[batch 20/20] avg loss: 0.034548607510566307		[learning rate: 0.00060194]
	Learning Rate: 0.000601939
	LOSS [training: 0.05127598644957698 | validation: 0.03158964557599598]
	TIME [epoch: 8.13 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036996328719244284		[learning rate: 0.00060085]
		[batch 20/20] avg loss: 0.04094671601240535		[learning rate: 0.00059975]
	Learning Rate: 0.000599755
	LOSS [training: 0.03897152236582481 | validation: 0.0648212791991715]
	TIME [epoch: 8.11 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02960762330037333		[learning rate: 0.00059867]
		[batch 20/20] avg loss: 0.02692962080909294		[learning rate: 0.00059758]
	Learning Rate: 0.000597578
	LOSS [training: 0.028268622054733135 | validation: 0.056161947098505124]
	TIME [epoch: 8.11 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02975826350876354		[learning rate: 0.00059649]
		[batch 20/20] avg loss: 0.04007074408504608		[learning rate: 0.00059541]
	Learning Rate: 0.00059541
	LOSS [training: 0.0349145037969048 | validation: 0.04465087536417656]
	TIME [epoch: 8.1 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07535821614504944		[learning rate: 0.00059433]
		[batch 20/20] avg loss: 0.0337934147956305		[learning rate: 0.00059325]
	Learning Rate: 0.000593249
	LOSS [training: 0.05457581547033996 | validation: 0.04980123319341617]
	TIME [epoch: 8.11 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046028629630079046		[learning rate: 0.00059217]
		[batch 20/20] avg loss: 0.03670954058509561		[learning rate: 0.0005911]
	Learning Rate: 0.000591096
	LOSS [training: 0.04136908510758734 | validation: 0.02180975236250701]
	TIME [epoch: 8.13 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03015938745156757		[learning rate: 0.00059002]
		[batch 20/20] avg loss: 0.042836638339258444		[learning rate: 0.00058895]
	Learning Rate: 0.000588951
	LOSS [training: 0.03649801289541302 | validation: 0.03422829897622654]
	TIME [epoch: 8.11 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03533937231003202		[learning rate: 0.00058788]
		[batch 20/20] avg loss: 0.03105365170863169		[learning rate: 0.00058681]
	Learning Rate: 0.000586813
	LOSS [training: 0.033196512009331866 | validation: 0.028635750830629712]
	TIME [epoch: 8.1 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036087078799091475		[learning rate: 0.00058575]
		[batch 20/20] avg loss: 0.02713636483186383		[learning rate: 0.00058468]
	Learning Rate: 0.000584684
	LOSS [training: 0.03161172181547765 | validation: 0.033602005943236216]
	TIME [epoch: 8.1 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031038176350306618		[learning rate: 0.00058362]
		[batch 20/20] avg loss: 0.04610876582102317		[learning rate: 0.00058256]
	Learning Rate: 0.000582562
	LOSS [training: 0.0385734710856649 | validation: 0.05174971679159686]
	TIME [epoch: 8.13 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04435017329015947		[learning rate: 0.0005815]
		[batch 20/20] avg loss: 0.03650530277389275		[learning rate: 0.00058045]
	Learning Rate: 0.000580448
	LOSS [training: 0.04042773803202611 | validation: 0.0470198351584504]
	TIME [epoch: 8.1 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03909849883216403		[learning rate: 0.00057939]
		[batch 20/20] avg loss: 0.06575963250510541		[learning rate: 0.00057834]
	Learning Rate: 0.000578341
	LOSS [training: 0.052429065668634714 | validation: 0.022027242121854328]
	TIME [epoch: 8.11 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022203652818366795		[learning rate: 0.00057729]
		[batch 20/20] avg loss: 0.04974365072886246		[learning rate: 0.00057624]
	Learning Rate: 0.000576243
	LOSS [training: 0.035973651773614626 | validation: 0.04186179755440432]
	TIME [epoch: 8.1 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028347167723495898		[learning rate: 0.0005752]
		[batch 20/20] avg loss: 0.026602233793342395		[learning rate: 0.00057415]
	Learning Rate: 0.000574151
	LOSS [training: 0.027474700758419145 | validation: 0.03478215547615695]
	TIME [epoch: 8.11 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027723765338399882		[learning rate: 0.00057311]
		[batch 20/20] avg loss: 0.04132736661656083		[learning rate: 0.00057207]
	Learning Rate: 0.000572068
	LOSS [training: 0.03452556597748036 | validation: 0.04999303198039283]
	TIME [epoch: 8.12 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035486649480456		[learning rate: 0.00057103]
		[batch 20/20] avg loss: 0.03302525998501701		[learning rate: 0.00056999]
	Learning Rate: 0.000569992
	LOSS [training: 0.0342559547327365 | validation: 0.023017116332052716]
	TIME [epoch: 8.1 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02098565588695466		[learning rate: 0.00056896]
		[batch 20/20] avg loss: 0.04097015506905387		[learning rate: 0.00056792]
	Learning Rate: 0.000567923
	LOSS [training: 0.030977905478004265 | validation: 0.05166604777669266]
	TIME [epoch: 8.1 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029545109383269253		[learning rate: 0.00056689]
		[batch 20/20] avg loss: 0.030606251047987428		[learning rate: 0.00056586]
	Learning Rate: 0.000565862
	LOSS [training: 0.030075680215628335 | validation: 0.016954337081233475]
	TIME [epoch: 8.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_890.pth
	Model improved!!!
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03782791422636127		[learning rate: 0.00056483]
		[batch 20/20] avg loss: 0.05062393722398156		[learning rate: 0.00056381]
	Learning Rate: 0.000563808
	LOSS [training: 0.04422592572517141 | validation: 0.05019968201208536]
	TIME [epoch: 8.13 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04393503532945815		[learning rate: 0.00056278]
		[batch 20/20] avg loss: 0.02796800855228162		[learning rate: 0.00056176]
	Learning Rate: 0.000561762
	LOSS [training: 0.03595152194086989 | validation: 0.031178700973926887]
	TIME [epoch: 8.11 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024435038431251957		[learning rate: 0.00056074]
		[batch 20/20] avg loss: 0.03180314909863313		[learning rate: 0.00055972]
	Learning Rate: 0.000559724
	LOSS [training: 0.028119093764942548 | validation: 0.026506495638205316]
	TIME [epoch: 8.1 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03406785296249886		[learning rate: 0.00055871]
		[batch 20/20] avg loss: 0.026501508659308178		[learning rate: 0.00055769]
	Learning Rate: 0.000557692
	LOSS [training: 0.03028468081090352 | validation: 0.0348576112449763]
	TIME [epoch: 8.09 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030448621651067254		[learning rate: 0.00055668]
		[batch 20/20] avg loss: 0.02381123311399309		[learning rate: 0.00055567]
	Learning Rate: 0.000555669
	LOSS [training: 0.027129927382530165 | validation: 0.017523567894145692]
	TIME [epoch: 8.12 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02693573913150223		[learning rate: 0.00055466]
		[batch 20/20] avg loss: 0.030803716198716614		[learning rate: 0.00055365]
	Learning Rate: 0.000553652
	LOSS [training: 0.02886972766510943 | validation: 0.027993720920812946]
	TIME [epoch: 8.12 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039806154548151056		[learning rate: 0.00055265]
		[batch 20/20] avg loss: 0.0324474129652318		[learning rate: 0.00055164]
	Learning Rate: 0.000551643
	LOSS [training: 0.03612678375669143 | validation: 0.035029956308097715]
	TIME [epoch: 8.1 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027698590215917		[learning rate: 0.00055064]
		[batch 20/20] avg loss: 0.032080061718788536		[learning rate: 0.00054964]
	Learning Rate: 0.000549641
	LOSS [training: 0.02988932596735277 | validation: 0.026308702858722673]
	TIME [epoch: 8.1 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05411043995633732		[learning rate: 0.00054864]
		[batch 20/20] avg loss: 0.02436516405021108		[learning rate: 0.00054765]
	Learning Rate: 0.000547646
	LOSS [training: 0.039237802003274216 | validation: 0.03401119919350862]
	TIME [epoch: 8.1 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02618010807274061		[learning rate: 0.00054665]
		[batch 20/20] avg loss: 0.03580696130575811		[learning rate: 0.00054566]
	Learning Rate: 0.000545659
	LOSS [training: 0.030993534689249362 | validation: 0.034240595634837404]
	TIME [epoch: 8.12 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029126462835802835		[learning rate: 0.00054467]
		[batch 20/20] avg loss: 0.029368914845287086		[learning rate: 0.00054368]
	Learning Rate: 0.000543678
	LOSS [training: 0.029247688840544968 | validation: 0.02360474130762101]
	TIME [epoch: 8.1 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026955559241610787		[learning rate: 0.00054269]
		[batch 20/20] avg loss: 0.028778518817340327		[learning rate: 0.00054171]
	Learning Rate: 0.000541705
	LOSS [training: 0.02786703902947556 | validation: 0.03510800639741126]
	TIME [epoch: 8.1 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03715942794231205		[learning rate: 0.00054072]
		[batch 20/20] avg loss: 0.03845406003109296		[learning rate: 0.00053974]
	Learning Rate: 0.00053974
	LOSS [training: 0.037806743986702494 | validation: 0.029540074016493986]
	TIME [epoch: 8.1 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034370664639546786		[learning rate: 0.00053876]
		[batch 20/20] avg loss: 0.0478096265356597		[learning rate: 0.00053778]
	Learning Rate: 0.000537781
	LOSS [training: 0.04109014558760324 | validation: 0.02859371198744794]
	TIME [epoch: 8.11 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03561676431949237		[learning rate: 0.0005368]
		[batch 20/20] avg loss: 0.031078168312765636		[learning rate: 0.00053583]
	Learning Rate: 0.000535829
	LOSS [training: 0.03334746631612901 | validation: 0.049356905621509684]
	TIME [epoch: 8.11 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03724854299864424		[learning rate: 0.00053486]
		[batch 20/20] avg loss: 0.024555608398435902		[learning rate: 0.00053388]
	Learning Rate: 0.000533885
	LOSS [training: 0.030902075698540067 | validation: 0.02543288639517141]
	TIME [epoch: 8.1 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02484316051708451		[learning rate: 0.00053291]
		[batch 20/20] avg loss: 0.030396398970862853		[learning rate: 0.00053195]
	Learning Rate: 0.000531947
	LOSS [training: 0.027619779743973682 | validation: 0.02723914235811361]
	TIME [epoch: 8.09 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029169051810221558		[learning rate: 0.00053098]
		[batch 20/20] avg loss: 0.025481049422677364		[learning rate: 0.00053002]
	Learning Rate: 0.000530017
	LOSS [training: 0.027325050616449464 | validation: 0.029485303812432723]
	TIME [epoch: 8.1 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02034307453592836		[learning rate: 0.00052905]
		[batch 20/20] avg loss: 0.04937929851984107		[learning rate: 0.00052809]
	Learning Rate: 0.000528093
	LOSS [training: 0.03486118652788471 | validation: 0.054081858128354865]
	TIME [epoch: 8.12 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04519087162009973		[learning rate: 0.00052713]
		[batch 20/20] avg loss: 0.021572920935169576		[learning rate: 0.00052618]
	Learning Rate: 0.000526177
	LOSS [training: 0.03338189627763466 | validation: 0.031802777377890876]
	TIME [epoch: 8.1 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020435714387030204		[learning rate: 0.00052522]
		[batch 20/20] avg loss: 0.047910569978332304		[learning rate: 0.00052427]
	Learning Rate: 0.000524267
	LOSS [training: 0.034173142182681256 | validation: 0.054985082731824675]
	TIME [epoch: 8.1 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04939101510225252		[learning rate: 0.00052332]
		[batch 20/20] avg loss: 0.03793429217994023		[learning rate: 0.00052236]
	Learning Rate: 0.000522365
	LOSS [training: 0.043662653641096366 | validation: 0.04979785628659674]
	TIME [epoch: 8.1 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06989269704799865		[learning rate: 0.00052142]
		[batch 20/20] avg loss: 0.026667995212722017		[learning rate: 0.00052047]
	Learning Rate: 0.000520469
	LOSS [training: 0.04828034613036032 | validation: 0.023369192314233574]
	TIME [epoch: 8.11 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027484759500615595		[learning rate: 0.00051952]
		[batch 20/20] avg loss: 0.03558383983237997		[learning rate: 0.00051858]
	Learning Rate: 0.00051858
	LOSS [training: 0.03153429966649778 | validation: 0.05461192993225851]
	TIME [epoch: 8.11 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030781951813120413		[learning rate: 0.00051764]
		[batch 20/20] avg loss: 0.046584681410648		[learning rate: 0.0005167]
	Learning Rate: 0.000516698
	LOSS [training: 0.038683316611884205 | validation: 0.040484161692745976]
	TIME [epoch: 8.1 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03404202539543592		[learning rate: 0.00051576]
		[batch 20/20] avg loss: 0.0236977334003435		[learning rate: 0.00051482]
	Learning Rate: 0.000514823
	LOSS [training: 0.0288698793978897 | validation: 0.029207044444955182]
	TIME [epoch: 8.1 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029344893079947965		[learning rate: 0.00051389]
		[batch 20/20] avg loss: 0.04399268742617477		[learning rate: 0.00051295]
	Learning Rate: 0.000512955
	LOSS [training: 0.03666879025306137 | validation: 0.030143744971677554]
	TIME [epoch: 8.1 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0354995557335817		[learning rate: 0.00051202]
		[batch 20/20] avg loss: 0.02261480306483252		[learning rate: 0.00051109]
	Learning Rate: 0.000511093
	LOSS [training: 0.029057179399207107 | validation: 0.03195871540365495]
	TIME [epoch: 8.13 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02936427495093627		[learning rate: 0.00051016]
		[batch 20/20] avg loss: 0.017701386205626782		[learning rate: 0.00050924]
	Learning Rate: 0.000509238
	LOSS [training: 0.023532830578281524 | validation: 0.03174814465898132]
	TIME [epoch: 8.1 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027305396179980047		[learning rate: 0.00050831]
		[batch 20/20] avg loss: 0.023757518389691357		[learning rate: 0.00050739]
	Learning Rate: 0.00050739
	LOSS [training: 0.0255314572848357 | validation: 0.05760487540556728]
	TIME [epoch: 8.09 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03223930333517959		[learning rate: 0.00050647]
		[batch 20/20] avg loss: 0.021704176603386617		[learning rate: 0.00050555]
	Learning Rate: 0.000505549
	LOSS [training: 0.0269717399692831 | validation: 0.029866397204411657]
	TIME [epoch: 8.09 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028853850247208795		[learning rate: 0.00050463]
		[batch 20/20] avg loss: 0.027658205274515173		[learning rate: 0.00050371]
	Learning Rate: 0.000503714
	LOSS [training: 0.028256027760861984 | validation: 0.032906854140589706]
	TIME [epoch: 8.11 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035870274475057765		[learning rate: 0.0005028]
		[batch 20/20] avg loss: 0.03527370430320455		[learning rate: 0.00050189]
	Learning Rate: 0.000501886
	LOSS [training: 0.035571989389131156 | validation: 0.033504874789708816]
	TIME [epoch: 8.11 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021991516730291046		[learning rate: 0.00050097]
		[batch 20/20] avg loss: 0.037744880055967524		[learning rate: 0.00050006]
	Learning Rate: 0.000500065
	LOSS [training: 0.029868198393129285 | validation: 0.055997851030089314]
	TIME [epoch: 8.1 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036251772933674216		[learning rate: 0.00049916]
		[batch 20/20] avg loss: 0.03092471784630115		[learning rate: 0.00049825]
	Learning Rate: 0.00049825
	LOSS [training: 0.033588245389987684 | validation: 0.028492300335842533]
	TIME [epoch: 8.1 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02225782211278832		[learning rate: 0.00049735]
		[batch 20/20] avg loss: 0.021682966920745828		[learning rate: 0.00049644]
	Learning Rate: 0.000496442
	LOSS [training: 0.02197039451676707 | validation: 0.04328094771298738]
	TIME [epoch: 8.1 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02573082392678659		[learning rate: 0.00049554]
		[batch 20/20] avg loss: 0.025961129137323145		[learning rate: 0.00049464]
	Learning Rate: 0.00049464
	LOSS [training: 0.025845976532054865 | validation: 0.040761784280077984]
	TIME [epoch: 8.13 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02790447835865727		[learning rate: 0.00049374]
		[batch 20/20] avg loss: 0.03001512882754522		[learning rate: 0.00049285]
	Learning Rate: 0.000492845
	LOSS [training: 0.02895980359310124 | validation: 0.03985761257093058]
	TIME [epoch: 8.1 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03042148213697975		[learning rate: 0.00049195]
		[batch 20/20] avg loss: 0.02289884576484925		[learning rate: 0.00049106]
	Learning Rate: 0.000491057
	LOSS [training: 0.0266601639509145 | validation: 0.02002700196284772]
	TIME [epoch: 8.1 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022047113946883905		[learning rate: 0.00049016]
		[batch 20/20] avg loss: 0.02707396882840467		[learning rate: 0.00048927]
	Learning Rate: 0.000489275
	LOSS [training: 0.024560541387644286 | validation: 0.04398787632907926]
	TIME [epoch: 8.1 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05462066783575721		[learning rate: 0.00048839]
		[batch 20/20] avg loss: 0.03168679439608503		[learning rate: 0.0004875]
	Learning Rate: 0.000487499
	LOSS [training: 0.04315373111592112 | validation: 0.07028680866942343]
	TIME [epoch: 8.11 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029322857965235666		[learning rate: 0.00048661]
		[batch 20/20] avg loss: 0.03996017669133463		[learning rate: 0.00048573]
	Learning Rate: 0.00048573
	LOSS [training: 0.034641517328285146 | validation: 0.024171420869584695]
	TIME [epoch: 8.11 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035559598293325397		[learning rate: 0.00048485]
		[batch 20/20] avg loss: 0.034969734679871266		[learning rate: 0.00048397]
	Learning Rate: 0.000483967
	LOSS [training: 0.03526466648659833 | validation: 0.018874759306704837]
	TIME [epoch: 8.1 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02496247286796596		[learning rate: 0.00048309]
		[batch 20/20] avg loss: 0.024168935794886416		[learning rate: 0.00048221]
	Learning Rate: 0.000482211
	LOSS [training: 0.024565704331426186 | validation: 0.02872819698849603]
	TIME [epoch: 8.1 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04440056941327673		[learning rate: 0.00048133]
		[batch 20/20] avg loss: 0.02982024245241583		[learning rate: 0.00048046]
	Learning Rate: 0.000480461
	LOSS [training: 0.03711040593284627 | validation: 0.02805456505680901]
	TIME [epoch: 8.09 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041854794906655154		[learning rate: 0.00047959]
		[batch 20/20] avg loss: 0.01972368623149827		[learning rate: 0.00047872]
	Learning Rate: 0.000478717
	LOSS [training: 0.03078924056907671 | validation: 0.017568929213372223]
	TIME [epoch: 8.13 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01466763665979225		[learning rate: 0.00047785]
		[batch 20/20] avg loss: 0.03702195563927408		[learning rate: 0.00047698]
	Learning Rate: 0.00047698
	LOSS [training: 0.025844796149533167 | validation: 0.018614943682868118]
	TIME [epoch: 8.1 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018812783139499155		[learning rate: 0.00047611]
		[batch 20/20] avg loss: 0.021718744739591504		[learning rate: 0.00047525]
	Learning Rate: 0.000475249
	LOSS [training: 0.02026576393954533 | validation: 0.01587145401533921]
	TIME [epoch: 8.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_938.pth
	Model improved!!!
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0299711393530022		[learning rate: 0.00047439]
		[batch 20/20] avg loss: 0.021281243684684627		[learning rate: 0.00047352]
	Learning Rate: 0.000473524
	LOSS [training: 0.025626191518843416 | validation: 0.03154310589255385]
	TIME [epoch: 8.1 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024664647763711852		[learning rate: 0.00047266]
		[batch 20/20] avg loss: 0.04533048107324043		[learning rate: 0.00047181]
	Learning Rate: 0.000471806
	LOSS [training: 0.034997564418476144 | validation: 0.08850162035731747]
	TIME [epoch: 8.12 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037271753339302716		[learning rate: 0.00047095]
		[batch 20/20] avg loss: 0.026565404553148115		[learning rate: 0.00047009]
	Learning Rate: 0.000470093
	LOSS [training: 0.03191857894622542 | validation: 0.03030170887617202]
	TIME [epoch: 8.1 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04173639313684421		[learning rate: 0.00046924]
		[batch 20/20] avg loss: 0.020184380889297252		[learning rate: 0.00046839]
	Learning Rate: 0.000468388
	LOSS [training: 0.030960387013070727 | validation: 0.025962629120815983]
	TIME [epoch: 8.1 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019261932815777056		[learning rate: 0.00046754]
		[batch 20/20] avg loss: 0.02452315568869243		[learning rate: 0.00046669]
	Learning Rate: 0.000466688
	LOSS [training: 0.02189254425223474 | validation: 0.020264945707899565]
	TIME [epoch: 8.1 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04406966978603501		[learning rate: 0.00046584]
		[batch 20/20] avg loss: 0.020913565069600328		[learning rate: 0.00046499]
	Learning Rate: 0.000464994
	LOSS [training: 0.03249161742781768 | validation: 0.052946409924553586]
	TIME [epoch: 8.11 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029625067717598597		[learning rate: 0.00046415]
		[batch 20/20] avg loss: 0.035072069293276845		[learning rate: 0.00046331]
	Learning Rate: 0.000463307
	LOSS [training: 0.03234856850543772 | validation: 0.024281264600403512]
	TIME [epoch: 8.12 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03759610936110031		[learning rate: 0.00046247]
		[batch 20/20] avg loss: 0.02680135663158507		[learning rate: 0.00046163]
	Learning Rate: 0.000461625
	LOSS [training: 0.03219873299634269 | validation: 0.011751596659575985]
	TIME [epoch: 8.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_946.pth
	Model improved!!!
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023207618986167276		[learning rate: 0.00046079]
		[batch 20/20] avg loss: 0.025187973288994314		[learning rate: 0.00045995]
	Learning Rate: 0.00045995
	LOSS [training: 0.024197796137580795 | validation: 0.025794101138793338]
	TIME [epoch: 8.1 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02156316672466238		[learning rate: 0.00045911]
		[batch 20/20] avg loss: 0.021533644527006922		[learning rate: 0.00045828]
	Learning Rate: 0.000458281
	LOSS [training: 0.021548405625834655 | validation: 0.023589188111822843]
	TIME [epoch: 8.09 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03444370656276628		[learning rate: 0.00045745]
		[batch 20/20] avg loss: 0.03793771661154262		[learning rate: 0.00045662]
	Learning Rate: 0.000456618
	LOSS [training: 0.036190711587154444 | validation: 0.11017869692790216]
	TIME [epoch: 8.12 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04593974939817389		[learning rate: 0.00045579]
		[batch 20/20] avg loss: 0.03017712926623658		[learning rate: 0.00045496]
	Learning Rate: 0.000454961
	LOSS [training: 0.038058439332205235 | validation: 0.02241584480269506]
	TIME [epoch: 8.1 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029336915235809762		[learning rate: 0.00045413]
		[batch 20/20] avg loss: 0.05008922647109733		[learning rate: 0.00045331]
	Learning Rate: 0.000453309
	LOSS [training: 0.03971307085345354 | validation: 0.056636859500070205]
	TIME [epoch: 8.1 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024587064432580144		[learning rate: 0.00045249]
		[batch 20/20] avg loss: 0.029866226531325635		[learning rate: 0.00045166]
	Learning Rate: 0.000451664
	LOSS [training: 0.027226645481952895 | validation: 0.03590026380433376]
	TIME [epoch: 8.1 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028761510491846227		[learning rate: 0.00045084]
		[batch 20/20] avg loss: 0.022969432372078234		[learning rate: 0.00045003]
	Learning Rate: 0.000450025
	LOSS [training: 0.025865471431962227 | validation: 0.02182100909944918]
	TIME [epoch: 8.11 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04902627904942917		[learning rate: 0.00044921]
		[batch 20/20] avg loss: 0.026111883555091548		[learning rate: 0.00044839]
	Learning Rate: 0.000448392
	LOSS [training: 0.03756908130226035 | validation: 0.025159429902708218]
	TIME [epoch: 8.11 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019375513679215507		[learning rate: 0.00044758]
		[batch 20/20] avg loss: 0.027420227958000632		[learning rate: 0.00044676]
	Learning Rate: 0.000446765
	LOSS [training: 0.023397870818608068 | validation: 0.036527014420411605]
	TIME [epoch: 8.09 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031410066179738266		[learning rate: 0.00044595]
		[batch 20/20] avg loss: 0.03446929596865368		[learning rate: 0.00044514]
	Learning Rate: 0.000445143
	LOSS [training: 0.03293968107419597 | validation: 0.05132893874227873]
	TIME [epoch: 8.1 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027442872578278016		[learning rate: 0.00044434]
		[batch 20/20] avg loss: 0.020924765415450512		[learning rate: 0.00044353]
	Learning Rate: 0.000443528
	LOSS [training: 0.02418381899686427 | validation: 0.017245183195730245]
	TIME [epoch: 8.09 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018883110340147446		[learning rate: 0.00044272]
		[batch 20/20] avg loss: 0.02974648684859061		[learning rate: 0.00044192]
	Learning Rate: 0.000441918
	LOSS [training: 0.02431479859436903 | validation: 0.01780900194892211]
	TIME [epoch: 8.12 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02045969292471791		[learning rate: 0.00044112]
		[batch 20/20] avg loss: 0.017314342948075115		[learning rate: 0.00044031]
	Learning Rate: 0.000440315
	LOSS [training: 0.01888701793639651 | validation: 0.026049975011686235]
	TIME [epoch: 8.1 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025542992861285096		[learning rate: 0.00043952]
		[batch 20/20] avg loss: 0.03222859540582756		[learning rate: 0.00043872]
	Learning Rate: 0.000438717
	LOSS [training: 0.028885794133556325 | validation: 0.03300645698443752]
	TIME [epoch: 8.09 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039810777661420445		[learning rate: 0.00043792]
		[batch 20/20] avg loss: 0.009792953147075407		[learning rate: 0.00043712]
	Learning Rate: 0.000437125
	LOSS [training: 0.024801865404247926 | validation: 0.011198136618742287]
	TIME [epoch: 8.09 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_961.pth
	Model improved!!!
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01879919210742841		[learning rate: 0.00043633]
		[batch 20/20] avg loss: 0.031674735050148985		[learning rate: 0.00043554]
	Learning Rate: 0.000435538
	LOSS [training: 0.025236963578788695 | validation: 0.033320276775556386]
	TIME [epoch: 8.12 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03249074301112219		[learning rate: 0.00043475]
		[batch 20/20] avg loss: 0.013899293134406562		[learning rate: 0.00043396]
	Learning Rate: 0.000433958
	LOSS [training: 0.023195018072764376 | validation: 0.017420441976111953]
	TIME [epoch: 8.11 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01961623905472365		[learning rate: 0.00043317]
		[batch 20/20] avg loss: 0.020954280094628665		[learning rate: 0.00043238]
	Learning Rate: 0.000432383
	LOSS [training: 0.020285259574676157 | validation: 0.011645075221146696]
	TIME [epoch: 8.1 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015896782108706615		[learning rate: 0.0004316]
		[batch 20/20] avg loss: 0.021489782601473165		[learning rate: 0.00043081]
	Learning Rate: 0.000430814
	LOSS [training: 0.018693282355089887 | validation: 0.04593908403935456]
	TIME [epoch: 8.1 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03037664362414118		[learning rate: 0.00043003]
		[batch 20/20] avg loss: 0.03219326284729057		[learning rate: 0.00042925]
	Learning Rate: 0.00042925
	LOSS [training: 0.03128495323571588 | validation: 0.02160767354237609]
	TIME [epoch: 8.1 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027801865273520347		[learning rate: 0.00042847]
		[batch 20/20] avg loss: 0.024453224902805624		[learning rate: 0.00042769]
	Learning Rate: 0.000427692
	LOSS [training: 0.026127545088162986 | validation: 0.014350854338175961]
	TIME [epoch: 8.13 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027292999307724088		[learning rate: 0.00042692]
		[batch 20/20] avg loss: 0.03465288254086911		[learning rate: 0.00042614]
	Learning Rate: 0.00042614
	LOSS [training: 0.030972940924296595 | validation: 0.025120860612440874]
	TIME [epoch: 8.1 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021025156496005785		[learning rate: 0.00042537]
		[batch 20/20] avg loss: 0.022045523643990643		[learning rate: 0.00042459]
	Learning Rate: 0.000424594
	LOSS [training: 0.02153534006999822 | validation: 0.03672968998490531]
	TIME [epoch: 8.1 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030695618866663138		[learning rate: 0.00042382]
		[batch 20/20] avg loss: 0.021933627070068972		[learning rate: 0.00042305]
	Learning Rate: 0.000423053
	LOSS [training: 0.026314622968366046 | validation: 0.018920070880081637]
	TIME [epoch: 8.1 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031866606782504504		[learning rate: 0.00042228]
		[batch 20/20] avg loss: 0.025864781459135733		[learning rate: 0.00042152]
	Learning Rate: 0.000421518
	LOSS [training: 0.02886569412082012 | validation: 0.04062407942253401]
	TIME [epoch: 8.12 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025293979696824433		[learning rate: 0.00042075]
		[batch 20/20] avg loss: 0.03026972749567409		[learning rate: 0.00041999]
	Learning Rate: 0.000419988
	LOSS [training: 0.02778185359624926 | validation: 0.03454130668478272]
	TIME [epoch: 8.11 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02834438722514121		[learning rate: 0.00041923]
		[batch 20/20] avg loss: 0.03527366005636603		[learning rate: 0.00041846]
	Learning Rate: 0.000418464
	LOSS [training: 0.03180902364075362 | validation: 0.02135997467876736]
	TIME [epoch: 8.1 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02394356066476505		[learning rate: 0.0004177]
		[batch 20/20] avg loss: 0.020159324953278088		[learning rate: 0.00041695]
	Learning Rate: 0.000416945
	LOSS [training: 0.02205144280902157 | validation: 0.03805227278338881]
	TIME [epoch: 8.1 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03294750376460924		[learning rate: 0.00041619]
		[batch 20/20] avg loss: 0.028151629581854652		[learning rate: 0.00041543]
	Learning Rate: 0.000415432
	LOSS [training: 0.03054956667323195 | validation: 0.018197023295876975]
	TIME [epoch: 8.1 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01932948806581381		[learning rate: 0.00041468]
		[batch 20/20] avg loss: 0.030047771871844632		[learning rate: 0.00041392]
	Learning Rate: 0.000413924
	LOSS [training: 0.024688629968829225 | validation: 0.045088720651103295]
	TIME [epoch: 8.13 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023797593877867364		[learning rate: 0.00041317]
		[batch 20/20] avg loss: 0.031018526854423266		[learning rate: 0.00041242]
	Learning Rate: 0.000412422
	LOSS [training: 0.02740806036614532 | validation: 0.022338160711795698]
	TIME [epoch: 8.1 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03444909957879241		[learning rate: 0.00041167]
		[batch 20/20] avg loss: 0.024446203812004297		[learning rate: 0.00041093]
	Learning Rate: 0.000410926
	LOSS [training: 0.029447651695398352 | validation: 0.02278760493814428]
	TIME [epoch: 8.1 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02715884367359942		[learning rate: 0.00041018]
		[batch 20/20] avg loss: 0.013342462705304611		[learning rate: 0.00040943]
	Learning Rate: 0.000409434
	LOSS [training: 0.020250653189452014 | validation: 0.022333552037480265]
	TIME [epoch: 8.1 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023858440637939293		[learning rate: 0.00040869]
		[batch 20/20] avg loss: 0.02549101105580697		[learning rate: 0.00040795]
	Learning Rate: 0.000407948
	LOSS [training: 0.024674725846873132 | validation: 0.03381890064024228]
	TIME [epoch: 8.12 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023218206722135547		[learning rate: 0.00040721]
		[batch 20/20] avg loss: 0.03169754523395134		[learning rate: 0.00040647]
	Learning Rate: 0.000406468
	LOSS [training: 0.027457875978043444 | validation: 0.044098565088857275]
	TIME [epoch: 8.11 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026450593306192184		[learning rate: 0.00040573]
		[batch 20/20] avg loss: 0.032420753203215046		[learning rate: 0.00040499]
	Learning Rate: 0.000404993
	LOSS [training: 0.029435673254703608 | validation: 0.03581102318308327]
	TIME [epoch: 8.1 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05069499145251331		[learning rate: 0.00040426]
		[batch 20/20] avg loss: 0.028341300439132654		[learning rate: 0.00040352]
	Learning Rate: 0.000403523
	LOSS [training: 0.03951814594582299 | validation: 0.0684732065619251]
	TIME [epoch: 8.1 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02708988198582966		[learning rate: 0.00040279]
		[batch 20/20] avg loss: 0.01848922807945598		[learning rate: 0.00040206]
	Learning Rate: 0.000402059
	LOSS [training: 0.022789555032642822 | validation: 0.022022667882904676]
	TIME [epoch: 8.11 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023652736553909665		[learning rate: 0.00040133]
		[batch 20/20] avg loss: 0.027402749747068834		[learning rate: 0.0004006]
	Learning Rate: 0.0004006
	LOSS [training: 0.02552774315048925 | validation: 0.018945162919004083]
	TIME [epoch: 8.13 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018364630685876925		[learning rate: 0.00039987]
		[batch 20/20] avg loss: 0.030129625428620965		[learning rate: 0.00039915]
	Learning Rate: 0.000399146
	LOSS [training: 0.02424712805724894 | validation: 0.020390469710888003]
	TIME [epoch: 8.1 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024048880179171905		[learning rate: 0.00039842]
		[batch 20/20] avg loss: 0.023495951936423197		[learning rate: 0.0003977]
	Learning Rate: 0.000397697
	LOSS [training: 0.02377241605779755 | validation: 0.03273603027559888]
	TIME [epoch: 8.1 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024951366052060976		[learning rate: 0.00039698]
		[batch 20/20] avg loss: 0.03294751695655921		[learning rate: 0.00039625]
	Learning Rate: 0.000396254
	LOSS [training: 0.028949441504310093 | validation: 0.03040184286466528]
	TIME [epoch: 8.1 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023371294911487997		[learning rate: 0.00039553]
		[batch 20/20] avg loss: 0.025374409459535674		[learning rate: 0.00039482]
	Learning Rate: 0.000394816
	LOSS [training: 0.02437285218551184 | validation: 0.028480840625327963]
	TIME [epoch: 8.13 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03696792632281066		[learning rate: 0.0003941]
		[batch 20/20] avg loss: 0.02038173429510207		[learning rate: 0.00039338]
	Learning Rate: 0.000393383
	LOSS [training: 0.028674830308956352 | validation: 0.025715019742956986]
	TIME [epoch: 8.1 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025229059510760283		[learning rate: 0.00039267]
		[batch 20/20] avg loss: 0.027512719956912922		[learning rate: 0.00039196]
	Learning Rate: 0.000391956
	LOSS [training: 0.02637088973383661 | validation: 0.03744213254145061]
	TIME [epoch: 8.1 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02292951071296992		[learning rate: 0.00039124]
		[batch 20/20] avg loss: 0.02417969036846814		[learning rate: 0.00039053]
	Learning Rate: 0.000390533
	LOSS [training: 0.023554600540719027 | validation: 0.027379193169990754]
	TIME [epoch: 8.1 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025817187968416682		[learning rate: 0.00038982]
		[batch 20/20] avg loss: 0.030370177871730845		[learning rate: 0.00038912]
	Learning Rate: 0.000389116
	LOSS [training: 0.028093682920073765 | validation: 0.01956300044129363]
	TIME [epoch: 8.11 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024802526671484217		[learning rate: 0.00038841]
		[batch 20/20] avg loss: 0.03711465623526452		[learning rate: 0.0003877]
	Learning Rate: 0.000387704
	LOSS [training: 0.030958591453374366 | validation: 0.02083979650990637]
	TIME [epoch: 8.13 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0324621163693236		[learning rate: 0.000387]
		[batch 20/20] avg loss: 0.046977322974206225		[learning rate: 0.0003863]
	Learning Rate: 0.000386297
	LOSS [training: 0.03971971967176491 | validation: 0.03490475971959933]
	TIME [epoch: 8.1 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04913128485817331		[learning rate: 0.0003856]
		[batch 20/20] avg loss: 0.05237934013660535		[learning rate: 0.00038489]
	Learning Rate: 0.000384895
	LOSS [training: 0.05075531249738933 | validation: 0.02274884386785765]
	TIME [epoch: 8.1 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029104788592920493		[learning rate: 0.0003842]
		[batch 20/20] avg loss: 0.036953016527461933		[learning rate: 0.0003835]
	Learning Rate: 0.000383498
	LOSS [training: 0.033028902560191205 | validation: 0.024693529309612106]
	TIME [epoch: 8.1 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020625030144917247		[learning rate: 0.0003828]
		[batch 20/20] avg loss: 0.02807691022606546		[learning rate: 0.00038211]
	Learning Rate: 0.000382106
	LOSS [training: 0.024350970185491354 | validation: 0.01967071823576552]
	TIME [epoch: 8.12 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017308280180581368		[learning rate: 0.00038141]
		[batch 20/20] avg loss: 0.026194300452132464		[learning rate: 0.00038072]
	Learning Rate: 0.00038072
	LOSS [training: 0.02175129031635692 | validation: 0.05320013107468191]
	TIME [epoch: 8.1 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02514715317226073		[learning rate: 0.00038003]
		[batch 20/20] avg loss: 0.020478330460570827		[learning rate: 0.00037934]
	Learning Rate: 0.000379338
	LOSS [training: 0.022812741816415776 | validation: 0.045007445801838075]
	TIME [epoch: 8.1 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023077608885705488		[learning rate: 0.00037865]
		[batch 20/20] avg loss: 0.020143802765701328		[learning rate: 0.00037796]
	Learning Rate: 0.000377961
	LOSS [training: 0.02161070582570341 | validation: 0.025416148570415797]
	TIME [epoch: 8.1 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023401489888448118		[learning rate: 0.00037727]
		[batch 20/20] avg loss: 0.022525810321006458		[learning rate: 0.00037659]
	Learning Rate: 0.00037659
	LOSS [training: 0.022963650104727285 | validation: 0.02188661511460617]
	TIME [epoch: 8.11 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01916709241553501		[learning rate: 0.00037591]
		[batch 20/20] avg loss: 0.025840200410117087		[learning rate: 0.00037522]
	Learning Rate: 0.000375223
	LOSS [training: 0.022503646412826046 | validation: 0.029931190868643927]
	TIME [epoch: 8.12 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01954300297662446		[learning rate: 0.00037454]
		[batch 20/20] avg loss: 0.01417741873073786		[learning rate: 0.00037386]
	Learning Rate: 0.000373861
	LOSS [training: 0.01686021085368116 | validation: 0.02930957425460002]
	TIME [epoch: 8.1 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017059606963352354		[learning rate: 0.00037318]
		[batch 20/20] avg loss: 0.010013789419137335		[learning rate: 0.0003725]
	Learning Rate: 0.000372505
	LOSS [training: 0.013536698191244844 | validation: 0.02211191641588819]
	TIME [epoch: 8.1 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018070922953061543		[learning rate: 0.00037183]
		[batch 20/20] avg loss: 0.02063226595335234		[learning rate: 0.00037115]
	Learning Rate: 0.000371153
	LOSS [training: 0.01935159445320694 | validation: 0.021069428102155993]
	TIME [epoch: 8.1 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01720243482147387		[learning rate: 0.00037048]
		[batch 20/20] avg loss: 0.03168628670075653		[learning rate: 0.00036981]
	Learning Rate: 0.000369806
	LOSS [training: 0.024444360761115204 | validation: 0.01948893555999104]
	TIME [epoch: 8.13 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04134954796663798		[learning rate: 0.00036913]
		[batch 20/20] avg loss: 0.03440784714398977		[learning rate: 0.00036846]
	Learning Rate: 0.000368464
	LOSS [training: 0.03787869755531387 | validation: 0.024644939074654086]
	TIME [epoch: 8.1 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02867206963485274		[learning rate: 0.00036779]
		[batch 20/20] avg loss: 0.03324696755959009		[learning rate: 0.00036713]
	Learning Rate: 0.000367127
	LOSS [training: 0.030959518597221423 | validation: 0.023274534562577755]
	TIME [epoch: 8.1 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010092387503020672		[learning rate: 0.00036646]
		[batch 20/20] avg loss: 0.016781020109272366		[learning rate: 0.00036579]
	Learning Rate: 0.000365794
	LOSS [training: 0.013436703806146522 | validation: 0.022232697413141166]
	TIME [epoch: 8.1 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021593690922152475		[learning rate: 0.00036513]
		[batch 20/20] avg loss: 0.0391253111866944		[learning rate: 0.00036447]
	Learning Rate: 0.000364467
	LOSS [training: 0.030359501054423442 | validation: 0.025249423623637322]
	TIME [epoch: 8.11 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03154183081657211		[learning rate: 0.0003638]
		[batch 20/20] avg loss: 0.02340386198084248		[learning rate: 0.00036314]
	Learning Rate: 0.000363144
	LOSS [training: 0.02747284639870729 | validation: 0.02306705010707455]
	TIME [epoch: 8.12 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029568907794720146		[learning rate: 0.00036248]
		[batch 20/20] avg loss: 0.009260522655594374		[learning rate: 0.00036183]
	Learning Rate: 0.000361826
	LOSS [training: 0.01941471522515726 | validation: 0.020970028634683283]
	TIME [epoch: 8.09 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0244707284700962		[learning rate: 0.00036117]
		[batch 20/20] avg loss: 0.025308016431437097		[learning rate: 0.00036051]
	Learning Rate: 0.000360513
	LOSS [training: 0.024889372450766646 | validation: 0.0410364892815801]
	TIME [epoch: 8.09 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03166549516495959		[learning rate: 0.00035986]
		[batch 20/20] avg loss: 0.02650650388463921		[learning rate: 0.0003592]
	Learning Rate: 0.000359205
	LOSS [training: 0.0290859995247994 | validation: 0.018264930432161375]
	TIME [epoch: 8.09 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023915644310804722		[learning rate: 0.00035855]
		[batch 20/20] avg loss: 0.028073407645704603		[learning rate: 0.0003579]
	Learning Rate: 0.000357901
	LOSS [training: 0.025994525978254662 | validation: 0.04496342841174375]
	TIME [epoch: 8.12 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02787723771377042		[learning rate: 0.00035725]
		[batch 20/20] avg loss: 0.031130028606042442		[learning rate: 0.0003566]
	Learning Rate: 0.000356602
	LOSS [training: 0.02950363315990643 | validation: 0.020927936287161306]
	TIME [epoch: 8.1 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02474574817906526		[learning rate: 0.00035595]
		[batch 20/20] avg loss: 0.02431760601102136		[learning rate: 0.00035531]
	Learning Rate: 0.000355308
	LOSS [training: 0.024531677095043312 | validation: 0.018605846090067296]
	TIME [epoch: 8.09 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02726219726855462		[learning rate: 0.00035466]
		[batch 20/20] avg loss: 0.03213266525338536		[learning rate: 0.00035402]
	Learning Rate: 0.000354019
	LOSS [training: 0.029697431260969992 | validation: 0.015140997126137187]
	TIME [epoch: 8.1 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025882648123193186		[learning rate: 0.00035338]
		[batch 20/20] avg loss: 0.03287957597306002		[learning rate: 0.00035273]
	Learning Rate: 0.000352734
	LOSS [training: 0.02938111204812661 | validation: 0.035570686921761005]
	TIME [epoch: 8.11 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017496734206526525		[learning rate: 0.00035209]
		[batch 20/20] avg loss: 0.0186720419277007		[learning rate: 0.00035145]
	Learning Rate: 0.000351454
	LOSS [training: 0.018084388067113617 | validation: 0.02583557574276516]
	TIME [epoch: 8.1 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023222742930298596		[learning rate: 0.00035082]
		[batch 20/20] avg loss: 0.015631383993712146		[learning rate: 0.00035018]
	Learning Rate: 0.000350179
	LOSS [training: 0.01942706346200537 | validation: 0.01767036394292093]
	TIME [epoch: 8.09 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025377953645198202		[learning rate: 0.00034954]
		[batch 20/20] avg loss: 0.017215297159154964		[learning rate: 0.00034891]
	Learning Rate: 0.000348908
	LOSS [training: 0.02129662540217658 | validation: 0.01821984655983893]
	TIME [epoch: 8.09 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017761271480763046		[learning rate: 0.00034827]
		[batch 20/20] avg loss: 0.024047494591083792		[learning rate: 0.00034764]
	Learning Rate: 0.000347641
	LOSS [training: 0.02090438303592342 | validation: 0.016671761361538102]
	TIME [epoch: 8.09 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02126949864317982		[learning rate: 0.00034701]
		[batch 20/20] avg loss: 0.01911222947517458		[learning rate: 0.00034638]
	Learning Rate: 0.00034638
	LOSS [training: 0.0201908640591772 | validation: 0.044084577472306055]
	TIME [epoch: 8.12 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023376873526769586		[learning rate: 0.00034575]
		[batch 20/20] avg loss: 0.015596007910777965		[learning rate: 0.00034512]
	Learning Rate: 0.000345123
	LOSS [training: 0.019486440718773774 | validation: 0.05121874408659367]
	TIME [epoch: 8.09 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020498170555264334		[learning rate: 0.0003445]
		[batch 20/20] avg loss: 0.022429241379306082		[learning rate: 0.00034387]
	Learning Rate: 0.00034387
	LOSS [training: 0.02146370596728521 | validation: 0.017794693196038474]
	TIME [epoch: 8.09 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020978962768213087		[learning rate: 0.00034325]
		[batch 20/20] avg loss: 0.027176329922863268		[learning rate: 0.00034262]
	Learning Rate: 0.000342622
	LOSS [training: 0.024077646345538183 | validation: 0.021133997216069275]
	TIME [epoch: 8.09 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018515835485808506		[learning rate: 0.000342]
		[batch 20/20] avg loss: 0.019149733818746774		[learning rate: 0.00034138]
	Learning Rate: 0.000341379
	LOSS [training: 0.01883278465227764 | validation: 0.07131338979891944]
	TIME [epoch: 8.11 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020582812313942777		[learning rate: 0.00034076]
		[batch 20/20] avg loss: 0.009529580124005836		[learning rate: 0.00034014]
	Learning Rate: 0.00034014
	LOSS [training: 0.015056196218974301 | validation: 0.013854931165589158]
	TIME [epoch: 8.1 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021231813350772687		[learning rate: 0.00033952]
		[batch 20/20] avg loss: 0.023314143886993214		[learning rate: 0.00033891]
	Learning Rate: 0.000338906
	LOSS [training: 0.02227297861888295 | validation: 0.025095897499066353]
	TIME [epoch: 8.09 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016684645566397927		[learning rate: 0.00033829]
		[batch 20/20] avg loss: 0.020422583375903504		[learning rate: 0.00033768]
	Learning Rate: 0.000337676
	LOSS [training: 0.01855361447115072 | validation: 0.016194046159984102]
	TIME [epoch: 8.09 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021109473032462783		[learning rate: 0.00033706]
		[batch 20/20] avg loss: 0.010803570266540153		[learning rate: 0.00033645]
	Learning Rate: 0.00033645
	LOSS [training: 0.01595652164950147 | validation: 0.011791356199047944]
	TIME [epoch: 8.09 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021658581524777672		[learning rate: 0.00033584]
		[batch 20/20] avg loss: 0.019426318950423967		[learning rate: 0.00033523]
	Learning Rate: 0.000335229
	LOSS [training: 0.020542450237600818 | validation: 0.025359888085782642]
	TIME [epoch: 8.11 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018955855823754527		[learning rate: 0.00033462]
		[batch 20/20] avg loss: 0.02073872422013536		[learning rate: 0.00033401]
	Learning Rate: 0.000334013
	LOSS [training: 0.019847290021944942 | validation: 0.025083302294042494]
	TIME [epoch: 8.1 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021119485115975795		[learning rate: 0.00033341]
		[batch 20/20] avg loss: 0.027006893827967247		[learning rate: 0.0003328]
	Learning Rate: 0.000332801
	LOSS [training: 0.024063189471971523 | validation: 0.029077053079920416]
	TIME [epoch: 8.09 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02254380055961191		[learning rate: 0.0003322]
		[batch 20/20] avg loss: 0.029849645318423025		[learning rate: 0.00033159]
	Learning Rate: 0.000331593
	LOSS [training: 0.026196722939017463 | validation: 0.02195429437866468]
	TIME [epoch: 8.09 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029185615243019047		[learning rate: 0.00033099]
		[batch 20/20] avg loss: 0.02686015447709763		[learning rate: 0.00033039]
	Learning Rate: 0.00033039
	LOSS [training: 0.028022884860058345 | validation: 0.09497793821449914]
	TIME [epoch: 8.11 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04946248816122975		[learning rate: 0.00032979]
		[batch 20/20] avg loss: 0.025615855377418284		[learning rate: 0.00032919]
	Learning Rate: 0.000329191
	LOSS [training: 0.03753917176932402 | validation: 0.022926129857108693]
	TIME [epoch: 8.1 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02349365738322964		[learning rate: 0.00032859]
		[batch 20/20] avg loss: 0.011248267364014108		[learning rate: 0.000328]
	Learning Rate: 0.000327996
	LOSS [training: 0.017370962373621877 | validation: 0.026825792904034685]
	TIME [epoch: 8.09 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03210502004430758		[learning rate: 0.0003274]
		[batch 20/20] avg loss: 0.02507519944990678		[learning rate: 0.00032681]
	Learning Rate: 0.000326806
	LOSS [training: 0.028590109747107184 | validation: 0.018053572562267414]
	TIME [epoch: 8.09 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02040700142868157		[learning rate: 0.00032621]
		[batch 20/20] avg loss: 0.02112899930609055		[learning rate: 0.00032562]
	Learning Rate: 0.00032562
	LOSS [training: 0.020768000367386064 | validation: 0.021867758742383014]
	TIME [epoch: 8.09 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011101936276298553		[learning rate: 0.00032503]
		[batch 20/20] avg loss: 0.029184912886107268		[learning rate: 0.00032444]
	Learning Rate: 0.000324438
	LOSS [training: 0.02014342458120291 | validation: 0.030541857040213678]
	TIME [epoch: 8.12 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02467389322351723		[learning rate: 0.00032385]
		[batch 20/20] avg loss: 0.024938402222837375		[learning rate: 0.00032326]
	Learning Rate: 0.00032326
	LOSS [training: 0.024806147723177307 | validation: 0.01377464223167182]
	TIME [epoch: 8.09 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014794526558313273		[learning rate: 0.00032267]
		[batch 20/20] avg loss: 0.01717923002723854		[learning rate: 0.00032209]
	Learning Rate: 0.000322087
	LOSS [training: 0.015986878292775907 | validation: 0.015335660750625403]
	TIME [epoch: 8.09 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01805632078575218		[learning rate: 0.0003215]
		[batch 20/20] avg loss: 0.01785257153349421		[learning rate: 0.00032092]
	Learning Rate: 0.000320918
	LOSS [training: 0.017954446159623196 | validation: 0.030047903697099414]
	TIME [epoch: 8.09 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029344507677627373		[learning rate: 0.00032034]
		[batch 20/20] avg loss: 0.01710585589522482		[learning rate: 0.00031975]
	Learning Rate: 0.000319754
	LOSS [training: 0.023225181786426095 | validation: 0.027304819628586455]
	TIME [epoch: 8.11 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029871606768581405		[learning rate: 0.00031917]
		[batch 20/20] avg loss: 0.023495087176492558		[learning rate: 0.00031859]
	Learning Rate: 0.000318593
	LOSS [training: 0.026683346972536985 | validation: 0.029254146469132866]
	TIME [epoch: 8.1 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013399143245237851		[learning rate: 0.00031801]
		[batch 20/20] avg loss: 0.024531460041862126		[learning rate: 0.00031744]
	Learning Rate: 0.000317437
	LOSS [training: 0.018965301643549988 | validation: 0.03161870931344131]
	TIME [epoch: 8.09 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02643765214410812		[learning rate: 0.00031686]
		[batch 20/20] avg loss: 0.02684694698769353		[learning rate: 0.00031629]
	Learning Rate: 0.000316285
	LOSS [training: 0.026642299565900817 | validation: 0.018341623031469426]
	TIME [epoch: 8.1 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024002386760561323		[learning rate: 0.00031571]
		[batch 20/20] avg loss: 0.01679397188832479		[learning rate: 0.00031514]
	Learning Rate: 0.000315137
	LOSS [training: 0.02039817932444306 | validation: 0.0137090699868746]
	TIME [epoch: 8.09 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011360696160841751		[learning rate: 0.00031457]
		[batch 20/20] avg loss: 0.015563915457847011		[learning rate: 0.00031399]
	Learning Rate: 0.000313994
	LOSS [training: 0.01346230580934438 | validation: 0.023151644984374532]
	TIME [epoch: 8.12 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017856334972405376		[learning rate: 0.00031342]
		[batch 20/20] avg loss: 0.010621123593533987		[learning rate: 0.00031285]
	Learning Rate: 0.000312854
	LOSS [training: 0.014238729282969684 | validation: 0.015515431297897871]
	TIME [epoch: 8.09 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010643369682683873		[learning rate: 0.00031229]
		[batch 20/20] avg loss: 0.012043918108533941		[learning rate: 0.00031172]
	Learning Rate: 0.000311719
	LOSS [training: 0.011343643895608905 | validation: 0.023265154854074613]
	TIME [epoch: 8.09 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013729329598417139		[learning rate: 0.00031115]
		[batch 20/20] avg loss: 0.013678635733369698		[learning rate: 0.00031059]
	Learning Rate: 0.000310588
	LOSS [training: 0.013703982665893418 | validation: 0.0153132211496802]
	TIME [epoch: 8.09 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011024484510310836		[learning rate: 0.00031002]
		[batch 20/20] avg loss: 0.018623829193541976		[learning rate: 0.00030946]
	Learning Rate: 0.000309461
	LOSS [training: 0.014824156851926404 | validation: 0.01750651172667341]
	TIME [epoch: 8.11 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021278045818839353		[learning rate: 0.0003089]
		[batch 20/20] avg loss: 0.013152433192598002		[learning rate: 0.00030834]
	Learning Rate: 0.000308338
	LOSS [training: 0.017215239505718676 | validation: 0.02790623821886503]
	TIME [epoch: 8.1 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015918694270480923		[learning rate: 0.00030778]
		[batch 20/20] avg loss: 0.013171760783183625		[learning rate: 0.00030722]
	Learning Rate: 0.000307219
	LOSS [training: 0.014545227526832275 | validation: 0.02058642757259714]
	TIME [epoch: 8.09 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01420016326814388		[learning rate: 0.00030666]
		[batch 20/20] avg loss: 0.030454447123899987		[learning rate: 0.0003061]
	Learning Rate: 0.000306104
	LOSS [training: 0.022327305196021933 | validation: 0.04348195467689381]
	TIME [epoch: 8.09 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022325322808902016		[learning rate: 0.00030555]
		[batch 20/20] avg loss: 0.024261704484381426		[learning rate: 0.00030499]
	Learning Rate: 0.000304993
	LOSS [training: 0.02329351364664172 | validation: 0.0192666538936875]
	TIME [epoch: 8.09 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021742292773090043		[learning rate: 0.00030444]
		[batch 20/20] avg loss: 0.042359362872836336		[learning rate: 0.00030389]
	Learning Rate: 0.000303886
	LOSS [training: 0.03205082782296319 | validation: 0.03397177870405281]
	TIME [epoch: 8.12 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017524422222783668		[learning rate: 0.00030333]
		[batch 20/20] avg loss: 0.017950907715727472		[learning rate: 0.00030278]
	Learning Rate: 0.000302783
	LOSS [training: 0.017737664969255572 | validation: 0.02827524829470884]
	TIME [epoch: 8.09 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036707939934008935		[learning rate: 0.00030223]
		[batch 20/20] avg loss: 0.012404241516780054		[learning rate: 0.00030168]
	Learning Rate: 0.000301684
	LOSS [training: 0.02455609072539449 | validation: 0.026587893267565266]
	TIME [epoch: 8.09 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03061235069178911		[learning rate: 0.00030114]
		[batch 20/20] avg loss: 0.02949429645742766		[learning rate: 0.00030059]
	Learning Rate: 0.000300589
	LOSS [training: 0.03005332357460839 | validation: 0.037453869630464716]
	TIME [epoch: 8.09 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014355410221299311		[learning rate: 0.00030004]
		[batch 20/20] avg loss: 0.013049961559815628		[learning rate: 0.0002995]
	Learning Rate: 0.000299499
	LOSS [training: 0.01370268589055747 | validation: 0.01595527080701055]
	TIME [epoch: 8.11 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016988675021163406		[learning rate: 0.00029895]
		[batch 20/20] avg loss: 0.015806019858675732		[learning rate: 0.00029841]
	Learning Rate: 0.000298412
	LOSS [training: 0.01639734743991957 | validation: 0.023731526572525433]
	TIME [epoch: 8.1 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020249662762779508		[learning rate: 0.00029787]
		[batch 20/20] avg loss: 0.009108701622086067		[learning rate: 0.00029733]
	Learning Rate: 0.000297329
	LOSS [training: 0.014679182192432787 | validation: 0.01122026735985181]
	TIME [epoch: 8.09 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025577139477514436		[learning rate: 0.00029679]
		[batch 20/20] avg loss: 0.03352479837961232		[learning rate: 0.00029625]
	Learning Rate: 0.00029625
	LOSS [training: 0.029550968928563383 | validation: 0.04404350946027624]
	TIME [epoch: 8.09 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01756314931291556		[learning rate: 0.00029571]
		[batch 20/20] avg loss: 0.027176046349068655		[learning rate: 0.00029517]
	Learning Rate: 0.000295175
	LOSS [training: 0.022369597830992106 | validation: 0.026431378235176436]
	TIME [epoch: 8.09 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018361134980573398		[learning rate: 0.00029464]
		[batch 20/20] avg loss: 0.03333611775687531		[learning rate: 0.0002941]
	Learning Rate: 0.000294103
	LOSS [training: 0.025848626368724353 | validation: 0.03136397508675309]
	TIME [epoch: 8.12 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011748433429839117		[learning rate: 0.00029357]
		[batch 20/20] avg loss: 0.01621115835153679		[learning rate: 0.00029304]
	Learning Rate: 0.000293036
	LOSS [training: 0.013979795890687957 | validation: 0.01611504527350788]
	TIME [epoch: 8.09 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010970488532762862		[learning rate: 0.0002925]
		[batch 20/20] avg loss: 0.0155714461602559		[learning rate: 0.00029197]
	Learning Rate: 0.000291973
	LOSS [training: 0.013270967346509382 | validation: 0.013367000458901036]
	TIME [epoch: 8.1 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014711481589704724		[learning rate: 0.00029144]
		[batch 20/20] avg loss: 0.011576359216363234		[learning rate: 0.00029091]
	Learning Rate: 0.000290913
	LOSS [training: 0.01314392040303398 | validation: 0.022225840578552716]
	TIME [epoch: 8.09 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025944933893727696		[learning rate: 0.00029038]
		[batch 20/20] avg loss: 0.02024960811473846		[learning rate: 0.00028986]
	Learning Rate: 0.000289857
	LOSS [training: 0.023097271004233078 | validation: 0.02372200617172097]
	TIME [epoch: 8.12 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015557672837106448		[learning rate: 0.00028933]
		[batch 20/20] avg loss: 0.01579421684613417		[learning rate: 0.00028881]
	Learning Rate: 0.000288805
	LOSS [training: 0.01567594484162031 | validation: 0.014594350023664864]
	TIME [epoch: 8.1 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017748508464717663		[learning rate: 0.00028828]
		[batch 20/20] avg loss: 0.010015018469146413		[learning rate: 0.00028776]
	Learning Rate: 0.000287757
	LOSS [training: 0.01388176346693204 | validation: 0.028274058306493773]
	TIME [epoch: 8.1 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019300379636608536		[learning rate: 0.00028723]
		[batch 20/20] avg loss: 0.016789017865376846		[learning rate: 0.00028671]
	Learning Rate: 0.000286713
	LOSS [training: 0.01804469875099269 | validation: 0.028026516886557752]
	TIME [epoch: 8.09 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0163398483884727		[learning rate: 0.00028619]
		[batch 20/20] avg loss: 0.015239719266384951		[learning rate: 0.00028567]
	Learning Rate: 0.000285672
	LOSS [training: 0.01578978382742883 | validation: 0.016044618595028187]
	TIME [epoch: 8.1 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01376111555651974		[learning rate: 0.00028515]
		[batch 20/20] avg loss: 0.010298015405887723		[learning rate: 0.00028464]
	Learning Rate: 0.000284636
	LOSS [training: 0.012029565481203732 | validation: 0.008961717834682123]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_1079.pth
	Model improved!!!
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014694291915564478		[learning rate: 0.00028412]
		[batch 20/20] avg loss: 0.019232749697510154		[learning rate: 0.0002836]
	Learning Rate: 0.000283603
	LOSS [training: 0.016963520806537315 | validation: 0.018996647521941506]
	TIME [epoch: 8.11 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024819437215306175		[learning rate: 0.00028309]
		[batch 20/20] avg loss: 0.029809740860161416		[learning rate: 0.00028257]
	Learning Rate: 0.000282574
	LOSS [training: 0.027314589037733795 | validation: 0.028985104303922163]
	TIME [epoch: 8.11 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02352797568033648		[learning rate: 0.00028206]
		[batch 20/20] avg loss: 0.014831941211499406		[learning rate: 0.00028155]
	Learning Rate: 0.000281548
	LOSS [training: 0.019179958445917945 | validation: 0.019365941298526507]
	TIME [epoch: 8.1 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015632364996479256		[learning rate: 0.00028104]
		[batch 20/20] avg loss: 0.013568916370525052		[learning rate: 0.00028053]
	Learning Rate: 0.000280526
	LOSS [training: 0.014600640683502156 | validation: 0.01877775608510153]
	TIME [epoch: 8.14 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00715257940274693		[learning rate: 0.00028002]
		[batch 20/20] avg loss: 0.01576457422083597		[learning rate: 0.00027951]
	Learning Rate: 0.000279508
	LOSS [training: 0.011458576811791448 | validation: 0.012200657390312225]
	TIME [epoch: 8.12 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008900903815533219		[learning rate: 0.000279]
		[batch 20/20] avg loss: 0.014819598672119178		[learning rate: 0.00027849]
	Learning Rate: 0.000278494
	LOSS [training: 0.011860251243826201 | validation: 0.015172654215089485]
	TIME [epoch: 8.12 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020202155214826788		[learning rate: 0.00027799]
		[batch 20/20] avg loss: 0.00640226793065402		[learning rate: 0.00027748]
	Learning Rate: 0.000277483
	LOSS [training: 0.013302211572740402 | validation: 0.016484429243818988]
	TIME [epoch: 8.11 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011144306635253894		[learning rate: 0.00027698]
		[batch 20/20] avg loss: 0.01627256840975108		[learning rate: 0.00027648]
	Learning Rate: 0.000276476
	LOSS [training: 0.013708437522502484 | validation: 0.010062766585732906]
	TIME [epoch: 8.13 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01560527097851221		[learning rate: 0.00027597]
		[batch 20/20] avg loss: 0.01816995775875113		[learning rate: 0.00027547]
	Learning Rate: 0.000275473
	LOSS [training: 0.01688761436863167 | validation: 0.01680671139092809]
	TIME [epoch: 8.12 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011716512995455739		[learning rate: 0.00027497]
		[batch 20/20] avg loss: 0.021807339540903563		[learning rate: 0.00027447]
	Learning Rate: 0.000274473
	LOSS [training: 0.01676192626817965 | validation: 0.03275227877607312]
	TIME [epoch: 8.12 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015536831837572932		[learning rate: 0.00027397]
		[batch 20/20] avg loss: 0.02440707642400832		[learning rate: 0.00027348]
	Learning Rate: 0.000273477
	LOSS [training: 0.01997195413079062 | validation: 0.03580511794355517]
	TIME [epoch: 8.11 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019167534780649215		[learning rate: 0.00027298]
		[batch 20/20] avg loss: 0.01784143079068746		[learning rate: 0.00027248]
	Learning Rate: 0.000272485
	LOSS [training: 0.01850448278566833 | validation: 0.007421946111763615]
	TIME [epoch: 8.11 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_1091.pth
	Model improved!!!
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017567988502234764		[learning rate: 0.00027199]
		[batch 20/20] avg loss: 0.015009530564738065		[learning rate: 0.0002715]
	Learning Rate: 0.000271496
	LOSS [training: 0.016288759533486415 | validation: 0.010764163837818124]
	TIME [epoch: 8.13 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018251475761401775		[learning rate: 0.000271]
		[batch 20/20] avg loss: 0.022183790646336576		[learning rate: 0.00027051]
	Learning Rate: 0.000270511
	LOSS [training: 0.020217633203869177 | validation: 0.012647544837818978]
	TIME [epoch: 8.1 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008998141251794132		[learning rate: 0.00027002]
		[batch 20/20] avg loss: 0.019598413372267073		[learning rate: 0.00026953]
	Learning Rate: 0.000269529
	LOSS [training: 0.0142982773120306 | validation: 0.012561344381015149]
	TIME [epoch: 8.11 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006024885416869486		[learning rate: 0.00026904]
		[batch 20/20] avg loss: 0.013232950894642163		[learning rate: 0.00026855]
	Learning Rate: 0.000268551
	LOSS [training: 0.009628918155755822 | validation: 0.021946551016957784]
	TIME [epoch: 8.11 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014489161137194196		[learning rate: 0.00026806]
		[batch 20/20] avg loss: 0.016780317169485942		[learning rate: 0.00026758]
	Learning Rate: 0.000267576
	LOSS [training: 0.015634739153340068 | validation: 0.021379672069785638]
	TIME [epoch: 8.13 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014181719648366325		[learning rate: 0.00026709]
		[batch 20/20] avg loss: 0.010948838288778129		[learning rate: 0.00026661]
	Learning Rate: 0.000266605
	LOSS [training: 0.012565278968572225 | validation: 0.017906151304744627]
	TIME [epoch: 8.11 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01712394782075732		[learning rate: 0.00026612]
		[batch 20/20] avg loss: 0.027674485943007466		[learning rate: 0.00026564]
	Learning Rate: 0.000265638
	LOSS [training: 0.022399216881882393 | validation: 0.023613936764984182]
	TIME [epoch: 8.1 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024971927052524837		[learning rate: 0.00026516]
		[batch 20/20] avg loss: 0.018735384535237876		[learning rate: 0.00026467]
	Learning Rate: 0.000264674
	LOSS [training: 0.021853655793881353 | validation: 0.016738543217964]
	TIME [epoch: 8.1 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021917810762607758		[learning rate: 0.00026419]
		[batch 20/20] avg loss: 0.019795985391781172		[learning rate: 0.00026371]
	Learning Rate: 0.000263713
	LOSS [training: 0.020856898077194465 | validation: 0.01614551495739922]
	TIME [epoch: 8.11 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01885449839276524		[learning rate: 0.00026323]
		[batch 20/20] avg loss: 0.011271004966159444		[learning rate: 0.00026276]
	Learning Rate: 0.000262756
	LOSS [training: 0.015062751679462342 | validation: 0.013076504874315614]
	TIME [epoch: 8.12 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024957235271470098		[learning rate: 0.00026228]
		[batch 20/20] avg loss: 0.02547912988963409		[learning rate: 0.0002618]
	Learning Rate: 0.000261802
	LOSS [training: 0.025218182580552095 | validation: 0.04273863766352559]
	TIME [epoch: 8.1 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02752986506989096		[learning rate: 0.00026133]
		[batch 20/20] avg loss: 0.015251402380811038		[learning rate: 0.00026085]
	Learning Rate: 0.000260852
	LOSS [training: 0.021390633725351 | validation: 0.024514536379823353]
	TIME [epoch: 8.1 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015729457628181946		[learning rate: 0.00026038]
		[batch 20/20] avg loss: 0.01923276672692411		[learning rate: 0.00025991]
	Learning Rate: 0.000259906
	LOSS [training: 0.017481112177553028 | validation: 0.019485188299577693]
	TIME [epoch: 8.1 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01112323197802607		[learning rate: 0.00025943]
		[batch 20/20] avg loss: 0.019115027722118896		[learning rate: 0.00025896]
	Learning Rate: 0.000258962
	LOSS [training: 0.015119129850072485 | validation: 0.020890680111717495]
	TIME [epoch: 8.11 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02464456800913273		[learning rate: 0.00025849]
		[batch 20/20] avg loss: 0.023069954247442712		[learning rate: 0.00025802]
	Learning Rate: 0.000258023
	LOSS [training: 0.023857261128287725 | validation: 0.031832995233681424]
	TIME [epoch: 8.11 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02126881162625947		[learning rate: 0.00025755]
		[batch 20/20] avg loss: 0.013603583290156877		[learning rate: 0.00025709]
	Learning Rate: 0.000257086
	LOSS [training: 0.017436197458208173 | validation: 0.02176289124445585]
	TIME [epoch: 8.1 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015227186494403203		[learning rate: 0.00025662]
		[batch 20/20] avg loss: 0.01529956927238992		[learning rate: 0.00025615]
	Learning Rate: 0.000256153
	LOSS [training: 0.015263377883396562 | validation: 0.02872031905534843]
	TIME [epoch: 8.09 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015325861176154948		[learning rate: 0.00025569]
		[batch 20/20] avg loss: 0.017358934947840077		[learning rate: 0.00025522]
	Learning Rate: 0.000255224
	LOSS [training: 0.016342398061997512 | validation: 0.015817363333930558]
	TIME [epoch: 8.1 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018187329093765837		[learning rate: 0.00025476]
		[batch 20/20] avg loss: 0.021563608606081877		[learning rate: 0.0002543]
	Learning Rate: 0.000254298
	LOSS [training: 0.01987546884992386 | validation: 0.013169269121254996]
	TIME [epoch: 8.12 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024204197890378136		[learning rate: 0.00025384]
		[batch 20/20] avg loss: 0.024635538611488365		[learning rate: 0.00025337]
	Learning Rate: 0.000253375
	LOSS [training: 0.024419868250933258 | validation: 0.02149004368317134]
	TIME [epoch: 8.1 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021420146651468956		[learning rate: 0.00025291]
		[batch 20/20] avg loss: 0.044090490554006385		[learning rate: 0.00025246]
	Learning Rate: 0.000252455
	LOSS [training: 0.03275531860273768 | validation: 0.0416123066856524]
	TIME [epoch: 8.1 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024717581899111146		[learning rate: 0.000252]
		[batch 20/20] avg loss: 0.02039592645886104		[learning rate: 0.00025154]
	Learning Rate: 0.000251539
	LOSS [training: 0.02255675417898609 | validation: 0.018802604034714825]
	TIME [epoch: 8.1 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0197876066601984		[learning rate: 0.00025108]
		[batch 20/20] avg loss: 0.015246621947722108		[learning rate: 0.00025063]
	Learning Rate: 0.000250626
	LOSS [training: 0.017517114303960257 | validation: 0.017778150550592398]
	TIME [epoch: 8.11 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02531080888412327		[learning rate: 0.00025017]
		[batch 20/20] avg loss: 0.01499055070887371		[learning rate: 0.00024972]
	Learning Rate: 0.000249717
	LOSS [training: 0.02015067979649849 | validation: 0.026487783635095692]
	TIME [epoch: 8.1 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014191199195855273		[learning rate: 0.00024926]
		[batch 20/20] avg loss: 0.02392740087422656		[learning rate: 0.00024881]
	Learning Rate: 0.00024881
	LOSS [training: 0.019059300035040916 | validation: 0.02291660632815924]
	TIME [epoch: 8.1 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017359448200006483		[learning rate: 0.00024836]
		[batch 20/20] avg loss: 0.011726878806273863		[learning rate: 0.00024791]
	Learning Rate: 0.000247907
	LOSS [training: 0.014543163503140171 | validation: 0.031044081890854978]
	TIME [epoch: 8.09 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017927725067643386		[learning rate: 0.00024746]
		[batch 20/20] avg loss: 0.01528537587287888		[learning rate: 0.00024701]
	Learning Rate: 0.000247008
	LOSS [training: 0.016606550470261133 | validation: 0.014445443649105511]
	TIME [epoch: 8.1 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011892941606333585		[learning rate: 0.00024656]
		[batch 20/20] avg loss: 0.019160845016583106		[learning rate: 0.00024611]
	Learning Rate: 0.000246111
	LOSS [training: 0.01552689331145835 | validation: 0.017102763893264948]
	TIME [epoch: 8.12 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021872384913695222		[learning rate: 0.00024566]
		[batch 20/20] avg loss: 0.02456789275615187		[learning rate: 0.00024522]
	Learning Rate: 0.000245218
	LOSS [training: 0.023220138834923548 | validation: 0.03434951689686001]
	TIME [epoch: 8.1 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012959715421179283		[learning rate: 0.00024477]
		[batch 20/20] avg loss: 0.018512058592111434		[learning rate: 0.00024433]
	Learning Rate: 0.000244328
	LOSS [training: 0.01573588700664536 | validation: 0.030000726747200286]
	TIME [epoch: 8.09 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015798322905724048		[learning rate: 0.00024388]
		[batch 20/20] avg loss: 0.007523585048353801		[learning rate: 0.00024344]
	Learning Rate: 0.000243442
	LOSS [training: 0.011660953977038927 | validation: 0.016102784514914953]
	TIME [epoch: 8.1 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013982492621003689		[learning rate: 0.000243]
		[batch 20/20] avg loss: 0.012933588173583279		[learning rate: 0.00024256]
	Learning Rate: 0.000242558
	LOSS [training: 0.013458040397293486 | validation: 0.02918542296632986]
	TIME [epoch: 8.12 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01772168089397399		[learning rate: 0.00024212]
		[batch 20/20] avg loss: 0.015879568298054182		[learning rate: 0.00024168]
	Learning Rate: 0.000241678
	LOSS [training: 0.01680062459601409 | validation: 0.02028273713892219]
	TIME [epoch: 8.11 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011479040090761578		[learning rate: 0.00024124]
		[batch 20/20] avg loss: 0.01548583275612427		[learning rate: 0.0002408]
	Learning Rate: 0.000240801
	LOSS [training: 0.013482436423442925 | validation: 0.03415135427300375]
	TIME [epoch: 8.1 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018514391331313426		[learning rate: 0.00024036]
		[batch 20/20] avg loss: 0.014557854677355491		[learning rate: 0.00023993]
	Learning Rate: 0.000239927
	LOSS [training: 0.016536123004334456 | validation: 0.016617920773310483]
	TIME [epoch: 8.1 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012069781281909725		[learning rate: 0.00023949]
		[batch 20/20] avg loss: 0.024984831884835447		[learning rate: 0.00023906]
	Learning Rate: 0.000239056
	LOSS [training: 0.018527306583372587 | validation: 0.027263959619075198]
	TIME [epoch: 8.1 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013286560094489638		[learning rate: 0.00023862]
		[batch 20/20] avg loss: 0.02127083487598238		[learning rate: 0.00023819]
	Learning Rate: 0.000238189
	LOSS [training: 0.017278697485236007 | validation: 0.02305182898270756]
	TIME [epoch: 8.12 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015275763326352249		[learning rate: 0.00023776]
		[batch 20/20] avg loss: 0.012023574404838417		[learning rate: 0.00023732]
	Learning Rate: 0.000237324
	LOSS [training: 0.013649668865595332 | validation: 0.01706871327952926]
	TIME [epoch: 8.1 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014838759697309137		[learning rate: 0.00023689]
		[batch 20/20] avg loss: 0.013506359129614611		[learning rate: 0.00023646]
	Learning Rate: 0.000236463
	LOSS [training: 0.014172559413461874 | validation: 0.020633663739465916]
	TIME [epoch: 8.1 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016494522966197182		[learning rate: 0.00023603]
		[batch 20/20] avg loss: 0.01070677869184591		[learning rate: 0.0002356]
	Learning Rate: 0.000235605
	LOSS [training: 0.013600650829021546 | validation: 0.01958884742667745]
	TIME [epoch: 8.1 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02240614429411576		[learning rate: 0.00023518]
		[batch 20/20] avg loss: 0.017668233671455775		[learning rate: 0.00023475]
	Learning Rate: 0.00023475
	LOSS [training: 0.020037188982785767 | validation: 0.012158591443687156]
	TIME [epoch: 8.13 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012229729698323494		[learning rate: 0.00023432]
		[batch 20/20] avg loss: 0.02073605454408429		[learning rate: 0.0002339]
	Learning Rate: 0.000233898
	LOSS [training: 0.01648289212120389 | validation: 0.028876591233114715]
	TIME [epoch: 8.1 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014048132023724546		[learning rate: 0.00023347]
		[batch 20/20] avg loss: 0.016920958248342804		[learning rate: 0.00023305]
	Learning Rate: 0.000233049
	LOSS [training: 0.015484545136033676 | validation: 0.01902817173874465]
	TIME [epoch: 8.1 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012318820021840524		[learning rate: 0.00023263]
		[batch 20/20] avg loss: 0.013465630665041898		[learning rate: 0.0002322]
	Learning Rate: 0.000232203
	LOSS [training: 0.01289222534344121 | validation: 0.010284645219083116]
	TIME [epoch: 8.1 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007370602003796603		[learning rate: 0.00023178]
		[batch 20/20] avg loss: 0.009489222697965885		[learning rate: 0.00023136]
	Learning Rate: 0.000231361
	LOSS [training: 0.008429912350881242 | validation: 0.014841565108794714]
	TIME [epoch: 8.1 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011806440036275017		[learning rate: 0.00023094]
		[batch 20/20] avg loss: 0.01579401468469288		[learning rate: 0.00023052]
	Learning Rate: 0.000230521
	LOSS [training: 0.013800227360483952 | validation: 0.024403685245953072]
	TIME [epoch: 8.12 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017771878258025398		[learning rate: 0.0002301]
		[batch 20/20] avg loss: 0.01615145687218139		[learning rate: 0.00022968]
	Learning Rate: 0.000229685
	LOSS [training: 0.016961667565103394 | validation: 0.017361514476273092]
	TIME [epoch: 8.1 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013816849374043774		[learning rate: 0.00022927]
		[batch 20/20] avg loss: 0.016675283832679057		[learning rate: 0.00022885]
	Learning Rate: 0.000228851
	LOSS [training: 0.015246066603361419 | validation: 0.0396656510371101]
	TIME [epoch: 8.09 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019957947201548424		[learning rate: 0.00022844]
		[batch 20/20] avg loss: 0.012751771781329631		[learning rate: 0.00022802]
	Learning Rate: 0.00022802
	LOSS [training: 0.016354859491439028 | validation: 0.013475445954988773]
	TIME [epoch: 8.1 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02264607365815205		[learning rate: 0.00022761]
		[batch 20/20] avg loss: 0.005708902185724235		[learning rate: 0.00022719]
	Learning Rate: 0.000227193
	LOSS [training: 0.01417748792193814 | validation: 0.01599222359266737]
	TIME [epoch: 8.12 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011016385472787011		[learning rate: 0.00022678]
		[batch 20/20] avg loss: 0.014906316016676985		[learning rate: 0.00022637]
	Learning Rate: 0.000226368
	LOSS [training: 0.012961350744731998 | validation: 0.012611752456859821]
	TIME [epoch: 8.1 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019035763258853283		[learning rate: 0.00022596]
		[batch 20/20] avg loss: 0.02623738842432982		[learning rate: 0.00022555]
	Learning Rate: 0.000225547
	LOSS [training: 0.022636575841591552 | validation: 0.03138113260816356]
	TIME [epoch: 8.1 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017756778679458615		[learning rate: 0.00022514]
		[batch 20/20] avg loss: 0.00604823431888672		[learning rate: 0.00022473]
	Learning Rate: 0.000224728
	LOSS [training: 0.011902506499172669 | validation: 0.017446305409160243]
	TIME [epoch: 8.1 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012016393430675525		[learning rate: 0.00022432]
		[batch 20/20] avg loss: 0.010443637662102156		[learning rate: 0.00022391]
	Learning Rate: 0.000223913
	LOSS [training: 0.01123001554638884 | validation: 0.01643334602983293]
	TIME [epoch: 8.11 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: -3.966663880810954e-05		[learning rate: 0.00022351]
		[batch 20/20] avg loss: 0.016921947143299317		[learning rate: 0.0002231]
	Learning Rate: 0.0002231
	LOSS [training: 0.008441140252245604 | validation: 0.0140118673005681]
	TIME [epoch: 8.12 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011905504898574882		[learning rate: 0.0002227]
		[batch 20/20] avg loss: 0.013463578808242838		[learning rate: 0.00022229]
	Learning Rate: 0.000222291
	LOSS [training: 0.01268454185340886 | validation: 0.011085328003520004]
	TIME [epoch: 8.1 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01672695932341254		[learning rate: 0.00022189]
		[batch 20/20] avg loss: 0.011952526209490801		[learning rate: 0.00022148]
	Learning Rate: 0.000221484
	LOSS [training: 0.014339742766451672 | validation: 0.010579493157436375]
	TIME [epoch: 8.1 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010052662323702236		[learning rate: 0.00022108]
		[batch 20/20] avg loss: 0.019514229304764284		[learning rate: 0.00022068]
	Learning Rate: 0.00022068
	LOSS [training: 0.014783445814233262 | validation: 0.0295501525482526]
	TIME [epoch: 8.1 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02192810351865232		[learning rate: 0.00022028]
		[batch 20/20] avg loss: 0.021511138867176316		[learning rate: 0.00021988]
	Learning Rate: 0.000219879
	LOSS [training: 0.021719621192914318 | validation: 0.028992571312920656]
	TIME [epoch: 8.12 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014541571270493981		[learning rate: 0.00021948]
		[batch 20/20] avg loss: 0.010376580117380277		[learning rate: 0.00021908]
	Learning Rate: 0.000219081
	LOSS [training: 0.012459075693937128 | validation: 0.011647841214114812]
	TIME [epoch: 8.1 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01420733003593722		[learning rate: 0.00021868]
		[batch 20/20] avg loss: 0.020047408852393415		[learning rate: 0.00021829]
	Learning Rate: 0.000218286
	LOSS [training: 0.017127369444165316 | validation: 0.010786824866717882]
	TIME [epoch: 8.09 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010909966440678036		[learning rate: 0.00021789]
		[batch 20/20] avg loss: 0.014018636113394262		[learning rate: 0.00021749]
	Learning Rate: 0.000217494
	LOSS [training: 0.012464301277036146 | validation: 0.013713708991901974]
	TIME [epoch: 8.1 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006653640607325187		[learning rate: 0.0002171]
		[batch 20/20] avg loss: 0.00932285289872219		[learning rate: 0.0002167]
	Learning Rate: 0.000216705
	LOSS [training: 0.007988246753023688 | validation: 0.020178387550133577]
	TIME [epoch: 8.11 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01563916981169273		[learning rate: 0.00021631]
		[batch 20/20] avg loss: 0.01715304809436835		[learning rate: 0.00021592]
	Learning Rate: 0.000215918
	LOSS [training: 0.016396108953030532 | validation: 0.010635862671543327]
	TIME [epoch: 8.11 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017356440991228062		[learning rate: 0.00021553]
		[batch 20/20] avg loss: 0.01449479742589023		[learning rate: 0.00021513]
	Learning Rate: 0.000215135
	LOSS [training: 0.015925619208559144 | validation: 0.02180784698018256]
	TIME [epoch: 8.1 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010560301800748444		[learning rate: 0.00021474]
		[batch 20/20] avg loss: 0.021981409258156163		[learning rate: 0.00021435]
	Learning Rate: 0.000214354
	LOSS [training: 0.016270855529452306 | validation: 0.025220194350801882]
	TIME [epoch: 8.1 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016575217572974758		[learning rate: 0.00021396]
		[batch 20/20] avg loss: 0.01681760647403526		[learning rate: 0.00021358]
	Learning Rate: 0.000213576
	LOSS [training: 0.016696412023505008 | validation: 0.022247528838547165]
	TIME [epoch: 8.09 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01898779241056415		[learning rate: 0.00021319]
		[batch 20/20] avg loss: 0.025252071581021485		[learning rate: 0.0002128]
	Learning Rate: 0.000212801
	LOSS [training: 0.02211993199579281 | validation: 0.026019262722525124]
	TIME [epoch: 8.13 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017777558554434015		[learning rate: 0.00021241]
		[batch 20/20] avg loss: 0.012725898717938037		[learning rate: 0.00021203]
	Learning Rate: 0.000212029
	LOSS [training: 0.015251728636186023 | validation: 0.016162820499969643]
	TIME [epoch: 8.1 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008992220402683373		[learning rate: 0.00021164]
		[batch 20/20] avg loss: 0.013671161013270604		[learning rate: 0.00021126]
	Learning Rate: 0.000211259
	LOSS [training: 0.011331690707976988 | validation: 0.012328550399816115]
	TIME [epoch: 8.09 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011221580532613958		[learning rate: 0.00021088]
		[batch 20/20] avg loss: 0.02697450070354272		[learning rate: 0.00021049]
	Learning Rate: 0.000210493
	LOSS [training: 0.01909804061807834 | validation: 0.021840102663972474]
	TIME [epoch: 8.09 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014950275378115298		[learning rate: 0.00021011]
		[batch 20/20] avg loss: 0.01906447937838124		[learning rate: 0.00020973]
	Learning Rate: 0.000209729
	LOSS [training: 0.017007377378248267 | validation: 0.016246468200024716]
	TIME [epoch: 8.11 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011331408813117108		[learning rate: 0.00020935]
		[batch 20/20] avg loss: 0.011879387476685644		[learning rate: 0.00020897]
	Learning Rate: 0.000208968
	LOSS [training: 0.011605398144901375 | validation: 0.015251464270542681]
	TIME [epoch: 8.1 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012522609409780413		[learning rate: 0.00020859]
		[batch 20/20] avg loss: 0.015917532277744527		[learning rate: 0.00020821]
	Learning Rate: 0.000208209
	LOSS [training: 0.01422007084376247 | validation: 0.038600709714673484]
	TIME [epoch: 8.09 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017454863443997118		[learning rate: 0.00020783]
		[batch 20/20] avg loss: 0.020823457820647578		[learning rate: 0.00020745]
	Learning Rate: 0.000207454
	LOSS [training: 0.019139160632322346 | validation: 0.011433733041991051]
	TIME [epoch: 8.09 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01353819156174508		[learning rate: 0.00020708]
		[batch 20/20] avg loss: 0.01746085508995373		[learning rate: 0.0002067]
	Learning Rate: 0.000206701
	LOSS [training: 0.0154995233258494 | validation: 0.023597232298641024]
	TIME [epoch: 8.09 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038565459967785855		[learning rate: 0.00020633]
		[batch 20/20] avg loss: 0.011537454587277129		[learning rate: 0.00020595]
	Learning Rate: 0.000205951
	LOSS [training: 0.025051457277531497 | validation: 0.024792970172542415]
	TIME [epoch: 8.12 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012891506983050133		[learning rate: 0.00020558]
		[batch 20/20] avg loss: 0.012932181485913593		[learning rate: 0.0002052]
	Learning Rate: 0.000205203
	LOSS [training: 0.012911844234481861 | validation: 0.026630634057017655]
	TIME [epoch: 8.1 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014299100656511046		[learning rate: 0.00020483]
		[batch 20/20] avg loss: 0.012841568761339775		[learning rate: 0.00020446]
	Learning Rate: 0.000204459
	LOSS [training: 0.013570334708925413 | validation: 0.016999372072464276]
	TIME [epoch: 8.09 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009541656861002555		[learning rate: 0.00020409]
		[batch 20/20] avg loss: 0.013341271160942159		[learning rate: 0.00020372]
	Learning Rate: 0.000203717
	LOSS [training: 0.011441464010972357 | validation: 0.02539503833516451]
	TIME [epoch: 8.09 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013941118453347293		[learning rate: 0.00020335]
		[batch 20/20] avg loss: 0.008599938371010899		[learning rate: 0.00020298]
	Learning Rate: 0.000202977
	LOSS [training: 0.011270528412179095 | validation: 0.022369986686024926]
	TIME [epoch: 8.11 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007295891711371928		[learning rate: 0.00020261]
		[batch 20/20] avg loss: 0.01602079003396336		[learning rate: 0.00020224]
	Learning Rate: 0.000202241
	LOSS [training: 0.011658340872667646 | validation: 0.027067660477389795]
	TIME [epoch: 8.11 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012703644796166202		[learning rate: 0.00020187]
		[batch 20/20] avg loss: 0.006717402615780551		[learning rate: 0.00020151]
	Learning Rate: 0.000201507
	LOSS [training: 0.009710523705973376 | validation: 0.017635977742573025]
	TIME [epoch: 8.1 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014276550105919012		[learning rate: 0.00020114]
		[batch 20/20] avg loss: 0.014018866505497182		[learning rate: 0.00020078]
	Learning Rate: 0.000200775
	LOSS [training: 0.014147708305708096 | validation: 0.0392533087636664]
	TIME [epoch: 8.1 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03703939427953889		[learning rate: 0.00020041]
		[batch 20/20] avg loss: 0.01283722910664228		[learning rate: 0.00020005]
	Learning Rate: 0.000200047
	LOSS [training: 0.024938311693090586 | validation: 0.02461884454384607]
	TIME [epoch: 8.1 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0265047989042758		[learning rate: 0.00019968]
		[batch 20/20] avg loss: 0.010330703755898323		[learning rate: 0.00019932]
	Learning Rate: 0.000199321
	LOSS [training: 0.01841775133008706 | validation: 0.013100072289178576]
	TIME [epoch: 8.13 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014014683712092768		[learning rate: 0.00019896]
		[batch 20/20] avg loss: 0.01584486900249418		[learning rate: 0.0001986]
	Learning Rate: 0.000198597
	LOSS [training: 0.01492977635729347 | validation: 0.016818884794720847]
	TIME [epoch: 8.1 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020034336265887756		[learning rate: 0.00019824]
		[batch 20/20] avg loss: 0.014676868502633477		[learning rate: 0.00019788]
	Learning Rate: 0.000197877
	LOSS [training: 0.017355602384260618 | validation: 0.016452086101220398]
	TIME [epoch: 8.1 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011870123099726898		[learning rate: 0.00019752]
		[batch 20/20] avg loss: 0.019315358123779186		[learning rate: 0.00019716]
	Learning Rate: 0.000197159
	LOSS [training: 0.015592740611753045 | validation: 0.007284844036781077]
	TIME [epoch: 8.09 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_1180.pth
	Model improved!!!
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014769766106570267		[learning rate: 0.0001968]
		[batch 20/20] avg loss: 0.014759008901650028		[learning rate: 0.00019644]
	Learning Rate: 0.000196443
	LOSS [training: 0.014764387504110146 | validation: 0.024057906325161103]
	TIME [epoch: 8.11 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015445378151154682		[learning rate: 0.00019609]
		[batch 20/20] avg loss: 0.018167383913075985		[learning rate: 0.00019573]
	Learning Rate: 0.00019573
	LOSS [training: 0.016806381032115333 | validation: 0.022935405492763763]
	TIME [epoch: 8.1 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01023167909060264		[learning rate: 0.00019537]
		[batch 20/20] avg loss: 0.012647492626914076		[learning rate: 0.00019502]
	Learning Rate: 0.00019502
	LOSS [training: 0.01143958585875836 | validation: 0.025900755764251257]
	TIME [epoch: 8.09 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015611325920422862		[learning rate: 0.00019467]
		[batch 20/20] avg loss: 0.012404444164045235		[learning rate: 0.00019431]
	Learning Rate: 0.000194312
	LOSS [training: 0.014007885042234047 | validation: 0.011001151133052547]
	TIME [epoch: 8.09 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005850959837916297		[learning rate: 0.00019396]
		[batch 20/20] avg loss: 0.019582018022033153		[learning rate: 0.00019361]
	Learning Rate: 0.000193607
	LOSS [training: 0.012716488929974725 | validation: 0.02781851371005993]
	TIME [epoch: 8.09 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009091878372744382		[learning rate: 0.00019326]
		[batch 20/20] avg loss: 0.013637262804520356		[learning rate: 0.0001929]
	Learning Rate: 0.000192904
	LOSS [training: 0.011364570588632371 | validation: 0.01552471449561961]
	TIME [epoch: 8.12 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012309461941401041		[learning rate: 0.00019255]
		[batch 20/20] avg loss: 0.010507962008525756		[learning rate: 0.0001922]
	Learning Rate: 0.000192204
	LOSS [training: 0.0114087119749634 | validation: 0.027384821112181025]
	TIME [epoch: 8.09 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027110669540661964		[learning rate: 0.00019186]
		[batch 20/20] avg loss: 0.01191051658261615		[learning rate: 0.00019151]
	Learning Rate: 0.000191507
	LOSS [training: 0.019510593061639055 | validation: 0.015859989295884114]
	TIME [epoch: 8.1 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0081262146750234		[learning rate: 0.00019116]
		[batch 20/20] avg loss: 0.011288114524511976		[learning rate: 0.00019081]
	Learning Rate: 0.000190812
	LOSS [training: 0.009707164599767686 | validation: 0.01705075120298273]
	TIME [epoch: 8.09 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014339134486590716		[learning rate: 0.00019047]
		[batch 20/20] avg loss: 0.01416904725205132		[learning rate: 0.00019012]
	Learning Rate: 0.000190119
	LOSS [training: 0.014254090869321018 | validation: 0.015544740929991805]
	TIME [epoch: 8.12 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016383604963320083		[learning rate: 0.00018977]
		[batch 20/20] avg loss: 0.015973857495288395		[learning rate: 0.00018943]
	Learning Rate: 0.000189429
	LOSS [training: 0.01617873122930424 | validation: 0.02176784899418519]
	TIME [epoch: 8.1 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018358446335095348		[learning rate: 0.00018909]
		[batch 20/20] avg loss: 0.01101988048133898		[learning rate: 0.00018874]
	Learning Rate: 0.000188742
	LOSS [training: 0.01468916340821716 | validation: 0.01052112392216998]
	TIME [epoch: 8.1 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012858340118495497		[learning rate: 0.0001884]
		[batch 20/20] avg loss: 0.007564094221466329		[learning rate: 0.00018806]
	Learning Rate: 0.000188057
	LOSS [training: 0.010211217169980915 | validation: 0.011234563252525004]
	TIME [epoch: 8.09 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01215309423366422		[learning rate: 0.00018772]
		[batch 20/20] avg loss: 0.012714483888927259		[learning rate: 0.00018737]
	Learning Rate: 0.000187375
	LOSS [training: 0.012433789061295738 | validation: 0.013036388244118496]
	TIME [epoch: 8.1 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01601974502935469		[learning rate: 0.00018703]
		[batch 20/20] avg loss: 0.0349016806319892		[learning rate: 0.00018669]
	Learning Rate: 0.000186695
	LOSS [training: 0.02546071283067195 | validation: 0.017992790708412507]
	TIME [epoch: 8.12 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012970715789075559		[learning rate: 0.00018636]
		[batch 20/20] avg loss: 0.011022682692364382		[learning rate: 0.00018602]
	Learning Rate: 0.000186017
	LOSS [training: 0.011996699240719968 | validation: 0.014536815550197087]
	TIME [epoch: 8.1 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03052590383598089		[learning rate: 0.00018568]
		[batch 20/20] avg loss: 0.0133438849439475		[learning rate: 0.00018534]
	Learning Rate: 0.000185342
	LOSS [training: 0.021934894389964193 | validation: 0.018596547705517368]
	TIME [epoch: 8.1 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01817756013755763		[learning rate: 0.00018501]
		[batch 20/20] avg loss: 0.012447678709714059		[learning rate: 0.00018467]
	Learning Rate: 0.000184669
	LOSS [training: 0.015312619423635843 | validation: 0.013480078120364484]
	TIME [epoch: 8.1 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01730073976263866		[learning rate: 0.00018433]
		[batch 20/20] avg loss: 0.009875809448133094		[learning rate: 0.000184]
	Learning Rate: 0.000183999
	LOSS [training: 0.013588274605385878 | validation: 0.008409415311999985]
	TIME [epoch: 8.13 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013209028503641449		[learning rate: 0.00018367]
		[batch 20/20] avg loss: 0.012011922019425211		[learning rate: 0.00018333]
	Learning Rate: 0.000183331
	LOSS [training: 0.012610475261533333 | validation: 0.02978119552200415]
	TIME [epoch: 8.1 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015680384424880046		[learning rate: 0.000183]
		[batch 20/20] avg loss: 0.013177606650162998		[learning rate: 0.00018267]
	Learning Rate: 0.000182666
	LOSS [training: 0.014428995537521519 | validation: 0.012237291817855229]
	TIME [epoch: 8.1 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018576324543300556		[learning rate: 0.00018233]
		[batch 20/20] avg loss: 0.013016628281127934		[learning rate: 0.000182]
	Learning Rate: 0.000182003
	LOSS [training: 0.015796476412214242 | validation: 0.01165081233671476]
	TIME [epoch: 8.1 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021043367064249804		[learning rate: 0.00018167]
		[batch 20/20] avg loss: 0.013971614856939341		[learning rate: 0.00018134]
	Learning Rate: 0.000181343
	LOSS [training: 0.01750749096059457 | validation: 0.023779716016354886]
	TIME [epoch: 8.11 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013951048904009142		[learning rate: 0.00018101]
		[batch 20/20] avg loss: 0.01602871978426476		[learning rate: 0.00018068]
	Learning Rate: 0.000180685
	LOSS [training: 0.01498988434413695 | validation: 0.018522516722411175]
	TIME [epoch: 8.12 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021421481954620437		[learning rate: 0.00018036]
		[batch 20/20] avg loss: 0.013169451642686153		[learning rate: 0.00018003]
	Learning Rate: 0.000180029
	LOSS [training: 0.017295466798653295 | validation: 0.014864616473761139]
	TIME [epoch: 8.11 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009044670375809825		[learning rate: 0.0001797]
		[batch 20/20] avg loss: 0.013830622546222776		[learning rate: 0.00017938]
	Learning Rate: 0.000179376
	LOSS [training: 0.0114376464610163 | validation: 0.02015577762683371]
	TIME [epoch: 8.1 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021486737887751326		[learning rate: 0.00017905]
		[batch 20/20] avg loss: 0.007106837790590318		[learning rate: 0.00017872]
	Learning Rate: 0.000178725
	LOSS [training: 0.014296787839170821 | validation: 0.005721066755043831]
	TIME [epoch: 8.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_1207.pth
	Model improved!!!
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009665314387295237		[learning rate: 0.0001784]
		[batch 20/20] avg loss: 0.02413961154776396		[learning rate: 0.00017808]
	Learning Rate: 0.000178076
	LOSS [training: 0.0169024629675296 | validation: 0.019696779937123016]
	TIME [epoch: 8.12 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025682897507201298		[learning rate: 0.00017775]
		[batch 20/20] avg loss: 0.014415061654749251		[learning rate: 0.00017743]
	Learning Rate: 0.00017743
	LOSS [training: 0.02004897958097528 | validation: 0.04523499680093062]
	TIME [epoch: 8.09 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04643850717343813		[learning rate: 0.00017711]
		[batch 20/20] avg loss: 0.012731844209781412		[learning rate: 0.00017679]
	Learning Rate: 0.000176786
	LOSS [training: 0.029585175691609768 | validation: 0.013180175065101003]
	TIME [epoch: 8.09 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01492310128045272		[learning rate: 0.00017646]
		[batch 20/20] avg loss: 0.01082166999183638		[learning rate: 0.00017614]
	Learning Rate: 0.000176144
	LOSS [training: 0.012872385636144549 | validation: 0.01430590441937192]
	TIME [epoch: 8.09 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017681755037357232		[learning rate: 0.00017582]
		[batch 20/20] avg loss: 0.012829147877409122		[learning rate: 0.0001755]
	Learning Rate: 0.000175505
	LOSS [training: 0.01525545145738318 | validation: 0.020308118853843538]
	TIME [epoch: 8.11 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011199542511219658		[learning rate: 0.00017519]
		[batch 20/20] avg loss: 0.009183758878254324		[learning rate: 0.00017487]
	Learning Rate: 0.000174868
	LOSS [training: 0.01019165069473699 | validation: 0.006484870526707681]
	TIME [epoch: 8.1 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018998398134231563		[learning rate: 0.00017455]
		[batch 20/20] avg loss: 0.008555729786257342		[learning rate: 0.00017423]
	Learning Rate: 0.000174233
	LOSS [training: 0.01377706396024445 | validation: 0.010727987636255272]
	TIME [epoch: 8.1 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017147701575956856		[learning rate: 0.00017392]
		[batch 20/20] avg loss: 0.008524859119773921		[learning rate: 0.0001736]
	Learning Rate: 0.000173601
	LOSS [training: 0.012836280347865386 | validation: 0.015548874020394891]
	TIME [epoch: 8.09 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0077432736992163785		[learning rate: 0.00017329]
		[batch 20/20] avg loss: 0.014362235360728361		[learning rate: 0.00017297]
	Learning Rate: 0.000172971
	LOSS [training: 0.011052754529972371 | validation: 0.003696572052831983]
	TIME [epoch: 8.09 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_1216.pth
	Model improved!!!
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015481548186598198		[learning rate: 0.00017266]
		[batch 20/20] avg loss: 0.0176751130171086		[learning rate: 0.00017234]
	Learning Rate: 0.000172343
	LOSS [training: 0.0165783306018534 | validation: 0.008762855541934938]
	TIME [epoch: 8.11 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010368378142921495		[learning rate: 0.00017203]
		[batch 20/20] avg loss: 0.017556418794812435		[learning rate: 0.00017172]
	Learning Rate: 0.000171718
	LOSS [training: 0.013962398468866965 | validation: 0.0073887391210391866]
	TIME [epoch: 8.09 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016101831922114535		[learning rate: 0.00017141]
		[batch 20/20] avg loss: 0.016369213272078065		[learning rate: 0.00017109]
	Learning Rate: 0.000171095
	LOSS [training: 0.0162355225970963 | validation: 0.0027357008831102344]
	TIME [epoch: 8.09 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_1219.pth
	Model improved!!!
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011900160701929615		[learning rate: 0.00017078]
		[batch 20/20] avg loss: 0.00835751274303866		[learning rate: 0.00017047]
	Learning Rate: 0.000170474
	LOSS [training: 0.010128836722484139 | validation: 0.012508763977261991]
	TIME [epoch: 8.09 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02269990489636594		[learning rate: 0.00017016]
		[batch 20/20] avg loss: 0.013906953337841916		[learning rate: 0.00016986]
	Learning Rate: 0.000169855
	LOSS [training: 0.018303429117103934 | validation: 0.012517858276818639]
	TIME [epoch: 8.11 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015039194729952127		[learning rate: 0.00016955]
		[batch 20/20] avg loss: 0.008503682977895896		[learning rate: 0.00016924]
	Learning Rate: 0.000169239
	LOSS [training: 0.011771438853924013 | validation: 0.01715889928287293]
	TIME [epoch: 8.1 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016833823651993828		[learning rate: 0.00016893]
		[batch 20/20] avg loss: 0.008905095851437353		[learning rate: 0.00016862]
	Learning Rate: 0.000168625
	LOSS [training: 0.012869459751715587 | validation: 0.009149071421151506]
	TIME [epoch: 8.09 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015616858594592784		[learning rate: 0.00016832]
		[batch 20/20] avg loss: 0.007428124735274296		[learning rate: 0.00016801]
	Learning Rate: 0.000168013
	LOSS [training: 0.01152249166493354 | validation: -0.0004205685180052594]
	TIME [epoch: 8.09 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study202/model_tr_study202_r5_20240219_233648/states/model_tr_study202_1224.pth
	Model improved!!!
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010969183433190902		[learning rate: 0.00016771]
		[batch 20/20] avg loss: 0.011996759819286815		[learning rate: 0.0001674]
	Learning Rate: 0.000167403
	LOSS [training: 0.011482971626238857 | validation: 0.015833362181366443]
	TIME [epoch: 8.09 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009569430761174957		[learning rate: 0.0001671]
		[batch 20/20] avg loss: 0.016254469388960278		[learning rate: 0.0001668]
	Learning Rate: 0.000166795
	LOSS [training: 0.012911950075067615 | validation: 0.012293950976476974]
	TIME [epoch: 8.11 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014923633539264496		[learning rate: 0.00016649]
		[batch 20/20] avg loss: 0.012523517036452117		[learning rate: 0.00016619]
	Learning Rate: 0.00016619
	LOSS [training: 0.013723575287858308 | validation: 0.015808225864896414]
	TIME [epoch: 8.09 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02155954951768459		[learning rate: 0.00016589]
		[batch 20/20] avg loss: 0.0160194861931034		[learning rate: 0.00016559]
	Learning Rate: 0.000165587
	LOSS [training: 0.018789517855393997 | validation: 0.005873649766997037]
	TIME [epoch: 8.09 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010437836976042478		[learning rate: 0.00016529]
		[batch 20/20] avg loss: 0.014889125542123644		[learning rate: 0.00016499]
	Learning Rate: 0.000164986
	LOSS [training: 0.012663481259083061 | validation: 0.016315602060740182]
	TIME [epoch: 8.09 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009775319485065822		[learning rate: 0.00016469]
		[batch 20/20] avg loss: 0.010345837615548548		[learning rate: 0.00016439]
	Learning Rate: 0.000164387
	LOSS [training: 0.010060578550307184 | validation: 0.01568208454420551]
	TIME [epoch: 8.11 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007109005955377131		[learning rate: 0.00016409]
		[batch 20/20] avg loss: 0.01883750848083091		[learning rate: 0.00016379]
	Learning Rate: 0.000163791
	LOSS [training: 0.01297325721810402 | validation: 0.0200536844181929]
	TIME [epoch: 8.09 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010565989892086583		[learning rate: 0.00016349]
		[batch 20/20] avg loss: 0.015003558846429238		[learning rate: 0.0001632]
	Learning Rate: 0.000163196
	LOSS [training: 0.01278477436925791 | validation: 0.03509977932754727]
	TIME [epoch: 8.09 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02746653058601472		[learning rate: 0.0001629]
		[batch 20/20] avg loss: 0.024251930820948425		[learning rate: 0.0001626]
	Learning Rate: 0.000162604
	LOSS [training: 0.025859230703481572 | validation: 0.014264969930800894]
	TIME [epoch: 8.09 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014338511103538898		[learning rate: 0.00016231]
		[batch 20/20] avg loss: 0.0103345084689478		[learning rate: 0.00016201]
	Learning Rate: 0.000162014
	LOSS [training: 0.012336509786243349 | validation: 0.01750667046083214]
	TIME [epoch: 8.09 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010376062251237274		[learning rate: 0.00016172]
		[batch 20/20] avg loss: 0.01138988541725633		[learning rate: 0.00016143]
	Learning Rate: 0.000161426
	LOSS [training: 0.010882973834246802 | validation: 0.01206731802031659]
	TIME [epoch: 8.11 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008701539953083554		[learning rate: 0.00016113]
		[batch 20/20] avg loss: 0.009102023485504124		[learning rate: 0.00016084]
	Learning Rate: 0.00016084
	LOSS [training: 0.008901781719293839 | validation: 0.011011818282717797]
	TIME [epoch: 8.09 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002386662476301029		[learning rate: 0.00016055]
		[batch 20/20] avg loss: 0.008054835622314451		[learning rate: 0.00016026]
	Learning Rate: 0.000160257
	LOSS [training: 0.00522074904930774 | validation: 0.009271760838970553]
	TIME [epoch: 8.08 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011649484804376716		[learning rate: 0.00015997]
		[batch 20/20] avg loss: 0.00642584069435526		[learning rate: 0.00015967]
	Learning Rate: 0.000159675
	LOSS [training: 0.009037662749365988 | validation: 0.016638419570089994]
	TIME [epoch: 8.09 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010324489384324662		[learning rate: 0.00015938]
		[batch 20/20] avg loss: 0.0033351239543511165		[learning rate: 0.0001591]
	Learning Rate: 0.000159096
	LOSS [training: 0.00682980666933789 | validation: 0.017808983912002987]
	TIME [epoch: 8.11 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004925512085772995		[learning rate: 0.00015881]
		[batch 20/20] avg loss: 0.006328311697761095		[learning rate: 0.00015852]
	Learning Rate: 0.000158518
	LOSS [training: 0.005626911891767046 | validation: 0.021198223891102222]
	TIME [epoch: 8.09 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009403631590694003		[learning rate: 0.00015823]
		[batch 20/20] avg loss: 0.012889780370229939		[learning rate: 0.00015794]
	Learning Rate: 0.000157943
	LOSS [training: 0.011146705980461972 | validation: 0.010972350789587346]
	TIME [epoch: 8.09 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010905117826499458		[learning rate: 0.00015766]
		[batch 20/20] avg loss: 0.01045419485822786		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.01067965634236366 | validation: 0.016843375230451502]
	TIME [epoch: 8.09 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007301559925174431		[learning rate: 0.00015708]
		[batch 20/20] avg loss: 0.017654317752157606		[learning rate: 0.0001568]
	Learning Rate: 0.000156799
	LOSS [training: 0.012477938838666018 | validation: 0.013129527682136688]
	TIME [epoch: 8.09 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008646752660365629		[learning rate: 0.00015651]
		[batch 20/20] avg loss: 0.010184385108378212		[learning rate: 0.00015623]
	Learning Rate: 0.00015623
	LOSS [training: 0.00941556888437192 | validation: 0.01850633884999391]
	TIME [epoch: 8.11 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01092871577153538		[learning rate: 0.00015595]
		[batch 20/20] avg loss: 0.011344443396115369		[learning rate: 0.00015566]
	Learning Rate: 0.000155663
	LOSS [training: 0.011136579583825376 | validation: 0.01098548490776085]
	TIME [epoch: 8.09 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011164957342728266		[learning rate: 0.00015538]
		[batch 20/20] avg loss: 0.008134778710243328		[learning rate: 0.0001551]
	Learning Rate: 0.000155098
	LOSS [training: 0.009649868026485797 | validation: 0.01955329147343339]
	TIME [epoch: 8.09 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014410004119662659		[learning rate: 0.00015482]
		[batch 20/20] avg loss: 0.01344762257973032		[learning rate: 0.00015453]
	Learning Rate: 0.000154535
	LOSS [training: 0.013928813349696489 | validation: 0.012762001776496786]
	TIME [epoch: 8.09 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010527750395847472		[learning rate: 0.00015425]
		[batch 20/20] avg loss: 0.010697885555807071		[learning rate: 0.00015397]
	Learning Rate: 0.000153974
	LOSS [training: 0.010612817975827272 | validation: 0.00954169683165451]
	TIME [epoch: 8.11 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01475332435177798		[learning rate: 0.00015369]
		[batch 20/20] avg loss: 0.0032184453562693915		[learning rate: 0.00015342]
	Learning Rate: 0.000153415
	LOSS [training: 0.008985884854023688 | validation: 0.015665442346685526]
	TIME [epoch: 8.09 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005859728206450722		[learning rate: 0.00015314]
		[batch 20/20] avg loss: 0.005027499261891692		[learning rate: 0.00015286]
	Learning Rate: 0.000152858
	LOSS [training: 0.005443613734171208 | validation: 0.008084280638363231]
	TIME [epoch: 8.08 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016128569686589562		[learning rate: 0.00015258]
		[batch 20/20] avg loss: 0.008871728832862448		[learning rate: 0.0001523]
	Learning Rate: 0.000152304
	LOSS [training: 0.012500149259726003 | validation: 0.012639097352463304]
	TIME [epoch: 8.09 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015364205842904408		[learning rate: 0.00015203]
		[batch 20/20] avg loss: 0.021077587297065447		[learning rate: 0.00015175]
	Learning Rate: 0.000151751
	LOSS [training: 0.018220896569984928 | validation: 0.026663898287245455]
	TIME [epoch: 8.09 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02483462434578218		[learning rate: 0.00015148]
		[batch 20/20] avg loss: 0.005388082854361568		[learning rate: 0.0001512]
	Learning Rate: 0.0001512
	LOSS [training: 0.015111353600071879 | validation: 0.011356274484900775]
	TIME [epoch: 8.11 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01467788422399319		[learning rate: 0.00015093]
		[batch 20/20] avg loss: 0.014369165599933315		[learning rate: 0.00015065]
	Learning Rate: 0.000150652
	LOSS [training: 0.014523524911963251 | validation: 0.016844048358927687]
	TIME [epoch: 8.09 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007164210966515613		[learning rate: 0.00015038]
		[batch 20/20] avg loss: 0.011431563576807057		[learning rate: 0.0001501]
	Learning Rate: 0.000150105
	LOSS [training: 0.009297887271661335 | validation: 0.005730709903034316]
	TIME [epoch: 8.09 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006626664682666164		[learning rate: 0.00014983]
		[batch 20/20] avg loss: 0.006919269112789957		[learning rate: 0.00014956]
	Learning Rate: 0.00014956
	LOSS [training: 0.0067729668977280595 | validation: 0.01696602400539478]
	TIME [epoch: 8.09 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022759628783862847		[learning rate: 0.00014929]
		[batch 20/20] avg loss: 0.04076086768355104		[learning rate: 0.00014902]
	Learning Rate: 0.000149017
	LOSS [training: 0.03176024823370695 | validation: 0.011452359174338773]
	TIME [epoch: 8.11 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005982861579764528		[learning rate: 0.00014875]
		[batch 20/20] avg loss: 0.008600137847981318		[learning rate: 0.00014848]
	Learning Rate: 0.000148477
	LOSS [training: 0.007291499713872923 | validation: 0.014249158882712519]
	TIME [epoch: 8.09 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007157283688173026		[learning rate: 0.00014821]
		[batch 20/20] avg loss: 0.01279339588689776		[learning rate: 0.00014794]
	Learning Rate: 0.000147938
	LOSS [training: 0.009975339787535393 | validation: 0.01767610788782767]
	TIME [epoch: 8.09 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008901295872637527		[learning rate: 0.00014767]
		[batch 20/20] avg loss: 0.008554761183359537		[learning rate: 0.0001474]
	Learning Rate: 0.000147401
	LOSS [training: 0.008728028527998533 | validation: 0.02157602464939561]
	TIME [epoch: 8.09 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01341238876146678		[learning rate: 0.00014713]
		[batch 20/20] avg loss: 0.015352102056728456		[learning rate: 0.00014687]
	Learning Rate: 0.000146866
	LOSS [training: 0.014382245409097619 | validation: 0.021175833004065295]
	TIME [epoch: 8.1 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009269499437174875		[learning rate: 0.0001466]
		[batch 20/20] avg loss: 0.011931864010064724		[learning rate: 0.00014633]
	Learning Rate: 0.000146333
	LOSS [training: 0.0106006817236198 | validation: 0.020876133362134994]
	TIME [epoch: 8.11 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008900071124520303		[learning rate: 0.00014607]
		[batch 20/20] avg loss: 0.0035142007627761663		[learning rate: 0.0001458]
	Learning Rate: 0.000145802
	LOSS [training: 0.006207135943648235 | validation: 0.0028314727563346414]
	TIME [epoch: 8.09 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005338874185204229		[learning rate: 0.00014554]
		[batch 20/20] avg loss: 0.021224430649405678		[learning rate: 0.00014527]
	Learning Rate: 0.000145273
	LOSS [training: 0.013281652417304954 | validation: 0.037678983364004506]
	TIME [epoch: 8.09 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015356740693365015		[learning rate: 0.00014501]
		[batch 20/20] avg loss: 0.01487583778837311		[learning rate: 0.00014475]
	Learning Rate: 0.000144746
	LOSS [training: 0.015116289240869057 | validation: 0.013713185684485544]
	TIME [epoch: 8.09 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00725919799917341		[learning rate: 0.00014448]
		[batch 20/20] avg loss: 0.0038056427665646916		[learning rate: 0.00014422]
	Learning Rate: 0.00014422
	LOSS [training: 0.00553242038286905 | validation: 0.016590271029327065]
	TIME [epoch: 8.11 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010309032856201708		[learning rate: 0.00014396]
		[batch 20/20] avg loss: 0.009207808194404745		[learning rate: 0.0001437]
	Learning Rate: 0.000143697
	LOSS [training: 0.009758420525303227 | validation: 0.01632431909887801]
	TIME [epoch: 8.09 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008016850146500979		[learning rate: 0.00014344]
		[batch 20/20] avg loss: 0.007625818502886179		[learning rate: 0.00014318]
	Learning Rate: 0.000143175
	LOSS [training: 0.007821334324693579 | validation: 0.011953853391734106]
	TIME [epoch: 8.08 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009018690687011036		[learning rate: 0.00014292]
		[batch 20/20] avg loss: 0.004620553589941226		[learning rate: 0.00014266]
	Learning Rate: 0.000142656
	LOSS [training: 0.006819622138476131 | validation: 0.021642597049088285]
	TIME [epoch: 8.09 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010338882424146533		[learning rate: 0.0001424]
		[batch 20/20] avg loss: 0.012058159022420262		[learning rate: 0.00014214]
	Learning Rate: 0.000142138
	LOSS [training: 0.011198520723283397 | validation: 0.016547526861301037]
	TIME [epoch: 8.1 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003951521682167026		[learning rate: 0.00014188]
		[batch 20/20] avg loss: 0.013598610685465829		[learning rate: 0.00014162]
	Learning Rate: 0.000141622
	LOSS [training: 0.008775066183816427 | validation: 0.017213375293460467]
	TIME [epoch: 8.1 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012675771229900282		[learning rate: 0.00014137]
		[batch 20/20] avg loss: 0.004122885129862476		[learning rate: 0.00014111]
	Learning Rate: 0.000141108
	LOSS [training: 0.00839932817988138 | validation: 0.014165312942167267]
	TIME [epoch: 8.08 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018443030228219297		[learning rate: 0.00014085]
		[batch 20/20] avg loss: 0.01569898636615668		[learning rate: 0.0001406]
	Learning Rate: 0.000140596
	LOSS [training: 0.017071008297187988 | validation: 0.03146404666525275]
	TIME [epoch: 8.09 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010788161400630672		[learning rate: 0.00014034]
		[batch 20/20] avg loss: 0.009849298142333845		[learning rate: 0.00014009]
	Learning Rate: 0.000140086
	LOSS [training: 0.010318729771482259 | validation: 0.014501633482840242]
	TIME [epoch: 8.08 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01114822941242104		[learning rate: 0.00013983]
		[batch 20/20] avg loss: 0.012253917568342168		[learning rate: 0.00013958]
	Learning Rate: 0.000139578
	LOSS [training: 0.011701073490381604 | validation: 0.016042454212441236]
	TIME [epoch: 8.11 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00783984253625378		[learning rate: 0.00013932]
		[batch 20/20] avg loss: 0.013761239841881942		[learning rate: 0.00013907]
	Learning Rate: 0.000139071
	LOSS [training: 0.01080054118906786 | validation: 0.013756516112445804]
	TIME [epoch: 8.09 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00779037510781839		[learning rate: 0.00013882]
		[batch 20/20] avg loss: 0.01102107874243039		[learning rate: 0.00013857]
	Learning Rate: 0.000138566
	LOSS [training: 0.009405726925124388 | validation: 0.008986644152300006]
	TIME [epoch: 8.08 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006925194317739085		[learning rate: 0.00013831]
		[batch 20/20] avg loss: 0.005572335372920155		[learning rate: 0.00013806]
	Learning Rate: 0.000138064
	LOSS [training: 0.006248764845329619 | validation: 0.014925860148450314]
	TIME [epoch: 8.09 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013288311963389287		[learning rate: 0.00013781]
		[batch 20/20] avg loss: 0.01017299806725011		[learning rate: 0.00013756]
	Learning Rate: 0.000137562
	LOSS [training: 0.0117306550153197 | validation: 0.016338861866002967]
	TIME [epoch: 8.1 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014765538724477178		[learning rate: 0.00013731]
		[batch 20/20] avg loss: 0.0065144687180341745		[learning rate: 0.00013706]
	Learning Rate: 0.000137063
	LOSS [training: 0.010640003721255677 | validation: 0.018972999280909376]
	TIME [epoch: 8.1 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00896794833429869		[learning rate: 0.00013681]
		[batch 20/20] avg loss: 0.011233025110777544		[learning rate: 0.00013657]
	Learning Rate: 0.000136566
	LOSS [training: 0.010100486722538117 | validation: 0.019097998626598375]
	TIME [epoch: 8.08 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008211776213321285		[learning rate: 0.00013632]
		[batch 20/20] avg loss: 0.01109193878314715		[learning rate: 0.00013607]
	Learning Rate: 0.00013607
	LOSS [training: 0.009651857498234216 | validation: 0.010095816040373407]
	TIME [epoch: 8.09 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007296978194499147		[learning rate: 0.00013582]
		[batch 20/20] avg loss: 0.026260101548312088		[learning rate: 0.00013558]
	Learning Rate: 0.000135576
	LOSS [training: 0.016778539871405616 | validation: 0.0068696106843569955]
	TIME [epoch: 8.09 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00836294837109258		[learning rate: 0.00013533]
		[batch 20/20] avg loss: 0.00492152460701697		[learning rate: 0.00013508]
	Learning Rate: 0.000135084
	LOSS [training: 0.006642236489054776 | validation: 0.015419875478202134]
	TIME [epoch: 8.11 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010155632637333042		[learning rate: 0.00013484]
		[batch 20/20] avg loss: 0.01386794630432713		[learning rate: 0.00013459]
	Learning Rate: 0.000134594
	LOSS [training: 0.012011789470830085 | validation: 0.00953312407071581]
	TIME [epoch: 8.09 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009776185321474756		[learning rate: 0.00013435]
		[batch 20/20] avg loss: 0.010332076507378916		[learning rate: 0.00013411]
	Learning Rate: 0.000134106
	LOSS [training: 0.010054130914426834 | validation: 0.012148769270833387]
	TIME [epoch: 8.09 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011656331527004118		[learning rate: 0.00013386]
		[batch 20/20] avg loss: 0.01003454360004408		[learning rate: 0.00013362]
	Learning Rate: 0.000133619
	LOSS [training: 0.010845437563524098 | validation: 0.014328209058677923]
	TIME [epoch: 8.08 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003620499817739592		[learning rate: 0.00013338]
		[batch 20/20] avg loss: 0.007514201067429746		[learning rate: 0.00013313]
	Learning Rate: 0.000133134
	LOSS [training: 0.005567350442584669 | validation: 0.009358361237380569]
	TIME [epoch: 8.1 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007729262847009871		[learning rate: 0.00013289]
		[batch 20/20] avg loss: 0.011334025406783226		[learning rate: 0.00013265]
	Learning Rate: 0.000132651
	LOSS [training: 0.009531644126896548 | validation: 0.0020917064930681536]
	TIME [epoch: 8.1 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008755826511306134		[learning rate: 0.00013241]
		[batch 20/20] avg loss: 0.00881542237245073		[learning rate: 0.00013217]
	Learning Rate: 0.00013217
	LOSS [training: 0.008785624441878432 | validation: 0.007676460675107956]
	TIME [epoch: 8.09 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006887669653815834		[learning rate: 0.00013193]
		[batch 20/20] avg loss: 0.013675587287986393		[learning rate: 0.00013169]
	Learning Rate: 0.00013169
	LOSS [training: 0.010281628470901114 | validation: 0.01475866181071046]
	TIME [epoch: 8.09 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006913699286486115		[learning rate: 0.00013145]
		[batch 20/20] avg loss: 0.010424650330778712		[learning rate: 0.00013121]
	Learning Rate: 0.000131212
	LOSS [training: 0.008669174808632413 | validation: 0.01271699350035045]
	TIME [epoch: 8.09 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008640845126889533		[learning rate: 0.00013097]
		[batch 20/20] avg loss: 0.006225564402643947		[learning rate: 0.00013074]
	Learning Rate: 0.000130736
	LOSS [training: 0.007433204764766742 | validation: 0.011782215614545496]
	TIME [epoch: 8.11 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007467755517176324		[learning rate: 0.0001305]
		[batch 20/20] avg loss: 0.012886948037201806		[learning rate: 0.00013026]
	Learning Rate: 0.000130261
	LOSS [training: 0.010177351777189066 | validation: 0.011445242212783615]
	TIME [epoch: 8.09 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013496368086814658		[learning rate: 0.00013002]
		[batch 20/20] avg loss: 0.009897116454697454		[learning rate: 0.00012979]
	Learning Rate: 0.000129789
	LOSS [training: 0.011696742270756056 | validation: 0.011913044375896374]
	TIME [epoch: 8.09 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007208145509267833		[learning rate: 0.00012955]
		[batch 20/20] avg loss: 0.012471513007557071		[learning rate: 0.00012932]
	Learning Rate: 0.000129318
	LOSS [training: 0.00983982925841245 | validation: 0.013747669845745887]
	TIME [epoch: 8.08 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009647069472740421		[learning rate: 0.00012908]
		[batch 20/20] avg loss: 0.018510809055159443		[learning rate: 0.00012885]
	Learning Rate: 0.000128848
	LOSS [training: 0.014078939263949933 | validation: 0.010567797564134406]
	TIME [epoch: 8.1 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013700142713122304		[learning rate: 0.00012861]
		[batch 20/20] avg loss: 0.00687219409021695		[learning rate: 0.00012838]
	Learning Rate: 0.000128381
	LOSS [training: 0.01028616840166963 | validation: 0.020519946760844763]
	TIME [epoch: 8.1 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012075818543973677		[learning rate: 0.00012815]
		[batch 20/20] avg loss: 0.011768114508199071		[learning rate: 0.00012791]
	Learning Rate: 0.000127915
	LOSS [training: 0.011921966526086372 | validation: 0.0073290988120369785]
	TIME [epoch: 8.09 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008746458890372429		[learning rate: 0.00012768]
		[batch 20/20] avg loss: 0.014636575253624903		[learning rate: 0.00012745]
	Learning Rate: 0.000127451
	LOSS [training: 0.011691517071998666 | validation: 0.024256978284982432]
	TIME [epoch: 8.08 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020409245318373258		[learning rate: 0.00012722]
		[batch 20/20] avg loss: 0.01667145981659639		[learning rate: 0.00012699]
	Learning Rate: 0.000126988
	LOSS [training: 0.018540352567484826 | validation: 0.019446566715472617]
	TIME [epoch: 8.09 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008101049931243859		[learning rate: 0.00012676]
		[batch 20/20] avg loss: 0.011047251673999267		[learning rate: 0.00012653]
	Learning Rate: 0.000126527
	LOSS [training: 0.009574150802621563 | validation: 0.014711952068091741]
	TIME [epoch: 8.11 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008179570436737215		[learning rate: 0.0001263]
		[batch 20/20] avg loss: 0.011952459556027384		[learning rate: 0.00012607]
	Learning Rate: 0.000126068
	LOSS [training: 0.010066014996382298 | validation: 0.020299235488552378]
	TIME [epoch: 8.08 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01881162100017917		[learning rate: 0.00012584]
		[batch 20/20] avg loss: 0.005999540403986267		[learning rate: 0.00012561]
	Learning Rate: 0.000125611
	LOSS [training: 0.012405580702082716 | validation: 0.010867383868451798]
	TIME [epoch: 8.09 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011708150098724179		[learning rate: 0.00012538]
		[batch 20/20] avg loss: 0.005805643979731962		[learning rate: 0.00012515]
	Learning Rate: 0.000125155
	LOSS [training: 0.008756897039228075 | validation: 0.007634701898815085]
	TIME [epoch: 8.08 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009232473817414587		[learning rate: 0.00012493]
		[batch 20/20] avg loss: 0.010776572924076713		[learning rate: 0.0001247]
	Learning Rate: 0.000124701
	LOSS [training: 0.010004523370745649 | validation: 0.0066748488520573974]
	TIME [epoch: 8.1 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013442478555764545		[learning rate: 0.00012447]
		[batch 20/20] avg loss: 0.0028514033572579795		[learning rate: 0.00012425]
	Learning Rate: 0.000124248
	LOSS [training: 0.008146940956511261 | validation: 0.011197103702802892]
	TIME [epoch: 8.09 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011596278528435002		[learning rate: 0.00012402]
		[batch 20/20] avg loss: 0.007388251609878526		[learning rate: 0.0001238]
	Learning Rate: 0.000123797
	LOSS [training: 0.009492265069156767 | validation: 0.025835569876209136]
	TIME [epoch: 8.09 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0071061344799606355		[learning rate: 0.00012357]
		[batch 20/20] avg loss: 0.007759573025379929		[learning rate: 0.00012335]
	Learning Rate: 0.000123348
	LOSS [training: 0.007432853752670281 | validation: 0.009888308293459146]
	TIME [epoch: 8.08 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010057297585484525		[learning rate: 0.00012312]
		[batch 20/20] avg loss: 0.005536709074981036		[learning rate: 0.0001229]
	Learning Rate: 0.0001229
	LOSS [training: 0.007797003330232778 | validation: 0.011826444552563892]
	TIME [epoch: 8.09 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006126103350528624		[learning rate: 0.00012268]
		[batch 20/20] avg loss: 0.005713488089295728		[learning rate: 0.00012245]
	Learning Rate: 0.000122454
	LOSS [training: 0.005919795719912177 | validation: 0.009541601112279375]
	TIME [epoch: 8.11 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01162557516948183		[learning rate: 0.00012223]
		[batch 20/20] avg loss: 0.007586094327828467		[learning rate: 0.00012201]
	Learning Rate: 0.00012201
	LOSS [training: 0.009605834748655149 | validation: 0.01137835561640467]
	TIME [epoch: 8.08 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008016111297870093		[learning rate: 0.00012179]
		[batch 20/20] avg loss: 0.019061072727280807		[learning rate: 0.00012157]
	Learning Rate: 0.000121567
	LOSS [training: 0.009931341928533906 | validation: 0.020228290741196663]
	TIME [epoch: 8.09 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013449737887184652		[learning rate: 0.00012135]
		[batch 20/20] avg loss: 0.014256849066846045		[learning rate: 0.00012113]
	Learning Rate: 0.000121126
	LOSS [training: 0.013853293477015347 | validation: 0.022794599227482525]
	TIME [epoch: 8.08 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009401714314290035		[learning rate: 0.00012091]
		[batch 20/20] avg loss: 0.006264367708307518		[learning rate: 0.00012069]
	Learning Rate: 0.000120686
	LOSS [training: 0.007833041011298775 | validation: 0.007585801236447941]
	TIME [epoch: 8.11 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008322600674717275		[learning rate: 0.00012047]
		[batch 20/20] avg loss: 0.008511447093107579		[learning rate: 0.00012025]
	Learning Rate: 0.000120248
	LOSS [training: 0.008417023883912429 | validation: 0.011172913572474954]
	TIME [epoch: 8.09 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010195900235667849		[learning rate: 0.00012003]
		[batch 20/20] avg loss: 0.004849627305983746		[learning rate: 0.00011981]
	Learning Rate: 0.000119812
	LOSS [training: 0.007522763770825798 | validation: 0.01252026978409686]
	TIME [epoch: 8.09 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011043109361168375		[learning rate: 0.00011959]
		[batch 20/20] avg loss: 0.011492708603980575		[learning rate: 0.00011938]
	Learning Rate: 0.000119377
	LOSS [training: 0.011267908982574475 | validation: 0.011034723704752631]
	TIME [epoch: 8.08 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009829372633397834		[learning rate: 0.00011916]
		[batch 20/20] avg loss: 0.008265323370749696		[learning rate: 0.00011894]
	Learning Rate: 0.000118944
	LOSS [training: 0.009047348002073763 | validation: 0.013925106738401423]
	TIME [epoch: 8.09 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007417990380168833		[learning rate: 0.00011873]
		[batch 20/20] avg loss: 0.009412502381761024		[learning rate: 0.00011851]
	Learning Rate: 0.000118512
	LOSS [training: 0.008415246380964931 | validation: 0.013908063955440456]
	TIME [epoch: 8.11 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01796069765081901		[learning rate: 0.0001183]
		[batch 20/20] avg loss: 0.0144820312876516		[learning rate: 0.00011808]
	Learning Rate: 0.000118082
	LOSS [training: 0.016221364469235305 | validation: 0.01199706768554644]
	TIME [epoch: 8.09 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005081383621245947		[learning rate: 0.00011787]
		[batch 20/20] avg loss: 0.01677638235610875		[learning rate: 0.00011765]
	Learning Rate: 0.000117654
	LOSS [training: 0.010928882988677347 | validation: 0.016516131323225894]
	TIME [epoch: 8.08 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004366531887675431		[learning rate: 0.00011744]
		[batch 20/20] avg loss: 0.007503414186354021		[learning rate: 0.00011723]
	Learning Rate: 0.000117227
	LOSS [training: 0.005934973037014726 | validation: 0.016575279153897157]
	TIME [epoch: 8.09 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008506288867954178		[learning rate: 0.00011701]
		[batch 20/20] avg loss: 0.006770989106451941		[learning rate: 0.0001168]
	Learning Rate: 0.000116801
	LOSS [training: 0.0076386389872030586 | validation: 0.008458877755261282]
	TIME [epoch: 8.11 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008701447377005582		[learning rate: 0.00011659]
		[batch 20/20] avg loss: 0.010908943309852459		[learning rate: 0.00011638]
	Learning Rate: 0.000116377
	LOSS [training: 0.00980519534342902 | validation: 0.023898820334690714]
	TIME [epoch: 8.09 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014982155359588679		[learning rate: 0.00011617]
		[batch 20/20] avg loss: 0.004734461756613083		[learning rate: 0.00011595]
	Learning Rate: 0.000115955
	LOSS [training: 0.009858308558100881 | validation: 0.013295019783031953]
	TIME [epoch: 8.09 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006115004720006848		[learning rate: 0.00011574]
		[batch 20/20] avg loss: 0.019324893561249383		[learning rate: 0.00011553]
	Learning Rate: 0.000115534
	LOSS [training: 0.012719949140628115 | validation: 0.014278559286827842]
	TIME [epoch: 8.08 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013155854054764998		[learning rate: 0.00011532]
		[batch 20/20] avg loss: 0.002474183375730542		[learning rate: 0.00011511]
	Learning Rate: 0.000115115
	LOSS [training: 0.007815018715247769 | validation: 0.0184526413894228]
	TIME [epoch: 8.09 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008028443046009758		[learning rate: 0.00011491]
		[batch 20/20] avg loss: 0.008508368690983939		[learning rate: 0.0001147]
	Learning Rate: 0.000114697
	LOSS [training: 0.008268405868496847 | validation: 0.01215172545207901]
	TIME [epoch: 8.11 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005062700047147536		[learning rate: 0.00011449]
		[batch 20/20] avg loss: 0.0073111287015279166		[learning rate: 0.00011428]
	Learning Rate: 0.000114281
	LOSS [training: 0.006186914374337727 | validation: 0.014441050037646867]
	TIME [epoch: 8.09 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008400593455868077		[learning rate: 0.00011407]
		[batch 20/20] avg loss: 0.009323979857108216		[learning rate: 0.00011387]
	Learning Rate: 0.000113866
	LOSS [training: 0.008862286656488146 | validation: 0.014122584898495317]
	TIME [epoch: 8.08 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011613429108413939		[learning rate: 0.00011366]
		[batch 20/20] avg loss: 0.005202136143997374		[learning rate: 0.00011345]
	Learning Rate: 0.000113453
	LOSS [training: 0.008407782626205656 | validation: 0.014855794808949798]
	TIME [epoch: 8.09 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011685526711078895		[learning rate: 0.00011325]
		[batch 20/20] avg loss: 0.003644918029063826		[learning rate: 0.00011304]
	Learning Rate: 0.000113041
	LOSS [training: 0.007665222370071359 | validation: 0.015214193940225851]
	TIME [epoch: 8.11 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004499269858782608		[learning rate: 0.00011284]
		[batch 20/20] avg loss: 0.018617593959893295		[learning rate: 0.00011263]
	Learning Rate: 0.000112631
	LOSS [training: 0.011558431909337951 | validation: 0.03114567474656505]
	TIME [epoch: 8.09 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011007755523129669		[learning rate: 0.00011243]
		[batch 20/20] avg loss: 0.013694672145677525		[learning rate: 0.00011222]
	Learning Rate: 0.000112222
	LOSS [training: 0.012351213834403598 | validation: 0.014665637975948949]
	TIME [epoch: 8.09 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009284622124288548		[learning rate: 0.00011202]
		[batch 20/20] avg loss: 0.004594915264130714		[learning rate: 0.00011181]
	Learning Rate: 0.000111815
	LOSS [training: 0.006939768694209631 | validation: 0.022026791596451233]
	TIME [epoch: 8.09 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008961564304909549		[learning rate: 0.00011161]
		[batch 20/20] avg loss: 0.01865027270312735		[learning rate: 0.00011141]
	Learning Rate: 0.000111409
	LOSS [training: 0.013805918504018452 | validation: 0.019787124708578657]
	TIME [epoch: 8.09 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015074448017502845		[learning rate: 0.00011121]
		[batch 20/20] avg loss: 0.011035431991425832		[learning rate: 0.000111]
	Learning Rate: 0.000111005
	LOSS [training: 0.013054940004464338 | validation: 0.013764438356335207]
	TIME [epoch: 8.11 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009580046756318013		[learning rate: 0.0001108]
		[batch 20/20] avg loss: 0.007840458411342812		[learning rate: 0.0001106]
	Learning Rate: 0.000110602
	LOSS [training: 0.008710252583830413 | validation: 0.020134017888046432]
	TIME [epoch: 8.09 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010694671772149346		[learning rate: 0.0001104]
		[batch 20/20] avg loss: 0.014997839578318423		[learning rate: 0.0001102]
	Learning Rate: 0.000110201
	LOSS [training: 0.012846255675233884 | validation: 0.017263785081189042]
	TIME [epoch: 8.08 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006135054976494853		[learning rate: 0.00011]
		[batch 20/20] avg loss: 0.008188283972297032		[learning rate: 0.0001098]
	Learning Rate: 0.000109801
	LOSS [training: 0.007161669474395943 | validation: 0.01518542672280793]
	TIME [epoch: 8.08 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0166680518600019		[learning rate: 0.0001096]
		[batch 20/20] avg loss: 0.016019481399745		[learning rate: 0.0001094]
	Learning Rate: 0.000109402
	LOSS [training: 0.01634376662987345 | validation: 0.018265167508939358]
	TIME [epoch: 8.11 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01312528123395014		[learning rate: 0.0001092]
		[batch 20/20] avg loss: 0.010632965576237509		[learning rate: 0.00010901]
	Learning Rate: 0.000109005
	LOSS [training: 0.011879123405093825 | validation: 0.01935770056231295]
	TIME [epoch: 8.09 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011213401008408284		[learning rate: 0.00010881]
		[batch 20/20] avg loss: 0.006001791589920346		[learning rate: 0.00010861]
	Learning Rate: 0.00010861
	LOSS [training: 0.008607596299164315 | validation: 0.011293282440613032]
	TIME [epoch: 8.08 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008756179805643926		[learning rate: 0.00010841]
		[batch 20/20] avg loss: 0.0073421879182127		[learning rate: 0.00010822]
	Learning Rate: 0.000108215
	LOSS [training: 0.008049183861928311 | validation: 0.01882098637801396]
	TIME [epoch: 8.08 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009778399798414964		[learning rate: 0.00010802]
		[batch 20/20] avg loss: 0.013929273714472872		[learning rate: 0.00010782]
	Learning Rate: 0.000107823
	LOSS [training: 0.01185383675644392 | validation: 0.023448701110665063]
	TIME [epoch: 8.09 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009797133118912232		[learning rate: 0.00010763]
		[batch 20/20] avg loss: 0.0078009048796435585		[learning rate: 0.00010743]
	Learning Rate: 0.000107432
	LOSS [training: 0.008799018999277894 | validation: 0.013490524709491011]
	TIME [epoch: 8.1 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013818741073812623		[learning rate: 0.00010724]
		[batch 20/20] avg loss: 0.013652768642237833		[learning rate: 0.00010704]
	Learning Rate: 0.000107042
	LOSS [training: 0.013735754858025227 | validation: 0.01047023751875828]
	TIME [epoch: 8.09 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015454986480131175		[learning rate: 0.00010685]
		[batch 20/20] avg loss: 0.009334947197081077		[learning rate: 0.00010665]
	Learning Rate: 0.000106653
	LOSS [training: 0.01239496683860613 | validation: 0.015857489220185917]
	TIME [epoch: 8.08 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009282948245566983		[learning rate: 0.00010646]
		[batch 20/20] avg loss: 0.0020386718726911448		[learning rate: 0.00010627]
	Learning Rate: 0.000106266
	LOSS [training: 0.005660810059129063 | validation: 0.007580989937584523]
	TIME [epoch: 8.09 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005357490843925021		[learning rate: 0.00010607]
		[batch 20/20] avg loss: 0.005214215042822936		[learning rate: 0.00010588]
	Learning Rate: 0.00010588
	LOSS [training: 0.0052858529433739785 | validation: 0.009954421287898446]
	TIME [epoch: 8.11 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003842714762109123		[learning rate: 0.00010569]
		[batch 20/20] avg loss: 0.011404150815979603		[learning rate: 0.0001055]
	Learning Rate: 0.000105496
	LOSS [training: 0.007623432789044362 | validation: 0.014367412451203737]
	TIME [epoch: 8.09 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005370157468474543		[learning rate: 0.0001053]
		[batch 20/20] avg loss: 0.010953453350279193		[learning rate: 0.00010511]
	Learning Rate: 0.000105113
	LOSS [training: 0.008161805409376867 | validation: 0.013328989446684102]
	TIME [epoch: 8.09 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037068452866133037		[learning rate: 0.00010492]
		[batch 20/20] avg loss: 0.006991191736406659		[learning rate: 0.00010473]
	Learning Rate: 0.000104732
	LOSS [training: 0.005349018511509982 | validation: 0.004636554369156549]
	TIME [epoch: 8.09 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012567296295536327		[learning rate: 0.00010454]
		[batch 20/20] avg loss: 0.00013967220661333866		[learning rate: 0.00010435]
	Learning Rate: 0.000104352
	LOSS [training: 0.0063534842510748315 | validation: 0.011349759101338806]
	TIME [epoch: 8.09 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00830543743607682		[learning rate: 0.00010416]
		[batch 20/20] avg loss: 0.008331305142757667		[learning rate: 0.00010397]
	Learning Rate: 0.000103973
	LOSS [training: 0.008318371289417242 | validation: 0.014036381206684444]
	TIME [epoch: 8.11 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009840286899140231		[learning rate: 0.00010378]
		[batch 20/20] avg loss: 0.006045063463369958		[learning rate: 0.0001036]
	Learning Rate: 0.000103596
	LOSS [training: 0.007942675181255096 | validation: 0.015701625162025966]
	TIME [epoch: 8.09 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0062955539020051535		[learning rate: 0.00010341]
		[batch 20/20] avg loss: 0.01192037463650737		[learning rate: 0.00010322]
	Learning Rate: 0.00010322
	LOSS [training: 0.009107964269256263 | validation: 0.014440540752444882]
	TIME [epoch: 8.09 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019502439850626142		[learning rate: 0.00010303]
		[batch 20/20] avg loss: 0.01078556661216469		[learning rate: 0.00010285]
	Learning Rate: 0.000102845
	LOSS [training: 0.015144003231395416 | validation: 0.019234415873320913]
	TIME [epoch: 8.08 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006284103922412068		[learning rate: 0.00010266]
		[batch 20/20] avg loss: 0.008707071120522441		[learning rate: 0.00010247]
	Learning Rate: 0.000102472
	LOSS [training: 0.007495587521467256 | validation: 0.023340577256971896]
	TIME [epoch: 8.11 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009480944696926574		[learning rate: 0.00010229]
		[batch 20/20] avg loss: 0.014229439015023129		[learning rate: 0.0001021]
	Learning Rate: 0.0001021
	LOSS [training: 0.011855191855974853 | validation: 0.008697487521621387]
	TIME [epoch: 8.09 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010876236832343907		[learning rate: 0.00010191]
		[batch 20/20] avg loss: 0.0064207165361595975		[learning rate: 0.00010173]
	Learning Rate: 0.00010173
	LOSS [training: 0.008648476684251751 | validation: 0.016284607427983724]
	TIME [epoch: 8.08 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010772636442038023		[learning rate: 0.00010154]
		[batch 20/20] avg loss: 0.00849309449422842		[learning rate: 0.00010136]
	Learning Rate: 0.00010136
	LOSS [training: 0.009632865468133222 | validation: 0.023005486773091062]
	TIME [epoch: 8.08 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009352497528946978		[learning rate: 0.00010118]
		[batch 20/20] avg loss: 0.008756927213441535		[learning rate: 0.00010099]
	Learning Rate: 0.000100993
	LOSS [training: 0.009054712371194259 | validation: 0.017697476474466775]
	TIME [epoch: 8.1 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007321249610295179		[learning rate: 0.00010081]
		[batch 20/20] avg loss: 0.013289550032606984		[learning rate: 0.00010063]
	Learning Rate: 0.000100626
	LOSS [training: 0.01030539982145108 | validation: 0.009042990969441346]
	TIME [epoch: 8.1 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01591793757538108		[learning rate: 0.00010044]
		[batch 20/20] avg loss: 0.011858813232458364		[learning rate: 0.00010026]
	Learning Rate: 0.000100261
	LOSS [training: 0.013888375403919717 | validation: 0.007605630183289925]
	TIME [epoch: 8.09 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001773577434748333		[learning rate: 0.00010008]
		[batch 20/20] avg loss: 0.007010835032947865		[learning rate: 9.9897e-05]
	Learning Rate: 9.98971e-05
	LOSS [training: 0.004392206233848099 | validation: 0.009759893075377044]
	TIME [epoch: 8.09 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007839449226022487		[learning rate: 9.9716e-05]
		[batch 20/20] avg loss: 0.0051736246715151455		[learning rate: 9.9535e-05]
	Learning Rate: 9.95345e-05
	LOSS [training: 0.006506536948768816 | validation: 0.012476598958851807]
	TIME [epoch: 8.08 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010663531035267322		[learning rate: 9.9354e-05]
		[batch 20/20] avg loss: 0.007287670725998565		[learning rate: 9.9173e-05]
	Learning Rate: 9.91733e-05
	LOSS [training: 0.008975600880632944 | validation: 0.016305612579447713]
	TIME [epoch: 8.11 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008017786666440829		[learning rate: 9.8993e-05]
		[batch 20/20] avg loss: 0.010866066363568868		[learning rate: 9.8813e-05]
	Learning Rate: 9.88134e-05
	LOSS [training: 0.009441926515004848 | validation: 0.019593342919798785]
	TIME [epoch: 8.09 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005821854812368089		[learning rate: 9.8634e-05]
		[batch 20/20] avg loss: 0.022555990720268627		[learning rate: 9.8455e-05]
	Learning Rate: 9.84548e-05
	LOSS [training: 0.014188922766318359 | validation: 0.016340753415133698]
	TIME [epoch: 8.09 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019696718316386662		[learning rate: 9.8276e-05]
		[batch 20/20] avg loss: 0.015176312995207905		[learning rate: 9.8098e-05]
	Learning Rate: 9.80975e-05
	LOSS [training: 0.017436515655797284 | validation: 0.01752548838690296]
	TIME [epoch: 8.08 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010522892750124553		[learning rate: 9.7919e-05]
		[batch 20/20] avg loss: 0.006867991638167936		[learning rate: 9.7742e-05]
	Learning Rate: 9.77415e-05
	LOSS [training: 0.008695442194146247 | validation: 0.014045740062188642]
	TIME [epoch: 8.1 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004817790181632221		[learning rate: 9.7564e-05]
		[batch 20/20] avg loss: 0.012313506977322133		[learning rate: 9.7387e-05]
	Learning Rate: 9.73868e-05
	LOSS [training: 0.008565648579477177 | validation: 0.023008765203721068]
	TIME [epoch: 8.1 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009307985580301461		[learning rate: 9.721e-05]
		[batch 20/20] avg loss: 0.005342373561798036		[learning rate: 9.7033e-05]
	Learning Rate: 9.70334e-05
	LOSS [training: 0.007325179571049748 | validation: 0.014074920819738525]
	TIME [epoch: 8.09 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008309990799371558		[learning rate: 9.6857e-05]
		[batch 20/20] avg loss: 0.008782803607665647		[learning rate: 9.6681e-05]
	Learning Rate: 9.66812e-05
	LOSS [training: 0.008546397203518604 | validation: 0.011344437148523406]
	TIME [epoch: 8.09 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00382254185803292		[learning rate: 9.6506e-05]
		[batch 20/20] avg loss: 0.012632181674579432		[learning rate: 9.633e-05]
	Learning Rate: 9.63304e-05
	LOSS [training: 0.008227361766306177 | validation: 0.025478762634700405]
	TIME [epoch: 8.09 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02503975076191785		[learning rate: 9.6155e-05]
		[batch 20/20] avg loss: 0.004901291292148513		[learning rate: 9.5981e-05]
	Learning Rate: 9.59808e-05
	LOSS [training: 0.014970521027033178 | validation: 0.008213984053211928]
	TIME [epoch: 8.11 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010853632317866782		[learning rate: 9.5806e-05]
		[batch 20/20] avg loss: 0.003116585057243488		[learning rate: 9.5632e-05]
	Learning Rate: 9.56324e-05
	LOSS [training: 0.006985108687555137 | validation: 0.013904436645420374]
	TIME [epoch: 8.09 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010584640840554157		[learning rate: 9.5459e-05]
		[batch 20/20] avg loss: 0.0059285578346767506		[learning rate: 9.5285e-05]
	Learning Rate: 9.52854e-05
	LOSS [training: 0.008256599337615455 | validation: 0.007711192685623989]
	TIME [epoch: 8.09 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00449564877507116		[learning rate: 9.5112e-05]
		[batch 20/20] avg loss: 0.008218879576957617		[learning rate: 9.494e-05]
	Learning Rate: 9.49396e-05
	LOSS [training: 0.006357264176014389 | validation: 0.012437856267520277]
	TIME [epoch: 8.09 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0048904317773723165		[learning rate: 9.4767e-05]
		[batch 20/20] avg loss: 0.009726950954234745		[learning rate: 9.4595e-05]
	Learning Rate: 9.45951e-05
	LOSS [training: 0.007308691365803528 | validation: 0.008080605691740954]
	TIME [epoch: 8.1 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008777686899247704		[learning rate: 9.4423e-05]
		[batch 20/20] avg loss: 0.012085724362851507		[learning rate: 9.4252e-05]
	Learning Rate: 9.42518e-05
	LOSS [training: 0.010431705631049604 | validation: 0.008209133789076308]
	TIME [epoch: 8.09 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008127727567898018		[learning rate: 9.4081e-05]
		[batch 20/20] avg loss: 0.007388432315410023		[learning rate: 9.391e-05]
	Learning Rate: 9.39097e-05
	LOSS [training: 0.007758079941654022 | validation: 0.01661118017223772]
	TIME [epoch: 8.09 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003874633030957712		[learning rate: 9.3739e-05]
		[batch 20/20] avg loss: 0.008346904326944234		[learning rate: 9.3569e-05]
	Learning Rate: 9.35689e-05
	LOSS [training: 0.006110768678950973 | validation: 0.020906704814336943]
	TIME [epoch: 8.09 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005852902821518973		[learning rate: 9.3399e-05]
		[batch 20/20] avg loss: 0.004453990240833242		[learning rate: 9.3229e-05]
	Learning Rate: 9.32294e-05
	LOSS [training: 0.005153446531176109 | validation: 0.012973270953037314]
	TIME [epoch: 8.09 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005923925956947908		[learning rate: 9.306e-05]
		[batch 20/20] avg loss: 0.010431155603588111		[learning rate: 9.2891e-05]
	Learning Rate: 9.2891e-05
	LOSS [training: 0.008177540780268009 | validation: 0.01017409375327479]
	TIME [epoch: 8.11 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00502731028403757		[learning rate: 9.2722e-05]
		[batch 20/20] avg loss: 0.0055935129482310875		[learning rate: 9.2554e-05]
	Learning Rate: 9.25539e-05
	LOSS [training: 0.005310411616134328 | validation: 0.011628738044278913]
	TIME [epoch: 8.09 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008009017660583363		[learning rate: 9.2386e-05]
		[batch 20/20] avg loss: 0.006308290535552532		[learning rate: 9.2218e-05]
	Learning Rate: 9.2218e-05
	LOSS [training: 0.007158654098067947 | validation: 0.005578627330114185]
	TIME [epoch: 8.09 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006466291928960546		[learning rate: 9.2051e-05]
		[batch 20/20] avg loss: 0.004353095541821012		[learning rate: 9.1883e-05]
	Learning Rate: 9.18834e-05
	LOSS [training: 0.005409693735390778 | validation: 0.014168101954467641]
	TIME [epoch: 8.08 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010740288107374294		[learning rate: 9.1716e-05]
		[batch 20/20] avg loss: 0.004562253848466549		[learning rate: 9.155e-05]
	Learning Rate: 9.15499e-05
	LOSS [training: 0.007651270977920422 | validation: 0.008887686646968046]
	TIME [epoch: 8.14 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010220358949135254		[learning rate: 9.1384e-05]
		[batch 20/20] avg loss: 0.0029761595184024728		[learning rate: 9.1218e-05]
	Learning Rate: 9.12177e-05
	LOSS [training: 0.006598259233768862 | validation: 0.011750626034851485]
	TIME [epoch: 8.09 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0053897842888519076		[learning rate: 9.1052e-05]
		[batch 20/20] avg loss: 0.009609895709717119		[learning rate: 9.0887e-05]
	Learning Rate: 9.08866e-05
	LOSS [training: 0.007499839999284513 | validation: 0.012089838941829249]
	TIME [epoch: 8.09 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022751937199376087		[learning rate: 9.0722e-05]
		[batch 20/20] avg loss: 0.006730503557670421		[learning rate: 9.0557e-05]
	Learning Rate: 9.05568e-05
	LOSS [training: 0.004502848638804015 | validation: 0.006224996442421316]
	TIME [epoch: 8.08 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003914597791346274		[learning rate: 9.0392e-05]
		[batch 20/20] avg loss: 0.0191895119077183		[learning rate: 9.0228e-05]
	Learning Rate: 9.02281e-05
	LOSS [training: 0.011552054849532287 | validation: 0.03325168371780497]
	TIME [epoch: 8.09 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013548815667802375		[learning rate: 9.0064e-05]
		[batch 20/20] avg loss: 0.01009957135009577		[learning rate: 8.9901e-05]
	Learning Rate: 8.99007e-05
	LOSS [training: 0.011824193508949074 | validation: 0.028358054131697036]
	TIME [epoch: 8.11 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015343527549807098		[learning rate: 8.9737e-05]
		[batch 20/20] avg loss: 0.008738577221660323		[learning rate: 8.9574e-05]
	Learning Rate: 8.95745e-05
	LOSS [training: 0.012041052385733709 | validation: 0.006025129240761522]
	TIME [epoch: 8.09 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004684913373659517		[learning rate: 8.9412e-05]
		[batch 20/20] avg loss: 0.00584104823529053		[learning rate: 8.9249e-05]
	Learning Rate: 8.92494e-05
	LOSS [training: 0.005262980804475024 | validation: 0.007358847306568337]
	TIME [epoch: 8.09 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003939339974079374		[learning rate: 8.9087e-05]
		[batch 20/20] avg loss: 0.014451757353035537		[learning rate: 8.8926e-05]
	Learning Rate: 8.89255e-05
	LOSS [training: 0.009195548663557457 | validation: 0.009895158472781017]
	TIME [epoch: 8.08 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00585421546078888		[learning rate: 8.8764e-05]
		[batch 20/20] avg loss: 0.011630598799800254		[learning rate: 8.8603e-05]
	Learning Rate: 8.86028e-05
	LOSS [training: 0.008742407130294567 | validation: 0.011497672803562588]
	TIME [epoch: 8.1 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00542913945655992		[learning rate: 8.8442e-05]
		[batch 20/20] avg loss: 0.00877701754900509		[learning rate: 8.8281e-05]
	Learning Rate: 8.82813e-05
	LOSS [training: 0.007103078502782506 | validation: 0.0110459664827357]
	TIME [epoch: 8.09 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009687848156623293		[learning rate: 8.8121e-05]
		[batch 20/20] avg loss: 0.006925304690532795		[learning rate: 8.7961e-05]
	Learning Rate: 8.79609e-05
	LOSS [training: 0.003947044753097562 | validation: 0.01600794009125903]
	TIME [epoch: 8.09 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006296455079913979		[learning rate: 8.7801e-05]
		[batch 20/20] avg loss: 0.006382397185509599		[learning rate: 8.7642e-05]
	Learning Rate: 8.76417e-05
	LOSS [training: 0.006339426132711788 | validation: 0.011787305230811885]
	TIME [epoch: 8.09 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01396820502205937		[learning rate: 8.7482e-05]
		[batch 20/20] avg loss: -0.0011968869083183308		[learning rate: 8.7324e-05]
	Learning Rate: 8.73236e-05
	LOSS [training: 0.006385659056870522 | validation: 0.007306553890221016]
	TIME [epoch: 8.09 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024729744345910135		[learning rate: 8.7165e-05]
		[batch 20/20] avg loss: 0.01018905040458779		[learning rate: 8.7007e-05]
	Learning Rate: 8.70067e-05
	LOSS [training: 0.006331012419589404 | validation: 0.008296213994252372]
	TIME [epoch: 8.11 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007433023051310483		[learning rate: 8.6849e-05]
		[batch 20/20] avg loss: 0.008207563062588411		[learning rate: 8.6691e-05]
	Learning Rate: 8.66909e-05
	LOSS [training: 0.007820293056949448 | validation: 0.008777732689831307]
	TIME [epoch: 8.09 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002544588236247893		[learning rate: 8.6533e-05]
		[batch 20/20] avg loss: 0.015573818873026165		[learning rate: 8.6376e-05]
	Learning Rate: 8.63763e-05
	LOSS [training: 0.009059203554637029 | validation: 0.01747158817707546]
	TIME [epoch: 8.08 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006791272163607008		[learning rate: 8.6219e-05]
		[batch 20/20] avg loss: 0.009454342813189017		[learning rate: 8.6063e-05]
	Learning Rate: 8.60629e-05
	LOSS [training: 0.008122807488398012 | validation: 0.010464566701600604]
	TIME [epoch: 8.09 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005406469912258215		[learning rate: 8.5907e-05]
		[batch 20/20] avg loss: 0.007522091747009556		[learning rate: 8.5751e-05]
	Learning Rate: 8.57505e-05
	LOSS [training: 0.006464280829633885 | validation: 0.014196265564930722]
	TIME [epoch: 8.1 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007516569188214577		[learning rate: 8.5595e-05]
		[batch 20/20] avg loss: 0.011344993221485553		[learning rate: 8.5439e-05]
	Learning Rate: 8.54394e-05
	LOSS [training: 0.009430781204850063 | validation: 0.013165673528060971]
	TIME [epoch: 8.09 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007378230517383016		[learning rate: 8.5284e-05]
		[batch 20/20] avg loss: 0.008977382660197143		[learning rate: 8.5129e-05]
	Learning Rate: 8.51293e-05
	LOSS [training: 0.008177806588790078 | validation: 0.010964296883548833]
	TIME [epoch: 8.09 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005111126841182678		[learning rate: 8.4975e-05]
		[batch 20/20] avg loss: 0.012134438888033675		[learning rate: 8.482e-05]
	Learning Rate: 8.48204e-05
	LOSS [training: 0.008622782864608176 | validation: 0.018311815398193626]
	TIME [epoch: 8.09 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010442386237651778		[learning rate: 8.4666e-05]
		[batch 20/20] avg loss: 0.011102962167462673		[learning rate: 8.4513e-05]
	Learning Rate: 8.45125e-05
	LOSS [training: 0.010772674202557227 | validation: 0.011521614933942428]
	TIME [epoch: 8.09 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00681083436111964		[learning rate: 8.4359e-05]
		[batch 20/20] avg loss: 0.0012151436025359002		[learning rate: 8.4206e-05]
	Learning Rate: 8.42058e-05
	LOSS [training: 0.00401298898182777 | validation: 0.009492345726861723]
	TIME [epoch: 8.11 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01689161669859994		[learning rate: 8.4053e-05]
		[batch 20/20] avg loss: 0.006265039311626232		[learning rate: 8.39e-05]
	Learning Rate: 8.39002e-05
	LOSS [training: 0.011578328005113086 | validation: 0.011239001460169618]
	TIME [epoch: 8.09 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0043047536672877066		[learning rate: 8.3748e-05]
		[batch 20/20] avg loss: 0.009982444907253602		[learning rate: 8.3596e-05]
	Learning Rate: 8.35958e-05
	LOSS [training: 0.0071435992872706565 | validation: 0.013911817338667636]
	TIME [epoch: 8.09 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004972835110751075		[learning rate: 8.3444e-05]
		[batch 20/20] avg loss: 0.007517813697429379		[learning rate: 8.3292e-05]
	Learning Rate: 8.32924e-05
	LOSS [training: 0.006245324404090228 | validation: 0.014484000273928203]
	TIME [epoch: 8.09 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007254339908039681		[learning rate: 8.3141e-05]
		[batch 20/20] avg loss: 0.007061233851099939		[learning rate: 8.299e-05]
	Learning Rate: 8.29901e-05
	LOSS [training: 0.0071577868795698095 | validation: 0.015430516305215329]
	TIME [epoch: 8.11 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005632209973594149		[learning rate: 8.2839e-05]
		[batch 20/20] avg loss: 0.007401154525552206		[learning rate: 8.2689e-05]
	Learning Rate: 8.26889e-05
	LOSS [training: 0.006516682249573177 | validation: 0.010449733659307132]
	TIME [epoch: 8.09 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007173998842120752		[learning rate: 8.2539e-05]
		[batch 20/20] avg loss: 0.005962541092534895		[learning rate: 8.2389e-05]
	Learning Rate: 8.23889e-05
	LOSS [training: 0.006568269967327822 | validation: 0.01642077630948263]
	TIME [epoch: 8.09 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004190917351512101		[learning rate: 8.2239e-05]
		[batch 20/20] avg loss: 0.010437821351252962		[learning rate: 8.209e-05]
	Learning Rate: 8.20899e-05
	LOSS [training: 0.007314369351382533 | validation: 0.02172195346989758]
	TIME [epoch: 8.08 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004124927917315997		[learning rate: 8.1941e-05]
		[batch 20/20] avg loss: 0.010922177513962381		[learning rate: 8.1792e-05]
	Learning Rate: 8.17919e-05
	LOSS [training: 0.007523552715639188 | validation: 0.0063898437699414605]
	TIME [epoch: 8.09 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005204027572280371		[learning rate: 8.1643e-05]
		[batch 20/20] avg loss: 0.004456560119608123		[learning rate: 8.1495e-05]
	Learning Rate: 8.14951e-05
	LOSS [training: 0.004830293845944246 | validation: 0.004024669421142466]
	TIME [epoch: 8.11 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014329310463935014		[learning rate: 8.1347e-05]
		[batch 20/20] avg loss: 0.010670626817502669		[learning rate: 8.1199e-05]
	Learning Rate: 8.11994e-05
	LOSS [training: 0.0060517789319480845 | validation: 0.004906463762158518]
	TIME [epoch: 8.09 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007822734995620164		[learning rate: 8.1052e-05]
		[batch 20/20] avg loss: 0.012724181289251684		[learning rate: 8.0905e-05]
	Learning Rate: 8.09047e-05
	LOSS [training: 0.010273458142435925 | validation: 0.011529120092901569]
	TIME [epoch: 8.08 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007660510854766362		[learning rate: 8.0758e-05]
		[batch 20/20] avg loss: 0.002651106781752055		[learning rate: 8.0611e-05]
	Learning Rate: 8.06111e-05
	LOSS [training: 0.005155808818259208 | validation: 0.0037806676844740123]
	TIME [epoch: 8.09 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004672521272515562		[learning rate: 8.0465e-05]
		[batch 20/20] avg loss: 0.00850955126247713		[learning rate: 8.0319e-05]
	Learning Rate: 8.03186e-05
	LOSS [training: 0.006591036267496344 | validation: 0.02015279873364742]
	TIME [epoch: 8.1 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013931681373904142		[learning rate: 8.0173e-05]
		[batch 20/20] avg loss: 0.005709108764846276		[learning rate: 8.0027e-05]
	Learning Rate: 8.00271e-05
	LOSS [training: 0.00982039506937521 | validation: 0.0056748698407366885]
	TIME [epoch: 8.09 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005636115536610216		[learning rate: 7.9882e-05]
		[batch 20/20] avg loss: 0.011837794329608582		[learning rate: 7.9737e-05]
	Learning Rate: 7.97367e-05
	LOSS [training: 0.008736954933109397 | validation: 0.015424823799166508]
	TIME [epoch: 8.09 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012902150039718743		[learning rate: 7.9592e-05]
		[batch 20/20] avg loss: 0.005649159219206302		[learning rate: 7.9447e-05]
	Learning Rate: 7.94473e-05
	LOSS [training: 0.009275654629462523 | validation: 0.01354526772799871]
	TIME [epoch: 8.09 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006255000210331226		[learning rate: 7.9303e-05]
		[batch 20/20] avg loss: 0.009574823348076227		[learning rate: 7.9159e-05]
	Learning Rate: 7.91589e-05
	LOSS [training: 0.007914911779203728 | validation: 0.012059217074568584]
	TIME [epoch: 8.09 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008356070429506526		[learning rate: 7.9015e-05]
		[batch 20/20] avg loss: 0.005666729709569754		[learning rate: 7.8872e-05]
	Learning Rate: 7.88717e-05
	LOSS [training: 0.00701140006953814 | validation: 0.01587702380806745]
	TIME [epoch: 8.12 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002410969691675745		[learning rate: 7.8728e-05]
		[batch 20/20] avg loss: 0.011252125678802056		[learning rate: 7.8585e-05]
	Learning Rate: 7.85854e-05
	LOSS [training: 0.006831547685238902 | validation: 0.013806809487763465]
	TIME [epoch: 8.08 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008328236572473875		[learning rate: 7.8443e-05]
		[batch 20/20] avg loss: 0.0016506791559979257		[learning rate: 7.83e-05]
	Learning Rate: 7.83003e-05
	LOSS [training: 0.004989457864235901 | validation: 0.01179741120174531]
	TIME [epoch: 8.08 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009714247416492348		[learning rate: 7.8158e-05]
		[batch 20/20] avg loss: 0.00368837537338784		[learning rate: 7.8016e-05]
	Learning Rate: 7.80161e-05
	LOSS [training: 0.006701311394940094 | validation: 0.018284778372675774]
	TIME [epoch: 8.09 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011804016312445263		[learning rate: 7.7874e-05]
		[batch 20/20] avg loss: 0.005310271069241047		[learning rate: 7.7733e-05]
	Learning Rate: 7.7733e-05
	LOSS [training: 0.008557143690843153 | validation: 0.011273226729342334]
	TIME [epoch: 8.11 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007785707526299412		[learning rate: 7.7592e-05]
		[batch 20/20] avg loss: 0.002621387940307911		[learning rate: 7.7451e-05]
	Learning Rate: 7.74509e-05
	LOSS [training: 0.005203547733303662 | validation: 0.016855133557880903]
	TIME [epoch: 8.1 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012229667737753375		[learning rate: 7.731e-05]
		[batch 20/20] avg loss: 0.008337747143715888		[learning rate: 7.717e-05]
	Learning Rate: 7.71698e-05
	LOSS [training: 0.01028370744073463 | validation: 0.012066499643245817]
	TIME [epoch: 8.08 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00351330119110676		[learning rate: 7.703e-05]
		[batch 20/20] avg loss: 0.006189131361305087		[learning rate: 7.689e-05]
	Learning Rate: 7.68897e-05
	LOSS [training: 0.004851216276205923 | validation: 0.009879808512866033]
	TIME [epoch: 8.09 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004911163458502223		[learning rate: 7.675e-05]
		[batch 20/20] avg loss: 0.006688236882004817		[learning rate: 7.6611e-05]
	Learning Rate: 7.66107e-05
	LOSS [training: 0.005799700170253519 | validation: 0.007617398378768181]
	TIME [epoch: 8.1 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005270831630430673		[learning rate: 7.6472e-05]
		[batch 20/20] avg loss: 0.011270011447287492		[learning rate: 7.6333e-05]
	Learning Rate: 7.63327e-05
	LOSS [training: 0.008270421538859082 | validation: 0.007364676730353165]
	TIME [epoch: 8.11 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011351951839868469		[learning rate: 7.6194e-05]
		[batch 20/20] avg loss: 0.00942076850231393		[learning rate: 7.6056e-05]
	Learning Rate: 7.60557e-05
	LOSS [training: 0.010386360171091201 | validation: 0.009390440448889485]
	TIME [epoch: 8.09 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005698543212713866		[learning rate: 7.5918e-05]
		[batch 20/20] avg loss: 0.008042092626893847		[learning rate: 7.578e-05]
	Learning Rate: 7.57797e-05
	LOSS [training: 0.006870317919803856 | validation: 0.007394332753796674]
	TIME [epoch: 8.08 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009496705067806263		[learning rate: 7.5642e-05]
		[batch 20/20] avg loss: 0.004301401341769341		[learning rate: 7.5505e-05]
	Learning Rate: 7.55047e-05
	LOSS [training: 0.006899053204787802 | validation: 0.009638567488878536]
	TIME [epoch: 8.09 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006752693558665293		[learning rate: 7.5368e-05]
		[batch 20/20] avg loss: 0.0070472340301518666		[learning rate: 7.5231e-05]
	Learning Rate: 7.52307e-05
	LOSS [training: 0.006899963794408578 | validation: 0.005786889652330504]
	TIME [epoch: 8.11 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037500339372249654		[learning rate: 7.5094e-05]
		[batch 20/20] avg loss: 0.008382616262190865		[learning rate: 7.4958e-05]
	Learning Rate: 7.49576e-05
	LOSS [training: 0.006066325099707915 | validation: -1.9621167616684036e-05]
	TIME [epoch: 8.09 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009797349971832962		[learning rate: 7.4822e-05]
		[batch 20/20] avg loss: 0.009125336386791196		[learning rate: 7.4686e-05]
	Learning Rate: 7.46856e-05
	LOSS [training: 0.009461343179312074 | validation: 0.012861091302581536]
	TIME [epoch: 8.08 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012209739454189595		[learning rate: 7.455e-05]
		[batch 20/20] avg loss: 0.005484817342054262		[learning rate: 7.4415e-05]
	Learning Rate: 7.44146e-05
	LOSS [training: 0.008847278398121928 | validation: 0.01358383593669989]
	TIME [epoch: 8.09 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004669129285677362		[learning rate: 7.4279e-05]
		[batch 20/20] avg loss: 0.008562904658032933		[learning rate: 7.4144e-05]
	Learning Rate: 7.41445e-05
	LOSS [training: 0.006616016971855146 | validation: 0.008153844735329303]
	TIME [epoch: 8.1 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007353008307593757		[learning rate: 7.401e-05]
		[batch 20/20] avg loss: 0.009074536694253375		[learning rate: 7.3875e-05]
	Learning Rate: 7.38754e-05
	LOSS [training: 0.008213772500923564 | validation: 0.016022073418142362]
	TIME [epoch: 8.11 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014545510670100265		[learning rate: 7.3741e-05]
		[batch 20/20] avg loss: 0.004415795854439782		[learning rate: 7.3607e-05]
	Learning Rate: 7.36073e-05
	LOSS [training: 0.009480653262270024 | validation: 0.01854261333094246]
	TIME [epoch: 8.08 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008802845142350637		[learning rate: 7.3474e-05]
		[batch 20/20] avg loss: 0.01217818550782192		[learning rate: 7.334e-05]
	Learning Rate: 7.33402e-05
	LOSS [training: 0.010490515325086278 | validation: 0.007164783655174582]
	TIME [epoch: 8.09 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010357899124081612		[learning rate: 7.3207e-05]
		[batch 20/20] avg loss: 0.011662741560011344		[learning rate: 7.3074e-05]
	Learning Rate: 7.30741e-05
	LOSS [training: 0.011010320342046477 | validation: 0.011325597361237602]
	TIME [epoch: 8.08 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00799907520162423		[learning rate: 7.2941e-05]
		[batch 20/20] avg loss: 0.00918080551471746		[learning rate: 7.2809e-05]
	Learning Rate: 7.28089e-05
	LOSS [training: 0.008589940358170846 | validation: 0.019110959504282488]
	TIME [epoch: 8.11 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003234605050259103		[learning rate: 7.2677e-05]
		[batch 20/20] avg loss: 0.010290457482161824		[learning rate: 7.2545e-05]
	Learning Rate: 7.25447e-05
	LOSS [training: 0.006762531266210464 | validation: 0.011779112697060846]
	TIME [epoch: 8.09 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033612312829139146		[learning rate: 7.2413e-05]
		[batch 20/20] avg loss: 0.006624343505415688		[learning rate: 7.2281e-05]
	Learning Rate: 7.22814e-05
	LOSS [training: 0.004992787394164801 | validation: 0.009524553996331231]
	TIME [epoch: 8.09 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004062789623852162		[learning rate: 7.215e-05]
		[batch 20/20] avg loss: 0.007305163810049263		[learning rate: 7.2019e-05]
	Learning Rate: 7.2019e-05
	LOSS [training: 0.005683976716950713 | validation: 0.010972378105092077]
	TIME [epoch: 8.09 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007963310275491261		[learning rate: 7.1888e-05]
		[batch 20/20] avg loss: 0.0061645449774329645		[learning rate: 7.1758e-05]
	Learning Rate: 7.17577e-05
	LOSS [training: 0.007063927626462112 | validation: 0.012805502407770722]
	TIME [epoch: 8.1 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005600728782276054		[learning rate: 7.1627e-05]
		[batch 20/20] avg loss: 0.0059472105727951755		[learning rate: 7.1497e-05]
	Learning Rate: 7.14973e-05
	LOSS [training: 0.005773969677535614 | validation: 0.006625848483096132]
	TIME [epoch: 8.1 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001975085102771964		[learning rate: 7.1367e-05]
		[batch 20/20] avg loss: 0.004908913468819566		[learning rate: 7.1238e-05]
	Learning Rate: 7.12378e-05
	LOSS [training: 0.0034419992857957654 | validation: 0.014584173684510456]
	TIME [epoch: 8.09 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006134617610895883		[learning rate: 7.1108e-05]
		[batch 20/20] avg loss: 0.006217241126034602		[learning rate: 7.0979e-05]
	Learning Rate: 7.09793e-05
	LOSS [training: 0.006175929368465243 | validation: 0.008555498117362791]
	TIME [epoch: 8.09 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003323687883219094		[learning rate: 7.085e-05]
		[batch 20/20] avg loss: 0.010575707488252466		[learning rate: 7.0722e-05]
	Learning Rate: 7.07217e-05
	LOSS [training: 0.00694969768573578 | validation: 0.010064675065492513]
	TIME [epoch: 8.09 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008031455478650681		[learning rate: 7.0593e-05]
		[batch 20/20] avg loss: 0.008565079891164135		[learning rate: 7.0465e-05]
	Learning Rate: 7.0465e-05
	LOSS [training: 0.008298267684907405 | validation: 0.006760273514209703]
	TIME [epoch: 8.11 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012829446918550768		[learning rate: 7.0337e-05]
		[batch 20/20] avg loss: 0.0032749038000852		[learning rate: 7.0209e-05]
	Learning Rate: 7.02093e-05
	LOSS [training: 0.0022789242459701387 | validation: 0.012262947513411936]
	TIME [epoch: 8.09 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003283088120528122		[learning rate: 7.0082e-05]
		[batch 20/20] avg loss: 0.005881624929402712		[learning rate: 6.9955e-05]
	Learning Rate: 6.99545e-05
	LOSS [training: 0.004582356524965417 | validation: 0.01059646579853972]
	TIME [epoch: 8.08 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008745477476076619		[learning rate: 6.9827e-05]
		[batch 20/20] avg loss: 0.011497323348519476		[learning rate: 6.9701e-05]
	Learning Rate: 6.97006e-05
	LOSS [training: 0.010121400412298047 | validation: 0.003472043965269583]
	TIME [epoch: 8.09 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005266481497139954		[learning rate: 6.9574e-05]
		[batch 20/20] avg loss: 0.009586837672399668		[learning rate: 6.9448e-05]
	Learning Rate: 6.94477e-05
	LOSS [training: 0.007426659584769811 | validation: 0.016331801854464558]
	TIME [epoch: 8.11 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014795496092773763		[learning rate: 6.9322e-05]
		[batch 20/20] avg loss: 0.013444116660865368		[learning rate: 6.9196e-05]
	Learning Rate: 6.91957e-05
	LOSS [training: 0.014119806376819566 | validation: 0.02254893652491884]
	TIME [epoch: 8.09 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011128199078314445		[learning rate: 6.907e-05]
		[batch 20/20] avg loss: 0.016234553038682754		[learning rate: 6.8945e-05]
	Learning Rate: 6.89446e-05
	LOSS [training: 0.013681376058498599 | validation: 0.008845890373296372]
	TIME [epoch: 8.09 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006276141899134538		[learning rate: 6.8819e-05]
		[batch 20/20] avg loss: 0.0062075341691312435		[learning rate: 6.8694e-05]
	Learning Rate: 6.86944e-05
	LOSS [training: 0.006241838034132891 | validation: 0.01365352629295028]
	TIME [epoch: 8.08 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0057032433063903005		[learning rate: 6.857e-05]
		[batch 20/20] avg loss: 0.00735314662821347		[learning rate: 6.8445e-05]
	Learning Rate: 6.84451e-05
	LOSS [training: 0.006528194967301884 | validation: 0.005650281741181264]
	TIME [epoch: 8.09 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033999376216128445		[learning rate: 6.8321e-05]
		[batch 20/20] avg loss: 0.0009770467415952742		[learning rate: 6.8197e-05]
	Learning Rate: 6.81967e-05
	LOSS [training: 0.0021884921816040593 | validation: 0.016321841980952614]
	TIME [epoch: 8.11 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008094560279204261		[learning rate: 6.8073e-05]
		[batch 20/20] avg loss: 0.007015860996961863		[learning rate: 6.7949e-05]
	Learning Rate: 6.79492e-05
	LOSS [training: 0.007555210638083063 | validation: 0.01277764905085603]
	TIME [epoch: 8.09 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00823707527026493		[learning rate: 6.7826e-05]
		[batch 20/20] avg loss: 0.008473491530151662		[learning rate: 6.7703e-05]
	Learning Rate: 6.77026e-05
	LOSS [training: 0.008355283400208295 | validation: 0.015697470882416804]
	TIME [epoch: 8.09 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012764136342786165		[learning rate: 6.758e-05]
		[batch 20/20] avg loss: 0.009577185013466171		[learning rate: 6.7457e-05]
	Learning Rate: 6.74569e-05
	LOSS [training: 0.011170660678126167 | validation: 0.012871868715024652]
	TIME [epoch: 8.08 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004378488076771421		[learning rate: 6.7334e-05]
		[batch 20/20] avg loss: 0.004721475448212682		[learning rate: 6.7212e-05]
	Learning Rate: 6.72121e-05
	LOSS [training: 0.004549981762492051 | validation: 0.016214786295010143]
	TIME [epoch: 8.11 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006156489823683045		[learning rate: 6.709e-05]
		[batch 20/20] avg loss: 0.010404256458404192		[learning rate: 6.6968e-05]
	Learning Rate: 6.69682e-05
	LOSS [training: 0.008280373141043618 | validation: 0.012772190115142568]
	TIME [epoch: 8.1 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005435137693700189		[learning rate: 6.6847e-05]
		[batch 20/20] avg loss: 0.008544737187905157		[learning rate: 6.6725e-05]
	Learning Rate: 6.67251e-05
	LOSS [training: 0.006989937440802671 | validation: 0.014663677043413731]
	TIME [epoch: 8.09 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006636426358747548		[learning rate: 6.6604e-05]
		[batch 20/20] avg loss: 0.01013958710871075		[learning rate: 6.6483e-05]
	Learning Rate: 6.6483e-05
	LOSS [training: 0.00838800673372915 | validation: 0.011419064755966882]
	TIME [epoch: 8.09 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005215181568467996		[learning rate: 6.6362e-05]
		[batch 20/20] avg loss: 0.004302472843620309		[learning rate: 6.6242e-05]
	Learning Rate: 6.62417e-05
	LOSS [training: 0.0047588272060441516 | validation: 0.01866821677034022]
	TIME [epoch: 8.09 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01059263379317363		[learning rate: 6.6121e-05]
		[batch 20/20] avg loss: 0.006248019576346025		[learning rate: 6.6001e-05]
	Learning Rate: 6.60013e-05
	LOSS [training: 0.00842032668475983 | validation: 0.014251041578018039]
	TIME [epoch: 8.11 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007666607975436307		[learning rate: 6.5881e-05]
		[batch 20/20] avg loss: 0.00787147641127047		[learning rate: 6.5762e-05]
	Learning Rate: 6.57618e-05
	LOSS [training: 0.007769042193353388 | validation: 0.006267183184012677]
	TIME [epoch: 8.08 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004285809881179656		[learning rate: 6.5642e-05]
		[batch 20/20] avg loss: 0.000605951009311035		[learning rate: 6.5523e-05]
	Learning Rate: 6.55232e-05
	LOSS [training: 0.0024458804452453455 | validation: 0.007751352219726052]
	TIME [epoch: 8.08 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004438421414756783		[learning rate: 6.5404e-05]
		[batch 20/20] avg loss: 0.008929115089324906		[learning rate: 6.5285e-05]
	Learning Rate: 6.52854e-05
	LOSS [training: 0.006683768252040846 | validation: 0.014564140868930875]
	TIME [epoch: 8.09 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004250999639406224		[learning rate: 6.5167e-05]
		[batch 20/20] avg loss: 0.0083248459620569		[learning rate: 6.5048e-05]
	Learning Rate: 6.50484e-05
	LOSS [training: 0.006287922800731561 | validation: 0.014594535800312431]
	TIME [epoch: 8.11 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008911934796070229		[learning rate: 6.493e-05]
		[batch 20/20] avg loss: 0.007624966432670521		[learning rate: 6.4812e-05]
	Learning Rate: 6.48124e-05
	LOSS [training: 0.008268450614370376 | validation: 0.016691168902104096]
	TIME [epoch: 8.1 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006319600594410314		[learning rate: 6.4695e-05]
		[batch 20/20] avg loss: 0.007963840782795096		[learning rate: 6.4577e-05]
	Learning Rate: 6.45772e-05
	LOSS [training: 0.007141720688602704 | validation: 0.009202851049941688]
	TIME [epoch: 8.1 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006913940886848109		[learning rate: 6.446e-05]
		[batch 20/20] avg loss: 0.006232524312465187		[learning rate: 6.4343e-05]
	Learning Rate: 6.43428e-05
	LOSS [training: 0.006573232599656649 | validation: 0.014909421213319472]
	TIME [epoch: 8.09 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0065130631832680365		[learning rate: 6.4226e-05]
		[batch 20/20] avg loss: 0.012154655041717859		[learning rate: 6.4109e-05]
	Learning Rate: 6.41093e-05
	LOSS [training: 0.009333859112492947 | validation: 0.00887774689000751]
	TIME [epoch: 8.1 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007885855292899794		[learning rate: 6.3993e-05]
		[batch 20/20] avg loss: 0.009289778770703083		[learning rate: 6.3877e-05]
	Learning Rate: 6.38767e-05
	LOSS [training: 0.008587817031801438 | validation: 0.01167519992250507]
	TIME [epoch: 8.12 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007765755327716771		[learning rate: 6.3761e-05]
		[batch 20/20] avg loss: 0.008106195376767406		[learning rate: 6.3645e-05]
	Learning Rate: 6.36449e-05
	LOSS [training: 0.00793597535224209 | validation: 0.004667098309262591]
	TIME [epoch: 8.1 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004093093047553864		[learning rate: 6.3529e-05]
		[batch 20/20] avg loss: 0.008905690351694726		[learning rate: 6.3414e-05]
	Learning Rate: 6.34139e-05
	LOSS [training: 0.0064993916996242945 | validation: 0.008090628639799754]
	TIME [epoch: 8.09 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005507382031957733		[learning rate: 6.3299e-05]
		[batch 20/20] avg loss: 0.0012428004294580376		[learning rate: 6.3184e-05]
	Learning Rate: 6.31837e-05
	LOSS [training: 0.0033750912307078857 | validation: 0.011230098931354907]
	TIME [epoch: 8.09 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012273691218429642		[learning rate: 6.3069e-05]
		[batch 20/20] avg loss: 0.005264254893308238		[learning rate: 6.2954e-05]
	Learning Rate: 6.29544e-05
	LOSS [training: 0.00876897305586894 | validation: 0.006252273224377753]
	TIME [epoch: 8.11 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008440558971980344		[learning rate: 6.284e-05]
		[batch 20/20] avg loss: 0.00661026853721703		[learning rate: 6.2726e-05]
	Learning Rate: 6.2726e-05
	LOSS [training: 0.007525413754598686 | validation: 0.009956203864354298]
	TIME [epoch: 8.1 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012709366501630234		[learning rate: 6.2612e-05]
		[batch 20/20] avg loss: 0.012796586831423215		[learning rate: 6.2498e-05]
	Learning Rate: 6.24983e-05
	LOSS [training: 0.012752976666526727 | validation: 0.013702776119848952]
	TIME [epoch: 8.1 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00966674616556116		[learning rate: 6.2385e-05]
		[batch 20/20] avg loss: 0.007400206872657608		[learning rate: 6.2272e-05]
	Learning Rate: 6.22715e-05
	LOSS [training: 0.008533476519109384 | validation: 0.01939314872165527]
	TIME [epoch: 8.1 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01901232666478609		[learning rate: 6.2158e-05]
		[batch 20/20] avg loss: 0.009597243383848738		[learning rate: 6.2046e-05]
	Learning Rate: 6.20455e-05
	LOSS [training: 0.014304785024317413 | validation: 0.009257998212318402]
	TIME [epoch: 8.09 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012668476983119067		[learning rate: 6.1933e-05]
		[batch 20/20] avg loss: 0.006407536268500998		[learning rate: 6.182e-05]
	Learning Rate: 6.18204e-05
	LOSS [training: 0.009538006625810032 | validation: 0.015763681502775496]
	TIME [epoch: 8.11 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005209997407160751		[learning rate: 6.1708e-05]
		[batch 20/20] avg loss: 0.009772144554452512		[learning rate: 6.1596e-05]
	Learning Rate: 6.1596e-05
	LOSS [training: 0.007491070980806631 | validation: 0.013377898128835618]
	TIME [epoch: 8.09 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009501547739668516		[learning rate: 6.1484e-05]
		[batch 20/20] avg loss: 0.0063026694035137635		[learning rate: 6.1372e-05]
	Learning Rate: 6.13725e-05
	LOSS [training: 0.007902108571591139 | validation: 0.013060429635035512]
	TIME [epoch: 8.09 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032199899804983903		[learning rate: 6.1261e-05]
		[batch 20/20] avg loss: 0.00476089782739688		[learning rate: 6.115e-05]
	Learning Rate: 6.11498e-05
	LOSS [training: 0.003990443903947636 | validation: 0.008328308417709011]
	TIME [epoch: 8.09 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005847354565410653		[learning rate: 6.1039e-05]
		[batch 20/20] avg loss: 0.007590065781120961		[learning rate: 6.0928e-05]
	Learning Rate: 6.09278e-05
	LOSS [training: 0.006718710173265807 | validation: 0.014499533962074141]
	TIME [epoch: 8.11 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011164926597595876		[learning rate: 6.0817e-05]
		[batch 20/20] avg loss: 0.004184838219846797		[learning rate: 6.0707e-05]
	Learning Rate: 6.07067e-05
	LOSS [training: 0.007674882408721337 | validation: 0.012934701827532914]
	TIME [epoch: 8.1 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004599340605755439		[learning rate: 6.0596e-05]
		[batch 20/20] avg loss: 0.0020437370405025335		[learning rate: 6.0486e-05]
	Learning Rate: 6.04864e-05
	LOSS [training: 0.0033215388231289866 | validation: 0.014540913051089052]
	TIME [epoch: 8.09 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032162893753575395		[learning rate: 6.0377e-05]
		[batch 20/20] avg loss: 0.007157813081224216		[learning rate: 6.0267e-05]
	Learning Rate: 6.02669e-05
	LOSS [training: 0.005187051228290877 | validation: 0.009032062765593742]
	TIME [epoch: 8.09 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008324653512334874		[learning rate: 6.0157e-05]
		[batch 20/20] avg loss: 0.006979331957920059		[learning rate: 6.0048e-05]
	Learning Rate: 6.00482e-05
	LOSS [training: 0.007651992735127467 | validation: 0.012703956172664881]
	TIME [epoch: 8.1 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007222445609340836		[learning rate: 5.9939e-05]
		[batch 20/20] avg loss: 0.005978525880290677		[learning rate: 5.983e-05]
	Learning Rate: 5.98303e-05
	LOSS [training: 0.006600485744815758 | validation: 0.01822563723089557]
	TIME [epoch: 8.12 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006285098940314567		[learning rate: 5.9722e-05]
		[batch 20/20] avg loss: 0.0030700115469140367		[learning rate: 5.9613e-05]
	Learning Rate: 5.96132e-05
	LOSS [training: 0.0046775552436143005 | validation: 0.01258528872220414]
	TIME [epoch: 8.1 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006360806869346774		[learning rate: 5.9505e-05]
		[batch 20/20] avg loss: 0.008270651007065057		[learning rate: 5.9397e-05]
	Learning Rate: 5.93968e-05
	LOSS [training: 0.007315728938205916 | validation: 0.004512735659956321]
	TIME [epoch: 8.09 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008434261431239555		[learning rate: 5.9289e-05]
		[batch 20/20] avg loss: 0.007641352358776593		[learning rate: 5.9181e-05]
	Learning Rate: 5.91813e-05
	LOSS [training: 0.008037806895008074 | validation: 0.007384990634497768]
	TIME [epoch: 8.1 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004496602315078313		[learning rate: 5.9074e-05]
		[batch 20/20] avg loss: 0.009339694565456125		[learning rate: 5.8966e-05]
	Learning Rate: 5.89665e-05
	LOSS [training: 0.00691814844026722 | validation: 0.006356971075797732]
	TIME [epoch: 8.12 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01162458787470914		[learning rate: 5.8859e-05]
		[batch 20/20] avg loss: 0.005319477727167031		[learning rate: 5.8752e-05]
	Learning Rate: 5.87525e-05
	LOSS [training: 0.008472032800938086 | validation: 0.014121510321217858]
	TIME [epoch: 8.1 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006511065448006241		[learning rate: 5.8646e-05]
		[batch 20/20] avg loss: 0.007063267456466076		[learning rate: 5.8539e-05]
	Learning Rate: 5.85393e-05
	LOSS [training: 0.006787166452236158 | validation: 0.00893570197146235]
	TIME [epoch: 8.09 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005308694769324619		[learning rate: 5.8433e-05]
		[batch 20/20] avg loss: 0.00012087290971016162		[learning rate: 5.8327e-05]
	Learning Rate: 5.83268e-05
	LOSS [training: 0.0027147838395173906 | validation: 0.009729813425668364]
	TIME [epoch: 8.1 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005226905553327619		[learning rate: 5.8221e-05]
		[batch 20/20] avg loss: 0.00722395875909621		[learning rate: 5.8115e-05]
	Learning Rate: 5.81152e-05
	LOSS [training: 0.006225432156211916 | validation: 0.008141951938399233]
	TIME [epoch: 8.09 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.7423751382648486e-05		[learning rate: 5.801e-05]
		[batch 20/20] avg loss: 0.00259727731455269		[learning rate: 5.7904e-05]
	Learning Rate: 5.79043e-05
	LOSS [training: 0.0013273505329676693 | validation: 0.014640457271306515]
	TIME [epoch: 8.11 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002906800192095381		[learning rate: 5.7799e-05]
		[batch 20/20] avg loss: 0.01126650582771731		[learning rate: 5.7694e-05]
	Learning Rate: 5.76941e-05
	LOSS [training: 0.007086653009906343 | validation: 0.011036966322766944]
	TIME [epoch: 8.09 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008182835514177447		[learning rate: 5.7589e-05]
		[batch 20/20] avg loss: 0.004771398244843345		[learning rate: 5.7485e-05]
	Learning Rate: 5.74847e-05
	LOSS [training: 0.006477116879510396 | validation: 0.01216223403893318]
	TIME [epoch: 8.09 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004779013445262506		[learning rate: 5.738e-05]
		[batch 20/20] avg loss: 0.005926395797656038		[learning rate: 5.7276e-05]
	Learning Rate: 5.72761e-05
	LOSS [training: 0.005352704621459271 | validation: 0.013205977386458304]
	TIME [epoch: 8.1 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008973982889229602		[learning rate: 5.7172e-05]
		[batch 20/20] avg loss: 0.0004820393421352414		[learning rate: 5.7068e-05]
	Learning Rate: 5.70683e-05
	LOSS [training: 0.004728011115682422 | validation: 0.007857329278756096]
	TIME [epoch: 8.11 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009974725997186601		[learning rate: 5.6965e-05]
		[batch 20/20] avg loss: 0.0034862772248318357		[learning rate: 5.6861e-05]
	Learning Rate: 5.68612e-05
	LOSS [training: 0.006730501611009217 | validation: 0.014674040034723456]
	TIME [epoch: 8.1 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004519323851962747		[learning rate: 5.6758e-05]
		[batch 20/20] avg loss: 0.004840045046809434		[learning rate: 5.6655e-05]
	Learning Rate: 5.66548e-05
	LOSS [training: 0.004679684449386091 | validation: 0.015739426625182705]
	TIME [epoch: 8.09 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007350241360414243		[learning rate: 5.6552e-05]
		[batch 20/20] avg loss: 0.004373514964578049		[learning rate: 5.6449e-05]
	Learning Rate: 5.64492e-05
	LOSS [training: 0.005861878162496145 | validation: 0.00890747739410049]
	TIME [epoch: 8.09 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005598618221924496		[learning rate: 5.6347e-05]
		[batch 20/20] avg loss: 0.0018731556616771255		[learning rate: 5.6244e-05]
	Learning Rate: 5.62444e-05
	LOSS [training: 0.0037358869418008117 | validation: 0.008542405708869508]
	TIME [epoch: 8.1 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006519917029611462		[learning rate: 5.6142e-05]
		[batch 20/20] avg loss: 0.008035478278464245		[learning rate: 5.604e-05]
	Learning Rate: 5.60403e-05
	LOSS [training: 0.007277697654037854 | validation: 0.008842751236363196]
	TIME [epoch: 8.11 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004657152537931736		[learning rate: 5.5938e-05]
		[batch 20/20] avg loss: 0.004934844595828027		[learning rate: 5.5837e-05]
	Learning Rate: 5.58369e-05
	LOSS [training: 0.004795998566879881 | validation: 0.010648692940380192]
	TIME [epoch: 8.09 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01117379372914099		[learning rate: 5.5735e-05]
		[batch 20/20] avg loss: 0.006856510075951177		[learning rate: 5.5634e-05]
	Learning Rate: 5.56342e-05
	LOSS [training: 0.009015151902546084 | validation: 0.014572102609267596]
	TIME [epoch: 8.09 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010574650238913082		[learning rate: 5.5533e-05]
		[batch 20/20] avg loss: 0.007655596223236136		[learning rate: 5.5432e-05]
	Learning Rate: 5.54323e-05
	LOSS [training: 0.009115123231074608 | validation: 0.01139536297626107]
	TIME [epoch: 8.09 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007384407931542314		[learning rate: 5.5332e-05]
		[batch 20/20] avg loss: 0.007582062975393889		[learning rate: 5.5231e-05]
	Learning Rate: 5.52312e-05
	LOSS [training: 0.007483235453468099 | validation: 0.01258202341807314]
	TIME [epoch: 8.12 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00656596011635813		[learning rate: 5.5131e-05]
		[batch 20/20] avg loss: 0.004102181947057237		[learning rate: 5.5031e-05]
	Learning Rate: 5.50307e-05
	LOSS [training: 0.005334071031707683 | validation: 0.008999749211974653]
	TIME [epoch: 8.09 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003959388346739754		[learning rate: 5.4931e-05]
		[batch 20/20] avg loss: 0.008982262144423266		[learning rate: 5.4831e-05]
	Learning Rate: 5.4831e-05
	LOSS [training: 0.006470825245581509 | validation: 0.01915125356791342]
	TIME [epoch: 8.09 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007485581312495842		[learning rate: 5.4731e-05]
		[batch 20/20] avg loss: 0.00251156970913916		[learning rate: 5.4632e-05]
	Learning Rate: 5.4632e-05
	LOSS [training: 0.004998575510817502 | validation: 0.012880404054540304]
	TIME [epoch: 8.09 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007448766578895155		[learning rate: 5.4533e-05]
		[batch 20/20] avg loss: 0.004590554318087209		[learning rate: 5.4434e-05]
	Learning Rate: 5.44338e-05
	LOSS [training: 0.006019660448491181 | validation: 0.01097970612445554]
	TIME [epoch: 8.1 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020395524868146754		[learning rate: 5.4335e-05]
		[batch 20/20] avg loss: 0.010449776466024805		[learning rate: 5.4236e-05]
	Learning Rate: 5.42362e-05
	LOSS [training: 0.006244664476419739 | validation: 0.006607962847868089]
	TIME [epoch: 8.11 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006837726604292922		[learning rate: 5.4138e-05]
		[batch 20/20] avg loss: 0.010615516702111263		[learning rate: 5.4039e-05]
	Learning Rate: 5.40394e-05
	LOSS [training: 0.008726621653202093 | validation: 0.018425654019926047]
	TIME [epoch: 8.09 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007973654231140373		[learning rate: 5.3941e-05]
		[batch 20/20] avg loss: 0.008462371269289278		[learning rate: 5.3843e-05]
	Learning Rate: 5.38433e-05
	LOSS [training: 0.008218012750214826 | validation: 0.0010800957200463933]
	TIME [epoch: 8.1 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030161283880211316		[learning rate: 5.3746e-05]
		[batch 20/20] avg loss: 0.00796924924636833		[learning rate: 5.3648e-05]
	Learning Rate: 5.36479e-05
	LOSS [training: 0.00549268881719473 | validation: 0.0008857330384798006]
	TIME [epoch: 8.09 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016308578480016475		[learning rate: 5.355e-05]
		[batch 20/20] avg loss: 0.00875316736613536		[learning rate: 5.3453e-05]
	Learning Rate: 5.34532e-05
	LOSS [training: 0.005192012607068504 | validation: 0.008077616972411399]
	TIME [epoch: 8.13 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009693592747966539		[learning rate: 5.3356e-05]
		[batch 20/20] avg loss: 0.02005447856005356		[learning rate: 5.3259e-05]
	Learning Rate: 5.32592e-05
	LOSS [training: 0.014874035654010048 | validation: 0.012378803453368933]
	TIME [epoch: 8.1 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036455512454045525		[learning rate: 5.3162e-05]
		[batch 20/20] avg loss: 0.009384766427611543		[learning rate: 5.3066e-05]
	Learning Rate: 5.30659e-05
	LOSS [training: 0.006515158836508048 | validation: 0.011306142428382224]
	TIME [epoch: 8.08 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014145790651154213		[learning rate: 5.297e-05]
		[batch 20/20] avg loss: 0.00696658277232347		[learning rate: 5.2873e-05]
	Learning Rate: 5.28734e-05
	LOSS [training: 0.004190580918719445 | validation: 0.01353853007165598]
	TIME [epoch: 8.09 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004249358909732594		[learning rate: 5.2777e-05]
		[batch 20/20] avg loss: 0.005563098190298758		[learning rate: 5.2681e-05]
	Learning Rate: 5.26815e-05
	LOSS [training: 0.004906228550015676 | validation: 0.013804933015220015]
	TIME [epoch: 8.11 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004710425880253982		[learning rate: 5.2586e-05]
		[batch 20/20] avg loss: 0.011013488915199137		[learning rate: 5.249e-05]
	Learning Rate: 5.24903e-05
	LOSS [training: 0.00786195739772656 | validation: 0.008370383806908384]
	TIME [epoch: 8.11 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029197363954685247		[learning rate: 5.2395e-05]
		[batch 20/20] avg loss: 0.00296851388506758		[learning rate: 5.23e-05]
	Learning Rate: 5.22998e-05
	LOSS [training: 0.002944125140268052 | validation: 0.0024441358320414506]
	TIME [epoch: 8.1 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0055706171404875495		[learning rate: 5.2205e-05]
		[batch 20/20] avg loss: 0.009451747335257539		[learning rate: 5.211e-05]
	Learning Rate: 5.211e-05
	LOSS [training: 0.0075111822378725445 | validation: 0.01141168127552061]
	TIME [epoch: 8.1 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0044641235231490765		[learning rate: 5.2015e-05]
		[batch 20/20] avg loss: 0.010232609663798705		[learning rate: 5.1921e-05]
	Learning Rate: 5.19209e-05
	LOSS [training: 0.007348366593473892 | validation: 0.011187474177548368]
	TIME [epoch: 8.1 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014588894734740697		[learning rate: 5.1827e-05]
		[batch 20/20] avg loss: 0.011783143773061345		[learning rate: 5.1732e-05]
	Learning Rate: 5.17325e-05
	LOSS [training: 0.006621016623267707 | validation: 0.01158711610216821]
	TIME [epoch: 8.12 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009921217225090624		[learning rate: 5.1639e-05]
		[batch 20/20] avg loss: 0.003662360202034548		[learning rate: 5.1545e-05]
	Learning Rate: 5.15447e-05
	LOSS [training: 0.006791788713562587 | validation: 0.01663907503063236]
	TIME [epoch: 8.09 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006368945136099614		[learning rate: 5.1451e-05]
		[batch 20/20] avg loss: 0.00744980655427583		[learning rate: 5.1358e-05]
	Learning Rate: 5.13577e-05
	LOSS [training: 0.0069093758451877225 | validation: 0.011464485598363952]
	TIME [epoch: 8.09 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009906463612218466		[learning rate: 5.1264e-05]
		[batch 20/20] avg loss: 0.0035704421336919764		[learning rate: 5.1171e-05]
	Learning Rate: 5.11713e-05
	LOSS [training: 0.006738452872955221 | validation: 0.009092517990060719]
	TIME [epoch: 8.09 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008336336358721745		[learning rate: 5.1078e-05]
		[batch 20/20] avg loss: 0.006684060634753686		[learning rate: 5.0986e-05]
	Learning Rate: 5.09856e-05
	LOSS [training: 0.007510198496737714 | validation: 0.018425136759175866]
	TIME [epoch: 8.11 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037446156376183663		[learning rate: 5.0893e-05]
		[batch 20/20] avg loss: 0.0041227911392947665		[learning rate: 5.0801e-05]
	Learning Rate: 5.08006e-05
	LOSS [training: 0.003933703388456567 | validation: 0.01678841802517353]
	TIME [epoch: 8.1 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004542411637117805		[learning rate: 5.0708e-05]
		[batch 20/20] avg loss: 0.00862833649120304		[learning rate: 5.0616e-05]
	Learning Rate: 5.06162e-05
	LOSS [training: 0.006585374064160423 | validation: 0.015328120755724064]
	TIME [epoch: 8.09 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007097567090689176		[learning rate: 5.0524e-05]
		[batch 20/20] avg loss: 0.009167337647014617		[learning rate: 5.0433e-05]
	Learning Rate: 5.04325e-05
	LOSS [training: 0.008132452368851897 | validation: 0.0165267636057853]
	TIME [epoch: 8.1 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002791881817560459		[learning rate: 5.0341e-05]
		[batch 20/20] avg loss: 0.005699741520365565		[learning rate: 5.0249e-05]
	Learning Rate: 5.02495e-05
	LOSS [training: 0.004245811668963013 | validation: 0.011140719777010243]
	TIME [epoch: 8.09 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00736054529399081		[learning rate: 5.0158e-05]
		[batch 20/20] avg loss: 0.00473022093941304		[learning rate: 5.0067e-05]
	Learning Rate: 5.00671e-05
	LOSS [training: 0.0060453831167019256 | validation: 0.01781837270560828]
	TIME [epoch: 8.12 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005886710870229695		[learning rate: 4.9976e-05]
		[batch 20/20] avg loss: 0.012571347941060065		[learning rate: 4.9885e-05]
	Learning Rate: 4.98854e-05
	LOSS [training: 0.00922902940564488 | validation: 0.014994213155283842]
	TIME [epoch: 8.1 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0039505795643025084		[learning rate: 4.9795e-05]
		[batch 20/20] avg loss: 0.010248434553246772		[learning rate: 4.9704e-05]
	Learning Rate: 4.97044e-05
	LOSS [training: 0.007099507058774641 | validation: 0.010912707450949707]
	TIME [epoch: 8.09 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009557887951798821		[learning rate: 4.9614e-05]
		[batch 20/20] avg loss: 0.007754659385455108		[learning rate: 4.9524e-05]
	Learning Rate: 4.9524e-05
	LOSS [training: 0.008656273668626964 | validation: 0.013097908165781876]
	TIME [epoch: 8.09 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006893157875844659		[learning rate: 4.9434e-05]
		[batch 20/20] avg loss: 0.00573341690363247		[learning rate: 4.9344e-05]
	Learning Rate: 4.93443e-05
	LOSS [training: 0.006313287389738564 | validation: 0.0076609973849493095]
	TIME [epoch: 8.11 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0013240963083306424		[learning rate: 4.9255e-05]
		[batch 20/20] avg loss: 0.007940204835482623		[learning rate: 4.9165e-05]
	Learning Rate: 4.91652e-05
	LOSS [training: 0.0033080542635759913 | validation: 0.01370323391560449]
	TIME [epoch: 8.1 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010174498427223531		[learning rate: 4.9076e-05]
		[batch 20/20] avg loss: 0.005006201078302656		[learning rate: 4.8987e-05]
	Learning Rate: 4.89868e-05
	LOSS [training: 0.007590349752763094 | validation: 0.007615715276543361]
	TIME [epoch: 8.09 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004218320033409289		[learning rate: 4.8898e-05]
		[batch 20/20] avg loss: 0.006514333256117809		[learning rate: 4.8809e-05]
	Learning Rate: 4.8809e-05
	LOSS [training: 0.0053663266447635475 | validation: 0.015534194404281467]
	TIME [epoch: 8.09 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034072616274647277		[learning rate: 4.872e-05]
		[batch 20/20] avg loss: 0.004104135911419313		[learning rate: 4.8632e-05]
	Learning Rate: 4.86319e-05
	LOSS [training: 0.0037556987694420214 | validation: 0.010133871570882472]
	TIME [epoch: 8.09 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0044476804847064485		[learning rate: 4.8544e-05]
		[batch 20/20] avg loss: 0.004605402051361219		[learning rate: 4.8455e-05]
	Learning Rate: 4.84554e-05
	LOSS [training: 0.004526541268033834 | validation: 0.009133264077437947]
	TIME [epoch: 8.11 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006477534365028457		[learning rate: 4.8367e-05]
		[batch 20/20] avg loss: 0.00925681964056808		[learning rate: 4.828e-05]
	Learning Rate: 4.82795e-05
	LOSS [training: 0.00786717700279827 | validation: 0.0094639122645491]
	TIME [epoch: 8.09 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004513416781394758		[learning rate: 4.8192e-05]
		[batch 20/20] avg loss: 0.007422960433683535		[learning rate: 4.8104e-05]
	Learning Rate: 4.81043e-05
	LOSS [training: 0.005968188607539148 | validation: 0.017874946255739082]
	TIME [epoch: 8.09 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00907244347492504		[learning rate: 4.8017e-05]
		[batch 20/20] avg loss: 0.00841892654320182		[learning rate: 4.793e-05]
	Learning Rate: 4.79298e-05
	LOSS [training: 0.00874568500906343 | validation: 0.00728229537681206]
	TIME [epoch: 8.09 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0048386010070466685		[learning rate: 4.7843e-05]
		[batch 20/20] avg loss: 0.0029017319357961834		[learning rate: 4.7756e-05]
	Learning Rate: 4.77558e-05
	LOSS [training: 0.0038701664714214253 | validation: 0.013290039219774338]
	TIME [epoch: 8.11 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010981598159467757		[learning rate: 4.7669e-05]
		[batch 20/20] avg loss: 0.004643520404782904		[learning rate: 4.7583e-05]
	Learning Rate: 4.75825e-05
	LOSS [training: 0.00781255928212533 | validation: 0.005788212468901873]
	TIME [epoch: 8.09 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010958088745023064		[learning rate: 4.7496e-05]
		[batch 20/20] avg loss: -1.659874517490785e-05		[learning rate: 4.741e-05]
	Learning Rate: 4.74098e-05
	LOSS [training: 0.005470744999924078 | validation: 0.008201589097933674]
	TIME [epoch: 8.09 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007536724790193193		[learning rate: 4.7324e-05]
		[batch 20/20] avg loss: 0.006228551637568565		[learning rate: 4.7238e-05]
	Learning Rate: 4.72378e-05
	LOSS [training: 0.006882638213880879 | validation: 0.014852084402905186]
	TIME [epoch: 8.08 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008809126022749999		[learning rate: 4.7152e-05]
		[batch 20/20] avg loss: 0.007102850935724563		[learning rate: 4.7066e-05]
	Learning Rate: 4.70664e-05
	LOSS [training: 0.00795598847923728 | validation: 0.014163216287483005]
	TIME [epoch: 8.09 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007035930118946663		[learning rate: 4.6981e-05]
		[batch 20/20] avg loss: 0.00820866446930667		[learning rate: 4.6896e-05]
	Learning Rate: 4.68955e-05
	LOSS [training: 0.007622297294126669 | validation: 0.01132826963371797]
	TIME [epoch: 8.12 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005923574378963408		[learning rate: 4.681e-05]
		[batch 20/20] avg loss: 0.004138849537442253		[learning rate: 4.6725e-05]
	Learning Rate: 4.67254e-05
	LOSS [training: 0.005031211958202831 | validation: 0.010890074632380618]
	TIME [epoch: 8.09 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010362852727380154		[learning rate: 4.6641e-05]
		[batch 20/20] avg loss: 0.007744398668417293		[learning rate: 4.6556e-05]
	Learning Rate: 4.65558e-05
	LOSS [training: 0.0033540566978396387 | validation: 0.008199991582195807]
	TIME [epoch: 8.1 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006350998136756927		[learning rate: 4.6471e-05]
		[batch 20/20] avg loss: 0.01118993343920443		[learning rate: 4.6387e-05]
	Learning Rate: 4.63868e-05
	LOSS [training: 0.005912516626440062 | validation: 0.013908024682261754]
	TIME [epoch: 8.1 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006525324827908246		[learning rate: 4.6303e-05]
		[batch 20/20] avg loss: 0.00015805633999331007		[learning rate: 4.6219e-05]
	Learning Rate: 4.62185e-05
	LOSS [training: 0.0033416905839507792 | validation: 0.012016336235481229]
	TIME [epoch: 8.11 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009304636222717856		[learning rate: 4.6135e-05]
		[batch 20/20] avg loss: 0.007855146592604808		[learning rate: 4.6051e-05]
	Learning Rate: 4.60508e-05
	LOSS [training: 0.008579891407661331 | validation: 0.01661554065309241]
	TIME [epoch: 8.11 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006518779757485871		[learning rate: 4.5967e-05]
		[batch 20/20] avg loss: 0.0019194857371871922		[learning rate: 4.5884e-05]
	Learning Rate: 4.58836e-05
	LOSS [training: 0.004219132747336532 | validation: 0.006463795294922242]
	TIME [epoch: 8.1 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002106098310602101		[learning rate: 4.58e-05]
		[batch 20/20] avg loss: 0.0051914289866745		[learning rate: 4.5717e-05]
	Learning Rate: 4.57171e-05
	LOSS [training: 0.0036487636486383 | validation: 0.013344513913989445]
	TIME [epoch: 8.09 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021560573518777455		[learning rate: 4.5634e-05]
		[batch 20/20] avg loss: 0.004427557944832502		[learning rate: 4.5551e-05]
	Learning Rate: 4.55512e-05
	LOSS [training: 0.0032918076483551247 | validation: 0.01128012838991864]
	TIME [epoch: 8.09 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037722172540341457		[learning rate: 4.5468e-05]
		[batch 20/20] avg loss: 0.007534082744389857		[learning rate: 4.5386e-05]
	Learning Rate: 4.53859e-05
	LOSS [training: 0.005653149999212 | validation: 0.011372816112928404]
	TIME [epoch: 8.11 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004825028120448468		[learning rate: 4.5303e-05]
		[batch 20/20] avg loss: 0.006155678095080349		[learning rate: 4.5221e-05]
	Learning Rate: 4.52212e-05
	LOSS [training: 0.00549035310776441 | validation: 0.010854444833882086]
	TIME [epoch: 8.09 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032736255154708926		[learning rate: 4.5139e-05]
		[batch 20/20] avg loss: 0.002518506399205666		[learning rate: 4.5057e-05]
	Learning Rate: 4.50571e-05
	LOSS [training: 0.00289606595733828 | validation: 0.009331187505056463]
	TIME [epoch: 8.09 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004854782131385204		[learning rate: 4.4975e-05]
		[batch 20/20] avg loss: 0.0037157243425309607		[learning rate: 4.4894e-05]
	Learning Rate: 4.48936e-05
	LOSS [training: 0.004285253236958082 | validation: 0.010263094353726472]
	TIME [epoch: 8.09 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009309412072364572		[learning rate: 4.4812e-05]
		[batch 20/20] avg loss: 0.0034898486146415554		[learning rate: 4.4731e-05]
	Learning Rate: 4.47307e-05
	LOSS [training: 0.006399630343503061 | validation: 0.008763875100650676]
	TIME [epoch: 8.11 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007937298548970285		[learning rate: 4.4649e-05]
		[batch 20/20] avg loss: 0.005411080961461198		[learning rate: 4.4568e-05]
	Learning Rate: 4.45683e-05
	LOSS [training: 0.006674189755215741 | validation: 0.009837570761057086]
	TIME [epoch: 8.09 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009184150052890675		[learning rate: 4.4487e-05]
		[batch 20/20] avg loss: 0.005195406128823867		[learning rate: 4.4407e-05]
	Learning Rate: 4.44066e-05
	LOSS [training: 0.007189778090857272 | validation: 0.008256140415347736]
	TIME [epoch: 8.09 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005768532119562862		[learning rate: 4.4326e-05]
		[batch 20/20] avg loss: 0.0034161431675213197		[learning rate: 4.4245e-05]
	Learning Rate: 4.42454e-05
	LOSS [training: 0.004592337643542091 | validation: 0.015335065673237672]
	TIME [epoch: 8.09 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005448185028073709		[learning rate: 4.4165e-05]
		[batch 20/20] avg loss: 0.00444282108801651		[learning rate: 4.4085e-05]
	Learning Rate: 4.40849e-05
	LOSS [training: 0.0024938197954119403 | validation: 0.012824609972084048]
	TIME [epoch: 8.1 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003850273750873931		[learning rate: 4.4005e-05]
		[batch 20/20] avg loss: 0.0026225773358815366		[learning rate: 4.3925e-05]
	Learning Rate: 4.39249e-05
	LOSS [training: 0.003236425543377734 | validation: 0.009541050429830254]
	TIME [epoch: 8.11 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008224173070741983		[learning rate: 4.3845e-05]
		[batch 20/20] avg loss: 0.005322396785252396		[learning rate: 4.3765e-05]
	Learning Rate: 4.37655e-05
	LOSS [training: 0.0067732849279971905 | validation: 0.010037329450798334]
	TIME [epoch: 8.1 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005242205353192304		[learning rate: 4.3686e-05]
		[batch 20/20] avg loss: 0.007008215811307117		[learning rate: 4.3607e-05]
	Learning Rate: 4.36067e-05
	LOSS [training: 0.00612521058224971 | validation: 0.01313887655211115]
	TIME [epoch: 8.09 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01072814373571826		[learning rate: 4.3527e-05]
		[batch 20/20] avg loss: 8.513529082563192e-05		[learning rate: 4.3448e-05]
	Learning Rate: 4.34484e-05
	LOSS [training: 0.0054066395132719445 | validation: 0.009759060342010704]
	TIME [epoch: 8.09 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010070673590217884		[learning rate: 4.3369e-05]
		[batch 20/20] avg loss: 0.0076728469676006494		[learning rate: 4.3291e-05]
	Learning Rate: 4.32907e-05
	LOSS [training: 0.008871760278909267 | validation: 0.0123472760518153]
	TIME [epoch: 8.11 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006927782795380488		[learning rate: 4.3212e-05]
		[batch 20/20] avg loss: 0.0035680931765986017		[learning rate: 4.3134e-05]
	Learning Rate: 4.31336e-05
	LOSS [training: 0.0052479379859895455 | validation: 0.014223613937133336]
	TIME [epoch: 8.1 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004921379274678912		[learning rate: 4.3055e-05]
		[batch 20/20] avg loss: 0.006742238407586562		[learning rate: 4.2977e-05]
	Learning Rate: 4.29771e-05
	LOSS [training: 0.005831808841132736 | validation: 0.005198002177286082]
	TIME [epoch: 8.09 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011439099798198425		[learning rate: 4.2899e-05]
		[batch 20/20] avg loss: 0.004714003940838573		[learning rate: 4.2821e-05]
	Learning Rate: 4.28211e-05
	LOSS [training: 0.0029289569603292076 | validation: 0.008010098965867301]
	TIME [epoch: 8.08 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007218120390426369		[learning rate: 4.2743e-05]
		[batch 20/20] avg loss: 0.0034303933766779426		[learning rate: 4.2666e-05]
	Learning Rate: 4.26657e-05
	LOSS [training: 0.005324256883552155 | validation: 0.017778062360252814]
	TIME [epoch: 8.11 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006225794172990679		[learning rate: 4.2588e-05]
		[batch 20/20] avg loss: 0.007548297202822782		[learning rate: 4.2511e-05]
	Learning Rate: 4.25109e-05
	LOSS [training: 0.00688704568790673 | validation: 0.013610006795429546]
	TIME [epoch: 8.11 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006369306633432511		[learning rate: 4.2434e-05]
		[batch 20/20] avg loss: 0.0007457696856320327		[learning rate: 4.2357e-05]
	Learning Rate: 4.23566e-05
	LOSS [training: 0.0035575381595322727 | validation: 0.009098717487264263]
	TIME [epoch: 8.09 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038243876964782673		[learning rate: 4.228e-05]
		[batch 20/20] avg loss: 0.005130937338616344		[learning rate: 4.2203e-05]
	Learning Rate: 4.22029e-05
	LOSS [training: 0.004477662517547305 | validation: 0.014160219256542962]
	TIME [epoch: 8.1 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008431684751865249		[learning rate: 4.2126e-05]
		[batch 20/20] avg loss: 0.0012834464144910395		[learning rate: 4.205e-05]
	Learning Rate: 4.20497e-05
	LOSS [training: 0.004857565583178143 | validation: 0.0052583233599461255]
	TIME [epoch: 8.1 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007909472062988289		[learning rate: 4.1973e-05]
		[batch 20/20] avg loss: -9.223237602247133e-05		[learning rate: 4.1897e-05]
	Learning Rate: 4.18971e-05
	LOSS [training: 0.003908619843482909 | validation: 0.007449114376185383]
	TIME [epoch: 8.11 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0075237380845537645		[learning rate: 4.1821e-05]
		[batch 20/20] avg loss: 0.007030976037815595		[learning rate: 4.1745e-05]
	Learning Rate: 4.17451e-05
	LOSS [training: 0.0072773570611846805 | validation: 0.012787939135951014]
	TIME [epoch: 8.1 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010883433588843335		[learning rate: 4.1669e-05]
		[batch 20/20] avg loss: 0.008538586814382612		[learning rate: 4.1594e-05]
	Learning Rate: 4.15936e-05
	LOSS [training: 0.009711010201612972 | validation: 0.008603732993336478]
	TIME [epoch: 8.09 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00942095154504646		[learning rate: 4.1518e-05]
		[batch 20/20] avg loss: 0.00344864886578525		[learning rate: 4.1443e-05]
	Learning Rate: 4.14426e-05
	LOSS [training: 0.006434800205415855 | validation: 0.0027635888283725095]
	TIME [epoch: 8.09 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0045272767087029845		[learning rate: 4.1367e-05]
		[batch 20/20] avg loss: 0.0038164422390235056		[learning rate: 4.1292e-05]
	Learning Rate: 4.12922e-05
	LOSS [training: 0.004171859473863244 | validation: 0.009602013738934215]
	TIME [epoch: 8.09 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008520600453212276		[learning rate: 4.1217e-05]
		[batch 20/20] avg loss: 0.007466417560110908		[learning rate: 4.1142e-05]
	Learning Rate: 4.11424e-05
	LOSS [training: 0.007993509006661592 | validation: 0.019579736682845836]
	TIME [epoch: 8.1 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006624386898508529		[learning rate: 4.1068e-05]
		[batch 20/20] avg loss: 0.004132980785979571		[learning rate: 4.0993e-05]
	Learning Rate: 4.09931e-05
	LOSS [training: 0.005378683842244049 | validation: 0.016138792088383]
	TIME [epoch: 8.08 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011179584837293612		[learning rate: 4.0919e-05]
		[batch 20/20] avg loss: 0.004853703538256001		[learning rate: 4.0844e-05]
	Learning Rate: 4.08443e-05
	LOSS [training: 0.008016644187774806 | validation: 0.012638682233641797]
	TIME [epoch: 8.08 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006634753901892959		[learning rate: 4.077e-05]
		[batch 20/20] avg loss: 0.006509101008266467		[learning rate: 4.0696e-05]
	Learning Rate: 4.06961e-05
	LOSS [training: 0.0065719274550797135 | validation: 0.010380519806904968]
	TIME [epoch: 8.08 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006678687855143586		[learning rate: 4.0622e-05]
		[batch 20/20] avg loss: 0.008816922538894167		[learning rate: 4.0548e-05]
	Learning Rate: 4.05484e-05
	LOSS [training: 0.007747805197018877 | validation: 0.01520750671586036]
	TIME [epoch: 8.11 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00609749284367454		[learning rate: 4.0475e-05]
		[batch 20/20] avg loss: 0.0029610992539970205		[learning rate: 4.0401e-05]
	Learning Rate: 4.04012e-05
	LOSS [training: 0.00452929604883578 | validation: 0.018354188265567284]
	TIME [epoch: 8.09 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00560705999666847		[learning rate: 4.0328e-05]
		[batch 20/20] avg loss: 0.006157573031343582		[learning rate: 4.0255e-05]
	Learning Rate: 4.02546e-05
	LOSS [training: 0.0058823165140060255 | validation: 0.013147875982018411]
	TIME [epoch: 8.08 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006514804736420321		[learning rate: 4.0182e-05]
		[batch 20/20] avg loss: 0.004880568078670701		[learning rate: 4.0109e-05]
	Learning Rate: 4.01085e-05
	LOSS [training: 0.00569768640754551 | validation: 0.012014904820045252]
	TIME [epoch: 8.09 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010513620797870415		[learning rate: 4.0036e-05]
		[batch 20/20] avg loss: 0.0013040340615725452		[learning rate: 3.9963e-05]
	Learning Rate: 3.9963e-05
	LOSS [training: 0.0059088274297214825 | validation: 0.01109372057859626]
	TIME [epoch: 8.1 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007813058391858221		[learning rate: 3.989e-05]
		[batch 20/20] avg loss: 0.008519266966579925		[learning rate: 3.9818e-05]
	Learning Rate: 3.9818e-05
	LOSS [training: 0.004650286402882873 | validation: 0.011830712474446746]
	TIME [epoch: 8.1 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008555828735481968		[learning rate: 3.9746e-05]
		[batch 20/20] avg loss: 0.009498387684991293		[learning rate: 3.9673e-05]
	Learning Rate: 3.96735e-05
	LOSS [training: 0.009027108210236631 | validation: 0.011516644711677246]
	TIME [epoch: 8.09 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003629641669835604		[learning rate: 3.9601e-05]
		[batch 20/20] avg loss: 0.001430712623163314		[learning rate: 3.9529e-05]
	Learning Rate: 3.95295e-05
	LOSS [training: 0.002530177146499459 | validation: 0.014704139105737674]
	TIME [epoch: 8.1 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.002144876661892384		[learning rate: 3.9458e-05]
		[batch 20/20] avg loss: 0.012630632509938785		[learning rate: 3.9386e-05]
	Learning Rate: 3.9386e-05
	LOSS [training: 0.0052428779240232 | validation: 0.009194944125069902]
	TIME [epoch: 8.1 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031227728921028016		[learning rate: 3.9315e-05]
		[batch 20/20] avg loss: 0.004485356363464246		[learning rate: 3.9243e-05]
	Learning Rate: 3.92431e-05
	LOSS [training: 0.0038040646277835237 | validation: 0.019321678228473367]
	TIME [epoch: 8.12 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004002168548330804		[learning rate: 3.9172e-05]
		[batch 20/20] avg loss: 0.010117199556399382		[learning rate: 3.9101e-05]
	Learning Rate: 3.91007e-05
	LOSS [training: 0.007059684052365092 | validation: 0.006956288145756817]
	TIME [epoch: 8.09 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007860618489567962		[learning rate: 3.903e-05]
		[batch 20/20] avg loss: 0.0030387564239736367		[learning rate: 3.8959e-05]
	Learning Rate: 3.89588e-05
	LOSS [training: 0.005449687456770799 | validation: 0.013060770242315355]
	TIME [epoch: 8.09 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004863835387593833		[learning rate: 3.8888e-05]
		[batch 20/20] avg loss: 0.005059205975209678		[learning rate: 3.8817e-05]
	Learning Rate: 3.88174e-05
	LOSS [training: 0.004961520681401756 | validation: 0.00793635937386243]
	TIME [epoch: 8.08 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009098944211487405		[learning rate: 3.8747e-05]
		[batch 20/20] avg loss: 0.004363168398106928		[learning rate: 3.8677e-05]
	Learning Rate: 3.86765e-05
	LOSS [training: 0.0067310563047971676 | validation: 0.011550179635971424]
	TIME [epoch: 8.1 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003609845608358437		[learning rate: 3.8606e-05]
		[batch 20/20] avg loss: 0.002129553041070599		[learning rate: 3.8536e-05]
	Learning Rate: 3.85362e-05
	LOSS [training: 0.0028696993247145173 | validation: 0.0125193289134687]
	TIME [epoch: 8.1 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0064348409695407114		[learning rate: 3.8466e-05]
		[batch 20/20] avg loss: 0.0047816072828569915		[learning rate: 3.8396e-05]
	Learning Rate: 3.83963e-05
	LOSS [training: 0.00560822412619885 | validation: 0.015054667146933515]
	TIME [epoch: 8.08 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005583468415461574		[learning rate: 3.8327e-05]
		[batch 20/20] avg loss: 0.005938739550365409		[learning rate: 3.8257e-05]
	Learning Rate: 3.8257e-05
	LOSS [training: 0.005761103982913491 | validation: 0.01353776894898168]
	TIME [epoch: 8.09 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0074985371956243305		[learning rate: 3.8187e-05]
		[batch 20/20] avg loss: 0.002797395558875167		[learning rate: 3.8118e-05]
	Learning Rate: 3.81181e-05
	LOSS [training: 0.005147966377249747 | validation: 0.00805390754229828]
	TIME [epoch: 8.08 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003440856599031416		[learning rate: 3.8049e-05]
		[batch 20/20] avg loss: 0.004114859663077743		[learning rate: 3.798e-05]
	Learning Rate: 3.79798e-05
	LOSS [training: 0.0037778581310545798 | validation: 0.0037147495950976095]
	TIME [epoch: 8.11 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009861873299084114		[learning rate: 3.7911e-05]
		[batch 20/20] avg loss: 0.00045050428215655534		[learning rate: 3.7842e-05]
	Learning Rate: 3.7842e-05
	LOSS [training: 0.005156188790620335 | validation: 0.009792598251736834]
	TIME [epoch: 8.09 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004608211476784257		[learning rate: 3.7773e-05]
		[batch 20/20] avg loss: 0.0056853159265147805		[learning rate: 3.7705e-05]
	Learning Rate: 3.77046e-05
	LOSS [training: 0.005146763701649519 | validation: 0.015470452264117623]
	TIME [epoch: 8.08 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037139296218418293		[learning rate: 3.7636e-05]
		[batch 20/20] avg loss: 0.003924149106496945		[learning rate: 3.7568e-05]
	Learning Rate: 3.75678e-05
	LOSS [training: 0.0038190393641693867 | validation: 0.006014224729282203]
	TIME [epoch: 8.09 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029051613256139037		[learning rate: 3.75e-05]
		[batch 20/20] avg loss: 0.012337821944631146		[learning rate: 3.7431e-05]
	Learning Rate: 3.74315e-05
	LOSS [training: 0.007621491635122525 | validation: 0.007663754549132274]
	TIME [epoch: 8.1 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018862031039380968		[learning rate: 3.7363e-05]
		[batch 20/20] avg loss: 0.006903439623571683		[learning rate: 3.7296e-05]
	Learning Rate: 3.72956e-05
	LOSS [training: 0.00439482136375489 | validation: 0.009919881353475712]
	TIME [epoch: 8.1 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002854373452213242		[learning rate: 3.7228e-05]
		[batch 20/20] avg loss: 0.007240617241033748		[learning rate: 3.716e-05]
	Learning Rate: 3.71603e-05
	LOSS [training: 0.005047495346623495 | validation: 0.004354589256471551]
	TIME [epoch: 8.08 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035094143678934637		[learning rate: 3.7093e-05]
		[batch 20/20] avg loss: 0.004238494646342907		[learning rate: 3.7025e-05]
	Learning Rate: 3.70254e-05
	LOSS [training: 0.0038739545071181853 | validation: 0.010019432076371505]
	TIME [epoch: 8.09 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0064894200870482185		[learning rate: 3.6958e-05]
		[batch 20/20] avg loss: 0.006409923841617378		[learning rate: 3.6891e-05]
	Learning Rate: 3.68911e-05
	LOSS [training: 0.006449671964332798 | validation: 0.011726143078438844]
	TIME [epoch: 8.08 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009374994941428894		[learning rate: 3.6824e-05]
		[batch 20/20] avg loss: 0.005270018056947613		[learning rate: 3.6757e-05]
	Learning Rate: 3.67572e-05
	LOSS [training: 0.007322506499188253 | validation: 0.01287059259272496]
	TIME [epoch: 8.11 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006166711671617781		[learning rate: 3.669e-05]
		[batch 20/20] avg loss: 0.005481775328624672		[learning rate: 3.6624e-05]
	Learning Rate: 3.66238e-05
	LOSS [training: 0.005824243500121226 | validation: 0.009823592985854926]
	TIME [epoch: 8.09 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009623870733299737		[learning rate: 3.6557e-05]
		[batch 20/20] avg loss: 0.006836482969985858		[learning rate: 3.6491e-05]
	Learning Rate: 3.64909e-05
	LOSS [training: 0.008230176851642795 | validation: 0.012422786967214305]
	TIME [epoch: 8.09 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001873880670596725		[learning rate: 3.6425e-05]
		[batch 20/20] avg loss: 0.004895733882166879		[learning rate: 3.6358e-05]
	Learning Rate: 3.63584e-05
	LOSS [training: 0.003384807276381802 | validation: 0.009611469716995436]
	TIME [epoch: 8.09 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009502403264087932		[learning rate: 3.6292e-05]
		[batch 20/20] avg loss: 0.004593694153511645		[learning rate: 3.6226e-05]
	Learning Rate: 3.62265e-05
	LOSS [training: 0.007048048708799789 | validation: 0.018221081344994845]
	TIME [epoch: 8.11 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026327381698006812		[learning rate: 3.6161e-05]
		[batch 20/20] avg loss: 0.011296163243317461		[learning rate: 3.6095e-05]
	Learning Rate: 3.6095e-05
	LOSS [training: 0.006964450706559072 | validation: 0.010446576018328742]
	TIME [epoch: 8.1 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005826016612901926		[learning rate: 3.6029e-05]
		[batch 20/20] avg loss: 0.008845620465276089		[learning rate: 3.5964e-05]
	Learning Rate: 3.5964e-05
	LOSS [training: 0.007335818539089005 | validation: 0.015433618331812168]
	TIME [epoch: 8.1 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008748899098061987		[learning rate: 3.5899e-05]
		[batch 20/20] avg loss: 0.006120228116765715		[learning rate: 3.5834e-05]
	Learning Rate: 3.58335e-05
	LOSS [training: 0.007434563607413852 | validation: 0.007331738097679398]
	TIME [epoch: 8.1 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004262005448007226		[learning rate: 3.5768e-05]
		[batch 20/20] avg loss: 0.0066351198458075135		[learning rate: 3.5703e-05]
	Learning Rate: 3.57035e-05
	LOSS [training: 0.005448562646907369 | validation: 0.009512472209089264]
	TIME [epoch: 8.09 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004216898705122677		[learning rate: 3.5639e-05]
		[batch 20/20] avg loss: 0.002553164297823287		[learning rate: 3.5574e-05]
	Learning Rate: 3.55739e-05
	LOSS [training: 0.0033850315014729813 | validation: 0.016666785363370868]
	TIME [epoch: 8.11 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009712049028507235		[learning rate: 3.5509e-05]
		[batch 20/20] avg loss: 0.007633389412719166		[learning rate: 3.5445e-05]
	Learning Rate: 3.54448e-05
	LOSS [training: 0.0086727192206132 | validation: 0.008744582925532342]
	TIME [epoch: 8.08 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006441029047521782		[learning rate: 3.538e-05]
		[batch 20/20] avg loss: 0.006265811162293962		[learning rate: 3.5316e-05]
	Learning Rate: 3.53162e-05
	LOSS [training: 0.006353420104907871 | validation: 0.008990756243516302]
	TIME [epoch: 8.09 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003370939182280184		[learning rate: 3.5252e-05]
		[batch 20/20] avg loss: 0.007950844123987333		[learning rate: 3.5188e-05]
	Learning Rate: 3.5188e-05
	LOSS [training: 0.005660891653133758 | validation: 0.006003199894738279]
	TIME [epoch: 8.09 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008911095100261184		[learning rate: 3.5124e-05]
		[batch 20/20] avg loss: 0.0006308359825884834		[learning rate: 3.506e-05]
	Learning Rate: 3.50603e-05
	LOSS [training: 0.004770965541424835 | validation: 0.012417614464341779]
	TIME [epoch: 8.12 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0005974765026620683		[learning rate: 3.4997e-05]
		[batch 20/20] avg loss: 0.0074555924495756125		[learning rate: 3.4933e-05]
	Learning Rate: 3.49331e-05
	LOSS [training: 0.003429057973456772 | validation: 0.008860238077348096]
	TIME [epoch: 8.1 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00928526248687786		[learning rate: 3.487e-05]
		[batch 20/20] avg loss: 0.004306293725629363		[learning rate: 3.4806e-05]
	Learning Rate: 3.48063e-05
	LOSS [training: 0.006795778106253611 | validation: 0.008343970081198493]
	TIME [epoch: 8.09 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00677397455643091		[learning rate: 3.4743e-05]
		[batch 20/20] avg loss: 0.004267928437007757		[learning rate: 3.468e-05]
	Learning Rate: 3.468e-05
	LOSS [training: 0.005520951496719334 | validation: 0.006487096531565621]
	TIME [epoch: 8.09 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007038150959144759		[learning rate: 3.4617e-05]
		[batch 20/20] avg loss: -0.0008567708932804956		[learning rate: 3.4554e-05]
	Learning Rate: 3.45541e-05
	LOSS [training: 0.003090690032932132 | validation: 0.008570666709950716]
	TIME [epoch: 8.09 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0068729450774397014		[learning rate: 3.4491e-05]
		[batch 20/20] avg loss: 0.0011229895400455118		[learning rate: 3.4429e-05]
	Learning Rate: 3.44287e-05
	LOSS [training: 0.003997967308742605 | validation: 0.009453148488920673]
	TIME [epoch: 8.11 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009663326102270511		[learning rate: 3.4366e-05]
		[batch 20/20] avg loss: 0.001686982421606017		[learning rate: 3.4304e-05]
	Learning Rate: 3.43038e-05
	LOSS [training: 0.005675154261938264 | validation: 0.011023222615259455]
	TIME [epoch: 8.09 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003315449260482222		[learning rate: 3.4241e-05]
		[batch 20/20] avg loss: 0.0032232792729637756		[learning rate: 3.4179e-05]
	Learning Rate: 3.41793e-05
	LOSS [training: 0.0032693642667229995 | validation: 0.014629514656464957]
	TIME [epoch: 8.08 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004700413352466288		[learning rate: 3.4117e-05]
		[batch 20/20] avg loss: 0.008257530765676238		[learning rate: 3.4055e-05]
	Learning Rate: 3.40553e-05
	LOSS [training: 0.006478972059071263 | validation: 0.005208304945690547]
	TIME [epoch: 8.09 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0076735281161970995		[learning rate: 3.3993e-05]
		[batch 20/20] avg loss: 0.005225113478887225		[learning rate: 3.3932e-05]
	Learning Rate: 3.39317e-05
	LOSS [training: 0.006449320797542163 | validation: 0.01514727152732246]
	TIME [epoch: 8.11 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019315234832146643		[learning rate: 3.387e-05]
		[batch 20/20] avg loss: 0.00264227286517295		[learning rate: 3.3809e-05]
	Learning Rate: 3.38085e-05
	LOSS [training: 0.0022868981741938077 | validation: 0.004919355960849961]
	TIME [epoch: 8.09 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006538722035036669		[learning rate: 3.3747e-05]
		[batch 20/20] avg loss: 0.002760386603059956		[learning rate: 3.3686e-05]
	Learning Rate: 3.36858e-05
	LOSS [training: 0.004649554319048313 | validation: 0.007411116920499809]
	TIME [epoch: 8.09 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006553472294969315		[learning rate: 3.3625e-05]
		[batch 20/20] avg loss: 0.0032438724687651995		[learning rate: 3.3564e-05]
	Learning Rate: 3.35636e-05
	LOSS [training: 0.004898672381867257 | validation: 0.006718166213887981]
	TIME [epoch: 8.09 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017858822551938803		[learning rate: 3.3503e-05]
		[batch 20/20] avg loss: 0.006858747436391308		[learning rate: 3.3442e-05]
	Learning Rate: 3.34418e-05
	LOSS [training: 0.004322314845792595 | validation: 0.010029344418614135]
	TIME [epoch: 8.09 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006538663647452776		[learning rate: 3.3381e-05]
		[batch 20/20] avg loss: 0.005124538577047621		[learning rate: 3.332e-05]
	Learning Rate: 3.33204e-05
	LOSS [training: 0.005831601112250199 | validation: 0.004823510989390534]
	TIME [epoch: 8.12 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036099716406054216		[learning rate: 3.326e-05]
		[batch 20/20] avg loss: 0.004726748534840371		[learning rate: 3.32e-05]
	Learning Rate: 3.31995e-05
	LOSS [training: 0.004168360087722897 | validation: 0.007320395862133934]
	TIME [epoch: 8.08 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006964724971560236		[learning rate: 3.3139e-05]
		[batch 20/20] avg loss: 0.005478702193586692		[learning rate: 3.3079e-05]
	Learning Rate: 3.3079e-05
	LOSS [training: 0.006221713582573464 | validation: 0.0040031845861305]
	TIME [epoch: 8.09 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009148959418126382		[learning rate: 3.3019e-05]
		[batch 20/20] avg loss: 0.003600514283499915		[learning rate: 3.2959e-05]
	Learning Rate: 3.2959e-05
	LOSS [training: 0.00637473685081315 | validation: 0.008862168493641819]
	TIME [epoch: 8.08 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008343697173936649		[learning rate: 3.2899e-05]
		[batch 20/20] avg loss: 0.00689440004607674		[learning rate: 3.2839e-05]
	Learning Rate: 3.28394e-05
	LOSS [training: 0.007619048610006694 | validation: 0.007770093278874749]
	TIME [epoch: 8.12 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003941448253465658		[learning rate: 3.278e-05]
		[batch 20/20] avg loss: 0.005744992248641211		[learning rate: 3.272e-05]
	Learning Rate: 3.27202e-05
	LOSS [training: 0.004843220251053434 | validation: 0.017848790382986864]
	TIME [epoch: 8.09 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005866229879325996		[learning rate: 3.2661e-05]
		[batch 20/20] avg loss: 0.004904247451404239		[learning rate: 3.2601e-05]
	Learning Rate: 3.26014e-05
	LOSS [training: 0.005385238665365119 | validation: 0.010877635420331037]
	TIME [epoch: 8.1 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006612258570657422		[learning rate: 3.2542e-05]
		[batch 20/20] avg loss: 0.0031020967628535437		[learning rate: 3.2483e-05]
	Learning Rate: 3.24831e-05
	LOSS [training: 0.004857177666755483 | validation: 0.013426035818546862]
	TIME [epoch: 8.09 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011285600953826608		[learning rate: 3.2424e-05]
		[batch 20/20] avg loss: 0.006737240764431826		[learning rate: 3.2365e-05]
	Learning Rate: 3.23652e-05
	LOSS [training: 0.009011420859129218 | validation: 0.014400113379142867]
	TIME [epoch: 8.13 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010283281547312876		[learning rate: 3.2306e-05]
		[batch 20/20] avg loss: 0.008758777242118333		[learning rate: 3.2248e-05]
	Learning Rate: 3.22478e-05
	LOSS [training: 0.009521029394715604 | validation: 0.005087504115400293]
	TIME [epoch: 8.11 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0058775576238889495		[learning rate: 3.2189e-05]
		[batch 20/20] avg loss: 0.004929396077936776		[learning rate: 3.2131e-05]
	Learning Rate: 3.21308e-05
	LOSS [training: 0.005403476850912863 | validation: 0.00029520053842736847]
	TIME [epoch: 8.09 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032401376129938117		[learning rate: 3.2072e-05]
		[batch 20/20] avg loss: 0.0037885980424866433		[learning rate: 3.2014e-05]
	Learning Rate: 3.20142e-05
	LOSS [training: 0.003514367827740228 | validation: 0.010827911075112986]
	TIME [epoch: 8.1 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007674469513110374		[learning rate: 3.1956e-05]
		[batch 20/20] avg loss: 0.009268567232596375		[learning rate: 3.1898e-05]
	Learning Rate: 3.1898e-05
	LOSS [training: 0.008471518372853374 | validation: 0.008263942084135657]
	TIME [epoch: 8.09 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008791360442877889		[learning rate: 3.184e-05]
		[batch 20/20] avg loss: 0.00735730827063614		[learning rate: 3.1782e-05]
	Learning Rate: 3.17822e-05
	LOSS [training: 0.008074334356757014 | validation: 0.016486340695108345]
	TIME [epoch: 8.12 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004176822105498565		[learning rate: 3.1725e-05]
		[batch 20/20] avg loss: 0.0045986927743669		[learning rate: 3.1667e-05]
	Learning Rate: 3.16669e-05
	LOSS [training: 0.0043877574399327325 | validation: 0.01125709685092132]
	TIME [epoch: 8.09 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012819267729314919		[learning rate: 3.1609e-05]
		[batch 20/20] avg loss: 0.01237372946669497		[learning rate: 3.1552e-05]
	Learning Rate: 3.1552e-05
	LOSS [training: 0.006827828119813232 | validation: 0.0059087975139557256]
	TIME [epoch: 8.09 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027970663953482268		[learning rate: 3.1495e-05]
		[batch 20/20] avg loss: 0.007365398247023844		[learning rate: 3.1437e-05]
	Learning Rate: 3.14375e-05
	LOSS [training: 0.005081232321186036 | validation: 0.009797328851362955]
	TIME [epoch: 8.09 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003690656012772968		[learning rate: 3.138e-05]
		[batch 20/20] avg loss: 0.0034946693656519546		[learning rate: 3.1323e-05]
	Learning Rate: 3.13234e-05
	LOSS [training: 0.0035926626892124613 | validation: 0.005350832856495732]
	TIME [epoch: 8.1 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0067133954822575365		[learning rate: 3.1266e-05]
		[batch 20/20] avg loss: 0.006609789624214442		[learning rate: 3.121e-05]
	Learning Rate: 3.12097e-05
	LOSS [training: 0.006661592553235991 | validation: 0.008585190243568444]
	TIME [epoch: 8.11 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007970783829415561		[learning rate: 3.1153e-05]
		[batch 20/20] avg loss: 0.0053070414433215585		[learning rate: 3.1096e-05]
	Learning Rate: 3.10964e-05
	LOSS [training: 0.006638912636368561 | validation: 0.016166850145267156]
	TIME [epoch: 8.09 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009022720732056577		[learning rate: 3.104e-05]
		[batch 20/20] avg loss: -0.0005632022874607373		[learning rate: 3.0984e-05]
	Learning Rate: 3.09836e-05
	LOSS [training: 0.004229759222297919 | validation: 0.006487041493322683]
	TIME [epoch: 8.09 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0065513868463189006		[learning rate: 3.0927e-05]
		[batch 20/20] avg loss: 0.0030710675452463782		[learning rate: 3.0871e-05]
	Learning Rate: 3.08711e-05
	LOSS [training: 0.00481122719578264 | validation: 0.011028756583093248]
	TIME [epoch: 8.08 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002322151902798374		[learning rate: 3.0815e-05]
		[batch 20/20] avg loss: 0.005596183441600717		[learning rate: 3.0759e-05]
	Learning Rate: 3.07591e-05
	LOSS [training: 0.003959167672199546 | validation: 0.014508416230290415]
	TIME [epoch: 8.11 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004461516760815214		[learning rate: 3.0703e-05]
		[batch 20/20] avg loss: 0.006036602396848457		[learning rate: 3.0647e-05]
	Learning Rate: 3.06475e-05
	LOSS [training: 0.005249059578831835 | validation: 0.01297816290661506]
	TIME [epoch: 8.1 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00357685788098343		[learning rate: 3.0592e-05]
		[batch 20/20] avg loss: 0.0059799034486707385		[learning rate: 3.0536e-05]
	Learning Rate: 3.05363e-05
	LOSS [training: 0.0047783806648270845 | validation: 0.01584612651283898]
	TIME [epoch: 8.09 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007356598088230625		[learning rate: 3.0481e-05]
		[batch 20/20] avg loss: 0.003574132154641281		[learning rate: 3.0425e-05]
	Learning Rate: 3.04254e-05
	LOSS [training: 0.005465365121435953 | validation: 0.01610718556516296]
	TIME [epoch: 8.09 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0065781020353609		[learning rate: 3.037e-05]
		[batch 20/20] avg loss: 0.005310125555142341		[learning rate: 3.0315e-05]
	Learning Rate: 3.0315e-05
	LOSS [training: 0.005944113795251621 | validation: 0.010744877540632143]
	TIME [epoch: 8.11 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00404127261857132		[learning rate: 3.026e-05]
		[batch 20/20] avg loss: 0.008989496864612635		[learning rate: 3.0205e-05]
	Learning Rate: 3.0205e-05
	LOSS [training: 0.006515384741591977 | validation: 0.009977630449345801]
	TIME [epoch: 8.11 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012539737384293444		[learning rate: 3.015e-05]
		[batch 20/20] avg loss: 0.005132734311946592		[learning rate: 3.0095e-05]
	Learning Rate: 3.00954e-05
	LOSS [training: 0.003193354025187968 | validation: 0.014440404978903143]
	TIME [epoch: 8.1 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0023776218323409573		[learning rate: 3.0041e-05]
		[batch 20/20] avg loss: 0.003846153450510637		[learning rate: 2.9986e-05]
	Learning Rate: 2.99862e-05
	LOSS [training: 0.0007342658090848397 | validation: 0.01070755161740376]
	TIME [epoch: 8.1 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003237028221318063		[learning rate: 2.9932e-05]
		[batch 20/20] avg loss: 0.005348060461396553		[learning rate: 2.9877e-05]
	Learning Rate: 2.98774e-05
	LOSS [training: 0.004292544341357308 | validation: 0.00673312959000077]
	TIME [epoch: 8.1 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006491455950926019		[learning rate: 2.9823e-05]
		[batch 20/20] avg loss: 0.0017866509664566424		[learning rate: 2.9769e-05]
	Learning Rate: 2.97689e-05
	LOSS [training: 0.00413905345869133 | validation: 0.012966849867068704]
	TIME [epoch: 8.12 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0002080654817257166		[learning rate: 2.9715e-05]
		[batch 20/20] avg loss: 0.0053315365673993935		[learning rate: 2.9661e-05]
	Learning Rate: 2.96609e-05
	LOSS [training: 0.002769801024562555 | validation: 0.008687919416381928]
	TIME [epoch: 8.11 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038575695443589034		[learning rate: 2.9607e-05]
		[batch 20/20] avg loss: 0.0022630292750192536		[learning rate: 2.9553e-05]
	Learning Rate: 2.95533e-05
	LOSS [training: 0.003060299409689079 | validation: 0.01156180607710634]
	TIME [epoch: 8.09 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00013329025691139837		[learning rate: 2.95e-05]
		[batch 20/20] avg loss: 0.00512724462281563		[learning rate: 2.9446e-05]
	Learning Rate: 2.9446e-05
	LOSS [training: 0.0024969771829521157 | validation: 0.009056425893080727]
	TIME [epoch: 8.09 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00769075962236869		[learning rate: 2.9393e-05]
		[batch 20/20] avg loss: 0.005029698754024331		[learning rate: 2.9339e-05]
	Learning Rate: 2.93391e-05
	LOSS [training: 0.006360229188196511 | validation: 0.008910088452782882]
	TIME [epoch: 8.11 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00271896899781911		[learning rate: 2.9286e-05]
		[batch 20/20] avg loss: 0.004340607354680669		[learning rate: 2.9233e-05]
	Learning Rate: 2.92327e-05
	LOSS [training: 0.0035297881762498895 | validation: 0.010250367429775267]
	TIME [epoch: 8.11 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.000589217585362073		[learning rate: 2.918e-05]
		[batch 20/20] avg loss: 0.010784953178424095		[learning rate: 2.9127e-05]
	Learning Rate: 2.91266e-05
	LOSS [training: 0.005097867796531011 | validation: 0.009224651115278093]
	TIME [epoch: 8.09 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006405053748348713		[learning rate: 2.9074e-05]
		[batch 20/20] avg loss: 0.006225395481409525		[learning rate: 2.9021e-05]
	Learning Rate: 2.90209e-05
	LOSS [training: 0.006315224614879119 | validation: 0.01134783279644014]
	TIME [epoch: 8.1 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006180618350261472		[learning rate: 2.8968e-05]
		[batch 20/20] avg loss: 0.008084141382635014		[learning rate: 2.8916e-05]
	Learning Rate: 2.89156e-05
	LOSS [training: 0.004351101608830581 | validation: 0.007648575129308581]
	TIME [epoch: 8.09 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006592774155746589		[learning rate: 2.8863e-05]
		[batch 20/20] avg loss: 0.003455922149583112		[learning rate: 2.8811e-05]
	Learning Rate: 2.88106e-05
	LOSS [training: 0.005024348152664853 | validation: 0.012900585839047645]
	TIME [epoch: 8.12 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033287468306870383		[learning rate: 2.8758e-05]
		[batch 20/20] avg loss: 0.003978901198303842		[learning rate: 2.8706e-05]
	Learning Rate: 2.87061e-05
	LOSS [training: 0.0036538240144954395 | validation: 0.006046577555006724]
	TIME [epoch: 8.1 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005350974583856452		[learning rate: 2.8654e-05]
		[batch 20/20] avg loss: 0.0012176607461362597		[learning rate: 2.8602e-05]
	Learning Rate: 2.86019e-05
	LOSS [training: 0.0032843176649963563 | validation: 0.008339004601527566]
	TIME [epoch: 8.1 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007929774081208275		[learning rate: 2.855e-05]
		[batch 20/20] avg loss: 0.007669281741857358		[learning rate: 2.8498e-05]
	Learning Rate: 2.84981e-05
	LOSS [training: 0.007799527911532817 | validation: 0.012077673112562356]
	TIME [epoch: 8.09 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004751872632612446		[learning rate: 2.8446e-05]
		[batch 20/20] avg loss: 0.00521064398510644		[learning rate: 2.8395e-05]
	Learning Rate: 2.83947e-05
	LOSS [training: 0.004981258308859442 | validation: 0.008192925488590373]
	TIME [epoch: 8.11 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006235322928430825		[learning rate: 2.8343e-05]
		[batch 20/20] avg loss: 0.0011291913105375897		[learning rate: 2.8292e-05]
	Learning Rate: 2.82916e-05
	LOSS [training: 0.0036822571194842072 | validation: 0.008061720567823059]
	TIME [epoch: 8.11 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006991010161906367		[learning rate: 2.824e-05]
		[batch 20/20] avg loss: 0.0073683146233073414		[learning rate: 2.8189e-05]
	Learning Rate: 2.8189e-05
	LOSS [training: 0.007179662392606855 | validation: 0.005781029889478853]
	TIME [epoch: 8.09 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00917185046453751		[learning rate: 2.8138e-05]
		[batch 20/20] avg loss: 0.00783221624319357		[learning rate: 2.8087e-05]
	Learning Rate: 2.80867e-05
	LOSS [training: 0.008502033353865538 | validation: 0.0161629837173567]
	TIME [epoch: 8.1 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011647418512341303		[learning rate: 2.8036e-05]
		[batch 20/20] avg loss: -0.0010740744842707725		[learning rate: 2.7985e-05]
	Learning Rate: 2.79847e-05
	LOSS [training: 0.005286672014035264 | validation: 0.0076187085563785666]
	TIME [epoch: 8.09 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003752172776323994		[learning rate: 2.7934e-05]
		[batch 20/20] avg loss: 0.010242734461921293		[learning rate: 2.7883e-05]
	Learning Rate: 2.78832e-05
	LOSS [training: 0.006997453619122643 | validation: 0.013685633912919682]
	TIME [epoch: 8.12 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003942975916540088		[learning rate: 2.7833e-05]
		[batch 20/20] avg loss: 0.006497736007559355		[learning rate: 2.7782e-05]
	Learning Rate: 2.7782e-05
	LOSS [training: 0.005220355962049721 | validation: 0.008607760335588941]
	TIME [epoch: 8.1 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002815096187399031		[learning rate: 2.7732e-05]
		[batch 20/20] avg loss: 0.004703100083159998		[learning rate: 2.7681e-05]
	Learning Rate: 2.76812e-05
	LOSS [training: 0.0037590981352795157 | validation: 0.014337247741527684]
	TIME [epoch: 8.09 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016286472523860181		[learning rate: 2.7631e-05]
		[batch 20/20] avg loss: -4.738856968298701e-05		[learning rate: 2.7581e-05]
	Learning Rate: 2.75807e-05
	LOSS [training: 0.0007906293413515156 | validation: 0.0060828836638843475]
	TIME [epoch: 8.1 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001332869366921416		[learning rate: 2.7531e-05]
		[batch 20/20] avg loss: 0.00554831962358542		[learning rate: 2.7481e-05]
	Learning Rate: 2.74806e-05
	LOSS [training: 0.0034405944952534185 | validation: 0.015537570010810586]
	TIME [epoch: 8.11 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001798024535639145		[learning rate: 2.7431e-05]
		[batch 20/20] avg loss: 0.003982368711964684		[learning rate: 2.7381e-05]
	Learning Rate: 2.73809e-05
	LOSS [training: 0.0028901966238019153 | validation: 0.012641599375676427]
	TIME [epoch: 8.11 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005957683816506467		[learning rate: 2.7331e-05]
		[batch 20/20] avg loss: 0.005522757783907822		[learning rate: 2.7282e-05]
	Learning Rate: 2.72815e-05
	LOSS [training: 0.005740220800207145 | validation: 0.014890360194644823]
	TIME [epoch: 8.09 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007972689265638988		[learning rate: 2.7232e-05]
		[batch 20/20] avg loss: 0.007375613932480053		[learning rate: 2.7183e-05]
	Learning Rate: 2.71825e-05
	LOSS [training: 0.007674151599059519 | validation: 0.00553037394038456]
	TIME [epoch: 8.09 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0077905433357275495		[learning rate: 2.7133e-05]
		[batch 20/20] avg loss: 0.0015835891092198836		[learning rate: 2.7084e-05]
	Learning Rate: 2.70839e-05
	LOSS [training: 0.004687066222473717 | validation: 0.010258866993327414]
	TIME [epoch: 8.1 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007877448728062397		[learning rate: 2.7035e-05]
		[batch 20/20] avg loss: 0.0067827539122067805		[learning rate: 2.6986e-05]
	Learning Rate: 2.69856e-05
	LOSS [training: 0.007330101320134589 | validation: 0.010404408801704621]
	TIME [epoch: 8.12 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.001520316242870672		[learning rate: 2.6937e-05]
		[batch 20/20] avg loss: 0.002421277231664858		[learning rate: 2.6888e-05]
	Learning Rate: 2.68876e-05
	LOSS [training: 0.001970796737267765 | validation: 0.008698584689891304]
	TIME [epoch: 8.09 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033377006063147418		[learning rate: 2.6839e-05]
		[batch 20/20] avg loss: 0.0027167180045091907		[learning rate: 2.679e-05]
	Learning Rate: 2.67901e-05
	LOSS [training: 0.003027209305411967 | validation: 0.010951473073345444]
	TIME [epoch: 8.09 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005336147195679366		[learning rate: 2.6741e-05]
		[batch 20/20] avg loss: -0.0003701626148222933		[learning rate: 2.6693e-05]
	Learning Rate: 2.66928e-05
	LOSS [training: 0.002482992290428537 | validation: 0.007771266217650763]
	TIME [epoch: 8.1 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007648374657571531		[learning rate: 2.6644e-05]
		[batch 20/20] avg loss: 0.0033980503979881642		[learning rate: 2.6596e-05]
	Learning Rate: 2.6596e-05
	LOSS [training: 0.005523212527779847 | validation: 0.00824333010356629]
	TIME [epoch: 8.11 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008783729794126383		[learning rate: 2.6548e-05]
		[batch 20/20] avg loss: 0.006657316339812186		[learning rate: 2.6499e-05]
	Learning Rate: 2.64994e-05
	LOSS [training: 0.007720523066969284 | validation: 0.009204575319860117]
	TIME [epoch: 8.1 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008475498008561975		[learning rate: 2.6451e-05]
		[batch 20/20] avg loss: 8.962662372683679e-05		[learning rate: 2.6403e-05]
	Learning Rate: 2.64033e-05
	LOSS [training: 0.004282562316144406 | validation: 0.0037291229789378814]
	TIME [epoch: 8.1 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004601462067016148		[learning rate: 2.6355e-05]
		[batch 20/20] avg loss: 0.00829065368122725		[learning rate: 2.6307e-05]
	Learning Rate: 2.63075e-05
	LOSS [training: 0.006446057874121699 | validation: 0.0037209825758662673]
	TIME [epoch: 8.1 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004082949856073376		[learning rate: 2.626e-05]
		[batch 20/20] avg loss: 0.007616392707707647		[learning rate: 2.6212e-05]
	Learning Rate: 2.6212e-05
	LOSS [training: 0.005849671281890512 | validation: 0.011604865781979376]
	TIME [epoch: 8.1 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027846689927805536		[learning rate: 2.6164e-05]
		[batch 20/20] avg loss: 0.0067476021973999325		[learning rate: 2.6117e-05]
	Learning Rate: 2.61169e-05
	LOSS [training: 0.004766135595090243 | validation: 0.014986270256773897]
	TIME [epoch: 8.12 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004316855254287977		[learning rate: 2.6069e-05]
		[batch 20/20] avg loss: 0.005940463377417787		[learning rate: 2.6022e-05]
	Learning Rate: 2.60221e-05
	LOSS [training: 0.005128659315852882 | validation: 0.006663200166963641]
	TIME [epoch: 8.09 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016309928589730662		[learning rate: 2.5975e-05]
		[batch 20/20] avg loss: 0.005750742481825665		[learning rate: 2.5928e-05]
	Learning Rate: 2.59277e-05
	LOSS [training: 0.003690867670399366 | validation: 0.0088422138578063]
	TIME [epoch: 8.1 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005725031165256228		[learning rate: 2.5881e-05]
		[batch 20/20] avg loss: 0.0007270993234118438		[learning rate: 2.5834e-05]
	Learning Rate: 2.58336e-05
	LOSS [training: 0.0032260652443340367 | validation: 0.005952539658692454]
	TIME [epoch: 8.1 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006395267523323307		[learning rate: 2.5787e-05]
		[batch 20/20] avg loss: 9.511952161523635e-05		[learning rate: 2.574e-05]
	Learning Rate: 2.57398e-05
	LOSS [training: 0.0032451935224692722 | validation: 0.0043926100440134525]
	TIME [epoch: 8.12 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007539544459999159		[learning rate: 2.5693e-05]
		[batch 20/20] avg loss: 0.0038109647466848203		[learning rate: 2.5646e-05]
	Learning Rate: 2.56464e-05
	LOSS [training: 0.00567525460334199 | validation: 0.009044509920431114]
	TIME [epoch: 8.1 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005101774644469018		[learning rate: 2.56e-05]
		[batch 20/20] avg loss: 0.004469990777129129		[learning rate: 2.5553e-05]
	Learning Rate: 2.55533e-05
	LOSS [training: 0.004785882710799073 | validation: 0.014662814204768525]
	TIME [epoch: 8.1 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006116249229838375		[learning rate: 2.5507e-05]
		[batch 20/20] avg loss: 0.005317800543779746		[learning rate: 2.5461e-05]
	Learning Rate: 2.54606e-05
	LOSS [training: 0.005717024886809061 | validation: 0.01860210815366652]
	TIME [epoch: 8.1 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00834556757653949		[learning rate: 2.5414e-05]
		[batch 20/20] avg loss: 0.006731638886856321		[learning rate: 2.5368e-05]
	Learning Rate: 2.53682e-05
	LOSS [training: 0.007538603231697906 | validation: 0.011644111912383072]
	TIME [epoch: 8.1 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006066449422723743		[learning rate: 2.5322e-05]
		[batch 20/20] avg loss: 8.132291510285607e-05		[learning rate: 2.5276e-05]
	Learning Rate: 2.52761e-05
	LOSS [training: 0.0030738861689133005 | validation: 0.012066930465169993]
	TIME [epoch: 8.12 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008166499357095257		[learning rate: 2.523e-05]
		[batch 20/20] avg loss: 0.011522372404608407		[learning rate: 2.5184e-05]
	Learning Rate: 2.51844e-05
	LOSS [training: 0.009844435880851831 | validation: 0.020846543741157297]
	TIME [epoch: 8.09 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010550563026427697		[learning rate: 2.5139e-05]
		[batch 20/20] avg loss: 0.006891895202904183		[learning rate: 2.5093e-05]
	Learning Rate: 2.5093e-05
	LOSS [training: 0.008721229114665939 | validation: 0.011352837689960833]
	TIME [epoch: 8.09 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003932502612355618		[learning rate: 2.5047e-05]
		[batch 20/20] avg loss: 0.005092797461073854		[learning rate: 2.5002e-05]
	Learning Rate: 2.50019e-05
	LOSS [training: 0.004512650036714737 | validation: 0.012026924735001904]
	TIME [epoch: 8.1 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005186607415342732		[learning rate: 2.4957e-05]
		[batch 20/20] avg loss: 0.002222010275362004		[learning rate: 2.4911e-05]
	Learning Rate: 2.49112e-05
	LOSS [training: 0.003704308845352367 | validation: 0.00692050767621488]
	TIME [epoch: 8.12 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00842629745821322		[learning rate: 2.4866e-05]
		[batch 20/20] avg loss: 0.0016336847838323214		[learning rate: 2.4821e-05]
	Learning Rate: 2.48208e-05
	LOSS [training: 0.005029991121022771 | validation: 0.013430544907788038]
	TIME [epoch: 8.09 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010141362916149949		[learning rate: 2.4776e-05]
		[batch 20/20] avg loss: -0.002067576294268294		[learning rate: 2.4731e-05]
	Learning Rate: 2.47307e-05
	LOSS [training: 0.0040368933109408275 | validation: 0.014744701472469299]
	TIME [epoch: 8.09 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004613857185109524		[learning rate: 2.4686e-05]
		[batch 20/20] avg loss: 0.003076726968510278		[learning rate: 2.4641e-05]
	Learning Rate: 2.4641e-05
	LOSS [training: 0.003845292076809901 | validation: 0.007084984388851381]
	TIME [epoch: 8.09 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006221042739202128		[learning rate: 2.4596e-05]
		[batch 20/20] avg loss: 0.009864607842666719		[learning rate: 2.4552e-05]
	Learning Rate: 2.45516e-05
	LOSS [training: 0.008042825290934426 | validation: 0.006791786600295772]
	TIME [epoch: 8.1 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003565182619989106		[learning rate: 2.4507e-05]
		[batch 20/20] avg loss: -0.0005076594468230104		[learning rate: 2.4462e-05]
	Learning Rate: 2.44625e-05
	LOSS [training: 0.0015287615865830478 | validation: 0.004249397275122055]
	TIME [epoch: 8.12 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008470560863375791		[learning rate: 2.4418e-05]
		[batch 20/20] avg loss: 0.0010651312629250508		[learning rate: 2.4374e-05]
	Learning Rate: 2.43737e-05
	LOSS [training: 0.004767846063150421 | validation: 0.009182908154455298]
	TIME [epoch: 8.09 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0056094334903198945		[learning rate: 2.4329e-05]
		[batch 20/20] avg loss: 0.004218789798945976		[learning rate: 2.4285e-05]
	Learning Rate: 2.42852e-05
	LOSS [training: 0.004914111644632934 | validation: 0.011572632528475834]
	TIME [epoch: 8.09 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007467834349525078		[learning rate: 2.4241e-05]
		[batch 20/20] avg loss: 0.005245395711971824		[learning rate: 2.4197e-05]
	Learning Rate: 2.41971e-05
	LOSS [training: 0.006356615030748451 | validation: 0.008744040710863323]
	TIME [epoch: 8.09 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022593811944704627		[learning rate: 2.4153e-05]
		[batch 20/20] avg loss: 0.00434322549236482		[learning rate: 2.4109e-05]
	Learning Rate: 2.41093e-05
	LOSS [training: 0.0033013033434176413 | validation: 0.01000523431249649]
	TIME [epoch: 8.11 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0046615602331178015		[learning rate: 2.4065e-05]
		[batch 20/20] avg loss: 0.006929088321595593		[learning rate: 2.4022e-05]
	Learning Rate: 2.40218e-05
	LOSS [training: 0.0057953242773566985 | validation: 0.004316937357854617]
	TIME [epoch: 8.09 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004226613858374984		[learning rate: 2.3978e-05]
		[batch 20/20] avg loss: 0.002014577808116208		[learning rate: 2.3935e-05]
	Learning Rate: 2.39346e-05
	LOSS [training: 0.0031205958332455957 | validation: 0.008732664097179415]
	TIME [epoch: 8.09 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008794539130822438		[learning rate: 2.3891e-05]
		[batch 20/20] avg loss: 0.0008813999661297983		[learning rate: 2.3848e-05]
	Learning Rate: 2.38477e-05
	LOSS [training: 0.004837969548476119 | validation: 0.011093873170104882]
	TIME [epoch: 8.09 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004706793351247391		[learning rate: 2.3804e-05]
		[batch 20/20] avg loss: 0.004067139745172485		[learning rate: 2.3761e-05]
	Learning Rate: 2.37612e-05
	LOSS [training: 0.004386966548209938 | validation: 0.006064503288611546]
	TIME [epoch: 8.1 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007481468724189863		[learning rate: 2.3718e-05]
		[batch 20/20] avg loss: -0.0004483192820760646		[learning rate: 2.3675e-05]
	Learning Rate: 2.3675e-05
	LOSS [training: 0.003516574721056899 | validation: 0.005648003079647714]
	TIME [epoch: 8.11 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017123444636073632		[learning rate: 2.3632e-05]
		[batch 20/20] avg loss: 0.004648359348602638		[learning rate: 2.3589e-05]
	Learning Rate: 2.35891e-05
	LOSS [training: 0.0031803519061050006 | validation: 0.0030949315324278748]
	TIME [epoch: 8.09 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015000909804277819		[learning rate: 2.3546e-05]
		[batch 20/20] avg loss: 0.0035508781557887234		[learning rate: 2.3503e-05]
	Learning Rate: 2.35034e-05
	LOSS [training: 0.002525484568108252 | validation: 0.008092134448632302]
	TIME [epoch: 8.09 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0028144561225913966		[learning rate: 2.3461e-05]
		[batch 20/20] avg loss: 0.009468416716736455		[learning rate: 2.3418e-05]
	Learning Rate: 2.34182e-05
	LOSS [training: 0.003326980297072529 | validation: 0.0036135935654033687]
	TIME [epoch: 8.09 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00229524996921249		[learning rate: 2.3376e-05]
		[batch 20/20] avg loss: 0.011610330338044029		[learning rate: 2.3333e-05]
	Learning Rate: 2.33332e-05
	LOSS [training: 0.006952790153628261 | validation: 0.012788181089272835]
	TIME [epoch: 8.11 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007210219964446261		[learning rate: 2.3291e-05]
		[batch 20/20] avg loss: 0.006941067508861083		[learning rate: 2.3248e-05]
	Learning Rate: 2.32485e-05
	LOSS [training: 0.007075643736653673 | validation: 0.012199106473816255]
	TIME [epoch: 8.09 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.004240684169796432		[learning rate: 2.3206e-05]
		[batch 20/20] avg loss: 0.008854869454255485		[learning rate: 2.3164e-05]
	Learning Rate: 2.31641e-05
	LOSS [training: 0.002307092642229526 | validation: 0.014120553538311214]
	TIME [epoch: 8.1 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00573322934002371		[learning rate: 2.3122e-05]
		[batch 20/20] avg loss: 0.0069503458315520435		[learning rate: 2.308e-05]
	Learning Rate: 2.30801e-05
	LOSS [training: 0.006341787585787876 | validation: 0.011119289018541562]
	TIME [epoch: 8.09 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007611922485499606		[learning rate: 2.3038e-05]
		[batch 20/20] avg loss: 0.0020114621247272193		[learning rate: 2.2996e-05]
	Learning Rate: 2.29963e-05
	LOSS [training: 0.004811692305113412 | validation: 0.011947096500640249]
	TIME [epoch: 8.11 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003495495808859636		[learning rate: 2.2955e-05]
		[batch 20/20] avg loss: 0.005892841286184556		[learning rate: 2.2913e-05]
	Learning Rate: 2.29128e-05
	LOSS [training: 0.004694168547522096 | validation: 0.006273850028388936]
	TIME [epoch: 8.1 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011724899280278452		[learning rate: 2.2871e-05]
		[batch 20/20] avg loss: 0.004679209474535619		[learning rate: 2.283e-05]
	Learning Rate: 2.28297e-05
	LOSS [training: 0.008202054377407036 | validation: 0.007541030351494813]
	TIME [epoch: 8.08 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005456725168080748		[learning rate: 2.2788e-05]
		[batch 20/20] avg loss: 0.005307300922819613		[learning rate: 2.2747e-05]
	Learning Rate: 2.27468e-05
	LOSS [training: 0.002926486719813844 | validation: 0.00789750746356072]
	TIME [epoch: 8.08 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004866766547637635		[learning rate: 2.2706e-05]
		[batch 20/20] avg loss: 0.005750044734646573		[learning rate: 2.2664e-05]
	Learning Rate: 2.26643e-05
	LOSS [training: 0.005308405641142104 | validation: 0.008306542479122209]
	TIME [epoch: 8.09 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009876587174841305		[learning rate: 2.2623e-05]
		[batch 20/20] avg loss: 0.003971837541853251		[learning rate: 2.2582e-05]
	Learning Rate: 2.2582e-05
	LOSS [training: 0.0024797481296686907 | validation: 0.007940968496880271]
	TIME [epoch: 8.11 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008209625202464078		[learning rate: 2.2541e-05]
		[batch 20/20] avg loss: 0.0017749559820760482		[learning rate: 2.25e-05]
	Learning Rate: 2.25001e-05
	LOSS [training: 0.004992290592270064 | validation: 0.008553456276014691]
	TIME [epoch: 8.09 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003506493805764426		[learning rate: 2.2459e-05]
		[batch 20/20] avg loss: 0.007225049828266719		[learning rate: 2.2418e-05]
	Learning Rate: 2.24184e-05
	LOSS [training: 0.005365771817015572 | validation: 0.006805643596270531]
	TIME [epoch: 8.08 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006941369153596243		[learning rate: 2.2378e-05]
		[batch 20/20] avg loss: 0.006243666010225029		[learning rate: 2.2337e-05]
	Learning Rate: 2.23371e-05
	LOSS [training: 0.006592517581910635 | validation: 0.008859071111024165]
	TIME [epoch: 8.08 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00820370458321165		[learning rate: 2.2297e-05]
		[batch 20/20] avg loss: 0.002912922084837266		[learning rate: 2.2256e-05]
	Learning Rate: 2.2256e-05
	LOSS [training: 0.005558313334024459 | validation: 0.009679187771338135]
	TIME [epoch: 8.1 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033754861201380605		[learning rate: 2.2216e-05]
		[batch 20/20] avg loss: 0.0036905454968621946		[learning rate: 2.2175e-05]
	Learning Rate: 2.21753e-05
	LOSS [training: 0.0035330158085001276 | validation: 0.008802855337005283]
	TIME [epoch: 8.09 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00520763449416908		[learning rate: 2.2135e-05]
		[batch 20/20] avg loss: 0.002278932575411166		[learning rate: 2.2095e-05]
	Learning Rate: 2.20948e-05
	LOSS [training: 0.003743283534790124 | validation: 0.0029913059904470413]
	TIME [epoch: 8.1 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026828823037066936		[learning rate: 2.2055e-05]
		[batch 20/20] avg loss: 0.005689200986538355		[learning rate: 2.2015e-05]
	Learning Rate: 2.20146e-05
	LOSS [training: 0.0041860416451225246 | validation: 0.010619752125162384]
	TIME [epoch: 8.09 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00010967606206531179		[learning rate: 2.1975e-05]
		[batch 20/20] avg loss: 0.008891056036188198		[learning rate: 2.1935e-05]
	Learning Rate: 2.19347e-05
	LOSS [training: 0.004500366049126755 | validation: 0.0151115286289546]
	TIME [epoch: 8.1 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005794100384017389		[learning rate: 2.1895e-05]
		[batch 20/20] avg loss: 0.005150004319483401		[learning rate: 2.1855e-05]
	Learning Rate: 2.18551e-05
	LOSS [training: 0.005472052351750395 | validation: 0.011580210217135808]
	TIME [epoch: 8.11 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005984368410213482		[learning rate: 2.1815e-05]
		[batch 20/20] avg loss: 0.004401149845018229		[learning rate: 2.1776e-05]
	Learning Rate: 2.17758e-05
	LOSS [training: 0.0024997933430197885 | validation: 0.004650380349397682]
	TIME [epoch: 8.1 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009446142633151168		[learning rate: 2.1736e-05]
		[batch 20/20] avg loss: 0.00455530865500068		[learning rate: 2.1697e-05]
	Learning Rate: 2.16968e-05
	LOSS [training: 0.007000725644075924 | validation: 0.0025548287810110122]
	TIME [epoch: 8.09 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006676464725322159		[learning rate: 2.1657e-05]
		[batch 20/20] avg loss: 0.007968329267333794		[learning rate: 2.1618e-05]
	Learning Rate: 2.1618e-05
	LOSS [training: 0.007322396996327973 | validation: 0.010213282613775448]
	TIME [epoch: 8.09 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007324057247558824		[learning rate: 2.1579e-05]
		[batch 20/20] avg loss: 0.001241586574879099		[learning rate: 2.154e-05]
	Learning Rate: 2.15396e-05
	LOSS [training: 0.004282821911218961 | validation: 0.008510790702854413]
	TIME [epoch: 8.11 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0012425577402904483		[learning rate: 2.15e-05]
		[batch 20/20] avg loss: 0.006687832195108568		[learning rate: 2.1461e-05]
	Learning Rate: 2.14614e-05
	LOSS [training: 0.0027226372274090594 | validation: 0.0069345244423608034]
	TIME [epoch: 8.1 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022790102952779524		[learning rate: 2.1422e-05]
		[batch 20/20] avg loss: 0.0047586776397849345		[learning rate: 2.1384e-05]
	Learning Rate: 2.13835e-05
	LOSS [training: 0.003518843967531443 | validation: 0.00863055904071665]
	TIME [epoch: 8.08 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0053995289998154655		[learning rate: 2.1345e-05]
		[batch 20/20] avg loss: 0.004553198167261728		[learning rate: 2.1306e-05]
	Learning Rate: 2.13059e-05
	LOSS [training: 0.004976363583538597 | validation: 0.008275013995042628]
	TIME [epoch: 8.09 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004926578744965022		[learning rate: 2.1267e-05]
		[batch 20/20] avg loss: 0.003320318541844136		[learning rate: 2.1229e-05]
	Learning Rate: 2.12286e-05
	LOSS [training: 0.00412344864340458 | validation: 0.00471668029266511]
	TIME [epoch: 8.09 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005062340109033092		[learning rate: 2.119e-05]
		[batch 20/20] avg loss: 0.004989979029127319		[learning rate: 2.1152e-05]
	Learning Rate: 2.11515e-05
	LOSS [training: 0.005026159569080205 | validation: 0.005006133792406529]
	TIME [epoch: 8.12 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002635821771303297		[learning rate: 2.1113e-05]
		[batch 20/20] avg loss: 0.00518451795624822		[learning rate: 2.1075e-05]
	Learning Rate: 2.10748e-05
	LOSS [training: 0.003910169863775758 | validation: 0.014555188532479162]
	TIME [epoch: 8.09 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005931137589104921		[learning rate: 2.1037e-05]
		[batch 20/20] avg loss: 0.004908850939857558		[learning rate: 2.0998e-05]
	Learning Rate: 2.09983e-05
	LOSS [training: 0.00541999426448124 | validation: 0.013152057468475028]
	TIME [epoch: 8.09 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0068315463324989095		[learning rate: 2.096e-05]
		[batch 20/20] avg loss: 0.00314725433773179		[learning rate: 2.0922e-05]
	Learning Rate: 2.09221e-05
	LOSS [training: 0.00498940033511535 | validation: 0.007677801820065515]
	TIME [epoch: 8.09 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0009263416526831873		[learning rate: 2.0884e-05]
		[batch 20/20] avg loss: 0.008132133919723257		[learning rate: 2.0846e-05]
	Learning Rate: 2.08462e-05
	LOSS [training: 0.0036028961335200353 | validation: 0.006850141294204273]
	TIME [epoch: 8.11 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005798360653582402		[learning rate: 2.0808e-05]
		[batch 20/20] avg loss: 0.004956830799210067		[learning rate: 2.0771e-05]
	Learning Rate: 2.07705e-05
	LOSS [training: 0.0053775957263962355 | validation: 0.009066774170881886]
	TIME [epoch: 8.1 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007984289669550712		[learning rate: 2.0733e-05]
		[batch 20/20] avg loss: 0.002419821472449541		[learning rate: 2.0695e-05]
	Learning Rate: 2.06951e-05
	LOSS [training: 0.005202055571000127 | validation: 0.011720354012535177]
	TIME [epoch: 8.09 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009806677012168401		[learning rate: 2.0658e-05]
		[batch 20/20] avg loss: -0.0019210889320313324		[learning rate: 2.062e-05]
	Learning Rate: 2.062e-05
	LOSS [training: 0.003942794040068535 | validation: 0.010265701350543551]
	TIME [epoch: 8.1 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009071981366472817		[learning rate: 2.0583e-05]
		[batch 20/20] avg loss: 0.00470335568235045		[learning rate: 2.0545e-05]
	Learning Rate: 2.05452e-05
	LOSS [training: 0.006887668524411634 | validation: 0.010009440997928954]
	TIME [epoch: 8.1 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006776072353535718		[learning rate: 2.0508e-05]
		[batch 20/20] avg loss: 0.0018908248108999493		[learning rate: 2.0471e-05]
	Learning Rate: 2.04706e-05
	LOSS [training: 0.004333448582217835 | validation: 0.009311222702934059]
	TIME [epoch: 8.11 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008094074646626297		[learning rate: 2.0433e-05]
		[batch 20/20] avg loss: 0.005412174488893934		[learning rate: 2.0396e-05]
	Learning Rate: 2.03964e-05
	LOSS [training: 0.006753124567760116 | validation: 0.01469150529307762]
	TIME [epoch: 8.09 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022326929228476565		[learning rate: 2.0359e-05]
		[batch 20/20] avg loss: 0.002488607054442537		[learning rate: 2.0322e-05]
	Learning Rate: 2.03223e-05
	LOSS [training: 0.0023606499886450975 | validation: 0.0074487923640054665]
	TIME [epoch: 8.09 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002645643446292434		[learning rate: 2.0285e-05]
		[batch 20/20] avg loss: 0.00228040380259423		[learning rate: 2.0249e-05]
	Learning Rate: 2.02486e-05
	LOSS [training: 0.002463023624443332 | validation: 0.007557335995048713]
	TIME [epoch: 8.09 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022829565286302234		[learning rate: 2.0212e-05]
		[batch 20/20] avg loss: 0.0010639339538508887		[learning rate: 2.0175e-05]
	Learning Rate: 2.01751e-05
	LOSS [training: 0.0016734452412405566 | validation: 0.012190981774405602]
	TIME [epoch: 8.1 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005507408160562544		[learning rate: 2.0138e-05]
		[batch 20/20] avg loss: -0.0012814535334009895		[learning rate: 2.0102e-05]
	Learning Rate: 2.01019e-05
	LOSS [training: 0.0021129773135807775 | validation: 0.002442204189385791]
	TIME [epoch: 8.09 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: -5.6661179323290455e-05		[learning rate: 2.0065e-05]
		[batch 20/20] avg loss: 0.007366307777673629		[learning rate: 2.0029e-05]
	Learning Rate: 2.00289e-05
	LOSS [training: 0.0036548232991751696 | validation: 0.008329928437286001]
	TIME [epoch: 8.09 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008894665319349784		[learning rate: 1.9993e-05]
		[batch 20/20] avg loss: 0.003212195203564247		[learning rate: 1.9956e-05]
	Learning Rate: 1.99563e-05
	LOSS [training: 0.006053430261457016 | validation: 0.007564363659767167]
	TIME [epoch: 8.09 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0062492339238775325		[learning rate: 1.992e-05]
		[batch 20/20] avg loss: 0.0046365568035333675		[learning rate: 1.9884e-05]
	Learning Rate: 1.98838e-05
	LOSS [training: 0.00544289536370545 | validation: 0.011121882116079056]
	TIME [epoch: 8.09 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032231984519117836		[learning rate: 1.9848e-05]
		[batch 20/20] avg loss: 0.009974738127127653		[learning rate: 1.9812e-05]
	Learning Rate: 1.98117e-05
	LOSS [training: 0.006598968289519719 | validation: 0.0004659928404991088]
	TIME [epoch: 8.11 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015331522990898352		[learning rate: 1.9776e-05]
		[batch 20/20] avg loss: 0.0026914618678034125		[learning rate: 1.974e-05]
	Learning Rate: 1.97398e-05
	LOSS [training: 0.002112307083446624 | validation: 0.008521626061783465]
	TIME [epoch: 8.09 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003131948279081305		[learning rate: 1.9704e-05]
		[batch 20/20] avg loss: 0.008056660819802743		[learning rate: 1.9668e-05]
	Learning Rate: 1.96681e-05
	LOSS [training: 0.005594304549442024 | validation: 0.013947736429062323]
	TIME [epoch: 8.09 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024097134971934908		[learning rate: 1.9632e-05]
		[batch 20/20] avg loss: 0.013602560270914606		[learning rate: 1.9597e-05]
	Learning Rate: 1.95968e-05
	LOSS [training: 0.008006136884054047 | validation: 0.012197557313255374]
	TIME [epoch: 8.09 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014608215609225735		[learning rate: 1.9561e-05]
		[batch 20/20] avg loss: 0.008460805159348562		[learning rate: 1.9526e-05]
	Learning Rate: 1.95256e-05
	LOSS [training: 0.004960813360135569 | validation: 0.009619955453600233]
	TIME [epoch: 8.11 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006665793612664446		[learning rate: 1.949e-05]
		[batch 20/20] avg loss: 0.001759438358257076		[learning rate: 1.9455e-05]
	Learning Rate: 1.94548e-05
	LOSS [training: 0.00421261598546076 | validation: 0.004831924291127631]
	TIME [epoch: 8.1 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004870215934212541		[learning rate: 1.9419e-05]
		[batch 20/20] avg loss: 0.005259689006318553		[learning rate: 1.9384e-05]
	Learning Rate: 1.93842e-05
	LOSS [training: 0.005064952470265547 | validation: 0.014603644472408632]
	TIME [epoch: 8.09 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006152320067187899		[learning rate: 1.9349e-05]
		[batch 20/20] avg loss: 0.0011930642463652593		[learning rate: 1.9314e-05]
	Learning Rate: 1.93138e-05
	LOSS [training: 0.0036726921567765795 | validation: 0.01566533080736173]
	TIME [epoch: 8.09 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0054530917776487225		[learning rate: 1.9279e-05]
		[batch 20/20] avg loss: 0.007728235600370432		[learning rate: 1.9244e-05]
	Learning Rate: 1.92437e-05
	LOSS [training: 0.006590663689009578 | validation: 0.009859376232302184]
	TIME [epoch: 8.09 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004363173165901887		[learning rate: 1.9209e-05]
		[batch 20/20] avg loss: 0.010485979911852713		[learning rate: 1.9174e-05]
	Learning Rate: 1.91739e-05
	LOSS [training: 0.007424576538877301 | validation: 0.009170046139762172]
	TIME [epoch: 8.11 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031201416981759575		[learning rate: 1.9139e-05]
		[batch 20/20] avg loss: 0.00521108659980548		[learning rate: 1.9104e-05]
	Learning Rate: 1.91043e-05
	LOSS [training: 0.004165614148990719 | validation: 0.013321119295125292]
	TIME [epoch: 8.09 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026955200859785653		[learning rate: 1.907e-05]
		[batch 20/20] avg loss: 0.0026509554704330844		[learning rate: 1.9035e-05]
	Learning Rate: 1.9035e-05
	LOSS [training: 0.0026732377782058255 | validation: 0.008972694318062097]
	TIME [epoch: 8.09 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004710413951773707		[learning rate: 1.9e-05]
		[batch 20/20] avg loss: 0.002897626910837887		[learning rate: 1.8966e-05]
	Learning Rate: 1.89659e-05
	LOSS [training: 0.003804020431305797 | validation: 0.010455034737826864]
	TIME [epoch: 8.08 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014973085371083944		[learning rate: 1.8931e-05]
		[batch 20/20] avg loss: 0.007151968934041133		[learning rate: 1.8897e-05]
	Learning Rate: 1.88971e-05
	LOSS [training: 0.004324638735574764 | validation: 0.009518822383182461]
	TIME [epoch: 8.11 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031591966575091413		[learning rate: 1.8863e-05]
		[batch 20/20] avg loss: 0.005113509823188967		[learning rate: 1.8829e-05]
	Learning Rate: 1.88285e-05
	LOSS [training: 0.004136353240349054 | validation: 0.011574445828509729]
	TIME [epoch: 8.09 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00012549236710323615		[learning rate: 1.8794e-05]
		[batch 20/20] avg loss: 0.014630075357651356		[learning rate: 1.876e-05]
	Learning Rate: 1.87602e-05
	LOSS [training: 0.007252291495274061 | validation: 0.010556313099919005]
	TIME [epoch: 8.09 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003998105875278449		[learning rate: 1.8726e-05]
		[batch 20/20] avg loss: 0.0052698696721487585		[learning rate: 1.8692e-05]
	Learning Rate: 1.86921e-05
	LOSS [training: 0.004633987773713603 | validation: 0.0152737945585866]
	TIME [epoch: 8.09 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006830337205905553		[learning rate: 1.8658e-05]
		[batch 20/20] avg loss: 0.0070680050175349115		[learning rate: 1.8624e-05]
	Learning Rate: 1.86243e-05
	LOSS [training: 0.0069491711117202325 | validation: 0.01194954216601221]
	TIME [epoch: 8.09 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01080466382271737		[learning rate: 1.859e-05]
		[batch 20/20] avg loss: 0.002505585057706097		[learning rate: 1.8557e-05]
	Learning Rate: 1.85567e-05
	LOSS [training: 0.006655124440211735 | validation: 0.012249402055137203]
	TIME [epoch: 8.11 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00835167757706803		[learning rate: 1.8523e-05]
		[batch 20/20] avg loss: 0.008625896917009757		[learning rate: 1.8489e-05]
	Learning Rate: 1.84893e-05
	LOSS [training: 0.008488787247038896 | validation: 0.011245620960458147]
	TIME [epoch: 8.09 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033053852427201756		[learning rate: 1.8456e-05]
		[batch 20/20] avg loss: 0.0056830896152987		[learning rate: 1.8422e-05]
	Learning Rate: 1.84222e-05
	LOSS [training: 0.004494237429009439 | validation: 0.010341147544326336]
	TIME [epoch: 8.09 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00297496228892563		[learning rate: 1.8389e-05]
		[batch 20/20] avg loss: 0.003954244016792943		[learning rate: 1.8355e-05]
	Learning Rate: 1.83554e-05
	LOSS [training: 0.0034646031528592857 | validation: 0.010120110326334829]
	TIME [epoch: 8.08 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004085438759689485		[learning rate: 1.8322e-05]
		[batch 20/20] avg loss: 0.004633703698999614		[learning rate: 1.8289e-05]
	Learning Rate: 1.82888e-05
	LOSS [training: 0.00435957122934455 | validation: 0.008828353862964048]
	TIME [epoch: 8.11 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0054869038829302875		[learning rate: 1.8256e-05]
		[batch 20/20] avg loss: 0.0066504254129620185		[learning rate: 1.8222e-05]
	Learning Rate: 1.82224e-05
	LOSS [training: 0.006068664647946154 | validation: 0.010231079665738316]
	TIME [epoch: 8.09 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0053439075525456015		[learning rate: 1.8189e-05]
		[batch 20/20] avg loss: 0.005109500425644209		[learning rate: 1.8156e-05]
	Learning Rate: 1.81563e-05
	LOSS [training: 0.005226703989094905 | validation: 0.009296860425987302]
	TIME [epoch: 8.09 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004442043943906349		[learning rate: 1.8123e-05]
		[batch 20/20] avg loss: 0.0048481676048136545		[learning rate: 1.809e-05]
	Learning Rate: 1.80904e-05
	LOSS [training: 0.004645105774360001 | validation: 0.007805880742862163]
	TIME [epoch: 8.09 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0018314868387714705		[learning rate: 1.8058e-05]
		[batch 20/20] avg loss: 0.0037916875206073794		[learning rate: 1.8025e-05]
	Learning Rate: 1.80247e-05
	LOSS [training: 0.0028115871796894247 | validation: 0.013967603020598424]
	TIME [epoch: 8.09 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0040959553845969655		[learning rate: 1.7992e-05]
		[batch 20/20] avg loss: 0.004369790441791556		[learning rate: 1.7959e-05]
	Learning Rate: 1.79593e-05
	LOSS [training: 0.00423287291319426 | validation: 0.0034770993841141424]
	TIME [epoch: 8.1 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004154227159604193		[learning rate: 1.7927e-05]
		[batch 20/20] avg loss: 0.006902520502650444		[learning rate: 1.7894e-05]
	Learning Rate: 1.78941e-05
	LOSS [training: 0.0055283738311273185 | validation: 0.010251688801915364]
	TIME [epoch: 8.09 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015414029456196792		[learning rate: 1.7862e-05]
		[batch 20/20] avg loss: 0.0057984657287055985		[learning rate: 1.7829e-05]
	Learning Rate: 1.78292e-05
	LOSS [training: 0.0036699343371626387 | validation: 0.009163463947802406]
	TIME [epoch: 8.1 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038300162658198482		[learning rate: 1.7797e-05]
		[batch 20/20] avg loss: 0.0043504123923600035		[learning rate: 1.7764e-05]
	Learning Rate: 1.77645e-05
	LOSS [training: 0.0040902143290899255 | validation: 0.003521974047496327]
	TIME [epoch: 8.09 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004886369543269126		[learning rate: 1.7732e-05]
		[batch 20/20] avg loss: 0.005564924553126313		[learning rate: 1.77e-05]
	Learning Rate: 1.77e-05
	LOSS [training: 0.005225647048197718 | validation: 0.009805998177978992]
	TIME [epoch: 8.11 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005814034384786178		[learning rate: 1.7668e-05]
		[batch 20/20] avg loss: 0.004798287564039597		[learning rate: 1.7636e-05]
	Learning Rate: 1.76358e-05
	LOSS [training: 0.005306160974412887 | validation: 0.009027872689841336]
	TIME [epoch: 8.09 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015876702010330305		[learning rate: 1.7604e-05]
		[batch 20/20] avg loss: 0.007766715352592497		[learning rate: 1.7572e-05]
	Learning Rate: 1.75718e-05
	LOSS [training: 0.0046771927768127644 | validation: 0.007058372083236583]
	TIME [epoch: 8.1 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00419090115883391		[learning rate: 1.754e-05]
		[batch 20/20] avg loss: 0.006581231679651946		[learning rate: 1.7508e-05]
	Learning Rate: 1.7508e-05
	LOSS [training: 0.005386066419242929 | validation: 0.010675086939724892]
	TIME [epoch: 8.09 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0060113685779522655		[learning rate: 1.7476e-05]
		[batch 20/20] avg loss: 0.003998163207598271		[learning rate: 1.7444e-05]
	Learning Rate: 1.74445e-05
	LOSS [training: 0.005004765892775267 | validation: 0.0093914662329704]
	TIME [epoch: 8.11 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004299892113070436		[learning rate: 1.7413e-05]
		[batch 20/20] avg loss: 0.0004672526333179216		[learning rate: 1.7381e-05]
	Learning Rate: 1.73812e-05
	LOSS [training: 0.0023835723731941787 | validation: 0.012689125533269296]
	TIME [epoch: 8.11 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00835715695004445		[learning rate: 1.735e-05]
		[batch 20/20] avg loss: 0.003070397784630741		[learning rate: 1.7318e-05]
	Learning Rate: 1.73181e-05
	LOSS [training: 0.005713777367337596 | validation: 0.008327391010644314]
	TIME [epoch: 8.09 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002420785278491798		[learning rate: 1.7287e-05]
		[batch 20/20] avg loss: 0.0028742379592201035		[learning rate: 1.7255e-05]
	Learning Rate: 1.72552e-05
	LOSS [training: 0.0026475116188559504 | validation: 0.007230091181890417]
	TIME [epoch: 8.09 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005341007003751064		[learning rate: 1.7224e-05]
		[batch 20/20] avg loss: 0.0014501055951179254		[learning rate: 1.7193e-05]
	Learning Rate: 1.71926e-05
	LOSS [training: 0.0033955562994344942 | validation: 0.00903162666206591]
	TIME [epoch: 8.09 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009124159101902198		[learning rate: 1.7161e-05]
		[batch 20/20] avg loss: 0.0058026007068345915		[learning rate: 1.713e-05]
	Learning Rate: 1.71302e-05
	LOSS [training: 0.007463379904368396 | validation: 0.013913841918447276]
	TIME [epoch: 8.12 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006348197957730886		[learning rate: 1.7099e-05]
		[batch 20/20] avg loss: 0.00473683657529834		[learning rate: 1.7068e-05]
	Learning Rate: 1.70681e-05
	LOSS [training: 0.005542517266514613 | validation: 0.013465401869744117]
	TIME [epoch: 8.09 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031601731348614714		[learning rate: 1.7037e-05]
		[batch 20/20] avg loss: 0.005709725256694194		[learning rate: 1.7006e-05]
	Learning Rate: 1.70061e-05
	LOSS [training: 0.004434949195777834 | validation: 0.008352367053767951]
	TIME [epoch: 8.1 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: -8.033614699769e-05		[learning rate: 1.6975e-05]
		[batch 20/20] avg loss: 0.006248667225844004		[learning rate: 1.6944e-05]
	Learning Rate: 1.69444e-05
	LOSS [training: 0.003084165539423157 | validation: 0.010566568328773035]
	TIME [epoch: 8.09 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0020247224688888696		[learning rate: 1.6914e-05]
		[batch 20/20] avg loss: 0.00626710232850132		[learning rate: 1.6883e-05]
	Learning Rate: 1.68829e-05
	LOSS [training: 0.002121189929806225 | validation: 0.008601589037116093]
	TIME [epoch: 8.11 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007441007439370361		[learning rate: 1.6852e-05]
		[batch 20/20] avg loss: 0.0010992152680939684		[learning rate: 1.6822e-05]
	Learning Rate: 1.68216e-05
	LOSS [training: 0.004270111353732165 | validation: 0.01076658832974536]
	TIME [epoch: 8.1 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00717723506291598		[learning rate: 1.6791e-05]
		[batch 20/20] avg loss: 0.005747414268901531		[learning rate: 1.6761e-05]
	Learning Rate: 1.67606e-05
	LOSS [training: 0.006462324665908755 | validation: 0.011136951299992629]
	TIME [epoch: 8.08 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004269948183599968		[learning rate: 1.673e-05]
		[batch 20/20] avg loss: 0.005324534987567016		[learning rate: 1.67e-05]
	Learning Rate: 1.66998e-05
	LOSS [training: 0.004797241585583492 | validation: 0.009587381990400181]
	TIME [epoch: 8.09 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007124686851494059		[learning rate: 1.6669e-05]
		[batch 20/20] avg loss: 0.006136583508304304		[learning rate: 1.6639e-05]
	Learning Rate: 1.66392e-05
	LOSS [training: 0.006630635179899182 | validation: 0.01343756837449571]
	TIME [epoch: 8.1 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007996906186582794		[learning rate: 1.6609e-05]
		[batch 20/20] avg loss: 0.004444658369256367		[learning rate: 1.6579e-05]
	Learning Rate: 1.65788e-05
	LOSS [training: 0.006220782277919581 | validation: 0.006506204607630945]
	TIME [epoch: 8.11 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00021262951280364583		[learning rate: 1.6549e-05]
		[batch 20/20] avg loss: -0.0020681296612443047		[learning rate: 1.6519e-05]
	Learning Rate: 1.65186e-05
	LOSS [training: -0.0009277500742203295 | validation: 0.011947267621210417]
	TIME [epoch: 8.09 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033523424147479997		[learning rate: 1.6489e-05]
		[batch 20/20] avg loss: 0.004751649756101621		[learning rate: 1.6459e-05]
	Learning Rate: 1.64587e-05
	LOSS [training: 0.00405199608542481 | validation: 0.01032729717839772]
	TIME [epoch: 8.09 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: -6.061669737886621e-05		[learning rate: 1.6429e-05]
		[batch 20/20] avg loss: 0.003141866046829654		[learning rate: 1.6399e-05]
	Learning Rate: 1.63989e-05
	LOSS [training: 0.001540624674725394 | validation: 0.009271787091514657]
	TIME [epoch: 8.08 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002538744777969099		[learning rate: 1.6369e-05]
		[batch 20/20] avg loss: 0.005905929757239496		[learning rate: 1.6339e-05]
	Learning Rate: 1.63394e-05
	LOSS [training: 0.004222337267604298 | validation: 0.00977916041196246]
	TIME [epoch: 8.11 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005520560895679639		[learning rate: 1.631e-05]
		[batch 20/20] avg loss: 0.001361811703744499		[learning rate: 1.628e-05]
	Learning Rate: 1.62801e-05
	LOSS [training: 0.003441186299712069 | validation: 0.005223851538342912]
	TIME [epoch: 8.11 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002405796166325803		[learning rate: 1.6251e-05]
		[batch 20/20] avg loss: 0.006296458027421395		[learning rate: 1.6221e-05]
	Learning Rate: 1.62211e-05
	LOSS [training: 0.004351127096873599 | validation: 0.004790692648048486]
	TIME [epoch: 8.1 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012868260076899511		[learning rate: 1.6192e-05]
		[batch 20/20] avg loss: 0.003406571552351402		[learning rate: 1.6162e-05]
	Learning Rate: 1.61622e-05
	LOSS [training: 0.002346698780020677 | validation: 0.008976658732440985]
	TIME [epoch: 8.09 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006885597302780995		[learning rate: 1.6133e-05]
		[batch 20/20] avg loss: 0.0013755807209181801		[learning rate: 1.6104e-05]
	Learning Rate: 1.61035e-05
	LOSS [training: 0.004130589011849587 | validation: 0.01084127633786597]
	TIME [epoch: 8.09 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00578519045139		[learning rate: 1.6074e-05]
		[batch 20/20] avg loss: 0.0054876760560512005		[learning rate: 1.6045e-05]
	Learning Rate: 1.60451e-05
	LOSS [training: 0.005636433253720599 | validation: 0.013360026606706387]
	TIME [epoch: 8.12 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010879248545315256		[learning rate: 1.6016e-05]
		[batch 20/20] avg loss: 0.0023626707132042095		[learning rate: 1.5987e-05]
	Learning Rate: 1.59869e-05
	LOSS [training: 0.006620959629259733 | validation: 0.01126466273789408]
	TIME [epoch: 8.09 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034946788046571623		[learning rate: 1.5958e-05]
		[batch 20/20] avg loss: 0.0063065849027130454		[learning rate: 1.5929e-05]
	Learning Rate: 1.59288e-05
	LOSS [training: 0.004900631853685103 | validation: 0.0072510507910950334]
	TIME [epoch: 8.09 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032492701480661646		[learning rate: 1.59e-05]
		[batch 20/20] avg loss: 0.005753563639701516		[learning rate: 1.5871e-05]
	Learning Rate: 1.5871e-05
	LOSS [training: 0.0045014168938838405 | validation: 0.01256373849996056]
	TIME [epoch: 8.09 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009035989676039449		[learning rate: 1.5842e-05]
		[batch 20/20] avg loss: 0.003795850717839184		[learning rate: 1.5813e-05]
	Learning Rate: 1.58134e-05
	LOSS [training: 0.0023497248427215645 | validation: 0.01362853067549821]
	TIME [epoch: 8.11 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006824532124557696		[learning rate: 1.5785e-05]
		[batch 20/20] avg loss: 0.009424686516343836		[learning rate: 1.5756e-05]
	Learning Rate: 1.57561e-05
	LOSS [training: 0.008124609320450765 | validation: 0.013223737945085908]
	TIME [epoch: 8.1 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007923957520794984		[learning rate: 1.5727e-05]
		[batch 20/20] avg loss: 0.001976132156516413		[learning rate: 1.5699e-05]
	Learning Rate: 1.56989e-05
	LOSS [training: 0.004950044838655699 | validation: 0.007142789045224288]
	TIME [epoch: 8.09 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004070959822519401		[learning rate: 1.567e-05]
		[batch 20/20] avg loss: 0.003702427272416092		[learning rate: 1.5642e-05]
	Learning Rate: 1.56419e-05
	LOSS [training: 0.003886693547467746 | validation: 0.008110830993791177]
	TIME [epoch: 8.08 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0008079694783466472		[learning rate: 1.5613e-05]
		[batch 20/20] avg loss: 0.005234935881827031		[learning rate: 1.5585e-05]
	Learning Rate: 1.55851e-05
	LOSS [training: 0.0030214526800868393 | validation: 0.011441317415065062]
	TIME [epoch: 8.1 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005954840420243537		[learning rate: 1.5557e-05]
		[batch 20/20] avg loss: -0.0004035914690949687		[learning rate: 1.5529e-05]
	Learning Rate: 1.55286e-05
	LOSS [training: 0.0027756244755742852 | validation: 0.011127124721084954]
	TIME [epoch: 8.11 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0016967673801254889		[learning rate: 1.55e-05]
		[batch 20/20] avg loss: 0.005113669984436371		[learning rate: 1.5472e-05]
	Learning Rate: 1.54722e-05
	LOSS [training: 0.00340521868228093 | validation: 0.011174383614587825]
	TIME [epoch: 8.09 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: -4.1981887298633923e-05		[learning rate: 1.5444e-05]
		[batch 20/20] avg loss: 0.005953648178057438		[learning rate: 1.5416e-05]
	Learning Rate: 1.54161e-05
	LOSS [training: 0.0029558331453794023 | validation: 0.004186752665141156]
	TIME [epoch: 8.09 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006931185403626565		[learning rate: 1.5388e-05]
		[batch 20/20] avg loss: 0.000515975394963967		[learning rate: 1.536e-05]
	Learning Rate: 1.53601e-05
	LOSS [training: 0.0037235803992952663 | validation: 0.012032592844995277]
	TIME [epoch: 8.09 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017748940033356906		[learning rate: 1.5332e-05]
		[batch 20/20] avg loss: 0.004776750267094452		[learning rate: 1.5304e-05]
	Learning Rate: 1.53044e-05
	LOSS [training: 0.0032758221352150708 | validation: 0.00453376609232611]
	TIME [epoch: 8.12 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004368338572664464		[learning rate: 1.5277e-05]
		[batch 20/20] avg loss: 0.0059124550810537415		[learning rate: 1.5249e-05]
	Learning Rate: 1.52488e-05
	LOSS [training: 0.005140396826859103 | validation: 0.007378118992462819]
	TIME [epoch: 8.1 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0001627142252669688		[learning rate: 1.5221e-05]
		[batch 20/20] avg loss: 0.0045272489409635035		[learning rate: 1.5194e-05]
	Learning Rate: 1.51935e-05
	LOSS [training: 0.002344981583115236 | validation: 0.012266594855512286]
	TIME [epoch: 8.1 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029508154645133335		[learning rate: 1.5166e-05]
		[batch 20/20] avg loss: 0.0075912177659379475		[learning rate: 1.5138e-05]
	Learning Rate: 1.51384e-05
	LOSS [training: 0.005271016615225639 | validation: 0.0037980978542249888]
	TIME [epoch: 8.1 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036526341071556672		[learning rate: 1.5111e-05]
		[batch 20/20] avg loss: 0.0030464431501856172		[learning rate: 1.5083e-05]
	Learning Rate: 1.50834e-05
	LOSS [training: 0.0033495386286706424 | validation: 0.0027153560682271222]
	TIME [epoch: 8.1 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038289970604340915		[learning rate: 1.5056e-05]
		[batch 20/20] avg loss: 0.0034277739788451654		[learning rate: 1.5029e-05]
	Learning Rate: 1.50287e-05
	LOSS [training: 0.003628385519639629 | validation: 0.008372012381038942]
	TIME [epoch: 8.11 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005017098558660648		[learning rate: 1.5001e-05]
		[batch 20/20] avg loss: 0.000503534608472158		[learning rate: 1.4974e-05]
	Learning Rate: 1.49741e-05
	LOSS [training: 0.002760316583566403 | validation: 0.006539540882606365]
	TIME [epoch: 8.09 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0044580136434564715		[learning rate: 1.4947e-05]
		[batch 20/20] avg loss: 0.0033393484091698107		[learning rate: 1.492e-05]
	Learning Rate: 1.49198e-05
	LOSS [training: 0.0038986810263131415 | validation: 0.009778401212547826]
	TIME [epoch: 8.09 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004805549787762599		[learning rate: 1.4893e-05]
		[batch 20/20] avg loss: 0.004001616772821964		[learning rate: 1.4866e-05]
	Learning Rate: 1.48657e-05
	LOSS [training: 0.0044035832802922815 | validation: 0.011873901843780474]
	TIME [epoch: 8.1 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019674487874930856		[learning rate: 1.4839e-05]
		[batch 20/20] avg loss: 0.0052750831474209755		[learning rate: 1.4812e-05]
	Learning Rate: 1.48117e-05
	LOSS [training: 0.003621265967457031 | validation: 0.0066330640956002145]
	TIME [epoch: 8.12 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006958678125453347		[learning rate: 1.4785e-05]
		[batch 20/20] avg loss: 0.006272968590876467		[learning rate: 1.4758e-05]
	Learning Rate: 1.4758e-05
	LOSS [training: 0.0066158233581649056 | validation: 0.007258569397178106]
	TIME [epoch: 8.09 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007250244510663885		[learning rate: 1.4731e-05]
		[batch 20/20] avg loss: 0.005588267249017063		[learning rate: 1.4704e-05]
	Learning Rate: 1.47044e-05
	LOSS [training: 0.006419255879840475 | validation: 0.009907851211994083]
	TIME [epoch: 8.1 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004048180490902079		[learning rate: 1.4678e-05]
		[batch 20/20] avg loss: 0.0038989239670215317		[learning rate: 1.4651e-05]
	Learning Rate: 1.4651e-05
	LOSS [training: 0.003973552228961805 | validation: 0.013277396270712317]
	TIME [epoch: 8.1 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029025219512038287		[learning rate: 1.4624e-05]
		[batch 20/20] avg loss: 0.00894897441588335		[learning rate: 1.4598e-05]
	Learning Rate: 1.45979e-05
	LOSS [training: 0.005925748183543589 | validation: 0.006893019985676671]
	TIME [epoch: 8.1 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006906614107592209		[learning rate: 1.4571e-05]
		[batch 20/20] avg loss: 0.0025450653922187262		[learning rate: 1.4545e-05]
	Learning Rate: 1.45449e-05
	LOSS [training: 0.0047258397499054675 | validation: 0.002883666962104199]
	TIME [epoch: 8.12 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029614388467114885		[learning rate: 1.4518e-05]
		[batch 20/20] avg loss: 0.00572827720826362		[learning rate: 1.4492e-05]
	Learning Rate: 1.44921e-05
	LOSS [training: 0.004344858027487554 | validation: 0.010754292471512937]
	TIME [epoch: 8.1 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010386696388159429		[learning rate: 1.4466e-05]
		[batch 20/20] avg loss: -0.0009043234863467264		[learning rate: 1.444e-05]
	Learning Rate: 1.44395e-05
	LOSS [training: 0.00474118645090635 | validation: 0.00518382278881513]
	TIME [epoch: 8.08 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00404953630100258		[learning rate: 1.4413e-05]
		[batch 20/20] avg loss: 0.006221316280937169		[learning rate: 1.4387e-05]
	Learning Rate: 1.43871e-05
	LOSS [training: 0.005135426290969874 | validation: 0.009811420113627987]
	TIME [epoch: 8.1 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005834115145062986		[learning rate: 1.4361e-05]
		[batch 20/20] avg loss: 0.0041276168155338425		[learning rate: 1.4335e-05]
	Learning Rate: 1.43349e-05
	LOSS [training: 0.004980865980298414 | validation: 0.009894007329499264]
	TIME [epoch: 8.11 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003355365326414131		[learning rate: 1.4309e-05]
		[batch 20/20] avg loss: 0.003413016697972518		[learning rate: 1.4283e-05]
	Learning Rate: 1.42829e-05
	LOSS [training: 0.0033841910121933244 | validation: 0.008878104062277094]
	TIME [epoch: 8.1 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005091803105625906		[learning rate: 1.4257e-05]
		[batch 20/20] avg loss: 0.005866495668279315		[learning rate: 1.4231e-05]
	Learning Rate: 1.4231e-05
	LOSS [training: 0.00547914938695261 | validation: 0.008273012961204269]
	TIME [epoch: 8.09 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017555536317508454		[learning rate: 1.4205e-05]
		[batch 20/20] avg loss: 0.005207936310914573		[learning rate: 1.4179e-05]
	Learning Rate: 1.41794e-05
	LOSS [training: 0.0034817449713327093 | validation: 0.00373975202406957]
	TIME [epoch: 8.1 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003093285360848178		[learning rate: 1.4154e-05]
		[batch 20/20] avg loss: 0.0038265880833355283		[learning rate: 1.4128e-05]
	Learning Rate: 1.41279e-05
	LOSS [training: 0.0034599367220918527 | validation: 0.009509345709855928]
	TIME [epoch: 8.1 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036562835971917945		[learning rate: 1.4102e-05]
		[batch 20/20] avg loss: 0.0071889712083122075		[learning rate: 1.4077e-05]
	Learning Rate: 1.40767e-05
	LOSS [training: 0.005422627402752 | validation: 0.007818638147558744]
	TIME [epoch: 8.11 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00480588419150149		[learning rate: 1.4051e-05]
		[batch 20/20] avg loss: 0.0075614062926096515		[learning rate: 1.4026e-05]
	Learning Rate: 1.40256e-05
	LOSS [training: 0.00618364524205557 | validation: 0.008621494235770324]
	TIME [epoch: 8.09 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0021008429825097033		[learning rate: 1.4e-05]
		[batch 20/20] avg loss: 0.0058366056294022785		[learning rate: 1.3975e-05]
	Learning Rate: 1.39747e-05
	LOSS [training: 0.0039687243059559904 | validation: 0.013672103200870247]
	TIME [epoch: 8.1 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00010038845558940403		[learning rate: 1.3949e-05]
		[batch 20/20] avg loss: 0.0043015532150462365		[learning rate: 1.3924e-05]
	Learning Rate: 1.3924e-05
	LOSS [training: 0.00220097083531782 | validation: 0.0020424593771271424]
	TIME [epoch: 8.09 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0041789105749387295		[learning rate: 1.3899e-05]
		[batch 20/20] avg loss: 0.003383909296590828		[learning rate: 1.3873e-05]
	Learning Rate: 1.38734e-05
	LOSS [training: 0.003781409935764779 | validation: 0.009768985873721363]
	TIME [epoch: 8.12 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005427602288547146		[learning rate: 1.3848e-05]
		[batch 20/20] avg loss: 0.0032764693693343337		[learning rate: 1.3823e-05]
	Learning Rate: 1.38231e-05
	LOSS [training: 0.00435203582894074 | validation: 0.011454009988610842]
	TIME [epoch: 8.11 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004013032713330499		[learning rate: 1.3798e-05]
		[batch 20/20] avg loss: 0.006343396158819429		[learning rate: 1.3773e-05]
	Learning Rate: 1.37729e-05
	LOSS [training: 0.005178214436074964 | validation: 0.006238386905083921]
	TIME [epoch: 8.09 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034603629884683807		[learning rate: 1.3748e-05]
		[batch 20/20] avg loss: 0.0070419590872060345		[learning rate: 1.3723e-05]
	Learning Rate: 1.37229e-05
	LOSS [training: 0.005251161037837208 | validation: 0.013462731982426639]
	TIME [epoch: 8.1 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0004996348445570088		[learning rate: 1.3698e-05]
		[batch 20/20] avg loss: 0.007354028960049176		[learning rate: 1.3673e-05]
	Learning Rate: 1.36731e-05
	LOSS [training: 0.003926831902303092 | validation: 0.014194272104439134]
	TIME [epoch: 8.11 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0053958230299149855		[learning rate: 1.3648e-05]
		[batch 20/20] avg loss: 0.0017814456521208042		[learning rate: 1.3624e-05]
	Learning Rate: 1.36235e-05
	LOSS [training: 0.003588634341017896 | validation: 0.008164551952965943]
	TIME [epoch: 8.11 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003919446811648441		[learning rate: 1.3599e-05]
		[batch 20/20] avg loss: 0.005116922908663057		[learning rate: 1.3574e-05]
	Learning Rate: 1.35741e-05
	LOSS [training: 0.0045181848601557485 | validation: 0.010089064844559261]
	TIME [epoch: 8.09 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007349575253078107		[learning rate: 1.3549e-05]
		[batch 20/20] avg loss: 0.007737144725362051		[learning rate: 1.3525e-05]
	Learning Rate: 1.35248e-05
	LOSS [training: 0.007543359989220079 | validation: 0.00621375196684589]
	TIME [epoch: 8.1 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005317474837785036		[learning rate: 1.35e-05]
		[batch 20/20] avg loss: 0.00867451688076099		[learning rate: 1.3476e-05]
	Learning Rate: 1.34757e-05
	LOSS [training: 0.006995995859273012 | validation: 0.012725482521312042]
	TIME [epoch: 8.1 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029144283037052855		[learning rate: 1.3451e-05]
		[batch 20/20] avg loss: 0.00473030637041315		[learning rate: 1.3427e-05]
	Learning Rate: 1.34268e-05
	LOSS [training: 0.0038223673370592175 | validation: 0.01639985111307118]
	TIME [epoch: 8.12 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006069713783495853		[learning rate: 1.3402e-05]
		[batch 20/20] avg loss: 0.005456671227767258		[learning rate: 1.3378e-05]
	Learning Rate: 1.33781e-05
	LOSS [training: 0.0030318213030584218 | validation: 0.006959693529250369]
	TIME [epoch: 8.1 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004341094640604032		[learning rate: 1.3354e-05]
		[batch 20/20] avg loss: 0.004876223652714778		[learning rate: 1.333e-05]
	Learning Rate: 1.33296e-05
	LOSS [training: 0.004608659146659404 | validation: 0.007506834966951357]
	TIME [epoch: 8.1 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013385522361043583		[learning rate: 1.3305e-05]
		[batch 20/20] avg loss: 0.007516855533101043		[learning rate: 1.3281e-05]
	Learning Rate: 1.32812e-05
	LOSS [training: 0.0044277038846027 | validation: 0.006439675452676534]
	TIME [epoch: 8.09 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003502944165483513		[learning rate: 1.3257e-05]
		[batch 20/20] avg loss: 0.008898480013733386		[learning rate: 1.3233e-05]
	Learning Rate: 1.3233e-05
	LOSS [training: 0.00620071208960845 | validation: 0.005579117688676738]
	TIME [epoch: 8.11 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007862409858795407		[learning rate: 1.3209e-05]
		[batch 20/20] avg loss: 0.005019950807325593		[learning rate: 1.3185e-05]
	Learning Rate: 1.3185e-05
	LOSS [training: 0.0064411803330605005 | validation: 0.0023424345442514694]
	TIME [epoch: 8.1 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003985180563423586		[learning rate: 1.3161e-05]
		[batch 20/20] avg loss: 0.006179075708088472		[learning rate: 1.3137e-05]
	Learning Rate: 1.31371e-05
	LOSS [training: 0.00508212813575603 | validation: 0.006173230552618257]
	TIME [epoch: 8.09 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0057898353505565015		[learning rate: 1.3113e-05]
		[batch 20/20] avg loss: 0.0038597745336700962		[learning rate: 1.3089e-05]
	Learning Rate: 1.30894e-05
	LOSS [training: 0.004824804942113298 | validation: 0.013500448327505205]
	TIME [epoch: 8.09 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003779634045552404		[learning rate: 1.3066e-05]
		[batch 20/20] avg loss: 0.0012851318067508592		[learning rate: 1.3042e-05]
	Learning Rate: 1.30419e-05
	LOSS [training: 0.0025323829261516316 | validation: 0.006804133779620769]
	TIME [epoch: 8.1 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005258937117517894		[learning rate: 1.3018e-05]
		[batch 20/20] avg loss: 0.0005701534025228563		[learning rate: 1.2995e-05]
	Learning Rate: 1.29946e-05
	LOSS [training: 0.002914545260020375 | validation: 0.014645739904754623]
	TIME [epoch: 8.12 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024393587406207544		[learning rate: 1.2971e-05]
		[batch 20/20] avg loss: 0.008141200999073209		[learning rate: 1.2947e-05]
	Learning Rate: 1.29475e-05
	LOSS [training: 0.005290279869846982 | validation: 0.014663239068738981]
	TIME [epoch: 8.09 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0020237040108966784		[learning rate: 1.2924e-05]
		[batch 20/20] avg loss: 0.0033367710139244285		[learning rate: 1.29e-05]
	Learning Rate: 1.29005e-05
	LOSS [training: 0.002680237512410554 | validation: 0.017876401161721768]
	TIME [epoch: 8.09 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0050385068703429296		[learning rate: 1.2877e-05]
		[batch 20/20] avg loss: 0.0019138189365306272		[learning rate: 1.2854e-05]
	Learning Rate: 1.28536e-05
	LOSS [training: 0.003476162903436778 | validation: 0.01270123889665606]
	TIME [epoch: 8.09 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0012475122092188327		[learning rate: 1.283e-05]
		[batch 20/20] avg loss: 0.004317140875143705		[learning rate: 1.2807e-05]
	Learning Rate: 1.2807e-05
	LOSS [training: 0.002782326542181269 | validation: 0.01365207391380779]
	TIME [epoch: 8.1 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0034582974069940937		[learning rate: 1.2784e-05]
		[batch 20/20] avg loss: 0.002382966674199792		[learning rate: 1.2761e-05]
	Learning Rate: 1.27605e-05
	LOSS [training: 0.002920632040596943 | validation: 0.008059667315052923]
	TIME [epoch: 8.1 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003422072565956444		[learning rate: 1.2737e-05]
		[batch 20/20] avg loss: 0.008015966349383993		[learning rate: 1.2714e-05]
	Learning Rate: 1.27142e-05
	LOSS [training: 0.005719019457670218 | validation: 0.009597987156587053]
	TIME [epoch: 8.1 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010848054004456997		[learning rate: 1.2691e-05]
		[batch 20/20] avg loss: 0.004591423022846846		[learning rate: 1.2668e-05]
	Learning Rate: 1.26681e-05
	LOSS [training: 0.007719738513651919 | validation: 0.004580848950053398]
	TIME [epoch: 8.09 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035347855055265437		[learning rate: 1.2645e-05]
		[batch 20/20] avg loss: 0.0061437436368635846		[learning rate: 1.2622e-05]
	Learning Rate: 1.26221e-05
	LOSS [training: 0.004839264571195064 | validation: 0.005169251021819164]
	TIME [epoch: 8.09 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004363321942541629		[learning rate: 1.2599e-05]
		[batch 20/20] avg loss: 0.004625109929140969		[learning rate: 1.2576e-05]
	Learning Rate: 1.25763e-05
	LOSS [training: 0.004494215935841299 | validation: 0.013292547133026439]
	TIME [epoch: 8.12 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0055707206874442295		[learning rate: 1.2553e-05]
		[batch 20/20] avg loss: 0.003437892016857523		[learning rate: 1.2531e-05]
	Learning Rate: 1.25307e-05
	LOSS [training: 0.004504306352150877 | validation: 0.008546511104501178]
	TIME [epoch: 8.09 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004523748628801218		[learning rate: 1.2508e-05]
		[batch 20/20] avg loss: 0.001066813112760161		[learning rate: 1.2485e-05]
	Learning Rate: 1.24852e-05
	LOSS [training: 0.002795280870780689 | validation: 0.005062755631244684]
	TIME [epoch: 8.1 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003417690260150717		[learning rate: 1.2463e-05]
		[batch 20/20] avg loss: 0.005931675475135991		[learning rate: 1.244e-05]
	Learning Rate: 1.24399e-05
	LOSS [training: 0.004674682867643355 | validation: 0.003964757605281967]
	TIME [epoch: 8.09 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004225258114230819		[learning rate: 1.2417e-05]
		[batch 20/20] avg loss: 0.0040888243729465795		[learning rate: 1.2395e-05]
	Learning Rate: 1.23947e-05
	LOSS [training: 0.004157041243588701 | validation: 0.01146795383036395]
	TIME [epoch: 8.11 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025800951128993554		[learning rate: 1.2372e-05]
		[batch 20/20] avg loss: 0.004629645102774745		[learning rate: 1.235e-05]
	Learning Rate: 1.23497e-05
	LOSS [training: 0.0036048701078370505 | validation: 0.008480861294292813]
	TIME [epoch: 8.11 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004650657210764875		[learning rate: 1.2327e-05]
		[batch 20/20] avg loss: 0.004493100992369302		[learning rate: 1.2305e-05]
	Learning Rate: 1.23049e-05
	LOSS [training: 0.004571879101567087 | validation: 0.009124105141601686]
	TIME [epoch: 8.1 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.00019811134522140507		[learning rate: 1.2283e-05]
		[batch 20/20] avg loss: 0.007222729554387607		[learning rate: 1.226e-05]
	Learning Rate: 1.22603e-05
	LOSS [training: 0.0035123091045831008 | validation: 0.01532284782273753]
	TIME [epoch: 8.09 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008652099200717743		[learning rate: 1.2238e-05]
		[batch 20/20] avg loss: 0.005967017965771778		[learning rate: 1.2216e-05]
	Learning Rate: 1.22158e-05
	LOSS [training: 0.0073095585832447605 | validation: 0.014018323841741488]
	TIME [epoch: 8.09 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0038579208483018592		[learning rate: 1.2194e-05]
		[batch 20/20] avg loss: 0.0049888756781720795		[learning rate: 1.2171e-05]
	Learning Rate: 1.21714e-05
	LOSS [training: 0.00442339826323697 | validation: 0.011625160352332933]
	TIME [epoch: 8.12 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024425377887093134		[learning rate: 1.2149e-05]
		[batch 20/20] avg loss: 0.002430460450715845		[learning rate: 1.2127e-05]
	Learning Rate: 1.21273e-05
	LOSS [training: 0.002436499119712579 | validation: 0.006962551752090526]
	TIME [epoch: 8.1 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005097832155371192		[learning rate: 1.2105e-05]
		[batch 20/20] avg loss: 0.0037669630408155757		[learning rate: 1.2083e-05]
	Learning Rate: 1.20833e-05
	LOSS [training: 0.004432397598093384 | validation: 0.003872461690941069]
	TIME [epoch: 8.1 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007865178314492155		[learning rate: 1.2061e-05]
		[batch 20/20] avg loss: 0.004625951805176416		[learning rate: 1.2039e-05]
	Learning Rate: 1.20394e-05
	LOSS [training: 0.002706234818312816 | validation: 0.009157565083740337]
	TIME [epoch: 8.1 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005677382730765629		[learning rate: 1.2018e-05]
		[batch 20/20] avg loss: -0.002877403399524737		[learning rate: 1.1996e-05]
	Learning Rate: 1.19957e-05
	LOSS [training: 0.0013999896656204467 | validation: 0.010042647154712073]
	TIME [epoch: 8.12 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0031676864438551266		[learning rate: 1.1974e-05]
		[batch 20/20] avg loss: 0.005543124037013332		[learning rate: 1.1952e-05]
	Learning Rate: 1.19522e-05
	LOSS [training: 0.0043554052404342295 | validation: 0.0089263296062024]
	TIME [epoch: 8.1 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002835190299048405		[learning rate: 1.193e-05]
		[batch 20/20] avg loss: 0.0033268013839705428		[learning rate: 1.1909e-05]
	Learning Rate: 1.19088e-05
	LOSS [training: 0.003080995841509474 | validation: 0.0077742652234826905]
	TIME [epoch: 8.1 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0057984283099358335		[learning rate: 1.1887e-05]
		[batch 20/20] avg loss: 0.006054268528981479		[learning rate: 1.1866e-05]
	Learning Rate: 1.18656e-05
	LOSS [training: 0.005926348419458656 | validation: 0.010449767710489315]
	TIME [epoch: 8.1 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004527844279629682		[learning rate: 1.1844e-05]
		[batch 20/20] avg loss: -0.00012828558385699333		[learning rate: 1.1823e-05]
	Learning Rate: 1.18225e-05
	LOSS [training: 0.0021997793478863445 | validation: 0.009212997938777976]
	TIME [epoch: 8.1 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00251260947008618		[learning rate: 1.1801e-05]
		[batch 20/20] avg loss: 0.003984125881286256		[learning rate: 1.178e-05]
	Learning Rate: 1.17796e-05
	LOSS [training: 0.0032483676756862173 | validation: 0.006803235457233261]
	TIME [epoch: 8.12 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006839614762119156		[learning rate: 1.1758e-05]
		[batch 20/20] avg loss: 0.005102862395449957		[learning rate: 1.1737e-05]
	Learning Rate: 1.17369e-05
	LOSS [training: 0.005971238578784557 | validation: 0.009766789232185422]
	TIME [epoch: 8.1 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002055792343197832		[learning rate: 1.1716e-05]
		[batch 20/20] avg loss: 0.00456765301879524		[learning rate: 1.1694e-05]
	Learning Rate: 1.16943e-05
	LOSS [training: 0.0033117226809965365 | validation: 0.007814681406265445]
	TIME [epoch: 8.1 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002193144354184014		[learning rate: 1.1673e-05]
		[batch 20/20] avg loss: 0.003634365308286415		[learning rate: 1.1652e-05]
	Learning Rate: 1.16518e-05
	LOSS [training: 0.0029137548312352145 | validation: 0.011540608278832094]
	TIME [epoch: 8.1 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003008662968551373		[learning rate: 1.1631e-05]
		[batch 20/20] avg loss: -0.00015824827137505166		[learning rate: 1.161e-05]
	Learning Rate: 1.16096e-05
	LOSS [training: 0.0014252073485881609 | validation: 0.006868099751809895]
	TIME [epoch: 8.12 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004889338196816824		[learning rate: 1.1588e-05]
		[batch 20/20] avg loss: 0.004882897238068336		[learning rate: 1.1567e-05]
	Learning Rate: 1.15674e-05
	LOSS [training: 0.00488611771744258 | validation: 0.013992121022107529]
	TIME [epoch: 8.1 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005443176190221122		[learning rate: 1.1546e-05]
		[batch 20/20] avg loss: 0.001035947386378342		[learning rate: 1.1525e-05]
	Learning Rate: 1.15255e-05
	LOSS [training: 0.003239561788299732 | validation: 0.007473722119947771]
	TIME [epoch: 8.1 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004454589511779282		[learning rate: 1.1505e-05]
		[batch 20/20] avg loss: 0.0029649882990103694		[learning rate: 1.1484e-05]
	Learning Rate: 1.14836e-05
	LOSS [training: 0.0037097889053948263 | validation: 0.012284992641936785]
	TIME [epoch: 8.1 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013518809953016772		[learning rate: 1.1463e-05]
		[batch 20/20] avg loss: 0.007135606324653644		[learning rate: 1.1442e-05]
	Learning Rate: 1.1442e-05
	LOSS [training: 0.00424374365997766 | validation: 0.01177276357201651]
	TIME [epoch: 8.1 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005578083850063935		[learning rate: 1.1421e-05]
		[batch 20/20] avg loss: 0.004267971932080612		[learning rate: 1.14e-05]
	Learning Rate: 1.14004e-05
	LOSS [training: 0.004923027891072274 | validation: 0.013431276586779284]
	TIME [epoch: 8.11 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006799302294181032		[learning rate: 1.138e-05]
		[batch 20/20] avg loss: 0.006551480392890765		[learning rate: 1.1359e-05]
	Learning Rate: 1.13591e-05
	LOSS [training: 0.006675391343535901 | validation: 0.011969140570666316]
	TIME [epoch: 8.1 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003537241963754729		[learning rate: 1.1338e-05]
		[batch 20/20] avg loss: 0.0025597860054233906		[learning rate: 1.1318e-05]
	Learning Rate: 1.13178e-05
	LOSS [training: 0.0030485139845890597 | validation: 0.010575157358924739]
	TIME [epoch: 8.1 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004524597226944184		[learning rate: 1.1297e-05]
		[batch 20/20] avg loss: 0.006810811563410365		[learning rate: 1.1277e-05]
	Learning Rate: 1.12768e-05
	LOSS [training: 0.005667704395177274 | validation: 0.01062264339356148]
	TIME [epoch: 8.09 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00037049980403594453		[learning rate: 1.1256e-05]
		[batch 20/20] avg loss: 0.002561665697953591		[learning rate: 1.1236e-05]
	Learning Rate: 1.12358e-05
	LOSS [training: 0.0014660827509947675 | validation: 0.008843116074550326]
	TIME [epoch: 8.13 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004059009263901741		[learning rate: 1.1215e-05]
		[batch 20/20] avg loss: 0.006170575474853748		[learning rate: 1.1195e-05]
	Learning Rate: 1.11951e-05
	LOSS [training: 0.005114792369377745 | validation: 0.01104335879332367]
	TIME [epoch: 8.1 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.002443976390735147		[learning rate: 1.1175e-05]
		[batch 20/20] avg loss: 0.004865994013529262		[learning rate: 1.1154e-05]
	Learning Rate: 1.11544e-05
	LOSS [training: 0.003654985202132204 | validation: 0.007696017746255786]
	TIME [epoch: 8.1 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0017695115236628918		[learning rate: 1.1134e-05]
		[batch 20/20] avg loss: 0.00698419746295022		[learning rate: 1.1114e-05]
	Learning Rate: 1.1114e-05
	LOSS [training: 0.004376854493306556 | validation: 0.010668674627641154]
	TIME [epoch: 8.1 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005525206004672887		[learning rate: 1.1094e-05]
		[batch 20/20] avg loss: 0.0023981332746965717		[learning rate: 1.1074e-05]
	Learning Rate: 1.10736e-05
	LOSS [training: 0.003961669639684728 | validation: 0.0064071728829695855]
	TIME [epoch: 8.11 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036509412254408383		[learning rate: 1.1054e-05]
		[batch 20/20] avg loss: 0.0045799185640011345		[learning rate: 1.1033e-05]
	Learning Rate: 1.10334e-05
	LOSS [training: 0.004115429894720987 | validation: 0.010257764199680875]
	TIME [epoch: 8.12 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004238461455919349		[learning rate: 1.1013e-05]
		[batch 20/20] avg loss: 0.007490186641033526		[learning rate: 1.0993e-05]
	Learning Rate: 1.09934e-05
	LOSS [training: 0.005864324048476437 | validation: 0.014758242904411143]
	TIME [epoch: 8.1 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032583109929793788		[learning rate: 1.0973e-05]
		[batch 20/20] avg loss: 0.005342814906310013		[learning rate: 1.0953e-05]
	Learning Rate: 1.09535e-05
	LOSS [training: 0.004300562949644696 | validation: 0.009322145052679604]
	TIME [epoch: 8.1 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0035770115526428266		[learning rate: 1.0934e-05]
		[batch 20/20] avg loss: 0.002279585952227547		[learning rate: 1.0914e-05]
	Learning Rate: 1.09137e-05
	LOSS [training: 0.0029282987524351864 | validation: 0.010829744234454137]
	TIME [epoch: 8.1 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0024361383024919235		[learning rate: 1.0894e-05]
		[batch 20/20] avg loss: 0.0035325286111816704		[learning rate: 1.0874e-05]
	Learning Rate: 1.08741e-05
	LOSS [training: 0.002984333456836797 | validation: 0.01130204137556639]
	TIME [epoch: 8.13 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0046271173611734315		[learning rate: 1.0854e-05]
		[batch 20/20] avg loss: 0.004489002362393623		[learning rate: 1.0835e-05]
	Learning Rate: 1.08347e-05
	LOSS [training: 0.004558059861783528 | validation: 0.01136149339941286]
	TIME [epoch: 8.1 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008033737690127645		[learning rate: 1.0815e-05]
		[batch 20/20] avg loss: 0.0011784959886101945		[learning rate: 1.0795e-05]
	Learning Rate: 1.07954e-05
	LOSS [training: 0.00460611683936892 | validation: 0.009565254091230609]
	TIME [epoch: 8.1 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006194070712142117		[learning rate: 1.0776e-05]
		[batch 20/20] avg loss: 0.0027173568797957074		[learning rate: 1.0756e-05]
	Learning Rate: 1.07562e-05
	LOSS [training: 0.004455713795968914 | validation: 0.008511504517364332]
	TIME [epoch: 8.09 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0015645695419225763		[learning rate: 1.0737e-05]
		[batch 20/20] avg loss: 0.0047577992865266705		[learning rate: 1.0717e-05]
	Learning Rate: 1.07171e-05
	LOSS [training: 0.0015966148723020473 | validation: 0.00981169209485254]
	TIME [epoch: 8.12 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003774532464371255		[learning rate: 1.0698e-05]
		[batch 20/20] avg loss: 0.006537136798914507		[learning rate: 1.0678e-05]
	Learning Rate: 1.06782e-05
	LOSS [training: 0.005155834631642881 | validation: 0.012539559474754543]
	TIME [epoch: 8.1 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0052424821086368536		[learning rate: 1.0659e-05]
		[batch 20/20] avg loss: 0.0034525193813574767		[learning rate: 1.0639e-05]
	Learning Rate: 1.06395e-05
	LOSS [training: 0.0043475007449971645 | validation: 0.009547416350320713]
	TIME [epoch: 8.1 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027763125439046036		[learning rate: 1.062e-05]
		[batch 20/20] avg loss: 0.004264221010057445		[learning rate: 1.0601e-05]
	Learning Rate: 1.06009e-05
	LOSS [training: 0.0035202667769810236 | validation: 0.012391687976002604]
	TIME [epoch: 8.1 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004949611869419615		[learning rate: 1.0582e-05]
		[batch 20/20] avg loss: 0.0035032275280775086		[learning rate: 1.0562e-05]
	Learning Rate: 1.05624e-05
	LOSS [training: 0.004226419698748562 | validation: 0.005940367198411215]
	TIME [epoch: 8.1 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003310145794787389		[learning rate: 1.0543e-05]
		[batch 20/20] avg loss: 0.007067992155138573		[learning rate: 1.0524e-05]
	Learning Rate: 1.05241e-05
	LOSS [training: 0.005189068974962981 | validation: 0.00408367169140464]
	TIME [epoch: 8.12 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: -0.0019544407538781812		[learning rate: 1.0505e-05]
		[batch 20/20] avg loss: 0.007967356817920051		[learning rate: 1.0486e-05]
	Learning Rate: 1.04859e-05
	LOSS [training: 0.0030064580320209354 | validation: 0.010429919986914945]
	TIME [epoch: 8.1 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0022928192473540545		[learning rate: 1.0467e-05]
		[batch 20/20] avg loss: 0.006775912546738189		[learning rate: 1.0448e-05]
	Learning Rate: 1.04478e-05
	LOSS [training: 0.004534365897046122 | validation: 0.012988418633206357]
	TIME [epoch: 8.1 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007067163175452076		[learning rate: 1.0429e-05]
		[batch 20/20] avg loss: 0.004062567849432272		[learning rate: 1.041e-05]
	Learning Rate: 1.04099e-05
	LOSS [training: 0.005564865512442175 | validation: 0.012298244961277153]
	TIME [epoch: 8.1 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003138162534909917		[learning rate: 1.0391e-05]
		[batch 20/20] avg loss: 0.003176781125568308		[learning rate: 1.0372e-05]
	Learning Rate: 1.03721e-05
	LOSS [training: 0.003157471830239113 | validation: 0.012290728803999361]
	TIME [epoch: 8.12 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005979862227979101		[learning rate: 1.0353e-05]
		[batch 20/20] avg loss: 0.006005049370504405		[learning rate: 1.0335e-05]
	Learning Rate: 1.03345e-05
	LOSS [training: 0.005992455799241754 | validation: 0.01079206906223943]
	TIME [epoch: 8.11 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006391659023842687		[learning rate: 1.0316e-05]
		[batch 20/20] avg loss: 0.0010997314342347279		[learning rate: 1.0297e-05]
	Learning Rate: 1.0297e-05
	LOSS [training: 0.0037456952290387075 | validation: 0.0031009125145244716]
	TIME [epoch: 8.1 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005028262420251915		[learning rate: 1.0278e-05]
		[batch 20/20] avg loss: 0.0029850499627175017		[learning rate: 1.026e-05]
	Learning Rate: 1.02596e-05
	LOSS [training: 0.004006656191484708 | validation: 0.0074735279832746845]
	TIME [epoch: 8.1 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005344075133639552		[learning rate: 1.0241e-05]
		[batch 20/20] avg loss: 0.006663647627616486		[learning rate: 1.0222e-05]
	Learning Rate: 1.02224e-05
	LOSS [training: 0.00600386138062802 | validation: 0.01280455418759452]
	TIME [epoch: 8.11 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005007097274399371		[learning rate: 1.0204e-05]
		[batch 20/20] avg loss: 0.003385605340417813		[learning rate: 1.0185e-05]
	Learning Rate: 1.01853e-05
	LOSS [training: 0.004196351307408592 | validation: 0.007447300707482172]
	TIME [epoch: 8.12 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004531918041195373		[learning rate: 1.0167e-05]
		[batch 20/20] avg loss: 0.001708070666541796		[learning rate: 1.0148e-05]
	Learning Rate: 1.01483e-05
	LOSS [training: 0.0031199943538685842 | validation: 0.006476601847182578]
	TIME [epoch: 8.11 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003625245751643598		[learning rate: 1.013e-05]
		[batch 20/20] avg loss: 0.0019500621332356651		[learning rate: 1.0112e-05]
	Learning Rate: 1.01115e-05
	LOSS [training: 0.0027876539424396317 | validation: 0.0043144983999201456]
	TIME [epoch: 8.1 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0013838763656471916		[learning rate: 1.0093e-05]
		[batch 20/20] avg loss: 0.006839310993409007		[learning rate: 1.0075e-05]
	Learning Rate: 1.00748e-05
	LOSS [training: 0.0041115936795281 | validation: 0.01012536583949003]
	TIME [epoch: 8.11 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003595927134206605		[learning rate: 1.0057e-05]
		[batch 20/20] avg loss: 0.004440805114383108		[learning rate: 1.0038e-05]
	Learning Rate: 1.00382e-05
	LOSS [training: 0.004018366124294856 | validation: 0.006941752782302095]
	TIME [epoch: 8.11 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00677568677928059		[learning rate: 1.002e-05]
		[batch 20/20] avg loss: 0.0019647933839253328		[learning rate: 1.0002e-05]
	Learning Rate: 1.00018e-05
	LOSS [training: 0.0043702400816029606 | validation: 0.011507451307115047]
	TIME [epoch: 8.11 sec]
Finished training in 16376.680 seconds.
