Args:
Namespace(name='model_tr_study206', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1', training_data='data/transition_rate_studies/tr_study206/tr_study206_training/r1', validation_data='data/transition_rate_studies/tr_study206/tr_study206_validation/r1', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2550239670

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.821405822431405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.821405822431405 | validation: 10.947335490283987]
	TIME [epoch: 78.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.812436245216253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.812436245216253 | validation: 12.282648229478015]
	TIME [epoch: 9.74 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 12.22947497156267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.22947497156267 | validation: 11.932740520844254]
	TIME [epoch: 9.74 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.629193540277978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.629193540277978 | validation: 10.509779647509422]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.792717463522237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.792717463522237 | validation: 9.491804901578881]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.002702762246658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.002702762246658 | validation: 11.001413814684133]
	TIME [epoch: 9.77 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.168631863777309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.168631863777309 | validation: 10.070950056883838]
	TIME [epoch: 9.75 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.029689306523467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.029689306523467 | validation: 11.083269065145874]
	TIME [epoch: 9.74 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.869669827191547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.869669827191547 | validation: 10.361621831051112]
	TIME [epoch: 9.76 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.744068854246342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.744068854246342 | validation: 10.584942400337704]
	TIME [epoch: 9.75 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.67091184818273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.67091184818273 | validation: 5.410689244979628]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.690926905923016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.690926905923016 | validation: 7.594348610442535]
	TIME [epoch: 9.77 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.354436156030227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.354436156030227 | validation: 10.267508786415787]
	TIME [epoch: 9.75 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.394384639137217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.394384639137217 | validation: 4.523419134963157]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.254152233970501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.254152233970501 | validation: 4.255619518626153]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.066749690203874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.066749690203874 | validation: 4.262826509549612]
	TIME [epoch: 9.74 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.786946417854866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.786946417854866 | validation: 5.745206859317914]
	TIME [epoch: 9.72 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.5561264675171556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.5561264675171556 | validation: 4.5150787585491585]
	TIME [epoch: 9.74 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.441637010707164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.441637010707164 | validation: 7.1966380231762574]
	TIME [epoch: 9.75 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.944538042883718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.944538042883718 | validation: 4.509521523496878]
	TIME [epoch: 9.74 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1571508152590395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1571508152590395 | validation: 6.334452928622457]
	TIME [epoch: 9.76 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.405744222919252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.405744222919252 | validation: 4.5917480745228065]
	TIME [epoch: 9.75 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.078676904575128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.078676904575128 | validation: 6.867770201813917]
	TIME [epoch: 9.74 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.534033842387958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.534033842387958 | validation: 4.1190204383353795]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.021789685623687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.021789685623687 | validation: 5.573220233676592]
	TIME [epoch: 9.75 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.243078405443662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.243078405443662 | validation: 4.379844232636096]
	TIME [epoch: 9.73 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.945946003682875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.945946003682875 | validation: 5.324967575705653]
	TIME [epoch: 9.73 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.444382618379921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.444382618379921 | validation: 5.437089825740688]
	TIME [epoch: 9.76 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.164782451578147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.164782451578147 | validation: 4.969757866048446]
	TIME [epoch: 9.74 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.887395295944602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.887395295944602 | validation: 5.198358449271588]
	TIME [epoch: 9.73 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8903103831714665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8903103831714665 | validation: 4.78368408470041]
	TIME [epoch: 9.77 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.661540886783599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.661540886783599 | validation: 5.039632064298628]
	TIME [epoch: 9.73 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7410390603515875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7410390603515875 | validation: 4.7734660406019715]
	TIME [epoch: 9.74 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.743929218685385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.743929218685385 | validation: 4.124630068741413]
	TIME [epoch: 9.77 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.580959472421649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.580959472421649 | validation: 4.720773114415195]
	TIME [epoch: 9.73 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.609724459712098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.609724459712098 | validation: 4.456921456428206]
	TIME [epoch: 9.73 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.552357639801675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.552357639801675 | validation: 5.16497788955896]
	TIME [epoch: 9.76 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.622420151849522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.622420151849522 | validation: 5.645856017771236]
	TIME [epoch: 9.74 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.712189827179735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.712189827179735 | validation: 5.104179503438486]
	TIME [epoch: 9.73 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.590920842094211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.590920842094211 | validation: 4.4529722601816655]
	TIME [epoch: 9.75 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.66735167781641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.66735167781641 | validation: 4.71251551844636]
	TIME [epoch: 9.74 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.623962900421537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.623962900421537 | validation: 4.71238994604893]
	TIME [epoch: 9.73 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.462448362605963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.462448362605963 | validation: 4.6344052747195414]
	TIME [epoch: 9.76 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.57846782005456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.57846782005456 | validation: 4.726569702710914]
	TIME [epoch: 9.74 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.746278264416421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.746278264416421 | validation: 4.008338729389719]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.460693596085481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.460693596085481 | validation: 4.87110462087965]
	TIME [epoch: 9.75 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.419515806264359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.419515806264359 | validation: 5.106249469790703]
	TIME [epoch: 9.73 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.556797490653065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.556797490653065 | validation: 5.45922252422348]
	TIME [epoch: 9.73 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.502146402963578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.502146402963578 | validation: 4.169992383356035]
	TIME [epoch: 9.75 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.37890448929337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.37890448929337 | validation: 4.842575620410271]
	TIME [epoch: 9.74 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.676190311940077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.676190311940077 | validation: 4.611510173292543]
	TIME [epoch: 9.73 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.5454708232572765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5454708232572765 | validation: 4.6559827915745755]
	TIME [epoch: 9.74 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.453848871837195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.453848871837195 | validation: 4.604399802214311]
	TIME [epoch: 9.75 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.458032987107549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.458032987107549 | validation: 5.054056350419453]
	TIME [epoch: 9.73 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.503377294777254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.503377294777254 | validation: 4.663088118237786]
	TIME [epoch: 9.74 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.499044951312746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.499044951312746 | validation: 4.182778040213105]
	TIME [epoch: 9.75 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.295789076635988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.295789076635988 | validation: 4.134057146585808]
	TIME [epoch: 9.72 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.252043730644481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.252043730644481 | validation: 5.136615934416335]
	TIME [epoch: 9.74 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.5872617772635325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5872617772635325 | validation: 4.241383435234277]
	TIME [epoch: 9.75 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.395293931545702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.395293931545702 | validation: 4.423743210436804]
	TIME [epoch: 9.73 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.4013542137059956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4013542137059956 | validation: 4.6341800180244235]
	TIME [epoch: 9.73 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.388803348628043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.388803348628043 | validation: 4.954138729007782]
	TIME [epoch: 9.75 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3111011153620895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3111011153620895 | validation: 4.849325008843776]
	TIME [epoch: 9.73 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.403468470457897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.403468470457897 | validation: 4.646056477879048]
	TIME [epoch: 9.73 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.36377856245704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.36377856245704 | validation: 4.606045583788214]
	TIME [epoch: 9.75 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.434573342673959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.434573342673959 | validation: 4.027382306948356]
	TIME [epoch: 9.72 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.291332298648531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.291332298648531 | validation: 4.382056837589488]
	TIME [epoch: 9.73 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3520394526494295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3520394526494295 | validation: 4.136271388038824]
	TIME [epoch: 9.76 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.28585758678239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.28585758678239 | validation: 4.6601483591454125]
	TIME [epoch: 9.73 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.872620235682481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.872620235682481 | validation: 6.206817936058908]
	TIME [epoch: 9.73 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.704766605631261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.704766605631261 | validation: 4.213599416547068]
	TIME [epoch: 9.76 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.346032674900658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.346032674900658 | validation: 4.933352627320677]
	TIME [epoch: 9.74 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.339088712167103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.339088712167103 | validation: 4.565961626000901]
	TIME [epoch: 9.73 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.300420970527231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.300420970527231 | validation: 4.457463579930639]
	TIME [epoch: 9.76 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.300281737790388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.300281737790388 | validation: 4.228981656730648]
	TIME [epoch: 9.73 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.287316170973263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.287316170973263 | validation: 4.269864517304342]
	TIME [epoch: 9.73 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.104636659340491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.104636659340491 | validation: 4.411965004363074]
	TIME [epoch: 9.75 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.249720370844313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.249720370844313 | validation: 4.577931288007213]
	TIME [epoch: 9.74 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2864947370522986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2864947370522986 | validation: 3.860110920237703]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.161934139468138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.161934139468138 | validation: 4.524930720499001]
	TIME [epoch: 9.77 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.214661602824513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.214661602824513 | validation: 4.3870234047052685]
	TIME [epoch: 9.76 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.202670809799154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.202670809799154 | validation: 4.513003584266539]
	TIME [epoch: 9.74 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.194018205683608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.194018205683608 | validation: 4.094526805459589]
	TIME [epoch: 9.76 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1239407896810665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1239407896810665 | validation: 4.303848883485558]
	TIME [epoch: 9.75 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.122776294802267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.122776294802267 | validation: 4.89826238844703]
	TIME [epoch: 9.75 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.419482998336532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.419482998336532 | validation: 3.74178480689261]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.097992556519712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.097992556519712 | validation: 4.147176077076031]
	TIME [epoch: 9.76 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3551312756134815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3551312756134815 | validation: 3.4430135693787136]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.066165752864002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.066165752864002 | validation: 3.740695228311395]
	TIME [epoch: 9.74 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.315409822245927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.315409822245927 | validation: 4.305049982121059]
	TIME [epoch: 9.74 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.211003500389984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.211003500389984 | validation: 4.584790998848764]
	TIME [epoch: 9.74 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.168851506996487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.168851506996487 | validation: 4.611768283970895]
	TIME [epoch: 9.75 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.216711711185255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.216711711185255 | validation: 4.044082612649895]
	TIME [epoch: 9.75 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.126687113066433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.126687113066433 | validation: 4.1437057027823645]
	TIME [epoch: 9.74 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.093825798365244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.093825798365244 | validation: 4.343895920358571]
	TIME [epoch: 9.74 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.055793594551187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.055793594551187 | validation: 4.393463731626078]
	TIME [epoch: 9.76 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.18196418833598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.18196418833598 | validation: 3.7050838206713883]
	TIME [epoch: 9.74 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0457591292138515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0457591292138515 | validation: 4.40182847546225]
	TIME [epoch: 9.73 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.075328299729099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.075328299729099 | validation: 4.388342814412341]
	TIME [epoch: 9.76 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.997563328000075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.997563328000075 | validation: 4.515904636299138]
	TIME [epoch: 9.73 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.066010482318381		[learning rate: 0.009971]
	Learning Rate: 0.00997096
	LOSS [training: 4.066010482318381 | validation: 4.355466743458903]
	TIME [epoch: 9.73 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.022946921211413		[learning rate: 0.0099348]
	Learning Rate: 0.00993477
	LOSS [training: 4.022946921211413 | validation: 4.098484094539645]
	TIME [epoch: 9.75 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.067227760499887		[learning rate: 0.0098987]
	Learning Rate: 0.00989872
	LOSS [training: 4.067227760499887 | validation: 4.062891322577992]
	TIME [epoch: 9.73 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9416996552239154		[learning rate: 0.0098628]
	Learning Rate: 0.00986279
	LOSS [training: 3.9416996552239154 | validation: 4.474684377413531]
	TIME [epoch: 9.73 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.000015627083537		[learning rate: 0.009827]
	Learning Rate: 0.009827
	LOSS [training: 4.000015627083537 | validation: 4.446240323708769]
	TIME [epoch: 9.76 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.994767774750575		[learning rate: 0.0097913]
	Learning Rate: 0.00979134
	LOSS [training: 3.994767774750575 | validation: 4.0818123192261035]
	TIME [epoch: 9.73 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9652384096123385		[learning rate: 0.0097558]
	Learning Rate: 0.00975581
	LOSS [training: 3.9652384096123385 | validation: 4.146352976980274]
	TIME [epoch: 9.73 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9775709123603162		[learning rate: 0.0097204]
	Learning Rate: 0.0097204
	LOSS [training: 3.9775709123603162 | validation: 3.964347703512316]
	TIME [epoch: 9.76 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.967167596924783		[learning rate: 0.0096851]
	Learning Rate: 0.00968513
	LOSS [training: 3.967167596924783 | validation: 4.362026063540396]
	TIME [epoch: 9.73 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9833559815591686		[learning rate: 0.00965]
	Learning Rate: 0.00964998
	LOSS [training: 3.9833559815591686 | validation: 3.8778656617338796]
	TIME [epoch: 9.73 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9775936344607685		[learning rate: 0.009615]
	Learning Rate: 0.00961496
	LOSS [training: 3.9775936344607685 | validation: 4.559732401226988]
	TIME [epoch: 9.76 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8792704289733946		[learning rate: 0.0095801]
	Learning Rate: 0.00958006
	LOSS [training: 3.8792704289733946 | validation: 4.217993890421189]
	TIME [epoch: 9.73 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0162879181088424		[learning rate: 0.0095453]
	Learning Rate: 0.0095453
	LOSS [training: 4.0162879181088424 | validation: 4.063542429817542]
	TIME [epoch: 9.73 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.965373026296234		[learning rate: 0.0095107]
	Learning Rate: 0.00951066
	LOSS [training: 3.965373026296234 | validation: 3.8955005449944826]
	TIME [epoch: 9.75 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.896863943513497		[learning rate: 0.0094761]
	Learning Rate: 0.00947614
	LOSS [training: 3.896863943513497 | validation: 4.014686686646202]
	TIME [epoch: 9.73 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.809218241384122		[learning rate: 0.0094418]
	Learning Rate: 0.00944175
	LOSS [training: 3.809218241384122 | validation: 4.133875827384321]
	TIME [epoch: 9.73 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9325874802078893		[learning rate: 0.0094075]
	Learning Rate: 0.00940749
	LOSS [training: 3.9325874802078893 | validation: 3.9024867475023624]
	TIME [epoch: 9.75 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8442015308117923		[learning rate: 0.0093733]
	Learning Rate: 0.00937335
	LOSS [training: 3.8442015308117923 | validation: 3.9507660467084293]
	TIME [epoch: 9.74 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.889445816453032		[learning rate: 0.0093393]
	Learning Rate: 0.00933933
	LOSS [training: 3.889445816453032 | validation: 3.9639949792961056]
	TIME [epoch: 9.73 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8874916678774674		[learning rate: 0.0093054]
	Learning Rate: 0.00930544
	LOSS [training: 3.8874916678774674 | validation: 4.369491863221822]
	TIME [epoch: 9.75 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8536735621223626		[learning rate: 0.0092717]
	Learning Rate: 0.00927167
	LOSS [training: 3.8536735621223626 | validation: 4.3927529729121675]
	TIME [epoch: 9.74 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9079143756713064		[learning rate: 0.009238]
	Learning Rate: 0.00923802
	LOSS [training: 3.9079143756713064 | validation: 3.6168186527111867]
	TIME [epoch: 9.72 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.062275484903805		[learning rate: 0.0092045]
	Learning Rate: 0.0092045
	LOSS [training: 4.062275484903805 | validation: 4.39907022595765]
	TIME [epoch: 9.74 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8840355656910655		[learning rate: 0.0091711]
	Learning Rate: 0.00917109
	LOSS [training: 3.8840355656910655 | validation: 4.212618785614942]
	TIME [epoch: 9.76 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.391342160163651		[learning rate: 0.0091378]
	Learning Rate: 0.00913781
	LOSS [training: 4.391342160163651 | validation: 4.6497078360607444]
	TIME [epoch: 9.74 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.05139053200711		[learning rate: 0.0091046]
	Learning Rate: 0.00910465
	LOSS [training: 4.05139053200711 | validation: 4.18975545067218]
	TIME [epoch: 9.74 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.006457430740063		[learning rate: 0.0090716]
	Learning Rate: 0.00907161
	LOSS [training: 4.006457430740063 | validation: 3.82507055896024]
	TIME [epoch: 9.75 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8868642756073393		[learning rate: 0.0090387]
	Learning Rate: 0.00903868
	LOSS [training: 3.8868642756073393 | validation: 3.8533309758291927]
	TIME [epoch: 9.73 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8783680597326784		[learning rate: 0.0090059]
	Learning Rate: 0.00900588
	LOSS [training: 3.8783680597326784 | validation: 3.80029020178329]
	TIME [epoch: 9.73 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.716797340919876		[learning rate: 0.0089732]
	Learning Rate: 0.0089732
	LOSS [training: 3.716797340919876 | validation: 3.7834189495150428]
	TIME [epoch: 9.77 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7054856726967658		[learning rate: 0.0089406]
	Learning Rate: 0.00894064
	LOSS [training: 3.7054856726967658 | validation: 3.831967487885296]
	TIME [epoch: 9.73 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.716712690742162		[learning rate: 0.0089082]
	Learning Rate: 0.00890819
	LOSS [training: 3.716712690742162 | validation: 3.9422271744176864]
	TIME [epoch: 9.73 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6631020800165564		[learning rate: 0.0088759]
	Learning Rate: 0.00887586
	LOSS [training: 3.6631020800165564 | validation: 4.382777045157313]
	TIME [epoch: 9.76 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.75484107130477		[learning rate: 0.0088437]
	Learning Rate: 0.00884365
	LOSS [training: 3.75484107130477 | validation: 3.610689999228222]
	TIME [epoch: 9.74 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.931169340393353		[learning rate: 0.0088116]
	Learning Rate: 0.00881156
	LOSS [training: 3.931169340393353 | validation: 3.4242169316295965]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6812560339290896		[learning rate: 0.0087796]
	Learning Rate: 0.00877958
	LOSS [training: 3.6812560339290896 | validation: 4.308742554574833]
	TIME [epoch: 9.75 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7018391925566805		[learning rate: 0.0087477]
	Learning Rate: 0.00874772
	LOSS [training: 3.7018391925566805 | validation: 4.108175539743047]
	TIME [epoch: 9.73 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6378653094401265		[learning rate: 0.008716]
	Learning Rate: 0.00871597
	LOSS [training: 3.6378653094401265 | validation: 3.643520002357902]
	TIME [epoch: 9.72 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6987988812339347		[learning rate: 0.0086843]
	Learning Rate: 0.00868434
	LOSS [training: 3.6987988812339347 | validation: 4.276518014679512]
	TIME [epoch: 9.76 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8143189647227502		[learning rate: 0.0086528]
	Learning Rate: 0.00865282
	LOSS [training: 3.8143189647227502 | validation: 3.628958656137687]
	TIME [epoch: 9.74 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6580484820482653		[learning rate: 0.0086214]
	Learning Rate: 0.00862142
	LOSS [training: 3.6580484820482653 | validation: 3.49778687464139]
	TIME [epoch: 9.74 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6254594139460417		[learning rate: 0.0085901]
	Learning Rate: 0.00859013
	LOSS [training: 3.6254594139460417 | validation: 3.7916103558747567]
	TIME [epoch: 9.76 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6832899404341894		[learning rate: 0.008559]
	Learning Rate: 0.00855896
	LOSS [training: 3.6832899404341894 | validation: 3.758945949114677]
	TIME [epoch: 9.73 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6894508231037597		[learning rate: 0.0085279]
	Learning Rate: 0.0085279
	LOSS [training: 3.6894508231037597 | validation: 3.632419080336274]
	TIME [epoch: 9.73 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.719688601442499		[learning rate: 0.008497]
	Learning Rate: 0.00849695
	LOSS [training: 3.719688601442499 | validation: 4.279700392568759]
	TIME [epoch: 9.74 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.693944038706362		[learning rate: 0.0084661]
	Learning Rate: 0.00846612
	LOSS [training: 3.693944038706362 | validation: 3.9331551067659447]
	TIME [epoch: 9.72 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6459636338480017		[learning rate: 0.0084354]
	Learning Rate: 0.00843539
	LOSS [training: 3.6459636338480017 | validation: 4.003107075742434]
	TIME [epoch: 9.73 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603536793710183		[learning rate: 0.0084048]
	Learning Rate: 0.00840478
	LOSS [training: 3.603536793710183 | validation: 3.8600302554595376]
	TIME [epoch: 9.75 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6893342563678964		[learning rate: 0.0083743]
	Learning Rate: 0.00837428
	LOSS [training: 3.6893342563678964 | validation: 3.6446187120815727]
	TIME [epoch: 9.73 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.138751798833615		[learning rate: 0.0083439]
	Learning Rate: 0.00834389
	LOSS [training: 4.138751798833615 | validation: 4.07205261206859]
	TIME [epoch: 9.72 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.807351088500102		[learning rate: 0.0083136]
	Learning Rate: 0.00831361
	LOSS [training: 3.807351088500102 | validation: 3.772127312423892]
	TIME [epoch: 9.74 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6436740868627284		[learning rate: 0.0082834]
	Learning Rate: 0.00828344
	LOSS [training: 3.6436740868627284 | validation: 3.8741910692052137]
	TIME [epoch: 9.74 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.544219068677146		[learning rate: 0.0082534]
	Learning Rate: 0.00825338
	LOSS [training: 3.544219068677146 | validation: 3.858143382583007]
	TIME [epoch: 9.73 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5704662876298143		[learning rate: 0.0082234]
	Learning Rate: 0.00822342
	LOSS [training: 3.5704662876298143 | validation: 3.8257194653663467]
	TIME [epoch: 9.75 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5904026397174675		[learning rate: 0.0081936]
	Learning Rate: 0.00819358
	LOSS [training: 3.5904026397174675 | validation: 3.54407281967607]
	TIME [epoch: 9.74 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5779364742767954		[learning rate: 0.0081638]
	Learning Rate: 0.00816384
	LOSS [training: 3.5779364742767954 | validation: 3.565766272950382]
	TIME [epoch: 9.72 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5807035385995802		[learning rate: 0.0081342]
	Learning Rate: 0.00813422
	LOSS [training: 3.5807035385995802 | validation: 3.6760461467720824]
	TIME [epoch: 9.73 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.632521113767896		[learning rate: 0.0081047]
	Learning Rate: 0.0081047
	LOSS [training: 3.632521113767896 | validation: 3.5593934231571542]
	TIME [epoch: 9.74 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.57278110103521		[learning rate: 0.0080753]
	Learning Rate: 0.00807529
	LOSS [training: 3.57278110103521 | validation: 4.2312135107936095]
	TIME [epoch: 9.72 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5020211426637644		[learning rate: 0.008046]
	Learning Rate: 0.00804598
	LOSS [training: 3.5020211426637644 | validation: 4.49361634450884]
	TIME [epoch: 9.73 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.854036352318039		[learning rate: 0.0080168]
	Learning Rate: 0.00801678
	LOSS [training: 3.854036352318039 | validation: 3.585544324871851]
	TIME [epoch: 9.75 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5807742900802992		[learning rate: 0.0079877]
	Learning Rate: 0.00798769
	LOSS [training: 3.5807742900802992 | validation: 3.7330283364238346]
	TIME [epoch: 9.71 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5318199666262133		[learning rate: 0.0079587]
	Learning Rate: 0.0079587
	LOSS [training: 3.5318199666262133 | validation: 4.07631415057939]
	TIME [epoch: 9.72 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5215600035116794		[learning rate: 0.0079298]
	Learning Rate: 0.00792982
	LOSS [training: 3.5215600035116794 | validation: 3.7132622689319152]
	TIME [epoch: 9.74 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5763284718643726		[learning rate: 0.007901]
	Learning Rate: 0.00790104
	LOSS [training: 3.5763284718643726 | validation: 3.5194729491504164]
	TIME [epoch: 9.72 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6216605957212225		[learning rate: 0.0078724]
	Learning Rate: 0.00787237
	LOSS [training: 3.6216605957212225 | validation: 3.4381559396262764]
	TIME [epoch: 9.72 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603066111724832		[learning rate: 0.0078438]
	Learning Rate: 0.0078438
	LOSS [training: 3.603066111724832 | validation: 3.5947420594495503]
	TIME [epoch: 9.74 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5188670642994575		[learning rate: 0.0078153]
	Learning Rate: 0.00781533
	LOSS [training: 3.5188670642994575 | validation: 3.737072471730677]
	TIME [epoch: 9.73 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.580838426567941		[learning rate: 0.007787]
	Learning Rate: 0.00778697
	LOSS [training: 3.580838426567941 | validation: 3.4342780577442937]
	TIME [epoch: 9.72 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5426104628788258		[learning rate: 0.0077587]
	Learning Rate: 0.00775871
	LOSS [training: 3.5426104628788258 | validation: 3.5391617182020276]
	TIME [epoch: 9.74 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.532640623651015		[learning rate: 0.0077306]
	Learning Rate: 0.00773055
	LOSS [training: 3.532640623651015 | validation: 3.6048787475538893]
	TIME [epoch: 9.72 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4940064008808847		[learning rate: 0.0077025]
	Learning Rate: 0.0077025
	LOSS [training: 3.4940064008808847 | validation: 3.485528152055971]
	TIME [epoch: 9.72 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.851402213989503		[learning rate: 0.0076745]
	Learning Rate: 0.00767455
	LOSS [training: 3.851402213989503 | validation: 4.821552459775269]
	TIME [epoch: 9.74 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7901905855002163		[learning rate: 0.0076467]
	Learning Rate: 0.00764669
	LOSS [training: 3.7901905855002163 | validation: 3.47523971501354]
	TIME [epoch: 9.72 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.486512867927928		[learning rate: 0.0076189]
	Learning Rate: 0.00761894
	LOSS [training: 3.486512867927928 | validation: 3.3900475752399553]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.685273937958955		[learning rate: 0.0075913]
	Learning Rate: 0.00759129
	LOSS [training: 3.685273937958955 | validation: 3.662992308078923]
	TIME [epoch: 9.76 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5324304153297703		[learning rate: 0.0075637]
	Learning Rate: 0.00756374
	LOSS [training: 3.5324304153297703 | validation: 3.458595482143047]
	TIME [epoch: 9.73 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.207932564927608		[learning rate: 0.0075363]
	Learning Rate: 0.00753629
	LOSS [training: 4.207932564927608 | validation: 4.086290311329973]
	TIME [epoch: 9.71 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493994271689452		[learning rate: 0.0075089]
	Learning Rate: 0.00750895
	LOSS [training: 3.493994271689452 | validation: 3.5785091753134504]
	TIME [epoch: 9.74 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.420880422848664		[learning rate: 0.0074817]
	Learning Rate: 0.00748169
	LOSS [training: 3.420880422848664 | validation: 3.4813712364621767]
	TIME [epoch: 9.74 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3631818659311348		[learning rate: 0.0074545]
	Learning Rate: 0.00745454
	LOSS [training: 3.3631818659311348 | validation: 3.5713147080537033]
	TIME [epoch: 9.72 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4800007909519444		[learning rate: 0.0074275]
	Learning Rate: 0.00742749
	LOSS [training: 3.4800007909519444 | validation: 3.9222450367162693]
	TIME [epoch: 9.75 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4531193944485943		[learning rate: 0.0074005]
	Learning Rate: 0.00740054
	LOSS [training: 3.4531193944485943 | validation: 3.7949798862212094]
	TIME [epoch: 9.72 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.394791773475176		[learning rate: 0.0073737]
	Learning Rate: 0.00737368
	LOSS [training: 3.394791773475176 | validation: 4.161886932194013]
	TIME [epoch: 9.71 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.41665134471358		[learning rate: 0.0073469]
	Learning Rate: 0.00734692
	LOSS [training: 3.41665134471358 | validation: 4.113340824205436]
	TIME [epoch: 9.74 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5749132389356033		[learning rate: 0.0073203]
	Learning Rate: 0.00732026
	LOSS [training: 3.5749132389356033 | validation: 3.5827255783343186]
	TIME [epoch: 9.72 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4665750982032364		[learning rate: 0.0072937]
	Learning Rate: 0.00729369
	LOSS [training: 3.4665750982032364 | validation: 3.434645219990313]
	TIME [epoch: 9.7 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.487321873495902		[learning rate: 0.0072672]
	Learning Rate: 0.00726722
	LOSS [training: 3.487321873495902 | validation: 3.7705925137774363]
	TIME [epoch: 9.72 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.466810675718554		[learning rate: 0.0072408]
	Learning Rate: 0.00724085
	LOSS [training: 3.466810675718554 | validation: 4.186742457996624]
	TIME [epoch: 9.74 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.23336800174982		[learning rate: 0.0072146]
	Learning Rate: 0.00721457
	LOSS [training: 4.23336800174982 | validation: 4.702722715819928]
	TIME [epoch: 9.72 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8175315911690872		[learning rate: 0.0071884]
	Learning Rate: 0.00718839
	LOSS [training: 3.8175315911690872 | validation: 3.755126109873152]
	TIME [epoch: 9.72 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.266464447004848		[learning rate: 0.0071623]
	Learning Rate: 0.0071623
	LOSS [training: 4.266464447004848 | validation: 3.5569932297084432]
	TIME [epoch: 9.73 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.398808098620654		[learning rate: 0.0071363]
	Learning Rate: 0.00713631
	LOSS [training: 3.398808098620654 | validation: 3.632007739452116]
	TIME [epoch: 9.71 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3775160072894237		[learning rate: 0.0071104]
	Learning Rate: 0.00711041
	LOSS [training: 3.3775160072894237 | validation: 3.516433352011528]
	TIME [epoch: 9.71 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7543354451938376		[learning rate: 0.0070846]
	Learning Rate: 0.00708461
	LOSS [training: 3.7543354451938376 | validation: 5.004942270527837]
	TIME [epoch: 9.74 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8980841833020152		[learning rate: 0.0070589]
	Learning Rate: 0.0070589
	LOSS [training: 3.8980841833020152 | validation: 3.640662384395067]
	TIME [epoch: 9.71 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4817703422114903		[learning rate: 0.0070333]
	Learning Rate: 0.00703328
	LOSS [training: 3.4817703422114903 | validation: 3.6920523818218203]
	TIME [epoch: 9.71 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.459267987751082		[learning rate: 0.0070078]
	Learning Rate: 0.00700776
	LOSS [training: 3.459267987751082 | validation: 3.792936496838946]
	TIME [epoch: 9.74 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4339267222621217		[learning rate: 0.0069823]
	Learning Rate: 0.00698232
	LOSS [training: 3.4339267222621217 | validation: 3.7294026843032033]
	TIME [epoch: 9.71 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.484018426122789		[learning rate: 0.006957]
	Learning Rate: 0.00695698
	LOSS [training: 3.484018426122789 | validation: 3.863382482345045]
	TIME [epoch: 9.71 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.453720583147793		[learning rate: 0.0069317]
	Learning Rate: 0.00693174
	LOSS [training: 3.453720583147793 | validation: 3.600309294324561]
	TIME [epoch: 9.74 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4452665359789423		[learning rate: 0.0069066]
	Learning Rate: 0.00690658
	LOSS [training: 3.4452665359789423 | validation: 3.62459091028326]
	TIME [epoch: 9.72 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3828322515733804		[learning rate: 0.0068815]
	Learning Rate: 0.00688152
	LOSS [training: 3.3828322515733804 | validation: 3.511887076508592]
	TIME [epoch: 9.72 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3641092488261775		[learning rate: 0.0068565]
	Learning Rate: 0.00685654
	LOSS [training: 3.3641092488261775 | validation: 3.5143875914972105]
	TIME [epoch: 9.74 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.435400776750973		[learning rate: 0.0068317]
	Learning Rate: 0.00683166
	LOSS [training: 3.435400776750973 | validation: 3.3702835696605797]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3953791300997387		[learning rate: 0.0068069]
	Learning Rate: 0.00680687
	LOSS [training: 3.3953791300997387 | validation: 3.6152936523292722]
	TIME [epoch: 9.72 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3638256789051946		[learning rate: 0.0067822]
	Learning Rate: 0.00678217
	LOSS [training: 3.3638256789051946 | validation: 3.593928707817883]
	TIME [epoch: 9.73 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4895120382546567		[learning rate: 0.0067576]
	Learning Rate: 0.00675755
	LOSS [training: 3.4895120382546567 | validation: 3.4900679669738595]
	TIME [epoch: 9.72 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3863363771802453		[learning rate: 0.006733]
	Learning Rate: 0.00673303
	LOSS [training: 3.3863363771802453 | validation: 3.7210462132321527]
	TIME [epoch: 9.71 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3711776302006933		[learning rate: 0.0067086]
	Learning Rate: 0.00670859
	LOSS [training: 3.3711776302006933 | validation: 3.9027774804990942]
	TIME [epoch: 9.73 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.454524457575637		[learning rate: 0.0066842]
	Learning Rate: 0.00668425
	LOSS [training: 3.454524457575637 | validation: 3.640250617869276]
	TIME [epoch: 9.72 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.377209960884266		[learning rate: 0.00666]
	Learning Rate: 0.00665999
	LOSS [training: 3.377209960884266 | validation: 3.4614774053179778]
	TIME [epoch: 9.7 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.346893376532715		[learning rate: 0.0066358]
	Learning Rate: 0.00663582
	LOSS [training: 3.346893376532715 | validation: 3.480273542882122]
	TIME [epoch: 9.72 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6165706577368426		[learning rate: 0.0066117]
	Learning Rate: 0.00661174
	LOSS [training: 3.6165706577368426 | validation: 3.772859025336917]
	TIME [epoch: 9.71 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.384367043502518		[learning rate: 0.0065877]
	Learning Rate: 0.00658775
	LOSS [training: 3.384367043502518 | validation: 4.2879542074088794]
	TIME [epoch: 9.7 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5717546010616585		[learning rate: 0.0065638]
	Learning Rate: 0.00656384
	LOSS [training: 3.5717546010616585 | validation: 3.3654252963908995]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2822040610889487		[learning rate: 0.00654]
	Learning Rate: 0.00654002
	LOSS [training: 3.2822040610889487 | validation: 3.5659071023560354]
	TIME [epoch: 9.75 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3948980197243346		[learning rate: 0.0065163]
	Learning Rate: 0.00651628
	LOSS [training: 3.3948980197243346 | validation: 3.6906102861064154]
	TIME [epoch: 9.73 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.376035830936723		[learning rate: 0.0064926]
	Learning Rate: 0.00649264
	LOSS [training: 3.376035830936723 | validation: 3.94091819255797]
	TIME [epoch: 9.76 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4446423096724446		[learning rate: 0.0064691]
	Learning Rate: 0.00646907
	LOSS [training: 3.4446423096724446 | validation: 3.5163632161947596]
	TIME [epoch: 9.74 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.331178551334505		[learning rate: 0.0064456]
	Learning Rate: 0.0064456
	LOSS [training: 3.331178551334505 | validation: 3.793049211313406]
	TIME [epoch: 9.73 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.409759892042757		[learning rate: 0.0064222]
	Learning Rate: 0.00642221
	LOSS [training: 3.409759892042757 | validation: 4.095460913609444]
	TIME [epoch: 9.72 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4912522546295706		[learning rate: 0.0063989]
	Learning Rate: 0.0063989
	LOSS [training: 3.4912522546295706 | validation: 3.365755070279287]
	TIME [epoch: 9.73 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.36906299173681		[learning rate: 0.0063757]
	Learning Rate: 0.00637568
	LOSS [training: 3.36906299173681 | validation: 3.6205572011729887]
	TIME [epoch: 9.72 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4711276071309927		[learning rate: 0.0063525]
	Learning Rate: 0.00635254
	LOSS [training: 3.4711276071309927 | validation: 3.432305739127786]
	TIME [epoch: 9.73 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.734148705400338		[learning rate: 0.0063295]
	Learning Rate: 0.00632949
	LOSS [training: 3.734148705400338 | validation: 3.878247539188398]
	TIME [epoch: 9.75 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.352043198045564		[learning rate: 0.0063065]
	Learning Rate: 0.00630652
	LOSS [training: 3.352043198045564 | validation: 3.6542713405675147]
	TIME [epoch: 9.71 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3744155944304266		[learning rate: 0.0062836]
	Learning Rate: 0.00628363
	LOSS [training: 3.3744155944304266 | validation: 3.6377485943815064]
	TIME [epoch: 9.72 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.316122818850996		[learning rate: 0.0062608]
	Learning Rate: 0.00626082
	LOSS [training: 3.316122818850996 | validation: 3.7867781961780747]
	TIME [epoch: 9.73 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.553492958343829		[learning rate: 0.0062381]
	Learning Rate: 0.0062381
	LOSS [training: 3.553492958343829 | validation: 4.799417731702484]
	TIME [epoch: 9.71 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8069854435621364		[learning rate: 0.0062155]
	Learning Rate: 0.00621547
	LOSS [training: 3.8069854435621364 | validation: 3.4372328219825294]
	TIME [epoch: 9.7 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2912120319344034		[learning rate: 0.0061929]
	Learning Rate: 0.00619291
	LOSS [training: 3.2912120319344034 | validation: 3.8248344365732967]
	TIME [epoch: 9.75 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.377516873568562		[learning rate: 0.0061704]
	Learning Rate: 0.00617043
	LOSS [training: 3.377516873568562 | validation: 3.476748826682879]
	TIME [epoch: 9.71 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.346609927888043		[learning rate: 0.006148]
	Learning Rate: 0.00614804
	LOSS [training: 3.346609927888043 | validation: 3.755974626983816]
	TIME [epoch: 9.71 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.302255420980451		[learning rate: 0.0061257]
	Learning Rate: 0.00612573
	LOSS [training: 3.302255420980451 | validation: 3.776480540586703]
	TIME [epoch: 9.73 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4926402242672467		[learning rate: 0.0061035]
	Learning Rate: 0.0061035
	LOSS [training: 3.4926402242672467 | validation: 3.4379481947292816]
	TIME [epoch: 9.72 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.302314743792917		[learning rate: 0.0060814]
	Learning Rate: 0.00608135
	LOSS [training: 3.302314743792917 | validation: 3.6112923844725446]
	TIME [epoch: 9.72 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.402899547050777		[learning rate: 0.0060593]
	Learning Rate: 0.00605928
	LOSS [training: 3.402899547050777 | validation: 3.9654643167943595]
	TIME [epoch: 9.74 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7768817413870925		[learning rate: 0.0060373]
	Learning Rate: 0.00603729
	LOSS [training: 3.7768817413870925 | validation: 3.5681669273445538]
	TIME [epoch: 9.72 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4671042843524305		[learning rate: 0.0060154]
	Learning Rate: 0.00601538
	LOSS [training: 3.4671042843524305 | validation: 3.546053326919761]
	TIME [epoch: 9.71 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.327924832935018		[learning rate: 0.0059936]
	Learning Rate: 0.00599355
	LOSS [training: 3.327924832935018 | validation: 3.5362605323941296]
	TIME [epoch: 9.74 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.335556791629375		[learning rate: 0.0059718]
	Learning Rate: 0.0059718
	LOSS [training: 3.335556791629375 | validation: 3.7602034797358646]
	TIME [epoch: 9.72 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3941034123376057		[learning rate: 0.0059501]
	Learning Rate: 0.00595013
	LOSS [training: 3.3941034123376057 | validation: 3.5403420322395753]
	TIME [epoch: 9.71 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.400750097386131		[learning rate: 0.0059285]
	Learning Rate: 0.00592853
	LOSS [training: 3.400750097386131 | validation: 3.485580478808083]
	TIME [epoch: 9.73 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3087121935351798		[learning rate: 0.005907]
	Learning Rate: 0.00590702
	LOSS [training: 3.3087121935351798 | validation: 3.5017101729902427]
	TIME [epoch: 9.72 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3042899565125987		[learning rate: 0.0058856]
	Learning Rate: 0.00588558
	LOSS [training: 3.3042899565125987 | validation: 3.569919655679397]
	TIME [epoch: 9.71 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3525479039886212		[learning rate: 0.0058642]
	Learning Rate: 0.00586422
	LOSS [training: 3.3525479039886212 | validation: 3.418468259672825]
	TIME [epoch: 9.75 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3244289959914006		[learning rate: 0.0058429]
	Learning Rate: 0.00584294
	LOSS [training: 3.3244289959914006 | validation: 3.8357396166196747]
	TIME [epoch: 9.73 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3553602728855614		[learning rate: 0.0058217]
	Learning Rate: 0.00582174
	LOSS [training: 3.3553602728855614 | validation: 3.448650085345972]
	TIME [epoch: 9.73 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3460963510335		[learning rate: 0.0058006]
	Learning Rate: 0.00580061
	LOSS [training: 3.3460963510335 | validation: 3.5444116771141467]
	TIME [epoch: 9.74 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2885239792642467		[learning rate: 0.0057796]
	Learning Rate: 0.00577956
	LOSS [training: 3.2885239792642467 | validation: 3.582699421586525]
	TIME [epoch: 9.74 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3139176149246863		[learning rate: 0.0057586]
	Learning Rate: 0.00575859
	LOSS [training: 3.3139176149246863 | validation: 3.4018776804889104]
	TIME [epoch: 9.71 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.303584769247763		[learning rate: 0.0057377]
	Learning Rate: 0.00573769
	LOSS [training: 3.303584769247763 | validation: 3.474812321971277]
	TIME [epoch: 9.74 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.503145064905363		[learning rate: 0.0057169]
	Learning Rate: 0.00571686
	LOSS [training: 4.503145064905363 | validation: 3.5860229005499566]
	TIME [epoch: 9.73 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.339132883857979		[learning rate: 0.0056961]
	Learning Rate: 0.00569612
	LOSS [training: 3.339132883857979 | validation: 3.5079544090564982]
	TIME [epoch: 9.73 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.76366700648598		[learning rate: 0.0056754]
	Learning Rate: 0.00567545
	LOSS [training: 3.76366700648598 | validation: 3.4827248929415977]
	TIME [epoch: 9.73 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.346253852111539		[learning rate: 0.0056548]
	Learning Rate: 0.00565485
	LOSS [training: 3.346253852111539 | validation: 3.706484656629442]
	TIME [epoch: 9.75 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.12232698414566		[learning rate: 0.0056343]
	Learning Rate: 0.00563433
	LOSS [training: 4.12232698414566 | validation: 3.4657448899668495]
	TIME [epoch: 9.72 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.285671738772927		[learning rate: 0.0056139]
	Learning Rate: 0.00561388
	LOSS [training: 3.285671738772927 | validation: 3.5459422981186366]
	TIME [epoch: 9.73 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3160505561220575		[learning rate: 0.0055935]
	Learning Rate: 0.00559351
	LOSS [training: 3.3160505561220575 | validation: 3.47215847684736]
	TIME [epoch: 9.75 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3454916875589236		[learning rate: 0.0055732]
	Learning Rate: 0.00557321
	LOSS [training: 3.3454916875589236 | validation: 3.45724315023942]
	TIME [epoch: 9.73 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.313939952377372		[learning rate: 0.005553]
	Learning Rate: 0.00555298
	LOSS [training: 3.313939952377372 | validation: 3.408133228817665]
	TIME [epoch: 9.73 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.305926388374933		[learning rate: 0.0055328]
	Learning Rate: 0.00553283
	LOSS [training: 3.305926388374933 | validation: 3.4298623930281544]
	TIME [epoch: 9.75 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.304756217263428		[learning rate: 0.0055128]
	Learning Rate: 0.00551275
	LOSS [training: 3.304756217263428 | validation: 3.334926251369117]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4557982103808436		[learning rate: 0.0054927]
	Learning Rate: 0.00549274
	LOSS [training: 3.4557982103808436 | validation: 3.7139599716867173]
	TIME [epoch: 9.73 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.435126879133197		[learning rate: 0.0054728]
	Learning Rate: 0.00547281
	LOSS [training: 3.435126879133197 | validation: 3.426533059129805]
	TIME [epoch: 9.76 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3022700078118006		[learning rate: 0.005453]
	Learning Rate: 0.00545295
	LOSS [training: 3.3022700078118006 | validation: 3.3613367381807304]
	TIME [epoch: 9.71 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2360658074396915		[learning rate: 0.0054332]
	Learning Rate: 0.00543316
	LOSS [training: 3.2360658074396915 | validation: 3.449190498657027]
	TIME [epoch: 9.72 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2795317018214702		[learning rate: 0.0054134]
	Learning Rate: 0.00541344
	LOSS [training: 3.2795317018214702 | validation: 3.620617736591417]
	TIME [epoch: 9.75 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3748185250074343		[learning rate: 0.0053938]
	Learning Rate: 0.0053938
	LOSS [training: 3.3748185250074343 | validation: 3.3593671615780307]
	TIME [epoch: 9.71 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.261973864892154		[learning rate: 0.0053742]
	Learning Rate: 0.00537422
	LOSS [training: 3.261973864892154 | validation: 3.63832060158004]
	TIME [epoch: 9.72 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4554485662734935		[learning rate: 0.0053547]
	Learning Rate: 0.00535472
	LOSS [training: 3.4554485662734935 | validation: 3.508726425227702]
	TIME [epoch: 9.74 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4311113782178566		[learning rate: 0.0053353]
	Learning Rate: 0.00533529
	LOSS [training: 3.4311113782178566 | validation: 3.812878743068684]
	TIME [epoch: 9.72 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.330353780031819		[learning rate: 0.0053159]
	Learning Rate: 0.00531593
	LOSS [training: 3.330353780031819 | validation: 3.46527771473355]
	TIME [epoch: 9.72 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1798549543765624		[learning rate: 0.0052966]
	Learning Rate: 0.00529663
	LOSS [training: 3.1798549543765624 | validation: 3.3936980996490953]
	TIME [epoch: 9.74 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.231380623586843		[learning rate: 0.0052774]
	Learning Rate: 0.00527741
	LOSS [training: 3.231380623586843 | validation: 3.3908113030258584]
	TIME [epoch: 9.72 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2731168083451707		[learning rate: 0.0052583]
	Learning Rate: 0.00525826
	LOSS [training: 3.2731168083451707 | validation: 3.34057164121485]
	TIME [epoch: 9.71 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2064732015449406		[learning rate: 0.0052392]
	Learning Rate: 0.00523918
	LOSS [training: 3.2064732015449406 | validation: 3.5535774546514296]
	TIME [epoch: 9.74 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3059610143929445		[learning rate: 0.0052202]
	Learning Rate: 0.00522017
	LOSS [training: 3.3059610143929445 | validation: 3.455531723463355]
	TIME [epoch: 9.72 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2840390694716177		[learning rate: 0.0052012]
	Learning Rate: 0.00520122
	LOSS [training: 3.2840390694716177 | validation: 3.44433105981724]
	TIME [epoch: 9.71 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.322199942210105		[learning rate: 0.0051823]
	Learning Rate: 0.00518234
	LOSS [training: 3.322199942210105 | validation: 3.4102988859337398]
	TIME [epoch: 9.72 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3233167480091135		[learning rate: 0.0051635]
	Learning Rate: 0.00516354
	LOSS [training: 3.3233167480091135 | validation: 3.3946105289901882]
	TIME [epoch: 9.72 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.295612552181402		[learning rate: 0.0051448]
	Learning Rate: 0.0051448
	LOSS [training: 3.295612552181402 | validation: 3.4577859729232387]
	TIME [epoch: 9.71 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.365862100662638		[learning rate: 0.0051261]
	Learning Rate: 0.00512613
	LOSS [training: 3.365862100662638 | validation: 3.527828444364483]
	TIME [epoch: 9.73 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3832515821050833		[learning rate: 0.0051075]
	Learning Rate: 0.00510753
	LOSS [training: 3.3832515821050833 | validation: 3.419173653192803]
	TIME [epoch: 9.73 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2964367227330498		[learning rate: 0.005089]
	Learning Rate: 0.00508899
	LOSS [training: 3.2964367227330498 | validation: 3.3467718691000083]
	TIME [epoch: 9.71 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3514975246823524		[learning rate: 0.0050705]
	Learning Rate: 0.00507052
	LOSS [training: 3.3514975246823524 | validation: 3.490850729473398]
	TIME [epoch: 9.73 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.221462158240288		[learning rate: 0.0050521]
	Learning Rate: 0.00505212
	LOSS [training: 3.221462158240288 | validation: 3.479724354507182]
	TIME [epoch: 9.73 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.242306771389017		[learning rate: 0.0050338]
	Learning Rate: 0.00503379
	LOSS [training: 3.242306771389017 | validation: 3.96262763726573]
	TIME [epoch: 9.71 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.475760340132903		[learning rate: 0.0050155]
	Learning Rate: 0.00501552
	LOSS [training: 3.475760340132903 | validation: 3.5836680708314823]
	TIME [epoch: 9.73 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2863177208465713		[learning rate: 0.0049973]
	Learning Rate: 0.00499732
	LOSS [training: 3.2863177208465713 | validation: 3.3719863257961507]
	TIME [epoch: 9.73 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2497053006735763		[learning rate: 0.0049792]
	Learning Rate: 0.00497918
	LOSS [training: 3.2497053006735763 | validation: 3.8932369838156604]
	TIME [epoch: 9.71 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5799617491655047		[learning rate: 0.0049611]
	Learning Rate: 0.00496111
	LOSS [training: 3.5799617491655047 | validation: 3.401959851028134]
	TIME [epoch: 9.73 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2273645944321268		[learning rate: 0.0049431]
	Learning Rate: 0.00494311
	LOSS [training: 3.2273645944321268 | validation: 3.327695232916751]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2376109976905476		[learning rate: 0.0049252]
	Learning Rate: 0.00492517
	LOSS [training: 3.2376109976905476 | validation: 3.505095924261086]
	TIME [epoch: 9.73 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.311399026758026		[learning rate: 0.0049073]
	Learning Rate: 0.00490729
	LOSS [training: 3.311399026758026 | validation: 3.5103827405253254]
	TIME [epoch: 9.72 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3815788889912985		[learning rate: 0.0048895]
	Learning Rate: 0.00488948
	LOSS [training: 3.3815788889912985 | validation: 3.3836271823866104]
	TIME [epoch: 9.74 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2328635932876524		[learning rate: 0.0048717]
	Learning Rate: 0.00487174
	LOSS [training: 3.2328635932876524 | validation: 3.6791936160143086]
	TIME [epoch: 9.72 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2988945141603794		[learning rate: 0.0048541]
	Learning Rate: 0.00485406
	LOSS [training: 3.2988945141603794 | validation: 3.5434019828757632]
	TIME [epoch: 9.71 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2956585727796166		[learning rate: 0.0048364]
	Learning Rate: 0.00483645
	LOSS [training: 3.2956585727796166 | validation: 3.6635288239712236]
	TIME [epoch: 9.74 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2709131857101155		[learning rate: 0.0048189]
	Learning Rate: 0.00481889
	LOSS [training: 3.2709131857101155 | validation: 3.5301204047269574]
	TIME [epoch: 9.72 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3799764338751084		[learning rate: 0.0048014]
	Learning Rate: 0.00480141
	LOSS [training: 3.3799764338751084 | validation: 3.604700644910636]
	TIME [epoch: 9.71 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.320573987005544		[learning rate: 0.004784]
	Learning Rate: 0.00478398
	LOSS [training: 3.320573987005544 | validation: 3.3911356341852867]
	TIME [epoch: 9.73 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.427428264325114		[learning rate: 0.0047666]
	Learning Rate: 0.00476662
	LOSS [training: 3.427428264325114 | validation: 3.4670011869446244]
	TIME [epoch: 9.73 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2757109754862577		[learning rate: 0.0047493]
	Learning Rate: 0.00474932
	LOSS [training: 3.2757109754862577 | validation: 3.37437009271036]
	TIME [epoch: 9.71 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3005937178514104		[learning rate: 0.0047321]
	Learning Rate: 0.00473209
	LOSS [training: 3.3005937178514104 | validation: 3.5696020473275882]
	TIME [epoch: 9.73 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5321952153541103		[learning rate: 0.0047149]
	Learning Rate: 0.00471491
	LOSS [training: 3.5321952153541103 | validation: 3.618608563795803]
	TIME [epoch: 9.72 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4109207803473423		[learning rate: 0.0046978]
	Learning Rate: 0.0046978
	LOSS [training: 3.4109207803473423 | validation: 3.3334523801352987]
	TIME [epoch: 9.73 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1975849692803835		[learning rate: 0.0046808]
	Learning Rate: 0.00468075
	LOSS [training: 3.1975849692803835 | validation: 3.369536015966777]
	TIME [epoch: 9.74 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2890478314208975		[learning rate: 0.0046638]
	Learning Rate: 0.00466377
	LOSS [training: 3.2890478314208975 | validation: 3.7060375458063595]
	TIME [epoch: 9.73 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.307633263229363		[learning rate: 0.0046468]
	Learning Rate: 0.00464684
	LOSS [training: 3.307633263229363 | validation: 3.4035911036946858]
	TIME [epoch: 9.72 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5377068861165712		[learning rate: 0.00463]
	Learning Rate: 0.00462998
	LOSS [training: 3.5377068861165712 | validation: 3.352578735614248]
	TIME [epoch: 9.75 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2874072063654283		[learning rate: 0.0046132]
	Learning Rate: 0.00461318
	LOSS [training: 3.2874072063654283 | validation: 3.505667262905085]
	TIME [epoch: 9.74 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.198994752376432		[learning rate: 0.0045964]
	Learning Rate: 0.00459643
	LOSS [training: 3.198994752376432 | validation: 3.6125569951364676]
	TIME [epoch: 9.73 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2650388438696076		[learning rate: 0.0045798]
	Learning Rate: 0.00457975
	LOSS [training: 3.2650388438696076 | validation: 3.37859824081546]
	TIME [epoch: 9.74 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.345938930787028		[learning rate: 0.0045631]
	Learning Rate: 0.00456313
	LOSS [training: 3.345938930787028 | validation: 3.513624726293648]
	TIME [epoch: 9.73 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2491275009740006		[learning rate: 0.0045466]
	Learning Rate: 0.00454657
	LOSS [training: 3.2491275009740006 | validation: 3.6047121942542013]
	TIME [epoch: 9.73 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2523177731619826		[learning rate: 0.0045301]
	Learning Rate: 0.00453007
	LOSS [training: 3.2523177731619826 | validation: 3.460828266337766]
	TIME [epoch: 9.74 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.232913881685252		[learning rate: 0.0045136]
	Learning Rate: 0.00451363
	LOSS [training: 3.232913881685252 | validation: 3.5235591238333335]
	TIME [epoch: 9.73 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.217476851739149		[learning rate: 0.0044973]
	Learning Rate: 0.00449725
	LOSS [training: 3.217476851739149 | validation: 3.4501604490677553]
	TIME [epoch: 9.71 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.240664842179519		[learning rate: 0.0044809]
	Learning Rate: 0.00448093
	LOSS [training: 3.240664842179519 | validation: 3.4394548366817865]
	TIME [epoch: 9.73 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2382948374278877		[learning rate: 0.0044647]
	Learning Rate: 0.00446467
	LOSS [training: 3.2382948374278877 | validation: 3.695651692462375]
	TIME [epoch: 9.73 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.272210872664549		[learning rate: 0.0044485]
	Learning Rate: 0.00444847
	LOSS [training: 3.272210872664549 | validation: 3.4014020386848927]
	TIME [epoch: 9.72 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.322705436174556		[learning rate: 0.0044323]
	Learning Rate: 0.00443232
	LOSS [training: 3.322705436174556 | validation: 3.5156629655153266]
	TIME [epoch: 9.72 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3046834450156135		[learning rate: 0.0044162]
	Learning Rate: 0.00441624
	LOSS [training: 3.3046834450156135 | validation: 3.363284694334362]
	TIME [epoch: 9.74 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.333043680869504		[learning rate: 0.0044002]
	Learning Rate: 0.00440021
	LOSS [training: 3.333043680869504 | validation: 3.4499273515770468]
	TIME [epoch: 9.72 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.19533078102645		[learning rate: 0.0043842]
	Learning Rate: 0.00438424
	LOSS [training: 3.19533078102645 | validation: 3.537643423165871]
	TIME [epoch: 9.72 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2729271252678593		[learning rate: 0.0043683]
	Learning Rate: 0.00436833
	LOSS [training: 3.2729271252678593 | validation: 3.4956153835156214]
	TIME [epoch: 9.74 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2656843562278226		[learning rate: 0.0043525]
	Learning Rate: 0.00435248
	LOSS [training: 3.2656843562278226 | validation: 3.387123012168536]
	TIME [epoch: 9.71 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2927197164200224		[learning rate: 0.0043367]
	Learning Rate: 0.00433668
	LOSS [training: 3.2927197164200224 | validation: 3.358158061671295]
	TIME [epoch: 9.72 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2380002487348833		[learning rate: 0.0043209]
	Learning Rate: 0.00432095
	LOSS [training: 3.2380002487348833 | validation: 3.6045876846249416]
	TIME [epoch: 9.74 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.272916694999066		[learning rate: 0.0043053]
	Learning Rate: 0.00430527
	LOSS [training: 3.272916694999066 | validation: 3.383789658091018]
	TIME [epoch: 9.72 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1836680093809817		[learning rate: 0.0042896]
	Learning Rate: 0.00428964
	LOSS [training: 3.1836680093809817 | validation: 3.4641161880381417]
	TIME [epoch: 9.72 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2759454301480537		[learning rate: 0.0042741]
	Learning Rate: 0.00427407
	LOSS [training: 3.2759454301480537 | validation: 3.4087617648178457]
	TIME [epoch: 9.73 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.396060141195124		[learning rate: 0.0042586]
	Learning Rate: 0.00425856
	LOSS [training: 3.396060141195124 | validation: 3.411423552819818]
	TIME [epoch: 9.72 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2953151623289942		[learning rate: 0.0042431]
	Learning Rate: 0.00424311
	LOSS [training: 3.2953151623289942 | validation: 3.3443193380739844]
	TIME [epoch: 9.71 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.177449068761065		[learning rate: 0.0042277]
	Learning Rate: 0.00422771
	LOSS [training: 3.177449068761065 | validation: 3.5700030690776363]
	TIME [epoch: 9.74 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.282022548665715		[learning rate: 0.0042124]
	Learning Rate: 0.00421237
	LOSS [training: 3.282022548665715 | validation: 3.386893440014502]
	TIME [epoch: 9.71 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.25742721584501		[learning rate: 0.0041971]
	Learning Rate: 0.00419708
	LOSS [training: 3.25742721584501 | validation: 3.40068992864156]
	TIME [epoch: 9.71 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.206724764401769		[learning rate: 0.0041818]
	Learning Rate: 0.00418185
	LOSS [training: 3.206724764401769 | validation: 3.3773228465876413]
	TIME [epoch: 9.73 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.223723102637875		[learning rate: 0.0041667]
	Learning Rate: 0.00416667
	LOSS [training: 3.223723102637875 | validation: 3.3353318764404065]
	TIME [epoch: 9.72 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.168186127262645		[learning rate: 0.0041516]
	Learning Rate: 0.00415155
	LOSS [training: 3.168186127262645 | validation: 3.418123064976622]
	TIME [epoch: 9.71 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1889629103743227		[learning rate: 0.0041365]
	Learning Rate: 0.00413649
	LOSS [training: 3.1889629103743227 | validation: 3.6729164428060814]
	TIME [epoch: 9.74 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3082626845880556		[learning rate: 0.0041215]
	Learning Rate: 0.00412147
	LOSS [training: 3.3082626845880556 | validation: 3.5351499178855867]
	TIME [epoch: 9.72 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.237658244955096		[learning rate: 0.0041065]
	Learning Rate: 0.00410652
	LOSS [training: 3.237658244955096 | validation: 3.4012397865332935]
	TIME [epoch: 9.72 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.265456349810164		[learning rate: 0.0040916]
	Learning Rate: 0.00409161
	LOSS [training: 3.265456349810164 | validation: 3.59987114390044]
	TIME [epoch: 9.73 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4081914553718646		[learning rate: 0.0040768]
	Learning Rate: 0.00407677
	LOSS [training: 3.4081914553718646 | validation: 3.5083716530434566]
	TIME [epoch: 9.73 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.296283027221249		[learning rate: 0.004062]
	Learning Rate: 0.00406197
	LOSS [training: 3.296283027221249 | validation: 3.3275585662422875]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1969602710918243		[learning rate: 0.0040472]
	Learning Rate: 0.00404723
	LOSS [training: 3.1969602710918243 | validation: 3.397464767993648]
	TIME [epoch: 9.75 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.232987538022359		[learning rate: 0.0040325]
	Learning Rate: 0.00403254
	LOSS [training: 3.232987538022359 | validation: 3.3278942066349844]
	TIME [epoch: 9.73 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2063839414357473		[learning rate: 0.0040179]
	Learning Rate: 0.00401791
	LOSS [training: 3.2063839414357473 | validation: 3.55145843263935]
	TIME [epoch: 9.73 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.263318783643019		[learning rate: 0.0040033]
	Learning Rate: 0.00400333
	LOSS [training: 3.263318783643019 | validation: 3.421616157208891]
	TIME [epoch: 9.73 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1543128430230603		[learning rate: 0.0039888]
	Learning Rate: 0.0039888
	LOSS [training: 3.1543128430230603 | validation: 3.3335910925901047]
	TIME [epoch: 9.75 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.306329278550855		[learning rate: 0.0039743]
	Learning Rate: 0.00397432
	LOSS [training: 3.306329278550855 | validation: 3.583819119337379]
	TIME [epoch: 9.73 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.211878586380307		[learning rate: 0.0039599]
	Learning Rate: 0.0039599
	LOSS [training: 3.211878586380307 | validation: 3.3006234333238718]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.257418292139465		[learning rate: 0.0039455]
	Learning Rate: 0.00394553
	LOSS [training: 3.257418292139465 | validation: 3.358580887060125]
	TIME [epoch: 9.75 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3580344707170164		[learning rate: 0.0039312]
	Learning Rate: 0.00393121
	LOSS [training: 3.3580344707170164 | validation: 3.441051379024074]
	TIME [epoch: 9.74 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1978045620838467		[learning rate: 0.0039169]
	Learning Rate: 0.00391694
	LOSS [training: 3.1978045620838467 | validation: 3.5059729292956066]
	TIME [epoch: 9.73 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2206612273571436		[learning rate: 0.0039027]
	Learning Rate: 0.00390273
	LOSS [training: 3.2206612273571436 | validation: 3.35986937112512]
	TIME [epoch: 9.73 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.14255064243918		[learning rate: 0.0038886]
	Learning Rate: 0.00388857
	LOSS [training: 3.14255064243918 | validation: 3.3791729050444084]
	TIME [epoch: 9.71 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1653189756364064		[learning rate: 0.0038745]
	Learning Rate: 0.00387445
	LOSS [training: 3.1653189756364064 | validation: 3.3260309703986555]
	TIME [epoch: 9.71 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.177393649016562		[learning rate: 0.0038604]
	Learning Rate: 0.00386039
	LOSS [training: 3.177393649016562 | validation: 3.504789651626311]
	TIME [epoch: 9.72 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3122059864438405		[learning rate: 0.0038464]
	Learning Rate: 0.00384638
	LOSS [training: 3.3122059864438405 | validation: 3.3531418780324103]
	TIME [epoch: 9.71 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1899157429344247		[learning rate: 0.0038324]
	Learning Rate: 0.00383242
	LOSS [training: 3.1899157429344247 | validation: 3.417704955448745]
	TIME [epoch: 9.71 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.174150260819182		[learning rate: 0.0038185]
	Learning Rate: 0.00381852
	LOSS [training: 3.174150260819182 | validation: 3.2961534909447185]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1689467004934597		[learning rate: 0.0038047]
	Learning Rate: 0.00380466
	LOSS [training: 3.1689467004934597 | validation: 3.3864526816128624]
	TIME [epoch: 9.74 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3596238663009714		[learning rate: 0.0037909]
	Learning Rate: 0.00379085
	LOSS [training: 3.3596238663009714 | validation: 3.4787718273479857]
	TIME [epoch: 9.74 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.226634723895984		[learning rate: 0.0037771]
	Learning Rate: 0.00377709
	LOSS [training: 3.226634723895984 | validation: 3.5028339127054786]
	TIME [epoch: 9.77 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2367676091539166		[learning rate: 0.0037634]
	Learning Rate: 0.00376339
	LOSS [training: 3.2367676091539166 | validation: 3.5094801517903944]
	TIME [epoch: 9.74 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1866391419761175		[learning rate: 0.0037497]
	Learning Rate: 0.00374973
	LOSS [training: 3.1866391419761175 | validation: 3.338380986361949]
	TIME [epoch: 9.75 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2095407188434164		[learning rate: 0.0037361]
	Learning Rate: 0.00373612
	LOSS [training: 3.2095407188434164 | validation: 3.343938656022838]
	TIME [epoch: 9.77 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.39223396095325		[learning rate: 0.0037226]
	Learning Rate: 0.00372256
	LOSS [training: 3.39223396095325 | validation: 3.3908975136466832]
	TIME [epoch: 9.75 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.105392486786653		[learning rate: 0.0037091]
	Learning Rate: 0.00370905
	LOSS [training: 3.105392486786653 | validation: 3.4998065748507976]
	TIME [epoch: 9.75 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1909240043713623		[learning rate: 0.0036956]
	Learning Rate: 0.00369559
	LOSS [training: 3.1909240043713623 | validation: 3.296137269134509]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1548362061541644		[learning rate: 0.0036822]
	Learning Rate: 0.00368218
	LOSS [training: 3.1548362061541644 | validation: 3.3488586554973563]
	TIME [epoch: 9.74 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1536317050594387		[learning rate: 0.0036688]
	Learning Rate: 0.00366882
	LOSS [training: 3.1536317050594387 | validation: 3.62535082976509]
	TIME [epoch: 9.74 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2060683133431334		[learning rate: 0.0036555]
	Learning Rate: 0.0036555
	LOSS [training: 3.2060683133431334 | validation: 3.350252487459797]
	TIME [epoch: 9.76 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1633815442010467		[learning rate: 0.0036422]
	Learning Rate: 0.00364224
	LOSS [training: 3.1633815442010467 | validation: 3.4373163475305275]
	TIME [epoch: 9.74 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1644782340622366		[learning rate: 0.003629]
	Learning Rate: 0.00362902
	LOSS [training: 3.1644782340622366 | validation: 3.557645916181926]
	TIME [epoch: 9.74 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1837905667882076		[learning rate: 0.0036159]
	Learning Rate: 0.00361585
	LOSS [training: 3.1837905667882076 | validation: 3.4208094395544872]
	TIME [epoch: 9.76 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1272888692213288		[learning rate: 0.0036027]
	Learning Rate: 0.00360273
	LOSS [training: 3.1272888692213288 | validation: 3.391101471885399]
	TIME [epoch: 9.74 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2118271235995755		[learning rate: 0.0035897]
	Learning Rate: 0.00358965
	LOSS [training: 3.2118271235995755 | validation: 3.3372497303302233]
	TIME [epoch: 9.73 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1609424645903177		[learning rate: 0.0035766]
	Learning Rate: 0.00357663
	LOSS [training: 3.1609424645903177 | validation: 3.630984818700314]
	TIME [epoch: 9.76 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2065082142092427		[learning rate: 0.0035636]
	Learning Rate: 0.00356365
	LOSS [training: 3.2065082142092427 | validation: 3.330614695149269]
	TIME [epoch: 9.74 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1743238316251245		[learning rate: 0.0035507]
	Learning Rate: 0.00355072
	LOSS [training: 3.1743238316251245 | validation: 3.503921808242625]
	TIME [epoch: 9.74 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2149825237994447		[learning rate: 0.0035378]
	Learning Rate: 0.00353783
	LOSS [training: 3.2149825237994447 | validation: 3.3089197254352736]
	TIME [epoch: 9.75 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1731477148350393		[learning rate: 0.003525]
	Learning Rate: 0.00352499
	LOSS [training: 3.1731477148350393 | validation: 3.3170666113402247]
	TIME [epoch: 9.74 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.142982898792242		[learning rate: 0.0035122]
	Learning Rate: 0.0035122
	LOSS [training: 3.142982898792242 | validation: 3.2845297397023234]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.18059897357374		[learning rate: 0.0034995]
	Learning Rate: 0.00349945
	LOSS [training: 3.18059897357374 | validation: 3.3838388444548846]
	TIME [epoch: 9.77 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.354643466903075		[learning rate: 0.0034868]
	Learning Rate: 0.00348675
	LOSS [training: 3.354643466903075 | validation: 3.3674091325468134]
	TIME [epoch: 9.75 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2314346900472812		[learning rate: 0.0034741]
	Learning Rate: 0.0034741
	LOSS [training: 3.2314346900472812 | validation: 3.442274404992437]
	TIME [epoch: 9.74 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.180460849249746		[learning rate: 0.0034615]
	Learning Rate: 0.00346149
	LOSS [training: 3.180460849249746 | validation: 3.3257488616040507]
	TIME [epoch: 9.75 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.152972638960932		[learning rate: 0.0034489]
	Learning Rate: 0.00344893
	LOSS [training: 3.152972638960932 | validation: 3.279170161359952]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1809118698806587		[learning rate: 0.0034364]
	Learning Rate: 0.00343641
	LOSS [training: 3.1809118698806587 | validation: 3.306519625191435]
	TIME [epoch: 9.74 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1232334333607454		[learning rate: 0.0034239]
	Learning Rate: 0.00342394
	LOSS [training: 3.1232334333607454 | validation: 3.2803898244902148]
	TIME [epoch: 9.74 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.127140279484509		[learning rate: 0.0034115]
	Learning Rate: 0.00341152
	LOSS [training: 3.127140279484509 | validation: 3.394777693528322]
	TIME [epoch: 9.75 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1493861508306966		[learning rate: 0.0033991]
	Learning Rate: 0.00339914
	LOSS [training: 3.1493861508306966 | validation: 3.294102253791381]
	TIME [epoch: 9.73 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.115949096501719		[learning rate: 0.0033868]
	Learning Rate: 0.0033868
	LOSS [training: 3.115949096501719 | validation: 3.5052860854834167]
	TIME [epoch: 9.74 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.20948810562997		[learning rate: 0.0033745]
	Learning Rate: 0.00337451
	LOSS [training: 3.20948810562997 | validation: 3.308241674284525]
	TIME [epoch: 9.75 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1273961098285663		[learning rate: 0.0033623]
	Learning Rate: 0.00336226
	LOSS [training: 3.1273961098285663 | validation: 3.4434358940898626]
	TIME [epoch: 9.74 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.111024072214702		[learning rate: 0.0033501]
	Learning Rate: 0.00335006
	LOSS [training: 3.111024072214702 | validation: 3.324979231366234]
	TIME [epoch: 9.74 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.118541344436042		[learning rate: 0.0033379]
	Learning Rate: 0.0033379
	LOSS [training: 3.118541344436042 | validation: 3.2983447333359353]
	TIME [epoch: 9.76 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0908166923608595		[learning rate: 0.0033258]
	Learning Rate: 0.00332579
	LOSS [training: 3.0908166923608595 | validation: 3.4102479471012637]
	TIME [epoch: 9.73 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.175588665269334		[learning rate: 0.0033137]
	Learning Rate: 0.00331372
	LOSS [training: 3.175588665269334 | validation: 3.28071212929396]
	TIME [epoch: 9.73 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.138687833336836		[learning rate: 0.0033017]
	Learning Rate: 0.00330169
	LOSS [training: 3.138687833336836 | validation: 3.304107964366995]
	TIME [epoch: 9.76 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.118518172009123		[learning rate: 0.0032897]
	Learning Rate: 0.00328971
	LOSS [training: 3.118518172009123 | validation: 3.4769794573224813]
	TIME [epoch: 9.73 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1081709582651738		[learning rate: 0.0032778]
	Learning Rate: 0.00327777
	LOSS [training: 3.1081709582651738 | validation: 3.584890307504923]
	TIME [epoch: 9.74 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.253579554462352		[learning rate: 0.0032659]
	Learning Rate: 0.00326588
	LOSS [training: 3.253579554462352 | validation: 3.4272429540990674]
	TIME [epoch: 9.76 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.148254449093435		[learning rate: 0.003254]
	Learning Rate: 0.00325403
	LOSS [training: 3.148254449093435 | validation: 3.5355166722183062]
	TIME [epoch: 9.73 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2065086009857806		[learning rate: 0.0032422]
	Learning Rate: 0.00324222
	LOSS [training: 3.2065086009857806 | validation: 3.2664213452825583]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_410.pth
	Model improved!!!
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1827413649096457		[learning rate: 0.0032305]
	Learning Rate: 0.00323045
	LOSS [training: 3.1827413649096457 | validation: 3.2852345283572744]
	TIME [epoch: 9.76 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.141648963141923		[learning rate: 0.0032187]
	Learning Rate: 0.00321873
	LOSS [training: 3.141648963141923 | validation: 3.32539217029828]
	TIME [epoch: 9.73 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.121037100801886		[learning rate: 0.003207]
	Learning Rate: 0.00320705
	LOSS [training: 3.121037100801886 | validation: 3.9458432095484692]
	TIME [epoch: 9.73 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.288622784941115		[learning rate: 0.0031954]
	Learning Rate: 0.00319541
	LOSS [training: 3.288622784941115 | validation: 3.267080414960503]
	TIME [epoch: 9.76 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1462161651313054		[learning rate: 0.0031838]
	Learning Rate: 0.00318381
	LOSS [training: 3.1462161651313054 | validation: 3.4271904421026558]
	TIME [epoch: 9.73 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.148497065406845		[learning rate: 0.0031723]
	Learning Rate: 0.00317226
	LOSS [training: 3.148497065406845 | validation: 3.324249561502394]
	TIME [epoch: 9.73 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1213785128310416		[learning rate: 0.0031607]
	Learning Rate: 0.00316075
	LOSS [training: 3.1213785128310416 | validation: 3.426681053086816]
	TIME [epoch: 9.75 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1607415023270717		[learning rate: 0.0031493]
	Learning Rate: 0.00314927
	LOSS [training: 3.1607415023270717 | validation: 3.3733950888749984]
	TIME [epoch: 9.73 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.164845459167332		[learning rate: 0.0031378]
	Learning Rate: 0.00313785
	LOSS [training: 3.164845459167332 | validation: 3.520003537439456]
	TIME [epoch: 9.72 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1785818183005716		[learning rate: 0.0031265]
	Learning Rate: 0.00312646
	LOSS [training: 3.1785818183005716 | validation: 3.4862422417274077]
	TIME [epoch: 9.75 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.113448904904804		[learning rate: 0.0031151]
	Learning Rate: 0.00311511
	LOSS [training: 3.113448904904804 | validation: 3.3244401245066695]
	TIME [epoch: 9.74 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.161714962258869		[learning rate: 0.0031038]
	Learning Rate: 0.00310381
	LOSS [training: 3.161714962258869 | validation: 3.3258429690985225]
	TIME [epoch: 9.74 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1114582760512386		[learning rate: 0.0030925]
	Learning Rate: 0.00309254
	LOSS [training: 3.1114582760512386 | validation: 3.3891393062596302]
	TIME [epoch: 9.76 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1261980403551455		[learning rate: 0.0030813]
	Learning Rate: 0.00308132
	LOSS [training: 3.1261980403551455 | validation: 3.7667188109531695]
	TIME [epoch: 9.74 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.222516447539981		[learning rate: 0.0030701]
	Learning Rate: 0.00307014
	LOSS [training: 3.222516447539981 | validation: 3.273750014025804]
	TIME [epoch: 9.73 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1741345044272276		[learning rate: 0.003059]
	Learning Rate: 0.003059
	LOSS [training: 3.1741345044272276 | validation: 3.3319329405078464]
	TIME [epoch: 9.75 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1949400169055413		[learning rate: 0.0030479]
	Learning Rate: 0.0030479
	LOSS [training: 3.1949400169055413 | validation: 3.3681882688982245]
	TIME [epoch: 9.74 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1756467108057116		[learning rate: 0.0030368]
	Learning Rate: 0.00303683
	LOSS [training: 3.1756467108057116 | validation: 3.3506862222162]
	TIME [epoch: 9.73 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1339745701144026		[learning rate: 0.0030258]
	Learning Rate: 0.00302581
	LOSS [training: 3.1339745701144026 | validation: 3.300409360814218]
	TIME [epoch: 9.74 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1094306552379445		[learning rate: 0.0030148]
	Learning Rate: 0.00301483
	LOSS [training: 3.1094306552379445 | validation: 3.294751609167712]
	TIME [epoch: 9.75 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.059763803801788		[learning rate: 0.0030039]
	Learning Rate: 0.00300389
	LOSS [training: 3.059763803801788 | validation: 3.261628865794114]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2255773449602723		[learning rate: 0.002993]
	Learning Rate: 0.00299299
	LOSS [training: 3.2255773449602723 | validation: 3.325751203553399]
	TIME [epoch: 9.74 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.110450795878152		[learning rate: 0.0029821]
	Learning Rate: 0.00298213
	LOSS [training: 3.110450795878152 | validation: 3.4335925129134615]
	TIME [epoch: 9.75 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1954795178985766		[learning rate: 0.0029713]
	Learning Rate: 0.00297131
	LOSS [training: 3.1954795178985766 | validation: 3.2804195049081977]
	TIME [epoch: 9.73 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1828684696881315		[learning rate: 0.0029605]
	Learning Rate: 0.00296052
	LOSS [training: 3.1828684696881315 | validation: 3.358687996668884]
	TIME [epoch: 9.74 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0907986208089095		[learning rate: 0.0029498]
	Learning Rate: 0.00294978
	LOSS [training: 3.0907986208089095 | validation: 3.4062463244979897]
	TIME [epoch: 9.75 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.178975114601964		[learning rate: 0.0029391]
	Learning Rate: 0.00293907
	LOSS [training: 3.178975114601964 | validation: 3.373412127872992]
	TIME [epoch: 9.73 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.061585636708593		[learning rate: 0.0029284]
	Learning Rate: 0.00292841
	LOSS [training: 3.061585636708593 | validation: 3.352240137659466]
	TIME [epoch: 9.73 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.093716190138185		[learning rate: 0.0029178]
	Learning Rate: 0.00291778
	LOSS [training: 3.093716190138185 | validation: 3.3143605121204467]
	TIME [epoch: 9.76 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.154073499566953		[learning rate: 0.0029072]
	Learning Rate: 0.00290719
	LOSS [training: 3.154073499566953 | validation: 3.315041713693615]
	TIME [epoch: 9.74 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1495900391738756		[learning rate: 0.0028966]
	Learning Rate: 0.00289664
	LOSS [training: 3.1495900391738756 | validation: 3.2947139322858177]
	TIME [epoch: 9.72 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1095157158191045		[learning rate: 0.0028861]
	Learning Rate: 0.00288613
	LOSS [training: 3.1095157158191045 | validation: 3.323475421372466]
	TIME [epoch: 9.75 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0873208780450754		[learning rate: 0.0028757]
	Learning Rate: 0.00287566
	LOSS [training: 3.0873208780450754 | validation: 3.413420954665521]
	TIME [epoch: 9.73 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0813433992672836		[learning rate: 0.0028652]
	Learning Rate: 0.00286522
	LOSS [training: 3.0813433992672836 | validation: 3.3145898204523903]
	TIME [epoch: 9.72 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1118918769222033		[learning rate: 0.0028548]
	Learning Rate: 0.00285482
	LOSS [training: 3.1118918769222033 | validation: 3.442602182624661]
	TIME [epoch: 9.75 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1449201299890723		[learning rate: 0.0028445]
	Learning Rate: 0.00284446
	LOSS [training: 3.1449201299890723 | validation: 3.3557547901996223]
	TIME [epoch: 9.73 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0951263997575134		[learning rate: 0.0028341]
	Learning Rate: 0.00283414
	LOSS [training: 3.0951263997575134 | validation: 3.2520411424255053]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_447.pth
	Model improved!!!
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1584430097136815		[learning rate: 0.0028239]
	Learning Rate: 0.00282385
	LOSS [training: 3.1584430097136815 | validation: 3.268636182743339]
	TIME [epoch: 9.76 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.060668629241879		[learning rate: 0.0028136]
	Learning Rate: 0.00281361
	LOSS [training: 3.060668629241879 | validation: 3.2810378795484896]
	TIME [epoch: 9.72 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.110577708031424		[learning rate: 0.0028034]
	Learning Rate: 0.00280339
	LOSS [training: 3.110577708031424 | validation: 3.315620306678776]
	TIME [epoch: 9.73 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1807292702313505		[learning rate: 0.0027932]
	Learning Rate: 0.00279322
	LOSS [training: 3.1807292702313505 | validation: 3.2859418931999085]
	TIME [epoch: 9.74 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.148144949404433		[learning rate: 0.0027831]
	Learning Rate: 0.00278308
	LOSS [training: 3.148144949404433 | validation: 3.358267821837473]
	TIME [epoch: 9.73 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2682073675072614		[learning rate: 0.002773]
	Learning Rate: 0.00277298
	LOSS [training: 3.2682073675072614 | validation: 3.8584103416547584]
	TIME [epoch: 9.71 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2294550974541183		[learning rate: 0.0027629]
	Learning Rate: 0.00276292
	LOSS [training: 3.2294550974541183 | validation: 3.2739382810181668]
	TIME [epoch: 9.74 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.198008260596283		[learning rate: 0.0027529]
	Learning Rate: 0.00275289
	LOSS [training: 3.198008260596283 | validation: 3.44636928766755]
	TIME [epoch: 9.72 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1358423675797806		[learning rate: 0.0027429]
	Learning Rate: 0.0027429
	LOSS [training: 3.1358423675797806 | validation: 3.281735691629068]
	TIME [epoch: 9.72 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0624660879524104		[learning rate: 0.0027329]
	Learning Rate: 0.00273295
	LOSS [training: 3.0624660879524104 | validation: 3.277365351761685]
	TIME [epoch: 9.74 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.105075026887318		[learning rate: 0.002723]
	Learning Rate: 0.00272303
	LOSS [training: 3.105075026887318 | validation: 3.3564676111041734]
	TIME [epoch: 9.73 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0808219034979945		[learning rate: 0.0027131]
	Learning Rate: 0.00271315
	LOSS [training: 3.0808219034979945 | validation: 3.2481804641606606]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_459.pth
	Model improved!!!
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.154768565449082		[learning rate: 0.0027033]
	Learning Rate: 0.0027033
	LOSS [training: 3.154768565449082 | validation: 3.3475050856323265]
	TIME [epoch: 9.74 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.089318532259937		[learning rate: 0.0026935]
	Learning Rate: 0.00269349
	LOSS [training: 3.089318532259937 | validation: 3.5255029222072642]
	TIME [epoch: 9.72 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.136109175684619		[learning rate: 0.0026837]
	Learning Rate: 0.00268372
	LOSS [training: 3.136109175684619 | validation: 3.318363783334478]
	TIME [epoch: 9.72 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.090698906790668		[learning rate: 0.002674]
	Learning Rate: 0.00267398
	LOSS [training: 3.090698906790668 | validation: 3.499634936040179]
	TIME [epoch: 9.74 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1910205452046583		[learning rate: 0.0026643]
	Learning Rate: 0.00266427
	LOSS [training: 3.1910205452046583 | validation: 3.4779483621310794]
	TIME [epoch: 9.74 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2149493659139465		[learning rate: 0.0026546]
	Learning Rate: 0.00265461
	LOSS [training: 3.2149493659139465 | validation: 3.2789907487775443]
	TIME [epoch: 9.73 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0976938060112404		[learning rate: 0.002645]
	Learning Rate: 0.00264497
	LOSS [training: 3.0976938060112404 | validation: 3.305953415196889]
	TIME [epoch: 9.73 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.136496851494863		[learning rate: 0.0026354]
	Learning Rate: 0.00263537
	LOSS [training: 3.136496851494863 | validation: 3.350214400855764]
	TIME [epoch: 9.73 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0857822768812553		[learning rate: 0.0026258]
	Learning Rate: 0.00262581
	LOSS [training: 3.0857822768812553 | validation: 3.2511134601790532]
	TIME [epoch: 9.72 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0773114314233316		[learning rate: 0.0026163]
	Learning Rate: 0.00261628
	LOSS [training: 3.0773114314233316 | validation: 3.3163429452782993]
	TIME [epoch: 9.72 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1051971350358527		[learning rate: 0.0026068]
	Learning Rate: 0.00260679
	LOSS [training: 3.1051971350358527 | validation: 3.243655752884278]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.041168462348146		[learning rate: 0.0025973]
	Learning Rate: 0.00259733
	LOSS [training: 3.041168462348146 | validation: 3.254122990915314]
	TIME [epoch: 9.73 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0799627359851174		[learning rate: 0.0025879]
	Learning Rate: 0.0025879
	LOSS [training: 3.0799627359851174 | validation: 3.2505265555436753]
	TIME [epoch: 9.74 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.102654003750095		[learning rate: 0.0025785]
	Learning Rate: 0.00257851
	LOSS [training: 3.102654003750095 | validation: 3.4092938144924934]
	TIME [epoch: 9.72 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.083594634835798		[learning rate: 0.0025691]
	Learning Rate: 0.00256915
	LOSS [training: 3.083594634835798 | validation: 3.336277635160649]
	TIME [epoch: 9.74 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.110415594459153		[learning rate: 0.0025598]
	Learning Rate: 0.00255983
	LOSS [training: 3.110415594459153 | validation: 3.7235047566937713]
	TIME [epoch: 9.73 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2136853729912715		[learning rate: 0.0025505]
	Learning Rate: 0.00255054
	LOSS [training: 3.2136853729912715 | validation: 3.2863845679211123]
	TIME [epoch: 9.75 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.088926603822335		[learning rate: 0.0025413]
	Learning Rate: 0.00254128
	LOSS [training: 3.088926603822335 | validation: 3.27450669421635]
	TIME [epoch: 9.71 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.062878421172885		[learning rate: 0.0025321]
	Learning Rate: 0.00253206
	LOSS [training: 3.062878421172885 | validation: 3.24316427781077]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1189037575004437		[learning rate: 0.0025229]
	Learning Rate: 0.00252287
	LOSS [training: 3.1189037575004437 | validation: 3.5382957552976206]
	TIME [epoch: 9.75 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1647386482873605		[learning rate: 0.0025137]
	Learning Rate: 0.00251371
	LOSS [training: 3.1647386482873605 | validation: 3.390825677231789]
	TIME [epoch: 9.73 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0781378888943216		[learning rate: 0.0025046]
	Learning Rate: 0.00250459
	LOSS [training: 3.0781378888943216 | validation: 3.4078999656291855]
	TIME [epoch: 9.72 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.096575047182843		[learning rate: 0.0024955]
	Learning Rate: 0.0024955
	LOSS [training: 3.096575047182843 | validation: 3.4413497499134453]
	TIME [epoch: 9.74 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0925999016394172		[learning rate: 0.0024864]
	Learning Rate: 0.00248645
	LOSS [training: 3.0925999016394172 | validation: 3.7010744983754846]
	TIME [epoch: 9.72 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1959276014519924		[learning rate: 0.0024774]
	Learning Rate: 0.00247742
	LOSS [training: 3.1959276014519924 | validation: 3.406279092941036]
	TIME [epoch: 9.7 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.094778592835339		[learning rate: 0.0024684]
	Learning Rate: 0.00246843
	LOSS [training: 3.094778592835339 | validation: 3.27556557917515]
	TIME [epoch: 9.74 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0720418708496307		[learning rate: 0.0024595]
	Learning Rate: 0.00245947
	LOSS [training: 3.0720418708496307 | validation: 3.2629315790140305]
	TIME [epoch: 9.71 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.042635971483523		[learning rate: 0.0024505]
	Learning Rate: 0.00245055
	LOSS [training: 3.042635971483523 | validation: 3.390832691831978]
	TIME [epoch: 9.72 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.067994641072518		[learning rate: 0.0024417]
	Learning Rate: 0.00244165
	LOSS [training: 3.067994641072518 | validation: 3.2396877843857785]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0274803397314054		[learning rate: 0.0024328]
	Learning Rate: 0.00243279
	LOSS [training: 3.0274803397314054 | validation: 3.3737638077522174]
	TIME [epoch: 9.73 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2349893652293256		[learning rate: 0.002424]
	Learning Rate: 0.00242396
	LOSS [training: 3.2349893652293256 | validation: 3.4995700431008605]
	TIME [epoch: 9.71 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1187430201485666		[learning rate: 0.0024152]
	Learning Rate: 0.00241517
	LOSS [training: 3.1187430201485666 | validation: 3.337667566100574]
	TIME [epoch: 9.75 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.125592207328622		[learning rate: 0.0024064]
	Learning Rate: 0.0024064
	LOSS [training: 3.125592207328622 | validation: 3.3587382384048388]
	TIME [epoch: 9.73 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.068628699366319		[learning rate: 0.0023977]
	Learning Rate: 0.00239767
	LOSS [training: 3.068628699366319 | validation: 3.292297558662866]
	TIME [epoch: 9.72 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0752912744831744		[learning rate: 0.002389]
	Learning Rate: 0.00238897
	LOSS [training: 3.0752912744831744 | validation: 3.2507956927858954]
	TIME [epoch: 9.75 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.141094054748174		[learning rate: 0.0023803]
	Learning Rate: 0.0023803
	LOSS [training: 3.141094054748174 | validation: 3.2629876323477993]
	TIME [epoch: 9.72 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0660078448480528		[learning rate: 0.0023717]
	Learning Rate: 0.00237166
	LOSS [training: 3.0660078448480528 | validation: 3.310447762593654]
	TIME [epoch: 9.73 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.034739421893064		[learning rate: 0.0023631]
	Learning Rate: 0.00236305
	LOSS [training: 3.034739421893064 | validation: 3.339258144938175]
	TIME [epoch: 9.73 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.102735469866574		[learning rate: 0.0023545]
	Learning Rate: 0.00235448
	LOSS [training: 3.102735469866574 | validation: 3.309936232649625]
	TIME [epoch: 9.73 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0478880086002977		[learning rate: 0.0023459]
	Learning Rate: 0.00234593
	LOSS [training: 3.0478880086002977 | validation: 3.2375566368155932]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_499.pth
	Model improved!!!
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.030970252290539		[learning rate: 0.0023374]
	Learning Rate: 0.00233742
	LOSS [training: 3.030970252290539 | validation: 3.370688176132601]
	TIME [epoch: 9.75 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.224319468520696		[learning rate: 0.0023289]
	Learning Rate: 0.00232894
	LOSS [training: 3.224319468520696 | validation: 3.2667642875984155]
	TIME [epoch: 9.73 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1103721458739657		[learning rate: 0.0023205]
	Learning Rate: 0.00232049
	LOSS [training: 3.1103721458739657 | validation: 3.2374293521460427]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_502.pth
	Model improved!!!
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.041613144438184		[learning rate: 0.0023121]
	Learning Rate: 0.00231206
	LOSS [training: 3.041613144438184 | validation: 3.307950335957764]
	TIME [epoch: 9.76 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.129845235794355		[learning rate: 0.0023037]
	Learning Rate: 0.00230367
	LOSS [training: 3.129845235794355 | validation: 3.2678668499493813]
	TIME [epoch: 9.75 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0409596564200463		[learning rate: 0.0022953]
	Learning Rate: 0.00229531
	LOSS [training: 3.0409596564200463 | validation: 3.246559120880484]
	TIME [epoch: 9.74 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0699344396547374		[learning rate: 0.002287]
	Learning Rate: 0.00228698
	LOSS [training: 3.0699344396547374 | validation: 3.231345819590524]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_506.pth
	Model improved!!!
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1632227682367473		[learning rate: 0.0022787]
	Learning Rate: 0.00227868
	LOSS [training: 3.1632227682367473 | validation: 3.293725857801885]
	TIME [epoch: 9.76 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0582619682929417		[learning rate: 0.0022704]
	Learning Rate: 0.00227042
	LOSS [training: 3.0582619682929417 | validation: 3.33819911242466]
	TIME [epoch: 9.74 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.058511577015803		[learning rate: 0.0022622]
	Learning Rate: 0.00226218
	LOSS [training: 3.058511577015803 | validation: 3.245963054755755]
	TIME [epoch: 9.74 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0620487651944757		[learning rate: 0.002254]
	Learning Rate: 0.00225397
	LOSS [training: 3.0620487651944757 | validation: 3.2786904696034482]
	TIME [epoch: 9.76 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0794331227955904		[learning rate: 0.0022458]
	Learning Rate: 0.00224579
	LOSS [training: 3.0794331227955904 | validation: 3.2707650919988813]
	TIME [epoch: 9.74 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0613079105963212		[learning rate: 0.0022376]
	Learning Rate: 0.00223764
	LOSS [training: 3.0613079105963212 | validation: 3.244291955225807]
	TIME [epoch: 9.74 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.043428375774089		[learning rate: 0.0022295]
	Learning Rate: 0.00222952
	LOSS [training: 3.043428375774089 | validation: 3.3505211004580135]
	TIME [epoch: 9.76 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.06180865523497		[learning rate: 0.0022214]
	Learning Rate: 0.00222142
	LOSS [training: 3.06180865523497 | validation: 3.336675386339516]
	TIME [epoch: 9.74 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1091303102679326		[learning rate: 0.0022134]
	Learning Rate: 0.00221336
	LOSS [training: 3.1091303102679326 | validation: 3.3038139523643655]
	TIME [epoch: 9.74 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.038209682137752		[learning rate: 0.0022053]
	Learning Rate: 0.00220533
	LOSS [training: 3.038209682137752 | validation: 3.2725563035897314]
	TIME [epoch: 9.77 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.058742375547871		[learning rate: 0.0021973]
	Learning Rate: 0.00219733
	LOSS [training: 3.058742375547871 | validation: 3.2970118220389977]
	TIME [epoch: 9.74 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0514941686779395		[learning rate: 0.0021894]
	Learning Rate: 0.00218935
	LOSS [training: 3.0514941686779395 | validation: 3.270872929727434]
	TIME [epoch: 9.74 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.070559580489909		[learning rate: 0.0021814]
	Learning Rate: 0.00218141
	LOSS [training: 3.070559580489909 | validation: 3.2880001394344647]
	TIME [epoch: 9.76 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.025144139799997		[learning rate: 0.0021735]
	Learning Rate: 0.00217349
	LOSS [training: 3.025144139799997 | validation: 3.2683878480327313]
	TIME [epoch: 9.73 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0706236157319813		[learning rate: 0.0021656]
	Learning Rate: 0.0021656
	LOSS [training: 3.0706236157319813 | validation: 3.2522073817268167]
	TIME [epoch: 9.74 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0533107819154326		[learning rate: 0.0021577]
	Learning Rate: 0.00215774
	LOSS [training: 3.0533107819154326 | validation: 3.289196433715714]
	TIME [epoch: 9.76 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0537109367191615		[learning rate: 0.0021499]
	Learning Rate: 0.00214991
	LOSS [training: 3.0537109367191615 | validation: 3.3124322084729108]
	TIME [epoch: 9.74 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.054746296162837		[learning rate: 0.0021421]
	Learning Rate: 0.00214211
	LOSS [training: 3.054746296162837 | validation: 3.3455310644812126]
	TIME [epoch: 9.74 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.06323546723993		[learning rate: 0.0021343]
	Learning Rate: 0.00213434
	LOSS [training: 3.06323546723993 | validation: 3.271195040489746]
	TIME [epoch: 9.76 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0707380873041346		[learning rate: 0.0021266]
	Learning Rate: 0.00212659
	LOSS [training: 3.0707380873041346 | validation: 3.3050788622442044]
	TIME [epoch: 9.73 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0948371967769854		[learning rate: 0.0021189]
	Learning Rate: 0.00211887
	LOSS [training: 3.0948371967769854 | validation: 3.817171698820996]
	TIME [epoch: 9.74 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2156118606064537		[learning rate: 0.0021112]
	Learning Rate: 0.00211119
	LOSS [training: 3.2156118606064537 | validation: 3.3484532707560666]
	TIME [epoch: 9.76 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0694163110090478		[learning rate: 0.0021035]
	Learning Rate: 0.00210352
	LOSS [training: 3.0694163110090478 | validation: 3.2624831659239897]
	TIME [epoch: 9.74 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.03113687115248		[learning rate: 0.0020959]
	Learning Rate: 0.00209589
	LOSS [training: 3.03113687115248 | validation: 3.230574638575342]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_530.pth
	Model improved!!!
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.042274744354754		[learning rate: 0.0020883]
	Learning Rate: 0.00208828
	LOSS [training: 3.042274744354754 | validation: 3.455726098624682]
	TIME [epoch: 9.75 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.121691973233283		[learning rate: 0.0020807]
	Learning Rate: 0.00208071
	LOSS [training: 3.121691973233283 | validation: 3.3262207894798554]
	TIME [epoch: 9.73 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0939729437177332		[learning rate: 0.0020732]
	Learning Rate: 0.00207315
	LOSS [training: 3.0939729437177332 | validation: 3.2593213512361365]
	TIME [epoch: 9.73 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.035212720330061		[learning rate: 0.0020656]
	Learning Rate: 0.00206563
	LOSS [training: 3.035212720330061 | validation: 3.2672138403396507]
	TIME [epoch: 9.75 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.074744691962585		[learning rate: 0.0020581]
	Learning Rate: 0.00205813
	LOSS [training: 3.074744691962585 | validation: 3.258147621029443]
	TIME [epoch: 9.73 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1014545025142275		[learning rate: 0.0020507]
	Learning Rate: 0.00205067
	LOSS [training: 3.1014545025142275 | validation: 3.2542344015423375]
	TIME [epoch: 9.73 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0350581514994444		[learning rate: 0.0020432]
	Learning Rate: 0.00204322
	LOSS [training: 3.0350581514994444 | validation: 3.259683190599357]
	TIME [epoch: 9.74 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0489228813250104		[learning rate: 0.0020358]
	Learning Rate: 0.00203581
	LOSS [training: 3.0489228813250104 | validation: 3.281011289752346]
	TIME [epoch: 9.73 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.037844977506555		[learning rate: 0.0020284]
	Learning Rate: 0.00202842
	LOSS [training: 3.037844977506555 | validation: 3.2693652659730765]
	TIME [epoch: 9.72 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.043252235508599		[learning rate: 0.0020211]
	Learning Rate: 0.00202106
	LOSS [training: 3.043252235508599 | validation: 3.2528308290699433]
	TIME [epoch: 9.75 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0341658405830136		[learning rate: 0.0020137]
	Learning Rate: 0.00201372
	LOSS [training: 3.0341658405830136 | validation: 3.2208832698814382]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.044964507832		[learning rate: 0.0020064]
	Learning Rate: 0.00200642
	LOSS [training: 3.044964507832 | validation: 3.2488607334506043]
	TIME [epoch: 9.72 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.092382196038648		[learning rate: 0.0019991]
	Learning Rate: 0.00199913
	LOSS [training: 3.092382196038648 | validation: 3.233788431725798]
	TIME [epoch: 9.75 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0786310886151824		[learning rate: 0.0019919]
	Learning Rate: 0.00199188
	LOSS [training: 3.0786310886151824 | validation: 3.4704296815106375]
	TIME [epoch: 9.73 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0823485927820835		[learning rate: 0.0019847]
	Learning Rate: 0.00198465
	LOSS [training: 3.0823485927820835 | validation: 3.2637797656748537]
	TIME [epoch: 9.73 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0309346317873755		[learning rate: 0.0019774]
	Learning Rate: 0.00197745
	LOSS [training: 3.0309346317873755 | validation: 3.3064994351629746]
	TIME [epoch: 9.74 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0265758299989662		[learning rate: 0.0019703]
	Learning Rate: 0.00197027
	LOSS [training: 3.0265758299989662 | validation: 3.2467620243616544]
	TIME [epoch: 9.74 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0452845323547186		[learning rate: 0.0019631]
	Learning Rate: 0.00196312
	LOSS [training: 3.0452845323547186 | validation: 3.2547773400995617]
	TIME [epoch: 9.73 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0516438811065134		[learning rate: 0.001956]
	Learning Rate: 0.001956
	LOSS [training: 3.0516438811065134 | validation: 3.27336630924482]
	TIME [epoch: 9.74 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.058773881394899		[learning rate: 0.0019489]
	Learning Rate: 0.0019489
	LOSS [training: 3.058773881394899 | validation: 3.3734643521932868]
	TIME [epoch: 9.75 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0557015747252216		[learning rate: 0.0019418]
	Learning Rate: 0.00194183
	LOSS [training: 3.0557015747252216 | validation: 3.248157231562276]
	TIME [epoch: 9.73 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1171600458197757		[learning rate: 0.0019348]
	Learning Rate: 0.00193478
	LOSS [training: 3.1171600458197757 | validation: 3.4936534264996904]
	TIME [epoch: 9.73 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.082467069145062		[learning rate: 0.0019278]
	Learning Rate: 0.00192776
	LOSS [training: 3.082467069145062 | validation: 3.344401828628039]
	TIME [epoch: 9.75 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.040189322078816		[learning rate: 0.0019208]
	Learning Rate: 0.00192076
	LOSS [training: 3.040189322078816 | validation: 3.31966873083212]
	TIME [epoch: 9.72 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.031126954173339		[learning rate: 0.0019138]
	Learning Rate: 0.00191379
	LOSS [training: 3.031126954173339 | validation: 3.2417748247605274]
	TIME [epoch: 9.73 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0290463318881136		[learning rate: 0.0019068]
	Learning Rate: 0.00190685
	LOSS [training: 3.0290463318881136 | validation: 3.3837082113437784]
	TIME [epoch: 9.76 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1864454919160403		[learning rate: 0.0018999]
	Learning Rate: 0.00189993
	LOSS [training: 3.1864454919160403 | validation: 3.402836630073349]
	TIME [epoch: 9.73 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.071739463023023		[learning rate: 0.001893]
	Learning Rate: 0.00189303
	LOSS [training: 3.071739463023023 | validation: 3.3032344253227355]
	TIME [epoch: 9.72 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.018163218016869		[learning rate: 0.0018862]
	Learning Rate: 0.00188616
	LOSS [training: 3.018163218016869 | validation: 3.4252000816910164]
	TIME [epoch: 9.76 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0840784218791604		[learning rate: 0.0018793]
	Learning Rate: 0.00187932
	LOSS [training: 3.0840784218791604 | validation: 3.2321120959710186]
	TIME [epoch: 9.73 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.029967658590982		[learning rate: 0.0018725]
	Learning Rate: 0.0018725
	LOSS [training: 3.029967658590982 | validation: 3.2349258865617116]
	TIME [epoch: 9.73 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0266791802395487		[learning rate: 0.0018657]
	Learning Rate: 0.0018657
	LOSS [training: 3.0266791802395487 | validation: 3.300589230933974]
	TIME [epoch: 9.76 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.062253446005499		[learning rate: 0.0018589]
	Learning Rate: 0.00185893
	LOSS [training: 3.062253446005499 | validation: 3.285156792940008]
	TIME [epoch: 9.72 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.034622718579926		[learning rate: 0.0018522]
	Learning Rate: 0.00185218
	LOSS [training: 3.034622718579926 | validation: 3.2500920039795598]
	TIME [epoch: 9.72 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.014914912127891		[learning rate: 0.0018455]
	Learning Rate: 0.00184546
	LOSS [training: 3.014914912127891 | validation: 3.2169302179689874]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_565.pth
	Model improved!!!
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0280794881830313		[learning rate: 0.0018388]
	Learning Rate: 0.00183877
	LOSS [training: 3.0280794881830313 | validation: 3.2595460126447264]
	TIME [epoch: 9.73 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.030212587876321		[learning rate: 0.0018321]
	Learning Rate: 0.00183209
	LOSS [training: 3.030212587876321 | validation: 3.240356902298357]
	TIME [epoch: 9.72 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.032170744352744		[learning rate: 0.0018254]
	Learning Rate: 0.00182544
	LOSS [training: 3.032170744352744 | validation: 3.2516081614382695]
	TIME [epoch: 9.74 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0830603441362325		[learning rate: 0.0018188]
	Learning Rate: 0.00181882
	LOSS [training: 3.0830603441362325 | validation: 3.2991545844636723]
	TIME [epoch: 9.73 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0373042038084366		[learning rate: 0.0018122]
	Learning Rate: 0.00181222
	LOSS [training: 3.0373042038084366 | validation: 3.251102784144755]
	TIME [epoch: 9.72 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.035923201780376		[learning rate: 0.0018056]
	Learning Rate: 0.00180564
	LOSS [training: 3.035923201780376 | validation: 3.258022545827305]
	TIME [epoch: 9.74 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0616312318457726		[learning rate: 0.0017991]
	Learning Rate: 0.00179909
	LOSS [training: 3.0616312318457726 | validation: 3.2565476361438717]
	TIME [epoch: 9.72 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.00760650651449		[learning rate: 0.0017926]
	Learning Rate: 0.00179256
	LOSS [training: 3.00760650651449 | validation: 3.28105635384773]
	TIME [epoch: 9.72 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.014424650097632		[learning rate: 0.0017861]
	Learning Rate: 0.00178605
	LOSS [training: 3.014424650097632 | validation: 3.2764668820927114]
	TIME [epoch: 9.75 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.024882199634425		[learning rate: 0.0017796]
	Learning Rate: 0.00177957
	LOSS [training: 3.024882199634425 | validation: 3.306117862815878]
	TIME [epoch: 9.73 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.026167434799792		[learning rate: 0.0017731]
	Learning Rate: 0.00177311
	LOSS [training: 3.026167434799792 | validation: 3.252112969993316]
	TIME [epoch: 9.73 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0156146349615343		[learning rate: 0.0017667]
	Learning Rate: 0.00176668
	LOSS [training: 3.0156146349615343 | validation: 3.2473447251419407]
	TIME [epoch: 9.74 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0099270790239587		[learning rate: 0.0017603]
	Learning Rate: 0.00176027
	LOSS [training: 3.0099270790239587 | validation: 3.3384806578389545]
	TIME [epoch: 9.74 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0480997778650907		[learning rate: 0.0017539]
	Learning Rate: 0.00175388
	LOSS [training: 3.0480997778650907 | validation: 3.2980834949609155]
	TIME [epoch: 9.72 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.099265665615365		[learning rate: 0.0017475]
	Learning Rate: 0.00174752
	LOSS [training: 3.099265665615365 | validation: 3.2464443588007823]
	TIME [epoch: 9.73 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0938586579123797		[learning rate: 0.0017412]
	Learning Rate: 0.00174117
	LOSS [training: 3.0938586579123797 | validation: 3.327425640848696]
	TIME [epoch: 9.74 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0488580495079134		[learning rate: 0.0017349]
	Learning Rate: 0.00173486
	LOSS [training: 3.0488580495079134 | validation: 3.2854992024677645]
	TIME [epoch: 9.73 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.018270366678783		[learning rate: 0.0017286]
	Learning Rate: 0.00172856
	LOSS [training: 3.018270366678783 | validation: 3.2612871629356364]
	TIME [epoch: 9.73 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.023029940153521		[learning rate: 0.0017223]
	Learning Rate: 0.00172229
	LOSS [training: 3.023029940153521 | validation: 3.3071402339383464]
	TIME [epoch: 9.73 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.037153740752564		[learning rate: 0.001716]
	Learning Rate: 0.00171604
	LOSS [training: 3.037153740752564 | validation: 3.3741538330860386]
	TIME [epoch: 9.72 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0736995292730898		[learning rate: 0.0017098]
	Learning Rate: 0.00170981
	LOSS [training: 3.0736995292730898 | validation: 3.3502078905526096]
	TIME [epoch: 9.72 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.058244316763184		[learning rate: 0.0017036]
	Learning Rate: 0.0017036
	LOSS [training: 3.058244316763184 | validation: 3.3361197050191307]
	TIME [epoch: 9.74 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.092092813824346		[learning rate: 0.0016974]
	Learning Rate: 0.00169742
	LOSS [training: 3.092092813824346 | validation: 3.2642818938897027]
	TIME [epoch: 9.72 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.023629055364546		[learning rate: 0.0016913]
	Learning Rate: 0.00169126
	LOSS [training: 3.023629055364546 | validation: 3.240851286366236]
	TIME [epoch: 9.72 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.007117022418185		[learning rate: 0.0016851]
	Learning Rate: 0.00168512
	LOSS [training: 3.007117022418185 | validation: 3.360843110884861]
	TIME [epoch: 9.75 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.050544414484188		[learning rate: 0.001679]
	Learning Rate: 0.00167901
	LOSS [training: 3.050544414484188 | validation: 3.3897355340384445]
	TIME [epoch: 9.72 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0591296378440394		[learning rate: 0.0016729]
	Learning Rate: 0.00167291
	LOSS [training: 3.0591296378440394 | validation: 3.2352572142570057]
	TIME [epoch: 9.72 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.081183847137266		[learning rate: 0.0016668]
	Learning Rate: 0.00166684
	LOSS [training: 3.081183847137266 | validation: 3.3019168361808062]
	TIME [epoch: 9.74 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.134832857804663		[learning rate: 0.0016608]
	Learning Rate: 0.00166079
	LOSS [training: 3.134832857804663 | validation: 3.2821910025916354]
	TIME [epoch: 9.72 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0544172845895243		[learning rate: 0.0016548]
	Learning Rate: 0.00165477
	LOSS [training: 3.0544172845895243 | validation: 3.2585505356033333]
	TIME [epoch: 9.73 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0213659523647394		[learning rate: 0.0016488]
	Learning Rate: 0.00164876
	LOSS [training: 3.0213659523647394 | validation: 3.226342210839286]
	TIME [epoch: 9.74 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0122869558010446		[learning rate: 0.0016428]
	Learning Rate: 0.00164278
	LOSS [training: 3.0122869558010446 | validation: 3.340326751506091]
	TIME [epoch: 9.72 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0629162501807796		[learning rate: 0.0016368]
	Learning Rate: 0.00163682
	LOSS [training: 3.0629162501807796 | validation: 3.2403473095172366]
	TIME [epoch: 9.72 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0410887710770225		[learning rate: 0.0016309]
	Learning Rate: 0.00163088
	LOSS [training: 3.0410887710770225 | validation: 3.2637135703857103]
	TIME [epoch: 9.75 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0386720641283147		[learning rate: 0.001625]
	Learning Rate: 0.00162496
	LOSS [training: 3.0386720641283147 | validation: 3.2375887130518897]
	TIME [epoch: 9.72 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.041378292706412		[learning rate: 0.0016191]
	Learning Rate: 0.00161906
	LOSS [training: 3.041378292706412 | validation: 3.2721495787433112]
	TIME [epoch: 9.72 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0620325538996314		[learning rate: 0.0016132]
	Learning Rate: 0.00161319
	LOSS [training: 3.0620325538996314 | validation: 3.2408001265489617]
	TIME [epoch: 9.75 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0331655638021706		[learning rate: 0.0016073]
	Learning Rate: 0.00160733
	LOSS [training: 3.0331655638021706 | validation: 3.2525817021810544]
	TIME [epoch: 9.72 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0297361693606346		[learning rate: 0.0016015]
	Learning Rate: 0.0016015
	LOSS [training: 3.0297361693606346 | validation: 3.2379974127256514]
	TIME [epoch: 9.72 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.018366879191441		[learning rate: 0.0015957]
	Learning Rate: 0.00159569
	LOSS [training: 3.018366879191441 | validation: 3.261276128886767]
	TIME [epoch: 9.74 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0144043154391644		[learning rate: 0.0015899]
	Learning Rate: 0.00158989
	LOSS [training: 3.0144043154391644 | validation: 3.2659701978589006]
	TIME [epoch: 9.73 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0254517164139516		[learning rate: 0.0015841]
	Learning Rate: 0.00158413
	LOSS [training: 3.0254517164139516 | validation: 3.2570510306227862]
	TIME [epoch: 9.72 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0215049055544076		[learning rate: 0.0015784]
	Learning Rate: 0.00157838
	LOSS [training: 3.0215049055544076 | validation: 3.2937644005310323]
	TIME [epoch: 9.74 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0264697668206546		[learning rate: 0.0015726]
	Learning Rate: 0.00157265
	LOSS [training: 3.0264697668206546 | validation: 3.324675561898616]
	TIME [epoch: 9.72 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.053470937325231		[learning rate: 0.0015669]
	Learning Rate: 0.00156694
	LOSS [training: 3.053470937325231 | validation: 3.272850581346211]
	TIME [epoch: 9.72 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.017699446566284		[learning rate: 0.0015613]
	Learning Rate: 0.00156125
	LOSS [training: 3.017699446566284 | validation: 3.221288933301555]
	TIME [epoch: 9.72 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0136408261823773		[learning rate: 0.0015556]
	Learning Rate: 0.00155559
	LOSS [training: 3.0136408261823773 | validation: 3.2568020523140206]
	TIME [epoch: 9.74 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0255584960419437		[learning rate: 0.0015499]
	Learning Rate: 0.00154994
	LOSS [training: 3.0255584960419437 | validation: 3.3432801987310983]
	TIME [epoch: 9.72 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.054415520951399		[learning rate: 0.0015443]
	Learning Rate: 0.00154432
	LOSS [training: 3.054415520951399 | validation: 3.2878941096007934]
	TIME [epoch: 9.73 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0294671874872225		[learning rate: 0.0015387]
	Learning Rate: 0.00153871
	LOSS [training: 3.0294671874872225 | validation: 3.2727353583879797]
	TIME [epoch: 9.74 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0804170294700737		[learning rate: 0.0015331]
	Learning Rate: 0.00153313
	LOSS [training: 3.0804170294700737 | validation: 3.2417232861395315]
	TIME [epoch: 9.72 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0078719116035186		[learning rate: 0.0015276]
	Learning Rate: 0.00152757
	LOSS [training: 3.0078719116035186 | validation: 3.261371325862638]
	TIME [epoch: 9.71 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0251219582355566		[learning rate: 0.001522]
	Learning Rate: 0.00152202
	LOSS [training: 3.0251219582355566 | validation: 3.2727928964380126]
	TIME [epoch: 9.75 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.999448703981521		[learning rate: 0.0015165]
	Learning Rate: 0.0015165
	LOSS [training: 2.999448703981521 | validation: 3.2589960412944965]
	TIME [epoch: 9.71 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1254401646850565		[learning rate: 0.001511]
	Learning Rate: 0.001511
	LOSS [training: 3.1254401646850565 | validation: 3.344528272034623]
	TIME [epoch: 9.72 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0416106435471018		[learning rate: 0.0015055]
	Learning Rate: 0.00150551
	LOSS [training: 3.0416106435471018 | validation: 3.2278518748415412]
	TIME [epoch: 9.74 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0129618417861606		[learning rate: 0.0015]
	Learning Rate: 0.00150005
	LOSS [training: 3.0129618417861606 | validation: 3.26623339594731]
	TIME [epoch: 9.72 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0140414505221735		[learning rate: 0.0014946]
	Learning Rate: 0.0014946
	LOSS [training: 3.0140414505221735 | validation: 3.2160624559710507]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_623.pth
	Model improved!!!
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0058674093722644		[learning rate: 0.0014892]
	Learning Rate: 0.00148918
	LOSS [training: 3.0058674093722644 | validation: 3.254263566954552]
	TIME [epoch: 9.75 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.034093010142987		[learning rate: 0.0014838]
	Learning Rate: 0.00148378
	LOSS [training: 3.034093010142987 | validation: 3.2889792241851037]
	TIME [epoch: 9.72 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0215313428952753		[learning rate: 0.0014784]
	Learning Rate: 0.00147839
	LOSS [training: 3.0215313428952753 | validation: 3.224492988744162]
	TIME [epoch: 9.72 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0235538728640066		[learning rate: 0.001473]
	Learning Rate: 0.00147303
	LOSS [training: 3.0235538728640066 | validation: 3.2664519055235197]
	TIME [epoch: 9.74 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.996868920661123		[learning rate: 0.0014677]
	Learning Rate: 0.00146768
	LOSS [training: 2.996868920661123 | validation: 3.258629896235336]
	TIME [epoch: 9.72 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.024429817920098		[learning rate: 0.0014624]
	Learning Rate: 0.00146235
	LOSS [training: 3.024429817920098 | validation: 3.251334932687064]
	TIME [epoch: 9.73 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.035015709954231		[learning rate: 0.001457]
	Learning Rate: 0.00145705
	LOSS [training: 3.035015709954231 | validation: 3.274979536453953]
	TIME [epoch: 9.75 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.016320798512965		[learning rate: 0.0014518]
	Learning Rate: 0.00145176
	LOSS [training: 3.016320798512965 | validation: 3.250631777204991]
	TIME [epoch: 9.7 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.999972716056785		[learning rate: 0.0014465]
	Learning Rate: 0.00144649
	LOSS [training: 2.999972716056785 | validation: 3.2391133934929486]
	TIME [epoch: 9.72 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.01491371704023		[learning rate: 0.0014412]
	Learning Rate: 0.00144124
	LOSS [training: 3.01491371704023 | validation: 3.2487668281277546]
	TIME [epoch: 9.75 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.006864539775367		[learning rate: 0.001436]
	Learning Rate: 0.00143601
	LOSS [training: 3.006864539775367 | validation: 3.255004057718594]
	TIME [epoch: 9.71 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0805732123373497		[learning rate: 0.0014308]
	Learning Rate: 0.0014308
	LOSS [training: 3.0805732123373497 | validation: 3.24807663408058]
	TIME [epoch: 9.72 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.994915168218781		[learning rate: 0.0014256]
	Learning Rate: 0.00142561
	LOSS [training: 2.994915168218781 | validation: 3.2360843963581103]
	TIME [epoch: 9.73 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9935634698125773		[learning rate: 0.0014204]
	Learning Rate: 0.00142043
	LOSS [training: 2.9935634698125773 | validation: 3.2383258777236748]
	TIME [epoch: 9.73 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9944550846631004		[learning rate: 0.0014153]
	Learning Rate: 0.00141528
	LOSS [training: 2.9944550846631004 | validation: 3.251750345144747]
	TIME [epoch: 9.72 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0116433948966725		[learning rate: 0.0014101]
	Learning Rate: 0.00141014
	LOSS [training: 3.0116433948966725 | validation: 3.2878795938435634]
	TIME [epoch: 9.75 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.163598332794918		[learning rate: 0.001405]
	Learning Rate: 0.00140503
	LOSS [training: 3.163598332794918 | validation: 3.2538179147484803]
	TIME [epoch: 9.72 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0115627308061454		[learning rate: 0.0013999]
	Learning Rate: 0.00139993
	LOSS [training: 3.0115627308061454 | validation: 3.2262355974911587]
	TIME [epoch: 9.73 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0206841249632843		[learning rate: 0.0013948]
	Learning Rate: 0.00139485
	LOSS [training: 3.0206841249632843 | validation: 3.264391105760166]
	TIME [epoch: 9.74 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0108274921472025		[learning rate: 0.0013898]
	Learning Rate: 0.00138978
	LOSS [training: 3.0108274921472025 | validation: 3.4109135069316108]
	TIME [epoch: 9.72 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.096880949748486		[learning rate: 0.0013847]
	Learning Rate: 0.00138474
	LOSS [training: 3.096880949748486 | validation: 3.2698777269230526]
	TIME [epoch: 9.72 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.029675924104683		[learning rate: 0.0013797]
	Learning Rate: 0.00137972
	LOSS [training: 3.029675924104683 | validation: 3.229385727270278]
	TIME [epoch: 9.72 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.010665688267717		[learning rate: 0.0013747]
	Learning Rate: 0.00137471
	LOSS [training: 3.010665688267717 | validation: 3.231174178645758]
	TIME [epoch: 9.73 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0038887206861378		[learning rate: 0.0013697]
	Learning Rate: 0.00136972
	LOSS [training: 3.0038887206861378 | validation: 3.217346787950435]
	TIME [epoch: 9.71 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.002126698844866		[learning rate: 0.0013647]
	Learning Rate: 0.00136475
	LOSS [training: 3.002126698844866 | validation: 3.412266767391076]
	TIME [epoch: 9.73 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.084754437158847		[learning rate: 0.0013598]
	Learning Rate: 0.0013598
	LOSS [training: 3.084754437158847 | validation: 3.232804941459516]
	TIME [epoch: 9.73 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.994874147823917		[learning rate: 0.0013549]
	Learning Rate: 0.00135486
	LOSS [training: 2.994874147823917 | validation: 3.2545505473519074]
	TIME [epoch: 9.72 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.073294217008077		[learning rate: 0.0013499]
	Learning Rate: 0.00134994
	LOSS [training: 3.073294217008077 | validation: 3.4262856887200694]
	TIME [epoch: 9.73 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0393983110681804		[learning rate: 0.001345]
	Learning Rate: 0.00134505
	LOSS [training: 3.0393983110681804 | validation: 3.223069448128511]
	TIME [epoch: 9.74 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9825612634330154		[learning rate: 0.0013402]
	Learning Rate: 0.00134016
	LOSS [training: 2.9825612634330154 | validation: 3.2268371583995337]
	TIME [epoch: 9.73 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0344843899049248		[learning rate: 0.0013353]
	Learning Rate: 0.0013353
	LOSS [training: 3.0344843899049248 | validation: 3.231110492001382]
	TIME [epoch: 9.71 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0810674814486037		[learning rate: 0.0013305]
	Learning Rate: 0.00133045
	LOSS [training: 3.0810674814486037 | validation: 3.2671072266998067]
	TIME [epoch: 9.75 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0002131290604885		[learning rate: 0.0013256]
	Learning Rate: 0.00132563
	LOSS [training: 3.0002131290604885 | validation: 3.249811823450664]
	TIME [epoch: 9.72 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.018004562443248		[learning rate: 0.0013208]
	Learning Rate: 0.00132082
	LOSS [training: 3.018004562443248 | validation: 3.227992954183602]
	TIME [epoch: 9.71 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.001832038707838		[learning rate: 0.001316]
	Learning Rate: 0.00131602
	LOSS [training: 3.001832038707838 | validation: 3.2615798961250646]
	TIME [epoch: 9.75 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.006283250641655		[learning rate: 0.0013112]
	Learning Rate: 0.00131125
	LOSS [training: 3.006283250641655 | validation: 3.240152377327913]
	TIME [epoch: 9.72 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.011648577696018		[learning rate: 0.0013065]
	Learning Rate: 0.00130649
	LOSS [training: 3.011648577696018 | validation: 3.3158571848307]
	TIME [epoch: 9.72 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.013018774702953		[learning rate: 0.0013017]
	Learning Rate: 0.00130175
	LOSS [training: 3.013018774702953 | validation: 3.2522191855531966]
	TIME [epoch: 9.74 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.996646701563875		[learning rate: 0.001297]
	Learning Rate: 0.00129702
	LOSS [training: 2.996646701563875 | validation: 3.278806140154697]
	TIME [epoch: 9.72 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0761957561307023		[learning rate: 0.0012923]
	Learning Rate: 0.00129232
	LOSS [training: 3.0761957561307023 | validation: 3.2487279539048495]
	TIME [epoch: 9.72 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.994845801860825		[learning rate: 0.0012876]
	Learning Rate: 0.00128763
	LOSS [training: 2.994845801860825 | validation: 3.2265614676759125]
	TIME [epoch: 9.74 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.004534730655722		[learning rate: 0.001283]
	Learning Rate: 0.00128295
	LOSS [training: 3.004534730655722 | validation: 3.227579262346809]
	TIME [epoch: 9.73 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.047874612946349		[learning rate: 0.0012783]
	Learning Rate: 0.0012783
	LOSS [training: 3.047874612946349 | validation: 3.2510563414971188]
	TIME [epoch: 9.72 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0287354109862354		[learning rate: 0.0012737]
	Learning Rate: 0.00127366
	LOSS [training: 3.0287354109862354 | validation: 3.2665406230525664]
	TIME [epoch: 9.74 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.001989549783133		[learning rate: 0.001269]
	Learning Rate: 0.00126904
	LOSS [training: 3.001989549783133 | validation: 3.3336349557028866]
	TIME [epoch: 9.72 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0346098363494773		[learning rate: 0.0012644]
	Learning Rate: 0.00126443
	LOSS [training: 3.0346098363494773 | validation: 3.2295199466685074]
	TIME [epoch: 9.72 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0136894689257505		[learning rate: 0.0012598]
	Learning Rate: 0.00125984
	LOSS [training: 3.0136894689257505 | validation: 3.243752563199478]
	TIME [epoch: 9.73 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0115781479519947		[learning rate: 0.0012553]
	Learning Rate: 0.00125527
	LOSS [training: 3.0115781479519947 | validation: 3.2469132662462146]
	TIME [epoch: 9.73 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.00087915869648		[learning rate: 0.0012507]
	Learning Rate: 0.00125071
	LOSS [training: 3.00087915869648 | validation: 3.2568293149250502]
	TIME [epoch: 9.71 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0017869746166452		[learning rate: 0.0012462]
	Learning Rate: 0.00124617
	LOSS [training: 3.0017869746166452 | validation: 3.2289796739975998]
	TIME [epoch: 9.74 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.01097220753648		[learning rate: 0.0012417]
	Learning Rate: 0.00124165
	LOSS [training: 3.01097220753648 | validation: 3.2303243667462427]
	TIME [epoch: 9.71 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9995871250970443		[learning rate: 0.0012371]
	Learning Rate: 0.00123715
	LOSS [training: 2.9995871250970443 | validation: 3.2230956279088865]
	TIME [epoch: 9.72 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.989832533479747		[learning rate: 0.0012327]
	Learning Rate: 0.00123266
	LOSS [training: 2.989832533479747 | validation: 3.251560878296607]
	TIME [epoch: 9.73 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.025777628037182		[learning rate: 0.0012282]
	Learning Rate: 0.00122818
	LOSS [training: 3.025777628037182 | validation: 3.2330144070078246]
	TIME [epoch: 9.72 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.999616919482262		[learning rate: 0.0012237]
	Learning Rate: 0.00122373
	LOSS [training: 2.999616919482262 | validation: 3.232424996425325]
	TIME [epoch: 9.71 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0001399991686393		[learning rate: 0.0012193]
	Learning Rate: 0.00121929
	LOSS [training: 3.0001399991686393 | validation: 3.2803396677669032]
	TIME [epoch: 9.71 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.020941699852726		[learning rate: 0.0012149]
	Learning Rate: 0.00121486
	LOSS [training: 3.020941699852726 | validation: 3.2340157449849496]
	TIME [epoch: 9.74 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0787276589990014		[learning rate: 0.0012105]
	Learning Rate: 0.00121045
	LOSS [training: 3.0787276589990014 | validation: 3.2903692999717555]
	TIME [epoch: 9.72 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0400602603509443		[learning rate: 0.0012061]
	Learning Rate: 0.00120606
	LOSS [training: 3.0400602603509443 | validation: 3.23393059353616]
	TIME [epoch: 9.72 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.019687602083505		[learning rate: 0.0012017]
	Learning Rate: 0.00120168
	LOSS [training: 3.019687602083505 | validation: 3.2579674031075996]
	TIME [epoch: 9.72 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.014251558144135		[learning rate: 0.0011973]
	Learning Rate: 0.00119732
	LOSS [training: 3.014251558144135 | validation: 3.235965356873231]
	TIME [epoch: 9.71 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9906587207106723		[learning rate: 0.001193]
	Learning Rate: 0.00119298
	LOSS [training: 2.9906587207106723 | validation: 3.258023295159726]
	TIME [epoch: 9.72 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0754005635210424		[learning rate: 0.0011886]
	Learning Rate: 0.00118865
	LOSS [training: 3.0754005635210424 | validation: 3.2456390747698656]
	TIME [epoch: 9.75 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9908554885689447		[learning rate: 0.0011843]
	Learning Rate: 0.00118433
	LOSS [training: 2.9908554885689447 | validation: 3.220292768723956]
	TIME [epoch: 9.71 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0127508809023427		[learning rate: 0.00118]
	Learning Rate: 0.00118003
	LOSS [training: 3.0127508809023427 | validation: 3.231231853557772]
	TIME [epoch: 9.73 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0047031011537833		[learning rate: 0.0011758]
	Learning Rate: 0.00117575
	LOSS [training: 3.0047031011537833 | validation: 3.2438561717773773]
	TIME [epoch: 9.74 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0073452326266183		[learning rate: 0.0011715]
	Learning Rate: 0.00117149
	LOSS [training: 3.0073452326266183 | validation: 3.2474662812765263]
	TIME [epoch: 9.71 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0070653992279626		[learning rate: 0.0011672]
	Learning Rate: 0.00116723
	LOSS [training: 3.0070653992279626 | validation: 3.221635427373065]
	TIME [epoch: 9.71 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.016328034469889		[learning rate: 0.001163]
	Learning Rate: 0.001163
	LOSS [training: 3.016328034469889 | validation: 3.2393616274650654]
	TIME [epoch: 9.73 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.991688260564376		[learning rate: 0.0011588]
	Learning Rate: 0.00115878
	LOSS [training: 2.991688260564376 | validation: 3.3124533773944838]
	TIME [epoch: 9.7 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.016798193600483		[learning rate: 0.0011546]
	Learning Rate: 0.00115457
	LOSS [training: 3.016798193600483 | validation: 3.2377470650664235]
	TIME [epoch: 9.72 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.994514242236673		[learning rate: 0.0011504]
	Learning Rate: 0.00115038
	LOSS [training: 2.994514242236673 | validation: 3.2247959036753775]
	TIME [epoch: 9.75 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9964247166758584		[learning rate: 0.0011462]
	Learning Rate: 0.00114621
	LOSS [training: 2.9964247166758584 | validation: 3.2499299569618145]
	TIME [epoch: 9.72 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.006250866003254		[learning rate: 0.001142]
	Learning Rate: 0.00114205
	LOSS [training: 3.006250866003254 | validation: 3.2456877678277283]
	TIME [epoch: 9.73 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9979991237740107		[learning rate: 0.0011379]
	Learning Rate: 0.0011379
	LOSS [training: 2.9979991237740107 | validation: 3.227294398085218]
	TIME [epoch: 9.74 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.985934436213782		[learning rate: 0.0011338]
	Learning Rate: 0.00113377
	LOSS [training: 2.985934436213782 | validation: 3.2548382098858304]
	TIME [epoch: 9.72 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0308860794777948		[learning rate: 0.0011297]
	Learning Rate: 0.00112966
	LOSS [training: 3.0308860794777948 | validation: 3.3512539585115144]
	TIME [epoch: 9.72 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.039295673325266		[learning rate: 0.0011256]
	Learning Rate: 0.00112556
	LOSS [training: 3.039295673325266 | validation: 3.231156050203545]
	TIME [epoch: 9.75 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9984978598784746		[learning rate: 0.0011215]
	Learning Rate: 0.00112147
	LOSS [training: 2.9984978598784746 | validation: 3.2291047737021468]
	TIME [epoch: 9.72 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.000830432702952		[learning rate: 0.0011174]
	Learning Rate: 0.0011174
	LOSS [training: 3.000830432702952 | validation: 3.2493998769561]
	TIME [epoch: 9.72 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9975607522603407		[learning rate: 0.0011133]
	Learning Rate: 0.00111335
	LOSS [training: 2.9975607522603407 | validation: 3.2326654813445557]
	TIME [epoch: 9.74 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.005466100810998		[learning rate: 0.0011093]
	Learning Rate: 0.00110931
	LOSS [training: 3.005466100810998 | validation: 3.2433261429977236]
	TIME [epoch: 9.73 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.005731585497302		[learning rate: 0.0011053]
	Learning Rate: 0.00110528
	LOSS [training: 3.005731585497302 | validation: 3.3103569301730373]
	TIME [epoch: 9.72 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0189416688804656		[learning rate: 0.0011013]
	Learning Rate: 0.00110127
	LOSS [training: 3.0189416688804656 | validation: 3.223328467127793]
	TIME [epoch: 9.72 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9842558774007597		[learning rate: 0.0010973]
	Learning Rate: 0.00109728
	LOSS [training: 2.9842558774007597 | validation: 3.2471977347865586]
	TIME [epoch: 9.72 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.00357507862746		[learning rate: 0.0010933]
	Learning Rate: 0.00109329
	LOSS [training: 3.00357507862746 | validation: 3.2480314014218727]
	TIME [epoch: 9.7 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0120261987225154		[learning rate: 0.0010893]
	Learning Rate: 0.00108933
	LOSS [training: 3.0120261987225154 | validation: 3.23380611013395]
	TIME [epoch: 9.72 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.010262994670076		[learning rate: 0.0010854]
	Learning Rate: 0.00108537
	LOSS [training: 3.010262994670076 | validation: 3.2556197315170623]
	TIME [epoch: 9.74 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9840980815909326		[learning rate: 0.0010814]
	Learning Rate: 0.00108143
	LOSS [training: 2.9840980815909326 | validation: 3.3470065059777987]
	TIME [epoch: 9.7 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0314923606289192		[learning rate: 0.0010775]
	Learning Rate: 0.00107751
	LOSS [training: 3.0314923606289192 | validation: 3.2451058247912914]
	TIME [epoch: 9.71 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9821054489455303		[learning rate: 0.0010736]
	Learning Rate: 0.0010736
	LOSS [training: 2.9821054489455303 | validation: 3.233986288478361]
	TIME [epoch: 9.73 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9851483219087713		[learning rate: 0.0010697]
	Learning Rate: 0.0010697
	LOSS [training: 2.9851483219087713 | validation: 3.2171191715893706]
	TIME [epoch: 9.71 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.995409640325187		[learning rate: 0.0010658]
	Learning Rate: 0.00106582
	LOSS [training: 2.995409640325187 | validation: 3.2275061049954745]
	TIME [epoch: 9.73 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.003247196966531		[learning rate: 0.001062]
	Learning Rate: 0.00106195
	LOSS [training: 3.003247196966531 | validation: 3.223792761792052]
	TIME [epoch: 9.75 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9791759547639702		[learning rate: 0.0010581]
	Learning Rate: 0.0010581
	LOSS [training: 2.9791759547639702 | validation: 3.2412244758766096]
	TIME [epoch: 9.71 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0028172037091796		[learning rate: 0.0010543]
	Learning Rate: 0.00105426
	LOSS [training: 3.0028172037091796 | validation: 3.2314284563808258]
	TIME [epoch: 9.71 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9982012443820496		[learning rate: 0.0010504]
	Learning Rate: 0.00105043
	LOSS [training: 2.9982012443820496 | validation: 3.213847104584029]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_720.pth
	Model improved!!!
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0093745712299564		[learning rate: 0.0010466]
	Learning Rate: 0.00104662
	LOSS [training: 3.0093745712299564 | validation: 3.249951857059218]
	TIME [epoch: 9.73 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9899877557352674		[learning rate: 0.0010428]
	Learning Rate: 0.00104282
	LOSS [training: 2.9899877557352674 | validation: 3.2810280945343653]
	TIME [epoch: 9.73 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.008261113927873		[learning rate: 0.001039]
	Learning Rate: 0.00103904
	LOSS [training: 3.008261113927873 | validation: 3.2289206190125768]
	TIME [epoch: 9.74 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.99525073358959		[learning rate: 0.0010353]
	Learning Rate: 0.00103527
	LOSS [training: 2.99525073358959 | validation: 3.2491747026769224]
	TIME [epoch: 9.72 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.985780191690079		[learning rate: 0.0010315]
	Learning Rate: 0.00103151
	LOSS [training: 2.985780191690079 | validation: 3.2362693015447577]
	TIME [epoch: 9.72 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0043016335654595		[learning rate: 0.0010278]
	Learning Rate: 0.00102777
	LOSS [training: 3.0043016335654595 | validation: 3.2669345045103295]
	TIME [epoch: 9.74 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.016688904951484		[learning rate: 0.001024]
	Learning Rate: 0.00102404
	LOSS [training: 3.016688904951484 | validation: 3.3206719441781036]
	TIME [epoch: 9.72 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.028368093226506		[learning rate: 0.0010203]
	Learning Rate: 0.00102032
	LOSS [training: 3.028368093226506 | validation: 3.2233299388438064]
	TIME [epoch: 9.72 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9929837502261556		[learning rate: 0.0010166]
	Learning Rate: 0.00101662
	LOSS [training: 2.9929837502261556 | validation: 3.2422730230591004]
	TIME [epoch: 9.75 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.985991004632792		[learning rate: 0.0010129]
	Learning Rate: 0.00101293
	LOSS [training: 2.985991004632792 | validation: 3.2215656812800813]
	TIME [epoch: 9.72 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9950320610474965		[learning rate: 0.0010093]
	Learning Rate: 0.00100925
	LOSS [training: 2.9950320610474965 | validation: 3.22151444484403]
	TIME [epoch: 9.72 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9738368819656236		[learning rate: 0.0010056]
	Learning Rate: 0.00100559
	LOSS [training: 2.9738368819656236 | validation: 3.26099230757264]
	TIME [epoch: 9.74 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.993078347729832		[learning rate: 0.0010019]
	Learning Rate: 0.00100194
	LOSS [training: 2.993078347729832 | validation: 3.3633604966706185]
	TIME [epoch: 9.72 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1074062878471174		[learning rate: 0.0009983]
	Learning Rate: 0.000998305
	LOSS [training: 3.1074062878471174 | validation: 3.219539649226159]
	TIME [epoch: 9.71 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.988487976705657		[learning rate: 0.00099468]
	Learning Rate: 0.000994682
	LOSS [training: 2.988487976705657 | validation: 3.2544034130963655]
	TIME [epoch: 9.74 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9922666037404104		[learning rate: 0.00099107]
	Learning Rate: 0.000991072
	LOSS [training: 2.9922666037404104 | validation: 3.216297033182716]
	TIME [epoch: 9.72 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9776407078234026		[learning rate: 0.00098748]
	Learning Rate: 0.000987475
	LOSS [training: 2.9776407078234026 | validation: 3.2349839473307065]
	TIME [epoch: 9.73 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0110854503778426		[learning rate: 0.00098389]
	Learning Rate: 0.000983892
	LOSS [training: 3.0110854503778426 | validation: 3.2411332724976765]
	TIME [epoch: 9.74 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0001972436864124		[learning rate: 0.00098032]
	Learning Rate: 0.000980321
	LOSS [training: 3.0001972436864124 | validation: 3.227703214699538]
	TIME [epoch: 9.73 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9864799429260187		[learning rate: 0.00097676]
	Learning Rate: 0.000976764
	LOSS [training: 2.9864799429260187 | validation: 3.2242368066176645]
	TIME [epoch: 9.72 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0051239426136562		[learning rate: 0.00097322]
	Learning Rate: 0.000973219
	LOSS [training: 3.0051239426136562 | validation: 3.2839062738646345]
	TIME [epoch: 9.74 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0904367150858634		[learning rate: 0.00096969]
	Learning Rate: 0.000969687
	LOSS [training: 3.0904367150858634 | validation: 3.2416145561824967]
	TIME [epoch: 9.73 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9823150724955663		[learning rate: 0.00096617]
	Learning Rate: 0.000966168
	LOSS [training: 2.9823150724955663 | validation: 3.2365034898483023]
	TIME [epoch: 9.72 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9840616140750433		[learning rate: 0.00096266]
	Learning Rate: 0.000962662
	LOSS [training: 2.9840616140750433 | validation: 3.208930348050625]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_744.pth
	Model improved!!!
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9986780203151064		[learning rate: 0.00095917]
	Learning Rate: 0.000959168
	LOSS [training: 2.9986780203151064 | validation: 3.209497846662575]
	TIME [epoch: 9.74 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9797294492402893		[learning rate: 0.00095569]
	Learning Rate: 0.000955687
	LOSS [training: 2.9797294492402893 | validation: 3.2258073773837688]
	TIME [epoch: 9.72 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9867079332398427		[learning rate: 0.00095222]
	Learning Rate: 0.000952219
	LOSS [training: 2.9867079332398427 | validation: 3.2490564809580986]
	TIME [epoch: 9.73 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.025432886439024		[learning rate: 0.00094876]
	Learning Rate: 0.000948763
	LOSS [training: 3.025432886439024 | validation: 3.2894926047255173]
	TIME [epoch: 9.74 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0397562844109287		[learning rate: 0.00094532]
	Learning Rate: 0.00094532
	LOSS [training: 3.0397562844109287 | validation: 3.22504946081258]
	TIME [epoch: 9.72 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9937357442371115		[learning rate: 0.00094189]
	Learning Rate: 0.000941889
	LOSS [training: 2.9937357442371115 | validation: 3.229410672204234]
	TIME [epoch: 9.72 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.000643326752308		[learning rate: 0.00093847]
	Learning Rate: 0.000938471
	LOSS [training: 3.000643326752308 | validation: 3.3058934834270657]
	TIME [epoch: 9.74 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.000515441815239		[learning rate: 0.00093507]
	Learning Rate: 0.000935066
	LOSS [training: 3.000515441815239 | validation: 3.229448255505332]
	TIME [epoch: 9.72 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.986113742373548		[learning rate: 0.00093167]
	Learning Rate: 0.000931672
	LOSS [training: 2.986113742373548 | validation: 3.225491737022019]
	TIME [epoch: 9.72 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9818894922250467		[learning rate: 0.00092829]
	Learning Rate: 0.000928291
	LOSS [training: 2.9818894922250467 | validation: 3.235321552515188]
	TIME [epoch: 9.74 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.978339842303345		[learning rate: 0.00092492]
	Learning Rate: 0.000924922
	LOSS [training: 2.978339842303345 | validation: 3.210883173130786]
	TIME [epoch: 9.71 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0172909702269406		[learning rate: 0.00092157]
	Learning Rate: 0.000921566
	LOSS [training: 3.0172909702269406 | validation: 3.230794595366136]
	TIME [epoch: 9.72 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.019598677193026		[learning rate: 0.00091822]
	Learning Rate: 0.000918221
	LOSS [training: 3.019598677193026 | validation: 3.224153304091234]
	TIME [epoch: 9.75 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.987429127625871		[learning rate: 0.00091489]
	Learning Rate: 0.000914889
	LOSS [training: 2.987429127625871 | validation: 3.223200088812839]
	TIME [epoch: 9.72 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.000055875637658		[learning rate: 0.00091157]
	Learning Rate: 0.000911569
	LOSS [training: 3.000055875637658 | validation: 3.2159129406971476]
	TIME [epoch: 9.72 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9965158492691586		[learning rate: 0.00090826]
	Learning Rate: 0.000908261
	LOSS [training: 2.9965158492691586 | validation: 3.207599112583644]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_760.pth
	Model improved!!!
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9986316328194933		[learning rate: 0.00090496]
	Learning Rate: 0.000904965
	LOSS [training: 2.9986316328194933 | validation: 3.2471279683550005]
	TIME [epoch: 9.72 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9998855541457754		[learning rate: 0.00090168]
	Learning Rate: 0.00090168
	LOSS [training: 2.9998855541457754 | validation: 3.234671803118422]
	TIME [epoch: 9.73 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0016895988884937		[learning rate: 0.00089841]
	Learning Rate: 0.000898408
	LOSS [training: 3.0016895988884937 | validation: 3.213353846761636]
	TIME [epoch: 9.76 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0128710790280797		[learning rate: 0.00089515]
	Learning Rate: 0.000895148
	LOSS [training: 3.0128710790280797 | validation: 3.2464074287197455]
	TIME [epoch: 9.71 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.030464524174127		[learning rate: 0.0008919]
	Learning Rate: 0.000891899
	LOSS [training: 3.030464524174127 | validation: 3.3549521463868675]
	TIME [epoch: 9.72 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.047550283216332		[learning rate: 0.00088866]
	Learning Rate: 0.000888663
	LOSS [training: 3.047550283216332 | validation: 3.2151807520803004]
	TIME [epoch: 9.73 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9873738923401865		[learning rate: 0.00088544]
	Learning Rate: 0.000885438
	LOSS [training: 2.9873738923401865 | validation: 3.209996849847264]
	TIME [epoch: 9.73 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9859274640563065		[learning rate: 0.00088222]
	Learning Rate: 0.000882224
	LOSS [training: 2.9859274640563065 | validation: 3.2601599949347735]
	TIME [epoch: 9.73 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.006142900349116		[learning rate: 0.00087902]
	Learning Rate: 0.000879022
	LOSS [training: 3.006142900349116 | validation: 3.226367663720022]
	TIME [epoch: 9.75 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0033328982889493		[learning rate: 0.00087583]
	Learning Rate: 0.000875833
	LOSS [training: 3.0033328982889493 | validation: 3.265840378290822]
	TIME [epoch: 9.73 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9967831386645454		[learning rate: 0.00087265]
	Learning Rate: 0.000872654
	LOSS [training: 2.9967831386645454 | validation: 3.2223691752562424]
	TIME [epoch: 9.72 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9781316138555676		[learning rate: 0.00086949]
	Learning Rate: 0.000869487
	LOSS [training: 2.9781316138555676 | validation: 3.2164928832116693]
	TIME [epoch: 9.74 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0310225250609895		[learning rate: 0.00086633]
	Learning Rate: 0.000866332
	LOSS [training: 3.0310225250609895 | validation: 3.2144043798964486]
	TIME [epoch: 9.72 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0087425433608		[learning rate: 0.00086319]
	Learning Rate: 0.000863188
	LOSS [training: 3.0087425433608 | validation: 3.2388926920510768]
	TIME [epoch: 9.73 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9978828118459013		[learning rate: 0.00086006]
	Learning Rate: 0.000860055
	LOSS [training: 2.9978828118459013 | validation: 3.2327756127793124]
	TIME [epoch: 9.75 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.995081198837267		[learning rate: 0.00085693]
	Learning Rate: 0.000856934
	LOSS [training: 2.995081198837267 | validation: 3.2555385385829108]
	TIME [epoch: 9.72 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.021503143249712		[learning rate: 0.00085382]
	Learning Rate: 0.000853824
	LOSS [training: 3.021503143249712 | validation: 3.250119245643468]
	TIME [epoch: 9.72 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.00310950164447		[learning rate: 0.00085073]
	Learning Rate: 0.000850726
	LOSS [training: 3.00310950164447 | validation: 3.2485969587724806]
	TIME [epoch: 9.71 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.010773715854465		[learning rate: 0.00084764]
	Learning Rate: 0.000847638
	LOSS [training: 3.010773715854465 | validation: 3.226857054350741]
	TIME [epoch: 9.73 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9816004025627016		[learning rate: 0.00084456]
	Learning Rate: 0.000844562
	LOSS [training: 2.9816004025627016 | validation: 3.238068018672911]
	TIME [epoch: 9.72 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.990049935944614		[learning rate: 0.0008415]
	Learning Rate: 0.000841497
	LOSS [training: 2.990049935944614 | validation: 3.2297298578080564]
	TIME [epoch: 9.73 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.997414868032672		[learning rate: 0.00083844]
	Learning Rate: 0.000838443
	LOSS [training: 2.997414868032672 | validation: 3.2259042425377595]
	TIME [epoch: 9.73 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9906388321017916		[learning rate: 0.0008354]
	Learning Rate: 0.000835401
	LOSS [training: 2.9906388321017916 | validation: 3.2338762363544227]
	TIME [epoch: 9.72 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0209803544670657		[learning rate: 0.00083237]
	Learning Rate: 0.000832369
	LOSS [training: 3.0209803544670657 | validation: 3.216527031951991]
	TIME [epoch: 9.72 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9905827493014234		[learning rate: 0.00082935]
	Learning Rate: 0.000829348
	LOSS [training: 2.9905827493014234 | validation: 3.2397628704807984]
	TIME [epoch: 9.75 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0021419754143475		[learning rate: 0.00082634]
	Learning Rate: 0.000826338
	LOSS [training: 3.0021419754143475 | validation: 3.3046769158029528]
	TIME [epoch: 9.73 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.006900846161543		[learning rate: 0.00082334]
	Learning Rate: 0.00082334
	LOSS [training: 3.006900846161543 | validation: 3.2229212791188595]
	TIME [epoch: 9.73 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9754423331429005		[learning rate: 0.00082035]
	Learning Rate: 0.000820352
	LOSS [training: 2.9754423331429005 | validation: 3.2717217005961348]
	TIME [epoch: 9.75 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9991357434328463		[learning rate: 0.00081737]
	Learning Rate: 0.000817375
	LOSS [training: 2.9991357434328463 | validation: 3.2279406457468998]
	TIME [epoch: 9.72 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0100471655195897		[learning rate: 0.00081441]
	Learning Rate: 0.000814408
	LOSS [training: 3.0100471655195897 | validation: 3.2192236458875425]
	TIME [epoch: 9.72 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9812529421529153		[learning rate: 0.00081145]
	Learning Rate: 0.000811453
	LOSS [training: 2.9812529421529153 | validation: 3.2031444745922926]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_791.pth
	Model improved!!!
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9743119622562992		[learning rate: 0.00080851]
	Learning Rate: 0.000808508
	LOSS [training: 2.9743119622562992 | validation: 3.1893064949649066]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_792.pth
	Model improved!!!
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9785450852324336		[learning rate: 0.00080557]
	Learning Rate: 0.000805574
	LOSS [training: 2.9785450852324336 | validation: 3.229170784019916]
	TIME [epoch: 9.71 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0159230236983516		[learning rate: 0.00080265]
	Learning Rate: 0.00080265
	LOSS [training: 3.0159230236983516 | validation: 3.270010492264213]
	TIME [epoch: 9.75 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.991450167333267		[learning rate: 0.00079974]
	Learning Rate: 0.000799737
	LOSS [training: 2.991450167333267 | validation: 3.213903647999011]
	TIME [epoch: 9.72 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.979564533397972		[learning rate: 0.00079684]
	Learning Rate: 0.000796835
	LOSS [training: 2.979564533397972 | validation: 3.214176909898364]
	TIME [epoch: 9.71 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9913107564762287		[learning rate: 0.00079394]
	Learning Rate: 0.000793943
	LOSS [training: 2.9913107564762287 | validation: 3.246652887799755]
	TIME [epoch: 9.73 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9983240048824507		[learning rate: 0.00079106]
	Learning Rate: 0.000791062
	LOSS [training: 2.9983240048824507 | validation: 3.2693321803957334]
	TIME [epoch: 9.72 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.991798054475971		[learning rate: 0.00078819]
	Learning Rate: 0.000788191
	LOSS [training: 2.991798054475971 | validation: 3.2047490116259048]
	TIME [epoch: 9.73 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9771140369197724		[learning rate: 0.00078533]
	Learning Rate: 0.000785331
	LOSS [training: 2.9771140369197724 | validation: 3.2322321587363763]
	TIME [epoch: 9.76 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.97181385117892		[learning rate: 0.00078248]
	Learning Rate: 0.000782481
	LOSS [training: 2.97181385117892 | validation: 3.352088223692472]
	TIME [epoch: 9.73 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0378325305711367		[learning rate: 0.00077964]
	Learning Rate: 0.000779641
	LOSS [training: 3.0378325305711367 | validation: 3.2215969850044166]
	TIME [epoch: 9.71 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0101240633270465		[learning rate: 0.00077681]
	Learning Rate: 0.000776812
	LOSS [training: 3.0101240633270465 | validation: 3.218697814407406]
	TIME [epoch: 9.73 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9838895633194733		[learning rate: 0.00077399]
	Learning Rate: 0.000773993
	LOSS [training: 2.9838895633194733 | validation: 3.2259739641451834]
	TIME [epoch: 9.72 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.977397318154059		[learning rate: 0.00077118]
	Learning Rate: 0.000771184
	LOSS [training: 2.977397318154059 | validation: 3.2349894554229595]
	TIME [epoch: 9.72 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9927492642436007		[learning rate: 0.00076839]
	Learning Rate: 0.000768385
	LOSS [training: 2.9927492642436007 | validation: 3.2278412078701133]
	TIME [epoch: 9.74 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9730674659607166		[learning rate: 0.0007656]
	Learning Rate: 0.000765597
	LOSS [training: 2.9730674659607166 | validation: 3.2149360554885393]
	TIME [epoch: 9.72 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.97509488310524		[learning rate: 0.00076282]
	Learning Rate: 0.000762818
	LOSS [training: 2.97509488310524 | validation: 3.3248669430497393]
	TIME [epoch: 9.71 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0331102217298387		[learning rate: 0.00076005]
	Learning Rate: 0.00076005
	LOSS [training: 3.0331102217298387 | validation: 3.2422721439700815]
	TIME [epoch: 9.74 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9794978420487963		[learning rate: 0.00075729]
	Learning Rate: 0.000757292
	LOSS [training: 2.9794978420487963 | validation: 3.243197217284195]
	TIME [epoch: 9.72 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9883508777124814		[learning rate: 0.00075454]
	Learning Rate: 0.000754543
	LOSS [training: 2.9883508777124814 | validation: 3.2248986721557436]
	TIME [epoch: 9.71 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9809268532228876		[learning rate: 0.00075181]
	Learning Rate: 0.000751805
	LOSS [training: 2.9809268532228876 | validation: 3.2407567363948853]
	TIME [epoch: 9.74 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.988996833010008		[learning rate: 0.00074908]
	Learning Rate: 0.000749077
	LOSS [training: 2.988996833010008 | validation: 3.253602081801007]
	TIME [epoch: 9.73 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.982686491499478		[learning rate: 0.00074636]
	Learning Rate: 0.000746358
	LOSS [training: 2.982686491499478 | validation: 3.2490497853630735]
	TIME [epoch: 9.71 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9836194538239367		[learning rate: 0.00074365]
	Learning Rate: 0.00074365
	LOSS [training: 2.9836194538239367 | validation: 3.2070273495535413]
	TIME [epoch: 9.72 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.965405039433447		[learning rate: 0.00074095]
	Learning Rate: 0.000740951
	LOSS [training: 2.965405039433447 | validation: 3.2132574950809327]
	TIME [epoch: 9.73 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.994400432581936		[learning rate: 0.00073826]
	Learning Rate: 0.000738262
	LOSS [training: 2.994400432581936 | validation: 3.2377796573418]
	TIME [epoch: 9.72 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.980554118619156		[learning rate: 0.00073558]
	Learning Rate: 0.000735583
	LOSS [training: 2.980554118619156 | validation: 3.21802905747222]
	TIME [epoch: 9.72 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.980776136668169		[learning rate: 0.00073291]
	Learning Rate: 0.000732913
	LOSS [training: 2.980776136668169 | validation: 3.2092253408371505]
	TIME [epoch: 9.74 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.99310533098954		[learning rate: 0.00073025]
	Learning Rate: 0.000730254
	LOSS [training: 2.99310533098954 | validation: 3.2497584463821174]
	TIME [epoch: 9.71 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.980524389311028		[learning rate: 0.0007276]
	Learning Rate: 0.000727603
	LOSS [training: 2.980524389311028 | validation: 3.2226134624002998]
	TIME [epoch: 9.71 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9922903963308443		[learning rate: 0.00072496]
	Learning Rate: 0.000724963
	LOSS [training: 2.9922903963308443 | validation: 3.29544932631581]
	TIME [epoch: 9.74 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.997964436910416		[learning rate: 0.00072233]
	Learning Rate: 0.000722332
	LOSS [training: 2.997964436910416 | validation: 3.2242173351663825]
	TIME [epoch: 9.72 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9812243897569566		[learning rate: 0.00071971]
	Learning Rate: 0.000719711
	LOSS [training: 2.9812243897569566 | validation: 3.2108269134329213]
	TIME [epoch: 9.71 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.972497698965303		[learning rate: 0.0007171]
	Learning Rate: 0.000717099
	LOSS [training: 2.972497698965303 | validation: 3.226070295849869]
	TIME [epoch: 9.72 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9810770177937362		[learning rate: 0.0007145]
	Learning Rate: 0.000714496
	LOSS [training: 2.9810770177937362 | validation: 3.242666496956178]
	TIME [epoch: 9.7 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9847709075138398		[learning rate: 0.0007119]
	Learning Rate: 0.000711903
	LOSS [training: 2.9847709075138398 | validation: 3.2242383658578646]
	TIME [epoch: 9.71 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9774794485552807		[learning rate: 0.00070932]
	Learning Rate: 0.00070932
	LOSS [training: 2.9774794485552807 | validation: 3.2164250188785584]
	TIME [epoch: 9.75 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0043753932292634		[learning rate: 0.00070675]
	Learning Rate: 0.000706746
	LOSS [training: 3.0043753932292634 | validation: 3.219131865204309]
	TIME [epoch: 9.71 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.984737848895933		[learning rate: 0.00070418]
	Learning Rate: 0.000704181
	LOSS [training: 2.984737848895933 | validation: 3.234937581949097]
	TIME [epoch: 9.71 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9881048173359575		[learning rate: 0.00070163]
	Learning Rate: 0.000701625
	LOSS [training: 2.9881048173359575 | validation: 3.2322814319883206]
	TIME [epoch: 9.74 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9818729564831643		[learning rate: 0.00069908]
	Learning Rate: 0.000699079
	LOSS [training: 2.9818729564831643 | validation: 3.231896431529682]
	TIME [epoch: 9.71 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9876598192812205		[learning rate: 0.00069654]
	Learning Rate: 0.000696542
	LOSS [training: 2.9876598192812205 | validation: 3.237565513958509]
	TIME [epoch: 9.72 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9796455808326385		[learning rate: 0.00069401]
	Learning Rate: 0.000694014
	LOSS [training: 2.9796455808326385 | validation: 3.211521781473119]
	TIME [epoch: 9.72 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.974065208930975		[learning rate: 0.0006915]
	Learning Rate: 0.000691496
	LOSS [training: 2.974065208930975 | validation: 3.230824951047407]
	TIME [epoch: 9.72 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9805456994074837		[learning rate: 0.00068899]
	Learning Rate: 0.000688986
	LOSS [training: 2.9805456994074837 | validation: 3.2617723541836607]
	TIME [epoch: 9.71 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.985720657790913		[learning rate: 0.00068649]
	Learning Rate: 0.000686486
	LOSS [training: 2.985720657790913 | validation: 3.2273707275055394]
	TIME [epoch: 9.74 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9740003398948427		[learning rate: 0.00068399]
	Learning Rate: 0.000683994
	LOSS [training: 2.9740003398948427 | validation: 3.2278747943389012]
	TIME [epoch: 9.72 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9942934322405397		[learning rate: 0.00068151]
	Learning Rate: 0.000681512
	LOSS [training: 2.9942934322405397 | validation: 3.2143577303644015]
	TIME [epoch: 9.71 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.973277091906427		[learning rate: 0.00067904]
	Learning Rate: 0.000679039
	LOSS [training: 2.973277091906427 | validation: 3.2421965103141663]
	TIME [epoch: 9.72 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.982589947120123		[learning rate: 0.00067657]
	Learning Rate: 0.000676575
	LOSS [training: 2.982589947120123 | validation: 3.220948189831319]
	TIME [epoch: 9.72 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9918500222400306		[learning rate: 0.00067412]
	Learning Rate: 0.00067412
	LOSS [training: 2.9918500222400306 | validation: 3.2365388401980666]
	TIME [epoch: 9.73 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.985345736128368		[learning rate: 0.00067167]
	Learning Rate: 0.000671673
	LOSS [training: 2.985345736128368 | validation: 3.2602646362299845]
	TIME [epoch: 9.74 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.995500020311024		[learning rate: 0.00066924]
	Learning Rate: 0.000669235
	LOSS [training: 2.995500020311024 | validation: 3.2471412991410586]
	TIME [epoch: 9.71 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.980669859893682		[learning rate: 0.00066681]
	Learning Rate: 0.000666807
	LOSS [training: 2.980669859893682 | validation: 3.2092442755899984]
	TIME [epoch: 9.71 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9796926936357235		[learning rate: 0.00066439]
	Learning Rate: 0.000664387
	LOSS [training: 2.9796926936357235 | validation: 3.2208392489761817]
	TIME [epoch: 9.71 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0164620219794687		[learning rate: 0.00066198]
	Learning Rate: 0.000661976
	LOSS [training: 3.0164620219794687 | validation: 3.2479333828369166]
	TIME [epoch: 9.72 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9812417035581285		[learning rate: 0.00065957]
	Learning Rate: 0.000659573
	LOSS [training: 2.9812417035581285 | validation: 3.294205152079095]
	TIME [epoch: 9.71 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.060555339981378		[learning rate: 0.00065718]
	Learning Rate: 0.00065718
	LOSS [training: 3.060555339981378 | validation: 3.219804903458711]
	TIME [epoch: 9.71 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9780661622220146		[learning rate: 0.00065479]
	Learning Rate: 0.000654795
	LOSS [training: 2.9780661622220146 | validation: 3.235119816420929]
	TIME [epoch: 9.73 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9830386769817276		[learning rate: 0.00065242]
	Learning Rate: 0.000652419
	LOSS [training: 2.9830386769817276 | validation: 3.256793406062603]
	TIME [epoch: 9.7 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.990048901070724		[learning rate: 0.00065005]
	Learning Rate: 0.000650051
	LOSS [training: 2.990048901070724 | validation: 3.2064644645037563]
	TIME [epoch: 9.72 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9751249213846775		[learning rate: 0.00064769]
	Learning Rate: 0.000647692
	LOSS [training: 2.9751249213846775 | validation: 3.2460161150684685]
	TIME [epoch: 9.75 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9809876863477847		[learning rate: 0.00064534]
	Learning Rate: 0.000645341
	LOSS [training: 2.9809876863477847 | validation: 3.2107292220746437]
	TIME [epoch: 9.71 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9684223403922285		[learning rate: 0.000643]
	Learning Rate: 0.000642999
	LOSS [training: 2.9684223403922285 | validation: 3.228284813233156]
	TIME [epoch: 9.71 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.971188179914373		[learning rate: 0.00064067]
	Learning Rate: 0.000640666
	LOSS [training: 2.971188179914373 | validation: 3.2297137533289044]
	TIME [epoch: 9.74 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.974127917147424		[learning rate: 0.00063834]
	Learning Rate: 0.000638341
	LOSS [training: 2.974127917147424 | validation: 3.235060775874099]
	TIME [epoch: 9.71 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9701588958282654		[learning rate: 0.00063602]
	Learning Rate: 0.000636024
	LOSS [training: 2.9701588958282654 | validation: 3.211552833312485]
	TIME [epoch: 9.71 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.97897066847363		[learning rate: 0.00063372]
	Learning Rate: 0.000633716
	LOSS [training: 2.97897066847363 | validation: 3.2132765997307975]
	TIME [epoch: 9.73 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.97388669769499		[learning rate: 0.00063142]
	Learning Rate: 0.000631416
	LOSS [training: 2.97388669769499 | validation: 3.2286981605070277]
	TIME [epoch: 9.72 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9791980659552997		[learning rate: 0.00062912]
	Learning Rate: 0.000629125
	LOSS [training: 2.9791980659552997 | validation: 3.211120512674984]
	TIME [epoch: 9.72 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.006772589792677		[learning rate: 0.00062684]
	Learning Rate: 0.000626842
	LOSS [training: 3.006772589792677 | validation: 3.220967737178006]
	TIME [epoch: 9.74 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0055794020962914		[learning rate: 0.00062457]
	Learning Rate: 0.000624567
	LOSS [training: 3.0055794020962914 | validation: 3.283255905365861]
	TIME [epoch: 9.72 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.984905964803961		[learning rate: 0.0006223]
	Learning Rate: 0.0006223
	LOSS [training: 2.984905964803961 | validation: 3.227465085740844]
	TIME [epoch: 9.72 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9778742856897447		[learning rate: 0.00062004]
	Learning Rate: 0.000620042
	LOSS [training: 2.9778742856897447 | validation: 3.241969347230115]
	TIME [epoch: 9.74 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9794300819227297		[learning rate: 0.00061779]
	Learning Rate: 0.000617792
	LOSS [training: 2.9794300819227297 | validation: 3.221325199816614]
	TIME [epoch: 9.72 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9704257957838474		[learning rate: 0.00061555]
	Learning Rate: 0.00061555
	LOSS [training: 2.9704257957838474 | validation: 3.24177814666953]
	TIME [epoch: 9.72 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.976657852945725		[learning rate: 0.00061332]
	Learning Rate: 0.000613316
	LOSS [training: 2.976657852945725 | validation: 3.216341881452556]
	TIME [epoch: 9.73 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9684409549944997		[learning rate: 0.00061109]
	Learning Rate: 0.00061109
	LOSS [training: 2.9684409549944997 | validation: 3.2318146749737653]
	TIME [epoch: 9.72 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.974093244884429		[learning rate: 0.00060887]
	Learning Rate: 0.000608872
	LOSS [training: 2.974093244884429 | validation: 3.2594681566829515]
	TIME [epoch: 9.7 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.98196813483636		[learning rate: 0.00060666]
	Learning Rate: 0.000606663
	LOSS [training: 2.98196813483636 | validation: 3.2079307305431612]
	TIME [epoch: 9.74 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.006033362644639		[learning rate: 0.00060446]
	Learning Rate: 0.000604461
	LOSS [training: 3.006033362644639 | validation: 3.2124358882508592]
	TIME [epoch: 9.72 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9733449101511717		[learning rate: 0.00060227]
	Learning Rate: 0.000602268
	LOSS [training: 2.9733449101511717 | validation: 3.2071179240726817]
	TIME [epoch: 9.71 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9874130566328754		[learning rate: 0.00060008]
	Learning Rate: 0.000600082
	LOSS [training: 2.9874130566328754 | validation: 3.248473866925863]
	TIME [epoch: 9.73 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9871879551408442		[learning rate: 0.0005979]
	Learning Rate: 0.000597904
	LOSS [training: 2.9871879551408442 | validation: 3.2210443098728034]
	TIME [epoch: 9.73 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9877247195073164		[learning rate: 0.00059573]
	Learning Rate: 0.000595734
	LOSS [training: 2.9877247195073164 | validation: 3.203872093070462]
	TIME [epoch: 9.73 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.977293975824739		[learning rate: 0.00059357]
	Learning Rate: 0.000593572
	LOSS [training: 2.977293975824739 | validation: 3.234858380562968]
	TIME [epoch: 9.73 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.980914414752328		[learning rate: 0.00059142]
	Learning Rate: 0.000591418
	LOSS [training: 2.980914414752328 | validation: 3.205404446597675]
	TIME [epoch: 9.72 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9653527075910238		[learning rate: 0.00058927]
	Learning Rate: 0.000589272
	LOSS [training: 2.9653527075910238 | validation: 3.2244712156630433]
	TIME [epoch: 9.72 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.977593202133621		[learning rate: 0.00058713]
	Learning Rate: 0.000587133
	LOSS [training: 2.977593202133621 | validation: 3.2145234624855106]
	TIME [epoch: 9.72 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9892698239865223		[learning rate: 0.000585]
	Learning Rate: 0.000585003
	LOSS [training: 2.9892698239865223 | validation: 3.2754227183742537]
	TIME [epoch: 9.73 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.992497448774314		[learning rate: 0.00058288]
	Learning Rate: 0.00058288
	LOSS [training: 2.992497448774314 | validation: 3.2108496192758604]
	TIME [epoch: 9.71 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.97534898554704		[learning rate: 0.00058076]
	Learning Rate: 0.000580764
	LOSS [training: 2.97534898554704 | validation: 3.2073910059972857]
	TIME [epoch: 9.71 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9653857556388594		[learning rate: 0.00057866]
	Learning Rate: 0.000578657
	LOSS [training: 2.9653857556388594 | validation: 3.2116949616567645]
	TIME [epoch: 9.73 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9914578528159357		[learning rate: 0.00057656]
	Learning Rate: 0.000576557
	LOSS [training: 2.9914578528159357 | validation: 3.232363520042607]
	TIME [epoch: 9.7 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9835993381794768		[learning rate: 0.00057446]
	Learning Rate: 0.000574465
	LOSS [training: 2.9835993381794768 | validation: 3.241703379816345]
	TIME [epoch: 9.7 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9943762921875687		[learning rate: 0.00057238]
	Learning Rate: 0.00057238
	LOSS [training: 2.9943762921875687 | validation: 3.250250423941543]
	TIME [epoch: 9.72 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.981420650343818		[learning rate: 0.0005703]
	Learning Rate: 0.000570303
	LOSS [training: 2.981420650343818 | validation: 3.2064641137746146]
	TIME [epoch: 9.71 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9815849503111225		[learning rate: 0.00056823]
	Learning Rate: 0.000568233
	LOSS [training: 2.9815849503111225 | validation: 3.252703280855254]
	TIME [epoch: 9.71 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9849826264011465		[learning rate: 0.00056617]
	Learning Rate: 0.000566171
	LOSS [training: 2.9849826264011465 | validation: 3.2232765220051456]
	TIME [epoch: 9.73 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9996773363296585		[learning rate: 0.00056412]
	Learning Rate: 0.000564116
	LOSS [training: 2.9996773363296585 | validation: 3.244846440531632]
	TIME [epoch: 9.7 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9839142586776433		[learning rate: 0.00056207]
	Learning Rate: 0.000562069
	LOSS [training: 2.9839142586776433 | validation: 3.2451576193035727]
	TIME [epoch: 9.72 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9994068081989615		[learning rate: 0.00056003]
	Learning Rate: 0.000560029
	LOSS [training: 2.9994068081989615 | validation: 3.2201143648499535]
	TIME [epoch: 9.74 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9810350326636716		[learning rate: 0.000558]
	Learning Rate: 0.000557997
	LOSS [training: 2.9810350326636716 | validation: 3.2260364018048846]
	TIME [epoch: 9.71 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.977059363408917		[learning rate: 0.00055597]
	Learning Rate: 0.000555972
	LOSS [training: 2.977059363408917 | validation: 3.2333022683527504]
	TIME [epoch: 9.73 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9761064513516815		[learning rate: 0.00055395]
	Learning Rate: 0.000553954
	LOSS [training: 2.9761064513516815 | validation: 3.2237331696224305]
	TIME [epoch: 9.74 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9748874014440583		[learning rate: 0.00055194]
	Learning Rate: 0.000551944
	LOSS [training: 2.9748874014440583 | validation: 3.2231014263443285]
	TIME [epoch: 9.72 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.976208860578037		[learning rate: 0.00054994]
	Learning Rate: 0.000549941
	LOSS [training: 2.976208860578037 | validation: 3.237819436239714]
	TIME [epoch: 9.71 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9842252821630177		[learning rate: 0.00054794]
	Learning Rate: 0.000547945
	LOSS [training: 2.9842252821630177 | validation: 3.2406161354688865]
	TIME [epoch: 9.74 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.984393692567951		[learning rate: 0.00054596]
	Learning Rate: 0.000545956
	LOSS [training: 2.984393692567951 | validation: 3.232198509800014]
	TIME [epoch: 9.72 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9740247755627762		[learning rate: 0.00054397]
	Learning Rate: 0.000543975
	LOSS [training: 2.9740247755627762 | validation: 3.218279379585291]
	TIME [epoch: 9.71 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963230557995973		[learning rate: 0.000542]
	Learning Rate: 0.000542001
	LOSS [training: 2.963230557995973 | validation: 3.2244004066931327]
	TIME [epoch: 9.73 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9901928966270654		[learning rate: 0.00054003]
	Learning Rate: 0.000540034
	LOSS [training: 2.9901928966270654 | validation: 3.226853440159179]
	TIME [epoch: 9.7 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9817799941167125		[learning rate: 0.00053807]
	Learning Rate: 0.000538074
	LOSS [training: 2.9817799941167125 | validation: 3.2183536884823503]
	TIME [epoch: 9.7 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9880987669768175		[learning rate: 0.00053612]
	Learning Rate: 0.000536121
	LOSS [training: 2.9880987669768175 | validation: 3.224534294733493]
	TIME [epoch: 9.72 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.984216186803377		[learning rate: 0.00053418]
	Learning Rate: 0.000534176
	LOSS [training: 2.984216186803377 | validation: 3.2418339350656526]
	TIME [epoch: 9.71 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9647786618506706		[learning rate: 0.00053224]
	Learning Rate: 0.000532237
	LOSS [training: 2.9647786618506706 | validation: 3.219453067876566]
	TIME [epoch: 9.7 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9798573010733005		[learning rate: 0.00053031]
	Learning Rate: 0.000530306
	LOSS [training: 2.9798573010733005 | validation: 3.2229283372554733]
	TIME [epoch: 9.72 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9660598970625345		[learning rate: 0.00052838]
	Learning Rate: 0.000528381
	LOSS [training: 2.9660598970625345 | validation: 3.199384982264656]
	TIME [epoch: 9.7 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.975745837271579		[learning rate: 0.00052646]
	Learning Rate: 0.000526464
	LOSS [training: 2.975745837271579 | validation: 3.252293538245733]
	TIME [epoch: 9.7 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9807329097871924		[learning rate: 0.00052455]
	Learning Rate: 0.000524553
	LOSS [training: 2.9807329097871924 | validation: 3.2319323452538775]
	TIME [epoch: 9.71 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9702728621958836		[learning rate: 0.00052265]
	Learning Rate: 0.000522649
	LOSS [training: 2.9702728621958836 | validation: 3.2098740600881515]
	TIME [epoch: 9.71 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.968376975255284		[learning rate: 0.00052075]
	Learning Rate: 0.000520753
	LOSS [training: 2.968376975255284 | validation: 3.213123650575007]
	TIME [epoch: 9.72 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.971588181505127		[learning rate: 0.00051886]
	Learning Rate: 0.000518863
	LOSS [training: 2.971588181505127 | validation: 3.2235618999870117]
	TIME [epoch: 9.72 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.971963376553569		[learning rate: 0.00051698]
	Learning Rate: 0.00051698
	LOSS [training: 2.971963376553569 | validation: 3.219638455190347]
	TIME [epoch: 9.73 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.97502848145271		[learning rate: 0.0005151]
	Learning Rate: 0.000515104
	LOSS [training: 2.97502848145271 | validation: 3.2196461533273624]
	TIME [epoch: 9.71 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9714385010668796		[learning rate: 0.00051323]
	Learning Rate: 0.000513235
	LOSS [training: 2.9714385010668796 | validation: 3.2322933974484727]
	TIME [epoch: 9.71 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9664994153495714		[learning rate: 0.00051137]
	Learning Rate: 0.000511372
	LOSS [training: 2.9664994153495714 | validation: 3.212538817784834]
	TIME [epoch: 9.72 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9780720291389056		[learning rate: 0.00050952]
	Learning Rate: 0.000509516
	LOSS [training: 2.9780720291389056 | validation: 3.205254632848157]
	TIME [epoch: 9.71 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9702472161486333		[learning rate: 0.00050767]
	Learning Rate: 0.000507667
	LOSS [training: 2.9702472161486333 | validation: 3.217866561324171]
	TIME [epoch: 9.71 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9795025803502506		[learning rate: 0.00050582]
	Learning Rate: 0.000505825
	LOSS [training: 2.9795025803502506 | validation: 3.2216726864438]
	TIME [epoch: 9.73 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9861837447200417		[learning rate: 0.00050399]
	Learning Rate: 0.000503989
	LOSS [training: 2.9861837447200417 | validation: 3.2255520607350934]
	TIME [epoch: 9.72 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.970109757740413		[learning rate: 0.00050216]
	Learning Rate: 0.00050216
	LOSS [training: 2.970109757740413 | validation: 3.213235822470515]
	TIME [epoch: 9.72 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9638665433582454		[learning rate: 0.00050034]
	Learning Rate: 0.000500338
	LOSS [training: 2.9638665433582454 | validation: 3.217744317301158]
	TIME [epoch: 9.74 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.972643052061201		[learning rate: 0.00049852]
	Learning Rate: 0.000498522
	LOSS [training: 2.972643052061201 | validation: 3.20618562780533]
	TIME [epoch: 9.71 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9687301746806747		[learning rate: 0.00049671]
	Learning Rate: 0.000496713
	LOSS [training: 2.9687301746806747 | validation: 3.221498383916272]
	TIME [epoch: 9.72 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9667223177035233		[learning rate: 0.00049491]
	Learning Rate: 0.00049491
	LOSS [training: 2.9667223177035233 | validation: 3.227973521555478]
	TIME [epoch: 9.74 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.969229926099635		[learning rate: 0.00049311]
	Learning Rate: 0.000493114
	LOSS [training: 2.969229926099635 | validation: 3.22850087639539]
	TIME [epoch: 9.71 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9729658923656404		[learning rate: 0.00049132]
	Learning Rate: 0.000491325
	LOSS [training: 2.9729658923656404 | validation: 3.2117664932614276]
	TIME [epoch: 9.72 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.974014285015799		[learning rate: 0.00048954]
	Learning Rate: 0.000489542
	LOSS [training: 2.974014285015799 | validation: 3.21739387126176]
	TIME [epoch: 9.73 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9675651951477717		[learning rate: 0.00048776]
	Learning Rate: 0.000487765
	LOSS [training: 2.9675651951477717 | validation: 3.2166500104103966]
	TIME [epoch: 9.72 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9664363823405666		[learning rate: 0.00048599]
	Learning Rate: 0.000485995
	LOSS [training: 2.9664363823405666 | validation: 3.1981715847408556]
	TIME [epoch: 9.72 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.991581419798448		[learning rate: 0.00048423]
	Learning Rate: 0.000484231
	LOSS [training: 2.991581419798448 | validation: 3.213915313378361]
	TIME [epoch: 9.74 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9826190727551296		[learning rate: 0.00048247]
	Learning Rate: 0.000482474
	LOSS [training: 2.9826190727551296 | validation: 3.2099444369815946]
	TIME [epoch: 9.72 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963102741874148		[learning rate: 0.00048072]
	Learning Rate: 0.000480723
	LOSS [training: 2.963102741874148 | validation: 3.213203906306814]
	TIME [epoch: 9.71 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9624876735814314		[learning rate: 0.00047898]
	Learning Rate: 0.000478978
	LOSS [training: 2.9624876735814314 | validation: 3.2192259783923407]
	TIME [epoch: 9.73 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9699905360927032		[learning rate: 0.00047724]
	Learning Rate: 0.00047724
	LOSS [training: 2.9699905360927032 | validation: 3.2263300109930446]
	TIME [epoch: 9.71 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.968258469556873		[learning rate: 0.00047551]
	Learning Rate: 0.000475508
	LOSS [training: 2.968258469556873 | validation: 3.2258557114792734]
	TIME [epoch: 9.72 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.973452362903893		[learning rate: 0.00047378]
	Learning Rate: 0.000473782
	LOSS [training: 2.973452362903893 | validation: 3.245273502786]
	TIME [epoch: 9.73 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9859258860679594		[learning rate: 0.00047206]
	Learning Rate: 0.000472063
	LOSS [training: 2.9859258860679594 | validation: 3.2143255307112066]
	TIME [epoch: 9.72 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.979919039266162		[learning rate: 0.00047035]
	Learning Rate: 0.00047035
	LOSS [training: 2.979919039266162 | validation: 3.257207620773785]
	TIME [epoch: 9.71 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9770934934713553		[learning rate: 0.00046864]
	Learning Rate: 0.000468643
	LOSS [training: 2.9770934934713553 | validation: 3.204683609385362]
	TIME [epoch: 9.72 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.981126885801207		[learning rate: 0.00046694]
	Learning Rate: 0.000466942
	LOSS [training: 2.981126885801207 | validation: 3.2022667661011304]
	TIME [epoch: 9.72 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.982094546689523		[learning rate: 0.00046525]
	Learning Rate: 0.000465248
	LOSS [training: 2.982094546689523 | validation: 3.2236916049128155]
	TIME [epoch: 9.72 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.971782251018045		[learning rate: 0.00046356]
	Learning Rate: 0.000463559
	LOSS [training: 2.971782251018045 | validation: 3.214346864415633]
	TIME [epoch: 9.73 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.969641049165516		[learning rate: 0.00046188]
	Learning Rate: 0.000461877
	LOSS [training: 2.969641049165516 | validation: 3.209620148217336]
	TIME [epoch: 9.73 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966057208697056		[learning rate: 0.0004602]
	Learning Rate: 0.000460201
	LOSS [training: 2.966057208697056 | validation: 3.2176138758151263]
	TIME [epoch: 9.71 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9793496399793087		[learning rate: 0.00045853]
	Learning Rate: 0.000458531
	LOSS [training: 2.9793496399793087 | validation: 3.211484252010935]
	TIME [epoch: 9.72 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.971068626248498		[learning rate: 0.00045687]
	Learning Rate: 0.000456867
	LOSS [training: 2.971068626248498 | validation: 3.2195848213885006]
	TIME [epoch: 9.74 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9764632652424368		[learning rate: 0.00045521]
	Learning Rate: 0.000455209
	LOSS [training: 2.9764632652424368 | validation: 3.221167626466627]
	TIME [epoch: 9.71 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9727481688397033		[learning rate: 0.00045356]
	Learning Rate: 0.000453557
	LOSS [training: 2.9727481688397033 | validation: 3.2438379760462124]
	TIME [epoch: 9.72 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9907211757475056		[learning rate: 0.00045191]
	Learning Rate: 0.000451911
	LOSS [training: 2.9907211757475056 | validation: 3.2117883690841826]
	TIME [epoch: 9.73 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9720905979394527		[learning rate: 0.00045027]
	Learning Rate: 0.000450271
	LOSS [training: 2.9720905979394527 | validation: 3.2229134632752596]
	TIME [epoch: 9.71 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9679686332070707		[learning rate: 0.00044864]
	Learning Rate: 0.000448637
	LOSS [training: 2.9679686332070707 | validation: 3.2075959256759883]
	TIME [epoch: 9.7 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9746798518384794		[learning rate: 0.00044701]
	Learning Rate: 0.000447009
	LOSS [training: 2.9746798518384794 | validation: 3.2120639217836042]
	TIME [epoch: 9.72 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.982281004742487		[learning rate: 0.00044539]
	Learning Rate: 0.000445386
	LOSS [training: 2.982281004742487 | validation: 3.2369906898811536]
	TIME [epoch: 9.71 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9874544483697796		[learning rate: 0.00044377]
	Learning Rate: 0.00044377
	LOSS [training: 2.9874544483697796 | validation: 3.2276039061072175]
	TIME [epoch: 9.7 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9775281067435566		[learning rate: 0.00044216]
	Learning Rate: 0.00044216
	LOSS [training: 2.9775281067435566 | validation: 3.2133598203197766]
	TIME [epoch: 9.72 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9608005492551457		[learning rate: 0.00044055]
	Learning Rate: 0.000440555
	LOSS [training: 2.9608005492551457 | validation: 3.2008642290043543]
	TIME [epoch: 9.7 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9768620159104247		[learning rate: 0.00043896]
	Learning Rate: 0.000438956
	LOSS [training: 2.9768620159104247 | validation: 3.2339003332438545]
	TIME [epoch: 9.71 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9761908032054665		[learning rate: 0.00043736]
	Learning Rate: 0.000437363
	LOSS [training: 2.9761908032054665 | validation: 3.213828561312354]
	TIME [epoch: 9.72 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963163185553669		[learning rate: 0.00043578]
	Learning Rate: 0.000435776
	LOSS [training: 2.963163185553669 | validation: 3.2134576726949002]
	TIME [epoch: 9.7 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9896527859168027		[learning rate: 0.00043419]
	Learning Rate: 0.000434194
	LOSS [training: 2.9896527859168027 | validation: 3.2246996496225266]
	TIME [epoch: 9.7 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.969461970735077		[learning rate: 0.00043262]
	Learning Rate: 0.000432619
	LOSS [training: 2.969461970735077 | validation: 3.2340544205492585]
	TIME [epoch: 9.72 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9682926470445863		[learning rate: 0.00043105]
	Learning Rate: 0.000431049
	LOSS [training: 2.9682926470445863 | validation: 3.210416001995042]
	TIME [epoch: 9.71 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9678730904845905		[learning rate: 0.00042948]
	Learning Rate: 0.000429484
	LOSS [training: 2.9678730904845905 | validation: 3.209983948899156]
	TIME [epoch: 9.7 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9701982061205485		[learning rate: 0.00042793]
	Learning Rate: 0.000427926
	LOSS [training: 2.9701982061205485 | validation: 3.2031624462402433]
	TIME [epoch: 9.71 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.964329907368422		[learning rate: 0.00042637]
	Learning Rate: 0.000426373
	LOSS [training: 2.964329907368422 | validation: 3.219072173751657]
	TIME [epoch: 9.71 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9670962562514918		[learning rate: 0.00042483]
	Learning Rate: 0.000424825
	LOSS [training: 2.9670962562514918 | validation: 3.2094380852867377]
	TIME [epoch: 9.7 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.967030066838505		[learning rate: 0.00042328]
	Learning Rate: 0.000423284
	LOSS [training: 2.967030066838505 | validation: 3.1996088803324687]
	TIME [epoch: 9.74 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.96179866044677		[learning rate: 0.00042175]
	Learning Rate: 0.000421748
	LOSS [training: 2.96179866044677 | validation: 3.213062322443941]
	TIME [epoch: 9.7 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9717225025299077		[learning rate: 0.00042022]
	Learning Rate: 0.000420217
	LOSS [training: 2.9717225025299077 | validation: 3.2090267709149805]
	TIME [epoch: 9.7 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.970815076581793		[learning rate: 0.00041869]
	Learning Rate: 0.000418692
	LOSS [training: 2.970815076581793 | validation: 3.207238545722857]
	TIME [epoch: 9.71 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9685562025610923		[learning rate: 0.00041717]
	Learning Rate: 0.000417173
	LOSS [training: 2.9685562025610923 | validation: 3.2283003442087455]
	TIME [epoch: 9.71 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.975016157790842		[learning rate: 0.00041566]
	Learning Rate: 0.000415659
	LOSS [training: 2.975016157790842 | validation: 3.222892755981809]
	TIME [epoch: 9.7 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9745655694068054		[learning rate: 0.00041415]
	Learning Rate: 0.00041415
	LOSS [training: 2.9745655694068054 | validation: 3.2134876942300927]
	TIME [epoch: 9.71 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9774877552488848		[learning rate: 0.00041265]
	Learning Rate: 0.000412647
	LOSS [training: 2.9774877552488848 | validation: 3.245486994900442]
	TIME [epoch: 9.72 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9958333312396705		[learning rate: 0.00041115]
	Learning Rate: 0.00041115
	LOSS [training: 2.9958333312396705 | validation: 3.3095065156329726]
	TIME [epoch: 9.7 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0017189850910255		[learning rate: 0.00040966]
	Learning Rate: 0.000409658
	LOSS [training: 3.0017189850910255 | validation: 3.232191976633252]
	TIME [epoch: 9.69 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.97559578353223		[learning rate: 0.00040817]
	Learning Rate: 0.000408171
	LOSS [training: 2.97559578353223 | validation: 3.2109322513053917]
	TIME [epoch: 9.72 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9741019926254353		[learning rate: 0.00040669]
	Learning Rate: 0.00040669
	LOSS [training: 2.9741019926254353 | validation: 3.2060008001768945]
	TIME [epoch: 9.7 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9640356281992855		[learning rate: 0.00040521]
	Learning Rate: 0.000405214
	LOSS [training: 2.9640356281992855 | validation: 3.2328167373175685]
	TIME [epoch: 9.7 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0006312783228046		[learning rate: 0.00040374]
	Learning Rate: 0.000403743
	LOSS [training: 3.0006312783228046 | validation: 3.2475696546290935]
	TIME [epoch: 9.72 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9777967276375374		[learning rate: 0.00040228]
	Learning Rate: 0.000402278
	LOSS [training: 2.9777967276375374 | validation: 3.227569820949094]
	TIME [epoch: 9.7 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9624430358384766		[learning rate: 0.00040082]
	Learning Rate: 0.000400818
	LOSS [training: 2.9624430358384766 | validation: 3.21122492956868]
	TIME [epoch: 9.7 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9674372562928126		[learning rate: 0.00039936]
	Learning Rate: 0.000399364
	LOSS [training: 2.9674372562928126 | validation: 3.207608573262288]
	TIME [epoch: 9.72 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.976573297321234		[learning rate: 0.00039791]
	Learning Rate: 0.000397914
	LOSS [training: 2.976573297321234 | validation: 3.23113406322995]
	TIME [epoch: 9.7 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9729858866447216		[learning rate: 0.00039647]
	Learning Rate: 0.00039647
	LOSS [training: 2.9729858866447216 | validation: 3.2640498943315537]
	TIME [epoch: 9.7 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9818536780487506		[learning rate: 0.00039503]
	Learning Rate: 0.000395031
	LOSS [training: 2.9818536780487506 | validation: 3.211123734459119]
	TIME [epoch: 9.73 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.968906012036828		[learning rate: 0.0003936]
	Learning Rate: 0.000393598
	LOSS [training: 2.968906012036828 | validation: 3.212216975019817]
	TIME [epoch: 9.71 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9714365571773045		[learning rate: 0.00039217]
	Learning Rate: 0.000392169
	LOSS [training: 2.9714365571773045 | validation: 3.20795787469128]
	TIME [epoch: 9.72 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9923769007082184		[learning rate: 0.00039075]
	Learning Rate: 0.000390746
	LOSS [training: 2.9923769007082184 | validation: 3.21041459049094]
	TIME [epoch: 9.73 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9582850225934774		[learning rate: 0.00038933]
	Learning Rate: 0.000389328
	LOSS [training: 2.9582850225934774 | validation: 3.2215687088207496]
	TIME [epoch: 9.72 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966328831177768		[learning rate: 0.00038792]
	Learning Rate: 0.000387915
	LOSS [training: 2.966328831177768 | validation: 3.2329693276812073]
	TIME [epoch: 9.72 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.974272046622701		[learning rate: 0.00038651]
	Learning Rate: 0.000386508
	LOSS [training: 2.974272046622701 | validation: 3.2197650914418707]
	TIME [epoch: 9.73 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.96875933831259		[learning rate: 0.0003851]
	Learning Rate: 0.000385105
	LOSS [training: 2.96875933831259 | validation: 3.207997075070724]
	TIME [epoch: 9.72 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9692949505749398		[learning rate: 0.00038371]
	Learning Rate: 0.000383707
	LOSS [training: 2.9692949505749398 | validation: 3.228121439147376]
	TIME [epoch: 9.72 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.974842777112334		[learning rate: 0.00038231]
	Learning Rate: 0.000382315
	LOSS [training: 2.974842777112334 | validation: 3.223294910324014]
	TIME [epoch: 9.73 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.980212584862798		[learning rate: 0.00038093]
	Learning Rate: 0.000380927
	LOSS [training: 2.980212584862798 | validation: 3.2234283477856662]
	TIME [epoch: 9.72 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.971326953685108		[learning rate: 0.00037954]
	Learning Rate: 0.000379545
	LOSS [training: 2.971326953685108 | validation: 3.208035789445611]
	TIME [epoch: 9.69 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9604576338129385		[learning rate: 0.00037817]
	Learning Rate: 0.000378167
	LOSS [training: 2.9604576338129385 | validation: 3.219565194105761]
	TIME [epoch: 9.71 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.973023292166328		[learning rate: 0.0003768]
	Learning Rate: 0.000376795
	LOSS [training: 2.973023292166328 | validation: 3.2300782551954375]
	TIME [epoch: 9.69 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.970071768788795		[learning rate: 0.00037543]
	Learning Rate: 0.000375428
	LOSS [training: 2.970071768788795 | validation: 3.2026210339937244]
	TIME [epoch: 9.71 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9656065442067905		[learning rate: 0.00037407]
	Learning Rate: 0.000374065
	LOSS [training: 2.9656065442067905 | validation: 3.2124390767707682]
	TIME [epoch: 9.72 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9588196984548696		[learning rate: 0.00037271]
	Learning Rate: 0.000372708
	LOSS [training: 2.9588196984548696 | validation: 3.2082751040454767]
	TIME [epoch: 9.72 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9701690179416866		[learning rate: 0.00037136]
	Learning Rate: 0.000371355
	LOSS [training: 2.9701690179416866 | validation: 3.221169762702816]
	TIME [epoch: 9.71 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9617621018353963		[learning rate: 0.00037001]
	Learning Rate: 0.000370008
	LOSS [training: 2.9617621018353963 | validation: 3.1990302837356985]
	TIME [epoch: 9.71 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.965561268751987		[learning rate: 0.00036866]
	Learning Rate: 0.000368665
	LOSS [training: 2.965561268751987 | validation: 3.212857607545583]
	TIME [epoch: 9.72 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9625557705059884		[learning rate: 0.00036733]
	Learning Rate: 0.000367327
	LOSS [training: 2.9625557705059884 | validation: 3.217424947184053]
	TIME [epoch: 9.7 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963673229284793		[learning rate: 0.00036599]
	Learning Rate: 0.000365994
	LOSS [training: 2.963673229284793 | validation: 3.2464154305508277]
	TIME [epoch: 9.71 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.976806943443256		[learning rate: 0.00036467]
	Learning Rate: 0.000364666
	LOSS [training: 2.976806943443256 | validation: 3.2228892725278695]
	TIME [epoch: 9.72 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.988313808094023		[learning rate: 0.00036334]
	Learning Rate: 0.000363342
	LOSS [training: 2.988313808094023 | validation: 3.2209278897405897]
	TIME [epoch: 9.7 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9687929370041166		[learning rate: 0.00036202]
	Learning Rate: 0.000362024
	LOSS [training: 2.9687929370041166 | validation: 3.2076426084006364]
	TIME [epoch: 9.71 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.965424108619778		[learning rate: 0.00036071]
	Learning Rate: 0.00036071
	LOSS [training: 2.965424108619778 | validation: 3.223585938887354]
	TIME [epoch: 9.73 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9681701185523828		[learning rate: 0.0003594]
	Learning Rate: 0.000359401
	LOSS [training: 2.9681701185523828 | validation: 3.213444807050886]
	TIME [epoch: 9.7 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9680618204230855		[learning rate: 0.0003581]
	Learning Rate: 0.000358096
	LOSS [training: 2.9680618204230855 | validation: 3.2091834117754763]
	TIME [epoch: 9.69 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961142828959842		[learning rate: 0.0003568]
	Learning Rate: 0.000356797
	LOSS [training: 2.961142828959842 | validation: 3.210035530208793]
	TIME [epoch: 9.74 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961771418513861		[learning rate: 0.0003555]
	Learning Rate: 0.000355502
	LOSS [training: 2.961771418513861 | validation: 3.2223702962258165]
	TIME [epoch: 9.72 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9612821903892073		[learning rate: 0.00035421]
	Learning Rate: 0.000354212
	LOSS [training: 2.9612821903892073 | validation: 3.2279812484940567]
	TIME [epoch: 9.72 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9797457905641793		[learning rate: 0.00035293]
	Learning Rate: 0.000352926
	LOSS [training: 2.9797457905641793 | validation: 3.2017911303693585]
	TIME [epoch: 9.73 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9582674230393358		[learning rate: 0.00035165]
	Learning Rate: 0.000351646
	LOSS [training: 2.9582674230393358 | validation: 3.216540722036641]
	TIME [epoch: 9.69 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9607232070085474		[learning rate: 0.00035037]
	Learning Rate: 0.00035037
	LOSS [training: 2.9607232070085474 | validation: 3.2071277312953588]
	TIME [epoch: 9.7 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9581416832072107		[learning rate: 0.0003491]
	Learning Rate: 0.000349098
	LOSS [training: 2.9581416832072107 | validation: 3.2002016085989293]
	TIME [epoch: 9.73 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966069276399024		[learning rate: 0.00034783]
	Learning Rate: 0.000347831
	LOSS [training: 2.966069276399024 | validation: 3.2176681796012043]
	TIME [epoch: 9.7 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9719351099907025		[learning rate: 0.00034657]
	Learning Rate: 0.000346569
	LOSS [training: 2.9719351099907025 | validation: 3.219090509650132]
	TIME [epoch: 9.7 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9576213071616855		[learning rate: 0.00034531]
	Learning Rate: 0.000345311
	LOSS [training: 2.9576213071616855 | validation: 3.2106743094159107]
	TIME [epoch: 9.72 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9639921062076002		[learning rate: 0.00034406]
	Learning Rate: 0.000344058
	LOSS [training: 2.9639921062076002 | validation: 3.2127751677106104]
	TIME [epoch: 9.7 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9648229071040264		[learning rate: 0.00034281]
	Learning Rate: 0.000342809
	LOSS [training: 2.9648229071040264 | validation: 3.2101144016911567]
	TIME [epoch: 9.7 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9785842075396087		[learning rate: 0.00034157]
	Learning Rate: 0.000341565
	LOSS [training: 2.9785842075396087 | validation: 3.2153643479727316]
	TIME [epoch: 9.72 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9641046582555917		[learning rate: 0.00034033]
	Learning Rate: 0.000340326
	LOSS [training: 2.9641046582555917 | validation: 3.2070434916608086]
	TIME [epoch: 9.7 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9644251791816916		[learning rate: 0.00033909]
	Learning Rate: 0.000339091
	LOSS [training: 2.9644251791816916 | validation: 3.209402697094207]
	TIME [epoch: 9.7 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.975382552782682		[learning rate: 0.00033786]
	Learning Rate: 0.00033786
	LOSS [training: 2.975382552782682 | validation: 3.215212063936657]
	TIME [epoch: 9.72 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9632881960488575		[learning rate: 0.00033663]
	Learning Rate: 0.000336634
	LOSS [training: 2.9632881960488575 | validation: 3.213475285253766]
	TIME [epoch: 9.71 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9619225822221806		[learning rate: 0.00033541]
	Learning Rate: 0.000335412
	LOSS [training: 2.9619225822221806 | validation: 3.2139482288228103]
	TIME [epoch: 9.7 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961154691362453		[learning rate: 0.0003342]
	Learning Rate: 0.000334195
	LOSS [training: 2.961154691362453 | validation: 3.210352423142323]
	TIME [epoch: 9.7 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9668909673045345		[learning rate: 0.00033298]
	Learning Rate: 0.000332982
	LOSS [training: 2.9668909673045345 | validation: 3.199784512353914]
	TIME [epoch: 9.71 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963443716145899		[learning rate: 0.00033177]
	Learning Rate: 0.000331774
	LOSS [training: 2.963443716145899 | validation: 3.217348369707344]
	TIME [epoch: 9.7 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9704649615802348		[learning rate: 0.00033057]
	Learning Rate: 0.00033057
	LOSS [training: 2.9704649615802348 | validation: 3.217807525800755]
	TIME [epoch: 9.7 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.964180567458602		[learning rate: 0.00032937]
	Learning Rate: 0.00032937
	LOSS [training: 2.964180567458602 | validation: 3.218908824213502]
	TIME [epoch: 9.71 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.97586984546935		[learning rate: 0.00032817]
	Learning Rate: 0.000328175
	LOSS [training: 2.97586984546935 | validation: 3.2031685350765797]
	TIME [epoch: 9.7 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9584169031162335		[learning rate: 0.00032698]
	Learning Rate: 0.000326984
	LOSS [training: 2.9584169031162335 | validation: 3.1975945180733256]
	TIME [epoch: 9.7 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9674196557479595		[learning rate: 0.0003258]
	Learning Rate: 0.000325797
	LOSS [training: 2.9674196557479595 | validation: 3.1959745128639496]
	TIME [epoch: 9.72 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9616793098066387		[learning rate: 0.00032461]
	Learning Rate: 0.000324615
	LOSS [training: 2.9616793098066387 | validation: 3.216512407370686]
	TIME [epoch: 9.69 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9640793869977573		[learning rate: 0.00032344]
	Learning Rate: 0.000323437
	LOSS [training: 2.9640793869977573 | validation: 3.209408354397073]
	TIME [epoch: 9.7 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9714220553268094		[learning rate: 0.00032226]
	Learning Rate: 0.000322263
	LOSS [training: 2.9714220553268094 | validation: 3.2022775586202785]
	TIME [epoch: 9.72 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9665788708416594		[learning rate: 0.00032109]
	Learning Rate: 0.000321094
	LOSS [training: 2.9665788708416594 | validation: 3.2196296138824336]
	TIME [epoch: 9.7 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9886336872870447		[learning rate: 0.00031993]
	Learning Rate: 0.000319928
	LOSS [training: 2.9886336872870447 | validation: 3.218268965834529]
	TIME [epoch: 9.71 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9657233830965986		[learning rate: 0.00031877]
	Learning Rate: 0.000318767
	LOSS [training: 2.9657233830965986 | validation: 3.201851505760933]
	TIME [epoch: 9.72 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963492391517597		[learning rate: 0.00031761]
	Learning Rate: 0.000317611
	LOSS [training: 2.963492391517597 | validation: 3.21001873642481]
	TIME [epoch: 9.71 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9631095028266037		[learning rate: 0.00031646]
	Learning Rate: 0.000316458
	LOSS [training: 2.9631095028266037 | validation: 3.197004287174618]
	TIME [epoch: 9.71 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966457242676653		[learning rate: 0.00031531]
	Learning Rate: 0.000315309
	LOSS [training: 2.966457242676653 | validation: 3.2426646478866488]
	TIME [epoch: 9.73 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.982231655299556		[learning rate: 0.00031417]
	Learning Rate: 0.000314165
	LOSS [training: 2.982231655299556 | validation: 3.2183081446754835]
	TIME [epoch: 9.71 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966474289470277		[learning rate: 0.00031302]
	Learning Rate: 0.000313025
	LOSS [training: 2.966474289470277 | validation: 3.2128543653951955]
	TIME [epoch: 9.7 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9643132485889216		[learning rate: 0.00031189]
	Learning Rate: 0.000311889
	LOSS [training: 2.9643132485889216 | validation: 3.2089040380479092]
	TIME [epoch: 9.73 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9705808132077958		[learning rate: 0.00031076]
	Learning Rate: 0.000310757
	LOSS [training: 2.9705808132077958 | validation: 3.206999671967907]
	TIME [epoch: 9.71 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9726451215232195		[learning rate: 0.00030963]
	Learning Rate: 0.000309629
	LOSS [training: 2.9726451215232195 | validation: 3.2182517776759303]
	TIME [epoch: 9.7 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.965083614956046		[learning rate: 0.00030851]
	Learning Rate: 0.000308506
	LOSS [training: 2.965083614956046 | validation: 3.2065962378581467]
	TIME [epoch: 9.73 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9666068191906634		[learning rate: 0.00030739]
	Learning Rate: 0.000307386
	LOSS [training: 2.9666068191906634 | validation: 3.1958092586252116]
	TIME [epoch: 9.7 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9585655530224315		[learning rate: 0.00030627]
	Learning Rate: 0.000306271
	LOSS [training: 2.9585655530224315 | validation: 3.203992174088299]
	TIME [epoch: 9.71 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.977074758478399		[learning rate: 0.00030516]
	Learning Rate: 0.000305159
	LOSS [training: 2.977074758478399 | validation: 3.1940817979809957]
	TIME [epoch: 9.71 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9651997611570784		[learning rate: 0.00030405]
	Learning Rate: 0.000304052
	LOSS [training: 2.9651997611570784 | validation: 3.2121913226007126]
	TIME [epoch: 9.71 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9671971389494134		[learning rate: 0.00030295]
	Learning Rate: 0.000302948
	LOSS [training: 2.9671971389494134 | validation: 3.2099690085787866]
	TIME [epoch: 9.71 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9643873692479206		[learning rate: 0.00030185]
	Learning Rate: 0.000301849
	LOSS [training: 2.9643873692479206 | validation: 3.199781805037217]
	TIME [epoch: 9.73 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9619431627868473		[learning rate: 0.00030075]
	Learning Rate: 0.000300753
	LOSS [training: 2.9619431627868473 | validation: 3.2014601315910296]
	TIME [epoch: 9.72 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.965781630675761		[learning rate: 0.00029966]
	Learning Rate: 0.000299662
	LOSS [training: 2.965781630675761 | validation: 3.2033296475665676]
	TIME [epoch: 9.71 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9643745392132774		[learning rate: 0.00029857]
	Learning Rate: 0.000298574
	LOSS [training: 2.9643745392132774 | validation: 3.2131153536121873]
	TIME [epoch: 9.73 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9742163275090006		[learning rate: 0.00029749]
	Learning Rate: 0.000297491
	LOSS [training: 2.9742163275090006 | validation: 3.2127805918385266]
	TIME [epoch: 9.72 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9652434748522034		[learning rate: 0.00029641]
	Learning Rate: 0.000296411
	LOSS [training: 2.9652434748522034 | validation: 3.2201464723773574]
	TIME [epoch: 9.7 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957780714409157		[learning rate: 0.00029534]
	Learning Rate: 0.000295336
	LOSS [training: 2.957780714409157 | validation: 3.2119934154885574]
	TIME [epoch: 9.73 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962744035448817		[learning rate: 0.00029426]
	Learning Rate: 0.000294264
	LOSS [training: 2.962744035448817 | validation: 3.2110270245424477]
	TIME [epoch: 9.72 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.967316849804915		[learning rate: 0.0002932]
	Learning Rate: 0.000293196
	LOSS [training: 2.967316849804915 | validation: 3.2167556966382196]
	TIME [epoch: 9.71 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958572559670743		[learning rate: 0.00029213]
	Learning Rate: 0.000292132
	LOSS [training: 2.958572559670743 | validation: 3.2127533539370177]
	TIME [epoch: 9.73 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.964543835401626		[learning rate: 0.00029107]
	Learning Rate: 0.000291072
	LOSS [training: 2.964543835401626 | validation: 3.2056656504586836]
	TIME [epoch: 9.74 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9580104559850806		[learning rate: 0.00029002]
	Learning Rate: 0.000290015
	LOSS [training: 2.9580104559850806 | validation: 3.212027123010947]
	TIME [epoch: 9.72 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.971534453725412		[learning rate: 0.00028896]
	Learning Rate: 0.000288963
	LOSS [training: 2.971534453725412 | validation: 3.218432249320819]
	TIME [epoch: 9.71 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9635701435172264		[learning rate: 0.00028791]
	Learning Rate: 0.000287914
	LOSS [training: 2.9635701435172264 | validation: 3.2072768102163924]
	TIME [epoch: 9.74 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.977073600402295		[learning rate: 0.00028687]
	Learning Rate: 0.000286869
	LOSS [training: 2.977073600402295 | validation: 3.2097025568221103]
	TIME [epoch: 9.72 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9631920350466734		[learning rate: 0.00028583]
	Learning Rate: 0.000285828
	LOSS [training: 2.9631920350466734 | validation: 3.206803348636828]
	TIME [epoch: 9.72 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.968348272706644		[learning rate: 0.00028479]
	Learning Rate: 0.000284791
	LOSS [training: 2.968348272706644 | validation: 3.2026631914751444]
	TIME [epoch: 9.73 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9660124904866363		[learning rate: 0.00028376]
	Learning Rate: 0.000283758
	LOSS [training: 2.9660124904866363 | validation: 3.2294972295589215]
	TIME [epoch: 9.7 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.965071974712917		[learning rate: 0.00028273]
	Learning Rate: 0.000282728
	LOSS [training: 2.965071974712917 | validation: 3.2085022437074624]
	TIME [epoch: 9.71 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961424358254745		[learning rate: 0.0002817]
	Learning Rate: 0.000281702
	LOSS [training: 2.961424358254745 | validation: 3.206294535778892]
	TIME [epoch: 9.74 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966296180615635		[learning rate: 0.00028068]
	Learning Rate: 0.000280679
	LOSS [training: 2.966296180615635 | validation: 3.1998925366068973]
	TIME [epoch: 9.72 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960191114877193		[learning rate: 0.00027966]
	Learning Rate: 0.000279661
	LOSS [training: 2.960191114877193 | validation: 3.2111602982650185]
	TIME [epoch: 9.71 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9574204974627767		[learning rate: 0.00027865]
	Learning Rate: 0.000278646
	LOSS [training: 2.9574204974627767 | validation: 3.204599172986358]
	TIME [epoch: 9.73 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9589524003247574		[learning rate: 0.00027763]
	Learning Rate: 0.000277635
	LOSS [training: 2.9589524003247574 | validation: 3.2103374582325883]
	TIME [epoch: 9.7 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956900103752397		[learning rate: 0.00027663]
	Learning Rate: 0.000276627
	LOSS [training: 2.956900103752397 | validation: 3.2086748577414586]
	TIME [epoch: 9.72 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9646661001478147		[learning rate: 0.00027562]
	Learning Rate: 0.000275623
	LOSS [training: 2.9646661001478147 | validation: 3.204350566397951]
	TIME [epoch: 9.73 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9628339274690974		[learning rate: 0.00027462]
	Learning Rate: 0.000274623
	LOSS [training: 2.9628339274690974 | validation: 3.201474126064552]
	TIME [epoch: 9.71 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9606114467812175		[learning rate: 0.00027363]
	Learning Rate: 0.000273626
	LOSS [training: 2.9606114467812175 | validation: 3.200610406537827]
	TIME [epoch: 9.71 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9557370373841296		[learning rate: 0.00027263]
	Learning Rate: 0.000272633
	LOSS [training: 2.9557370373841296 | validation: 3.2085255020212604]
	TIME [epoch: 9.73 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.971823027292758		[learning rate: 0.00027164]
	Learning Rate: 0.000271644
	LOSS [training: 2.971823027292758 | validation: 3.2225333984213864]
	TIME [epoch: 9.71 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962160944930816		[learning rate: 0.00027066]
	Learning Rate: 0.000270658
	LOSS [training: 2.962160944930816 | validation: 3.209084398934333]
	TIME [epoch: 9.71 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9648110853672973		[learning rate: 0.00026968]
	Learning Rate: 0.000269676
	LOSS [training: 2.9648110853672973 | validation: 3.2179445953323285]
	TIME [epoch: 9.73 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.978092885971393		[learning rate: 0.0002687]
	Learning Rate: 0.000268697
	LOSS [training: 2.978092885971393 | validation: 3.1996164492028654]
	TIME [epoch: 9.72 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.970789716252058		[learning rate: 0.00026772]
	Learning Rate: 0.000267722
	LOSS [training: 2.970789716252058 | validation: 3.2096403937483617]
	TIME [epoch: 9.7 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9569167872963105		[learning rate: 0.00026675]
	Learning Rate: 0.000266751
	LOSS [training: 2.9569167872963105 | validation: 3.2097349175472254]
	TIME [epoch: 9.73 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9659137452397952		[learning rate: 0.00026578]
	Learning Rate: 0.000265782
	LOSS [training: 2.9659137452397952 | validation: 3.2169971850088257]
	TIME [epoch: 9.71 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0019907199837688		[learning rate: 0.00026482]
	Learning Rate: 0.000264818
	LOSS [training: 3.0019907199837688 | validation: 3.262383283412811]
	TIME [epoch: 9.7 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9874789273013658		[learning rate: 0.00026386]
	Learning Rate: 0.000263857
	LOSS [training: 2.9874789273013658 | validation: 3.21610902795021]
	TIME [epoch: 9.72 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957411528532375		[learning rate: 0.0002629]
	Learning Rate: 0.000262899
	LOSS [training: 2.957411528532375 | validation: 3.2054822284938576]
	TIME [epoch: 9.71 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.96948804825825		[learning rate: 0.00026195]
	Learning Rate: 0.000261945
	LOSS [training: 2.96948804825825 | validation: 3.2181989998154577]
	TIME [epoch: 9.71 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958208277545567		[learning rate: 0.00026099]
	Learning Rate: 0.000260995
	LOSS [training: 2.958208277545567 | validation: 3.205509898603631]
	TIME [epoch: 9.72 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.965778757029582		[learning rate: 0.00026005]
	Learning Rate: 0.000260047
	LOSS [training: 2.965778757029582 | validation: 3.204567148406228]
	TIME [epoch: 9.72 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9594253198124156		[learning rate: 0.0002591]
	Learning Rate: 0.000259104
	LOSS [training: 2.9594253198124156 | validation: 3.2077625380420294]
	TIME [epoch: 9.7 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966117366066089		[learning rate: 0.00025816]
	Learning Rate: 0.000258163
	LOSS [training: 2.966117366066089 | validation: 3.2163702999891775]
	TIME [epoch: 9.7 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9684348425811677		[learning rate: 0.00025723]
	Learning Rate: 0.000257227
	LOSS [training: 2.9684348425811677 | validation: 3.2260364317169836]
	TIME [epoch: 9.73 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9812135898445815		[learning rate: 0.00025629]
	Learning Rate: 0.000256293
	LOSS [training: 2.9812135898445815 | validation: 3.211441298976576]
	TIME [epoch: 9.7 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9615573950770595		[learning rate: 0.00025536]
	Learning Rate: 0.000255363
	LOSS [training: 2.9615573950770595 | validation: 3.207943952736182]
	TIME [epoch: 9.69 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960098896461507		[learning rate: 0.00025444]
	Learning Rate: 0.000254436
	LOSS [training: 2.960098896461507 | validation: 3.2111311875683772]
	TIME [epoch: 9.73 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.96193867208049		[learning rate: 0.00025351]
	Learning Rate: 0.000253513
	LOSS [training: 2.96193867208049 | validation: 3.2035983630510088]
	TIME [epoch: 9.71 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9597042952646277		[learning rate: 0.00025259]
	Learning Rate: 0.000252593
	LOSS [training: 2.9597042952646277 | validation: 3.2164033853677934]
	TIME [epoch: 9.72 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962101312899323		[learning rate: 0.00025168]
	Learning Rate: 0.000251676
	LOSS [training: 2.962101312899323 | validation: 3.2044978710305148]
	TIME [epoch: 9.74 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961147564845936		[learning rate: 0.00025076]
	Learning Rate: 0.000250763
	LOSS [training: 2.961147564845936 | validation: 3.220717692258358]
	TIME [epoch: 9.7 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961996823265891		[learning rate: 0.00024985]
	Learning Rate: 0.000249853
	LOSS [training: 2.961996823265891 | validation: 3.228611292027057]
	TIME [epoch: 9.71 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.964230711400866		[learning rate: 0.00024895]
	Learning Rate: 0.000248946
	LOSS [training: 2.964230711400866 | validation: 3.2174611721672823]
	TIME [epoch: 9.73 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9605385992341633		[learning rate: 0.00024804]
	Learning Rate: 0.000248043
	LOSS [training: 2.9605385992341633 | validation: 3.2063556541183096]
	TIME [epoch: 9.71 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9540336792956574		[learning rate: 0.00024714]
	Learning Rate: 0.000247142
	LOSS [training: 2.9540336792956574 | validation: 3.2094849250145576]
	TIME [epoch: 9.71 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9567876485719347		[learning rate: 0.00024625]
	Learning Rate: 0.000246246
	LOSS [training: 2.9567876485719347 | validation: 3.19767139873466]
	TIME [epoch: 9.74 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9625602717901875		[learning rate: 0.00024535]
	Learning Rate: 0.000245352
	LOSS [training: 2.9625602717901875 | validation: 3.224388726585539]
	TIME [epoch: 9.71 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9715610812260826		[learning rate: 0.00024446]
	Learning Rate: 0.000244462
	LOSS [training: 2.9715610812260826 | validation: 3.1952150645423467]
	TIME [epoch: 9.71 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962318524283641		[learning rate: 0.00024357]
	Learning Rate: 0.000243574
	LOSS [training: 2.962318524283641 | validation: 3.2024372665876952]
	TIME [epoch: 9.73 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9555661125556987		[learning rate: 0.00024269]
	Learning Rate: 0.00024269
	LOSS [training: 2.9555661125556987 | validation: 3.2096635872218133]
	TIME [epoch: 9.72 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9622652632080526		[learning rate: 0.00024181]
	Learning Rate: 0.00024181
	LOSS [training: 2.9622652632080526 | validation: 3.20086041521828]
	TIME [epoch: 9.72 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9568714259132127		[learning rate: 0.00024093]
	Learning Rate: 0.000240932
	LOSS [training: 2.9568714259132127 | validation: 3.1984589011548166]
	TIME [epoch: 9.74 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9618588985827032		[learning rate: 0.00024006]
	Learning Rate: 0.000240058
	LOSS [training: 2.9618588985827032 | validation: 3.2203675953725996]
	TIME [epoch: 9.72 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963283851449681		[learning rate: 0.00023919]
	Learning Rate: 0.000239187
	LOSS [training: 2.963283851449681 | validation: 3.205745635474285]
	TIME [epoch: 9.7 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958272857376755		[learning rate: 0.00023832]
	Learning Rate: 0.000238319
	LOSS [training: 2.958272857376755 | validation: 3.2043977447709096]
	TIME [epoch: 9.72 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9704640077523057		[learning rate: 0.00023745]
	Learning Rate: 0.000237454
	LOSS [training: 2.9704640077523057 | validation: 3.213946833152135]
	TIME [epoch: 9.71 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9706414306862436		[learning rate: 0.00023659]
	Learning Rate: 0.000236592
	LOSS [training: 2.9706414306862436 | validation: 3.202188640150473]
	TIME [epoch: 9.71 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955522582855619		[learning rate: 0.00023573]
	Learning Rate: 0.000235733
	LOSS [training: 2.955522582855619 | validation: 3.2153531190357394]
	TIME [epoch: 9.71 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963107447563032		[learning rate: 0.00023488]
	Learning Rate: 0.000234878
	LOSS [training: 2.963107447563032 | validation: 3.200977435858623]
	TIME [epoch: 9.72 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9595310838630224		[learning rate: 0.00023403]
	Learning Rate: 0.000234026
	LOSS [training: 2.9595310838630224 | validation: 3.223833584221661]
	TIME [epoch: 9.71 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9672484505715175		[learning rate: 0.00023318]
	Learning Rate: 0.000233176
	LOSS [training: 2.9672484505715175 | validation: 3.207583401241742]
	TIME [epoch: 9.72 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9608267597741458		[learning rate: 0.00023233]
	Learning Rate: 0.00023233
	LOSS [training: 2.9608267597741458 | validation: 3.2111390025978075]
	TIME [epoch: 9.73 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9588993054307555		[learning rate: 0.00023149]
	Learning Rate: 0.000231487
	LOSS [training: 2.9588993054307555 | validation: 3.2042286720727646]
	TIME [epoch: 9.72 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958135174335046		[learning rate: 0.00023065]
	Learning Rate: 0.000230647
	LOSS [training: 2.958135174335046 | validation: 3.2152258646046854]
	TIME [epoch: 9.71 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956994028141398		[learning rate: 0.00022981]
	Learning Rate: 0.00022981
	LOSS [training: 2.956994028141398 | validation: 3.208033222978327]
	TIME [epoch: 9.73 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9577602559386773		[learning rate: 0.00022898]
	Learning Rate: 0.000228976
	LOSS [training: 2.9577602559386773 | validation: 3.197154766141773]
	TIME [epoch: 9.72 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960670295741167		[learning rate: 0.00022814]
	Learning Rate: 0.000228145
	LOSS [training: 2.960670295741167 | validation: 3.2181444263198067]
	TIME [epoch: 9.7 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963607404580322		[learning rate: 0.00022732]
	Learning Rate: 0.000227317
	LOSS [training: 2.963607404580322 | validation: 3.2086899017709603]
	TIME [epoch: 9.72 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961639474882797		[learning rate: 0.00022649]
	Learning Rate: 0.000226492
	LOSS [training: 2.961639474882797 | validation: 3.2077275119835313]
	TIME [epoch: 9.73 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9631759647243077		[learning rate: 0.00022567]
	Learning Rate: 0.00022567
	LOSS [training: 2.9631759647243077 | validation: 3.2120698091617284]
	TIME [epoch: 9.71 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9593465490423347		[learning rate: 0.00022485]
	Learning Rate: 0.000224851
	LOSS [training: 2.9593465490423347 | validation: 3.1981068947538493]
	TIME [epoch: 9.74 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9676721889030326		[learning rate: 0.00022403]
	Learning Rate: 0.000224035
	LOSS [training: 2.9676721889030326 | validation: 3.228117995874131]
	TIME [epoch: 9.71 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9773218594234563		[learning rate: 0.00022322]
	Learning Rate: 0.000223222
	LOSS [training: 2.9773218594234563 | validation: 3.2127871410079045]
	TIME [epoch: 9.71 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9551863014582795		[learning rate: 0.00022241]
	Learning Rate: 0.000222412
	LOSS [training: 2.9551863014582795 | validation: 3.206363902041534]
	TIME [epoch: 9.73 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954500458527529		[learning rate: 0.0002216]
	Learning Rate: 0.000221605
	LOSS [training: 2.954500458527529 | validation: 3.212268433214598]
	TIME [epoch: 9.71 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962397414478295		[learning rate: 0.0002208]
	Learning Rate: 0.0002208
	LOSS [training: 2.962397414478295 | validation: 3.1889507690664307]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_1149.pth
	Model improved!!!
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9622821837999593		[learning rate: 0.00022]
	Learning Rate: 0.000219999
	LOSS [training: 2.9622821837999593 | validation: 3.21014584393985]
	TIME [epoch: 9.73 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960157209141083		[learning rate: 0.0002192]
	Learning Rate: 0.000219201
	LOSS [training: 2.960157209141083 | validation: 3.2279061853851214]
	TIME [epoch: 9.71 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9732588432678755		[learning rate: 0.00021841]
	Learning Rate: 0.000218405
	LOSS [training: 2.9732588432678755 | validation: 3.2060387898725464]
	TIME [epoch: 9.71 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9544420049715234		[learning rate: 0.00021761]
	Learning Rate: 0.000217613
	LOSS [training: 2.9544420049715234 | validation: 3.1976559593133094]
	TIME [epoch: 9.73 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966519759204097		[learning rate: 0.00021682]
	Learning Rate: 0.000216823
	LOSS [training: 2.966519759204097 | validation: 3.211470899084036]
	TIME [epoch: 9.72 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953772307786978		[learning rate: 0.00021604]
	Learning Rate: 0.000216036
	LOSS [training: 2.953772307786978 | validation: 3.201391666818801]
	TIME [epoch: 9.71 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9579544335750088		[learning rate: 0.00021525]
	Learning Rate: 0.000215252
	LOSS [training: 2.9579544335750088 | validation: 3.208454022444393]
	TIME [epoch: 9.74 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.971096109225023		[learning rate: 0.00021447]
	Learning Rate: 0.000214471
	LOSS [training: 2.971096109225023 | validation: 3.2091384738445363]
	TIME [epoch: 9.72 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9616289197533137		[learning rate: 0.00021369]
	Learning Rate: 0.000213693
	LOSS [training: 2.9616289197533137 | validation: 3.2106562067114033]
	TIME [epoch: 9.72 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957468312362586		[learning rate: 0.00021292]
	Learning Rate: 0.000212917
	LOSS [training: 2.957468312362586 | validation: 3.214035334287904]
	TIME [epoch: 9.72 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9649835096037025		[learning rate: 0.00021214]
	Learning Rate: 0.000212144
	LOSS [training: 2.9649835096037025 | validation: 3.2067330180485634]
	TIME [epoch: 9.72 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9582671097711364		[learning rate: 0.00021137]
	Learning Rate: 0.000211375
	LOSS [training: 2.9582671097711364 | validation: 3.204986812673057]
	TIME [epoch: 9.71 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9575390214134734		[learning rate: 0.00021061]
	Learning Rate: 0.000210607
	LOSS [training: 2.9575390214134734 | validation: 3.2063623068029283]
	TIME [epoch: 9.73 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960504240339826		[learning rate: 0.00020984]
	Learning Rate: 0.000209843
	LOSS [training: 2.960504240339826 | validation: 3.2029666252378526]
	TIME [epoch: 9.71 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963226234392497		[learning rate: 0.00020908]
	Learning Rate: 0.000209082
	LOSS [training: 2.963226234392497 | validation: 3.1988429434550234]
	TIME [epoch: 9.71 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9561592595474186		[learning rate: 0.00020832]
	Learning Rate: 0.000208323
	LOSS [training: 2.9561592595474186 | validation: 3.2139448270120003]
	TIME [epoch: 9.72 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9643134986056046		[learning rate: 0.00020757]
	Learning Rate: 0.000207567
	LOSS [training: 2.9643134986056046 | validation: 3.208572803911484]
	TIME [epoch: 9.73 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9649123501806276		[learning rate: 0.00020681]
	Learning Rate: 0.000206814
	LOSS [training: 2.9649123501806276 | validation: 3.221188305890477]
	TIME [epoch: 9.71 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9753150066913605		[learning rate: 0.00020606]
	Learning Rate: 0.000206063
	LOSS [training: 2.9753150066913605 | validation: 3.200395142108458]
	TIME [epoch: 9.72 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.965730886384101		[learning rate: 0.00020532]
	Learning Rate: 0.000205315
	LOSS [training: 2.965730886384101 | validation: 3.2074323230452535]
	TIME [epoch: 9.73 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958300673656997		[learning rate: 0.00020457]
	Learning Rate: 0.00020457
	LOSS [training: 2.958300673656997 | validation: 3.212508205346517]
	TIME [epoch: 9.71 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9579687688675937		[learning rate: 0.00020383]
	Learning Rate: 0.000203828
	LOSS [training: 2.9579687688675937 | validation: 3.2019888497811775]
	TIME [epoch: 9.7 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9668923276144525		[learning rate: 0.00020309]
	Learning Rate: 0.000203088
	LOSS [training: 2.9668923276144525 | validation: 3.200416431516141]
	TIME [epoch: 9.73 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961804667547118		[learning rate: 0.00020235]
	Learning Rate: 0.000202351
	LOSS [training: 2.961804667547118 | validation: 3.2013788577486513]
	TIME [epoch: 9.71 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9690596759450796		[learning rate: 0.00020162]
	Learning Rate: 0.000201617
	LOSS [training: 2.9690596759450796 | validation: 3.227870185690519]
	TIME [epoch: 9.72 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9652796227351343		[learning rate: 0.00020088]
	Learning Rate: 0.000200885
	LOSS [training: 2.9652796227351343 | validation: 3.2041315434500635]
	TIME [epoch: 9.73 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9608773454823547		[learning rate: 0.00020016]
	Learning Rate: 0.000200156
	LOSS [training: 2.9608773454823547 | validation: 3.2090103658122153]
	TIME [epoch: 9.71 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9568871675856476		[learning rate: 0.00019943]
	Learning Rate: 0.00019943
	LOSS [training: 2.9568871675856476 | validation: 3.1975844352890634]
	TIME [epoch: 9.7 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960801688002042		[learning rate: 0.00019871]
	Learning Rate: 0.000198706
	LOSS [training: 2.960801688002042 | validation: 3.202648969157433]
	TIME [epoch: 9.73 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961968636706169		[learning rate: 0.00019798]
	Learning Rate: 0.000197985
	LOSS [training: 2.961968636706169 | validation: 3.212124575885607]
	TIME [epoch: 9.7 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9626923464892165		[learning rate: 0.00019727]
	Learning Rate: 0.000197266
	LOSS [training: 2.9626923464892165 | validation: 3.198798409017576]
	TIME [epoch: 9.7 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961733040825338		[learning rate: 0.00019655]
	Learning Rate: 0.00019655
	LOSS [training: 2.961733040825338 | validation: 3.208480099960444]
	TIME [epoch: 9.73 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9569282903096066		[learning rate: 0.00019584]
	Learning Rate: 0.000195837
	LOSS [training: 2.9569282903096066 | validation: 3.2061392106959716]
	TIME [epoch: 9.71 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9562777509619367		[learning rate: 0.00019513]
	Learning Rate: 0.000195126
	LOSS [training: 2.9562777509619367 | validation: 3.2049489837367653]
	TIME [epoch: 9.71 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952394649213216		[learning rate: 0.00019442]
	Learning Rate: 0.000194418
	LOSS [training: 2.952394649213216 | validation: 3.2155584181546204]
	TIME [epoch: 9.73 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9636999324128874		[learning rate: 0.00019371]
	Learning Rate: 0.000193713
	LOSS [training: 2.9636999324128874 | validation: 3.198437572821522]
	TIME [epoch: 9.71 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952765163517679		[learning rate: 0.00019301]
	Learning Rate: 0.00019301
	LOSS [training: 2.952765163517679 | validation: 3.2087644581398065]
	TIME [epoch: 9.71 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9588598357510874		[learning rate: 0.00019231]
	Learning Rate: 0.000192309
	LOSS [training: 2.9588598357510874 | validation: 3.215825267075015]
	TIME [epoch: 9.72 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9604363549744326		[learning rate: 0.00019161]
	Learning Rate: 0.000191611
	LOSS [training: 2.9604363549744326 | validation: 3.201524874715525]
	TIME [epoch: 9.72 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9629334917159933		[learning rate: 0.00019092]
	Learning Rate: 0.000190916
	LOSS [training: 2.9629334917159933 | validation: 3.199947873790144]
	TIME [epoch: 9.72 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9595987050864965		[learning rate: 0.00019022]
	Learning Rate: 0.000190223
	LOSS [training: 2.9595987050864965 | validation: 3.212770242701913]
	TIME [epoch: 9.72 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962042666080803		[learning rate: 0.00018953]
	Learning Rate: 0.000189533
	LOSS [training: 2.962042666080803 | validation: 3.2026331230638574]
	TIME [epoch: 9.72 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9531791551430504		[learning rate: 0.00018884]
	Learning Rate: 0.000188845
	LOSS [training: 2.9531791551430504 | validation: 3.2096257820746317]
	TIME [epoch: 9.7 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95213624756459		[learning rate: 0.00018816]
	Learning Rate: 0.00018816
	LOSS [training: 2.95213624756459 | validation: 3.195691778887589]
	TIME [epoch: 9.72 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9624112660954998		[learning rate: 0.00018748]
	Learning Rate: 0.000187477
	LOSS [training: 2.9624112660954998 | validation: 3.2033958422625144]
	TIME [epoch: 9.7 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95109773197035		[learning rate: 0.0001868]
	Learning Rate: 0.000186796
	LOSS [training: 2.95109773197035 | validation: 3.2014034971168277]
	TIME [epoch: 9.7 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9630727913643113		[learning rate: 0.00018612]
	Learning Rate: 0.000186119
	LOSS [training: 2.9630727913643113 | validation: 3.207129800666138]
	TIME [epoch: 9.73 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9588596537468232		[learning rate: 0.00018544]
	Learning Rate: 0.000185443
	LOSS [training: 2.9588596537468232 | validation: 3.2035375308236866]
	TIME [epoch: 9.7 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9528780076463383		[learning rate: 0.00018477]
	Learning Rate: 0.00018477
	LOSS [training: 2.9528780076463383 | validation: 3.201386408769269]
	TIME [epoch: 9.69 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951800292586074		[learning rate: 0.0001841]
	Learning Rate: 0.000184099
	LOSS [training: 2.951800292586074 | validation: 3.203614870733892]
	TIME [epoch: 9.7 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9622513004894366		[learning rate: 0.00018343]
	Learning Rate: 0.000183431
	LOSS [training: 2.9622513004894366 | validation: 3.214287551003871]
	TIME [epoch: 9.73 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956131968655641		[learning rate: 0.00018277]
	Learning Rate: 0.000182766
	LOSS [training: 2.956131968655641 | validation: 3.2229193108332557]
	TIME [epoch: 9.7 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9633001368633307		[learning rate: 0.0001821]
	Learning Rate: 0.000182102
	LOSS [training: 2.9633001368633307 | validation: 3.1954519358018936]
	TIME [epoch: 9.71 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954538311759383		[learning rate: 0.00018144]
	Learning Rate: 0.000181442
	LOSS [training: 2.954538311759383 | validation: 3.2015163453781748]
	TIME [epoch: 9.72 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956722313907112		[learning rate: 0.00018078]
	Learning Rate: 0.000180783
	LOSS [training: 2.956722313907112 | validation: 3.2029184824259755]
	TIME [epoch: 9.71 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9542141110437803		[learning rate: 0.00018013]
	Learning Rate: 0.000180127
	LOSS [training: 2.9542141110437803 | validation: 3.212028809292259]
	TIME [epoch: 9.7 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9628855662947196		[learning rate: 0.00017947]
	Learning Rate: 0.000179473
	LOSS [training: 2.9628855662947196 | validation: 3.2009553445568844]
	TIME [epoch: 9.73 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956109603486239		[learning rate: 0.00017882]
	Learning Rate: 0.000178822
	LOSS [training: 2.956109603486239 | validation: 3.2077714297823254]
	TIME [epoch: 9.7 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9569862119847574		[learning rate: 0.00017817]
	Learning Rate: 0.000178173
	LOSS [training: 2.9569862119847574 | validation: 3.2023242410308037]
	TIME [epoch: 9.7 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957219056869897		[learning rate: 0.00017753]
	Learning Rate: 0.000177526
	LOSS [training: 2.957219056869897 | validation: 3.2018125767206027]
	TIME [epoch: 9.73 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9597020093326987		[learning rate: 0.00017688]
	Learning Rate: 0.000176882
	LOSS [training: 2.9597020093326987 | validation: 3.2066050146599014]
	TIME [epoch: 9.71 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9659771483730157		[learning rate: 0.00017624]
	Learning Rate: 0.00017624
	LOSS [training: 2.9659771483730157 | validation: 3.2157317537893086]
	TIME [epoch: 9.7 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9609592306757806		[learning rate: 0.0001756]
	Learning Rate: 0.000175601
	LOSS [training: 2.9609592306757806 | validation: 3.2057814689266637]
	TIME [epoch: 9.73 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.964522848987566		[learning rate: 0.00017496]
	Learning Rate: 0.000174964
	LOSS [training: 2.964522848987566 | validation: 3.2149602970055073]
	TIME [epoch: 9.7 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9596280147265093		[learning rate: 0.00017433]
	Learning Rate: 0.000174329
	LOSS [training: 2.9596280147265093 | validation: 3.2029681926189197]
	TIME [epoch: 9.71 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957665120715084		[learning rate: 0.0001737]
	Learning Rate: 0.000173696
	LOSS [training: 2.957665120715084 | validation: 3.202637102134511]
	TIME [epoch: 9.72 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9597947811609986		[learning rate: 0.00017307]
	Learning Rate: 0.000173066
	LOSS [training: 2.9597947811609986 | validation: 3.1986344997427647]
	TIME [epoch: 9.7 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9571947618482923		[learning rate: 0.00017244]
	Learning Rate: 0.000172437
	LOSS [training: 2.9571947618482923 | validation: 3.1955334898099115]
	TIME [epoch: 9.71 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953095637706198		[learning rate: 0.00017181]
	Learning Rate: 0.000171812
	LOSS [training: 2.953095637706198 | validation: 3.2012544465886807]
	TIME [epoch: 9.73 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959524076736167		[learning rate: 0.00017119]
	Learning Rate: 0.000171188
	LOSS [training: 2.959524076736167 | validation: 3.2104635473077368]
	TIME [epoch: 9.71 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958762249904446		[learning rate: 0.00017057]
	Learning Rate: 0.000170567
	LOSS [training: 2.958762249904446 | validation: 3.2092238443196166]
	TIME [epoch: 9.71 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9548401123229326		[learning rate: 0.00016995]
	Learning Rate: 0.000169948
	LOSS [training: 2.9548401123229326 | validation: 3.209976527990997]
	TIME [epoch: 9.73 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9603638764126603		[learning rate: 0.00016933]
	Learning Rate: 0.000169331
	LOSS [training: 2.9603638764126603 | validation: 3.205208445671376]
	TIME [epoch: 9.72 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9613189096745756		[learning rate: 0.00016872]
	Learning Rate: 0.000168717
	LOSS [training: 2.9613189096745756 | validation: 3.2315365270579175]
	TIME [epoch: 9.71 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9834846470257856		[learning rate: 0.0001681]
	Learning Rate: 0.000168104
	LOSS [training: 2.9834846470257856 | validation: 3.2114995475724415]
	TIME [epoch: 9.74 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954129369474374		[learning rate: 0.00016749]
	Learning Rate: 0.000167494
	LOSS [training: 2.954129369474374 | validation: 3.2048509287269544]
	TIME [epoch: 9.7 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9618158397148027		[learning rate: 0.00016689]
	Learning Rate: 0.000166886
	LOSS [training: 2.9618158397148027 | validation: 3.2198887686315607]
	TIME [epoch: 9.7 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.965690291633506		[learning rate: 0.00016628]
	Learning Rate: 0.000166281
	LOSS [training: 2.965690291633506 | validation: 3.208249223990508]
	TIME [epoch: 9.72 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953134948690611		[learning rate: 0.00016568]
	Learning Rate: 0.000165677
	LOSS [training: 2.953134948690611 | validation: 3.202271597252291]
	TIME [epoch: 9.71 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9599907672922066		[learning rate: 0.00016508]
	Learning Rate: 0.000165076
	LOSS [training: 2.9599907672922066 | validation: 3.2023356839106656]
	TIME [epoch: 9.71 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9585116600847505		[learning rate: 0.00016448]
	Learning Rate: 0.000164477
	LOSS [training: 2.9585116600847505 | validation: 3.1960430986049606]
	TIME [epoch: 9.71 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9575219820333887		[learning rate: 0.00016388]
	Learning Rate: 0.00016388
	LOSS [training: 2.9575219820333887 | validation: 3.200282449068327]
	TIME [epoch: 9.72 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9622000835491504		[learning rate: 0.00016329]
	Learning Rate: 0.000163285
	LOSS [training: 2.9622000835491504 | validation: 3.217206706189498]
	TIME [epoch: 9.69 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9601979462226833		[learning rate: 0.00016269]
	Learning Rate: 0.000162693
	LOSS [training: 2.9601979462226833 | validation: 3.2063812544813226]
	TIME [epoch: 9.7 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9551300783405594		[learning rate: 0.0001621]
	Learning Rate: 0.000162102
	LOSS [training: 2.9551300783405594 | validation: 3.2078267007944654]
	TIME [epoch: 9.71 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966549170975861		[learning rate: 0.00016151]
	Learning Rate: 0.000161514
	LOSS [training: 2.966549170975861 | validation: 3.2057707984654895]
	TIME [epoch: 9.7 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9571238828231126		[learning rate: 0.00016093]
	Learning Rate: 0.000160928
	LOSS [training: 2.9571238828231126 | validation: 3.209048904452634]
	TIME [epoch: 9.71 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9649995443843222		[learning rate: 0.00016034]
	Learning Rate: 0.000160344
	LOSS [training: 2.9649995443843222 | validation: 3.219518578856088]
	TIME [epoch: 9.73 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955710748663912		[learning rate: 0.00015976]
	Learning Rate: 0.000159762
	LOSS [training: 2.955710748663912 | validation: 3.20981884307737]
	TIME [epoch: 9.7 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951603607942764		[learning rate: 0.00015918]
	Learning Rate: 0.000159182
	LOSS [training: 2.951603607942764 | validation: 3.195378056465249]
	TIME [epoch: 9.7 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9604036039862134		[learning rate: 0.0001586]
	Learning Rate: 0.000158605
	LOSS [training: 2.9604036039862134 | validation: 3.2020226204965603]
	TIME [epoch: 9.73 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9556233314347855		[learning rate: 0.00015803]
	Learning Rate: 0.000158029
	LOSS [training: 2.9556233314347855 | validation: 3.1970503500353344]
	TIME [epoch: 9.71 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955731845708303		[learning rate: 0.00015746]
	Learning Rate: 0.000157456
	LOSS [training: 2.955731845708303 | validation: 3.2159756931371706]
	TIME [epoch: 9.69 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954266974342647		[learning rate: 0.00015688]
	Learning Rate: 0.000156884
	LOSS [training: 2.954266974342647 | validation: 3.2130351818011174]
	TIME [epoch: 9.73 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9588103567435136		[learning rate: 0.00015631]
	Learning Rate: 0.000156315
	LOSS [training: 2.9588103567435136 | validation: 3.210883127305684]
	TIME [epoch: 9.7 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954468672086957		[learning rate: 0.00015575]
	Learning Rate: 0.000155748
	LOSS [training: 2.954468672086957 | validation: 3.201811570895691]
	TIME [epoch: 9.7 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9609668631807238		[learning rate: 0.00015518]
	Learning Rate: 0.000155182
	LOSS [training: 2.9609668631807238 | validation: 3.203954016281902]
	TIME [epoch: 9.73 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9504571549667977		[learning rate: 0.00015462]
	Learning Rate: 0.000154619
	LOSS [training: 2.9504571549667977 | validation: 3.192847669131204]
	TIME [epoch: 9.71 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955203051131588		[learning rate: 0.00015406]
	Learning Rate: 0.000154058
	LOSS [training: 2.955203051131588 | validation: 3.2129604188087093]
	TIME [epoch: 9.7 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9535572360792886		[learning rate: 0.0001535]
	Learning Rate: 0.000153499
	LOSS [training: 2.9535572360792886 | validation: 3.185697482597513]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_1249.pth
	Model improved!!!
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9605009097998383		[learning rate: 0.00015294]
	Learning Rate: 0.000152942
	LOSS [training: 2.9605009097998383 | validation: 3.2079658804941227]
	TIME [epoch: 9.72 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9561643119124925		[learning rate: 0.00015239]
	Learning Rate: 0.000152387
	LOSS [training: 2.9561643119124925 | validation: 3.20665377471786]
	TIME [epoch: 9.72 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9633337825307113		[learning rate: 0.00015183]
	Learning Rate: 0.000151834
	LOSS [training: 2.9633337825307113 | validation: 3.2044076541473734]
	TIME [epoch: 9.75 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9532888070972434		[learning rate: 0.00015128]
	Learning Rate: 0.000151283
	LOSS [training: 2.9532888070972434 | validation: 3.2094524534140327]
	TIME [epoch: 9.73 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956300205146045		[learning rate: 0.00015073]
	Learning Rate: 0.000150734
	LOSS [training: 2.956300205146045 | validation: 3.1950598303946367]
	TIME [epoch: 9.72 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955290736529709		[learning rate: 0.00015019]
	Learning Rate: 0.000150187
	LOSS [training: 2.955290736529709 | validation: 3.2031250470728474]
	TIME [epoch: 9.74 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9617006506000987		[learning rate: 0.00014964]
	Learning Rate: 0.000149642
	LOSS [training: 2.9617006506000987 | validation: 3.19368129691238]
	TIME [epoch: 9.73 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9602894320370723		[learning rate: 0.0001491]
	Learning Rate: 0.000149099
	LOSS [training: 2.9602894320370723 | validation: 3.208742566190508]
	TIME [epoch: 9.72 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9576808407163035		[learning rate: 0.00014856]
	Learning Rate: 0.000148558
	LOSS [training: 2.9576808407163035 | validation: 3.2145433236882135]
	TIME [epoch: 9.73 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9570607450464736		[learning rate: 0.00014802]
	Learning Rate: 0.000148018
	LOSS [training: 2.9570607450464736 | validation: 3.195441305258873]
	TIME [epoch: 9.73 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9568576189332836		[learning rate: 0.00014748]
	Learning Rate: 0.000147481
	LOSS [training: 2.9568576189332836 | validation: 3.1988155140020855]
	TIME [epoch: 9.72 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9548282493170275		[learning rate: 0.00014695]
	Learning Rate: 0.000146946
	LOSS [training: 2.9548282493170275 | validation: 3.199948404154028]
	TIME [epoch: 9.74 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9559121261514893		[learning rate: 0.00014641]
	Learning Rate: 0.000146413
	LOSS [training: 2.9559121261514893 | validation: 3.201874204445823]
	TIME [epoch: 9.73 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9572132186578997		[learning rate: 0.00014588]
	Learning Rate: 0.000145881
	LOSS [training: 2.9572132186578997 | validation: 3.209633230706078]
	TIME [epoch: 9.72 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9563708162685787		[learning rate: 0.00014535]
	Learning Rate: 0.000145352
	LOSS [training: 2.9563708162685787 | validation: 3.204915564980746]
	TIME [epoch: 9.72 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.967776347043887		[learning rate: 0.00014482]
	Learning Rate: 0.000144825
	LOSS [training: 2.967776347043887 | validation: 3.2238803965124965]
	TIME [epoch: 9.73 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959630010779186		[learning rate: 0.0001443]
	Learning Rate: 0.000144299
	LOSS [training: 2.959630010779186 | validation: 3.19820538751154]
	TIME [epoch: 9.72 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9500885208491354		[learning rate: 0.00014378]
	Learning Rate: 0.000143775
	LOSS [training: 2.9500885208491354 | validation: 3.2032998733404963]
	TIME [epoch: 9.73 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9581094621360284		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 2.9581094621360284 | validation: 3.2010924240324803]
	TIME [epoch: 9.74 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951431813206412		[learning rate: 0.00014273]
	Learning Rate: 0.000142734
	LOSS [training: 2.951431813206412 | validation: 3.200066626044568]
	TIME [epoch: 9.72 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957875471735224		[learning rate: 0.00014222]
	Learning Rate: 0.000142216
	LOSS [training: 2.957875471735224 | validation: 3.206060447084202]
	TIME [epoch: 9.72 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9492355720218333		[learning rate: 0.0001417]
	Learning Rate: 0.0001417
	LOSS [training: 2.9492355720218333 | validation: 3.19163219086469]
	TIME [epoch: 9.74 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9612571745986656		[learning rate: 0.00014119]
	Learning Rate: 0.000141185
	LOSS [training: 2.9612571745986656 | validation: 3.2069502182666008]
	TIME [epoch: 9.72 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9566018387939543		[learning rate: 0.00014067]
	Learning Rate: 0.000140673
	LOSS [training: 2.9566018387939543 | validation: 3.2080647945845637]
	TIME [epoch: 9.72 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957983499492206		[learning rate: 0.00014016]
	Learning Rate: 0.000140162
	LOSS [training: 2.957983499492206 | validation: 3.20409288819611]
	TIME [epoch: 9.75 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960232834218755		[learning rate: 0.00013965]
	Learning Rate: 0.000139654
	LOSS [training: 2.960232834218755 | validation: 3.1978231971392037]
	TIME [epoch: 9.72 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950179611131161		[learning rate: 0.00013915]
	Learning Rate: 0.000139147
	LOSS [training: 2.950179611131161 | validation: 3.224108357595402]
	TIME [epoch: 9.72 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955316398639524		[learning rate: 0.00013864]
	Learning Rate: 0.000138642
	LOSS [training: 2.955316398639524 | validation: 3.2104819558198803]
	TIME [epoch: 9.75 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9572203717283587		[learning rate: 0.00013814]
	Learning Rate: 0.000138139
	LOSS [training: 2.9572203717283587 | validation: 3.216564256955794]
	TIME [epoch: 9.72 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9544472449058303		[learning rate: 0.00013764]
	Learning Rate: 0.000137638
	LOSS [training: 2.9544472449058303 | validation: 3.203338783290165]
	TIME [epoch: 9.71 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954053189365849		[learning rate: 0.00013714]
	Learning Rate: 0.000137138
	LOSS [training: 2.954053189365849 | validation: 3.198854282787023]
	TIME [epoch: 9.74 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950885945731792		[learning rate: 0.00013664]
	Learning Rate: 0.00013664
	LOSS [training: 2.950885945731792 | validation: 3.2007316665134478]
	TIME [epoch: 9.72 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.965004301907583		[learning rate: 0.00013614]
	Learning Rate: 0.000136144
	LOSS [training: 2.965004301907583 | validation: 3.209141211890592]
	TIME [epoch: 9.72 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963446350605993		[learning rate: 0.00013565]
	Learning Rate: 0.00013565
	LOSS [training: 2.963446350605993 | validation: 3.2277535740445513]
	TIME [epoch: 9.74 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960648928548994		[learning rate: 0.00013516]
	Learning Rate: 0.000135158
	LOSS [training: 2.960648928548994 | validation: 3.187288628658048]
	TIME [epoch: 9.73 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957685302413077		[learning rate: 0.00013467]
	Learning Rate: 0.000134668
	LOSS [training: 2.957685302413077 | validation: 3.1946135033047267]
	TIME [epoch: 9.72 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9575348285183627		[learning rate: 0.00013418]
	Learning Rate: 0.000134179
	LOSS [training: 2.9575348285183627 | validation: 3.210461850785476]
	TIME [epoch: 9.75 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9551573000650047		[learning rate: 0.00013369]
	Learning Rate: 0.000133692
	LOSS [training: 2.9551573000650047 | validation: 3.202306655266858]
	TIME [epoch: 9.72 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9520376993287507		[learning rate: 0.00013321]
	Learning Rate: 0.000133207
	LOSS [training: 2.9520376993287507 | validation: 3.20981888787887]
	TIME [epoch: 9.72 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9509442355077184		[learning rate: 0.00013272]
	Learning Rate: 0.000132723
	LOSS [training: 2.9509442355077184 | validation: 3.1952400243230343]
	TIME [epoch: 9.73 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9571152376309433		[learning rate: 0.00013224]
	Learning Rate: 0.000132242
	LOSS [training: 2.9571152376309433 | validation: 3.2096320456988283]
	TIME [epoch: 9.72 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953686743228469		[learning rate: 0.00013176]
	Learning Rate: 0.000131762
	LOSS [training: 2.953686743228469 | validation: 3.2110336825788615]
	TIME [epoch: 9.72 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9575534095364864		[learning rate: 0.00013128]
	Learning Rate: 0.000131284
	LOSS [training: 2.9575534095364864 | validation: 3.206006367008448]
	TIME [epoch: 9.74 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950006310141445		[learning rate: 0.00013081]
	Learning Rate: 0.000130807
	LOSS [training: 2.950006310141445 | validation: 3.20316104547328]
	TIME [epoch: 9.72 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9560797687322933		[learning rate: 0.00013033]
	Learning Rate: 0.000130332
	LOSS [training: 2.9560797687322933 | validation: 3.1982716430903406]
	TIME [epoch: 9.72 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9567089512341296		[learning rate: 0.00012986]
	Learning Rate: 0.000129859
	LOSS [training: 2.9567089512341296 | validation: 3.215050308730738]
	TIME [epoch: 9.73 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9586309420500925		[learning rate: 0.00012939]
	Learning Rate: 0.000129388
	LOSS [training: 2.9586309420500925 | validation: 3.1967414262581775]
	TIME [epoch: 9.72 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957770309587009		[learning rate: 0.00012892]
	Learning Rate: 0.000128919
	LOSS [training: 2.957770309587009 | validation: 3.2085493972588837]
	TIME [epoch: 9.72 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954430559207194		[learning rate: 0.00012845]
	Learning Rate: 0.000128451
	LOSS [training: 2.954430559207194 | validation: 3.195954340645625]
	TIME [epoch: 9.72 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9539837991140736		[learning rate: 0.00012798]
	Learning Rate: 0.000127985
	LOSS [training: 2.9539837991140736 | validation: 3.207357182272906]
	TIME [epoch: 9.73 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960664161354713		[learning rate: 0.00012752]
	Learning Rate: 0.00012752
	LOSS [training: 2.960664161354713 | validation: 3.2001488422883315]
	TIME [epoch: 9.72 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957615162296181		[learning rate: 0.00012706]
	Learning Rate: 0.000127057
	LOSS [training: 2.957615162296181 | validation: 3.1995666658172452]
	TIME [epoch: 9.72 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9596221890694325		[learning rate: 0.0001266]
	Learning Rate: 0.000126596
	LOSS [training: 2.9596221890694325 | validation: 3.2136908643157245]
	TIME [epoch: 9.73 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9518298227250908		[learning rate: 0.00012614]
	Learning Rate: 0.000126137
	LOSS [training: 2.9518298227250908 | validation: 3.18515720723923]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_1303.pth
	Model improved!!!
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959205854064551		[learning rate: 0.00012568]
	Learning Rate: 0.000125679
	LOSS [training: 2.959205854064551 | validation: 3.209604727692921]
	TIME [epoch: 9.72 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9576132758306795		[learning rate: 0.00012522]
	Learning Rate: 0.000125223
	LOSS [training: 2.9576132758306795 | validation: 3.2066147738929094]
	TIME [epoch: 9.73 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9507580284581483		[learning rate: 0.00012477]
	Learning Rate: 0.000124769
	LOSS [training: 2.9507580284581483 | validation: 3.2041417397833163]
	TIME [epoch: 9.71 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960276586766714		[learning rate: 0.00012432]
	Learning Rate: 0.000124316
	LOSS [training: 2.960276586766714 | validation: 3.207533552397224]
	TIME [epoch: 9.71 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9596752164752766		[learning rate: 0.00012386]
	Learning Rate: 0.000123865
	LOSS [training: 2.9596752164752766 | validation: 3.2059937407978993]
	TIME [epoch: 9.73 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955176030195604		[learning rate: 0.00012342]
	Learning Rate: 0.000123415
	LOSS [training: 2.955176030195604 | validation: 3.20293719922323]
	TIME [epoch: 9.71 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959690292157615		[learning rate: 0.00012297]
	Learning Rate: 0.000122967
	LOSS [training: 2.959690292157615 | validation: 3.207546256691159]
	TIME [epoch: 9.71 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9578676476108803		[learning rate: 0.00012252]
	Learning Rate: 0.000122521
	LOSS [training: 2.9578676476108803 | validation: 3.196327546015993]
	TIME [epoch: 9.74 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9574797854666857		[learning rate: 0.00012208]
	Learning Rate: 0.000122076
	LOSS [training: 2.9574797854666857 | validation: 3.205179480677957]
	TIME [epoch: 9.71 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9540950672088657		[learning rate: 0.00012163]
	Learning Rate: 0.000121633
	LOSS [training: 2.9540950672088657 | validation: 3.1974782844099723]
	TIME [epoch: 9.71 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95611750947511		[learning rate: 0.00012119]
	Learning Rate: 0.000121192
	LOSS [training: 2.95611750947511 | validation: 3.2111562557942084]
	TIME [epoch: 9.74 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9541215255855975		[learning rate: 0.00012075]
	Learning Rate: 0.000120752
	LOSS [training: 2.9541215255855975 | validation: 3.202005316142503]
	TIME [epoch: 9.72 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9540750175521255		[learning rate: 0.00012031]
	Learning Rate: 0.000120314
	LOSS [training: 2.9540750175521255 | validation: 3.2021332262051594]
	TIME [epoch: 9.72 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9596155583559356		[learning rate: 0.00011988]
	Learning Rate: 0.000119877
	LOSS [training: 2.9596155583559356 | validation: 3.211355102698046]
	TIME [epoch: 9.74 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949080259694257		[learning rate: 0.00011944]
	Learning Rate: 0.000119442
	LOSS [training: 2.949080259694257 | validation: 3.211755146428991]
	TIME [epoch: 9.71 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958351075243077		[learning rate: 0.00011901]
	Learning Rate: 0.000119009
	LOSS [training: 2.958351075243077 | validation: 3.2078690367392024]
	TIME [epoch: 9.72 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9564083158661587		[learning rate: 0.00011858]
	Learning Rate: 0.000118577
	LOSS [training: 2.9564083158661587 | validation: 3.2117253927269234]
	TIME [epoch: 9.73 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961204167397996		[learning rate: 0.00011815]
	Learning Rate: 0.000118147
	LOSS [training: 2.961204167397996 | validation: 3.195011604605667]
	TIME [epoch: 9.72 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954697468788936		[learning rate: 0.00011772]
	Learning Rate: 0.000117718
	LOSS [training: 2.954697468788936 | validation: 3.1984987729483603]
	TIME [epoch: 9.71 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953232825793754		[learning rate: 0.00011729]
	Learning Rate: 0.000117291
	LOSS [training: 2.953232825793754 | validation: 3.205865065988735]
	TIME [epoch: 9.74 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956550726338569		[learning rate: 0.00011686]
	Learning Rate: 0.000116865
	LOSS [training: 2.956550726338569 | validation: 3.2154027830346776]
	TIME [epoch: 9.72 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959415834582675		[learning rate: 0.00011644]
	Learning Rate: 0.000116441
	LOSS [training: 2.959415834582675 | validation: 3.2058149748247002]
	TIME [epoch: 9.72 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9571988602784547		[learning rate: 0.00011602]
	Learning Rate: 0.000116018
	LOSS [training: 2.9571988602784547 | validation: 3.195267468694803]
	TIME [epoch: 9.73 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952396844375347		[learning rate: 0.0001156]
	Learning Rate: 0.000115597
	LOSS [training: 2.952396844375347 | validation: 3.199193181504989]
	TIME [epoch: 9.72 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9562512382985737		[learning rate: 0.00011518]
	Learning Rate: 0.000115178
	LOSS [training: 2.9562512382985737 | validation: 3.207387968314817]
	TIME [epoch: 9.71 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958429911331743		[learning rate: 0.00011476]
	Learning Rate: 0.00011476
	LOSS [training: 2.958429911331743 | validation: 3.1948304412333597]
	TIME [epoch: 9.73 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9579785138750916		[learning rate: 0.00011434]
	Learning Rate: 0.000114343
	LOSS [training: 2.9579785138750916 | validation: 3.2089815452634536]
	TIME [epoch: 9.71 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956210489003163		[learning rate: 0.00011393]
	Learning Rate: 0.000113928
	LOSS [training: 2.956210489003163 | validation: 3.1931238144203475]
	TIME [epoch: 9.72 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961918818072857		[learning rate: 0.00011351]
	Learning Rate: 0.000113515
	LOSS [training: 2.961918818072857 | validation: 3.198978035187881]
	TIME [epoch: 9.72 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9543608610413106		[learning rate: 0.0001131]
	Learning Rate: 0.000113103
	LOSS [training: 2.9543608610413106 | validation: 3.2014259956936666]
	TIME [epoch: 9.73 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9535973154861623		[learning rate: 0.00011269]
	Learning Rate: 0.000112692
	LOSS [training: 2.9535973154861623 | validation: 3.208327820045781]
	TIME [epoch: 9.71 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9576921083646353		[learning rate: 0.00011228]
	Learning Rate: 0.000112283
	LOSS [training: 2.9576921083646353 | validation: 3.201725051172165]
	TIME [epoch: 9.72 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9508005113780347		[learning rate: 0.00011188]
	Learning Rate: 0.000111876
	LOSS [training: 2.9508005113780347 | validation: 3.205386825805273]
	TIME [epoch: 9.72 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95355076928018		[learning rate: 0.00011147]
	Learning Rate: 0.00011147
	LOSS [training: 2.95355076928018 | validation: 3.2090920411306287]
	TIME [epoch: 9.71 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9560458984091342		[learning rate: 0.00011107]
	Learning Rate: 0.000111065
	LOSS [training: 2.9560458984091342 | validation: 3.2084270383156817]
	TIME [epoch: 9.7 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957224185350363		[learning rate: 0.00011066]
	Learning Rate: 0.000110662
	LOSS [training: 2.957224185350363 | validation: 3.205771741732152]
	TIME [epoch: 9.73 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95916439965774		[learning rate: 0.00011026]
	Learning Rate: 0.000110261
	LOSS [training: 2.95916439965774 | validation: 3.2077833158495825]
	TIME [epoch: 9.71 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9584475023943027		[learning rate: 0.00010986]
	Learning Rate: 0.000109861
	LOSS [training: 2.9584475023943027 | validation: 3.2058482116571425]
	TIME [epoch: 9.71 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9559556249995986		[learning rate: 0.00010946]
	Learning Rate: 0.000109462
	LOSS [training: 2.9559556249995986 | validation: 3.2059291104295324]
	TIME [epoch: 9.73 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9506919050757303		[learning rate: 0.00010906]
	Learning Rate: 0.000109065
	LOSS [training: 2.9506919050757303 | validation: 3.2084747230155433]
	TIME [epoch: 9.71 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954011714580291		[learning rate: 0.00010867]
	Learning Rate: 0.000108669
	LOSS [training: 2.954011714580291 | validation: 3.201452252790621]
	TIME [epoch: 9.71 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9560005205435176		[learning rate: 0.00010827]
	Learning Rate: 0.000108275
	LOSS [training: 2.9560005205435176 | validation: 3.211428512415755]
	TIME [epoch: 9.74 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9506935008115027		[learning rate: 0.00010788]
	Learning Rate: 0.000107882
	LOSS [training: 2.9506935008115027 | validation: 3.186714502684688]
	TIME [epoch: 9.71 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95430037602823		[learning rate: 0.00010749]
	Learning Rate: 0.00010749
	LOSS [training: 2.95430037602823 | validation: 3.207272539422009]
	TIME [epoch: 9.71 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9520757100058383		[learning rate: 0.0001071]
	Learning Rate: 0.0001071
	LOSS [training: 2.9520757100058383 | validation: 3.1997370353869927]
	TIME [epoch: 9.74 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952797101674052		[learning rate: 0.00010671]
	Learning Rate: 0.000106711
	LOSS [training: 2.952797101674052 | validation: 3.200758261816555]
	TIME [epoch: 9.71 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.947203995395323		[learning rate: 0.00010632]
	Learning Rate: 0.000106324
	LOSS [training: 2.947203995395323 | validation: 3.200956880852091]
	TIME [epoch: 9.7 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9560539587885746		[learning rate: 0.00010594]
	Learning Rate: 0.000105938
	LOSS [training: 2.9560539587885746 | validation: 3.2044812681530415]
	TIME [epoch: 9.73 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9528680811984866		[learning rate: 0.00010555]
	Learning Rate: 0.000105554
	LOSS [training: 2.9528680811984866 | validation: 3.19261171552914]
	TIME [epoch: 9.71 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955372934332388		[learning rate: 0.00010517]
	Learning Rate: 0.000105171
	LOSS [training: 2.955372934332388 | validation: 3.2003949074293154]
	TIME [epoch: 9.71 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.94880686428928		[learning rate: 0.00010479]
	Learning Rate: 0.000104789
	LOSS [training: 2.94880686428928 | validation: 3.195801630980138]
	TIME [epoch: 9.73 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95388703117058		[learning rate: 0.00010441]
	Learning Rate: 0.000104409
	LOSS [training: 2.95388703117058 | validation: 3.202798303352769]
	TIME [epoch: 9.71 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958820363416202		[learning rate: 0.00010403]
	Learning Rate: 0.00010403
	LOSS [training: 2.958820363416202 | validation: 3.192627865205349]
	TIME [epoch: 9.71 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956471654244124		[learning rate: 0.00010365]
	Learning Rate: 0.000103652
	LOSS [training: 2.956471654244124 | validation: 3.2056613052513216]
	TIME [epoch: 9.73 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95689989913048		[learning rate: 0.00010328]
	Learning Rate: 0.000103276
	LOSS [training: 2.95689989913048 | validation: 3.188430554005445]
	TIME [epoch: 9.71 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9517867686009556		[learning rate: 0.0001029]
	Learning Rate: 0.000102901
	LOSS [training: 2.9517867686009556 | validation: 3.1957413353034987]
	TIME [epoch: 9.71 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9519737433039017		[learning rate: 0.00010253]
	Learning Rate: 0.000102528
	LOSS [training: 2.9519737433039017 | validation: 3.2004853793760004]
	TIME [epoch: 9.73 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953986109942291		[learning rate: 0.00010216]
	Learning Rate: 0.000102156
	LOSS [training: 2.953986109942291 | validation: 3.199312846371602]
	TIME [epoch: 9.72 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9487061647773354		[learning rate: 0.00010179]
	Learning Rate: 0.000101785
	LOSS [training: 2.9487061647773354 | validation: 3.205671787632472]
	TIME [epoch: 9.71 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956590088073464		[learning rate: 0.00010142]
	Learning Rate: 0.000101416
	LOSS [training: 2.956590088073464 | validation: 3.202920259108909]
	TIME [epoch: 9.72 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961139268716363		[learning rate: 0.00010105]
	Learning Rate: 0.000101048
	LOSS [training: 2.961139268716363 | validation: 3.1996350782881566]
	TIME [epoch: 9.72 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.96083943615732		[learning rate: 0.00010068]
	Learning Rate: 0.000100681
	LOSS [training: 2.96083943615732 | validation: 3.2136724825011336]
	TIME [epoch: 9.72 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962115238329692		[learning rate: 0.00010032]
	Learning Rate: 0.000100316
	LOSS [training: 2.962115238329692 | validation: 3.2085432806196947]
	TIME [epoch: 9.72 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952139842686329		[learning rate: 9.9952e-05]
	Learning Rate: 9.99515e-05
	LOSS [training: 2.952139842686329 | validation: 3.1990354919665367]
	TIME [epoch: 9.73 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9532997095484355		[learning rate: 9.9589e-05]
	Learning Rate: 9.95888e-05
	LOSS [training: 2.9532997095484355 | validation: 3.1867991015710238]
	TIME [epoch: 9.71 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9508790083385823		[learning rate: 9.9227e-05]
	Learning Rate: 9.92274e-05
	LOSS [training: 2.9508790083385823 | validation: 3.205438292094367]
	TIME [epoch: 9.72 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953026310828929		[learning rate: 9.8867e-05]
	Learning Rate: 9.88673e-05
	LOSS [training: 2.953026310828929 | validation: 3.214600571108809]
	TIME [epoch: 9.72 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9590524054299006		[learning rate: 9.8509e-05]
	Learning Rate: 9.85085e-05
	LOSS [training: 2.9590524054299006 | validation: 3.208399458196424]
	TIME [epoch: 9.71 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953422020594278		[learning rate: 9.8151e-05]
	Learning Rate: 9.8151e-05
	LOSS [training: 2.953422020594278 | validation: 3.192611893415126]
	TIME [epoch: 9.71 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9513178649565317		[learning rate: 9.7795e-05]
	Learning Rate: 9.77948e-05
	LOSS [training: 2.9513178649565317 | validation: 3.199519916582318]
	TIME [epoch: 9.73 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952991894636772		[learning rate: 9.744e-05]
	Learning Rate: 9.74399e-05
	LOSS [training: 2.952991894636772 | validation: 3.2024270417154956]
	TIME [epoch: 9.71 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959325216012574		[learning rate: 9.7086e-05]
	Learning Rate: 9.70863e-05
	LOSS [training: 2.959325216012574 | validation: 3.1996723972025154]
	TIME [epoch: 9.71 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9495652172432356		[learning rate: 9.6734e-05]
	Learning Rate: 9.6734e-05
	LOSS [training: 2.9495652172432356 | validation: 3.1990701696201835]
	TIME [epoch: 9.73 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9517249876817866		[learning rate: 9.6383e-05]
	Learning Rate: 9.63829e-05
	LOSS [training: 2.9517249876817866 | validation: 3.2047360230478192]
	TIME [epoch: 9.72 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959730651733053		[learning rate: 9.6033e-05]
	Learning Rate: 9.60331e-05
	LOSS [training: 2.959730651733053 | validation: 3.212626182469105]
	TIME [epoch: 9.71 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962301109955646		[learning rate: 9.5685e-05]
	Learning Rate: 9.56846e-05
	LOSS [training: 2.962301109955646 | validation: 3.2113348638660217]
	TIME [epoch: 9.74 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9562991921995785		[learning rate: 9.5337e-05]
	Learning Rate: 9.53374e-05
	LOSS [training: 2.9562991921995785 | validation: 3.1978974222377703]
	TIME [epoch: 9.71 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9553647096580655		[learning rate: 9.4991e-05]
	Learning Rate: 9.49914e-05
	LOSS [training: 2.9553647096580655 | validation: 3.197028447960828]
	TIME [epoch: 9.71 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951081363966972		[learning rate: 9.4647e-05]
	Learning Rate: 9.46466e-05
	LOSS [training: 2.951081363966972 | validation: 3.2017708292383795]
	TIME [epoch: 9.74 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957545944674851		[learning rate: 9.4303e-05]
	Learning Rate: 9.43032e-05
	LOSS [training: 2.957545944674851 | validation: 3.203582894256644]
	TIME [epoch: 9.72 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9570598897752554		[learning rate: 9.3961e-05]
	Learning Rate: 9.39609e-05
	LOSS [training: 2.9570598897752554 | validation: 3.2081790719217715]
	TIME [epoch: 9.71 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9542983854090528		[learning rate: 9.362e-05]
	Learning Rate: 9.362e-05
	LOSS [training: 2.9542983854090528 | validation: 3.2042832431563126]
	TIME [epoch: 9.73 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9508698930503647		[learning rate: 9.328e-05]
	Learning Rate: 9.32802e-05
	LOSS [training: 2.9508698930503647 | validation: 3.207224153545699]
	TIME [epoch: 9.71 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9534226522000626		[learning rate: 9.2942e-05]
	Learning Rate: 9.29417e-05
	LOSS [training: 2.9534226522000626 | validation: 3.210373710710198]
	TIME [epoch: 9.71 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958256740963692		[learning rate: 9.2604e-05]
	Learning Rate: 9.26044e-05
	LOSS [training: 2.958256740963692 | validation: 3.2203486284495098]
	TIME [epoch: 9.73 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966416786819592		[learning rate: 9.2268e-05]
	Learning Rate: 9.22683e-05
	LOSS [training: 2.966416786819592 | validation: 3.206971647348628]
	TIME [epoch: 9.71 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9576887428301326		[learning rate: 9.1933e-05]
	Learning Rate: 9.19335e-05
	LOSS [training: 2.9576887428301326 | validation: 3.1981511405121994]
	TIME [epoch: 9.71 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954914072280501		[learning rate: 9.16e-05]
	Learning Rate: 9.15998e-05
	LOSS [training: 2.954914072280501 | validation: 3.2055249886502697]
	TIME [epoch: 9.73 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.94916559583723		[learning rate: 9.1267e-05]
	Learning Rate: 9.12674e-05
	LOSS [training: 2.94916559583723 | validation: 3.1950707872875883]
	TIME [epoch: 9.71 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9515127659676437		[learning rate: 9.0936e-05]
	Learning Rate: 9.09362e-05
	LOSS [training: 2.9515127659676437 | validation: 3.2048228724835655]
	TIME [epoch: 9.71 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9580841163140184		[learning rate: 9.0606e-05]
	Learning Rate: 9.06062e-05
	LOSS [training: 2.9580841163140184 | validation: 3.186565008183265]
	TIME [epoch: 9.73 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9522039095023507		[learning rate: 9.0277e-05]
	Learning Rate: 9.02774e-05
	LOSS [training: 2.9522039095023507 | validation: 3.206248053104722]
	TIME [epoch: 9.72 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9545747556635624		[learning rate: 8.995e-05]
	Learning Rate: 8.99498e-05
	LOSS [training: 2.9545747556635624 | validation: 3.1981180690314237]
	TIME [epoch: 9.71 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9543268130960447		[learning rate: 8.9623e-05]
	Learning Rate: 8.96233e-05
	LOSS [training: 2.9543268130960447 | validation: 3.204107240542545]
	TIME [epoch: 9.72 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95432841150136		[learning rate: 8.9298e-05]
	Learning Rate: 8.92981e-05
	LOSS [training: 2.95432841150136 | validation: 3.2051908790885615]
	TIME [epoch: 9.72 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9504989922984035		[learning rate: 8.8974e-05]
	Learning Rate: 8.8974e-05
	LOSS [training: 2.9504989922984035 | validation: 3.2088390426178433]
	TIME [epoch: 9.71 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955465486882331		[learning rate: 8.8651e-05]
	Learning Rate: 8.86511e-05
	LOSS [training: 2.955465486882331 | validation: 3.202525489540838]
	TIME [epoch: 9.71 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9597814213207903		[learning rate: 8.8329e-05]
	Learning Rate: 8.83294e-05
	LOSS [training: 2.9597814213207903 | validation: 3.200100844613695]
	TIME [epoch: 9.73 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9512775450126054		[learning rate: 8.8009e-05]
	Learning Rate: 8.80088e-05
	LOSS [training: 2.9512775450126054 | validation: 3.2087927082588372]
	TIME [epoch: 9.71 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953806729040287		[learning rate: 8.7689e-05]
	Learning Rate: 8.76895e-05
	LOSS [training: 2.953806729040287 | validation: 3.203074561203609]
	TIME [epoch: 9.71 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9555154324639403		[learning rate: 8.7371e-05]
	Learning Rate: 8.73712e-05
	LOSS [training: 2.9555154324639403 | validation: 3.209627027592394]
	TIME [epoch: 9.73 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966212965197074		[learning rate: 8.7054e-05]
	Learning Rate: 8.70542e-05
	LOSS [training: 2.966212965197074 | validation: 3.2037713337408498]
	TIME [epoch: 9.71 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9567812608312862		[learning rate: 8.6738e-05]
	Learning Rate: 8.67382e-05
	LOSS [training: 2.9567812608312862 | validation: 3.199021772842662]
	TIME [epoch: 9.71 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952684905366353		[learning rate: 8.6423e-05]
	Learning Rate: 8.64235e-05
	LOSS [training: 2.952684905366353 | validation: 3.214059675463777]
	TIME [epoch: 9.74 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9537598337051714		[learning rate: 8.611e-05]
	Learning Rate: 8.61098e-05
	LOSS [training: 2.9537598337051714 | validation: 3.2006372191067203]
	TIME [epoch: 9.71 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958241270102402		[learning rate: 8.5797e-05]
	Learning Rate: 8.57973e-05
	LOSS [training: 2.958241270102402 | validation: 3.1874603116021185]
	TIME [epoch: 9.72 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9537019969582885		[learning rate: 8.5486e-05]
	Learning Rate: 8.54859e-05
	LOSS [training: 2.9537019969582885 | validation: 3.1935724186240853]
	TIME [epoch: 9.73 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9535024133116208		[learning rate: 8.5176e-05]
	Learning Rate: 8.51757e-05
	LOSS [training: 2.9535024133116208 | validation: 3.2137104130557432]
	TIME [epoch: 9.71 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957535398424666		[learning rate: 8.4867e-05]
	Learning Rate: 8.48666e-05
	LOSS [training: 2.957535398424666 | validation: 3.2011644408195408]
	TIME [epoch: 9.71 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950448486924063		[learning rate: 8.4559e-05]
	Learning Rate: 8.45586e-05
	LOSS [training: 2.950448486924063 | validation: 3.2001767639188152]
	TIME [epoch: 9.74 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950595587473284		[learning rate: 8.4252e-05]
	Learning Rate: 8.42518e-05
	LOSS [training: 2.950595587473284 | validation: 3.2044926330501613]
	TIME [epoch: 9.72 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955315217711303		[learning rate: 8.3946e-05]
	Learning Rate: 8.3946e-05
	LOSS [training: 2.955315217711303 | validation: 3.1869573820309776]
	TIME [epoch: 9.72 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9469473524372085		[learning rate: 8.3641e-05]
	Learning Rate: 8.36414e-05
	LOSS [training: 2.9469473524372085 | validation: 3.2011784727823476]
	TIME [epoch: 9.74 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952652257070717		[learning rate: 8.3338e-05]
	Learning Rate: 8.33378e-05
	LOSS [training: 2.952652257070717 | validation: 3.2098681335512493]
	TIME [epoch: 9.71 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9543354667703716		[learning rate: 8.3035e-05]
	Learning Rate: 8.30354e-05
	LOSS [training: 2.9543354667703716 | validation: 3.2048138228144136]
	TIME [epoch: 9.71 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9527073372192647		[learning rate: 8.2734e-05]
	Learning Rate: 8.2734e-05
	LOSS [training: 2.9527073372192647 | validation: 3.205954528342536]
	TIME [epoch: 9.73 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.96021596725447		[learning rate: 8.2434e-05]
	Learning Rate: 8.24338e-05
	LOSS [training: 2.96021596725447 | validation: 3.2100904291438694]
	TIME [epoch: 9.72 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9502128649499806		[learning rate: 8.2135e-05]
	Learning Rate: 8.21346e-05
	LOSS [training: 2.9502128649499806 | validation: 3.2071296628256825]
	TIME [epoch: 9.72 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9540679346104532		[learning rate: 8.1837e-05]
	Learning Rate: 8.18366e-05
	LOSS [training: 2.9540679346104532 | validation: 3.2052345703262937]
	TIME [epoch: 9.73 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95127264206569		[learning rate: 8.154e-05]
	Learning Rate: 8.15396e-05
	LOSS [training: 2.95127264206569 | validation: 3.1991239481373395]
	TIME [epoch: 9.72 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9545224547102875		[learning rate: 8.1244e-05]
	Learning Rate: 8.12437e-05
	LOSS [training: 2.9545224547102875 | validation: 3.2074661086885397]
	TIME [epoch: 9.71 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9610407284964326		[learning rate: 8.0949e-05]
	Learning Rate: 8.09488e-05
	LOSS [training: 2.9610407284964326 | validation: 3.203178883655246]
	TIME [epoch: 9.73 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9527683695766487		[learning rate: 8.0655e-05]
	Learning Rate: 8.0655e-05
	LOSS [training: 2.9527683695766487 | validation: 3.19336239518222]
	TIME [epoch: 9.71 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9496140922880802		[learning rate: 8.0362e-05]
	Learning Rate: 8.03623e-05
	LOSS [training: 2.9496140922880802 | validation: 3.1964474517217183]
	TIME [epoch: 9.71 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958163402508553		[learning rate: 8.0071e-05]
	Learning Rate: 8.00707e-05
	LOSS [training: 2.958163402508553 | validation: 3.2015628927916464]
	TIME [epoch: 9.73 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9542805872950453		[learning rate: 7.978e-05]
	Learning Rate: 7.97801e-05
	LOSS [training: 2.9542805872950453 | validation: 3.2090397016426215]
	TIME [epoch: 9.72 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954239161070187		[learning rate: 7.9491e-05]
	Learning Rate: 7.94906e-05
	LOSS [training: 2.954239161070187 | validation: 3.209956204638848]
	TIME [epoch: 9.71 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9546980046850067		[learning rate: 7.9202e-05]
	Learning Rate: 7.92021e-05
	LOSS [training: 2.9546980046850067 | validation: 3.1989952259604104]
	TIME [epoch: 9.72 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952752088414763		[learning rate: 7.8915e-05]
	Learning Rate: 7.89147e-05
	LOSS [training: 2.952752088414763 | validation: 3.194022166717391]
	TIME [epoch: 9.73 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955189945926503		[learning rate: 7.8628e-05]
	Learning Rate: 7.86283e-05
	LOSS [training: 2.955189945926503 | validation: 3.1978649500841834]
	TIME [epoch: 9.71 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953319410924159		[learning rate: 7.8343e-05]
	Learning Rate: 7.8343e-05
	LOSS [training: 2.953319410924159 | validation: 3.200861393554437]
	TIME [epoch: 9.72 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952854193800319		[learning rate: 7.8059e-05]
	Learning Rate: 7.80586e-05
	LOSS [training: 2.952854193800319 | validation: 3.196198145087708]
	TIME [epoch: 9.73 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9486560484823165		[learning rate: 7.7775e-05]
	Learning Rate: 7.77754e-05
	LOSS [training: 2.9486560484823165 | validation: 3.2020618033420836]
	TIME [epoch: 9.71 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956010759289677		[learning rate: 7.7493e-05]
	Learning Rate: 7.74931e-05
	LOSS [training: 2.956010759289677 | validation: 3.1987559906636456]
	TIME [epoch: 9.71 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9479840250407388		[learning rate: 7.7212e-05]
	Learning Rate: 7.72119e-05
	LOSS [training: 2.9479840250407388 | validation: 3.2047458947636756]
	TIME [epoch: 9.73 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955444988081998		[learning rate: 7.6932e-05]
	Learning Rate: 7.69317e-05
	LOSS [training: 2.955444988081998 | validation: 3.204905528240272]
	TIME [epoch: 9.71 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.946582563529939		[learning rate: 7.6653e-05]
	Learning Rate: 7.66525e-05
	LOSS [training: 2.946582563529939 | validation: 3.2104379042642375]
	TIME [epoch: 9.71 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954986662916398		[learning rate: 7.6374e-05]
	Learning Rate: 7.63743e-05
	LOSS [training: 2.954986662916398 | validation: 3.2075549106771373]
	TIME [epoch: 9.74 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9516546349080057		[learning rate: 7.6097e-05]
	Learning Rate: 7.60972e-05
	LOSS [training: 2.9516546349080057 | validation: 3.2073869922671054]
	TIME [epoch: 9.71 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950528432985153		[learning rate: 7.5821e-05]
	Learning Rate: 7.5821e-05
	LOSS [training: 2.950528432985153 | validation: 3.20510464862585]
	TIME [epoch: 9.71 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9569006482651563		[learning rate: 7.5546e-05]
	Learning Rate: 7.55458e-05
	LOSS [training: 2.9569006482651563 | validation: 3.2025617188506454]
	TIME [epoch: 9.72 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9585511539653204		[learning rate: 7.5272e-05]
	Learning Rate: 7.52717e-05
	LOSS [training: 2.9585511539653204 | validation: 3.195293407112919]
	TIME [epoch: 9.71 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9537343692319395		[learning rate: 7.4999e-05]
	Learning Rate: 7.49985e-05
	LOSS [training: 2.9537343692319395 | validation: 3.1950358402827055]
	TIME [epoch: 9.71 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9576105824653944		[learning rate: 7.4726e-05]
	Learning Rate: 7.47263e-05
	LOSS [training: 2.9576105824653944 | validation: 3.2092162289331227]
	TIME [epoch: 9.74 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9508018753327994		[learning rate: 7.4455e-05]
	Learning Rate: 7.44552e-05
	LOSS [training: 2.9508018753327994 | validation: 3.2021751300130514]
	TIME [epoch: 9.71 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950522772442844		[learning rate: 7.4185e-05]
	Learning Rate: 7.4185e-05
	LOSS [training: 2.950522772442844 | validation: 3.1935049928094976]
	TIME [epoch: 9.71 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9480139309345192		[learning rate: 7.3916e-05]
	Learning Rate: 7.39157e-05
	LOSS [training: 2.9480139309345192 | validation: 3.2062947965569073]
	TIME [epoch: 9.72 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9517071342259578		[learning rate: 7.3647e-05]
	Learning Rate: 7.36475e-05
	LOSS [training: 2.9517071342259578 | validation: 3.1951615151309283]
	TIME [epoch: 9.72 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954304997184207		[learning rate: 7.338e-05]
	Learning Rate: 7.33802e-05
	LOSS [training: 2.954304997184207 | validation: 3.2016679546932383]
	TIME [epoch: 9.71 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9520322104929937		[learning rate: 7.3114e-05]
	Learning Rate: 7.31139e-05
	LOSS [training: 2.9520322104929937 | validation: 3.21092097767079]
	TIME [epoch: 9.73 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9617875703398227		[learning rate: 7.2849e-05]
	Learning Rate: 7.28486e-05
	LOSS [training: 2.9617875703398227 | validation: 3.193158171781669]
	TIME [epoch: 9.71 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9558676563944686		[learning rate: 7.2584e-05]
	Learning Rate: 7.25842e-05
	LOSS [training: 2.9558676563944686 | validation: 3.203199896170604]
	TIME [epoch: 9.71 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9559297150280566		[learning rate: 7.2321e-05]
	Learning Rate: 7.23208e-05
	LOSS [training: 2.9559297150280566 | validation: 3.2176636919379087]
	TIME [epoch: 9.72 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9611163241245952		[learning rate: 7.2058e-05]
	Learning Rate: 7.20583e-05
	LOSS [training: 2.9611163241245952 | validation: 3.2072487044325726]
	TIME [epoch: 9.71 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9524493043357305		[learning rate: 7.1797e-05]
	Learning Rate: 7.17968e-05
	LOSS [training: 2.9524493043357305 | validation: 3.1940429256109293]
	TIME [epoch: 9.7 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9503772767415937		[learning rate: 7.1536e-05]
	Learning Rate: 7.15363e-05
	LOSS [training: 2.9503772767415937 | validation: 3.210356358320532]
	TIME [epoch: 9.73 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9504503535268443		[learning rate: 7.1277e-05]
	Learning Rate: 7.12767e-05
	LOSS [training: 2.9504503535268443 | validation: 3.201998831039706]
	TIME [epoch: 9.72 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9511284921701346		[learning rate: 7.1018e-05]
	Learning Rate: 7.1018e-05
	LOSS [training: 2.9511284921701346 | validation: 3.192108477406656]
	TIME [epoch: 9.71 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953029246643916		[learning rate: 7.076e-05]
	Learning Rate: 7.07603e-05
	LOSS [training: 2.953029246643916 | validation: 3.2091755969755815]
	TIME [epoch: 9.73 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950471217687864		[learning rate: 7.0503e-05]
	Learning Rate: 7.05035e-05
	LOSS [training: 2.950471217687864 | validation: 3.1963478510200583]
	TIME [epoch: 9.71 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9543906550316796		[learning rate: 7.0248e-05]
	Learning Rate: 7.02476e-05
	LOSS [training: 2.9543906550316796 | validation: 3.2006703372151954]
	TIME [epoch: 9.71 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9496445740230044		[learning rate: 6.9993e-05]
	Learning Rate: 6.99927e-05
	LOSS [training: 2.9496445740230044 | validation: 3.2128451419656883]
	TIME [epoch: 9.71 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9521692126401966		[learning rate: 6.9739e-05]
	Learning Rate: 6.97387e-05
	LOSS [training: 2.9521692126401966 | validation: 3.202497312596571]
	TIME [epoch: 9.72 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9527902248570514		[learning rate: 6.9486e-05]
	Learning Rate: 6.94856e-05
	LOSS [training: 2.9527902248570514 | validation: 3.2058743121563467]
	TIME [epoch: 9.71 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9507203680515035		[learning rate: 6.9233e-05]
	Learning Rate: 6.92334e-05
	LOSS [training: 2.9507203680515035 | validation: 3.1936162494544744]
	TIME [epoch: 9.72 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9475663716487004		[learning rate: 6.8982e-05]
	Learning Rate: 6.89822e-05
	LOSS [training: 2.9475663716487004 | validation: 3.2032419014496556]
	TIME [epoch: 9.73 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9551193581605966		[learning rate: 6.8732e-05]
	Learning Rate: 6.87318e-05
	LOSS [training: 2.9551193581605966 | validation: 3.1960642662621646]
	TIME [epoch: 9.7 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9497072167865066		[learning rate: 6.8482e-05]
	Learning Rate: 6.84824e-05
	LOSS [training: 2.9497072167865066 | validation: 3.198963960335554]
	TIME [epoch: 9.7 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953169304437103		[learning rate: 6.8234e-05]
	Learning Rate: 6.82339e-05
	LOSS [training: 2.953169304437103 | validation: 3.2014821023104107]
	TIME [epoch: 9.73 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951829499022927		[learning rate: 6.7986e-05]
	Learning Rate: 6.79862e-05
	LOSS [training: 2.951829499022927 | validation: 3.196845980865474]
	TIME [epoch: 9.71 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9484492409663696		[learning rate: 6.774e-05]
	Learning Rate: 6.77395e-05
	LOSS [training: 2.9484492409663696 | validation: 3.205662500588129]
	TIME [epoch: 9.71 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9551332853062005		[learning rate: 6.7494e-05]
	Learning Rate: 6.74937e-05
	LOSS [training: 2.9551332853062005 | validation: 3.201161080406134]
	TIME [epoch: 9.73 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948249656104891		[learning rate: 6.7249e-05]
	Learning Rate: 6.72488e-05
	LOSS [training: 2.948249656104891 | validation: 3.1994822876667017]
	TIME [epoch: 9.71 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949978256341539		[learning rate: 6.7005e-05]
	Learning Rate: 6.70047e-05
	LOSS [training: 2.949978256341539 | validation: 3.206280614693307]
	TIME [epoch: 9.71 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949618229679945		[learning rate: 6.6762e-05]
	Learning Rate: 6.67616e-05
	LOSS [training: 2.949618229679945 | validation: 3.205403961469422]
	TIME [epoch: 9.73 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9545553185029623		[learning rate: 6.6519e-05]
	Learning Rate: 6.65192e-05
	LOSS [training: 2.9545553185029623 | validation: 3.201743451628263]
	TIME [epoch: 9.71 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949818467275803		[learning rate: 6.6278e-05]
	Learning Rate: 6.62778e-05
	LOSS [training: 2.949818467275803 | validation: 3.208241946187402]
	TIME [epoch: 9.71 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9520181124480964		[learning rate: 6.6037e-05]
	Learning Rate: 6.60373e-05
	LOSS [training: 2.9520181124480964 | validation: 3.203698494541915]
	TIME [epoch: 9.73 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948373760686592		[learning rate: 6.5798e-05]
	Learning Rate: 6.57977e-05
	LOSS [training: 2.948373760686592 | validation: 3.201044083752862]
	TIME [epoch: 9.71 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9483340536895173		[learning rate: 6.5559e-05]
	Learning Rate: 6.55589e-05
	LOSS [training: 2.9483340536895173 | validation: 3.198034131314982]
	TIME [epoch: 9.71 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948576559316513		[learning rate: 6.5321e-05]
	Learning Rate: 6.5321e-05
	LOSS [training: 2.948576559316513 | validation: 3.2023641313847713]
	TIME [epoch: 9.73 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.94791112737398		[learning rate: 6.5084e-05]
	Learning Rate: 6.50839e-05
	LOSS [training: 2.94791112737398 | validation: 3.1959346629319065]
	TIME [epoch: 9.73 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9514236131474907		[learning rate: 6.4848e-05]
	Learning Rate: 6.48477e-05
	LOSS [training: 2.9514236131474907 | validation: 3.201511618850673]
	TIME [epoch: 9.73 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9576493262167904		[learning rate: 6.4612e-05]
	Learning Rate: 6.46124e-05
	LOSS [training: 2.9576493262167904 | validation: 3.2183681968575573]
	TIME [epoch: 9.74 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962171399944312		[learning rate: 6.4378e-05]
	Learning Rate: 6.43779e-05
	LOSS [training: 2.962171399944312 | validation: 3.2130085560056525]
	TIME [epoch: 9.72 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9535878348816187		[learning rate: 6.4144e-05]
	Learning Rate: 6.41443e-05
	LOSS [training: 2.9535878348816187 | validation: 3.202323295836799]
	TIME [epoch: 9.71 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9568536365967724		[learning rate: 6.3911e-05]
	Learning Rate: 6.39115e-05
	LOSS [training: 2.9568536365967724 | validation: 3.18804275551172]
	TIME [epoch: 9.73 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9507394212416638		[learning rate: 6.368e-05]
	Learning Rate: 6.36795e-05
	LOSS [training: 2.9507394212416638 | validation: 3.204427922456709]
	TIME [epoch: 9.71 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948016822839117		[learning rate: 6.3448e-05]
	Learning Rate: 6.34485e-05
	LOSS [training: 2.948016822839117 | validation: 3.204979134704794]
	TIME [epoch: 9.71 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9515929514465786		[learning rate: 6.3218e-05]
	Learning Rate: 6.32182e-05
	LOSS [training: 2.9515929514465786 | validation: 3.200759706427848]
	TIME [epoch: 9.73 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9539855983404655		[learning rate: 6.2989e-05]
	Learning Rate: 6.29888e-05
	LOSS [training: 2.9539855983404655 | validation: 3.2040542989778467]
	TIME [epoch: 9.71 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9534864955587485		[learning rate: 6.276e-05]
	Learning Rate: 6.27602e-05
	LOSS [training: 2.9534864955587485 | validation: 3.2050314018122354]
	TIME [epoch: 9.71 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9535986105933527		[learning rate: 6.2532e-05]
	Learning Rate: 6.25324e-05
	LOSS [training: 2.9535986105933527 | validation: 3.2078243828095747]
	TIME [epoch: 9.72 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9551936450285483		[learning rate: 6.2305e-05]
	Learning Rate: 6.23055e-05
	LOSS [training: 2.9551936450285483 | validation: 3.2154579258111693]
	TIME [epoch: 9.73 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9511252898415514		[learning rate: 6.2079e-05]
	Learning Rate: 6.20794e-05
	LOSS [training: 2.9511252898415514 | validation: 3.2036534743648026]
	TIME [epoch: 9.71 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9546262194555504		[learning rate: 6.1854e-05]
	Learning Rate: 6.18541e-05
	LOSS [training: 2.9546262194555504 | validation: 3.2079215444646185]
	TIME [epoch: 9.72 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9516741039142302		[learning rate: 6.163e-05]
	Learning Rate: 6.16296e-05
	LOSS [training: 2.9516741039142302 | validation: 3.200049859869512]
	TIME [epoch: 9.73 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9518943378091045		[learning rate: 6.1406e-05]
	Learning Rate: 6.1406e-05
	LOSS [training: 2.9518943378091045 | validation: 3.2066327844556315]
	TIME [epoch: 9.71 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952224707778453		[learning rate: 6.1183e-05]
	Learning Rate: 6.11831e-05
	LOSS [training: 2.952224707778453 | validation: 3.2158875148853214]
	TIME [epoch: 9.71 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948454746727791		[learning rate: 6.0961e-05]
	Learning Rate: 6.09611e-05
	LOSS [training: 2.948454746727791 | validation: 3.1962926668839464]
	TIME [epoch: 9.73 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952893535251808		[learning rate: 6.074e-05]
	Learning Rate: 6.07399e-05
	LOSS [training: 2.952893535251808 | validation: 3.1920865826424603]
	TIME [epoch: 9.71 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9527348229554855		[learning rate: 6.0519e-05]
	Learning Rate: 6.05194e-05
	LOSS [training: 2.9527348229554855 | validation: 3.205688226813303]
	TIME [epoch: 9.71 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960851835926653		[learning rate: 6.03e-05]
	Learning Rate: 6.02998e-05
	LOSS [training: 2.960851835926653 | validation: 3.213198385364371]
	TIME [epoch: 9.73 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955489386948765		[learning rate: 6.0081e-05]
	Learning Rate: 6.0081e-05
	LOSS [training: 2.955489386948765 | validation: 3.2037291318620293]
	TIME [epoch: 9.71 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9482006726260517		[learning rate: 5.9863e-05]
	Learning Rate: 5.98629e-05
	LOSS [training: 2.9482006726260517 | validation: 3.1928090913506346]
	TIME [epoch: 9.71 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9518433745144694		[learning rate: 5.9646e-05]
	Learning Rate: 5.96457e-05
	LOSS [training: 2.9518433745144694 | validation: 3.1998570790678396]
	TIME [epoch: 9.73 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9491409577430727		[learning rate: 5.9429e-05]
	Learning Rate: 5.94292e-05
	LOSS [training: 2.9491409577430727 | validation: 3.199843679714754]
	TIME [epoch: 9.71 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953948565117231		[learning rate: 5.9214e-05]
	Learning Rate: 5.92136e-05
	LOSS [training: 2.953948565117231 | validation: 3.1901243306569183]
	TIME [epoch: 9.71 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9497668418240957		[learning rate: 5.8999e-05]
	Learning Rate: 5.89987e-05
	LOSS [training: 2.9497668418240957 | validation: 3.2031246705718694]
	TIME [epoch: 9.73 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954303731520924		[learning rate: 5.8785e-05]
	Learning Rate: 5.87846e-05
	LOSS [training: 2.954303731520924 | validation: 3.202999566039574]
	TIME [epoch: 9.7 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958129450833284		[learning rate: 5.8571e-05]
	Learning Rate: 5.85712e-05
	LOSS [training: 2.958129450833284 | validation: 3.2087034567335557]
	TIME [epoch: 9.72 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953154780871481		[learning rate: 5.8359e-05]
	Learning Rate: 5.83586e-05
	LOSS [training: 2.953154780871481 | validation: 3.201128074225977]
	TIME [epoch: 9.72 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.947840558218866		[learning rate: 5.8147e-05]
	Learning Rate: 5.81469e-05
	LOSS [training: 2.947840558218866 | validation: 3.186301296215522]
	TIME [epoch: 9.72 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9486511657060595		[learning rate: 5.7936e-05]
	Learning Rate: 5.79358e-05
	LOSS [training: 2.9486511657060595 | validation: 3.202720391173886]
	TIME [epoch: 9.71 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9515198754635996		[learning rate: 5.7726e-05]
	Learning Rate: 5.77256e-05
	LOSS [training: 2.9515198754635996 | validation: 3.1977297880721136]
	TIME [epoch: 9.73 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955111432938656		[learning rate: 5.7516e-05]
	Learning Rate: 5.75161e-05
	LOSS [training: 2.955111432938656 | validation: 3.196419298156767]
	TIME [epoch: 9.72 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950180764955495		[learning rate: 5.7307e-05]
	Learning Rate: 5.73074e-05
	LOSS [training: 2.950180764955495 | validation: 3.2045108832356046]
	TIME [epoch: 9.72 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9544786800510585		[learning rate: 5.7099e-05]
	Learning Rate: 5.70994e-05
	LOSS [training: 2.9544786800510585 | validation: 3.189250224483804]
	TIME [epoch: 9.73 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949149377072114		[learning rate: 5.6892e-05]
	Learning Rate: 5.68922e-05
	LOSS [training: 2.949149377072114 | validation: 3.2047337078852185]
	TIME [epoch: 9.71 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9511062171654974		[learning rate: 5.6686e-05]
	Learning Rate: 5.66857e-05
	LOSS [training: 2.9511062171654974 | validation: 3.213954738142304]
	TIME [epoch: 9.7 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9566060122450484		[learning rate: 5.648e-05]
	Learning Rate: 5.648e-05
	LOSS [training: 2.9566060122450484 | validation: 3.2030896359354166]
	TIME [epoch: 9.73 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9506305968872875		[learning rate: 5.6275e-05]
	Learning Rate: 5.6275e-05
	LOSS [training: 2.9506305968872875 | validation: 3.1965494641823056]
	TIME [epoch: 9.72 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9507517980496574		[learning rate: 5.6071e-05]
	Learning Rate: 5.60708e-05
	LOSS [training: 2.9507517980496574 | validation: 3.195775137407478]
	TIME [epoch: 9.7 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9501893562298083		[learning rate: 5.5867e-05]
	Learning Rate: 5.58673e-05
	LOSS [training: 2.9501893562298083 | validation: 3.2003048346387146]
	TIME [epoch: 9.73 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9514589537933373		[learning rate: 5.5665e-05]
	Learning Rate: 5.56646e-05
	LOSS [training: 2.9514589537933373 | validation: 3.2038076182503143]
	TIME [epoch: 9.71 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9524043058369065		[learning rate: 5.5463e-05]
	Learning Rate: 5.54626e-05
	LOSS [training: 2.9524043058369065 | validation: 3.1991116391972647]
	TIME [epoch: 9.71 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9545995652249823		[learning rate: 5.5261e-05]
	Learning Rate: 5.52613e-05
	LOSS [training: 2.9545995652249823 | validation: 3.201850821365797]
	TIME [epoch: 9.71 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.94874373548143		[learning rate: 5.5061e-05]
	Learning Rate: 5.50608e-05
	LOSS [training: 2.94874373548143 | validation: 3.2062686484282215]
	TIME [epoch: 9.72 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955409433443858		[learning rate: 5.4861e-05]
	Learning Rate: 5.48609e-05
	LOSS [training: 2.955409433443858 | validation: 3.2087835133198896]
	TIME [epoch: 9.7 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952852926499026		[learning rate: 5.4662e-05]
	Learning Rate: 5.46618e-05
	LOSS [training: 2.952852926499026 | validation: 3.2019909158287727]
	TIME [epoch: 9.72 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9515837378359366		[learning rate: 5.4463e-05]
	Learning Rate: 5.44635e-05
	LOSS [training: 2.9515837378359366 | validation: 3.1987065189089368]
	TIME [epoch: 9.72 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9512727726835934		[learning rate: 5.4266e-05]
	Learning Rate: 5.42658e-05
	LOSS [training: 2.9512727726835934 | validation: 3.202441506119186]
	TIME [epoch: 9.7 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9560097809308146		[learning rate: 5.4069e-05]
	Learning Rate: 5.40689e-05
	LOSS [training: 2.9560097809308146 | validation: 3.1913150631213227]
	TIME [epoch: 9.71 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950431860365538		[learning rate: 5.3873e-05]
	Learning Rate: 5.38727e-05
	LOSS [training: 2.950431860365538 | validation: 3.2024355036207437]
	TIME [epoch: 9.72 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951155559201619		[learning rate: 5.3677e-05]
	Learning Rate: 5.36772e-05
	LOSS [training: 2.951155559201619 | validation: 3.2101376812859996]
	TIME [epoch: 9.71 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9517534932958402		[learning rate: 5.3482e-05]
	Learning Rate: 5.34824e-05
	LOSS [training: 2.9517534932958402 | validation: 3.2055286723286565]
	TIME [epoch: 9.71 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949103799862928		[learning rate: 5.3288e-05]
	Learning Rate: 5.32883e-05
	LOSS [training: 2.949103799862928 | validation: 3.1989474100598945]
	TIME [epoch: 9.73 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9506772272248893		[learning rate: 5.3095e-05]
	Learning Rate: 5.30949e-05
	LOSS [training: 2.9506772272248893 | validation: 3.20668283239704]
	TIME [epoch: 9.72 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95286231053218		[learning rate: 5.2902e-05]
	Learning Rate: 5.29022e-05
	LOSS [training: 2.95286231053218 | validation: 3.1856000130090276]
	TIME [epoch: 9.71 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9534753228696173		[learning rate: 5.271e-05]
	Learning Rate: 5.27102e-05
	LOSS [training: 2.9534753228696173 | validation: 3.2044107678456384]
	TIME [epoch: 9.72 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953062689548413		[learning rate: 5.2519e-05]
	Learning Rate: 5.25189e-05
	LOSS [training: 2.953062689548413 | validation: 3.197368401102199]
	TIME [epoch: 9.7 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951619915028498		[learning rate: 5.2328e-05]
	Learning Rate: 5.23283e-05
	LOSS [training: 2.951619915028498 | validation: 3.1874181419182053]
	TIME [epoch: 9.69 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9529928408729083		[learning rate: 5.2138e-05]
	Learning Rate: 5.21384e-05
	LOSS [training: 2.9529928408729083 | validation: 3.1946299800471785]
	TIME [epoch: 9.73 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9494681001798377		[learning rate: 5.1949e-05]
	Learning Rate: 5.19492e-05
	LOSS [training: 2.9494681001798377 | validation: 3.209203036484901]
	TIME [epoch: 9.71 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9449671051271125		[learning rate: 5.1761e-05]
	Learning Rate: 5.17607e-05
	LOSS [training: 2.9449671051271125 | validation: 3.1963661418064153]
	TIME [epoch: 9.7 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952987074058852		[learning rate: 5.1573e-05]
	Learning Rate: 5.15729e-05
	LOSS [training: 2.952987074058852 | validation: 3.199941346437673]
	TIME [epoch: 9.73 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9508424551445134		[learning rate: 5.1386e-05]
	Learning Rate: 5.13857e-05
	LOSS [training: 2.9508424551445134 | validation: 3.1972271717897933]
	TIME [epoch: 9.7 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952367485975473		[learning rate: 5.1199e-05]
	Learning Rate: 5.11992e-05
	LOSS [training: 2.952367485975473 | validation: 3.211335431154349]
	TIME [epoch: 9.7 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9497614921684585		[learning rate: 5.1013e-05]
	Learning Rate: 5.10134e-05
	LOSS [training: 2.9497614921684585 | validation: 3.2021492372911893]
	TIME [epoch: 9.72 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948700991975401		[learning rate: 5.0828e-05]
	Learning Rate: 5.08283e-05
	LOSS [training: 2.948700991975401 | validation: 3.213326425030733]
	TIME [epoch: 9.69 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9521150109800356		[learning rate: 5.0644e-05]
	Learning Rate: 5.06438e-05
	LOSS [training: 2.9521150109800356 | validation: 3.197016675078871]
	TIME [epoch: 9.7 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9539811934956233		[learning rate: 5.046e-05]
	Learning Rate: 5.046e-05
	LOSS [training: 2.9539811934956233 | validation: 3.2055590134265186]
	TIME [epoch: 9.73 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9494367007808764		[learning rate: 5.0277e-05]
	Learning Rate: 5.02769e-05
	LOSS [training: 2.9494367007808764 | validation: 3.1889196065731107]
	TIME [epoch: 9.71 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9518767046172236		[learning rate: 5.0094e-05]
	Learning Rate: 5.00944e-05
	LOSS [training: 2.9518767046172236 | validation: 3.1875803837867798]
	TIME [epoch: 9.7 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9531244741951506		[learning rate: 4.9913e-05]
	Learning Rate: 4.99127e-05
	LOSS [training: 2.9531244741951506 | validation: 3.1996918808888473]
	TIME [epoch: 9.71 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.947767099590899		[learning rate: 4.9732e-05]
	Learning Rate: 4.97315e-05
	LOSS [training: 2.947767099590899 | validation: 3.202418437548044]
	TIME [epoch: 9.72 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951444820295404		[learning rate: 4.9551e-05]
	Learning Rate: 4.9551e-05
	LOSS [training: 2.951444820295404 | validation: 3.200367640916438]
	TIME [epoch: 9.7 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952643570529337		[learning rate: 4.9371e-05]
	Learning Rate: 4.93712e-05
	LOSS [training: 2.952643570529337 | validation: 3.198737732594294]
	TIME [epoch: 9.72 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9546377639998114		[learning rate: 4.9192e-05]
	Learning Rate: 4.9192e-05
	LOSS [training: 2.9546377639998114 | validation: 3.2037590029101604]
	TIME [epoch: 9.72 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949641483033015		[learning rate: 4.9014e-05]
	Learning Rate: 4.90135e-05
	LOSS [training: 2.949641483033015 | validation: 3.197598549557166]
	TIME [epoch: 9.7 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9493839373542428		[learning rate: 4.8836e-05]
	Learning Rate: 4.88356e-05
	LOSS [training: 2.9493839373542428 | validation: 3.2075282000657515]
	TIME [epoch: 9.71 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9523157654476284		[learning rate: 4.8658e-05]
	Learning Rate: 4.86584e-05
	LOSS [training: 2.9523157654476284 | validation: 3.199069451392682]
	TIME [epoch: 9.71 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.945800305444527		[learning rate: 4.8482e-05]
	Learning Rate: 4.84818e-05
	LOSS [training: 2.945800305444527 | validation: 3.190254160075267]
	TIME [epoch: 9.7 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953759659675838		[learning rate: 4.8306e-05]
	Learning Rate: 4.83059e-05
	LOSS [training: 2.953759659675838 | validation: 3.1968761434417376]
	TIME [epoch: 9.71 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9483818230305037		[learning rate: 4.8131e-05]
	Learning Rate: 4.81306e-05
	LOSS [training: 2.9483818230305037 | validation: 3.1994589022075353]
	TIME [epoch: 9.72 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950257148445916		[learning rate: 4.7956e-05]
	Learning Rate: 4.79559e-05
	LOSS [training: 2.950257148445916 | validation: 3.200150427525259]
	TIME [epoch: 9.69 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948033103723259		[learning rate: 4.7782e-05]
	Learning Rate: 4.77819e-05
	LOSS [training: 2.948033103723259 | validation: 3.2051762475505643]
	TIME [epoch: 9.7 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9547743253781915		[learning rate: 4.7608e-05]
	Learning Rate: 4.76085e-05
	LOSS [training: 2.9547743253781915 | validation: 3.201566031751109]
	TIME [epoch: 9.73 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956316482633528		[learning rate: 4.7436e-05]
	Learning Rate: 4.74357e-05
	LOSS [training: 2.956316482633528 | validation: 3.2043852369016284]
	TIME [epoch: 9.7 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9478145621363465		[learning rate: 4.7264e-05]
	Learning Rate: 4.72636e-05
	LOSS [training: 2.9478145621363465 | validation: 3.1942930274711263]
	TIME [epoch: 9.7 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950055000434801		[learning rate: 4.7092e-05]
	Learning Rate: 4.7092e-05
	LOSS [training: 2.950055000434801 | validation: 3.207233798958686]
	TIME [epoch: 9.73 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9531629438435667		[learning rate: 4.6921e-05]
	Learning Rate: 4.69211e-05
	LOSS [training: 2.9531629438435667 | validation: 3.2095724798004106]
	TIME [epoch: 9.71 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950209995662614		[learning rate: 4.6751e-05]
	Learning Rate: 4.67508e-05
	LOSS [training: 2.950209995662614 | validation: 3.188909749446171]
	TIME [epoch: 9.72 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9556657068226597		[learning rate: 4.6581e-05]
	Learning Rate: 4.65812e-05
	LOSS [training: 2.9556657068226597 | validation: 3.2035765597830017]
	TIME [epoch: 9.74 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951683078146778		[learning rate: 4.6412e-05]
	Learning Rate: 4.64121e-05
	LOSS [training: 2.951683078146778 | validation: 3.1998176003830014]
	TIME [epoch: 9.72 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9548002209026487		[learning rate: 4.6244e-05]
	Learning Rate: 4.62437e-05
	LOSS [training: 2.9548002209026487 | validation: 3.208287730960981]
	TIME [epoch: 9.71 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9512011853353983		[learning rate: 4.6076e-05]
	Learning Rate: 4.60759e-05
	LOSS [training: 2.9512011853353983 | validation: 3.1901763429399783]
	TIME [epoch: 9.73 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9566537875553833		[learning rate: 4.5909e-05]
	Learning Rate: 4.59087e-05
	LOSS [training: 2.9566537875553833 | validation: 3.2009057189916836]
	TIME [epoch: 9.7 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9529792645028583		[learning rate: 4.5742e-05]
	Learning Rate: 4.57421e-05
	LOSS [training: 2.9529792645028583 | validation: 3.2046257289467253]
	TIME [epoch: 9.7 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954121481839617		[learning rate: 4.5576e-05]
	Learning Rate: 4.55761e-05
	LOSS [training: 2.954121481839617 | validation: 3.1953351140725648]
	TIME [epoch: 9.73 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9589722588611296		[learning rate: 4.5411e-05]
	Learning Rate: 4.54107e-05
	LOSS [training: 2.9589722588611296 | validation: 3.1967666120956797]
	TIME [epoch: 9.69 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956175448244123		[learning rate: 4.5246e-05]
	Learning Rate: 4.52459e-05
	LOSS [training: 2.956175448244123 | validation: 3.203617326711257]
	TIME [epoch: 9.7 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952360381464233		[learning rate: 4.5082e-05]
	Learning Rate: 4.50817e-05
	LOSS [training: 2.952360381464233 | validation: 3.206774195809554]
	TIME [epoch: 9.72 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952942763418462		[learning rate: 4.4918e-05]
	Learning Rate: 4.49181e-05
	LOSS [training: 2.952942763418462 | validation: 3.1980317935489317]
	TIME [epoch: 9.7 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9550144704640786		[learning rate: 4.4755e-05]
	Learning Rate: 4.47551e-05
	LOSS [training: 2.9550144704640786 | validation: 3.1986207141925718]
	TIME [epoch: 9.7 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950527329513821		[learning rate: 4.4593e-05]
	Learning Rate: 4.45926e-05
	LOSS [training: 2.950527329513821 | validation: 3.1988433705928383]
	TIME [epoch: 9.71 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9480881231645713		[learning rate: 4.4431e-05]
	Learning Rate: 4.44308e-05
	LOSS [training: 2.9480881231645713 | validation: 3.1943140216589945]
	TIME [epoch: 9.71 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9558167330630325		[learning rate: 4.427e-05]
	Learning Rate: 4.42696e-05
	LOSS [training: 2.9558167330630325 | validation: 3.1906702645820877]
	TIME [epoch: 9.69 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9503379898616817		[learning rate: 4.4109e-05]
	Learning Rate: 4.41089e-05
	LOSS [training: 2.9503379898616817 | validation: 3.209357912064668]
	TIME [epoch: 9.72 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9461328600743633		[learning rate: 4.3949e-05]
	Learning Rate: 4.39489e-05
	LOSS [training: 2.9461328600743633 | validation: 3.2030697876204663]
	TIME [epoch: 9.71 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948576389907147		[learning rate: 4.3789e-05]
	Learning Rate: 4.37893e-05
	LOSS [training: 2.948576389907147 | validation: 3.196592126489552]
	TIME [epoch: 9.71 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952431542816572		[learning rate: 4.363e-05]
	Learning Rate: 4.36304e-05
	LOSS [training: 2.952431542816572 | validation: 3.1995671916013224]
	TIME [epoch: 9.72 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9490124981854473		[learning rate: 4.3472e-05]
	Learning Rate: 4.34721e-05
	LOSS [training: 2.9490124981854473 | validation: 3.199902907885633]
	TIME [epoch: 9.72 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953410175956587		[learning rate: 4.3314e-05]
	Learning Rate: 4.33143e-05
	LOSS [training: 2.953410175956587 | validation: 3.201618958095647]
	TIME [epoch: 9.69 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9477588456264385		[learning rate: 4.3157e-05]
	Learning Rate: 4.31571e-05
	LOSS [training: 2.9477588456264385 | validation: 3.1979532399361164]
	TIME [epoch: 9.7 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9532422390675492		[learning rate: 4.3001e-05]
	Learning Rate: 4.30005e-05
	LOSS [training: 2.9532422390675492 | validation: 3.196976312462141]
	TIME [epoch: 9.71 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9575495806105425		[learning rate: 4.2844e-05]
	Learning Rate: 4.28445e-05
	LOSS [training: 2.9575495806105425 | validation: 3.19768734373323]
	TIME [epoch: 9.7 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955880173443478		[learning rate: 4.2689e-05]
	Learning Rate: 4.2689e-05
	LOSS [training: 2.955880173443478 | validation: 3.2128200959867605]
	TIME [epoch: 9.71 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9536196970319293		[learning rate: 4.2534e-05]
	Learning Rate: 4.25341e-05
	LOSS [training: 2.9536196970319293 | validation: 3.202300346754939]
	TIME [epoch: 9.73 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9557953797514336		[learning rate: 4.238e-05]
	Learning Rate: 4.23797e-05
	LOSS [training: 2.9557953797514336 | validation: 3.197498088124578]
	TIME [epoch: 9.7 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9536643915303484		[learning rate: 4.2226e-05]
	Learning Rate: 4.22259e-05
	LOSS [training: 2.9536643915303484 | validation: 3.1964688060259165]
	TIME [epoch: 9.7 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949577270985794		[learning rate: 4.2073e-05]
	Learning Rate: 4.20727e-05
	LOSS [training: 2.949577270985794 | validation: 3.1993296529073447]
	TIME [epoch: 9.73 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949629704752462		[learning rate: 4.192e-05]
	Learning Rate: 4.192e-05
	LOSS [training: 2.949629704752462 | validation: 3.200275624471513]
	TIME [epoch: 9.71 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9505611649671253		[learning rate: 4.1768e-05]
	Learning Rate: 4.17679e-05
	LOSS [training: 2.9505611649671253 | validation: 3.198490380183108]
	TIME [epoch: 9.71 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9493284563537996		[learning rate: 4.1616e-05]
	Learning Rate: 4.16163e-05
	LOSS [training: 2.9493284563537996 | validation: 3.193335209419048]
	TIME [epoch: 9.73 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9447776208056795		[learning rate: 4.1465e-05]
	Learning Rate: 4.14652e-05
	LOSS [training: 2.9447776208056795 | validation: 3.19255176254503]
	TIME [epoch: 9.71 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95077943349452		[learning rate: 4.1315e-05]
	Learning Rate: 4.13148e-05
	LOSS [training: 2.95077943349452 | validation: 3.2010663264158623]
	TIME [epoch: 9.71 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95227084076702		[learning rate: 4.1165e-05]
	Learning Rate: 4.11648e-05
	LOSS [training: 2.95227084076702 | validation: 3.1931256284480543]
	TIME [epoch: 9.73 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949336633259036		[learning rate: 4.1015e-05]
	Learning Rate: 4.10154e-05
	LOSS [training: 2.949336633259036 | validation: 3.1919833611946657]
	TIME [epoch: 9.71 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9500621920118117		[learning rate: 4.0867e-05]
	Learning Rate: 4.08666e-05
	LOSS [training: 2.9500621920118117 | validation: 3.1985648623017333]
	TIME [epoch: 9.71 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9503922193111003		[learning rate: 4.0718e-05]
	Learning Rate: 4.07183e-05
	LOSS [training: 2.9503922193111003 | validation: 3.1958321241828664]
	TIME [epoch: 9.73 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9560588586724257		[learning rate: 4.0571e-05]
	Learning Rate: 4.05705e-05
	LOSS [training: 2.9560588586724257 | validation: 3.1905485854413063]
	TIME [epoch: 9.7 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9528555437855837		[learning rate: 4.0423e-05]
	Learning Rate: 4.04233e-05
	LOSS [training: 2.9528555437855837 | validation: 3.2061104984528743]
	TIME [epoch: 9.71 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949825110120707		[learning rate: 4.0277e-05]
	Learning Rate: 4.02766e-05
	LOSS [training: 2.949825110120707 | validation: 3.208060414854765]
	TIME [epoch: 9.71 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9516048899350453		[learning rate: 4.013e-05]
	Learning Rate: 4.01304e-05
	LOSS [training: 2.9516048899350453 | validation: 3.196841318809991]
	TIME [epoch: 9.72 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95355606066116		[learning rate: 3.9985e-05]
	Learning Rate: 3.99848e-05
	LOSS [training: 2.95355606066116 | validation: 3.1966857625218585]
	TIME [epoch: 9.72 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9566287809881024		[learning rate: 3.984e-05]
	Learning Rate: 3.98397e-05
	LOSS [training: 2.9566287809881024 | validation: 3.1871152352246646]
	TIME [epoch: 9.73 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9509498290929606		[learning rate: 3.9695e-05]
	Learning Rate: 3.96951e-05
	LOSS [training: 2.9509498290929606 | validation: 3.203902232773277]
	TIME [epoch: 9.72 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955019683265488		[learning rate: 3.9551e-05]
	Learning Rate: 3.9551e-05
	LOSS [training: 2.955019683265488 | validation: 3.189278760030581]
	TIME [epoch: 9.71 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9507418047103435		[learning rate: 3.9408e-05]
	Learning Rate: 3.94075e-05
	LOSS [training: 2.9507418047103435 | validation: 3.18271446220353]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r1_20240219_235952/states/model_tr_study206_1623.pth
	Model improved!!!
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9497500001027652		[learning rate: 3.9264e-05]
	Learning Rate: 3.92645e-05
	LOSS [training: 2.9497500001027652 | validation: 3.192118880787014]
	TIME [epoch: 9.71 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952005268780625		[learning rate: 3.9122e-05]
	Learning Rate: 3.9122e-05
	LOSS [training: 2.952005268780625 | validation: 3.1931074454806008]
	TIME [epoch: 9.7 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951065670475809		[learning rate: 3.898e-05]
	Learning Rate: 3.898e-05
	LOSS [training: 2.951065670475809 | validation: 3.188717679569225]
	TIME [epoch: 9.72 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.94677874694477		[learning rate: 3.8839e-05]
	Learning Rate: 3.88386e-05
	LOSS [training: 2.94677874694477 | validation: 3.190712973157157]
	TIME [epoch: 9.72 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9507212646304675		[learning rate: 3.8698e-05]
	Learning Rate: 3.86976e-05
	LOSS [training: 2.9507212646304675 | validation: 3.2024464089070994]
	TIME [epoch: 9.71 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956981912668804		[learning rate: 3.8557e-05]
	Learning Rate: 3.85572e-05
	LOSS [training: 2.956981912668804 | validation: 3.2066638708441055]
	TIME [epoch: 9.72 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9470149594036794		[learning rate: 3.8417e-05]
	Learning Rate: 3.84173e-05
	LOSS [training: 2.9470149594036794 | validation: 3.20055683649146]
	TIME [epoch: 9.73 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9485623036974866		[learning rate: 3.8278e-05]
	Learning Rate: 3.82778e-05
	LOSS [training: 2.9485623036974866 | validation: 3.205609804161046]
	TIME [epoch: 9.71 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9544630325080816		[learning rate: 3.8139e-05]
	Learning Rate: 3.81389e-05
	LOSS [training: 2.9544630325080816 | validation: 3.198883168714267]
	TIME [epoch: 9.72 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955785881178142		[learning rate: 3.8001e-05]
	Learning Rate: 3.80005e-05
	LOSS [training: 2.955785881178142 | validation: 3.212577070735317]
	TIME [epoch: 9.73 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.947294631562886		[learning rate: 3.7863e-05]
	Learning Rate: 3.78626e-05
	LOSS [training: 2.947294631562886 | validation: 3.186015564205135]
	TIME [epoch: 9.71 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953503416216667		[learning rate: 3.7725e-05]
	Learning Rate: 3.77252e-05
	LOSS [training: 2.953503416216667 | validation: 3.203362518843269]
	TIME [epoch: 9.7 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951835211445791		[learning rate: 3.7588e-05]
	Learning Rate: 3.75883e-05
	LOSS [training: 2.951835211445791 | validation: 3.20324914217482]
	TIME [epoch: 9.73 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9490899690521877		[learning rate: 3.7452e-05]
	Learning Rate: 3.74519e-05
	LOSS [training: 2.9490899690521877 | validation: 3.1920249296121126]
	TIME [epoch: 9.71 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958256408955328		[learning rate: 3.7316e-05]
	Learning Rate: 3.7316e-05
	LOSS [training: 2.958256408955328 | validation: 3.2016252039458735]
	TIME [epoch: 9.7 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95049483359403		[learning rate: 3.7181e-05]
	Learning Rate: 3.71805e-05
	LOSS [training: 2.95049483359403 | validation: 3.20649218335878]
	TIME [epoch: 9.74 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959169721352636		[learning rate: 3.7046e-05]
	Learning Rate: 3.70456e-05
	LOSS [training: 2.959169721352636 | validation: 3.20060782668332]
	TIME [epoch: 9.72 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9505978393068353		[learning rate: 3.6911e-05]
	Learning Rate: 3.69112e-05
	LOSS [training: 2.9505978393068353 | validation: 3.189852209676717]
	TIME [epoch: 9.71 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956048239664185		[learning rate: 3.6777e-05]
	Learning Rate: 3.67772e-05
	LOSS [training: 2.956048239664185 | validation: 3.2007026623178794]
	TIME [epoch: 9.73 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951229919361031		[learning rate: 3.6644e-05]
	Learning Rate: 3.66438e-05
	LOSS [training: 2.951229919361031 | validation: 3.1978759785806448]
	TIME [epoch: 9.71 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9495251784398375		[learning rate: 3.6511e-05]
	Learning Rate: 3.65108e-05
	LOSS [training: 2.9495251784398375 | validation: 3.205769480678827]
	TIME [epoch: 9.71 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949446056649331		[learning rate: 3.6378e-05]
	Learning Rate: 3.63783e-05
	LOSS [training: 2.949446056649331 | validation: 3.2040195617447855]
	TIME [epoch: 9.73 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9539023502303836		[learning rate: 3.6246e-05]
	Learning Rate: 3.62463e-05
	LOSS [training: 2.9539023502303836 | validation: 3.1966749152345466]
	TIME [epoch: 9.71 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9502568460964538		[learning rate: 3.6115e-05]
	Learning Rate: 3.61147e-05
	LOSS [training: 2.9502568460964538 | validation: 3.201190114378127]
	TIME [epoch: 9.71 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9587226723308655		[learning rate: 3.5984e-05]
	Learning Rate: 3.59837e-05
	LOSS [training: 2.9587226723308655 | validation: 3.1991468077743126]
	TIME [epoch: 9.73 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9555511960693104		[learning rate: 3.5853e-05]
	Learning Rate: 3.58531e-05
	LOSS [training: 2.9555511960693104 | validation: 3.194436061485946]
	TIME [epoch: 9.72 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9497126415566832		[learning rate: 3.5723e-05]
	Learning Rate: 3.5723e-05
	LOSS [training: 2.9497126415566832 | validation: 3.193786227330305]
	TIME [epoch: 9.71 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9512624930024964		[learning rate: 3.5593e-05]
	Learning Rate: 3.55933e-05
	LOSS [training: 2.9512624930024964 | validation: 3.184367760456699]
	TIME [epoch: 9.73 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9509723306427804		[learning rate: 3.5464e-05]
	Learning Rate: 3.54641e-05
	LOSS [training: 2.9509723306427804 | validation: 3.201936335272965]
	TIME [epoch: 9.71 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9555345944851568		[learning rate: 3.5335e-05]
	Learning Rate: 3.53354e-05
	LOSS [training: 2.9555345944851568 | validation: 3.217448686094016]
	TIME [epoch: 9.71 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951457224910092		[learning rate: 3.5207e-05]
	Learning Rate: 3.52072e-05
	LOSS [training: 2.951457224910092 | validation: 3.2046384321665453]
	TIME [epoch: 9.73 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.947824187691386		[learning rate: 3.5079e-05]
	Learning Rate: 3.50794e-05
	LOSS [training: 2.947824187691386 | validation: 3.2003711517005518]
	TIME [epoch: 9.71 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9538648192809642		[learning rate: 3.4952e-05]
	Learning Rate: 3.49521e-05
	LOSS [training: 2.9538648192809642 | validation: 3.2010601877962]
	TIME [epoch: 9.7 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948705201235758		[learning rate: 3.4825e-05]
	Learning Rate: 3.48253e-05
	LOSS [training: 2.948705201235758 | validation: 3.1931226957405094]
	TIME [epoch: 9.72 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9505672805902994		[learning rate: 3.4699e-05]
	Learning Rate: 3.46989e-05
	LOSS [training: 2.9505672805902994 | validation: 3.1876802310071977]
	TIME [epoch: 9.72 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9482126164091813		[learning rate: 3.4573e-05]
	Learning Rate: 3.4573e-05
	LOSS [training: 2.9482126164091813 | validation: 3.202859442172599]
	TIME [epoch: 9.7 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9514791934684		[learning rate: 3.4448e-05]
	Learning Rate: 3.44475e-05
	LOSS [training: 2.9514791934684 | validation: 3.1952870944987377]
	TIME [epoch: 9.73 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.946516692298698		[learning rate: 3.4323e-05]
	Learning Rate: 3.43225e-05
	LOSS [training: 2.946516692298698 | validation: 3.2053594341352367]
	TIME [epoch: 9.72 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9507933721830524		[learning rate: 3.4198e-05]
	Learning Rate: 3.4198e-05
	LOSS [training: 2.9507933721830524 | validation: 3.200795314127232]
	TIME [epoch: 9.71 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9496453514353576		[learning rate: 3.4074e-05]
	Learning Rate: 3.40738e-05
	LOSS [training: 2.9496453514353576 | validation: 3.1942648277234684]
	TIME [epoch: 9.71 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9505059022594686		[learning rate: 3.395e-05]
	Learning Rate: 3.39502e-05
	LOSS [training: 2.9505059022594686 | validation: 3.199786845570234]
	TIME [epoch: 9.72 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9528031460608775		[learning rate: 3.3827e-05]
	Learning Rate: 3.3827e-05
	LOSS [training: 2.9528031460608775 | validation: 3.19915194830861]
	TIME [epoch: 9.71 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9470173782860214		[learning rate: 3.3704e-05]
	Learning Rate: 3.37042e-05
	LOSS [training: 2.9470173782860214 | validation: 3.199790169821781]
	TIME [epoch: 9.71 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9528230123409833		[learning rate: 3.3582e-05]
	Learning Rate: 3.35819e-05
	LOSS [training: 2.9528230123409833 | validation: 3.1873506144745063]
	TIME [epoch: 9.72 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949441218626513		[learning rate: 3.346e-05]
	Learning Rate: 3.346e-05
	LOSS [training: 2.949441218626513 | validation: 3.192201944951154]
	TIME [epoch: 9.71 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951840266623285		[learning rate: 3.3339e-05]
	Learning Rate: 3.33386e-05
	LOSS [training: 2.951840266623285 | validation: 3.1941159376204236]
	TIME [epoch: 9.7 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.946736086616026		[learning rate: 3.3218e-05]
	Learning Rate: 3.32176e-05
	LOSS [training: 2.946736086616026 | validation: 3.2004037590552765]
	TIME [epoch: 9.73 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9477944655596944		[learning rate: 3.3097e-05]
	Learning Rate: 3.30971e-05
	LOSS [training: 2.9477944655596944 | validation: 3.1935940172015114]
	TIME [epoch: 9.69 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9519677273223444		[learning rate: 3.2977e-05]
	Learning Rate: 3.2977e-05
	LOSS [training: 2.9519677273223444 | validation: 3.1961205117453564]
	TIME [epoch: 9.7 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948643678737956		[learning rate: 3.2857e-05]
	Learning Rate: 3.28573e-05
	LOSS [training: 2.948643678737956 | validation: 3.2015414424204733]
	TIME [epoch: 9.72 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9545754424990522		[learning rate: 3.2738e-05]
	Learning Rate: 3.2738e-05
	LOSS [training: 2.9545754424990522 | validation: 3.1927140638692637]
	TIME [epoch: 9.7 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9509614641164665		[learning rate: 3.2619e-05]
	Learning Rate: 3.26192e-05
	LOSS [training: 2.9509614641164665 | validation: 3.2029567732976854]
	TIME [epoch: 9.71 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9518274091174033		[learning rate: 3.2501e-05]
	Learning Rate: 3.25009e-05
	LOSS [training: 2.9518274091174033 | validation: 3.189027501417833]
	TIME [epoch: 9.73 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9465706321264458		[learning rate: 3.2383e-05]
	Learning Rate: 3.23829e-05
	LOSS [training: 2.9465706321264458 | validation: 3.191060218273788]
	TIME [epoch: 9.71 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9519290245722565		[learning rate: 3.2265e-05]
	Learning Rate: 3.22654e-05
	LOSS [training: 2.9519290245722565 | validation: 3.197275848631849]
	TIME [epoch: 9.7 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9523687438300583		[learning rate: 3.2148e-05]
	Learning Rate: 3.21483e-05
	LOSS [training: 2.9523687438300583 | validation: 3.195563761678203]
	TIME [epoch: 9.73 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949603837557551		[learning rate: 3.2032e-05]
	Learning Rate: 3.20316e-05
	LOSS [training: 2.949603837557551 | validation: 3.1948916292536977]
	TIME [epoch: 9.71 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9515150538556303		[learning rate: 3.1915e-05]
	Learning Rate: 3.19154e-05
	LOSS [training: 2.9515150538556303 | validation: 3.1982788950292638]
	TIME [epoch: 9.71 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948095055989333		[learning rate: 3.18e-05]
	Learning Rate: 3.17996e-05
	LOSS [training: 2.948095055989333 | validation: 3.1897939488202383]
	TIME [epoch: 9.73 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950809234120984		[learning rate: 3.1684e-05]
	Learning Rate: 3.16842e-05
	LOSS [training: 2.950809234120984 | validation: 3.2015222162102175]
	TIME [epoch: 9.71 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9494034777360776		[learning rate: 3.1569e-05]
	Learning Rate: 3.15692e-05
	LOSS [training: 2.9494034777360776 | validation: 3.1993436075805093]
	TIME [epoch: 9.71 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954432350763176		[learning rate: 3.1455e-05]
	Learning Rate: 3.14546e-05
	LOSS [training: 2.954432350763176 | validation: 3.211318989756335]
	TIME [epoch: 9.72 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9472772038512893		[learning rate: 3.134e-05]
	Learning Rate: 3.13405e-05
	LOSS [training: 2.9472772038512893 | validation: 3.1932051207763177]
	TIME [epoch: 9.71 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9495914133333945		[learning rate: 3.1227e-05]
	Learning Rate: 3.12267e-05
	LOSS [training: 2.9495914133333945 | validation: 3.1909623745579836]
	TIME [epoch: 9.7 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95378084803853		[learning rate: 3.1113e-05]
	Learning Rate: 3.11134e-05
	LOSS [training: 2.95378084803853 | validation: 3.1940690284202833]
	TIME [epoch: 9.73 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949561429840822		[learning rate: 3.1e-05]
	Learning Rate: 3.10005e-05
	LOSS [training: 2.949561429840822 | validation: 3.20260047900077]
	TIME [epoch: 9.71 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9516445097387596		[learning rate: 3.0888e-05]
	Learning Rate: 3.0888e-05
	LOSS [training: 2.9516445097387596 | validation: 3.195837800130683]
	TIME [epoch: 9.7 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952127982679008		[learning rate: 3.0776e-05]
	Learning Rate: 3.07759e-05
	LOSS [training: 2.952127982679008 | validation: 3.19895685136922]
	TIME [epoch: 9.72 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949647269680114		[learning rate: 3.0664e-05]
	Learning Rate: 3.06642e-05
	LOSS [training: 2.949647269680114 | validation: 3.191576004059126]
	TIME [epoch: 9.71 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9476330786693614		[learning rate: 3.0553e-05]
	Learning Rate: 3.05529e-05
	LOSS [training: 2.9476330786693614 | validation: 3.1961937916811793]
	TIME [epoch: 9.7 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95134824516989		[learning rate: 3.0442e-05]
	Learning Rate: 3.0442e-05
	LOSS [training: 2.95134824516989 | validation: 3.208387490430043]
	TIME [epoch: 9.71 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.947669007090917		[learning rate: 3.0332e-05]
	Learning Rate: 3.03316e-05
	LOSS [training: 2.947669007090917 | validation: 3.199532798973252]
	TIME [epoch: 9.73 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9493635691782742		[learning rate: 3.0221e-05]
	Learning Rate: 3.02215e-05
	LOSS [training: 2.9493635691782742 | validation: 3.202910369997417]
	TIME [epoch: 9.71 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9518355976779613		[learning rate: 3.0112e-05]
	Learning Rate: 3.01118e-05
	LOSS [training: 2.9518355976779613 | validation: 3.2048113980701745]
	TIME [epoch: 9.71 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952394722222007		[learning rate: 3.0003e-05]
	Learning Rate: 3.00025e-05
	LOSS [training: 2.952394722222007 | validation: 3.1989378463687252]
	TIME [epoch: 9.72 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9507018066418604		[learning rate: 2.9894e-05]
	Learning Rate: 2.98936e-05
	LOSS [training: 2.9507018066418604 | validation: 3.1987000252791127]
	TIME [epoch: 9.71 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.947919966602464		[learning rate: 2.9785e-05]
	Learning Rate: 2.97852e-05
	LOSS [training: 2.947919966602464 | validation: 3.199120091883093]
	TIME [epoch: 9.7 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9489417063359147		[learning rate: 2.9677e-05]
	Learning Rate: 2.96771e-05
	LOSS [training: 2.9489417063359147 | validation: 3.1831100492965376]
	TIME [epoch: 9.73 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949065582113872		[learning rate: 2.9569e-05]
	Learning Rate: 2.95694e-05
	LOSS [training: 2.949065582113872 | validation: 3.2021249348927596]
	TIME [epoch: 9.7 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9528924501253		[learning rate: 2.9462e-05]
	Learning Rate: 2.94621e-05
	LOSS [training: 2.9528924501253 | validation: 3.201009257150845]
	TIME [epoch: 9.71 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949418807799526		[learning rate: 2.9355e-05]
	Learning Rate: 2.93551e-05
	LOSS [training: 2.949418807799526 | validation: 3.209540486952509]
	TIME [epoch: 9.73 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9507505464022548		[learning rate: 2.9249e-05]
	Learning Rate: 2.92486e-05
	LOSS [training: 2.9507505464022548 | validation: 3.1862589476698373]
	TIME [epoch: 9.71 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9537844891361686		[learning rate: 2.9142e-05]
	Learning Rate: 2.91425e-05
	LOSS [training: 2.9537844891361686 | validation: 3.203144125588811]
	TIME [epoch: 9.7 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9489322433432883		[learning rate: 2.9037e-05]
	Learning Rate: 2.90367e-05
	LOSS [training: 2.9489322433432883 | validation: 3.197506076368634]
	TIME [epoch: 9.73 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951107074071793		[learning rate: 2.8931e-05]
	Learning Rate: 2.89313e-05
	LOSS [training: 2.951107074071793 | validation: 3.1962116569643246]
	TIME [epoch: 9.71 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9508676812197927		[learning rate: 2.8826e-05]
	Learning Rate: 2.88263e-05
	LOSS [training: 2.9508676812197927 | validation: 3.1989925535068675]
	TIME [epoch: 9.71 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9467766832934275		[learning rate: 2.8722e-05]
	Learning Rate: 2.87217e-05
	LOSS [training: 2.9467766832934275 | validation: 3.204155404851432]
	TIME [epoch: 9.73 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9475464921688546		[learning rate: 2.8617e-05]
	Learning Rate: 2.86175e-05
	LOSS [training: 2.9475464921688546 | validation: 3.200277108306815]
	TIME [epoch: 9.71 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9531399778142022		[learning rate: 2.8514e-05]
	Learning Rate: 2.85136e-05
	LOSS [training: 2.9531399778142022 | validation: 3.2058847451414874]
	TIME [epoch: 9.7 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950298668332212		[learning rate: 2.841e-05]
	Learning Rate: 2.84102e-05
	LOSS [training: 2.950298668332212 | validation: 3.2070005109440776]
	TIME [epoch: 9.72 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948315237870399		[learning rate: 2.8307e-05]
	Learning Rate: 2.83071e-05
	LOSS [training: 2.948315237870399 | validation: 3.192774210334909]
	TIME [epoch: 9.71 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9530062456126855		[learning rate: 2.8204e-05]
	Learning Rate: 2.82043e-05
	LOSS [training: 2.9530062456126855 | validation: 3.194845174534519]
	TIME [epoch: 9.7 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952125758053478		[learning rate: 2.8102e-05]
	Learning Rate: 2.8102e-05
	LOSS [training: 2.952125758053478 | validation: 3.1996319407109035]
	TIME [epoch: 9.73 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.946458789489267		[learning rate: 2.8e-05]
	Learning Rate: 2.8e-05
	LOSS [training: 2.946458789489267 | validation: 3.201874368784775]
	TIME [epoch: 9.71 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9511300513535508		[learning rate: 2.7898e-05]
	Learning Rate: 2.78984e-05
	LOSS [training: 2.9511300513535508 | validation: 3.1954754333806803]
	TIME [epoch: 9.71 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948819117253565		[learning rate: 2.7797e-05]
	Learning Rate: 2.77971e-05
	LOSS [training: 2.948819117253565 | validation: 3.199182444928469]
	TIME [epoch: 9.73 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9453167207423774		[learning rate: 2.7696e-05]
	Learning Rate: 2.76963e-05
	LOSS [training: 2.9453167207423774 | validation: 3.199931800026743]
	TIME [epoch: 9.71 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9465863193563777		[learning rate: 2.7596e-05]
	Learning Rate: 2.75957e-05
	LOSS [training: 2.9465863193563777 | validation: 3.195125746397465]
	TIME [epoch: 9.71 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9462340472480157		[learning rate: 2.7496e-05]
	Learning Rate: 2.74956e-05
	LOSS [training: 2.9462340472480157 | validation: 3.189536565266235]
	TIME [epoch: 9.73 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949251834563394		[learning rate: 2.7396e-05]
	Learning Rate: 2.73958e-05
	LOSS [training: 2.949251834563394 | validation: 3.199026646554798]
	TIME [epoch: 9.71 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956057726706242		[learning rate: 2.7296e-05]
	Learning Rate: 2.72964e-05
	LOSS [training: 2.956057726706242 | validation: 3.200182371120942]
	TIME [epoch: 9.71 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948325574590875		[learning rate: 2.7197e-05]
	Learning Rate: 2.71973e-05
	LOSS [training: 2.948325574590875 | validation: 3.1929851985715216]
	TIME [epoch: 9.72 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.94951450839356		[learning rate: 2.7099e-05]
	Learning Rate: 2.70986e-05
	LOSS [training: 2.94951450839356 | validation: 3.194698588780982]
	TIME [epoch: 9.71 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9509911258629566		[learning rate: 2.7e-05]
	Learning Rate: 2.70003e-05
	LOSS [training: 2.9509911258629566 | validation: 3.2003687545093307]
	TIME [epoch: 9.71 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950286871851326		[learning rate: 2.6902e-05]
	Learning Rate: 2.69023e-05
	LOSS [training: 2.950286871851326 | validation: 3.20203744406836]
	TIME [epoch: 9.72 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954236872919286		[learning rate: 2.6805e-05]
	Learning Rate: 2.68047e-05
	LOSS [training: 2.954236872919286 | validation: 3.184536615627694]
	TIME [epoch: 9.72 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9517006734789573		[learning rate: 2.6707e-05]
	Learning Rate: 2.67074e-05
	LOSS [training: 2.9517006734789573 | validation: 3.198809294460732]
	TIME [epoch: 9.71 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9494480971570995		[learning rate: 2.661e-05]
	Learning Rate: 2.66105e-05
	LOSS [training: 2.9494480971570995 | validation: 3.1921538326242036]
	TIME [epoch: 9.72 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948166739163902		[learning rate: 2.6514e-05]
	Learning Rate: 2.65139e-05
	LOSS [training: 2.948166739163902 | validation: 3.2018758517783907]
	TIME [epoch: 9.72 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9551195882889694		[learning rate: 2.6418e-05]
	Learning Rate: 2.64177e-05
	LOSS [training: 2.9551195882889694 | validation: 3.19983417069403]
	TIME [epoch: 9.71 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9557842906097274		[learning rate: 2.6322e-05]
	Learning Rate: 2.63218e-05
	LOSS [training: 2.9557842906097274 | validation: 3.194942752768241]
	TIME [epoch: 9.71 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9570515178725274		[learning rate: 2.6226e-05]
	Learning Rate: 2.62263e-05
	LOSS [training: 2.9570515178725274 | validation: 3.1997082897564377]
	TIME [epoch: 9.73 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9460742821407777		[learning rate: 2.6131e-05]
	Learning Rate: 2.61311e-05
	LOSS [training: 2.9460742821407777 | validation: 3.2026413356783348]
	TIME [epoch: 9.71 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9523873606929163		[learning rate: 2.6036e-05]
	Learning Rate: 2.60363e-05
	LOSS [training: 2.9523873606929163 | validation: 3.2050429971501098]
	TIME [epoch: 9.71 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9475815915852612		[learning rate: 2.5942e-05]
	Learning Rate: 2.59418e-05
	LOSS [training: 2.9475815915852612 | validation: 3.1990247514520322]
	TIME [epoch: 9.74 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948296573895143		[learning rate: 2.5848e-05]
	Learning Rate: 2.58477e-05
	LOSS [training: 2.948296573895143 | validation: 3.204639045010998]
	TIME [epoch: 9.71 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949688535568916		[learning rate: 2.5754e-05]
	Learning Rate: 2.57539e-05
	LOSS [training: 2.949688535568916 | validation: 3.186387901628251]
	TIME [epoch: 9.71 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9519250390310514		[learning rate: 2.566e-05]
	Learning Rate: 2.56604e-05
	LOSS [training: 2.9519250390310514 | validation: 3.1956671647183352]
	TIME [epoch: 9.74 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954940907890331		[learning rate: 2.5567e-05]
	Learning Rate: 2.55673e-05
	LOSS [training: 2.954940907890331 | validation: 3.201022341724146]
	TIME [epoch: 9.71 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9485596447864486		[learning rate: 2.5474e-05]
	Learning Rate: 2.54745e-05
	LOSS [training: 2.9485596447864486 | validation: 3.192326037555281]
	TIME [epoch: 9.7 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950448265181543		[learning rate: 2.5382e-05]
	Learning Rate: 2.5382e-05
	LOSS [training: 2.950448265181543 | validation: 3.2007312624349766]
	TIME [epoch: 9.74 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949182511378499		[learning rate: 2.529e-05]
	Learning Rate: 2.52899e-05
	LOSS [training: 2.949182511378499 | validation: 3.199609414783788]
	TIME [epoch: 9.71 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955332654810289		[learning rate: 2.5198e-05]
	Learning Rate: 2.51981e-05
	LOSS [training: 2.955332654810289 | validation: 3.1910809984945345]
	TIME [epoch: 9.7 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.943603950926443		[learning rate: 2.5107e-05]
	Learning Rate: 2.51067e-05
	LOSS [training: 2.943603950926443 | validation: 3.202806415337309]
	TIME [epoch: 9.73 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953485407047628		[learning rate: 2.5016e-05]
	Learning Rate: 2.50156e-05
	LOSS [training: 2.953485407047628 | validation: 3.190124512688687]
	TIME [epoch: 9.71 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9495202174664366		[learning rate: 2.4925e-05]
	Learning Rate: 2.49248e-05
	LOSS [training: 2.9495202174664366 | validation: 3.196452427046444]
	TIME [epoch: 9.71 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9503628486086635		[learning rate: 2.4834e-05]
	Learning Rate: 2.48343e-05
	LOSS [training: 2.9503628486086635 | validation: 3.191318129225205]
	TIME [epoch: 9.73 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9525727853439947		[learning rate: 2.4744e-05]
	Learning Rate: 2.47442e-05
	LOSS [training: 2.9525727853439947 | validation: 3.2020749984389796]
	TIME [epoch: 9.71 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953694281187977		[learning rate: 2.4654e-05]
	Learning Rate: 2.46544e-05
	LOSS [training: 2.953694281187977 | validation: 3.1933026342684077]
	TIME [epoch: 9.71 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9504033831961225		[learning rate: 2.4565e-05]
	Learning Rate: 2.45649e-05
	LOSS [training: 2.9504033831961225 | validation: 3.208700861304898]
	TIME [epoch: 9.72 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9483474084508203		[learning rate: 2.4476e-05]
	Learning Rate: 2.44758e-05
	LOSS [training: 2.9483474084508203 | validation: 3.2033652656099196]
	TIME [epoch: 9.71 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9476129939772435		[learning rate: 2.4387e-05]
	Learning Rate: 2.4387e-05
	LOSS [training: 2.9476129939772435 | validation: 3.186866071758381]
	TIME [epoch: 9.71 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9481239841834475		[learning rate: 2.4298e-05]
	Learning Rate: 2.42985e-05
	LOSS [training: 2.9481239841834475 | validation: 3.1973953780741518]
	TIME [epoch: 9.73 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9475059101841388		[learning rate: 2.421e-05]
	Learning Rate: 2.42103e-05
	LOSS [training: 2.9475059101841388 | validation: 3.194145445797711]
	TIME [epoch: 9.71 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.944406021235868		[learning rate: 2.4122e-05]
	Learning Rate: 2.41224e-05
	LOSS [training: 2.944406021235868 | validation: 3.1926343164522315]
	TIME [epoch: 9.71 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9516040200861555		[learning rate: 2.4035e-05]
	Learning Rate: 2.40349e-05
	LOSS [training: 2.9516040200861555 | validation: 3.201502554685621]
	TIME [epoch: 9.73 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.944702548580614		[learning rate: 2.3948e-05]
	Learning Rate: 2.39477e-05
	LOSS [training: 2.944702548580614 | validation: 3.1915604695910718]
	TIME [epoch: 9.71 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9513162171361484		[learning rate: 2.3861e-05]
	Learning Rate: 2.38608e-05
	LOSS [training: 2.9513162171361484 | validation: 3.1956728914429444]
	TIME [epoch: 9.71 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949863589222697		[learning rate: 2.3774e-05]
	Learning Rate: 2.37742e-05
	LOSS [training: 2.949863589222697 | validation: 3.195792819391278]
	TIME [epoch: 9.72 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9449717679864666		[learning rate: 2.3688e-05]
	Learning Rate: 2.36879e-05
	LOSS [training: 2.9449717679864666 | validation: 3.208502727770765]
	TIME [epoch: 9.72 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949948247545452		[learning rate: 2.3602e-05]
	Learning Rate: 2.36019e-05
	LOSS [training: 2.949948247545452 | validation: 3.202956655047048]
	TIME [epoch: 9.7 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9485445595933877		[learning rate: 2.3516e-05]
	Learning Rate: 2.35163e-05
	LOSS [training: 2.9485445595933877 | validation: 3.197618048892822]
	TIME [epoch: 9.71 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9481177417940216		[learning rate: 2.3431e-05]
	Learning Rate: 2.34309e-05
	LOSS [training: 2.9481177417940216 | validation: 3.1898764349137894]
	TIME [epoch: 9.72 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9472289596566967		[learning rate: 2.3346e-05]
	Learning Rate: 2.33459e-05
	LOSS [training: 2.9472289596566967 | validation: 3.1913842047403933]
	TIME [epoch: 9.71 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.943966367368082		[learning rate: 2.3261e-05]
	Learning Rate: 2.32612e-05
	LOSS [training: 2.943966367368082 | validation: 3.190524990684811]
	TIME [epoch: 9.7 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9538034283412116		[learning rate: 2.3177e-05]
	Learning Rate: 2.31768e-05
	LOSS [training: 2.9538034283412116 | validation: 3.202689118928165]
	TIME [epoch: 9.73 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9465923332776254		[learning rate: 2.3093e-05]
	Learning Rate: 2.30926e-05
	LOSS [training: 2.9465923332776254 | validation: 3.201074840752542]
	TIME [epoch: 9.71 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.946505629861531		[learning rate: 2.3009e-05]
	Learning Rate: 2.30088e-05
	LOSS [training: 2.946505629861531 | validation: 3.1867523995946194]
	TIME [epoch: 9.71 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952032227783005		[learning rate: 2.2925e-05]
	Learning Rate: 2.29253e-05
	LOSS [training: 2.952032227783005 | validation: 3.1963799963136696]
	TIME [epoch: 9.73 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950216093833638		[learning rate: 2.2842e-05]
	Learning Rate: 2.28421e-05
	LOSS [training: 2.950216093833638 | validation: 3.1863292630434814]
	TIME [epoch: 9.71 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957599278828492		[learning rate: 2.2759e-05]
	Learning Rate: 2.27592e-05
	LOSS [training: 2.957599278828492 | validation: 3.195869359280652]
	TIME [epoch: 9.7 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.94731751215761		[learning rate: 2.2677e-05]
	Learning Rate: 2.26767e-05
	LOSS [training: 2.94731751215761 | validation: 3.2001235986306518]
	TIME [epoch: 9.73 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951614024754204		[learning rate: 2.2594e-05]
	Learning Rate: 2.25944e-05
	LOSS [training: 2.951614024754204 | validation: 3.194490182402055]
	TIME [epoch: 9.71 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9536786653821436		[learning rate: 2.2512e-05]
	Learning Rate: 2.25124e-05
	LOSS [training: 2.9536786653821436 | validation: 3.200202650814016]
	TIME [epoch: 9.71 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952857001876315		[learning rate: 2.2431e-05]
	Learning Rate: 2.24307e-05
	LOSS [training: 2.952857001876315 | validation: 3.1981874477824364]
	TIME [epoch: 9.73 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9501842680218933		[learning rate: 2.2349e-05]
	Learning Rate: 2.23493e-05
	LOSS [training: 2.9501842680218933 | validation: 3.1972980681929393]
	TIME [epoch: 9.71 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951601277773448		[learning rate: 2.2268e-05]
	Learning Rate: 2.22682e-05
	LOSS [training: 2.951601277773448 | validation: 3.193652699793328]
	TIME [epoch: 9.71 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.946686890040705		[learning rate: 2.2187e-05]
	Learning Rate: 2.21873e-05
	LOSS [training: 2.946686890040705 | validation: 3.1914467921268272]
	TIME [epoch: 9.72 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949880220831973		[learning rate: 2.2107e-05]
	Learning Rate: 2.21068e-05
	LOSS [training: 2.949880220831973 | validation: 3.1984596136140464]
	TIME [epoch: 9.7 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9441366209130226		[learning rate: 2.2027e-05]
	Learning Rate: 2.20266e-05
	LOSS [training: 2.9441366209130226 | validation: 3.1956767473544483]
	TIME [epoch: 9.71 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.94901989752633		[learning rate: 2.1947e-05]
	Learning Rate: 2.19467e-05
	LOSS [training: 2.94901989752633 | validation: 3.2103971710982537]
	TIME [epoch: 9.72 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.944605753026074		[learning rate: 2.1867e-05]
	Learning Rate: 2.1867e-05
	LOSS [training: 2.944605753026074 | validation: 3.202384045641945]
	TIME [epoch: 9.71 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953900139314186		[learning rate: 2.1788e-05]
	Learning Rate: 2.17877e-05
	LOSS [training: 2.953900139314186 | validation: 3.202146953149454]
	TIME [epoch: 9.7 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9523147796829634		[learning rate: 2.1709e-05]
	Learning Rate: 2.17086e-05
	LOSS [training: 2.9523147796829634 | validation: 3.198297028544425]
	TIME [epoch: 9.72 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9497929979472484		[learning rate: 2.163e-05]
	Learning Rate: 2.16298e-05
	LOSS [training: 2.9497929979472484 | validation: 3.1950312271211248]
	TIME [epoch: 9.71 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952060993460444		[learning rate: 2.1551e-05]
	Learning Rate: 2.15513e-05
	LOSS [training: 2.952060993460444 | validation: 3.1987004264695056]
	TIME [epoch: 9.71 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.94929159907637		[learning rate: 2.1473e-05]
	Learning Rate: 2.14731e-05
	LOSS [training: 2.94929159907637 | validation: 3.188036995693817]
	TIME [epoch: 9.72 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950432372358124		[learning rate: 2.1395e-05]
	Learning Rate: 2.13952e-05
	LOSS [training: 2.950432372358124 | validation: 3.2079609772289075]
	TIME [epoch: 9.71 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9467991124185042		[learning rate: 2.1318e-05]
	Learning Rate: 2.13175e-05
	LOSS [training: 2.9467991124185042 | validation: 3.211288410294332]
	TIME [epoch: 9.71 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9463943235831733		[learning rate: 2.124e-05]
	Learning Rate: 2.12402e-05
	LOSS [training: 2.9463943235831733 | validation: 3.200959978437839]
	TIME [epoch: 9.71 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95223039876562		[learning rate: 2.1163e-05]
	Learning Rate: 2.11631e-05
	LOSS [training: 2.95223039876562 | validation: 3.1991835582890396]
	TIME [epoch: 9.72 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9515728092545244		[learning rate: 2.1086e-05]
	Learning Rate: 2.10863e-05
	LOSS [training: 2.9515728092545244 | validation: 3.203346071949768]
	TIME [epoch: 9.71 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950611496457023		[learning rate: 2.101e-05]
	Learning Rate: 2.10098e-05
	LOSS [training: 2.950611496457023 | validation: 3.1941341121999467]
	TIME [epoch: 9.71 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.946564300874819		[learning rate: 2.0934e-05]
	Learning Rate: 2.09335e-05
	LOSS [training: 2.946564300874819 | validation: 3.2078541302536783]
	TIME [epoch: 9.72 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951190941777937		[learning rate: 2.0858e-05]
	Learning Rate: 2.08575e-05
	LOSS [training: 2.951190941777937 | validation: 3.2011705896939553]
	TIME [epoch: 9.7 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948020329323519		[learning rate: 2.0782e-05]
	Learning Rate: 2.07819e-05
	LOSS [training: 2.948020329323519 | validation: 3.2059895910497596]
	TIME [epoch: 9.7 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9458413556941165		[learning rate: 2.0706e-05]
	Learning Rate: 2.07064e-05
	LOSS [training: 2.9458413556941165 | validation: 3.2007564843532315]
	TIME [epoch: 9.73 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9445564832091566		[learning rate: 2.0631e-05]
	Learning Rate: 2.06313e-05
	LOSS [training: 2.9445564832091566 | validation: 3.197628738340551]
	TIME [epoch: 9.7 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9487923984202604		[learning rate: 2.0556e-05]
	Learning Rate: 2.05564e-05
	LOSS [training: 2.9487923984202604 | validation: 3.191093321080974]
	TIME [epoch: 9.71 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9522958651921343		[learning rate: 2.0482e-05]
	Learning Rate: 2.04818e-05
	LOSS [training: 2.9522958651921343 | validation: 3.2026061729643858]
	TIME [epoch: 9.73 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951870793103896		[learning rate: 2.0407e-05]
	Learning Rate: 2.04075e-05
	LOSS [training: 2.951870793103896 | validation: 3.200800639065676]
	TIME [epoch: 9.71 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.94974340428178		[learning rate: 2.0333e-05]
	Learning Rate: 2.03334e-05
	LOSS [training: 2.94974340428178 | validation: 3.1931699369202904]
	TIME [epoch: 9.7 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9535539586364887		[learning rate: 2.026e-05]
	Learning Rate: 2.02596e-05
	LOSS [training: 2.9535539586364887 | validation: 3.1984074914905762]
	TIME [epoch: 9.73 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949779234462898		[learning rate: 2.0186e-05]
	Learning Rate: 2.01861e-05
	LOSS [training: 2.949779234462898 | validation: 3.1969570450449756]
	TIME [epoch: 9.7 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9525423372740893		[learning rate: 2.0113e-05]
	Learning Rate: 2.01129e-05
	LOSS [training: 2.9525423372740893 | validation: 3.196450289819562]
	TIME [epoch: 9.7 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9487564499154404		[learning rate: 2.004e-05]
	Learning Rate: 2.00399e-05
	LOSS [training: 2.9487564499154404 | validation: 3.192851651162298]
	TIME [epoch: 9.73 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.946507638164495		[learning rate: 1.9967e-05]
	Learning Rate: 1.99671e-05
	LOSS [training: 2.946507638164495 | validation: 3.2044633328519514]
	TIME [epoch: 9.7 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.946831074463442		[learning rate: 1.9895e-05]
	Learning Rate: 1.98947e-05
	LOSS [training: 2.946831074463442 | validation: 3.211020683468056]
	TIME [epoch: 9.7 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9520742729926637		[learning rate: 1.9822e-05]
	Learning Rate: 1.98225e-05
	LOSS [training: 2.9520742729926637 | validation: 3.205087321421239]
	TIME [epoch: 9.72 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9504921618439837		[learning rate: 1.9751e-05]
	Learning Rate: 1.97505e-05
	LOSS [training: 2.9504921618439837 | validation: 3.1947279323332753]
	TIME [epoch: 9.71 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9476218991278254		[learning rate: 1.9679e-05]
	Learning Rate: 1.96789e-05
	LOSS [training: 2.9476218991278254 | validation: 3.1978071825175785]
	TIME [epoch: 9.71 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954693182643415		[learning rate: 1.9607e-05]
	Learning Rate: 1.96074e-05
	LOSS [training: 2.954693182643415 | validation: 3.1983953573905275]
	TIME [epoch: 9.72 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948275096577569		[learning rate: 1.9536e-05]
	Learning Rate: 1.95363e-05
	LOSS [training: 2.948275096577569 | validation: 3.188246554942318]
	TIME [epoch: 9.71 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951010907304933		[learning rate: 1.9465e-05]
	Learning Rate: 1.94654e-05
	LOSS [training: 2.951010907304933 | validation: 3.198170606391794]
	TIME [epoch: 9.7 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952406190155932		[learning rate: 1.9395e-05]
	Learning Rate: 1.93948e-05
	LOSS [training: 2.952406190155932 | validation: 3.1954226276948745]
	TIME [epoch: 9.71 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95323897446021		[learning rate: 1.9324e-05]
	Learning Rate: 1.93244e-05
	LOSS [training: 2.95323897446021 | validation: 3.1990743232545786]
	TIME [epoch: 9.71 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951359360142473		[learning rate: 1.9254e-05]
	Learning Rate: 1.92542e-05
	LOSS [training: 2.951359360142473 | validation: 3.2002176537608955]
	TIME [epoch: 9.71 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9468419290967818		[learning rate: 1.9184e-05]
	Learning Rate: 1.91844e-05
	LOSS [training: 2.9468419290967818 | validation: 3.200075139268096]
	TIME [epoch: 9.72 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9526720107834126		[learning rate: 1.9115e-05]
	Learning Rate: 1.91147e-05
	LOSS [training: 2.9526720107834126 | validation: 3.204183064573502]
	TIME [epoch: 9.71 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9516360802190382		[learning rate: 1.9045e-05]
	Learning Rate: 1.90454e-05
	LOSS [training: 2.9516360802190382 | validation: 3.2045943810169457]
	TIME [epoch: 9.7 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.943269720639287		[learning rate: 1.8976e-05]
	Learning Rate: 1.89763e-05
	LOSS [training: 2.943269720639287 | validation: 3.1953707459830354]
	TIME [epoch: 9.72 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950090906132788		[learning rate: 1.8907e-05]
	Learning Rate: 1.89074e-05
	LOSS [training: 2.950090906132788 | validation: 3.2034151810244786]
	TIME [epoch: 9.71 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9485888053856164		[learning rate: 1.8839e-05]
	Learning Rate: 1.88388e-05
	LOSS [training: 2.9485888053856164 | validation: 3.201439565104754]
	TIME [epoch: 9.7 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952367765404541		[learning rate: 1.877e-05]
	Learning Rate: 1.87704e-05
	LOSS [training: 2.952367765404541 | validation: 3.1947354855158823]
	TIME [epoch: 9.71 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9509735029109914		[learning rate: 1.8702e-05]
	Learning Rate: 1.87023e-05
	LOSS [training: 2.9509735029109914 | validation: 3.204371996156805]
	TIME [epoch: 9.73 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949224097938173		[learning rate: 1.8634e-05]
	Learning Rate: 1.86344e-05
	LOSS [training: 2.949224097938173 | validation: 3.199293041349599]
	TIME [epoch: 9.7 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9562509130726147		[learning rate: 1.8567e-05]
	Learning Rate: 1.85668e-05
	LOSS [training: 2.9562509130726147 | validation: 3.197972364815959]
	TIME [epoch: 9.71 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949390075366501		[learning rate: 1.8499e-05]
	Learning Rate: 1.84994e-05
	LOSS [training: 2.949390075366501 | validation: 3.19590984818063]
	TIME [epoch: 9.72 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949511483738227		[learning rate: 1.8432e-05]
	Learning Rate: 1.84323e-05
	LOSS [training: 2.949511483738227 | validation: 3.1880754971588834]
	TIME [epoch: 9.7 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950919214160749		[learning rate: 1.8365e-05]
	Learning Rate: 1.83654e-05
	LOSS [training: 2.950919214160749 | validation: 3.202806829394059]
	TIME [epoch: 9.71 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953180828580713		[learning rate: 1.8299e-05]
	Learning Rate: 1.82987e-05
	LOSS [training: 2.953180828580713 | validation: 3.1926277031704924]
	TIME [epoch: 9.72 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9551494339152233		[learning rate: 1.8232e-05]
	Learning Rate: 1.82323e-05
	LOSS [training: 2.9551494339152233 | validation: 3.1961475911665334]
	TIME [epoch: 9.7 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9468015303573267		[learning rate: 1.8166e-05]
	Learning Rate: 1.81662e-05
	LOSS [training: 2.9468015303573267 | validation: 3.2037566758258915]
	TIME [epoch: 9.71 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952074914844142		[learning rate: 1.81e-05]
	Learning Rate: 1.81002e-05
	LOSS [training: 2.952074914844142 | validation: 3.2036176695706975]
	TIME [epoch: 9.72 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9543673367225485		[learning rate: 1.8035e-05]
	Learning Rate: 1.80346e-05
	LOSS [training: 2.9543673367225485 | validation: 3.1982470890274217]
	TIME [epoch: 9.71 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949249650344282		[learning rate: 1.7969e-05]
	Learning Rate: 1.79691e-05
	LOSS [training: 2.949249650344282 | validation: 3.205935521281998]
	TIME [epoch: 9.71 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9458766606249864		[learning rate: 1.7904e-05]
	Learning Rate: 1.79039e-05
	LOSS [training: 2.9458766606249864 | validation: 3.200199638491945]
	TIME [epoch: 9.73 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9477343478175553		[learning rate: 1.7839e-05]
	Learning Rate: 1.78389e-05
	LOSS [training: 2.9477343478175553 | validation: 3.2029997636667122]
	TIME [epoch: 9.71 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9481601436769798		[learning rate: 1.7774e-05]
	Learning Rate: 1.77742e-05
	LOSS [training: 2.9481601436769798 | validation: 3.1978938892770534]
	TIME [epoch: 9.71 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953236594684992		[learning rate: 1.771e-05]
	Learning Rate: 1.77097e-05
	LOSS [training: 2.953236594684992 | validation: 3.199701687284047]
	TIME [epoch: 9.72 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953216355738232		[learning rate: 1.7645e-05]
	Learning Rate: 1.76454e-05
	LOSS [training: 2.953216355738232 | validation: 3.2052440366962998]
	TIME [epoch: 9.7 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.942422796240179		[learning rate: 1.7581e-05]
	Learning Rate: 1.75814e-05
	LOSS [training: 2.942422796240179 | validation: 3.1954813082043234]
	TIME [epoch: 9.71 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9486505759529544		[learning rate: 1.7518e-05]
	Learning Rate: 1.75176e-05
	LOSS [training: 2.9486505759529544 | validation: 3.2055143931028205]
	TIME [epoch: 9.72 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9491892561760777		[learning rate: 1.7454e-05]
	Learning Rate: 1.7454e-05
	LOSS [training: 2.9491892561760777 | validation: 3.191254556204943]
	TIME [epoch: 9.71 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9476314905137118		[learning rate: 1.7391e-05]
	Learning Rate: 1.73907e-05
	LOSS [training: 2.9476314905137118 | validation: 3.2054508645233137]
	TIME [epoch: 9.71 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9553175042106843		[learning rate: 1.7328e-05]
	Learning Rate: 1.73275e-05
	LOSS [training: 2.9553175042106843 | validation: 3.192582832087642]
	TIME [epoch: 9.72 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9488265954166644		[learning rate: 1.7265e-05]
	Learning Rate: 1.72647e-05
	LOSS [training: 2.9488265954166644 | validation: 3.1985646555233234]
	TIME [epoch: 9.71 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.946797950558118		[learning rate: 1.7202e-05]
	Learning Rate: 1.7202e-05
	LOSS [training: 2.946797950558118 | validation: 3.193457380061143]
	TIME [epoch: 9.71 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9511871700080983		[learning rate: 1.714e-05]
	Learning Rate: 1.71396e-05
	LOSS [training: 2.9511871700080983 | validation: 3.1966913607220837]
	TIME [epoch: 9.73 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9495124780289568		[learning rate: 1.7077e-05]
	Learning Rate: 1.70774e-05
	LOSS [training: 2.9495124780289568 | validation: 3.200121452281258]
	TIME [epoch: 9.71 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9478329982180766		[learning rate: 1.7015e-05]
	Learning Rate: 1.70154e-05
	LOSS [training: 2.9478329982180766 | validation: 3.1921598174577883]
	TIME [epoch: 9.71 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9515729316530126		[learning rate: 1.6954e-05]
	Learning Rate: 1.69537e-05
	LOSS [training: 2.9515729316530126 | validation: 3.2017109085098645]
	TIME [epoch: 9.72 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9536057691160615		[learning rate: 1.6892e-05]
	Learning Rate: 1.68921e-05
	LOSS [training: 2.9536057691160615 | validation: 3.2104732962759863]
	TIME [epoch: 9.71 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9496055387097173		[learning rate: 1.6831e-05]
	Learning Rate: 1.68308e-05
	LOSS [training: 2.9496055387097173 | validation: 3.1886891352560918]
	TIME [epoch: 9.7 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9524734120663254		[learning rate: 1.677e-05]
	Learning Rate: 1.67697e-05
	LOSS [training: 2.9524734120663254 | validation: 3.2003148367439542]
	TIME [epoch: 9.72 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9491224497544715		[learning rate: 1.6709e-05]
	Learning Rate: 1.67089e-05
	LOSS [training: 2.9491224497544715 | validation: 3.194368310496373]
	TIME [epoch: 9.71 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9483967374204294		[learning rate: 1.6648e-05]
	Learning Rate: 1.66482e-05
	LOSS [training: 2.9483967374204294 | validation: 3.1920209210466637]
	TIME [epoch: 9.7 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9499091581827064		[learning rate: 1.6588e-05]
	Learning Rate: 1.65878e-05
	LOSS [training: 2.9499091581827064 | validation: 3.207263522584281]
	TIME [epoch: 9.72 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9523009383828347		[learning rate: 1.6528e-05]
	Learning Rate: 1.65276e-05
	LOSS [training: 2.9523009383828347 | validation: 3.192518587034416]
	TIME [epoch: 9.72 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.944691209987927		[learning rate: 1.6468e-05]
	Learning Rate: 1.64677e-05
	LOSS [training: 2.944691209987927 | validation: 3.191021418124127]
	TIME [epoch: 9.71 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951586129750996		[learning rate: 1.6408e-05]
	Learning Rate: 1.64079e-05
	LOSS [training: 2.951586129750996 | validation: 3.197823464897613]
	TIME [epoch: 9.72 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949823787925582		[learning rate: 1.6348e-05]
	Learning Rate: 1.63483e-05
	LOSS [training: 2.949823787925582 | validation: 3.1903320887686992]
	TIME [epoch: 9.72 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9475341311235375		[learning rate: 1.6289e-05]
	Learning Rate: 1.6289e-05
	LOSS [training: 2.9475341311235375 | validation: 3.197335892530174]
	TIME [epoch: 9.7 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9480230765370576		[learning rate: 1.623e-05]
	Learning Rate: 1.62299e-05
	LOSS [training: 2.9480230765370576 | validation: 3.196307840464554]
	TIME [epoch: 9.71 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951171952724329		[learning rate: 1.6171e-05]
	Learning Rate: 1.6171e-05
	LOSS [training: 2.951171952724329 | validation: 3.1961911405159147]
	TIME [epoch: 9.73 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9471192262198915		[learning rate: 1.6112e-05]
	Learning Rate: 1.61123e-05
	LOSS [training: 2.9471192262198915 | validation: 3.2149329441645027]
	TIME [epoch: 9.71 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9527832660045186		[learning rate: 1.6054e-05]
	Learning Rate: 1.60538e-05
	LOSS [training: 2.9527832660045186 | validation: 3.1995606621795436]
	TIME [epoch: 9.7 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952378730543476		[learning rate: 1.5996e-05]
	Learning Rate: 1.59956e-05
	LOSS [training: 2.952378730543476 | validation: 3.1875534991459595]
	TIME [epoch: 9.73 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9514029224733918		[learning rate: 1.5938e-05]
	Learning Rate: 1.59375e-05
	LOSS [training: 2.9514029224733918 | validation: 3.199531643101382]
	TIME [epoch: 9.71 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9526393842422465		[learning rate: 1.588e-05]
	Learning Rate: 1.58797e-05
	LOSS [training: 2.9526393842422465 | validation: 3.2155179067931456]
	TIME [epoch: 9.71 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.946011294704182		[learning rate: 1.5822e-05]
	Learning Rate: 1.58221e-05
	LOSS [training: 2.946011294704182 | validation: 3.1948345973033634]
	TIME [epoch: 9.73 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949786812248994		[learning rate: 1.5765e-05]
	Learning Rate: 1.57647e-05
	LOSS [training: 2.949786812248994 | validation: 3.203041517222766]
	TIME [epoch: 9.71 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950998684765706		[learning rate: 1.5707e-05]
	Learning Rate: 1.57074e-05
	LOSS [training: 2.950998684765706 | validation: 3.2005825711267915]
	TIME [epoch: 9.71 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951248605400779		[learning rate: 1.565e-05]
	Learning Rate: 1.56504e-05
	LOSS [training: 2.951248605400779 | validation: 3.1939672800640864]
	TIME [epoch: 9.73 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.943101828694852		[learning rate: 1.5594e-05]
	Learning Rate: 1.55936e-05
	LOSS [training: 2.943101828694852 | validation: 3.1987137481059444]
	TIME [epoch: 9.7 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9493418015149104		[learning rate: 1.5537e-05]
	Learning Rate: 1.5537e-05
	LOSS [training: 2.9493418015149104 | validation: 3.194061656978005]
	TIME [epoch: 9.7 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9549231293698712		[learning rate: 1.5481e-05]
	Learning Rate: 1.54807e-05
	LOSS [training: 2.9549231293698712 | validation: 3.204014017004987]
	TIME [epoch: 9.73 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9481386727272696		[learning rate: 1.5424e-05]
	Learning Rate: 1.54245e-05
	LOSS [training: 2.9481386727272696 | validation: 3.205004221600282]
	TIME [epoch: 9.71 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9492832061455228		[learning rate: 1.5369e-05]
	Learning Rate: 1.53685e-05
	LOSS [training: 2.9492832061455228 | validation: 3.202934548244838]
	TIME [epoch: 9.7 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948454029796275		[learning rate: 1.5313e-05]
	Learning Rate: 1.53127e-05
	LOSS [training: 2.948454029796275 | validation: 3.2039088370048776]
	TIME [epoch: 9.72 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9426630856518887		[learning rate: 1.5257e-05]
	Learning Rate: 1.52572e-05
	LOSS [training: 2.9426630856518887 | validation: 3.2003852700600577]
	TIME [epoch: 9.71 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.947616466891327		[learning rate: 1.5202e-05]
	Learning Rate: 1.52018e-05
	LOSS [training: 2.947616466891327 | validation: 3.2027497354554244]
	TIME [epoch: 9.71 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9501567036790126		[learning rate: 1.5147e-05]
	Learning Rate: 1.51466e-05
	LOSS [training: 2.9501567036790126 | validation: 3.190602306143454]
	TIME [epoch: 9.72 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9464857954310055		[learning rate: 1.5092e-05]
	Learning Rate: 1.50917e-05
	LOSS [training: 2.9464857954310055 | validation: 3.200759322250814]
	TIME [epoch: 9.71 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9528555427948744		[learning rate: 1.5037e-05]
	Learning Rate: 1.50369e-05
	LOSS [training: 2.9528555427948744 | validation: 3.213129181879168]
	TIME [epoch: 9.7 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9444765295576696		[learning rate: 1.4982e-05]
	Learning Rate: 1.49823e-05
	LOSS [training: 2.9444765295576696 | validation: 3.1891663710702693]
	TIME [epoch: 9.73 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9541496296614573		[learning rate: 1.4928e-05]
	Learning Rate: 1.49279e-05
	LOSS [training: 2.9541496296614573 | validation: 3.1942283441496575]
	TIME [epoch: 9.71 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951738047384403		[learning rate: 1.4874e-05]
	Learning Rate: 1.48738e-05
	LOSS [training: 2.951738047384403 | validation: 3.2010905583357907]
	TIME [epoch: 9.71 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9488249020388078		[learning rate: 1.482e-05]
	Learning Rate: 1.48198e-05
	LOSS [training: 2.9488249020388078 | validation: 3.2017088750208638]
	TIME [epoch: 9.71 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952089899765855		[learning rate: 1.4766e-05]
	Learning Rate: 1.4766e-05
	LOSS [training: 2.952089899765855 | validation: 3.1947207441471934]
	TIME [epoch: 9.72 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9481662882273483		[learning rate: 1.4712e-05]
	Learning Rate: 1.47124e-05
	LOSS [training: 2.9481662882273483 | validation: 3.2026864837886984]
	TIME [epoch: 9.71 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952596772451504		[learning rate: 1.4659e-05]
	Learning Rate: 1.4659e-05
	LOSS [training: 2.952596772451504 | validation: 3.1981853006752603]
	TIME [epoch: 9.72 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9500521225153533		[learning rate: 1.4606e-05]
	Learning Rate: 1.46058e-05
	LOSS [training: 2.9500521225153533 | validation: 3.2004571773830377]
	TIME [epoch: 9.72 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9504101735916586		[learning rate: 1.4553e-05]
	Learning Rate: 1.45528e-05
	LOSS [training: 2.9504101735916586 | validation: 3.1961718320876082]
	TIME [epoch: 9.71 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.947621658153674		[learning rate: 1.45e-05]
	Learning Rate: 1.45e-05
	LOSS [training: 2.947621658153674 | validation: 3.2064472185427157]
	TIME [epoch: 9.7 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948200726015359		[learning rate: 1.4447e-05]
	Learning Rate: 1.44474e-05
	LOSS [training: 2.948200726015359 | validation: 3.197530085455476]
	TIME [epoch: 9.73 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.946877893904713		[learning rate: 1.4395e-05]
	Learning Rate: 1.4395e-05
	LOSS [training: 2.946877893904713 | validation: 3.1925639036970246]
	TIME [epoch: 9.71 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.944183875106237		[learning rate: 1.4343e-05]
	Learning Rate: 1.43427e-05
	LOSS [training: 2.944183875106237 | validation: 3.1951072120644106]
	TIME [epoch: 9.71 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950215074637497		[learning rate: 1.4291e-05]
	Learning Rate: 1.42907e-05
	LOSS [training: 2.950215074637497 | validation: 3.205690455846831]
	TIME [epoch: 9.73 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9497669315273116		[learning rate: 1.4239e-05]
	Learning Rate: 1.42388e-05
	LOSS [training: 2.9497669315273116 | validation: 3.189058296539773]
	TIME [epoch: 9.71 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955036259413549		[learning rate: 1.4187e-05]
	Learning Rate: 1.41871e-05
	LOSS [training: 2.955036259413549 | validation: 3.1946130469727643]
	TIME [epoch: 9.71 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9484747481980063		[learning rate: 1.4136e-05]
	Learning Rate: 1.41357e-05
	LOSS [training: 2.9484747481980063 | validation: 3.1920692399559427]
	TIME [epoch: 9.73 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9511644804926087		[learning rate: 1.4084e-05]
	Learning Rate: 1.40844e-05
	LOSS [training: 2.9511644804926087 | validation: 3.19732632820528]
	TIME [epoch: 9.71 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9519952139431758		[learning rate: 1.4033e-05]
	Learning Rate: 1.40332e-05
	LOSS [training: 2.9519952139431758 | validation: 3.1960599474483775]
	TIME [epoch: 9.71 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.945066630385888		[learning rate: 1.3982e-05]
	Learning Rate: 1.39823e-05
	LOSS [training: 2.945066630385888 | validation: 3.19638847094149]
	TIME [epoch: 9.73 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.946484545658003		[learning rate: 1.3932e-05]
	Learning Rate: 1.39316e-05
	LOSS [training: 2.946484545658003 | validation: 3.199682425852484]
	TIME [epoch: 9.71 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9473225651868304		[learning rate: 1.3881e-05]
	Learning Rate: 1.3881e-05
	LOSS [training: 2.9473225651868304 | validation: 3.19471331626858]
	TIME [epoch: 9.71 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9480028371338918		[learning rate: 1.3831e-05]
	Learning Rate: 1.38306e-05
	LOSS [training: 2.9480028371338918 | validation: 3.200624296194086]
	TIME [epoch: 9.73 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951255583844802		[learning rate: 1.378e-05]
	Learning Rate: 1.37804e-05
	LOSS [training: 2.951255583844802 | validation: 3.1987528727331345]
	TIME [epoch: 9.71 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.94462174358871		[learning rate: 1.373e-05]
	Learning Rate: 1.37304e-05
	LOSS [training: 2.94462174358871 | validation: 3.1894508829032167]
	TIME [epoch: 9.71 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9587582706404794		[learning rate: 1.3681e-05]
	Learning Rate: 1.36806e-05
	LOSS [training: 2.9587582706404794 | validation: 3.2066785482392355]
	TIME [epoch: 9.73 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951411238169606		[learning rate: 1.3631e-05]
	Learning Rate: 1.3631e-05
	LOSS [training: 2.951411238169606 | validation: 3.189785070746566]
	TIME [epoch: 9.71 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9504755105410574		[learning rate: 1.3581e-05]
	Learning Rate: 1.35815e-05
	LOSS [training: 2.9504755105410574 | validation: 3.1971624627222632]
	TIME [epoch: 9.7 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.947751223699845		[learning rate: 1.3532e-05]
	Learning Rate: 1.35322e-05
	LOSS [training: 2.947751223699845 | validation: 3.203648486241716]
	TIME [epoch: 9.72 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.94705588260727		[learning rate: 1.3483e-05]
	Learning Rate: 1.34831e-05
	LOSS [training: 2.94705588260727 | validation: 3.18979939847927]
	TIME [epoch: 9.71 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9549612317476095		[learning rate: 1.3434e-05]
	Learning Rate: 1.34342e-05
	LOSS [training: 2.9549612317476095 | validation: 3.204019547013285]
	TIME [epoch: 9.69 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.947070212169341		[learning rate: 1.3385e-05]
	Learning Rate: 1.33854e-05
	LOSS [training: 2.947070212169341 | validation: 3.1980659809522014]
	TIME [epoch: 9.71 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9541279786944155		[learning rate: 1.3337e-05]
	Learning Rate: 1.33368e-05
	LOSS [training: 2.9541279786944155 | validation: 3.197490967979139]
	TIME [epoch: 9.7 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952189325443186		[learning rate: 1.3288e-05]
	Learning Rate: 1.32884e-05
	LOSS [training: 2.952189325443186 | validation: 3.206551692837627]
	TIME [epoch: 9.7 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9476121063051557		[learning rate: 1.324e-05]
	Learning Rate: 1.32402e-05
	LOSS [training: 2.9476121063051557 | validation: 3.197428402849106]
	TIME [epoch: 9.71 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954665882892042		[learning rate: 1.3192e-05]
	Learning Rate: 1.31922e-05
	LOSS [training: 2.954665882892042 | validation: 3.206480392900068]
	TIME [epoch: 9.7 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9515764960796433		[learning rate: 1.3144e-05]
	Learning Rate: 1.31443e-05
	LOSS [training: 2.9515764960796433 | validation: 3.1964531220204986]
	TIME [epoch: 9.7 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948727362177496		[learning rate: 1.3097e-05]
	Learning Rate: 1.30966e-05
	LOSS [training: 2.948727362177496 | validation: 3.2045710608563143]
	TIME [epoch: 9.71 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950734975983747		[learning rate: 1.3049e-05]
	Learning Rate: 1.30491e-05
	LOSS [training: 2.950734975983747 | validation: 3.2037132183322288]
	TIME [epoch: 9.71 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949451619536007		[learning rate: 1.3002e-05]
	Learning Rate: 1.30017e-05
	LOSS [training: 2.949451619536007 | validation: 3.1890934243842812]
	TIME [epoch: 9.7 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9488954652257653		[learning rate: 1.2955e-05]
	Learning Rate: 1.29545e-05
	LOSS [training: 2.9488954652257653 | validation: 3.1956598274719794]
	TIME [epoch: 9.7 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9492985635936284		[learning rate: 1.2907e-05]
	Learning Rate: 1.29075e-05
	LOSS [training: 2.9492985635936284 | validation: 3.1981339665406323]
	TIME [epoch: 9.71 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9469432087585257		[learning rate: 1.2861e-05]
	Learning Rate: 1.28607e-05
	LOSS [training: 2.9469432087585257 | validation: 3.1948693824868677]
	TIME [epoch: 9.7 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9538190519850844		[learning rate: 1.2814e-05]
	Learning Rate: 1.2814e-05
	LOSS [training: 2.9538190519850844 | validation: 3.1991366056639605]
	TIME [epoch: 9.7 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9490067685650523		[learning rate: 1.2767e-05]
	Learning Rate: 1.27675e-05
	LOSS [training: 2.9490067685650523 | validation: 3.202512360277786]
	TIME [epoch: 9.72 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.947954812650269		[learning rate: 1.2721e-05]
	Learning Rate: 1.27212e-05
	LOSS [training: 2.947954812650269 | validation: 3.1941390250172423]
	TIME [epoch: 9.71 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9523843771103917		[learning rate: 1.2675e-05]
	Learning Rate: 1.2675e-05
	LOSS [training: 2.9523843771103917 | validation: 3.20624957469705]
	TIME [epoch: 9.7 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9495536365503696		[learning rate: 1.2629e-05]
	Learning Rate: 1.2629e-05
	LOSS [training: 2.9495536365503696 | validation: 3.188999125086625]
	TIME [epoch: 9.73 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9492872576291185		[learning rate: 1.2583e-05]
	Learning Rate: 1.25832e-05
	LOSS [training: 2.9492872576291185 | validation: 3.1981519917051298]
	TIME [epoch: 9.7 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9501778444849807		[learning rate: 1.2537e-05]
	Learning Rate: 1.25375e-05
	LOSS [training: 2.9501778444849807 | validation: 3.193973942388407]
	TIME [epoch: 9.7 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9491004919457544		[learning rate: 1.2492e-05]
	Learning Rate: 1.2492e-05
	LOSS [training: 2.9491004919457544 | validation: 3.1942110228954084]
	TIME [epoch: 9.73 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9467489450379807		[learning rate: 1.2447e-05]
	Learning Rate: 1.24467e-05
	LOSS [training: 2.9467489450379807 | validation: 3.209139908813242]
	TIME [epoch: 9.7 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9478745959200943		[learning rate: 1.2401e-05]
	Learning Rate: 1.24015e-05
	LOSS [training: 2.9478745959200943 | validation: 3.196989289813505]
	TIME [epoch: 9.7 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9515896999747477		[learning rate: 1.2356e-05]
	Learning Rate: 1.23565e-05
	LOSS [training: 2.9515896999747477 | validation: 3.189834007312053]
	TIME [epoch: 9.73 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9463307641618224		[learning rate: 1.2312e-05]
	Learning Rate: 1.23116e-05
	LOSS [training: 2.9463307641618224 | validation: 3.1940731076516524]
	TIME [epoch: 9.71 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9501988804977883		[learning rate: 1.2267e-05]
	Learning Rate: 1.2267e-05
	LOSS [training: 2.9501988804977883 | validation: 3.2015306867469913]
	TIME [epoch: 9.7 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9450333100230512		[learning rate: 1.2222e-05]
	Learning Rate: 1.22224e-05
	LOSS [training: 2.9450333100230512 | validation: 3.190284272169727]
	TIME [epoch: 9.73 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949952099912472		[learning rate: 1.2178e-05]
	Learning Rate: 1.21781e-05
	LOSS [training: 2.949952099912472 | validation: 3.1998559606128776]
	TIME [epoch: 9.71 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9502620460800153		[learning rate: 1.2134e-05]
	Learning Rate: 1.21339e-05
	LOSS [training: 2.9502620460800153 | validation: 3.200738939092939]
	TIME [epoch: 9.7 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9489054190787902		[learning rate: 1.209e-05]
	Learning Rate: 1.20899e-05
	LOSS [training: 2.9489054190787902 | validation: 3.190878953824979]
	TIME [epoch: 9.72 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950262688550454		[learning rate: 1.2046e-05]
	Learning Rate: 1.2046e-05
	LOSS [training: 2.950262688550454 | validation: 3.1979682248730814]
	TIME [epoch: 9.7 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9494816897683513		[learning rate: 1.2002e-05]
	Learning Rate: 1.20023e-05
	LOSS [training: 2.9494816897683513 | validation: 3.196391800967068]
	TIME [epoch: 9.7 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9517923836666777		[learning rate: 1.1959e-05]
	Learning Rate: 1.19587e-05
	LOSS [training: 2.9517923836666777 | validation: 3.196902029602136]
	TIME [epoch: 9.73 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9472876245558917		[learning rate: 1.1915e-05]
	Learning Rate: 1.19153e-05
	LOSS [training: 2.9472876245558917 | validation: 3.1985617729237164]
	TIME [epoch: 9.71 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9445321798110173		[learning rate: 1.1872e-05]
	Learning Rate: 1.18721e-05
	LOSS [training: 2.9445321798110173 | validation: 3.1831510787961377]
	TIME [epoch: 9.7 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.946689940590085		[learning rate: 1.1829e-05]
	Learning Rate: 1.1829e-05
	LOSS [training: 2.946689940590085 | validation: 3.195173457396675]
	TIME [epoch: 9.72 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951380462865109		[learning rate: 1.1786e-05]
	Learning Rate: 1.17861e-05
	LOSS [training: 2.951380462865109 | validation: 3.1959941320564633]
	TIME [epoch: 9.7 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9489816400033755		[learning rate: 1.1743e-05]
	Learning Rate: 1.17433e-05
	LOSS [training: 2.9489816400033755 | validation: 3.1966234022920164]
	TIME [epoch: 9.71 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.947556374661082		[learning rate: 1.1701e-05]
	Learning Rate: 1.17007e-05
	LOSS [training: 2.947556374661082 | validation: 3.1862166887118315]
	TIME [epoch: 9.72 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951237450424367		[learning rate: 1.1658e-05]
	Learning Rate: 1.16582e-05
	LOSS [training: 2.951237450424367 | validation: 3.1983696527250443]
	TIME [epoch: 9.72 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9501545694114126		[learning rate: 1.1616e-05]
	Learning Rate: 1.16159e-05
	LOSS [training: 2.9501545694114126 | validation: 3.198724195482512]
	TIME [epoch: 9.7 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9469911254921533		[learning rate: 1.1574e-05]
	Learning Rate: 1.15737e-05
	LOSS [training: 2.9469911254921533 | validation: 3.2006940414041187]
	TIME [epoch: 9.71 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.947914940151297		[learning rate: 1.1532e-05]
	Learning Rate: 1.15317e-05
	LOSS [training: 2.947914940151297 | validation: 3.2067397483884124]
	TIME [epoch: 9.71 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.944809450471982		[learning rate: 1.149e-05]
	Learning Rate: 1.14899e-05
	LOSS [training: 2.944809450471982 | validation: 3.2038688576333345]
	TIME [epoch: 9.71 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9506275957369397		[learning rate: 1.1448e-05]
	Learning Rate: 1.14482e-05
	LOSS [training: 2.9506275957369397 | validation: 3.2012115332725997]
	TIME [epoch: 9.72 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9506285578569127		[learning rate: 1.1407e-05]
	Learning Rate: 1.14066e-05
	LOSS [training: 2.9506285578569127 | validation: 3.1933517481083813]
	TIME [epoch: 9.73 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95234498963038		[learning rate: 1.1365e-05]
	Learning Rate: 1.13652e-05
	LOSS [training: 2.95234498963038 | validation: 3.198248252646697]
	TIME [epoch: 9.7 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9521453181889385		[learning rate: 1.1324e-05]
	Learning Rate: 1.1324e-05
	LOSS [training: 2.9521453181889385 | validation: 3.201335218514631]
	TIME [epoch: 9.71 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.946666616386523		[learning rate: 1.1283e-05]
	Learning Rate: 1.12829e-05
	LOSS [training: 2.946666616386523 | validation: 3.199086550733536]
	TIME [epoch: 9.72 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9481235819161573		[learning rate: 1.1242e-05]
	Learning Rate: 1.1242e-05
	LOSS [training: 2.9481235819161573 | validation: 3.203450907648337]
	TIME [epoch: 9.7 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9465956760620244		[learning rate: 1.1201e-05]
	Learning Rate: 1.12012e-05
	LOSS [training: 2.9465956760620244 | validation: 3.2023669422282355]
	TIME [epoch: 9.71 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9495837760821155		[learning rate: 1.1161e-05]
	Learning Rate: 1.11605e-05
	LOSS [training: 2.9495837760821155 | validation: 3.2007503780868856]
	TIME [epoch: 9.73 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9494562692348287		[learning rate: 1.112e-05]
	Learning Rate: 1.112e-05
	LOSS [training: 2.9494562692348287 | validation: 3.2123168717035355]
	TIME [epoch: 9.71 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9523498922140483		[learning rate: 1.108e-05]
	Learning Rate: 1.10797e-05
	LOSS [training: 2.9523498922140483 | validation: 3.193403475283026]
	TIME [epoch: 9.71 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9497781730827564		[learning rate: 1.1039e-05]
	Learning Rate: 1.10394e-05
	LOSS [training: 2.9497781730827564 | validation: 3.1933998447479635]
	TIME [epoch: 9.73 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949319963133797		[learning rate: 1.0999e-05]
	Learning Rate: 1.09994e-05
	LOSS [training: 2.949319963133797 | validation: 3.194087852813871]
	TIME [epoch: 9.7 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952730334595726		[learning rate: 1.0959e-05]
	Learning Rate: 1.09595e-05
	LOSS [training: 2.952730334595726 | validation: 3.203127872426167]
	TIME [epoch: 9.71 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948566427778282		[learning rate: 1.092e-05]
	Learning Rate: 1.09197e-05
	LOSS [training: 2.948566427778282 | validation: 3.1971437651494345]
	TIME [epoch: 9.73 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95018048139804		[learning rate: 1.088e-05]
	Learning Rate: 1.08801e-05
	LOSS [training: 2.95018048139804 | validation: 3.201334163093367]
	TIME [epoch: 9.71 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9503089928452857		[learning rate: 1.0841e-05]
	Learning Rate: 1.08406e-05
	LOSS [training: 2.9503089928452857 | validation: 3.1978599773283283]
	TIME [epoch: 9.7 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954179585080125		[learning rate: 1.0801e-05]
	Learning Rate: 1.08012e-05
	LOSS [training: 2.954179585080125 | validation: 3.204716155935098]
	TIME [epoch: 9.73 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951186973771745		[learning rate: 1.0762e-05]
	Learning Rate: 1.0762e-05
	LOSS [training: 2.951186973771745 | validation: 3.202115922407561]
	TIME [epoch: 9.71 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952237565287434		[learning rate: 1.0723e-05]
	Learning Rate: 1.0723e-05
	LOSS [training: 2.952237565287434 | validation: 3.204431435764834]
	TIME [epoch: 9.71 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95419669645556		[learning rate: 1.0684e-05]
	Learning Rate: 1.06841e-05
	LOSS [training: 2.95419669645556 | validation: 3.1910673381623362]
	TIME [epoch: 9.72 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950907987657926		[learning rate: 1.0645e-05]
	Learning Rate: 1.06453e-05
	LOSS [training: 2.950907987657926 | validation: 3.2010895610907806]
	TIME [epoch: 9.71 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9465411086894333		[learning rate: 1.0607e-05]
	Learning Rate: 1.06067e-05
	LOSS [training: 2.9465411086894333 | validation: 3.1955252954964295]
	TIME [epoch: 9.71 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9500219073593748		[learning rate: 1.0568e-05]
	Learning Rate: 1.05682e-05
	LOSS [training: 2.9500219073593748 | validation: 3.1993026942139013]
	TIME [epoch: 9.72 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951706616491701		[learning rate: 1.053e-05]
	Learning Rate: 1.05298e-05
	LOSS [training: 2.951706616491701 | validation: 3.2071520296293308]
	TIME [epoch: 9.71 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9565969989877052		[learning rate: 1.0492e-05]
	Learning Rate: 1.04916e-05
	LOSS [training: 2.9565969989877052 | validation: 3.1922781611764255]
	TIME [epoch: 9.71 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9467453852331515		[learning rate: 1.0454e-05]
	Learning Rate: 1.04535e-05
	LOSS [training: 2.9467453852331515 | validation: 3.2073118931726583]
	TIME [epoch: 9.72 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9546317166504963		[learning rate: 1.0416e-05]
	Learning Rate: 1.04156e-05
	LOSS [training: 2.9546317166504963 | validation: 3.191274401193973]
	TIME [epoch: 9.71 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9498043909310425		[learning rate: 1.0378e-05]
	Learning Rate: 1.03778e-05
	LOSS [training: 2.9498043909310425 | validation: 3.193341346895607]
	TIME [epoch: 9.7 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9493115227993547		[learning rate: 1.034e-05]
	Learning Rate: 1.03401e-05
	LOSS [training: 2.9493115227993547 | validation: 3.1999535502421894]
	TIME [epoch: 9.72 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9440807838603527		[learning rate: 1.0303e-05]
	Learning Rate: 1.03026e-05
	LOSS [training: 2.9440807838603527 | validation: 3.2072207400895967]
	TIME [epoch: 9.7 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9556890264122426		[learning rate: 1.0265e-05]
	Learning Rate: 1.02652e-05
	LOSS [training: 2.9556890264122426 | validation: 3.2057170322758637]
	TIME [epoch: 9.7 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9474901072383597		[learning rate: 1.0228e-05]
	Learning Rate: 1.0228e-05
	LOSS [training: 2.9474901072383597 | validation: 3.203188768427687]
	TIME [epoch: 9.71 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951840511542891		[learning rate: 1.0191e-05]
	Learning Rate: 1.01909e-05
	LOSS [training: 2.951840511542891 | validation: 3.202738761136367]
	TIME [epoch: 9.72 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9487965145542665		[learning rate: 1.0154e-05]
	Learning Rate: 1.01539e-05
	LOSS [training: 2.9487965145542665 | validation: 3.205178722490929]
	TIME [epoch: 9.7 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95111606154017		[learning rate: 1.0117e-05]
	Learning Rate: 1.0117e-05
	LOSS [training: 2.95111606154017 | validation: 3.1961170059625763]
	TIME [epoch: 9.71 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.94631470802289		[learning rate: 1.008e-05]
	Learning Rate: 1.00803e-05
	LOSS [training: 2.94631470802289 | validation: 3.198827477403826]
	TIME [epoch: 9.72 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9566150654312455		[learning rate: 1.0044e-05]
	Learning Rate: 1.00437e-05
	LOSS [training: 2.9566150654312455 | validation: 3.202386022409096]
	TIME [epoch: 9.7 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948292070033953		[learning rate: 1.0007e-05]
	Learning Rate: 1.00073e-05
	LOSS [training: 2.948292070033953 | validation: 3.1929202642078214]
	TIME [epoch: 9.7 sec]
Finished training in 19589.892 seconds.
