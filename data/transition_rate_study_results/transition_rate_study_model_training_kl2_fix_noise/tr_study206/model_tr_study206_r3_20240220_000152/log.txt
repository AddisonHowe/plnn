Args:
Namespace(name='model_tr_study206', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3', training_data='data/transition_rate_studies/tr_study206/tr_study206_training/r3', validation_data='data/transition_rate_studies/tr_study206/tr_study206_validation/r3', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 577907831

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.459747848988227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.459747848988227 | validation: 9.660274379829007]
	TIME [epoch: 78.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.064576995498415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.064576995498415 | validation: 10.56289960635489]
	TIME [epoch: 9.71 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.357282847418121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.357282847418121 | validation: 10.81762733345571]
	TIME [epoch: 9.7 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.246030807649365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.246030807649365 | validation: 9.648195719827728]
	TIME [epoch: 9.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.405841552615376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.405841552615376 | validation: 7.475565972734391]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.811429924859901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.811429924859901 | validation: 7.8994487133851985]
	TIME [epoch: 9.7 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.629298368136691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.629298368136691 | validation: 9.664384841946152]
	TIME [epoch: 9.73 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.785442618334659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.785442618334659 | validation: 7.074292058393259]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.698687515717606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.698687515717606 | validation: 8.502662265206581]
	TIME [epoch: 9.71 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.29617134720269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.29617134720269 | validation: 9.596873131741656]
	TIME [epoch: 9.7 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.359536822002934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.359536822002934 | validation: 6.454884087882888]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.375935285497864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.375935285497864 | validation: 6.395024336668312]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.802456916756233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.802456916756233 | validation: 6.712246502318672]
	TIME [epoch: 9.69 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.021342123218483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.021342123218483 | validation: 6.469230131142276]
	TIME [epoch: 9.71 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.03391369675227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.03391369675227 | validation: 6.103497336869634]
	TIME [epoch: 9.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.67918025536937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.67918025536937 | validation: 5.889510599808382]
	TIME [epoch: 9.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.549622153000739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.549622153000739 | validation: 6.217329844621034]
	TIME [epoch: 9.68 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.633205548587219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.633205548587219 | validation: 6.686447464785207]
	TIME [epoch: 9.71 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.547087106461977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.547087106461977 | validation: 6.198710368966998]
	TIME [epoch: 9.68 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.521194183326628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.521194183326628 | validation: 6.718681102479573]
	TIME [epoch: 9.68 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.605110507083422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.605110507083422 | validation: 6.257488239691441]
	TIME [epoch: 9.69 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.543994189238869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.543994189238869 | validation: 6.353899602977726]
	TIME [epoch: 9.71 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.500362298571704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.500362298571704 | validation: 6.045892523795169]
	TIME [epoch: 9.69 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.501429870139608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.501429870139608 | validation: 6.039123357289434]
	TIME [epoch: 9.69 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.513564081291748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.513564081291748 | validation: 6.105704881958161]
	TIME [epoch: 9.7 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.397861194991898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.397861194991898 | validation: 5.992502548260319]
	TIME [epoch: 9.71 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.420821762065621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.420821762065621 | validation: 6.2078462888439585]
	TIME [epoch: 9.69 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3470014413065226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3470014413065226 | validation: 6.298211775001087]
	TIME [epoch: 9.69 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.424145306005867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.424145306005867 | validation: 5.982194584649585]
	TIME [epoch: 9.7 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.176218686344045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.176218686344045 | validation: 6.342341283361025]
	TIME [epoch: 9.69 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.374352167205274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.374352167205274 | validation: 6.270376088072766]
	TIME [epoch: 9.68 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2798614315872525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2798614315872525 | validation: 5.609882825421472]
	TIME [epoch: 9.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.298534843068111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.298534843068111 | validation: 6.112385982837366]
	TIME [epoch: 9.7 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3967353719690605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3967353719690605 | validation: 5.789388567530766]
	TIME [epoch: 9.68 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.260621024149385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.260621024149385 | validation: 6.041503932523146]
	TIME [epoch: 9.69 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.412232297715558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.412232297715558 | validation: 5.866521474412379]
	TIME [epoch: 9.69 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.320393296088261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.320393296088261 | validation: 5.921801088897168]
	TIME [epoch: 9.69 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.209703603302114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.209703603302114 | validation: 5.947536732473025]
	TIME [epoch: 9.69 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3363828273026765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3363828273026765 | validation: 5.804324349171353]
	TIME [epoch: 9.68 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.345732353464415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.345732353464415 | validation: 5.577130434942603]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.550009299052458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.550009299052458 | validation: 6.567922673083341]
	TIME [epoch: 9.68 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.505483259410353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.505483259410353 | validation: 6.369856384501286]
	TIME [epoch: 9.68 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.444398122451159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.444398122451159 | validation: 5.787328042600282]
	TIME [epoch: 9.69 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.116233961405232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.116233961405232 | validation: 5.706630322508715]
	TIME [epoch: 9.71 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.375391600344798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.375391600344798 | validation: 5.906618785264966]
	TIME [epoch: 9.69 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.207638065472446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.207638065472446 | validation: 6.039848249444006]
	TIME [epoch: 9.68 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.223958415636287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.223958415636287 | validation: 6.013367112203009]
	TIME [epoch: 9.68 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2814572365473165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2814572365473165 | validation: 5.963460863350167]
	TIME [epoch: 9.71 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.201528707499341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.201528707499341 | validation: 5.559734746927745]
	TIME [epoch: 9.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.13621798027254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.13621798027254 | validation: 5.838718126753264]
	TIME [epoch: 9.68 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.108708000596142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.108708000596142 | validation: 5.882604097521746]
	TIME [epoch: 9.69 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.141920738949962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.141920738949962 | validation: 5.950620528540587]
	TIME [epoch: 9.68 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1381963453236965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1381963453236965 | validation: 5.576696126188931]
	TIME [epoch: 9.7 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2177592177246535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2177592177246535 | validation: 5.659350713129913]
	TIME [epoch: 9.69 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8915794562748154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8915794562748154 | validation: 6.222407747395907]
	TIME [epoch: 9.7 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1582625732110765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1582625732110765 | validation: 5.773296490584386]
	TIME [epoch: 9.68 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.198058598935655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.198058598935655 | validation: 6.044002430117099]
	TIME [epoch: 9.68 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.093055395194263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.093055395194263 | validation: 6.320942647814019]
	TIME [epoch: 9.68 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.033460925046073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.033460925046073 | validation: 5.632484746814892]
	TIME [epoch: 9.71 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9246907409402496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9246907409402496 | validation: 4.831992452711971]
	TIME [epoch: 9.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.627204914174515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.627204914174515 | validation: 4.96852297162103]
	TIME [epoch: 9.69 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.466950859418499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.466950859418499 | validation: 4.880621812183499]
	TIME [epoch: 9.7 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.003632391364044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.003632391364044 | validation: 4.881173575786023]
	TIME [epoch: 9.7 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3658012895588563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3658012895588563 | validation: 6.717642505268088]
	TIME [epoch: 9.69 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.841457375300816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.841457375300816 | validation: 4.120340581090325]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.418358185351089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.418358185351089 | validation: 7.0258299094147745]
	TIME [epoch: 9.71 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.971751392148177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.971751392148177 | validation: 3.910842822690609]
	TIME [epoch: 9.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.381146687205093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.381146687205093 | validation: 4.958224056777117]
	TIME [epoch: 9.68 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2880890126302256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2880890126302256 | validation: 4.344417994490906]
	TIME [epoch: 9.67 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2887177462314177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2887177462314177 | validation: 4.602754990721669]
	TIME [epoch: 9.7 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.062947324368406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.062947324368406 | validation: 4.075322697650821]
	TIME [epoch: 9.68 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4011568425546583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4011568425546583 | validation: 3.9099800825889623]
	TIME [epoch: 9.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.042914126177118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.042914126177118 | validation: 4.964568449757706]
	TIME [epoch: 9.68 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.419035163220233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.419035163220233 | validation: 5.32272639034636]
	TIME [epoch: 9.7 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774919784874939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.774919784874939 | validation: 4.375505552822544]
	TIME [epoch: 9.68 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1804706800655085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1804706800655085 | validation: 4.352696243597983]
	TIME [epoch: 9.68 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.202328696424091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.202328696424091 | validation: 3.930905631248378]
	TIME [epoch: 9.7 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2304672352998027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2304672352998027 | validation: 4.782325265890477]
	TIME [epoch: 9.68 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.16058838423042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.16058838423042 | validation: 3.8890303520889278]
	TIME [epoch: 9.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9019986059198475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9019986059198475 | validation: 4.005904989205748]
	TIME [epoch: 9.69 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9643509197866065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9643509197866065 | validation: 4.100617578356665]
	TIME [epoch: 9.71 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.104147424315043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.104147424315043 | validation: 3.937764172061576]
	TIME [epoch: 9.69 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3724908277426584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3724908277426584 | validation: 3.9306263442152876]
	TIME [epoch: 9.68 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6814951081332556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6814951081332556 | validation: 3.97615867004705]
	TIME [epoch: 9.69 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.05266728015748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.05266728015748 | validation: 4.202673266820787]
	TIME [epoch: 9.7 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.028731193344759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.028731193344759 | validation: 4.0635526030575315]
	TIME [epoch: 9.68 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.968588809381207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.968588809381207 | validation: 4.191700150300964]
	TIME [epoch: 9.69 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.162079831204257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.162079831204257 | validation: 4.115252011624501]
	TIME [epoch: 9.68 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.00172948078812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.00172948078812 | validation: 3.9873484940080948]
	TIME [epoch: 9.69 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0529989186117517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0529989186117517 | validation: 4.61545457194624]
	TIME [epoch: 9.68 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0650023164274494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0650023164274494 | validation: 4.073021470030929]
	TIME [epoch: 9.69 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0232537592278836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0232537592278836 | validation: 3.5813047592667178]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8949872459569024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8949872459569024 | validation: 3.848139981516506]
	TIME [epoch: 9.69 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8848775863629843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8848775863629843 | validation: 3.7897725122527937]
	TIME [epoch: 9.68 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8401108362086966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8401108362086966 | validation: 4.159078338216586]
	TIME [epoch: 9.68 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.050230785875461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.050230785875461 | validation: 3.509605996009515]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5883787129939315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5883787129939315 | validation: 3.885783779419006]
	TIME [epoch: 9.68 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7978634796416944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7978634796416944 | validation: 2.1490111078978607]
	TIME [epoch: 9.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.593768961282583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.593768961282583 | validation: 1.9551453577627422]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3724052387839172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3724052387839172 | validation: 1.2488619631987745]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3791780761195964		[learning rate: 0.009971]
	Learning Rate: 0.00997096
	LOSS [training: 1.3791780761195964 | validation: 1.8391275525721364]
	TIME [epoch: 9.69 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1437829564629038		[learning rate: 0.0099348]
	Learning Rate: 0.00993477
	LOSS [training: 1.1437829564629038 | validation: 2.4276670559419418]
	TIME [epoch: 9.68 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5160010462229265		[learning rate: 0.0098987]
	Learning Rate: 0.00989872
	LOSS [training: 1.5160010462229265 | validation: 1.5184144171481115]
	TIME [epoch: 9.7 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0288621439143448		[learning rate: 0.0098628]
	Learning Rate: 0.00986279
	LOSS [training: 1.0288621439143448 | validation: 1.6761494673529234]
	TIME [epoch: 9.68 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0043785433171233		[learning rate: 0.009827]
	Learning Rate: 0.009827
	LOSS [training: 1.0043785433171233 | validation: 2.0023273559333266]
	TIME [epoch: 9.69 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3993050380213141		[learning rate: 0.0097913]
	Learning Rate: 0.00979134
	LOSS [training: 1.3993050380213141 | validation: 1.0605842765382847]
	TIME [epoch: 9.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9202934798571754		[learning rate: 0.0097558]
	Learning Rate: 0.00975581
	LOSS [training: 0.9202934798571754 | validation: 1.9264239667102354]
	TIME [epoch: 9.72 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.414898867626507		[learning rate: 0.0097204]
	Learning Rate: 0.0097204
	LOSS [training: 1.414898867626507 | validation: 4.045914716931283]
	TIME [epoch: 9.69 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.492871825646161		[learning rate: 0.0096851]
	Learning Rate: 0.00968513
	LOSS [training: 2.492871825646161 | validation: 1.58985388935969]
	TIME [epoch: 9.7 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8765365967395071		[learning rate: 0.00965]
	Learning Rate: 0.00964998
	LOSS [training: 0.8765365967395071 | validation: 1.1606115065650517]
	TIME [epoch: 9.71 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2129479742816622		[learning rate: 0.009615]
	Learning Rate: 0.00961496
	LOSS [training: 1.2129479742816622 | validation: 1.8849996813667502]
	TIME [epoch: 9.72 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.205779305975618		[learning rate: 0.0095801]
	Learning Rate: 0.00958006
	LOSS [training: 1.205779305975618 | validation: 1.3193179093558514]
	TIME [epoch: 9.7 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9048983098003027		[learning rate: 0.0095453]
	Learning Rate: 0.0095453
	LOSS [training: 0.9048983098003027 | validation: 2.0021205205431287]
	TIME [epoch: 9.69 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1322886329085489		[learning rate: 0.0095107]
	Learning Rate: 0.00951066
	LOSS [training: 1.1322886329085489 | validation: 2.0784117605950887]
	TIME [epoch: 9.71 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.992829727764526		[learning rate: 0.0094761]
	Learning Rate: 0.00947614
	LOSS [training: 0.992829727764526 | validation: 2.008334481117784]
	TIME [epoch: 9.69 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5507782200019555		[learning rate: 0.0094418]
	Learning Rate: 0.00944175
	LOSS [training: 1.5507782200019555 | validation: 0.9604007450692046]
	TIME [epoch: 9.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9162353061783005		[learning rate: 0.0094075]
	Learning Rate: 0.00940749
	LOSS [training: 0.9162353061783005 | validation: 1.3281093316406123]
	TIME [epoch: 9.68 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2555287346668063		[learning rate: 0.0093733]
	Learning Rate: 0.00937335
	LOSS [training: 1.2555287346668063 | validation: 1.5746971739104643]
	TIME [epoch: 9.71 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0831890688885442		[learning rate: 0.0093393]
	Learning Rate: 0.00933933
	LOSS [training: 1.0831890688885442 | validation: 1.4047264521817957]
	TIME [epoch: 9.68 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7773579188608704		[learning rate: 0.0093054]
	Learning Rate: 0.00930544
	LOSS [training: 0.7773579188608704 | validation: 1.8024372534776103]
	TIME [epoch: 9.68 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1795192762029374		[learning rate: 0.0092717]
	Learning Rate: 0.00927167
	LOSS [training: 1.1795192762029374 | validation: 1.4527670289223271]
	TIME [epoch: 9.69 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9569483611560724		[learning rate: 0.009238]
	Learning Rate: 0.00923802
	LOSS [training: 0.9569483611560724 | validation: 1.8939847504762861]
	TIME [epoch: 9.7 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1049713201012383		[learning rate: 0.0092045]
	Learning Rate: 0.0092045
	LOSS [training: 1.1049713201012383 | validation: 1.1097369507658204]
	TIME [epoch: 9.68 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9753014663424743		[learning rate: 0.0091711]
	Learning Rate: 0.00917109
	LOSS [training: 0.9753014663424743 | validation: 1.1026065220073817]
	TIME [epoch: 9.73 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9420295620845367		[learning rate: 0.0091378]
	Learning Rate: 0.00913781
	LOSS [training: 0.9420295620845367 | validation: 1.2380098045580075]
	TIME [epoch: 9.69 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9305721032244529		[learning rate: 0.0091046]
	Learning Rate: 0.00910465
	LOSS [training: 0.9305721032244529 | validation: 1.1856820030176949]
	TIME [epoch: 9.7 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9933409386182926		[learning rate: 0.0090716]
	Learning Rate: 0.00907161
	LOSS [training: 0.9933409386182926 | validation: 1.763024139702447]
	TIME [epoch: 9.69 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0362557432487602		[learning rate: 0.0090387]
	Learning Rate: 0.00903868
	LOSS [training: 1.0362557432487602 | validation: 1.8340292591426646]
	TIME [epoch: 9.68 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0335452505626772		[learning rate: 0.0090059]
	Learning Rate: 0.00900588
	LOSS [training: 1.0335452505626772 | validation: 1.2958763306290952]
	TIME [epoch: 9.7 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.842542374750896		[learning rate: 0.0089732]
	Learning Rate: 0.0089732
	LOSS [training: 0.842542374750896 | validation: 1.1991578566830017]
	TIME [epoch: 9.68 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8337611598870327		[learning rate: 0.0089406]
	Learning Rate: 0.00894064
	LOSS [training: 0.8337611598870327 | validation: 1.1753949032337025]
	TIME [epoch: 9.69 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1286072648004397		[learning rate: 0.0089082]
	Learning Rate: 0.00890819
	LOSS [training: 1.1286072648004397 | validation: 1.3978291536459255]
	TIME [epoch: 9.69 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8970180458347532		[learning rate: 0.0088759]
	Learning Rate: 0.00887586
	LOSS [training: 0.8970180458347532 | validation: 1.1064090556271264]
	TIME [epoch: 9.71 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.125746955931661		[learning rate: 0.0088437]
	Learning Rate: 0.00884365
	LOSS [training: 1.125746955931661 | validation: 1.5217913038280808]
	TIME [epoch: 9.67 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1446699991218623		[learning rate: 0.0088116]
	Learning Rate: 0.00881156
	LOSS [training: 1.1446699991218623 | validation: 1.4193048876946213]
	TIME [epoch: 9.69 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9145259994312991		[learning rate: 0.0087796]
	Learning Rate: 0.00877958
	LOSS [training: 0.9145259994312991 | validation: 1.2692259863270705]
	TIME [epoch: 9.69 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0793460087675655		[learning rate: 0.0087477]
	Learning Rate: 0.00874772
	LOSS [training: 1.0793460087675655 | validation: 1.4136578132701738]
	TIME [epoch: 9.71 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.071375143836684		[learning rate: 0.008716]
	Learning Rate: 0.00871597
	LOSS [training: 1.071375143836684 | validation: 1.8161117681413124]
	TIME [epoch: 9.68 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.121689087773838		[learning rate: 0.0086843]
	Learning Rate: 0.00868434
	LOSS [training: 1.121689087773838 | validation: 1.4011678322774743]
	TIME [epoch: 9.68 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9247141139561951		[learning rate: 0.0086528]
	Learning Rate: 0.00865282
	LOSS [training: 0.9247141139561951 | validation: 1.0841959136057586]
	TIME [epoch: 9.7 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7740945521476652		[learning rate: 0.0086214]
	Learning Rate: 0.00862142
	LOSS [training: 0.7740945521476652 | validation: 1.2475113900592347]
	TIME [epoch: 9.7 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0504318971046116		[learning rate: 0.0085901]
	Learning Rate: 0.00859013
	LOSS [training: 1.0504318971046116 | validation: 1.1276253737210942]
	TIME [epoch: 9.69 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.066963324621947		[learning rate: 0.008559]
	Learning Rate: 0.00855896
	LOSS [training: 1.066963324621947 | validation: 1.144300336272865]
	TIME [epoch: 9.68 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0286496893047725		[learning rate: 0.0085279]
	Learning Rate: 0.0085279
	LOSS [training: 1.0286496893047725 | validation: 1.1377159959925331]
	TIME [epoch: 9.7 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.865785256936282		[learning rate: 0.008497]
	Learning Rate: 0.00849695
	LOSS [training: 0.865785256936282 | validation: 1.5509576162298249]
	TIME [epoch: 9.69 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9990599443038889		[learning rate: 0.0084661]
	Learning Rate: 0.00846612
	LOSS [training: 0.9990599443038889 | validation: 1.0242352253499964]
	TIME [epoch: 9.68 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8871286453334621		[learning rate: 0.0084354]
	Learning Rate: 0.00843539
	LOSS [training: 0.8871286453334621 | validation: 1.0409952740601274]
	TIME [epoch: 9.69 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7820979285772828		[learning rate: 0.0084048]
	Learning Rate: 0.00840478
	LOSS [training: 0.7820979285772828 | validation: 1.038846585186365]
	TIME [epoch: 9.71 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8644319570955303		[learning rate: 0.0083743]
	Learning Rate: 0.00837428
	LOSS [training: 0.8644319570955303 | validation: 1.3917663616507252]
	TIME [epoch: 9.68 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8321190695296808		[learning rate: 0.0083439]
	Learning Rate: 0.00834389
	LOSS [training: 0.8321190695296808 | validation: 1.2802794383453977]
	TIME [epoch: 9.68 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0169244995013074		[learning rate: 0.0083136]
	Learning Rate: 0.00831361
	LOSS [training: 1.0169244995013074 | validation: 1.3037226785689726]
	TIME [epoch: 9.69 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9418521990629809		[learning rate: 0.0082834]
	Learning Rate: 0.00828344
	LOSS [training: 0.9418521990629809 | validation: 1.0754276186605773]
	TIME [epoch: 9.72 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8144045944821048		[learning rate: 0.0082534]
	Learning Rate: 0.00825338
	LOSS [training: 0.8144045944821048 | validation: 1.1001635642856753]
	TIME [epoch: 9.68 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8376198102338579		[learning rate: 0.0082234]
	Learning Rate: 0.00822342
	LOSS [training: 0.8376198102338579 | validation: 1.3710822474204736]
	TIME [epoch: 9.69 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.932177827487096		[learning rate: 0.0081936]
	Learning Rate: 0.00819358
	LOSS [training: 0.932177827487096 | validation: 1.4011023468218002]
	TIME [epoch: 9.69 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7614104212395408		[learning rate: 0.0081638]
	Learning Rate: 0.00816384
	LOSS [training: 0.7614104212395408 | validation: 1.4302621937004139]
	TIME [epoch: 9.69 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9688472309616266		[learning rate: 0.0081342]
	Learning Rate: 0.00813422
	LOSS [training: 0.9688472309616266 | validation: 1.4494477975676585]
	TIME [epoch: 9.68 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9045580968655973		[learning rate: 0.0081047]
	Learning Rate: 0.0081047
	LOSS [training: 0.9045580968655973 | validation: 1.1255597284454952]
	TIME [epoch: 9.69 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7563446372771699		[learning rate: 0.0080753]
	Learning Rate: 0.00807529
	LOSS [training: 0.7563446372771699 | validation: 1.3984484585467525]
	TIME [epoch: 9.7 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0837124782680312		[learning rate: 0.008046]
	Learning Rate: 0.00804598
	LOSS [training: 1.0837124782680312 | validation: 1.773666775214645]
	TIME [epoch: 9.7 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9168374426013127		[learning rate: 0.0080168]
	Learning Rate: 0.00801678
	LOSS [training: 0.9168374426013127 | validation: 1.1514154183027299]
	TIME [epoch: 9.69 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8715894054659412		[learning rate: 0.0079877]
	Learning Rate: 0.00798769
	LOSS [training: 0.8715894054659412 | validation: 0.9798484017137215]
	TIME [epoch: 9.7 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0500614570723936		[learning rate: 0.0079587]
	Learning Rate: 0.0079587
	LOSS [training: 1.0500614570723936 | validation: 1.0744906305938653]
	TIME [epoch: 9.72 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7926177880571055		[learning rate: 0.0079298]
	Learning Rate: 0.00792982
	LOSS [training: 0.7926177880571055 | validation: 1.059991453207392]
	TIME [epoch: 9.68 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7567622059729313		[learning rate: 0.007901]
	Learning Rate: 0.00790104
	LOSS [training: 1.7567622059729313 | validation: 4.016244865487791]
	TIME [epoch: 9.68 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7914923569953694		[learning rate: 0.0078724]
	Learning Rate: 0.00787237
	LOSS [training: 1.7914923569953694 | validation: 1.256649109007895]
	TIME [epoch: 9.7 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8691657242765206		[learning rate: 0.0078438]
	Learning Rate: 0.0078438
	LOSS [training: 0.8691657242765206 | validation: 1.1000897622155257]
	TIME [epoch: 9.72 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9646717486500626		[learning rate: 0.0078153]
	Learning Rate: 0.00781533
	LOSS [training: 0.9646717486500626 | validation: 1.0411285279410414]
	TIME [epoch: 9.68 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9027952249063347		[learning rate: 0.007787]
	Learning Rate: 0.00778697
	LOSS [training: 0.9027952249063347 | validation: 1.2733820918965035]
	TIME [epoch: 9.68 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8201791398856162		[learning rate: 0.0077587]
	Learning Rate: 0.00775871
	LOSS [training: 0.8201791398856162 | validation: 0.9797668062946017]
	TIME [epoch: 9.71 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7236322802554422		[learning rate: 0.0077306]
	Learning Rate: 0.00773055
	LOSS [training: 0.7236322802554422 | validation: 1.1933833619545917]
	TIME [epoch: 9.69 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8623236436446596		[learning rate: 0.0077025]
	Learning Rate: 0.0077025
	LOSS [training: 0.8623236436446596 | validation: 0.9516334454651159]
	TIME [epoch: 9.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6602924909944956		[learning rate: 0.0076745]
	Learning Rate: 0.00767455
	LOSS [training: 0.6602924909944956 | validation: 1.0521108761693574]
	TIME [epoch: 9.67 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8157362140001864		[learning rate: 0.0076467]
	Learning Rate: 0.00764669
	LOSS [training: 0.8157362140001864 | validation: 1.4293941083757564]
	TIME [epoch: 9.72 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0614833426381094		[learning rate: 0.0076189]
	Learning Rate: 0.00761894
	LOSS [training: 1.0614833426381094 | validation: 0.9732551625811169]
	TIME [epoch: 9.69 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8231798404046753		[learning rate: 0.0075913]
	Learning Rate: 0.00759129
	LOSS [training: 0.8231798404046753 | validation: 1.08020789454662]
	TIME [epoch: 9.68 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.679964790773534		[learning rate: 0.0075637]
	Learning Rate: 0.00756374
	LOSS [training: 0.679964790773534 | validation: 1.0316932035026793]
	TIME [epoch: 9.68 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7358451778981332		[learning rate: 0.0075363]
	Learning Rate: 0.00753629
	LOSS [training: 0.7358451778981332 | validation: 1.0912551241874673]
	TIME [epoch: 9.72 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7256508054315852		[learning rate: 0.0075089]
	Learning Rate: 0.00750895
	LOSS [training: 0.7256508054315852 | validation: 1.5078077320475511]
	TIME [epoch: 9.69 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9201698479449915		[learning rate: 0.0074817]
	Learning Rate: 0.00748169
	LOSS [training: 0.9201698479449915 | validation: 1.7970716688420907]
	TIME [epoch: 9.69 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1740991889815786		[learning rate: 0.0074545]
	Learning Rate: 0.00745454
	LOSS [training: 1.1740991889815786 | validation: 1.1598907592123333]
	TIME [epoch: 9.71 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6635070442766695		[learning rate: 0.0074275]
	Learning Rate: 0.00742749
	LOSS [training: 0.6635070442766695 | validation: 1.145457142507279]
	TIME [epoch: 9.7 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7518170147282278		[learning rate: 0.0074005]
	Learning Rate: 0.00740054
	LOSS [training: 0.7518170147282278 | validation: 1.027469836232331]
	TIME [epoch: 9.69 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.733186985084022		[learning rate: 0.0073737]
	Learning Rate: 0.00737368
	LOSS [training: 0.733186985084022 | validation: 1.067347203926138]
	TIME [epoch: 9.68 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7667570048209033		[learning rate: 0.0073469]
	Learning Rate: 0.00734692
	LOSS [training: 0.7667570048209033 | validation: 1.0105767926631015]
	TIME [epoch: 9.71 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.700528010886671		[learning rate: 0.0073203]
	Learning Rate: 0.00732026
	LOSS [training: 0.700528010886671 | validation: 1.2437335369365112]
	TIME [epoch: 9.69 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7225767108357071		[learning rate: 0.0072937]
	Learning Rate: 0.00729369
	LOSS [training: 0.7225767108357071 | validation: 1.2790606032878078]
	TIME [epoch: 9.68 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7195231604522504		[learning rate: 0.0072672]
	Learning Rate: 0.00726722
	LOSS [training: 0.7195231604522504 | validation: 1.2001973411270508]
	TIME [epoch: 9.69 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.706898333071278		[learning rate: 0.0072408]
	Learning Rate: 0.00724085
	LOSS [training: 0.706898333071278 | validation: 1.3451758006998915]
	TIME [epoch: 9.71 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7743322730881707		[learning rate: 0.0072146]
	Learning Rate: 0.00721457
	LOSS [training: 0.7743322730881707 | validation: 1.1015152836615498]
	TIME [epoch: 9.68 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7399239828350691		[learning rate: 0.0071884]
	Learning Rate: 0.00718839
	LOSS [training: 0.7399239828350691 | validation: 1.1781219998255095]
	TIME [epoch: 9.69 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9581708518486215		[learning rate: 0.0071623]
	Learning Rate: 0.0071623
	LOSS [training: 0.9581708518486215 | validation: 1.2199340805707046]
	TIME [epoch: 9.7 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6573472659735937		[learning rate: 0.0071363]
	Learning Rate: 0.00713631
	LOSS [training: 0.6573472659735937 | validation: 1.1335636389455157]
	TIME [epoch: 9.72 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6948743100410979		[learning rate: 0.0071104]
	Learning Rate: 0.00711041
	LOSS [training: 0.6948743100410979 | validation: 1.686553547608068]
	TIME [epoch: 9.69 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8088758300928788		[learning rate: 0.0070846]
	Learning Rate: 0.00708461
	LOSS [training: 0.8088758300928788 | validation: 1.0844428623570186]
	TIME [epoch: 9.69 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.668958348262984		[learning rate: 0.0070589]
	Learning Rate: 0.0070589
	LOSS [training: 0.668958348262984 | validation: 1.5362362576863562]
	TIME [epoch: 9.71 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8765814464064304		[learning rate: 0.0070333]
	Learning Rate: 0.00703328
	LOSS [training: 0.8765814464064304 | validation: 1.0845556423840872]
	TIME [epoch: 9.7 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7173715291145475		[learning rate: 0.0070078]
	Learning Rate: 0.00700776
	LOSS [training: 0.7173715291145475 | validation: 1.2326109876743987]
	TIME [epoch: 9.69 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9245470826696248		[learning rate: 0.0069823]
	Learning Rate: 0.00698232
	LOSS [training: 0.9245470826696248 | validation: 1.0421885843626455]
	TIME [epoch: 9.68 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.719357572259667		[learning rate: 0.006957]
	Learning Rate: 0.00695698
	LOSS [training: 0.719357572259667 | validation: 0.9442341124125722]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8260265610441613		[learning rate: 0.0069317]
	Learning Rate: 0.00693174
	LOSS [training: 0.8260265610441613 | validation: 1.2569708166072875]
	TIME [epoch: 9.68 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7714777134985272		[learning rate: 0.0069066]
	Learning Rate: 0.00690658
	LOSS [training: 0.7714777134985272 | validation: 1.259011598379999]
	TIME [epoch: 9.69 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6868777893533465		[learning rate: 0.0068815]
	Learning Rate: 0.00688152
	LOSS [training: 0.6868777893533465 | validation: 1.16774888218995]
	TIME [epoch: 9.7 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8108314535706642		[learning rate: 0.0068565]
	Learning Rate: 0.00685654
	LOSS [training: 0.8108314535706642 | validation: 1.0979334840756017]
	TIME [epoch: 9.72 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7521134054985559		[learning rate: 0.0068317]
	Learning Rate: 0.00683166
	LOSS [training: 0.7521134054985559 | validation: 1.3016608102278715]
	TIME [epoch: 9.68 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.669244317504153		[learning rate: 0.0068069]
	Learning Rate: 0.00680687
	LOSS [training: 0.669244317504153 | validation: 0.9238572693217328]
	TIME [epoch: 9.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8867590255115975		[learning rate: 0.0067822]
	Learning Rate: 0.00678217
	LOSS [training: 0.8867590255115975 | validation: 0.9826124230014268]
	TIME [epoch: 9.71 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9812326794116032		[learning rate: 0.0067576]
	Learning Rate: 0.00675755
	LOSS [training: 0.9812326794116032 | validation: 1.271323260823368]
	TIME [epoch: 9.71 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7191706446626246		[learning rate: 0.006733]
	Learning Rate: 0.00673303
	LOSS [training: 0.7191706446626246 | validation: 1.0486869154884202]
	TIME [epoch: 9.69 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7469902348965581		[learning rate: 0.0067086]
	Learning Rate: 0.00670859
	LOSS [training: 0.7469902348965581 | validation: 0.9294523924489585]
	TIME [epoch: 9.69 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.725069461810666		[learning rate: 0.0066842]
	Learning Rate: 0.00668425
	LOSS [training: 0.725069461810666 | validation: 1.160813697821666]
	TIME [epoch: 9.69 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7405390921284679		[learning rate: 0.00666]
	Learning Rate: 0.00665999
	LOSS [training: 0.7405390921284679 | validation: 1.1300763526402835]
	TIME [epoch: 9.68 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.86959874996422		[learning rate: 0.0066358]
	Learning Rate: 0.00663582
	LOSS [training: 0.86959874996422 | validation: 1.0042612195869653]
	TIME [epoch: 9.68 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.713178063785624		[learning rate: 0.0066117]
	Learning Rate: 0.00661174
	LOSS [training: 0.713178063785624 | validation: 0.9019606536173194]
	TIME [epoch: 9.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7190887051100503		[learning rate: 0.0065877]
	Learning Rate: 0.00658775
	LOSS [training: 0.7190887051100503 | validation: 1.0475270452361825]
	TIME [epoch: 9.71 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7620223866690765		[learning rate: 0.0065638]
	Learning Rate: 0.00656384
	LOSS [training: 0.7620223866690765 | validation: 1.1016664179392164]
	TIME [epoch: 9.67 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8496153532057207		[learning rate: 0.00654]
	Learning Rate: 0.00654002
	LOSS [training: 0.8496153532057207 | validation: 0.9781168442946103]
	TIME [epoch: 9.66 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6717042679591076		[learning rate: 0.0065163]
	Learning Rate: 0.00651628
	LOSS [training: 0.6717042679591076 | validation: 1.1998622553891556]
	TIME [epoch: 9.68 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6575731008364392		[learning rate: 0.0064926]
	Learning Rate: 0.00649264
	LOSS [training: 0.6575731008364392 | validation: 1.1164349968919052]
	TIME [epoch: 9.69 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.785832750575044		[learning rate: 0.0064691]
	Learning Rate: 0.00646907
	LOSS [training: 0.785832750575044 | validation: 1.739826933570791]
	TIME [epoch: 9.66 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9087640832880048		[learning rate: 0.0064456]
	Learning Rate: 0.0064456
	LOSS [training: 0.9087640832880048 | validation: 1.238913867062613]
	TIME [epoch: 9.65 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6706614252495228		[learning rate: 0.0064222]
	Learning Rate: 0.00642221
	LOSS [training: 0.6706614252495228 | validation: 1.02448483955433]
	TIME [epoch: 9.68 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6816227408234778		[learning rate: 0.0063989]
	Learning Rate: 0.0063989
	LOSS [training: 0.6816227408234778 | validation: 1.3472605284397128]
	TIME [epoch: 9.67 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7674713609291182		[learning rate: 0.0063757]
	Learning Rate: 0.00637568
	LOSS [training: 0.7674713609291182 | validation: 1.0213564994374804]
	TIME [epoch: 9.66 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6279011639298642		[learning rate: 0.0063525]
	Learning Rate: 0.00635254
	LOSS [training: 0.6279011639298642 | validation: 1.3993489092166322]
	TIME [epoch: 9.66 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7909645007705537		[learning rate: 0.0063295]
	Learning Rate: 0.00632949
	LOSS [training: 0.7909645007705537 | validation: 0.8775313700182639]
	TIME [epoch: 9.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6068372341639117		[learning rate: 0.0063065]
	Learning Rate: 0.00630652
	LOSS [training: 0.6068372341639117 | validation: 1.2772281858479084]
	TIME [epoch: 9.69 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6745387471851162		[learning rate: 0.0062836]
	Learning Rate: 0.00628363
	LOSS [training: 0.6745387471851162 | validation: 0.8316215377299088]
	TIME [epoch: 9.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6019109670759304		[learning rate: 0.0062608]
	Learning Rate: 0.00626082
	LOSS [training: 0.6019109670759304 | validation: 0.9456627480012838]
	TIME [epoch: 9.69 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5887343185791492		[learning rate: 0.0062381]
	Learning Rate: 0.0062381
	LOSS [training: 0.5887343185791492 | validation: 1.2131413457016833]
	TIME [epoch: 9.71 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7527899818993137		[learning rate: 0.0062155]
	Learning Rate: 0.00621547
	LOSS [training: 0.7527899818993137 | validation: 1.1252100520831412]
	TIME [epoch: 9.7 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6766128698721208		[learning rate: 0.0061929]
	Learning Rate: 0.00619291
	LOSS [training: 0.6766128698721208 | validation: 1.0168424268247518]
	TIME [epoch: 9.68 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6817376245732276		[learning rate: 0.0061704]
	Learning Rate: 0.00617043
	LOSS [training: 0.6817376245732276 | validation: 1.0877696373811627]
	TIME [epoch: 9.69 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6451353862153113		[learning rate: 0.006148]
	Learning Rate: 0.00614804
	LOSS [training: 0.6451353862153113 | validation: 0.9832661950161159]
	TIME [epoch: 9.7 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.696232700744415		[learning rate: 0.0061257]
	Learning Rate: 0.00612573
	LOSS [training: 0.696232700744415 | validation: 1.2217085095714921]
	TIME [epoch: 9.69 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6203959910968034		[learning rate: 0.0061035]
	Learning Rate: 0.0061035
	LOSS [training: 0.6203959910968034 | validation: 0.8734328967268578]
	TIME [epoch: 9.67 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6635447337380918		[learning rate: 0.0060814]
	Learning Rate: 0.00608135
	LOSS [training: 0.6635447337380918 | validation: 1.1416046040479457]
	TIME [epoch: 9.69 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6167832492130523		[learning rate: 0.0060593]
	Learning Rate: 0.00605928
	LOSS [training: 0.6167832492130523 | validation: 0.8997711436267606]
	TIME [epoch: 9.69 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6163582476624146		[learning rate: 0.0060373]
	Learning Rate: 0.00603729
	LOSS [training: 0.6163582476624146 | validation: 0.9955470193838033]
	TIME [epoch: 9.69 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6411587144320227		[learning rate: 0.0060154]
	Learning Rate: 0.00601538
	LOSS [training: 0.6411587144320227 | validation: 1.0731262455860855]
	TIME [epoch: 9.69 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8401478671552651		[learning rate: 0.0059936]
	Learning Rate: 0.00599355
	LOSS [training: 0.8401478671552651 | validation: 1.0089155019965395]
	TIME [epoch: 9.72 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7502307928624636		[learning rate: 0.0059718]
	Learning Rate: 0.0059718
	LOSS [training: 0.7502307928624636 | validation: 0.9968919624916487]
	TIME [epoch: 9.69 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6343834036419158		[learning rate: 0.0059501]
	Learning Rate: 0.00595013
	LOSS [training: 0.6343834036419158 | validation: 0.9219670286769192]
	TIME [epoch: 9.69 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5814586172396169		[learning rate: 0.0059285]
	Learning Rate: 0.00592853
	LOSS [training: 0.5814586172396169 | validation: 1.012560224824849]
	TIME [epoch: 9.69 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7071923386723598		[learning rate: 0.005907]
	Learning Rate: 0.00590702
	LOSS [training: 0.7071923386723598 | validation: 1.0064894591783722]
	TIME [epoch: 9.71 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.726720856971526		[learning rate: 0.0058856]
	Learning Rate: 0.00588558
	LOSS [training: 0.726720856971526 | validation: 1.0689590929784278]
	TIME [epoch: 9.69 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6465884796132895		[learning rate: 0.0058642]
	Learning Rate: 0.00586422
	LOSS [training: 0.6465884796132895 | validation: 0.8543951395511522]
	TIME [epoch: 9.69 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6261377695658357		[learning rate: 0.0058429]
	Learning Rate: 0.00584294
	LOSS [training: 0.6261377695658357 | validation: 0.9358984123786027]
	TIME [epoch: 9.69 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6349159820643384		[learning rate: 0.0058217]
	Learning Rate: 0.00582174
	LOSS [training: 0.6349159820643384 | validation: 0.9792752222679064]
	TIME [epoch: 9.7 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6177755790400314		[learning rate: 0.0058006]
	Learning Rate: 0.00580061
	LOSS [training: 0.6177755790400314 | validation: 1.177309327072814]
	TIME [epoch: 9.69 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8233168501908548		[learning rate: 0.0057796]
	Learning Rate: 0.00577956
	LOSS [training: 0.8233168501908548 | validation: 1.1686718772220401]
	TIME [epoch: 9.69 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7339653293465382		[learning rate: 0.0057586]
	Learning Rate: 0.00575859
	LOSS [training: 0.7339653293465382 | validation: 1.0673135194095762]
	TIME [epoch: 9.71 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5863669573717998		[learning rate: 0.0057377]
	Learning Rate: 0.00573769
	LOSS [training: 0.5863669573717998 | validation: 1.1334631227727412]
	TIME [epoch: 9.69 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6304554786768902		[learning rate: 0.0057169]
	Learning Rate: 0.00571686
	LOSS [training: 0.6304554786768902 | validation: 1.2976350133230778]
	TIME [epoch: 9.68 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6604769366414257		[learning rate: 0.0056961]
	Learning Rate: 0.00569612
	LOSS [training: 0.6604769366414257 | validation: 0.8813261115024323]
	TIME [epoch: 9.68 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6127839090064882		[learning rate: 0.0056754]
	Learning Rate: 0.00567545
	LOSS [training: 0.6127839090064882 | validation: 1.0161120250995572]
	TIME [epoch: 9.71 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7105550796806847		[learning rate: 0.0056548]
	Learning Rate: 0.00565485
	LOSS [training: 0.7105550796806847 | validation: 1.186222304303043]
	TIME [epoch: 9.68 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7555091903032588		[learning rate: 0.0056343]
	Learning Rate: 0.00563433
	LOSS [training: 0.7555091903032588 | validation: 1.0283498289589492]
	TIME [epoch: 9.69 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.616338655554553		[learning rate: 0.0056139]
	Learning Rate: 0.00561388
	LOSS [training: 0.616338655554553 | validation: 1.0365413243212453]
	TIME [epoch: 9.69 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8918272406642543		[learning rate: 0.0055935]
	Learning Rate: 0.00559351
	LOSS [training: 0.8918272406642543 | validation: 1.195803585420714]
	TIME [epoch: 9.7 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.829424009386981		[learning rate: 0.0055732]
	Learning Rate: 0.00557321
	LOSS [training: 0.829424009386981 | validation: 0.9711071029974413]
	TIME [epoch: 9.68 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5873315459963311		[learning rate: 0.005553]
	Learning Rate: 0.00555298
	LOSS [training: 0.5873315459963311 | validation: 0.9126204002347639]
	TIME [epoch: 9.68 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6938693402430747		[learning rate: 0.0055328]
	Learning Rate: 0.00553283
	LOSS [training: 0.6938693402430747 | validation: 0.9315966101787708]
	TIME [epoch: 9.69 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6164462105932884		[learning rate: 0.0055128]
	Learning Rate: 0.00551275
	LOSS [training: 0.6164462105932884 | validation: 1.198228379097086]
	TIME [epoch: 9.68 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6231464382610146		[learning rate: 0.0054927]
	Learning Rate: 0.00549274
	LOSS [training: 0.6231464382610146 | validation: 0.9626280514372223]
	TIME [epoch: 9.67 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5741362024182983		[learning rate: 0.0054728]
	Learning Rate: 0.00547281
	LOSS [training: 0.5741362024182983 | validation: 1.1845886530140786]
	TIME [epoch: 9.67 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5616746390223757		[learning rate: 0.005453]
	Learning Rate: 0.00545295
	LOSS [training: 1.5616746390223757 | validation: 1.0829476317211921]
	TIME [epoch: 9.7 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.619545466831157		[learning rate: 0.0054332]
	Learning Rate: 0.00543316
	LOSS [training: 0.619545466831157 | validation: 0.9120564919701878]
	TIME [epoch: 9.68 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5818063434749157		[learning rate: 0.0054134]
	Learning Rate: 0.00541344
	LOSS [training: 0.5818063434749157 | validation: 0.8752433679089461]
	TIME [epoch: 9.68 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.551721857547726		[learning rate: 0.0053938]
	Learning Rate: 0.0053938
	LOSS [training: 0.551721857547726 | validation: 1.1518756448420735]
	TIME [epoch: 9.68 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6174821210284256		[learning rate: 0.0053742]
	Learning Rate: 0.00537422
	LOSS [training: 0.6174821210284256 | validation: 0.9336229068248679]
	TIME [epoch: 9.71 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5860675647199194		[learning rate: 0.0053547]
	Learning Rate: 0.00535472
	LOSS [training: 0.5860675647199194 | validation: 1.0130526800400694]
	TIME [epoch: 9.69 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7509859345086027		[learning rate: 0.0053353]
	Learning Rate: 0.00533529
	LOSS [training: 0.7509859345086027 | validation: 0.8785361682975216]
	TIME [epoch: 9.68 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6762405199590139		[learning rate: 0.0053159]
	Learning Rate: 0.00531593
	LOSS [training: 0.6762405199590139 | validation: 1.0855323509491555]
	TIME [epoch: 9.69 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6225739567206074		[learning rate: 0.0052966]
	Learning Rate: 0.00529663
	LOSS [training: 0.6225739567206074 | validation: 1.0614923643080243]
	TIME [epoch: 9.7 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6124649478999347		[learning rate: 0.0052774]
	Learning Rate: 0.00527741
	LOSS [training: 0.6124649478999347 | validation: 1.1764398801038616]
	TIME [epoch: 9.69 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5772341347014166		[learning rate: 0.0052583]
	Learning Rate: 0.00525826
	LOSS [training: 0.5772341347014166 | validation: 0.884669073913271]
	TIME [epoch: 9.69 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5988285506819668		[learning rate: 0.0052392]
	Learning Rate: 0.00523918
	LOSS [training: 0.5988285506819668 | validation: 0.8839949061331086]
	TIME [epoch: 9.69 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6107442914354897		[learning rate: 0.0052202]
	Learning Rate: 0.00522017
	LOSS [training: 0.6107442914354897 | validation: 0.9664930699154023]
	TIME [epoch: 9.68 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6067857680605592		[learning rate: 0.0052012]
	Learning Rate: 0.00520122
	LOSS [training: 0.6067857680605592 | validation: 0.8985612382526271]
	TIME [epoch: 9.69 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5701569930051452		[learning rate: 0.0051823]
	Learning Rate: 0.00518234
	LOSS [training: 0.5701569930051452 | validation: 0.9347131452414729]
	TIME [epoch: 9.69 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7976214566351798		[learning rate: 0.0051635]
	Learning Rate: 0.00516354
	LOSS [training: 0.7976214566351798 | validation: 1.0799127595386842]
	TIME [epoch: 9.72 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6151020070046738		[learning rate: 0.0051448]
	Learning Rate: 0.0051448
	LOSS [training: 0.6151020070046738 | validation: 0.913895040458466]
	TIME [epoch: 9.69 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5798773087885528		[learning rate: 0.0051261]
	Learning Rate: 0.00512613
	LOSS [training: 0.5798773087885528 | validation: 1.4081088623668245]
	TIME [epoch: 9.69 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7196925637814455		[learning rate: 0.0051075]
	Learning Rate: 0.00510753
	LOSS [training: 0.7196925637814455 | validation: 0.8337154901892572]
	TIME [epoch: 9.68 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7237911889559869		[learning rate: 0.005089]
	Learning Rate: 0.00508899
	LOSS [training: 0.7237911889559869 | validation: 2.42344430751522]
	TIME [epoch: 9.72 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1272765500316135		[learning rate: 0.0050705]
	Learning Rate: 0.00507052
	LOSS [training: 1.1272765500316135 | validation: 0.9607458182471363]
	TIME [epoch: 9.69 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6152217217550318		[learning rate: 0.0050521]
	Learning Rate: 0.00505212
	LOSS [training: 0.6152217217550318 | validation: 0.9166778052192959]
	TIME [epoch: 9.69 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.555398527419474		[learning rate: 0.0050338]
	Learning Rate: 0.00503379
	LOSS [training: 0.555398527419474 | validation: 0.7596793915850671]
	TIME [epoch: 9.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6000084489005605		[learning rate: 0.0050155]
	Learning Rate: 0.00501552
	LOSS [training: 0.6000084489005605 | validation: 1.0462379665710415]
	TIME [epoch: 9.7 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.684563339806658		[learning rate: 0.0049973]
	Learning Rate: 0.00499732
	LOSS [training: 0.684563339806658 | validation: 0.9283895349938971]
	TIME [epoch: 9.68 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6302362640507607		[learning rate: 0.0049792]
	Learning Rate: 0.00497918
	LOSS [training: 0.6302362640507607 | validation: 0.8994684148518336]
	TIME [epoch: 9.68 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5761153843239948		[learning rate: 0.0049611]
	Learning Rate: 0.00496111
	LOSS [training: 0.5761153843239948 | validation: 0.8521132809640699]
	TIME [epoch: 9.7 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6475143122542052		[learning rate: 0.0049431]
	Learning Rate: 0.00494311
	LOSS [training: 0.6475143122542052 | validation: 1.054704905699046]
	TIME [epoch: 9.68 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6735425086261969		[learning rate: 0.0049252]
	Learning Rate: 0.00492517
	LOSS [training: 0.6735425086261969 | validation: 0.8746211041061709]
	TIME [epoch: 9.68 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.527730579745282		[learning rate: 0.0049073]
	Learning Rate: 0.00490729
	LOSS [training: 0.527730579745282 | validation: 0.8424564013243804]
	TIME [epoch: 9.68 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6346079662477699		[learning rate: 0.0048895]
	Learning Rate: 0.00488948
	LOSS [training: 0.6346079662477699 | validation: 0.879337258531655]
	TIME [epoch: 9.7 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5909251403134841		[learning rate: 0.0048717]
	Learning Rate: 0.00487174
	LOSS [training: 0.5909251403134841 | validation: 0.9532113946369924]
	TIME [epoch: 9.68 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6597340239582368		[learning rate: 0.0048541]
	Learning Rate: 0.00485406
	LOSS [training: 0.6597340239582368 | validation: 1.1477542600237787]
	TIME [epoch: 9.68 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6385256871622588		[learning rate: 0.0048364]
	Learning Rate: 0.00483645
	LOSS [training: 0.6385256871622588 | validation: 0.8914070036700844]
	TIME [epoch: 9.68 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4966706558542917		[learning rate: 0.0048189]
	Learning Rate: 0.00481889
	LOSS [training: 0.4966706558542917 | validation: 1.0853998278287238]
	TIME [epoch: 9.69 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5877645883731448		[learning rate: 0.0048014]
	Learning Rate: 0.00480141
	LOSS [training: 0.5877645883731448 | validation: 0.996352328067708]
	TIME [epoch: 9.67 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5789885876439499		[learning rate: 0.004784]
	Learning Rate: 0.00478398
	LOSS [training: 0.5789885876439499 | validation: 0.9649915152784868]
	TIME [epoch: 9.68 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7418916150828851		[learning rate: 0.0047666]
	Learning Rate: 0.00476662
	LOSS [training: 0.7418916150828851 | validation: 1.0864330306311578]
	TIME [epoch: 9.68 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5755852247504336		[learning rate: 0.0047493]
	Learning Rate: 0.00474932
	LOSS [training: 0.5755852247504336 | validation: 0.9939997889501799]
	TIME [epoch: 9.69 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5664692196723247		[learning rate: 0.0047321]
	Learning Rate: 0.00473209
	LOSS [training: 0.5664692196723247 | validation: 0.8638898719717449]
	TIME [epoch: 9.67 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5781255204115568		[learning rate: 0.0047149]
	Learning Rate: 0.00471491
	LOSS [training: 0.5781255204115568 | validation: 0.963519696962193]
	TIME [epoch: 9.66 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6838221441857658		[learning rate: 0.0046978]
	Learning Rate: 0.0046978
	LOSS [training: 0.6838221441857658 | validation: 0.9085541401909148]
	TIME [epoch: 9.7 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6739001649803864		[learning rate: 0.0046808]
	Learning Rate: 0.00468075
	LOSS [training: 0.6739001649803864 | validation: 1.2563382973422408]
	TIME [epoch: 9.68 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6227260655870431		[learning rate: 0.0046638]
	Learning Rate: 0.00466377
	LOSS [training: 0.6227260655870431 | validation: 1.0420996107441893]
	TIME [epoch: 9.67 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5303671448098962		[learning rate: 0.0046468]
	Learning Rate: 0.00464684
	LOSS [training: 0.5303671448098962 | validation: 0.9532775656213167]
	TIME [epoch: 9.69 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6168183105794027		[learning rate: 0.00463]
	Learning Rate: 0.00462998
	LOSS [training: 0.6168183105794027 | validation: 1.1100541197264808]
	TIME [epoch: 9.7 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6383120674632295		[learning rate: 0.0046132]
	Learning Rate: 0.00461318
	LOSS [training: 0.6383120674632295 | validation: 0.8704292387275644]
	TIME [epoch: 9.67 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6457963533340617		[learning rate: 0.0045964]
	Learning Rate: 0.00459643
	LOSS [training: 0.6457963533340617 | validation: 1.030625203733057]
	TIME [epoch: 9.68 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5782290473325971		[learning rate: 0.0045798]
	Learning Rate: 0.00457975
	LOSS [training: 0.5782290473325971 | validation: 1.2273710723091977]
	TIME [epoch: 9.69 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6743820231063709		[learning rate: 0.0045631]
	Learning Rate: 0.00456313
	LOSS [training: 0.6743820231063709 | validation: 1.5787857386228723]
	TIME [epoch: 9.69 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7501126200899629		[learning rate: 0.0045466]
	Learning Rate: 0.00454657
	LOSS [training: 0.7501126200899629 | validation: 0.897923720046977]
	TIME [epoch: 9.68 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5017438510447401		[learning rate: 0.0045301]
	Learning Rate: 0.00453007
	LOSS [training: 0.5017438510447401 | validation: 1.0139882011317196]
	TIME [epoch: 9.68 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6048003633949433		[learning rate: 0.0045136]
	Learning Rate: 0.00451363
	LOSS [training: 0.6048003633949433 | validation: 0.9246541142465671]
	TIME [epoch: 9.69 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6567704683534378		[learning rate: 0.0044973]
	Learning Rate: 0.00449725
	LOSS [training: 0.6567704683534378 | validation: 1.0400079679654954]
	TIME [epoch: 9.67 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.600676184871637		[learning rate: 0.0044809]
	Learning Rate: 0.00448093
	LOSS [training: 0.600676184871637 | validation: 0.8132261525134443]
	TIME [epoch: 9.67 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5990904329402418		[learning rate: 0.0044647]
	Learning Rate: 0.00446467
	LOSS [training: 0.5990904329402418 | validation: 1.0383442956133708]
	TIME [epoch: 9.66 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6185279420324261		[learning rate: 0.0044485]
	Learning Rate: 0.00444847
	LOSS [training: 0.6185279420324261 | validation: 0.9144968928402716]
	TIME [epoch: 9.7 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5420833761444814		[learning rate: 0.0044323]
	Learning Rate: 0.00443232
	LOSS [training: 0.5420833761444814 | validation: 0.8173244307514145]
	TIME [epoch: 9.66 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5807995849655742		[learning rate: 0.0044162]
	Learning Rate: 0.00441624
	LOSS [training: 0.5807995849655742 | validation: 0.918563947082647]
	TIME [epoch: 9.67 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5827992043669142		[learning rate: 0.0044002]
	Learning Rate: 0.00440021
	LOSS [training: 0.5827992043669142 | validation: 0.9578300397855362]
	TIME [epoch: 9.67 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6295022391042424		[learning rate: 0.0043842]
	Learning Rate: 0.00438424
	LOSS [training: 0.6295022391042424 | validation: 0.7560104189582757]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5320544171896111		[learning rate: 0.0043683]
	Learning Rate: 0.00436833
	LOSS [training: 0.5320544171896111 | validation: 0.788719854995781]
	TIME [epoch: 9.98 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5034900808431322		[learning rate: 0.0043525]
	Learning Rate: 0.00435248
	LOSS [training: 0.5034900808431322 | validation: 0.9479290652576401]
	TIME [epoch: 9.69 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5227676241679091		[learning rate: 0.0043367]
	Learning Rate: 0.00433668
	LOSS [training: 0.5227676241679091 | validation: 0.9171762229314107]
	TIME [epoch: 9.7 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.709723649010307		[learning rate: 0.0043209]
	Learning Rate: 0.00432095
	LOSS [training: 0.709723649010307 | validation: 0.9947868812560096]
	TIME [epoch: 9.7 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5154509625088961		[learning rate: 0.0043053]
	Learning Rate: 0.00430527
	LOSS [training: 0.5154509625088961 | validation: 0.8955259960456643]
	TIME [epoch: 9.68 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4902635433885399		[learning rate: 0.0042896]
	Learning Rate: 0.00428964
	LOSS [training: 0.4902635433885399 | validation: 0.9567709540251409]
	TIME [epoch: 9.69 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5258780554894528		[learning rate: 0.0042741]
	Learning Rate: 0.00427407
	LOSS [training: 0.5258780554894528 | validation: 0.8548465199984862]
	TIME [epoch: 9.7 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6586243658070514		[learning rate: 0.0042586]
	Learning Rate: 0.00425856
	LOSS [training: 0.6586243658070514 | validation: 0.7809897857478779]
	TIME [epoch: 9.69 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6135227878488931		[learning rate: 0.0042431]
	Learning Rate: 0.00424311
	LOSS [training: 0.6135227878488931 | validation: 1.1658174021916452]
	TIME [epoch: 9.68 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6063428410650864		[learning rate: 0.0042277]
	Learning Rate: 0.00422771
	LOSS [training: 0.6063428410650864 | validation: 0.8221511901053375]
	TIME [epoch: 9.69 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5436855148721946		[learning rate: 0.0042124]
	Learning Rate: 0.00421237
	LOSS [training: 0.5436855148721946 | validation: 0.8517335538476251]
	TIME [epoch: 9.7 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6152990545574321		[learning rate: 0.0041971]
	Learning Rate: 0.00419708
	LOSS [training: 0.6152990545574321 | validation: 1.2546825492696392]
	TIME [epoch: 9.69 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7074008734165173		[learning rate: 0.0041818]
	Learning Rate: 0.00418185
	LOSS [training: 0.7074008734165173 | validation: 0.8958463702460324]
	TIME [epoch: 9.68 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5055460527873502		[learning rate: 0.0041667]
	Learning Rate: 0.00416667
	LOSS [training: 0.5055460527873502 | validation: 0.8720479092299158]
	TIME [epoch: 9.69 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5404833699350926		[learning rate: 0.0041516]
	Learning Rate: 0.00415155
	LOSS [training: 0.5404833699350926 | validation: 0.7403463302358112]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6069455463070479		[learning rate: 0.0041365]
	Learning Rate: 0.00413649
	LOSS [training: 0.6069455463070479 | validation: 0.9000920137156269]
	TIME [epoch: 9.69 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4938216447463789		[learning rate: 0.0041215]
	Learning Rate: 0.00412147
	LOSS [training: 0.4938216447463789 | validation: 0.896006312036513]
	TIME [epoch: 9.68 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5184959127974987		[learning rate: 0.0041065]
	Learning Rate: 0.00410652
	LOSS [training: 0.5184959127974987 | validation: 0.9091733841065164]
	TIME [epoch: 9.71 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5006906191711196		[learning rate: 0.0040916]
	Learning Rate: 0.00409161
	LOSS [training: 0.5006906191711196 | validation: 0.8758954505720808]
	TIME [epoch: 9.68 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.601974164785123		[learning rate: 0.0040768]
	Learning Rate: 0.00407677
	LOSS [training: 0.601974164785123 | validation: 0.9806295169850532]
	TIME [epoch: 9.68 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5252708062037641		[learning rate: 0.004062]
	Learning Rate: 0.00406197
	LOSS [training: 0.5252708062037641 | validation: 0.9066146141820213]
	TIME [epoch: 9.68 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5287027648457979		[learning rate: 0.0040472]
	Learning Rate: 0.00404723
	LOSS [training: 0.5287027648457979 | validation: 0.9042575698538308]
	TIME [epoch: 9.71 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5654234450042842		[learning rate: 0.0040325]
	Learning Rate: 0.00403254
	LOSS [training: 0.5654234450042842 | validation: 0.7684567227505846]
	TIME [epoch: 9.69 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5171552749969265		[learning rate: 0.0040179]
	Learning Rate: 0.00401791
	LOSS [training: 0.5171552749969265 | validation: 0.8762725158141246]
	TIME [epoch: 9.69 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5270030523308575		[learning rate: 0.0040033]
	Learning Rate: 0.00400333
	LOSS [training: 0.5270030523308575 | validation: 0.9666617011343697]
	TIME [epoch: 9.68 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5601452568915931		[learning rate: 0.0039888]
	Learning Rate: 0.0039888
	LOSS [training: 0.5601452568915931 | validation: 0.8609633489474063]
	TIME [epoch: 9.71 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5400976419103214		[learning rate: 0.0039743]
	Learning Rate: 0.00397432
	LOSS [training: 0.5400976419103214 | validation: 0.9314334024071238]
	TIME [epoch: 9.68 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5390103945538621		[learning rate: 0.0039599]
	Learning Rate: 0.0039599
	LOSS [training: 0.5390103945538621 | validation: 0.8384308882870893]
	TIME [epoch: 9.69 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5536424093760324		[learning rate: 0.0039455]
	Learning Rate: 0.00394553
	LOSS [training: 0.5536424093760324 | validation: 0.8785532428347969]
	TIME [epoch: 9.69 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5188976084008963		[learning rate: 0.0039312]
	Learning Rate: 0.00393121
	LOSS [training: 0.5188976084008963 | validation: 0.9760546366487657]
	TIME [epoch: 9.7 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5309130357372291		[learning rate: 0.0039169]
	Learning Rate: 0.00391694
	LOSS [training: 0.5309130357372291 | validation: 1.120746511677069]
	TIME [epoch: 9.68 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6152231705465915		[learning rate: 0.0039027]
	Learning Rate: 0.00390273
	LOSS [training: 0.6152231705465915 | validation: 1.0005963296371752]
	TIME [epoch: 9.68 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6250712652408672		[learning rate: 0.0038886]
	Learning Rate: 0.00388857
	LOSS [training: 0.6250712652408672 | validation: 0.9013690445995469]
	TIME [epoch: 9.7 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5963736776730396		[learning rate: 0.0038745]
	Learning Rate: 0.00387445
	LOSS [training: 0.5963736776730396 | validation: 0.9009630843778179]
	TIME [epoch: 9.69 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5983422513204579		[learning rate: 0.0038604]
	Learning Rate: 0.00386039
	LOSS [training: 0.5983422513204579 | validation: 1.006393278264402]
	TIME [epoch: 9.68 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5840959296329645		[learning rate: 0.0038464]
	Learning Rate: 0.00384638
	LOSS [training: 0.5840959296329645 | validation: 0.8827159105254226]
	TIME [epoch: 9.69 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5090069631386459		[learning rate: 0.0038324]
	Learning Rate: 0.00383242
	LOSS [training: 0.5090069631386459 | validation: 0.7827293415969432]
	TIME [epoch: 9.71 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5441826202308319		[learning rate: 0.0038185]
	Learning Rate: 0.00381852
	LOSS [training: 0.5441826202308319 | validation: 0.9279305390928744]
	TIME [epoch: 9.68 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5051808575105885		[learning rate: 0.0038047]
	Learning Rate: 0.00380466
	LOSS [training: 0.5051808575105885 | validation: 1.1702568927520722]
	TIME [epoch: 9.68 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6865698848029933		[learning rate: 0.0037909]
	Learning Rate: 0.00379085
	LOSS [training: 0.6865698848029933 | validation: 1.1259778993903506]
	TIME [epoch: 9.68 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5461887372389764		[learning rate: 0.0037771]
	Learning Rate: 0.00377709
	LOSS [training: 0.5461887372389764 | validation: 0.9138862348788555]
	TIME [epoch: 9.7 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5444922150564258		[learning rate: 0.0037634]
	Learning Rate: 0.00376339
	LOSS [training: 0.5444922150564258 | validation: 0.9277716753104313]
	TIME [epoch: 9.68 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5280307555998905		[learning rate: 0.0037497]
	Learning Rate: 0.00374973
	LOSS [training: 0.5280307555998905 | validation: 0.8812293357425064]
	TIME [epoch: 9.68 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5033530686015686		[learning rate: 0.0037361]
	Learning Rate: 0.00373612
	LOSS [training: 0.5033530686015686 | validation: 0.8210573912789311]
	TIME [epoch: 9.68 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5144653027393097		[learning rate: 0.0037226]
	Learning Rate: 0.00372256
	LOSS [training: 0.5144653027393097 | validation: 0.7671816680723424]
	TIME [epoch: 9.7 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5405657979891724		[learning rate: 0.0037091]
	Learning Rate: 0.00370905
	LOSS [training: 0.5405657979891724 | validation: 0.8105576014190788]
	TIME [epoch: 9.68 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4998051920055703		[learning rate: 0.0036956]
	Learning Rate: 0.00369559
	LOSS [training: 0.4998051920055703 | validation: 0.8348981704882562]
	TIME [epoch: 9.68 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5575719925437651		[learning rate: 0.0036822]
	Learning Rate: 0.00368218
	LOSS [training: 0.5575719925437651 | validation: 0.9155712215470867]
	TIME [epoch: 9.7 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7312444571368012		[learning rate: 0.0036688]
	Learning Rate: 0.00366882
	LOSS [training: 0.7312444571368012 | validation: 0.9751002017399667]
	TIME [epoch: 9.68 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6187942410610606		[learning rate: 0.0036555]
	Learning Rate: 0.0036555
	LOSS [training: 0.6187942410610606 | validation: 0.8715349882816338]
	TIME [epoch: 9.68 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5668360828335819		[learning rate: 0.0036422]
	Learning Rate: 0.00364224
	LOSS [training: 0.5668360828335819 | validation: 1.0228329832168306]
	TIME [epoch: 9.68 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.566172048704124		[learning rate: 0.003629]
	Learning Rate: 0.00362902
	LOSS [training: 0.566172048704124 | validation: 0.8323368348666711]
	TIME [epoch: 9.71 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5680256470388668		[learning rate: 0.0036159]
	Learning Rate: 0.00361585
	LOSS [training: 0.5680256470388668 | validation: 1.0352233167835119]
	TIME [epoch: 9.67 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5275250553830716		[learning rate: 0.0036027]
	Learning Rate: 0.00360273
	LOSS [training: 0.5275250553830716 | validation: 0.7833064668454899]
	TIME [epoch: 9.68 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5125745138095852		[learning rate: 0.0035897]
	Learning Rate: 0.00358965
	LOSS [training: 0.5125745138095852 | validation: 0.8257356323499196]
	TIME [epoch: 9.68 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5703864128998019		[learning rate: 0.0035766]
	Learning Rate: 0.00357663
	LOSS [training: 0.5703864128998019 | validation: 0.8965434737054563]
	TIME [epoch: 9.7 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8377132850293565		[learning rate: 0.0035636]
	Learning Rate: 0.00356365
	LOSS [training: 0.8377132850293565 | validation: 0.914765590612488]
	TIME [epoch: 9.68 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5543198678214836		[learning rate: 0.0035507]
	Learning Rate: 0.00355072
	LOSS [training: 0.5543198678214836 | validation: 0.7972093215708931]
	TIME [epoch: 9.68 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5184032492724663		[learning rate: 0.0035378]
	Learning Rate: 0.00353783
	LOSS [training: 0.5184032492724663 | validation: 0.8402624477194869]
	TIME [epoch: 9.69 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5722406963602327		[learning rate: 0.003525]
	Learning Rate: 0.00352499
	LOSS [training: 0.5722406963602327 | validation: 0.8032671433157356]
	TIME [epoch: 9.69 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5535663397767808		[learning rate: 0.0035122]
	Learning Rate: 0.0035122
	LOSS [training: 0.5535663397767808 | validation: 0.8024511255114931]
	TIME [epoch: 9.68 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4893563673808955		[learning rate: 0.0034995]
	Learning Rate: 0.00349945
	LOSS [training: 0.4893563673808955 | validation: 0.8422197775940525]
	TIME [epoch: 9.68 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5863223007282821		[learning rate: 0.0034868]
	Learning Rate: 0.00348675
	LOSS [training: 0.5863223007282821 | validation: 0.912669869826298]
	TIME [epoch: 9.69 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.514008019789982		[learning rate: 0.0034741]
	Learning Rate: 0.0034741
	LOSS [training: 0.514008019789982 | validation: 0.815276415343515]
	TIME [epoch: 9.68 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5177863049007809		[learning rate: 0.0034615]
	Learning Rate: 0.00346149
	LOSS [training: 0.5177863049007809 | validation: 0.815886630403189]
	TIME [epoch: 9.68 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5720772290607453		[learning rate: 0.0034489]
	Learning Rate: 0.00344893
	LOSS [training: 0.5720772290607453 | validation: 0.9410251672586096]
	TIME [epoch: 9.67 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5498128678133342		[learning rate: 0.0034364]
	Learning Rate: 0.00343641
	LOSS [training: 0.5498128678133342 | validation: 0.9585935560519131]
	TIME [epoch: 9.7 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.555598432911632		[learning rate: 0.0034239]
	Learning Rate: 0.00342394
	LOSS [training: 0.555598432911632 | validation: 0.9479874153800867]
	TIME [epoch: 9.68 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49684003718384534		[learning rate: 0.0034115]
	Learning Rate: 0.00341152
	LOSS [training: 0.49684003718384534 | validation: 0.8212821288702816]
	TIME [epoch: 9.68 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5030207876942763		[learning rate: 0.0033991]
	Learning Rate: 0.00339914
	LOSS [training: 0.5030207876942763 | validation: 0.8674500272388005]
	TIME [epoch: 9.68 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5562565755031119		[learning rate: 0.0033868]
	Learning Rate: 0.0033868
	LOSS [training: 0.5562565755031119 | validation: 1.0174197942175522]
	TIME [epoch: 9.7 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5621161182134118		[learning rate: 0.0033745]
	Learning Rate: 0.00337451
	LOSS [training: 0.5621161182134118 | validation: 0.7036558562697576]
	TIME [epoch: 9.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5384728963919722		[learning rate: 0.0033623]
	Learning Rate: 0.00336226
	LOSS [training: 0.5384728963919722 | validation: 0.7332234770363718]
	TIME [epoch: 9.67 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47805376107388653		[learning rate: 0.0033501]
	Learning Rate: 0.00335006
	LOSS [training: 0.47805376107388653 | validation: 0.8898045683140754]
	TIME [epoch: 9.7 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.492596480212829		[learning rate: 0.0033379]
	Learning Rate: 0.0033379
	LOSS [training: 0.492596480212829 | validation: 0.7992873029782951]
	TIME [epoch: 9.68 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5446627852612325		[learning rate: 0.0033258]
	Learning Rate: 0.00332579
	LOSS [training: 0.5446627852612325 | validation: 0.8252909166280944]
	TIME [epoch: 9.67 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5065855856873227		[learning rate: 0.0033137]
	Learning Rate: 0.00331372
	LOSS [training: 0.5065855856873227 | validation: 0.9610083405419704]
	TIME [epoch: 9.68 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5555690948471318		[learning rate: 0.0033017]
	Learning Rate: 0.00330169
	LOSS [training: 0.5555690948471318 | validation: 0.8376516055360521]
	TIME [epoch: 9.69 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5195498701001715		[learning rate: 0.0032897]
	Learning Rate: 0.00328971
	LOSS [training: 0.5195498701001715 | validation: 0.7914185503287339]
	TIME [epoch: 9.67 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5623719474724843		[learning rate: 0.0032778]
	Learning Rate: 0.00327777
	LOSS [training: 0.5623719474724843 | validation: 0.858391483315159]
	TIME [epoch: 9.68 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5098293030000431		[learning rate: 0.0032659]
	Learning Rate: 0.00326588
	LOSS [training: 0.5098293030000431 | validation: 1.1009014487712827]
	TIME [epoch: 9.68 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6075698506619231		[learning rate: 0.003254]
	Learning Rate: 0.00325403
	LOSS [training: 0.6075698506619231 | validation: 0.7987478840495171]
	TIME [epoch: 9.7 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5159021785052635		[learning rate: 0.0032422]
	Learning Rate: 0.00324222
	LOSS [training: 0.5159021785052635 | validation: 0.9084513807941024]
	TIME [epoch: 9.67 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5256148372813124		[learning rate: 0.0032305]
	Learning Rate: 0.00323045
	LOSS [training: 0.5256148372813124 | validation: 1.0003185691189527]
	TIME [epoch: 9.67 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.604577446487643		[learning rate: 0.0032187]
	Learning Rate: 0.00321873
	LOSS [training: 0.604577446487643 | validation: 1.0748161072263154]
	TIME [epoch: 9.68 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5777265128583954		[learning rate: 0.003207]
	Learning Rate: 0.00320705
	LOSS [training: 0.5777265128583954 | validation: 0.8953651895685871]
	TIME [epoch: 9.69 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5004134274675524		[learning rate: 0.0031954]
	Learning Rate: 0.00319541
	LOSS [training: 0.5004134274675524 | validation: 0.8804958801866548]
	TIME [epoch: 9.67 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5304784122866285		[learning rate: 0.0031838]
	Learning Rate: 0.00318381
	LOSS [training: 0.5304784122866285 | validation: 0.9005595658740813]
	TIME [epoch: 9.68 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5255903785515734		[learning rate: 0.0031723]
	Learning Rate: 0.00317226
	LOSS [training: 0.5255903785515734 | validation: 0.8021017073897266]
	TIME [epoch: 9.69 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49165158424270305		[learning rate: 0.0031607]
	Learning Rate: 0.00316075
	LOSS [training: 0.49165158424270305 | validation: 0.8718119481765245]
	TIME [epoch: 9.68 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8724711334424056		[learning rate: 0.0031493]
	Learning Rate: 0.00314927
	LOSS [training: 0.8724711334424056 | validation: 0.932433444896007]
	TIME [epoch: 9.68 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6342219160645547		[learning rate: 0.0031378]
	Learning Rate: 0.00313785
	LOSS [training: 0.6342219160645547 | validation: 0.871221871867372]
	TIME [epoch: 9.68 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5156905194444794		[learning rate: 0.0031265]
	Learning Rate: 0.00312646
	LOSS [training: 0.5156905194444794 | validation: 0.7956990584191344]
	TIME [epoch: 9.7 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7288515311024111		[learning rate: 0.0031151]
	Learning Rate: 0.00311511
	LOSS [training: 0.7288515311024111 | validation: 0.7824911833631923]
	TIME [epoch: 9.68 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4790763227274196		[learning rate: 0.0031038]
	Learning Rate: 0.00310381
	LOSS [training: 0.4790763227274196 | validation: 0.813834617281715]
	TIME [epoch: 9.67 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45581218554854086		[learning rate: 0.0030925]
	Learning Rate: 0.00309254
	LOSS [training: 0.45581218554854086 | validation: 0.9354030791919228]
	TIME [epoch: 9.68 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6394557552844702		[learning rate: 0.0030813]
	Learning Rate: 0.00308132
	LOSS [training: 0.6394557552844702 | validation: 0.8170932146240034]
	TIME [epoch: 9.7 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49444750701459156		[learning rate: 0.0030701]
	Learning Rate: 0.00307014
	LOSS [training: 0.49444750701459156 | validation: 0.8685398537957549]
	TIME [epoch: 9.68 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5038001723993721		[learning rate: 0.003059]
	Learning Rate: 0.003059
	LOSS [training: 0.5038001723993721 | validation: 0.8905629321663332]
	TIME [epoch: 9.68 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5808574633911462		[learning rate: 0.0030479]
	Learning Rate: 0.0030479
	LOSS [training: 0.5808574633911462 | validation: 0.975277105778727]
	TIME [epoch: 9.68 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5436918511167679		[learning rate: 0.0030368]
	Learning Rate: 0.00303683
	LOSS [training: 0.5436918511167679 | validation: 0.8610239733230671]
	TIME [epoch: 9.7 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4972860392876945		[learning rate: 0.0030258]
	Learning Rate: 0.00302581
	LOSS [training: 0.4972860392876945 | validation: 0.8768938499117189]
	TIME [epoch: 9.68 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4615175496806894		[learning rate: 0.0030148]
	Learning Rate: 0.00301483
	LOSS [training: 0.4615175496806894 | validation: 1.2072819911157162]
	TIME [epoch: 9.68 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7423197262338219		[learning rate: 0.0030039]
	Learning Rate: 0.00300389
	LOSS [training: 0.7423197262338219 | validation: 0.8209087935146526]
	TIME [epoch: 9.7 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5420050689069692		[learning rate: 0.002993]
	Learning Rate: 0.00299299
	LOSS [training: 0.5420050689069692 | validation: 0.9006669881323728]
	TIME [epoch: 9.68 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5048080266859161		[learning rate: 0.0029821]
	Learning Rate: 0.00298213
	LOSS [training: 0.5048080266859161 | validation: 0.7732766123591814]
	TIME [epoch: 9.67 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5078534856007841		[learning rate: 0.0029713]
	Learning Rate: 0.00297131
	LOSS [training: 0.5078534856007841 | validation: 0.7889682745341321]
	TIME [epoch: 9.67 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.554300528020262		[learning rate: 0.0029605]
	Learning Rate: 0.00296052
	LOSS [training: 0.554300528020262 | validation: 1.2358698806292747]
	TIME [epoch: 9.7 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6290020695456736		[learning rate: 0.0029498]
	Learning Rate: 0.00294978
	LOSS [training: 0.6290020695456736 | validation: 0.9682232330756742]
	TIME [epoch: 9.68 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5073725516106731		[learning rate: 0.0029391]
	Learning Rate: 0.00293907
	LOSS [training: 0.5073725516106731 | validation: 0.864132994600879]
	TIME [epoch: 9.68 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43519669511550757		[learning rate: 0.0029284]
	Learning Rate: 0.00292841
	LOSS [training: 0.43519669511550757 | validation: 0.7736901426381616]
	TIME [epoch: 9.68 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5437364323360917		[learning rate: 0.0029178]
	Learning Rate: 0.00291778
	LOSS [training: 0.5437364323360917 | validation: 0.9017973264543675]
	TIME [epoch: 9.7 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46864631136724116		[learning rate: 0.0029072]
	Learning Rate: 0.00290719
	LOSS [training: 0.46864631136724116 | validation: 0.7983284918612598]
	TIME [epoch: 9.68 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5616165042444231		[learning rate: 0.0028966]
	Learning Rate: 0.00289664
	LOSS [training: 0.5616165042444231 | validation: 0.7557699475625256]
	TIME [epoch: 9.67 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49501198825507736		[learning rate: 0.0028861]
	Learning Rate: 0.00288613
	LOSS [training: 0.49501198825507736 | validation: 0.8199841519887534]
	TIME [epoch: 9.7 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4542198474250193		[learning rate: 0.0028757]
	Learning Rate: 0.00287566
	LOSS [training: 0.4542198474250193 | validation: 0.9067980446428514]
	TIME [epoch: 9.68 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.488327691650167		[learning rate: 0.0028652]
	Learning Rate: 0.00286522
	LOSS [training: 0.488327691650167 | validation: 0.8820999247708126]
	TIME [epoch: 9.68 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.473742234710696		[learning rate: 0.0028548]
	Learning Rate: 0.00285482
	LOSS [training: 0.473742234710696 | validation: 0.8566092452296505]
	TIME [epoch: 9.68 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4793789652948866		[learning rate: 0.0028445]
	Learning Rate: 0.00284446
	LOSS [training: 0.4793789652948866 | validation: 0.7916888812010717]
	TIME [epoch: 9.7 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5079907400836206		[learning rate: 0.0028341]
	Learning Rate: 0.00283414
	LOSS [training: 0.5079907400836206 | validation: 0.89154936999944]
	TIME [epoch: 9.68 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5028820512525727		[learning rate: 0.0028239]
	Learning Rate: 0.00282385
	LOSS [training: 0.5028820512525727 | validation: 0.7917745508649756]
	TIME [epoch: 9.68 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.498309194993466		[learning rate: 0.0028136]
	Learning Rate: 0.00281361
	LOSS [training: 0.498309194993466 | validation: 0.8555689098676829]
	TIME [epoch: 9.67 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4791635533947349		[learning rate: 0.0028034]
	Learning Rate: 0.00280339
	LOSS [training: 0.4791635533947349 | validation: 0.8645275809523616]
	TIME [epoch: 9.7 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5962403255157267		[learning rate: 0.0027932]
	Learning Rate: 0.00279322
	LOSS [training: 0.5962403255157267 | validation: 0.9428703873173108]
	TIME [epoch: 9.68 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48308677482786333		[learning rate: 0.0027831]
	Learning Rate: 0.00278308
	LOSS [training: 0.48308677482786333 | validation: 0.7801145250822847]
	TIME [epoch: 9.68 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4585612120182304		[learning rate: 0.002773]
	Learning Rate: 0.00277298
	LOSS [training: 0.4585612120182304 | validation: 0.7082753634839432]
	TIME [epoch: 9.68 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47357945233849696		[learning rate: 0.0027629]
	Learning Rate: 0.00276292
	LOSS [training: 0.47357945233849696 | validation: 0.8071090585233875]
	TIME [epoch: 9.7 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4509935068197599		[learning rate: 0.0027529]
	Learning Rate: 0.00275289
	LOSS [training: 0.4509935068197599 | validation: 0.8035081309829383]
	TIME [epoch: 9.68 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4786377564547003		[learning rate: 0.0027429]
	Learning Rate: 0.0027429
	LOSS [training: 0.4786377564547003 | validation: 0.8725860146721007]
	TIME [epoch: 9.68 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5136406992579845		[learning rate: 0.0027329]
	Learning Rate: 0.00273295
	LOSS [training: 0.5136406992579845 | validation: 0.741648582929874]
	TIME [epoch: 9.69 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5111359449965736		[learning rate: 0.002723]
	Learning Rate: 0.00272303
	LOSS [training: 0.5111359449965736 | validation: 0.6870606649110758]
	TIME [epoch: 9.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_458.pth
	Model improved!!!
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5214700970042399		[learning rate: 0.0027131]
	Learning Rate: 0.00271315
	LOSS [training: 0.5214700970042399 | validation: 0.7767105641400178]
	TIME [epoch: 9.68 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4610087192375814		[learning rate: 0.0027033]
	Learning Rate: 0.0027033
	LOSS [training: 0.4610087192375814 | validation: 0.8402443181645495]
	TIME [epoch: 9.68 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5047943333181768		[learning rate: 0.0026935]
	Learning Rate: 0.00269349
	LOSS [training: 0.5047943333181768 | validation: 0.6995313660357984]
	TIME [epoch: 9.7 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48775813119408706		[learning rate: 0.0026837]
	Learning Rate: 0.00268372
	LOSS [training: 0.48775813119408706 | validation: 1.032518055012412]
	TIME [epoch: 9.68 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6010396126609239		[learning rate: 0.002674]
	Learning Rate: 0.00267398
	LOSS [training: 0.6010396126609239 | validation: 0.8173468566800222]
	TIME [epoch: 9.68 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5366975412498667		[learning rate: 0.0026643]
	Learning Rate: 0.00266427
	LOSS [training: 0.5366975412498667 | validation: 0.9646438250768433]
	TIME [epoch: 9.68 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49821269175805394		[learning rate: 0.0026546]
	Learning Rate: 0.00265461
	LOSS [training: 0.49821269175805394 | validation: 0.8481729329989566]
	TIME [epoch: 9.7 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4724344121467907		[learning rate: 0.002645]
	Learning Rate: 0.00264497
	LOSS [training: 0.4724344121467907 | validation: 0.8241237943910531]
	TIME [epoch: 9.67 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45490665110436723		[learning rate: 0.0026354]
	Learning Rate: 0.00263537
	LOSS [training: 0.45490665110436723 | validation: 0.7658863193151909]
	TIME [epoch: 9.67 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4753979271043895		[learning rate: 0.0026258]
	Learning Rate: 0.00262581
	LOSS [training: 0.4753979271043895 | validation: 0.8125139614142826]
	TIME [epoch: 9.68 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49675995856714594		[learning rate: 0.0026163]
	Learning Rate: 0.00261628
	LOSS [training: 0.49675995856714594 | validation: 0.7728893940539419]
	TIME [epoch: 9.7 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.487850168527497		[learning rate: 0.0026068]
	Learning Rate: 0.00260679
	LOSS [training: 0.487850168527497 | validation: 0.8321105482201506]
	TIME [epoch: 9.68 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48046905432609605		[learning rate: 0.0025973]
	Learning Rate: 0.00259733
	LOSS [training: 0.48046905432609605 | validation: 0.7202630086633696]
	TIME [epoch: 9.67 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48805247938148383		[learning rate: 0.0025879]
	Learning Rate: 0.0025879
	LOSS [training: 0.48805247938148383 | validation: 0.9555384715310874]
	TIME [epoch: 9.69 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49463704903583794		[learning rate: 0.0025785]
	Learning Rate: 0.00257851
	LOSS [training: 0.49463704903583794 | validation: 0.7510810853565246]
	TIME [epoch: 9.68 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5784441906867894		[learning rate: 0.0025691]
	Learning Rate: 0.00256915
	LOSS [training: 0.5784441906867894 | validation: 0.8185196073059191]
	TIME [epoch: 9.67 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5090245299773363		[learning rate: 0.0025598]
	Learning Rate: 0.00255983
	LOSS [training: 0.5090245299773363 | validation: 0.9725288013932337]
	TIME [epoch: 9.68 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5342395026464818		[learning rate: 0.0025505]
	Learning Rate: 0.00255054
	LOSS [training: 0.5342395026464818 | validation: 0.7745019203632887]
	TIME [epoch: 9.69 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.469953595300918		[learning rate: 0.0025413]
	Learning Rate: 0.00254128
	LOSS [training: 0.469953595300918 | validation: 0.7634874886730156]
	TIME [epoch: 9.67 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.450865505552214		[learning rate: 0.0025321]
	Learning Rate: 0.00253206
	LOSS [training: 0.450865505552214 | validation: 0.9417127160407571]
	TIME [epoch: 9.67 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4866467815694353		[learning rate: 0.0025229]
	Learning Rate: 0.00252287
	LOSS [training: 0.4866467815694353 | validation: 0.8525369284928681]
	TIME [epoch: 9.67 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43940649422727596		[learning rate: 0.0025137]
	Learning Rate: 0.00251371
	LOSS [training: 0.43940649422727596 | validation: 0.8723272500917842]
	TIME [epoch: 9.7 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5116039882175993		[learning rate: 0.0025046]
	Learning Rate: 0.00250459
	LOSS [training: 0.5116039882175993 | validation: 0.7772686531027355]
	TIME [epoch: 9.68 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6150323269817306		[learning rate: 0.0024955]
	Learning Rate: 0.0024955
	LOSS [training: 0.6150323269817306 | validation: 0.7891883367976931]
	TIME [epoch: 9.67 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5804545807204182		[learning rate: 0.0024864]
	Learning Rate: 0.00248645
	LOSS [training: 0.5804545807204182 | validation: 1.004512186034088]
	TIME [epoch: 9.69 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4758506173098308		[learning rate: 0.0024774]
	Learning Rate: 0.00247742
	LOSS [training: 0.4758506173098308 | validation: 0.6984144715612243]
	TIME [epoch: 9.68 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42933280533313234		[learning rate: 0.0024684]
	Learning Rate: 0.00246843
	LOSS [training: 0.42933280533313234 | validation: 0.8464647672864123]
	TIME [epoch: 9.68 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48396751864374626		[learning rate: 0.0024595]
	Learning Rate: 0.00245947
	LOSS [training: 0.48396751864374626 | validation: 0.8598450259862339]
	TIME [epoch: 9.67 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49159646998632966		[learning rate: 0.0024505]
	Learning Rate: 0.00245055
	LOSS [training: 0.49159646998632966 | validation: 0.8332066141328462]
	TIME [epoch: 9.69 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5246299725688991		[learning rate: 0.0024417]
	Learning Rate: 0.00244165
	LOSS [training: 0.5246299725688991 | validation: 0.8466024523603705]
	TIME [epoch: 9.67 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5493955923949774		[learning rate: 0.0024328]
	Learning Rate: 0.00243279
	LOSS [training: 0.5493955923949774 | validation: 0.896031623081152]
	TIME [epoch: 9.67 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4882929964534406		[learning rate: 0.002424]
	Learning Rate: 0.00242396
	LOSS [training: 0.4882929964534406 | validation: 0.8197861221783825]
	TIME [epoch: 9.67 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6396533459437419		[learning rate: 0.0024152]
	Learning Rate: 0.00241517
	LOSS [training: 0.6396533459437419 | validation: 0.8564694406954497]
	TIME [epoch: 9.7 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48376762768182235		[learning rate: 0.0024064]
	Learning Rate: 0.0024064
	LOSS [training: 0.48376762768182235 | validation: 0.6811531082736879]
	TIME [epoch: 9.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4611554353489109		[learning rate: 0.0023977]
	Learning Rate: 0.00239767
	LOSS [training: 0.4611554353489109 | validation: 0.733259543410945]
	TIME [epoch: 9.68 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4444047891843283		[learning rate: 0.002389]
	Learning Rate: 0.00238897
	LOSS [training: 0.4444047891843283 | validation: 0.788799747929003]
	TIME [epoch: 9.67 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4579746206427271		[learning rate: 0.0023803]
	Learning Rate: 0.0023803
	LOSS [training: 0.4579746206427271 | validation: 0.7056783106697438]
	TIME [epoch: 9.68 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4670989743789867		[learning rate: 0.0023717]
	Learning Rate: 0.00237166
	LOSS [training: 0.4670989743789867 | validation: 0.7777880676260963]
	TIME [epoch: 9.67 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.62599879542525		[learning rate: 0.0023631]
	Learning Rate: 0.00236305
	LOSS [training: 0.62599879542525 | validation: 0.833299752118573]
	TIME [epoch: 9.67 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5769718147583638		[learning rate: 0.0023545]
	Learning Rate: 0.00235448
	LOSS [training: 0.5769718147583638 | validation: 0.8006167354272252]
	TIME [epoch: 9.68 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5353976216287054		[learning rate: 0.0023459]
	Learning Rate: 0.00234593
	LOSS [training: 0.5353976216287054 | validation: 0.6723328107792226]
	TIME [epoch: 9.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_499.pth
	Model improved!!!
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5103943065137964		[learning rate: 0.0023374]
	Learning Rate: 0.00233742
	LOSS [training: 0.5103943065137964 | validation: 0.763889942497392]
	TIME [epoch: 9.68 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46273172678568314		[learning rate: 0.0023289]
	Learning Rate: 0.00232894
	LOSS [training: 0.46273172678568314 | validation: 0.8008993968545787]
	TIME [epoch: 9.67 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4521272064271895		[learning rate: 0.0023205]
	Learning Rate: 0.00232049
	LOSS [training: 0.4521272064271895 | validation: 0.6687182544836864]
	TIME [epoch: 9.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_502.pth
	Model improved!!!
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4302675033948402		[learning rate: 0.0023121]
	Learning Rate: 0.00231206
	LOSS [training: 0.4302675033948402 | validation: 0.6696790215237292]
	TIME [epoch: 9.67 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4619928343985717		[learning rate: 0.0023037]
	Learning Rate: 0.00230367
	LOSS [training: 0.4619928343985717 | validation: 0.8296756712826769]
	TIME [epoch: 9.67 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5354781402930138		[learning rate: 0.0022953]
	Learning Rate: 0.00229531
	LOSS [training: 0.5354781402930138 | validation: 0.7815595498654796]
	TIME [epoch: 9.67 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5008648399629163		[learning rate: 0.002287]
	Learning Rate: 0.00228698
	LOSS [training: 0.5008648399629163 | validation: 0.772035437122796]
	TIME [epoch: 9.69 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4373306263687199		[learning rate: 0.0022787]
	Learning Rate: 0.00227868
	LOSS [training: 0.4373306263687199 | validation: 0.8468294604858326]
	TIME [epoch: 9.67 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5112240673679478		[learning rate: 0.0022704]
	Learning Rate: 0.00227042
	LOSS [training: 0.5112240673679478 | validation: 0.8542215251240137]
	TIME [epoch: 9.67 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4622473796650436		[learning rate: 0.0022622]
	Learning Rate: 0.00226218
	LOSS [training: 0.4622473796650436 | validation: 0.7523295155202305]
	TIME [epoch: 9.69 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47829277482897775		[learning rate: 0.002254]
	Learning Rate: 0.00225397
	LOSS [training: 0.47829277482897775 | validation: 0.9452425250243158]
	TIME [epoch: 9.67 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7063524861538577		[learning rate: 0.0022458]
	Learning Rate: 0.00224579
	LOSS [training: 0.7063524861538577 | validation: 1.0455336191935627]
	TIME [epoch: 9.67 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6085017457940733		[learning rate: 0.0022376]
	Learning Rate: 0.00223764
	LOSS [training: 0.6085017457940733 | validation: 0.6655329641577421]
	TIME [epoch: 9.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5219932158589355		[learning rate: 0.0022295]
	Learning Rate: 0.00222952
	LOSS [training: 0.5219932158589355 | validation: 0.7991162036439734]
	TIME [epoch: 9.69 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.577725998399554		[learning rate: 0.0022214]
	Learning Rate: 0.00222142
	LOSS [training: 0.577725998399554 | validation: 0.7079107377641941]
	TIME [epoch: 9.66 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5423355460098669		[learning rate: 0.0022134]
	Learning Rate: 0.00221336
	LOSS [training: 0.5423355460098669 | validation: 0.5939855458797214]
	TIME [epoch: 9.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_515.pth
	Model improved!!!
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6449068800421417		[learning rate: 0.0022053]
	Learning Rate: 0.00220533
	LOSS [training: 0.6449068800421417 | validation: 0.8145539791930412]
	TIME [epoch: 9.67 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46766960840411		[learning rate: 0.0021973]
	Learning Rate: 0.00219733
	LOSS [training: 0.46766960840411 | validation: 0.7922463320178943]
	TIME [epoch: 9.68 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5523885448189507		[learning rate: 0.0021894]
	Learning Rate: 0.00218935
	LOSS [training: 0.5523885448189507 | validation: 0.6800596284742576]
	TIME [epoch: 9.66 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4946099855578579		[learning rate: 0.0021814]
	Learning Rate: 0.00218141
	LOSS [training: 0.4946099855578579 | validation: 0.7925523624332032]
	TIME [epoch: 9.66 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46893644269905305		[learning rate: 0.0021735]
	Learning Rate: 0.00217349
	LOSS [training: 0.46893644269905305 | validation: 0.8651264117219084]
	TIME [epoch: 9.67 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45528471330594156		[learning rate: 0.0021656]
	Learning Rate: 0.0021656
	LOSS [training: 0.45528471330594156 | validation: 0.801413118227812]
	TIME [epoch: 9.69 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47877964689460056		[learning rate: 0.0021577]
	Learning Rate: 0.00215774
	LOSS [training: 0.47877964689460056 | validation: 0.7535139101746461]
	TIME [epoch: 9.66 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47769664828142605		[learning rate: 0.0021499]
	Learning Rate: 0.00214991
	LOSS [training: 0.47769664828142605 | validation: 0.7352819018575822]
	TIME [epoch: 9.67 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44565974180364576		[learning rate: 0.0021421]
	Learning Rate: 0.00214211
	LOSS [training: 0.44565974180364576 | validation: 0.7469120939146069]
	TIME [epoch: 9.68 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.481895548955665		[learning rate: 0.0021343]
	Learning Rate: 0.00213434
	LOSS [training: 0.481895548955665 | validation: 0.7323418979103425]
	TIME [epoch: 9.67 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5171357831519756		[learning rate: 0.0021266]
	Learning Rate: 0.00212659
	LOSS [training: 0.5171357831519756 | validation: 0.7133055588445149]
	TIME [epoch: 9.66 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5535685851176811		[learning rate: 0.0021189]
	Learning Rate: 0.00211887
	LOSS [training: 0.5535685851176811 | validation: 0.7248584102817672]
	TIME [epoch: 9.66 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4460473881695649		[learning rate: 0.0021112]
	Learning Rate: 0.00211119
	LOSS [training: 0.4460473881695649 | validation: 0.8049104245799965]
	TIME [epoch: 9.69 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43647913141843386		[learning rate: 0.0021035]
	Learning Rate: 0.00210352
	LOSS [training: 0.43647913141843386 | validation: 0.6670460956960735]
	TIME [epoch: 9.66 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8345648118433122		[learning rate: 0.0020959]
	Learning Rate: 0.00209589
	LOSS [training: 0.8345648118433122 | validation: 0.7331620036194596]
	TIME [epoch: 9.67 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5671094890765995		[learning rate: 0.0020883]
	Learning Rate: 0.00208828
	LOSS [training: 0.5671094890765995 | validation: 0.8120845097680518]
	TIME [epoch: 9.66 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47473241044962255		[learning rate: 0.0020807]
	Learning Rate: 0.00208071
	LOSS [training: 0.47473241044962255 | validation: 0.8441835504093581]
	TIME [epoch: 9.69 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49790484759365655		[learning rate: 0.0020732]
	Learning Rate: 0.00207315
	LOSS [training: 0.49790484759365655 | validation: 0.929331539052771]
	TIME [epoch: 9.67 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48275506383048317		[learning rate: 0.0020656]
	Learning Rate: 0.00206563
	LOSS [training: 0.48275506383048317 | validation: 0.864004327086981]
	TIME [epoch: 9.67 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5119937210030084		[learning rate: 0.0020581]
	Learning Rate: 0.00205813
	LOSS [training: 0.5119937210030084 | validation: 0.9864223755584288]
	TIME [epoch: 9.67 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5090782655807586		[learning rate: 0.0020507]
	Learning Rate: 0.00205067
	LOSS [training: 0.5090782655807586 | validation: 0.6923427584441043]
	TIME [epoch: 9.69 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4790093035864128		[learning rate: 0.0020432]
	Learning Rate: 0.00204322
	LOSS [training: 0.4790093035864128 | validation: 0.7878416262007474]
	TIME [epoch: 9.66 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4731763990918365		[learning rate: 0.0020358]
	Learning Rate: 0.00203581
	LOSS [training: 0.4731763990918365 | validation: 0.7320809682446688]
	TIME [epoch: 9.67 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43475519149516		[learning rate: 0.0020284]
	Learning Rate: 0.00202842
	LOSS [training: 0.43475519149516 | validation: 0.9092808127046612]
	TIME [epoch: 9.69 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45786999185143556		[learning rate: 0.0020211]
	Learning Rate: 0.00202106
	LOSS [training: 0.45786999185143556 | validation: 0.8509927070617527]
	TIME [epoch: 9.67 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48579415846676105		[learning rate: 0.0020137]
	Learning Rate: 0.00201372
	LOSS [training: 0.48579415846676105 | validation: 0.9582127718459205]
	TIME [epoch: 9.67 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47133721185140043		[learning rate: 0.0020064]
	Learning Rate: 0.00200642
	LOSS [training: 0.47133721185140043 | validation: 0.8472076936191054]
	TIME [epoch: 9.67 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.587731663757953		[learning rate: 0.0019991]
	Learning Rate: 0.00199913
	LOSS [training: 0.587731663757953 | validation: 0.770425438036967]
	TIME [epoch: 9.69 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45014088550825104		[learning rate: 0.0019919]
	Learning Rate: 0.00199188
	LOSS [training: 0.45014088550825104 | validation: 0.7591332409768707]
	TIME [epoch: 9.67 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4745425530571398		[learning rate: 0.0019847]
	Learning Rate: 0.00198465
	LOSS [training: 0.4745425530571398 | validation: 0.7413907649803716]
	TIME [epoch: 9.67 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6013039498493233		[learning rate: 0.0019774]
	Learning Rate: 0.00197745
	LOSS [training: 0.6013039498493233 | validation: 1.0280246668290995]
	TIME [epoch: 9.66 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6224705804639058		[learning rate: 0.0019703]
	Learning Rate: 0.00197027
	LOSS [training: 0.6224705804639058 | validation: 0.718737228619882]
	TIME [epoch: 9.69 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43496804082074086		[learning rate: 0.0019631]
	Learning Rate: 0.00196312
	LOSS [training: 0.43496804082074086 | validation: 0.8344038318768804]
	TIME [epoch: 9.66 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5114649612770028		[learning rate: 0.001956]
	Learning Rate: 0.001956
	LOSS [training: 0.5114649612770028 | validation: 0.8177480832559073]
	TIME [epoch: 9.67 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5255203687965898		[learning rate: 0.0019489]
	Learning Rate: 0.0019489
	LOSS [training: 0.5255203687965898 | validation: 0.85574315551083]
	TIME [epoch: 9.67 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6811317589270842		[learning rate: 0.0019418]
	Learning Rate: 0.00194183
	LOSS [training: 0.6811317589270842 | validation: 0.8094493955004773]
	TIME [epoch: 9.68 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4860415215608954		[learning rate: 0.0019348]
	Learning Rate: 0.00193478
	LOSS [training: 0.4860415215608954 | validation: 0.9292515495343383]
	TIME [epoch: 9.66 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6150046545489256		[learning rate: 0.0019278]
	Learning Rate: 0.00192776
	LOSS [training: 0.6150046545489256 | validation: 0.9522112168666876]
	TIME [epoch: 9.66 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5040618660140008		[learning rate: 0.0019208]
	Learning Rate: 0.00192076
	LOSS [training: 0.5040618660140008 | validation: 0.6501730060326776]
	TIME [epoch: 9.68 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4632585968894857		[learning rate: 0.0019138]
	Learning Rate: 0.00191379
	LOSS [training: 0.4632585968894857 | validation: 0.6955419094994219]
	TIME [epoch: 9.66 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47103852442069644		[learning rate: 0.0019068]
	Learning Rate: 0.00190685
	LOSS [training: 0.47103852442069644 | validation: 0.6165491208847172]
	TIME [epoch: 9.66 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4605461004157352		[learning rate: 0.0018999]
	Learning Rate: 0.00189993
	LOSS [training: 0.4605461004157352 | validation: 0.7361343734573894]
	TIME [epoch: 9.66 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5656364575393612		[learning rate: 0.001893]
	Learning Rate: 0.00189303
	LOSS [training: 0.5656364575393612 | validation: 0.9180963534072771]
	TIME [epoch: 9.68 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4272735828330384		[learning rate: 0.0018862]
	Learning Rate: 0.00188616
	LOSS [training: 0.4272735828330384 | validation: 0.8242690464325435]
	TIME [epoch: 9.66 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45237656888711564		[learning rate: 0.0018793]
	Learning Rate: 0.00187932
	LOSS [training: 0.45237656888711564 | validation: 1.1393377276973038]
	TIME [epoch: 9.67 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6996028756237844		[learning rate: 0.0018725]
	Learning Rate: 0.0018725
	LOSS [training: 0.6996028756237844 | validation: 0.6677506701762467]
	TIME [epoch: 9.66 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4389689863074052		[learning rate: 0.0018657]
	Learning Rate: 0.0018657
	LOSS [training: 0.4389689863074052 | validation: 0.8174493487778867]
	TIME [epoch: 9.67 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45186818148743796		[learning rate: 0.0018589]
	Learning Rate: 0.00185893
	LOSS [training: 0.45186818148743796 | validation: 0.6511953384149739]
	TIME [epoch: 9.66 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42074128030965036		[learning rate: 0.0018522]
	Learning Rate: 0.00185218
	LOSS [training: 0.42074128030965036 | validation: 0.8311080374742426]
	TIME [epoch: 9.66 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4162133760453834		[learning rate: 0.0018455]
	Learning Rate: 0.00184546
	LOSS [training: 0.4162133760453834 | validation: 0.8160427436433645]
	TIME [epoch: 9.68 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5253072574053371		[learning rate: 0.0018388]
	Learning Rate: 0.00183877
	LOSS [training: 0.5253072574053371 | validation: 0.9759875498638035]
	TIME [epoch: 9.67 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46791864989277077		[learning rate: 0.0018321]
	Learning Rate: 0.00183209
	LOSS [training: 0.46791864989277077 | validation: 0.8100451229473805]
	TIME [epoch: 9.66 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45103333321795		[learning rate: 0.0018254]
	Learning Rate: 0.00182544
	LOSS [training: 0.45103333321795 | validation: 0.7945906211808511]
	TIME [epoch: 9.66 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4646673214878187		[learning rate: 0.0018188]
	Learning Rate: 0.00181882
	LOSS [training: 0.4646673214878187 | validation: 0.7516046933342923]
	TIME [epoch: 9.68 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4411598155448454		[learning rate: 0.0018122]
	Learning Rate: 0.00181222
	LOSS [training: 0.4411598155448454 | validation: 0.7774130936923376]
	TIME [epoch: 9.66 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45803574200257857		[learning rate: 0.0018056]
	Learning Rate: 0.00180564
	LOSS [training: 0.45803574200257857 | validation: 0.6641881140134936]
	TIME [epoch: 9.66 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4403377347115563		[learning rate: 0.0017991]
	Learning Rate: 0.00179909
	LOSS [training: 0.4403377347115563 | validation: 0.6784768492221194]
	TIME [epoch: 9.65 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44087844989850195		[learning rate: 0.0017926]
	Learning Rate: 0.00179256
	LOSS [training: 0.44087844989850195 | validation: 0.7355387420525688]
	TIME [epoch: 9.68 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45799931188363585		[learning rate: 0.0017861]
	Learning Rate: 0.00178605
	LOSS [training: 0.45799931188363585 | validation: 0.7206998484406854]
	TIME [epoch: 9.66 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4409886782319393		[learning rate: 0.0017796]
	Learning Rate: 0.00177957
	LOSS [training: 0.4409886782319393 | validation: 0.7455872855152609]
	TIME [epoch: 9.65 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43866352301807987		[learning rate: 0.0017731]
	Learning Rate: 0.00177311
	LOSS [training: 0.43866352301807987 | validation: 0.7999894527177415]
	TIME [epoch: 9.65 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5951631302874724		[learning rate: 0.0017667]
	Learning Rate: 0.00176668
	LOSS [training: 0.5951631302874724 | validation: 0.6897980028457609]
	TIME [epoch: 9.68 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44132098958196997		[learning rate: 0.0017603]
	Learning Rate: 0.00176027
	LOSS [training: 0.44132098958196997 | validation: 0.6397575444023098]
	TIME [epoch: 9.65 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45826728859947996		[learning rate: 0.0017539]
	Learning Rate: 0.00175388
	LOSS [training: 0.45826728859947996 | validation: 0.7501152132654219]
	TIME [epoch: 9.65 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4968795986795877		[learning rate: 0.0017475]
	Learning Rate: 0.00174752
	LOSS [training: 0.4968795986795877 | validation: 0.7550808916745569]
	TIME [epoch: 9.66 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4193632614300169		[learning rate: 0.0017412]
	Learning Rate: 0.00174117
	LOSS [training: 0.4193632614300169 | validation: 0.8274352209838678]
	TIME [epoch: 9.66 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5045112174356283		[learning rate: 0.0017349]
	Learning Rate: 0.00173486
	LOSS [training: 0.5045112174356283 | validation: 0.7875941024458222]
	TIME [epoch: 9.66 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43991330801944173		[learning rate: 0.0017286]
	Learning Rate: 0.00172856
	LOSS [training: 0.43991330801944173 | validation: 0.7266669093636742]
	TIME [epoch: 9.66 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4345606689765722		[learning rate: 0.0017223]
	Learning Rate: 0.00172229
	LOSS [training: 0.4345606689765722 | validation: 0.7213501404287888]
	TIME [epoch: 9.68 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39698538848901616		[learning rate: 0.001716]
	Learning Rate: 0.00171604
	LOSS [training: 0.39698538848901616 | validation: 0.7561772682588387]
	TIME [epoch: 9.66 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4497270957725636		[learning rate: 0.0017098]
	Learning Rate: 0.00170981
	LOSS [training: 0.4497270957725636 | validation: 0.7663420038802334]
	TIME [epoch: 9.66 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48081444314900956		[learning rate: 0.0017036]
	Learning Rate: 0.0017036
	LOSS [training: 0.48081444314900956 | validation: 0.699103699943197]
	TIME [epoch: 9.66 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4764518556407051		[learning rate: 0.0016974]
	Learning Rate: 0.00169742
	LOSS [training: 0.4764518556407051 | validation: 0.7800461897844962]
	TIME [epoch: 9.68 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41335517373622854		[learning rate: 0.0016913]
	Learning Rate: 0.00169126
	LOSS [training: 0.41335517373622854 | validation: 0.7232029643953298]
	TIME [epoch: 9.65 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43100020288668073		[learning rate: 0.0016851]
	Learning Rate: 0.00168512
	LOSS [training: 0.43100020288668073 | validation: 0.7403858783021567]
	TIME [epoch: 9.65 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4207473897229949		[learning rate: 0.001679]
	Learning Rate: 0.00167901
	LOSS [training: 0.4207473897229949 | validation: 0.8001073323878395]
	TIME [epoch: 9.66 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4686408256190722		[learning rate: 0.0016729]
	Learning Rate: 0.00167291
	LOSS [training: 0.4686408256190722 | validation: 0.787713476446546]
	TIME [epoch: 9.67 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5125168012595582		[learning rate: 0.0016668]
	Learning Rate: 0.00166684
	LOSS [training: 0.5125168012595582 | validation: 0.7011996800519833]
	TIME [epoch: 9.66 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43588686299247514		[learning rate: 0.0016608]
	Learning Rate: 0.00166079
	LOSS [training: 0.43588686299247514 | validation: 0.7662196096274793]
	TIME [epoch: 9.66 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44061289449966534		[learning rate: 0.0016548]
	Learning Rate: 0.00165477
	LOSS [training: 0.44061289449966534 | validation: 0.7039842037113766]
	TIME [epoch: 9.68 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4864295595897533		[learning rate: 0.0016488]
	Learning Rate: 0.00164876
	LOSS [training: 0.4864295595897533 | validation: 0.7754903657765065]
	TIME [epoch: 9.67 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5538723942098165		[learning rate: 0.0016428]
	Learning Rate: 0.00164278
	LOSS [training: 0.5538723942098165 | validation: 0.6414963667350818]
	TIME [epoch: 9.66 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39311511443447544		[learning rate: 0.0016368]
	Learning Rate: 0.00163682
	LOSS [training: 0.39311511443447544 | validation: 0.9216003055681904]
	TIME [epoch: 9.65 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4474901674550192		[learning rate: 0.0016309]
	Learning Rate: 0.00163088
	LOSS [training: 0.4474901674550192 | validation: 0.6672560967330164]
	TIME [epoch: 9.68 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5238315884149609		[learning rate: 0.001625]
	Learning Rate: 0.00162496
	LOSS [training: 0.5238315884149609 | validation: 0.5907820142284651]
	TIME [epoch: 9.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_600.pth
	Model improved!!!
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5597647060920248		[learning rate: 0.0016191]
	Learning Rate: 0.00161906
	LOSS [training: 0.5597647060920248 | validation: 0.8069037915544104]
	TIME [epoch: 9.67 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4527199989867815		[learning rate: 0.0016132]
	Learning Rate: 0.00161319
	LOSS [training: 0.4527199989867815 | validation: 1.0243782352027795]
	TIME [epoch: 9.67 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4841145036793959		[learning rate: 0.0016073]
	Learning Rate: 0.00160733
	LOSS [training: 0.4841145036793959 | validation: 0.6745047069178557]
	TIME [epoch: 9.68 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38774043051922114		[learning rate: 0.0016015]
	Learning Rate: 0.0016015
	LOSS [training: 0.38774043051922114 | validation: 0.7298179877220196]
	TIME [epoch: 9.67 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5030058764564376		[learning rate: 0.0015957]
	Learning Rate: 0.00159569
	LOSS [training: 0.5030058764564376 | validation: 0.7016373677341572]
	TIME [epoch: 9.67 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.422269624006407		[learning rate: 0.0015899]
	Learning Rate: 0.00158989
	LOSS [training: 0.422269624006407 | validation: 0.6570091424908847]
	TIME [epoch: 9.67 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41240199621706974		[learning rate: 0.0015841]
	Learning Rate: 0.00158413
	LOSS [training: 0.41240199621706974 | validation: 0.7676278767834657]
	TIME [epoch: 9.69 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43653766733313654		[learning rate: 0.0015784]
	Learning Rate: 0.00157838
	LOSS [training: 0.43653766733313654 | validation: 0.6878793285573142]
	TIME [epoch: 9.67 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42956739815509504		[learning rate: 0.0015726]
	Learning Rate: 0.00157265
	LOSS [training: 0.42956739815509504 | validation: 0.8323657733037877]
	TIME [epoch: 9.66 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4081255594196126		[learning rate: 0.0015669]
	Learning Rate: 0.00156694
	LOSS [training: 0.4081255594196126 | validation: 0.64216662605307]
	TIME [epoch: 9.69 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4169070305487498		[learning rate: 0.0015613]
	Learning Rate: 0.00156125
	LOSS [training: 0.4169070305487498 | validation: 0.7030105200703969]
	TIME [epoch: 9.67 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4034028385722145		[learning rate: 0.0015556]
	Learning Rate: 0.00155559
	LOSS [training: 0.4034028385722145 | validation: 0.7402801622946614]
	TIME [epoch: 9.67 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37772927649421384		[learning rate: 0.0015499]
	Learning Rate: 0.00154994
	LOSS [training: 0.37772927649421384 | validation: 0.7640107006739152]
	TIME [epoch: 9.67 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48575070173443924		[learning rate: 0.0015443]
	Learning Rate: 0.00154432
	LOSS [training: 0.48575070173443924 | validation: 0.6009344917712934]
	TIME [epoch: 9.68 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42998432354565913		[learning rate: 0.0015387]
	Learning Rate: 0.00153871
	LOSS [training: 0.42998432354565913 | validation: 0.6019683203434673]
	TIME [epoch: 9.67 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.444676726279709		[learning rate: 0.0015331]
	Learning Rate: 0.00153313
	LOSS [training: 0.444676726279709 | validation: 0.7439969748874475]
	TIME [epoch: 9.66 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4560241851488208		[learning rate: 0.0015276]
	Learning Rate: 0.00152757
	LOSS [training: 0.4560241851488208 | validation: 0.578180548751376]
	TIME [epoch: 9.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_617.pth
	Model improved!!!
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39040909294639836		[learning rate: 0.001522]
	Learning Rate: 0.00152202
	LOSS [training: 0.39040909294639836 | validation: 0.6039010802098314]
	TIME [epoch: 9.7 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38718316303076455		[learning rate: 0.0015165]
	Learning Rate: 0.0015165
	LOSS [training: 0.38718316303076455 | validation: 0.6244673916311984]
	TIME [epoch: 9.67 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3917242048697315		[learning rate: 0.001511]
	Learning Rate: 0.001511
	LOSS [training: 0.3917242048697315 | validation: 0.7680647461341011]
	TIME [epoch: 9.67 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4885464620877153		[learning rate: 0.0015055]
	Learning Rate: 0.00150551
	LOSS [training: 0.4885464620877153 | validation: 0.7929574783601504]
	TIME [epoch: 9.67 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4785836118009055		[learning rate: 0.0015]
	Learning Rate: 0.00150005
	LOSS [training: 0.4785836118009055 | validation: 1.4429677943723227]
	TIME [epoch: 9.67 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8124433634385726		[learning rate: 0.0014946]
	Learning Rate: 0.0014946
	LOSS [training: 0.8124433634385726 | validation: 0.7039399556469154]
	TIME [epoch: 9.66 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39431137000338534		[learning rate: 0.0014892]
	Learning Rate: 0.00148918
	LOSS [training: 0.39431137000338534 | validation: 0.7017887606104886]
	TIME [epoch: 9.66 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38455375633785877		[learning rate: 0.0014838]
	Learning Rate: 0.00148378
	LOSS [training: 0.38455375633785877 | validation: 0.6869838865944914]
	TIME [epoch: 9.68 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8517627484182151		[learning rate: 0.0014784]
	Learning Rate: 0.00147839
	LOSS [training: 0.8517627484182151 | validation: 0.8132421505545608]
	TIME [epoch: 9.66 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4246636753824887		[learning rate: 0.001473]
	Learning Rate: 0.00147303
	LOSS [training: 0.4246636753824887 | validation: 0.6555174485797641]
	TIME [epoch: 9.66 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.404007721042984		[learning rate: 0.0014677]
	Learning Rate: 0.00146768
	LOSS [training: 0.404007721042984 | validation: 0.6922776573425887]
	TIME [epoch: 9.66 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41605797433340364		[learning rate: 0.0014624]
	Learning Rate: 0.00146235
	LOSS [training: 0.41605797433340364 | validation: 0.6976404038049566]
	TIME [epoch: 9.68 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4138734564200127		[learning rate: 0.001457]
	Learning Rate: 0.00145705
	LOSS [training: 0.4138734564200127 | validation: 0.6368918533621253]
	TIME [epoch: 9.66 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4427446577744716		[learning rate: 0.0014518]
	Learning Rate: 0.00145176
	LOSS [training: 0.4427446577744716 | validation: 0.7223690406482678]
	TIME [epoch: 9.66 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4299868277044558		[learning rate: 0.0014465]
	Learning Rate: 0.00144649
	LOSS [training: 0.4299868277044558 | validation: 0.797161697554951]
	TIME [epoch: 9.66 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3935143638543914		[learning rate: 0.0014412]
	Learning Rate: 0.00144124
	LOSS [training: 0.3935143638543914 | validation: 0.6937698514241198]
	TIME [epoch: 9.68 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41078217986198656		[learning rate: 0.001436]
	Learning Rate: 0.00143601
	LOSS [training: 0.41078217986198656 | validation: 0.6912825538848278]
	TIME [epoch: 9.66 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4147152112476532		[learning rate: 0.0014308]
	Learning Rate: 0.0014308
	LOSS [training: 0.4147152112476532 | validation: 0.6448280441836297]
	TIME [epoch: 9.65 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.425629905963953		[learning rate: 0.0014256]
	Learning Rate: 0.00142561
	LOSS [training: 0.425629905963953 | validation: 0.6212000028409177]
	TIME [epoch: 9.67 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.595895623543243		[learning rate: 0.0014204]
	Learning Rate: 0.00142043
	LOSS [training: 0.595895623543243 | validation: 0.798578190805516]
	TIME [epoch: 9.69 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.49650183718601804		[learning rate: 0.0014153]
	Learning Rate: 0.00141528
	LOSS [training: 0.49650183718601804 | validation: 0.569219755561584]
	TIME [epoch: 9.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_638.pth
	Model improved!!!
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40430959068629263		[learning rate: 0.0014101]
	Learning Rate: 0.00141014
	LOSS [training: 0.40430959068629263 | validation: 0.5999167959518833]
	TIME [epoch: 9.66 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3739072717435369		[learning rate: 0.001405]
	Learning Rate: 0.00140503
	LOSS [training: 0.3739072717435369 | validation: 0.6384539519964167]
	TIME [epoch: 9.69 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4435141411090989		[learning rate: 0.0013999]
	Learning Rate: 0.00139993
	LOSS [training: 0.4435141411090989 | validation: 0.6592952455322633]
	TIME [epoch: 9.67 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4826488746009681		[learning rate: 0.0013948]
	Learning Rate: 0.00139485
	LOSS [training: 0.4826488746009681 | validation: 0.5290344527469947]
	TIME [epoch: 9.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_642.pth
	Model improved!!!
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4421239509450464		[learning rate: 0.0013898]
	Learning Rate: 0.00138978
	LOSS [training: 0.4421239509450464 | validation: 0.6470301669526216]
	TIME [epoch: 9.68 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39607568016624145		[learning rate: 0.0013847]
	Learning Rate: 0.00138474
	LOSS [training: 0.39607568016624145 | validation: 0.9122350773604104]
	TIME [epoch: 9.7 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4125401076743194		[learning rate: 0.0013797]
	Learning Rate: 0.00137972
	LOSS [training: 0.4125401076743194 | validation: 0.6855201283985409]
	TIME [epoch: 9.67 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39826451848530986		[learning rate: 0.0013747]
	Learning Rate: 0.00137471
	LOSS [training: 0.39826451848530986 | validation: 0.5742081005213919]
	TIME [epoch: 9.68 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42048091489070194		[learning rate: 0.0013697]
	Learning Rate: 0.00136972
	LOSS [training: 0.42048091489070194 | validation: 0.6076784028292189]
	TIME [epoch: 9.68 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3610318402591578		[learning rate: 0.0013647]
	Learning Rate: 0.00136475
	LOSS [training: 0.3610318402591578 | validation: 0.6225569894683324]
	TIME [epoch: 9.7 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3896905693434637		[learning rate: 0.0013598]
	Learning Rate: 0.0013598
	LOSS [training: 0.3896905693434637 | validation: 0.5959562689883827]
	TIME [epoch: 9.67 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4005893537916654		[learning rate: 0.0013549]
	Learning Rate: 0.00135486
	LOSS [training: 0.4005893537916654 | validation: 0.5580167477978946]
	TIME [epoch: 9.68 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42026013707330045		[learning rate: 0.0013499]
	Learning Rate: 0.00134994
	LOSS [training: 0.42026013707330045 | validation: 0.6153725484127565]
	TIME [epoch: 9.69 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3908312139762368		[learning rate: 0.001345]
	Learning Rate: 0.00134505
	LOSS [training: 0.3908312139762368 | validation: 0.6268426501766294]
	TIME [epoch: 9.68 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38821814634962054		[learning rate: 0.0013402]
	Learning Rate: 0.00134016
	LOSS [training: 0.38821814634962054 | validation: 0.564714287287426]
	TIME [epoch: 9.68 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38972301942533477		[learning rate: 0.0013353]
	Learning Rate: 0.0013353
	LOSS [training: 0.38972301942533477 | validation: 0.5787700323226225]
	TIME [epoch: 9.68 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5572802880787677		[learning rate: 0.0013305]
	Learning Rate: 0.00133045
	LOSS [training: 0.5572802880787677 | validation: 0.7960136431936369]
	TIME [epoch: 9.7 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4152406360398445		[learning rate: 0.0013256]
	Learning Rate: 0.00132563
	LOSS [training: 0.4152406360398445 | validation: 0.6051065888163969]
	TIME [epoch: 9.68 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35773972384874975		[learning rate: 0.0013208]
	Learning Rate: 0.00132082
	LOSS [training: 0.35773972384874975 | validation: 0.6101943067371631]
	TIME [epoch: 9.68 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4055580020175209		[learning rate: 0.001316]
	Learning Rate: 0.00131602
	LOSS [training: 0.4055580020175209 | validation: 0.5764534651702943]
	TIME [epoch: 9.67 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39207090400177885		[learning rate: 0.0013112]
	Learning Rate: 0.00131125
	LOSS [training: 0.39207090400177885 | validation: 0.5928818470050684]
	TIME [epoch: 9.7 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3903121308799399		[learning rate: 0.0013065]
	Learning Rate: 0.00130649
	LOSS [training: 0.3903121308799399 | validation: 0.7864414791202294]
	TIME [epoch: 9.67 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40300970121152824		[learning rate: 0.0013017]
	Learning Rate: 0.00130175
	LOSS [training: 0.40300970121152824 | validation: 0.7246034110080836]
	TIME [epoch: 9.67 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39136781373466867		[learning rate: 0.001297]
	Learning Rate: 0.00129702
	LOSS [training: 0.39136781373466867 | validation: 0.6423791839095256]
	TIME [epoch: 9.69 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39504745107924244		[learning rate: 0.0012923]
	Learning Rate: 0.00129232
	LOSS [training: 0.39504745107924244 | validation: 0.6155447932861579]
	TIME [epoch: 9.69 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.375446905497832		[learning rate: 0.0012876]
	Learning Rate: 0.00128763
	LOSS [training: 0.375446905497832 | validation: 0.5977044199851815]
	TIME [epoch: 9.68 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6979454191940917		[learning rate: 0.001283]
	Learning Rate: 0.00128295
	LOSS [training: 0.6979454191940917 | validation: 0.6363396491399455]
	TIME [epoch: 9.68 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47925876822404484		[learning rate: 0.0012783]
	Learning Rate: 0.0012783
	LOSS [training: 0.47925876822404484 | validation: 0.6687345688903602]
	TIME [epoch: 9.69 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3679870897925215		[learning rate: 0.0012737]
	Learning Rate: 0.00127366
	LOSS [training: 0.3679870897925215 | validation: 0.6743814672778845]
	TIME [epoch: 9.68 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3958177207916473		[learning rate: 0.001269]
	Learning Rate: 0.00126904
	LOSS [training: 0.3958177207916473 | validation: 0.6455373148713827]
	TIME [epoch: 9.68 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3847578055145008		[learning rate: 0.0012644]
	Learning Rate: 0.00126443
	LOSS [training: 0.3847578055145008 | validation: 0.6931085418480106]
	TIME [epoch: 9.68 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3964012740273226		[learning rate: 0.0012598]
	Learning Rate: 0.00125984
	LOSS [training: 0.3964012740273226 | validation: 0.7619042550025867]
	TIME [epoch: 9.7 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3829659753006743		[learning rate: 0.0012553]
	Learning Rate: 0.00125527
	LOSS [training: 0.3829659753006743 | validation: 0.7029934507045804]
	TIME [epoch: 9.68 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4056272537459972		[learning rate: 0.0012507]
	Learning Rate: 0.00125071
	LOSS [training: 0.4056272537459972 | validation: 0.6781772750866567]
	TIME [epoch: 9.67 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.390215650383584		[learning rate: 0.0012462]
	Learning Rate: 0.00124617
	LOSS [training: 0.390215650383584 | validation: 0.7006564861077428]
	TIME [epoch: 9.68 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3839927403476895		[learning rate: 0.0012417]
	Learning Rate: 0.00124165
	LOSS [training: 0.3839927403476895 | validation: 0.6015891516645216]
	TIME [epoch: 9.69 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3811872207049546		[learning rate: 0.0012371]
	Learning Rate: 0.00123715
	LOSS [training: 0.3811872207049546 | validation: 0.7156788947067821]
	TIME [epoch: 9.67 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39015263858299354		[learning rate: 0.0012327]
	Learning Rate: 0.00123266
	LOSS [training: 0.39015263858299354 | validation: 0.6521720937265886]
	TIME [epoch: 9.67 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3716987634477865		[learning rate: 0.0012282]
	Learning Rate: 0.00122818
	LOSS [training: 0.3716987634477865 | validation: 0.5988797955603711]
	TIME [epoch: 9.69 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40647315076308443		[learning rate: 0.0012237]
	Learning Rate: 0.00122373
	LOSS [training: 0.40647315076308443 | validation: 0.6135779654744543]
	TIME [epoch: 9.68 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5661076008429942		[learning rate: 0.0012193]
	Learning Rate: 0.00121929
	LOSS [training: 0.5661076008429942 | validation: 0.8175977725318647]
	TIME [epoch: 9.67 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6212824488857963		[learning rate: 0.0012149]
	Learning Rate: 0.00121486
	LOSS [training: 0.6212824488857963 | validation: 0.611317685827948]
	TIME [epoch: 9.67 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41383656869223995		[learning rate: 0.0012105]
	Learning Rate: 0.00121045
	LOSS [training: 0.41383656869223995 | validation: 0.8811297162732978]
	TIME [epoch: 9.69 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4608463547264237		[learning rate: 0.0012061]
	Learning Rate: 0.00120606
	LOSS [training: 0.4608463547264237 | validation: 0.7719882966657707]
	TIME [epoch: 9.67 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3686049300811944		[learning rate: 0.0012017]
	Learning Rate: 0.00120168
	LOSS [training: 0.3686049300811944 | validation: 0.668030434753114]
	TIME [epoch: 9.67 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4056959901968903		[learning rate: 0.0011973]
	Learning Rate: 0.00119732
	LOSS [training: 0.4056959901968903 | validation: 0.6260476707862571]
	TIME [epoch: 9.67 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38735361286295145		[learning rate: 0.001193]
	Learning Rate: 0.00119298
	LOSS [training: 0.38735361286295145 | validation: 0.7568949828100072]
	TIME [epoch: 9.7 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42512998917927874		[learning rate: 0.0011886]
	Learning Rate: 0.00118865
	LOSS [training: 0.42512998917927874 | validation: 0.7727620728113344]
	TIME [epoch: 9.67 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36920630837911955		[learning rate: 0.0011843]
	Learning Rate: 0.00118433
	LOSS [training: 0.36920630837911955 | validation: 0.6412302569874619]
	TIME [epoch: 9.67 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40728910782044947		[learning rate: 0.00118]
	Learning Rate: 0.00118003
	LOSS [training: 0.40728910782044947 | validation: 0.6376576895011651]
	TIME [epoch: 9.68 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40477710862990285		[learning rate: 0.0011758]
	Learning Rate: 0.00117575
	LOSS [training: 0.40477710862990285 | validation: 0.829177402500145]
	TIME [epoch: 9.69 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3916817926152609		[learning rate: 0.0011715]
	Learning Rate: 0.00117149
	LOSS [training: 0.3916817926152609 | validation: 0.7722754224404048]
	TIME [epoch: 9.67 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43456446522139486		[learning rate: 0.0011672]
	Learning Rate: 0.00116723
	LOSS [training: 0.43456446522139486 | validation: 0.6899338952982805]
	TIME [epoch: 9.67 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38616645888854495		[learning rate: 0.001163]
	Learning Rate: 0.001163
	LOSS [training: 0.38616645888854495 | validation: 0.623523113135704]
	TIME [epoch: 9.69 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.388800295081975		[learning rate: 0.0011588]
	Learning Rate: 0.00115878
	LOSS [training: 0.388800295081975 | validation: 0.6422616296686519]
	TIME [epoch: 9.67 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43042186282476075		[learning rate: 0.0011546]
	Learning Rate: 0.00115457
	LOSS [training: 0.43042186282476075 | validation: 0.6639088832689247]
	TIME [epoch: 9.68 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39261544266093595		[learning rate: 0.0011504]
	Learning Rate: 0.00115038
	LOSS [training: 0.39261544266093595 | validation: 0.7131007125988619]
	TIME [epoch: 9.67 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39959138421620694		[learning rate: 0.0011462]
	Learning Rate: 0.00114621
	LOSS [training: 0.39959138421620694 | validation: 0.7040739001436863]
	TIME [epoch: 9.7 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3608740887997236		[learning rate: 0.001142]
	Learning Rate: 0.00114205
	LOSS [training: 0.3608740887997236 | validation: 0.7361188113590115]
	TIME [epoch: 9.67 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4084989625002381		[learning rate: 0.0011379]
	Learning Rate: 0.0011379
	LOSS [training: 0.4084989625002381 | validation: 0.7411974862673104]
	TIME [epoch: 9.67 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42033804154513066		[learning rate: 0.0011338]
	Learning Rate: 0.00113377
	LOSS [training: 0.42033804154513066 | validation: 0.725791652652723]
	TIME [epoch: 9.68 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39088846944594235		[learning rate: 0.0011297]
	Learning Rate: 0.00112966
	LOSS [training: 0.39088846944594235 | validation: 0.6928073072471196]
	TIME [epoch: 9.7 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40105024818848045		[learning rate: 0.0011256]
	Learning Rate: 0.00112556
	LOSS [training: 0.40105024818848045 | validation: 0.6561084095647368]
	TIME [epoch: 9.67 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39681648615075005		[learning rate: 0.0011215]
	Learning Rate: 0.00112147
	LOSS [training: 0.39681648615075005 | validation: 0.7209216548836411]
	TIME [epoch: 9.67 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3819514636380865		[learning rate: 0.0011174]
	Learning Rate: 0.0011174
	LOSS [training: 0.3819514636380865 | validation: 0.7480318309742106]
	TIME [epoch: 9.68 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4051883854577524		[learning rate: 0.0011133]
	Learning Rate: 0.00111335
	LOSS [training: 0.4051883854577524 | validation: 0.6485768172037506]
	TIME [epoch: 9.69 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3990966041221481		[learning rate: 0.0011093]
	Learning Rate: 0.00110931
	LOSS [training: 0.3990966041221481 | validation: 0.8106333983843546]
	TIME [epoch: 9.68 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38040062112526896		[learning rate: 0.0011053]
	Learning Rate: 0.00110528
	LOSS [training: 0.38040062112526896 | validation: 0.7279374953706933]
	TIME [epoch: 9.68 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35859749201262536		[learning rate: 0.0011013]
	Learning Rate: 0.00110127
	LOSS [training: 0.35859749201262536 | validation: 0.6371743489537337]
	TIME [epoch: 9.7 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39781089947465154		[learning rate: 0.0010973]
	Learning Rate: 0.00109728
	LOSS [training: 0.39781089947465154 | validation: 0.6430457747684324]
	TIME [epoch: 9.68 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41288447099774117		[learning rate: 0.0010933]
	Learning Rate: 0.00109329
	LOSS [training: 0.41288447099774117 | validation: 0.5460630526281877]
	TIME [epoch: 9.68 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.347294007843734		[learning rate: 0.0010893]
	Learning Rate: 0.00108933
	LOSS [training: 0.347294007843734 | validation: 0.5680047456811721]
	TIME [epoch: 9.68 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36041790538853685		[learning rate: 0.0010854]
	Learning Rate: 0.00108537
	LOSS [training: 0.36041790538853685 | validation: 0.7735978654992546]
	TIME [epoch: 9.69 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37127494178515646		[learning rate: 0.0010814]
	Learning Rate: 0.00108143
	LOSS [training: 0.37127494178515646 | validation: 0.5408460761946688]
	TIME [epoch: 9.68 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4052927206160408		[learning rate: 0.0010775]
	Learning Rate: 0.00107751
	LOSS [training: 0.4052927206160408 | validation: 0.5722800020385492]
	TIME [epoch: 9.68 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4275874141653824		[learning rate: 0.0010736]
	Learning Rate: 0.0010736
	LOSS [training: 0.4275874141653824 | validation: 0.6209981953915766]
	TIME [epoch: 9.67 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3504559854033905		[learning rate: 0.0010697]
	Learning Rate: 0.0010697
	LOSS [training: 0.3504559854033905 | validation: 0.5095255726271236]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_715.pth
	Model improved!!!
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32804168450878185		[learning rate: 0.0010658]
	Learning Rate: 0.00106582
	LOSS [training: 0.32804168450878185 | validation: 0.6958445773069292]
	TIME [epoch: 9.68 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3431929141810296		[learning rate: 0.001062]
	Learning Rate: 0.00106195
	LOSS [training: 0.3431929141810296 | validation: 0.5396647266511515]
	TIME [epoch: 9.68 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4108653485323351		[learning rate: 0.0010581]
	Learning Rate: 0.0010581
	LOSS [training: 0.4108653485323351 | validation: 0.5177209794047595]
	TIME [epoch: 9.69 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3943741784467401		[learning rate: 0.0010543]
	Learning Rate: 0.00105426
	LOSS [training: 0.3943741784467401 | validation: 0.5187096661453612]
	TIME [epoch: 9.69 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36244536242225917		[learning rate: 0.0010504]
	Learning Rate: 0.00105043
	LOSS [training: 0.36244536242225917 | validation: 0.5890656018786286]
	TIME [epoch: 9.67 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3772234883619194		[learning rate: 0.0010466]
	Learning Rate: 0.00104662
	LOSS [training: 0.3772234883619194 | validation: 0.5727187357364244]
	TIME [epoch: 9.68 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3416752726191281		[learning rate: 0.0010428]
	Learning Rate: 0.00104282
	LOSS [training: 0.3416752726191281 | validation: 0.7115275933349833]
	TIME [epoch: 9.7 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39417415701714814		[learning rate: 0.001039]
	Learning Rate: 0.00103904
	LOSS [training: 0.39417415701714814 | validation: 0.6721267034270698]
	TIME [epoch: 9.68 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42297920057132055		[learning rate: 0.0010353]
	Learning Rate: 0.00103527
	LOSS [training: 0.42297920057132055 | validation: 0.7370598402090685]
	TIME [epoch: 9.67 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4642333727515987		[learning rate: 0.0010315]
	Learning Rate: 0.00103151
	LOSS [training: 0.4642333727515987 | validation: 0.5522818617374171]
	TIME [epoch: 9.67 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37764613710539374		[learning rate: 0.0010278]
	Learning Rate: 0.00102777
	LOSS [training: 0.37764613710539374 | validation: 0.6262823098772968]
	TIME [epoch: 9.7 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34460624650709704		[learning rate: 0.001024]
	Learning Rate: 0.00102404
	LOSS [training: 0.34460624650709704 | validation: 0.6387726560926439]
	TIME [epoch: 9.68 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35695380395969395		[learning rate: 0.0010203]
	Learning Rate: 0.00102032
	LOSS [training: 0.35695380395969395 | validation: 0.6151724140888107]
	TIME [epoch: 9.67 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36863234206779316		[learning rate: 0.0010166]
	Learning Rate: 0.00101662
	LOSS [training: 0.36863234206779316 | validation: 0.579214006561268]
	TIME [epoch: 9.68 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3370541280681286		[learning rate: 0.0010129]
	Learning Rate: 0.00101293
	LOSS [training: 0.3370541280681286 | validation: 0.6623812638543092]
	TIME [epoch: 9.69 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38213660369689634		[learning rate: 0.0010093]
	Learning Rate: 0.00100925
	LOSS [training: 0.38213660369689634 | validation: 0.6515151494820688]
	TIME [epoch: 9.67 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3608606717472568		[learning rate: 0.0010056]
	Learning Rate: 0.00100559
	LOSS [training: 0.3608606717472568 | validation: 0.5259378721883229]
	TIME [epoch: 9.67 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3304866237546233		[learning rate: 0.0010019]
	Learning Rate: 0.00100194
	LOSS [training: 0.3304866237546233 | validation: 0.5848762294895251]
	TIME [epoch: 9.7 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36455512605450824		[learning rate: 0.0009983]
	Learning Rate: 0.000998305
	LOSS [training: 0.36455512605450824 | validation: 0.578901427943093]
	TIME [epoch: 9.68 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40636368423412933		[learning rate: 0.00099468]
	Learning Rate: 0.000994682
	LOSS [training: 0.40636368423412933 | validation: 0.583347344062162]
	TIME [epoch: 9.68 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3726462862797845		[learning rate: 0.00099107]
	Learning Rate: 0.000991072
	LOSS [training: 0.3726462862797845 | validation: 0.5087532743223327]
	TIME [epoch: 9.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_736.pth
	Model improved!!!
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3561924060017771		[learning rate: 0.00098748]
	Learning Rate: 0.000987475
	LOSS [training: 0.3561924060017771 | validation: 0.5119333365093608]
	TIME [epoch: 9.69 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5319202331432237		[learning rate: 0.00098389]
	Learning Rate: 0.000983892
	LOSS [training: 0.5319202331432237 | validation: 0.8237807112106922]
	TIME [epoch: 9.66 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6524464581153969		[learning rate: 0.00098032]
	Learning Rate: 0.000980321
	LOSS [training: 0.6524464581153969 | validation: 0.7365241884717927]
	TIME [epoch: 9.66 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4467722352571549		[learning rate: 0.00097676]
	Learning Rate: 0.000976764
	LOSS [training: 0.4467722352571549 | validation: 0.5424879655614769]
	TIME [epoch: 9.66 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3733836854231136		[learning rate: 0.00097322]
	Learning Rate: 0.000973219
	LOSS [training: 0.3733836854231136 | validation: 0.6196130566209592]
	TIME [epoch: 9.68 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35492108962122526		[learning rate: 0.00096969]
	Learning Rate: 0.000969687
	LOSS [training: 0.35492108962122526 | validation: 0.5312545414214211]
	TIME [epoch: 9.66 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5556618891057662		[learning rate: 0.00096617]
	Learning Rate: 0.000966168
	LOSS [training: 0.5556618891057662 | validation: 0.49146549106325665]
	TIME [epoch: 9.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_743.pth
	Model improved!!!
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38352551837903487		[learning rate: 0.00096266]
	Learning Rate: 0.000962662
	LOSS [training: 0.38352551837903487 | validation: 0.754853906524356]
	TIME [epoch: 9.68 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3924035255590786		[learning rate: 0.00095917]
	Learning Rate: 0.000959168
	LOSS [training: 0.3924035255590786 | validation: 0.580107069388077]
	TIME [epoch: 9.67 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3726242015453022		[learning rate: 0.00095569]
	Learning Rate: 0.000955687
	LOSS [training: 0.3726242015453022 | validation: 0.5501436079516575]
	TIME [epoch: 9.66 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4249876363420307		[learning rate: 0.00095222]
	Learning Rate: 0.000952219
	LOSS [training: 0.4249876363420307 | validation: 0.5998264768856573]
	TIME [epoch: 9.66 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5020574233750823		[learning rate: 0.00094876]
	Learning Rate: 0.000948763
	LOSS [training: 0.5020574233750823 | validation: 0.6207267405659903]
	TIME [epoch: 9.68 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40521032214381		[learning rate: 0.00094532]
	Learning Rate: 0.00094532
	LOSS [training: 0.40521032214381 | validation: 0.6113073109607301]
	TIME [epoch: 9.66 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9352538515169033		[learning rate: 0.00094189]
	Learning Rate: 0.000941889
	LOSS [training: 0.9352538515169033 | validation: 0.6715798327058022]
	TIME [epoch: 9.66 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48594709797916574		[learning rate: 0.00093847]
	Learning Rate: 0.000938471
	LOSS [training: 0.48594709797916574 | validation: 0.6128456387498897]
	TIME [epoch: 9.66 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3464165225795143		[learning rate: 0.00093507]
	Learning Rate: 0.000935066
	LOSS [training: 0.3464165225795143 | validation: 0.5608439177863297]
	TIME [epoch: 9.68 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33944118945701895		[learning rate: 0.00093167]
	Learning Rate: 0.000931672
	LOSS [training: 0.33944118945701895 | validation: 0.5033099218202671]
	TIME [epoch: 9.66 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32426997849564126		[learning rate: 0.00092829]
	Learning Rate: 0.000928291
	LOSS [training: 0.32426997849564126 | validation: 0.8365440355576618]
	TIME [epoch: 9.66 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47291664863100874		[learning rate: 0.00092492]
	Learning Rate: 0.000924922
	LOSS [training: 0.47291664863100874 | validation: 0.6260004924202734]
	TIME [epoch: 9.66 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.505587514088828		[learning rate: 0.00092157]
	Learning Rate: 0.000921566
	LOSS [training: 0.505587514088828 | validation: 0.5669894792714429]
	TIME [epoch: 9.68 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38643437345828724		[learning rate: 0.00091822]
	Learning Rate: 0.000918221
	LOSS [training: 0.38643437345828724 | validation: 0.5186029468332154]
	TIME [epoch: 9.66 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7220151729669954		[learning rate: 0.00091489]
	Learning Rate: 0.000914889
	LOSS [training: 0.7220151729669954 | validation: 0.4488654841672143]
	TIME [epoch: 9.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_758.pth
	Model improved!!!
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3885683043861635		[learning rate: 0.00091157]
	Learning Rate: 0.000911569
	LOSS [training: 0.3885683043861635 | validation: 0.4591636091852132]
	TIME [epoch: 9.68 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35425479554656936		[learning rate: 0.00090826]
	Learning Rate: 0.000908261
	LOSS [training: 0.35425479554656936 | validation: 0.5035107919352773]
	TIME [epoch: 9.67 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34653911529555714		[learning rate: 0.00090496]
	Learning Rate: 0.000904965
	LOSS [training: 0.34653911529555714 | validation: 0.7797205110847554]
	TIME [epoch: 9.66 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3868632089028011		[learning rate: 0.00090168]
	Learning Rate: 0.00090168
	LOSS [training: 0.3868632089028011 | validation: 0.7252856671432235]
	TIME [epoch: 9.65 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44025064450621		[learning rate: 0.00089841]
	Learning Rate: 0.000898408
	LOSS [training: 0.44025064450621 | validation: 0.5248472457300355]
	TIME [epoch: 9.67 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37048603387118767		[learning rate: 0.00089515]
	Learning Rate: 0.000895148
	LOSS [training: 0.37048603387118767 | validation: 0.45296414044308736]
	TIME [epoch: 9.66 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3736300158010657		[learning rate: 0.0008919]
	Learning Rate: 0.000891899
	LOSS [training: 0.3736300158010657 | validation: 0.5846322245747918]
	TIME [epoch: 9.65 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4157902539787499		[learning rate: 0.00088866]
	Learning Rate: 0.000888663
	LOSS [training: 0.4157902539787499 | validation: 0.5704934625076107]
	TIME [epoch: 9.66 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3433021872160742		[learning rate: 0.00088544]
	Learning Rate: 0.000885438
	LOSS [training: 0.3433021872160742 | validation: 0.6384166000881253]
	TIME [epoch: 9.68 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35532321314361115		[learning rate: 0.00088222]
	Learning Rate: 0.000882224
	LOSS [training: 0.35532321314361115 | validation: 0.760958547066028]
	TIME [epoch: 9.65 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3716916274057856		[learning rate: 0.00087902]
	Learning Rate: 0.000879022
	LOSS [training: 0.3716916274057856 | validation: 0.5412276561414271]
	TIME [epoch: 9.66 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31575692372815223		[learning rate: 0.00087583]
	Learning Rate: 0.000875833
	LOSS [training: 0.31575692372815223 | validation: 0.5349975532055773]
	TIME [epoch: 9.66 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32375226895128695		[learning rate: 0.00087265]
	Learning Rate: 0.000872654
	LOSS [training: 0.32375226895128695 | validation: 0.49778846619378336]
	TIME [epoch: 9.67 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35815479434813974		[learning rate: 0.00086949]
	Learning Rate: 0.000869487
	LOSS [training: 0.35815479434813974 | validation: 0.5503364286219931]
	TIME [epoch: 9.66 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31508606355182656		[learning rate: 0.00086633]
	Learning Rate: 0.000866332
	LOSS [training: 0.31508606355182656 | validation: 0.7199985951149176]
	TIME [epoch: 9.65 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35842955243099084		[learning rate: 0.00086319]
	Learning Rate: 0.000863188
	LOSS [training: 0.35842955243099084 | validation: 0.5568695270979952]
	TIME [epoch: 9.66 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3303206981810285		[learning rate: 0.00086006]
	Learning Rate: 0.000860055
	LOSS [training: 0.3303206981810285 | validation: 0.5207922488266794]
	TIME [epoch: 9.66 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34222560191174983		[learning rate: 0.00085693]
	Learning Rate: 0.000856934
	LOSS [training: 0.34222560191174983 | validation: 0.5307773955075527]
	TIME [epoch: 9.65 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30414940435123505		[learning rate: 0.00085382]
	Learning Rate: 0.000853824
	LOSS [training: 0.30414940435123505 | validation: 0.6689866002278002]
	TIME [epoch: 9.65 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3805875834969563		[learning rate: 0.00085073]
	Learning Rate: 0.000850726
	LOSS [training: 0.3805875834969563 | validation: 0.6851506002854748]
	TIME [epoch: 9.67 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32876541532571957		[learning rate: 0.00084764]
	Learning Rate: 0.000847638
	LOSS [training: 0.32876541532571957 | validation: 0.6604532959554382]
	TIME [epoch: 9.65 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37097427731765936		[learning rate: 0.00084456]
	Learning Rate: 0.000844562
	LOSS [training: 0.37097427731765936 | validation: 0.5721125161110734]
	TIME [epoch: 9.66 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3834665035042689		[learning rate: 0.0008415]
	Learning Rate: 0.000841497
	LOSS [training: 0.3834665035042689 | validation: 0.6340127927560092]
	TIME [epoch: 9.65 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38267844912289595		[learning rate: 0.00083844]
	Learning Rate: 0.000838443
	LOSS [training: 0.38267844912289595 | validation: 0.6478254961348838]
	TIME [epoch: 9.67 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3372564267944834		[learning rate: 0.0008354]
	Learning Rate: 0.000835401
	LOSS [training: 0.3372564267944834 | validation: 0.5464921640072845]
	TIME [epoch: 9.65 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35221889744589274		[learning rate: 0.00083237]
	Learning Rate: 0.000832369
	LOSS [training: 0.35221889744589274 | validation: 0.6570604502924422]
	TIME [epoch: 9.65 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35755707438425477		[learning rate: 0.00082935]
	Learning Rate: 0.000829348
	LOSS [training: 0.35755707438425477 | validation: 0.5862188195314734]
	TIME [epoch: 9.66 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.414583789202036		[learning rate: 0.00082634]
	Learning Rate: 0.000826338
	LOSS [training: 0.414583789202036 | validation: 0.5946487955969767]
	TIME [epoch: 9.67 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5133020343217991		[learning rate: 0.00082334]
	Learning Rate: 0.00082334
	LOSS [training: 0.5133020343217991 | validation: 0.585673617908852]
	TIME [epoch: 9.65 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39567905880280174		[learning rate: 0.00082035]
	Learning Rate: 0.000820352
	LOSS [training: 0.39567905880280174 | validation: 0.5569472075874694]
	TIME [epoch: 9.65 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3220721306537463		[learning rate: 0.00081737]
	Learning Rate: 0.000817375
	LOSS [training: 0.3220721306537463 | validation: 0.648472587607787]
	TIME [epoch: 9.67 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34433238801865435		[learning rate: 0.00081441]
	Learning Rate: 0.000814408
	LOSS [training: 0.34433238801865435 | validation: 0.6346535349929286]
	TIME [epoch: 9.66 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38104594697628424		[learning rate: 0.00081145]
	Learning Rate: 0.000811453
	LOSS [training: 0.38104594697628424 | validation: 0.5642567374990617]
	TIME [epoch: 9.66 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3255543217034183		[learning rate: 0.00080851]
	Learning Rate: 0.000808508
	LOSS [training: 0.3255543217034183 | validation: 0.52985128568114]
	TIME [epoch: 9.65 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31620402882617415		[learning rate: 0.00080557]
	Learning Rate: 0.000805574
	LOSS [training: 0.31620402882617415 | validation: 0.4973982105105235]
	TIME [epoch: 9.67 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3294274314481763		[learning rate: 0.00080265]
	Learning Rate: 0.00080265
	LOSS [training: 0.3294274314481763 | validation: 0.5234946609366449]
	TIME [epoch: 9.65 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3478542252804931		[learning rate: 0.00079974]
	Learning Rate: 0.000799737
	LOSS [training: 0.3478542252804931 | validation: 0.5791571118300195]
	TIME [epoch: 9.65 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3241196231785167		[learning rate: 0.00079684]
	Learning Rate: 0.000796835
	LOSS [training: 0.3241196231785167 | validation: 0.6277038208463118]
	TIME [epoch: 9.65 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33456641728605224		[learning rate: 0.00079394]
	Learning Rate: 0.000793943
	LOSS [training: 0.33456641728605224 | validation: 0.6103967828383735]
	TIME [epoch: 9.67 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3346942222437643		[learning rate: 0.00079106]
	Learning Rate: 0.000791062
	LOSS [training: 0.3346942222437643 | validation: 0.604512455069023]
	TIME [epoch: 9.65 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3546394278837215		[learning rate: 0.00078819]
	Learning Rate: 0.000788191
	LOSS [training: 0.3546394278837215 | validation: 0.640914773860305]
	TIME [epoch: 9.65 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45448662947006524		[learning rate: 0.00078533]
	Learning Rate: 0.000785331
	LOSS [training: 0.45448662947006524 | validation: 0.6703642743712206]
	TIME [epoch: 9.65 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3428262590368904		[learning rate: 0.00078248]
	Learning Rate: 0.000782481
	LOSS [training: 0.3428262590368904 | validation: 0.5512770402500521]
	TIME [epoch: 9.67 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3416734078558947		[learning rate: 0.00077964]
	Learning Rate: 0.000779641
	LOSS [training: 0.3416734078558947 | validation: 0.5767664084746016]
	TIME [epoch: 9.65 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35551261315700944		[learning rate: 0.00077681]
	Learning Rate: 0.000776812
	LOSS [training: 0.35551261315700944 | validation: 0.5958647219163743]
	TIME [epoch: 9.65 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33640513207168754		[learning rate: 0.00077399]
	Learning Rate: 0.000773993
	LOSS [training: 0.33640513207168754 | validation: 0.6537091929044815]
	TIME [epoch: 9.67 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3431947361688936		[learning rate: 0.00077118]
	Learning Rate: 0.000771184
	LOSS [training: 0.3431947361688936 | validation: 0.5900455179490601]
	TIME [epoch: 9.65 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37565872287841373		[learning rate: 0.00076839]
	Learning Rate: 0.000768385
	LOSS [training: 0.37565872287841373 | validation: 0.7156423715070339]
	TIME [epoch: 9.65 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35249296636521976		[learning rate: 0.0007656]
	Learning Rate: 0.000765597
	LOSS [training: 0.35249296636521976 | validation: 0.5826724759312292]
	TIME [epoch: 9.65 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3512374926100802		[learning rate: 0.00076282]
	Learning Rate: 0.000762818
	LOSS [training: 0.3512374926100802 | validation: 0.5479108754405958]
	TIME [epoch: 9.67 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3494475196987673		[learning rate: 0.00076005]
	Learning Rate: 0.00076005
	LOSS [training: 0.3494475196987673 | validation: 0.550731125515279]
	TIME [epoch: 9.65 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38994979344796066		[learning rate: 0.00075729]
	Learning Rate: 0.000757292
	LOSS [training: 0.38994979344796066 | validation: 0.6967502371758529]
	TIME [epoch: 9.66 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3859155272329121		[learning rate: 0.00075454]
	Learning Rate: 0.000754543
	LOSS [training: 0.3859155272329121 | validation: 0.7354305716986874]
	TIME [epoch: 9.65 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3658040128420506		[learning rate: 0.00075181]
	Learning Rate: 0.000751805
	LOSS [training: 0.3658040128420506 | validation: 0.5514248767456957]
	TIME [epoch: 9.67 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.442944809829363		[learning rate: 0.00074908]
	Learning Rate: 0.000749077
	LOSS [training: 0.442944809829363 | validation: 0.5818003934809418]
	TIME [epoch: 9.65 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4171686212021587		[learning rate: 0.00074636]
	Learning Rate: 0.000746358
	LOSS [training: 0.4171686212021587 | validation: 0.6769232332756743]
	TIME [epoch: 9.66 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37219531084029656		[learning rate: 0.00074365]
	Learning Rate: 0.00074365
	LOSS [training: 0.37219531084029656 | validation: 0.5240386125899801]
	TIME [epoch: 9.66 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36560096446409324		[learning rate: 0.00074095]
	Learning Rate: 0.000740951
	LOSS [training: 0.36560096446409324 | validation: 0.5301742423142843]
	TIME [epoch: 9.67 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33793142154884004		[learning rate: 0.00073826]
	Learning Rate: 0.000738262
	LOSS [training: 0.33793142154884004 | validation: 0.4673652845284916]
	TIME [epoch: 9.65 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41350309123782064		[learning rate: 0.00073558]
	Learning Rate: 0.000735583
	LOSS [training: 0.41350309123782064 | validation: 0.5822260833219298]
	TIME [epoch: 9.65 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32284788721969915		[learning rate: 0.00073291]
	Learning Rate: 0.000732913
	LOSS [training: 0.32284788721969915 | validation: 0.5898081717348543]
	TIME [epoch: 9.67 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3250278168241506		[learning rate: 0.00073025]
	Learning Rate: 0.000730254
	LOSS [training: 0.3250278168241506 | validation: 0.6599952710470754]
	TIME [epoch: 9.66 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4235092719195671		[learning rate: 0.0007276]
	Learning Rate: 0.000727603
	LOSS [training: 0.4235092719195671 | validation: 0.480793603915064]
	TIME [epoch: 9.66 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3705073365784203		[learning rate: 0.00072496]
	Learning Rate: 0.000724963
	LOSS [training: 0.3705073365784203 | validation: 0.44948605842356615]
	TIME [epoch: 9.65 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32128058461622117		[learning rate: 0.00072233]
	Learning Rate: 0.000722332
	LOSS [training: 0.32128058461622117 | validation: 0.555456036364114]
	TIME [epoch: 9.68 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32953705403096195		[learning rate: 0.00071971]
	Learning Rate: 0.000719711
	LOSS [training: 0.32953705403096195 | validation: 0.5406220117797464]
	TIME [epoch: 9.66 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3578321635555831		[learning rate: 0.0007171]
	Learning Rate: 0.000717099
	LOSS [training: 0.3578321635555831 | validation: 0.5528875456514679]
	TIME [epoch: 9.65 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.371275549070967		[learning rate: 0.0007145]
	Learning Rate: 0.000714496
	LOSS [training: 0.371275549070967 | validation: 0.5456171127774299]
	TIME [epoch: 9.65 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3333914281750047		[learning rate: 0.0007119]
	Learning Rate: 0.000711903
	LOSS [training: 0.3333914281750047 | validation: 0.5817092803303097]
	TIME [epoch: 9.67 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3370106996828347		[learning rate: 0.00070932]
	Learning Rate: 0.00070932
	LOSS [training: 0.3370106996828347 | validation: 0.5791086116340701]
	TIME [epoch: 9.66 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5531521156061806		[learning rate: 0.00070675]
	Learning Rate: 0.000706746
	LOSS [training: 0.5531521156061806 | validation: 0.5853436574920806]
	TIME [epoch: 9.65 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5139796090564587		[learning rate: 0.00070418]
	Learning Rate: 0.000704181
	LOSS [training: 0.5139796090564587 | validation: 0.6399091905023544]
	TIME [epoch: 9.67 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38446876085517007		[learning rate: 0.00070163]
	Learning Rate: 0.000701625
	LOSS [training: 0.38446876085517007 | validation: 0.6652454046751382]
	TIME [epoch: 9.66 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3373081799686929		[learning rate: 0.00069908]
	Learning Rate: 0.000699079
	LOSS [training: 0.3373081799686929 | validation: 0.5060238979661361]
	TIME [epoch: 9.65 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42135229393793805		[learning rate: 0.00069654]
	Learning Rate: 0.000696542
	LOSS [training: 0.42135229393793805 | validation: 0.5845454676834669]
	TIME [epoch: 9.66 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3302323427426432		[learning rate: 0.00069401]
	Learning Rate: 0.000694014
	LOSS [training: 0.3302323427426432 | validation: 0.5040775569761003]
	TIME [epoch: 9.67 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3517324231374204		[learning rate: 0.0006915]
	Learning Rate: 0.000691496
	LOSS [training: 0.3517324231374204 | validation: 0.592559403662709]
	TIME [epoch: 9.66 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3440766228773079		[learning rate: 0.00068899]
	Learning Rate: 0.000688986
	LOSS [training: 0.3440766228773079 | validation: 0.5408130957106217]
	TIME [epoch: 9.66 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34118738369262075		[learning rate: 0.00068649]
	Learning Rate: 0.000686486
	LOSS [training: 0.34118738369262075 | validation: 0.6211158218540299]
	TIME [epoch: 9.66 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3961168006100277		[learning rate: 0.00068399]
	Learning Rate: 0.000683994
	LOSS [training: 0.3961168006100277 | validation: 0.465283826515933]
	TIME [epoch: 9.68 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34476958319127066		[learning rate: 0.00068151]
	Learning Rate: 0.000681512
	LOSS [training: 0.34476958319127066 | validation: 0.5412092620577593]
	TIME [epoch: 9.66 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30658746845983254		[learning rate: 0.00067904]
	Learning Rate: 0.000679039
	LOSS [training: 0.30658746845983254 | validation: 0.5328295732742451]
	TIME [epoch: 9.66 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3326210757982213		[learning rate: 0.00067657]
	Learning Rate: 0.000676575
	LOSS [training: 0.3326210757982213 | validation: 0.6020789621788397]
	TIME [epoch: 9.65 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3562334202707129		[learning rate: 0.00067412]
	Learning Rate: 0.00067412
	LOSS [training: 0.3562334202707129 | validation: 0.6826522644559266]
	TIME [epoch: 9.68 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38921776175065137		[learning rate: 0.00067167]
	Learning Rate: 0.000671673
	LOSS [training: 0.38921776175065137 | validation: 0.6658603178315323]
	TIME [epoch: 9.66 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37131052223286476		[learning rate: 0.00066924]
	Learning Rate: 0.000669235
	LOSS [training: 0.37131052223286476 | validation: 0.5485048272955636]
	TIME [epoch: 9.65 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34809914905878137		[learning rate: 0.00066681]
	Learning Rate: 0.000666807
	LOSS [training: 0.34809914905878137 | validation: 0.5270467358193345]
	TIME [epoch: 9.67 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33741046306382494		[learning rate: 0.00066439]
	Learning Rate: 0.000664387
	LOSS [training: 0.33741046306382494 | validation: 0.5039831791917682]
	TIME [epoch: 9.66 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5336652553581325		[learning rate: 0.00066198]
	Learning Rate: 0.000661976
	LOSS [training: 0.5336652553581325 | validation: 0.4745892722423221]
	TIME [epoch: 9.66 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32319531808422103		[learning rate: 0.00065957]
	Learning Rate: 0.000659573
	LOSS [training: 0.32319531808422103 | validation: 0.6377510923278102]
	TIME [epoch: 9.66 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33846075588975866		[learning rate: 0.00065718]
	Learning Rate: 0.00065718
	LOSS [training: 0.33846075588975866 | validation: 0.5942260219518124]
	TIME [epoch: 9.66 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3216288129314622		[learning rate: 0.00065479]
	Learning Rate: 0.000654795
	LOSS [training: 0.3216288129314622 | validation: 0.5350106164063826]
	TIME [epoch: 9.66 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31184715804563906		[learning rate: 0.00065242]
	Learning Rate: 0.000652419
	LOSS [training: 0.31184715804563906 | validation: 0.49611422893778945]
	TIME [epoch: 9.66 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3331901010179655		[learning rate: 0.00065005]
	Learning Rate: 0.000650051
	LOSS [training: 0.3331901010179655 | validation: 0.485235091890489]
	TIME [epoch: 9.66 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38669779276463234		[learning rate: 0.00064769]
	Learning Rate: 0.000647692
	LOSS [training: 0.38669779276463234 | validation: 0.5094911202824075]
	TIME [epoch: 9.67 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5259462625523255		[learning rate: 0.00064534]
	Learning Rate: 0.000645341
	LOSS [training: 0.5259462625523255 | validation: 0.7522188875489869]
	TIME [epoch: 9.65 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3809939448196645		[learning rate: 0.000643]
	Learning Rate: 0.000642999
	LOSS [training: 0.3809939448196645 | validation: 0.5315847815813981]
	TIME [epoch: 9.66 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30812402796792726		[learning rate: 0.00064067]
	Learning Rate: 0.000640666
	LOSS [training: 0.30812402796792726 | validation: 0.530638219415687]
	TIME [epoch: 9.66 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32901077985935495		[learning rate: 0.00063834]
	Learning Rate: 0.000638341
	LOSS [training: 0.32901077985935495 | validation: 0.5263649867529372]
	TIME [epoch: 9.67 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34985301925835927		[learning rate: 0.00063602]
	Learning Rate: 0.000636024
	LOSS [training: 0.34985301925835927 | validation: 0.4476882400417319]
	TIME [epoch: 9.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_858.pth
	Model improved!!!
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32487898499851775		[learning rate: 0.00063372]
	Learning Rate: 0.000633716
	LOSS [training: 0.32487898499851775 | validation: 0.44659054858451386]
	TIME [epoch: 9.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_859.pth
	Model improved!!!
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4459923041916075		[learning rate: 0.00063142]
	Learning Rate: 0.000631416
	LOSS [training: 0.4459923041916075 | validation: 0.583650120059422]
	TIME [epoch: 9.68 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34160005854851366		[learning rate: 0.00062912]
	Learning Rate: 0.000629125
	LOSS [training: 0.34160005854851366 | validation: 0.45796616425024705]
	TIME [epoch: 9.66 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33782105121006756		[learning rate: 0.00062684]
	Learning Rate: 0.000626842
	LOSS [training: 0.33782105121006756 | validation: 0.4604699233236805]
	TIME [epoch: 9.67 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3141861382232466		[learning rate: 0.00062457]
	Learning Rate: 0.000624567
	LOSS [training: 0.3141861382232466 | validation: 0.45498456542245536]
	TIME [epoch: 9.66 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44911195186492103		[learning rate: 0.0006223]
	Learning Rate: 0.0006223
	LOSS [training: 0.44911195186492103 | validation: 0.49814870402208206]
	TIME [epoch: 9.69 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34470973169468755		[learning rate: 0.00062004]
	Learning Rate: 0.000620042
	LOSS [training: 0.34470973169468755 | validation: 0.5311901030549384]
	TIME [epoch: 9.66 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36116736660988824		[learning rate: 0.00061779]
	Learning Rate: 0.000617792
	LOSS [training: 0.36116736660988824 | validation: 0.46024312376027027]
	TIME [epoch: 9.66 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6344718025677537		[learning rate: 0.00061555]
	Learning Rate: 0.00061555
	LOSS [training: 0.6344718025677537 | validation: 0.5417719309285388]
	TIME [epoch: 9.66 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6395109912328956		[learning rate: 0.00061332]
	Learning Rate: 0.000613316
	LOSS [training: 0.6395109912328956 | validation: 0.4715139857547888]
	TIME [epoch: 9.68 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34267506620287225		[learning rate: 0.00061109]
	Learning Rate: 0.00061109
	LOSS [training: 0.34267506620287225 | validation: 0.41980279518473096]
	TIME [epoch: 9.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_869.pth
	Model improved!!!
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3034185695784567		[learning rate: 0.00060887]
	Learning Rate: 0.000608872
	LOSS [training: 0.3034185695784567 | validation: 0.43162268524499686]
	TIME [epoch: 9.66 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3089899895284499		[learning rate: 0.00060666]
	Learning Rate: 0.000606663
	LOSS [training: 0.3089899895284499 | validation: 0.42917836658020037]
	TIME [epoch: 9.67 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47519701970327793		[learning rate: 0.00060446]
	Learning Rate: 0.000604461
	LOSS [training: 0.47519701970327793 | validation: 0.7431218071399767]
	TIME [epoch: 9.68 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45875257942555303		[learning rate: 0.00060227]
	Learning Rate: 0.000602268
	LOSS [training: 0.45875257942555303 | validation: 0.4092256565517829]
	TIME [epoch: 9.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_873.pth
	Model improved!!!
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31281468729312206		[learning rate: 0.00060008]
	Learning Rate: 0.000600082
	LOSS [training: 0.31281468729312206 | validation: 0.3962982922641683]
	TIME [epoch: 9.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_874.pth
	Model improved!!!
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32038135186308947		[learning rate: 0.0005979]
	Learning Rate: 0.000597904
	LOSS [training: 0.32038135186308947 | validation: 0.4265438340172244]
	TIME [epoch: 9.69 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37767449645199164		[learning rate: 0.00059573]
	Learning Rate: 0.000595734
	LOSS [training: 0.37767449645199164 | validation: 0.5957448623997076]
	TIME [epoch: 9.66 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3325103789740864		[learning rate: 0.00059357]
	Learning Rate: 0.000593572
	LOSS [training: 0.3325103789740864 | validation: 0.4018421229459109]
	TIME [epoch: 9.65 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3035923839594439		[learning rate: 0.00059142]
	Learning Rate: 0.000591418
	LOSS [training: 0.3035923839594439 | validation: 0.4444359908072415]
	TIME [epoch: 9.65 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3735144221296625		[learning rate: 0.00058927]
	Learning Rate: 0.000589272
	LOSS [training: 0.3735144221296625 | validation: 0.4845685900516922]
	TIME [epoch: 9.69 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3255913918453885		[learning rate: 0.00058713]
	Learning Rate: 0.000587133
	LOSS [training: 0.3255913918453885 | validation: 0.5026015085759694]
	TIME [epoch: 9.66 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8194030186760932		[learning rate: 0.000585]
	Learning Rate: 0.000585003
	LOSS [training: 0.8194030186760932 | validation: 0.5242437645579093]
	TIME [epoch: 9.66 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3653265825454958		[learning rate: 0.00058288]
	Learning Rate: 0.00058288
	LOSS [training: 0.3653265825454958 | validation: 0.41401383087053695]
	TIME [epoch: 9.66 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33625973147404653		[learning rate: 0.00058076]
	Learning Rate: 0.000580764
	LOSS [training: 0.33625973147404653 | validation: 0.510295624879233]
	TIME [epoch: 9.68 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3992776306907243		[learning rate: 0.00057866]
	Learning Rate: 0.000578657
	LOSS [training: 0.3992776306907243 | validation: 0.4958729993235922]
	TIME [epoch: 9.66 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3468839238746158		[learning rate: 0.00057656]
	Learning Rate: 0.000576557
	LOSS [training: 0.3468839238746158 | validation: 0.4481317169565236]
	TIME [epoch: 9.66 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36785352998348053		[learning rate: 0.00057446]
	Learning Rate: 0.000574465
	LOSS [training: 0.36785352998348053 | validation: 0.45308955979830356]
	TIME [epoch: 9.69 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32782646477445176		[learning rate: 0.00057238]
	Learning Rate: 0.00057238
	LOSS [training: 0.32782646477445176 | validation: 0.4179468759872954]
	TIME [epoch: 9.67 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3421205221778115		[learning rate: 0.0005703]
	Learning Rate: 0.000570303
	LOSS [training: 0.3421205221778115 | validation: 0.4395021082014758]
	TIME [epoch: 9.67 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3217121730584854		[learning rate: 0.00056823]
	Learning Rate: 0.000568233
	LOSS [training: 0.3217121730584854 | validation: 0.42058679986226727]
	TIME [epoch: 9.66 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4924889325425331		[learning rate: 0.00056617]
	Learning Rate: 0.000566171
	LOSS [training: 0.4924889325425331 | validation: 0.5818943793855412]
	TIME [epoch: 9.69 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.516642893336295		[learning rate: 0.00056412]
	Learning Rate: 0.000564116
	LOSS [training: 0.516642893336295 | validation: 0.37272889987119456]
	TIME [epoch: 9.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_891.pth
	Model improved!!!
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3284694824609566		[learning rate: 0.00056207]
	Learning Rate: 0.000562069
	LOSS [training: 0.3284694824609566 | validation: 0.4120098157633341]
	TIME [epoch: 9.66 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33131278569170264		[learning rate: 0.00056003]
	Learning Rate: 0.000560029
	LOSS [training: 0.33131278569170264 | validation: 0.3678063262186]
	TIME [epoch: 9.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_893.pth
	Model improved!!!
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33104370526891974		[learning rate: 0.000558]
	Learning Rate: 0.000557997
	LOSS [training: 0.33104370526891974 | validation: 0.48718810108360433]
	TIME [epoch: 9.68 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32722724744678705		[learning rate: 0.00055597]
	Learning Rate: 0.000555972
	LOSS [training: 0.32722724744678705 | validation: 0.4817448069400858]
	TIME [epoch: 9.66 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33366319900716296		[learning rate: 0.00055395]
	Learning Rate: 0.000553954
	LOSS [training: 0.33366319900716296 | validation: 0.5877497637853177]
	TIME [epoch: 9.66 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30874355130045517		[learning rate: 0.00055194]
	Learning Rate: 0.000551944
	LOSS [training: 0.30874355130045517 | validation: 0.43745730329188426]
	TIME [epoch: 9.67 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3029801015060401		[learning rate: 0.00054994]
	Learning Rate: 0.000549941
	LOSS [training: 0.3029801015060401 | validation: 0.5128629929569551]
	TIME [epoch: 9.67 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3177694305458923		[learning rate: 0.00054794]
	Learning Rate: 0.000547945
	LOSS [training: 0.3177694305458923 | validation: 0.4141904791141692]
	TIME [epoch: 9.66 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3013408711910466		[learning rate: 0.00054596]
	Learning Rate: 0.000545956
	LOSS [training: 0.3013408711910466 | validation: 0.43257990765414556]
	TIME [epoch: 9.67 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33804719861498855		[learning rate: 0.00054397]
	Learning Rate: 0.000543975
	LOSS [training: 0.33804719861498855 | validation: 0.5932392466769715]
	TIME [epoch: 9.68 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33721894798330077		[learning rate: 0.000542]
	Learning Rate: 0.000542001
	LOSS [training: 0.33721894798330077 | validation: 0.41843839693145846]
	TIME [epoch: 9.66 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33172040403475994		[learning rate: 0.00054003]
	Learning Rate: 0.000540034
	LOSS [training: 0.33172040403475994 | validation: 0.5796363563893795]
	TIME [epoch: 9.65 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37905646263434023		[learning rate: 0.00053807]
	Learning Rate: 0.000538074
	LOSS [training: 0.37905646263434023 | validation: 0.4268068214421899]
	TIME [epoch: 9.65 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29528606917881306		[learning rate: 0.00053612]
	Learning Rate: 0.000536121
	LOSS [training: 0.29528606917881306 | validation: 0.4676816556475347]
	TIME [epoch: 9.68 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3876494564415485		[learning rate: 0.00053418]
	Learning Rate: 0.000534176
	LOSS [training: 0.3876494564415485 | validation: 0.4214154718230463]
	TIME [epoch: 9.66 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3045452801079039		[learning rate: 0.00053224]
	Learning Rate: 0.000532237
	LOSS [training: 0.3045452801079039 | validation: 0.4994719527106578]
	TIME [epoch: 9.66 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.338484950282984		[learning rate: 0.00053031]
	Learning Rate: 0.000530306
	LOSS [training: 0.338484950282984 | validation: 0.4142694297368915]
	TIME [epoch: 9.67 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3052355980562188		[learning rate: 0.00052838]
	Learning Rate: 0.000528381
	LOSS [training: 0.3052355980562188 | validation: 0.4766576916894044]
	TIME [epoch: 9.67 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3110035894590838		[learning rate: 0.00052646]
	Learning Rate: 0.000526464
	LOSS [training: 0.3110035894590838 | validation: 0.5416936582296417]
	TIME [epoch: 9.66 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.417769797279716		[learning rate: 0.00052455]
	Learning Rate: 0.000524553
	LOSS [training: 0.417769797279716 | validation: 0.4224986045797697]
	TIME [epoch: 9.66 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3544941643971925		[learning rate: 0.00052265]
	Learning Rate: 0.000522649
	LOSS [training: 0.3544941643971925 | validation: 0.4450544150989971]
	TIME [epoch: 9.67 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3260420156899612		[learning rate: 0.00052075]
	Learning Rate: 0.000520753
	LOSS [training: 0.3260420156899612 | validation: 0.46742085728824095]
	TIME [epoch: 9.67 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29064778552639103		[learning rate: 0.00051886]
	Learning Rate: 0.000518863
	LOSS [training: 0.29064778552639103 | validation: 0.38761749629044423]
	TIME [epoch: 9.66 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29208400060016404		[learning rate: 0.00051698]
	Learning Rate: 0.00051698
	LOSS [training: 0.29208400060016404 | validation: 0.44170798211518936]
	TIME [epoch: 9.65 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29510948023485783		[learning rate: 0.0005151]
	Learning Rate: 0.000515104
	LOSS [training: 0.29510948023485783 | validation: 0.4525313662663497]
	TIME [epoch: 9.68 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5336061478837386		[learning rate: 0.00051323]
	Learning Rate: 0.000513235
	LOSS [training: 0.5336061478837386 | validation: 0.6015838310020947]
	TIME [epoch: 9.66 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43710218900164566		[learning rate: 0.00051137]
	Learning Rate: 0.000511372
	LOSS [training: 0.43710218900164566 | validation: 0.376770752105994]
	TIME [epoch: 9.66 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3331997098920601		[learning rate: 0.00050952]
	Learning Rate: 0.000509516
	LOSS [training: 0.3331997098920601 | validation: 0.6734452032784701]
	TIME [epoch: 9.67 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4951894898733099		[learning rate: 0.00050767]
	Learning Rate: 0.000507667
	LOSS [training: 0.4951894898733099 | validation: 0.38524436370570625]
	TIME [epoch: 9.69 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32215203571693973		[learning rate: 0.00050582]
	Learning Rate: 0.000505825
	LOSS [training: 0.32215203571693973 | validation: 0.4466273113669969]
	TIME [epoch: 9.66 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41460059128665205		[learning rate: 0.00050399]
	Learning Rate: 0.000503989
	LOSS [training: 0.41460059128665205 | validation: 0.35532085911443895]
	TIME [epoch: 9.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_922.pth
	Model improved!!!
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29471505578387475		[learning rate: 0.00050216]
	Learning Rate: 0.00050216
	LOSS [training: 0.29471505578387475 | validation: 0.42484302772184074]
	TIME [epoch: 9.68 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2827852883552112		[learning rate: 0.00050034]
	Learning Rate: 0.000500338
	LOSS [training: 0.2827852883552112 | validation: 0.3872412143992056]
	TIME [epoch: 9.69 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26827285605647544		[learning rate: 0.00049852]
	Learning Rate: 0.000498522
	LOSS [training: 0.26827285605647544 | validation: 0.39850436481554463]
	TIME [epoch: 9.67 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3545929446501671		[learning rate: 0.00049671]
	Learning Rate: 0.000496713
	LOSS [training: 0.3545929446501671 | validation: 0.5071157735252954]
	TIME [epoch: 9.68 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3017114786387639		[learning rate: 0.00049491]
	Learning Rate: 0.00049491
	LOSS [training: 0.3017114786387639 | validation: 0.47994180619180604]
	TIME [epoch: 9.69 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3201400690881085		[learning rate: 0.00049311]
	Learning Rate: 0.000493114
	LOSS [training: 0.3201400690881085 | validation: 0.3990906345016802]
	TIME [epoch: 9.68 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29817649792500645		[learning rate: 0.00049132]
	Learning Rate: 0.000491325
	LOSS [training: 0.29817649792500645 | validation: 0.37537324310312087]
	TIME [epoch: 9.67 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29889053840330976		[learning rate: 0.00048954]
	Learning Rate: 0.000489542
	LOSS [training: 0.29889053840330976 | validation: 0.38647341886183173]
	TIME [epoch: 9.68 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2773144717436707		[learning rate: 0.00048776]
	Learning Rate: 0.000487765
	LOSS [training: 0.2773144717436707 | validation: 0.37313542211161405]
	TIME [epoch: 9.7 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3095257919567206		[learning rate: 0.00048599]
	Learning Rate: 0.000485995
	LOSS [training: 0.3095257919567206 | validation: 0.46057447191594875]
	TIME [epoch: 9.68 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36600244646135327		[learning rate: 0.00048423]
	Learning Rate: 0.000484231
	LOSS [training: 0.36600244646135327 | validation: 0.7078592615247734]
	TIME [epoch: 9.68 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4105657933272475		[learning rate: 0.00048247]
	Learning Rate: 0.000482474
	LOSS [training: 0.4105657933272475 | validation: 0.41146967983211413]
	TIME [epoch: 9.68 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28085584386322066		[learning rate: 0.00048072]
	Learning Rate: 0.000480723
	LOSS [training: 0.28085584386322066 | validation: 0.379972929518529]
	TIME [epoch: 9.7 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29187454453017403		[learning rate: 0.00047898]
	Learning Rate: 0.000478978
	LOSS [training: 0.29187454453017403 | validation: 0.4006575327487427]
	TIME [epoch: 9.68 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30090541178639907		[learning rate: 0.00047724]
	Learning Rate: 0.00047724
	LOSS [training: 0.30090541178639907 | validation: 0.4621277122804088]
	TIME [epoch: 9.68 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3206533732424255		[learning rate: 0.00047551]
	Learning Rate: 0.000475508
	LOSS [training: 0.3206533732424255 | validation: 0.4575330615056912]
	TIME [epoch: 9.69 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38056150965330426		[learning rate: 0.00047378]
	Learning Rate: 0.000473782
	LOSS [training: 0.38056150965330426 | validation: 0.3896422105614596]
	TIME [epoch: 9.69 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3024936391114753		[learning rate: 0.00047206]
	Learning Rate: 0.000472063
	LOSS [training: 0.3024936391114753 | validation: 0.3841347107049539]
	TIME [epoch: 9.68 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29238687627207616		[learning rate: 0.00047035]
	Learning Rate: 0.00047035
	LOSS [training: 0.29238687627207616 | validation: 0.39714811341582584]
	TIME [epoch: 9.67 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3033982171635479		[learning rate: 0.00046864]
	Learning Rate: 0.000468643
	LOSS [training: 0.3033982171635479 | validation: 0.546417881437576]
	TIME [epoch: 9.7 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7904414412989695		[learning rate: 0.00046694]
	Learning Rate: 0.000466942
	LOSS [training: 0.7904414412989695 | validation: 0.6478642215793905]
	TIME [epoch: 9.68 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3897304398820903		[learning rate: 0.00046525]
	Learning Rate: 0.000465248
	LOSS [training: 0.3897304398820903 | validation: 0.36964447331387534]
	TIME [epoch: 9.68 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.336565412911401		[learning rate: 0.00046356]
	Learning Rate: 0.000463559
	LOSS [training: 0.336565412911401 | validation: 0.5075930847447068]
	TIME [epoch: 9.68 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34276187425970606		[learning rate: 0.00046188]
	Learning Rate: 0.000461877
	LOSS [training: 0.34276187425970606 | validation: 0.34836351599097043]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_946.pth
	Model improved!!!
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3067099949333273		[learning rate: 0.0004602]
	Learning Rate: 0.000460201
	LOSS [training: 0.3067099949333273 | validation: 0.3537730831384188]
	TIME [epoch: 9.68 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2792904021992532		[learning rate: 0.00045853]
	Learning Rate: 0.000458531
	LOSS [training: 0.2792904021992532 | validation: 0.34193620103112965]
	TIME [epoch: 9.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_948.pth
	Model improved!!!
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39055229152044857		[learning rate: 0.00045687]
	Learning Rate: 0.000456867
	LOSS [training: 0.39055229152044857 | validation: 0.5720826634074122]
	TIME [epoch: 9.68 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3726554895533109		[learning rate: 0.00045521]
	Learning Rate: 0.000455209
	LOSS [training: 0.3726554895533109 | validation: 0.3795600775501554]
	TIME [epoch: 9.7 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2953112152741835		[learning rate: 0.00045356]
	Learning Rate: 0.000453557
	LOSS [training: 0.2953112152741835 | validation: 0.44154137131419646]
	TIME [epoch: 9.68 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33921525580077405		[learning rate: 0.00045191]
	Learning Rate: 0.000451911
	LOSS [training: 0.33921525580077405 | validation: 0.43626454829681055]
	TIME [epoch: 9.68 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2927868676497226		[learning rate: 0.00045027]
	Learning Rate: 0.000450271
	LOSS [training: 0.2927868676497226 | validation: 0.4029620406398607]
	TIME [epoch: 9.7 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2875866993566226		[learning rate: 0.00044864]
	Learning Rate: 0.000448637
	LOSS [training: 0.2875866993566226 | validation: 0.38042365670778205]
	TIME [epoch: 9.68 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3145004642999395		[learning rate: 0.00044701]
	Learning Rate: 0.000447009
	LOSS [training: 0.3145004642999395 | validation: 0.4275697231588126]
	TIME [epoch: 9.68 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3015842295721442		[learning rate: 0.00044539]
	Learning Rate: 0.000445386
	LOSS [training: 0.3015842295721442 | validation: 0.44034966398860276]
	TIME [epoch: 9.68 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2815614905022489		[learning rate: 0.00044377]
	Learning Rate: 0.00044377
	LOSS [training: 0.2815614905022489 | validation: 0.3666821414393896]
	TIME [epoch: 9.69 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3408979007724212		[learning rate: 0.00044216]
	Learning Rate: 0.00044216
	LOSS [training: 0.3408979007724212 | validation: 0.6671320921656714]
	TIME [epoch: 9.68 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4793846213064333		[learning rate: 0.00044055]
	Learning Rate: 0.000440555
	LOSS [training: 0.4793846213064333 | validation: 0.5348920632300628]
	TIME [epoch: 9.68 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3836124934337065		[learning rate: 0.00043896]
	Learning Rate: 0.000438956
	LOSS [training: 0.3836124934337065 | validation: 0.400145657329685]
	TIME [epoch: 9.68 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2780870472133211		[learning rate: 0.00043736]
	Learning Rate: 0.000437363
	LOSS [training: 0.2780870472133211 | validation: 0.41382097577118304]
	TIME [epoch: 9.71 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29014144996938585		[learning rate: 0.00043578]
	Learning Rate: 0.000435776
	LOSS [training: 0.29014144996938585 | validation: 0.414312150451492]
	TIME [epoch: 9.68 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2875713008617946		[learning rate: 0.00043419]
	Learning Rate: 0.000434194
	LOSS [training: 0.2875713008617946 | validation: 0.3837727998520964]
	TIME [epoch: 9.68 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2954004247162409		[learning rate: 0.00043262]
	Learning Rate: 0.000432619
	LOSS [training: 0.2954004247162409 | validation: 0.5570532438794604]
	TIME [epoch: 9.69 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33799144725146324		[learning rate: 0.00043105]
	Learning Rate: 0.000431049
	LOSS [training: 0.33799144725146324 | validation: 0.38311408710723655]
	TIME [epoch: 9.68 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2912460866413284		[learning rate: 0.00042948]
	Learning Rate: 0.000429484
	LOSS [training: 0.2912460866413284 | validation: 0.416184873717597]
	TIME [epoch: 9.66 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32012831760816257		[learning rate: 0.00042793]
	Learning Rate: 0.000427926
	LOSS [training: 0.32012831760816257 | validation: 0.3569669125383813]
	TIME [epoch: 9.66 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3469469909395892		[learning rate: 0.00042637]
	Learning Rate: 0.000426373
	LOSS [training: 0.3469469909395892 | validation: 0.41881577175482426]
	TIME [epoch: 9.68 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2824226226301763		[learning rate: 0.00042483]
	Learning Rate: 0.000424825
	LOSS [training: 0.2824226226301763 | validation: 0.5141182426545887]
	TIME [epoch: 9.68 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31889617334580056		[learning rate: 0.00042328]
	Learning Rate: 0.000423284
	LOSS [training: 0.31889617334580056 | validation: 0.5446628415797653]
	TIME [epoch: 9.66 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3245925455385937		[learning rate: 0.00042175]
	Learning Rate: 0.000421748
	LOSS [training: 0.3245925455385937 | validation: 0.4227050490375823]
	TIME [epoch: 9.66 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2930303285371174		[learning rate: 0.00042022]
	Learning Rate: 0.000420217
	LOSS [training: 0.2930303285371174 | validation: 0.5785561602087029]
	TIME [epoch: 9.69 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30367410731656896		[learning rate: 0.00041869]
	Learning Rate: 0.000418692
	LOSS [training: 0.30367410731656896 | validation: 0.38729969605205145]
	TIME [epoch: 9.66 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2871619693360148		[learning rate: 0.00041717]
	Learning Rate: 0.000417173
	LOSS [training: 0.2871619693360148 | validation: 0.3907777053605392]
	TIME [epoch: 9.66 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3138305051692149		[learning rate: 0.00041566]
	Learning Rate: 0.000415659
	LOSS [training: 0.3138305051692149 | validation: 0.4616500273459432]
	TIME [epoch: 9.67 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32213272886670846		[learning rate: 0.00041415]
	Learning Rate: 0.00041415
	LOSS [training: 0.32213272886670846 | validation: 0.37304222399710646]
	TIME [epoch: 9.69 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2996501624860912		[learning rate: 0.00041265]
	Learning Rate: 0.000412647
	LOSS [training: 0.2996501624860912 | validation: 0.45619025240557465]
	TIME [epoch: 9.66 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3299001998714218		[learning rate: 0.00041115]
	Learning Rate: 0.00041115
	LOSS [training: 0.3299001998714218 | validation: 0.4375488560116962]
	TIME [epoch: 9.66 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44867945835674006		[learning rate: 0.00040966]
	Learning Rate: 0.000409658
	LOSS [training: 0.44867945835674006 | validation: 0.42104072351520105]
	TIME [epoch: 9.67 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33219781196236603		[learning rate: 0.00040817]
	Learning Rate: 0.000408171
	LOSS [training: 0.33219781196236603 | validation: 0.4082004012100679]
	TIME [epoch: 9.67 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31503921012503144		[learning rate: 0.00040669]
	Learning Rate: 0.00040669
	LOSS [training: 0.31503921012503144 | validation: 0.36582373682217123]
	TIME [epoch: 9.67 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29725278919254977		[learning rate: 0.00040521]
	Learning Rate: 0.000405214
	LOSS [training: 0.29725278919254977 | validation: 0.4707829020629315]
	TIME [epoch: 9.67 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.294015812767101		[learning rate: 0.00040374]
	Learning Rate: 0.000403743
	LOSS [training: 0.294015812767101 | validation: 0.46788226877081246]
	TIME [epoch: 9.68 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3671967391069547		[learning rate: 0.00040228]
	Learning Rate: 0.000402278
	LOSS [training: 0.3671967391069547 | validation: 0.4466258400362784]
	TIME [epoch: 9.67 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31241053657660445		[learning rate: 0.00040082]
	Learning Rate: 0.000400818
	LOSS [training: 0.31241053657660445 | validation: 0.3892528661330302]
	TIME [epoch: 9.66 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2719264031349029		[learning rate: 0.00039936]
	Learning Rate: 0.000399364
	LOSS [training: 0.2719264031349029 | validation: 0.36702948305765093]
	TIME [epoch: 9.66 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.275634301789646		[learning rate: 0.00039791]
	Learning Rate: 0.000397914
	LOSS [training: 0.275634301789646 | validation: 0.42611393522866714]
	TIME [epoch: 9.68 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2813827908494805		[learning rate: 0.00039647]
	Learning Rate: 0.00039647
	LOSS [training: 0.2813827908494805 | validation: 0.38207749325085727]
	TIME [epoch: 9.66 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28450093537377347		[learning rate: 0.00039503]
	Learning Rate: 0.000395031
	LOSS [training: 0.28450093537377347 | validation: 0.379508498858998]
	TIME [epoch: 9.67 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.272950002709046		[learning rate: 0.0003936]
	Learning Rate: 0.000393598
	LOSS [training: 0.272950002709046 | validation: 0.3703854874445338]
	TIME [epoch: 9.67 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27537168326208505		[learning rate: 0.00039217]
	Learning Rate: 0.000392169
	LOSS [training: 0.27537168326208505 | validation: 0.39135808945353084]
	TIME [epoch: 9.68 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2840401847653151		[learning rate: 0.00039075]
	Learning Rate: 0.000390746
	LOSS [training: 0.2840401847653151 | validation: 0.39312812914946127]
	TIME [epoch: 9.66 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3353567548048223		[learning rate: 0.00038933]
	Learning Rate: 0.000389328
	LOSS [training: 0.3353567548048223 | validation: 0.3484231870813511]
	TIME [epoch: 9.67 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2731258521141006		[learning rate: 0.00038792]
	Learning Rate: 0.000387915
	LOSS [training: 0.2731258521141006 | validation: 0.4960328653392494]
	TIME [epoch: 9.68 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3433910929665543		[learning rate: 0.00038651]
	Learning Rate: 0.000386508
	LOSS [training: 0.3433910929665543 | validation: 0.4656559788547584]
	TIME [epoch: 9.67 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33358965551382413		[learning rate: 0.0003851]
	Learning Rate: 0.000385105
	LOSS [training: 0.33358965551382413 | validation: 0.42698167054165953]
	TIME [epoch: 9.67 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2763601799197454		[learning rate: 0.00038371]
	Learning Rate: 0.000383707
	LOSS [training: 0.2763601799197454 | validation: 0.43338524708976806]
	TIME [epoch: 9.67 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2906768799732776		[learning rate: 0.00038231]
	Learning Rate: 0.000382315
	LOSS [training: 0.2906768799732776 | validation: 0.42085805849860214]
	TIME [epoch: 9.69 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28240731930560187		[learning rate: 0.00038093]
	Learning Rate: 0.000380927
	LOSS [training: 0.28240731930560187 | validation: 0.40597126192494165]
	TIME [epoch: 9.66 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27396332250188316		[learning rate: 0.00037954]
	Learning Rate: 0.000379545
	LOSS [training: 0.27396332250188316 | validation: 0.38906513218235944]
	TIME [epoch: 9.67 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2890509197206805		[learning rate: 0.00037817]
	Learning Rate: 0.000378167
	LOSS [training: 0.2890509197206805 | validation: 0.38206281007598253]
	TIME [epoch: 9.66 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30176096917072603		[learning rate: 0.0003768]
	Learning Rate: 0.000376795
	LOSS [training: 0.30176096917072603 | validation: 0.3878926318952143]
	TIME [epoch: 9.68 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2972429447614497		[learning rate: 0.00037543]
	Learning Rate: 0.000375428
	LOSS [training: 0.2972429447614497 | validation: 0.3738266859230744]
	TIME [epoch: 9.67 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27246049329583616		[learning rate: 0.00037407]
	Learning Rate: 0.000374065
	LOSS [training: 0.27246049329583616 | validation: 0.37399792767134343]
	TIME [epoch: 9.66 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26221754250317647		[learning rate: 0.00037271]
	Learning Rate: 0.000372708
	LOSS [training: 0.26221754250317647 | validation: 0.3657575361153671]
	TIME [epoch: 9.66 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2664451776538045		[learning rate: 0.00037136]
	Learning Rate: 0.000371355
	LOSS [training: 0.2664451776538045 | validation: 0.5060981325255741]
	TIME [epoch: 9.68 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3169017847076351		[learning rate: 0.00037001]
	Learning Rate: 0.000370008
	LOSS [training: 0.3169017847076351 | validation: 0.5199918406926228]
	TIME [epoch: 9.66 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34686361946495764		[learning rate: 0.00036866]
	Learning Rate: 0.000368665
	LOSS [training: 0.34686361946495764 | validation: 0.5242599238591309]
	TIME [epoch: 9.66 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32121393285084515		[learning rate: 0.00036733]
	Learning Rate: 0.000367327
	LOSS [training: 0.32121393285084515 | validation: 0.4333479085480056]
	TIME [epoch: 9.68 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2778923162942246		[learning rate: 0.00036599]
	Learning Rate: 0.000365994
	LOSS [training: 0.2778923162942246 | validation: 0.41000738182244095]
	TIME [epoch: 9.67 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2794272007670058		[learning rate: 0.00036467]
	Learning Rate: 0.000364666
	LOSS [training: 0.2794272007670058 | validation: 0.43025078158984015]
	TIME [epoch: 9.66 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2885135817069961		[learning rate: 0.00036334]
	Learning Rate: 0.000363342
	LOSS [training: 0.2885135817069961 | validation: 0.4030977141231334]
	TIME [epoch: 9.67 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32833088727683124		[learning rate: 0.00036202]
	Learning Rate: 0.000362024
	LOSS [training: 0.32833088727683124 | validation: 0.4349891539243679]
	TIME [epoch: 9.69 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35033994015762326		[learning rate: 0.00036071]
	Learning Rate: 0.00036071
	LOSS [training: 0.35033994015762326 | validation: 0.45284713991700515]
	TIME [epoch: 9.66 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3347793581960527		[learning rate: 0.0003594]
	Learning Rate: 0.000359401
	LOSS [training: 0.3347793581960527 | validation: 0.40948198901076366]
	TIME [epoch: 9.66 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26353492321842314		[learning rate: 0.0003581]
	Learning Rate: 0.000358096
	LOSS [training: 0.26353492321842314 | validation: 0.47935697655587356]
	TIME [epoch: 9.66 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30489101288451204		[learning rate: 0.0003568]
	Learning Rate: 0.000356797
	LOSS [training: 0.30489101288451204 | validation: 0.4340970308594306]
	TIME [epoch: 9.69 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2849987950721036		[learning rate: 0.0003555]
	Learning Rate: 0.000355502
	LOSS [training: 0.2849987950721036 | validation: 0.42809430877103183]
	TIME [epoch: 9.67 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3290322757711599		[learning rate: 0.00035421]
	Learning Rate: 0.000354212
	LOSS [training: 0.3290322757711599 | validation: 0.3654132909312996]
	TIME [epoch: 9.67 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26687277397387765		[learning rate: 0.00035293]
	Learning Rate: 0.000352926
	LOSS [training: 0.26687277397387765 | validation: 0.40594632131543096]
	TIME [epoch: 9.68 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2658717184159919		[learning rate: 0.00035165]
	Learning Rate: 0.000351646
	LOSS [training: 0.2658717184159919 | validation: 0.37060305856559517]
	TIME [epoch: 9.67 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2680628995808472		[learning rate: 0.00035037]
	Learning Rate: 0.00035037
	LOSS [training: 0.2680628995808472 | validation: 0.35601992249114284]
	TIME [epoch: 9.67 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25296473680026665		[learning rate: 0.0003491]
	Learning Rate: 0.000349098
	LOSS [training: 0.25296473680026665 | validation: 0.4273297066748814]
	TIME [epoch: 9.67 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28165986989439357		[learning rate: 0.00034783]
	Learning Rate: 0.000347831
	LOSS [training: 0.28165986989439357 | validation: 0.3567326787945645]
	TIME [epoch: 9.68 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27105303401678177		[learning rate: 0.00034657]
	Learning Rate: 0.000346569
	LOSS [training: 0.27105303401678177 | validation: 0.36193804251389083]
	TIME [epoch: 9.67 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2689723822921436		[learning rate: 0.00034531]
	Learning Rate: 0.000345311
	LOSS [training: 0.2689723822921436 | validation: 0.3526551345013646]
	TIME [epoch: 9.67 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25436602763248006		[learning rate: 0.00034406]
	Learning Rate: 0.000344058
	LOSS [training: 0.25436602763248006 | validation: 0.4220870166223509]
	TIME [epoch: 9.67 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3119696218900664		[learning rate: 0.00034281]
	Learning Rate: 0.000342809
	LOSS [training: 0.3119696218900664 | validation: 0.34966240515842406]
	TIME [epoch: 9.69 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26654606027564665		[learning rate: 0.00034157]
	Learning Rate: 0.000341565
	LOSS [training: 0.26654606027564665 | validation: 0.37783807000122804]
	TIME [epoch: 9.66 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2625805515392081		[learning rate: 0.00034033]
	Learning Rate: 0.000340326
	LOSS [training: 0.2625805515392081 | validation: 0.3650241228007877]
	TIME [epoch: 9.67 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30480408373362466		[learning rate: 0.00033909]
	Learning Rate: 0.000339091
	LOSS [training: 0.30480408373362466 | validation: 0.5031556620381893]
	TIME [epoch: 9.67 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30737916480123595		[learning rate: 0.00033786]
	Learning Rate: 0.00033786
	LOSS [training: 0.30737916480123595 | validation: 0.3579000586664215]
	TIME [epoch: 9.69 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2621337809590229		[learning rate: 0.00033663]
	Learning Rate: 0.000336634
	LOSS [training: 0.2621337809590229 | validation: 0.3681726597657681]
	TIME [epoch: 9.66 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27955948970939526		[learning rate: 0.00033541]
	Learning Rate: 0.000335412
	LOSS [training: 0.27955948970939526 | validation: 0.38941145671628613]
	TIME [epoch: 9.67 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33928345492703815		[learning rate: 0.0003342]
	Learning Rate: 0.000334195
	LOSS [training: 0.33928345492703815 | validation: 0.417084349748889]
	TIME [epoch: 9.67 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4220068007380802		[learning rate: 0.00033298]
	Learning Rate: 0.000332982
	LOSS [training: 0.4220068007380802 | validation: 0.3757408574869158]
	TIME [epoch: 9.68 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3536065623573955		[learning rate: 0.00033177]
	Learning Rate: 0.000331774
	LOSS [training: 0.3536065623573955 | validation: 0.39428198785810714]
	TIME [epoch: 9.66 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29110369092116806		[learning rate: 0.00033057]
	Learning Rate: 0.00033057
	LOSS [training: 0.29110369092116806 | validation: 0.36431843662330976]
	TIME [epoch: 9.67 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3181213100386425		[learning rate: 0.00032937]
	Learning Rate: 0.00032937
	LOSS [training: 0.3181213100386425 | validation: 0.49957688867815675]
	TIME [epoch: 9.68 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.290550498875344		[learning rate: 0.00032817]
	Learning Rate: 0.000328175
	LOSS [training: 0.290550498875344 | validation: 0.3486878717376743]
	TIME [epoch: 9.67 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2514143054206511		[learning rate: 0.00032698]
	Learning Rate: 0.000326984
	LOSS [training: 0.2514143054206511 | validation: 0.3499627401491038]
	TIME [epoch: 9.66 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2489647998089118		[learning rate: 0.0003258]
	Learning Rate: 0.000325797
	LOSS [training: 0.2489647998089118 | validation: 0.3774283426959366]
	TIME [epoch: 9.67 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2464797803377782		[learning rate: 0.00032461]
	Learning Rate: 0.000324615
	LOSS [training: 0.2464797803377782 | validation: 0.39761035660162464]
	TIME [epoch: 9.69 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2823074879031671		[learning rate: 0.00032344]
	Learning Rate: 0.000323437
	LOSS [training: 0.2823074879031671 | validation: 0.42766814815855925]
	TIME [epoch: 9.67 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.259831267116308		[learning rate: 0.00032226]
	Learning Rate: 0.000322263
	LOSS [training: 0.259831267116308 | validation: 0.4532864692002479]
	TIME [epoch: 9.67 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29288008417545175		[learning rate: 0.00032109]
	Learning Rate: 0.000321094
	LOSS [training: 0.29288008417545175 | validation: 0.36258656354383334]
	TIME [epoch: 9.68 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27593797640374995		[learning rate: 0.00031993]
	Learning Rate: 0.000319928
	LOSS [training: 0.27593797640374995 | validation: 0.44128363875241333]
	TIME [epoch: 9.68 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2951322738219954		[learning rate: 0.00031877]
	Learning Rate: 0.000318767
	LOSS [training: 0.2951322738219954 | validation: 0.41237093471487896]
	TIME [epoch: 9.67 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3601599335399599		[learning rate: 0.00031761]
	Learning Rate: 0.000317611
	LOSS [training: 0.3601599335399599 | validation: 0.38040754603611804]
	TIME [epoch: 9.67 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3239264307368719		[learning rate: 0.00031646]
	Learning Rate: 0.000316458
	LOSS [training: 0.3239264307368719 | validation: 0.47577250914442787]
	TIME [epoch: 9.68 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3590658572100123		[learning rate: 0.00031531]
	Learning Rate: 0.000315309
	LOSS [training: 0.3590658572100123 | validation: 0.628202865226073]
	TIME [epoch: 9.67 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33688872101303824		[learning rate: 0.00031417]
	Learning Rate: 0.000314165
	LOSS [training: 0.33688872101303824 | validation: 0.4742813131751031]
	TIME [epoch: 9.67 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27951267027856563		[learning rate: 0.00031302]
	Learning Rate: 0.000313025
	LOSS [training: 0.27951267027856563 | validation: 0.4074323792720624]
	TIME [epoch: 9.67 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30764664431220984		[learning rate: 0.00031189]
	Learning Rate: 0.000311889
	LOSS [training: 0.30764664431220984 | validation: 0.4176178891355064]
	TIME [epoch: 9.69 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2848153647863484		[learning rate: 0.00031076]
	Learning Rate: 0.000310757
	LOSS [training: 0.2848153647863484 | validation: 0.4111664516604688]
	TIME [epoch: 9.67 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31009764974021375		[learning rate: 0.00030963]
	Learning Rate: 0.000309629
	LOSS [training: 0.31009764974021375 | validation: 0.5615917105191965]
	TIME [epoch: 9.66 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2890084935038312		[learning rate: 0.00030851]
	Learning Rate: 0.000308506
	LOSS [training: 0.2890084935038312 | validation: 0.4137026498596967]
	TIME [epoch: 9.66 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28719347582543253		[learning rate: 0.00030739]
	Learning Rate: 0.000307386
	LOSS [training: 0.28719347582543253 | validation: 0.48516227589109606]
	TIME [epoch: 9.7 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29247597128371433		[learning rate: 0.00030627]
	Learning Rate: 0.000306271
	LOSS [training: 0.29247597128371433 | validation: 0.3934965708007922]
	TIME [epoch: 9.66 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27546404979554134		[learning rate: 0.00030516]
	Learning Rate: 0.000305159
	LOSS [training: 0.27546404979554134 | validation: 0.46768348944856825]
	TIME [epoch: 9.66 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2932450707972782		[learning rate: 0.00030405]
	Learning Rate: 0.000304052
	LOSS [training: 0.2932450707972782 | validation: 0.40565156361866]
	TIME [epoch: 9.67 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2658546312937508		[learning rate: 0.00030295]
	Learning Rate: 0.000302948
	LOSS [training: 0.2658546312937508 | validation: 0.4006267525732011]
	TIME [epoch: 9.68 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27102526061162935		[learning rate: 0.00030185]
	Learning Rate: 0.000301849
	LOSS [training: 0.27102526061162935 | validation: 0.35478298144444465]
	TIME [epoch: 9.67 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.272197049088049		[learning rate: 0.00030075]
	Learning Rate: 0.000300753
	LOSS [training: 0.272197049088049 | validation: 0.37757514016907245]
	TIME [epoch: 9.67 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2694312325347506		[learning rate: 0.00029966]
	Learning Rate: 0.000299662
	LOSS [training: 0.2694312325347506 | validation: 0.4114225030352647]
	TIME [epoch: 9.69 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2751776017851408		[learning rate: 0.00029857]
	Learning Rate: 0.000298574
	LOSS [training: 0.2751776017851408 | validation: 0.43601111721633246]
	TIME [epoch: 9.67 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.342518918447081		[learning rate: 0.00029749]
	Learning Rate: 0.000297491
	LOSS [training: 0.342518918447081 | validation: 0.39601145520278325]
	TIME [epoch: 9.66 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2549965719162922		[learning rate: 0.00029641]
	Learning Rate: 0.000296411
	LOSS [training: 0.2549965719162922 | validation: 0.40006691643852593]
	TIME [epoch: 9.66 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25382396747601776		[learning rate: 0.00029534]
	Learning Rate: 0.000295336
	LOSS [training: 0.25382396747601776 | validation: 0.3795782631348044]
	TIME [epoch: 9.69 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2741964826007546		[learning rate: 0.00029426]
	Learning Rate: 0.000294264
	LOSS [training: 0.2741964826007546 | validation: 0.394967630519516]
	TIME [epoch: 9.66 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2745748917997671		[learning rate: 0.0002932]
	Learning Rate: 0.000293196
	LOSS [training: 0.2745748917997671 | validation: 0.37418241165630534]
	TIME [epoch: 9.66 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2715675937619347		[learning rate: 0.00029213]
	Learning Rate: 0.000292132
	LOSS [training: 0.2715675937619347 | validation: 0.3717093981136666]
	TIME [epoch: 9.67 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29884997326510476		[learning rate: 0.00029107]
	Learning Rate: 0.000291072
	LOSS [training: 0.29884997326510476 | validation: 0.38363586680791323]
	TIME [epoch: 9.68 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30560688677617953		[learning rate: 0.00029002]
	Learning Rate: 0.000290015
	LOSS [training: 0.30560688677617953 | validation: 0.429702730620353]
	TIME [epoch: 9.66 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2718603303124406		[learning rate: 0.00028896]
	Learning Rate: 0.000288963
	LOSS [training: 0.2718603303124406 | validation: 0.41501338341947674]
	TIME [epoch: 9.66 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2616040083517118		[learning rate: 0.00028791]
	Learning Rate: 0.000287914
	LOSS [training: 0.2616040083517118 | validation: 0.3592906293186632]
	TIME [epoch: 9.66 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2561577473741621		[learning rate: 0.00028687]
	Learning Rate: 0.000286869
	LOSS [training: 0.2561577473741621 | validation: 0.3525730173007635]
	TIME [epoch: 9.68 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24750019466130047		[learning rate: 0.00028583]
	Learning Rate: 0.000285828
	LOSS [training: 0.24750019466130047 | validation: 0.3529068258286712]
	TIME [epoch: 9.67 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30348723522990656		[learning rate: 0.00028479]
	Learning Rate: 0.000284791
	LOSS [training: 0.30348723522990656 | validation: 0.36102797940271175]
	TIME [epoch: 9.66 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2565976887713363		[learning rate: 0.00028376]
	Learning Rate: 0.000283758
	LOSS [training: 0.2565976887713363 | validation: 0.3717581927159889]
	TIME [epoch: 9.68 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24800993314668687		[learning rate: 0.00028273]
	Learning Rate: 0.000282728
	LOSS [training: 0.24800993314668687 | validation: 0.3627385001790877]
	TIME [epoch: 9.67 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26867452415341603		[learning rate: 0.0002817]
	Learning Rate: 0.000281702
	LOSS [training: 0.26867452415341603 | validation: 0.37596247389522147]
	TIME [epoch: 9.67 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29267772307599155		[learning rate: 0.00028068]
	Learning Rate: 0.000280679
	LOSS [training: 0.29267772307599155 | validation: 0.3859147556718869]
	TIME [epoch: 9.66 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24462523193420144		[learning rate: 0.00027966]
	Learning Rate: 0.000279661
	LOSS [training: 0.24462523193420144 | validation: 0.3477605929763698]
	TIME [epoch: 9.68 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2584026782273233		[learning rate: 0.00027865]
	Learning Rate: 0.000278646
	LOSS [training: 0.2584026782273233 | validation: 0.3555681380231729]
	TIME [epoch: 9.67 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26562446873080503		[learning rate: 0.00027763]
	Learning Rate: 0.000277635
	LOSS [training: 0.26562446873080503 | validation: 0.3874166183993372]
	TIME [epoch: 9.67 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2541864511811786		[learning rate: 0.00027663]
	Learning Rate: 0.000276627
	LOSS [training: 0.2541864511811786 | validation: 0.3601036560727313]
	TIME [epoch: 9.67 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24559046607490598		[learning rate: 0.00027562]
	Learning Rate: 0.000275623
	LOSS [training: 0.24559046607490598 | validation: 0.3639683828824969]
	TIME [epoch: 9.69 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2446117894702185		[learning rate: 0.00027462]
	Learning Rate: 0.000274623
	LOSS [training: 0.2446117894702185 | validation: 0.36591815775990855]
	TIME [epoch: 9.65 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27562048677130335		[learning rate: 0.00027363]
	Learning Rate: 0.000273626
	LOSS [training: 0.27562048677130335 | validation: 0.41601659459032464]
	TIME [epoch: 9.66 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2452437817740228		[learning rate: 0.00027263]
	Learning Rate: 0.000272633
	LOSS [training: 0.2452437817740228 | validation: 0.36309371689157643]
	TIME [epoch: 9.68 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23626695812763737		[learning rate: 0.00027164]
	Learning Rate: 0.000271644
	LOSS [training: 0.23626695812763737 | validation: 0.3691948609618973]
	TIME [epoch: 9.67 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24459079740715678		[learning rate: 0.00027066]
	Learning Rate: 0.000270658
	LOSS [training: 0.24459079740715678 | validation: 0.5194599702324519]
	TIME [epoch: 9.66 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36578640565478465		[learning rate: 0.00026968]
	Learning Rate: 0.000269676
	LOSS [training: 0.36578640565478465 | validation: 0.4931545220047619]
	TIME [epoch: 9.66 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.288303559982682		[learning rate: 0.0002687]
	Learning Rate: 0.000268697
	LOSS [training: 0.288303559982682 | validation: 0.353080992445057]
	TIME [epoch: 9.67 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2617391043356287		[learning rate: 0.00026772]
	Learning Rate: 0.000267722
	LOSS [training: 0.2617391043356287 | validation: 0.40637822876970076]
	TIME [epoch: 9.67 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27918691439704124		[learning rate: 0.00026675]
	Learning Rate: 0.000266751
	LOSS [training: 0.27918691439704124 | validation: 0.5389945555519958]
	TIME [epoch: 9.66 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3020651578132961		[learning rate: 0.00026578]
	Learning Rate: 0.000265782
	LOSS [training: 0.3020651578132961 | validation: 0.39151144805106286]
	TIME [epoch: 9.66 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26419442016351624		[learning rate: 0.00026482]
	Learning Rate: 0.000264818
	LOSS [training: 0.26419442016351624 | validation: 0.35421909873245305]
	TIME [epoch: 9.69 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25817866983756776		[learning rate: 0.00026386]
	Learning Rate: 0.000263857
	LOSS [training: 0.25817866983756776 | validation: 0.4582039530713472]
	TIME [epoch: 9.67 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2775564489546131		[learning rate: 0.0002629]
	Learning Rate: 0.000262899
	LOSS [training: 0.2775564489546131 | validation: 0.39803145435532644]
	TIME [epoch: 9.66 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26704547870571005		[learning rate: 0.00026195]
	Learning Rate: 0.000261945
	LOSS [training: 0.26704547870571005 | validation: 0.38091781038844913]
	TIME [epoch: 9.67 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2780284988595501		[learning rate: 0.00026099]
	Learning Rate: 0.000260995
	LOSS [training: 0.2780284988595501 | validation: 0.38237060461948175]
	TIME [epoch: 9.69 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2985137157171562		[learning rate: 0.00026005]
	Learning Rate: 0.000260047
	LOSS [training: 0.2985137157171562 | validation: 0.4628196316359947]
	TIME [epoch: 9.67 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.285207045229006		[learning rate: 0.0002591]
	Learning Rate: 0.000259104
	LOSS [training: 0.285207045229006 | validation: 0.4648169934707646]
	TIME [epoch: 9.67 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26370928909147995		[learning rate: 0.00025816]
	Learning Rate: 0.000258163
	LOSS [training: 0.26370928909147995 | validation: 0.38306300146405264]
	TIME [epoch: 9.68 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24963804916847399		[learning rate: 0.00025723]
	Learning Rate: 0.000257227
	LOSS [training: 0.24963804916847399 | validation: 0.3506708469100245]
	TIME [epoch: 9.67 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29377342093765796		[learning rate: 0.00025629]
	Learning Rate: 0.000256293
	LOSS [training: 0.29377342093765796 | validation: 0.37161679662194297]
	TIME [epoch: 9.67 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27149939823931424		[learning rate: 0.00025536]
	Learning Rate: 0.000255363
	LOSS [training: 0.27149939823931424 | validation: 0.3391039742309097]
	TIME [epoch: 9.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_1109.pth
	Model improved!!!
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24692493690849515		[learning rate: 0.00025444]
	Learning Rate: 0.000254436
	LOSS [training: 0.24692493690849515 | validation: 0.3906875075271789]
	TIME [epoch: 9.68 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26786912566622795		[learning rate: 0.00025351]
	Learning Rate: 0.000253513
	LOSS [training: 0.26786912566622795 | validation: 0.42442385328590404]
	TIME [epoch: 9.67 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27763183398251373		[learning rate: 0.00025259]
	Learning Rate: 0.000252593
	LOSS [training: 0.27763183398251373 | validation: 0.45428158395551776]
	TIME [epoch: 9.66 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29641786837966105		[learning rate: 0.00025168]
	Learning Rate: 0.000251676
	LOSS [training: 0.29641786837966105 | validation: 0.4741362071522909]
	TIME [epoch: 9.66 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27871510063849264		[learning rate: 0.00025076]
	Learning Rate: 0.000250763
	LOSS [training: 0.27871510063849264 | validation: 0.4373054662953244]
	TIME [epoch: 9.68 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2767387236192489		[learning rate: 0.00024985]
	Learning Rate: 0.000249853
	LOSS [training: 0.2767387236192489 | validation: 0.4129553377393997]
	TIME [epoch: 9.66 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2690283738087881		[learning rate: 0.00024895]
	Learning Rate: 0.000248946
	LOSS [training: 0.2690283738087881 | validation: 0.3647294950230923]
	TIME [epoch: 9.66 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2497279235491358		[learning rate: 0.00024804]
	Learning Rate: 0.000248043
	LOSS [training: 0.2497279235491358 | validation: 0.37074287258973315]
	TIME [epoch: 9.67 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26510154246959167		[learning rate: 0.00024714]
	Learning Rate: 0.000247142
	LOSS [training: 0.26510154246959167 | validation: 0.36202707474910273]
	TIME [epoch: 9.68 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26118580218340204		[learning rate: 0.00024625]
	Learning Rate: 0.000246246
	LOSS [training: 0.26118580218340204 | validation: 0.36527906660910153]
	TIME [epoch: 9.67 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2606991219384125		[learning rate: 0.00024535]
	Learning Rate: 0.000245352
	LOSS [training: 0.2606991219384125 | validation: 0.3706305245763557]
	TIME [epoch: 9.67 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24516929891155148		[learning rate: 0.00024446]
	Learning Rate: 0.000244462
	LOSS [training: 0.24516929891155148 | validation: 0.34491653285783486]
	TIME [epoch: 9.68 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23972801394645543		[learning rate: 0.00024357]
	Learning Rate: 0.000243574
	LOSS [training: 0.23972801394645543 | validation: 0.3518328614125032]
	TIME [epoch: 9.66 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2389311552501089		[learning rate: 0.00024269]
	Learning Rate: 0.00024269
	LOSS [training: 0.2389311552501089 | validation: 0.3517735554538976]
	TIME [epoch: 9.66 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.250571079555472		[learning rate: 0.00024181]
	Learning Rate: 0.00024181
	LOSS [training: 0.250571079555472 | validation: 0.3769668823891317]
	TIME [epoch: 9.66 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2623549003650112		[learning rate: 0.00024093]
	Learning Rate: 0.000240932
	LOSS [training: 0.2623549003650112 | validation: 0.39781686352879314]
	TIME [epoch: 9.69 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2616983438338564		[learning rate: 0.00024006]
	Learning Rate: 0.000240058
	LOSS [training: 0.2616983438338564 | validation: 0.35136172368552443]
	TIME [epoch: 9.66 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24158062920017048		[learning rate: 0.00023919]
	Learning Rate: 0.000239187
	LOSS [training: 0.24158062920017048 | validation: 0.3585131852390163]
	TIME [epoch: 9.67 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2595026205939538		[learning rate: 0.00023832]
	Learning Rate: 0.000238319
	LOSS [training: 0.2595026205939538 | validation: 0.4243229066464732]
	TIME [epoch: 9.66 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26337476886495803		[learning rate: 0.00023745]
	Learning Rate: 0.000237454
	LOSS [training: 0.26337476886495803 | validation: 0.41729692241025557]
	TIME [epoch: 9.69 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2666979564694093		[learning rate: 0.00023659]
	Learning Rate: 0.000236592
	LOSS [training: 0.2666979564694093 | validation: 0.43428876519459986]
	TIME [epoch: 9.66 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2701594502671047		[learning rate: 0.00023573]
	Learning Rate: 0.000235733
	LOSS [training: 0.2701594502671047 | validation: 0.508173976003171]
	TIME [epoch: 9.66 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28162204147288267		[learning rate: 0.00023488]
	Learning Rate: 0.000234878
	LOSS [training: 0.28162204147288267 | validation: 0.4528814775036173]
	TIME [epoch: 9.67 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26399793693050594		[learning rate: 0.00023403]
	Learning Rate: 0.000234026
	LOSS [training: 0.26399793693050594 | validation: 0.3524002532913916]
	TIME [epoch: 9.67 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2696729679803546		[learning rate: 0.00023318]
	Learning Rate: 0.000233176
	LOSS [training: 0.2696729679803546 | validation: 0.37015191821677446]
	TIME [epoch: 9.66 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25306518520648585		[learning rate: 0.00023233]
	Learning Rate: 0.00023233
	LOSS [training: 0.25306518520648585 | validation: 0.43392159712310274]
	TIME [epoch: 9.66 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24464575220408524		[learning rate: 0.00023149]
	Learning Rate: 0.000231487
	LOSS [training: 0.24464575220408524 | validation: 0.37347428937561633]
	TIME [epoch: 9.68 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25097405599999156		[learning rate: 0.00023065]
	Learning Rate: 0.000230647
	LOSS [training: 0.25097405599999156 | validation: 0.38362417227681334]
	TIME [epoch: 9.66 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2520215100365884		[learning rate: 0.00022981]
	Learning Rate: 0.00022981
	LOSS [training: 0.2520215100365884 | validation: 0.3571498399009451]
	TIME [epoch: 9.66 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2645097204727273		[learning rate: 0.00022898]
	Learning Rate: 0.000228976
	LOSS [training: 0.2645097204727273 | validation: 0.3608231405652702]
	TIME [epoch: 9.66 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26432006065121744		[learning rate: 0.00022814]
	Learning Rate: 0.000228145
	LOSS [training: 0.26432006065121744 | validation: 0.3503656254110287]
	TIME [epoch: 9.68 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27336053595623816		[learning rate: 0.00022732]
	Learning Rate: 0.000227317
	LOSS [training: 0.27336053595623816 | validation: 0.36438949752275646]
	TIME [epoch: 9.65 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25311584659340924		[learning rate: 0.00022649]
	Learning Rate: 0.000226492
	LOSS [training: 0.25311584659340924 | validation: 0.3748263847199491]
	TIME [epoch: 9.66 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28050669402326733		[learning rate: 0.00022567]
	Learning Rate: 0.00022567
	LOSS [training: 0.28050669402326733 | validation: 0.39558149107419904]
	TIME [epoch: 9.66 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2904499776663872		[learning rate: 0.00022485]
	Learning Rate: 0.000224851
	LOSS [training: 0.2904499776663872 | validation: 0.38268324683918054]
	TIME [epoch: 9.67 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3568629232569725		[learning rate: 0.00022403]
	Learning Rate: 0.000224035
	LOSS [training: 0.3568629232569725 | validation: 0.3469260065596902]
	TIME [epoch: 9.65 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2766154789729337		[learning rate: 0.00022322]
	Learning Rate: 0.000223222
	LOSS [training: 0.2766154789729337 | validation: 0.3457084031260437]
	TIME [epoch: 9.65 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24354233014294654		[learning rate: 0.00022241]
	Learning Rate: 0.000222412
	LOSS [training: 0.24354233014294654 | validation: 0.355614506316279]
	TIME [epoch: 9.66 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25192267804564084		[learning rate: 0.0002216]
	Learning Rate: 0.000221605
	LOSS [training: 0.25192267804564084 | validation: 0.3239290850299154]
	TIME [epoch: 9.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_1148.pth
	Model improved!!!
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2286674358303104		[learning rate: 0.0002208]
	Learning Rate: 0.0002208
	LOSS [training: 0.2286674358303104 | validation: 0.3545618798394253]
	TIME [epoch: 9.66 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24457691018921923		[learning rate: 0.00022]
	Learning Rate: 0.000219999
	LOSS [training: 0.24457691018921923 | validation: 0.34210017931521675]
	TIME [epoch: 9.65 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.241696328178942		[learning rate: 0.0002192]
	Learning Rate: 0.000219201
	LOSS [training: 0.241696328178942 | validation: 0.3237577040231497]
	TIME [epoch: 9.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_1151.pth
	Model improved!!!
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27601644234738226		[learning rate: 0.00021841]
	Learning Rate: 0.000218405
	LOSS [training: 0.27601644234738226 | validation: 0.34588131704999314]
	TIME [epoch: 9.66 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2442135748452939		[learning rate: 0.00021761]
	Learning Rate: 0.000217613
	LOSS [training: 0.2442135748452939 | validation: 0.3216168320386775]
	TIME [epoch: 9.65 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_1153.pth
	Model improved!!!
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24739016471065564		[learning rate: 0.00021682]
	Learning Rate: 0.000216823
	LOSS [training: 0.24739016471065564 | validation: 0.32198210646098974]
	TIME [epoch: 9.65 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2797260640742128		[learning rate: 0.00021604]
	Learning Rate: 0.000216036
	LOSS [training: 0.2797260640742128 | validation: 0.3345108025854858]
	TIME [epoch: 9.67 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2570879300107142		[learning rate: 0.00021525]
	Learning Rate: 0.000215252
	LOSS [training: 0.2570879300107142 | validation: 0.3300371931290709]
	TIME [epoch: 9.65 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25217836002035376		[learning rate: 0.00021447]
	Learning Rate: 0.000214471
	LOSS [training: 0.25217836002035376 | validation: 0.3529529146791198]
	TIME [epoch: 9.65 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2907360266531862		[learning rate: 0.00021369]
	Learning Rate: 0.000213693
	LOSS [training: 0.2907360266531862 | validation: 0.3162354310369009]
	TIME [epoch: 9.65 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_1158.pth
	Model improved!!!
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25382218322316075		[learning rate: 0.00021292]
	Learning Rate: 0.000212917
	LOSS [training: 0.25382218322316075 | validation: 0.3543057504752937]
	TIME [epoch: 9.68 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26125552864428914		[learning rate: 0.00021214]
	Learning Rate: 0.000212144
	LOSS [training: 0.26125552864428914 | validation: 0.3385655829586702]
	TIME [epoch: 9.66 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26507982465168495		[learning rate: 0.00021137]
	Learning Rate: 0.000211375
	LOSS [training: 0.26507982465168495 | validation: 0.33262627813381984]
	TIME [epoch: 9.66 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26045123260851366		[learning rate: 0.00021061]
	Learning Rate: 0.000210607
	LOSS [training: 0.26045123260851366 | validation: 0.33337490536904313]
	TIME [epoch: 9.68 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25859907087439765		[learning rate: 0.00020984]
	Learning Rate: 0.000209843
	LOSS [training: 0.25859907087439765 | validation: 0.34294331017074314]
	TIME [epoch: 9.66 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2849593357520773		[learning rate: 0.00020908]
	Learning Rate: 0.000209082
	LOSS [training: 0.2849593357520773 | validation: 0.3155068066485643]
	TIME [epoch: 9.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_1164.pth
	Model improved!!!
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2353031843680986		[learning rate: 0.00020832]
	Learning Rate: 0.000208323
	LOSS [training: 0.2353031843680986 | validation: 0.3305410398628754]
	TIME [epoch: 9.65 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22890574790087634		[learning rate: 0.00020757]
	Learning Rate: 0.000207567
	LOSS [training: 0.22890574790087634 | validation: 0.3048414915632134]
	TIME [epoch: 9.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_1166.pth
	Model improved!!!
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3291171767861148		[learning rate: 0.00020681]
	Learning Rate: 0.000206814
	LOSS [training: 0.3291171767861148 | validation: 0.3198591493073777]
	TIME [epoch: 9.66 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2862257707466615		[learning rate: 0.00020606]
	Learning Rate: 0.000206063
	LOSS [training: 0.2862257707466615 | validation: 0.30912567125424495]
	TIME [epoch: 9.65 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24996908123553715		[learning rate: 0.00020532]
	Learning Rate: 0.000205315
	LOSS [training: 0.24996908123553715 | validation: 0.3625557811751453]
	TIME [epoch: 9.66 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2485643263911424		[learning rate: 0.00020457]
	Learning Rate: 0.00020457
	LOSS [training: 0.2485643263911424 | validation: 0.3480872960844487]
	TIME [epoch: 9.68 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2555358379886147		[learning rate: 0.00020383]
	Learning Rate: 0.000203828
	LOSS [training: 0.2555358379886147 | validation: 0.32362122429582074]
	TIME [epoch: 9.66 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2398563694534816		[learning rate: 0.00020309]
	Learning Rate: 0.000203088
	LOSS [training: 0.2398563694534816 | validation: 0.31861953417709926]
	TIME [epoch: 9.66 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2309011102378848		[learning rate: 0.00020235]
	Learning Rate: 0.000202351
	LOSS [training: 0.2309011102378848 | validation: 0.3802948427361736]
	TIME [epoch: 9.68 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24683433142701766		[learning rate: 0.00020162]
	Learning Rate: 0.000201617
	LOSS [training: 0.24683433142701766 | validation: 0.3317548499210048]
	TIME [epoch: 9.66 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2384360662771458		[learning rate: 0.00020088]
	Learning Rate: 0.000200885
	LOSS [training: 0.2384360662771458 | validation: 0.3169838452178574]
	TIME [epoch: 9.66 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22781778389234414		[learning rate: 0.00020016]
	Learning Rate: 0.000200156
	LOSS [training: 0.22781778389234414 | validation: 0.3047837022853247]
	TIME [epoch: 9.65 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_1176.pth
	Model improved!!!
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23408208653687557		[learning rate: 0.00019943]
	Learning Rate: 0.00019943
	LOSS [training: 0.23408208653687557 | validation: 0.4009084019593835]
	TIME [epoch: 9.68 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2594245186720871		[learning rate: 0.00019871]
	Learning Rate: 0.000198706
	LOSS [training: 0.2594245186720871 | validation: 0.28330924796497686]
	TIME [epoch: 9.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_1178.pth
	Model improved!!!
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2531654280300547		[learning rate: 0.00019798]
	Learning Rate: 0.000197985
	LOSS [training: 0.2531654280300547 | validation: 0.30975156509369073]
	TIME [epoch: 9.66 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24558845799329304		[learning rate: 0.00019727]
	Learning Rate: 0.000197266
	LOSS [training: 0.24558845799329304 | validation: 0.30695864709816434]
	TIME [epoch: 9.65 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25041284606538217		[learning rate: 0.00019655]
	Learning Rate: 0.00019655
	LOSS [training: 0.25041284606538217 | validation: 0.40703767527207574]
	TIME [epoch: 9.68 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2720791634311891		[learning rate: 0.00019584]
	Learning Rate: 0.000195837
	LOSS [training: 0.2720791634311891 | validation: 0.314599960530328]
	TIME [epoch: 9.65 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29908464013958036		[learning rate: 0.00019513]
	Learning Rate: 0.000195126
	LOSS [training: 0.29908464013958036 | validation: 0.3228974559867942]
	TIME [epoch: 9.65 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24648739323892807		[learning rate: 0.00019442]
	Learning Rate: 0.000194418
	LOSS [training: 0.24648739323892807 | validation: 0.355723829475647]
	TIME [epoch: 9.66 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24555688629954275		[learning rate: 0.00019371]
	Learning Rate: 0.000193713
	LOSS [training: 0.24555688629954275 | validation: 0.3339872755860855]
	TIME [epoch: 9.66 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23856235183426402		[learning rate: 0.00019301]
	Learning Rate: 0.00019301
	LOSS [training: 0.23856235183426402 | validation: 0.3279189848130789]
	TIME [epoch: 9.66 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2394808889530867		[learning rate: 0.00019231]
	Learning Rate: 0.000192309
	LOSS [training: 0.2394808889530867 | validation: 0.3074239474359903]
	TIME [epoch: 9.65 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22833881854572785		[learning rate: 0.00019161]
	Learning Rate: 0.000191611
	LOSS [training: 0.22833881854572785 | validation: 0.34970046981709785]
	TIME [epoch: 9.67 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24368353655340674		[learning rate: 0.00019092]
	Learning Rate: 0.000190916
	LOSS [training: 0.24368353655340674 | validation: 0.36722639682249997]
	TIME [epoch: 9.65 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25145197153079263		[learning rate: 0.00019022]
	Learning Rate: 0.000190223
	LOSS [training: 0.25145197153079263 | validation: 0.4298732061895524]
	TIME [epoch: 9.65 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2865930373630806		[learning rate: 0.00018953]
	Learning Rate: 0.000189533
	LOSS [training: 0.2865930373630806 | validation: 0.37233151638339207]
	TIME [epoch: 9.65 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2502456469924197		[learning rate: 0.00018884]
	Learning Rate: 0.000188845
	LOSS [training: 0.2502456469924197 | validation: 0.3371102028114633]
	TIME [epoch: 9.67 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2364921166798696		[learning rate: 0.00018816]
	Learning Rate: 0.00018816
	LOSS [training: 0.2364921166798696 | validation: 0.34782291903750223]
	TIME [epoch: 9.66 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24198931070456342		[learning rate: 0.00018748]
	Learning Rate: 0.000187477
	LOSS [training: 0.24198931070456342 | validation: 0.34728091827089996]
	TIME [epoch: 9.65 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25050870393938596		[learning rate: 0.0001868]
	Learning Rate: 0.000186796
	LOSS [training: 0.25050870393938596 | validation: 0.3535146226969444]
	TIME [epoch: 9.65 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2490226158711999		[learning rate: 0.00018612]
	Learning Rate: 0.000186119
	LOSS [training: 0.2490226158711999 | validation: 0.34146535542922146]
	TIME [epoch: 9.67 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2455804598379066		[learning rate: 0.00018544]
	Learning Rate: 0.000185443
	LOSS [training: 0.2455804598379066 | validation: 0.3677409869155238]
	TIME [epoch: 9.65 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24007521940497822		[learning rate: 0.00018477]
	Learning Rate: 0.00018477
	LOSS [training: 0.24007521940497822 | validation: 0.34628689972939924]
	TIME [epoch: 9.65 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24351862370790878		[learning rate: 0.0001841]
	Learning Rate: 0.000184099
	LOSS [training: 0.24351862370790878 | validation: 0.3344587935719941]
	TIME [epoch: 9.66 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2322606901228767		[learning rate: 0.00018343]
	Learning Rate: 0.000183431
	LOSS [training: 0.2322606901228767 | validation: 0.3549020211094998]
	TIME [epoch: 9.66 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2586246674327805		[learning rate: 0.00018277]
	Learning Rate: 0.000182766
	LOSS [training: 0.2586246674327805 | validation: 0.39092496184667164]
	TIME [epoch: 9.65 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27764296449110565		[learning rate: 0.0001821]
	Learning Rate: 0.000182102
	LOSS [training: 0.27764296449110565 | validation: 0.4177549599530466]
	TIME [epoch: 9.65 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24177132943579033		[learning rate: 0.00018144]
	Learning Rate: 0.000181442
	LOSS [training: 0.24177132943579033 | validation: 0.3394128748856268]
	TIME [epoch: 9.67 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26999244245988324		[learning rate: 0.00018078]
	Learning Rate: 0.000180783
	LOSS [training: 0.26999244245988324 | validation: 0.32890230940428516]
	TIME [epoch: 9.66 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22825747537116836		[learning rate: 0.00018013]
	Learning Rate: 0.000180127
	LOSS [training: 0.22825747537116836 | validation: 0.3226959983733977]
	TIME [epoch: 9.65 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23306104289832907		[learning rate: 0.00017947]
	Learning Rate: 0.000179473
	LOSS [training: 0.23306104289832907 | validation: 0.3929499179026649]
	TIME [epoch: 9.65 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23524035699834572		[learning rate: 0.00017882]
	Learning Rate: 0.000178822
	LOSS [training: 0.23524035699834572 | validation: 0.3095784909929255]
	TIME [epoch: 9.68 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25425365669600786		[learning rate: 0.00017817]
	Learning Rate: 0.000178173
	LOSS [training: 0.25425365669600786 | validation: 0.33095090030678437]
	TIME [epoch: 9.65 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22400949880857643		[learning rate: 0.00017753]
	Learning Rate: 0.000177526
	LOSS [training: 0.22400949880857643 | validation: 0.3208231465363427]
	TIME [epoch: 9.65 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22848910274844977		[learning rate: 0.00017688]
	Learning Rate: 0.000176882
	LOSS [training: 0.22848910274844977 | validation: 0.36296322818138693]
	TIME [epoch: 9.65 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22643012107886334		[learning rate: 0.00017624]
	Learning Rate: 0.00017624
	LOSS [training: 0.22643012107886334 | validation: 0.34570122059698605]
	TIME [epoch: 9.66 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23161083120249798		[learning rate: 0.0001756]
	Learning Rate: 0.000175601
	LOSS [training: 0.23161083120249798 | validation: 0.326783082324136]
	TIME [epoch: 9.65 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23574041443041213		[learning rate: 0.00017496]
	Learning Rate: 0.000174964
	LOSS [training: 0.23574041443041213 | validation: 0.30414731123661504]
	TIME [epoch: 9.65 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2319266838700708		[learning rate: 0.00017433]
	Learning Rate: 0.000174329
	LOSS [training: 0.2319266838700708 | validation: 0.3049451790024672]
	TIME [epoch: 9.67 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22082246728441085		[learning rate: 0.0001737]
	Learning Rate: 0.000173696
	LOSS [training: 0.22082246728441085 | validation: 0.3335390537694417]
	TIME [epoch: 9.65 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2618115592193897		[learning rate: 0.00017307]
	Learning Rate: 0.000173066
	LOSS [training: 0.2618115592193897 | validation: 0.37999367509809245]
	TIME [epoch: 9.65 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24700739622053694		[learning rate: 0.00017244]
	Learning Rate: 0.000172437
	LOSS [training: 0.24700739622053694 | validation: 0.3332388903294637]
	TIME [epoch: 9.66 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2598672043796222		[learning rate: 0.00017181]
	Learning Rate: 0.000171812
	LOSS [training: 0.2598672043796222 | validation: 0.38126987783163674]
	TIME [epoch: 9.67 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3609212124124454		[learning rate: 0.00017119]
	Learning Rate: 0.000171188
	LOSS [training: 0.3609212124124454 | validation: 0.32433103305203503]
	TIME [epoch: 9.65 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2809517627927273		[learning rate: 0.00017057]
	Learning Rate: 0.000170567
	LOSS [training: 0.2809517627927273 | validation: 0.31041378199909425]
	TIME [epoch: 9.65 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22889549587262775		[learning rate: 0.00016995]
	Learning Rate: 0.000169948
	LOSS [training: 0.22889549587262775 | validation: 0.33025859100446653]
	TIME [epoch: 9.65 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21376491868097322		[learning rate: 0.00016933]
	Learning Rate: 0.000169331
	LOSS [training: 0.21376491868097322 | validation: 0.3256217494312469]
	TIME [epoch: 9.67 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22162427211322927		[learning rate: 0.00016872]
	Learning Rate: 0.000168717
	LOSS [training: 0.22162427211322927 | validation: 0.3282714707383984]
	TIME [epoch: 9.65 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23918176567507654		[learning rate: 0.0001681]
	Learning Rate: 0.000168104
	LOSS [training: 0.23918176567507654 | validation: 0.39184713457474585]
	TIME [epoch: 9.65 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27604037321575936		[learning rate: 0.00016749]
	Learning Rate: 0.000167494
	LOSS [training: 0.27604037321575936 | validation: 0.38182445797953746]
	TIME [epoch: 9.65 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2548146223107496		[learning rate: 0.00016689]
	Learning Rate: 0.000166886
	LOSS [training: 0.2548146223107496 | validation: 0.34131199917205296]
	TIME [epoch: 9.66 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2358956555342786		[learning rate: 0.00016628]
	Learning Rate: 0.000166281
	LOSS [training: 0.2358956555342786 | validation: 0.3358204182719261]
	TIME [epoch: 9.65 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21133665422380257		[learning rate: 0.00016568]
	Learning Rate: 0.000165677
	LOSS [training: 0.21133665422380257 | validation: 0.3100704908489292]
	TIME [epoch: 9.65 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22375241417570635		[learning rate: 0.00016508]
	Learning Rate: 0.000165076
	LOSS [training: 0.22375241417570635 | validation: 0.29701524991098743]
	TIME [epoch: 9.67 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2256444539638384		[learning rate: 0.00016448]
	Learning Rate: 0.000164477
	LOSS [training: 0.2256444539638384 | validation: 0.31412669639624946]
	TIME [epoch: 9.66 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21459142568459036		[learning rate: 0.00016388]
	Learning Rate: 0.00016388
	LOSS [training: 0.21459142568459036 | validation: 0.3274442750197696]
	TIME [epoch: 9.65 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22346975020207624		[learning rate: 0.00016329]
	Learning Rate: 0.000163285
	LOSS [training: 0.22346975020207624 | validation: 0.3058731045096355]
	TIME [epoch: 9.65 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.219257492192672		[learning rate: 0.00016269]
	Learning Rate: 0.000162693
	LOSS [training: 0.219257492192672 | validation: 0.3072260354387032]
	TIME [epoch: 9.68 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2204559824681196		[learning rate: 0.0001621]
	Learning Rate: 0.000162102
	LOSS [training: 0.2204559824681196 | validation: 0.2879083045436714]
	TIME [epoch: 9.66 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23302795346522576		[learning rate: 0.00016151]
	Learning Rate: 0.000161514
	LOSS [training: 0.23302795346522576 | validation: 0.32522130465738974]
	TIME [epoch: 9.65 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21320984823577724		[learning rate: 0.00016093]
	Learning Rate: 0.000160928
	LOSS [training: 0.21320984823577724 | validation: 0.3150869457837785]
	TIME [epoch: 9.66 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22447072858588868		[learning rate: 0.00016034]
	Learning Rate: 0.000160344
	LOSS [training: 0.22447072858588868 | validation: 0.33765810253193773]
	TIME [epoch: 9.67 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21779211840865048		[learning rate: 0.00015976]
	Learning Rate: 0.000159762
	LOSS [training: 0.21779211840865048 | validation: 0.34066240940496195]
	TIME [epoch: 9.65 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2255847406444662		[learning rate: 0.00015918]
	Learning Rate: 0.000159182
	LOSS [training: 0.2255847406444662 | validation: 0.3329601745057288]
	TIME [epoch: 9.65 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21089405489391194		[learning rate: 0.0001586]
	Learning Rate: 0.000158605
	LOSS [training: 0.21089405489391194 | validation: 0.3141078105097678]
	TIME [epoch: 9.66 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21770077956124534		[learning rate: 0.00015803]
	Learning Rate: 0.000158029
	LOSS [training: 0.21770077956124534 | validation: 0.3060417358162047]
	TIME [epoch: 9.67 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2354827779342638		[learning rate: 0.00015746]
	Learning Rate: 0.000157456
	LOSS [training: 0.2354827779342638 | validation: 0.3775834817439268]
	TIME [epoch: 9.66 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2404797329755171		[learning rate: 0.00015688]
	Learning Rate: 0.000156884
	LOSS [training: 0.2404797329755171 | validation: 0.40335671089008485]
	TIME [epoch: 9.65 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2523765969707533		[learning rate: 0.00015631]
	Learning Rate: 0.000156315
	LOSS [training: 0.2523765969707533 | validation: 0.34631930742331307]
	TIME [epoch: 9.67 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24725538448550594		[learning rate: 0.00015575]
	Learning Rate: 0.000155748
	LOSS [training: 0.24725538448550594 | validation: 0.33820180504885167]
	TIME [epoch: 9.65 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23252686854767876		[learning rate: 0.00015518]
	Learning Rate: 0.000155182
	LOSS [training: 0.23252686854767876 | validation: 0.3014315555042628]
	TIME [epoch: 9.65 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21593829362442668		[learning rate: 0.00015462]
	Learning Rate: 0.000154619
	LOSS [training: 0.21593829362442668 | validation: 0.3316762990736678]
	TIME [epoch: 9.66 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23861444454989272		[learning rate: 0.00015406]
	Learning Rate: 0.000154058
	LOSS [training: 0.23861444454989272 | validation: 0.31014819231125507]
	TIME [epoch: 9.68 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2672004936562208		[learning rate: 0.0001535]
	Learning Rate: 0.000153499
	LOSS [training: 0.2672004936562208 | validation: 0.33087214005236215]
	TIME [epoch: 9.65 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24999930713060584		[learning rate: 0.00015294]
	Learning Rate: 0.000152942
	LOSS [training: 0.24999930713060584 | validation: 0.3265704910315858]
	TIME [epoch: 9.65 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2352961466104908		[learning rate: 0.00015239]
	Learning Rate: 0.000152387
	LOSS [training: 0.2352961466104908 | validation: 0.32342821020747464]
	TIME [epoch: 9.65 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2533813974881661		[learning rate: 0.00015183]
	Learning Rate: 0.000151834
	LOSS [training: 0.2533813974881661 | validation: 0.3235996512799536]
	TIME [epoch: 9.67 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23978211857026022		[learning rate: 0.00015128]
	Learning Rate: 0.000151283
	LOSS [training: 0.23978211857026022 | validation: 0.309441826231926]
	TIME [epoch: 9.65 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26609411507315056		[learning rate: 0.00015073]
	Learning Rate: 0.000150734
	LOSS [training: 0.26609411507315056 | validation: 0.3486132089008294]
	TIME [epoch: 9.65 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27963310412642084		[learning rate: 0.00015019]
	Learning Rate: 0.000150187
	LOSS [training: 0.27963310412642084 | validation: 0.315173331689887]
	TIME [epoch: 9.67 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21785172905661793		[learning rate: 0.00014964]
	Learning Rate: 0.000149642
	LOSS [training: 0.21785172905661793 | validation: 0.33083461262038666]
	TIME [epoch: 9.66 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23308250441441009		[learning rate: 0.0001491]
	Learning Rate: 0.000149099
	LOSS [training: 0.23308250441441009 | validation: 0.32264146374408514]
	TIME [epoch: 9.66 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21528377001283783		[learning rate: 0.00014856]
	Learning Rate: 0.000148558
	LOSS [training: 0.21528377001283783 | validation: 0.3056599572282612]
	TIME [epoch: 9.65 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20961711286214452		[learning rate: 0.00014802]
	Learning Rate: 0.000148018
	LOSS [training: 0.20961711286214452 | validation: 0.33440668914452476]
	TIME [epoch: 9.67 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20866974445749875		[learning rate: 0.00014748]
	Learning Rate: 0.000147481
	LOSS [training: 0.20866974445749875 | validation: 0.3242053052252372]
	TIME [epoch: 9.66 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2169703498521962		[learning rate: 0.00014695]
	Learning Rate: 0.000146946
	LOSS [training: 0.2169703498521962 | validation: 0.31804017547236607]
	TIME [epoch: 9.65 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22535827572295478		[learning rate: 0.00014641]
	Learning Rate: 0.000146413
	LOSS [training: 0.22535827572295478 | validation: 0.34975744747502285]
	TIME [epoch: 9.65 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2273620763943751		[learning rate: 0.00014588]
	Learning Rate: 0.000145881
	LOSS [training: 0.2273620763943751 | validation: 0.3324512925602204]
	TIME [epoch: 9.68 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.235210256382843		[learning rate: 0.00014535]
	Learning Rate: 0.000145352
	LOSS [training: 0.235210256382843 | validation: 0.32386637439526583]
	TIME [epoch: 9.65 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21741295405472433		[learning rate: 0.00014482]
	Learning Rate: 0.000144825
	LOSS [training: 0.21741295405472433 | validation: 0.32625808036308923]
	TIME [epoch: 9.66 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2121056399512325		[learning rate: 0.0001443]
	Learning Rate: 0.000144299
	LOSS [training: 0.2121056399512325 | validation: 0.335097875414288]
	TIME [epoch: 9.67 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21526703089523544		[learning rate: 0.00014378]
	Learning Rate: 0.000143775
	LOSS [training: 0.21526703089523544 | validation: 0.3389344213690525]
	TIME [epoch: 9.69 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23309760422458997		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 0.23309760422458997 | validation: 0.3432214670997224]
	TIME [epoch: 9.66 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23304954309033526		[learning rate: 0.00014273]
	Learning Rate: 0.000142734
	LOSS [training: 0.23304954309033526 | validation: 0.3359041510456464]
	TIME [epoch: 9.65 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21686171452175845		[learning rate: 0.00014222]
	Learning Rate: 0.000142216
	LOSS [training: 0.21686171452175845 | validation: 0.31394815338320425]
	TIME [epoch: 9.67 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25846563394787136		[learning rate: 0.0001417]
	Learning Rate: 0.0001417
	LOSS [training: 0.25846563394787136 | validation: 0.40007755529076966]
	TIME [epoch: 9.66 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3462990527943364		[learning rate: 0.00014119]
	Learning Rate: 0.000141185
	LOSS [training: 0.3462990527943364 | validation: 0.3639168989462344]
	TIME [epoch: 9.66 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2982690368691069		[learning rate: 0.00014067]
	Learning Rate: 0.000140673
	LOSS [training: 0.2982690368691069 | validation: 0.3093502853387912]
	TIME [epoch: 9.66 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23120139823560898		[learning rate: 0.00014016]
	Learning Rate: 0.000140162
	LOSS [training: 0.23120139823560898 | validation: 0.3157586030048281]
	TIME [epoch: 9.68 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22666319033236557		[learning rate: 0.00013965]
	Learning Rate: 0.000139654
	LOSS [training: 0.22666319033236557 | validation: 0.3096646063891291]
	TIME [epoch: 9.66 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22670138025691164		[learning rate: 0.00013915]
	Learning Rate: 0.000139147
	LOSS [training: 0.22670138025691164 | validation: 0.33113747911980485]
	TIME [epoch: 9.66 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24922781729401627		[learning rate: 0.00013864]
	Learning Rate: 0.000138642
	LOSS [training: 0.24922781729401627 | validation: 0.40446530457938634]
	TIME [epoch: 9.65 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26886418591891836		[learning rate: 0.00013814]
	Learning Rate: 0.000138139
	LOSS [training: 0.26886418591891836 | validation: 0.32368973502829784]
	TIME [epoch: 9.68 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23570989021689415		[learning rate: 0.00013764]
	Learning Rate: 0.000137638
	LOSS [training: 0.23570989021689415 | validation: 0.3036647267949834]
	TIME [epoch: 9.65 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2264454961324635		[learning rate: 0.00013714]
	Learning Rate: 0.000137138
	LOSS [training: 0.2264454961324635 | validation: 0.32408027638943215]
	TIME [epoch: 9.65 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23506367253807384		[learning rate: 0.00013664]
	Learning Rate: 0.00013664
	LOSS [training: 0.23506367253807384 | validation: 0.30383584848171225]
	TIME [epoch: 9.66 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21751230502858904		[learning rate: 0.00013614]
	Learning Rate: 0.000136144
	LOSS [training: 0.21751230502858904 | validation: 0.33104670298121736]
	TIME [epoch: 9.68 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2200700261245953		[learning rate: 0.00013565]
	Learning Rate: 0.00013565
	LOSS [training: 0.2200700261245953 | validation: 0.35629166603769435]
	TIME [epoch: 9.66 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24872641629126907		[learning rate: 0.00013516]
	Learning Rate: 0.000135158
	LOSS [training: 0.24872641629126907 | validation: 0.3829482656815283]
	TIME [epoch: 9.65 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23850679070409608		[learning rate: 0.00013467]
	Learning Rate: 0.000134668
	LOSS [training: 0.23850679070409608 | validation: 0.3227317942639425]
	TIME [epoch: 9.67 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21449776678481342		[learning rate: 0.00013418]
	Learning Rate: 0.000134179
	LOSS [training: 0.21449776678481342 | validation: 0.3196903657366447]
	TIME [epoch: 9.66 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21883400445741366		[learning rate: 0.00013369]
	Learning Rate: 0.000133692
	LOSS [training: 0.21883400445741366 | validation: 0.31557935422426514]
	TIME [epoch: 9.65 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21848357299872384		[learning rate: 0.00013321]
	Learning Rate: 0.000133207
	LOSS [training: 0.21848357299872384 | validation: 0.2988892493473246]
	TIME [epoch: 9.65 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31521404306947076		[learning rate: 0.00013272]
	Learning Rate: 0.000132723
	LOSS [training: 0.31521404306947076 | validation: 0.47953074377310073]
	TIME [epoch: 9.67 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4716541833210893		[learning rate: 0.00013224]
	Learning Rate: 0.000132242
	LOSS [training: 0.4716541833210893 | validation: 0.39967215656399624]
	TIME [epoch: 9.66 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.315516343058187		[learning rate: 0.00013176]
	Learning Rate: 0.000131762
	LOSS [training: 0.315516343058187 | validation: 0.31956180525990774]
	TIME [epoch: 9.66 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22803259367566947		[learning rate: 0.00013128]
	Learning Rate: 0.000131284
	LOSS [training: 0.22803259367566947 | validation: 0.2854092882410128]
	TIME [epoch: 9.66 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20808336442467823		[learning rate: 0.00013081]
	Learning Rate: 0.000130807
	LOSS [training: 0.20808336442467823 | validation: 0.2948816911028289]
	TIME [epoch: 9.68 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21571238723122454		[learning rate: 0.00013033]
	Learning Rate: 0.000130332
	LOSS [training: 0.21571238723122454 | validation: 0.3008991426385404]
	TIME [epoch: 9.67 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22155767458272219		[learning rate: 0.00012986]
	Learning Rate: 0.000129859
	LOSS [training: 0.22155767458272219 | validation: 0.2871112634676874]
	TIME [epoch: 9.66 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21676127722829897		[learning rate: 0.00012939]
	Learning Rate: 0.000129388
	LOSS [training: 0.21676127722829897 | validation: 0.2901427862286634]
	TIME [epoch: 9.66 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2330943524912043		[learning rate: 0.00012892]
	Learning Rate: 0.000128919
	LOSS [training: 0.2330943524912043 | validation: 0.3015918580721273]
	TIME [epoch: 9.68 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2544596347113736		[learning rate: 0.00012845]
	Learning Rate: 0.000128451
	LOSS [training: 0.2544596347113736 | validation: 0.2988736869627917]
	TIME [epoch: 9.65 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.278323591322932		[learning rate: 0.00012798]
	Learning Rate: 0.000127985
	LOSS [training: 0.278323591322932 | validation: 0.30428329874553234]
	TIME [epoch: 9.67 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2287367400202948		[learning rate: 0.00012752]
	Learning Rate: 0.00012752
	LOSS [training: 0.2287367400202948 | validation: 0.3361492587390005]
	TIME [epoch: 9.68 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20435131183029207		[learning rate: 0.00012706]
	Learning Rate: 0.000127057
	LOSS [training: 0.20435131183029207 | validation: 0.2876674518650968]
	TIME [epoch: 9.66 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22313127041189146		[learning rate: 0.0001266]
	Learning Rate: 0.000126596
	LOSS [training: 0.22313127041189146 | validation: 0.2840615316774344]
	TIME [epoch: 9.66 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2350732371693082		[learning rate: 0.00012614]
	Learning Rate: 0.000126137
	LOSS [training: 0.2350732371693082 | validation: 0.3180792662862449]
	TIME [epoch: 9.66 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21978127112336782		[learning rate: 0.00012568]
	Learning Rate: 0.000125679
	LOSS [training: 0.21978127112336782 | validation: 0.30365735798051957]
	TIME [epoch: 9.68 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21661174304073177		[learning rate: 0.00012522]
	Learning Rate: 0.000125223
	LOSS [training: 0.21661174304073177 | validation: 0.32247946697752855]
	TIME [epoch: 9.66 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21796484759735577		[learning rate: 0.00012477]
	Learning Rate: 0.000124769
	LOSS [training: 0.21796484759735577 | validation: 0.29458145091905286]
	TIME [epoch: 9.66 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22434300959694364		[learning rate: 0.00012432]
	Learning Rate: 0.000124316
	LOSS [training: 0.22434300959694364 | validation: 0.3058246184254059]
	TIME [epoch: 9.67 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21523693482826803		[learning rate: 0.00012386]
	Learning Rate: 0.000123865
	LOSS [training: 0.21523693482826803 | validation: 0.26983810504833583]
	TIME [epoch: 9.67 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_1308.pth
	Model improved!!!
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2490105864814011		[learning rate: 0.00012342]
	Learning Rate: 0.000123415
	LOSS [training: 0.2490105864814011 | validation: 0.30892550629975013]
	TIME [epoch: 9.66 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22194218657771816		[learning rate: 0.00012297]
	Learning Rate: 0.000122967
	LOSS [training: 0.22194218657771816 | validation: 0.29537409084155236]
	TIME [epoch: 9.67 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23998929467042887		[learning rate: 0.00012252]
	Learning Rate: 0.000122521
	LOSS [training: 0.23998929467042887 | validation: 0.29432358793869884]
	TIME [epoch: 9.67 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2276234052013682		[learning rate: 0.00012208]
	Learning Rate: 0.000122076
	LOSS [training: 0.2276234052013682 | validation: 0.29950861097160825]
	TIME [epoch: 9.67 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24352985531688165		[learning rate: 0.00012163]
	Learning Rate: 0.000121633
	LOSS [training: 0.24352985531688165 | validation: 0.31031226892952724]
	TIME [epoch: 9.66 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21086117594119216		[learning rate: 0.00012119]
	Learning Rate: 0.000121192
	LOSS [training: 0.21086117594119216 | validation: 0.2919951944766845]
	TIME [epoch: 9.66 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21437603157377869		[learning rate: 0.00012075]
	Learning Rate: 0.000120752
	LOSS [training: 0.21437603157377869 | validation: 0.28587541342831796]
	TIME [epoch: 9.68 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24421670715386118		[learning rate: 0.00012031]
	Learning Rate: 0.000120314
	LOSS [training: 0.24421670715386118 | validation: 0.2897070526695112]
	TIME [epoch: 9.67 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22690813076393396		[learning rate: 0.00011988]
	Learning Rate: 0.000119877
	LOSS [training: 0.22690813076393396 | validation: 0.305753543525101]
	TIME [epoch: 9.66 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20596650910818695		[learning rate: 0.00011944]
	Learning Rate: 0.000119442
	LOSS [training: 0.20596650910818695 | validation: 0.32640724015408135]
	TIME [epoch: 9.66 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22187585992977316		[learning rate: 0.00011901]
	Learning Rate: 0.000119009
	LOSS [training: 0.22187585992977316 | validation: 0.2931483413098199]
	TIME [epoch: 9.68 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21730370925612083		[learning rate: 0.00011858]
	Learning Rate: 0.000118577
	LOSS [training: 0.21730370925612083 | validation: 0.3208463678264811]
	TIME [epoch: 9.67 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23714400578435493		[learning rate: 0.00011815]
	Learning Rate: 0.000118147
	LOSS [training: 0.23714400578435493 | validation: 0.3428635008262427]
	TIME [epoch: 9.67 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2377193059013007		[learning rate: 0.00011772]
	Learning Rate: 0.000117718
	LOSS [training: 0.2377193059013007 | validation: 0.3379265648524941]
	TIME [epoch: 9.67 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2424627753069411		[learning rate: 0.00011729]
	Learning Rate: 0.000117291
	LOSS [training: 0.2424627753069411 | validation: 0.3216990592169984]
	TIME [epoch: 9.68 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22708490759307282		[learning rate: 0.00011686]
	Learning Rate: 0.000116865
	LOSS [training: 0.22708490759307282 | validation: 0.3580685330327431]
	TIME [epoch: 9.67 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22100978427787799		[learning rate: 0.00011644]
	Learning Rate: 0.000116441
	LOSS [training: 0.22100978427787799 | validation: 0.3144370231192476]
	TIME [epoch: 9.67 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21013207341394652		[learning rate: 0.00011602]
	Learning Rate: 0.000116018
	LOSS [training: 0.21013207341394652 | validation: 0.2820274689552561]
	TIME [epoch: 9.68 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2197866175462166		[learning rate: 0.0001156]
	Learning Rate: 0.000115597
	LOSS [training: 0.2197866175462166 | validation: 0.2981855293808988]
	TIME [epoch: 9.68 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2199249809779144		[learning rate: 0.00011518]
	Learning Rate: 0.000115178
	LOSS [training: 0.2199249809779144 | validation: 0.3330813561367374]
	TIME [epoch: 9.67 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26025867972866207		[learning rate: 0.00011476]
	Learning Rate: 0.00011476
	LOSS [training: 0.26025867972866207 | validation: 0.31250033431172297]
	TIME [epoch: 9.67 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22664298611392467		[learning rate: 0.00011434]
	Learning Rate: 0.000114343
	LOSS [training: 0.22664298611392467 | validation: 0.29477316328189695]
	TIME [epoch: 9.69 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.210682933347672		[learning rate: 0.00011393]
	Learning Rate: 0.000113928
	LOSS [training: 0.210682933347672 | validation: 0.29047850582448526]
	TIME [epoch: 9.67 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21333577902857295		[learning rate: 0.00011351]
	Learning Rate: 0.000113515
	LOSS [training: 0.21333577902857295 | validation: 0.3041260415301042]
	TIME [epoch: 9.67 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20928898235500207		[learning rate: 0.0001131]
	Learning Rate: 0.000113103
	LOSS [training: 0.20928898235500207 | validation: 0.3276857676234407]
	TIME [epoch: 9.67 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20988195075622856		[learning rate: 0.00011269]
	Learning Rate: 0.000112692
	LOSS [training: 0.20988195075622856 | validation: 0.316717487490143]
	TIME [epoch: 9.69 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22051699091812577		[learning rate: 0.00011228]
	Learning Rate: 0.000112283
	LOSS [training: 0.22051699091812577 | validation: 0.31251433053063876]
	TIME [epoch: 9.67 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21833361402554044		[learning rate: 0.00011188]
	Learning Rate: 0.000111876
	LOSS [training: 0.21833361402554044 | validation: 0.3027739471817095]
	TIME [epoch: 9.67 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2382814670198825		[learning rate: 0.00011147]
	Learning Rate: 0.00011147
	LOSS [training: 0.2382814670198825 | validation: 0.2981957403183552]
	TIME [epoch: 9.67 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22196258646863348		[learning rate: 0.00011107]
	Learning Rate: 0.000111065
	LOSS [training: 0.22196258646863348 | validation: 0.30824981254897216]
	TIME [epoch: 9.68 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2104413810502083		[learning rate: 0.00011066]
	Learning Rate: 0.000110662
	LOSS [training: 0.2104413810502083 | validation: 0.30144634869915393]
	TIME [epoch: 9.67 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21225386164690438		[learning rate: 0.00011026]
	Learning Rate: 0.000110261
	LOSS [training: 0.21225386164690438 | validation: 0.2943579237727084]
	TIME [epoch: 9.67 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23981135240950163		[learning rate: 0.00010986]
	Learning Rate: 0.000109861
	LOSS [training: 0.23981135240950163 | validation: 0.2905198165425821]
	TIME [epoch: 9.68 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2732605603923818		[learning rate: 0.00010946]
	Learning Rate: 0.000109462
	LOSS [training: 0.2732605603923818 | validation: 0.3435476784007942]
	TIME [epoch: 9.67 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2544017996979086		[learning rate: 0.00010906]
	Learning Rate: 0.000109065
	LOSS [training: 0.2544017996979086 | validation: 0.29798937857076874]
	TIME [epoch: 9.66 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2156187281567815		[learning rate: 0.00010867]
	Learning Rate: 0.000108669
	LOSS [training: 0.2156187281567815 | validation: 0.2969726236991432]
	TIME [epoch: 9.66 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21437698834499308		[learning rate: 0.00010827]
	Learning Rate: 0.000108275
	LOSS [training: 0.21437698834499308 | validation: 0.30342728325782603]
	TIME [epoch: 9.68 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20907129783948583		[learning rate: 0.00010788]
	Learning Rate: 0.000107882
	LOSS [training: 0.20907129783948583 | validation: 0.29839081403900286]
	TIME [epoch: 9.66 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22405255516692937		[learning rate: 0.00010749]
	Learning Rate: 0.00010749
	LOSS [training: 0.22405255516692937 | validation: 0.3135938610835587]
	TIME [epoch: 9.66 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21752611296434857		[learning rate: 0.0001071]
	Learning Rate: 0.0001071
	LOSS [training: 0.21752611296434857 | validation: 0.2918998272955123]
	TIME [epoch: 9.66 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20538099392509063		[learning rate: 0.00010671]
	Learning Rate: 0.000106711
	LOSS [training: 0.20538099392509063 | validation: 0.29917545471777696]
	TIME [epoch: 9.69 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20532578564849788		[learning rate: 0.00010632]
	Learning Rate: 0.000106324
	LOSS [training: 0.20532578564849788 | validation: 0.29758649425289135]
	TIME [epoch: 9.67 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20433836528818833		[learning rate: 0.00010594]
	Learning Rate: 0.000105938
	LOSS [training: 0.20433836528818833 | validation: 0.29213107376060127]
	TIME [epoch: 9.66 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2035226499257435		[learning rate: 0.00010555]
	Learning Rate: 0.000105554
	LOSS [training: 0.2035226499257435 | validation: 0.2981089153106153]
	TIME [epoch: 9.67 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20857654535000392		[learning rate: 0.00010517]
	Learning Rate: 0.000105171
	LOSS [training: 0.20857654535000392 | validation: 0.29514744247969577]
	TIME [epoch: 9.68 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2044714404159203		[learning rate: 0.00010479]
	Learning Rate: 0.000104789
	LOSS [training: 0.2044714404159203 | validation: 0.2998296411838709]
	TIME [epoch: 9.67 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21076962538677088		[learning rate: 0.00010441]
	Learning Rate: 0.000104409
	LOSS [training: 0.21076962538677088 | validation: 0.29744784698955323]
	TIME [epoch: 9.66 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21111217133882093		[learning rate: 0.00010403]
	Learning Rate: 0.00010403
	LOSS [training: 0.21111217133882093 | validation: 0.3098142524775389]
	TIME [epoch: 9.68 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23150703919623666		[learning rate: 0.00010365]
	Learning Rate: 0.000103652
	LOSS [training: 0.23150703919623666 | validation: 0.29236441632405075]
	TIME [epoch: 9.66 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21221297646429177		[learning rate: 0.00010328]
	Learning Rate: 0.000103276
	LOSS [training: 0.21221297646429177 | validation: 0.30814643134986336]
	TIME [epoch: 9.66 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21227720866435215		[learning rate: 0.0001029]
	Learning Rate: 0.000102901
	LOSS [training: 0.21227720866435215 | validation: 0.2826267974087545]
	TIME [epoch: 9.66 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20730717491277262		[learning rate: 0.00010253]
	Learning Rate: 0.000102528
	LOSS [training: 0.20730717491277262 | validation: 0.2901466702171394]
	TIME [epoch: 9.68 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20934726912772303		[learning rate: 0.00010216]
	Learning Rate: 0.000102156
	LOSS [training: 0.20934726912772303 | validation: 0.2761302135335454]
	TIME [epoch: 9.66 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20583673779123837		[learning rate: 0.00010179]
	Learning Rate: 0.000101785
	LOSS [training: 0.20583673779123837 | validation: 0.29659722060151156]
	TIME [epoch: 9.66 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20705375073401439		[learning rate: 0.00010142]
	Learning Rate: 0.000101416
	LOSS [training: 0.20705375073401439 | validation: 0.30467130739030945]
	TIME [epoch: 9.66 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20983512693402387		[learning rate: 0.00010105]
	Learning Rate: 0.000101048
	LOSS [training: 0.20983512693402387 | validation: 0.2757185954960816]
	TIME [epoch: 9.68 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21075196479820896		[learning rate: 0.00010068]
	Learning Rate: 0.000100681
	LOSS [training: 0.21075196479820896 | validation: 0.28716033598999346]
	TIME [epoch: 9.66 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21654605767638846		[learning rate: 0.00010032]
	Learning Rate: 0.000100316
	LOSS [training: 0.21654605767638846 | validation: 0.32445026248536557]
	TIME [epoch: 9.66 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21121604961231624		[learning rate: 9.9952e-05]
	Learning Rate: 9.99515e-05
	LOSS [training: 0.21121604961231624 | validation: 0.30650401899673313]
	TIME [epoch: 9.67 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2115439226092326		[learning rate: 9.9589e-05]
	Learning Rate: 9.95888e-05
	LOSS [training: 0.2115439226092326 | validation: 0.308629888298337]
	TIME [epoch: 9.66 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20704045963423692		[learning rate: 9.9227e-05]
	Learning Rate: 9.92274e-05
	LOSS [training: 0.20704045963423692 | validation: 0.2838128803918986]
	TIME [epoch: 9.65 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.205778442856528		[learning rate: 9.8867e-05]
	Learning Rate: 9.88673e-05
	LOSS [training: 0.205778442856528 | validation: 0.27919451399277767]
	TIME [epoch: 9.66 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2020577646876819		[learning rate: 9.8509e-05]
	Learning Rate: 9.85085e-05
	LOSS [training: 0.2020577646876819 | validation: 0.2936302192430854]
	TIME [epoch: 9.67 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.201259380429603		[learning rate: 9.8151e-05]
	Learning Rate: 9.8151e-05
	LOSS [training: 0.201259380429603 | validation: 0.2751543908414588]
	TIME [epoch: 9.66 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20716708896557337		[learning rate: 9.7795e-05]
	Learning Rate: 9.77948e-05
	LOSS [training: 0.20716708896557337 | validation: 0.2866658989188413]
	TIME [epoch: 9.66 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2062025969267805		[learning rate: 9.744e-05]
	Learning Rate: 9.74399e-05
	LOSS [training: 0.2062025969267805 | validation: 0.2851346254493335]
	TIME [epoch: 9.65 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20672128671336426		[learning rate: 9.7086e-05]
	Learning Rate: 9.70863e-05
	LOSS [training: 0.20672128671336426 | validation: 0.2909343207816653]
	TIME [epoch: 9.68 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2256780609385504		[learning rate: 9.6734e-05]
	Learning Rate: 9.6734e-05
	LOSS [training: 0.2256780609385504 | validation: 0.32464832979511193]
	TIME [epoch: 9.65 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21404415502921817		[learning rate: 9.6383e-05]
	Learning Rate: 9.63829e-05
	LOSS [training: 0.21404415502921817 | validation: 0.31282324938536077]
	TIME [epoch: 9.65 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20108248058826278		[learning rate: 9.6033e-05]
	Learning Rate: 9.60331e-05
	LOSS [training: 0.20108248058826278 | validation: 0.29345935919015276]
	TIME [epoch: 9.65 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19801561151552344		[learning rate: 9.5685e-05]
	Learning Rate: 9.56846e-05
	LOSS [training: 0.19801561151552344 | validation: 0.2732436394412176]
	TIME [epoch: 9.67 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21463890910770944		[learning rate: 9.5337e-05]
	Learning Rate: 9.53374e-05
	LOSS [training: 0.21463890910770944 | validation: 0.2849150849863155]
	TIME [epoch: 9.65 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.232286549147672		[learning rate: 9.4991e-05]
	Learning Rate: 9.49914e-05
	LOSS [training: 0.232286549147672 | validation: 0.2796146122099012]
	TIME [epoch: 9.65 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2152022684582881		[learning rate: 9.4647e-05]
	Learning Rate: 9.46466e-05
	LOSS [training: 0.2152022684582881 | validation: 0.3162536797211453]
	TIME [epoch: 9.67 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2143171934586466		[learning rate: 9.4303e-05]
	Learning Rate: 9.43032e-05
	LOSS [training: 0.2143171934586466 | validation: 0.28918288322887475]
	TIME [epoch: 9.66 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2007342647480586		[learning rate: 9.3961e-05]
	Learning Rate: 9.39609e-05
	LOSS [training: 0.2007342647480586 | validation: 0.28091066845841794]
	TIME [epoch: 9.66 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20527754075311075		[learning rate: 9.362e-05]
	Learning Rate: 9.362e-05
	LOSS [training: 0.20527754075311075 | validation: 0.2883017191955445]
	TIME [epoch: 9.65 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21543781051364955		[learning rate: 9.328e-05]
	Learning Rate: 9.32802e-05
	LOSS [training: 0.21543781051364955 | validation: 0.2848521660641509]
	TIME [epoch: 9.68 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20555604248819953		[learning rate: 9.2942e-05]
	Learning Rate: 9.29417e-05
	LOSS [training: 0.20555604248819953 | validation: 0.30263384538405647]
	TIME [epoch: 9.66 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20232693099560922		[learning rate: 9.2604e-05]
	Learning Rate: 9.26044e-05
	LOSS [training: 0.20232693099560922 | validation: 0.2946142992810083]
	TIME [epoch: 9.66 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2058675029903445		[learning rate: 9.2268e-05]
	Learning Rate: 9.22683e-05
	LOSS [training: 0.2058675029903445 | validation: 0.27575258242748973]
	TIME [epoch: 9.66 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2125076045831628		[learning rate: 9.1933e-05]
	Learning Rate: 9.19335e-05
	LOSS [training: 0.2125076045831628 | validation: 0.29055930720707457]
	TIME [epoch: 9.68 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22223573033607863		[learning rate: 9.16e-05]
	Learning Rate: 9.15998e-05
	LOSS [training: 0.22223573033607863 | validation: 0.2991585888459913]
	TIME [epoch: 9.65 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20899821189462003		[learning rate: 9.1267e-05]
	Learning Rate: 9.12674e-05
	LOSS [training: 0.20899821189462003 | validation: 0.28619118946643307]
	TIME [epoch: 9.66 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20351374091527533		[learning rate: 9.0936e-05]
	Learning Rate: 9.09362e-05
	LOSS [training: 0.20351374091527533 | validation: 0.27980178937977973]
	TIME [epoch: 9.66 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20647907383244232		[learning rate: 9.0606e-05]
	Learning Rate: 9.06062e-05
	LOSS [training: 0.20647907383244232 | validation: 0.27550583570863224]
	TIME [epoch: 9.67 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23689928927803092		[learning rate: 9.0277e-05]
	Learning Rate: 9.02774e-05
	LOSS [training: 0.23689928927803092 | validation: 0.3208375217143838]
	TIME [epoch: 9.65 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25219307317313133		[learning rate: 8.995e-05]
	Learning Rate: 8.99498e-05
	LOSS [training: 0.25219307317313133 | validation: 0.3025960823025889]
	TIME [epoch: 9.66 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21421100035751212		[learning rate: 8.9623e-05]
	Learning Rate: 8.96233e-05
	LOSS [training: 0.21421100035751212 | validation: 0.2958264760842729]
	TIME [epoch: 9.67 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.210951452473011		[learning rate: 8.9298e-05]
	Learning Rate: 8.92981e-05
	LOSS [training: 0.210951452473011 | validation: 0.29640280125408786]
	TIME [epoch: 9.66 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21119341467199973		[learning rate: 8.8974e-05]
	Learning Rate: 8.8974e-05
	LOSS [training: 0.21119341467199973 | validation: 0.29174962189818265]
	TIME [epoch: 9.66 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20387305172589318		[learning rate: 8.8651e-05]
	Learning Rate: 8.86511e-05
	LOSS [training: 0.20387305172589318 | validation: 0.30139782540522486]
	TIME [epoch: 9.66 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20467065809329088		[learning rate: 8.8329e-05]
	Learning Rate: 8.83294e-05
	LOSS [training: 0.20467065809329088 | validation: 0.28985830571411536]
	TIME [epoch: 9.67 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20917798347818212		[learning rate: 8.8009e-05]
	Learning Rate: 8.80088e-05
	LOSS [training: 0.20917798347818212 | validation: 0.29805518971002753]
	TIME [epoch: 9.67 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2097845103795739		[learning rate: 8.7689e-05]
	Learning Rate: 8.76895e-05
	LOSS [training: 0.2097845103795739 | validation: 0.3044560141300451]
	TIME [epoch: 9.66 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20132756986880276		[learning rate: 8.7371e-05]
	Learning Rate: 8.73712e-05
	LOSS [training: 0.20132756986880276 | validation: 0.2980866123280484]
	TIME [epoch: 9.66 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21732151950916695		[learning rate: 8.7054e-05]
	Learning Rate: 8.70542e-05
	LOSS [training: 0.21732151950916695 | validation: 0.3069208084988754]
	TIME [epoch: 9.68 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22789906081979075		[learning rate: 8.6738e-05]
	Learning Rate: 8.67382e-05
	LOSS [training: 0.22789906081979075 | validation: 0.3006777491793799]
	TIME [epoch: 9.65 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20100129339063097		[learning rate: 8.6423e-05]
	Learning Rate: 8.64235e-05
	LOSS [training: 0.20100129339063097 | validation: 0.28535088365725764]
	TIME [epoch: 9.66 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20510405023475733		[learning rate: 8.611e-05]
	Learning Rate: 8.61098e-05
	LOSS [training: 0.20510405023475733 | validation: 0.30330318116873145]
	TIME [epoch: 9.67 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20006390663717216		[learning rate: 8.5797e-05]
	Learning Rate: 8.57973e-05
	LOSS [training: 0.20006390663717216 | validation: 0.28825227515802904]
	TIME [epoch: 9.68 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19575393782888487		[learning rate: 8.5486e-05]
	Learning Rate: 8.54859e-05
	LOSS [training: 0.19575393782888487 | validation: 0.29233194302002313]
	TIME [epoch: 9.66 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20583334841639184		[learning rate: 8.5176e-05]
	Learning Rate: 8.51757e-05
	LOSS [training: 0.20583334841639184 | validation: 0.2761205589982402]
	TIME [epoch: 9.66 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19794492301460892		[learning rate: 8.4867e-05]
	Learning Rate: 8.48666e-05
	LOSS [training: 0.19794492301460892 | validation: 0.2705548325003923]
	TIME [epoch: 9.67 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20212664067763858		[learning rate: 8.4559e-05]
	Learning Rate: 8.45586e-05
	LOSS [training: 0.20212664067763858 | validation: 0.3127701159396883]
	TIME [epoch: 9.66 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20891629867015582		[learning rate: 8.4252e-05]
	Learning Rate: 8.42518e-05
	LOSS [training: 0.20891629867015582 | validation: 0.2990580161864962]
	TIME [epoch: 9.66 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2118451406180371		[learning rate: 8.3946e-05]
	Learning Rate: 8.3946e-05
	LOSS [training: 0.2118451406180371 | validation: 0.3384835096240484]
	TIME [epoch: 9.66 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22731838668682366		[learning rate: 8.3641e-05]
	Learning Rate: 8.36414e-05
	LOSS [training: 0.22731838668682366 | validation: 0.34444477555436664]
	TIME [epoch: 9.68 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23563143105219986		[learning rate: 8.3338e-05]
	Learning Rate: 8.33378e-05
	LOSS [training: 0.23563143105219986 | validation: 0.31583554782428663]
	TIME [epoch: 9.66 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21499118694211844		[learning rate: 8.3035e-05]
	Learning Rate: 8.30354e-05
	LOSS [training: 0.21499118694211844 | validation: 0.29507283507640336]
	TIME [epoch: 9.66 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20825581308242463		[learning rate: 8.2734e-05]
	Learning Rate: 8.2734e-05
	LOSS [training: 0.20825581308242463 | validation: 0.2735553937675651]
	TIME [epoch: 9.66 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20507264602601588		[learning rate: 8.2434e-05]
	Learning Rate: 8.24338e-05
	LOSS [training: 0.20507264602601588 | validation: 0.28501317492871325]
	TIME [epoch: 9.68 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20327157305656923		[learning rate: 8.2135e-05]
	Learning Rate: 8.21346e-05
	LOSS [training: 0.20327157305656923 | validation: 0.27085363960381104]
	TIME [epoch: 9.66 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20312631835727854		[learning rate: 8.1837e-05]
	Learning Rate: 8.18366e-05
	LOSS [training: 0.20312631835727854 | validation: 0.27040444903792626]
	TIME [epoch: 9.66 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18788209031359432		[learning rate: 8.154e-05]
	Learning Rate: 8.15396e-05
	LOSS [training: 0.18788209031359432 | validation: 0.3054169289085926]
	TIME [epoch: 9.66 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20280277174795538		[learning rate: 8.1244e-05]
	Learning Rate: 8.12437e-05
	LOSS [training: 0.20280277174795538 | validation: 0.28826976660068154]
	TIME [epoch: 9.68 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19670788072431292		[learning rate: 8.0949e-05]
	Learning Rate: 8.09488e-05
	LOSS [training: 0.19670788072431292 | validation: 0.3135793903316429]
	TIME [epoch: 9.66 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2180710539452992		[learning rate: 8.0655e-05]
	Learning Rate: 8.0655e-05
	LOSS [training: 0.2180710539452992 | validation: 0.3390551540321482]
	TIME [epoch: 9.66 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21356101084203116		[learning rate: 8.0362e-05]
	Learning Rate: 8.03623e-05
	LOSS [training: 0.21356101084203116 | validation: 0.2947836073838963]
	TIME [epoch: 9.68 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2052891229804013		[learning rate: 8.0071e-05]
	Learning Rate: 8.00707e-05
	LOSS [training: 0.2052891229804013 | validation: 0.2983935611969055]
	TIME [epoch: 9.66 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2036640755232292		[learning rate: 7.978e-05]
	Learning Rate: 7.97801e-05
	LOSS [training: 0.2036640755232292 | validation: 0.30839813130770344]
	TIME [epoch: 9.66 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20286925719534668		[learning rate: 7.9491e-05]
	Learning Rate: 7.94906e-05
	LOSS [training: 0.20286925719534668 | validation: 0.29167079528073053]
	TIME [epoch: 9.66 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19886654186787522		[learning rate: 7.9202e-05]
	Learning Rate: 7.92021e-05
	LOSS [training: 0.19886654186787522 | validation: 0.2985397073240003]
	TIME [epoch: 9.67 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21130853584485046		[learning rate: 7.8915e-05]
	Learning Rate: 7.89147e-05
	LOSS [training: 0.21130853584485046 | validation: 0.28840165918777017]
	TIME [epoch: 9.66 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2045727038245869		[learning rate: 7.8628e-05]
	Learning Rate: 7.86283e-05
	LOSS [training: 0.2045727038245869 | validation: 0.28024015629271865]
	TIME [epoch: 9.66 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19631525223175508		[learning rate: 7.8343e-05]
	Learning Rate: 7.8343e-05
	LOSS [training: 0.19631525223175508 | validation: 0.28499341145950824]
	TIME [epoch: 9.66 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.207102169722832		[learning rate: 7.8059e-05]
	Learning Rate: 7.80586e-05
	LOSS [training: 0.207102169722832 | validation: 0.29250267304014516]
	TIME [epoch: 9.68 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20522136572332306		[learning rate: 7.7775e-05]
	Learning Rate: 7.77754e-05
	LOSS [training: 0.20522136572332306 | validation: 0.2755427420247942]
	TIME [epoch: 9.66 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2033401597262363		[learning rate: 7.7493e-05]
	Learning Rate: 7.74931e-05
	LOSS [training: 0.2033401597262363 | validation: 0.29796876390424976]
	TIME [epoch: 9.66 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20786225075741163		[learning rate: 7.7212e-05]
	Learning Rate: 7.72119e-05
	LOSS [training: 0.20786225075741163 | validation: 0.2960221519615837]
	TIME [epoch: 9.67 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2386575760988519		[learning rate: 7.6932e-05]
	Learning Rate: 7.69317e-05
	LOSS [training: 0.2386575760988519 | validation: 0.3049814782148295]
	TIME [epoch: 9.66 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24686686327706925		[learning rate: 7.6653e-05]
	Learning Rate: 7.66525e-05
	LOSS [training: 0.24686686327706925 | validation: 0.3270713218355806]
	TIME [epoch: 9.66 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2687477259535216		[learning rate: 7.6374e-05]
	Learning Rate: 7.63743e-05
	LOSS [training: 0.2687477259535216 | validation: 0.30511171614015703]
	TIME [epoch: 9.65 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25174501858560827		[learning rate: 7.6097e-05]
	Learning Rate: 7.60972e-05
	LOSS [training: 0.25174501858560827 | validation: 0.3084432480832243]
	TIME [epoch: 9.68 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21736130194994635		[learning rate: 7.5821e-05]
	Learning Rate: 7.5821e-05
	LOSS [training: 0.21736130194994635 | validation: 0.28913184585678897]
	TIME [epoch: 9.66 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2030766536043454		[learning rate: 7.5546e-05]
	Learning Rate: 7.55458e-05
	LOSS [training: 0.2030766536043454 | validation: 0.29170163660686016]
	TIME [epoch: 9.66 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1983111171707508		[learning rate: 7.5272e-05]
	Learning Rate: 7.52717e-05
	LOSS [training: 0.1983111171707508 | validation: 0.2921847228993277]
	TIME [epoch: 9.65 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19588871187744578		[learning rate: 7.4999e-05]
	Learning Rate: 7.49985e-05
	LOSS [training: 0.19588871187744578 | validation: 0.2824777617448507]
	TIME [epoch: 9.68 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1950042232677925		[learning rate: 7.4726e-05]
	Learning Rate: 7.47263e-05
	LOSS [training: 0.1950042232677925 | validation: 0.29789664160403223]
	TIME [epoch: 9.65 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19665941251166394		[learning rate: 7.4455e-05]
	Learning Rate: 7.44552e-05
	LOSS [training: 0.19665941251166394 | validation: 0.2752062462236241]
	TIME [epoch: 9.65 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20101283503872258		[learning rate: 7.4185e-05]
	Learning Rate: 7.4185e-05
	LOSS [training: 0.20101283503872258 | validation: 0.2783605356938204]
	TIME [epoch: 9.66 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2178602110841819		[learning rate: 7.3916e-05]
	Learning Rate: 7.39157e-05
	LOSS [training: 0.2178602110841819 | validation: 0.2935304634223171]
	TIME [epoch: 9.67 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2193104396818967		[learning rate: 7.3647e-05]
	Learning Rate: 7.36475e-05
	LOSS [training: 0.2193104396818967 | validation: 0.30008705362123034]
	TIME [epoch: 9.66 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2496119937844062		[learning rate: 7.338e-05]
	Learning Rate: 7.33802e-05
	LOSS [training: 0.2496119937844062 | validation: 0.3006841864291701]
	TIME [epoch: 9.66 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2258602670383983		[learning rate: 7.3114e-05]
	Learning Rate: 7.31139e-05
	LOSS [training: 0.2258602670383983 | validation: 0.28760713019170114]
	TIME [epoch: 9.67 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2029516489819776		[learning rate: 7.2849e-05]
	Learning Rate: 7.28486e-05
	LOSS [training: 0.2029516489819776 | validation: 0.30602170590738154]
	TIME [epoch: 9.67 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20270684355597707		[learning rate: 7.2584e-05]
	Learning Rate: 7.25842e-05
	LOSS [training: 0.20270684355597707 | validation: 0.30321183726453965]
	TIME [epoch: 9.65 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20177922376860086		[learning rate: 7.2321e-05]
	Learning Rate: 7.23208e-05
	LOSS [training: 0.20177922376860086 | validation: 0.2964196848282876]
	TIME [epoch: 9.66 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21675898774993882		[learning rate: 7.2058e-05]
	Learning Rate: 7.20583e-05
	LOSS [training: 0.21675898774993882 | validation: 0.31023421018171476]
	TIME [epoch: 9.68 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21326303598891924		[learning rate: 7.1797e-05]
	Learning Rate: 7.17968e-05
	LOSS [training: 0.21326303598891924 | validation: 0.2965252878586573]
	TIME [epoch: 9.67 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20098130299657818		[learning rate: 7.1536e-05]
	Learning Rate: 7.15363e-05
	LOSS [training: 0.20098130299657818 | validation: 0.2890732888383154]
	TIME [epoch: 9.66 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20666746377351472		[learning rate: 7.1277e-05]
	Learning Rate: 7.12767e-05
	LOSS [training: 0.20666746377351472 | validation: 0.2981141807311779]
	TIME [epoch: 9.66 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2102157452925458		[learning rate: 7.1018e-05]
	Learning Rate: 7.1018e-05
	LOSS [training: 0.2102157452925458 | validation: 0.31429599291037963]
	TIME [epoch: 9.68 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.209740631076838		[learning rate: 7.076e-05]
	Learning Rate: 7.07603e-05
	LOSS [training: 0.209740631076838 | validation: 0.30087980063832453]
	TIME [epoch: 9.66 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20075236348574976		[learning rate: 7.0503e-05]
	Learning Rate: 7.05035e-05
	LOSS [training: 0.20075236348574976 | validation: 0.306362345414605]
	TIME [epoch: 9.66 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1909961889396613		[learning rate: 7.0248e-05]
	Learning Rate: 7.02476e-05
	LOSS [training: 0.1909961889396613 | validation: 0.30135963777136504]
	TIME [epoch: 9.66 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20255018127031024		[learning rate: 6.9993e-05]
	Learning Rate: 6.99927e-05
	LOSS [training: 0.20255018127031024 | validation: 0.29712888215689737]
	TIME [epoch: 9.67 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20061677683666645		[learning rate: 6.9739e-05]
	Learning Rate: 6.97387e-05
	LOSS [training: 0.20061677683666645 | validation: 0.30136209539043296]
	TIME [epoch: 9.66 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19164640141298114		[learning rate: 6.9486e-05]
	Learning Rate: 6.94856e-05
	LOSS [training: 0.19164640141298114 | validation: 0.286212931561544]
	TIME [epoch: 9.66 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20140748913634515		[learning rate: 6.9233e-05]
	Learning Rate: 6.92334e-05
	LOSS [training: 0.20140748913634515 | validation: 0.29903325629248106]
	TIME [epoch: 9.68 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.197232658450958		[learning rate: 6.8982e-05]
	Learning Rate: 6.89822e-05
	LOSS [training: 0.197232658450958 | validation: 0.2966396495834415]
	TIME [epoch: 9.66 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20429482758997025		[learning rate: 6.8732e-05]
	Learning Rate: 6.87318e-05
	LOSS [training: 0.20429482758997025 | validation: 0.2899477731987376]
	TIME [epoch: 9.66 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20799436266998778		[learning rate: 6.8482e-05]
	Learning Rate: 6.84824e-05
	LOSS [training: 0.20799436266998778 | validation: 0.2885891912189868]
	TIME [epoch: 9.66 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19908197239188632		[learning rate: 6.8234e-05]
	Learning Rate: 6.82339e-05
	LOSS [training: 0.19908197239188632 | validation: 0.28567530386669443]
	TIME [epoch: 9.68 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20204461415521252		[learning rate: 6.7986e-05]
	Learning Rate: 6.79862e-05
	LOSS [training: 0.20204461415521252 | validation: 0.2746745203947221]
	TIME [epoch: 9.65 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20067647222448998		[learning rate: 6.774e-05]
	Learning Rate: 6.77395e-05
	LOSS [training: 0.20067647222448998 | validation: 0.27396381410692267]
	TIME [epoch: 9.66 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20434468558613336		[learning rate: 6.7494e-05]
	Learning Rate: 6.74937e-05
	LOSS [training: 0.20434468558613336 | validation: 0.32307885309968415]
	TIME [epoch: 9.65 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20921097897385002		[learning rate: 6.7249e-05]
	Learning Rate: 6.72488e-05
	LOSS [training: 0.20921097897385002 | validation: 0.3125575836893573]
	TIME [epoch: 9.68 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2279682905780538		[learning rate: 6.7005e-05]
	Learning Rate: 6.70047e-05
	LOSS [training: 0.2279682905780538 | validation: 0.3568298718600721]
	TIME [epoch: 9.66 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23761082017306526		[learning rate: 6.6762e-05]
	Learning Rate: 6.67616e-05
	LOSS [training: 0.23761082017306526 | validation: 0.33282742569579127]
	TIME [epoch: 9.66 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22111788408883895		[learning rate: 6.6519e-05]
	Learning Rate: 6.65192e-05
	LOSS [training: 0.22111788408883895 | validation: 0.30846633539192025]
	TIME [epoch: 9.67 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20426583424896733		[learning rate: 6.6278e-05]
	Learning Rate: 6.62778e-05
	LOSS [training: 0.20426583424896733 | validation: 0.2948373620762564]
	TIME [epoch: 9.69 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2004962776184147		[learning rate: 6.6037e-05]
	Learning Rate: 6.60373e-05
	LOSS [training: 0.2004962776184147 | validation: 0.293032221997719]
	TIME [epoch: 9.7 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19751491914134472		[learning rate: 6.5798e-05]
	Learning Rate: 6.57977e-05
	LOSS [training: 0.19751491914134472 | validation: 0.2912779343796462]
	TIME [epoch: 9.66 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19537745702399653		[learning rate: 6.5559e-05]
	Learning Rate: 6.55589e-05
	LOSS [training: 0.19537745702399653 | validation: 0.29349819275757455]
	TIME [epoch: 9.68 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18835833777683683		[learning rate: 6.5321e-05]
	Learning Rate: 6.5321e-05
	LOSS [training: 0.18835833777683683 | validation: 0.27720865444669474]
	TIME [epoch: 9.67 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1974928728674231		[learning rate: 6.5084e-05]
	Learning Rate: 6.50839e-05
	LOSS [training: 0.1974928728674231 | validation: 0.2861626966090572]
	TIME [epoch: 9.66 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19345551531015825		[learning rate: 6.4848e-05]
	Learning Rate: 6.48477e-05
	LOSS [training: 0.19345551531015825 | validation: 0.2859816898139254]
	TIME [epoch: 9.67 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18917201514511492		[learning rate: 6.4612e-05]
	Learning Rate: 6.46124e-05
	LOSS [training: 0.18917201514511492 | validation: 0.28926402081773994]
	TIME [epoch: 9.68 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20318156275569638		[learning rate: 6.4378e-05]
	Learning Rate: 6.43779e-05
	LOSS [training: 0.20318156275569638 | validation: 0.288017281415981]
	TIME [epoch: 9.67 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1983414342960816		[learning rate: 6.4144e-05]
	Learning Rate: 6.41443e-05
	LOSS [training: 0.1983414342960816 | validation: 0.26439388461882246]
	TIME [epoch: 9.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_1489.pth
	Model improved!!!
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2017121238347273		[learning rate: 6.3911e-05]
	Learning Rate: 6.39115e-05
	LOSS [training: 0.2017121238347273 | validation: 0.29058040297718424]
	TIME [epoch: 9.67 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19316555766684726		[learning rate: 6.368e-05]
	Learning Rate: 6.36795e-05
	LOSS [training: 0.19316555766684726 | validation: 0.27412736997535986]
	TIME [epoch: 9.67 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20562946282629574		[learning rate: 6.3448e-05]
	Learning Rate: 6.34485e-05
	LOSS [training: 0.20562946282629574 | validation: 0.2743758067957432]
	TIME [epoch: 9.66 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21130017326735667		[learning rate: 6.3218e-05]
	Learning Rate: 6.32182e-05
	LOSS [training: 0.21130017326735667 | validation: 0.2831362531892822]
	TIME [epoch: 9.65 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2309566294460264		[learning rate: 6.2989e-05]
	Learning Rate: 6.29888e-05
	LOSS [training: 0.2309566294460264 | validation: 0.3057721665879544]
	TIME [epoch: 9.67 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24471976946638105		[learning rate: 6.276e-05]
	Learning Rate: 6.27602e-05
	LOSS [training: 0.24471976946638105 | validation: 0.30034457931158254]
	TIME [epoch: 9.66 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22871862444743338		[learning rate: 6.2532e-05]
	Learning Rate: 6.25324e-05
	LOSS [training: 0.22871862444743338 | validation: 0.2859541785462878]
	TIME [epoch: 9.66 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20693200114026347		[learning rate: 6.2305e-05]
	Learning Rate: 6.23055e-05
	LOSS [training: 0.20693200114026347 | validation: 0.2764037759151346]
	TIME [epoch: 9.65 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2202436862344978		[learning rate: 6.2079e-05]
	Learning Rate: 6.20794e-05
	LOSS [training: 0.2202436862344978 | validation: 0.2873080980256179]
	TIME [epoch: 9.68 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2306005607005594		[learning rate: 6.1854e-05]
	Learning Rate: 6.18541e-05
	LOSS [training: 0.2306005607005594 | validation: 0.3031160718571357]
	TIME [epoch: 9.66 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25161557447560934		[learning rate: 6.163e-05]
	Learning Rate: 6.16296e-05
	LOSS [training: 0.25161557447560934 | validation: 0.3034335119714341]
	TIME [epoch: 9.66 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23964500771027808		[learning rate: 6.1406e-05]
	Learning Rate: 6.1406e-05
	LOSS [training: 0.23964500771027808 | validation: 0.28113999068342155]
	TIME [epoch: 9.65 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19623057558972404		[learning rate: 6.1183e-05]
	Learning Rate: 6.11831e-05
	LOSS [training: 0.19623057558972404 | validation: 0.27726039356810717]
	TIME [epoch: 9.68 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19237302730311484		[learning rate: 6.0961e-05]
	Learning Rate: 6.09611e-05
	LOSS [training: 0.19237302730311484 | validation: 0.27576040902815807]
	TIME [epoch: 9.65 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20118874431245987		[learning rate: 6.074e-05]
	Learning Rate: 6.07399e-05
	LOSS [training: 0.20118874431245987 | validation: 0.27529880098790127]
	TIME [epoch: 9.66 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2075223285901689		[learning rate: 6.0519e-05]
	Learning Rate: 6.05194e-05
	LOSS [training: 0.2075223285901689 | validation: 0.29016003510845506]
	TIME [epoch: 9.66 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2003464897091039		[learning rate: 6.03e-05]
	Learning Rate: 6.02998e-05
	LOSS [training: 0.2003464897091039 | validation: 0.28269838063309716]
	TIME [epoch: 9.67 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1937889562915759		[learning rate: 6.0081e-05]
	Learning Rate: 6.0081e-05
	LOSS [training: 0.1937889562915759 | validation: 0.2697409908205882]
	TIME [epoch: 9.65 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19477795391442893		[learning rate: 5.9863e-05]
	Learning Rate: 5.98629e-05
	LOSS [training: 0.19477795391442893 | validation: 0.28686513713162776]
	TIME [epoch: 9.65 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2046045501404341		[learning rate: 5.9646e-05]
	Learning Rate: 5.96457e-05
	LOSS [training: 0.2046045501404341 | validation: 0.2785320390167885]
	TIME [epoch: 9.67 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20314596135935253		[learning rate: 5.9429e-05]
	Learning Rate: 5.94292e-05
	LOSS [training: 0.20314596135935253 | validation: 0.2875019496507818]
	TIME [epoch: 9.65 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2011438237879596		[learning rate: 5.9214e-05]
	Learning Rate: 5.92136e-05
	LOSS [training: 0.2011438237879596 | validation: 0.27561831151011523]
	TIME [epoch: 9.66 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2010605473772355		[learning rate: 5.8999e-05]
	Learning Rate: 5.89987e-05
	LOSS [training: 0.2010605473772355 | validation: 0.2848077980903706]
	TIME [epoch: 9.65 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20123916660837943		[learning rate: 5.8785e-05]
	Learning Rate: 5.87846e-05
	LOSS [training: 0.20123916660837943 | validation: 0.29388694239329644]
	TIME [epoch: 9.67 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19599642470744286		[learning rate: 5.8571e-05]
	Learning Rate: 5.85712e-05
	LOSS [training: 0.19599642470744286 | validation: 0.2745762635759364]
	TIME [epoch: 9.65 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19561410446337593		[learning rate: 5.8359e-05]
	Learning Rate: 5.83586e-05
	LOSS [training: 0.19561410446337593 | validation: 0.29231669079221123]
	TIME [epoch: 9.65 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19512492033811435		[learning rate: 5.8147e-05]
	Learning Rate: 5.81469e-05
	LOSS [training: 0.19512492033811435 | validation: 0.27160214027288265]
	TIME [epoch: 9.65 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1891650843489983		[learning rate: 5.7936e-05]
	Learning Rate: 5.79358e-05
	LOSS [training: 0.1891650843489983 | validation: 0.2654018574580857]
	TIME [epoch: 9.67 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2012017586339805		[learning rate: 5.7726e-05]
	Learning Rate: 5.77256e-05
	LOSS [training: 0.2012017586339805 | validation: 0.27670875817240015]
	TIME [epoch: 9.66 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20125229671321238		[learning rate: 5.7516e-05]
	Learning Rate: 5.75161e-05
	LOSS [training: 0.20125229671321238 | validation: 0.27272548340845343]
	TIME [epoch: 9.65 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19329129083000327		[learning rate: 5.7307e-05]
	Learning Rate: 5.73074e-05
	LOSS [training: 0.19329129083000327 | validation: 0.26818857848407457]
	TIME [epoch: 9.66 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20645061509735535		[learning rate: 5.7099e-05]
	Learning Rate: 5.70994e-05
	LOSS [training: 0.20645061509735535 | validation: 0.3001560756217749]
	TIME [epoch: 9.66 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21130022050131977		[learning rate: 5.6892e-05]
	Learning Rate: 5.68922e-05
	LOSS [training: 0.21130022050131977 | validation: 0.3076722221791614]
	TIME [epoch: 9.66 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20773620841510249		[learning rate: 5.6686e-05]
	Learning Rate: 5.66857e-05
	LOSS [training: 0.20773620841510249 | validation: 0.30123842049304295]
	TIME [epoch: 9.66 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20541052370868468		[learning rate: 5.648e-05]
	Learning Rate: 5.648e-05
	LOSS [training: 0.20541052370868468 | validation: 0.29592863005243547]
	TIME [epoch: 9.67 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19879613153774287		[learning rate: 5.6275e-05]
	Learning Rate: 5.6275e-05
	LOSS [training: 0.19879613153774287 | validation: 0.28662813644179913]
	TIME [epoch: 9.66 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18969654179530931		[learning rate: 5.6071e-05]
	Learning Rate: 5.60708e-05
	LOSS [training: 0.18969654179530931 | validation: 0.28716157234368966]
	TIME [epoch: 9.66 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19176330063122365		[learning rate: 5.5867e-05]
	Learning Rate: 5.58673e-05
	LOSS [training: 0.19176330063122365 | validation: 0.28972891595938266]
	TIME [epoch: 9.65 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19410279059982108		[learning rate: 5.5665e-05]
	Learning Rate: 5.56646e-05
	LOSS [training: 0.19410279059982108 | validation: 0.28521311008720435]
	TIME [epoch: 9.67 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19528038392678		[learning rate: 5.5463e-05]
	Learning Rate: 5.54626e-05
	LOSS [training: 0.19528038392678 | validation: 0.29221145027641343]
	TIME [epoch: 9.66 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19854835952909466		[learning rate: 5.5261e-05]
	Learning Rate: 5.52613e-05
	LOSS [training: 0.19854835952909466 | validation: 0.3147379709972616]
	TIME [epoch: 9.65 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20700879469616199		[learning rate: 5.5061e-05]
	Learning Rate: 5.50608e-05
	LOSS [training: 0.20700879469616199 | validation: 0.32464260983449356]
	TIME [epoch: 9.65 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22384340786256102		[learning rate: 5.4861e-05]
	Learning Rate: 5.48609e-05
	LOSS [training: 0.22384340786256102 | validation: 0.3099553337242353]
	TIME [epoch: 9.67 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21225786783194356		[learning rate: 5.4662e-05]
	Learning Rate: 5.46618e-05
	LOSS [training: 0.21225786783194356 | validation: 0.30909968111010094]
	TIME [epoch: 9.65 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21061867921374205		[learning rate: 5.4463e-05]
	Learning Rate: 5.44635e-05
	LOSS [training: 0.21061867921374205 | validation: 0.3136464969011755]
	TIME [epoch: 9.66 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20061941829203658		[learning rate: 5.4266e-05]
	Learning Rate: 5.42658e-05
	LOSS [training: 0.20061941829203658 | validation: 0.29330922454466857]
	TIME [epoch: 9.66 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20161853052226403		[learning rate: 5.4069e-05]
	Learning Rate: 5.40689e-05
	LOSS [training: 0.20161853052226403 | validation: 0.30282877111261525]
	TIME [epoch: 9.67 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1987034003177995		[learning rate: 5.3873e-05]
	Learning Rate: 5.38727e-05
	LOSS [training: 0.1987034003177995 | validation: 0.3061619408498622]
	TIME [epoch: 9.65 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20220694784673432		[learning rate: 5.3677e-05]
	Learning Rate: 5.36772e-05
	LOSS [training: 0.20220694784673432 | validation: 0.28460333041608255]
	TIME [epoch: 9.65 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19535582798043322		[learning rate: 5.3482e-05]
	Learning Rate: 5.34824e-05
	LOSS [training: 0.19535582798043322 | validation: 0.3032004203259872]
	TIME [epoch: 9.67 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19648361962390576		[learning rate: 5.3288e-05]
	Learning Rate: 5.32883e-05
	LOSS [training: 0.19648361962390576 | validation: 0.2820649737761554]
	TIME [epoch: 9.65 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20087969655276922		[learning rate: 5.3095e-05]
	Learning Rate: 5.30949e-05
	LOSS [training: 0.20087969655276922 | validation: 0.27425930288721434]
	TIME [epoch: 9.65 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18469377135309978		[learning rate: 5.2902e-05]
	Learning Rate: 5.29022e-05
	LOSS [training: 0.18469377135309978 | validation: 0.30954237852978717]
	TIME [epoch: 9.66 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1994562121269588		[learning rate: 5.271e-05]
	Learning Rate: 5.27102e-05
	LOSS [training: 0.1994562121269588 | validation: 0.29590154766130644]
	TIME [epoch: 9.68 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20571991203232973		[learning rate: 5.2519e-05]
	Learning Rate: 5.25189e-05
	LOSS [training: 0.20571991203232973 | validation: 0.296781160968668]
	TIME [epoch: 9.65 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1988010630279165		[learning rate: 5.2328e-05]
	Learning Rate: 5.23283e-05
	LOSS [training: 0.1988010630279165 | validation: 0.2754400272067922]
	TIME [epoch: 9.66 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19425971688609073		[learning rate: 5.2138e-05]
	Learning Rate: 5.21384e-05
	LOSS [training: 0.19425971688609073 | validation: 0.27218579754979155]
	TIME [epoch: 9.65 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20045213394328726		[learning rate: 5.1949e-05]
	Learning Rate: 5.19492e-05
	LOSS [training: 0.20045213394328726 | validation: 0.286770985963014]
	TIME [epoch: 9.67 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1953245745038294		[learning rate: 5.1761e-05]
	Learning Rate: 5.17607e-05
	LOSS [training: 0.1953245745038294 | validation: 0.28003658689833677]
	TIME [epoch: 9.66 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19089854485424937		[learning rate: 5.1573e-05]
	Learning Rate: 5.15729e-05
	LOSS [training: 0.19089854485424937 | validation: 0.2855075876028758]
	TIME [epoch: 9.65 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19633619453011644		[learning rate: 5.1386e-05]
	Learning Rate: 5.13857e-05
	LOSS [training: 0.19633619453011644 | validation: 0.27889061234230883]
	TIME [epoch: 9.66 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19499755804250438		[learning rate: 5.1199e-05]
	Learning Rate: 5.11992e-05
	LOSS [training: 0.19499755804250438 | validation: 0.2658110178381261]
	TIME [epoch: 9.67 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19488638301074696		[learning rate: 5.1013e-05]
	Learning Rate: 5.10134e-05
	LOSS [training: 0.19488638301074696 | validation: 0.2879620608989533]
	TIME [epoch: 9.65 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1985191146971625		[learning rate: 5.0828e-05]
	Learning Rate: 5.08283e-05
	LOSS [training: 0.1985191146971625 | validation: 0.2939486329722982]
	TIME [epoch: 9.66 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2046532822145979		[learning rate: 5.0644e-05]
	Learning Rate: 5.06438e-05
	LOSS [training: 0.2046532822145979 | validation: 0.2888581386873423]
	TIME [epoch: 9.67 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19701820335567294		[learning rate: 5.046e-05]
	Learning Rate: 5.046e-05
	LOSS [training: 0.19701820335567294 | validation: 0.286718150590719]
	TIME [epoch: 9.66 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1911273359598077		[learning rate: 5.0277e-05]
	Learning Rate: 5.02769e-05
	LOSS [training: 0.1911273359598077 | validation: 0.2996839663894709]
	TIME [epoch: 9.65 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19964409707494815		[learning rate: 5.0094e-05]
	Learning Rate: 5.00944e-05
	LOSS [training: 0.19964409707494815 | validation: 0.28114268469504416]
	TIME [epoch: 9.66 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18820258684739283		[learning rate: 4.9913e-05]
	Learning Rate: 4.99127e-05
	LOSS [training: 0.18820258684739283 | validation: 0.2784214927111245]
	TIME [epoch: 9.68 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20048197614921098		[learning rate: 4.9732e-05]
	Learning Rate: 4.97315e-05
	LOSS [training: 0.20048197614921098 | validation: 0.30553357386078817]
	TIME [epoch: 9.65 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20691309666705693		[learning rate: 4.9551e-05]
	Learning Rate: 4.9551e-05
	LOSS [training: 0.20691309666705693 | validation: 0.3201371716733159]
	TIME [epoch: 9.66 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1999476132968537		[learning rate: 4.9371e-05]
	Learning Rate: 4.93712e-05
	LOSS [training: 0.1999476132968537 | validation: 0.2957822888509816]
	TIME [epoch: 9.65 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.208608606653765		[learning rate: 4.9192e-05]
	Learning Rate: 4.9192e-05
	LOSS [training: 0.208608606653765 | validation: 0.28113780655263165]
	TIME [epoch: 9.68 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2013172186929914		[learning rate: 4.9014e-05]
	Learning Rate: 4.90135e-05
	LOSS [training: 0.2013172186929914 | validation: 0.2874364564054946]
	TIME [epoch: 9.65 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20417979321138297		[learning rate: 4.8836e-05]
	Learning Rate: 4.88356e-05
	LOSS [training: 0.20417979321138297 | validation: 0.29730066882788314]
	TIME [epoch: 9.64 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2055298097413417		[learning rate: 4.8658e-05]
	Learning Rate: 4.86584e-05
	LOSS [training: 0.2055298097413417 | validation: 0.28566026277297524]
	TIME [epoch: 9.67 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20873102249952682		[learning rate: 4.8482e-05]
	Learning Rate: 4.84818e-05
	LOSS [training: 0.20873102249952682 | validation: 0.3102026571846851]
	TIME [epoch: 9.66 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22213981487620868		[learning rate: 4.8306e-05]
	Learning Rate: 4.83059e-05
	LOSS [training: 0.22213981487620868 | validation: 0.32276169651467773]
	TIME [epoch: 9.65 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2183030209192986		[learning rate: 4.8131e-05]
	Learning Rate: 4.81306e-05
	LOSS [training: 0.2183030209192986 | validation: 0.30859052521043356]
	TIME [epoch: 9.65 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20988350808780928		[learning rate: 4.7956e-05]
	Learning Rate: 4.79559e-05
	LOSS [training: 0.20988350808780928 | validation: 0.27938437715542525]
	TIME [epoch: 9.67 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19972093715039835		[learning rate: 4.7782e-05]
	Learning Rate: 4.77819e-05
	LOSS [training: 0.19972093715039835 | validation: 0.28410271871698856]
	TIME [epoch: 9.65 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2016524143651941		[learning rate: 4.7608e-05]
	Learning Rate: 4.76085e-05
	LOSS [training: 0.2016524143651941 | validation: 0.294184553197042]
	TIME [epoch: 9.65 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2007924895423614		[learning rate: 4.7436e-05]
	Learning Rate: 4.74357e-05
	LOSS [training: 0.2007924895423614 | validation: 0.26899431386454564]
	TIME [epoch: 9.65 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20186971470762347		[learning rate: 4.7264e-05]
	Learning Rate: 4.72636e-05
	LOSS [training: 0.20186971470762347 | validation: 0.29740420988079125]
	TIME [epoch: 9.67 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20262012661848844		[learning rate: 4.7092e-05]
	Learning Rate: 4.7092e-05
	LOSS [training: 0.20262012661848844 | validation: 0.2701260756820842]
	TIME [epoch: 9.66 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20660429846360415		[learning rate: 4.6921e-05]
	Learning Rate: 4.69211e-05
	LOSS [training: 0.20660429846360415 | validation: 0.2800626999468485]
	TIME [epoch: 9.66 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21609198164269622		[learning rate: 4.6751e-05]
	Learning Rate: 4.67508e-05
	LOSS [training: 0.21609198164269622 | validation: 0.28103558129759226]
	TIME [epoch: 9.66 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21551435282542517		[learning rate: 4.6581e-05]
	Learning Rate: 4.65812e-05
	LOSS [training: 0.21551435282542517 | validation: 0.2890622590076684]
	TIME [epoch: 9.67 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22848108225940794		[learning rate: 4.6412e-05]
	Learning Rate: 4.64121e-05
	LOSS [training: 0.22848108225940794 | validation: 0.27087005167876915]
	TIME [epoch: 9.65 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23156542885163406		[learning rate: 4.6244e-05]
	Learning Rate: 4.62437e-05
	LOSS [training: 0.23156542885163406 | validation: 0.30461740997402104]
	TIME [epoch: 9.66 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22620720870139896		[learning rate: 4.6076e-05]
	Learning Rate: 4.60759e-05
	LOSS [training: 0.22620720870139896 | validation: 0.2973567854588827]
	TIME [epoch: 9.67 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20564836786856358		[learning rate: 4.5909e-05]
	Learning Rate: 4.59087e-05
	LOSS [training: 0.20564836786856358 | validation: 0.2957736912478935]
	TIME [epoch: 9.65 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20927919382690213		[learning rate: 4.5742e-05]
	Learning Rate: 4.57421e-05
	LOSS [training: 0.20927919382690213 | validation: 0.29594253044335445]
	TIME [epoch: 9.65 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20514498908788661		[learning rate: 4.5576e-05]
	Learning Rate: 4.55761e-05
	LOSS [training: 0.20514498908788661 | validation: 0.2928702847304172]
	TIME [epoch: 9.66 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20032084340905282		[learning rate: 4.5411e-05]
	Learning Rate: 4.54107e-05
	LOSS [training: 0.20032084340905282 | validation: 0.27051719850822303]
	TIME [epoch: 9.67 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20143563997455355		[learning rate: 4.5246e-05]
	Learning Rate: 4.52459e-05
	LOSS [training: 0.20143563997455355 | validation: 0.285230119221267]
	TIME [epoch: 9.66 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19457102796909934		[learning rate: 4.5082e-05]
	Learning Rate: 4.50817e-05
	LOSS [training: 0.19457102796909934 | validation: 0.27935856175946105]
	TIME [epoch: 9.66 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.204710550172938		[learning rate: 4.4918e-05]
	Learning Rate: 4.49181e-05
	LOSS [training: 0.204710550172938 | validation: 0.2861288877200545]
	TIME [epoch: 9.66 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21911769652814747		[learning rate: 4.4755e-05]
	Learning Rate: 4.47551e-05
	LOSS [training: 0.21911769652814747 | validation: 0.29247542818657496]
	TIME [epoch: 9.68 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21458686650350872		[learning rate: 4.4593e-05]
	Learning Rate: 4.45926e-05
	LOSS [training: 0.21458686650350872 | validation: 0.2763466702555514]
	TIME [epoch: 9.66 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20575197232218706		[learning rate: 4.4431e-05]
	Learning Rate: 4.44308e-05
	LOSS [training: 0.20575197232218706 | validation: 0.2719158598454582]
	TIME [epoch: 9.66 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20644554050200764		[learning rate: 4.427e-05]
	Learning Rate: 4.42696e-05
	LOSS [training: 0.20644554050200764 | validation: 0.3027690037029171]
	TIME [epoch: 9.66 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20528905541766668		[learning rate: 4.4109e-05]
	Learning Rate: 4.41089e-05
	LOSS [training: 0.20528905541766668 | validation: 0.2826347617343764]
	TIME [epoch: 9.67 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20715845630019358		[learning rate: 4.3949e-05]
	Learning Rate: 4.39489e-05
	LOSS [training: 0.20715845630019358 | validation: 0.27968367058272464]
	TIME [epoch: 9.66 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20390547471924494		[learning rate: 4.3789e-05]
	Learning Rate: 4.37893e-05
	LOSS [training: 0.20390547471924494 | validation: 0.28294328521217105]
	TIME [epoch: 9.65 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1999390497104005		[learning rate: 4.363e-05]
	Learning Rate: 4.36304e-05
	LOSS [training: 0.1999390497104005 | validation: 0.2735131663528492]
	TIME [epoch: 9.67 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19728997116832597		[learning rate: 4.3472e-05]
	Learning Rate: 4.34721e-05
	LOSS [training: 0.19728997116832597 | validation: 0.28830588061931023]
	TIME [epoch: 9.65 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2026240210411982		[learning rate: 4.3314e-05]
	Learning Rate: 4.33143e-05
	LOSS [training: 0.2026240210411982 | validation: 0.26590067696227077]
	TIME [epoch: 9.65 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20817715466592687		[learning rate: 4.3157e-05]
	Learning Rate: 4.31571e-05
	LOSS [training: 0.20817715466592687 | validation: 0.2859525956156568]
	TIME [epoch: 9.66 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20825008383035942		[learning rate: 4.3001e-05]
	Learning Rate: 4.30005e-05
	LOSS [training: 0.20825008383035942 | validation: 0.27319841873783185]
	TIME [epoch: 9.68 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19666202856241932		[learning rate: 4.2844e-05]
	Learning Rate: 4.28445e-05
	LOSS [training: 0.19666202856241932 | validation: 0.2723627312276891]
	TIME [epoch: 9.65 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20184912481373987		[learning rate: 4.2689e-05]
	Learning Rate: 4.2689e-05
	LOSS [training: 0.20184912481373987 | validation: 0.26843616956305555]
	TIME [epoch: 9.65 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19654973867516623		[learning rate: 4.2534e-05]
	Learning Rate: 4.25341e-05
	LOSS [training: 0.19654973867516623 | validation: 0.28516790914364387]
	TIME [epoch: 9.65 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.201049957173989		[learning rate: 4.238e-05]
	Learning Rate: 4.23797e-05
	LOSS [training: 0.201049957173989 | validation: 0.2775461985165354]
	TIME [epoch: 9.68 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1910021852989188		[learning rate: 4.2226e-05]
	Learning Rate: 4.22259e-05
	LOSS [training: 0.1910021852989188 | validation: 0.2994352738006083]
	TIME [epoch: 9.66 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19269576735608934		[learning rate: 4.2073e-05]
	Learning Rate: 4.20727e-05
	LOSS [training: 0.19269576735608934 | validation: 0.27393482178026424]
	TIME [epoch: 9.66 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1983789801720911		[learning rate: 4.192e-05]
	Learning Rate: 4.192e-05
	LOSS [training: 0.1983789801720911 | validation: 0.2678252066649256]
	TIME [epoch: 9.66 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20418462267225027		[learning rate: 4.1768e-05]
	Learning Rate: 4.17679e-05
	LOSS [training: 0.20418462267225027 | validation: 0.28204409289423327]
	TIME [epoch: 9.67 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19683646544681563		[learning rate: 4.1616e-05]
	Learning Rate: 4.16163e-05
	LOSS [training: 0.19683646544681563 | validation: 0.27117928681013054]
	TIME [epoch: 9.65 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19897148474181758		[learning rate: 4.1465e-05]
	Learning Rate: 4.14652e-05
	LOSS [training: 0.19897148474181758 | validation: 0.25662177900369965]
	TIME [epoch: 9.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_1609.pth
	Model improved!!!
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19794495005230778		[learning rate: 4.1315e-05]
	Learning Rate: 4.13148e-05
	LOSS [training: 0.19794495005230778 | validation: 0.2803572204529279]
	TIME [epoch: 9.68 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2006816892396251		[learning rate: 4.1165e-05]
	Learning Rate: 4.11648e-05
	LOSS [training: 0.2006816892396251 | validation: 0.28786657183166514]
	TIME [epoch: 9.66 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1941950515130179		[learning rate: 4.1015e-05]
	Learning Rate: 4.10154e-05
	LOSS [training: 0.1941950515130179 | validation: 0.282602174409373]
	TIME [epoch: 9.65 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19312085996412884		[learning rate: 4.0867e-05]
	Learning Rate: 4.08666e-05
	LOSS [training: 0.19312085996412884 | validation: 0.2733242501726689]
	TIME [epoch: 9.66 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19560840419181852		[learning rate: 4.0718e-05]
	Learning Rate: 4.07183e-05
	LOSS [training: 0.19560840419181852 | validation: 0.2839800704584409]
	TIME [epoch: 9.68 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19381654609597226		[learning rate: 4.0571e-05]
	Learning Rate: 4.05705e-05
	LOSS [training: 0.19381654609597226 | validation: 0.29795850952090897]
	TIME [epoch: 9.65 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20216712306747991		[learning rate: 4.0423e-05]
	Learning Rate: 4.04233e-05
	LOSS [training: 0.20216712306747991 | validation: 0.2942456641813657]
	TIME [epoch: 9.65 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1925215915407421		[learning rate: 4.0277e-05]
	Learning Rate: 4.02766e-05
	LOSS [training: 0.1925215915407421 | validation: 0.28198832424078235]
	TIME [epoch: 9.66 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1870550579460854		[learning rate: 4.013e-05]
	Learning Rate: 4.01304e-05
	LOSS [training: 0.1870550579460854 | validation: 0.2784744469508844]
	TIME [epoch: 9.67 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19810306926189392		[learning rate: 3.9985e-05]
	Learning Rate: 3.99848e-05
	LOSS [training: 0.19810306926189392 | validation: 0.2693052644785785]
	TIME [epoch: 9.66 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19616053250339044		[learning rate: 3.984e-05]
	Learning Rate: 3.98397e-05
	LOSS [training: 0.19616053250339044 | validation: 0.275976840098286]
	TIME [epoch: 9.65 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1916735311132594		[learning rate: 3.9695e-05]
	Learning Rate: 3.96951e-05
	LOSS [training: 0.1916735311132594 | validation: 0.29192467877964645]
	TIME [epoch: 9.67 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19126959697729082		[learning rate: 3.9551e-05]
	Learning Rate: 3.9551e-05
	LOSS [training: 0.19126959697729082 | validation: 0.2691588349263154]
	TIME [epoch: 9.66 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1958036211905797		[learning rate: 3.9408e-05]
	Learning Rate: 3.94075e-05
	LOSS [training: 0.1958036211905797 | validation: 0.2849201666385666]
	TIME [epoch: 9.65 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1950670987397668		[learning rate: 3.9264e-05]
	Learning Rate: 3.92645e-05
	LOSS [training: 0.1950670987397668 | validation: 0.2838762197268467]
	TIME [epoch: 9.65 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1984094560188327		[learning rate: 3.9122e-05]
	Learning Rate: 3.9122e-05
	LOSS [training: 0.1984094560188327 | validation: 0.28523816281016223]
	TIME [epoch: 9.68 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20154454701711583		[learning rate: 3.898e-05]
	Learning Rate: 3.898e-05
	LOSS [training: 0.20154454701711583 | validation: 0.2728421569994377]
	TIME [epoch: 9.66 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19337433659758924		[learning rate: 3.8839e-05]
	Learning Rate: 3.88386e-05
	LOSS [training: 0.19337433659758924 | validation: 0.27527854147362846]
	TIME [epoch: 9.66 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19508736113017422		[learning rate: 3.8698e-05]
	Learning Rate: 3.86976e-05
	LOSS [training: 0.19508736113017422 | validation: 0.2758165326871405]
	TIME [epoch: 9.65 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19528255385192578		[learning rate: 3.8557e-05]
	Learning Rate: 3.85572e-05
	LOSS [training: 0.19528255385192578 | validation: 0.2727540460638172]
	TIME [epoch: 9.67 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19730668491715744		[learning rate: 3.8417e-05]
	Learning Rate: 3.84173e-05
	LOSS [training: 0.19730668491715744 | validation: 0.30137569640796036]
	TIME [epoch: 9.66 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2133239780733287		[learning rate: 3.8278e-05]
	Learning Rate: 3.82778e-05
	LOSS [training: 0.2133239780733287 | validation: 0.3177559345215469]
	TIME [epoch: 9.66 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19933878799002827		[learning rate: 3.8139e-05]
	Learning Rate: 3.81389e-05
	LOSS [training: 0.19933878799002827 | validation: 0.2973741515573347]
	TIME [epoch: 9.66 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20141685201591		[learning rate: 3.8001e-05]
	Learning Rate: 3.80005e-05
	LOSS [training: 0.20141685201591 | validation: 0.28487168651536476]
	TIME [epoch: 9.67 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19301764519274067		[learning rate: 3.7863e-05]
	Learning Rate: 3.78626e-05
	LOSS [training: 0.19301764519274067 | validation: 0.2618948031565575]
	TIME [epoch: 9.65 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19142490953704397		[learning rate: 3.7725e-05]
	Learning Rate: 3.77252e-05
	LOSS [training: 0.19142490953704397 | validation: 0.26235265156692117]
	TIME [epoch: 9.65 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19801975714982073		[learning rate: 3.7588e-05]
	Learning Rate: 3.75883e-05
	LOSS [training: 0.19801975714982073 | validation: 0.271164891475646]
	TIME [epoch: 9.68 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20280291409280649		[learning rate: 3.7452e-05]
	Learning Rate: 3.74519e-05
	LOSS [training: 0.20280291409280649 | validation: 0.2855804834208336]
	TIME [epoch: 9.66 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20421204462375148		[learning rate: 3.7316e-05]
	Learning Rate: 3.7316e-05
	LOSS [training: 0.20421204462375148 | validation: 0.2759544753780064]
	TIME [epoch: 9.66 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20151889170024617		[learning rate: 3.7181e-05]
	Learning Rate: 3.71805e-05
	LOSS [training: 0.20151889170024617 | validation: 0.2817921049193192]
	TIME [epoch: 9.67 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2005950804094086		[learning rate: 3.7046e-05]
	Learning Rate: 3.70456e-05
	LOSS [training: 0.2005950804094086 | validation: 0.27736134394191553]
	TIME [epoch: 9.68 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19413619830620002		[learning rate: 3.6911e-05]
	Learning Rate: 3.69112e-05
	LOSS [training: 0.19413619830620002 | validation: 0.27212147524792907]
	TIME [epoch: 9.66 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19329081469626355		[learning rate: 3.6777e-05]
	Learning Rate: 3.67772e-05
	LOSS [training: 0.19329081469626355 | validation: 0.2642864373075502]
	TIME [epoch: 9.66 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1967650103176372		[learning rate: 3.6644e-05]
	Learning Rate: 3.66438e-05
	LOSS [training: 0.1967650103176372 | validation: 0.28900499616241115]
	TIME [epoch: 9.67 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20179045442769547		[learning rate: 3.6511e-05]
	Learning Rate: 3.65108e-05
	LOSS [training: 0.20179045442769547 | validation: 0.3139170636701984]
	TIME [epoch: 9.68 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20511726050452714		[learning rate: 3.6378e-05]
	Learning Rate: 3.63783e-05
	LOSS [training: 0.20511726050452714 | validation: 0.278407908256367]
	TIME [epoch: 9.66 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19044439359608184		[learning rate: 3.6246e-05]
	Learning Rate: 3.62463e-05
	LOSS [training: 0.19044439359608184 | validation: 0.2694867209775342]
	TIME [epoch: 9.66 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19294717871678096		[learning rate: 3.6115e-05]
	Learning Rate: 3.61147e-05
	LOSS [training: 0.19294717871678096 | validation: 0.26540808713177305]
	TIME [epoch: 9.66 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19377131500702616		[learning rate: 3.5984e-05]
	Learning Rate: 3.59837e-05
	LOSS [training: 0.19377131500702616 | validation: 0.2871265403544723]
	TIME [epoch: 9.67 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18963139250876543		[learning rate: 3.5853e-05]
	Learning Rate: 3.58531e-05
	LOSS [training: 0.18963139250876543 | validation: 0.2804620942936874]
	TIME [epoch: 9.65 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1950765932452838		[learning rate: 3.5723e-05]
	Learning Rate: 3.5723e-05
	LOSS [training: 0.1950765932452838 | validation: 0.26976444103447855]
	TIME [epoch: 9.65 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19166662594563533		[learning rate: 3.5593e-05]
	Learning Rate: 3.55933e-05
	LOSS [training: 0.19166662594563533 | validation: 0.29326445920852096]
	TIME [epoch: 9.67 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19354473313218631		[learning rate: 3.5464e-05]
	Learning Rate: 3.54641e-05
	LOSS [training: 0.19354473313218631 | validation: 0.2882558716557688]
	TIME [epoch: 9.65 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.195079042825585		[learning rate: 3.5335e-05]
	Learning Rate: 3.53354e-05
	LOSS [training: 0.195079042825585 | validation: 0.27503625955681815]
	TIME [epoch: 9.65 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19208857923505235		[learning rate: 3.5207e-05]
	Learning Rate: 3.52072e-05
	LOSS [training: 0.19208857923505235 | validation: 0.27693816742463884]
	TIME [epoch: 9.66 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19481619937018185		[learning rate: 3.5079e-05]
	Learning Rate: 3.50794e-05
	LOSS [training: 0.19481619937018185 | validation: 0.29100951413079096]
	TIME [epoch: 9.68 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19923939497099336		[learning rate: 3.4952e-05]
	Learning Rate: 3.49521e-05
	LOSS [training: 0.19923939497099336 | validation: 0.29149295279315884]
	TIME [epoch: 9.65 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19459121113684144		[learning rate: 3.4825e-05]
	Learning Rate: 3.48253e-05
	LOSS [training: 0.19459121113684144 | validation: 0.28944745821401213]
	TIME [epoch: 9.66 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19327944523075263		[learning rate: 3.4699e-05]
	Learning Rate: 3.46989e-05
	LOSS [training: 0.19327944523075263 | validation: 0.2976065077456095]
	TIME [epoch: 9.66 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20035730417392794		[learning rate: 3.4573e-05]
	Learning Rate: 3.4573e-05
	LOSS [training: 0.20035730417392794 | validation: 0.2874622850695944]
	TIME [epoch: 9.68 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1985150098677259		[learning rate: 3.4448e-05]
	Learning Rate: 3.44475e-05
	LOSS [training: 0.1985150098677259 | validation: 0.2952826819332572]
	TIME [epoch: 9.65 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19824984453077557		[learning rate: 3.4323e-05]
	Learning Rate: 3.43225e-05
	LOSS [training: 0.19824984453077557 | validation: 0.2779253411496575]
	TIME [epoch: 9.66 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20118221897824498		[learning rate: 3.4198e-05]
	Learning Rate: 3.4198e-05
	LOSS [training: 0.20118221897824498 | validation: 0.3048961783017343]
	TIME [epoch: 9.67 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20707311791277952		[learning rate: 3.4074e-05]
	Learning Rate: 3.40738e-05
	LOSS [training: 0.20707311791277952 | validation: 0.3055251519197147]
	TIME [epoch: 9.66 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19444419962923404		[learning rate: 3.395e-05]
	Learning Rate: 3.39502e-05
	LOSS [training: 0.19444419962923404 | validation: 0.2812767711088202]
	TIME [epoch: 9.66 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1917011328196678		[learning rate: 3.3827e-05]
	Learning Rate: 3.3827e-05
	LOSS [training: 0.1917011328196678 | validation: 0.2804251473184881]
	TIME [epoch: 9.66 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1907694519899595		[learning rate: 3.3704e-05]
	Learning Rate: 3.37042e-05
	LOSS [training: 0.1907694519899595 | validation: 0.29306987582476124]
	TIME [epoch: 9.67 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1890352529181488		[learning rate: 3.3582e-05]
	Learning Rate: 3.35819e-05
	LOSS [training: 0.1890352529181488 | validation: 0.26473604010173135]
	TIME [epoch: 9.66 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18903553759284178		[learning rate: 3.346e-05]
	Learning Rate: 3.346e-05
	LOSS [training: 0.18903553759284178 | validation: 0.2840590210978339]
	TIME [epoch: 9.66 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18115860052430016		[learning rate: 3.3339e-05]
	Learning Rate: 3.33386e-05
	LOSS [training: 0.18115860052430016 | validation: 0.28332462473710324]
	TIME [epoch: 9.65 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19213993473056595		[learning rate: 3.3218e-05]
	Learning Rate: 3.32176e-05
	LOSS [training: 0.19213993473056595 | validation: 0.2817420109397737]
	TIME [epoch: 9.68 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1979423046147351		[learning rate: 3.3097e-05]
	Learning Rate: 3.30971e-05
	LOSS [training: 0.1979423046147351 | validation: 0.2848743395501089]
	TIME [epoch: 9.66 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19729461714449162		[learning rate: 3.2977e-05]
	Learning Rate: 3.2977e-05
	LOSS [training: 0.19729461714449162 | validation: 0.2801809484868257]
	TIME [epoch: 9.66 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1972782960741135		[learning rate: 3.2857e-05]
	Learning Rate: 3.28573e-05
	LOSS [training: 0.1972782960741135 | validation: 0.2833445391544954]
	TIME [epoch: 9.66 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1881084823024366		[learning rate: 3.2738e-05]
	Learning Rate: 3.2738e-05
	LOSS [training: 0.1881084823024366 | validation: 0.28824993426782247]
	TIME [epoch: 9.68 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19532646942184612		[learning rate: 3.2619e-05]
	Learning Rate: 3.26192e-05
	LOSS [training: 0.19532646942184612 | validation: 0.2934647482280893]
	TIME [epoch: 9.66 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19390724430264444		[learning rate: 3.2501e-05]
	Learning Rate: 3.25009e-05
	LOSS [training: 0.19390724430264444 | validation: 0.2845744758958059]
	TIME [epoch: 9.65 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1894474198771995		[learning rate: 3.2383e-05]
	Learning Rate: 3.23829e-05
	LOSS [training: 0.1894474198771995 | validation: 0.28088833416491926]
	TIME [epoch: 9.67 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19059699498393348		[learning rate: 3.2265e-05]
	Learning Rate: 3.22654e-05
	LOSS [training: 0.19059699498393348 | validation: 0.2708922683184737]
	TIME [epoch: 9.66 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19323976544921115		[learning rate: 3.2148e-05]
	Learning Rate: 3.21483e-05
	LOSS [training: 0.19323976544921115 | validation: 0.28186969703354103]
	TIME [epoch: 9.66 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1940836678740963		[learning rate: 3.2032e-05]
	Learning Rate: 3.20316e-05
	LOSS [training: 0.1940836678740963 | validation: 0.2738616294142944]
	TIME [epoch: 9.66 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19453943831049514		[learning rate: 3.1915e-05]
	Learning Rate: 3.19154e-05
	LOSS [training: 0.19453943831049514 | validation: 0.27697646068170584]
	TIME [epoch: 9.68 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19301573880363762		[learning rate: 3.18e-05]
	Learning Rate: 3.17996e-05
	LOSS [training: 0.19301573880363762 | validation: 0.2739371895545204]
	TIME [epoch: 9.66 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19666732300519885		[learning rate: 3.1684e-05]
	Learning Rate: 3.16842e-05
	LOSS [training: 0.19666732300519885 | validation: 0.26840333438146163]
	TIME [epoch: 9.67 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19021593503600281		[learning rate: 3.1569e-05]
	Learning Rate: 3.15692e-05
	LOSS [training: 0.19021593503600281 | validation: 0.27716584798663574]
	TIME [epoch: 9.65 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19049220733488806		[learning rate: 3.1455e-05]
	Learning Rate: 3.14546e-05
	LOSS [training: 0.19049220733488806 | validation: 0.29539884433951785]
	TIME [epoch: 9.68 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19090458329968824		[learning rate: 3.134e-05]
	Learning Rate: 3.13405e-05
	LOSS [training: 0.19090458329968824 | validation: 0.27580295861791326]
	TIME [epoch: 9.66 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19423487159121455		[learning rate: 3.1227e-05]
	Learning Rate: 3.12267e-05
	LOSS [training: 0.19423487159121455 | validation: 0.2883870667898853]
	TIME [epoch: 9.66 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19167584829116543		[learning rate: 3.1113e-05]
	Learning Rate: 3.11134e-05
	LOSS [training: 0.19167584829116543 | validation: 0.28538465384128886]
	TIME [epoch: 9.66 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19393209194075278		[learning rate: 3.1e-05]
	Learning Rate: 3.10005e-05
	LOSS [training: 0.19393209194075278 | validation: 0.28692337529357337]
	TIME [epoch: 9.68 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18952310223026747		[learning rate: 3.0888e-05]
	Learning Rate: 3.0888e-05
	LOSS [training: 0.18952310223026747 | validation: 0.2948910348364056]
	TIME [epoch: 9.66 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18730110087194368		[learning rate: 3.0776e-05]
	Learning Rate: 3.07759e-05
	LOSS [training: 0.18730110087194368 | validation: 0.2935347619450314]
	TIME [epoch: 9.66 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20511746544058523		[learning rate: 3.0664e-05]
	Learning Rate: 3.06642e-05
	LOSS [training: 0.20511746544058523 | validation: 0.3057668642876865]
	TIME [epoch: 9.68 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19860990990679445		[learning rate: 3.0553e-05]
	Learning Rate: 3.05529e-05
	LOSS [training: 0.19860990990679445 | validation: 0.2866713626400895]
	TIME [epoch: 9.66 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19542522159257336		[learning rate: 3.0442e-05]
	Learning Rate: 3.0442e-05
	LOSS [training: 0.19542522159257336 | validation: 0.2847841966720187]
	TIME [epoch: 9.65 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19760486956400716		[learning rate: 3.0332e-05]
	Learning Rate: 3.03316e-05
	LOSS [training: 0.19760486956400716 | validation: 0.28804812628581233]
	TIME [epoch: 9.66 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20091324481258307		[learning rate: 3.0221e-05]
	Learning Rate: 3.02215e-05
	LOSS [training: 0.20091324481258307 | validation: 0.2781855095887469]
	TIME [epoch: 9.68 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2009760461509679		[learning rate: 3.0112e-05]
	Learning Rate: 3.01118e-05
	LOSS [training: 0.2009760461509679 | validation: 0.2618313278605606]
	TIME [epoch: 9.66 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19935986168166683		[learning rate: 3.0003e-05]
	Learning Rate: 3.00025e-05
	LOSS [training: 0.19935986168166683 | validation: 0.26038285524346055]
	TIME [epoch: 9.66 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1945311623668958		[learning rate: 2.9894e-05]
	Learning Rate: 2.98936e-05
	LOSS [training: 0.1945311623668958 | validation: 0.265951002637554]
	TIME [epoch: 9.66 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19467191709411827		[learning rate: 2.9785e-05]
	Learning Rate: 2.97852e-05
	LOSS [training: 0.19467191709411827 | validation: 0.2623881345262004]
	TIME [epoch: 9.68 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1982502246703493		[learning rate: 2.9677e-05]
	Learning Rate: 2.96771e-05
	LOSS [training: 0.1982502246703493 | validation: 0.26631606763601284]
	TIME [epoch: 9.66 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1949030539167247		[learning rate: 2.9569e-05]
	Learning Rate: 2.95694e-05
	LOSS [training: 0.1949030539167247 | validation: 0.28076635502980873]
	TIME [epoch: 9.66 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2035315095729439		[learning rate: 2.9462e-05]
	Learning Rate: 2.94621e-05
	LOSS [training: 0.2035315095729439 | validation: 0.2828521815710875]
	TIME [epoch: 9.67 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19810418594806145		[learning rate: 2.9355e-05]
	Learning Rate: 2.93551e-05
	LOSS [training: 0.19810418594806145 | validation: 0.29045724927836597]
	TIME [epoch: 9.67 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19115043948514002		[learning rate: 2.9249e-05]
	Learning Rate: 2.92486e-05
	LOSS [training: 0.19115043948514002 | validation: 0.26162881538093286]
	TIME [epoch: 9.66 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.187209708478216		[learning rate: 2.9142e-05]
	Learning Rate: 2.91425e-05
	LOSS [training: 0.187209708478216 | validation: 0.2658122384780684]
	TIME [epoch: 9.66 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20033003468103905		[learning rate: 2.9037e-05]
	Learning Rate: 2.90367e-05
	LOSS [training: 0.20033003468103905 | validation: 0.27225726292030905]
	TIME [epoch: 9.68 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19260010730173543		[learning rate: 2.8931e-05]
	Learning Rate: 2.89313e-05
	LOSS [training: 0.19260010730173543 | validation: 0.2594269083599424]
	TIME [epoch: 9.66 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19440923558376408		[learning rate: 2.8826e-05]
	Learning Rate: 2.88263e-05
	LOSS [training: 0.19440923558376408 | validation: 0.2716555239377063]
	TIME [epoch: 9.66 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19417399724971837		[learning rate: 2.8722e-05]
	Learning Rate: 2.87217e-05
	LOSS [training: 0.19417399724971837 | validation: 0.2642914244151667]
	TIME [epoch: 9.66 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19373465247422456		[learning rate: 2.8617e-05]
	Learning Rate: 2.86175e-05
	LOSS [training: 0.19373465247422456 | validation: 0.28096351320577545]
	TIME [epoch: 9.68 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18880804623163402		[learning rate: 2.8514e-05]
	Learning Rate: 2.85136e-05
	LOSS [training: 0.18880804623163402 | validation: 0.2530348509498154]
	TIME [epoch: 9.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_1712.pth
	Model improved!!!
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2001772529136067		[learning rate: 2.841e-05]
	Learning Rate: 2.84102e-05
	LOSS [training: 0.2001772529136067 | validation: 0.273555467560619]
	TIME [epoch: 9.66 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19834129886068788		[learning rate: 2.8307e-05]
	Learning Rate: 2.83071e-05
	LOSS [training: 0.19834129886068788 | validation: 0.2872525020064831]
	TIME [epoch: 9.66 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1965394950690364		[learning rate: 2.8204e-05]
	Learning Rate: 2.82043e-05
	LOSS [training: 0.1965394950690364 | validation: 0.29917488627479943]
	TIME [epoch: 9.68 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19826675406250593		[learning rate: 2.8102e-05]
	Learning Rate: 2.8102e-05
	LOSS [training: 0.19826675406250593 | validation: 0.28752711905819217]
	TIME [epoch: 9.65 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19249488859635813		[learning rate: 2.8e-05]
	Learning Rate: 2.8e-05
	LOSS [training: 0.19249488859635813 | validation: 0.268104255814872]
	TIME [epoch: 9.66 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18920798131496486		[learning rate: 2.7898e-05]
	Learning Rate: 2.78984e-05
	LOSS [training: 0.18920798131496486 | validation: 0.27718521534456303]
	TIME [epoch: 9.67 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19256576982112208		[learning rate: 2.7797e-05]
	Learning Rate: 2.77971e-05
	LOSS [training: 0.19256576982112208 | validation: 0.2702857730552678]
	TIME [epoch: 9.67 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19034654749303598		[learning rate: 2.7696e-05]
	Learning Rate: 2.76963e-05
	LOSS [training: 0.19034654749303598 | validation: 0.2688465340192919]
	TIME [epoch: 9.65 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1978749012330675		[learning rate: 2.7596e-05]
	Learning Rate: 2.75957e-05
	LOSS [training: 0.1978749012330675 | validation: 0.2761775784983551]
	TIME [epoch: 9.66 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18958097361514326		[learning rate: 2.7496e-05]
	Learning Rate: 2.74956e-05
	LOSS [training: 0.18958097361514326 | validation: 0.26885544742714684]
	TIME [epoch: 9.68 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1984648698800371		[learning rate: 2.7396e-05]
	Learning Rate: 2.73958e-05
	LOSS [training: 0.1984648698800371 | validation: 0.2654928906563876]
	TIME [epoch: 9.66 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19498701860581952		[learning rate: 2.7296e-05]
	Learning Rate: 2.72964e-05
	LOSS [training: 0.19498701860581952 | validation: 0.2689488066874653]
	TIME [epoch: 9.65 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1991970657258569		[learning rate: 2.7197e-05]
	Learning Rate: 2.71973e-05
	LOSS [training: 0.1991970657258569 | validation: 0.2625152353683624]
	TIME [epoch: 9.66 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19608055754477852		[learning rate: 2.7099e-05]
	Learning Rate: 2.70986e-05
	LOSS [training: 0.19608055754477852 | validation: 0.2719351454248665]
	TIME [epoch: 9.68 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18263636075869072		[learning rate: 2.7e-05]
	Learning Rate: 2.70003e-05
	LOSS [training: 0.18263636075869072 | validation: 0.2801860534721777]
	TIME [epoch: 9.66 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2000665735100088		[learning rate: 2.6902e-05]
	Learning Rate: 2.69023e-05
	LOSS [training: 0.2000665735100088 | validation: 0.2655853357035581]
	TIME [epoch: 9.66 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20052836363948345		[learning rate: 2.6805e-05]
	Learning Rate: 2.68047e-05
	LOSS [training: 0.20052836363948345 | validation: 0.2834080469620916]
	TIME [epoch: 9.67 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1951775822174143		[learning rate: 2.6707e-05]
	Learning Rate: 2.67074e-05
	LOSS [training: 0.1951775822174143 | validation: 0.2734119026607981]
	TIME [epoch: 9.67 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1937620712303792		[learning rate: 2.661e-05]
	Learning Rate: 2.66105e-05
	LOSS [training: 0.1937620712303792 | validation: 0.30256651730054446]
	TIME [epoch: 9.66 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20064808140112245		[learning rate: 2.6514e-05]
	Learning Rate: 2.65139e-05
	LOSS [training: 0.20064808140112245 | validation: 0.2937429183440361]
	TIME [epoch: 9.66 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1978600332297771		[learning rate: 2.6418e-05]
	Learning Rate: 2.64177e-05
	LOSS [training: 0.1978600332297771 | validation: 0.29333953854670686]
	TIME [epoch: 9.68 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1997168428789756		[learning rate: 2.6322e-05]
	Learning Rate: 2.63218e-05
	LOSS [training: 0.1997168428789756 | validation: 0.2897528656418168]
	TIME [epoch: 9.66 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20783229612080856		[learning rate: 2.6226e-05]
	Learning Rate: 2.62263e-05
	LOSS [training: 0.20783229612080856 | validation: 0.31465551999129404]
	TIME [epoch: 9.66 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20872071860528538		[learning rate: 2.6131e-05]
	Learning Rate: 2.61311e-05
	LOSS [training: 0.20872071860528538 | validation: 0.30426613252106854]
	TIME [epoch: 9.66 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2075729816158816		[learning rate: 2.6036e-05]
	Learning Rate: 2.60363e-05
	LOSS [training: 0.2075729816158816 | validation: 0.29882064515717943]
	TIME [epoch: 9.68 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2094305821568924		[learning rate: 2.5942e-05]
	Learning Rate: 2.59418e-05
	LOSS [training: 0.2094305821568924 | validation: 0.3086853822819337]
	TIME [epoch: 9.67 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19794700016945535		[learning rate: 2.5848e-05]
	Learning Rate: 2.58477e-05
	LOSS [training: 0.19794700016945535 | validation: 0.28385253476695943]
	TIME [epoch: 9.66 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19376298852260268		[learning rate: 2.5754e-05]
	Learning Rate: 2.57539e-05
	LOSS [training: 0.19376298852260268 | validation: 0.28230674495948627]
	TIME [epoch: 9.66 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1986561740039523		[learning rate: 2.566e-05]
	Learning Rate: 2.56604e-05
	LOSS [training: 0.1986561740039523 | validation: 0.2712094330111304]
	TIME [epoch: 9.68 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19350239177095307		[learning rate: 2.5567e-05]
	Learning Rate: 2.55673e-05
	LOSS [training: 0.19350239177095307 | validation: 0.2738388925117222]
	TIME [epoch: 9.66 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19612496112632202		[learning rate: 2.5474e-05]
	Learning Rate: 2.54745e-05
	LOSS [training: 0.19612496112632202 | validation: 0.26829901216351576]
	TIME [epoch: 9.67 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18869894922244146		[learning rate: 2.5382e-05]
	Learning Rate: 2.5382e-05
	LOSS [training: 0.18869894922244146 | validation: 0.27749138673520946]
	TIME [epoch: 9.66 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19995993642001245		[learning rate: 2.529e-05]
	Learning Rate: 2.52899e-05
	LOSS [training: 0.19995993642001245 | validation: 0.2683582199691124]
	TIME [epoch: 9.67 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1928500864752715		[learning rate: 2.5198e-05]
	Learning Rate: 2.51981e-05
	LOSS [training: 0.1928500864752715 | validation: 0.28046154260320494]
	TIME [epoch: 9.65 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2023949609452		[learning rate: 2.5107e-05]
	Learning Rate: 2.51067e-05
	LOSS [training: 0.2023949609452 | validation: 0.27321607142813165]
	TIME [epoch: 9.66 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19353975587610317		[learning rate: 2.5016e-05]
	Learning Rate: 2.50156e-05
	LOSS [training: 0.19353975587610317 | validation: 0.26858200951711714]
	TIME [epoch: 9.67 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20309315694106989		[learning rate: 2.4925e-05]
	Learning Rate: 2.49248e-05
	LOSS [training: 0.20309315694106989 | validation: 0.27701695321078185]
	TIME [epoch: 9.66 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20169792935514116		[learning rate: 2.4834e-05]
	Learning Rate: 2.48343e-05
	LOSS [training: 0.20169792935514116 | validation: 0.277383282609509]
	TIME [epoch: 9.66 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19545710499978375		[learning rate: 2.4744e-05]
	Learning Rate: 2.47442e-05
	LOSS [training: 0.19545710499978375 | validation: 0.29694641814346484]
	TIME [epoch: 9.66 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20562757370406531		[learning rate: 2.4654e-05]
	Learning Rate: 2.46544e-05
	LOSS [training: 0.20562757370406531 | validation: 0.278672781552338]
	TIME [epoch: 9.68 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20025824033631906		[learning rate: 2.4565e-05]
	Learning Rate: 2.45649e-05
	LOSS [training: 0.20025824033631906 | validation: 0.2798219376489518]
	TIME [epoch: 9.66 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19636308920855847		[learning rate: 2.4476e-05]
	Learning Rate: 2.44758e-05
	LOSS [training: 0.19636308920855847 | validation: 0.2696867679835016]
	TIME [epoch: 9.65 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19859462876554695		[learning rate: 2.4387e-05]
	Learning Rate: 2.4387e-05
	LOSS [training: 0.19859462876554695 | validation: 0.28039218973878227]
	TIME [epoch: 9.66 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19241026621474006		[learning rate: 2.4298e-05]
	Learning Rate: 2.42985e-05
	LOSS [training: 0.19241026621474006 | validation: 0.29032935207979105]
	TIME [epoch: 9.68 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1903720290756061		[learning rate: 2.421e-05]
	Learning Rate: 2.42103e-05
	LOSS [training: 0.1903720290756061 | validation: 0.27841249889271014]
	TIME [epoch: 9.66 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1918261220536867		[learning rate: 2.4122e-05]
	Learning Rate: 2.41224e-05
	LOSS [training: 0.1918261220536867 | validation: 0.2728901597557011]
	TIME [epoch: 9.65 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19641824438382205		[learning rate: 2.4035e-05]
	Learning Rate: 2.40349e-05
	LOSS [training: 0.19641824438382205 | validation: 0.2909709182598965]
	TIME [epoch: 9.66 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19938036286591865		[learning rate: 2.3948e-05]
	Learning Rate: 2.39477e-05
	LOSS [training: 0.19938036286591865 | validation: 0.2881525634639755]
	TIME [epoch: 9.67 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19099873779151427		[learning rate: 2.3861e-05]
	Learning Rate: 2.38608e-05
	LOSS [training: 0.19099873779151427 | validation: 0.272417834857819]
	TIME [epoch: 9.67 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19304265306219476		[learning rate: 2.3774e-05]
	Learning Rate: 2.37742e-05
	LOSS [training: 0.19304265306219476 | validation: 0.29590908571352664]
	TIME [epoch: 9.65 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18603967142570105		[learning rate: 2.3688e-05]
	Learning Rate: 2.36879e-05
	LOSS [training: 0.18603967142570105 | validation: 0.2585483175726553]
	TIME [epoch: 9.68 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19272605598499298		[learning rate: 2.3602e-05]
	Learning Rate: 2.36019e-05
	LOSS [training: 0.19272605598499298 | validation: 0.28066436913073645]
	TIME [epoch: 9.65 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19780399641536453		[learning rate: 2.3516e-05]
	Learning Rate: 2.35163e-05
	LOSS [training: 0.19780399641536453 | validation: 0.27902501080194037]
	TIME [epoch: 9.66 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20291924423474747		[learning rate: 2.3431e-05]
	Learning Rate: 2.34309e-05
	LOSS [training: 0.20291924423474747 | validation: 0.2651447176926241]
	TIME [epoch: 9.66 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19512825389509822		[learning rate: 2.3346e-05]
	Learning Rate: 2.33459e-05
	LOSS [training: 0.19512825389509822 | validation: 0.28403125042817484]
	TIME [epoch: 9.69 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19850168049250683		[learning rate: 2.3261e-05]
	Learning Rate: 2.32612e-05
	LOSS [training: 0.19850168049250683 | validation: 0.2712520572858414]
	TIME [epoch: 9.65 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19448730450268692		[learning rate: 2.3177e-05]
	Learning Rate: 2.31768e-05
	LOSS [training: 0.19448730450268692 | validation: 0.2926409758703152]
	TIME [epoch: 9.66 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19770309568165997		[learning rate: 2.3093e-05]
	Learning Rate: 2.30926e-05
	LOSS [training: 0.19770309568165997 | validation: 0.2946774998604368]
	TIME [epoch: 9.65 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1951727891278126		[learning rate: 2.3009e-05]
	Learning Rate: 2.30088e-05
	LOSS [training: 0.1951727891278126 | validation: 0.2680290507459576]
	TIME [epoch: 9.68 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19379196603112478		[learning rate: 2.2925e-05]
	Learning Rate: 2.29253e-05
	LOSS [training: 0.19379196603112478 | validation: 0.25926020430987956]
	TIME [epoch: 9.66 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19535984291530192		[learning rate: 2.2842e-05]
	Learning Rate: 2.28421e-05
	LOSS [training: 0.19535984291530192 | validation: 0.27767062085470234]
	TIME [epoch: 9.67 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19076094931107046		[learning rate: 2.2759e-05]
	Learning Rate: 2.27592e-05
	LOSS [training: 0.19076094931107046 | validation: 0.2853013572181108]
	TIME [epoch: 9.67 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19522186004191683		[learning rate: 2.2677e-05]
	Learning Rate: 2.26767e-05
	LOSS [training: 0.19522186004191683 | validation: 0.2503089025639047]
	TIME [epoch: 9.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_1775.pth
	Model improved!!!
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19611233005405126		[learning rate: 2.2594e-05]
	Learning Rate: 2.25944e-05
	LOSS [training: 0.19611233005405126 | validation: 0.2606793888071817]
	TIME [epoch: 9.65 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19359853419268638		[learning rate: 2.2512e-05]
	Learning Rate: 2.25124e-05
	LOSS [training: 0.19359853419268638 | validation: 0.28205614750940583]
	TIME [epoch: 9.66 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19362443604354676		[learning rate: 2.2431e-05]
	Learning Rate: 2.24307e-05
	LOSS [training: 0.19362443604354676 | validation: 0.28421642704224925]
	TIME [epoch: 9.67 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19302923428198998		[learning rate: 2.2349e-05]
	Learning Rate: 2.23493e-05
	LOSS [training: 0.19302923428198998 | validation: 0.28944711754424096]
	TIME [epoch: 9.66 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19688697701459318		[learning rate: 2.2268e-05]
	Learning Rate: 2.22682e-05
	LOSS [training: 0.19688697701459318 | validation: 0.26966647243596165]
	TIME [epoch: 9.65 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1836409983884118		[learning rate: 2.2187e-05]
	Learning Rate: 2.21873e-05
	LOSS [training: 0.1836409983884118 | validation: 0.2819485049955955]
	TIME [epoch: 9.66 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18879100028325047		[learning rate: 2.2107e-05]
	Learning Rate: 2.21068e-05
	LOSS [training: 0.18879100028325047 | validation: 0.266779444690323]
	TIME [epoch: 9.67 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1864101053358372		[learning rate: 2.2027e-05]
	Learning Rate: 2.20266e-05
	LOSS [training: 0.1864101053358372 | validation: 0.2602700748642542]
	TIME [epoch: 9.66 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19056248348586927		[learning rate: 2.1947e-05]
	Learning Rate: 2.19467e-05
	LOSS [training: 0.19056248348586927 | validation: 0.2715501619123642]
	TIME [epoch: 9.65 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19247227650787008		[learning rate: 2.1867e-05]
	Learning Rate: 2.1867e-05
	LOSS [training: 0.19247227650787008 | validation: 0.28912678139252146]
	TIME [epoch: 9.66 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19492901056572814		[learning rate: 2.1788e-05]
	Learning Rate: 2.17877e-05
	LOSS [training: 0.19492901056572814 | validation: 0.2912239618898821]
	TIME [epoch: 9.67 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19219664513020604		[learning rate: 2.1709e-05]
	Learning Rate: 2.17086e-05
	LOSS [training: 0.19219664513020604 | validation: 0.2763262691241551]
	TIME [epoch: 9.66 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19105203821092123		[learning rate: 2.163e-05]
	Learning Rate: 2.16298e-05
	LOSS [training: 0.19105203821092123 | validation: 0.2716436689395575]
	TIME [epoch: 9.66 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1966872206785647		[learning rate: 2.1551e-05]
	Learning Rate: 2.15513e-05
	LOSS [training: 0.1966872206785647 | validation: 0.2562540716073563]
	TIME [epoch: 9.68 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1917371714756505		[learning rate: 2.1473e-05]
	Learning Rate: 2.14731e-05
	LOSS [training: 0.1917371714756505 | validation: 0.28759378706870403]
	TIME [epoch: 9.66 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19419586854808352		[learning rate: 2.1395e-05]
	Learning Rate: 2.13952e-05
	LOSS [training: 0.19419586854808352 | validation: 0.276553425590303]
	TIME [epoch: 9.66 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19192830409729353		[learning rate: 2.1318e-05]
	Learning Rate: 2.13175e-05
	LOSS [training: 0.19192830409729353 | validation: 0.271623145895283]
	TIME [epoch: 9.65 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19693328533868132		[learning rate: 2.124e-05]
	Learning Rate: 2.12402e-05
	LOSS [training: 0.19693328533868132 | validation: 0.2534770795128804]
	TIME [epoch: 9.67 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19209719206776998		[learning rate: 2.1163e-05]
	Learning Rate: 2.11631e-05
	LOSS [training: 0.19209719206776998 | validation: 0.26539966610127147]
	TIME [epoch: 9.65 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19443810896705657		[learning rate: 2.1086e-05]
	Learning Rate: 2.10863e-05
	LOSS [training: 0.19443810896705657 | validation: 0.27789474512382456]
	TIME [epoch: 9.66 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19175694262823245		[learning rate: 2.101e-05]
	Learning Rate: 2.10098e-05
	LOSS [training: 0.19175694262823245 | validation: 0.2855271997043181]
	TIME [epoch: 9.65 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19450656447801756		[learning rate: 2.0934e-05]
	Learning Rate: 2.09335e-05
	LOSS [training: 0.19450656447801756 | validation: 0.2690049544425767]
	TIME [epoch: 9.68 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19553068248080782		[learning rate: 2.0858e-05]
	Learning Rate: 2.08575e-05
	LOSS [training: 0.19553068248080782 | validation: 0.2681376349171696]
	TIME [epoch: 9.65 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20153717961240494		[learning rate: 2.0782e-05]
	Learning Rate: 2.07819e-05
	LOSS [training: 0.20153717961240494 | validation: 0.2715630346590483]
	TIME [epoch: 9.66 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19678637714832586		[learning rate: 2.0706e-05]
	Learning Rate: 2.07064e-05
	LOSS [training: 0.19678637714832586 | validation: 0.2772084274416767]
	TIME [epoch: 9.65 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19863750341821737		[learning rate: 2.0631e-05]
	Learning Rate: 2.06313e-05
	LOSS [training: 0.19863750341821737 | validation: 0.26623131957021345]
	TIME [epoch: 9.67 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19554383535635447		[learning rate: 2.0556e-05]
	Learning Rate: 2.05564e-05
	LOSS [training: 0.19554383535635447 | validation: 0.27884051647544816]
	TIME [epoch: 9.65 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19950372226177304		[learning rate: 2.0482e-05]
	Learning Rate: 2.04818e-05
	LOSS [training: 0.19950372226177304 | validation: 0.24226823430272937]
	TIME [epoch: 9.66 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r3_20240220_000152/states/model_tr_study206_1803.pth
	Model improved!!!
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20089330551965504		[learning rate: 2.0407e-05]
	Learning Rate: 2.04075e-05
	LOSS [training: 0.20089330551965504 | validation: 0.2743147029843868]
	TIME [epoch: 9.67 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19991702360556732		[learning rate: 2.0333e-05]
	Learning Rate: 2.03334e-05
	LOSS [training: 0.19991702360556732 | validation: 0.28468102904184006]
	TIME [epoch: 9.65 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1938783367015711		[learning rate: 2.026e-05]
	Learning Rate: 2.02596e-05
	LOSS [training: 0.1938783367015711 | validation: 0.2558001201280374]
	TIME [epoch: 9.65 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1966472574134721		[learning rate: 2.0186e-05]
	Learning Rate: 2.01861e-05
	LOSS [training: 0.1966472574134721 | validation: 0.2653835946266378]
	TIME [epoch: 9.64 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19637243634538853		[learning rate: 2.0113e-05]
	Learning Rate: 2.01129e-05
	LOSS [training: 0.19637243634538853 | validation: 0.27290826560584225]
	TIME [epoch: 9.66 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1913716369324414		[learning rate: 2.004e-05]
	Learning Rate: 2.00399e-05
	LOSS [training: 0.1913716369324414 | validation: 0.26532678562264167]
	TIME [epoch: 9.65 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19095213017180404		[learning rate: 1.9967e-05]
	Learning Rate: 1.99671e-05
	LOSS [training: 0.19095213017180404 | validation: 0.2766301490930077]
	TIME [epoch: 9.65 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19929753858150387		[learning rate: 1.9895e-05]
	Learning Rate: 1.98947e-05
	LOSS [training: 0.19929753858150387 | validation: 0.27638537483949394]
	TIME [epoch: 9.65 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19426021126328713		[learning rate: 1.9822e-05]
	Learning Rate: 1.98225e-05
	LOSS [training: 0.19426021126328713 | validation: 0.27475170254905834]
	TIME [epoch: 9.66 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19711932751294953		[learning rate: 1.9751e-05]
	Learning Rate: 1.97505e-05
	LOSS [training: 0.19711932751294953 | validation: 0.26188550177519604]
	TIME [epoch: 9.65 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19286224690030712		[learning rate: 1.9679e-05]
	Learning Rate: 1.96789e-05
	LOSS [training: 0.19286224690030712 | validation: 0.26457176079695965]
	TIME [epoch: 9.64 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18967236527443915		[learning rate: 1.9607e-05]
	Learning Rate: 1.96074e-05
	LOSS [training: 0.18967236527443915 | validation: 0.2801131445217004]
	TIME [epoch: 9.66 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20424140081769354		[learning rate: 1.9536e-05]
	Learning Rate: 1.95363e-05
	LOSS [training: 0.20424140081769354 | validation: 0.2792182103546689]
	TIME [epoch: 9.65 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19407185612670128		[learning rate: 1.9465e-05]
	Learning Rate: 1.94654e-05
	LOSS [training: 0.19407185612670128 | validation: 0.2650116709256386]
	TIME [epoch: 9.65 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1957126675496234		[learning rate: 1.9395e-05]
	Learning Rate: 1.93948e-05
	LOSS [training: 0.1957126675496234 | validation: 0.27246392198085406]
	TIME [epoch: 9.65 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2081737210888238		[learning rate: 1.9324e-05]
	Learning Rate: 1.93244e-05
	LOSS [training: 0.2081737210888238 | validation: 0.2939696326970359]
	TIME [epoch: 9.66 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.202469620001927		[learning rate: 1.9254e-05]
	Learning Rate: 1.92542e-05
	LOSS [training: 0.202469620001927 | validation: 0.28378582931792556]
	TIME [epoch: 9.65 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19837804735880066		[learning rate: 1.9184e-05]
	Learning Rate: 1.91844e-05
	LOSS [training: 0.19837804735880066 | validation: 0.26326750589051756]
	TIME [epoch: 9.65 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19799200362416564		[learning rate: 1.9115e-05]
	Learning Rate: 1.91147e-05
	LOSS [training: 0.19799200362416564 | validation: 0.2630600321662407]
	TIME [epoch: 9.65 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1981471273285445		[learning rate: 1.9045e-05]
	Learning Rate: 1.90454e-05
	LOSS [training: 0.1981471273285445 | validation: 0.2583506625630031]
	TIME [epoch: 9.67 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19435924227638765		[learning rate: 1.8976e-05]
	Learning Rate: 1.89763e-05
	LOSS [training: 0.19435924227638765 | validation: 0.2652657822597723]
	TIME [epoch: 9.65 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20047545876916825		[learning rate: 1.8907e-05]
	Learning Rate: 1.89074e-05
	LOSS [training: 0.20047545876916825 | validation: 0.27650689619033375]
	TIME [epoch: 9.65 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19759505127602087		[learning rate: 1.8839e-05]
	Learning Rate: 1.88388e-05
	LOSS [training: 0.19759505127602087 | validation: 0.2775226811214355]
	TIME [epoch: 9.65 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19749406518691612		[learning rate: 1.877e-05]
	Learning Rate: 1.87704e-05
	LOSS [training: 0.19749406518691612 | validation: 0.27671428851459917]
	TIME [epoch: 9.66 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19956659906924062		[learning rate: 1.8702e-05]
	Learning Rate: 1.87023e-05
	LOSS [training: 0.19956659906924062 | validation: 0.2904783203357803]
	TIME [epoch: 9.64 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19719568160248085		[learning rate: 1.8634e-05]
	Learning Rate: 1.86344e-05
	LOSS [training: 0.19719568160248085 | validation: 0.28840229289527275]
	TIME [epoch: 9.65 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19903101487883237		[learning rate: 1.8567e-05]
	Learning Rate: 1.85668e-05
	LOSS [training: 0.19903101487883237 | validation: 0.25826517589140024]
	TIME [epoch: 9.66 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2042090179519877		[learning rate: 1.8499e-05]
	Learning Rate: 1.84994e-05
	LOSS [training: 0.2042090179519877 | validation: 0.2784779061968565]
	TIME [epoch: 9.65 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19605932124723183		[learning rate: 1.8432e-05]
	Learning Rate: 1.84323e-05
	LOSS [training: 0.19605932124723183 | validation: 0.27186206256719764]
	TIME [epoch: 9.65 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19900538774745274		[learning rate: 1.8365e-05]
	Learning Rate: 1.83654e-05
	LOSS [training: 0.19900538774745274 | validation: 0.27156865251109824]
	TIME [epoch: 9.65 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20197874734170576		[learning rate: 1.8299e-05]
	Learning Rate: 1.82987e-05
	LOSS [training: 0.20197874734170576 | validation: 0.2567172491252756]
	TIME [epoch: 9.67 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2102687793602398		[learning rate: 1.8232e-05]
	Learning Rate: 1.82323e-05
	LOSS [training: 0.2102687793602398 | validation: 0.26454439444928085]
	TIME [epoch: 9.64 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2108563400650985		[learning rate: 1.8166e-05]
	Learning Rate: 1.81662e-05
	LOSS [training: 0.2108563400650985 | validation: 0.2564318360692158]
	TIME [epoch: 9.64 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19830526748634333		[learning rate: 1.81e-05]
	Learning Rate: 1.81002e-05
	LOSS [training: 0.19830526748634333 | validation: 0.27931010606380896]
	TIME [epoch: 9.65 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2011790046427456		[learning rate: 1.8035e-05]
	Learning Rate: 1.80346e-05
	LOSS [training: 0.2011790046427456 | validation: 0.28541429391663087]
	TIME [epoch: 9.67 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18940566837334566		[learning rate: 1.7969e-05]
	Learning Rate: 1.79691e-05
	LOSS [training: 0.18940566837334566 | validation: 0.2794439925668086]
	TIME [epoch: 9.65 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19521097423059675		[learning rate: 1.7904e-05]
	Learning Rate: 1.79039e-05
	LOSS [training: 0.19521097423059675 | validation: 0.2872475316196389]
	TIME [epoch: 9.65 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19929137848083825		[learning rate: 1.7839e-05]
	Learning Rate: 1.78389e-05
	LOSS [training: 0.19929137848083825 | validation: 0.2801255723984772]
	TIME [epoch: 9.66 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20054635219602002		[learning rate: 1.7774e-05]
	Learning Rate: 1.77742e-05
	LOSS [training: 0.20054635219602002 | validation: 0.27971863379294504]
	TIME [epoch: 9.66 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19582775792863336		[learning rate: 1.771e-05]
	Learning Rate: 1.77097e-05
	LOSS [training: 0.19582775792863336 | validation: 0.2736547377196136]
	TIME [epoch: 9.65 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1934987157159028		[learning rate: 1.7645e-05]
	Learning Rate: 1.76454e-05
	LOSS [training: 0.1934987157159028 | validation: 0.2712431234787328]
	TIME [epoch: 9.65 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1964982048937241		[learning rate: 1.7581e-05]
	Learning Rate: 1.75814e-05
	LOSS [training: 0.1964982048937241 | validation: 0.2778244750770775]
	TIME [epoch: 9.67 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19914507660585953		[learning rate: 1.7518e-05]
	Learning Rate: 1.75176e-05
	LOSS [training: 0.19914507660585953 | validation: 0.2651459216513633]
	TIME [epoch: 9.65 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1992287160455768		[learning rate: 1.7454e-05]
	Learning Rate: 1.7454e-05
	LOSS [training: 0.1992287160455768 | validation: 0.26839521420879997]
	TIME [epoch: 9.65 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19030163733147476		[learning rate: 1.7391e-05]
	Learning Rate: 1.73907e-05
	LOSS [training: 0.19030163733147476 | validation: 0.2638970580519653]
	TIME [epoch: 9.64 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19451802189048292		[learning rate: 1.7328e-05]
	Learning Rate: 1.73275e-05
	LOSS [training: 0.19451802189048292 | validation: 0.27030829167618314]
	TIME [epoch: 9.67 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19492547826250683		[learning rate: 1.7265e-05]
	Learning Rate: 1.72647e-05
	LOSS [training: 0.19492547826250683 | validation: 0.2737735150804112]
	TIME [epoch: 9.65 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19547101522829763		[learning rate: 1.7202e-05]
	Learning Rate: 1.7202e-05
	LOSS [training: 0.19547101522829763 | validation: 0.26506368065683616]
	TIME [epoch: 9.64 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19930288067971894		[learning rate: 1.714e-05]
	Learning Rate: 1.71396e-05
	LOSS [training: 0.19930288067971894 | validation: 0.2577041691681879]
	TIME [epoch: 9.65 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18714455765929477		[learning rate: 1.7077e-05]
	Learning Rate: 1.70774e-05
	LOSS [training: 0.18714455765929477 | validation: 0.2565132477438039]
	TIME [epoch: 9.67 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19115473589680776		[learning rate: 1.7015e-05]
	Learning Rate: 1.70154e-05
	LOSS [training: 0.19115473589680776 | validation: 0.27824768608756256]
	TIME [epoch: 9.64 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19248733393658943		[learning rate: 1.6954e-05]
	Learning Rate: 1.69537e-05
	LOSS [training: 0.19248733393658943 | validation: 0.26207845585233336]
	TIME [epoch: 9.65 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19022640413288155		[learning rate: 1.6892e-05]
	Learning Rate: 1.68921e-05
	LOSS [training: 0.19022640413288155 | validation: 0.26193407483594106]
	TIME [epoch: 9.65 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18838040064947076		[learning rate: 1.6831e-05]
	Learning Rate: 1.68308e-05
	LOSS [training: 0.18838040064947076 | validation: 0.2732716350154131]
	TIME [epoch: 9.66 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.198503125740434		[learning rate: 1.677e-05]
	Learning Rate: 1.67697e-05
	LOSS [training: 0.198503125740434 | validation: 0.2861637545331952]
	TIME [epoch: 9.65 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19350075166601305		[learning rate: 1.6709e-05]
	Learning Rate: 1.67089e-05
	LOSS [training: 0.19350075166601305 | validation: 0.26958516469388494]
	TIME [epoch: 9.65 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20046005026534391		[learning rate: 1.6648e-05]
	Learning Rate: 1.66482e-05
	LOSS [training: 0.20046005026534391 | validation: 0.28131405141115773]
	TIME [epoch: 9.66 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19861261184839166		[learning rate: 1.6588e-05]
	Learning Rate: 1.65878e-05
	LOSS [training: 0.19861261184839166 | validation: 0.27697290550226306]
	TIME [epoch: 9.65 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19144755023327784		[learning rate: 1.6528e-05]
	Learning Rate: 1.65276e-05
	LOSS [training: 0.19144755023327784 | validation: 0.27487359319776006]
	TIME [epoch: 9.65 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19430085141580405		[learning rate: 1.6468e-05]
	Learning Rate: 1.64677e-05
	LOSS [training: 0.19430085141580405 | validation: 0.2744569786861939]
	TIME [epoch: 9.65 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19902755196280716		[learning rate: 1.6408e-05]
	Learning Rate: 1.64079e-05
	LOSS [training: 0.19902755196280716 | validation: 0.2692599734240736]
	TIME [epoch: 9.67 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19948770721330097		[learning rate: 1.6348e-05]
	Learning Rate: 1.63483e-05
	LOSS [training: 0.19948770721330097 | validation: 0.26624114905656415]
	TIME [epoch: 9.65 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19216148251826382		[learning rate: 1.6289e-05]
	Learning Rate: 1.6289e-05
	LOSS [training: 0.19216148251826382 | validation: 0.2763275489100648]
	TIME [epoch: 9.65 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20172509117821624		[learning rate: 1.623e-05]
	Learning Rate: 1.62299e-05
	LOSS [training: 0.20172509117821624 | validation: 0.2748628556856934]
	TIME [epoch: 9.65 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1921551162075747		[learning rate: 1.6171e-05]
	Learning Rate: 1.6171e-05
	LOSS [training: 0.1921551162075747 | validation: 0.2620665783064985]
	TIME [epoch: 9.67 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19582527037538772		[learning rate: 1.6112e-05]
	Learning Rate: 1.61123e-05
	LOSS [training: 0.19582527037538772 | validation: 0.2799405734600131]
	TIME [epoch: 9.65 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1983788808416884		[learning rate: 1.6054e-05]
	Learning Rate: 1.60538e-05
	LOSS [training: 0.1983788808416884 | validation: 0.27411527249005496]
	TIME [epoch: 9.64 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19823141711343314		[learning rate: 1.5996e-05]
	Learning Rate: 1.59956e-05
	LOSS [training: 0.19823141711343314 | validation: 0.26643262019607195]
	TIME [epoch: 9.66 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1995805173079151		[learning rate: 1.5938e-05]
	Learning Rate: 1.59375e-05
	LOSS [training: 0.1995805173079151 | validation: 0.2709531437577819]
	TIME [epoch: 9.65 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19555075420630497		[learning rate: 1.588e-05]
	Learning Rate: 1.58797e-05
	LOSS [training: 0.19555075420630497 | validation: 0.2541402014541741]
	TIME [epoch: 9.65 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19826530308870266		[learning rate: 1.5822e-05]
	Learning Rate: 1.58221e-05
	LOSS [training: 0.19826530308870266 | validation: 0.2648891171387324]
	TIME [epoch: 9.65 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1936982467423499		[learning rate: 1.5765e-05]
	Learning Rate: 1.57647e-05
	LOSS [training: 0.1936982467423499 | validation: 0.2610303477865506]
	TIME [epoch: 9.66 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1955353877431771		[learning rate: 1.5707e-05]
	Learning Rate: 1.57074e-05
	LOSS [training: 0.1955353877431771 | validation: 0.26651061563163775]
	TIME [epoch: 9.65 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1896763652171743		[learning rate: 1.565e-05]
	Learning Rate: 1.56504e-05
	LOSS [training: 0.1896763652171743 | validation: 0.2715234071489602]
	TIME [epoch: 9.65 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1924656195823115		[learning rate: 1.5594e-05]
	Learning Rate: 1.55936e-05
	LOSS [training: 0.1924656195823115 | validation: 0.2773110672022364]
	TIME [epoch: 9.64 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18831915882768643		[learning rate: 1.5537e-05]
	Learning Rate: 1.5537e-05
	LOSS [training: 0.18831915882768643 | validation: 0.2712470085745147]
	TIME [epoch: 9.67 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19332059114997088		[learning rate: 1.5481e-05]
	Learning Rate: 1.54807e-05
	LOSS [training: 0.19332059114997088 | validation: 0.2734057746034476]
	TIME [epoch: 9.65 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19403512382869165		[learning rate: 1.5424e-05]
	Learning Rate: 1.54245e-05
	LOSS [training: 0.19403512382869165 | validation: 0.29360552543144836]
	TIME [epoch: 9.65 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1970565039467314		[learning rate: 1.5369e-05]
	Learning Rate: 1.53685e-05
	LOSS [training: 0.1970565039467314 | validation: 0.26300078207824124]
	TIME [epoch: 9.66 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19574095401931657		[learning rate: 1.5313e-05]
	Learning Rate: 1.53127e-05
	LOSS [training: 0.19574095401931657 | validation: 0.28320816120514253]
	TIME [epoch: 9.67 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1987307055090162		[learning rate: 1.5257e-05]
	Learning Rate: 1.52572e-05
	LOSS [training: 0.1987307055090162 | validation: 0.26385910939213797]
	TIME [epoch: 9.64 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.190044074433727		[learning rate: 1.5202e-05]
	Learning Rate: 1.52018e-05
	LOSS [training: 0.190044074433727 | validation: 0.26254476377885605]
	TIME [epoch: 9.65 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19344211872749437		[learning rate: 1.5147e-05]
	Learning Rate: 1.51466e-05
	LOSS [training: 0.19344211872749437 | validation: 0.26380624927227564]
	TIME [epoch: 9.66 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19385508454866057		[learning rate: 1.5092e-05]
	Learning Rate: 1.50917e-05
	LOSS [training: 0.19385508454866057 | validation: 0.2660286080377329]
	TIME [epoch: 9.66 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19924361422709408		[learning rate: 1.5037e-05]
	Learning Rate: 1.50369e-05
	LOSS [training: 0.19924361422709408 | validation: 0.2541294053226057]
	TIME [epoch: 9.65 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19042602211898974		[learning rate: 1.4982e-05]
	Learning Rate: 1.49823e-05
	LOSS [training: 0.19042602211898974 | validation: 0.2557137815603433]
	TIME [epoch: 9.65 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1922367448105744		[learning rate: 1.4928e-05]
	Learning Rate: 1.49279e-05
	LOSS [training: 0.1922367448105744 | validation: 0.26533481308101475]
	TIME [epoch: 9.67 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19279753415210843		[learning rate: 1.4874e-05]
	Learning Rate: 1.48738e-05
	LOSS [training: 0.19279753415210843 | validation: 0.26646404034201626]
	TIME [epoch: 9.65 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1976444511457138		[learning rate: 1.482e-05]
	Learning Rate: 1.48198e-05
	LOSS [training: 0.1976444511457138 | validation: 0.26059301872094554]
	TIME [epoch: 9.65 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18870591793036992		[learning rate: 1.4766e-05]
	Learning Rate: 1.4766e-05
	LOSS [training: 0.18870591793036992 | validation: 0.2543717132186788]
	TIME [epoch: 9.65 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19204651221498767		[learning rate: 1.4712e-05]
	Learning Rate: 1.47124e-05
	LOSS [training: 0.19204651221498767 | validation: 0.27199554561084444]
	TIME [epoch: 9.67 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19504339788400937		[learning rate: 1.4659e-05]
	Learning Rate: 1.4659e-05
	LOSS [training: 0.19504339788400937 | validation: 0.25701992833143084]
	TIME [epoch: 9.64 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1991988511955703		[learning rate: 1.4606e-05]
	Learning Rate: 1.46058e-05
	LOSS [training: 0.1991988511955703 | validation: 0.26921948743773333]
	TIME [epoch: 9.65 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1888888806035498		[learning rate: 1.4553e-05]
	Learning Rate: 1.45528e-05
	LOSS [training: 0.1888888806035498 | validation: 0.26618925526549836]
	TIME [epoch: 9.66 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1907198207513598		[learning rate: 1.45e-05]
	Learning Rate: 1.45e-05
	LOSS [training: 0.1907198207513598 | validation: 0.2667578423182879]
	TIME [epoch: 9.67 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19102066591963018		[learning rate: 1.4447e-05]
	Learning Rate: 1.44474e-05
	LOSS [training: 0.19102066591963018 | validation: 0.27333921669027694]
	TIME [epoch: 9.65 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1892749985673646		[learning rate: 1.4395e-05]
	Learning Rate: 1.4395e-05
	LOSS [training: 0.1892749985673646 | validation: 0.26570125344481843]
	TIME [epoch: 9.66 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18640781322614886		[learning rate: 1.4343e-05]
	Learning Rate: 1.43427e-05
	LOSS [training: 0.18640781322614886 | validation: 0.2615665389221219]
	TIME [epoch: 9.67 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19217343380099594		[learning rate: 1.4291e-05]
	Learning Rate: 1.42907e-05
	LOSS [training: 0.19217343380099594 | validation: 0.2669113307929984]
	TIME [epoch: 9.65 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19116244780788122		[learning rate: 1.4239e-05]
	Learning Rate: 1.42388e-05
	LOSS [training: 0.19116244780788122 | validation: 0.25823329126307715]
	TIME [epoch: 9.65 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20242072579180545		[learning rate: 1.4187e-05]
	Learning Rate: 1.41871e-05
	LOSS [training: 0.20242072579180545 | validation: 0.26096321986030274]
	TIME [epoch: 9.65 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19285622160887006		[learning rate: 1.4136e-05]
	Learning Rate: 1.41357e-05
	LOSS [training: 0.19285622160887006 | validation: 0.2733548086289357]
	TIME [epoch: 9.67 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1992296229323936		[learning rate: 1.4084e-05]
	Learning Rate: 1.40844e-05
	LOSS [training: 0.1992296229323936 | validation: 0.2668937288620286]
	TIME [epoch: 9.66 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20012283364914568		[learning rate: 1.4033e-05]
	Learning Rate: 1.40332e-05
	LOSS [training: 0.20012283364914568 | validation: 0.26442815436597633]
	TIME [epoch: 9.65 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20163836704542396		[learning rate: 1.3982e-05]
	Learning Rate: 1.39823e-05
	LOSS [training: 0.20163836704542396 | validation: 0.2517658386849216]
	TIME [epoch: 9.65 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1937636247633081		[learning rate: 1.3932e-05]
	Learning Rate: 1.39316e-05
	LOSS [training: 0.1937636247633081 | validation: 0.26733725928235547]
	TIME [epoch: 9.67 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18666094119894514		[learning rate: 1.3881e-05]
	Learning Rate: 1.3881e-05
	LOSS [training: 0.18666094119894514 | validation: 0.25899580245067255]
	TIME [epoch: 9.65 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1944354817262472		[learning rate: 1.3831e-05]
	Learning Rate: 1.38306e-05
	LOSS [training: 0.1944354817262472 | validation: 0.26807263622397925]
	TIME [epoch: 9.65 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1935506985143561		[learning rate: 1.378e-05]
	Learning Rate: 1.37804e-05
	LOSS [training: 0.1935506985143561 | validation: 0.2650156681277988]
	TIME [epoch: 9.66 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18550538375186848		[learning rate: 1.373e-05]
	Learning Rate: 1.37304e-05
	LOSS [training: 0.18550538375186848 | validation: 0.26164974895373744]
	TIME [epoch: 9.67 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19551946749636712		[learning rate: 1.3681e-05]
	Learning Rate: 1.36806e-05
	LOSS [training: 0.19551946749636712 | validation: 0.26543543130195596]
	TIME [epoch: 9.66 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2015383422659661		[learning rate: 1.3631e-05]
	Learning Rate: 1.3631e-05
	LOSS [training: 0.2015383422659661 | validation: 0.24608306589047743]
	TIME [epoch: 9.64 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19595279817259986		[learning rate: 1.3581e-05]
	Learning Rate: 1.35815e-05
	LOSS [training: 0.19595279817259986 | validation: 0.2707238935639868]
	TIME [epoch: 9.66 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1943092620234178		[learning rate: 1.3532e-05]
	Learning Rate: 1.35322e-05
	LOSS [training: 0.1943092620234178 | validation: 0.2659168626551562]
	TIME [epoch: 9.64 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18861854893037075		[learning rate: 1.3483e-05]
	Learning Rate: 1.34831e-05
	LOSS [training: 0.18861854893037075 | validation: 0.2539048181424044]
	TIME [epoch: 9.64 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19076694109601172		[learning rate: 1.3434e-05]
	Learning Rate: 1.34342e-05
	LOSS [training: 0.19076694109601172 | validation: 0.2608402796344885]
	TIME [epoch: 9.65 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19579724978918855		[learning rate: 1.3385e-05]
	Learning Rate: 1.33854e-05
	LOSS [training: 0.19579724978918855 | validation: 0.27053536903761183]
	TIME [epoch: 9.67 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19550983068220132		[learning rate: 1.3337e-05]
	Learning Rate: 1.33368e-05
	LOSS [training: 0.19550983068220132 | validation: 0.27138515989548045]
	TIME [epoch: 9.65 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19535814685037495		[learning rate: 1.3288e-05]
	Learning Rate: 1.32884e-05
	LOSS [training: 0.19535814685037495 | validation: 0.2842732740461078]
	TIME [epoch: 9.64 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19686711262460072		[learning rate: 1.324e-05]
	Learning Rate: 1.32402e-05
	LOSS [training: 0.19686711262460072 | validation: 0.2674118636198554]
	TIME [epoch: 9.65 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19439541642477348		[learning rate: 1.3192e-05]
	Learning Rate: 1.31922e-05
	LOSS [training: 0.19439541642477348 | validation: 0.2686375246680274]
	TIME [epoch: 9.66 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19456968230702448		[learning rate: 1.3144e-05]
	Learning Rate: 1.31443e-05
	LOSS [training: 0.19456968230702448 | validation: 0.26482148333693334]
	TIME [epoch: 9.64 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18819251316020452		[learning rate: 1.3097e-05]
	Learning Rate: 1.30966e-05
	LOSS [training: 0.18819251316020452 | validation: 0.27137770495908475]
	TIME [epoch: 9.64 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1941871753108641		[learning rate: 1.3049e-05]
	Learning Rate: 1.30491e-05
	LOSS [training: 0.1941871753108641 | validation: 0.26597553225908216]
	TIME [epoch: 9.64 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1871334729725766		[learning rate: 1.3002e-05]
	Learning Rate: 1.30017e-05
	LOSS [training: 0.1871334729725766 | validation: 0.2865923097338465]
	TIME [epoch: 9.65 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19054451746395706		[learning rate: 1.2955e-05]
	Learning Rate: 1.29545e-05
	LOSS [training: 0.19054451746395706 | validation: 0.2610861345061376]
	TIME [epoch: 9.64 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19157733301846563		[learning rate: 1.2907e-05]
	Learning Rate: 1.29075e-05
	LOSS [training: 0.19157733301846563 | validation: 0.2740187036807236]
	TIME [epoch: 9.64 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1951862828217243		[learning rate: 1.2861e-05]
	Learning Rate: 1.28607e-05
	LOSS [training: 0.1951862828217243 | validation: 0.25265020569924324]
	TIME [epoch: 9.66 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19071631610513506		[learning rate: 1.2814e-05]
	Learning Rate: 1.2814e-05
	LOSS [training: 0.19071631610513506 | validation: 0.26255703727604224]
	TIME [epoch: 9.65 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19382795258530988		[learning rate: 1.2767e-05]
	Learning Rate: 1.27675e-05
	LOSS [training: 0.19382795258530988 | validation: 0.2731917106683473]
	TIME [epoch: 9.65 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20568099104417975		[learning rate: 1.2721e-05]
	Learning Rate: 1.27212e-05
	LOSS [training: 0.20568099104417975 | validation: 0.2650464832702034]
	TIME [epoch: 9.64 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19967067403958855		[learning rate: 1.2675e-05]
	Learning Rate: 1.2675e-05
	LOSS [training: 0.19967067403958855 | validation: 0.2669018977605776]
	TIME [epoch: 9.66 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1992284170211886		[learning rate: 1.2629e-05]
	Learning Rate: 1.2629e-05
	LOSS [training: 0.1992284170211886 | validation: 0.2881080779450438]
	TIME [epoch: 9.65 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19947344058829647		[learning rate: 1.2583e-05]
	Learning Rate: 1.25832e-05
	LOSS [training: 0.19947344058829647 | validation: 0.26026742135834]
	TIME [epoch: 9.64 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18837943656294415		[learning rate: 1.2537e-05]
	Learning Rate: 1.25375e-05
	LOSS [training: 0.18837943656294415 | validation: 0.26727500561584255]
	TIME [epoch: 9.64 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20278784036159064		[learning rate: 1.2492e-05]
	Learning Rate: 1.2492e-05
	LOSS [training: 0.20278784036159064 | validation: 0.2739916988079254]
	TIME [epoch: 9.66 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1971894299279389		[learning rate: 1.2447e-05]
	Learning Rate: 1.24467e-05
	LOSS [training: 0.1971894299279389 | validation: 0.28247592192793325]
	TIME [epoch: 9.64 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1928095332587444		[learning rate: 1.2401e-05]
	Learning Rate: 1.24015e-05
	LOSS [training: 0.1928095332587444 | validation: 0.2610333812261984]
	TIME [epoch: 9.64 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1979244248735311		[learning rate: 1.2356e-05]
	Learning Rate: 1.23565e-05
	LOSS [training: 0.1979244248735311 | validation: 0.25794344153079823]
	TIME [epoch: 9.65 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20105684012625047		[learning rate: 1.2312e-05]
	Learning Rate: 1.23116e-05
	LOSS [training: 0.20105684012625047 | validation: 0.27042329867072534]
	TIME [epoch: 9.65 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1897878104340987		[learning rate: 1.2267e-05]
	Learning Rate: 1.2267e-05
	LOSS [training: 0.1897878104340987 | validation: 0.262417071607454]
	TIME [epoch: 9.64 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19447317129444264		[learning rate: 1.2222e-05]
	Learning Rate: 1.22224e-05
	LOSS [training: 0.19447317129444264 | validation: 0.27466215961179546]
	TIME [epoch: 9.64 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18975248575047357		[learning rate: 1.2178e-05]
	Learning Rate: 1.21781e-05
	LOSS [training: 0.18975248575047357 | validation: 0.2762474748712557]
	TIME [epoch: 9.65 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19349356878013907		[learning rate: 1.2134e-05]
	Learning Rate: 1.21339e-05
	LOSS [training: 0.19349356878013907 | validation: 0.27669932833749605]
	TIME [epoch: 9.64 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2020210825726358		[learning rate: 1.209e-05]
	Learning Rate: 1.20899e-05
	LOSS [training: 0.2020210825726358 | validation: 0.26786017505701476]
	TIME [epoch: 9.64 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1960816920316379		[learning rate: 1.2046e-05]
	Learning Rate: 1.2046e-05
	LOSS [training: 0.1960816920316379 | validation: 0.2862850424253662]
	TIME [epoch: 9.63 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2051283264364474		[learning rate: 1.2002e-05]
	Learning Rate: 1.20023e-05
	LOSS [training: 0.2051283264364474 | validation: 0.2816869229401757]
	TIME [epoch: 9.66 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19642781098180648		[learning rate: 1.1959e-05]
	Learning Rate: 1.19587e-05
	LOSS [training: 0.19642781098180648 | validation: 0.2607571943055921]
	TIME [epoch: 9.64 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19422658969692141		[learning rate: 1.1915e-05]
	Learning Rate: 1.19153e-05
	LOSS [training: 0.19422658969692141 | validation: 0.27330149726418723]
	TIME [epoch: 9.64 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18971904058642688		[learning rate: 1.1872e-05]
	Learning Rate: 1.18721e-05
	LOSS [training: 0.18971904058642688 | validation: 0.26282298593061965]
	TIME [epoch: 9.65 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20747614397751332		[learning rate: 1.1829e-05]
	Learning Rate: 1.1829e-05
	LOSS [training: 0.20747614397751332 | validation: 0.27320325560180836]
	TIME [epoch: 9.66 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19644873144649877		[learning rate: 1.1786e-05]
	Learning Rate: 1.17861e-05
	LOSS [training: 0.19644873144649877 | validation: 0.27313686690992744]
	TIME [epoch: 9.64 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19895082134898284		[learning rate: 1.1743e-05]
	Learning Rate: 1.17433e-05
	LOSS [training: 0.19895082134898284 | validation: 0.26726464665598487]
	TIME [epoch: 9.64 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19556046952304745		[learning rate: 1.1701e-05]
	Learning Rate: 1.17007e-05
	LOSS [training: 0.19556046952304745 | validation: 0.2828729650365258]
	TIME [epoch: 9.66 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1996080306167523		[learning rate: 1.1658e-05]
	Learning Rate: 1.16582e-05
	LOSS [training: 0.1996080306167523 | validation: 0.2610735140608075]
	TIME [epoch: 9.65 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19564635486366697		[learning rate: 1.1616e-05]
	Learning Rate: 1.16159e-05
	LOSS [training: 0.19564635486366697 | validation: 0.26627522108942125]
	TIME [epoch: 9.64 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1969396175324247		[learning rate: 1.1574e-05]
	Learning Rate: 1.15737e-05
	LOSS [training: 0.1969396175324247 | validation: 0.2727501432853473]
	TIME [epoch: 9.64 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19713631476812668		[learning rate: 1.1532e-05]
	Learning Rate: 1.15317e-05
	LOSS [training: 0.19713631476812668 | validation: 0.2738531241012478]
	TIME [epoch: 9.66 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2034659371921923		[learning rate: 1.149e-05]
	Learning Rate: 1.14899e-05
	LOSS [training: 0.2034659371921923 | validation: 0.2633302094111862]
	TIME [epoch: 9.65 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1957819633734068		[learning rate: 1.1448e-05]
	Learning Rate: 1.14482e-05
	LOSS [training: 0.1957819633734068 | validation: 0.25666453826045044]
	TIME [epoch: 9.63 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20161773627877294		[learning rate: 1.1407e-05]
	Learning Rate: 1.14066e-05
	LOSS [training: 0.20161773627877294 | validation: 0.26628853807428166]
	TIME [epoch: 9.64 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18769656820885144		[learning rate: 1.1365e-05]
	Learning Rate: 1.13652e-05
	LOSS [training: 0.18769656820885144 | validation: 0.2749297813023134]
	TIME [epoch: 9.66 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19362618036958884		[learning rate: 1.1324e-05]
	Learning Rate: 1.1324e-05
	LOSS [training: 0.19362618036958884 | validation: 0.27131561112548497]
	TIME [epoch: 9.64 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19894020755848363		[learning rate: 1.1283e-05]
	Learning Rate: 1.12829e-05
	LOSS [training: 0.19894020755848363 | validation: 0.26936786038970584]
	TIME [epoch: 9.64 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19729823141948816		[learning rate: 1.1242e-05]
	Learning Rate: 1.1242e-05
	LOSS [training: 0.19729823141948816 | validation: 0.24338749414399416]
	TIME [epoch: 9.65 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1985361380802455		[learning rate: 1.1201e-05]
	Learning Rate: 1.12012e-05
	LOSS [training: 0.1985361380802455 | validation: 0.26347063721582975]
	TIME [epoch: 9.65 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19007781404635302		[learning rate: 1.1161e-05]
	Learning Rate: 1.11605e-05
	LOSS [training: 0.19007781404635302 | validation: 0.26944102198453207]
	TIME [epoch: 9.64 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19416106624653015		[learning rate: 1.112e-05]
	Learning Rate: 1.112e-05
	LOSS [training: 0.19416106624653015 | validation: 0.2602527187037832]
	TIME [epoch: 9.64 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19364360231758238		[learning rate: 1.108e-05]
	Learning Rate: 1.10797e-05
	LOSS [training: 0.19364360231758238 | validation: 0.2668894515823]
	TIME [epoch: 9.66 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20026613782541292		[learning rate: 1.1039e-05]
	Learning Rate: 1.10394e-05
	LOSS [training: 0.20026613782541292 | validation: 0.2679179010354031]
	TIME [epoch: 9.64 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.195190469933758		[learning rate: 1.0999e-05]
	Learning Rate: 1.09994e-05
	LOSS [training: 0.195190469933758 | validation: 0.25631483906247915]
	TIME [epoch: 9.64 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18685151887072055		[learning rate: 1.0959e-05]
	Learning Rate: 1.09595e-05
	LOSS [training: 0.18685151887072055 | validation: 0.2794967839238684]
	TIME [epoch: 9.64 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20477351758786347		[learning rate: 1.092e-05]
	Learning Rate: 1.09197e-05
	LOSS [training: 0.20477351758786347 | validation: 0.257519224679724]
	TIME [epoch: 9.67 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1940748965772295		[learning rate: 1.088e-05]
	Learning Rate: 1.08801e-05
	LOSS [training: 0.1940748965772295 | validation: 0.2564095158242215]
	TIME [epoch: 9.65 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19342378679456734		[learning rate: 1.0841e-05]
	Learning Rate: 1.08406e-05
	LOSS [training: 0.19342378679456734 | validation: 0.2689661362402201]
	TIME [epoch: 9.64 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19700788667049365		[learning rate: 1.0801e-05]
	Learning Rate: 1.08012e-05
	LOSS [training: 0.19700788667049365 | validation: 0.26806163503073344]
	TIME [epoch: 9.65 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19756103516638784		[learning rate: 1.0762e-05]
	Learning Rate: 1.0762e-05
	LOSS [training: 0.19756103516638784 | validation: 0.2489741607252936]
	TIME [epoch: 9.67 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20623726275262352		[learning rate: 1.0723e-05]
	Learning Rate: 1.0723e-05
	LOSS [training: 0.20623726275262352 | validation: 0.27616575965485907]
	TIME [epoch: 9.65 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19877204258358822		[learning rate: 1.0684e-05]
	Learning Rate: 1.06841e-05
	LOSS [training: 0.19877204258358822 | validation: 0.2694559482714538]
	TIME [epoch: 9.64 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19501515801270158		[learning rate: 1.0645e-05]
	Learning Rate: 1.06453e-05
	LOSS [training: 0.19501515801270158 | validation: 0.28139090706473335]
	TIME [epoch: 9.65 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20164486649317964		[learning rate: 1.0607e-05]
	Learning Rate: 1.06067e-05
	LOSS [training: 0.20164486649317964 | validation: 0.26274138923593143]
	TIME [epoch: 9.66 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19314782872090136		[learning rate: 1.0568e-05]
	Learning Rate: 1.05682e-05
	LOSS [training: 0.19314782872090136 | validation: 0.2761501474663483]
	TIME [epoch: 9.64 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.186799096301627		[learning rate: 1.053e-05]
	Learning Rate: 1.05298e-05
	LOSS [training: 0.186799096301627 | validation: 0.2644982121947089]
	TIME [epoch: 9.64 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19119135671251847		[learning rate: 1.0492e-05]
	Learning Rate: 1.04916e-05
	LOSS [training: 0.19119135671251847 | validation: 0.28547571566007124]
	TIME [epoch: 9.66 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19467308548624412		[learning rate: 1.0454e-05]
	Learning Rate: 1.04535e-05
	LOSS [training: 0.19467308548624412 | validation: 0.2794648712769249]
	TIME [epoch: 9.65 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1944740251805583		[learning rate: 1.0416e-05]
	Learning Rate: 1.04156e-05
	LOSS [training: 0.1944740251805583 | validation: 0.2846086799648585]
	TIME [epoch: 9.65 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19815026512313688		[learning rate: 1.0378e-05]
	Learning Rate: 1.03778e-05
	LOSS [training: 0.19815026512313688 | validation: 0.2637745033385422]
	TIME [epoch: 9.64 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19393862999029565		[learning rate: 1.034e-05]
	Learning Rate: 1.03401e-05
	LOSS [training: 0.19393862999029565 | validation: 0.266107091513449]
	TIME [epoch: 9.67 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19584414764479302		[learning rate: 1.0303e-05]
	Learning Rate: 1.03026e-05
	LOSS [training: 0.19584414764479302 | validation: 0.25967747192024004]
	TIME [epoch: 9.64 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19802077166318216		[learning rate: 1.0265e-05]
	Learning Rate: 1.02652e-05
	LOSS [training: 0.19802077166318216 | validation: 0.2710615425617667]
	TIME [epoch: 9.66 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19418082408913911		[learning rate: 1.0228e-05]
	Learning Rate: 1.0228e-05
	LOSS [training: 0.19418082408913911 | validation: 0.26813440260900523]
	TIME [epoch: 9.65 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1930582155539364		[learning rate: 1.0191e-05]
	Learning Rate: 1.01909e-05
	LOSS [training: 0.1930582155539364 | validation: 0.28751729699782175]
	TIME [epoch: 9.67 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20042570740865293		[learning rate: 1.0154e-05]
	Learning Rate: 1.01539e-05
	LOSS [training: 0.20042570740865293 | validation: 0.26552519663466606]
	TIME [epoch: 9.65 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18798360687670237		[learning rate: 1.0117e-05]
	Learning Rate: 1.0117e-05
	LOSS [training: 0.18798360687670237 | validation: 0.27312277626611836]
	TIME [epoch: 9.65 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19292865938648257		[learning rate: 1.008e-05]
	Learning Rate: 1.00803e-05
	LOSS [training: 0.19292865938648257 | validation: 0.2679577440332237]
	TIME [epoch: 9.65 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18792705388097836		[learning rate: 1.0044e-05]
	Learning Rate: 1.00437e-05
	LOSS [training: 0.18792705388097836 | validation: 0.26499428712457507]
	TIME [epoch: 9.67 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19041333650654862		[learning rate: 1.0007e-05]
	Learning Rate: 1.00073e-05
	LOSS [training: 0.19041333650654862 | validation: 0.2744780545538372]
	TIME [epoch: 9.64 sec]
Finished training in 19505.621 seconds.
