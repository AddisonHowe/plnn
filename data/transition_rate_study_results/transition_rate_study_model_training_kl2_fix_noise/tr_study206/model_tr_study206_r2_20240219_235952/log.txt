Args:
Namespace(name='model_tr_study206', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2', training_data='data/transition_rate_studies/tr_study206/tr_study206_training/r2', validation_data='data/transition_rate_studies/tr_study206/tr_study206_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2984201633

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.28108011703776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.28108011703776 | validation: 10.102900699081495]
	TIME [epoch: 78.9 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.065303657198793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.065303657198793 | validation: 10.667032594345338]
	TIME [epoch: 9.75 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.954329832249497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.954329832249497 | validation: 10.08031290767104]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.58576400653174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.58576400653174 | validation: 9.752871004537576]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.21728164903279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.21728164903279 | validation: 10.486896130425851]
	TIME [epoch: 9.76 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.822450877400737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.822450877400737 | validation: 10.468278410447885]
	TIME [epoch: 9.74 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.451120500123745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.451120500123745 | validation: 9.053024035057106]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.50282809775761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.50282809775761 | validation: 9.061492376208436]
	TIME [epoch: 9.76 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.82283341813046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.82283341813046 | validation: 7.212234284668965]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.045735726738949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.045735726738949 | validation: 6.296695750546692]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.209695026515675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.209695026515675 | validation: 9.159555898034336]
	TIME [epoch: 9.75 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.870288578159484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.870288578159484 | validation: 7.101536129731832]
	TIME [epoch: 9.73 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.940731519293472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.940731519293472 | validation: 5.8747184999375355]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.59929274430143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.59929274430143 | validation: 6.192764394573637]
	TIME [epoch: 9.72 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.268590009221176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.268590009221176 | validation: 6.414510003688356]
	TIME [epoch: 9.74 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2151413203558565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2151413203558565 | validation: 6.340851357226862]
	TIME [epoch: 9.73 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.060389699343476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.060389699343476 | validation: 7.0956528655689155]
	TIME [epoch: 9.72 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.223723816718642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.223723816718642 | validation: 6.398507672335438]
	TIME [epoch: 9.75 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.165071209101518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.165071209101518 | validation: 6.175822516001881]
	TIME [epoch: 9.73 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.840318438267921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.840318438267921 | validation: 7.196582098902609]
	TIME [epoch: 9.72 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.10935754044025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.10935754044025 | validation: 9.826280521128545]
	TIME [epoch: 9.72 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.647213244117334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.647213244117334 | validation: 8.604636885889345]
	TIME [epoch: 9.75 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.79919273894311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.79919273894311 | validation: 5.579034951907017]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.289611889251928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.289611889251928 | validation: 5.599476768197378]
	TIME [epoch: 9.71 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3617511129341855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3617511129341855 | validation: 6.477605150347776]
	TIME [epoch: 9.71 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.265252989164122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.265252989164122 | validation: 5.883713310556793]
	TIME [epoch: 9.74 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1094052739378935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1094052739378935 | validation: 6.0958881689593065]
	TIME [epoch: 9.71 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.100659887212444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.100659887212444 | validation: 5.731257398079174]
	TIME [epoch: 9.71 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.884106896492876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.884106896492876 | validation: 6.446869386244487]
	TIME [epoch: 9.72 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.88516920349143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.88516920349143 | validation: 6.653198644452025]
	TIME [epoch: 9.72 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2326788511196085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2326788511196085 | validation: 6.267872698703452]
	TIME [epoch: 9.72 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.040073458404562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.040073458404562 | validation: 6.262840287697721]
	TIME [epoch: 9.71 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.139723293596978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.139723293596978 | validation: 5.480321414604551]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.6847290456846675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.6847290456846675 | validation: 8.463005205018344]
	TIME [epoch: 9.72 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.595013088138144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.595013088138144 | validation: 6.069653016535292]
	TIME [epoch: 9.72 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.753823999027698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.753823999027698 | validation: 7.853713896740677]
	TIME [epoch: 9.72 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.102400046001138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.102400046001138 | validation: 6.091916242414648]
	TIME [epoch: 9.75 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.682005360814744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.682005360814744 | validation: 7.297266696380441]
	TIME [epoch: 9.72 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.876252303216062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.876252303216062 | validation: 5.691773085092794]
	TIME [epoch: 9.71 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.048631971985326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.048631971985326 | validation: 5.535713843507028]
	TIME [epoch: 9.71 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.743723164208393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.743723164208393 | validation: 7.238871070252699]
	TIME [epoch: 9.74 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.323672318850623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.323672318850623 | validation: 5.909315427326451]
	TIME [epoch: 9.72 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.008485231827487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.008485231827487 | validation: 7.094104043204001]
	TIME [epoch: 9.72 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.258031667312705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.258031667312705 | validation: 5.669959795202019]
	TIME [epoch: 9.74 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.04946587433821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.04946587433821 | validation: 5.749550761240796]
	TIME [epoch: 9.72 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.977294003208981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.977294003208981 | validation: 5.85433933588994]
	TIME [epoch: 9.72 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.885871750114889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.885871750114889 | validation: 5.4959100577493265]
	TIME [epoch: 9.72 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.068085129814239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.068085129814239 | validation: 5.577960006343692]
	TIME [epoch: 9.74 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.936152135770075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.936152135770075 | validation: 5.3702537530176935]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.002120101519017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.002120101519017 | validation: 6.217524383504922]
	TIME [epoch: 9.73 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.902518124498917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.902518124498917 | validation: 5.910695773296072]
	TIME [epoch: 9.73 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.942880220682999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.942880220682999 | validation: 6.100868450731493]
	TIME [epoch: 9.76 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.974707619929108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.974707619929108 | validation: 5.968440408197373]
	TIME [epoch: 9.71 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.903575788341454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.903575788341454 | validation: 5.857233332299268]
	TIME [epoch: 9.73 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.893457704514338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.893457704514338 | validation: 5.705473087796657]
	TIME [epoch: 9.73 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.885889963687439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.885889963687439 | validation: 5.479896798994118]
	TIME [epoch: 9.75 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.693645773320098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.693645773320098 | validation: 6.314348102653164]
	TIME [epoch: 9.74 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.986218608468788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.986218608468788 | validation: 5.577715502257907]
	TIME [epoch: 9.73 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8813349562581365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8813349562581365 | validation: 5.301941532261086]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.756217483125289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.756217483125289 | validation: 5.993232946954245]
	TIME [epoch: 9.73 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.981283677583276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.981283677583276 | validation: 6.41233824115466]
	TIME [epoch: 9.71 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.976671043218432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.976671043218432 | validation: 5.677724682488041]
	TIME [epoch: 9.73 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.902329473673407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.902329473673407 | validation: 5.57364685914961]
	TIME [epoch: 9.75 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.729979793056822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.729979793056822 | validation: 5.8647011784341]
	TIME [epoch: 9.72 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.779914611978915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.779914611978915 | validation: 6.6624740369145865]
	TIME [epoch: 9.73 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.481034758984547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.481034758984547 | validation: 6.509409311568024]
	TIME [epoch: 9.73 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.931184024033125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.931184024033125 | validation: 5.778685640959647]
	TIME [epoch: 9.74 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.800332824145264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.800332824145264 | validation: 5.873682618727428]
	TIME [epoch: 9.73 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.764678170589323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.764678170589323 | validation: 5.3338750658258665]
	TIME [epoch: 9.73 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.728596858760932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.728596858760932 | validation: 5.739159774057454]
	TIME [epoch: 9.75 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.685085781408247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.685085781408247 | validation: 6.097001916944058]
	TIME [epoch: 9.72 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7885768594460485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7885768594460485 | validation: 5.7967353308705825]
	TIME [epoch: 9.73 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.6990817371385685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6990817371385685 | validation: 5.839813887601244]
	TIME [epoch: 9.72 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.705088923456659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.705088923456659 | validation: 5.71364026843105]
	TIME [epoch: 9.76 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.657821652204444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.657821652204444 | validation: 7.2729172122901655]
	TIME [epoch: 9.73 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.206157353395787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.206157353395787 | validation: 5.531511789581919]
	TIME [epoch: 9.73 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.662513726735199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.662513726735199 | validation: 5.407920278198853]
	TIME [epoch: 9.73 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.5672906195509535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5672906195509535 | validation: 5.463517601355862]
	TIME [epoch: 9.75 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.599566956681825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.599566956681825 | validation: 5.75030186948514]
	TIME [epoch: 9.72 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.747222154878983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.747222154878983 | validation: 5.124489497742465]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.444050148584304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.444050148584304 | validation: 5.859786889236841]
	TIME [epoch: 9.75 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.866415199094169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.866415199094169 | validation: 5.842400020173446]
	TIME [epoch: 9.72 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.677832443193599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.677832443193599 | validation: 5.019075472259613]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.6569305328117885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6569305328117885 | validation: 5.30863611463593]
	TIME [epoch: 9.72 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.6267414292803295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6267414292803295 | validation: 5.596218893872979]
	TIME [epoch: 9.75 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.565776040049873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.565776040049873 | validation: 5.400752541966194]
	TIME [epoch: 9.74 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.604297968334414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.604297968334414 | validation: 5.451913606312518]
	TIME [epoch: 9.73 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.5009994604113075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5009994604113075 | validation: 5.9378625170308235]
	TIME [epoch: 9.72 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.703740247984191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.703740247984191 | validation: 5.907517431996034]
	TIME [epoch: 9.75 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.651424927050419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.651424927050419 | validation: 5.734392365477081]
	TIME [epoch: 9.73 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.62443846719961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.62443846719961 | validation: 5.658599114025115]
	TIME [epoch: 9.72 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.577873169081032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.577873169081032 | validation: 5.610760612776367]
	TIME [epoch: 9.73 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.583627713443654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.583627713443654 | validation: 5.679843545261766]
	TIME [epoch: 9.73 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.659794050794457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.659794050794457 | validation: 5.079645685363214]
	TIME [epoch: 9.7 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.516247333464908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.516247333464908 | validation: 5.928209089536074]
	TIME [epoch: 9.72 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.6966017261382165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6966017261382165 | validation: 5.198824212623995]
	TIME [epoch: 9.75 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.646128153986564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.646128153986564 | validation: 5.507326018126763]
	TIME [epoch: 9.72 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.575024571339082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.575024571339082 | validation: 5.586269415018192]
	TIME [epoch: 9.72 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.62578106866046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.62578106866046 | validation: 5.58460894711698]
	TIME [epoch: 9.71 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.6162625706857074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6162625706857074 | validation: 5.274150991902622]
	TIME [epoch: 9.74 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.551556097289523		[learning rate: 0.009971]
	Learning Rate: 0.00997096
	LOSS [training: 4.551556097289523 | validation: 5.21760434149119]
	TIME [epoch: 9.71 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.608344965536642		[learning rate: 0.0099348]
	Learning Rate: 0.00993477
	LOSS [training: 4.608344965536642 | validation: 5.439152761788186]
	TIME [epoch: 9.71 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.521879124194073		[learning rate: 0.0098987]
	Learning Rate: 0.00989872
	LOSS [training: 4.521879124194073 | validation: 5.42110871883081]
	TIME [epoch: 9.72 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.591869805515003		[learning rate: 0.0098628]
	Learning Rate: 0.00986279
	LOSS [training: 4.591869805515003 | validation: 5.153803331326509]
	TIME [epoch: 9.74 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.496211866982979		[learning rate: 0.009827]
	Learning Rate: 0.009827
	LOSS [training: 4.496211866982979 | validation: 5.402028561530524]
	TIME [epoch: 9.72 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.5713050454768585		[learning rate: 0.0097913]
	Learning Rate: 0.00979134
	LOSS [training: 4.5713050454768585 | validation: 5.3496797290721405]
	TIME [epoch: 9.72 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.482619756572312		[learning rate: 0.0097558]
	Learning Rate: 0.00975581
	LOSS [training: 4.482619756572312 | validation: 5.525490259735009]
	TIME [epoch: 9.75 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.519282582784296		[learning rate: 0.0097204]
	Learning Rate: 0.0097204
	LOSS [training: 4.519282582784296 | validation: 5.430096239196307]
	TIME [epoch: 9.73 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.626107664091901		[learning rate: 0.0096851]
	Learning Rate: 0.00968513
	LOSS [training: 4.626107664091901 | validation: 4.979987107572166]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.425807766545676		[learning rate: 0.00965]
	Learning Rate: 0.00964998
	LOSS [training: 4.425807766545676 | validation: 6.132497466437473]
	TIME [epoch: 9.74 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.686289590223591		[learning rate: 0.009615]
	Learning Rate: 0.00961496
	LOSS [training: 4.686289590223591 | validation: 5.936202029571553]
	TIME [epoch: 9.74 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.580476813487205		[learning rate: 0.0095801]
	Learning Rate: 0.00958006
	LOSS [training: 4.580476813487205 | validation: 5.5061330538187825]
	TIME [epoch: 9.72 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.5090922087973215		[learning rate: 0.0095453]
	Learning Rate: 0.0095453
	LOSS [training: 4.5090922087973215 | validation: 5.59255407061765]
	TIME [epoch: 9.71 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.49415513434924		[learning rate: 0.0095107]
	Learning Rate: 0.00951066
	LOSS [training: 4.49415513434924 | validation: 5.504366415017922]
	TIME [epoch: 9.72 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.444357371124635		[learning rate: 0.0094761]
	Learning Rate: 0.00947614
	LOSS [training: 4.444357371124635 | validation: 5.553494373626158]
	TIME [epoch: 9.74 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.497680016689574		[learning rate: 0.0094418]
	Learning Rate: 0.00944175
	LOSS [training: 4.497680016689574 | validation: 5.165964867030687]
	TIME [epoch: 9.71 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.492060736195152		[learning rate: 0.0094075]
	Learning Rate: 0.00940749
	LOSS [training: 4.492060736195152 | validation: 5.509132632354736]
	TIME [epoch: 9.71 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.458995769653304		[learning rate: 0.0093733]
	Learning Rate: 0.00937335
	LOSS [training: 4.458995769653304 | validation: 5.169848181628083]
	TIME [epoch: 9.72 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.374394402136597		[learning rate: 0.0093393]
	Learning Rate: 0.00933933
	LOSS [training: 4.374394402136597 | validation: 4.992069839102645]
	TIME [epoch: 9.71 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.520676772671355		[learning rate: 0.0093054]
	Learning Rate: 0.00930544
	LOSS [training: 4.520676772671355 | validation: 5.339670660944458]
	TIME [epoch: 9.72 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.52195892352357		[learning rate: 0.0092717]
	Learning Rate: 0.00927167
	LOSS [training: 4.52195892352357 | validation: 5.2553755989419875]
	TIME [epoch: 9.71 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.39978327324038		[learning rate: 0.009238]
	Learning Rate: 0.00923802
	LOSS [training: 4.39978327324038 | validation: 5.151681523193071]
	TIME [epoch: 9.73 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.49078436341393		[learning rate: 0.0092045]
	Learning Rate: 0.0092045
	LOSS [training: 4.49078436341393 | validation: 5.2292535926137855]
	TIME [epoch: 9.7 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.480692891825958		[learning rate: 0.0091711]
	Learning Rate: 0.00917109
	LOSS [training: 4.480692891825958 | validation: 5.213209853454579]
	TIME [epoch: 9.72 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.388330885902376		[learning rate: 0.0091378]
	Learning Rate: 0.00913781
	LOSS [training: 4.388330885902376 | validation: 4.987750232669775]
	TIME [epoch: 9.71 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.441847957418742		[learning rate: 0.0091046]
	Learning Rate: 0.00910465
	LOSS [training: 4.441847957418742 | validation: 5.497275031436718]
	TIME [epoch: 9.74 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.433911578881976		[learning rate: 0.0090716]
	Learning Rate: 0.00907161
	LOSS [training: 4.433911578881976 | validation: 5.209471653899527]
	TIME [epoch: 9.69 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.4428136758188534		[learning rate: 0.0090387]
	Learning Rate: 0.00903868
	LOSS [training: 4.4428136758188534 | validation: 6.258327601051512]
	TIME [epoch: 9.71 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.725537015805495		[learning rate: 0.0090059]
	Learning Rate: 0.00900588
	LOSS [training: 4.725537015805495 | validation: 7.889096750651041]
	TIME [epoch: 9.72 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.515359349286663		[learning rate: 0.0089732]
	Learning Rate: 0.0089732
	LOSS [training: 5.515359349286663 | validation: 5.112413736141186]
	TIME [epoch: 9.74 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3826316459607195		[learning rate: 0.0089406]
	Learning Rate: 0.00894064
	LOSS [training: 4.3826316459607195 | validation: 5.420179693199429]
	TIME [epoch: 9.71 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.385713980154672		[learning rate: 0.0089082]
	Learning Rate: 0.00890819
	LOSS [training: 4.385713980154672 | validation: 5.101520874575691]
	TIME [epoch: 9.71 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.46005408929381		[learning rate: 0.0088759]
	Learning Rate: 0.00887586
	LOSS [training: 4.46005408929381 | validation: 5.208164784534229]
	TIME [epoch: 9.73 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.356137595365292		[learning rate: 0.0088437]
	Learning Rate: 0.00884365
	LOSS [training: 4.356137595365292 | validation: 5.5182465830498515]
	TIME [epoch: 9.72 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.345238079738753		[learning rate: 0.0088116]
	Learning Rate: 0.00881156
	LOSS [training: 4.345238079738753 | validation: 5.573590174778231]
	TIME [epoch: 9.71 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.399326274802689		[learning rate: 0.0087796]
	Learning Rate: 0.00877958
	LOSS [training: 4.399326274802689 | validation: 5.093009931859725]
	TIME [epoch: 9.71 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.286894451760573		[learning rate: 0.0087477]
	Learning Rate: 0.00874772
	LOSS [training: 4.286894451760573 | validation: 6.047709674152064]
	TIME [epoch: 9.74 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.556346410362498		[learning rate: 0.008716]
	Learning Rate: 0.00871597
	LOSS [training: 4.556346410362498 | validation: 5.313009599114735]
	TIME [epoch: 9.7 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.333064486336417		[learning rate: 0.0086843]
	Learning Rate: 0.00868434
	LOSS [training: 4.333064486336417 | validation: 5.664587023301076]
	TIME [epoch: 9.72 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.288350517035204		[learning rate: 0.0086528]
	Learning Rate: 0.00865282
	LOSS [training: 4.288350517035204 | validation: 5.2132327442364526]
	TIME [epoch: 9.7 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.390229935776915		[learning rate: 0.0086214]
	Learning Rate: 0.00862142
	LOSS [training: 4.390229935776915 | validation: 5.114948810226959]
	TIME [epoch: 9.74 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.257017474991512		[learning rate: 0.0085901]
	Learning Rate: 0.00859013
	LOSS [training: 4.257017474991512 | validation: 4.969348764139908]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.242313588912424		[learning rate: 0.008559]
	Learning Rate: 0.00855896
	LOSS [training: 4.242313588912424 | validation: 5.44802626149242]
	TIME [epoch: 9.72 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.382236264036427		[learning rate: 0.0085279]
	Learning Rate: 0.0085279
	LOSS [training: 4.382236264036427 | validation: 5.169747112485255]
	TIME [epoch: 9.72 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.285756710226041		[learning rate: 0.008497]
	Learning Rate: 0.00849695
	LOSS [training: 4.285756710226041 | validation: 5.595382451251917]
	TIME [epoch: 9.73 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.607137837651728		[learning rate: 0.0084661]
	Learning Rate: 0.00846612
	LOSS [training: 4.607137837651728 | validation: 5.09030591206201]
	TIME [epoch: 9.71 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.346850863703824		[learning rate: 0.0084354]
	Learning Rate: 0.00843539
	LOSS [training: 4.346850863703824 | validation: 5.276985010188444]
	TIME [epoch: 9.72 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.265411932215853		[learning rate: 0.0084048]
	Learning Rate: 0.00840478
	LOSS [training: 4.265411932215853 | validation: 5.676343118712352]
	TIME [epoch: 9.74 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.4034602100371645		[learning rate: 0.0083743]
	Learning Rate: 0.00837428
	LOSS [training: 4.4034602100371645 | validation: 5.055140772054964]
	TIME [epoch: 9.72 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2367156012763		[learning rate: 0.0083439]
	Learning Rate: 0.00834389
	LOSS [training: 4.2367156012763 | validation: 4.9759837681166506]
	TIME [epoch: 9.71 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.238525800667277		[learning rate: 0.0083136]
	Learning Rate: 0.00831361
	LOSS [training: 4.238525800667277 | validation: 5.644009494618593]
	TIME [epoch: 9.72 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.480414172057892		[learning rate: 0.0082834]
	Learning Rate: 0.00828344
	LOSS [training: 4.480414172057892 | validation: 4.933824226108451]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.351885675865047		[learning rate: 0.0082534]
	Learning Rate: 0.00825338
	LOSS [training: 4.351885675865047 | validation: 5.031151665414797]
	TIME [epoch: 9.72 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3786873047779		[learning rate: 0.0082234]
	Learning Rate: 0.00822342
	LOSS [training: 4.3786873047779 | validation: 6.074206308311091]
	TIME [epoch: 9.72 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.453543006381022		[learning rate: 0.0081936]
	Learning Rate: 0.00819358
	LOSS [training: 4.453543006381022 | validation: 5.360127495075383]
	TIME [epoch: 9.74 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2063903048246685		[learning rate: 0.0081638]
	Learning Rate: 0.00816384
	LOSS [training: 4.2063903048246685 | validation: 5.377514694100535]
	TIME [epoch: 9.75 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.236587307117988		[learning rate: 0.0081342]
	Learning Rate: 0.00813422
	LOSS [training: 4.236587307117988 | validation: 4.944749520664024]
	TIME [epoch: 9.74 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.24249683502183		[learning rate: 0.0081047]
	Learning Rate: 0.0081047
	LOSS [training: 4.24249683502183 | validation: 5.332695122447488]
	TIME [epoch: 9.72 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.508422993940492		[learning rate: 0.0080753]
	Learning Rate: 0.00807529
	LOSS [training: 4.508422993940492 | validation: 5.048804019763149]
	TIME [epoch: 9.74 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.416142549659321		[learning rate: 0.008046]
	Learning Rate: 0.00804598
	LOSS [training: 4.416142549659321 | validation: 4.929496423218767]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.299861068766035		[learning rate: 0.0080168]
	Learning Rate: 0.00801678
	LOSS [training: 4.299861068766035 | validation: 5.164828852308093]
	TIME [epoch: 9.73 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.201718410017369		[learning rate: 0.0079877]
	Learning Rate: 0.00798769
	LOSS [training: 4.201718410017369 | validation: 5.1796642213477]
	TIME [epoch: 9.72 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.540153145843176		[learning rate: 0.0079587]
	Learning Rate: 0.0079587
	LOSS [training: 4.540153145843176 | validation: 5.058508764726423]
	TIME [epoch: 9.75 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.350931240956758		[learning rate: 0.0079298]
	Learning Rate: 0.00792982
	LOSS [training: 4.350931240956758 | validation: 5.606851273111215]
	TIME [epoch: 9.72 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.216894618522552		[learning rate: 0.007901]
	Learning Rate: 0.00790104
	LOSS [training: 4.216894618522552 | validation: 5.331478679114201]
	TIME [epoch: 9.72 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.25112239801398		[learning rate: 0.0078724]
	Learning Rate: 0.00787237
	LOSS [training: 4.25112239801398 | validation: 5.062982326290475]
	TIME [epoch: 9.72 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.206899580607446		[learning rate: 0.0078438]
	Learning Rate: 0.0078438
	LOSS [training: 4.206899580607446 | validation: 5.2100674155425395]
	TIME [epoch: 9.73 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2165543897702396		[learning rate: 0.0078153]
	Learning Rate: 0.00781533
	LOSS [training: 4.2165543897702396 | validation: 5.101687228537587]
	TIME [epoch: 9.7 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.204163719476309		[learning rate: 0.007787]
	Learning Rate: 0.00778697
	LOSS [training: 4.204163719476309 | validation: 5.156696836648835]
	TIME [epoch: 9.71 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.193697894475127		[learning rate: 0.0077587]
	Learning Rate: 0.00775871
	LOSS [training: 4.193697894475127 | validation: 5.08003279591164]
	TIME [epoch: 9.73 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.246074646890007		[learning rate: 0.0077306]
	Learning Rate: 0.00773055
	LOSS [training: 4.246074646890007 | validation: 5.3120255172725885]
	TIME [epoch: 9.72 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.205886220705299		[learning rate: 0.0077025]
	Learning Rate: 0.0077025
	LOSS [training: 4.205886220705299 | validation: 5.110759313718986]
	TIME [epoch: 9.71 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.223019992167029		[learning rate: 0.0076745]
	Learning Rate: 0.00767455
	LOSS [training: 4.223019992167029 | validation: 5.066129451133663]
	TIME [epoch: 9.72 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.177545000986646		[learning rate: 0.0076467]
	Learning Rate: 0.00764669
	LOSS [training: 4.177545000986646 | validation: 4.998402961464799]
	TIME [epoch: 9.73 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2450598360521905		[learning rate: 0.0076189]
	Learning Rate: 0.00761894
	LOSS [training: 4.2450598360521905 | validation: 5.0229153982468215]
	TIME [epoch: 9.72 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.164651841823177		[learning rate: 0.0075913]
	Learning Rate: 0.00759129
	LOSS [training: 4.164651841823177 | validation: 4.9533626331301885]
	TIME [epoch: 9.71 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2683396800942575		[learning rate: 0.0075637]
	Learning Rate: 0.00756374
	LOSS [training: 4.2683396800942575 | validation: 4.941458155027107]
	TIME [epoch: 9.71 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.197388148944565		[learning rate: 0.0075363]
	Learning Rate: 0.00753629
	LOSS [training: 4.197388148944565 | validation: 5.547108765510384]
	TIME [epoch: 9.74 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.280728166394661		[learning rate: 0.0075089]
	Learning Rate: 0.00750895
	LOSS [training: 4.280728166394661 | validation: 4.962838666743721]
	TIME [epoch: 9.72 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.394074965741247		[learning rate: 0.0074817]
	Learning Rate: 0.00748169
	LOSS [training: 4.394074965741247 | validation: 5.174879357973696]
	TIME [epoch: 9.71 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.223693966730283		[learning rate: 0.0074545]
	Learning Rate: 0.00745454
	LOSS [training: 4.223693966730283 | validation: 4.996205951004758]
	TIME [epoch: 9.72 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1036067249630905		[learning rate: 0.0074275]
	Learning Rate: 0.00742749
	LOSS [training: 4.1036067249630905 | validation: 5.2398316310459405]
	TIME [epoch: 9.73 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.154335537795022		[learning rate: 0.0074005]
	Learning Rate: 0.00740054
	LOSS [training: 4.154335537795022 | validation: 5.790001925449287]
	TIME [epoch: 9.71 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2886421572447295		[learning rate: 0.0073737]
	Learning Rate: 0.00737368
	LOSS [training: 4.2886421572447295 | validation: 5.27694338630816]
	TIME [epoch: 9.72 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.173535752807291		[learning rate: 0.0073469]
	Learning Rate: 0.00734692
	LOSS [training: 4.173535752807291 | validation: 5.346309130237447]
	TIME [epoch: 9.74 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.173152662043053		[learning rate: 0.0073203]
	Learning Rate: 0.00732026
	LOSS [training: 4.173152662043053 | validation: 5.6802371591420195]
	TIME [epoch: 9.72 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3206828353085855		[learning rate: 0.0072937]
	Learning Rate: 0.00729369
	LOSS [training: 4.3206828353085855 | validation: 4.96833005885945]
	TIME [epoch: 9.71 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.44116929588971		[learning rate: 0.0072672]
	Learning Rate: 0.00726722
	LOSS [training: 4.44116929588971 | validation: 5.262184366874185]
	TIME [epoch: 9.72 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1699895756932985		[learning rate: 0.0072408]
	Learning Rate: 0.00724085
	LOSS [training: 4.1699895756932985 | validation: 4.926241152517503]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.073588700324555		[learning rate: 0.0072146]
	Learning Rate: 0.00721457
	LOSS [training: 4.073588700324555 | validation: 5.976247846627366]
	TIME [epoch: 9.72 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.824533709945312		[learning rate: 0.0071884]
	Learning Rate: 0.00718839
	LOSS [training: 4.824533709945312 | validation: 5.047556391944457]
	TIME [epoch: 9.71 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.492150462891042		[learning rate: 0.0071623]
	Learning Rate: 0.0071623
	LOSS [training: 4.492150462891042 | validation: 5.960136714078756]
	TIME [epoch: 9.71 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.552575506757969		[learning rate: 0.0071363]
	Learning Rate: 0.00713631
	LOSS [training: 4.552575506757969 | validation: 5.04537233573834]
	TIME [epoch: 9.73 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.072523223145431		[learning rate: 0.0071104]
	Learning Rate: 0.00711041
	LOSS [training: 4.072523223145431 | validation: 4.901449559143236]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.058320129793995		[learning rate: 0.0070846]
	Learning Rate: 0.00708461
	LOSS [training: 4.058320129793995 | validation: 5.238852701649471]
	TIME [epoch: 9.72 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.300661159646869		[learning rate: 0.0070589]
	Learning Rate: 0.0070589
	LOSS [training: 4.300661159646869 | validation: 4.870980362470622]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.394878472761961		[learning rate: 0.0070333]
	Learning Rate: 0.00703328
	LOSS [training: 4.394878472761961 | validation: 4.948505408837348]
	TIME [epoch: 9.73 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.127151673476845		[learning rate: 0.0070078]
	Learning Rate: 0.00700776
	LOSS [training: 4.127151673476845 | validation: 5.0673497393231095]
	TIME [epoch: 9.73 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1067240181708895		[learning rate: 0.0069823]
	Learning Rate: 0.00698232
	LOSS [training: 4.1067240181708895 | validation: 5.157291296164092]
	TIME [epoch: 9.71 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.027197403911004		[learning rate: 0.006957]
	Learning Rate: 0.00695698
	LOSS [training: 4.027197403911004 | validation: 5.099946810031156]
	TIME [epoch: 9.74 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.369487464721562		[learning rate: 0.0069317]
	Learning Rate: 0.00693174
	LOSS [training: 4.369487464721562 | validation: 4.958895707982479]
	TIME [epoch: 9.72 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.508940120333472		[learning rate: 0.0069066]
	Learning Rate: 0.00690658
	LOSS [training: 4.508940120333472 | validation: 4.854778427112315]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.020422068773146		[learning rate: 0.0068815]
	Learning Rate: 0.00688152
	LOSS [training: 4.020422068773146 | validation: 4.863037669973611]
	TIME [epoch: 9.72 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.176694085405643		[learning rate: 0.0068565]
	Learning Rate: 0.00685654
	LOSS [training: 4.176694085405643 | validation: 5.158278371948539]
	TIME [epoch: 9.74 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.090554770248707		[learning rate: 0.0068317]
	Learning Rate: 0.00683166
	LOSS [training: 4.090554770248707 | validation: 4.974868987316327]
	TIME [epoch: 9.72 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.189166633457394		[learning rate: 0.0068069]
	Learning Rate: 0.00680687
	LOSS [training: 4.189166633457394 | validation: 4.901586540655726]
	TIME [epoch: 9.72 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.225120049402729		[learning rate: 0.0067822]
	Learning Rate: 0.00678217
	LOSS [training: 4.225120049402729 | validation: 5.336435868766767]
	TIME [epoch: 9.73 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1980360649212765		[learning rate: 0.0067576]
	Learning Rate: 0.00675755
	LOSS [training: 4.1980360649212765 | validation: 4.964614250628209]
	TIME [epoch: 9.73 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.075448402609085		[learning rate: 0.006733]
	Learning Rate: 0.00673303
	LOSS [training: 4.075448402609085 | validation: 4.990225107423238]
	TIME [epoch: 9.72 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.216997578493693		[learning rate: 0.0067086]
	Learning Rate: 0.00670859
	LOSS [training: 4.216997578493693 | validation: 5.321130578388405]
	TIME [epoch: 9.72 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.115005116117082		[learning rate: 0.0066842]
	Learning Rate: 0.00668425
	LOSS [training: 4.115005116117082 | validation: 5.87455495536281]
	TIME [epoch: 9.74 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.340806395160959		[learning rate: 0.00666]
	Learning Rate: 0.00665999
	LOSS [training: 4.340806395160959 | validation: 5.4031541504218605]
	TIME [epoch: 9.72 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.136600241271642		[learning rate: 0.0066358]
	Learning Rate: 0.00663582
	LOSS [training: 4.136600241271642 | validation: 4.952922685411348]
	TIME [epoch: 9.72 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.043857190140134		[learning rate: 0.0066117]
	Learning Rate: 0.00661174
	LOSS [training: 4.043857190140134 | validation: 5.062162535629999]
	TIME [epoch: 9.72 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9962813711366136		[learning rate: 0.0065877]
	Learning Rate: 0.00658775
	LOSS [training: 3.9962813711366136 | validation: 4.938087677709412]
	TIME [epoch: 9.74 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.145886037923339		[learning rate: 0.0065638]
	Learning Rate: 0.00656384
	LOSS [training: 4.145886037923339 | validation: 5.084735780993407]
	TIME [epoch: 9.72 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1894046722224845		[learning rate: 0.00654]
	Learning Rate: 0.00654002
	LOSS [training: 4.1894046722224845 | validation: 4.861386366849971]
	TIME [epoch: 9.72 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1717072517854366		[learning rate: 0.0065163]
	Learning Rate: 0.00651628
	LOSS [training: 4.1717072517854366 | validation: 5.554043411024519]
	TIME [epoch: 9.73 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.280733976066979		[learning rate: 0.0064926]
	Learning Rate: 0.00649264
	LOSS [training: 4.280733976066979 | validation: 4.9304861377072955]
	TIME [epoch: 9.73 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1806159838976225		[learning rate: 0.0064691]
	Learning Rate: 0.00646907
	LOSS [training: 4.1806159838976225 | validation: 4.931807089996101]
	TIME [epoch: 9.72 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0356146774912185		[learning rate: 0.0064456]
	Learning Rate: 0.0064456
	LOSS [training: 4.0356146774912185 | validation: 5.141049531792996]
	TIME [epoch: 9.72 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.092179473664197		[learning rate: 0.0064222]
	Learning Rate: 0.00642221
	LOSS [training: 4.092179473664197 | validation: 5.4262920437498865]
	TIME [epoch: 9.74 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.129670188698701		[learning rate: 0.0063989]
	Learning Rate: 0.0063989
	LOSS [training: 4.129670188698701 | validation: 5.105127878957093]
	TIME [epoch: 9.73 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.171243189974723		[learning rate: 0.0063757]
	Learning Rate: 0.00637568
	LOSS [training: 4.171243189974723 | validation: 5.781719976297178]
	TIME [epoch: 9.72 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.157722271174231		[learning rate: 0.0063525]
	Learning Rate: 0.00635254
	LOSS [training: 4.157722271174231 | validation: 5.0437757073319]
	TIME [epoch: 9.72 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.168990224103056		[learning rate: 0.0063295]
	Learning Rate: 0.00632949
	LOSS [training: 4.168990224103056 | validation: 4.906359177647735]
	TIME [epoch: 9.74 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.046801874880133		[learning rate: 0.0063065]
	Learning Rate: 0.00630652
	LOSS [training: 4.046801874880133 | validation: 4.938000261941199]
	TIME [epoch: 9.72 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.168333569647227		[learning rate: 0.0062836]
	Learning Rate: 0.00628363
	LOSS [training: 4.168333569647227 | validation: 4.887235740455897]
	TIME [epoch: 9.72 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.14580677674729		[learning rate: 0.0062608]
	Learning Rate: 0.00626082
	LOSS [training: 4.14580677674729 | validation: 5.3908078727762545]
	TIME [epoch: 9.72 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.091353982398195		[learning rate: 0.0062381]
	Learning Rate: 0.0062381
	LOSS [training: 4.091353982398195 | validation: 4.966463809990783]
	TIME [epoch: 9.75 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.07538152168768		[learning rate: 0.0062155]
	Learning Rate: 0.00621547
	LOSS [training: 4.07538152168768 | validation: 4.946462068092055]
	TIME [epoch: 9.72 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.119148794387807		[learning rate: 0.0061929]
	Learning Rate: 0.00619291
	LOSS [training: 4.119148794387807 | validation: 5.206072860134793]
	TIME [epoch: 9.71 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.272121652968833		[learning rate: 0.0061704]
	Learning Rate: 0.00617043
	LOSS [training: 4.272121652968833 | validation: 4.9137000391167565]
	TIME [epoch: 9.73 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.986235603190882		[learning rate: 0.006148]
	Learning Rate: 0.00614804
	LOSS [training: 3.986235603190882 | validation: 4.905312759022955]
	TIME [epoch: 9.73 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0875509343921		[learning rate: 0.0061257]
	Learning Rate: 0.00612573
	LOSS [training: 4.0875509343921 | validation: 5.459407203366943]
	TIME [epoch: 9.72 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.196109444730162		[learning rate: 0.0061035]
	Learning Rate: 0.0061035
	LOSS [training: 4.196109444730162 | validation: 4.896478469118046]
	TIME [epoch: 9.72 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.056619490826086		[learning rate: 0.0060814]
	Learning Rate: 0.00608135
	LOSS [training: 4.056619490826086 | validation: 5.181914382131818]
	TIME [epoch: 9.74 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.140818474744336		[learning rate: 0.0060593]
	Learning Rate: 0.00605928
	LOSS [training: 4.140818474744336 | validation: 4.8887836598883165]
	TIME [epoch: 9.72 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.013683567074017		[learning rate: 0.0060373]
	Learning Rate: 0.00603729
	LOSS [training: 4.013683567074017 | validation: 4.855367642127275]
	TIME [epoch: 9.72 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.985641451319097		[learning rate: 0.0060154]
	Learning Rate: 0.00601538
	LOSS [training: 3.985641451319097 | validation: 5.123045488552767]
	TIME [epoch: 9.72 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.943261243856432		[learning rate: 0.0059936]
	Learning Rate: 0.00599355
	LOSS [training: 3.943261243856432 | validation: 5.961962759418583]
	TIME [epoch: 9.74 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.374588652637523		[learning rate: 0.0059718]
	Learning Rate: 0.0059718
	LOSS [training: 4.374588652637523 | validation: 5.117022114468328]
	TIME [epoch: 9.71 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9938340342560976		[learning rate: 0.0059501]
	Learning Rate: 0.00595013
	LOSS [training: 3.9938340342560976 | validation: 4.896552174679177]
	TIME [epoch: 9.73 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.124196649888902		[learning rate: 0.0059285]
	Learning Rate: 0.00592853
	LOSS [training: 4.124196649888902 | validation: 4.848985683661952]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.981040333173617		[learning rate: 0.005907]
	Learning Rate: 0.00590702
	LOSS [training: 3.981040333173617 | validation: 4.856404729202607]
	TIME [epoch: 9.75 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.050964348255934		[learning rate: 0.0058856]
	Learning Rate: 0.00588558
	LOSS [training: 4.050964348255934 | validation: 5.216487014285412]
	TIME [epoch: 9.72 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.082069418271531		[learning rate: 0.0058642]
	Learning Rate: 0.00586422
	LOSS [training: 4.082069418271531 | validation: 4.831659320641294]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.01400782976591		[learning rate: 0.0058429]
	Learning Rate: 0.00584294
	LOSS [training: 4.01400782976591 | validation: 5.130396362964848]
	TIME [epoch: 9.74 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9657126629417787		[learning rate: 0.0058217]
	Learning Rate: 0.00582174
	LOSS [training: 3.9657126629417787 | validation: 5.209051380147671]
	TIME [epoch: 9.72 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.023055434227094		[learning rate: 0.0058006]
	Learning Rate: 0.00580061
	LOSS [training: 4.023055434227094 | validation: 4.945273285003223]
	TIME [epoch: 9.72 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.001747778338553		[learning rate: 0.0057796]
	Learning Rate: 0.00577956
	LOSS [training: 4.001747778338553 | validation: 4.903968369629778]
	TIME [epoch: 9.72 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1483737271453025		[learning rate: 0.0057586]
	Learning Rate: 0.00575859
	LOSS [training: 4.1483737271453025 | validation: 4.858458796082313]
	TIME [epoch: 9.74 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.329768509061242		[learning rate: 0.0057377]
	Learning Rate: 0.00573769
	LOSS [training: 4.329768509061242 | validation: 4.941803872191329]
	TIME [epoch: 9.72 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.190116104803565		[learning rate: 0.0057169]
	Learning Rate: 0.00571686
	LOSS [training: 4.190116104803565 | validation: 5.014525060430578]
	TIME [epoch: 9.71 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.026532534509245		[learning rate: 0.0056961]
	Learning Rate: 0.00569612
	LOSS [training: 4.026532534509245 | validation: 5.125409123422308]
	TIME [epoch: 9.72 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.092260850435029		[learning rate: 0.0056754]
	Learning Rate: 0.00567545
	LOSS [training: 4.092260850435029 | validation: 5.191820154641181]
	TIME [epoch: 9.74 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.981729824917823		[learning rate: 0.0056548]
	Learning Rate: 0.00565485
	LOSS [training: 3.981729824917823 | validation: 4.977653015160435]
	TIME [epoch: 9.71 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.084370519583645		[learning rate: 0.0056343]
	Learning Rate: 0.00563433
	LOSS [training: 4.084370519583645 | validation: 4.8664575755141835]
	TIME [epoch: 9.72 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9425739111109004		[learning rate: 0.0056139]
	Learning Rate: 0.00561388
	LOSS [training: 3.9425739111109004 | validation: 5.012272118847317]
	TIME [epoch: 9.73 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.005483985659788		[learning rate: 0.0055935]
	Learning Rate: 0.00559351
	LOSS [training: 4.005483985659788 | validation: 5.914148952571686]
	TIME [epoch: 9.72 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.4352382267865424		[learning rate: 0.0055732]
	Learning Rate: 0.00557321
	LOSS [training: 4.4352382267865424 | validation: 4.937136720567698]
	TIME [epoch: 9.72 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.116428041474914		[learning rate: 0.005553]
	Learning Rate: 0.00555298
	LOSS [training: 4.116428041474914 | validation: 5.5270483198230655]
	TIME [epoch: 9.72 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.211809845976318		[learning rate: 0.0055328]
	Learning Rate: 0.00553283
	LOSS [training: 4.211809845976318 | validation: 4.8249232011399945]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.977314703839145		[learning rate: 0.0055128]
	Learning Rate: 0.00551275
	LOSS [training: 3.977314703839145 | validation: 4.878720302457382]
	TIME [epoch: 9.72 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9350004012741238		[learning rate: 0.0054927]
	Learning Rate: 0.00549274
	LOSS [training: 3.9350004012741238 | validation: 4.842307060767255]
	TIME [epoch: 9.71 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9970787487738035		[learning rate: 0.0054728]
	Learning Rate: 0.00547281
	LOSS [training: 3.9970787487738035 | validation: 4.964703337933171]
	TIME [epoch: 9.71 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9224344349578546		[learning rate: 0.005453]
	Learning Rate: 0.00545295
	LOSS [training: 3.9224344349578546 | validation: 4.940832577086617]
	TIME [epoch: 9.73 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.259008687488373		[learning rate: 0.0054332]
	Learning Rate: 0.00543316
	LOSS [training: 4.259008687488373 | validation: 5.330904575191214]
	TIME [epoch: 9.71 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.157550275252169		[learning rate: 0.0054134]
	Learning Rate: 0.00541344
	LOSS [training: 4.157550275252169 | validation: 5.238951016148678]
	TIME [epoch: 9.71 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.073002952136761		[learning rate: 0.0053938]
	Learning Rate: 0.0053938
	LOSS [training: 4.073002952136761 | validation: 4.935307908405961]
	TIME [epoch: 9.72 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.018082676192236		[learning rate: 0.0053742]
	Learning Rate: 0.00537422
	LOSS [training: 4.018082676192236 | validation: 5.272677914099681]
	TIME [epoch: 9.72 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.031725138935829		[learning rate: 0.0053547]
	Learning Rate: 0.00535472
	LOSS [training: 4.031725138935829 | validation: 4.8443545705653905]
	TIME [epoch: 9.71 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9915973748896727		[learning rate: 0.0053353]
	Learning Rate: 0.00533529
	LOSS [training: 3.9915973748896727 | validation: 4.810510573036909]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.072395104494293		[learning rate: 0.0053159]
	Learning Rate: 0.00531593
	LOSS [training: 4.072395104494293 | validation: 4.925314149213166]
	TIME [epoch: 9.73 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9594479454346088		[learning rate: 0.0052966]
	Learning Rate: 0.00529663
	LOSS [training: 3.9594479454346088 | validation: 4.905298449810691]
	TIME [epoch: 9.71 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.917926267428392		[learning rate: 0.0052774]
	Learning Rate: 0.00527741
	LOSS [training: 3.917926267428392 | validation: 4.843255694961402]
	TIME [epoch: 9.71 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.991418602796042		[learning rate: 0.0052583]
	Learning Rate: 0.00525826
	LOSS [training: 3.991418602796042 | validation: 5.06000636349581]
	TIME [epoch: 9.71 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.033836637193466		[learning rate: 0.0052392]
	Learning Rate: 0.00523918
	LOSS [training: 4.033836637193466 | validation: 4.970201741858748]
	TIME [epoch: 9.73 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.006729754010522		[learning rate: 0.0052202]
	Learning Rate: 0.00522017
	LOSS [training: 4.006729754010522 | validation: 4.888110461931159]
	TIME [epoch: 9.71 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.977104926228627		[learning rate: 0.0052012]
	Learning Rate: 0.00520122
	LOSS [training: 3.977104926228627 | validation: 4.869134254141982]
	TIME [epoch: 9.71 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9018782273415837		[learning rate: 0.0051823]
	Learning Rate: 0.00518234
	LOSS [training: 3.9018782273415837 | validation: 4.9648695010038]
	TIME [epoch: 9.71 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9992706298139753		[learning rate: 0.0051635]
	Learning Rate: 0.00516354
	LOSS [training: 3.9992706298139753 | validation: 5.026332497478168]
	TIME [epoch: 9.74 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9893659681990727		[learning rate: 0.0051448]
	Learning Rate: 0.0051448
	LOSS [training: 3.9893659681990727 | validation: 4.833112891687387]
	TIME [epoch: 9.71 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.067983217067523		[learning rate: 0.0051261]
	Learning Rate: 0.00512613
	LOSS [training: 4.067983217067523 | validation: 4.862210448602109]
	TIME [epoch: 9.71 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1470697847103155		[learning rate: 0.0051075]
	Learning Rate: 0.00510753
	LOSS [training: 4.1470697847103155 | validation: 4.8399710128084745]
	TIME [epoch: 9.74 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9042018380304016		[learning rate: 0.005089]
	Learning Rate: 0.00508899
	LOSS [training: 3.9042018380304016 | validation: 4.886489559962798]
	TIME [epoch: 9.71 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.255494593009589		[learning rate: 0.0050705]
	Learning Rate: 0.00507052
	LOSS [training: 4.255494593009589 | validation: 5.150322196364807]
	TIME [epoch: 9.7 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9338230235470846		[learning rate: 0.0050521]
	Learning Rate: 0.00505212
	LOSS [training: 3.9338230235470846 | validation: 4.8443936100475256]
	TIME [epoch: 9.71 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.182486496065744		[learning rate: 0.0050338]
	Learning Rate: 0.00503379
	LOSS [training: 4.182486496065744 | validation: 5.267662265673793]
	TIME [epoch: 9.72 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0185076318194515		[learning rate: 0.0050155]
	Learning Rate: 0.00501552
	LOSS [training: 4.0185076318194515 | validation: 4.882707938836159]
	TIME [epoch: 9.72 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.95316425104854		[learning rate: 0.0049973]
	Learning Rate: 0.00499732
	LOSS [training: 3.95316425104854 | validation: 5.220072545057073]
	TIME [epoch: 9.71 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.036648755268024		[learning rate: 0.0049792]
	Learning Rate: 0.00497918
	LOSS [training: 4.036648755268024 | validation: 4.849796050509259]
	TIME [epoch: 9.71 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9414194178053124		[learning rate: 0.0049611]
	Learning Rate: 0.00496111
	LOSS [training: 3.9414194178053124 | validation: 4.854438815524536]
	TIME [epoch: 9.73 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9371233743864367		[learning rate: 0.0049431]
	Learning Rate: 0.00494311
	LOSS [training: 3.9371233743864367 | validation: 4.859508135873309]
	TIME [epoch: 9.71 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.943616886935793		[learning rate: 0.0049252]
	Learning Rate: 0.00492517
	LOSS [training: 3.943616886935793 | validation: 4.974603865389639]
	TIME [epoch: 9.7 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.876180418412349		[learning rate: 0.0049073]
	Learning Rate: 0.00490729
	LOSS [training: 3.876180418412349 | validation: 4.919675020966748]
	TIME [epoch: 9.73 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9626791700266972		[learning rate: 0.0048895]
	Learning Rate: 0.00488948
	LOSS [training: 3.9626791700266972 | validation: 5.326268110878625]
	TIME [epoch: 9.72 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.210944828767511		[learning rate: 0.0048717]
	Learning Rate: 0.00487174
	LOSS [training: 4.210944828767511 | validation: 4.8851349982388035]
	TIME [epoch: 9.71 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9485730750216192		[learning rate: 0.0048541]
	Learning Rate: 0.00485406
	LOSS [training: 3.9485730750216192 | validation: 4.815364085946721]
	TIME [epoch: 9.7 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.922582260166903		[learning rate: 0.0048364]
	Learning Rate: 0.00483645
	LOSS [training: 3.922582260166903 | validation: 5.20670418362401]
	TIME [epoch: 9.73 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.98397577732798		[learning rate: 0.0048189]
	Learning Rate: 0.00481889
	LOSS [training: 3.98397577732798 | validation: 4.886297994742812]
	TIME [epoch: 9.71 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.924717886352204		[learning rate: 0.0048014]
	Learning Rate: 0.00480141
	LOSS [training: 3.924717886352204 | validation: 4.8652998013001385]
	TIME [epoch: 9.71 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9863529722603723		[learning rate: 0.004784]
	Learning Rate: 0.00478398
	LOSS [training: 3.9863529722603723 | validation: 5.180463166394602]
	TIME [epoch: 9.7 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.001508142623323		[learning rate: 0.0047666]
	Learning Rate: 0.00476662
	LOSS [training: 4.001508142623323 | validation: 4.993271567054302]
	TIME [epoch: 9.73 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.915212014766736		[learning rate: 0.0047493]
	Learning Rate: 0.00474932
	LOSS [training: 3.915212014766736 | validation: 4.830605082334532]
	TIME [epoch: 9.72 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.83520606143058		[learning rate: 0.0047321]
	Learning Rate: 0.00473209
	LOSS [training: 3.83520606143058 | validation: 5.244701070736765]
	TIME [epoch: 9.71 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.224843384656531		[learning rate: 0.0047149]
	Learning Rate: 0.00471491
	LOSS [training: 4.224843384656531 | validation: 4.85541883161782]
	TIME [epoch: 9.71 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1144058319674714		[learning rate: 0.0046978]
	Learning Rate: 0.0046978
	LOSS [training: 4.1144058319674714 | validation: 5.045501297886718]
	TIME [epoch: 9.73 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9311958184109392		[learning rate: 0.0046808]
	Learning Rate: 0.00468075
	LOSS [training: 3.9311958184109392 | validation: 4.898900005776944]
	TIME [epoch: 9.71 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9015483605940275		[learning rate: 0.0046638]
	Learning Rate: 0.00466377
	LOSS [training: 3.9015483605940275 | validation: 5.145899954209109]
	TIME [epoch: 9.71 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.013668670220366		[learning rate: 0.0046468]
	Learning Rate: 0.00464684
	LOSS [training: 4.013668670220366 | validation: 4.939431843785581]
	TIME [epoch: 9.72 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.905288199164464		[learning rate: 0.00463]
	Learning Rate: 0.00462998
	LOSS [training: 3.905288199164464 | validation: 4.948810768890678]
	TIME [epoch: 9.72 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.976811185195365		[learning rate: 0.0046132]
	Learning Rate: 0.00461318
	LOSS [training: 3.976811185195365 | validation: 4.820032328836892]
	TIME [epoch: 9.71 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9507331775404237		[learning rate: 0.0045964]
	Learning Rate: 0.00459643
	LOSS [training: 3.9507331775404237 | validation: 5.256313493552446]
	TIME [epoch: 9.71 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9851225625649733		[learning rate: 0.0045798]
	Learning Rate: 0.00457975
	LOSS [training: 3.9851225625649733 | validation: 5.258749184646464]
	TIME [epoch: 9.73 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0399869581417205		[learning rate: 0.0045631]
	Learning Rate: 0.00456313
	LOSS [training: 4.0399869581417205 | validation: 4.862541006436776]
	TIME [epoch: 9.71 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9986663106780234		[learning rate: 0.0045466]
	Learning Rate: 0.00454657
	LOSS [training: 3.9986663106780234 | validation: 4.903488417718997]
	TIME [epoch: 9.71 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9383747219237284		[learning rate: 0.0045301]
	Learning Rate: 0.00453007
	LOSS [training: 3.9383747219237284 | validation: 4.903280519154189]
	TIME [epoch: 9.71 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9739438925554795		[learning rate: 0.0045136]
	Learning Rate: 0.00451363
	LOSS [training: 3.9739438925554795 | validation: 4.894698992806816]
	TIME [epoch: 9.74 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.044966590651207		[learning rate: 0.0044973]
	Learning Rate: 0.00449725
	LOSS [training: 4.044966590651207 | validation: 4.987481164188248]
	TIME [epoch: 9.71 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9351973570404426		[learning rate: 0.0044809]
	Learning Rate: 0.00448093
	LOSS [training: 3.9351973570404426 | validation: 5.048008699546722]
	TIME [epoch: 9.71 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9436419903624094		[learning rate: 0.0044647]
	Learning Rate: 0.00446467
	LOSS [training: 3.9436419903624094 | validation: 4.918362528638938]
	TIME [epoch: 9.72 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.95222538921828		[learning rate: 0.0044485]
	Learning Rate: 0.00444847
	LOSS [training: 3.95222538921828 | validation: 4.827485076442599]
	TIME [epoch: 9.73 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.941158754822723		[learning rate: 0.0044323]
	Learning Rate: 0.00443232
	LOSS [training: 3.941158754822723 | validation: 4.989162209168979]
	TIME [epoch: 9.71 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9741293260469015		[learning rate: 0.0044162]
	Learning Rate: 0.00441624
	LOSS [training: 3.9741293260469015 | validation: 4.807731755731254]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8992176055079972		[learning rate: 0.0044002]
	Learning Rate: 0.00440021
	LOSS [training: 3.8992176055079972 | validation: 4.864366011127957]
	TIME [epoch: 9.73 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9815492906123553		[learning rate: 0.0043842]
	Learning Rate: 0.00438424
	LOSS [training: 3.9815492906123553 | validation: 4.897129535004292]
	TIME [epoch: 9.71 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.911300744052441		[learning rate: 0.0043683]
	Learning Rate: 0.00436833
	LOSS [training: 3.911300744052441 | validation: 5.139398818851429]
	TIME [epoch: 9.71 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9605467153024634		[learning rate: 0.0043525]
	Learning Rate: 0.00435248
	LOSS [training: 3.9605467153024634 | validation: 4.891765822042806]
	TIME [epoch: 9.71 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9469902648935444		[learning rate: 0.0043367]
	Learning Rate: 0.00433668
	LOSS [training: 3.9469902648935444 | validation: 4.89437316064208]
	TIME [epoch: 9.72 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.942122264170569		[learning rate: 0.0043209]
	Learning Rate: 0.00432095
	LOSS [training: 3.942122264170569 | validation: 5.031877204860727]
	TIME [epoch: 9.71 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.964264028823224		[learning rate: 0.0043053]
	Learning Rate: 0.00430527
	LOSS [training: 3.964264028823224 | validation: 4.96489869572358]
	TIME [epoch: 9.7 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.884154100355481		[learning rate: 0.0042896]
	Learning Rate: 0.00428964
	LOSS [training: 3.884154100355481 | validation: 4.940526949497256]
	TIME [epoch: 9.7 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9732710893410434		[learning rate: 0.0042741]
	Learning Rate: 0.00427407
	LOSS [training: 3.9732710893410434 | validation: 4.862403397609952]
	TIME [epoch: 9.72 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.043234938577365		[learning rate: 0.0042586]
	Learning Rate: 0.00425856
	LOSS [training: 4.043234938577365 | validation: 4.848572205479687]
	TIME [epoch: 9.7 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.111756001127602		[learning rate: 0.0042431]
	Learning Rate: 0.00424311
	LOSS [training: 4.111756001127602 | validation: 5.053213302203757]
	TIME [epoch: 9.71 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9951741371995957		[learning rate: 0.0042277]
	Learning Rate: 0.00422771
	LOSS [training: 3.9951741371995957 | validation: 4.897784164841292]
	TIME [epoch: 9.72 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.250798404385703		[learning rate: 0.0042124]
	Learning Rate: 0.00421237
	LOSS [training: 4.250798404385703 | validation: 4.8361597856742735]
	TIME [epoch: 9.72 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9319124588111456		[learning rate: 0.0041971]
	Learning Rate: 0.00419708
	LOSS [training: 3.9319124588111456 | validation: 4.83124542686666]
	TIME [epoch: 9.72 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9435584640786168		[learning rate: 0.0041818]
	Learning Rate: 0.00418185
	LOSS [training: 3.9435584640786168 | validation: 4.895535684653654]
	TIME [epoch: 9.71 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.048292150563347		[learning rate: 0.0041667]
	Learning Rate: 0.00416667
	LOSS [training: 4.048292150563347 | validation: 4.955688487247003]
	TIME [epoch: 9.72 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9828093308996317		[learning rate: 0.0041516]
	Learning Rate: 0.00415155
	LOSS [training: 3.9828093308996317 | validation: 5.049688398769712]
	TIME [epoch: 9.7 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9709072731149577		[learning rate: 0.0041365]
	Learning Rate: 0.00413649
	LOSS [training: 3.9709072731149577 | validation: 4.82029567646398]
	TIME [epoch: 9.69 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.045272827455953		[learning rate: 0.0041215]
	Learning Rate: 0.00412147
	LOSS [training: 4.045272827455953 | validation: 4.911329176671966]
	TIME [epoch: 9.7 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.057407025234733		[learning rate: 0.0041065]
	Learning Rate: 0.00410652
	LOSS [training: 4.057407025234733 | validation: 5.074363829628911]
	TIME [epoch: 9.74 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9512528908124778		[learning rate: 0.0040916]
	Learning Rate: 0.00409161
	LOSS [training: 3.9512528908124778 | validation: 5.0373399287259]
	TIME [epoch: 9.7 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.934535136877347		[learning rate: 0.0040768]
	Learning Rate: 0.00407677
	LOSS [training: 3.934535136877347 | validation: 4.959874656060937]
	TIME [epoch: 9.71 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.942876824897714		[learning rate: 0.004062]
	Learning Rate: 0.00406197
	LOSS [training: 3.942876824897714 | validation: 4.8990226603491065]
	TIME [epoch: 9.71 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.940011564760142		[learning rate: 0.0040472]
	Learning Rate: 0.00404723
	LOSS [training: 3.940011564760142 | validation: 4.916416117988243]
	TIME [epoch: 9.73 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9315721252414244		[learning rate: 0.0040325]
	Learning Rate: 0.00403254
	LOSS [training: 3.9315721252414244 | validation: 4.814932552030041]
	TIME [epoch: 9.7 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.003597853767678		[learning rate: 0.0040179]
	Learning Rate: 0.00401791
	LOSS [training: 4.003597853767678 | validation: 4.922071459152638]
	TIME [epoch: 9.7 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9478072954359744		[learning rate: 0.0040033]
	Learning Rate: 0.00400333
	LOSS [training: 3.9478072954359744 | validation: 4.821883031155678]
	TIME [epoch: 9.71 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9067648533612465		[learning rate: 0.0039888]
	Learning Rate: 0.0039888
	LOSS [training: 3.9067648533612465 | validation: 5.015299643406673]
	TIME [epoch: 9.7 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.968943008481916		[learning rate: 0.0039743]
	Learning Rate: 0.00397432
	LOSS [training: 3.968943008481916 | validation: 4.8897311828372425]
	TIME [epoch: 9.71 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.888913254977737		[learning rate: 0.0039599]
	Learning Rate: 0.0039599
	LOSS [training: 3.888913254977737 | validation: 5.041629553232152]
	TIME [epoch: 9.7 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.951884542279687		[learning rate: 0.0039455]
	Learning Rate: 0.00394553
	LOSS [training: 3.951884542279687 | validation: 4.868611078777648]
	TIME [epoch: 9.73 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.967799767194026		[learning rate: 0.0039312]
	Learning Rate: 0.00393121
	LOSS [training: 3.967799767194026 | validation: 4.860383474236464]
	TIME [epoch: 9.71 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.908716336637659		[learning rate: 0.0039169]
	Learning Rate: 0.00391694
	LOSS [training: 3.908716336637659 | validation: 5.158015121659485]
	TIME [epoch: 9.7 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9706389609301835		[learning rate: 0.0039027]
	Learning Rate: 0.00390273
	LOSS [training: 3.9706389609301835 | validation: 4.828315660150674]
	TIME [epoch: 9.7 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.906008946311547		[learning rate: 0.0038886]
	Learning Rate: 0.00388857
	LOSS [training: 3.906008946311547 | validation: 5.161587115596802]
	TIME [epoch: 9.73 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.959723908154461		[learning rate: 0.0038745]
	Learning Rate: 0.00387445
	LOSS [training: 3.959723908154461 | validation: 5.288112856325085]
	TIME [epoch: 9.71 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0723279780376265		[learning rate: 0.0038604]
	Learning Rate: 0.00386039
	LOSS [training: 4.0723279780376265 | validation: 4.871820201628449]
	TIME [epoch: 9.71 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.884530243307659		[learning rate: 0.0038464]
	Learning Rate: 0.00384638
	LOSS [training: 3.884530243307659 | validation: 4.809320014430963]
	TIME [epoch: 9.72 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.901080112354454		[learning rate: 0.0038324]
	Learning Rate: 0.00383242
	LOSS [training: 3.901080112354454 | validation: 5.013076767564654]
	TIME [epoch: 9.72 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.965528113349759		[learning rate: 0.0038185]
	Learning Rate: 0.00381852
	LOSS [training: 3.965528113349759 | validation: 5.013385021513907]
	TIME [epoch: 9.7 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.040128211288815		[learning rate: 0.0038047]
	Learning Rate: 0.00380466
	LOSS [training: 4.040128211288815 | validation: 5.064097350056305]
	TIME [epoch: 9.71 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.949091635013461		[learning rate: 0.0037909]
	Learning Rate: 0.00379085
	LOSS [training: 3.949091635013461 | validation: 4.832357272524226]
	TIME [epoch: 9.72 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.858025380301528		[learning rate: 0.0037771]
	Learning Rate: 0.00377709
	LOSS [training: 3.858025380301528 | validation: 4.875612675515368]
	TIME [epoch: 9.72 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.925470601049001		[learning rate: 0.0037634]
	Learning Rate: 0.00376339
	LOSS [training: 3.925470601049001 | validation: 4.848672489413911]
	TIME [epoch: 9.7 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.904141424640012		[learning rate: 0.0037497]
	Learning Rate: 0.00374973
	LOSS [training: 3.904141424640012 | validation: 4.891221547739043]
	TIME [epoch: 9.73 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8981083172519		[learning rate: 0.0037361]
	Learning Rate: 0.00373612
	LOSS [training: 3.8981083172519 | validation: 4.823860156074487]
	TIME [epoch: 9.73 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.991419972823035		[learning rate: 0.0037226]
	Learning Rate: 0.00372256
	LOSS [training: 3.991419972823035 | validation: 5.264845516775681]
	TIME [epoch: 9.7 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9762847779773445		[learning rate: 0.0037091]
	Learning Rate: 0.00370905
	LOSS [training: 3.9762847779773445 | validation: 4.8879953188617815]
	TIME [epoch: 9.71 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8890674709465545		[learning rate: 0.0036956]
	Learning Rate: 0.00369559
	LOSS [training: 3.8890674709465545 | validation: 4.8621457089722995]
	TIME [epoch: 9.71 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8946792006855864		[learning rate: 0.0036822]
	Learning Rate: 0.00368218
	LOSS [training: 3.8946792006855864 | validation: 4.923278446734249]
	TIME [epoch: 9.73 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9750355019667665		[learning rate: 0.0036688]
	Learning Rate: 0.00366882
	LOSS [training: 3.9750355019667665 | validation: 5.005882171960589]
	TIME [epoch: 9.71 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.86406126841879		[learning rate: 0.0036555]
	Learning Rate: 0.0036555
	LOSS [training: 3.86406126841879 | validation: 4.99627988215925]
	TIME [epoch: 9.7 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.915742415926192		[learning rate: 0.0036422]
	Learning Rate: 0.00364224
	LOSS [training: 3.915742415926192 | validation: 4.9677738128248805]
	TIME [epoch: 9.72 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.073120402748517		[learning rate: 0.003629]
	Learning Rate: 0.00362902
	LOSS [training: 4.073120402748517 | validation: 4.9581364089841875]
	TIME [epoch: 9.71 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8951947984825233		[learning rate: 0.0036159]
	Learning Rate: 0.00361585
	LOSS [training: 3.8951947984825233 | validation: 4.864168644183764]
	TIME [epoch: 9.72 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8850298034822224		[learning rate: 0.0036027]
	Learning Rate: 0.00360273
	LOSS [training: 3.8850298034822224 | validation: 5.014523912875651]
	TIME [epoch: 9.71 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9380802682403067		[learning rate: 0.0035897]
	Learning Rate: 0.00358965
	LOSS [training: 3.9380802682403067 | validation: 4.81591011640951]
	TIME [epoch: 9.71 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8525594705482087		[learning rate: 0.0035766]
	Learning Rate: 0.00357663
	LOSS [training: 3.8525594705482087 | validation: 4.889096856304269]
	TIME [epoch: 9.71 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8557834039793164		[learning rate: 0.0035636]
	Learning Rate: 0.00356365
	LOSS [training: 3.8557834039793164 | validation: 4.859612881902445]
	TIME [epoch: 9.72 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.884220519788699		[learning rate: 0.0035507]
	Learning Rate: 0.00355072
	LOSS [training: 3.884220519788699 | validation: 4.8304683906580514]
	TIME [epoch: 9.71 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8714262442311367		[learning rate: 0.0035378]
	Learning Rate: 0.00353783
	LOSS [training: 3.8714262442311367 | validation: 5.048127417232863]
	TIME [epoch: 9.72 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9584599340879363		[learning rate: 0.003525]
	Learning Rate: 0.00352499
	LOSS [training: 3.9584599340879363 | validation: 4.881089711712224]
	TIME [epoch: 9.72 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.920747975263511		[learning rate: 0.0035122]
	Learning Rate: 0.0035122
	LOSS [training: 3.920747975263511 | validation: 4.890412142673477]
	TIME [epoch: 9.72 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.970116922945318		[learning rate: 0.0034995]
	Learning Rate: 0.00349945
	LOSS [training: 3.970116922945318 | validation: 4.944714200194803]
	TIME [epoch: 9.72 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.971165122767161		[learning rate: 0.0034868]
	Learning Rate: 0.00348675
	LOSS [training: 3.971165122767161 | validation: 4.991819715661123]
	TIME [epoch: 9.74 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9437723508617273		[learning rate: 0.0034741]
	Learning Rate: 0.0034741
	LOSS [training: 3.9437723508617273 | validation: 4.882959814212306]
	TIME [epoch: 9.71 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1701721434048675		[learning rate: 0.0034615]
	Learning Rate: 0.00346149
	LOSS [training: 4.1701721434048675 | validation: 4.895380452758748]
	TIME [epoch: 9.7 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8391707804324184		[learning rate: 0.0034489]
	Learning Rate: 0.00344893
	LOSS [training: 3.8391707804324184 | validation: 5.148921207948643]
	TIME [epoch: 9.74 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9478033433435464		[learning rate: 0.0034364]
	Learning Rate: 0.00343641
	LOSS [training: 3.9478033433435464 | validation: 4.867349360602488]
	TIME [epoch: 9.72 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8390501979734126		[learning rate: 0.0034239]
	Learning Rate: 0.00342394
	LOSS [training: 3.8390501979734126 | validation: 5.058574227838512]
	TIME [epoch: 9.71 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.039428620520017		[learning rate: 0.0034115]
	Learning Rate: 0.00341152
	LOSS [training: 4.039428620520017 | validation: 4.9867610805915055]
	TIME [epoch: 9.72 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9017123810184637		[learning rate: 0.0033991]
	Learning Rate: 0.00339914
	LOSS [training: 3.9017123810184637 | validation: 4.852871782546125]
	TIME [epoch: 9.74 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.909003865993911		[learning rate: 0.0033868]
	Learning Rate: 0.0033868
	LOSS [training: 3.909003865993911 | validation: 4.92543693048136]
	TIME [epoch: 9.72 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8835087631733813		[learning rate: 0.0033745]
	Learning Rate: 0.00337451
	LOSS [training: 3.8835087631733813 | validation: 5.0536276535519775]
	TIME [epoch: 9.71 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.886728345158206		[learning rate: 0.0033623]
	Learning Rate: 0.00336226
	LOSS [training: 3.886728345158206 | validation: 4.878225743540859]
	TIME [epoch: 9.72 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8577770816972063		[learning rate: 0.0033501]
	Learning Rate: 0.00335006
	LOSS [training: 3.8577770816972063 | validation: 4.887126127664103]
	TIME [epoch: 9.73 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.892625832709269		[learning rate: 0.0033379]
	Learning Rate: 0.0033379
	LOSS [training: 3.892625832709269 | validation: 4.835882077289797]
	TIME [epoch: 9.72 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.922957095442564		[learning rate: 0.0033258]
	Learning Rate: 0.00332579
	LOSS [training: 3.922957095442564 | validation: 4.855286093319034]
	TIME [epoch: 9.7 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9026873239352637		[learning rate: 0.0033137]
	Learning Rate: 0.00331372
	LOSS [training: 3.9026873239352637 | validation: 4.900687876676957]
	TIME [epoch: 9.73 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8577703560233365		[learning rate: 0.0033017]
	Learning Rate: 0.00330169
	LOSS [training: 3.8577703560233365 | validation: 4.846948527440766]
	TIME [epoch: 9.72 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8589190189925064		[learning rate: 0.0032897]
	Learning Rate: 0.00328971
	LOSS [training: 3.8589190189925064 | validation: 5.156364193383895]
	TIME [epoch: 9.71 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9641882190733426		[learning rate: 0.0032778]
	Learning Rate: 0.00327777
	LOSS [training: 3.9641882190733426 | validation: 4.865476833193432]
	TIME [epoch: 9.71 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8996168166722227		[learning rate: 0.0032659]
	Learning Rate: 0.00326588
	LOSS [training: 3.8996168166722227 | validation: 4.975899892678193]
	TIME [epoch: 9.73 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.918258306607767		[learning rate: 0.003254]
	Learning Rate: 0.00325403
	LOSS [training: 3.918258306607767 | validation: 4.932808217955561]
	TIME [epoch: 9.71 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9035649191026542		[learning rate: 0.0032422]
	Learning Rate: 0.00324222
	LOSS [training: 3.9035649191026542 | validation: 4.891140118830375]
	TIME [epoch: 9.71 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8608719410367334		[learning rate: 0.0032305]
	Learning Rate: 0.00323045
	LOSS [training: 3.8608719410367334 | validation: 4.894254827054564]
	TIME [epoch: 9.71 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8673281498934338		[learning rate: 0.0032187]
	Learning Rate: 0.00321873
	LOSS [training: 3.8673281498934338 | validation: 4.86548890170304]
	TIME [epoch: 9.73 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.921812982221437		[learning rate: 0.003207]
	Learning Rate: 0.00320705
	LOSS [training: 3.921812982221437 | validation: 4.917083262749105]
	TIME [epoch: 9.71 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9260387583933523		[learning rate: 0.0031954]
	Learning Rate: 0.00319541
	LOSS [training: 3.9260387583933523 | validation: 4.833831641970171]
	TIME [epoch: 9.71 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.850140908820257		[learning rate: 0.0031838]
	Learning Rate: 0.00318381
	LOSS [training: 3.850140908820257 | validation: 4.886920469009973]
	TIME [epoch: 9.71 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8469337488048985		[learning rate: 0.0031723]
	Learning Rate: 0.00317226
	LOSS [training: 3.8469337488048985 | validation: 4.897404496433439]
	TIME [epoch: 9.73 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8998832927172784		[learning rate: 0.0031607]
	Learning Rate: 0.00316075
	LOSS [training: 3.8998832927172784 | validation: 4.899698047065726]
	TIME [epoch: 9.71 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.894495076930776		[learning rate: 0.0031493]
	Learning Rate: 0.00314927
	LOSS [training: 3.894495076930776 | validation: 4.870870998964569]
	TIME [epoch: 9.72 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.913499345082026		[learning rate: 0.0031378]
	Learning Rate: 0.00313785
	LOSS [training: 3.913499345082026 | validation: 4.880835025475562]
	TIME [epoch: 9.73 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.034612420071552		[learning rate: 0.0031265]
	Learning Rate: 0.00312646
	LOSS [training: 4.034612420071552 | validation: 4.956164909680425]
	TIME [epoch: 9.72 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9210083495908443		[learning rate: 0.0031151]
	Learning Rate: 0.00311511
	LOSS [training: 3.9210083495908443 | validation: 4.920584084793906]
	TIME [epoch: 9.71 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8701561272722116		[learning rate: 0.0031038]
	Learning Rate: 0.00310381
	LOSS [training: 3.8701561272722116 | validation: 4.980005561520397]
	TIME [epoch: 9.71 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.914819704271127		[learning rate: 0.0030925]
	Learning Rate: 0.00309254
	LOSS [training: 3.914819704271127 | validation: 4.935948785265935]
	TIME [epoch: 9.73 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.858430039269055		[learning rate: 0.0030813]
	Learning Rate: 0.00308132
	LOSS [training: 3.858430039269055 | validation: 4.833635414137829]
	TIME [epoch: 9.72 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8986141536560552		[learning rate: 0.0030701]
	Learning Rate: 0.00307014
	LOSS [training: 3.8986141536560552 | validation: 4.834448127659141]
	TIME [epoch: 9.72 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8763193343145588		[learning rate: 0.003059]
	Learning Rate: 0.003059
	LOSS [training: 3.8763193343145588 | validation: 4.921642898421502]
	TIME [epoch: 9.72 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8725673978578867		[learning rate: 0.0030479]
	Learning Rate: 0.0030479
	LOSS [training: 3.8725673978578867 | validation: 4.840249318338116]
	TIME [epoch: 9.73 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.896124737984529		[learning rate: 0.0030368]
	Learning Rate: 0.00303683
	LOSS [training: 3.896124737984529 | validation: 4.824980126348507]
	TIME [epoch: 9.72 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8828401278779254		[learning rate: 0.0030258]
	Learning Rate: 0.00302581
	LOSS [training: 3.8828401278779254 | validation: 4.837406244009798]
	TIME [epoch: 9.72 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8839639741964427		[learning rate: 0.0030148]
	Learning Rate: 0.00301483
	LOSS [training: 3.8839639741964427 | validation: 4.869065715997456]
	TIME [epoch: 9.73 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.892592416865598		[learning rate: 0.0030039]
	Learning Rate: 0.00300389
	LOSS [training: 3.892592416865598 | validation: 4.852376771347898]
	TIME [epoch: 9.72 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.838355214060099		[learning rate: 0.002993]
	Learning Rate: 0.00299299
	LOSS [training: 3.838355214060099 | validation: 4.819839720524792]
	TIME [epoch: 9.71 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.89074126936094		[learning rate: 0.0029821]
	Learning Rate: 0.00298213
	LOSS [training: 3.89074126936094 | validation: 4.832718135412478]
	TIME [epoch: 9.71 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8791983409108175		[learning rate: 0.0029713]
	Learning Rate: 0.00297131
	LOSS [training: 3.8791983409108175 | validation: 4.900775205514712]
	TIME [epoch: 9.74 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9221315881713275		[learning rate: 0.0029605]
	Learning Rate: 0.00296052
	LOSS [training: 3.9221315881713275 | validation: 4.817267966431365]
	TIME [epoch: 9.71 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8631803440049373		[learning rate: 0.0029498]
	Learning Rate: 0.00294978
	LOSS [training: 3.8631803440049373 | validation: 4.885649447381423]
	TIME [epoch: 9.71 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8616069208367008		[learning rate: 0.0029391]
	Learning Rate: 0.00293907
	LOSS [training: 3.8616069208367008 | validation: 4.870774911795819]
	TIME [epoch: 9.71 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.843938139636344		[learning rate: 0.0029284]
	Learning Rate: 0.00292841
	LOSS [training: 3.843938139636344 | validation: 4.898692711721489]
	TIME [epoch: 9.73 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8629583504344525		[learning rate: 0.0029178]
	Learning Rate: 0.00291778
	LOSS [training: 3.8629583504344525 | validation: 4.963106886223496]
	TIME [epoch: 9.71 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.889295035479841		[learning rate: 0.0029072]
	Learning Rate: 0.00290719
	LOSS [training: 3.889295035479841 | validation: 4.889427808786407]
	TIME [epoch: 9.7 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.871907148460438		[learning rate: 0.0028966]
	Learning Rate: 0.00289664
	LOSS [training: 3.871907148460438 | validation: 4.833742041907291]
	TIME [epoch: 9.71 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8625412500818945		[learning rate: 0.0028861]
	Learning Rate: 0.00288613
	LOSS [training: 3.8625412500818945 | validation: 4.814384490998679]
	TIME [epoch: 9.73 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.844500521713126		[learning rate: 0.0028757]
	Learning Rate: 0.00287566
	LOSS [training: 3.844500521713126 | validation: 5.3990583419805285]
	TIME [epoch: 9.71 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.042702971989404		[learning rate: 0.0028652]
	Learning Rate: 0.00286522
	LOSS [training: 4.042702971989404 | validation: 4.849525067027227]
	TIME [epoch: 9.71 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.85906537521861		[learning rate: 0.0028548]
	Learning Rate: 0.00285482
	LOSS [training: 3.85906537521861 | validation: 4.846728244442267]
	TIME [epoch: 9.73 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8420165372536905		[learning rate: 0.0028445]
	Learning Rate: 0.00284446
	LOSS [training: 3.8420165372536905 | validation: 4.840652167625359]
	TIME [epoch: 9.71 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8563866841133874		[learning rate: 0.0028341]
	Learning Rate: 0.00283414
	LOSS [training: 3.8563866841133874 | validation: 4.863644137275342]
	TIME [epoch: 9.7 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8307293948511663		[learning rate: 0.0028239]
	Learning Rate: 0.00282385
	LOSS [training: 3.8307293948511663 | validation: 4.81883321975027]
	TIME [epoch: 9.72 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.850586687752636		[learning rate: 0.0028136]
	Learning Rate: 0.00281361
	LOSS [training: 3.850586687752636 | validation: 4.839274864480871]
	TIME [epoch: 9.73 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9256076475680297		[learning rate: 0.0028034]
	Learning Rate: 0.00280339
	LOSS [training: 3.9256076475680297 | validation: 4.926470023945438]
	TIME [epoch: 9.71 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8995078833723524		[learning rate: 0.0027932]
	Learning Rate: 0.00279322
	LOSS [training: 3.8995078833723524 | validation: 4.8388596009383775]
	TIME [epoch: 9.7 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8753450376894607		[learning rate: 0.0027831]
	Learning Rate: 0.00278308
	LOSS [training: 3.8753450376894607 | validation: 5.060668544947375]
	TIME [epoch: 9.71 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.987881811480075		[learning rate: 0.002773]
	Learning Rate: 0.00277298
	LOSS [training: 3.987881811480075 | validation: 4.921173978688853]
	TIME [epoch: 9.73 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.83503528603932		[learning rate: 0.0027629]
	Learning Rate: 0.00276292
	LOSS [training: 3.83503528603932 | validation: 4.84738054596879]
	TIME [epoch: 9.71 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.845755667049697		[learning rate: 0.0027529]
	Learning Rate: 0.00275289
	LOSS [training: 3.845755667049697 | validation: 4.83121148886772]
	TIME [epoch: 9.71 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8683652306976115		[learning rate: 0.0027429]
	Learning Rate: 0.0027429
	LOSS [training: 3.8683652306976115 | validation: 5.152065938749989]
	TIME [epoch: 9.71 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.014089300820684		[learning rate: 0.0027329]
	Learning Rate: 0.00273295
	LOSS [training: 4.014089300820684 | validation: 4.809926533754594]
	TIME [epoch: 9.74 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8661303086059937		[learning rate: 0.002723]
	Learning Rate: 0.00272303
	LOSS [training: 3.8661303086059937 | validation: 4.851588638703776]
	TIME [epoch: 9.72 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8305510929228204		[learning rate: 0.0027131]
	Learning Rate: 0.00271315
	LOSS [training: 3.8305510929228204 | validation: 4.849952383317342]
	TIME [epoch: 9.72 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8630287103644876		[learning rate: 0.0027033]
	Learning Rate: 0.0027033
	LOSS [training: 3.8630287103644876 | validation: 4.8302082095381715]
	TIME [epoch: 9.73 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8386078653722704		[learning rate: 0.0026935]
	Learning Rate: 0.00269349
	LOSS [training: 3.8386078653722704 | validation: 4.905999848511126]
	TIME [epoch: 9.72 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.886107778720789		[learning rate: 0.0026837]
	Learning Rate: 0.00268372
	LOSS [training: 3.886107778720789 | validation: 4.881382115125539]
	TIME [epoch: 9.72 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8487002114633087		[learning rate: 0.002674]
	Learning Rate: 0.00267398
	LOSS [training: 3.8487002114633087 | validation: 4.913879765361513]
	TIME [epoch: 9.72 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.872866914897579		[learning rate: 0.0026643]
	Learning Rate: 0.00266427
	LOSS [training: 3.872866914897579 | validation: 4.846748678142428]
	TIME [epoch: 9.74 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8617523782755176		[learning rate: 0.0026546]
	Learning Rate: 0.00265461
	LOSS [training: 3.8617523782755176 | validation: 4.847829922065131]
	TIME [epoch: 9.72 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9456867332196746		[learning rate: 0.002645]
	Learning Rate: 0.00264497
	LOSS [training: 3.9456867332196746 | validation: 4.827073190564981]
	TIME [epoch: 9.73 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.828669431842949		[learning rate: 0.0026354]
	Learning Rate: 0.00263537
	LOSS [training: 3.828669431842949 | validation: 4.813078632577108]
	TIME [epoch: 9.71 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.849699491922442		[learning rate: 0.0026258]
	Learning Rate: 0.00262581
	LOSS [training: 3.849699491922442 | validation: 4.851190809298386]
	TIME [epoch: 9.75 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9533384781064336		[learning rate: 0.0026163]
	Learning Rate: 0.00261628
	LOSS [training: 3.9533384781064336 | validation: 4.969079544679767]
	TIME [epoch: 9.72 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.873695211160207		[learning rate: 0.0026068]
	Learning Rate: 0.00260679
	LOSS [training: 3.873695211160207 | validation: 4.849120025350017]
	TIME [epoch: 9.71 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.874321804648691		[learning rate: 0.0025973]
	Learning Rate: 0.00259733
	LOSS [training: 3.874321804648691 | validation: 5.024970270071362]
	TIME [epoch: 9.73 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9341367307246555		[learning rate: 0.0025879]
	Learning Rate: 0.0025879
	LOSS [training: 3.9341367307246555 | validation: 4.8209408221981676]
	TIME [epoch: 9.73 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8632671663860463		[learning rate: 0.0025785]
	Learning Rate: 0.00257851
	LOSS [training: 3.8632671663860463 | validation: 4.854708292906055]
	TIME [epoch: 9.72 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.994540597470221		[learning rate: 0.0025691]
	Learning Rate: 0.00256915
	LOSS [training: 3.994540597470221 | validation: 4.861294862271454]
	TIME [epoch: 9.71 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8335605640284185		[learning rate: 0.0025598]
	Learning Rate: 0.00255983
	LOSS [training: 3.8335605640284185 | validation: 4.880810638398582]
	TIME [epoch: 9.75 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.906852702333526		[learning rate: 0.0025505]
	Learning Rate: 0.00255054
	LOSS [training: 3.906852702333526 | validation: 4.985590856192416]
	TIME [epoch: 9.72 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9133353057786637		[learning rate: 0.0025413]
	Learning Rate: 0.00254128
	LOSS [training: 3.9133353057786637 | validation: 4.845457771340853]
	TIME [epoch: 9.73 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.854097111349147		[learning rate: 0.0025321]
	Learning Rate: 0.00253206
	LOSS [training: 3.854097111349147 | validation: 4.850595347778416]
	TIME [epoch: 9.7 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.82250168906178		[learning rate: 0.0025229]
	Learning Rate: 0.00252287
	LOSS [training: 3.82250168906178 | validation: 4.860776009641085]
	TIME [epoch: 9.73 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.87162502874014		[learning rate: 0.0025137]
	Learning Rate: 0.00251371
	LOSS [training: 3.87162502874014 | validation: 4.94324313393855]
	TIME [epoch: 9.72 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8956852306377647		[learning rate: 0.0025046]
	Learning Rate: 0.00250459
	LOSS [training: 3.8956852306377647 | validation: 4.815112647726852]
	TIME [epoch: 9.72 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.835749350695501		[learning rate: 0.0024955]
	Learning Rate: 0.0024955
	LOSS [training: 3.835749350695501 | validation: 4.981779236508381]
	TIME [epoch: 9.71 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9279375003294974		[learning rate: 0.0024864]
	Learning Rate: 0.00248645
	LOSS [training: 3.9279375003294974 | validation: 4.827371087683864]
	TIME [epoch: 9.74 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8613429000894386		[learning rate: 0.0024774]
	Learning Rate: 0.00247742
	LOSS [training: 3.8613429000894386 | validation: 4.868060977025952]
	TIME [epoch: 9.72 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.828541645845186		[learning rate: 0.0024684]
	Learning Rate: 0.00246843
	LOSS [training: 3.828541645845186 | validation: 4.917673846859554]
	TIME [epoch: 9.72 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8607516371404245		[learning rate: 0.0024595]
	Learning Rate: 0.00245947
	LOSS [training: 3.8607516371404245 | validation: 4.916216907578825]
	TIME [epoch: 9.73 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.880049687097211		[learning rate: 0.0024505]
	Learning Rate: 0.00245055
	LOSS [training: 3.880049687097211 | validation: 4.858024677177735]
	TIME [epoch: 9.72 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9094957100010603		[learning rate: 0.0024417]
	Learning Rate: 0.00244165
	LOSS [training: 3.9094957100010603 | validation: 4.903964659847815]
	TIME [epoch: 9.71 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8477252850273347		[learning rate: 0.0024328]
	Learning Rate: 0.00243279
	LOSS [training: 3.8477252850273347 | validation: 4.831979433674533]
	TIME [epoch: 9.72 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8464399567953946		[learning rate: 0.002424]
	Learning Rate: 0.00242396
	LOSS [training: 3.8464399567953946 | validation: 4.826750848963565]
	TIME [epoch: 9.74 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8300277501030893		[learning rate: 0.0024152]
	Learning Rate: 0.00241517
	LOSS [training: 3.8300277501030893 | validation: 4.810886398154813]
	TIME [epoch: 9.71 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8650079834434172		[learning rate: 0.0024064]
	Learning Rate: 0.0024064
	LOSS [training: 3.8650079834434172 | validation: 4.928310255314406]
	TIME [epoch: 9.71 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.875196543000444		[learning rate: 0.0023977]
	Learning Rate: 0.00239767
	LOSS [training: 3.875196543000444 | validation: 4.835250848598287]
	TIME [epoch: 9.7 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.821316044773714		[learning rate: 0.002389]
	Learning Rate: 0.00238897
	LOSS [training: 3.821316044773714 | validation: 4.912446355033877]
	TIME [epoch: 9.73 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8422917943464143		[learning rate: 0.0023803]
	Learning Rate: 0.0023803
	LOSS [training: 3.8422917943464143 | validation: 4.827938994127633]
	TIME [epoch: 9.71 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8615371403216256		[learning rate: 0.0023717]
	Learning Rate: 0.00237166
	LOSS [training: 3.8615371403216256 | validation: 4.840909577245193]
	TIME [epoch: 9.69 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.848321552041323		[learning rate: 0.0023631]
	Learning Rate: 0.00236305
	LOSS [training: 3.848321552041323 | validation: 4.817995262453072]
	TIME [epoch: 9.73 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.844617372464191		[learning rate: 0.0023545]
	Learning Rate: 0.00235448
	LOSS [training: 3.844617372464191 | validation: 4.822033048617071]
	TIME [epoch: 9.72 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9194905901128627		[learning rate: 0.0023459]
	Learning Rate: 0.00234593
	LOSS [training: 3.9194905901128627 | validation: 4.852096099422705]
	TIME [epoch: 9.72 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8659557523598673		[learning rate: 0.0023374]
	Learning Rate: 0.00233742
	LOSS [training: 3.8659557523598673 | validation: 4.880102526498341]
	TIME [epoch: 9.72 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8534129497330616		[learning rate: 0.0023289]
	Learning Rate: 0.00232894
	LOSS [training: 3.8534129497330616 | validation: 4.841427049246526]
	TIME [epoch: 9.73 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.84548418903602		[learning rate: 0.0023205]
	Learning Rate: 0.00232049
	LOSS [training: 3.84548418903602 | validation: 4.854873648203973]
	TIME [epoch: 9.72 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8212805328578554		[learning rate: 0.0023121]
	Learning Rate: 0.00231206
	LOSS [training: 3.8212805328578554 | validation: 4.839801580810136]
	TIME [epoch: 9.72 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.821740522530547		[learning rate: 0.0023037]
	Learning Rate: 0.00230367
	LOSS [training: 3.821740522530547 | validation: 4.864847311053203]
	TIME [epoch: 9.72 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8906479758446446		[learning rate: 0.0022953]
	Learning Rate: 0.00229531
	LOSS [training: 3.8906479758446446 | validation: 4.901931005615704]
	TIME [epoch: 9.73 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.880543527618883		[learning rate: 0.002287]
	Learning Rate: 0.00228698
	LOSS [training: 3.880543527618883 | validation: 4.835725253420994]
	TIME [epoch: 9.72 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8763786264095126		[learning rate: 0.0022787]
	Learning Rate: 0.00227868
	LOSS [training: 3.8763786264095126 | validation: 4.8639900154308355]
	TIME [epoch: 9.72 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.826409200007896		[learning rate: 0.0022704]
	Learning Rate: 0.00227042
	LOSS [training: 3.826409200007896 | validation: 4.82734120793821]
	TIME [epoch: 9.71 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.839712139792779		[learning rate: 0.0022622]
	Learning Rate: 0.00226218
	LOSS [training: 3.839712139792779 | validation: 4.968003939943101]
	TIME [epoch: 9.73 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8681065246314645		[learning rate: 0.002254]
	Learning Rate: 0.00225397
	LOSS [training: 3.8681065246314645 | validation: 4.831278818055289]
	TIME [epoch: 9.7 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.834620022193755		[learning rate: 0.0022458]
	Learning Rate: 0.00224579
	LOSS [training: 3.834620022193755 | validation: 4.9287350213383085]
	TIME [epoch: 9.71 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.865560145506348		[learning rate: 0.0022376]
	Learning Rate: 0.00223764
	LOSS [training: 3.865560145506348 | validation: 4.854773762522915]
	TIME [epoch: 9.72 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.838703180982658		[learning rate: 0.0022295]
	Learning Rate: 0.00222952
	LOSS [training: 3.838703180982658 | validation: 4.8566024870998215]
	TIME [epoch: 9.73 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.847402082323174		[learning rate: 0.0022214]
	Learning Rate: 0.00222142
	LOSS [training: 3.847402082323174 | validation: 4.863796180284068]
	TIME [epoch: 9.73 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8847879206948024		[learning rate: 0.0022134]
	Learning Rate: 0.00221336
	LOSS [training: 3.8847879206948024 | validation: 4.847761226021951]
	TIME [epoch: 9.71 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8344226178920593		[learning rate: 0.0022053]
	Learning Rate: 0.00220533
	LOSS [training: 3.8344226178920593 | validation: 4.872820719916419]
	TIME [epoch: 9.74 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8416205639211176		[learning rate: 0.0021973]
	Learning Rate: 0.00219733
	LOSS [training: 3.8416205639211176 | validation: 4.933688267704547]
	TIME [epoch: 9.71 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8618819046440507		[learning rate: 0.0021894]
	Learning Rate: 0.00218935
	LOSS [training: 3.8618819046440507 | validation: 4.853524425273238]
	TIME [epoch: 9.72 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.855450891717826		[learning rate: 0.0021814]
	Learning Rate: 0.00218141
	LOSS [training: 3.855450891717826 | validation: 4.839177015162484]
	TIME [epoch: 9.71 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.847546228212174		[learning rate: 0.0021735]
	Learning Rate: 0.00217349
	LOSS [training: 3.847546228212174 | validation: 4.853963562843933]
	TIME [epoch: 9.74 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.832650780122113		[learning rate: 0.0021656]
	Learning Rate: 0.0021656
	LOSS [training: 3.832650780122113 | validation: 4.838074736889607]
	TIME [epoch: 9.71 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.874805263309902		[learning rate: 0.0021577]
	Learning Rate: 0.00215774
	LOSS [training: 3.874805263309902 | validation: 4.828894364975896]
	TIME [epoch: 9.71 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.938677298122934		[learning rate: 0.0021499]
	Learning Rate: 0.00214991
	LOSS [training: 3.938677298122934 | validation: 5.369567362337793]
	TIME [epoch: 9.71 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.011027217246296		[learning rate: 0.0021421]
	Learning Rate: 0.00214211
	LOSS [training: 4.011027217246296 | validation: 4.910647421775385]
	TIME [epoch: 9.73 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8330499854714417		[learning rate: 0.0021343]
	Learning Rate: 0.00213434
	LOSS [training: 3.8330499854714417 | validation: 4.822688322907314]
	TIME [epoch: 9.72 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8509722963122806		[learning rate: 0.0021266]
	Learning Rate: 0.00212659
	LOSS [training: 3.8509722963122806 | validation: 4.852145416686966]
	TIME [epoch: 9.71 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8552740519489945		[learning rate: 0.0021189]
	Learning Rate: 0.00211887
	LOSS [training: 3.8552740519489945 | validation: 4.834423157610047]
	TIME [epoch: 9.73 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8307438678019956		[learning rate: 0.0021112]
	Learning Rate: 0.00211119
	LOSS [training: 3.8307438678019956 | validation: 5.01021754443875]
	TIME [epoch: 9.72 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.867120448000837		[learning rate: 0.0021035]
	Learning Rate: 0.00210352
	LOSS [training: 3.867120448000837 | validation: 4.8538736794220405]
	TIME [epoch: 9.71 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.873142728219353		[learning rate: 0.0020959]
	Learning Rate: 0.00209589
	LOSS [training: 3.873142728219353 | validation: 4.8920131773281375]
	TIME [epoch: 9.71 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8344977243743017		[learning rate: 0.0020883]
	Learning Rate: 0.00208828
	LOSS [training: 3.8344977243743017 | validation: 4.833300461914313]
	TIME [epoch: 9.73 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.820608537153767		[learning rate: 0.0020807]
	Learning Rate: 0.00208071
	LOSS [training: 3.820608537153767 | validation: 4.806542685180492]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_532.pth
	Model improved!!!
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8452770923057242		[learning rate: 0.0020732]
	Learning Rate: 0.00207315
	LOSS [training: 3.8452770923057242 | validation: 4.811569117420937]
	TIME [epoch: 9.73 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.843900775570583		[learning rate: 0.0020656]
	Learning Rate: 0.00206563
	LOSS [training: 3.843900775570583 | validation: 4.812586120326983]
	TIME [epoch: 9.72 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.817109860871194		[learning rate: 0.0020581]
	Learning Rate: 0.00205813
	LOSS [training: 3.817109860871194 | validation: 4.855112373780781]
	TIME [epoch: 9.74 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.803475788959015		[learning rate: 0.0020507]
	Learning Rate: 0.00205067
	LOSS [training: 3.803475788959015 | validation: 4.89656671589742]
	TIME [epoch: 9.71 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.829821206914601		[learning rate: 0.0020432]
	Learning Rate: 0.00204322
	LOSS [training: 3.829821206914601 | validation: 4.8571856102425945]
	TIME [epoch: 9.72 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.863767349671953		[learning rate: 0.0020358]
	Learning Rate: 0.00203581
	LOSS [training: 3.863767349671953 | validation: 4.885548618971596]
	TIME [epoch: 9.74 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.915068590238357		[learning rate: 0.0020284]
	Learning Rate: 0.00202842
	LOSS [training: 3.915068590238357 | validation: 4.832413386530842]
	TIME [epoch: 9.73 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.836155202333574		[learning rate: 0.0020211]
	Learning Rate: 0.00202106
	LOSS [training: 3.836155202333574 | validation: 4.908369602889504]
	TIME [epoch: 9.73 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8312967184254725		[learning rate: 0.0020137]
	Learning Rate: 0.00201372
	LOSS [training: 3.8312967184254725 | validation: 4.821710481395732]
	TIME [epoch: 9.71 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.923820934711461		[learning rate: 0.0020064]
	Learning Rate: 0.00200642
	LOSS [training: 3.923820934711461 | validation: 4.950303593481245]
	TIME [epoch: 9.74 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8802607712315846		[learning rate: 0.0019991]
	Learning Rate: 0.00199913
	LOSS [training: 3.8802607712315846 | validation: 4.856068285000095]
	TIME [epoch: 9.73 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8241669369703954		[learning rate: 0.0019919]
	Learning Rate: 0.00199188
	LOSS [training: 3.8241669369703954 | validation: 4.851588343374311]
	TIME [epoch: 9.82 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.818547679065392		[learning rate: 0.0019847]
	Learning Rate: 0.00198465
	LOSS [training: 3.818547679065392 | validation: 4.837719225954961]
	TIME [epoch: 9.72 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8294325350107536		[learning rate: 0.0019774]
	Learning Rate: 0.00197745
	LOSS [training: 3.8294325350107536 | validation: 4.832696422416306]
	TIME [epoch: 9.74 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.815721516419692		[learning rate: 0.0019703]
	Learning Rate: 0.00197027
	LOSS [training: 3.815721516419692 | validation: 4.957787253641652]
	TIME [epoch: 9.72 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8487738407525405		[learning rate: 0.0019631]
	Learning Rate: 0.00196312
	LOSS [training: 3.8487738407525405 | validation: 4.847091493337303]
	TIME [epoch: 9.72 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8281652628246277		[learning rate: 0.001956]
	Learning Rate: 0.001956
	LOSS [training: 3.8281652628246277 | validation: 4.939487180545579]
	TIME [epoch: 9.72 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8431687866408537		[learning rate: 0.0019489]
	Learning Rate: 0.0019489
	LOSS [training: 3.8431687866408537 | validation: 4.839229863738371]
	TIME [epoch: 9.74 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.813039630337684		[learning rate: 0.0019418]
	Learning Rate: 0.00194183
	LOSS [training: 3.813039630337684 | validation: 4.852599602345714]
	TIME [epoch: 9.72 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8414503367423647		[learning rate: 0.0019348]
	Learning Rate: 0.00193478
	LOSS [training: 3.8414503367423647 | validation: 4.819203966637544]
	TIME [epoch: 9.73 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8706311814029517		[learning rate: 0.0019278]
	Learning Rate: 0.00192776
	LOSS [training: 3.8706311814029517 | validation: 4.889637880862281]
	TIME [epoch: 9.74 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.824490207357779		[learning rate: 0.0019208]
	Learning Rate: 0.00192076
	LOSS [training: 3.824490207357779 | validation: 4.857632817301809]
	TIME [epoch: 9.73 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8461683372270663		[learning rate: 0.0019138]
	Learning Rate: 0.00191379
	LOSS [training: 3.8461683372270663 | validation: 4.892968042293518]
	TIME [epoch: 9.72 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8563284452761737		[learning rate: 0.0019068]
	Learning Rate: 0.00190685
	LOSS [training: 3.8563284452761737 | validation: 4.831593505366274]
	TIME [epoch: 9.72 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8055022719207243		[learning rate: 0.0018999]
	Learning Rate: 0.00189993
	LOSS [training: 3.8055022719207243 | validation: 4.863279913206382]
	TIME [epoch: 9.74 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.81619697333686		[learning rate: 0.001893]
	Learning Rate: 0.00189303
	LOSS [training: 3.81619697333686 | validation: 4.820469214500537]
	TIME [epoch: 9.72 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8095735171543113		[learning rate: 0.0018862]
	Learning Rate: 0.00188616
	LOSS [training: 3.8095735171543113 | validation: 4.829285985467341]
	TIME [epoch: 9.71 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8126744664333616		[learning rate: 0.0018793]
	Learning Rate: 0.00187932
	LOSS [training: 3.8126744664333616 | validation: 4.8077372378002865]
	TIME [epoch: 9.71 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8298251421201734		[learning rate: 0.0018725]
	Learning Rate: 0.0018725
	LOSS [training: 3.8298251421201734 | validation: 4.859448683831072]
	TIME [epoch: 9.73 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.836214345372327		[learning rate: 0.0018657]
	Learning Rate: 0.0018657
	LOSS [training: 3.836214345372327 | validation: 4.851277791765734]
	TIME [epoch: 9.72 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.839082823933011		[learning rate: 0.0018589]
	Learning Rate: 0.00185893
	LOSS [training: 3.839082823933011 | validation: 4.823300702191728]
	TIME [epoch: 9.72 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.814394278397242		[learning rate: 0.0018522]
	Learning Rate: 0.00185218
	LOSS [training: 3.814394278397242 | validation: 4.829191456877208]
	TIME [epoch: 9.73 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8167432736702893		[learning rate: 0.0018455]
	Learning Rate: 0.00184546
	LOSS [training: 3.8167432736702893 | validation: 4.868774747315034]
	TIME [epoch: 9.72 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.866616207675668		[learning rate: 0.0018388]
	Learning Rate: 0.00183877
	LOSS [training: 3.866616207675668 | validation: 4.991989808828328]
	TIME [epoch: 9.72 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.86264246150987		[learning rate: 0.0018321]
	Learning Rate: 0.00183209
	LOSS [training: 3.86264246150987 | validation: 4.938175741760599]
	TIME [epoch: 9.72 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.84019119245594		[learning rate: 0.0018254]
	Learning Rate: 0.00182544
	LOSS [training: 3.84019119245594 | validation: 4.824312159793034]
	TIME [epoch: 9.73 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8084355720588396		[learning rate: 0.0018188]
	Learning Rate: 0.00181882
	LOSS [training: 3.8084355720588396 | validation: 4.947752884713192]
	TIME [epoch: 9.72 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8549727478782514		[learning rate: 0.0018122]
	Learning Rate: 0.00181222
	LOSS [training: 3.8549727478782514 | validation: 4.823700585402256]
	TIME [epoch: 9.71 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.810507838597677		[learning rate: 0.0018056]
	Learning Rate: 0.00180564
	LOSS [training: 3.810507838597677 | validation: 4.95777676757449]
	TIME [epoch: 9.71 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.851361868921285		[learning rate: 0.0017991]
	Learning Rate: 0.00179909
	LOSS [training: 3.851361868921285 | validation: 4.8152739476468]
	TIME [epoch: 9.74 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.822879424397793		[learning rate: 0.0017926]
	Learning Rate: 0.00179256
	LOSS [training: 3.822879424397793 | validation: 4.901142960003224]
	TIME [epoch: 9.71 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.807844450513103		[learning rate: 0.0017861]
	Learning Rate: 0.00178605
	LOSS [training: 3.807844450513103 | validation: 4.845291932237134]
	TIME [epoch: 9.72 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.799926727278295		[learning rate: 0.0017796]
	Learning Rate: 0.00177957
	LOSS [training: 3.799926727278295 | validation: 4.815258168470183]
	TIME [epoch: 9.72 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8007786800317858		[learning rate: 0.0017731]
	Learning Rate: 0.00177311
	LOSS [training: 3.8007786800317858 | validation: 4.931630710979528]
	TIME [epoch: 9.74 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8347232227421784		[learning rate: 0.0017667]
	Learning Rate: 0.00176668
	LOSS [training: 3.8347232227421784 | validation: 4.844283845627561]
	TIME [epoch: 9.72 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8118487438498763		[learning rate: 0.0017603]
	Learning Rate: 0.00176027
	LOSS [training: 3.8118487438498763 | validation: 4.809314668049789]
	TIME [epoch: 9.72 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.820579325755631		[learning rate: 0.0017539]
	Learning Rate: 0.00175388
	LOSS [training: 3.820579325755631 | validation: 4.819081838501322]
	TIME [epoch: 9.74 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8038209436407255		[learning rate: 0.0017475]
	Learning Rate: 0.00174752
	LOSS [training: 3.8038209436407255 | validation: 4.833359874232814]
	TIME [epoch: 9.72 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8687894806982555		[learning rate: 0.0017412]
	Learning Rate: 0.00174117
	LOSS [training: 3.8687894806982555 | validation: 4.81370490678784]
	TIME [epoch: 9.71 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.832715302877861		[learning rate: 0.0017349]
	Learning Rate: 0.00173486
	LOSS [training: 3.832715302877861 | validation: 4.822516861886425]
	TIME [epoch: 9.71 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.867700924214816		[learning rate: 0.0017286]
	Learning Rate: 0.00172856
	LOSS [training: 3.867700924214816 | validation: 4.80953690572492]
	TIME [epoch: 9.74 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.833463056355787		[learning rate: 0.0017223]
	Learning Rate: 0.00172229
	LOSS [training: 3.833463056355787 | validation: 4.862695780571271]
	TIME [epoch: 9.72 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.823507562571975		[learning rate: 0.001716]
	Learning Rate: 0.00171604
	LOSS [training: 3.823507562571975 | validation: 4.851149367761263]
	TIME [epoch: 9.71 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8863195638852135		[learning rate: 0.0017098]
	Learning Rate: 0.00170981
	LOSS [training: 3.8863195638852135 | validation: 4.80589806093237]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_586.pth
	Model improved!!!
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8096767904616606		[learning rate: 0.0017036]
	Learning Rate: 0.0017036
	LOSS [training: 3.8096767904616606 | validation: 4.868627019079356]
	TIME [epoch: 9.75 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.847521411688548		[learning rate: 0.0016974]
	Learning Rate: 0.00169742
	LOSS [training: 3.847521411688548 | validation: 4.8486814899087065]
	TIME [epoch: 9.74 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.844786686519343		[learning rate: 0.0016913]
	Learning Rate: 0.00169126
	LOSS [training: 3.844786686519343 | validation: 4.833997536640206]
	TIME [epoch: 9.74 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8281691722518523		[learning rate: 0.0016851]
	Learning Rate: 0.00168512
	LOSS [training: 3.8281691722518523 | validation: 4.938032143064496]
	TIME [epoch: 9.72 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9006514190922372		[learning rate: 0.001679]
	Learning Rate: 0.00167901
	LOSS [training: 3.9006514190922372 | validation: 4.84216234645359]
	TIME [epoch: 9.73 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8267101826746677		[learning rate: 0.0016729]
	Learning Rate: 0.00167291
	LOSS [training: 3.8267101826746677 | validation: 4.868173100783578]
	TIME [epoch: 9.72 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.815703697842988		[learning rate: 0.0016668]
	Learning Rate: 0.00166684
	LOSS [training: 3.815703697842988 | validation: 4.830096872714313]
	TIME [epoch: 9.72 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8117046552449523		[learning rate: 0.0016608]
	Learning Rate: 0.00166079
	LOSS [training: 3.8117046552449523 | validation: 4.820805021561417]
	TIME [epoch: 9.74 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8271660417691096		[learning rate: 0.0016548]
	Learning Rate: 0.00165477
	LOSS [training: 3.8271660417691096 | validation: 4.8461849070006044]
	TIME [epoch: 9.73 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8331013918865295		[learning rate: 0.0016488]
	Learning Rate: 0.00164876
	LOSS [training: 3.8331013918865295 | validation: 4.812942229260785]
	TIME [epoch: 9.72 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.834900645067916		[learning rate: 0.0016428]
	Learning Rate: 0.00164278
	LOSS [training: 3.834900645067916 | validation: 4.818245705995088]
	TIME [epoch: 9.73 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8668613772639433		[learning rate: 0.0016368]
	Learning Rate: 0.00163682
	LOSS [training: 3.8668613772639433 | validation: 4.878234720257232]
	TIME [epoch: 9.74 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8199845753889563		[learning rate: 0.0016309]
	Learning Rate: 0.00163088
	LOSS [training: 3.8199845753889563 | validation: 4.864161746379679]
	TIME [epoch: 9.72 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8057063825920396		[learning rate: 0.001625]
	Learning Rate: 0.00162496
	LOSS [training: 3.8057063825920396 | validation: 4.915917613360166]
	TIME [epoch: 9.73 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8216856870287508		[learning rate: 0.0016191]
	Learning Rate: 0.00161906
	LOSS [training: 3.8216856870287508 | validation: 4.918124908946893]
	TIME [epoch: 9.73 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8587187834818444		[learning rate: 0.0016132]
	Learning Rate: 0.00161319
	LOSS [training: 3.8587187834818444 | validation: 4.82290610266597]
	TIME [epoch: 9.74 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.81145628327556		[learning rate: 0.0016073]
	Learning Rate: 0.00160733
	LOSS [training: 3.81145628327556 | validation: 4.836430513671206]
	TIME [epoch: 9.72 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.840983225048511		[learning rate: 0.0016015]
	Learning Rate: 0.0016015
	LOSS [training: 3.840983225048511 | validation: 4.83622576723797]
	TIME [epoch: 9.73 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8260191698003028		[learning rate: 0.0015957]
	Learning Rate: 0.00159569
	LOSS [training: 3.8260191698003028 | validation: 4.8397234130536555]
	TIME [epoch: 9.74 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.822267667359962		[learning rate: 0.0015899]
	Learning Rate: 0.00158989
	LOSS [training: 3.822267667359962 | validation: 4.829777044827871]
	TIME [epoch: 9.73 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.827537857596166		[learning rate: 0.0015841]
	Learning Rate: 0.00158413
	LOSS [training: 3.827537857596166 | validation: 4.814411437621403]
	TIME [epoch: 9.72 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8246690305777156		[learning rate: 0.0015784]
	Learning Rate: 0.00157838
	LOSS [training: 3.8246690305777156 | validation: 4.821384562708771]
	TIME [epoch: 9.71 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8304627117714056		[learning rate: 0.0015726]
	Learning Rate: 0.00157265
	LOSS [training: 3.8304627117714056 | validation: 4.809938784898444]
	TIME [epoch: 9.75 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8040587401192036		[learning rate: 0.0015669]
	Learning Rate: 0.00156694
	LOSS [training: 3.8040587401192036 | validation: 4.850819237935493]
	TIME [epoch: 9.72 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8194625969073543		[learning rate: 0.0015613]
	Learning Rate: 0.00156125
	LOSS [training: 3.8194625969073543 | validation: 4.82150512443439]
	TIME [epoch: 9.72 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8461101614080695		[learning rate: 0.0015556]
	Learning Rate: 0.00155559
	LOSS [training: 3.8461101614080695 | validation: 4.843330024260762]
	TIME [epoch: 9.72 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.807289369198817		[learning rate: 0.0015499]
	Learning Rate: 0.00154994
	LOSS [training: 3.807289369198817 | validation: 4.82795701994295]
	TIME [epoch: 9.75 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8175549470198833		[learning rate: 0.0015443]
	Learning Rate: 0.00154432
	LOSS [training: 3.8175549470198833 | validation: 4.81761008981875]
	TIME [epoch: 9.72 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8420481414972896		[learning rate: 0.0015387]
	Learning Rate: 0.00153871
	LOSS [training: 3.8420481414972896 | validation: 4.801220083329309]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7935161416680243		[learning rate: 0.0015331]
	Learning Rate: 0.00153313
	LOSS [training: 3.7935161416680243 | validation: 4.81218790204431]
	TIME [epoch: 9.74 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8077543295344247		[learning rate: 0.0015276]
	Learning Rate: 0.00152757
	LOSS [training: 3.8077543295344247 | validation: 4.831004454169679]
	TIME [epoch: 9.73 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7927701309424386		[learning rate: 0.001522]
	Learning Rate: 0.00152202
	LOSS [training: 3.7927701309424386 | validation: 4.921114261782164]
	TIME [epoch: 9.72 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.83066584443918		[learning rate: 0.0015165]
	Learning Rate: 0.0015165
	LOSS [training: 3.83066584443918 | validation: 4.893914538574453]
	TIME [epoch: 9.72 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.858278408625381		[learning rate: 0.001511]
	Learning Rate: 0.001511
	LOSS [training: 3.858278408625381 | validation: 4.81448894470245]
	TIME [epoch: 9.74 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8845557233707373		[learning rate: 0.0015055]
	Learning Rate: 0.00150551
	LOSS [training: 3.8845557233707373 | validation: 4.876422283753242]
	TIME [epoch: 9.72 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9486349824225173		[learning rate: 0.0015]
	Learning Rate: 0.00150005
	LOSS [training: 3.9486349824225173 | validation: 4.947666627857005]
	TIME [epoch: 9.71 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8284376970910934		[learning rate: 0.0014946]
	Learning Rate: 0.0014946
	LOSS [training: 3.8284376970910934 | validation: 4.8398532444311915]
	TIME [epoch: 9.72 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8112993795139283		[learning rate: 0.0014892]
	Learning Rate: 0.00148918
	LOSS [training: 3.8112993795139283 | validation: 4.830626891226616]
	TIME [epoch: 9.74 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.833810327841557		[learning rate: 0.0014838]
	Learning Rate: 0.00148378
	LOSS [training: 3.833810327841557 | validation: 4.877659402006607]
	TIME [epoch: 9.71 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8104992164762144		[learning rate: 0.0014784]
	Learning Rate: 0.00147839
	LOSS [training: 3.8104992164762144 | validation: 4.854760633551631]
	TIME [epoch: 9.71 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8133255292050556		[learning rate: 0.001473]
	Learning Rate: 0.00147303
	LOSS [training: 3.8133255292050556 | validation: 4.81978000200577]
	TIME [epoch: 9.72 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.830692821341782		[learning rate: 0.0014677]
	Learning Rate: 0.00146768
	LOSS [training: 3.830692821341782 | validation: 4.826498608597194]
	TIME [epoch: 9.73 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.845556610379552		[learning rate: 0.0014624]
	Learning Rate: 0.00146235
	LOSS [training: 3.845556610379552 | validation: 4.906232151405265]
	TIME [epoch: 9.71 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8435776445362015		[learning rate: 0.001457]
	Learning Rate: 0.00145705
	LOSS [training: 3.8435776445362015 | validation: 4.894862749448103]
	TIME [epoch: 9.71 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.838690075825042		[learning rate: 0.0014518]
	Learning Rate: 0.00145176
	LOSS [training: 3.838690075825042 | validation: 4.808980284389617]
	TIME [epoch: 9.73 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8549802967407922		[learning rate: 0.0014465]
	Learning Rate: 0.00144649
	LOSS [training: 3.8549802967407922 | validation: 4.926479400227485]
	TIME [epoch: 9.72 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.900361721109487		[learning rate: 0.0014412]
	Learning Rate: 0.00144124
	LOSS [training: 3.900361721109487 | validation: 4.797288619920546]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_633.pth
	Model improved!!!
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.811769347273033		[learning rate: 0.001436]
	Learning Rate: 0.00143601
	LOSS [training: 3.811769347273033 | validation: 4.830705631781325]
	TIME [epoch: 9.72 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8182404563425165		[learning rate: 0.0014308]
	Learning Rate: 0.0014308
	LOSS [training: 3.8182404563425165 | validation: 4.8111508219742865]
	TIME [epoch: 9.74 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8152985610873977		[learning rate: 0.0014256]
	Learning Rate: 0.00142561
	LOSS [training: 3.8152985610873977 | validation: 4.852858375682945]
	TIME [epoch: 9.72 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8181173988202763		[learning rate: 0.0014204]
	Learning Rate: 0.00142043
	LOSS [training: 3.8181173988202763 | validation: 4.851383810400469]
	TIME [epoch: 9.72 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8493553053109486		[learning rate: 0.0014153]
	Learning Rate: 0.00141528
	LOSS [training: 3.8493553053109486 | validation: 4.938774689260949]
	TIME [epoch: 9.72 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8370448877861465		[learning rate: 0.0014101]
	Learning Rate: 0.00141014
	LOSS [training: 3.8370448877861465 | validation: 4.82629302572466]
	TIME [epoch: 9.75 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8332674122307595		[learning rate: 0.001405]
	Learning Rate: 0.00140503
	LOSS [training: 3.8332674122307595 | validation: 4.970494474250121]
	TIME [epoch: 9.72 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.864347806885106		[learning rate: 0.0013999]
	Learning Rate: 0.00139993
	LOSS [training: 3.864347806885106 | validation: 4.822891847981546]
	TIME [epoch: 9.72 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8363846368570385		[learning rate: 0.0013948]
	Learning Rate: 0.00139485
	LOSS [training: 3.8363846368570385 | validation: 4.808049137420958]
	TIME [epoch: 9.73 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8090869359067483		[learning rate: 0.0013898]
	Learning Rate: 0.00138978
	LOSS [training: 3.8090869359067483 | validation: 4.812212717539834]
	TIME [epoch: 9.73 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8350072740732		[learning rate: 0.0013847]
	Learning Rate: 0.00138474
	LOSS [training: 3.8350072740732 | validation: 4.836810124083662]
	TIME [epoch: 9.72 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8268252806760925		[learning rate: 0.0013797]
	Learning Rate: 0.00137972
	LOSS [training: 3.8268252806760925 | validation: 4.8688704507716]
	TIME [epoch: 9.71 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8362744274036613		[learning rate: 0.0013747]
	Learning Rate: 0.00137471
	LOSS [training: 3.8362744274036613 | validation: 4.814226438645594]
	TIME [epoch: 9.74 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8090051318457134		[learning rate: 0.0013697]
	Learning Rate: 0.00136972
	LOSS [training: 3.8090051318457134 | validation: 4.830667708291806]
	TIME [epoch: 9.72 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.855012251343706		[learning rate: 0.0013647]
	Learning Rate: 0.00136475
	LOSS [training: 3.855012251343706 | validation: 4.845183580631489]
	TIME [epoch: 9.72 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8405667161574306		[learning rate: 0.0013598]
	Learning Rate: 0.0013598
	LOSS [training: 3.8405667161574306 | validation: 4.865255468690036]
	TIME [epoch: 9.73 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8129594998541116		[learning rate: 0.0013549]
	Learning Rate: 0.00135486
	LOSS [training: 3.8129594998541116 | validation: 4.7914720612392685]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_650.pth
	Model improved!!!
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7892601799174854		[learning rate: 0.0013499]
	Learning Rate: 0.00134994
	LOSS [training: 3.7892601799174854 | validation: 4.849361707201139]
	TIME [epoch: 9.73 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8494826887611007		[learning rate: 0.001345]
	Learning Rate: 0.00134505
	LOSS [training: 3.8494826887611007 | validation: 4.793289960797308]
	TIME [epoch: 9.73 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.822526595574981		[learning rate: 0.0013402]
	Learning Rate: 0.00134016
	LOSS [training: 3.822526595574981 | validation: 4.822919636152808]
	TIME [epoch: 9.75 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8031215384387926		[learning rate: 0.0013353]
	Learning Rate: 0.0013353
	LOSS [training: 3.8031215384387926 | validation: 4.813529135763986]
	TIME [epoch: 9.74 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.869444670721369		[learning rate: 0.0013305]
	Learning Rate: 0.00133045
	LOSS [training: 3.869444670721369 | validation: 4.857593851172951]
	TIME [epoch: 9.73 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.834671862398511		[learning rate: 0.0013256]
	Learning Rate: 0.00132563
	LOSS [training: 3.834671862398511 | validation: 4.825321296496383]
	TIME [epoch: 9.71 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8223396904085		[learning rate: 0.0013208]
	Learning Rate: 0.00132082
	LOSS [training: 3.8223396904085 | validation: 4.839670718192163]
	TIME [epoch: 9.74 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8287423697209526		[learning rate: 0.001316]
	Learning Rate: 0.00131602
	LOSS [training: 3.8287423697209526 | validation: 4.799343219378084]
	TIME [epoch: 9.73 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.842198619440583		[learning rate: 0.0013112]
	Learning Rate: 0.00131125
	LOSS [training: 3.842198619440583 | validation: 4.839274040440334]
	TIME [epoch: 9.72 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8114409789411368		[learning rate: 0.0013065]
	Learning Rate: 0.00130649
	LOSS [training: 3.8114409789411368 | validation: 4.808961793738614]
	TIME [epoch: 9.72 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7967050540197222		[learning rate: 0.0013017]
	Learning Rate: 0.00130175
	LOSS [training: 3.7967050540197222 | validation: 4.814060359711201]
	TIME [epoch: 9.74 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8154488029776545		[learning rate: 0.001297]
	Learning Rate: 0.00129702
	LOSS [training: 3.8154488029776545 | validation: 4.80247775779813]
	TIME [epoch: 9.71 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.809689928900267		[learning rate: 0.0012923]
	Learning Rate: 0.00129232
	LOSS [training: 3.809689928900267 | validation: 4.812891700765456]
	TIME [epoch: 9.72 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.802088455163145		[learning rate: 0.0012876]
	Learning Rate: 0.00128763
	LOSS [training: 3.802088455163145 | validation: 4.814837398765934]
	TIME [epoch: 9.72 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.805256343216731		[learning rate: 0.001283]
	Learning Rate: 0.00128295
	LOSS [training: 3.805256343216731 | validation: 4.810293019875453]
	TIME [epoch: 9.74 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8113798519964774		[learning rate: 0.0012783]
	Learning Rate: 0.0012783
	LOSS [training: 3.8113798519964774 | validation: 4.81851872338413]
	TIME [epoch: 9.72 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8114447210383497		[learning rate: 0.0012737]
	Learning Rate: 0.00127366
	LOSS [training: 3.8114447210383497 | validation: 4.8166357311588195]
	TIME [epoch: 9.71 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.828118418793899		[learning rate: 0.001269]
	Learning Rate: 0.00126904
	LOSS [training: 3.828118418793899 | validation: 4.818628814291653]
	TIME [epoch: 9.74 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8237156300629223		[learning rate: 0.0012644]
	Learning Rate: 0.00126443
	LOSS [training: 3.8237156300629223 | validation: 4.816084040293072]
	TIME [epoch: 9.72 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7931677575672254		[learning rate: 0.0012598]
	Learning Rate: 0.00125984
	LOSS [training: 3.7931677575672254 | validation: 4.83020249883778]
	TIME [epoch: 9.72 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.822314414275694		[learning rate: 0.0012553]
	Learning Rate: 0.00125527
	LOSS [training: 3.822314414275694 | validation: 4.822614309681943]
	TIME [epoch: 9.72 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.816827154625057		[learning rate: 0.0012507]
	Learning Rate: 0.00125071
	LOSS [training: 3.816827154625057 | validation: 4.881532736639894]
	TIME [epoch: 9.74 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8177487269254655		[learning rate: 0.0012462]
	Learning Rate: 0.00124617
	LOSS [training: 3.8177487269254655 | validation: 4.8568226687344165]
	TIME [epoch: 9.72 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.814663710681059		[learning rate: 0.0012417]
	Learning Rate: 0.00124165
	LOSS [training: 3.814663710681059 | validation: 4.811126869728258]
	TIME [epoch: 9.73 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.791414165134281		[learning rate: 0.0012371]
	Learning Rate: 0.00123715
	LOSS [training: 3.791414165134281 | validation: 4.834550914958291]
	TIME [epoch: 9.72 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8464453538050085		[learning rate: 0.0012327]
	Learning Rate: 0.00123266
	LOSS [training: 3.8464453538050085 | validation: 4.813424387953326]
	TIME [epoch: 9.74 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8047377012497763		[learning rate: 0.0012282]
	Learning Rate: 0.00122818
	LOSS [training: 3.8047377012497763 | validation: 4.814983519716291]
	TIME [epoch: 9.72 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.796876018265091		[learning rate: 0.0012237]
	Learning Rate: 0.00122373
	LOSS [training: 3.796876018265091 | validation: 4.8544708371864855]
	TIME [epoch: 9.73 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8059925816744515		[learning rate: 0.0012193]
	Learning Rate: 0.00121929
	LOSS [training: 3.8059925816744515 | validation: 4.813050202301506]
	TIME [epoch: 9.74 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8121041942072296		[learning rate: 0.0012149]
	Learning Rate: 0.00121486
	LOSS [training: 3.8121041942072296 | validation: 4.820738791955789]
	TIME [epoch: 9.74 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.834617514717221		[learning rate: 0.0012105]
	Learning Rate: 0.00121045
	LOSS [training: 3.834617514717221 | validation: 4.841732521302564]
	TIME [epoch: 9.72 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8079047356987736		[learning rate: 0.0012061]
	Learning Rate: 0.00120606
	LOSS [training: 3.8079047356987736 | validation: 4.833381534156662]
	TIME [epoch: 9.72 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8025856326480545		[learning rate: 0.0012017]
	Learning Rate: 0.00120168
	LOSS [training: 3.8025856326480545 | validation: 4.831333362326724]
	TIME [epoch: 9.74 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7969278699163844		[learning rate: 0.0011973]
	Learning Rate: 0.00119732
	LOSS [training: 3.7969278699163844 | validation: 4.814843433943089]
	TIME [epoch: 9.72 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.805000105240276		[learning rate: 0.001193]
	Learning Rate: 0.00119298
	LOSS [training: 3.805000105240276 | validation: 4.81822268517338]
	TIME [epoch: 9.72 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8449379391627594		[learning rate: 0.0011886]
	Learning Rate: 0.00118865
	LOSS [training: 3.8449379391627594 | validation: 4.8038750973561415]
	TIME [epoch: 9.72 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8023424819908755		[learning rate: 0.0011843]
	Learning Rate: 0.00118433
	LOSS [training: 3.8023424819908755 | validation: 4.851794908552498]
	TIME [epoch: 9.74 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.813522510566861		[learning rate: 0.00118]
	Learning Rate: 0.00118003
	LOSS [training: 3.813522510566861 | validation: 4.823900493368711]
	TIME [epoch: 9.72 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8151432659166042		[learning rate: 0.0011758]
	Learning Rate: 0.00117575
	LOSS [training: 3.8151432659166042 | validation: 4.884390985996611]
	TIME [epoch: 9.72 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8691530075062337		[learning rate: 0.0011715]
	Learning Rate: 0.00117149
	LOSS [training: 3.8691530075062337 | validation: 4.839402649973614]
	TIME [epoch: 9.72 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8128264099723976		[learning rate: 0.0011672]
	Learning Rate: 0.00116723
	LOSS [training: 3.8128264099723976 | validation: 4.891479759456079]
	TIME [epoch: 9.74 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.830307069218287		[learning rate: 0.001163]
	Learning Rate: 0.001163
	LOSS [training: 3.830307069218287 | validation: 4.798382404143076]
	TIME [epoch: 9.72 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.813514993771695		[learning rate: 0.0011588]
	Learning Rate: 0.00115878
	LOSS [training: 3.813514993771695 | validation: 4.880965839170334]
	TIME [epoch: 9.72 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8179059989899096		[learning rate: 0.0011546]
	Learning Rate: 0.00115457
	LOSS [training: 3.8179059989899096 | validation: 4.817654833461427]
	TIME [epoch: 9.73 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.802769983905968		[learning rate: 0.0011504]
	Learning Rate: 0.00115038
	LOSS [training: 3.802769983905968 | validation: 4.817850606108347]
	TIME [epoch: 9.73 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.817271993708611		[learning rate: 0.0011462]
	Learning Rate: 0.00114621
	LOSS [training: 3.817271993708611 | validation: 4.828450585038845]
	TIME [epoch: 9.73 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7995454688970463		[learning rate: 0.001142]
	Learning Rate: 0.00114205
	LOSS [training: 3.7995454688970463 | validation: 4.829170577638488]
	TIME [epoch: 9.72 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.80597621987013		[learning rate: 0.0011379]
	Learning Rate: 0.0011379
	LOSS [training: 3.80597621987013 | validation: 4.838892731214154]
	TIME [epoch: 9.74 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8111455681289845		[learning rate: 0.0011338]
	Learning Rate: 0.00113377
	LOSS [training: 3.8111455681289845 | validation: 4.832096752633885]
	TIME [epoch: 9.72 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.81739564080337		[learning rate: 0.0011297]
	Learning Rate: 0.00112966
	LOSS [training: 3.81739564080337 | validation: 4.8582951391536]
	TIME [epoch: 9.72 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8001185548403478		[learning rate: 0.0011256]
	Learning Rate: 0.00112556
	LOSS [training: 3.8001185548403478 | validation: 4.828331003740029]
	TIME [epoch: 9.72 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.793220982651193		[learning rate: 0.0011215]
	Learning Rate: 0.00112147
	LOSS [training: 3.793220982651193 | validation: 4.8086564512217125]
	TIME [epoch: 9.74 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8219695763843093		[learning rate: 0.0011174]
	Learning Rate: 0.0011174
	LOSS [training: 3.8219695763843093 | validation: 4.835935124872133]
	TIME [epoch: 9.72 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8250620815744263		[learning rate: 0.0011133]
	Learning Rate: 0.00111335
	LOSS [training: 3.8250620815744263 | validation: 4.878493170556405]
	TIME [epoch: 9.72 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.851851336897904		[learning rate: 0.0011093]
	Learning Rate: 0.00110931
	LOSS [training: 3.851851336897904 | validation: 4.85283761733463]
	TIME [epoch: 9.74 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.802650896745928		[learning rate: 0.0011053]
	Learning Rate: 0.00110528
	LOSS [training: 3.802650896745928 | validation: 4.904642103844786]
	TIME [epoch: 9.73 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8252820795126694		[learning rate: 0.0011013]
	Learning Rate: 0.00110127
	LOSS [training: 3.8252820795126694 | validation: 4.826239321312337]
	TIME [epoch: 9.72 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.797886590016706		[learning rate: 0.0010973]
	Learning Rate: 0.00109728
	LOSS [training: 3.797886590016706 | validation: 4.82393925947953]
	TIME [epoch: 9.72 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.819621834939299		[learning rate: 0.0010933]
	Learning Rate: 0.00109329
	LOSS [training: 3.819621834939299 | validation: 4.879108080889717]
	TIME [epoch: 9.74 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8246441595190532		[learning rate: 0.0010893]
	Learning Rate: 0.00108933
	LOSS [training: 3.8246441595190532 | validation: 4.945806469821157]
	TIME [epoch: 9.73 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.862442174905053		[learning rate: 0.0010854]
	Learning Rate: 0.00108537
	LOSS [training: 3.862442174905053 | validation: 4.813653092652595]
	TIME [epoch: 9.72 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7924275078991743		[learning rate: 0.0010814]
	Learning Rate: 0.00108143
	LOSS [training: 3.7924275078991743 | validation: 4.799706639318556]
	TIME [epoch: 9.72 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8376541887841307		[learning rate: 0.0010775]
	Learning Rate: 0.00107751
	LOSS [training: 3.8376541887841307 | validation: 4.845155052334268]
	TIME [epoch: 9.75 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8140870410189804		[learning rate: 0.0010736]
	Learning Rate: 0.0010736
	LOSS [training: 3.8140870410189804 | validation: 4.811843448279943]
	TIME [epoch: 9.72 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8100454866378266		[learning rate: 0.0010697]
	Learning Rate: 0.0010697
	LOSS [training: 3.8100454866378266 | validation: 4.804480802667458]
	TIME [epoch: 9.72 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7989708180903152		[learning rate: 0.0010658]
	Learning Rate: 0.00106582
	LOSS [training: 3.7989708180903152 | validation: 4.854537264046766]
	TIME [epoch: 9.73 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.798545832338732		[learning rate: 0.001062]
	Learning Rate: 0.00106195
	LOSS [training: 3.798545832338732 | validation: 4.839416728468792]
	TIME [epoch: 9.75 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8322532152112876		[learning rate: 0.0010581]
	Learning Rate: 0.0010581
	LOSS [training: 3.8322532152112876 | validation: 4.898525001815882]
	TIME [epoch: 9.72 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8365285295894167		[learning rate: 0.0010543]
	Learning Rate: 0.00105426
	LOSS [training: 3.8365285295894167 | validation: 4.876134556461992]
	TIME [epoch: 9.72 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8127229662477817		[learning rate: 0.0010504]
	Learning Rate: 0.00105043
	LOSS [training: 3.8127229662477817 | validation: 4.833291266495482]
	TIME [epoch: 9.73 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.806468995876339		[learning rate: 0.0010466]
	Learning Rate: 0.00104662
	LOSS [training: 3.806468995876339 | validation: 4.8301648171052385]
	TIME [epoch: 9.74 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7955868682660103		[learning rate: 0.0010428]
	Learning Rate: 0.00104282
	LOSS [training: 3.7955868682660103 | validation: 4.845165337531732]
	TIME [epoch: 9.73 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8079256056328306		[learning rate: 0.001039]
	Learning Rate: 0.00103904
	LOSS [training: 3.8079256056328306 | validation: 4.826220485515432]
	TIME [epoch: 9.73 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8064519427204138		[learning rate: 0.0010353]
	Learning Rate: 0.00103527
	LOSS [training: 3.8064519427204138 | validation: 4.88980002846373]
	TIME [epoch: 9.75 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8301364373695463		[learning rate: 0.0010315]
	Learning Rate: 0.00103151
	LOSS [training: 3.8301364373695463 | validation: 4.81503091749418]
	TIME [epoch: 9.72 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.792996791663257		[learning rate: 0.0010278]
	Learning Rate: 0.00102777
	LOSS [training: 3.792996791663257 | validation: 4.810833822767361]
	TIME [epoch: 9.73 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8023398806207487		[learning rate: 0.001024]
	Learning Rate: 0.00102404
	LOSS [training: 3.8023398806207487 | validation: 4.827331785223584]
	TIME [epoch: 9.73 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8012834522069405		[learning rate: 0.0010203]
	Learning Rate: 0.00102032
	LOSS [training: 3.8012834522069405 | validation: 4.796056223779521]
	TIME [epoch: 9.75 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7921308548142436		[learning rate: 0.0010166]
	Learning Rate: 0.00101662
	LOSS [training: 3.7921308548142436 | validation: 4.808465153048458]
	TIME [epoch: 9.72 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8008177739087223		[learning rate: 0.0010129]
	Learning Rate: 0.00101293
	LOSS [training: 3.8008177739087223 | validation: 4.814608695372575]
	TIME [epoch: 9.73 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7945464943168474		[learning rate: 0.0010093]
	Learning Rate: 0.00100925
	LOSS [training: 3.7945464943168474 | validation: 4.929976152054967]
	TIME [epoch: 9.74 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8268301538408407		[learning rate: 0.0010056]
	Learning Rate: 0.00100559
	LOSS [training: 3.8268301538408407 | validation: 4.807817304351078]
	TIME [epoch: 9.74 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.796236380986349		[learning rate: 0.0010019]
	Learning Rate: 0.00100194
	LOSS [training: 3.796236380986349 | validation: 4.821320492500737]
	TIME [epoch: 9.73 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.792971880222814		[learning rate: 0.0009983]
	Learning Rate: 0.000998305
	LOSS [training: 3.792971880222814 | validation: 4.805946487056334]
	TIME [epoch: 9.73 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7920951059734107		[learning rate: 0.00099468]
	Learning Rate: 0.000994682
	LOSS [training: 3.7920951059734107 | validation: 4.830433616191987]
	TIME [epoch: 9.74 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.811597442133693		[learning rate: 0.00099107]
	Learning Rate: 0.000991072
	LOSS [training: 3.811597442133693 | validation: 4.830268008430517]
	TIME [epoch: 9.73 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.803637096316328		[learning rate: 0.00098748]
	Learning Rate: 0.000987475
	LOSS [training: 3.803637096316328 | validation: 4.844568663221828]
	TIME [epoch: 9.73 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.828034791753533		[learning rate: 0.00098389]
	Learning Rate: 0.000983892
	LOSS [training: 3.828034791753533 | validation: 4.813052785779138]
	TIME [epoch: 9.72 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.813199082649401		[learning rate: 0.00098032]
	Learning Rate: 0.000980321
	LOSS [training: 3.813199082649401 | validation: 4.8428862139123625]
	TIME [epoch: 9.74 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8051038608312404		[learning rate: 0.00097676]
	Learning Rate: 0.000976764
	LOSS [training: 3.8051038608312404 | validation: 4.967631204717823]
	TIME [epoch: 9.72 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.844152525278074		[learning rate: 0.00097322]
	Learning Rate: 0.000973219
	LOSS [training: 3.844152525278074 | validation: 4.811937336586316]
	TIME [epoch: 9.72 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.810601856356135		[learning rate: 0.00096969]
	Learning Rate: 0.000969687
	LOSS [training: 3.810601856356135 | validation: 4.817031160644704]
	TIME [epoch: 9.72 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7912927836096215		[learning rate: 0.00096617]
	Learning Rate: 0.000966168
	LOSS [training: 3.7912927836096215 | validation: 4.822774447498026]
	TIME [epoch: 9.74 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7989689234301856		[learning rate: 0.00096266]
	Learning Rate: 0.000962662
	LOSS [training: 3.7989689234301856 | validation: 4.815026913168743]
	TIME [epoch: 9.73 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.806799180767233		[learning rate: 0.00095917]
	Learning Rate: 0.000959168
	LOSS [training: 3.806799180767233 | validation: 4.802937663674678]
	TIME [epoch: 9.72 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7965094095473253		[learning rate: 0.00095569]
	Learning Rate: 0.000955687
	LOSS [training: 3.7965094095473253 | validation: 4.810991166971221]
	TIME [epoch: 9.74 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7955185038753667		[learning rate: 0.00095222]
	Learning Rate: 0.000952219
	LOSS [training: 3.7955185038753667 | validation: 4.826050002707602]
	TIME [epoch: 9.73 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8037857133429953		[learning rate: 0.00094876]
	Learning Rate: 0.000948763
	LOSS [training: 3.8037857133429953 | validation: 4.825379149406504]
	TIME [epoch: 9.72 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7890280681134305		[learning rate: 0.00094532]
	Learning Rate: 0.00094532
	LOSS [training: 3.7890280681134305 | validation: 4.967666075685786]
	TIME [epoch: 9.72 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8922369429238968		[learning rate: 0.00094189]
	Learning Rate: 0.000941889
	LOSS [training: 3.8922369429238968 | validation: 4.846680198475503]
	TIME [epoch: 9.75 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.81186028394635		[learning rate: 0.00093847]
	Learning Rate: 0.000938471
	LOSS [training: 3.81186028394635 | validation: 4.802930870020273]
	TIME [epoch: 9.72 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8110049641717945		[learning rate: 0.00093507]
	Learning Rate: 0.000935066
	LOSS [training: 3.8110049641717945 | validation: 4.802791234969159]
	TIME [epoch: 9.73 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7921405235343544		[learning rate: 0.00093167]
	Learning Rate: 0.000931672
	LOSS [training: 3.7921405235343544 | validation: 4.814402035094387]
	TIME [epoch: 9.73 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8139170650110636		[learning rate: 0.00092829]
	Learning Rate: 0.000928291
	LOSS [training: 3.8139170650110636 | validation: 4.833755765848704]
	TIME [epoch: 9.75 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8167255097703228		[learning rate: 0.00092492]
	Learning Rate: 0.000924922
	LOSS [training: 3.8167255097703228 | validation: 4.877399786686637]
	TIME [epoch: 9.72 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.829204295685674		[learning rate: 0.00092157]
	Learning Rate: 0.000921566
	LOSS [training: 3.829204295685674 | validation: 4.8335077442724055]
	TIME [epoch: 9.73 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7955210956949186		[learning rate: 0.00091822]
	Learning Rate: 0.000918221
	LOSS [training: 3.7955210956949186 | validation: 4.835243035982971]
	TIME [epoch: 9.73 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.811803742883903		[learning rate: 0.00091489]
	Learning Rate: 0.000914889
	LOSS [training: 3.811803742883903 | validation: 4.84524030996696]
	TIME [epoch: 9.73 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.802193608341356		[learning rate: 0.00091157]
	Learning Rate: 0.000911569
	LOSS [training: 3.802193608341356 | validation: 4.80841576243695]
	TIME [epoch: 9.72 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7987739130975315		[learning rate: 0.00090826]
	Learning Rate: 0.000908261
	LOSS [training: 3.7987739130975315 | validation: 4.800766464801237]
	TIME [epoch: 9.73 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.791111381465811		[learning rate: 0.00090496]
	Learning Rate: 0.000904965
	LOSS [training: 3.791111381465811 | validation: 4.851140256480269]
	TIME [epoch: 9.75 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8177258973176746		[learning rate: 0.00090168]
	Learning Rate: 0.00090168
	LOSS [training: 3.8177258973176746 | validation: 4.809525479441976]
	TIME [epoch: 9.73 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8138963649845055		[learning rate: 0.00089841]
	Learning Rate: 0.000898408
	LOSS [training: 3.8138963649845055 | validation: 4.835753716811279]
	TIME [epoch: 9.73 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8001246234783914		[learning rate: 0.00089515]
	Learning Rate: 0.000895148
	LOSS [training: 3.8001246234783914 | validation: 4.799192956593855]
	TIME [epoch: 9.73 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8283531687579666		[learning rate: 0.0008919]
	Learning Rate: 0.000891899
	LOSS [training: 3.8283531687579666 | validation: 4.836729104163493]
	TIME [epoch: 9.75 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.786058615618431		[learning rate: 0.00088866]
	Learning Rate: 0.000888663
	LOSS [training: 3.786058615618431 | validation: 4.8142716267867085]
	TIME [epoch: 9.72 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7903260471748146		[learning rate: 0.00088544]
	Learning Rate: 0.000885438
	LOSS [training: 3.7903260471748146 | validation: 4.849740998387619]
	TIME [epoch: 9.72 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8166855849781363		[learning rate: 0.00088222]
	Learning Rate: 0.000882224
	LOSS [training: 3.8166855849781363 | validation: 4.819302473348041]
	TIME [epoch: 9.72 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8214104574472336		[learning rate: 0.00087902]
	Learning Rate: 0.000879022
	LOSS [training: 3.8214104574472336 | validation: 4.808311032697854]
	TIME [epoch: 9.74 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8020387442799928		[learning rate: 0.00087583]
	Learning Rate: 0.000875833
	LOSS [training: 3.8020387442799928 | validation: 4.809436158762046]
	TIME [epoch: 9.72 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7996151507298324		[learning rate: 0.00087265]
	Learning Rate: 0.000872654
	LOSS [training: 3.7996151507298324 | validation: 4.8001939326580105]
	TIME [epoch: 9.72 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7904883370989397		[learning rate: 0.00086949]
	Learning Rate: 0.000869487
	LOSS [training: 3.7904883370989397 | validation: 4.82373035856217]
	TIME [epoch: 9.74 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8002301803364262		[learning rate: 0.00086633]
	Learning Rate: 0.000866332
	LOSS [training: 3.8002301803364262 | validation: 4.824227674958291]
	TIME [epoch: 9.74 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.819392163084242		[learning rate: 0.00086319]
	Learning Rate: 0.000863188
	LOSS [training: 3.819392163084242 | validation: 4.823031284963649]
	TIME [epoch: 9.72 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8413367659931246		[learning rate: 0.00086006]
	Learning Rate: 0.000860055
	LOSS [training: 3.8413367659931246 | validation: 4.808161578596188]
	TIME [epoch: 9.72 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.813368749493148		[learning rate: 0.00085693]
	Learning Rate: 0.000856934
	LOSS [training: 3.813368749493148 | validation: 4.799226542228093]
	TIME [epoch: 9.74 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7939708927152522		[learning rate: 0.00085382]
	Learning Rate: 0.000853824
	LOSS [training: 3.7939708927152522 | validation: 4.815501213455451]
	TIME [epoch: 9.72 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.799955501994558		[learning rate: 0.00085073]
	Learning Rate: 0.000850726
	LOSS [training: 3.799955501994558 | validation: 4.845015934923169]
	TIME [epoch: 9.72 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8190040847796154		[learning rate: 0.00084764]
	Learning Rate: 0.000847638
	LOSS [training: 3.8190040847796154 | validation: 4.854676365453755]
	TIME [epoch: 9.71 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8033030614013925		[learning rate: 0.00084456]
	Learning Rate: 0.000844562
	LOSS [training: 3.8033030614013925 | validation: 4.814237821461687]
	TIME [epoch: 9.74 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8202200349892728		[learning rate: 0.0008415]
	Learning Rate: 0.000841497
	LOSS [training: 3.8202200349892728 | validation: 4.794784076962433]
	TIME [epoch: 9.72 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8041697076782817		[learning rate: 0.00083844]
	Learning Rate: 0.000838443
	LOSS [training: 3.8041697076782817 | validation: 4.813839276970855]
	TIME [epoch: 9.71 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.804892616712467		[learning rate: 0.0008354]
	Learning Rate: 0.000835401
	LOSS [training: 3.804892616712467 | validation: 4.797431111849]
	TIME [epoch: 9.71 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.794072296668277		[learning rate: 0.00083237]
	Learning Rate: 0.000832369
	LOSS [training: 3.794072296668277 | validation: 4.80548875766725]
	TIME [epoch: 9.73 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8124464704987133		[learning rate: 0.00082935]
	Learning Rate: 0.000829348
	LOSS [training: 3.8124464704987133 | validation: 4.85580906495859]
	TIME [epoch: 9.71 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.80637280699563		[learning rate: 0.00082634]
	Learning Rate: 0.000826338
	LOSS [training: 3.80637280699563 | validation: 4.810848626466411]
	TIME [epoch: 9.72 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7998820160534463		[learning rate: 0.00082334]
	Learning Rate: 0.00082334
	LOSS [training: 3.7998820160534463 | validation: 4.817879421317502]
	TIME [epoch: 9.73 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7978674774550925		[learning rate: 0.00082035]
	Learning Rate: 0.000820352
	LOSS [training: 3.7978674774550925 | validation: 4.813049224970086]
	TIME [epoch: 9.72 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7883073179634303		[learning rate: 0.00081737]
	Learning Rate: 0.000817375
	LOSS [training: 3.7883073179634303 | validation: 4.821681583495395]
	TIME [epoch: 9.72 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7923456004562595		[learning rate: 0.00081441]
	Learning Rate: 0.000814408
	LOSS [training: 3.7923456004562595 | validation: 4.798927413001981]
	TIME [epoch: 9.71 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7827635416398193		[learning rate: 0.00081145]
	Learning Rate: 0.000811453
	LOSS [training: 3.7827635416398193 | validation: 4.816234056093431]
	TIME [epoch: 9.73 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7927401373578533		[learning rate: 0.00080851]
	Learning Rate: 0.000808508
	LOSS [training: 3.7927401373578533 | validation: 4.819163519621689]
	TIME [epoch: 9.72 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.843981183529965		[learning rate: 0.00080557]
	Learning Rate: 0.000805574
	LOSS [training: 3.843981183529965 | validation: 4.810641363640383]
	TIME [epoch: 9.71 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7899100886353034		[learning rate: 0.00080265]
	Learning Rate: 0.00080265
	LOSS [training: 3.7899100886353034 | validation: 4.810130746190405]
	TIME [epoch: 9.72 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8077310613726696		[learning rate: 0.00079974]
	Learning Rate: 0.000799737
	LOSS [training: 3.8077310613726696 | validation: 4.8176744156375655]
	TIME [epoch: 9.74 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8071240566200104		[learning rate: 0.00079684]
	Learning Rate: 0.000796835
	LOSS [training: 3.8071240566200104 | validation: 4.81086888116314]
	TIME [epoch: 9.71 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.802447740108547		[learning rate: 0.00079394]
	Learning Rate: 0.000793943
	LOSS [training: 3.802447740108547 | validation: 4.871141496155679]
	TIME [epoch: 9.72 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8034198600295746		[learning rate: 0.00079106]
	Learning Rate: 0.000791062
	LOSS [training: 3.8034198600295746 | validation: 4.801759279334562]
	TIME [epoch: 9.73 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.803893388348807		[learning rate: 0.00078819]
	Learning Rate: 0.000788191
	LOSS [training: 3.803893388348807 | validation: 4.816730655950069]
	TIME [epoch: 9.72 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.796757140810275		[learning rate: 0.00078533]
	Learning Rate: 0.000785331
	LOSS [training: 3.796757140810275 | validation: 4.80191130700121]
	TIME [epoch: 9.71 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7935030170119823		[learning rate: 0.00078248]
	Learning Rate: 0.000782481
	LOSS [training: 3.7935030170119823 | validation: 4.819615107579856]
	TIME [epoch: 9.71 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8168078241564887		[learning rate: 0.00077964]
	Learning Rate: 0.000779641
	LOSS [training: 3.8168078241564887 | validation: 4.821748715615295]
	TIME [epoch: 9.72 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.80755050724684		[learning rate: 0.00077681]
	Learning Rate: 0.000776812
	LOSS [training: 3.80755050724684 | validation: 4.8195816405007]
	TIME [epoch: 9.71 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7975375187449374		[learning rate: 0.00077399]
	Learning Rate: 0.000773993
	LOSS [training: 3.7975375187449374 | validation: 4.827653702887025]
	TIME [epoch: 9.71 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.810375975033216		[learning rate: 0.00077118]
	Learning Rate: 0.000771184
	LOSS [training: 3.810375975033216 | validation: 4.832668492817641]
	TIME [epoch: 9.71 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8079465481472328		[learning rate: 0.00076839]
	Learning Rate: 0.000768385
	LOSS [training: 3.8079465481472328 | validation: 4.8779746574672505]
	TIME [epoch: 10.1 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.811126728561807		[learning rate: 0.0007656]
	Learning Rate: 0.000765597
	LOSS [training: 3.811126728561807 | validation: 4.822764245648476]
	TIME [epoch: 9.74 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.797799523133554		[learning rate: 0.00076282]
	Learning Rate: 0.000762818
	LOSS [training: 3.797799523133554 | validation: 4.845832238075503]
	TIME [epoch: 9.73 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.807393984921977		[learning rate: 0.00076005]
	Learning Rate: 0.00076005
	LOSS [training: 3.807393984921977 | validation: 4.85081060816046]
	TIME [epoch: 9.75 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8225343861803447		[learning rate: 0.00075729]
	Learning Rate: 0.000757292
	LOSS [training: 3.8225343861803447 | validation: 4.976785476063202]
	TIME [epoch: 9.75 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8650545272593377		[learning rate: 0.00075454]
	Learning Rate: 0.000754543
	LOSS [training: 3.8650545272593377 | validation: 4.806048835766904]
	TIME [epoch: 9.74 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.805319017544014		[learning rate: 0.00075181]
	Learning Rate: 0.000751805
	LOSS [training: 3.805319017544014 | validation: 4.822050542170035]
	TIME [epoch: 9.74 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8004973109946776		[learning rate: 0.00074908]
	Learning Rate: 0.000749077
	LOSS [training: 3.8004973109946776 | validation: 4.880080176976994]
	TIME [epoch: 9.76 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.822227872182089		[learning rate: 0.00074636]
	Learning Rate: 0.000746358
	LOSS [training: 3.822227872182089 | validation: 4.811772496075347]
	TIME [epoch: 9.74 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7921744353889886		[learning rate: 0.00074365]
	Learning Rate: 0.00074365
	LOSS [training: 3.7921744353889886 | validation: 4.798647521106967]
	TIME [epoch: 9.73 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.792074169123539		[learning rate: 0.00074095]
	Learning Rate: 0.000740951
	LOSS [training: 3.792074169123539 | validation: 4.800386638755877]
	TIME [epoch: 9.73 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8012765829591935		[learning rate: 0.00073826]
	Learning Rate: 0.000738262
	LOSS [training: 3.8012765829591935 | validation: 4.827207419519302]
	TIME [epoch: 9.75 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7847186478823955		[learning rate: 0.00073558]
	Learning Rate: 0.000735583
	LOSS [training: 3.7847186478823955 | validation: 4.8114707610654195]
	TIME [epoch: 9.74 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8102993321211516		[learning rate: 0.00073291]
	Learning Rate: 0.000732913
	LOSS [training: 3.8102993321211516 | validation: 4.804833291678938]
	TIME [epoch: 9.73 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.79091378223173		[learning rate: 0.00073025]
	Learning Rate: 0.000730254
	LOSS [training: 3.79091378223173 | validation: 4.828711729000656]
	TIME [epoch: 9.74 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.806578420208287		[learning rate: 0.0007276]
	Learning Rate: 0.000727603
	LOSS [training: 3.806578420208287 | validation: 4.821157654519336]
	TIME [epoch: 9.76 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7916464682239566		[learning rate: 0.00072496]
	Learning Rate: 0.000724963
	LOSS [training: 3.7916464682239566 | validation: 4.816793084535]
	TIME [epoch: 9.74 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.783025796102359		[learning rate: 0.00072233]
	Learning Rate: 0.000722332
	LOSS [training: 3.783025796102359 | validation: 4.821604663395558]
	TIME [epoch: 9.73 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7822908070073686		[learning rate: 0.00071971]
	Learning Rate: 0.000719711
	LOSS [training: 3.7822908070073686 | validation: 4.817261714393162]
	TIME [epoch: 9.75 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8123143284434646		[learning rate: 0.0007171]
	Learning Rate: 0.000717099
	LOSS [training: 3.8123143284434646 | validation: 4.811684801598702]
	TIME [epoch: 9.75 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.794752083871664		[learning rate: 0.0007145]
	Learning Rate: 0.000714496
	LOSS [training: 3.794752083871664 | validation: 4.824243826147718]
	TIME [epoch: 9.74 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7864755356159456		[learning rate: 0.0007119]
	Learning Rate: 0.000711903
	LOSS [training: 3.7864755356159456 | validation: 4.8181378612075845]
	TIME [epoch: 9.74 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7856829129948957		[learning rate: 0.00070932]
	Learning Rate: 0.00070932
	LOSS [training: 3.7856829129948957 | validation: 4.816737237569384]
	TIME [epoch: 9.76 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8016386467465155		[learning rate: 0.00070675]
	Learning Rate: 0.000706746
	LOSS [training: 3.8016386467465155 | validation: 4.810566084771477]
	TIME [epoch: 9.74 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.826933463416504		[learning rate: 0.00070418]
	Learning Rate: 0.000704181
	LOSS [training: 3.826933463416504 | validation: 4.8077738836054]
	TIME [epoch: 9.74 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7879031519053266		[learning rate: 0.00070163]
	Learning Rate: 0.000701625
	LOSS [training: 3.7879031519053266 | validation: 4.814583388454082]
	TIME [epoch: 9.73 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7909972070855424		[learning rate: 0.00069908]
	Learning Rate: 0.000699079
	LOSS [training: 3.7909972070855424 | validation: 4.798528490617517]
	TIME [epoch: 9.76 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780355183831081		[learning rate: 0.00069654]
	Learning Rate: 0.000696542
	LOSS [training: 3.780355183831081 | validation: 4.803456497832939]
	TIME [epoch: 9.73 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7883853883815575		[learning rate: 0.00069401]
	Learning Rate: 0.000694014
	LOSS [training: 3.7883853883815575 | validation: 4.81938840443841]
	TIME [epoch: 9.74 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.790351180039232		[learning rate: 0.0006915]
	Learning Rate: 0.000691496
	LOSS [training: 3.790351180039232 | validation: 4.837799392717436]
	TIME [epoch: 9.75 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.803375416940308		[learning rate: 0.00068899]
	Learning Rate: 0.000688986
	LOSS [training: 3.803375416940308 | validation: 4.813572700463699]
	TIME [epoch: 9.74 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.789330356148119		[learning rate: 0.00068649]
	Learning Rate: 0.000686486
	LOSS [training: 3.789330356148119 | validation: 4.822823955182899]
	TIME [epoch: 9.73 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7952578071331082		[learning rate: 0.00068399]
	Learning Rate: 0.000683994
	LOSS [training: 3.7952578071331082 | validation: 4.826253195744901]
	TIME [epoch: 9.73 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7937982062671636		[learning rate: 0.00068151]
	Learning Rate: 0.000681512
	LOSS [training: 3.7937982062671636 | validation: 4.8077614864387295]
	TIME [epoch: 9.75 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.810031106059357		[learning rate: 0.00067904]
	Learning Rate: 0.000679039
	LOSS [training: 3.810031106059357 | validation: 4.846906453215613]
	TIME [epoch: 9.73 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8092528223951105		[learning rate: 0.00067657]
	Learning Rate: 0.000676575
	LOSS [training: 3.8092528223951105 | validation: 4.790089785181436]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_841.pth
	Model improved!!!
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7838755402177613		[learning rate: 0.00067412]
	Learning Rate: 0.00067412
	LOSS [training: 3.7838755402177613 | validation: 4.826724691625137]
	TIME [epoch: 9.73 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.787382192789793		[learning rate: 0.00067167]
	Learning Rate: 0.000671673
	LOSS [training: 3.787382192789793 | validation: 4.805966422128091]
	TIME [epoch: 9.75 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7794717464313137		[learning rate: 0.00066924]
	Learning Rate: 0.000669235
	LOSS [training: 3.7794717464313137 | validation: 4.80141929297754]
	TIME [epoch: 9.72 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778807128193045		[learning rate: 0.00066681]
	Learning Rate: 0.000666807
	LOSS [training: 3.778807128193045 | validation: 4.803278225790096]
	TIME [epoch: 9.72 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7972886012238503		[learning rate: 0.00066439]
	Learning Rate: 0.000664387
	LOSS [training: 3.7972886012238503 | validation: 4.806321248188292]
	TIME [epoch: 9.73 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782899602582021		[learning rate: 0.00066198]
	Learning Rate: 0.000661976
	LOSS [training: 3.782899602582021 | validation: 4.8143588113736575]
	TIME [epoch: 9.74 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.788240302749107		[learning rate: 0.00065957]
	Learning Rate: 0.000659573
	LOSS [training: 3.788240302749107 | validation: 4.806633020819216]
	TIME [epoch: 9.73 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8043038646985723		[learning rate: 0.00065718]
	Learning Rate: 0.00065718
	LOSS [training: 3.8043038646985723 | validation: 4.851103079937935]
	TIME [epoch: 9.72 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.83197560833759		[learning rate: 0.00065479]
	Learning Rate: 0.000654795
	LOSS [training: 3.83197560833759 | validation: 4.822043288834701]
	TIME [epoch: 9.74 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.821831693028436		[learning rate: 0.00065242]
	Learning Rate: 0.000652419
	LOSS [training: 3.821831693028436 | validation: 4.8069281801597254]
	TIME [epoch: 9.73 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.81049381768194		[learning rate: 0.00065005]
	Learning Rate: 0.000650051
	LOSS [training: 3.81049381768194 | validation: 4.803044002881258]
	TIME [epoch: 9.72 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7885312838953475		[learning rate: 0.00064769]
	Learning Rate: 0.000647692
	LOSS [training: 3.7885312838953475 | validation: 4.800245119533751]
	TIME [epoch: 9.72 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7892922959000748		[learning rate: 0.00064534]
	Learning Rate: 0.000645341
	LOSS [training: 3.7892922959000748 | validation: 4.8120109526061245]
	TIME [epoch: 9.74 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.789235832713519		[learning rate: 0.000643]
	Learning Rate: 0.000642999
	LOSS [training: 3.789235832713519 | validation: 4.8020647895074955]
	TIME [epoch: 9.73 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8174277058490858		[learning rate: 0.00064067]
	Learning Rate: 0.000640666
	LOSS [training: 3.8174277058490858 | validation: 4.821925924469082]
	TIME [epoch: 9.72 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.791271216722536		[learning rate: 0.00063834]
	Learning Rate: 0.000638341
	LOSS [training: 3.791271216722536 | validation: 4.832542401921649]
	TIME [epoch: 9.72 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7930363608543347		[learning rate: 0.00063602]
	Learning Rate: 0.000636024
	LOSS [training: 3.7930363608543347 | validation: 4.836968030109085]
	TIME [epoch: 9.74 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7936402960498476		[learning rate: 0.00063372]
	Learning Rate: 0.000633716
	LOSS [training: 3.7936402960498476 | validation: 4.810371464364302]
	TIME [epoch: 9.72 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.788035880425683		[learning rate: 0.00063142]
	Learning Rate: 0.000631416
	LOSS [training: 3.788035880425683 | validation: 4.8118596106333325]
	TIME [epoch: 9.73 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.805037989907889		[learning rate: 0.00062912]
	Learning Rate: 0.000629125
	LOSS [training: 3.805037989907889 | validation: 4.812558161340062]
	TIME [epoch: 9.74 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7935092276498237		[learning rate: 0.00062684]
	Learning Rate: 0.000626842
	LOSS [training: 3.7935092276498237 | validation: 4.845916110936717]
	TIME [epoch: 9.73 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8103418916688154		[learning rate: 0.00062457]
	Learning Rate: 0.000624567
	LOSS [training: 3.8103418916688154 | validation: 4.871190191008301]
	TIME [epoch: 9.72 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8382095042937037		[learning rate: 0.0006223]
	Learning Rate: 0.0006223
	LOSS [training: 3.8382095042937037 | validation: 4.813895168369037]
	TIME [epoch: 9.72 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7870402008073953		[learning rate: 0.00062004]
	Learning Rate: 0.000620042
	LOSS [training: 3.7870402008073953 | validation: 4.798321853747705]
	TIME [epoch: 9.74 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8029322887042563		[learning rate: 0.00061779]
	Learning Rate: 0.000617792
	LOSS [training: 3.8029322887042563 | validation: 4.86396126899076]
	TIME [epoch: 9.73 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7987813428020125		[learning rate: 0.00061555]
	Learning Rate: 0.00061555
	LOSS [training: 3.7987813428020125 | validation: 4.805209314525639]
	TIME [epoch: 9.72 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7912702364894386		[learning rate: 0.00061332]
	Learning Rate: 0.000613316
	LOSS [training: 3.7912702364894386 | validation: 4.829219005474203]
	TIME [epoch: 9.73 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7909108838741665		[learning rate: 0.00061109]
	Learning Rate: 0.00061109
	LOSS [training: 3.7909108838741665 | validation: 4.832538196775163]
	TIME [epoch: 9.74 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.791348098796199		[learning rate: 0.00060887]
	Learning Rate: 0.000608872
	LOSS [training: 3.791348098796199 | validation: 4.8029438093067265]
	TIME [epoch: 9.72 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.792342438168331		[learning rate: 0.00060666]
	Learning Rate: 0.000606663
	LOSS [training: 3.792342438168331 | validation: 4.8316171922192614]
	TIME [epoch: 9.72 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.814852172267925		[learning rate: 0.00060446]
	Learning Rate: 0.000604461
	LOSS [training: 3.814852172267925 | validation: 4.808099471851764]
	TIME [epoch: 9.73 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7832842770027364		[learning rate: 0.00060227]
	Learning Rate: 0.000602268
	LOSS [training: 3.7832842770027364 | validation: 4.842300538973377]
	TIME [epoch: 9.74 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.81149610612387		[learning rate: 0.00060008]
	Learning Rate: 0.000600082
	LOSS [training: 3.81149610612387 | validation: 4.808542113197517]
	TIME [epoch: 9.72 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8032880857524107		[learning rate: 0.0005979]
	Learning Rate: 0.000597904
	LOSS [training: 3.8032880857524107 | validation: 4.802027786396457]
	TIME [epoch: 9.72 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8077334503007507		[learning rate: 0.00059573]
	Learning Rate: 0.000595734
	LOSS [training: 3.8077334503007507 | validation: 4.807842072563153]
	TIME [epoch: 9.75 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.788511919062431		[learning rate: 0.00059357]
	Learning Rate: 0.000593572
	LOSS [training: 3.788511919062431 | validation: 4.808553068541356]
	TIME [epoch: 9.73 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7855271074877535		[learning rate: 0.00059142]
	Learning Rate: 0.000591418
	LOSS [training: 3.7855271074877535 | validation: 4.848057913425715]
	TIME [epoch: 9.73 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.815694962257915		[learning rate: 0.00058927]
	Learning Rate: 0.000589272
	LOSS [training: 3.815694962257915 | validation: 4.807841052196345]
	TIME [epoch: 9.72 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.792419260931426		[learning rate: 0.00058713]
	Learning Rate: 0.000587133
	LOSS [training: 3.792419260931426 | validation: 4.82232377967831]
	TIME [epoch: 9.75 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8005093855050314		[learning rate: 0.000585]
	Learning Rate: 0.000585003
	LOSS [training: 3.8005093855050314 | validation: 4.821264508741806]
	TIME [epoch: 9.72 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7823194643464135		[learning rate: 0.00058288]
	Learning Rate: 0.00058288
	LOSS [training: 3.7823194643464135 | validation: 4.79801376687027]
	TIME [epoch: 9.72 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.80089729233982		[learning rate: 0.00058076]
	Learning Rate: 0.000580764
	LOSS [training: 3.80089729233982 | validation: 4.807641327842159]
	TIME [epoch: 9.72 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.786030558477956		[learning rate: 0.00057866]
	Learning Rate: 0.000578657
	LOSS [training: 3.786030558477956 | validation: 4.796619457466768]
	TIME [epoch: 9.75 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7872580545011374		[learning rate: 0.00057656]
	Learning Rate: 0.000576557
	LOSS [training: 3.7872580545011374 | validation: 4.797457729513733]
	TIME [epoch: 9.72 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7903166039880274		[learning rate: 0.00057446]
	Learning Rate: 0.000574465
	LOSS [training: 3.7903166039880274 | validation: 4.816252431055739]
	TIME [epoch: 9.73 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.793361612187796		[learning rate: 0.00057238]
	Learning Rate: 0.00057238
	LOSS [training: 3.793361612187796 | validation: 4.816555073604924]
	TIME [epoch: 9.73 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7874211392394797		[learning rate: 0.0005703]
	Learning Rate: 0.000570303
	LOSS [training: 3.7874211392394797 | validation: 4.798078965203208]
	TIME [epoch: 9.73 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7831934680700217		[learning rate: 0.00056823]
	Learning Rate: 0.000568233
	LOSS [training: 3.7831934680700217 | validation: 4.801059919871822]
	TIME [epoch: 9.72 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.790224142604662		[learning rate: 0.00056617]
	Learning Rate: 0.000566171
	LOSS [training: 3.790224142604662 | validation: 4.8047866343848105]
	TIME [epoch: 9.73 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7862093484202015		[learning rate: 0.00056412]
	Learning Rate: 0.000564116
	LOSS [training: 3.7862093484202015 | validation: 4.830012371784668]
	TIME [epoch: 9.74 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8070022171150284		[learning rate: 0.00056207]
	Learning Rate: 0.000562069
	LOSS [training: 3.8070022171150284 | validation: 4.796795816802894]
	TIME [epoch: 9.73 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.799012033285721		[learning rate: 0.00056003]
	Learning Rate: 0.000560029
	LOSS [training: 3.799012033285721 | validation: 4.843339430686026]
	TIME [epoch: 9.73 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.81338088684979		[learning rate: 0.000558]
	Learning Rate: 0.000557997
	LOSS [training: 3.81338088684979 | validation: 4.828155230049841]
	TIME [epoch: 9.73 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.792968716110606		[learning rate: 0.00055597]
	Learning Rate: 0.000555972
	LOSS [training: 3.792968716110606 | validation: 4.811669147019844]
	TIME [epoch: 9.75 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8089242143930884		[learning rate: 0.00055395]
	Learning Rate: 0.000553954
	LOSS [training: 3.8089242143930884 | validation: 4.8016121560092735]
	TIME [epoch: 9.73 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7916100838338758		[learning rate: 0.00055194]
	Learning Rate: 0.000551944
	LOSS [training: 3.7916100838338758 | validation: 4.8087160634175055]
	TIME [epoch: 9.73 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7934562940605154		[learning rate: 0.00054994]
	Learning Rate: 0.000549941
	LOSS [training: 3.7934562940605154 | validation: 4.8060268066672425]
	TIME [epoch: 9.73 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.789970138744331		[learning rate: 0.00054794]
	Learning Rate: 0.000547945
	LOSS [training: 3.789970138744331 | validation: 4.807228833834213]
	TIME [epoch: 9.74 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7892890200315046		[learning rate: 0.00054596]
	Learning Rate: 0.000545956
	LOSS [training: 3.7892890200315046 | validation: 4.808342095163318]
	TIME [epoch: 9.73 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.784825723914463		[learning rate: 0.00054397]
	Learning Rate: 0.000543975
	LOSS [training: 3.784825723914463 | validation: 4.8472114932909]
	TIME [epoch: 9.73 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.798347307619392		[learning rate: 0.000542]
	Learning Rate: 0.000542001
	LOSS [training: 3.798347307619392 | validation: 4.82549852545923]
	TIME [epoch: 9.74 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7987161982687487		[learning rate: 0.00054003]
	Learning Rate: 0.000540034
	LOSS [training: 3.7987161982687487 | validation: 4.841732859180316]
	TIME [epoch: 9.73 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7915502717614005		[learning rate: 0.00053807]
	Learning Rate: 0.000538074
	LOSS [training: 3.7915502717614005 | validation: 4.804258344475877]
	TIME [epoch: 9.73 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7940708452175747		[learning rate: 0.00053612]
	Learning Rate: 0.000536121
	LOSS [training: 3.7940708452175747 | validation: 4.807506772138287]
	TIME [epoch: 9.72 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8001571439869424		[learning rate: 0.00053418]
	Learning Rate: 0.000534176
	LOSS [training: 3.8001571439869424 | validation: 4.813038492180781]
	TIME [epoch: 9.75 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7875982391605634		[learning rate: 0.00053224]
	Learning Rate: 0.000532237
	LOSS [training: 3.7875982391605634 | validation: 4.8172498512256015]
	TIME [epoch: 9.73 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7941315177597827		[learning rate: 0.00053031]
	Learning Rate: 0.000530306
	LOSS [training: 3.7941315177597827 | validation: 4.811538988193765]
	TIME [epoch: 9.73 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7889511973602077		[learning rate: 0.00052838]
	Learning Rate: 0.000528381
	LOSS [training: 3.7889511973602077 | validation: 4.7983104124958995]
	TIME [epoch: 9.73 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.785554584193156		[learning rate: 0.00052646]
	Learning Rate: 0.000526464
	LOSS [training: 3.785554584193156 | validation: 4.818609408742928]
	TIME [epoch: 9.74 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7817145003233974		[learning rate: 0.00052455]
	Learning Rate: 0.000524553
	LOSS [training: 3.7817145003233974 | validation: 4.805497040311397]
	TIME [epoch: 9.73 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7883856802319116		[learning rate: 0.00052265]
	Learning Rate: 0.000522649
	LOSS [training: 3.7883856802319116 | validation: 4.800842504055021]
	TIME [epoch: 9.73 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7965229529651743		[learning rate: 0.00052075]
	Learning Rate: 0.000520753
	LOSS [training: 3.7965229529651743 | validation: 4.819509435632012]
	TIME [epoch: 9.74 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.808828279153441		[learning rate: 0.00051886]
	Learning Rate: 0.000518863
	LOSS [training: 3.808828279153441 | validation: 4.811631448479367]
	TIME [epoch: 9.73 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7990008194822615		[learning rate: 0.00051698]
	Learning Rate: 0.00051698
	LOSS [training: 3.7990008194822615 | validation: 4.807501664495688]
	TIME [epoch: 9.73 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7825346882001796		[learning rate: 0.0005151]
	Learning Rate: 0.000515104
	LOSS [training: 3.7825346882001796 | validation: 4.801920856243657]
	TIME [epoch: 9.73 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7839000768099305		[learning rate: 0.00051323]
	Learning Rate: 0.000513235
	LOSS [training: 3.7839000768099305 | validation: 4.803764426958696]
	TIME [epoch: 9.74 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.811309983833425		[learning rate: 0.00051137]
	Learning Rate: 0.000511372
	LOSS [training: 3.811309983833425 | validation: 4.814130443757207]
	TIME [epoch: 9.73 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.795900911401886		[learning rate: 0.00050952]
	Learning Rate: 0.000509516
	LOSS [training: 3.795900911401886 | validation: 4.81303252050609]
	TIME [epoch: 9.73 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7868302167989634		[learning rate: 0.00050767]
	Learning Rate: 0.000507667
	LOSS [training: 3.7868302167989634 | validation: 4.8092471964674814]
	TIME [epoch: 9.72 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7895856338681995		[learning rate: 0.00050582]
	Learning Rate: 0.000505825
	LOSS [training: 3.7895856338681995 | validation: 4.791628516945918]
	TIME [epoch: 9.75 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7895014363865385		[learning rate: 0.00050399]
	Learning Rate: 0.000503989
	LOSS [training: 3.7895014363865385 | validation: 4.812706591548284]
	TIME [epoch: 9.73 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780749622017064		[learning rate: 0.00050216]
	Learning Rate: 0.00050216
	LOSS [training: 3.780749622017064 | validation: 4.810036258077226]
	TIME [epoch: 9.73 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.797534172949233		[learning rate: 0.00050034]
	Learning Rate: 0.000500338
	LOSS [training: 3.797534172949233 | validation: 4.801651888888527]
	TIME [epoch: 9.73 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7820333964821216		[learning rate: 0.00049852]
	Learning Rate: 0.000498522
	LOSS [training: 3.7820333964821216 | validation: 4.801257255144255]
	TIME [epoch: 9.75 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7896877678414134		[learning rate: 0.00049671]
	Learning Rate: 0.000496713
	LOSS [training: 3.7896877678414134 | validation: 4.797493697981209]
	TIME [epoch: 9.73 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7868359389700457		[learning rate: 0.00049491]
	Learning Rate: 0.00049491
	LOSS [training: 3.7868359389700457 | validation: 4.795448748731894]
	TIME [epoch: 9.73 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7831741238331587		[learning rate: 0.00049311]
	Learning Rate: 0.000493114
	LOSS [training: 3.7831741238331587 | validation: 4.796309388076054]
	TIME [epoch: 9.75 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7856901290846197		[learning rate: 0.00049132]
	Learning Rate: 0.000491325
	LOSS [training: 3.7856901290846197 | validation: 4.7956808179138]
	TIME [epoch: 9.73 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7853543345566423		[learning rate: 0.00048954]
	Learning Rate: 0.000489542
	LOSS [training: 3.7853543345566423 | validation: 4.832572615612005]
	TIME [epoch: 9.73 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8099120321433815		[learning rate: 0.00048776]
	Learning Rate: 0.000487765
	LOSS [training: 3.8099120321433815 | validation: 4.800467341993533]
	TIME [epoch: 9.73 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7935171162608547		[learning rate: 0.00048599]
	Learning Rate: 0.000485995
	LOSS [training: 3.7935171162608547 | validation: 4.809805229147582]
	TIME [epoch: 9.75 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7951737569535156		[learning rate: 0.00048423]
	Learning Rate: 0.000484231
	LOSS [training: 3.7951737569535156 | validation: 4.795025802976254]
	TIME [epoch: 9.73 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.785293642109829		[learning rate: 0.00048247]
	Learning Rate: 0.000482474
	LOSS [training: 3.785293642109829 | validation: 4.796722149793636]
	TIME [epoch: 9.73 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77660124604811		[learning rate: 0.00048072]
	Learning Rate: 0.000480723
	LOSS [training: 3.77660124604811 | validation: 4.800236138619162]
	TIME [epoch: 9.73 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8065855193493774		[learning rate: 0.00047898]
	Learning Rate: 0.000478978
	LOSS [training: 3.8065855193493774 | validation: 4.815072863220936]
	TIME [epoch: 9.75 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779678287175949		[learning rate: 0.00047724]
	Learning Rate: 0.00047724
	LOSS [training: 3.779678287175949 | validation: 4.805315857401453]
	TIME [epoch: 9.73 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7851802288314134		[learning rate: 0.00047551]
	Learning Rate: 0.000475508
	LOSS [training: 3.7851802288314134 | validation: 4.800620492055298]
	TIME [epoch: 9.73 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7772015622469874		[learning rate: 0.00047378]
	Learning Rate: 0.000473782
	LOSS [training: 3.7772015622469874 | validation: 4.819966465261819]
	TIME [epoch: 9.73 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.78717006236401		[learning rate: 0.00047206]
	Learning Rate: 0.000472063
	LOSS [training: 3.78717006236401 | validation: 4.814348778327066]
	TIME [epoch: 9.75 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7969426456747555		[learning rate: 0.00047035]
	Learning Rate: 0.00047035
	LOSS [training: 3.7969426456747555 | validation: 4.8576686777172915]
	TIME [epoch: 9.73 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.810261865837886		[learning rate: 0.00046864]
	Learning Rate: 0.000468643
	LOSS [training: 3.810261865837886 | validation: 4.800423873812837]
	TIME [epoch: 9.73 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7901743196603475		[learning rate: 0.00046694]
	Learning Rate: 0.000466942
	LOSS [training: 3.7901743196603475 | validation: 4.8056521534709145]
	TIME [epoch: 9.75 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7922394184883927		[learning rate: 0.00046525]
	Learning Rate: 0.000465248
	LOSS [training: 3.7922394184883927 | validation: 4.8038529106966665]
	TIME [epoch: 9.73 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.790333774162849		[learning rate: 0.00046356]
	Learning Rate: 0.000463559
	LOSS [training: 3.790333774162849 | validation: 4.828157386702505]
	TIME [epoch: 9.73 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.790807125755456		[learning rate: 0.00046188]
	Learning Rate: 0.000461877
	LOSS [training: 3.790807125755456 | validation: 4.817876283982979]
	TIME [epoch: 9.73 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7974572707970493		[learning rate: 0.0004602]
	Learning Rate: 0.000460201
	LOSS [training: 3.7974572707970493 | validation: 4.809320512319634]
	TIME [epoch: 9.75 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7904659346664373		[learning rate: 0.00045853]
	Learning Rate: 0.000458531
	LOSS [training: 3.7904659346664373 | validation: 4.801716374714634]
	TIME [epoch: 9.73 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.786404598811008		[learning rate: 0.00045687]
	Learning Rate: 0.000456867
	LOSS [training: 3.786404598811008 | validation: 4.818027246857429]
	TIME [epoch: 9.73 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782160104194265		[learning rate: 0.00045521]
	Learning Rate: 0.000455209
	LOSS [training: 3.782160104194265 | validation: 4.800145246239929]
	TIME [epoch: 9.74 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7953125288713645		[learning rate: 0.00045356]
	Learning Rate: 0.000453557
	LOSS [training: 3.7953125288713645 | validation: 4.801058418476832]
	TIME [epoch: 9.75 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7811211742872883		[learning rate: 0.00045191]
	Learning Rate: 0.000451911
	LOSS [training: 3.7811211742872883 | validation: 4.793671926301987]
	TIME [epoch: 9.73 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7879763929069057		[learning rate: 0.00045027]
	Learning Rate: 0.000450271
	LOSS [training: 3.7879763929069057 | validation: 4.828500850446121]
	TIME [epoch: 9.72 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7887252051780798		[learning rate: 0.00044864]
	Learning Rate: 0.000448637
	LOSS [training: 3.7887252051780798 | validation: 4.8248605844095165]
	TIME [epoch: 9.76 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.800736087910225		[learning rate: 0.00044701]
	Learning Rate: 0.000447009
	LOSS [training: 3.800736087910225 | validation: 4.810949290307952]
	TIME [epoch: 9.73 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7821088700252674		[learning rate: 0.00044539]
	Learning Rate: 0.000445386
	LOSS [training: 3.7821088700252674 | validation: 4.812167322707041]
	TIME [epoch: 9.73 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.783626746121039		[learning rate: 0.00044377]
	Learning Rate: 0.00044377
	LOSS [training: 3.783626746121039 | validation: 4.803205942748518]
	TIME [epoch: 9.73 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8143682702037		[learning rate: 0.00044216]
	Learning Rate: 0.00044216
	LOSS [training: 3.8143682702037 | validation: 4.793886087802593]
	TIME [epoch: 9.75 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782810113271419		[learning rate: 0.00044055]
	Learning Rate: 0.000440555
	LOSS [training: 3.782810113271419 | validation: 4.800735083814841]
	TIME [epoch: 9.73 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777333925035083		[learning rate: 0.00043896]
	Learning Rate: 0.000438956
	LOSS [training: 3.777333925035083 | validation: 4.810130523634661]
	TIME [epoch: 9.73 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7913947081925414		[learning rate: 0.00043736]
	Learning Rate: 0.000437363
	LOSS [training: 3.7913947081925414 | validation: 4.8045670074387195]
	TIME [epoch: 9.73 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7892863627382765		[learning rate: 0.00043578]
	Learning Rate: 0.000435776
	LOSS [training: 3.7892863627382765 | validation: 4.8177094377492145]
	TIME [epoch: 9.75 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7928803657780095		[learning rate: 0.00043419]
	Learning Rate: 0.000434194
	LOSS [training: 3.7928803657780095 | validation: 4.803880866689442]
	TIME [epoch: 9.72 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7880167571840544		[learning rate: 0.00043262]
	Learning Rate: 0.000432619
	LOSS [training: 3.7880167571840544 | validation: 4.813350058050039]
	TIME [epoch: 9.72 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777201173568249		[learning rate: 0.00043105]
	Learning Rate: 0.000431049
	LOSS [training: 3.777201173568249 | validation: 4.815378147131179]
	TIME [epoch: 9.73 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.786489452725141		[learning rate: 0.00042948]
	Learning Rate: 0.000429484
	LOSS [training: 3.786489452725141 | validation: 4.801948321150242]
	TIME [epoch: 9.73 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7878572335008207		[learning rate: 0.00042793]
	Learning Rate: 0.000427926
	LOSS [training: 3.7878572335008207 | validation: 4.80670016517731]
	TIME [epoch: 9.72 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7843344319047505		[learning rate: 0.00042637]
	Learning Rate: 0.000426373
	LOSS [training: 3.7843344319047505 | validation: 4.8138257328246015]
	TIME [epoch: 9.72 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7874662427890633		[learning rate: 0.00042483]
	Learning Rate: 0.000424825
	LOSS [training: 3.7874662427890633 | validation: 4.805822935692329]
	TIME [epoch: 9.75 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.783898301164649		[learning rate: 0.00042328]
	Learning Rate: 0.000423284
	LOSS [training: 3.783898301164649 | validation: 4.818533990447007]
	TIME [epoch: 9.73 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7886277981826595		[learning rate: 0.00042175]
	Learning Rate: 0.000421748
	LOSS [training: 3.7886277981826595 | validation: 4.804305631458231]
	TIME [epoch: 9.73 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7854838351802407		[learning rate: 0.00042022]
	Learning Rate: 0.000420217
	LOSS [training: 3.7854838351802407 | validation: 4.823431129105927]
	TIME [epoch: 9.72 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.794601522307835		[learning rate: 0.00041869]
	Learning Rate: 0.000418692
	LOSS [training: 3.794601522307835 | validation: 4.836085386575902]
	TIME [epoch: 9.74 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.789943587246701		[learning rate: 0.00041717]
	Learning Rate: 0.000417173
	LOSS [training: 3.789943587246701 | validation: 4.810958273486117]
	TIME [epoch: 9.72 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7811638061818145		[learning rate: 0.00041566]
	Learning Rate: 0.000415659
	LOSS [training: 3.7811638061818145 | validation: 4.826986660031668]
	TIME [epoch: 9.72 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782641464575076		[learning rate: 0.00041415]
	Learning Rate: 0.00041415
	LOSS [training: 3.782641464575076 | validation: 4.804639150844204]
	TIME [epoch: 9.72 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781979809331186		[learning rate: 0.00041265]
	Learning Rate: 0.000412647
	LOSS [training: 3.781979809331186 | validation: 4.81589548234626]
	TIME [epoch: 9.74 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7934122727795923		[learning rate: 0.00041115]
	Learning Rate: 0.00041115
	LOSS [training: 3.7934122727795923 | validation: 4.81852785150225]
	TIME [epoch: 9.73 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8036638313044038		[learning rate: 0.00040966]
	Learning Rate: 0.000409658
	LOSS [training: 3.8036638313044038 | validation: 4.808740187852584]
	TIME [epoch: 9.72 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8041250125016974		[learning rate: 0.00040817]
	Learning Rate: 0.000408171
	LOSS [training: 3.8041250125016974 | validation: 4.793739755172819]
	TIME [epoch: 9.74 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.787264697407843		[learning rate: 0.00040669]
	Learning Rate: 0.00040669
	LOSS [training: 3.787264697407843 | validation: 4.821415456286167]
	TIME [epoch: 9.73 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.797688569036557		[learning rate: 0.00040521]
	Learning Rate: 0.000405214
	LOSS [training: 3.797688569036557 | validation: 4.799980676942853]
	TIME [epoch: 9.72 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7718168879148335		[learning rate: 0.00040374]
	Learning Rate: 0.000403743
	LOSS [training: 3.7718168879148335 | validation: 4.797572276388598]
	TIME [epoch: 9.72 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8002995595798255		[learning rate: 0.00040228]
	Learning Rate: 0.000402278
	LOSS [training: 3.8002995595798255 | validation: 4.804095993745715]
	TIME [epoch: 9.74 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781602358307805		[learning rate: 0.00040082]
	Learning Rate: 0.000400818
	LOSS [training: 3.781602358307805 | validation: 4.813857666520954]
	TIME [epoch: 9.72 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7912150416142354		[learning rate: 0.00039936]
	Learning Rate: 0.000399364
	LOSS [training: 3.7912150416142354 | validation: 4.801759340957309]
	TIME [epoch: 9.72 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778167099190701		[learning rate: 0.00039791]
	Learning Rate: 0.000397914
	LOSS [training: 3.778167099190701 | validation: 4.815120610734197]
	TIME [epoch: 9.72 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7873506742554026		[learning rate: 0.00039647]
	Learning Rate: 0.00039647
	LOSS [training: 3.7873506742554026 | validation: 4.820844499961305]
	TIME [epoch: 9.74 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7804545703644004		[learning rate: 0.00039503]
	Learning Rate: 0.000395031
	LOSS [training: 3.7804545703644004 | validation: 4.811015108019524]
	TIME [epoch: 9.72 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8052705556807402		[learning rate: 0.0003936]
	Learning Rate: 0.000393598
	LOSS [training: 3.8052705556807402 | validation: 4.863362355415446]
	TIME [epoch: 9.72 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7994901201527633		[learning rate: 0.00039217]
	Learning Rate: 0.000392169
	LOSS [training: 3.7994901201527633 | validation: 4.809360781080881]
	TIME [epoch: 9.73 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7851569059706547		[learning rate: 0.00039075]
	Learning Rate: 0.000390746
	LOSS [training: 3.7851569059706547 | validation: 4.804981699864113]
	TIME [epoch: 9.73 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.787790713645652		[learning rate: 0.00038933]
	Learning Rate: 0.000389328
	LOSS [training: 3.787790713645652 | validation: 4.808043417465333]
	TIME [epoch: 9.73 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.784528637184878		[learning rate: 0.00038792]
	Learning Rate: 0.000387915
	LOSS [training: 3.784528637184878 | validation: 4.79625539743241]
	TIME [epoch: 9.72 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7872378511838534		[learning rate: 0.00038651]
	Learning Rate: 0.000386508
	LOSS [training: 3.7872378511838534 | validation: 4.803817134214942]
	TIME [epoch: 9.74 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.793718130210169		[learning rate: 0.0003851]
	Learning Rate: 0.000385105
	LOSS [training: 3.793718130210169 | validation: 4.832982992169201]
	TIME [epoch: 9.72 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7879682831619164		[learning rate: 0.00038371]
	Learning Rate: 0.000383707
	LOSS [training: 3.7879682831619164 | validation: 4.818308644208895]
	TIME [epoch: 9.73 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7930739462999115		[learning rate: 0.00038231]
	Learning Rate: 0.000382315
	LOSS [training: 3.7930739462999115 | validation: 4.813576251736239]
	TIME [epoch: 9.72 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7905344067311697		[learning rate: 0.00038093]
	Learning Rate: 0.000380927
	LOSS [training: 3.7905344067311697 | validation: 4.824150854045326]
	TIME [epoch: 9.74 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7918782147605397		[learning rate: 0.00037954]
	Learning Rate: 0.000379545
	LOSS [training: 3.7918782147605397 | validation: 4.825488262291539]
	TIME [epoch: 9.72 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7849873431787593		[learning rate: 0.00037817]
	Learning Rate: 0.000378167
	LOSS [training: 3.7849873431787593 | validation: 4.839332795814213]
	TIME [epoch: 9.73 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7860219584686376		[learning rate: 0.0003768]
	Learning Rate: 0.000376795
	LOSS [training: 3.7860219584686376 | validation: 4.850888979459298]
	TIME [epoch: 9.73 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.802495028240698		[learning rate: 0.00037543]
	Learning Rate: 0.000375428
	LOSS [training: 3.802495028240698 | validation: 4.810362566509991]
	TIME [epoch: 9.75 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.788507355155258		[learning rate: 0.00037407]
	Learning Rate: 0.000374065
	LOSS [training: 3.788507355155258 | validation: 4.8236842031026015]
	TIME [epoch: 9.73 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7939991096635546		[learning rate: 0.00037271]
	Learning Rate: 0.000372708
	LOSS [training: 3.7939991096635546 | validation: 4.7988475006929505]
	TIME [epoch: 9.73 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.795711319406176		[learning rate: 0.00037136]
	Learning Rate: 0.000371355
	LOSS [training: 3.795711319406176 | validation: 4.821312663933478]
	TIME [epoch: 9.75 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.791379815396497		[learning rate: 0.00037001]
	Learning Rate: 0.000370008
	LOSS [training: 3.791379815396497 | validation: 4.805652278411331]
	TIME [epoch: 9.74 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7793990672490465		[learning rate: 0.00036866]
	Learning Rate: 0.000368665
	LOSS [training: 3.7793990672490465 | validation: 4.818422393889387]
	TIME [epoch: 9.72 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7820012050590974		[learning rate: 0.00036733]
	Learning Rate: 0.000367327
	LOSS [training: 3.7820012050590974 | validation: 4.797156845530353]
	TIME [epoch: 9.73 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.78459554275729		[learning rate: 0.00036599]
	Learning Rate: 0.000365994
	LOSS [training: 3.78459554275729 | validation: 4.807510768088074]
	TIME [epoch: 9.75 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7917345484075056		[learning rate: 0.00036467]
	Learning Rate: 0.000364666
	LOSS [training: 3.7917345484075056 | validation: 4.808150641202601]
	TIME [epoch: 9.73 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.783676512185251		[learning rate: 0.00036334]
	Learning Rate: 0.000363342
	LOSS [training: 3.783676512185251 | validation: 4.807171637843578]
	TIME [epoch: 9.73 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779034980510649		[learning rate: 0.00036202]
	Learning Rate: 0.000362024
	LOSS [training: 3.779034980510649 | validation: 4.813962727087694]
	TIME [epoch: 9.73 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.785606048505359		[learning rate: 0.00036071]
	Learning Rate: 0.00036071
	LOSS [training: 3.785606048505359 | validation: 4.802075336461709]
	TIME [epoch: 9.76 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7817029496912182		[learning rate: 0.0003594]
	Learning Rate: 0.000359401
	LOSS [training: 3.7817029496912182 | validation: 4.7985343940077625]
	TIME [epoch: 9.73 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.797635560422961		[learning rate: 0.0003581]
	Learning Rate: 0.000358096
	LOSS [training: 3.797635560422961 | validation: 4.806900661379981]
	TIME [epoch: 9.73 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7883530209921163		[learning rate: 0.0003568]
	Learning Rate: 0.000356797
	LOSS [training: 3.7883530209921163 | validation: 4.803675191257184]
	TIME [epoch: 9.74 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780743704269215		[learning rate: 0.0003555]
	Learning Rate: 0.000355502
	LOSS [training: 3.780743704269215 | validation: 4.794893695395675]
	TIME [epoch: 9.74 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7845345522237266		[learning rate: 0.00035421]
	Learning Rate: 0.000354212
	LOSS [training: 3.7845345522237266 | validation: 4.844954869908614]
	TIME [epoch: 9.73 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.818472318667576		[learning rate: 0.00035293]
	Learning Rate: 0.000352926
	LOSS [training: 3.818472318667576 | validation: 4.831922544143314]
	TIME [epoch: 9.73 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.792267722924021		[learning rate: 0.00035165]
	Learning Rate: 0.000351646
	LOSS [training: 3.792267722924021 | validation: 4.805133043215532]
	TIME [epoch: 9.75 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7880313061454904		[learning rate: 0.00035037]
	Learning Rate: 0.00035037
	LOSS [training: 3.7880313061454904 | validation: 4.810216732838065]
	TIME [epoch: 9.73 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.784646240162136		[learning rate: 0.0003491]
	Learning Rate: 0.000349098
	LOSS [training: 3.784646240162136 | validation: 4.789267791643105]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_1023.pth
	Model improved!!!
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.792376279796811		[learning rate: 0.00034783]
	Learning Rate: 0.000347831
	LOSS [training: 3.792376279796811 | validation: 4.810030118601914]
	TIME [epoch: 9.72 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.795964521439443		[learning rate: 0.00034657]
	Learning Rate: 0.000346569
	LOSS [training: 3.795964521439443 | validation: 4.835126759928822]
	TIME [epoch: 9.74 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8124790539930045		[learning rate: 0.00034531]
	Learning Rate: 0.000345311
	LOSS [training: 3.8124790539930045 | validation: 4.804253282590454]
	TIME [epoch: 9.72 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781730128309005		[learning rate: 0.00034406]
	Learning Rate: 0.000344058
	LOSS [training: 3.781730128309005 | validation: 4.807794400193319]
	TIME [epoch: 9.72 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782077699473889		[learning rate: 0.00034281]
	Learning Rate: 0.000342809
	LOSS [training: 3.782077699473889 | validation: 4.805529983409179]
	TIME [epoch: 9.73 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7834549369639676		[learning rate: 0.00034157]
	Learning Rate: 0.000341565
	LOSS [training: 3.7834549369639676 | validation: 4.817289600203614]
	TIME [epoch: 9.74 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7861130795849127		[learning rate: 0.00034033]
	Learning Rate: 0.000340326
	LOSS [training: 3.7861130795849127 | validation: 4.798747665502147]
	TIME [epoch: 9.72 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777795275928964		[learning rate: 0.00033909]
	Learning Rate: 0.000339091
	LOSS [training: 3.777795275928964 | validation: 4.797111373306266]
	TIME [epoch: 9.72 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7783056807297184		[learning rate: 0.00033786]
	Learning Rate: 0.00033786
	LOSS [training: 3.7783056807297184 | validation: 4.795650073966843]
	TIME [epoch: 9.73 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7780558395376973		[learning rate: 0.00033663]
	Learning Rate: 0.000336634
	LOSS [training: 3.7780558395376973 | validation: 4.799074662907088]
	TIME [epoch: 9.72 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7785674190384198		[learning rate: 0.00033541]
	Learning Rate: 0.000335412
	LOSS [training: 3.7785674190384198 | validation: 4.8150747397104565]
	TIME [epoch: 9.72 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7918154543114753		[learning rate: 0.0003342]
	Learning Rate: 0.000334195
	LOSS [training: 3.7918154543114753 | validation: 4.80935235078918]
	TIME [epoch: 9.71 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.796764363278899		[learning rate: 0.00033298]
	Learning Rate: 0.000332982
	LOSS [training: 3.796764363278899 | validation: 4.807586004738284]
	TIME [epoch: 9.73 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7822160167144903		[learning rate: 0.00033177]
	Learning Rate: 0.000331774
	LOSS [training: 3.7822160167144903 | validation: 4.7956941086950495]
	TIME [epoch: 9.72 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7819958358425083		[learning rate: 0.00033057]
	Learning Rate: 0.00033057
	LOSS [training: 3.7819958358425083 | validation: 4.818008934768196]
	TIME [epoch: 9.71 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7810942353291765		[learning rate: 0.00032937]
	Learning Rate: 0.00032937
	LOSS [training: 3.7810942353291765 | validation: 4.8101914153355265]
	TIME [epoch: 9.71 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779519874400645		[learning rate: 0.00032817]
	Learning Rate: 0.000328175
	LOSS [training: 3.779519874400645 | validation: 4.803294770049969]
	TIME [epoch: 9.74 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782504831034194		[learning rate: 0.00032698]
	Learning Rate: 0.000326984
	LOSS [training: 3.782504831034194 | validation: 4.808018581693977]
	TIME [epoch: 9.71 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7815070855239314		[learning rate: 0.0003258]
	Learning Rate: 0.000325797
	LOSS [training: 3.7815070855239314 | validation: 4.796634541638993]
	TIME [epoch: 9.71 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7797509790385604		[learning rate: 0.00032461]
	Learning Rate: 0.000324615
	LOSS [training: 3.7797509790385604 | validation: 4.7994580448623365]
	TIME [epoch: 9.72 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.792810987854451		[learning rate: 0.00032344]
	Learning Rate: 0.000323437
	LOSS [training: 3.792810987854451 | validation: 4.799212576255876]
	TIME [epoch: 9.72 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7799060147545047		[learning rate: 0.00032226]
	Learning Rate: 0.000322263
	LOSS [training: 3.7799060147545047 | validation: 4.7970250312511284]
	TIME [epoch: 9.72 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7787567933178203		[learning rate: 0.00032109]
	Learning Rate: 0.000321094
	LOSS [training: 3.7787567933178203 | validation: 4.805534847288388]
	TIME [epoch: 9.72 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7865966864756992		[learning rate: 0.00031993]
	Learning Rate: 0.000319928
	LOSS [training: 3.7865966864756992 | validation: 4.809950007383152]
	TIME [epoch: 9.73 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7819693314159624		[learning rate: 0.00031877]
	Learning Rate: 0.000318767
	LOSS [training: 3.7819693314159624 | validation: 4.794311861978776]
	TIME [epoch: 9.71 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7772795347019716		[learning rate: 0.00031761]
	Learning Rate: 0.000317611
	LOSS [training: 3.7772795347019716 | validation: 4.810236709290517]
	TIME [epoch: 9.71 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.785152343190428		[learning rate: 0.00031646]
	Learning Rate: 0.000316458
	LOSS [training: 3.785152343190428 | validation: 4.806408035783232]
	TIME [epoch: 9.72 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7789005081654197		[learning rate: 0.00031531]
	Learning Rate: 0.000315309
	LOSS [training: 3.7789005081654197 | validation: 4.815374642188232]
	TIME [epoch: 9.73 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.78339399373803		[learning rate: 0.00031417]
	Learning Rate: 0.000314165
	LOSS [training: 3.78339399373803 | validation: 4.797791433165957]
	TIME [epoch: 9.72 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7814608080965435		[learning rate: 0.00031302]
	Learning Rate: 0.000313025
	LOSS [training: 3.7814608080965435 | validation: 4.819801152931971]
	TIME [epoch: 9.71 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.792559020587742		[learning rate: 0.00031189]
	Learning Rate: 0.000311889
	LOSS [training: 3.792559020587742 | validation: 4.795194202793718]
	TIME [epoch: 9.72 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778497375522081		[learning rate: 0.00031076]
	Learning Rate: 0.000310757
	LOSS [training: 3.778497375522081 | validation: 4.80116810386389]
	TIME [epoch: 9.73 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.784665037090253		[learning rate: 0.00030963]
	Learning Rate: 0.000309629
	LOSS [training: 3.784665037090253 | validation: 4.794077576710499]
	TIME [epoch: 9.72 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7839687190806837		[learning rate: 0.00030851]
	Learning Rate: 0.000308506
	LOSS [training: 3.7839687190806837 | validation: 4.807991389861235]
	TIME [epoch: 9.71 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7794296146379693		[learning rate: 0.00030739]
	Learning Rate: 0.000307386
	LOSS [training: 3.7794296146379693 | validation: 4.789982637803712]
	TIME [epoch: 9.73 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7971301864975766		[learning rate: 0.00030627]
	Learning Rate: 0.000306271
	LOSS [training: 3.7971301864975766 | validation: 4.806079952748458]
	TIME [epoch: 9.71 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7773656832223153		[learning rate: 0.00030516]
	Learning Rate: 0.000305159
	LOSS [training: 3.7773656832223153 | validation: 4.797680390005507]
	TIME [epoch: 9.71 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7836996676519616		[learning rate: 0.00030405]
	Learning Rate: 0.000304052
	LOSS [training: 3.7836996676519616 | validation: 4.799186783971462]
	TIME [epoch: 9.71 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782510087056236		[learning rate: 0.00030295]
	Learning Rate: 0.000302948
	LOSS [training: 3.782510087056236 | validation: 4.8070358478083834]
	TIME [epoch: 9.73 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781514742714991		[learning rate: 0.00030185]
	Learning Rate: 0.000301849
	LOSS [training: 3.781514742714991 | validation: 4.799587379235478]
	TIME [epoch: 9.72 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.786907470284875		[learning rate: 0.00030075]
	Learning Rate: 0.000300753
	LOSS [training: 3.786907470284875 | validation: 4.803327019251241]
	TIME [epoch: 9.72 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7826055778749534		[learning rate: 0.00029966]
	Learning Rate: 0.000299662
	LOSS [training: 3.7826055778749534 | validation: 4.810866753239295]
	TIME [epoch: 9.71 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7757172146117126		[learning rate: 0.00029857]
	Learning Rate: 0.000298574
	LOSS [training: 3.7757172146117126 | validation: 4.793135266620289]
	TIME [epoch: 9.74 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776878118398087		[learning rate: 0.00029749]
	Learning Rate: 0.000297491
	LOSS [training: 3.776878118398087 | validation: 4.801604058185439]
	TIME [epoch: 9.71 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.788928152541635		[learning rate: 0.00029641]
	Learning Rate: 0.000296411
	LOSS [training: 3.788928152541635 | validation: 4.813727023590825]
	TIME [epoch: 9.71 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7828239435325344		[learning rate: 0.00029534]
	Learning Rate: 0.000295336
	LOSS [training: 3.7828239435325344 | validation: 4.813936547347489]
	TIME [epoch: 9.72 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8045446081169247		[learning rate: 0.00029426]
	Learning Rate: 0.000294264
	LOSS [training: 3.8045446081169247 | validation: 4.808185278682843]
	TIME [epoch: 9.73 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8107656026176357		[learning rate: 0.0002932]
	Learning Rate: 0.000293196
	LOSS [training: 3.8107656026176357 | validation: 4.8091904633934215]
	TIME [epoch: 9.72 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7900162357868856		[learning rate: 0.00029213]
	Learning Rate: 0.000292132
	LOSS [training: 3.7900162357868856 | validation: 4.819905067308927]
	TIME [epoch: 9.71 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782101311262364		[learning rate: 0.00029107]
	Learning Rate: 0.000291072
	LOSS [training: 3.782101311262364 | validation: 4.806141207537129]
	TIME [epoch: 9.74 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8002001606796902		[learning rate: 0.00029002]
	Learning Rate: 0.000290015
	LOSS [training: 3.8002001606796902 | validation: 4.817043315904711]
	TIME [epoch: 9.72 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7819182047928863		[learning rate: 0.00028896]
	Learning Rate: 0.000288963
	LOSS [training: 3.7819182047928863 | validation: 4.803028150057091]
	TIME [epoch: 9.71 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7850802199410922		[learning rate: 0.00028791]
	Learning Rate: 0.000287914
	LOSS [training: 3.7850802199410922 | validation: 4.808483115617187]
	TIME [epoch: 9.71 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779723772628441		[learning rate: 0.00028687]
	Learning Rate: 0.000286869
	LOSS [training: 3.779723772628441 | validation: 4.801756723391114]
	TIME [epoch: 9.73 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774751595525255		[learning rate: 0.00028583]
	Learning Rate: 0.000285828
	LOSS [training: 3.774751595525255 | validation: 4.804438762192076]
	TIME [epoch: 9.71 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7762777529884404		[learning rate: 0.00028479]
	Learning Rate: 0.000284791
	LOSS [training: 3.7762777529884404 | validation: 4.798869900498628]
	TIME [epoch: 9.71 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7773716920108966		[learning rate: 0.00028376]
	Learning Rate: 0.000283758
	LOSS [training: 3.7773716920108966 | validation: 4.798421510508286]
	TIME [epoch: 9.72 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7924820993745363		[learning rate: 0.00028273]
	Learning Rate: 0.000282728
	LOSS [training: 3.7924820993745363 | validation: 4.794633008854764]
	TIME [epoch: 9.73 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780382046343071		[learning rate: 0.0002817]
	Learning Rate: 0.000281702
	LOSS [training: 3.780382046343071 | validation: 4.7957223006324705]
	TIME [epoch: 9.71 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.785964122110529		[learning rate: 0.00028068]
	Learning Rate: 0.000280679
	LOSS [training: 3.785964122110529 | validation: 4.798660126106344]
	TIME [epoch: 9.71 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77952119707314		[learning rate: 0.00027966]
	Learning Rate: 0.000279661
	LOSS [training: 3.77952119707314 | validation: 4.801382003241229]
	TIME [epoch: 9.74 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781010945255993		[learning rate: 0.00027865]
	Learning Rate: 0.000278646
	LOSS [training: 3.781010945255993 | validation: 4.816655933556772]
	TIME [epoch: 9.71 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782131673093282		[learning rate: 0.00027763]
	Learning Rate: 0.000277635
	LOSS [training: 3.782131673093282 | validation: 4.798869323679683]
	TIME [epoch: 9.71 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.78116090917061		[learning rate: 0.00027663]
	Learning Rate: 0.000276627
	LOSS [training: 3.78116090917061 | validation: 4.803254803207099]
	TIME [epoch: 9.71 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7783135036020212		[learning rate: 0.00027562]
	Learning Rate: 0.000275623
	LOSS [training: 3.7783135036020212 | validation: 4.80353276692982]
	TIME [epoch: 9.73 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778628250818322		[learning rate: 0.00027462]
	Learning Rate: 0.000274623
	LOSS [training: 3.778628250818322 | validation: 4.788902817924557]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_1089.pth
	Model improved!!!
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7750784151207		[learning rate: 0.00027363]
	Learning Rate: 0.000273626
	LOSS [training: 3.7750784151207 | validation: 4.809128575430961]
	TIME [epoch: 9.72 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7873719990550847		[learning rate: 0.00027263]
	Learning Rate: 0.000272633
	LOSS [training: 3.7873719990550847 | validation: 4.809825771567479]
	TIME [epoch: 9.72 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782624964048833		[learning rate: 0.00027164]
	Learning Rate: 0.000271644
	LOSS [training: 3.782624964048833 | validation: 4.810659405484866]
	TIME [epoch: 9.73 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7797430648253387		[learning rate: 0.00027066]
	Learning Rate: 0.000270658
	LOSS [training: 3.7797430648253387 | validation: 4.810149006065893]
	TIME [epoch: 9.72 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.785595624737856		[learning rate: 0.00026968]
	Learning Rate: 0.000269676
	LOSS [training: 3.785595624737856 | validation: 4.795062317112327]
	TIME [epoch: 9.71 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.788602958078826		[learning rate: 0.0002687]
	Learning Rate: 0.000268697
	LOSS [training: 3.788602958078826 | validation: 4.806021809523174]
	TIME [epoch: 9.72 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7803944879464124		[learning rate: 0.00026772]
	Learning Rate: 0.000267722
	LOSS [training: 3.7803944879464124 | validation: 4.80039357211312]
	TIME [epoch: 9.72 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7773936582583767		[learning rate: 0.00026675]
	Learning Rate: 0.000266751
	LOSS [training: 3.7773936582583767 | validation: 4.799863108723682]
	TIME [epoch: 9.71 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.784743013778113		[learning rate: 0.00026578]
	Learning Rate: 0.000265782
	LOSS [training: 3.784743013778113 | validation: 4.804494936196062]
	TIME [epoch: 9.71 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7817592929584123		[learning rate: 0.00026482]
	Learning Rate: 0.000264818
	LOSS [training: 3.7817592929584123 | validation: 4.792315368985256]
	TIME [epoch: 9.73 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7857226103644486		[learning rate: 0.00026386]
	Learning Rate: 0.000263857
	LOSS [training: 3.7857226103644486 | validation: 4.798345145735901]
	TIME [epoch: 9.72 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7822812351494255		[learning rate: 0.0002629]
	Learning Rate: 0.000262899
	LOSS [training: 3.7822812351494255 | validation: 4.803496437729653]
	TIME [epoch: 9.71 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780996558640438		[learning rate: 0.00026195]
	Learning Rate: 0.000261945
	LOSS [training: 3.780996558640438 | validation: 4.796728176077411]
	TIME [epoch: 9.72 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7841896437236775		[learning rate: 0.00026099]
	Learning Rate: 0.000260995
	LOSS [training: 3.7841896437236775 | validation: 4.806230011048545]
	TIME [epoch: 9.74 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7916998325364957		[learning rate: 0.00026005]
	Learning Rate: 0.000260047
	LOSS [training: 3.7916998325364957 | validation: 4.803274145291559]
	TIME [epoch: 9.71 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7776239206991633		[learning rate: 0.0002591]
	Learning Rate: 0.000259104
	LOSS [training: 3.7776239206991633 | validation: 4.8031755489252]
	TIME [epoch: 9.71 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.786175811436189		[learning rate: 0.00025816]
	Learning Rate: 0.000258163
	LOSS [training: 3.786175811436189 | validation: 4.7984937203387235]
	TIME [epoch: 9.72 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8008228769814076		[learning rate: 0.00025723]
	Learning Rate: 0.000257227
	LOSS [training: 3.8008228769814076 | validation: 4.795549227781307]
	TIME [epoch: 9.72 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781371079388203		[learning rate: 0.00025629]
	Learning Rate: 0.000256293
	LOSS [training: 3.781371079388203 | validation: 4.794639361887148]
	TIME [epoch: 9.71 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7784729470220775		[learning rate: 0.00025536]
	Learning Rate: 0.000255363
	LOSS [training: 3.7784729470220775 | validation: 4.8021590218690795]
	TIME [epoch: 9.71 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7881408300131505		[learning rate: 0.00025444]
	Learning Rate: 0.000254436
	LOSS [training: 3.7881408300131505 | validation: 4.797846523781254]
	TIME [epoch: 9.73 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7795010796644073		[learning rate: 0.00025351]
	Learning Rate: 0.000253513
	LOSS [training: 3.7795010796644073 | validation: 4.814970473044931]
	TIME [epoch: 9.71 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.80117192720432		[learning rate: 0.00025259]
	Learning Rate: 0.000252593
	LOSS [training: 3.80117192720432 | validation: 4.815683538083228]
	TIME [epoch: 9.71 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7854929928169		[learning rate: 0.00025168]
	Learning Rate: 0.000251676
	LOSS [training: 3.7854929928169 | validation: 4.797006564640082]
	TIME [epoch: 9.72 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7954762314904165		[learning rate: 0.00025076]
	Learning Rate: 0.000250763
	LOSS [training: 3.7954762314904165 | validation: 4.810973130412216]
	TIME [epoch: 9.73 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.796128236472332		[learning rate: 0.00024985]
	Learning Rate: 0.000249853
	LOSS [training: 3.796128236472332 | validation: 4.81307689083709]
	TIME [epoch: 9.72 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.785360089256668		[learning rate: 0.00024895]
	Learning Rate: 0.000248946
	LOSS [training: 3.785360089256668 | validation: 4.80978697614833]
	TIME [epoch: 9.71 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7927356061510893		[learning rate: 0.00024804]
	Learning Rate: 0.000248043
	LOSS [training: 3.7927356061510893 | validation: 4.8047946258364815]
	TIME [epoch: 9.72 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.784379609261849		[learning rate: 0.00024714]
	Learning Rate: 0.000247142
	LOSS [training: 3.784379609261849 | validation: 4.7976784421235585]
	TIME [epoch: 9.73 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7808916780464004		[learning rate: 0.00024625]
	Learning Rate: 0.000246246
	LOSS [training: 3.7808916780464004 | validation: 4.79008150957745]
	TIME [epoch: 9.72 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7849179368686463		[learning rate: 0.00024535]
	Learning Rate: 0.000245352
	LOSS [training: 3.7849179368686463 | validation: 4.794090832983901]
	TIME [epoch: 9.71 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7821661799563784		[learning rate: 0.00024446]
	Learning Rate: 0.000244462
	LOSS [training: 3.7821661799563784 | validation: 4.79703066503255]
	TIME [epoch: 9.72 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777933438224278		[learning rate: 0.00024357]
	Learning Rate: 0.000243574
	LOSS [training: 3.777933438224278 | validation: 4.794671452433553]
	TIME [epoch: 9.72 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7922177933224943		[learning rate: 0.00024269]
	Learning Rate: 0.00024269
	LOSS [training: 3.7922177933224943 | validation: 4.811777724633117]
	TIME [epoch: 9.71 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.783449045858562		[learning rate: 0.00024181]
	Learning Rate: 0.00024181
	LOSS [training: 3.783449045858562 | validation: 4.803214315273176]
	TIME [epoch: 9.7 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774418194361		[learning rate: 0.00024093]
	Learning Rate: 0.000240932
	LOSS [training: 3.774418194361 | validation: 4.792570865692234]
	TIME [epoch: 9.73 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780131084547609		[learning rate: 0.00024006]
	Learning Rate: 0.000240058
	LOSS [training: 3.780131084547609 | validation: 4.787675330224025]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_1126.pth
	Model improved!!!
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7823049334745185		[learning rate: 0.00023919]
	Learning Rate: 0.000239187
	LOSS [training: 3.7823049334745185 | validation: 4.79679542133009]
	TIME [epoch: 9.72 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7825225658546784		[learning rate: 0.00023832]
	Learning Rate: 0.000238319
	LOSS [training: 3.7825225658546784 | validation: 4.811956281983131]
	TIME [epoch: 9.72 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7856966348488355		[learning rate: 0.00023745]
	Learning Rate: 0.000237454
	LOSS [training: 3.7856966348488355 | validation: 4.796144160181763]
	TIME [epoch: 9.74 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.784685355322087		[learning rate: 0.00023659]
	Learning Rate: 0.000236592
	LOSS [training: 3.784685355322087 | validation: 4.798292858540483]
	TIME [epoch: 9.72 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.784113415242269		[learning rate: 0.00023573]
	Learning Rate: 0.000235733
	LOSS [training: 3.784113415242269 | validation: 4.818011081062691]
	TIME [epoch: 9.73 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7829865228229616		[learning rate: 0.00023488]
	Learning Rate: 0.000234878
	LOSS [training: 3.7829865228229616 | validation: 4.800967176525037]
	TIME [epoch: 9.74 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781896687081192		[learning rate: 0.00023403]
	Learning Rate: 0.000234026
	LOSS [training: 3.781896687081192 | validation: 4.829951623296228]
	TIME [epoch: 9.73 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7938013701518982		[learning rate: 0.00023318]
	Learning Rate: 0.000233176
	LOSS [training: 3.7938013701518982 | validation: 4.816474361425324]
	TIME [epoch: 9.72 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7821977665252158		[learning rate: 0.00023233]
	Learning Rate: 0.00023233
	LOSS [training: 3.7821977665252158 | validation: 4.80228383974728]
	TIME [epoch: 9.72 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.785937113977858		[learning rate: 0.00023149]
	Learning Rate: 0.000231487
	LOSS [training: 3.785937113977858 | validation: 4.803524758115262]
	TIME [epoch: 9.74 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.787086462600077		[learning rate: 0.00023065]
	Learning Rate: 0.000230647
	LOSS [training: 3.787086462600077 | validation: 4.799839667733898]
	TIME [epoch: 9.72 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.787118521840913		[learning rate: 0.00022981]
	Learning Rate: 0.00022981
	LOSS [training: 3.787118521840913 | validation: 4.807227045066389]
	TIME [epoch: 9.73 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.784712543765276		[learning rate: 0.00022898]
	Learning Rate: 0.000228976
	LOSS [training: 3.784712543765276 | validation: 4.797593423769833]
	TIME [epoch: 9.73 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7829849768232777		[learning rate: 0.00022814]
	Learning Rate: 0.000228145
	LOSS [training: 3.7829849768232777 | validation: 4.82048915637717]
	TIME [epoch: 9.74 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7798590271425767		[learning rate: 0.00022732]
	Learning Rate: 0.000227317
	LOSS [training: 3.7798590271425767 | validation: 4.796793106311165]
	TIME [epoch: 9.72 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7731972397153077		[learning rate: 0.00022649]
	Learning Rate: 0.000226492
	LOSS [training: 3.7731972397153077 | validation: 4.79745443284588]
	TIME [epoch: 9.71 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7789031862522693		[learning rate: 0.00022567]
	Learning Rate: 0.00022567
	LOSS [training: 3.7789031862522693 | validation: 4.80387136897834]
	TIME [epoch: 9.72 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7797325223938074		[learning rate: 0.00022485]
	Learning Rate: 0.000224851
	LOSS [training: 3.7797325223938074 | validation: 4.811558465808941]
	TIME [epoch: 9.74 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780716292432504		[learning rate: 0.00022403]
	Learning Rate: 0.000224035
	LOSS [training: 3.780716292432504 | validation: 4.804343199493076]
	TIME [epoch: 9.73 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775401617829948		[learning rate: 0.00022322]
	Learning Rate: 0.000223222
	LOSS [training: 3.775401617829948 | validation: 4.791328607827437]
	TIME [epoch: 9.72 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7816268278746		[learning rate: 0.00022241]
	Learning Rate: 0.000222412
	LOSS [training: 3.7816268278746 | validation: 4.798669125087166]
	TIME [epoch: 9.73 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7823805782154567		[learning rate: 0.0002216]
	Learning Rate: 0.000221605
	LOSS [training: 3.7823805782154567 | validation: 4.792410578621274]
	TIME [epoch: 9.72 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782056283508342		[learning rate: 0.0002208]
	Learning Rate: 0.0002208
	LOSS [training: 3.782056283508342 | validation: 4.800484378885439]
	TIME [epoch: 9.72 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.787387424983997		[learning rate: 0.00022]
	Learning Rate: 0.000219999
	LOSS [training: 3.787387424983997 | validation: 4.8055094350417225]
	TIME [epoch: 9.73 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7830145790880514		[learning rate: 0.0002192]
	Learning Rate: 0.000219201
	LOSS [training: 3.7830145790880514 | validation: 4.799449991838934]
	TIME [epoch: 9.74 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782447147850651		[learning rate: 0.00021841]
	Learning Rate: 0.000218405
	LOSS [training: 3.782447147850651 | validation: 4.800106289373268]
	TIME [epoch: 9.72 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.78451421552072		[learning rate: 0.00021761]
	Learning Rate: 0.000217613
	LOSS [training: 3.78451421552072 | validation: 4.815283847723273]
	TIME [epoch: 9.72 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.788888902724749		[learning rate: 0.00021682]
	Learning Rate: 0.000216823
	LOSS [training: 3.788888902724749 | validation: 4.803839130095183]
	TIME [epoch: 9.72 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7826982344156237		[learning rate: 0.00021604]
	Learning Rate: 0.000216036
	LOSS [training: 3.7826982344156237 | validation: 4.800852236283736]
	TIME [epoch: 9.73 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7805156968621945		[learning rate: 0.00021525]
	Learning Rate: 0.000215252
	LOSS [training: 3.7805156968621945 | validation: 4.80443693517923]
	TIME [epoch: 9.73 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77840679445422		[learning rate: 0.00021447]
	Learning Rate: 0.000214471
	LOSS [training: 3.77840679445422 | validation: 4.801925320704068]
	TIME [epoch: 9.72 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782259355151693		[learning rate: 0.00021369]
	Learning Rate: 0.000213693
	LOSS [training: 3.782259355151693 | validation: 4.8059669516046695]
	TIME [epoch: 9.74 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7837414528859314		[learning rate: 0.00021292]
	Learning Rate: 0.000212917
	LOSS [training: 3.7837414528859314 | validation: 4.79799776639732]
	TIME [epoch: 9.74 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7817557904945778		[learning rate: 0.00021214]
	Learning Rate: 0.000212144
	LOSS [training: 3.7817557904945778 | validation: 4.793057137573285]
	TIME [epoch: 9.72 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782022256595514		[learning rate: 0.00021137]
	Learning Rate: 0.000211375
	LOSS [training: 3.782022256595514 | validation: 4.809560363968802]
	TIME [epoch: 9.72 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782515897396184		[learning rate: 0.00021061]
	Learning Rate: 0.000210607
	LOSS [training: 3.782515897396184 | validation: 4.800205576844263]
	TIME [epoch: 9.74 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.785518307028093		[learning rate: 0.00020984]
	Learning Rate: 0.000209843
	LOSS [training: 3.785518307028093 | validation: 4.809735581362524]
	TIME [epoch: 9.72 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.78144599849755		[learning rate: 0.00020908]
	Learning Rate: 0.000209082
	LOSS [training: 3.78144599849755 | validation: 4.803652293826822]
	TIME [epoch: 9.72 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782918980064541		[learning rate: 0.00020832]
	Learning Rate: 0.000208323
	LOSS [training: 3.782918980064541 | validation: 4.788948055783414]
	TIME [epoch: 9.71 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7766534425415883		[learning rate: 0.00020757]
	Learning Rate: 0.000207567
	LOSS [training: 3.7766534425415883 | validation: 4.805730547517836]
	TIME [epoch: 9.73 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7778326680763095		[learning rate: 0.00020681]
	Learning Rate: 0.000206814
	LOSS [training: 3.7778326680763095 | validation: 4.819304805753613]
	TIME [epoch: 9.72 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7984681652988614		[learning rate: 0.00020606]
	Learning Rate: 0.000206063
	LOSS [training: 3.7984681652988614 | validation: 4.799126594632049]
	TIME [epoch: 9.72 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.784481572366899		[learning rate: 0.00020532]
	Learning Rate: 0.000205315
	LOSS [training: 3.784481572366899 | validation: 4.794414988276283]
	TIME [epoch: 9.72 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.78126194589061		[learning rate: 0.00020457]
	Learning Rate: 0.00020457
	LOSS [training: 3.78126194589061 | validation: 4.799544490277872]
	TIME [epoch: 9.73 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7849860607032397		[learning rate: 0.00020383]
	Learning Rate: 0.000203828
	LOSS [training: 3.7849860607032397 | validation: 4.799257683433076]
	TIME [epoch: 9.72 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7766955178166652		[learning rate: 0.00020309]
	Learning Rate: 0.000203088
	LOSS [training: 3.7766955178166652 | validation: 4.806537744456082]
	TIME [epoch: 9.73 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7804905576755834		[learning rate: 0.00020235]
	Learning Rate: 0.000202351
	LOSS [training: 3.7804905576755834 | validation: 4.803474638783318]
	TIME [epoch: 9.73 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7752535012470396		[learning rate: 0.00020162]
	Learning Rate: 0.000201617
	LOSS [training: 3.7752535012470396 | validation: 4.804086821508766]
	TIME [epoch: 9.73 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7809480620375937		[learning rate: 0.00020088]
	Learning Rate: 0.000200885
	LOSS [training: 3.7809480620375937 | validation: 4.801368523806089]
	TIME [epoch: 9.72 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77388512144885		[learning rate: 0.00020016]
	Learning Rate: 0.000200156
	LOSS [training: 3.77388512144885 | validation: 4.8038269106159985]
	TIME [epoch: 9.72 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7829027441763046		[learning rate: 0.00019943]
	Learning Rate: 0.00019943
	LOSS [training: 3.7829027441763046 | validation: 4.809234019511799]
	TIME [epoch: 9.74 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782409897900954		[learning rate: 0.00019871]
	Learning Rate: 0.000198706
	LOSS [training: 3.782409897900954 | validation: 4.794455759661734]
	TIME [epoch: 9.72 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7799367106106994		[learning rate: 0.00019798]
	Learning Rate: 0.000197985
	LOSS [training: 3.7799367106106994 | validation: 4.795007187082702]
	TIME [epoch: 9.72 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7797731655208366		[learning rate: 0.00019727]
	Learning Rate: 0.000197266
	LOSS [training: 3.7797731655208366 | validation: 4.801080361071354]
	TIME [epoch: 9.73 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7849310160581013		[learning rate: 0.00019655]
	Learning Rate: 0.00019655
	LOSS [training: 3.7849310160581013 | validation: 4.802216159910934]
	TIME [epoch: 9.74 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7800889716359274		[learning rate: 0.00019584]
	Learning Rate: 0.000195837
	LOSS [training: 3.7800889716359274 | validation: 4.796865173810706]
	TIME [epoch: 9.72 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778867071474429		[learning rate: 0.00019513]
	Learning Rate: 0.000195126
	LOSS [training: 3.778867071474429 | validation: 4.79279557619897]
	TIME [epoch: 9.71 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.784262116012722		[learning rate: 0.00019442]
	Learning Rate: 0.000194418
	LOSS [training: 3.784262116012722 | validation: 4.813007199032992]
	TIME [epoch: 9.73 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7948375421118703		[learning rate: 0.00019371]
	Learning Rate: 0.000193713
	LOSS [training: 3.7948375421118703 | validation: 4.82709001482027]
	TIME [epoch: 9.73 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.800999701823103		[learning rate: 0.00019301]
	Learning Rate: 0.00019301
	LOSS [training: 3.800999701823103 | validation: 4.8154020929833035]
	TIME [epoch: 9.72 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7843500626243953		[learning rate: 0.00019231]
	Learning Rate: 0.000192309
	LOSS [training: 3.7843500626243953 | validation: 4.805681183048188]
	TIME [epoch: 9.72 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777537067040717		[learning rate: 0.00019161]
	Learning Rate: 0.000191611
	LOSS [training: 3.777537067040717 | validation: 4.802533159581436]
	TIME [epoch: 9.75 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7814592221606196		[learning rate: 0.00019092]
	Learning Rate: 0.000190916
	LOSS [training: 3.7814592221606196 | validation: 4.801342040057675]
	TIME [epoch: 9.73 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7763570494756693		[learning rate: 0.00019022]
	Learning Rate: 0.000190223
	LOSS [training: 3.7763570494756693 | validation: 4.802895565327498]
	TIME [epoch: 9.73 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7808907255462807		[learning rate: 0.00018953]
	Learning Rate: 0.000189533
	LOSS [training: 3.7808907255462807 | validation: 4.8025074199458135]
	TIME [epoch: 9.72 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7768875675314613		[learning rate: 0.00018884]
	Learning Rate: 0.000188845
	LOSS [training: 3.7768875675314613 | validation: 4.817060272085839]
	TIME [epoch: 9.74 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7798462936217136		[learning rate: 0.00018816]
	Learning Rate: 0.00018816
	LOSS [training: 3.7798462936217136 | validation: 4.8036388053304995]
	TIME [epoch: 9.72 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781823173169233		[learning rate: 0.00018748]
	Learning Rate: 0.000187477
	LOSS [training: 3.781823173169233 | validation: 4.8021086248258165]
	TIME [epoch: 9.72 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775759380647171		[learning rate: 0.0001868]
	Learning Rate: 0.000186796
	LOSS [training: 3.775759380647171 | validation: 4.806933379356971]
	TIME [epoch: 9.73 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778468431543611		[learning rate: 0.00018612]
	Learning Rate: 0.000186119
	LOSS [training: 3.778468431543611 | validation: 4.808285897794422]
	TIME [epoch: 9.74 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7805733914892046		[learning rate: 0.00018544]
	Learning Rate: 0.000185443
	LOSS [training: 3.7805733914892046 | validation: 4.816441429074359]
	TIME [epoch: 9.72 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7825666813387335		[learning rate: 0.00018477]
	Learning Rate: 0.00018477
	LOSS [training: 3.7825666813387335 | validation: 4.799660874938876]
	TIME [epoch: 9.72 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7843262065792374		[learning rate: 0.0001841]
	Learning Rate: 0.000184099
	LOSS [training: 3.7843262065792374 | validation: 4.818139193969797]
	TIME [epoch: 9.73 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7876252343394		[learning rate: 0.00018343]
	Learning Rate: 0.000183431
	LOSS [training: 3.7876252343394 | validation: 4.798546538420226]
	TIME [epoch: 9.73 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7807927150125664		[learning rate: 0.00018277]
	Learning Rate: 0.000182766
	LOSS [training: 3.7807927150125664 | validation: 4.794682455048667]
	TIME [epoch: 9.72 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7811023591012995		[learning rate: 0.0001821]
	Learning Rate: 0.000182102
	LOSS [training: 3.7811023591012995 | validation: 4.8134168795158105]
	TIME [epoch: 9.72 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780196300548133		[learning rate: 0.00018144]
	Learning Rate: 0.000181442
	LOSS [training: 3.780196300548133 | validation: 4.8044730445196215]
	TIME [epoch: 9.73 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7821643851364946		[learning rate: 0.00018078]
	Learning Rate: 0.000180783
	LOSS [training: 3.7821643851364946 | validation: 4.8003239956858526]
	TIME [epoch: 9.73 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781583558636229		[learning rate: 0.00018013]
	Learning Rate: 0.000180127
	LOSS [training: 3.781583558636229 | validation: 4.792520155930268]
	TIME [epoch: 9.72 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774964686890524		[learning rate: 0.00017947]
	Learning Rate: 0.000179473
	LOSS [training: 3.774964686890524 | validation: 4.808180127866939]
	TIME [epoch: 9.72 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7799318401222797		[learning rate: 0.00017882]
	Learning Rate: 0.000178822
	LOSS [training: 3.7799318401222797 | validation: 4.800923892103647]
	TIME [epoch: 9.73 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7725388548014323		[learning rate: 0.00017817]
	Learning Rate: 0.000178173
	LOSS [training: 3.7725388548014323 | validation: 4.792059607398867]
	TIME [epoch: 9.72 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7865321450003955		[learning rate: 0.00017753]
	Learning Rate: 0.000177526
	LOSS [training: 3.7865321450003955 | validation: 4.79017807857879]
	TIME [epoch: 9.72 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780545317827302		[learning rate: 0.00017688]
	Learning Rate: 0.000176882
	LOSS [training: 3.780545317827302 | validation: 4.793935410012783]
	TIME [epoch: 9.73 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.784542386907842		[learning rate: 0.00017624]
	Learning Rate: 0.00017624
	LOSS [training: 3.784542386907842 | validation: 4.80231607987831]
	TIME [epoch: 9.73 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7768012750932316		[learning rate: 0.0001756]
	Learning Rate: 0.000175601
	LOSS [training: 3.7768012750932316 | validation: 4.825248438848237]
	TIME [epoch: 9.72 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.794543086309711		[learning rate: 0.00017496]
	Learning Rate: 0.000174964
	LOSS [training: 3.794543086309711 | validation: 4.800610388717861]
	TIME [epoch: 9.72 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7771694300909564		[learning rate: 0.00017433]
	Learning Rate: 0.000174329
	LOSS [training: 3.7771694300909564 | validation: 4.802086889882467]
	TIME [epoch: 9.74 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7781921097143467		[learning rate: 0.0001737]
	Learning Rate: 0.000173696
	LOSS [training: 3.7781921097143467 | validation: 4.801119034756074]
	TIME [epoch: 9.72 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.784838433079571		[learning rate: 0.00017307]
	Learning Rate: 0.000173066
	LOSS [training: 3.784838433079571 | validation: 4.821845662667999]
	TIME [epoch: 9.72 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779134132955287		[learning rate: 0.00017244]
	Learning Rate: 0.000172437
	LOSS [training: 3.779134132955287 | validation: 4.790535906338015]
	TIME [epoch: 9.72 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.78085337234369		[learning rate: 0.00017181]
	Learning Rate: 0.000171812
	LOSS [training: 3.78085337234369 | validation: 4.789573106206496]
	TIME [epoch: 9.75 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7854030282804474		[learning rate: 0.00017119]
	Learning Rate: 0.000171188
	LOSS [training: 3.7854030282804474 | validation: 4.805823025024279]
	TIME [epoch: 9.72 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7814356181217095		[learning rate: 0.00017057]
	Learning Rate: 0.000170567
	LOSS [training: 3.7814356181217095 | validation: 4.794565245927342]
	TIME [epoch: 9.72 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7865278289570234		[learning rate: 0.00016995]
	Learning Rate: 0.000169948
	LOSS [training: 3.7865278289570234 | validation: 4.7969221545775875]
	TIME [epoch: 9.72 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782883392888356		[learning rate: 0.00016933]
	Learning Rate: 0.000169331
	LOSS [training: 3.782883392888356 | validation: 4.813417441564524]
	TIME [epoch: 9.73 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776410059102942		[learning rate: 0.00016872]
	Learning Rate: 0.000168717
	LOSS [training: 3.776410059102942 | validation: 4.795440548880009]
	TIME [epoch: 9.72 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7767489132466294		[learning rate: 0.0001681]
	Learning Rate: 0.000168104
	LOSS [training: 3.7767489132466294 | validation: 4.798367171750128]
	TIME [epoch: 9.72 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781472440478189		[learning rate: 0.00016749]
	Learning Rate: 0.000167494
	LOSS [training: 3.781472440478189 | validation: 4.806376568723579]
	TIME [epoch: 9.73 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7863826414567		[learning rate: 0.00016689]
	Learning Rate: 0.000166886
	LOSS [training: 3.7863826414567 | validation: 4.788896364577758]
	TIME [epoch: 9.73 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7818829079240133		[learning rate: 0.00016628]
	Learning Rate: 0.000166281
	LOSS [training: 3.7818829079240133 | validation: 4.796379012249321]
	TIME [epoch: 9.71 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7774478842186836		[learning rate: 0.00016568]
	Learning Rate: 0.000165677
	LOSS [training: 3.7774478842186836 | validation: 4.795002555159023]
	TIME [epoch: 9.72 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7833954330993613		[learning rate: 0.00016508]
	Learning Rate: 0.000165076
	LOSS [training: 3.7833954330993613 | validation: 4.7927621536871685]
	TIME [epoch: 9.74 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7861116908392645		[learning rate: 0.00016448]
	Learning Rate: 0.000164477
	LOSS [training: 3.7861116908392645 | validation: 4.803592583815624]
	TIME [epoch: 9.72 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778798588742178		[learning rate: 0.00016388]
	Learning Rate: 0.00016388
	LOSS [training: 3.778798588742178 | validation: 4.806392020648487]
	TIME [epoch: 9.72 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778962082264927		[learning rate: 0.00016329]
	Learning Rate: 0.000163285
	LOSS [training: 3.778962082264927 | validation: 4.792046804179189]
	TIME [epoch: 9.72 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780921745733729		[learning rate: 0.00016269]
	Learning Rate: 0.000162693
	LOSS [training: 3.780921745733729 | validation: 4.7939925468537385]
	TIME [epoch: 9.74 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775759424116278		[learning rate: 0.0001621]
	Learning Rate: 0.000162102
	LOSS [training: 3.775759424116278 | validation: 4.803299092738328]
	TIME [epoch: 9.72 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.78769212866123		[learning rate: 0.00016151]
	Learning Rate: 0.000161514
	LOSS [training: 3.78769212866123 | validation: 4.820306891129249]
	TIME [epoch: 9.72 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780711926737722		[learning rate: 0.00016093]
	Learning Rate: 0.000160928
	LOSS [training: 3.780711926737722 | validation: 4.800953682617213]
	TIME [epoch: 9.72 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778273616199839		[learning rate: 0.00016034]
	Learning Rate: 0.000160344
	LOSS [training: 3.778273616199839 | validation: 4.819349007119343]
	TIME [epoch: 9.72 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.78707104394739		[learning rate: 0.00015976]
	Learning Rate: 0.000159762
	LOSS [training: 3.78707104394739 | validation: 4.8009456181133405]
	TIME [epoch: 9.72 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7814244662086423		[learning rate: 0.00015918]
	Learning Rate: 0.000159182
	LOSS [training: 3.7814244662086423 | validation: 4.80287320063282]
	TIME [epoch: 9.71 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7829761948514102		[learning rate: 0.0001586]
	Learning Rate: 0.000158605
	LOSS [training: 3.7829761948514102 | validation: 4.800382556201092]
	TIME [epoch: 9.74 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778447690394695		[learning rate: 0.00015803]
	Learning Rate: 0.000158029
	LOSS [training: 3.778447690394695 | validation: 4.815092816826343]
	TIME [epoch: 9.73 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7830415533625135		[learning rate: 0.00015746]
	Learning Rate: 0.000157456
	LOSS [training: 3.7830415533625135 | validation: 4.806278851913636]
	TIME [epoch: 9.72 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782610034778854		[learning rate: 0.00015688]
	Learning Rate: 0.000156884
	LOSS [training: 3.782610034778854 | validation: 4.806364354354257]
	TIME [epoch: 9.72 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781069964538203		[learning rate: 0.00015631]
	Learning Rate: 0.000156315
	LOSS [training: 3.781069964538203 | validation: 4.82002985262028]
	TIME [epoch: 9.74 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77943279171305		[learning rate: 0.00015575]
	Learning Rate: 0.000155748
	LOSS [training: 3.77943279171305 | validation: 4.785006773251127]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_1245.pth
	Model improved!!!
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7814244048233547		[learning rate: 0.00015518]
	Learning Rate: 0.000155182
	LOSS [training: 3.7814244048233547 | validation: 4.800946681304306]
	TIME [epoch: 9.73 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777874653262277		[learning rate: 0.00015462]
	Learning Rate: 0.000154619
	LOSS [training: 3.777874653262277 | validation: 4.794050494564967]
	TIME [epoch: 9.72 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777375562295293		[learning rate: 0.00015406]
	Learning Rate: 0.000154058
	LOSS [training: 3.777375562295293 | validation: 4.795847576717591]
	TIME [epoch: 9.74 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.782563240288648		[learning rate: 0.0001535]
	Learning Rate: 0.000153499
	LOSS [training: 3.782563240288648 | validation: 4.800428062657777]
	TIME [epoch: 9.72 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.787469049570184		[learning rate: 0.00015294]
	Learning Rate: 0.000152942
	LOSS [training: 3.787469049570184 | validation: 4.804980191971139]
	TIME [epoch: 9.73 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7756267012971088		[learning rate: 0.00015239]
	Learning Rate: 0.000152387
	LOSS [training: 3.7756267012971088 | validation: 4.785292323712658]
	TIME [epoch: 9.74 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7859326659299994		[learning rate: 0.00015183]
	Learning Rate: 0.000151834
	LOSS [training: 3.7859326659299994 | validation: 4.789152732341618]
	TIME [epoch: 9.72 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773415780026542		[learning rate: 0.00015128]
	Learning Rate: 0.000151283
	LOSS [training: 3.773415780026542 | validation: 4.798849537730214]
	TIME [epoch: 9.72 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781258752425296		[learning rate: 0.00015073]
	Learning Rate: 0.000150734
	LOSS [training: 3.781258752425296 | validation: 4.783008429929647]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_1254.pth
	Model improved!!!
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7809048520120285		[learning rate: 0.00015019]
	Learning Rate: 0.000150187
	LOSS [training: 3.7809048520120285 | validation: 4.800105376993722]
	TIME [epoch: 9.75 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7806869771701286		[learning rate: 0.00014964]
	Learning Rate: 0.000149642
	LOSS [training: 3.7806869771701286 | validation: 4.812466159710901]
	TIME [epoch: 9.73 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773523621017447		[learning rate: 0.0001491]
	Learning Rate: 0.000149099
	LOSS [training: 3.773523621017447 | validation: 4.8065189102672585]
	TIME [epoch: 9.72 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777397807155866		[learning rate: 0.00014856]
	Learning Rate: 0.000148558
	LOSS [training: 3.777397807155866 | validation: 4.799368245330335]
	TIME [epoch: 9.72 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7773005571862583		[learning rate: 0.00014802]
	Learning Rate: 0.000148018
	LOSS [training: 3.7773005571862583 | validation: 4.800456715218103]
	TIME [epoch: 9.73 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7813322082762246		[learning rate: 0.00014748]
	Learning Rate: 0.000147481
	LOSS [training: 3.7813322082762246 | validation: 4.7982191622694925]
	TIME [epoch: 9.72 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7769055948149495		[learning rate: 0.00014695]
	Learning Rate: 0.000146946
	LOSS [training: 3.7769055948149495 | validation: 4.807360215547397]
	TIME [epoch: 9.72 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7845619765266547		[learning rate: 0.00014641]
	Learning Rate: 0.000146413
	LOSS [training: 3.7845619765266547 | validation: 4.808681731789751]
	TIME [epoch: 9.73 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777090017154213		[learning rate: 0.00014588]
	Learning Rate: 0.000145881
	LOSS [training: 3.777090017154213 | validation: 4.795312770284394]
	TIME [epoch: 9.73 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7812741574804427		[learning rate: 0.00014535]
	Learning Rate: 0.000145352
	LOSS [training: 3.7812741574804427 | validation: 4.797924425065386]
	TIME [epoch: 9.72 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779176482594534		[learning rate: 0.00014482]
	Learning Rate: 0.000144825
	LOSS [training: 3.779176482594534 | validation: 4.816302885947517]
	TIME [epoch: 9.72 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7775782751963822		[learning rate: 0.0001443]
	Learning Rate: 0.000144299
	LOSS [training: 3.7775782751963822 | validation: 4.795514476551088]
	TIME [epoch: 9.74 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773111397121282		[learning rate: 0.00014378]
	Learning Rate: 0.000143775
	LOSS [training: 3.773111397121282 | validation: 4.785059514878516]
	TIME [epoch: 9.72 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7797917953744133		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 3.7797917953744133 | validation: 4.797918841932345]
	TIME [epoch: 9.72 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7825507594325543		[learning rate: 0.00014273]
	Learning Rate: 0.000142734
	LOSS [training: 3.7825507594325543 | validation: 4.7867106614699155]
	TIME [epoch: 9.71 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7737578566342735		[learning rate: 0.00014222]
	Learning Rate: 0.000142216
	LOSS [training: 3.7737578566342735 | validation: 4.792398893986756]
	TIME [epoch: 9.74 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7833173388652086		[learning rate: 0.0001417]
	Learning Rate: 0.0001417
	LOSS [training: 3.7833173388652086 | validation: 4.796656895619926]
	TIME [epoch: 9.72 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778851412001091		[learning rate: 0.00014119]
	Learning Rate: 0.000141185
	LOSS [training: 3.778851412001091 | validation: 4.779850951635574]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_1272.pth
	Model improved!!!
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7808889546276823		[learning rate: 0.00014067]
	Learning Rate: 0.000140673
	LOSS [training: 3.7808889546276823 | validation: 4.800662741674052]
	TIME [epoch: 9.72 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7747103366279915		[learning rate: 0.00014016]
	Learning Rate: 0.000140162
	LOSS [training: 3.7747103366279915 | validation: 4.793021606379317]
	TIME [epoch: 9.72 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7769156393188057		[learning rate: 0.00013965]
	Learning Rate: 0.000139654
	LOSS [training: 3.7769156393188057 | validation: 4.8007555774390145]
	TIME [epoch: 9.71 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7767397620805156		[learning rate: 0.00013915]
	Learning Rate: 0.000139147
	LOSS [training: 3.7767397620805156 | validation: 4.799266326834517]
	TIME [epoch: 9.71 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776453196439583		[learning rate: 0.00013864]
	Learning Rate: 0.000138642
	LOSS [training: 3.776453196439583 | validation: 4.819890174133392]
	TIME [epoch: 9.73 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7768342351220996		[learning rate: 0.00013814]
	Learning Rate: 0.000138139
	LOSS [training: 3.7768342351220996 | validation: 4.7989517907992445]
	TIME [epoch: 9.71 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7811164214992017		[learning rate: 0.00013764]
	Learning Rate: 0.000137638
	LOSS [training: 3.7811164214992017 | validation: 4.797474435508434]
	TIME [epoch: 9.71 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777681266138301		[learning rate: 0.00013714]
	Learning Rate: 0.000137138
	LOSS [training: 3.777681266138301 | validation: 4.792836027765791]
	TIME [epoch: 9.71 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778297021222289		[learning rate: 0.00013664]
	Learning Rate: 0.00013664
	LOSS [training: 3.778297021222289 | validation: 4.791129617468386]
	TIME [epoch: 9.73 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7783846116146167		[learning rate: 0.00013614]
	Learning Rate: 0.000136144
	LOSS [training: 3.7783846116146167 | validation: 4.808092822776338]
	TIME [epoch: 9.71 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7796149335933635		[learning rate: 0.00013565]
	Learning Rate: 0.00013565
	LOSS [training: 3.7796149335933635 | validation: 4.800831214054561]
	TIME [epoch: 9.72 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7831925880443875		[learning rate: 0.00013516]
	Learning Rate: 0.000135158
	LOSS [training: 3.7831925880443875 | validation: 4.790644447097079]
	TIME [epoch: 9.73 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778277189542296		[learning rate: 0.00013467]
	Learning Rate: 0.000134668
	LOSS [training: 3.778277189542296 | validation: 4.805003722823582]
	TIME [epoch: 9.73 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7819767117008274		[learning rate: 0.00013418]
	Learning Rate: 0.000134179
	LOSS [training: 3.7819767117008274 | validation: 4.794052162631041]
	TIME [epoch: 9.71 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7790566270170003		[learning rate: 0.00013369]
	Learning Rate: 0.000133692
	LOSS [training: 3.7790566270170003 | validation: 4.8018879394796485]
	TIME [epoch: 9.72 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7752206874701293		[learning rate: 0.00013321]
	Learning Rate: 0.000133207
	LOSS [training: 3.7752206874701293 | validation: 4.796379629762023]
	TIME [epoch: 9.73 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.78314093160053		[learning rate: 0.00013272]
	Learning Rate: 0.000132723
	LOSS [training: 3.78314093160053 | validation: 4.792864699076172]
	TIME [epoch: 9.71 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776612930647821		[learning rate: 0.00013224]
	Learning Rate: 0.000132242
	LOSS [training: 3.776612930647821 | validation: 4.796544648461588]
	TIME [epoch: 9.71 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774595112705344		[learning rate: 0.00013176]
	Learning Rate: 0.000131762
	LOSS [training: 3.774595112705344 | validation: 4.800963417400525]
	TIME [epoch: 9.71 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7728333683455006		[learning rate: 0.00013128]
	Learning Rate: 0.000131284
	LOSS [training: 3.7728333683455006 | validation: 4.793284376446252]
	TIME [epoch: 9.74 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7779160988881495		[learning rate: 0.00013081]
	Learning Rate: 0.000130807
	LOSS [training: 3.7779160988881495 | validation: 4.797241298884897]
	TIME [epoch: 9.72 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779365847074716		[learning rate: 0.00013033]
	Learning Rate: 0.000130332
	LOSS [training: 3.779365847074716 | validation: 4.787527882807567]
	TIME [epoch: 9.72 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7799100770017175		[learning rate: 0.00012986]
	Learning Rate: 0.000129859
	LOSS [training: 3.7799100770017175 | validation: 4.804996858760928]
	TIME [epoch: 9.72 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.784336407087416		[learning rate: 0.00012939]
	Learning Rate: 0.000129388
	LOSS [training: 3.784336407087416 | validation: 4.8063887768628275]
	TIME [epoch: 9.74 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7838146547468847		[learning rate: 0.00012892]
	Learning Rate: 0.000128919
	LOSS [training: 3.7838146547468847 | validation: 4.804707687118742]
	TIME [epoch: 9.71 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7796489210739823		[learning rate: 0.00012845]
	Learning Rate: 0.000128451
	LOSS [training: 3.7796489210739823 | validation: 4.800292437658903]
	TIME [epoch: 9.72 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7797407064372806		[learning rate: 0.00012798]
	Learning Rate: 0.000127985
	LOSS [training: 3.7797407064372806 | validation: 4.815723996717341]
	TIME [epoch: 9.72 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7875258042346074		[learning rate: 0.00012752]
	Learning Rate: 0.00012752
	LOSS [training: 3.7875258042346074 | validation: 4.8083082648716395]
	TIME [epoch: 9.72 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779643691672875		[learning rate: 0.00012706]
	Learning Rate: 0.000127057
	LOSS [training: 3.779643691672875 | validation: 4.80279727744726]
	TIME [epoch: 9.71 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7741703064044954		[learning rate: 0.0001266]
	Learning Rate: 0.000126596
	LOSS [training: 3.7741703064044954 | validation: 4.797950764907462]
	TIME [epoch: 9.72 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775549307243092		[learning rate: 0.00012614]
	Learning Rate: 0.000126137
	LOSS [training: 3.775549307243092 | validation: 4.794534162802978]
	TIME [epoch: 9.73 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7857097265217474		[learning rate: 0.00012568]
	Learning Rate: 0.000125679
	LOSS [training: 3.7857097265217474 | validation: 4.803811729221376]
	TIME [epoch: 9.71 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7827646822159826		[learning rate: 0.00012522]
	Learning Rate: 0.000125223
	LOSS [training: 3.7827646822159826 | validation: 4.798900130474448]
	TIME [epoch: 9.72 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776209976663327		[learning rate: 0.00012477]
	Learning Rate: 0.000124769
	LOSS [training: 3.776209976663327 | validation: 4.79711372091942]
	TIME [epoch: 9.72 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7794918231781987		[learning rate: 0.00012432]
	Learning Rate: 0.000124316
	LOSS [training: 3.7794918231781987 | validation: 4.7898193015172]
	TIME [epoch: 9.73 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7792057014308305		[learning rate: 0.00012386]
	Learning Rate: 0.000123865
	LOSS [training: 3.7792057014308305 | validation: 4.800138158680168]
	TIME [epoch: 9.71 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7718762061527698		[learning rate: 0.00012342]
	Learning Rate: 0.000123415
	LOSS [training: 3.7718762061527698 | validation: 4.794755596536574]
	TIME [epoch: 9.72 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778729489717153		[learning rate: 0.00012297]
	Learning Rate: 0.000122967
	LOSS [training: 3.778729489717153 | validation: 4.802380426804067]
	TIME [epoch: 9.71 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7850127331287053		[learning rate: 0.00012252]
	Learning Rate: 0.000122521
	LOSS [training: 3.7850127331287053 | validation: 4.799592444616333]
	TIME [epoch: 9.73 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774481099332695		[learning rate: 0.00012208]
	Learning Rate: 0.000122076
	LOSS [training: 3.774481099332695 | validation: 4.798824012947964]
	TIME [epoch: 9.71 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777231070787427		[learning rate: 0.00012163]
	Learning Rate: 0.000121633
	LOSS [training: 3.777231070787427 | validation: 4.805620229911809]
	TIME [epoch: 9.71 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7769754633183297		[learning rate: 0.00012119]
	Learning Rate: 0.000121192
	LOSS [training: 3.7769754633183297 | validation: 4.804414697281206]
	TIME [epoch: 9.72 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7801112317578704		[learning rate: 0.00012075]
	Learning Rate: 0.000120752
	LOSS [training: 3.7801112317578704 | validation: 4.7915632774299945]
	TIME [epoch: 9.72 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771896635298019		[learning rate: 0.00012031]
	Learning Rate: 0.000120314
	LOSS [training: 3.771896635298019 | validation: 4.803170953118861]
	TIME [epoch: 9.71 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7728050469685		[learning rate: 0.00011988]
	Learning Rate: 0.000119877
	LOSS [training: 3.7728050469685 | validation: 4.804087753962607]
	TIME [epoch: 9.72 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7771766642754123		[learning rate: 0.00011944]
	Learning Rate: 0.000119442
	LOSS [training: 3.7771766642754123 | validation: 4.803069425580591]
	TIME [epoch: 9.73 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781670061128576		[learning rate: 0.00011901]
	Learning Rate: 0.000119009
	LOSS [training: 3.781670061128576 | validation: 4.805193088531227]
	TIME [epoch: 9.73 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7815194047002265		[learning rate: 0.00011858]
	Learning Rate: 0.000118577
	LOSS [training: 3.7815194047002265 | validation: 4.803179599189936]
	TIME [epoch: 9.71 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780987367823669		[learning rate: 0.00011815]
	Learning Rate: 0.000118147
	LOSS [training: 3.780987367823669 | validation: 4.79856131971067]
	TIME [epoch: 9.71 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7816632170568782		[learning rate: 0.00011772]
	Learning Rate: 0.000117718
	LOSS [training: 3.7816632170568782 | validation: 4.80539065978475]
	TIME [epoch: 9.73 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774658675748151		[learning rate: 0.00011729]
	Learning Rate: 0.000117291
	LOSS [training: 3.774658675748151 | validation: 4.802867393530253]
	TIME [epoch: 9.72 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77903643553527		[learning rate: 0.00011686]
	Learning Rate: 0.000116865
	LOSS [training: 3.77903643553527 | validation: 4.803660295677174]
	TIME [epoch: 9.72 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7772258837438764		[learning rate: 0.00011644]
	Learning Rate: 0.000116441
	LOSS [training: 3.7772258837438764 | validation: 4.797088925785021]
	TIME [epoch: 9.73 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7760498667329245		[learning rate: 0.00011602]
	Learning Rate: 0.000116018
	LOSS [training: 3.7760498667329245 | validation: 4.804348249129798]
	TIME [epoch: 9.73 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7706514841791803		[learning rate: 0.0001156]
	Learning Rate: 0.000115597
	LOSS [training: 3.7706514841791803 | validation: 4.795238144364696]
	TIME [epoch: 9.72 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777227783987481		[learning rate: 0.00011518]
	Learning Rate: 0.000115178
	LOSS [training: 3.777227783987481 | validation: 4.790038340569102]
	TIME [epoch: 9.72 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7727205222182603		[learning rate: 0.00011476]
	Learning Rate: 0.00011476
	LOSS [training: 3.7727205222182603 | validation: 4.799465609244346]
	TIME [epoch: 9.74 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775500866300052		[learning rate: 0.00011434]
	Learning Rate: 0.000114343
	LOSS [training: 3.775500866300052 | validation: 4.792954410822218]
	TIME [epoch: 9.72 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774469980140978		[learning rate: 0.00011393]
	Learning Rate: 0.000113928
	LOSS [training: 3.774469980140978 | validation: 4.805451064799015]
	TIME [epoch: 9.72 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.78086389186347		[learning rate: 0.00011351]
	Learning Rate: 0.000113515
	LOSS [training: 3.78086389186347 | validation: 4.797910928968191]
	TIME [epoch: 9.72 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780293659290696		[learning rate: 0.0001131]
	Learning Rate: 0.000113103
	LOSS [training: 3.780293659290696 | validation: 4.799187919194669]
	TIME [epoch: 9.74 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7809589050777648		[learning rate: 0.00011269]
	Learning Rate: 0.000112692
	LOSS [training: 3.7809589050777648 | validation: 4.7924090134100314]
	TIME [epoch: 9.71 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7766249693096703		[learning rate: 0.00011228]
	Learning Rate: 0.000112283
	LOSS [training: 3.7766249693096703 | validation: 4.79752216156695]
	TIME [epoch: 9.71 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7722328997664425		[learning rate: 0.00011188]
	Learning Rate: 0.000111876
	LOSS [training: 3.7722328997664425 | validation: 4.788613156565566]
	TIME [epoch: 9.72 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772371302241877		[learning rate: 0.00011147]
	Learning Rate: 0.00011147
	LOSS [training: 3.772371302241877 | validation: 4.797070228307234]
	TIME [epoch: 9.73 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781163737880945		[learning rate: 0.00011107]
	Learning Rate: 0.000111065
	LOSS [training: 3.781163737880945 | validation: 4.796048723642675]
	TIME [epoch: 9.71 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776687795373404		[learning rate: 0.00011066]
	Learning Rate: 0.000110662
	LOSS [training: 3.776687795373404 | validation: 4.80105499513224]
	TIME [epoch: 9.72 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7812480041845262		[learning rate: 0.00011026]
	Learning Rate: 0.000110261
	LOSS [training: 3.7812480041845262 | validation: 4.7866091299791895]
	TIME [epoch: 9.72 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7781923911743833		[learning rate: 0.00010986]
	Learning Rate: 0.000109861
	LOSS [training: 3.7781923911743833 | validation: 4.793698284442002]
	TIME [epoch: 9.72 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7797817548340094		[learning rate: 0.00010946]
	Learning Rate: 0.000109462
	LOSS [training: 3.7797817548340094 | validation: 4.796771523148858]
	TIME [epoch: 9.72 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775906774314059		[learning rate: 0.00010906]
	Learning Rate: 0.000109065
	LOSS [training: 3.775906774314059 | validation: 4.796174001061575]
	TIME [epoch: 9.72 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781549367569032		[learning rate: 0.00010867]
	Learning Rate: 0.000108669
	LOSS [training: 3.781549367569032 | validation: 4.7981002964781725]
	TIME [epoch: 9.73 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7774693825566197		[learning rate: 0.00010827]
	Learning Rate: 0.000108275
	LOSS [training: 3.7774693825566197 | validation: 4.789350915621005]
	TIME [epoch: 9.72 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7839650879266338		[learning rate: 0.00010788]
	Learning Rate: 0.000107882
	LOSS [training: 3.7839650879266338 | validation: 4.802895753237444]
	TIME [epoch: 9.71 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779964122413844		[learning rate: 0.00010749]
	Learning Rate: 0.00010749
	LOSS [training: 3.779964122413844 | validation: 4.788568157958446]
	TIME [epoch: 9.71 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7769851561252628		[learning rate: 0.0001071]
	Learning Rate: 0.0001071
	LOSS [training: 3.7769851561252628 | validation: 4.792496612928691]
	TIME [epoch: 9.73 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7824560414099024		[learning rate: 0.00010671]
	Learning Rate: 0.000106711
	LOSS [training: 3.7824560414099024 | validation: 4.798871498635809]
	TIME [epoch: 9.71 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7811797140672234		[learning rate: 0.00010632]
	Learning Rate: 0.000106324
	LOSS [training: 3.7811797140672234 | validation: 4.792791103361928]
	TIME [epoch: 9.71 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7839802815249657		[learning rate: 0.00010594]
	Learning Rate: 0.000105938
	LOSS [training: 3.7839802815249657 | validation: 4.791398475313541]
	TIME [epoch: 9.72 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7770690434948713		[learning rate: 0.00010555]
	Learning Rate: 0.000105554
	LOSS [training: 3.7770690434948713 | validation: 4.796866561340563]
	TIME [epoch: 9.72 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7732097280592547		[learning rate: 0.00010517]
	Learning Rate: 0.000105171
	LOSS [training: 3.7732097280592547 | validation: 4.804317101635024]
	TIME [epoch: 9.71 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7772365804077355		[learning rate: 0.00010479]
	Learning Rate: 0.000104789
	LOSS [training: 3.7772365804077355 | validation: 4.798161101510858]
	TIME [epoch: 9.71 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7776598420978487		[learning rate: 0.00010441]
	Learning Rate: 0.000104409
	LOSS [training: 3.7776598420978487 | validation: 4.7882546840967795]
	TIME [epoch: 9.73 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7768192942533205		[learning rate: 0.00010403]
	Learning Rate: 0.00010403
	LOSS [training: 3.7768192942533205 | validation: 4.793507414217249]
	TIME [epoch: 9.71 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775005257906076		[learning rate: 0.00010365]
	Learning Rate: 0.000103652
	LOSS [training: 3.775005257906076 | validation: 4.793741266425199]
	TIME [epoch: 9.72 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781090543289365		[learning rate: 0.00010328]
	Learning Rate: 0.000103276
	LOSS [training: 3.781090543289365 | validation: 4.793717087701823]
	TIME [epoch: 9.71 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776671550928031		[learning rate: 0.0001029]
	Learning Rate: 0.000102901
	LOSS [training: 3.776671550928031 | validation: 4.794770729258791]
	TIME [epoch: 9.74 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7777647112990578		[learning rate: 0.00010253]
	Learning Rate: 0.000102528
	LOSS [training: 3.7777647112990578 | validation: 4.808160621373918]
	TIME [epoch: 9.72 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7752809041494606		[learning rate: 0.00010216]
	Learning Rate: 0.000102156
	LOSS [training: 3.7752809041494606 | validation: 4.806371838272185]
	TIME [epoch: 9.71 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772272182359834		[learning rate: 0.00010179]
	Learning Rate: 0.000101785
	LOSS [training: 3.772272182359834 | validation: 4.796826860410873]
	TIME [epoch: 9.72 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7782444967256104		[learning rate: 0.00010142]
	Learning Rate: 0.000101416
	LOSS [training: 3.7782444967256104 | validation: 4.794439867013948]
	TIME [epoch: 9.73 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7781650222668417		[learning rate: 0.00010105]
	Learning Rate: 0.000101048
	LOSS [training: 3.7781650222668417 | validation: 4.809074660763092]
	TIME [epoch: 9.72 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7809310734990724		[learning rate: 0.00010068]
	Learning Rate: 0.000100681
	LOSS [training: 3.7809310734990724 | validation: 4.808322798966535]
	TIME [epoch: 9.71 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7797845163280748		[learning rate: 0.00010032]
	Learning Rate: 0.000100316
	LOSS [training: 3.7797845163280748 | validation: 4.792963427693914]
	TIME [epoch: 9.72 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7701382574816136		[learning rate: 9.9952e-05]
	Learning Rate: 9.99515e-05
	LOSS [training: 3.7701382574816136 | validation: 4.786063776202363]
	TIME [epoch: 9.72 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7754051213409148		[learning rate: 9.9589e-05]
	Learning Rate: 9.95888e-05
	LOSS [training: 3.7754051213409148 | validation: 4.801193583246875]
	TIME [epoch: 9.71 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7835856323032138		[learning rate: 9.9227e-05]
	Learning Rate: 9.92274e-05
	LOSS [training: 3.7835856323032138 | validation: 4.799252397107749]
	TIME [epoch: 9.72 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7890757080347663		[learning rate: 9.8867e-05]
	Learning Rate: 9.88673e-05
	LOSS [training: 3.7890757080347663 | validation: 4.811182682123236]
	TIME [epoch: 9.74 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7898640917175834		[learning rate: 9.8509e-05]
	Learning Rate: 9.85085e-05
	LOSS [training: 3.7898640917175834 | validation: 4.800044301177821]
	TIME [epoch: 9.72 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776969627220177		[learning rate: 9.8151e-05]
	Learning Rate: 9.8151e-05
	LOSS [training: 3.776969627220177 | validation: 4.797170954341125]
	TIME [epoch: 9.71 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77518135854901		[learning rate: 9.7795e-05]
	Learning Rate: 9.77948e-05
	LOSS [training: 3.77518135854901 | validation: 4.801424892699778]
	TIME [epoch: 9.71 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776069731529165		[learning rate: 9.744e-05]
	Learning Rate: 9.74399e-05
	LOSS [training: 3.776069731529165 | validation: 4.792802748021554]
	TIME [epoch: 9.73 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7767634444105433		[learning rate: 9.7086e-05]
	Learning Rate: 9.70863e-05
	LOSS [training: 3.7767634444105433 | validation: 4.8088497195645585]
	TIME [epoch: 9.71 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778573887653246		[learning rate: 9.6734e-05]
	Learning Rate: 9.6734e-05
	LOSS [training: 3.778573887653246 | validation: 4.801881113924627]
	TIME [epoch: 9.72 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781492176260973		[learning rate: 9.6383e-05]
	Learning Rate: 9.63829e-05
	LOSS [training: 3.781492176260973 | validation: 4.797189771562659]
	TIME [epoch: 9.72 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7776119791707066		[learning rate: 9.6033e-05]
	Learning Rate: 9.60331e-05
	LOSS [training: 3.7776119791707066 | validation: 4.8101261446996135]
	TIME [epoch: 9.73 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7800824079727393		[learning rate: 9.5685e-05]
	Learning Rate: 9.56846e-05
	LOSS [training: 3.7800824079727393 | validation: 4.79604366473487]
	TIME [epoch: 9.72 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777284944566575		[learning rate: 9.5337e-05]
	Learning Rate: 9.53374e-05
	LOSS [training: 3.777284944566575 | validation: 4.806352137046502]
	TIME [epoch: 9.71 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7819494475752378		[learning rate: 9.4991e-05]
	Learning Rate: 9.49914e-05
	LOSS [training: 3.7819494475752378 | validation: 4.799425272485005]
	TIME [epoch: 9.73 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7811841302023126		[learning rate: 9.4647e-05]
	Learning Rate: 9.46466e-05
	LOSS [training: 3.7811841302023126 | validation: 4.793816799684982]
	TIME [epoch: 9.72 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773070169666209		[learning rate: 9.4303e-05]
	Learning Rate: 9.43032e-05
	LOSS [training: 3.773070169666209 | validation: 4.788660059070155]
	TIME [epoch: 9.72 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7794370782293294		[learning rate: 9.3961e-05]
	Learning Rate: 9.39609e-05
	LOSS [training: 3.7794370782293294 | validation: 4.796417612407978]
	TIME [epoch: 9.71 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7776529098990594		[learning rate: 9.362e-05]
	Learning Rate: 9.362e-05
	LOSS [training: 3.7776529098990594 | validation: 4.780504056131238]
	TIME [epoch: 9.73 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7801217914525624		[learning rate: 9.328e-05]
	Learning Rate: 9.32802e-05
	LOSS [training: 3.7801217914525624 | validation: 4.792307295658011]
	TIME [epoch: 9.72 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7767079817758473		[learning rate: 9.2942e-05]
	Learning Rate: 9.29417e-05
	LOSS [training: 3.7767079817758473 | validation: 4.809338699556785]
	TIME [epoch: 9.71 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7835900647677576		[learning rate: 9.2604e-05]
	Learning Rate: 9.26044e-05
	LOSS [training: 3.7835900647677576 | validation: 4.80385099739136]
	TIME [epoch: 9.72 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777843867488624		[learning rate: 9.2268e-05]
	Learning Rate: 9.22683e-05
	LOSS [training: 3.777843867488624 | validation: 4.786191720533599]
	TIME [epoch: 9.73 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7754828850811686		[learning rate: 9.1933e-05]
	Learning Rate: 9.19335e-05
	LOSS [training: 3.7754828850811686 | validation: 4.805078144951687]
	TIME [epoch: 9.72 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7750751061681087		[learning rate: 9.16e-05]
	Learning Rate: 9.15998e-05
	LOSS [training: 3.7750751061681087 | validation: 4.79839812288738]
	TIME [epoch: 9.72 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7765858770828507		[learning rate: 9.1267e-05]
	Learning Rate: 9.12674e-05
	LOSS [training: 3.7765858770828507 | validation: 4.807958907460167]
	TIME [epoch: 9.73 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773625135735661		[learning rate: 9.0936e-05]
	Learning Rate: 9.09362e-05
	LOSS [training: 3.773625135735661 | validation: 4.794349366595421]
	TIME [epoch: 9.73 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777305800226178		[learning rate: 9.0606e-05]
	Learning Rate: 9.06062e-05
	LOSS [training: 3.777305800226178 | validation: 4.801316402149959]
	TIME [epoch: 9.71 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780504023478499		[learning rate: 9.0277e-05]
	Learning Rate: 9.02774e-05
	LOSS [training: 3.780504023478499 | validation: 4.790903259870938]
	TIME [epoch: 9.71 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7811874203475306		[learning rate: 8.995e-05]
	Learning Rate: 8.99498e-05
	LOSS [training: 3.7811874203475306 | validation: 4.8034021765186585]
	TIME [epoch: 9.74 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7774812790229504		[learning rate: 8.9623e-05]
	Learning Rate: 8.96233e-05
	LOSS [training: 3.7774812790229504 | validation: 4.794916795550899]
	TIME [epoch: 9.72 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778755552774052		[learning rate: 8.9298e-05]
	Learning Rate: 8.92981e-05
	LOSS [training: 3.778755552774052 | validation: 4.795601419426843]
	TIME [epoch: 9.71 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780864386722898		[learning rate: 8.8974e-05]
	Learning Rate: 8.8974e-05
	LOSS [training: 3.780864386722898 | validation: 4.805458904917265]
	TIME [epoch: 9.71 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7873385704662907		[learning rate: 8.8651e-05]
	Learning Rate: 8.86511e-05
	LOSS [training: 3.7873385704662907 | validation: 4.799841381414518]
	TIME [epoch: 9.71 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774186141237192		[learning rate: 8.8329e-05]
	Learning Rate: 8.83294e-05
	LOSS [training: 3.774186141237192 | validation: 4.8121917664012575]
	TIME [epoch: 9.72 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7805745295969855		[learning rate: 8.8009e-05]
	Learning Rate: 8.80088e-05
	LOSS [training: 3.7805745295969855 | validation: 4.803925470133593]
	TIME [epoch: 9.71 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.78343152946234		[learning rate: 8.7689e-05]
	Learning Rate: 8.76895e-05
	LOSS [training: 3.78343152946234 | validation: 4.801807125051855]
	TIME [epoch: 9.73 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773531405211602		[learning rate: 8.7371e-05]
	Learning Rate: 8.73712e-05
	LOSS [training: 3.773531405211602 | validation: 4.790807520599933]
	TIME [epoch: 9.72 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7754356467986883		[learning rate: 8.7054e-05]
	Learning Rate: 8.70542e-05
	LOSS [training: 3.7754356467986883 | validation: 4.801898975852559]
	TIME [epoch: 9.71 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7784473634501907		[learning rate: 8.6738e-05]
	Learning Rate: 8.67382e-05
	LOSS [training: 3.7784473634501907 | validation: 4.803989914814585]
	TIME [epoch: 9.71 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7832238996481364		[learning rate: 8.6423e-05]
	Learning Rate: 8.64235e-05
	LOSS [training: 3.7832238996481364 | validation: 4.784770177469442]
	TIME [epoch: 9.73 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7776842173836735		[learning rate: 8.611e-05]
	Learning Rate: 8.61098e-05
	LOSS [training: 3.7776842173836735 | validation: 4.7881321002092605]
	TIME [epoch: 9.72 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7783238425698222		[learning rate: 8.5797e-05]
	Learning Rate: 8.57973e-05
	LOSS [training: 3.7783238425698222 | validation: 4.789196780301312]
	TIME [epoch: 9.71 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7790036681123107		[learning rate: 8.5486e-05]
	Learning Rate: 8.54859e-05
	LOSS [training: 3.7790036681123107 | validation: 4.796478743118744]
	TIME [epoch: 9.71 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775373183710106		[learning rate: 8.5176e-05]
	Learning Rate: 8.51757e-05
	LOSS [training: 3.775373183710106 | validation: 4.795011751767541]
	TIME [epoch: 9.74 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775635216313533		[learning rate: 8.4867e-05]
	Learning Rate: 8.48666e-05
	LOSS [training: 3.775635216313533 | validation: 4.804326760575252]
	TIME [epoch: 9.71 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7780598405789156		[learning rate: 8.4559e-05]
	Learning Rate: 8.45586e-05
	LOSS [training: 3.7780598405789156 | validation: 4.806424323603193]
	TIME [epoch: 9.71 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7815803838929107		[learning rate: 8.4252e-05]
	Learning Rate: 8.42518e-05
	LOSS [training: 3.7815803838929107 | validation: 4.793978790392273]
	TIME [epoch: 9.71 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778086384131298		[learning rate: 8.3946e-05]
	Learning Rate: 8.3946e-05
	LOSS [training: 3.778086384131298 | validation: 4.805909873641234]
	TIME [epoch: 9.73 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.784125352292299		[learning rate: 8.3641e-05]
	Learning Rate: 8.36414e-05
	LOSS [training: 3.784125352292299 | validation: 4.7937750278033135]
	TIME [epoch: 9.72 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7876523681272927		[learning rate: 8.3338e-05]
	Learning Rate: 8.33378e-05
	LOSS [training: 3.7876523681272927 | validation: 4.8032065834564905]
	TIME [epoch: 9.72 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781983971679524		[learning rate: 8.3035e-05]
	Learning Rate: 8.30354e-05
	LOSS [training: 3.781983971679524 | validation: 4.799856844098893]
	TIME [epoch: 9.73 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779882748775916		[learning rate: 8.2734e-05]
	Learning Rate: 8.2734e-05
	LOSS [training: 3.779882748775916 | validation: 4.78751958391025]
	TIME [epoch: 9.73 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780042953809823		[learning rate: 8.2434e-05]
	Learning Rate: 8.24338e-05
	LOSS [training: 3.780042953809823 | validation: 4.791576978910181]
	TIME [epoch: 9.72 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7766560423212097		[learning rate: 8.2135e-05]
	Learning Rate: 8.21346e-05
	LOSS [training: 3.7766560423212097 | validation: 4.795426680676395]
	TIME [epoch: 9.72 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776232024521859		[learning rate: 8.1837e-05]
	Learning Rate: 8.18366e-05
	LOSS [training: 3.776232024521859 | validation: 4.7997513067173365]
	TIME [epoch: 9.73 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7753963775196246		[learning rate: 8.154e-05]
	Learning Rate: 8.15396e-05
	LOSS [training: 3.7753963775196246 | validation: 4.794309671612917]
	TIME [epoch: 9.72 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7753019756032002		[learning rate: 8.1244e-05]
	Learning Rate: 8.12437e-05
	LOSS [training: 3.7753019756032002 | validation: 4.8072626806389005]
	TIME [epoch: 9.72 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7820534457325308		[learning rate: 8.0949e-05]
	Learning Rate: 8.09488e-05
	LOSS [training: 3.7820534457325308 | validation: 4.7936600814437105]
	TIME [epoch: 9.71 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7699669056673764		[learning rate: 8.0655e-05]
	Learning Rate: 8.0655e-05
	LOSS [training: 3.7699669056673764 | validation: 4.798510168981181]
	TIME [epoch: 9.73 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7748300952948286		[learning rate: 8.0362e-05]
	Learning Rate: 8.03623e-05
	LOSS [training: 3.7748300952948286 | validation: 4.792804743619249]
	TIME [epoch: 9.72 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773568201418164		[learning rate: 8.0071e-05]
	Learning Rate: 8.00707e-05
	LOSS [training: 3.773568201418164 | validation: 4.799592345631111]
	TIME [epoch: 9.71 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772362298850775		[learning rate: 7.978e-05]
	Learning Rate: 7.97801e-05
	LOSS [training: 3.772362298850775 | validation: 4.793295717309461]
	TIME [epoch: 9.72 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7806168997846767		[learning rate: 7.9491e-05]
	Learning Rate: 7.94906e-05
	LOSS [training: 3.7806168997846767 | validation: 4.783857644391203]
	TIME [epoch: 9.72 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7738330642687172		[learning rate: 7.9202e-05]
	Learning Rate: 7.92021e-05
	LOSS [training: 3.7738330642687172 | validation: 4.8011807106722735]
	TIME [epoch: 9.71 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7771199537553803		[learning rate: 7.8915e-05]
	Learning Rate: 7.89147e-05
	LOSS [training: 3.7771199537553803 | validation: 4.7937220401382605]
	TIME [epoch: 9.72 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7785323320724897		[learning rate: 7.8628e-05]
	Learning Rate: 7.86283e-05
	LOSS [training: 3.7785323320724897 | validation: 4.785755678711913]
	TIME [epoch: 9.73 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777288191126382		[learning rate: 7.8343e-05]
	Learning Rate: 7.8343e-05
	LOSS [training: 3.777288191126382 | validation: 4.792350226617572]
	TIME [epoch: 9.7 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7728840072876446		[learning rate: 7.8059e-05]
	Learning Rate: 7.80586e-05
	LOSS [training: 3.7728840072876446 | validation: 4.802479303559964]
	TIME [epoch: 9.72 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7726896543022646		[learning rate: 7.7775e-05]
	Learning Rate: 7.77754e-05
	LOSS [training: 3.7726896543022646 | validation: 4.799737657516273]
	TIME [epoch: 9.71 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775742562203822		[learning rate: 7.7493e-05]
	Learning Rate: 7.74931e-05
	LOSS [training: 3.775742562203822 | validation: 4.796809842947002]
	TIME [epoch: 9.74 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7799580075694608		[learning rate: 7.7212e-05]
	Learning Rate: 7.72119e-05
	LOSS [training: 3.7799580075694608 | validation: 4.796359009879252]
	TIME [epoch: 9.71 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775937984094397		[learning rate: 7.6932e-05]
	Learning Rate: 7.69317e-05
	LOSS [training: 3.775937984094397 | validation: 4.783932545761654]
	TIME [epoch: 9.72 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7779467125348836		[learning rate: 7.6653e-05]
	Learning Rate: 7.66525e-05
	LOSS [training: 3.7779467125348836 | validation: 4.798932730330837]
	TIME [epoch: 9.72 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77754876766289		[learning rate: 7.6374e-05]
	Learning Rate: 7.63743e-05
	LOSS [training: 3.77754876766289 | validation: 4.797078299213946]
	TIME [epoch: 9.71 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773225603489263		[learning rate: 7.6097e-05]
	Learning Rate: 7.60972e-05
	LOSS [training: 3.773225603489263 | validation: 4.802228453208378]
	TIME [epoch: 9.7 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7831493018442997		[learning rate: 7.5821e-05]
	Learning Rate: 7.5821e-05
	LOSS [training: 3.7831493018442997 | validation: 4.803039310858911]
	TIME [epoch: 9.72 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772280207367751		[learning rate: 7.5546e-05]
	Learning Rate: 7.55458e-05
	LOSS [training: 3.772280207367751 | validation: 4.804513951470991]
	TIME [epoch: 9.71 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77550799784487		[learning rate: 7.5272e-05]
	Learning Rate: 7.52717e-05
	LOSS [training: 3.77550799784487 | validation: 4.794439549018332]
	TIME [epoch: 9.73 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7765005331808963		[learning rate: 7.4999e-05]
	Learning Rate: 7.49985e-05
	LOSS [training: 3.7765005331808963 | validation: 4.797433246600817]
	TIME [epoch: 9.72 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7762813471040433		[learning rate: 7.4726e-05]
	Learning Rate: 7.47263e-05
	LOSS [training: 3.7762813471040433 | validation: 4.7964090287345025]
	TIME [epoch: 9.72 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776118397648399		[learning rate: 7.4455e-05]
	Learning Rate: 7.44552e-05
	LOSS [training: 3.776118397648399 | validation: 4.796243973948967]
	TIME [epoch: 9.73 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7758422731652375		[learning rate: 7.4185e-05]
	Learning Rate: 7.4185e-05
	LOSS [training: 3.7758422731652375 | validation: 4.794689637568691]
	TIME [epoch: 9.72 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778452471738504		[learning rate: 7.3916e-05]
	Learning Rate: 7.39157e-05
	LOSS [training: 3.778452471738504 | validation: 4.800976542617864]
	TIME [epoch: 9.72 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773909311389965		[learning rate: 7.3647e-05]
	Learning Rate: 7.36475e-05
	LOSS [training: 3.773909311389965 | validation: 4.797361153312352]
	TIME [epoch: 9.72 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774188967881693		[learning rate: 7.338e-05]
	Learning Rate: 7.33802e-05
	LOSS [training: 3.774188967881693 | validation: 4.7927687574404345]
	TIME [epoch: 9.72 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7796845789105133		[learning rate: 7.3114e-05]
	Learning Rate: 7.31139e-05
	LOSS [training: 3.7796845789105133 | validation: 4.806797976620957]
	TIME [epoch: 9.71 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7789499350231757		[learning rate: 7.2849e-05]
	Learning Rate: 7.28486e-05
	LOSS [training: 3.7789499350231757 | validation: 4.792956216253239]
	TIME [epoch: 9.71 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774045708211863		[learning rate: 7.2584e-05]
	Learning Rate: 7.25842e-05
	LOSS [training: 3.774045708211863 | validation: 4.791220782502016]
	TIME [epoch: 9.73 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7756932743074003		[learning rate: 7.2321e-05]
	Learning Rate: 7.23208e-05
	LOSS [training: 3.7756932743074003 | validation: 4.788050368857342]
	TIME [epoch: 9.72 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774685157025148		[learning rate: 7.2058e-05]
	Learning Rate: 7.20583e-05
	LOSS [training: 3.774685157025148 | validation: 4.79437324978859]
	TIME [epoch: 9.71 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.770972407360378		[learning rate: 7.1797e-05]
	Learning Rate: 7.17968e-05
	LOSS [training: 3.770972407360378 | validation: 4.792893844911696]
	TIME [epoch: 9.71 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776904528805286		[learning rate: 7.1536e-05]
	Learning Rate: 7.15363e-05
	LOSS [training: 3.776904528805286 | validation: 4.792482198542729]
	TIME [epoch: 9.72 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77739928025955		[learning rate: 7.1277e-05]
	Learning Rate: 7.12767e-05
	LOSS [training: 3.77739928025955 | validation: 4.800151212924837]
	TIME [epoch: 9.71 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7790855708254343		[learning rate: 7.1018e-05]
	Learning Rate: 7.1018e-05
	LOSS [training: 3.7790855708254343 | validation: 4.795496225732898]
	TIME [epoch: 9.71 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777850585112457		[learning rate: 7.076e-05]
	Learning Rate: 7.07603e-05
	LOSS [training: 3.777850585112457 | validation: 4.788879819919619]
	TIME [epoch: 9.71 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7757832135633898		[learning rate: 7.0503e-05]
	Learning Rate: 7.05035e-05
	LOSS [training: 3.7757832135633898 | validation: 4.78987803957686]
	TIME [epoch: 9.74 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7739741281544803		[learning rate: 7.0248e-05]
	Learning Rate: 7.02476e-05
	LOSS [training: 3.7739741281544803 | validation: 4.8007594313916115]
	TIME [epoch: 9.71 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772189036068916		[learning rate: 6.9993e-05]
	Learning Rate: 6.99927e-05
	LOSS [training: 3.772189036068916 | validation: 4.7885988965602335]
	TIME [epoch: 9.71 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7788134272039464		[learning rate: 6.9739e-05]
	Learning Rate: 6.97387e-05
	LOSS [training: 3.7788134272039464 | validation: 4.796291909897756]
	TIME [epoch: 9.72 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7817808904140846		[learning rate: 6.9486e-05]
	Learning Rate: 6.94856e-05
	LOSS [training: 3.7817808904140846 | validation: 4.788846971106937]
	TIME [epoch: 9.74 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775167833207518		[learning rate: 6.9233e-05]
	Learning Rate: 6.92334e-05
	LOSS [training: 3.775167833207518 | validation: 4.799641786701052]
	TIME [epoch: 9.71 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775275961145546		[learning rate: 6.8982e-05]
	Learning Rate: 6.89822e-05
	LOSS [training: 3.775275961145546 | validation: 4.811268444888489]
	TIME [epoch: 9.71 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7744885054982857		[learning rate: 6.8732e-05]
	Learning Rate: 6.87318e-05
	LOSS [training: 3.7744885054982857 | validation: 4.79847036827716]
	TIME [epoch: 9.73 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776997850768506		[learning rate: 6.8482e-05]
	Learning Rate: 6.84824e-05
	LOSS [training: 3.776997850768506 | validation: 4.790650023264276]
	TIME [epoch: 9.73 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7771875455456425		[learning rate: 6.8234e-05]
	Learning Rate: 6.82339e-05
	LOSS [training: 3.7771875455456425 | validation: 4.781095680507686]
	TIME [epoch: 9.72 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7739041992817945		[learning rate: 6.7986e-05]
	Learning Rate: 6.79862e-05
	LOSS [training: 3.7739041992817945 | validation: 4.788696735868924]
	TIME [epoch: 9.71 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7732804806098725		[learning rate: 6.774e-05]
	Learning Rate: 6.77395e-05
	LOSS [training: 3.7732804806098725 | validation: 4.795813891832463]
	TIME [epoch: 9.72 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776875844412532		[learning rate: 6.7494e-05]
	Learning Rate: 6.74937e-05
	LOSS [training: 3.776875844412532 | validation: 4.799888820663287]
	TIME [epoch: 9.72 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779202475018755		[learning rate: 6.7249e-05]
	Learning Rate: 6.72488e-05
	LOSS [training: 3.779202475018755 | validation: 4.791433029765229]
	TIME [epoch: 9.71 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780964554070195		[learning rate: 6.7005e-05]
	Learning Rate: 6.70047e-05
	LOSS [training: 3.780964554070195 | validation: 4.800844744529074]
	TIME [epoch: 9.72 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776773874194659		[learning rate: 6.6762e-05]
	Learning Rate: 6.67616e-05
	LOSS [training: 3.776773874194659 | validation: 4.7979224351114285]
	TIME [epoch: 9.72 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775243911588498		[learning rate: 6.6519e-05]
	Learning Rate: 6.65192e-05
	LOSS [training: 3.775243911588498 | validation: 4.804256420395775]
	TIME [epoch: 9.71 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779418198944975		[learning rate: 6.6278e-05]
	Learning Rate: 6.62778e-05
	LOSS [training: 3.779418198944975 | validation: 4.79951749169485]
	TIME [epoch: 9.71 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7741591905865683		[learning rate: 6.6037e-05]
	Learning Rate: 6.60373e-05
	LOSS [training: 3.7741591905865683 | validation: 4.80617377873855]
	TIME [epoch: 9.72 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7803825894259795		[learning rate: 6.5798e-05]
	Learning Rate: 6.57977e-05
	LOSS [training: 3.7803825894259795 | validation: 4.8060841867675235]
	TIME [epoch: 9.71 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7796617578354224		[learning rate: 6.5559e-05]
	Learning Rate: 6.55589e-05
	LOSS [training: 3.7796617578354224 | validation: 4.798301937695996]
	TIME [epoch: 9.72 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773936076147263		[learning rate: 6.5321e-05]
	Learning Rate: 6.5321e-05
	LOSS [training: 3.773936076147263 | validation: 4.789950729101461]
	TIME [epoch: 9.72 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777007621730828		[learning rate: 6.5084e-05]
	Learning Rate: 6.50839e-05
	LOSS [training: 3.777007621730828 | validation: 4.798481753993103]
	TIME [epoch: 9.73 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7748888938435874		[learning rate: 6.4848e-05]
	Learning Rate: 6.48477e-05
	LOSS [training: 3.7748888938435874 | validation: 4.794004325895474]
	TIME [epoch: 9.72 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7757228835115155		[learning rate: 6.4612e-05]
	Learning Rate: 6.46124e-05
	LOSS [training: 3.7757228835115155 | validation: 4.792168990229204]
	TIME [epoch: 9.71 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7756978785274335		[learning rate: 6.4378e-05]
	Learning Rate: 6.43779e-05
	LOSS [training: 3.7756978785274335 | validation: 4.800556650758422]
	TIME [epoch: 9.71 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775625423235302		[learning rate: 6.4144e-05]
	Learning Rate: 6.41443e-05
	LOSS [training: 3.775625423235302 | validation: 4.79770941804314]
	TIME [epoch: 9.74 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775551966136635		[learning rate: 6.3911e-05]
	Learning Rate: 6.39115e-05
	LOSS [training: 3.775551966136635 | validation: 4.803185037372908]
	TIME [epoch: 9.72 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773626552565007		[learning rate: 6.368e-05]
	Learning Rate: 6.36795e-05
	LOSS [training: 3.773626552565007 | validation: 4.794496356079072]
	TIME [epoch: 9.71 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.785022491363713		[learning rate: 6.3448e-05]
	Learning Rate: 6.34485e-05
	LOSS [training: 3.785022491363713 | validation: 4.80657317232444]
	TIME [epoch: 9.72 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7742335740218858		[learning rate: 6.3218e-05]
	Learning Rate: 6.32182e-05
	LOSS [training: 3.7742335740218858 | validation: 4.792088686204833]
	TIME [epoch: 9.73 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77714837938359		[learning rate: 6.2989e-05]
	Learning Rate: 6.29888e-05
	LOSS [training: 3.77714837938359 | validation: 4.798758805538737]
	TIME [epoch: 9.72 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776962013179298		[learning rate: 6.276e-05]
	Learning Rate: 6.27602e-05
	LOSS [training: 3.776962013179298 | validation: 4.7911440584432174]
	TIME [epoch: 9.71 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775178925181541		[learning rate: 6.2532e-05]
	Learning Rate: 6.25324e-05
	LOSS [training: 3.775178925181541 | validation: 4.788236256238636]
	TIME [epoch: 9.72 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7768211387656523		[learning rate: 6.2305e-05]
	Learning Rate: 6.23055e-05
	LOSS [training: 3.7768211387656523 | validation: 4.792473219587771]
	TIME [epoch: 9.73 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7705375350945154		[learning rate: 6.2079e-05]
	Learning Rate: 6.20794e-05
	LOSS [training: 3.7705375350945154 | validation: 4.788027201345228]
	TIME [epoch: 9.71 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774333548833809		[learning rate: 6.1854e-05]
	Learning Rate: 6.18541e-05
	LOSS [training: 3.774333548833809 | validation: 4.796193509078765]
	TIME [epoch: 9.72 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77756430505469		[learning rate: 6.163e-05]
	Learning Rate: 6.16296e-05
	LOSS [training: 3.77756430505469 | validation: 4.799023075291307]
	TIME [epoch: 9.73 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778424652856267		[learning rate: 6.1406e-05]
	Learning Rate: 6.1406e-05
	LOSS [training: 3.778424652856267 | validation: 4.795282588231004]
	TIME [epoch: 9.72 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7748429197034525		[learning rate: 6.1183e-05]
	Learning Rate: 6.11831e-05
	LOSS [training: 3.7748429197034525 | validation: 4.80550244913707]
	TIME [epoch: 9.72 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7795796597026348		[learning rate: 6.0961e-05]
	Learning Rate: 6.09611e-05
	LOSS [training: 3.7795796597026348 | validation: 4.808911414027606]
	TIME [epoch: 9.72 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775357712451965		[learning rate: 6.074e-05]
	Learning Rate: 6.07399e-05
	LOSS [training: 3.775357712451965 | validation: 4.794155148196272]
	TIME [epoch: 9.73 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774396538919872		[learning rate: 6.0519e-05]
	Learning Rate: 6.05194e-05
	LOSS [training: 3.774396538919872 | validation: 4.805546562652231]
	TIME [epoch: 9.72 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7734523474645023		[learning rate: 6.03e-05]
	Learning Rate: 6.02998e-05
	LOSS [training: 3.7734523474645023 | validation: 4.787263697882333]
	TIME [epoch: 9.72 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7766580754647294		[learning rate: 6.0081e-05]
	Learning Rate: 6.0081e-05
	LOSS [training: 3.7766580754647294 | validation: 4.79698532405221]
	TIME [epoch: 9.74 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7769652470591253		[learning rate: 5.9863e-05]
	Learning Rate: 5.98629e-05
	LOSS [training: 3.7769652470591253 | validation: 4.799119697062253]
	TIME [epoch: 9.73 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7759454629147924		[learning rate: 5.9646e-05]
	Learning Rate: 5.96457e-05
	LOSS [training: 3.7759454629147924 | validation: 4.786421552103987]
	TIME [epoch: 9.72 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773810964758981		[learning rate: 5.9429e-05]
	Learning Rate: 5.94292e-05
	LOSS [training: 3.773810964758981 | validation: 4.796925751656545]
	TIME [epoch: 9.72 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7759624864483308		[learning rate: 5.9214e-05]
	Learning Rate: 5.92136e-05
	LOSS [training: 3.7759624864483308 | validation: 4.7941961038166445]
	TIME [epoch: 9.73 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7795974514973345		[learning rate: 5.8999e-05]
	Learning Rate: 5.89987e-05
	LOSS [training: 3.7795974514973345 | validation: 4.788095346274635]
	TIME [epoch: 9.71 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773967926612806		[learning rate: 5.8785e-05]
	Learning Rate: 5.87846e-05
	LOSS [training: 3.773967926612806 | validation: 4.795626988184116]
	TIME [epoch: 9.7 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7791177283133566		[learning rate: 5.8571e-05]
	Learning Rate: 5.85712e-05
	LOSS [training: 3.7791177283133566 | validation: 4.792058186465401]
	TIME [epoch: 9.71 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7740111494696365		[learning rate: 5.8359e-05]
	Learning Rate: 5.83586e-05
	LOSS [training: 3.7740111494696365 | validation: 4.785341341508365]
	TIME [epoch: 9.74 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7805617945635364		[learning rate: 5.8147e-05]
	Learning Rate: 5.81469e-05
	LOSS [training: 3.7805617945635364 | validation: 4.796560325285485]
	TIME [epoch: 9.72 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779543592682059		[learning rate: 5.7936e-05]
	Learning Rate: 5.79358e-05
	LOSS [training: 3.779543592682059 | validation: 4.809676757626964]
	TIME [epoch: 9.71 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776554396819614		[learning rate: 5.7726e-05]
	Learning Rate: 5.77256e-05
	LOSS [training: 3.776554396819614 | validation: 4.792628728491896]
	TIME [epoch: 9.71 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77805701207813		[learning rate: 5.7516e-05]
	Learning Rate: 5.75161e-05
	LOSS [training: 3.77805701207813 | validation: 4.806283063109397]
	TIME [epoch: 9.73 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772801579320897		[learning rate: 5.7307e-05]
	Learning Rate: 5.73074e-05
	LOSS [training: 3.772801579320897 | validation: 4.781609638666175]
	TIME [epoch: 9.73 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7737677132573637		[learning rate: 5.7099e-05]
	Learning Rate: 5.70994e-05
	LOSS [training: 3.7737677132573637 | validation: 4.799647970402828]
	TIME [epoch: 9.72 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778374088235701		[learning rate: 5.6892e-05]
	Learning Rate: 5.68922e-05
	LOSS [training: 3.778374088235701 | validation: 4.797980469682106]
	TIME [epoch: 9.72 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7743433756809943		[learning rate: 5.6686e-05]
	Learning Rate: 5.66857e-05
	LOSS [training: 3.7743433756809943 | validation: 4.796364549824232]
	TIME [epoch: 9.72 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772576737027183		[learning rate: 5.648e-05]
	Learning Rate: 5.648e-05
	LOSS [training: 3.772576737027183 | validation: 4.792946876134842]
	TIME [epoch: 9.72 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778109377584947		[learning rate: 5.6275e-05]
	Learning Rate: 5.6275e-05
	LOSS [training: 3.778109377584947 | validation: 4.793156244831598]
	TIME [epoch: 9.72 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7721523136718424		[learning rate: 5.6071e-05]
	Learning Rate: 5.60708e-05
	LOSS [training: 3.7721523136718424 | validation: 4.803295244367413]
	TIME [epoch: 9.73 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7755622013132815		[learning rate: 5.5867e-05]
	Learning Rate: 5.58673e-05
	LOSS [training: 3.7755622013132815 | validation: 4.8045424693559715]
	TIME [epoch: 9.72 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7722765248767836		[learning rate: 5.5665e-05]
	Learning Rate: 5.56646e-05
	LOSS [training: 3.7722765248767836 | validation: 4.798455351265615]
	TIME [epoch: 9.72 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7752191727599693		[learning rate: 5.5463e-05]
	Learning Rate: 5.54626e-05
	LOSS [training: 3.7752191727599693 | validation: 4.809757976042932]
	TIME [epoch: 9.72 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779876740453961		[learning rate: 5.5261e-05]
	Learning Rate: 5.52613e-05
	LOSS [training: 3.779876740453961 | validation: 4.793805493498772]
	TIME [epoch: 9.74 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7753299169980843		[learning rate: 5.5061e-05]
	Learning Rate: 5.50608e-05
	LOSS [training: 3.7753299169980843 | validation: 4.8095593593014865]
	TIME [epoch: 9.72 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7768634664972778		[learning rate: 5.4861e-05]
	Learning Rate: 5.48609e-05
	LOSS [training: 3.7768634664972778 | validation: 4.805323672728669]
	TIME [epoch: 9.72 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7768541007424545		[learning rate: 5.4662e-05]
	Learning Rate: 5.46618e-05
	LOSS [training: 3.7768541007424545 | validation: 4.7965209250391805]
	TIME [epoch: 9.72 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7780052971288898		[learning rate: 5.4463e-05]
	Learning Rate: 5.44635e-05
	LOSS [training: 3.7780052971288898 | validation: 4.799759369768832]
	TIME [epoch: 9.72 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776032809740716		[learning rate: 5.4266e-05]
	Learning Rate: 5.42658e-05
	LOSS [training: 3.776032809740716 | validation: 4.796534895248656]
	TIME [epoch: 9.71 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772865842166604		[learning rate: 5.4069e-05]
	Learning Rate: 5.40689e-05
	LOSS [training: 3.772865842166604 | validation: 4.796970445017921]
	TIME [epoch: 9.71 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777403238157068		[learning rate: 5.3873e-05]
	Learning Rate: 5.38727e-05
	LOSS [training: 3.777403238157068 | validation: 4.796724896576918]
	TIME [epoch: 9.73 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777857358983969		[learning rate: 5.3677e-05]
	Learning Rate: 5.36772e-05
	LOSS [training: 3.777857358983969 | validation: 4.804552736890605]
	TIME [epoch: 9.71 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7800469965460435		[learning rate: 5.3482e-05]
	Learning Rate: 5.34824e-05
	LOSS [training: 3.7800469965460435 | validation: 4.803659469960981]
	TIME [epoch: 9.72 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7737531481631237		[learning rate: 5.3288e-05]
	Learning Rate: 5.32883e-05
	LOSS [training: 3.7737531481631237 | validation: 4.791155146404972]
	TIME [epoch: 9.72 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773030074763251		[learning rate: 5.3095e-05]
	Learning Rate: 5.30949e-05
	LOSS [training: 3.773030074763251 | validation: 4.793237034487594]
	TIME [epoch: 9.74 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775671085966299		[learning rate: 5.2902e-05]
	Learning Rate: 5.29022e-05
	LOSS [training: 3.775671085966299 | validation: 4.79643119935687]
	TIME [epoch: 9.72 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7757274395466682		[learning rate: 5.271e-05]
	Learning Rate: 5.27102e-05
	LOSS [training: 3.7757274395466682 | validation: 4.795383726318009]
	TIME [epoch: 9.72 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776786843316843		[learning rate: 5.2519e-05]
	Learning Rate: 5.25189e-05
	LOSS [training: 3.776786843316843 | validation: 4.800363807667526]
	TIME [epoch: 9.71 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7731859557384433		[learning rate: 5.2328e-05]
	Learning Rate: 5.23283e-05
	LOSS [training: 3.7731859557384433 | validation: 4.80378908708751]
	TIME [epoch: 9.73 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7728325089177233		[learning rate: 5.2138e-05]
	Learning Rate: 5.21384e-05
	LOSS [training: 3.7728325089177233 | validation: 4.799233493424711]
	TIME [epoch: 9.72 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7721189028541624		[learning rate: 5.1949e-05]
	Learning Rate: 5.19492e-05
	LOSS [training: 3.7721189028541624 | validation: 4.801216194775701]
	TIME [epoch: 9.72 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773249411802844		[learning rate: 5.1761e-05]
	Learning Rate: 5.17607e-05
	LOSS [training: 3.773249411802844 | validation: 4.796867507690882]
	TIME [epoch: 9.72 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7765640818930692		[learning rate: 5.1573e-05]
	Learning Rate: 5.15729e-05
	LOSS [training: 3.7765640818930692 | validation: 4.799362415791726]
	TIME [epoch: 9.72 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776590507868329		[learning rate: 5.1386e-05]
	Learning Rate: 5.13857e-05
	LOSS [training: 3.776590507868329 | validation: 4.79870203135245]
	TIME [epoch: 9.72 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775101394023418		[learning rate: 5.1199e-05]
	Learning Rate: 5.11992e-05
	LOSS [training: 3.775101394023418 | validation: 4.787654429543364]
	TIME [epoch: 9.71 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7781055540152364		[learning rate: 5.1013e-05]
	Learning Rate: 5.10134e-05
	LOSS [training: 3.7781055540152364 | validation: 4.800906099159744]
	TIME [epoch: 9.73 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7734633839824427		[learning rate: 5.0828e-05]
	Learning Rate: 5.08283e-05
	LOSS [training: 3.7734633839824427 | validation: 4.792138310472867]
	TIME [epoch: 9.71 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776278768767047		[learning rate: 5.0644e-05]
	Learning Rate: 5.06438e-05
	LOSS [training: 3.776278768767047 | validation: 4.798131833090861]
	TIME [epoch: 9.71 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77970620224563		[learning rate: 5.046e-05]
	Learning Rate: 5.046e-05
	LOSS [training: 3.77970620224563 | validation: 4.798954628249304]
	TIME [epoch: 9.71 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775583637437028		[learning rate: 5.0277e-05]
	Learning Rate: 5.02769e-05
	LOSS [training: 3.775583637437028 | validation: 4.785651487927766]
	TIME [epoch: 9.73 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7731000026421766		[learning rate: 5.0094e-05]
	Learning Rate: 5.00944e-05
	LOSS [training: 3.7731000026421766 | validation: 4.809743543294363]
	TIME [epoch: 9.72 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776834691296272		[learning rate: 4.9913e-05]
	Learning Rate: 4.99127e-05
	LOSS [training: 3.776834691296272 | validation: 4.800369148751761]
	TIME [epoch: 9.71 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7754351633394743		[learning rate: 4.9732e-05]
	Learning Rate: 4.97315e-05
	LOSS [training: 3.7754351633394743 | validation: 4.797416450498937]
	TIME [epoch: 9.73 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7753938023083458		[learning rate: 4.9551e-05]
	Learning Rate: 4.9551e-05
	LOSS [training: 3.7753938023083458 | validation: 4.806883922635291]
	TIME [epoch: 9.72 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7791320935862993		[learning rate: 4.9371e-05]
	Learning Rate: 4.93712e-05
	LOSS [training: 3.7791320935862993 | validation: 4.792825867011212]
	TIME [epoch: 9.72 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7768416529455515		[learning rate: 4.9192e-05]
	Learning Rate: 4.9192e-05
	LOSS [training: 3.7768416529455515 | validation: 4.793249717474615]
	TIME [epoch: 9.72 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7756659130311703		[learning rate: 4.9014e-05]
	Learning Rate: 4.90135e-05
	LOSS [training: 3.7756659130311703 | validation: 4.801527667748904]
	TIME [epoch: 9.74 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7765894453216844		[learning rate: 4.8836e-05]
	Learning Rate: 4.88356e-05
	LOSS [training: 3.7765894453216844 | validation: 4.802600317524211]
	TIME [epoch: 9.72 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77520905552962		[learning rate: 4.8658e-05]
	Learning Rate: 4.86584e-05
	LOSS [training: 3.77520905552962 | validation: 4.802424140566301]
	TIME [epoch: 9.72 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777903942774981		[learning rate: 4.8482e-05]
	Learning Rate: 4.84818e-05
	LOSS [training: 3.777903942774981 | validation: 4.782290684298556]
	TIME [epoch: 9.71 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776564649538068		[learning rate: 4.8306e-05]
	Learning Rate: 4.83059e-05
	LOSS [training: 3.776564649538068 | validation: 4.815141548856121]
	TIME [epoch: 9.73 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779686175135148		[learning rate: 4.8131e-05]
	Learning Rate: 4.81306e-05
	LOSS [training: 3.779686175135148 | validation: 4.785881280495782]
	TIME [epoch: 9.73 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7744257395858805		[learning rate: 4.7956e-05]
	Learning Rate: 4.79559e-05
	LOSS [training: 3.7744257395858805 | validation: 4.785904887280669]
	TIME [epoch: 9.71 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7801937765292477		[learning rate: 4.7782e-05]
	Learning Rate: 4.77819e-05
	LOSS [training: 3.7801937765292477 | validation: 4.80071057722609]
	TIME [epoch: 9.71 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7722017316593233		[learning rate: 4.7608e-05]
	Learning Rate: 4.76085e-05
	LOSS [training: 3.7722017316593233 | validation: 4.794773359607309]
	TIME [epoch: 9.73 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779343410724787		[learning rate: 4.7436e-05]
	Learning Rate: 4.74357e-05
	LOSS [training: 3.779343410724787 | validation: 4.7917861265472155]
	TIME [epoch: 9.71 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772587125410314		[learning rate: 4.7264e-05]
	Learning Rate: 4.72636e-05
	LOSS [training: 3.772587125410314 | validation: 4.790482699119433]
	TIME [epoch: 9.7 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7795549605390755		[learning rate: 4.7092e-05]
	Learning Rate: 4.7092e-05
	LOSS [training: 3.7795549605390755 | validation: 4.802886183774568]
	TIME [epoch: 9.72 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77551606343755		[learning rate: 4.6921e-05]
	Learning Rate: 4.69211e-05
	LOSS [training: 3.77551606343755 | validation: 4.804519393003788]
	TIME [epoch: 9.73 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7766600421139267		[learning rate: 4.6751e-05]
	Learning Rate: 4.67508e-05
	LOSS [training: 3.7766600421139267 | validation: 4.80327203096187]
	TIME [epoch: 9.71 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773248543118644		[learning rate: 4.6581e-05]
	Learning Rate: 4.65812e-05
	LOSS [training: 3.773248543118644 | validation: 4.8043182992931195]
	TIME [epoch: 9.7 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7735772256228026		[learning rate: 4.6412e-05]
	Learning Rate: 4.64121e-05
	LOSS [training: 3.7735772256228026 | validation: 4.790867221955023]
	TIME [epoch: 9.72 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779899811380422		[learning rate: 4.6244e-05]
	Learning Rate: 4.62437e-05
	LOSS [training: 3.779899811380422 | validation: 4.7949659034048935]
	TIME [epoch: 9.71 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7760715215971103		[learning rate: 4.6076e-05]
	Learning Rate: 4.60759e-05
	LOSS [training: 3.7760715215971103 | validation: 4.797214563406645]
	TIME [epoch: 9.72 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7774007127165525		[learning rate: 4.5909e-05]
	Learning Rate: 4.59087e-05
	LOSS [training: 3.7774007127165525 | validation: 4.803804538253055]
	TIME [epoch: 9.72 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779070983200973		[learning rate: 4.5742e-05]
	Learning Rate: 4.57421e-05
	LOSS [training: 3.779070983200973 | validation: 4.795961030965008]
	TIME [epoch: 9.72 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7774079701535186		[learning rate: 4.5576e-05]
	Learning Rate: 4.55761e-05
	LOSS [training: 3.7774079701535186 | validation: 4.787464669001294]
	TIME [epoch: 9.71 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7761539508765183		[learning rate: 4.5411e-05]
	Learning Rate: 4.54107e-05
	LOSS [training: 3.7761539508765183 | validation: 4.793416306449708]
	TIME [epoch: 9.69 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7805933820240236		[learning rate: 4.5246e-05]
	Learning Rate: 4.52459e-05
	LOSS [training: 3.7805933820240236 | validation: 4.816854685610362]
	TIME [epoch: 9.72 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7739798933496274		[learning rate: 4.5082e-05]
	Learning Rate: 4.50817e-05
	LOSS [training: 3.7739798933496274 | validation: 4.793152443096143]
	TIME [epoch: 9.71 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7792155554447353		[learning rate: 4.4918e-05]
	Learning Rate: 4.49181e-05
	LOSS [training: 3.7792155554447353 | validation: 4.7916099403672945]
	TIME [epoch: 9.71 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7788956138524825		[learning rate: 4.4755e-05]
	Learning Rate: 4.47551e-05
	LOSS [training: 3.7788956138524825 | validation: 4.793424148465978]
	TIME [epoch: 9.7 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7762672493366822		[learning rate: 4.4593e-05]
	Learning Rate: 4.45926e-05
	LOSS [training: 3.7762672493366822 | validation: 4.800376938839896]
	TIME [epoch: 9.73 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774354659582508		[learning rate: 4.4431e-05]
	Learning Rate: 4.44308e-05
	LOSS [training: 3.774354659582508 | validation: 4.793958126281672]
	TIME [epoch: 9.72 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7771804557064415		[learning rate: 4.427e-05]
	Learning Rate: 4.42696e-05
	LOSS [training: 3.7771804557064415 | validation: 4.800813175583558]
	TIME [epoch: 9.71 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775634598306481		[learning rate: 4.4109e-05]
	Learning Rate: 4.41089e-05
	LOSS [training: 3.775634598306481 | validation: 4.787305373808365]
	TIME [epoch: 9.71 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773170374993547		[learning rate: 4.3949e-05]
	Learning Rate: 4.39489e-05
	LOSS [training: 3.773170374993547 | validation: 4.798441120559003]
	TIME [epoch: 9.74 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777540470900194		[learning rate: 4.3789e-05]
	Learning Rate: 4.37893e-05
	LOSS [training: 3.777540470900194 | validation: 4.801541172320833]
	TIME [epoch: 9.71 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773686129275481		[learning rate: 4.363e-05]
	Learning Rate: 4.36304e-05
	LOSS [training: 3.773686129275481 | validation: 4.798062156825236]
	TIME [epoch: 9.71 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7793030554686724		[learning rate: 4.3472e-05]
	Learning Rate: 4.34721e-05
	LOSS [training: 3.7793030554686724 | validation: 4.801741648972276]
	TIME [epoch: 9.7 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7760167277737127		[learning rate: 4.3314e-05]
	Learning Rate: 4.33143e-05
	LOSS [training: 3.7760167277737127 | validation: 4.79322632077705]
	TIME [epoch: 9.73 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780047489708076		[learning rate: 4.3157e-05]
	Learning Rate: 4.31571e-05
	LOSS [training: 3.780047489708076 | validation: 4.7944698984514496]
	TIME [epoch: 9.69 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776526509063545		[learning rate: 4.3001e-05]
	Learning Rate: 4.30005e-05
	LOSS [training: 3.776526509063545 | validation: 4.813214217126194]
	TIME [epoch: 9.69 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778727069430161		[learning rate: 4.2844e-05]
	Learning Rate: 4.28445e-05
	LOSS [training: 3.778727069430161 | validation: 4.802452337642075]
	TIME [epoch: 9.72 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771872054444136		[learning rate: 4.2689e-05]
	Learning Rate: 4.2689e-05
	LOSS [training: 3.771872054444136 | validation: 4.785958458979977]
	TIME [epoch: 9.71 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7773580156075885		[learning rate: 4.2534e-05]
	Learning Rate: 4.25341e-05
	LOSS [training: 3.7773580156075885 | validation: 4.790693213475307]
	TIME [epoch: 9.71 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7760016774852234		[learning rate: 4.238e-05]
	Learning Rate: 4.23797e-05
	LOSS [training: 3.7760016774852234 | validation: 4.798290570319304]
	TIME [epoch: 9.69 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7710780305630416		[learning rate: 4.2226e-05]
	Learning Rate: 4.22259e-05
	LOSS [training: 3.7710780305630416 | validation: 4.78303701844496]
	TIME [epoch: 9.71 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7768588502613705		[learning rate: 4.2073e-05]
	Learning Rate: 4.20727e-05
	LOSS [training: 3.7768588502613705 | validation: 4.794284729380723]
	TIME [epoch: 9.69 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7784794847427747		[learning rate: 4.192e-05]
	Learning Rate: 4.192e-05
	LOSS [training: 3.7784794847427747 | validation: 4.795773664594804]
	TIME [epoch: 9.71 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774602870477512		[learning rate: 4.1768e-05]
	Learning Rate: 4.17679e-05
	LOSS [training: 3.774602870477512 | validation: 4.801923243713019]
	TIME [epoch: 9.7 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7752820074417643		[learning rate: 4.1616e-05]
	Learning Rate: 4.16163e-05
	LOSS [training: 3.7752820074417643 | validation: 4.798148831753058]
	TIME [epoch: 9.73 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778542481543308		[learning rate: 4.1465e-05]
	Learning Rate: 4.14652e-05
	LOSS [training: 3.778542481543308 | validation: 4.795264987023144]
	TIME [epoch: 9.7 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7735057820726032		[learning rate: 4.1315e-05]
	Learning Rate: 4.13148e-05
	LOSS [training: 3.7735057820726032 | validation: 4.786819998911356]
	TIME [epoch: 9.71 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777681316542741		[learning rate: 4.1165e-05]
	Learning Rate: 4.11648e-05
	LOSS [training: 3.777681316542741 | validation: 4.796194368257867]
	TIME [epoch: 9.7 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773649438094199		[learning rate: 4.1015e-05]
	Learning Rate: 4.10154e-05
	LOSS [training: 3.773649438094199 | validation: 4.790376978765913]
	TIME [epoch: 9.73 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776034331406204		[learning rate: 4.0867e-05]
	Learning Rate: 4.08666e-05
	LOSS [training: 3.776034331406204 | validation: 4.802048877417817]
	TIME [epoch: 9.71 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776862736862173		[learning rate: 4.0718e-05]
	Learning Rate: 4.07183e-05
	LOSS [training: 3.776862736862173 | validation: 4.793995881331902]
	TIME [epoch: 9.71 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7716492416151226		[learning rate: 4.0571e-05]
	Learning Rate: 4.05705e-05
	LOSS [training: 3.7716492416151226 | validation: 4.793411209502579]
	TIME [epoch: 9.72 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7765577284208396		[learning rate: 4.0423e-05]
	Learning Rate: 4.04233e-05
	LOSS [training: 3.7765577284208396 | validation: 4.7946577574270535]
	TIME [epoch: 9.72 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7742144998916145		[learning rate: 4.0277e-05]
	Learning Rate: 4.02766e-05
	LOSS [training: 3.7742144998916145 | validation: 4.786941435371345]
	TIME [epoch: 9.71 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779320118800141		[learning rate: 4.013e-05]
	Learning Rate: 4.01304e-05
	LOSS [training: 3.779320118800141 | validation: 4.787959092446591]
	TIME [epoch: 9.72 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7772413803548814		[learning rate: 3.9985e-05]
	Learning Rate: 3.99848e-05
	LOSS [training: 3.7772413803548814 | validation: 4.781610240829793]
	TIME [epoch: 9.73 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7737826975073956		[learning rate: 3.984e-05]
	Learning Rate: 3.98397e-05
	LOSS [training: 3.7737826975073956 | validation: 4.799642150285364]
	TIME [epoch: 9.7 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776906546774719		[learning rate: 3.9695e-05]
	Learning Rate: 3.96951e-05
	LOSS [training: 3.776906546774719 | validation: 4.801266382765095]
	TIME [epoch: 9.71 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773695900721461		[learning rate: 3.9551e-05]
	Learning Rate: 3.9551e-05
	LOSS [training: 3.773695900721461 | validation: 4.788594619675605]
	TIME [epoch: 9.7 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7755782282253847		[learning rate: 3.9408e-05]
	Learning Rate: 3.94075e-05
	LOSS [training: 3.7755782282253847 | validation: 4.802906709349498]
	TIME [epoch: 9.74 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7753051865139575		[learning rate: 3.9264e-05]
	Learning Rate: 3.92645e-05
	LOSS [training: 3.7753051865139575 | validation: 4.791195411779133]
	TIME [epoch: 9.72 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7780830090536126		[learning rate: 3.9122e-05]
	Learning Rate: 3.9122e-05
	LOSS [training: 3.7780830090536126 | validation: 4.791358098920703]
	TIME [epoch: 9.71 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7752517964909758		[learning rate: 3.898e-05]
	Learning Rate: 3.898e-05
	LOSS [training: 3.7752517964909758 | validation: 4.782807564143119]
	TIME [epoch: 9.71 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7766536015631074		[learning rate: 3.8839e-05]
	Learning Rate: 3.88386e-05
	LOSS [training: 3.7766536015631074 | validation: 4.7936964310234895]
	TIME [epoch: 9.71 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7726533864411436		[learning rate: 3.8698e-05]
	Learning Rate: 3.86976e-05
	LOSS [training: 3.7726533864411436 | validation: 4.794098859855776]
	TIME [epoch: 9.7 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7811094910064207		[learning rate: 3.8557e-05]
	Learning Rate: 3.85572e-05
	LOSS [training: 3.7811094910064207 | validation: 4.7964820999152655]
	TIME [epoch: 9.7 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7739616532306677		[learning rate: 3.8417e-05]
	Learning Rate: 3.84173e-05
	LOSS [training: 3.7739616532306677 | validation: 4.788185683709473]
	TIME [epoch: 9.72 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774800205742209		[learning rate: 3.8278e-05]
	Learning Rate: 3.82778e-05
	LOSS [training: 3.774800205742209 | validation: 4.791395678972193]
	TIME [epoch: 9.71 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77617901182468		[learning rate: 3.8139e-05]
	Learning Rate: 3.81389e-05
	LOSS [training: 3.77617901182468 | validation: 4.799449006226018]
	TIME [epoch: 9.7 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776324027847019		[learning rate: 3.8001e-05]
	Learning Rate: 3.80005e-05
	LOSS [training: 3.776324027847019 | validation: 4.776902993343638]
	TIME [epoch: 9.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r2_20240219_235952/states/model_tr_study206_1633.pth
	Model improved!!!
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7751789520083783		[learning rate: 3.7863e-05]
	Learning Rate: 3.78626e-05
	LOSS [training: 3.7751789520083783 | validation: 4.791212787374014]
	TIME [epoch: 9.73 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7751496384774703		[learning rate: 3.7725e-05]
	Learning Rate: 3.77252e-05
	LOSS [training: 3.7751496384774703 | validation: 4.793433376178359]
	TIME [epoch: 14.6 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7761921871314548		[learning rate: 3.7588e-05]
	Learning Rate: 3.75883e-05
	LOSS [training: 3.7761921871314548 | validation: 4.794573829426254]
	TIME [epoch: 9.71 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7774979493095167		[learning rate: 3.7452e-05]
	Learning Rate: 3.74519e-05
	LOSS [training: 3.7774979493095167 | validation: 4.799812053216944]
	TIME [epoch: 9.72 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7797457343814047		[learning rate: 3.7316e-05]
	Learning Rate: 3.7316e-05
	LOSS [training: 3.7797457343814047 | validation: 4.789113349194323]
	TIME [epoch: 9.72 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7751840270640153		[learning rate: 3.7181e-05]
	Learning Rate: 3.71805e-05
	LOSS [training: 3.7751840270640153 | validation: 4.804743840570731]
	TIME [epoch: 9.71 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778208247426965		[learning rate: 3.7046e-05]
	Learning Rate: 3.70456e-05
	LOSS [training: 3.778208247426965 | validation: 4.789469808609595]
	TIME [epoch: 9.71 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7722271567882117		[learning rate: 3.6911e-05]
	Learning Rate: 3.69112e-05
	LOSS [training: 3.7722271567882117 | validation: 4.799323434291177]
	TIME [epoch: 9.72 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7769282393742216		[learning rate: 3.6777e-05]
	Learning Rate: 3.67772e-05
	LOSS [training: 3.7769282393742216 | validation: 4.797686900864353]
	TIME [epoch: 9.71 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775346675799297		[learning rate: 3.6644e-05]
	Learning Rate: 3.66438e-05
	LOSS [training: 3.775346675799297 | validation: 4.802490284808585]
	TIME [epoch: 9.71 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773671895020029		[learning rate: 3.6511e-05]
	Learning Rate: 3.65108e-05
	LOSS [training: 3.773671895020029 | validation: 4.787410727904539]
	TIME [epoch: 9.7 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774146480181691		[learning rate: 3.6378e-05]
	Learning Rate: 3.63783e-05
	LOSS [training: 3.774146480181691 | validation: 4.797243190133458]
	TIME [epoch: 9.72 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773779776558618		[learning rate: 3.6246e-05]
	Learning Rate: 3.62463e-05
	LOSS [training: 3.773779776558618 | validation: 4.796320286931459]
	TIME [epoch: 9.71 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7783815752978227		[learning rate: 3.6115e-05]
	Learning Rate: 3.61147e-05
	LOSS [training: 3.7783815752978227 | validation: 4.804532838916793]
	TIME [epoch: 9.69 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771012186400965		[learning rate: 3.5984e-05]
	Learning Rate: 3.59837e-05
	LOSS [training: 3.771012186400965 | validation: 4.800560237757548]
	TIME [epoch: 9.71 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77484487923295		[learning rate: 3.5853e-05]
	Learning Rate: 3.58531e-05
	LOSS [training: 3.77484487923295 | validation: 4.795464555561941]
	TIME [epoch: 9.72 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778493566440063		[learning rate: 3.5723e-05]
	Learning Rate: 3.5723e-05
	LOSS [training: 3.778493566440063 | validation: 4.79304045051635]
	TIME [epoch: 9.71 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779103929420413		[learning rate: 3.5593e-05]
	Learning Rate: 3.55933e-05
	LOSS [training: 3.779103929420413 | validation: 4.798160334488324]
	TIME [epoch: 9.71 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771880489420377		[learning rate: 3.5464e-05]
	Learning Rate: 3.54641e-05
	LOSS [training: 3.771880489420377 | validation: 4.802791500770262]
	TIME [epoch: 9.72 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7751929402423245		[learning rate: 3.5335e-05]
	Learning Rate: 3.53354e-05
	LOSS [training: 3.7751929402423245 | validation: 4.804670090838014]
	TIME [epoch: 9.73 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7796997983156864		[learning rate: 3.5207e-05]
	Learning Rate: 3.52072e-05
	LOSS [training: 3.7796997983156864 | validation: 4.798068339887853]
	TIME [epoch: 9.71 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775225865721757		[learning rate: 3.5079e-05]
	Learning Rate: 3.50794e-05
	LOSS [training: 3.775225865721757 | validation: 4.799571908079842]
	TIME [epoch: 9.69 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7744060615334534		[learning rate: 3.4952e-05]
	Learning Rate: 3.49521e-05
	LOSS [training: 3.7744060615334534 | validation: 4.794949551989306]
	TIME [epoch: 9.74 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7717516577712202		[learning rate: 3.4825e-05]
	Learning Rate: 3.48253e-05
	LOSS [training: 3.7717516577712202 | validation: 4.802265313072409]
	TIME [epoch: 9.7 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774713613924341		[learning rate: 3.4699e-05]
	Learning Rate: 3.46989e-05
	LOSS [training: 3.774713613924341 | validation: 4.789840044087929]
	TIME [epoch: 9.71 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774366554466108		[learning rate: 3.4573e-05]
	Learning Rate: 3.4573e-05
	LOSS [training: 3.774366554466108 | validation: 4.802618140625163]
	TIME [epoch: 9.71 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7741772363635535		[learning rate: 3.4448e-05]
	Learning Rate: 3.44475e-05
	LOSS [training: 3.7741772363635535 | validation: 4.79211093993432]
	TIME [epoch: 9.74 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7727429112699182		[learning rate: 3.4323e-05]
	Learning Rate: 3.43225e-05
	LOSS [training: 3.7727429112699182 | validation: 4.7964613409731465]
	TIME [epoch: 9.71 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7721585999158713		[learning rate: 3.4198e-05]
	Learning Rate: 3.4198e-05
	LOSS [training: 3.7721585999158713 | validation: 4.791052647994818]
	TIME [epoch: 9.71 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7801319420538766		[learning rate: 3.4074e-05]
	Learning Rate: 3.40738e-05
	LOSS [training: 3.7801319420538766 | validation: 4.790641391540756]
	TIME [epoch: 9.72 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771788928885089		[learning rate: 3.395e-05]
	Learning Rate: 3.39502e-05
	LOSS [training: 3.771788928885089 | validation: 4.801036284131753]
	TIME [epoch: 9.71 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7711743063613397		[learning rate: 3.3827e-05]
	Learning Rate: 3.3827e-05
	LOSS [training: 3.7711743063613397 | validation: 4.793941093534483]
	TIME [epoch: 9.7 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776047502152312		[learning rate: 3.3704e-05]
	Learning Rate: 3.37042e-05
	LOSS [training: 3.776047502152312 | validation: 4.7991113759889235]
	TIME [epoch: 9.7 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7753591610790282		[learning rate: 3.3582e-05]
	Learning Rate: 3.35819e-05
	LOSS [training: 3.7753591610790282 | validation: 4.7970975778151015]
	TIME [epoch: 9.72 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780984751337647		[learning rate: 3.346e-05]
	Learning Rate: 3.346e-05
	LOSS [training: 3.780984751337647 | validation: 4.80007482052254]
	TIME [epoch: 9.71 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7758492108295223		[learning rate: 3.3339e-05]
	Learning Rate: 3.33386e-05
	LOSS [training: 3.7758492108295223 | validation: 4.786669260441293]
	TIME [epoch: 9.71 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776579325334228		[learning rate: 3.3218e-05]
	Learning Rate: 3.32176e-05
	LOSS [training: 3.776579325334228 | validation: 4.793877509188233]
	TIME [epoch: 9.7 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776639341997803		[learning rate: 3.3097e-05]
	Learning Rate: 3.30971e-05
	LOSS [training: 3.776639341997803 | validation: 4.80485066916791]
	TIME [epoch: 9.74 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773123780047638		[learning rate: 3.2977e-05]
	Learning Rate: 3.2977e-05
	LOSS [training: 3.773123780047638 | validation: 4.795332479020248]
	TIME [epoch: 9.72 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7743302313114335		[learning rate: 3.2857e-05]
	Learning Rate: 3.28573e-05
	LOSS [training: 3.7743302313114335 | validation: 4.794670225206477]
	TIME [epoch: 9.71 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7779324979793074		[learning rate: 3.2738e-05]
	Learning Rate: 3.2738e-05
	LOSS [training: 3.7779324979793074 | validation: 4.784542597803216]
	TIME [epoch: 9.71 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7788586339925345		[learning rate: 3.2619e-05]
	Learning Rate: 3.26192e-05
	LOSS [training: 3.7788586339925345 | validation: 4.804801984275379]
	TIME [epoch: 9.73 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779520132113538		[learning rate: 3.2501e-05]
	Learning Rate: 3.25009e-05
	LOSS [training: 3.779520132113538 | validation: 4.789450334749351]
	TIME [epoch: 9.71 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772695929032006		[learning rate: 3.2383e-05]
	Learning Rate: 3.23829e-05
	LOSS [training: 3.772695929032006 | validation: 4.795678110712762]
	TIME [epoch: 9.71 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7739802196637173		[learning rate: 3.2265e-05]
	Learning Rate: 3.22654e-05
	LOSS [training: 3.7739802196637173 | validation: 4.793502920357837]
	TIME [epoch: 9.71 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780741121211741		[learning rate: 3.2148e-05]
	Learning Rate: 3.21483e-05
	LOSS [training: 3.780741121211741 | validation: 4.795877201382596]
	TIME [epoch: 9.72 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77872982189986		[learning rate: 3.2032e-05]
	Learning Rate: 3.20316e-05
	LOSS [training: 3.77872982189986 | validation: 4.788588329248291]
	TIME [epoch: 9.73 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7740119736410405		[learning rate: 3.1915e-05]
	Learning Rate: 3.19154e-05
	LOSS [training: 3.7740119736410405 | validation: 4.792910169326652]
	TIME [epoch: 9.71 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771143689145352		[learning rate: 3.18e-05]
	Learning Rate: 3.17996e-05
	LOSS [training: 3.771143689145352 | validation: 4.790915016569353]
	TIME [epoch: 9.73 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776159030520476		[learning rate: 3.1684e-05]
	Learning Rate: 3.16842e-05
	LOSS [training: 3.776159030520476 | validation: 4.7901311478552895]
	TIME [epoch: 9.7 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775354647448526		[learning rate: 3.1569e-05]
	Learning Rate: 3.15692e-05
	LOSS [training: 3.775354647448526 | validation: 4.798260111316647]
	TIME [epoch: 9.69 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772107744776315		[learning rate: 3.1455e-05]
	Learning Rate: 3.14546e-05
	LOSS [training: 3.772107744776315 | validation: 4.793763953176395]
	TIME [epoch: 9.71 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7754771859986405		[learning rate: 3.134e-05]
	Learning Rate: 3.13405e-05
	LOSS [training: 3.7754771859986405 | validation: 4.8067569748306145]
	TIME [epoch: 9.71 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7773620136320867		[learning rate: 3.1227e-05]
	Learning Rate: 3.12267e-05
	LOSS [training: 3.7773620136320867 | validation: 4.8066619581072265]
	TIME [epoch: 9.71 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776743287422792		[learning rate: 3.1113e-05]
	Learning Rate: 3.11134e-05
	LOSS [training: 3.776743287422792 | validation: 4.798379427972557]
	TIME [epoch: 9.7 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7747678217922926		[learning rate: 3.1e-05]
	Learning Rate: 3.10005e-05
	LOSS [training: 3.7747678217922926 | validation: 4.793613025403572]
	TIME [epoch: 9.72 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773143595341402		[learning rate: 3.0888e-05]
	Learning Rate: 3.0888e-05
	LOSS [training: 3.773143595341402 | validation: 4.804654666016971]
	TIME [epoch: 9.72 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7772876680084955		[learning rate: 3.0776e-05]
	Learning Rate: 3.07759e-05
	LOSS [training: 3.7772876680084955 | validation: 4.7883913849232504]
	TIME [epoch: 9.7 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7772278601918665		[learning rate: 3.0664e-05]
	Learning Rate: 3.06642e-05
	LOSS [training: 3.7772278601918665 | validation: 4.794256168896552]
	TIME [epoch: 9.7 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771300594009759		[learning rate: 3.0553e-05]
	Learning Rate: 3.05529e-05
	LOSS [training: 3.771300594009759 | validation: 4.790901739865133]
	TIME [epoch: 9.73 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.769190872476746		[learning rate: 3.0442e-05]
	Learning Rate: 3.0442e-05
	LOSS [training: 3.769190872476746 | validation: 4.790607143501102]
	TIME [epoch: 9.71 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7738381188251777		[learning rate: 3.0332e-05]
	Learning Rate: 3.03316e-05
	LOSS [training: 3.7738381188251777 | validation: 4.793616499131415]
	TIME [epoch: 9.71 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77641668490442		[learning rate: 3.0221e-05]
	Learning Rate: 3.02215e-05
	LOSS [training: 3.77641668490442 | validation: 4.788654651916427]
	TIME [epoch: 9.71 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7706863376711985		[learning rate: 3.0112e-05]
	Learning Rate: 3.01118e-05
	LOSS [training: 3.7706863376711985 | validation: 4.796319181717753]
	TIME [epoch: 9.73 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7727065937168867		[learning rate: 3.0003e-05]
	Learning Rate: 3.00025e-05
	LOSS [training: 3.7727065937168867 | validation: 4.792056316121774]
	TIME [epoch: 9.7 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7788759402630077		[learning rate: 2.9894e-05]
	Learning Rate: 2.98936e-05
	LOSS [training: 3.7788759402630077 | validation: 4.786553354382297]
	TIME [epoch: 9.72 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7723044289586922		[learning rate: 2.9785e-05]
	Learning Rate: 2.97852e-05
	LOSS [training: 3.7723044289586922 | validation: 4.803743444439915]
	TIME [epoch: 9.71 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777996548958157		[learning rate: 2.9677e-05]
	Learning Rate: 2.96771e-05
	LOSS [training: 3.777996548958157 | validation: 4.798230292031211]
	TIME [epoch: 9.74 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772972199544003		[learning rate: 2.9569e-05]
	Learning Rate: 2.95694e-05
	LOSS [training: 3.772972199544003 | validation: 4.788851092988854]
	TIME [epoch: 9.72 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7734837078642807		[learning rate: 2.9462e-05]
	Learning Rate: 2.94621e-05
	LOSS [training: 3.7734837078642807 | validation: 4.798346777720622]
	TIME [epoch: 9.71 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773675010078315		[learning rate: 2.9355e-05]
	Learning Rate: 2.93551e-05
	LOSS [training: 3.773675010078315 | validation: 4.7993889127222955]
	TIME [epoch: 9.72 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77543731263095		[learning rate: 2.9249e-05]
	Learning Rate: 2.92486e-05
	LOSS [training: 3.77543731263095 | validation: 4.800352444588229]
	TIME [epoch: 9.72 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7792046642926054		[learning rate: 2.9142e-05]
	Learning Rate: 2.91425e-05
	LOSS [training: 3.7792046642926054 | validation: 4.794313043115955]
	TIME [epoch: 9.71 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7720169858834383		[learning rate: 2.9037e-05]
	Learning Rate: 2.90367e-05
	LOSS [training: 3.7720169858834383 | validation: 4.795550408891717]
	TIME [epoch: 9.71 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7740432586094315		[learning rate: 2.8931e-05]
	Learning Rate: 2.89313e-05
	LOSS [training: 3.7740432586094315 | validation: 4.785874445201167]
	TIME [epoch: 9.73 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7732740802328832		[learning rate: 2.8826e-05]
	Learning Rate: 2.88263e-05
	LOSS [training: 3.7732740802328832 | validation: 4.794914602798482]
	TIME [epoch: 9.72 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7743006605560843		[learning rate: 2.8722e-05]
	Learning Rate: 2.87217e-05
	LOSS [training: 3.7743006605560843 | validation: 4.799213015132304]
	TIME [epoch: 9.71 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781551187755079		[learning rate: 2.8617e-05]
	Learning Rate: 2.86175e-05
	LOSS [training: 3.781551187755079 | validation: 4.789037580524435]
	TIME [epoch: 9.71 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7783900593917616		[learning rate: 2.8514e-05]
	Learning Rate: 2.85136e-05
	LOSS [training: 3.7783900593917616 | validation: 4.794977657078694]
	TIME [epoch: 9.73 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7833977298704466		[learning rate: 2.841e-05]
	Learning Rate: 2.84102e-05
	LOSS [training: 3.7833977298704466 | validation: 4.800772158295486]
	TIME [epoch: 9.71 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779002253117157		[learning rate: 2.8307e-05]
	Learning Rate: 2.83071e-05
	LOSS [training: 3.779002253117157 | validation: 4.802142636290768]
	TIME [epoch: 9.7 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7760327268816773		[learning rate: 2.8204e-05]
	Learning Rate: 2.82043e-05
	LOSS [training: 3.7760327268816773 | validation: 4.795253308878022]
	TIME [epoch: 9.71 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7757775349624523		[learning rate: 2.8102e-05]
	Learning Rate: 2.8102e-05
	LOSS [training: 3.7757775349624523 | validation: 4.801369688505496]
	TIME [epoch: 9.71 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777170293587986		[learning rate: 2.8e-05]
	Learning Rate: 2.8e-05
	LOSS [training: 3.777170293587986 | validation: 4.810753312119944]
	TIME [epoch: 9.69 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7773851342529263		[learning rate: 2.7898e-05]
	Learning Rate: 2.78984e-05
	LOSS [training: 3.7773851342529263 | validation: 4.7979800080336155]
	TIME [epoch: 9.7 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7735347883584716		[learning rate: 2.7797e-05]
	Learning Rate: 2.77971e-05
	LOSS [training: 3.7735347883584716 | validation: 4.801768675418382]
	TIME [epoch: 9.72 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776757589723504		[learning rate: 2.7696e-05]
	Learning Rate: 2.76963e-05
	LOSS [training: 3.776757589723504 | validation: 4.795549766272296]
	TIME [epoch: 9.71 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7758801081089395		[learning rate: 2.7596e-05]
	Learning Rate: 2.75957e-05
	LOSS [training: 3.7758801081089395 | validation: 4.792208234573737]
	TIME [epoch: 9.71 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7730538102966755		[learning rate: 2.7496e-05]
	Learning Rate: 2.74956e-05
	LOSS [training: 3.7730538102966755 | validation: 4.799782571470547]
	TIME [epoch: 9.71 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773741859295554		[learning rate: 2.7396e-05]
	Learning Rate: 2.73958e-05
	LOSS [training: 3.773741859295554 | validation: 4.80109957384985]
	TIME [epoch: 9.73 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772291613361773		[learning rate: 2.7296e-05]
	Learning Rate: 2.72964e-05
	LOSS [training: 3.772291613361773 | validation: 4.785226318784994]
	TIME [epoch: 9.72 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7791260646253377		[learning rate: 2.7197e-05]
	Learning Rate: 2.71973e-05
	LOSS [training: 3.7791260646253377 | validation: 4.798484945552442]
	TIME [epoch: 9.71 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7740231811747846		[learning rate: 2.7099e-05]
	Learning Rate: 2.70986e-05
	LOSS [training: 3.7740231811747846 | validation: 4.792043009430841]
	TIME [epoch: 9.71 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775030967418373		[learning rate: 2.7e-05]
	Learning Rate: 2.70003e-05
	LOSS [training: 3.775030967418373 | validation: 4.792697847429339]
	TIME [epoch: 9.74 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7730059568095995		[learning rate: 2.6902e-05]
	Learning Rate: 2.69023e-05
	LOSS [training: 3.7730059568095995 | validation: 4.80537147429793]
	TIME [epoch: 9.71 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7746044276853263		[learning rate: 2.6805e-05]
	Learning Rate: 2.68047e-05
	LOSS [training: 3.7746044276853263 | validation: 4.801313374511236]
	TIME [epoch: 9.72 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777083833694512		[learning rate: 2.6707e-05]
	Learning Rate: 2.67074e-05
	LOSS [training: 3.777083833694512 | validation: 4.799088192516314]
	TIME [epoch: 9.71 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7720888343680565		[learning rate: 2.661e-05]
	Learning Rate: 2.66105e-05
	LOSS [training: 3.7720888343680565 | validation: 4.805472723885165]
	TIME [epoch: 9.72 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781188137955197		[learning rate: 2.6514e-05]
	Learning Rate: 2.65139e-05
	LOSS [training: 3.781188137955197 | validation: 4.803851613232997]
	TIME [epoch: 9.71 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7757033479338795		[learning rate: 2.6418e-05]
	Learning Rate: 2.64177e-05
	LOSS [training: 3.7757033479338795 | validation: 4.789121840501592]
	TIME [epoch: 9.71 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7765629705209016		[learning rate: 2.6322e-05]
	Learning Rate: 2.63218e-05
	LOSS [training: 3.7765629705209016 | validation: 4.802223071029156]
	TIME [epoch: 9.73 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7739945012545824		[learning rate: 2.6226e-05]
	Learning Rate: 2.62263e-05
	LOSS [training: 3.7739945012545824 | validation: 4.807693118630694]
	TIME [epoch: 9.71 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775461531647892		[learning rate: 2.6131e-05]
	Learning Rate: 2.61311e-05
	LOSS [training: 3.775461531647892 | validation: 4.79769157644722]
	TIME [epoch: 9.71 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7755305600712554		[learning rate: 2.6036e-05]
	Learning Rate: 2.60363e-05
	LOSS [training: 3.7755305600712554 | validation: 4.797976273232677]
	TIME [epoch: 9.71 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7757711480312297		[learning rate: 2.5942e-05]
	Learning Rate: 2.59418e-05
	LOSS [training: 3.7757711480312297 | validation: 4.793514769683524]
	TIME [epoch: 9.73 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7734074360802303		[learning rate: 2.5848e-05]
	Learning Rate: 2.58477e-05
	LOSS [training: 3.7734074360802303 | validation: 4.803382764991474]
	TIME [epoch: 9.71 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7749864551912635		[learning rate: 2.5754e-05]
	Learning Rate: 2.57539e-05
	LOSS [training: 3.7749864551912635 | validation: 4.7932660004302985]
	TIME [epoch: 9.72 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7807156975347977		[learning rate: 2.566e-05]
	Learning Rate: 2.56604e-05
	LOSS [training: 3.7807156975347977 | validation: 4.794856001758323]
	TIME [epoch: 9.72 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778038511780366		[learning rate: 2.5567e-05]
	Learning Rate: 2.55673e-05
	LOSS [training: 3.778038511780366 | validation: 4.808927805752181]
	TIME [epoch: 9.73 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7765186958191164		[learning rate: 2.5474e-05]
	Learning Rate: 2.54745e-05
	LOSS [training: 3.7765186958191164 | validation: 4.80766495607336]
	TIME [epoch: 9.71 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779066121206337		[learning rate: 2.5382e-05]
	Learning Rate: 2.5382e-05
	LOSS [training: 3.779066121206337 | validation: 4.812265008287104]
	TIME [epoch: 9.71 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7789596122448197		[learning rate: 2.529e-05]
	Learning Rate: 2.52899e-05
	LOSS [training: 3.7789596122448197 | validation: 4.798453818249644]
	TIME [epoch: 9.73 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775121977925546		[learning rate: 2.5198e-05]
	Learning Rate: 2.51981e-05
	LOSS [training: 3.775121977925546 | validation: 4.806565045507737]
	TIME [epoch: 9.72 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7719408546967017		[learning rate: 2.5107e-05]
	Learning Rate: 2.51067e-05
	LOSS [training: 3.7719408546967017 | validation: 4.795440906850457]
	TIME [epoch: 9.71 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779723334772876		[learning rate: 2.5016e-05]
	Learning Rate: 2.50156e-05
	LOSS [training: 3.779723334772876 | validation: 4.802727379445076]
	TIME [epoch: 9.71 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7732630016847195		[learning rate: 2.4925e-05]
	Learning Rate: 2.49248e-05
	LOSS [training: 3.7732630016847195 | validation: 4.795549363538915]
	TIME [epoch: 9.73 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7672849562682784		[learning rate: 2.4834e-05]
	Learning Rate: 2.48343e-05
	LOSS [training: 3.7672849562682784 | validation: 4.798259475428531]
	TIME [epoch: 9.72 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7770189958788434		[learning rate: 2.4744e-05]
	Learning Rate: 2.47442e-05
	LOSS [training: 3.7770189958788434 | validation: 4.784530822938745]
	TIME [epoch: 9.71 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774837295176429		[learning rate: 2.4654e-05]
	Learning Rate: 2.46544e-05
	LOSS [training: 3.774837295176429 | validation: 4.791417441096691]
	TIME [epoch: 9.71 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771805713066988		[learning rate: 2.4565e-05]
	Learning Rate: 2.45649e-05
	LOSS [training: 3.771805713066988 | validation: 4.80667752889777]
	TIME [epoch: 9.73 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7782974062800863		[learning rate: 2.4476e-05]
	Learning Rate: 2.44758e-05
	LOSS [training: 3.7782974062800863 | validation: 4.794459514291872]
	TIME [epoch: 9.72 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7775703011338337		[learning rate: 2.4387e-05]
	Learning Rate: 2.4387e-05
	LOSS [training: 3.7775703011338337 | validation: 4.801492677788943]
	TIME [epoch: 9.7 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772999457180043		[learning rate: 2.4298e-05]
	Learning Rate: 2.42985e-05
	LOSS [training: 3.772999457180043 | validation: 4.803665137159133]
	TIME [epoch: 9.72 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7743131316859566		[learning rate: 2.421e-05]
	Learning Rate: 2.42103e-05
	LOSS [training: 3.7743131316859566 | validation: 4.812515741179863]
	TIME [epoch: 9.72 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775159769178681		[learning rate: 2.4122e-05]
	Learning Rate: 2.41224e-05
	LOSS [training: 3.775159769178681 | validation: 4.793747088764186]
	TIME [epoch: 9.71 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775157533370887		[learning rate: 2.4035e-05]
	Learning Rate: 2.40349e-05
	LOSS [training: 3.775157533370887 | validation: 4.78602458176785]
	TIME [epoch: 9.71 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776392016066313		[learning rate: 2.3948e-05]
	Learning Rate: 2.39477e-05
	LOSS [training: 3.776392016066313 | validation: 4.797019793569819]
	TIME [epoch: 9.72 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7767253611660387		[learning rate: 2.3861e-05]
	Learning Rate: 2.38608e-05
	LOSS [training: 3.7767253611660387 | validation: 4.795771550174152]
	TIME [epoch: 9.71 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7744187065816073		[learning rate: 2.3774e-05]
	Learning Rate: 2.37742e-05
	LOSS [training: 3.7744187065816073 | validation: 4.794231966968542]
	TIME [epoch: 9.72 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7719245163937076		[learning rate: 2.3688e-05]
	Learning Rate: 2.36879e-05
	LOSS [training: 3.7719245163937076 | validation: 4.794103771293676]
	TIME [epoch: 9.71 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7749513017004888		[learning rate: 2.3602e-05]
	Learning Rate: 2.36019e-05
	LOSS [training: 3.7749513017004888 | validation: 4.7948980465045]
	TIME [epoch: 9.73 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7744636831922556		[learning rate: 2.3516e-05]
	Learning Rate: 2.35163e-05
	LOSS [training: 3.7744636831922556 | validation: 4.799385835807454]
	TIME [epoch: 9.71 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772372137501242		[learning rate: 2.3431e-05]
	Learning Rate: 2.34309e-05
	LOSS [training: 3.772372137501242 | validation: 4.7974243411247075]
	TIME [epoch: 9.71 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777677295249751		[learning rate: 2.3346e-05]
	Learning Rate: 2.33459e-05
	LOSS [training: 3.777677295249751 | validation: 4.79338084273953]
	TIME [epoch: 9.71 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780131581275767		[learning rate: 2.3261e-05]
	Learning Rate: 2.32612e-05
	LOSS [training: 3.780131581275767 | validation: 4.786283380803812]
	TIME [epoch: 9.72 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7747978800967004		[learning rate: 2.3177e-05]
	Learning Rate: 2.31768e-05
	LOSS [training: 3.7747978800967004 | validation: 4.800241419469997]
	TIME [epoch: 9.71 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774701589115181		[learning rate: 2.3093e-05]
	Learning Rate: 2.30926e-05
	LOSS [training: 3.774701589115181 | validation: 4.793318866901873]
	TIME [epoch: 9.7 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777413043610854		[learning rate: 2.3009e-05]
	Learning Rate: 2.30088e-05
	LOSS [training: 3.777413043610854 | validation: 4.795470853635535]
	TIME [epoch: 9.73 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771083960418225		[learning rate: 2.2925e-05]
	Learning Rate: 2.29253e-05
	LOSS [training: 3.771083960418225 | validation: 4.802224010585574]
	TIME [epoch: 9.71 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775271646811633		[learning rate: 2.2842e-05]
	Learning Rate: 2.28421e-05
	LOSS [training: 3.775271646811633 | validation: 4.796880809570246]
	TIME [epoch: 9.7 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7776614362142906		[learning rate: 2.2759e-05]
	Learning Rate: 2.27592e-05
	LOSS [training: 3.7776614362142906 | validation: 4.784526886769251]
	TIME [epoch: 9.71 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7697490070675963		[learning rate: 2.2677e-05]
	Learning Rate: 2.26767e-05
	LOSS [training: 3.7697490070675963 | validation: 4.792799664722401]
	TIME [epoch: 9.74 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7736951036074586		[learning rate: 2.2594e-05]
	Learning Rate: 2.25944e-05
	LOSS [training: 3.7736951036074586 | validation: 4.79505488208953]
	TIME [epoch: 9.71 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7780522630409363		[learning rate: 2.2512e-05]
	Learning Rate: 2.25124e-05
	LOSS [training: 3.7780522630409363 | validation: 4.798468731718488]
	TIME [epoch: 9.71 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7731638256735875		[learning rate: 2.2431e-05]
	Learning Rate: 2.24307e-05
	LOSS [training: 3.7731638256735875 | validation: 4.792592481528862]
	TIME [epoch: 9.71 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777334155831169		[learning rate: 2.2349e-05]
	Learning Rate: 2.23493e-05
	LOSS [training: 3.777334155831169 | validation: 4.797249185974716]
	TIME [epoch: 9.74 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77543451980524		[learning rate: 2.2268e-05]
	Learning Rate: 2.22682e-05
	LOSS [training: 3.77543451980524 | validation: 4.797268048725095]
	TIME [epoch: 9.7 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774964313851266		[learning rate: 2.2187e-05]
	Learning Rate: 2.21873e-05
	LOSS [training: 3.774964313851266 | validation: 4.797454405440135]
	TIME [epoch: 9.7 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776727543798497		[learning rate: 2.2107e-05]
	Learning Rate: 2.21068e-05
	LOSS [training: 3.776727543798497 | validation: 4.793128664987471]
	TIME [epoch: 9.72 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7715991849470085		[learning rate: 2.2027e-05]
	Learning Rate: 2.20266e-05
	LOSS [training: 3.7715991849470085 | validation: 4.794213840832076]
	TIME [epoch: 9.72 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7739202088904626		[learning rate: 2.1947e-05]
	Learning Rate: 2.19467e-05
	LOSS [training: 3.7739202088904626 | validation: 4.797681363180206]
	TIME [epoch: 9.71 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7766178075278036		[learning rate: 2.1867e-05]
	Learning Rate: 2.1867e-05
	LOSS [training: 3.7766178075278036 | validation: 4.796512067019544]
	TIME [epoch: 9.71 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7721357200021624		[learning rate: 2.1788e-05]
	Learning Rate: 2.17877e-05
	LOSS [training: 3.7721357200021624 | validation: 4.80011947590831]
	TIME [epoch: 9.74 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775954582670567		[learning rate: 2.1709e-05]
	Learning Rate: 2.17086e-05
	LOSS [training: 3.775954582670567 | validation: 4.8019449275382176]
	TIME [epoch: 9.71 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7747414944976034		[learning rate: 2.163e-05]
	Learning Rate: 2.16298e-05
	LOSS [training: 3.7747414944976034 | validation: 4.794261691676585]
	TIME [epoch: 9.7 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7749432512992867		[learning rate: 2.1551e-05]
	Learning Rate: 2.15513e-05
	LOSS [training: 3.7749432512992867 | validation: 4.790762571258868]
	TIME [epoch: 9.71 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773376670291886		[learning rate: 2.1473e-05]
	Learning Rate: 2.14731e-05
	LOSS [training: 3.773376670291886 | validation: 4.793709809270165]
	TIME [epoch: 9.73 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.769869193278685		[learning rate: 2.1395e-05]
	Learning Rate: 2.13952e-05
	LOSS [training: 3.769869193278685 | validation: 4.7875453020066665]
	TIME [epoch: 9.72 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776234038060717		[learning rate: 2.1318e-05]
	Learning Rate: 2.13175e-05
	LOSS [training: 3.776234038060717 | validation: 4.799069118298649]
	TIME [epoch: 9.71 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774079817116352		[learning rate: 2.124e-05]
	Learning Rate: 2.12402e-05
	LOSS [training: 3.774079817116352 | validation: 4.7995760514473655]
	TIME [epoch: 9.72 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7744266824505943		[learning rate: 2.1163e-05]
	Learning Rate: 2.11631e-05
	LOSS [training: 3.7744266824505943 | validation: 4.788286853092083]
	TIME [epoch: 9.72 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7727458467974215		[learning rate: 2.1086e-05]
	Learning Rate: 2.10863e-05
	LOSS [training: 3.7727458467974215 | validation: 4.806259488804064]
	TIME [epoch: 9.7 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771477453918987		[learning rate: 2.101e-05]
	Learning Rate: 2.10098e-05
	LOSS [training: 3.771477453918987 | validation: 4.785327696885885]
	TIME [epoch: 9.7 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7734855981463005		[learning rate: 2.0934e-05]
	Learning Rate: 2.09335e-05
	LOSS [training: 3.7734855981463005 | validation: 4.795411551035213]
	TIME [epoch: 9.72 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7740350153940545		[learning rate: 2.0858e-05]
	Learning Rate: 2.08575e-05
	LOSS [training: 3.7740350153940545 | validation: 4.79966542888029]
	TIME [epoch: 9.71 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777850283187644		[learning rate: 2.0782e-05]
	Learning Rate: 2.07819e-05
	LOSS [training: 3.777850283187644 | validation: 4.787716767701397]
	TIME [epoch: 9.71 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7791602120504932		[learning rate: 2.0706e-05]
	Learning Rate: 2.07064e-05
	LOSS [training: 3.7791602120504932 | validation: 4.801062410746868]
	TIME [epoch: 9.7 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7784787625205345		[learning rate: 2.0631e-05]
	Learning Rate: 2.06313e-05
	LOSS [training: 3.7784787625205345 | validation: 4.789382491195406]
	TIME [epoch: 9.74 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7727135271807706		[learning rate: 2.0556e-05]
	Learning Rate: 2.05564e-05
	LOSS [training: 3.7727135271807706 | validation: 4.794091203539323]
	TIME [epoch: 9.72 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771027515192196		[learning rate: 2.0482e-05]
	Learning Rate: 2.04818e-05
	LOSS [training: 3.771027515192196 | validation: 4.798406953485302]
	TIME [epoch: 9.72 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7766351998814307		[learning rate: 2.0407e-05]
	Learning Rate: 2.04075e-05
	LOSS [training: 3.7766351998814307 | validation: 4.7927483752860525]
	TIME [epoch: 9.71 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776442937771422		[learning rate: 2.0333e-05]
	Learning Rate: 2.03334e-05
	LOSS [training: 3.776442937771422 | validation: 4.79824237268911]
	TIME [epoch: 9.73 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775670135488516		[learning rate: 2.026e-05]
	Learning Rate: 2.02596e-05
	LOSS [training: 3.775670135488516 | validation: 4.795949621312221]
	TIME [epoch: 9.71 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7728079463270605		[learning rate: 2.0186e-05]
	Learning Rate: 2.01861e-05
	LOSS [training: 3.7728079463270605 | validation: 4.803142654852976]
	TIME [epoch: 9.72 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7701807359457575		[learning rate: 2.0113e-05]
	Learning Rate: 2.01129e-05
	LOSS [training: 3.7701807359457575 | validation: 4.7952873287953945]
	TIME [epoch: 9.72 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772636930877939		[learning rate: 2.004e-05]
	Learning Rate: 2.00399e-05
	LOSS [training: 3.772636930877939 | validation: 4.7881152138049625]
	TIME [epoch: 9.72 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7775561240534627		[learning rate: 1.9967e-05]
	Learning Rate: 1.99671e-05
	LOSS [training: 3.7775561240534627 | validation: 4.791947741771925]
	TIME [epoch: 9.71 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7733121532175993		[learning rate: 1.9895e-05]
	Learning Rate: 1.98947e-05
	LOSS [training: 3.7733121532175993 | validation: 4.7933751626806]
	TIME [epoch: 9.71 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776008176418922		[learning rate: 1.9822e-05]
	Learning Rate: 1.98225e-05
	LOSS [training: 3.776008176418922 | validation: 4.796699476184288]
	TIME [epoch: 9.73 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7787220432500597		[learning rate: 1.9751e-05]
	Learning Rate: 1.97505e-05
	LOSS [training: 3.7787220432500597 | validation: 4.790940060855333]
	TIME [epoch: 9.71 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772356756771102		[learning rate: 1.9679e-05]
	Learning Rate: 1.96789e-05
	LOSS [training: 3.772356756771102 | validation: 4.798443074018861]
	TIME [epoch: 9.71 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7704315296425603		[learning rate: 1.9607e-05]
	Learning Rate: 1.96074e-05
	LOSS [training: 3.7704315296425603 | validation: 4.789479155276837]
	TIME [epoch: 9.71 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7750437735618703		[learning rate: 1.9536e-05]
	Learning Rate: 1.95363e-05
	LOSS [training: 3.7750437735618703 | validation: 4.796212673403306]
	TIME [epoch: 9.72 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774751810769878		[learning rate: 1.9465e-05]
	Learning Rate: 1.94654e-05
	LOSS [training: 3.774751810769878 | validation: 4.8095385429808415]
	TIME [epoch: 9.71 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772902637629074		[learning rate: 1.9395e-05]
	Learning Rate: 1.93948e-05
	LOSS [training: 3.772902637629074 | validation: 4.800906514966907]
	TIME [epoch: 9.71 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7760762773225296		[learning rate: 1.9324e-05]
	Learning Rate: 1.93244e-05
	LOSS [training: 3.7760762773225296 | validation: 4.797246538342296]
	TIME [epoch: 9.72 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7739355017584457		[learning rate: 1.9254e-05]
	Learning Rate: 1.92542e-05
	LOSS [training: 3.7739355017584457 | validation: 4.795733447377257]
	TIME [epoch: 9.72 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7759765737630424		[learning rate: 1.9184e-05]
	Learning Rate: 1.91844e-05
	LOSS [training: 3.7759765737630424 | validation: 4.7957300316686995]
	TIME [epoch: 9.71 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7805919080242547		[learning rate: 1.9115e-05]
	Learning Rate: 1.91147e-05
	LOSS [training: 3.7805919080242547 | validation: 4.794567439552355]
	TIME [epoch: 9.71 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7754853290146593		[learning rate: 1.9045e-05]
	Learning Rate: 1.90454e-05
	LOSS [training: 3.7754853290146593 | validation: 4.796525381565898]
	TIME [epoch: 9.73 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771603284859535		[learning rate: 1.8976e-05]
	Learning Rate: 1.89763e-05
	LOSS [training: 3.771603284859535 | validation: 4.79128445259404]
	TIME [epoch: 9.71 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775607731835474		[learning rate: 1.8907e-05]
	Learning Rate: 1.89074e-05
	LOSS [training: 3.775607731835474 | validation: 4.804373057595363]
	TIME [epoch: 9.72 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774589680377737		[learning rate: 1.8839e-05]
	Learning Rate: 1.88388e-05
	LOSS [training: 3.774589680377737 | validation: 4.793319752432829]
	TIME [epoch: 9.72 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7721942441721716		[learning rate: 1.877e-05]
	Learning Rate: 1.87704e-05
	LOSS [training: 3.7721942441721716 | validation: 4.803005311753059]
	TIME [epoch: 9.72 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774798560335035		[learning rate: 1.8702e-05]
	Learning Rate: 1.87023e-05
	LOSS [training: 3.774798560335035 | validation: 4.795457060583094]
	TIME [epoch: 9.72 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7790724060976117		[learning rate: 1.8634e-05]
	Learning Rate: 1.86344e-05
	LOSS [training: 3.7790724060976117 | validation: 4.785168304803108]
	TIME [epoch: 9.7 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7769478140746364		[learning rate: 1.8567e-05]
	Learning Rate: 1.85668e-05
	LOSS [training: 3.7769478140746364 | validation: 4.7946118022114375]
	TIME [epoch: 9.7 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7748100119746235		[learning rate: 1.8499e-05]
	Learning Rate: 1.84994e-05
	LOSS [training: 3.7748100119746235 | validation: 4.787856611620848]
	TIME [epoch: 9.71 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7685876213148903		[learning rate: 1.8432e-05]
	Learning Rate: 1.84323e-05
	LOSS [training: 3.7685876213148903 | validation: 4.802088500653624]
	TIME [epoch: 9.7 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773639118997868		[learning rate: 1.8365e-05]
	Learning Rate: 1.83654e-05
	LOSS [training: 3.773639118997868 | validation: 4.805834309815579]
	TIME [epoch: 9.69 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7728215497273525		[learning rate: 1.8299e-05]
	Learning Rate: 1.82987e-05
	LOSS [training: 3.7728215497273525 | validation: 4.799001585896859]
	TIME [epoch: 9.72 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779264943407848		[learning rate: 1.8232e-05]
	Learning Rate: 1.82323e-05
	LOSS [training: 3.779264943407848 | validation: 4.799006022940617]
	TIME [epoch: 9.7 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776343615168957		[learning rate: 1.8166e-05]
	Learning Rate: 1.81662e-05
	LOSS [training: 3.776343615168957 | validation: 4.797440394781986]
	TIME [epoch: 9.71 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7777542781034286		[learning rate: 1.81e-05]
	Learning Rate: 1.81002e-05
	LOSS [training: 3.7777542781034286 | validation: 4.80008229659466]
	TIME [epoch: 9.72 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773284047010874		[learning rate: 1.8035e-05]
	Learning Rate: 1.80346e-05
	LOSS [training: 3.773284047010874 | validation: 4.787221575174378]
	TIME [epoch: 9.72 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7727254230338056		[learning rate: 1.7969e-05]
	Learning Rate: 1.79691e-05
	LOSS [training: 3.7727254230338056 | validation: 4.801329801022865]
	TIME [epoch: 9.7 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777408480461488		[learning rate: 1.7904e-05]
	Learning Rate: 1.79039e-05
	LOSS [training: 3.777408480461488 | validation: 4.791813432062515]
	TIME [epoch: 9.71 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7770312837382463		[learning rate: 1.7839e-05]
	Learning Rate: 1.78389e-05
	LOSS [training: 3.7770312837382463 | validation: 4.796219520206002]
	TIME [epoch: 9.71 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7754957401034006		[learning rate: 1.7774e-05]
	Learning Rate: 1.77742e-05
	LOSS [training: 3.7754957401034006 | validation: 4.800555353588905]
	TIME [epoch: 9.73 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775863313498384		[learning rate: 1.771e-05]
	Learning Rate: 1.77097e-05
	LOSS [training: 3.775863313498384 | validation: 4.7963456518316905]
	TIME [epoch: 9.7 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7738102950419483		[learning rate: 1.7645e-05]
	Learning Rate: 1.76454e-05
	LOSS [training: 3.7738102950419483 | validation: 4.800660325861281]
	TIME [epoch: 9.7 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7737425833582265		[learning rate: 1.7581e-05]
	Learning Rate: 1.75814e-05
	LOSS [training: 3.7737425833582265 | validation: 4.806186196094923]
	TIME [epoch: 9.71 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771448435368261		[learning rate: 1.7518e-05]
	Learning Rate: 1.75176e-05
	LOSS [training: 3.771448435368261 | validation: 4.798282224995403]
	TIME [epoch: 9.71 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7742270056523473		[learning rate: 1.7454e-05]
	Learning Rate: 1.7454e-05
	LOSS [training: 3.7742270056523473 | validation: 4.797510098853997]
	TIME [epoch: 9.7 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7761645233602295		[learning rate: 1.7391e-05]
	Learning Rate: 1.73907e-05
	LOSS [training: 3.7761645233602295 | validation: 4.7994816360116275]
	TIME [epoch: 9.71 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.769945669454986		[learning rate: 1.7328e-05]
	Learning Rate: 1.73275e-05
	LOSS [training: 3.769945669454986 | validation: 4.79100073251463]
	TIME [epoch: 9.73 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7787569122303895		[learning rate: 1.7265e-05]
	Learning Rate: 1.72647e-05
	LOSS [training: 3.7787569122303895 | validation: 4.798522944210309]
	TIME [epoch: 9.71 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773470613463735		[learning rate: 1.7202e-05]
	Learning Rate: 1.7202e-05
	LOSS [training: 3.773470613463735 | validation: 4.793149697992243]
	TIME [epoch: 9.71 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7781420051442938		[learning rate: 1.714e-05]
	Learning Rate: 1.71396e-05
	LOSS [training: 3.7781420051442938 | validation: 4.802756640508003]
	TIME [epoch: 9.7 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7741397460511776		[learning rate: 1.7077e-05]
	Learning Rate: 1.70774e-05
	LOSS [training: 3.7741397460511776 | validation: 4.796522489363714]
	TIME [epoch: 9.71 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774896828785903		[learning rate: 1.7015e-05]
	Learning Rate: 1.70154e-05
	LOSS [training: 3.774896828785903 | validation: 4.7997731570737345]
	TIME [epoch: 9.71 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778087775410191		[learning rate: 1.6954e-05]
	Learning Rate: 1.69537e-05
	LOSS [training: 3.778087775410191 | validation: 4.7991456614833705]
	TIME [epoch: 9.7 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77340863989752		[learning rate: 1.6892e-05]
	Learning Rate: 1.68921e-05
	LOSS [training: 3.77340863989752 | validation: 4.805883602093567]
	TIME [epoch: 9.71 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7770627497939566		[learning rate: 1.6831e-05]
	Learning Rate: 1.68308e-05
	LOSS [training: 3.7770627497939566 | validation: 4.79234112448471]
	TIME [epoch: 9.73 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774785265433853		[learning rate: 1.677e-05]
	Learning Rate: 1.67697e-05
	LOSS [training: 3.774785265433853 | validation: 4.797764019184574]
	TIME [epoch: 9.69 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7738023908979317		[learning rate: 1.6709e-05]
	Learning Rate: 1.67089e-05
	LOSS [training: 3.7738023908979317 | validation: 4.800769638953654]
	TIME [epoch: 9.7 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771934779476125		[learning rate: 1.6648e-05]
	Learning Rate: 1.66482e-05
	LOSS [training: 3.771934779476125 | validation: 4.785674617235242]
	TIME [epoch: 9.71 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773416401675567		[learning rate: 1.6588e-05]
	Learning Rate: 1.65878e-05
	LOSS [training: 3.773416401675567 | validation: 4.789127911701237]
	TIME [epoch: 9.7 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776463746998713		[learning rate: 1.6528e-05]
	Learning Rate: 1.65276e-05
	LOSS [training: 3.776463746998713 | validation: 4.782422149528306]
	TIME [epoch: 9.7 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7718195500123146		[learning rate: 1.6468e-05]
	Learning Rate: 1.64677e-05
	LOSS [training: 3.7718195500123146 | validation: 4.790736852316331]
	TIME [epoch: 9.69 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772947682084053		[learning rate: 1.6408e-05]
	Learning Rate: 1.64079e-05
	LOSS [training: 3.772947682084053 | validation: 4.784041334693559]
	TIME [epoch: 9.71 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77511688367795		[learning rate: 1.6348e-05]
	Learning Rate: 1.63483e-05
	LOSS [training: 3.77511688367795 | validation: 4.791158084920584]
	TIME [epoch: 9.71 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77595060603526		[learning rate: 1.6289e-05]
	Learning Rate: 1.6289e-05
	LOSS [training: 3.77595060603526 | validation: 4.786188968260159]
	TIME [epoch: 9.69 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7731833299919844		[learning rate: 1.623e-05]
	Learning Rate: 1.62299e-05
	LOSS [training: 3.7731833299919844 | validation: 4.784763095518194]
	TIME [epoch: 9.7 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7749762110140352		[learning rate: 1.6171e-05]
	Learning Rate: 1.6171e-05
	LOSS [training: 3.7749762110140352 | validation: 4.784693970091372]
	TIME [epoch: 9.72 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7700009607752625		[learning rate: 1.6112e-05]
	Learning Rate: 1.61123e-05
	LOSS [training: 3.7700009607752625 | validation: 4.798870870583442]
	TIME [epoch: 9.69 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7720206034061894		[learning rate: 1.6054e-05]
	Learning Rate: 1.60538e-05
	LOSS [training: 3.7720206034061894 | validation: 4.786749394550493]
	TIME [epoch: 9.69 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771859890166907		[learning rate: 1.5996e-05]
	Learning Rate: 1.59956e-05
	LOSS [training: 3.771859890166907 | validation: 4.796671502351611]
	TIME [epoch: 9.7 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7816016306670006		[learning rate: 1.5938e-05]
	Learning Rate: 1.59375e-05
	LOSS [training: 3.7816016306670006 | validation: 4.7982995594149545]
	TIME [epoch: 9.71 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7724430245893763		[learning rate: 1.588e-05]
	Learning Rate: 1.58797e-05
	LOSS [training: 3.7724430245893763 | validation: 4.797555483987941]
	TIME [epoch: 9.7 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776736268045916		[learning rate: 1.5822e-05]
	Learning Rate: 1.58221e-05
	LOSS [training: 3.776736268045916 | validation: 4.793647717520368]
	TIME [epoch: 9.7 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771812343021326		[learning rate: 1.5765e-05]
	Learning Rate: 1.57647e-05
	LOSS [training: 3.771812343021326 | validation: 4.798160019165728]
	TIME [epoch: 9.73 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777420678062022		[learning rate: 1.5707e-05]
	Learning Rate: 1.57074e-05
	LOSS [training: 3.777420678062022 | validation: 4.797003927012706]
	TIME [epoch: 9.7 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780037100891648		[learning rate: 1.565e-05]
	Learning Rate: 1.56504e-05
	LOSS [training: 3.780037100891648 | validation: 4.795856136438157]
	TIME [epoch: 9.69 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776046168026658		[learning rate: 1.5594e-05]
	Learning Rate: 1.55936e-05
	LOSS [training: 3.776046168026658 | validation: 4.79383907507757]
	TIME [epoch: 9.69 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7737925573407383		[learning rate: 1.5537e-05]
	Learning Rate: 1.5537e-05
	LOSS [training: 3.7737925573407383 | validation: 4.79438599144569]
	TIME [epoch: 9.71 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.770495586505935		[learning rate: 1.5481e-05]
	Learning Rate: 1.54807e-05
	LOSS [training: 3.770495586505935 | validation: 4.78708719159173]
	TIME [epoch: 9.71 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774898562892118		[learning rate: 1.5424e-05]
	Learning Rate: 1.54245e-05
	LOSS [training: 3.774898562892118 | validation: 4.79284684999445]
	TIME [epoch: 9.68 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773638347781163		[learning rate: 1.5369e-05]
	Learning Rate: 1.53685e-05
	LOSS [training: 3.773638347781163 | validation: 4.795537651127754]
	TIME [epoch: 9.69 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781746313498457		[learning rate: 1.5313e-05]
	Learning Rate: 1.53127e-05
	LOSS [training: 3.781746313498457 | validation: 4.7926994196410515]
	TIME [epoch: 9.71 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775729631379403		[learning rate: 1.5257e-05]
	Learning Rate: 1.52572e-05
	LOSS [training: 3.775729631379403 | validation: 4.785932221229201]
	TIME [epoch: 9.69 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7701551228697534		[learning rate: 1.5202e-05]
	Learning Rate: 1.52018e-05
	LOSS [training: 3.7701551228697534 | validation: 4.80082822367041]
	TIME [epoch: 9.7 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775684315299414		[learning rate: 1.5147e-05]
	Learning Rate: 1.51466e-05
	LOSS [training: 3.775684315299414 | validation: 4.7917039519967926]
	TIME [epoch: 9.71 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7759509622161134		[learning rate: 1.5092e-05]
	Learning Rate: 1.50917e-05
	LOSS [training: 3.7759509622161134 | validation: 4.803420637667766]
	TIME [epoch: 9.71 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7774839143307863		[learning rate: 1.5037e-05]
	Learning Rate: 1.50369e-05
	LOSS [training: 3.7774839143307863 | validation: 4.791764874165503]
	TIME [epoch: 9.72 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778222212552996		[learning rate: 1.4982e-05]
	Learning Rate: 1.49823e-05
	LOSS [training: 3.778222212552996 | validation: 4.787726123097092]
	TIME [epoch: 9.73 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7721335867215586		[learning rate: 1.4928e-05]
	Learning Rate: 1.49279e-05
	LOSS [training: 3.7721335867215586 | validation: 4.794187045735768]
	TIME [epoch: 9.74 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775016331171775		[learning rate: 1.4874e-05]
	Learning Rate: 1.48738e-05
	LOSS [training: 3.775016331171775 | validation: 4.791969082777817]
	TIME [epoch: 9.72 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7781946951450522		[learning rate: 1.482e-05]
	Learning Rate: 1.48198e-05
	LOSS [training: 3.7781946951450522 | validation: 4.794268301219023]
	TIME [epoch: 9.71 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773822989131838		[learning rate: 1.4766e-05]
	Learning Rate: 1.4766e-05
	LOSS [training: 3.773822989131838 | validation: 4.809717808773922]
	TIME [epoch: 9.75 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7732337085840206		[learning rate: 1.4712e-05]
	Learning Rate: 1.47124e-05
	LOSS [training: 3.7732337085840206 | validation: 4.802949584626181]
	TIME [epoch: 9.73 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7710199764439145		[learning rate: 1.4659e-05]
	Learning Rate: 1.4659e-05
	LOSS [training: 3.7710199764439145 | validation: 4.7917502545067014]
	TIME [epoch: 9.71 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7713791695969996		[learning rate: 1.4606e-05]
	Learning Rate: 1.46058e-05
	LOSS [training: 3.7713791695969996 | validation: 4.801004913378876]
	TIME [epoch: 9.72 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779171914392651		[learning rate: 1.4553e-05]
	Learning Rate: 1.45528e-05
	LOSS [training: 3.779171914392651 | validation: 4.797886366702454]
	TIME [epoch: 9.73 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7761042386257473		[learning rate: 1.45e-05]
	Learning Rate: 1.45e-05
	LOSS [training: 3.7761042386257473 | validation: 4.795951407630445]
	TIME [epoch: 9.72 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7749317034503873		[learning rate: 1.4447e-05]
	Learning Rate: 1.44474e-05
	LOSS [training: 3.7749317034503873 | validation: 4.781002028700972]
	TIME [epoch: 9.72 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7748279052607487		[learning rate: 1.4395e-05]
	Learning Rate: 1.4395e-05
	LOSS [training: 3.7748279052607487 | validation: 4.796349774964031]
	TIME [epoch: 9.7 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7741367100751027		[learning rate: 1.4343e-05]
	Learning Rate: 1.43427e-05
	LOSS [training: 3.7741367100751027 | validation: 4.794585155385363]
	TIME [epoch: 9.72 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7787718024448167		[learning rate: 1.4291e-05]
	Learning Rate: 1.42907e-05
	LOSS [training: 3.7787718024448167 | validation: 4.797704706511978]
	TIME [epoch: 9.71 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7738722773684166		[learning rate: 1.4239e-05]
	Learning Rate: 1.42388e-05
	LOSS [training: 3.7738722773684166 | validation: 4.787902174759011]
	TIME [epoch: 9.71 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777694188401359		[learning rate: 1.4187e-05]
	Learning Rate: 1.41871e-05
	LOSS [training: 3.777694188401359 | validation: 4.80056870732722]
	TIME [epoch: 9.7 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775059714804745		[learning rate: 1.4136e-05]
	Learning Rate: 1.41357e-05
	LOSS [training: 3.775059714804745 | validation: 4.798259324042162]
	TIME [epoch: 9.72 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774875933896348		[learning rate: 1.4084e-05]
	Learning Rate: 1.40844e-05
	LOSS [training: 3.774875933896348 | validation: 4.794475291567555]
	TIME [epoch: 9.71 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771148978600247		[learning rate: 1.4033e-05]
	Learning Rate: 1.40332e-05
	LOSS [training: 3.771148978600247 | validation: 4.785766737129316]
	TIME [epoch: 9.71 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773127669746797		[learning rate: 1.3982e-05]
	Learning Rate: 1.39823e-05
	LOSS [training: 3.773127669746797 | validation: 4.787141232502321]
	TIME [epoch: 9.72 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7758397134295074		[learning rate: 1.3932e-05]
	Learning Rate: 1.39316e-05
	LOSS [training: 3.7758397134295074 | validation: 4.786230094387698]
	TIME [epoch: 9.74 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776672752545943		[learning rate: 1.3881e-05]
	Learning Rate: 1.3881e-05
	LOSS [training: 3.776672752545943 | validation: 4.790008401786516]
	TIME [epoch: 9.72 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7735183854876064		[learning rate: 1.3831e-05]
	Learning Rate: 1.38306e-05
	LOSS [training: 3.7735183854876064 | validation: 4.798880552403772]
	TIME [epoch: 9.71 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776985493122101		[learning rate: 1.378e-05]
	Learning Rate: 1.37804e-05
	LOSS [training: 3.776985493122101 | validation: 4.803792920198609]
	TIME [epoch: 9.7 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7753294235360513		[learning rate: 1.373e-05]
	Learning Rate: 1.37304e-05
	LOSS [training: 3.7753294235360513 | validation: 4.7837879776363215]
	TIME [epoch: 9.71 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772550708879014		[learning rate: 1.3681e-05]
	Learning Rate: 1.36806e-05
	LOSS [training: 3.772550708879014 | validation: 4.7990403014155865]
	TIME [epoch: 9.73 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7706120348464127		[learning rate: 1.3631e-05]
	Learning Rate: 1.3631e-05
	LOSS [training: 3.7706120348464127 | validation: 4.797253161949569]
	TIME [epoch: 9.73 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775395991849565		[learning rate: 1.3581e-05]
	Learning Rate: 1.35815e-05
	LOSS [training: 3.775395991849565 | validation: 4.795066986837725]
	TIME [epoch: 9.74 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7724317925807993		[learning rate: 1.3532e-05]
	Learning Rate: 1.35322e-05
	LOSS [training: 3.7724317925807993 | validation: 4.785936889454545]
	TIME [epoch: 9.72 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777098690818606		[learning rate: 1.3483e-05]
	Learning Rate: 1.34831e-05
	LOSS [training: 3.777098690818606 | validation: 4.78931193838875]
	TIME [epoch: 9.72 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772798857359939		[learning rate: 1.3434e-05]
	Learning Rate: 1.34342e-05
	LOSS [training: 3.772798857359939 | validation: 4.798067714475915]
	TIME [epoch: 9.71 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7741511662780303		[learning rate: 1.3385e-05]
	Learning Rate: 1.33854e-05
	LOSS [training: 3.7741511662780303 | validation: 4.799822157567681]
	TIME [epoch: 9.73 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781636822751834		[learning rate: 1.3337e-05]
	Learning Rate: 1.33368e-05
	LOSS [training: 3.781636822751834 | validation: 4.799342166784114]
	TIME [epoch: 9.72 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7736511145471106		[learning rate: 1.3288e-05]
	Learning Rate: 1.32884e-05
	LOSS [training: 3.7736511145471106 | validation: 4.797519680700157]
	TIME [epoch: 9.71 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7740665144237147		[learning rate: 1.324e-05]
	Learning Rate: 1.32402e-05
	LOSS [training: 3.7740665144237147 | validation: 4.797515668854219]
	TIME [epoch: 9.72 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775372406563084		[learning rate: 1.3192e-05]
	Learning Rate: 1.31922e-05
	LOSS [training: 3.775372406563084 | validation: 4.7986573578376595]
	TIME [epoch: 9.72 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780043726130882		[learning rate: 1.3144e-05]
	Learning Rate: 1.31443e-05
	LOSS [training: 3.780043726130882 | validation: 4.807729762378239]
	TIME [epoch: 9.71 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7739880086839683		[learning rate: 1.3097e-05]
	Learning Rate: 1.30966e-05
	LOSS [training: 3.7739880086839683 | validation: 4.7997690092084255]
	TIME [epoch: 9.71 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776377835047201		[learning rate: 1.3049e-05]
	Learning Rate: 1.30491e-05
	LOSS [training: 3.776377835047201 | validation: 4.793994036932301]
	TIME [epoch: 9.72 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775620757051528		[learning rate: 1.3002e-05]
	Learning Rate: 1.30017e-05
	LOSS [training: 3.775620757051528 | validation: 4.797235132920449]
	TIME [epoch: 9.72 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7763451805760084		[learning rate: 1.2955e-05]
	Learning Rate: 1.29545e-05
	LOSS [training: 3.7763451805760084 | validation: 4.807506923593888]
	TIME [epoch: 9.72 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7780106772869324		[learning rate: 1.2907e-05]
	Learning Rate: 1.29075e-05
	LOSS [training: 3.7780106772869324 | validation: 4.798333768632396]
	TIME [epoch: 9.72 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7765722778873894		[learning rate: 1.2861e-05]
	Learning Rate: 1.28607e-05
	LOSS [training: 3.7765722778873894 | validation: 4.800020891375032]
	TIME [epoch: 9.74 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773564692356389		[learning rate: 1.2814e-05]
	Learning Rate: 1.2814e-05
	LOSS [training: 3.773564692356389 | validation: 4.796753049640488]
	TIME [epoch: 9.71 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7763521610537127		[learning rate: 1.2767e-05]
	Learning Rate: 1.27675e-05
	LOSS [training: 3.7763521610537127 | validation: 4.794265043353916]
	TIME [epoch: 9.69 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776357846981157		[learning rate: 1.2721e-05]
	Learning Rate: 1.27212e-05
	LOSS [training: 3.776357846981157 | validation: 4.796290543952475]
	TIME [epoch: 9.71 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772389966806286		[learning rate: 1.2675e-05]
	Learning Rate: 1.2675e-05
	LOSS [training: 3.772389966806286 | validation: 4.818565374399718]
	TIME [epoch: 9.73 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775421483510688		[learning rate: 1.2629e-05]
	Learning Rate: 1.2629e-05
	LOSS [training: 3.775421483510688 | validation: 4.807650528537351]
	TIME [epoch: 9.71 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7721831617908563		[learning rate: 1.2583e-05]
	Learning Rate: 1.25832e-05
	LOSS [training: 3.7721831617908563 | validation: 4.799535920315241]
	TIME [epoch: 9.71 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7731501771972598		[learning rate: 1.2537e-05]
	Learning Rate: 1.25375e-05
	LOSS [training: 3.7731501771972598 | validation: 4.7968451835357975]
	TIME [epoch: 9.72 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7746541534908538		[learning rate: 1.2492e-05]
	Learning Rate: 1.2492e-05
	LOSS [training: 3.7746541534908538 | validation: 4.7880419195297605]
	TIME [epoch: 9.71 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7781543638254313		[learning rate: 1.2447e-05]
	Learning Rate: 1.24467e-05
	LOSS [training: 3.7781543638254313 | validation: 4.802087540535021]
	TIME [epoch: 9.71 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7776038028421333		[learning rate: 1.2401e-05]
	Learning Rate: 1.24015e-05
	LOSS [training: 3.7776038028421333 | validation: 4.797850541717341]
	TIME [epoch: 9.72 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7757075229629358		[learning rate: 1.2356e-05]
	Learning Rate: 1.23565e-05
	LOSS [training: 3.7757075229629358 | validation: 4.797578775059386]
	TIME [epoch: 9.72 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7810697565663176		[learning rate: 1.2312e-05]
	Learning Rate: 1.23116e-05
	LOSS [training: 3.7810697565663176 | validation: 4.807643744874676]
	TIME [epoch: 9.71 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7718863945951355		[learning rate: 1.2267e-05]
	Learning Rate: 1.2267e-05
	LOSS [training: 3.7718863945951355 | validation: 4.792717283772858]
	TIME [epoch: 9.71 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7755186221620143		[learning rate: 1.2222e-05]
	Learning Rate: 1.22224e-05
	LOSS [training: 3.7755186221620143 | validation: 4.788597984857222]
	TIME [epoch: 9.71 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7769668537132035		[learning rate: 1.2178e-05]
	Learning Rate: 1.21781e-05
	LOSS [training: 3.7769668537132035 | validation: 4.794800463423379]
	TIME [epoch: 9.71 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7731903821074413		[learning rate: 1.2134e-05]
	Learning Rate: 1.21339e-05
	LOSS [training: 3.7731903821074413 | validation: 4.79102336801148]
	TIME [epoch: 9.7 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780237165315351		[learning rate: 1.209e-05]
	Learning Rate: 1.20899e-05
	LOSS [training: 3.780237165315351 | validation: 4.799095853183216]
	TIME [epoch: 9.69 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7752167467232773		[learning rate: 1.2046e-05]
	Learning Rate: 1.2046e-05
	LOSS [training: 3.7752167467232773 | validation: 4.806977847101641]
	TIME [epoch: 9.7 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7768411713464096		[learning rate: 1.2002e-05]
	Learning Rate: 1.20023e-05
	LOSS [training: 3.7768411713464096 | validation: 4.796880140392502]
	TIME [epoch: 9.69 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7722894304839145		[learning rate: 1.1959e-05]
	Learning Rate: 1.19587e-05
	LOSS [training: 3.7722894304839145 | validation: 4.8021620982959154]
	TIME [epoch: 9.68 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7735868510844277		[learning rate: 1.1915e-05]
	Learning Rate: 1.19153e-05
	LOSS [training: 3.7735868510844277 | validation: 4.80316403836742]
	TIME [epoch: 9.68 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7706141690479114		[learning rate: 1.1872e-05]
	Learning Rate: 1.18721e-05
	LOSS [training: 3.7706141690479114 | validation: 4.790750303218661]
	TIME [epoch: 9.72 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7755106134218392		[learning rate: 1.1829e-05]
	Learning Rate: 1.1829e-05
	LOSS [training: 3.7755106134218392 | validation: 4.79312295112684]
	TIME [epoch: 9.7 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7740407817712467		[learning rate: 1.1786e-05]
	Learning Rate: 1.17861e-05
	LOSS [training: 3.7740407817712467 | validation: 4.792154352121473]
	TIME [epoch: 9.71 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77630351230995		[learning rate: 1.1743e-05]
	Learning Rate: 1.17433e-05
	LOSS [training: 3.77630351230995 | validation: 4.797929588984594]
	TIME [epoch: 9.71 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775027867754426		[learning rate: 1.1701e-05]
	Learning Rate: 1.17007e-05
	LOSS [training: 3.775027867754426 | validation: 4.789780155462007]
	TIME [epoch: 9.74 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7725743784357917		[learning rate: 1.1658e-05]
	Learning Rate: 1.16582e-05
	LOSS [training: 3.7725743784357917 | validation: 4.794294324583539]
	TIME [epoch: 9.71 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7789475223607703		[learning rate: 1.1616e-05]
	Learning Rate: 1.16159e-05
	LOSS [training: 3.7789475223607703 | validation: 4.794895893159153]
	TIME [epoch: 9.72 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7746694128359715		[learning rate: 1.1574e-05]
	Learning Rate: 1.15737e-05
	LOSS [training: 3.7746694128359715 | validation: 4.796012969786182]
	TIME [epoch: 9.7 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7697959417227396		[learning rate: 1.1532e-05]
	Learning Rate: 1.15317e-05
	LOSS [training: 3.7697959417227396 | validation: 4.793186980529263]
	TIME [epoch: 9.74 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771497363159078		[learning rate: 1.149e-05]
	Learning Rate: 1.14899e-05
	LOSS [training: 3.771497363159078 | validation: 4.790800577785552]
	TIME [epoch: 9.72 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771673548607599		[learning rate: 1.1448e-05]
	Learning Rate: 1.14482e-05
	LOSS [training: 3.771673548607599 | validation: 4.806678037393226]
	TIME [epoch: 9.7 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7774138276704123		[learning rate: 1.1407e-05]
	Learning Rate: 1.14066e-05
	LOSS [training: 3.7774138276704123 | validation: 4.799102297059502]
	TIME [epoch: 9.71 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7730992944882864		[learning rate: 1.1365e-05]
	Learning Rate: 1.13652e-05
	LOSS [training: 3.7730992944882864 | validation: 4.792326680926193]
	TIME [epoch: 9.7 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7754138455138864		[learning rate: 1.1324e-05]
	Learning Rate: 1.1324e-05
	LOSS [training: 3.7754138455138864 | validation: 4.788095042311695]
	TIME [epoch: 9.71 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.776537998506941		[learning rate: 1.1283e-05]
	Learning Rate: 1.12829e-05
	LOSS [training: 3.776537998506941 | validation: 4.799404742268782]
	TIME [epoch: 9.68 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77772246626398		[learning rate: 1.1242e-05]
	Learning Rate: 1.1242e-05
	LOSS [training: 3.77772246626398 | validation: 4.794573676193478]
	TIME [epoch: 9.73 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7729147133745777		[learning rate: 1.1201e-05]
	Learning Rate: 1.12012e-05
	LOSS [training: 3.7729147133745777 | validation: 4.796564670245555]
	TIME [epoch: 9.7 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774085159745387		[learning rate: 1.1161e-05]
	Learning Rate: 1.11605e-05
	LOSS [training: 3.774085159745387 | validation: 4.800527693565748]
	TIME [epoch: 9.7 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7769637999396877		[learning rate: 1.112e-05]
	Learning Rate: 1.112e-05
	LOSS [training: 3.7769637999396877 | validation: 4.795235609196675]
	TIME [epoch: 9.71 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7734386286494557		[learning rate: 1.108e-05]
	Learning Rate: 1.10797e-05
	LOSS [training: 3.7734386286494557 | validation: 4.792244146501405]
	TIME [epoch: 9.73 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77410277867069		[learning rate: 1.1039e-05]
	Learning Rate: 1.10394e-05
	LOSS [training: 3.77410277867069 | validation: 4.79699973986205]
	TIME [epoch: 9.71 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7755286168662385		[learning rate: 1.0999e-05]
	Learning Rate: 1.09994e-05
	LOSS [training: 3.7755286168662385 | validation: 4.80088728597336]
	TIME [epoch: 9.7 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77585677663562		[learning rate: 1.0959e-05]
	Learning Rate: 1.09595e-05
	LOSS [training: 3.77585677663562 | validation: 4.796567149828498]
	TIME [epoch: 9.71 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7709115089845637		[learning rate: 1.092e-05]
	Learning Rate: 1.09197e-05
	LOSS [training: 3.7709115089845637 | validation: 4.798053950724796]
	TIME [epoch: 9.71 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7693707418100004		[learning rate: 1.088e-05]
	Learning Rate: 1.08801e-05
	LOSS [training: 3.7693707418100004 | validation: 4.795545383650151]
	TIME [epoch: 9.71 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7693530552918837		[learning rate: 1.0841e-05]
	Learning Rate: 1.08406e-05
	LOSS [training: 3.7693530552918837 | validation: 4.791160461725226]
	TIME [epoch: 9.71 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771329432223016		[learning rate: 1.0801e-05]
	Learning Rate: 1.08012e-05
	LOSS [training: 3.771329432223016 | validation: 4.792970920903712]
	TIME [epoch: 9.72 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7746127242984286		[learning rate: 1.0762e-05]
	Learning Rate: 1.0762e-05
	LOSS [training: 3.7746127242984286 | validation: 4.795917666597462]
	TIME [epoch: 9.71 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774816652145642		[learning rate: 1.0723e-05]
	Learning Rate: 1.0723e-05
	LOSS [training: 3.774816652145642 | validation: 4.791128751215011]
	TIME [epoch: 9.72 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7761814699034475		[learning rate: 1.0684e-05]
	Learning Rate: 1.06841e-05
	LOSS [training: 3.7761814699034475 | validation: 4.798134909722861]
	TIME [epoch: 9.71 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774117701435963		[learning rate: 1.0645e-05]
	Learning Rate: 1.06453e-05
	LOSS [training: 3.774117701435963 | validation: 4.792580292256095]
	TIME [epoch: 9.72 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7744207887244414		[learning rate: 1.0607e-05]
	Learning Rate: 1.06067e-05
	LOSS [training: 3.7744207887244414 | validation: 4.799097905933507]
	TIME [epoch: 9.7 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775756897602381		[learning rate: 1.0568e-05]
	Learning Rate: 1.05682e-05
	LOSS [training: 3.775756897602381 | validation: 4.798050228753334]
	TIME [epoch: 9.7 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7728603472137436		[learning rate: 1.053e-05]
	Learning Rate: 1.05298e-05
	LOSS [training: 3.7728603472137436 | validation: 4.799691955143995]
	TIME [epoch: 9.71 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7706252610147097		[learning rate: 1.0492e-05]
	Learning Rate: 1.04916e-05
	LOSS [training: 3.7706252610147097 | validation: 4.789938156493405]
	TIME [epoch: 9.72 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.769274990251611		[learning rate: 1.0454e-05]
	Learning Rate: 1.04535e-05
	LOSS [training: 3.769274990251611 | validation: 4.793971226407528]
	TIME [epoch: 9.7 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.779355124237006		[learning rate: 1.0416e-05]
	Learning Rate: 1.04156e-05
	LOSS [training: 3.779355124237006 | validation: 4.790108460064785]
	TIME [epoch: 9.7 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7779924521395833		[learning rate: 1.0378e-05]
	Learning Rate: 1.03778e-05
	LOSS [training: 3.7779924521395833 | validation: 4.788137646987559]
	TIME [epoch: 9.71 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7743113468308493		[learning rate: 1.034e-05]
	Learning Rate: 1.03401e-05
	LOSS [training: 3.7743113468308493 | validation: 4.801289129994634]
	TIME [epoch: 9.73 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.769562852387458		[learning rate: 1.0303e-05]
	Learning Rate: 1.03026e-05
	LOSS [training: 3.769562852387458 | validation: 4.79954794736161]
	TIME [epoch: 9.71 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771449597479049		[learning rate: 1.0265e-05]
	Learning Rate: 1.02652e-05
	LOSS [training: 3.771449597479049 | validation: 4.795939111808698]
	TIME [epoch: 9.7 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77960750614906		[learning rate: 1.0228e-05]
	Learning Rate: 1.0228e-05
	LOSS [training: 3.77960750614906 | validation: 4.796978001357322]
	TIME [epoch: 9.73 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772195836317537		[learning rate: 1.0191e-05]
	Learning Rate: 1.01909e-05
	LOSS [training: 3.772195836317537 | validation: 4.800391530263603]
	TIME [epoch: 9.7 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7737211668930812		[learning rate: 1.0154e-05]
	Learning Rate: 1.01539e-05
	LOSS [training: 3.7737211668930812 | validation: 4.795417206587908]
	TIME [epoch: 9.71 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7737914912590482		[learning rate: 1.0117e-05]
	Learning Rate: 1.0117e-05
	LOSS [training: 3.7737914912590482 | validation: 4.78023569345665]
	TIME [epoch: 9.7 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774215968918882		[learning rate: 1.008e-05]
	Learning Rate: 1.00803e-05
	LOSS [training: 3.774215968918882 | validation: 4.791761966481012]
	TIME [epoch: 9.72 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774863577064753		[learning rate: 1.0044e-05]
	Learning Rate: 1.00437e-05
	LOSS [training: 3.774863577064753 | validation: 4.7965318481776995]
	TIME [epoch: 9.7 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7740684294925395		[learning rate: 1.0007e-05]
	Learning Rate: 1.00073e-05
	LOSS [training: 3.7740684294925395 | validation: 4.8026383688588865]
	TIME [epoch: 9.69 sec]
Finished training in 19589.510 seconds.
