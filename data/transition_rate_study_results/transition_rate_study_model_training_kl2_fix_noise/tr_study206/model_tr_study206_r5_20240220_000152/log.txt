Args:
Namespace(name='model_tr_study206', outdir='out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5', training_data='data/transition_rate_studies/tr_study206/tr_study206_training/r5', validation_data='data/transition_rate_studies/tr_study206/tr_study206_validation/r5', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 437163093

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.550454929324632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.550454929324632 | validation: 9.886707544471724]
	TIME [epoch: 79.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.531043971714066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.531043971714066 | validation: 11.3502359442482]
	TIME [epoch: 9.74 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.993619536547303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.993619536547303 | validation: 11.158651071518799]
	TIME [epoch: 9.72 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.849988016140298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.849988016140298 | validation: 11.293122626372096]
	TIME [epoch: 9.74 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.182846938148067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.182846938148067 | validation: 9.356927945599963]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.321272282290021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.321272282290021 | validation: 9.185450493797433]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.74861924972376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.74861924972376 | validation: 8.130118489886081]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.317692772160239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.317692772160239 | validation: 7.336035454352707]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.296407413513838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.296407413513838 | validation: 5.952831790972974]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.057831673971096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.057831673971096 | validation: 5.324459740354638]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.574086249320795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.574086249320795 | validation: 6.065811385509412]
	TIME [epoch: 9.71 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.6180150214788425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.6180150214788425 | validation: 6.683985029034302]
	TIME [epoch: 9.7 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.426604908983748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.426604908983748 | validation: 5.733388302371411]
	TIME [epoch: 9.72 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.961151489679656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.961151489679656 | validation: 7.242108046699509]
	TIME [epoch: 9.71 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.1795383409969284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.1795383409969284 | validation: 5.9035611992867905]
	TIME [epoch: 9.7 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.102364844583386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.102364844583386 | validation: 4.961158262534199]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.817386468951449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.817386468951449 | validation: 5.237190659376981]
	TIME [epoch: 9.72 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.906817849829658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.906817849829658 | validation: 5.179940891314361]
	TIME [epoch: 9.69 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.795683151418544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.795683151418544 | validation: 5.485783218870879]
	TIME [epoch: 9.7 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.8063499497340745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.8063499497340745 | validation: 5.406123173401896]
	TIME [epoch: 9.72 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5119408133675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5119408133675 | validation: 6.200201960809113]
	TIME [epoch: 9.7 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.972064224445452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.972064224445452 | validation: 5.909809861053819]
	TIME [epoch: 9.7 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.665200444578461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.665200444578461 | validation: 5.703859377129549]
	TIME [epoch: 9.7 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.724138571762607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.724138571762607 | validation: 5.184459938962978]
	TIME [epoch: 9.71 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.631324679900096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.631324679900096 | validation: 4.786894095621844]
	TIME [epoch: 9.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5283819612188045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5283819612188045 | validation: 5.5454931583425155]
	TIME [epoch: 9.7 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.465604222518714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.465604222518714 | validation: 5.1185990164486945]
	TIME [epoch: 9.72 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5651517134381425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5651517134381425 | validation: 5.33560904818223]
	TIME [epoch: 9.7 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.867906924443526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.867906924443526 | validation: 4.692261208884396]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.343148878540733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.343148878540733 | validation: 5.8989833407906485]
	TIME [epoch: 9.72 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.517529848557207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.517529848557207 | validation: 5.111340472716235]
	TIME [epoch: 9.7 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.496886004874357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.496886004874357 | validation: 7.614718760650722]
	TIME [epoch: 9.7 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.452003260207187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.452003260207187 | validation: 4.573887905389733]
	TIME [epoch: 9.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.777275939789183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.777275939789183 | validation: 5.348037902358019]
	TIME [epoch: 9.74 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.287404816650641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.287404816650641 | validation: 5.790186309989742]
	TIME [epoch: 9.7 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.309983799820276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.309983799820276 | validation: 4.925760863451934]
	TIME [epoch: 9.71 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.216459418891262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.216459418891262 | validation: 4.479431391703194]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.280702225820919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.280702225820919 | validation: 4.7666617477655855]
	TIME [epoch: 9.7 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.241844472475802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.241844472475802 | validation: 5.050828663316211]
	TIME [epoch: 9.69 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3275041931506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3275041931506 | validation: 5.797477850662163]
	TIME [epoch: 9.71 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.578763066335616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.578763066335616 | validation: 4.758278328960754]
	TIME [epoch: 9.73 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0712764646074255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.0712764646074255 | validation: 4.826262514395636]
	TIME [epoch: 9.7 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.083142285829152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.083142285829152 | validation: 4.88635767788257]
	TIME [epoch: 9.7 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.954638266532859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.954638266532859 | validation: 5.391973778054326]
	TIME [epoch: 9.72 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.226722698648643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.226722698648643 | validation: 4.90208213768406]
	TIME [epoch: 9.7 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.138069807514571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.138069807514571 | validation: 6.712961857701353]
	TIME [epoch: 9.69 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.013303818449454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.013303818449454 | validation: 4.482985456094752]
	TIME [epoch: 9.72 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3012636581022505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3012636581022505 | validation: 4.733239574005454]
	TIME [epoch: 9.69 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.998422180962796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.998422180962796 | validation: 4.4366541351464]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.870676324879321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.870676324879321 | validation: 4.84697050198535]
	TIME [epoch: 9.71 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.952969291971149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.952969291971149 | validation: 4.1478021550973585]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9114558546103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9114558546103 | validation: 4.4921282889514895]
	TIME [epoch: 9.7 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7145853432148686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7145853432148686 | validation: 5.295692660778939]
	TIME [epoch: 9.69 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.043598857742154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.043598857742154 | validation: 5.750112398130681]
	TIME [epoch: 9.71 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.009611985671868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.009611985671868 | validation: 5.671552807876947]
	TIME [epoch: 9.69 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.933241517891313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.933241517891313 | validation: 4.297441670400156]
	TIME [epoch: 9.69 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.83221216814521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.83221216814521 | validation: 4.139428171497131]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7903775438515055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7903775438515055 | validation: 4.676994611383834]
	TIME [epoch: 9.71 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.811525962010988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.811525962010988 | validation: 4.072876344531297]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.632459743890062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.632459743890062 | validation: 4.884008041804977]
	TIME [epoch: 9.7 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.763676341488468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.763676341488468 | validation: 4.531987976085741]
	TIME [epoch: 9.73 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.650540478193639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.650540478193639 | validation: 4.289035368520406]
	TIME [epoch: 9.7 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.642466121023675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.642466121023675 | validation: 4.850722769694409]
	TIME [epoch: 9.7 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.706600081282162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.706600081282162 | validation: 4.314035017426175]
	TIME [epoch: 9.73 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.695309808170437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.695309808170437 | validation: 4.073917679452798]
	TIME [epoch: 9.71 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9450538713985335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9450538713985335 | validation: 4.089941722295884]
	TIME [epoch: 9.69 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.607666130436155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.607666130436155 | validation: 4.657567451927336]
	TIME [epoch: 9.7 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.890066296745632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.890066296745632 | validation: 4.266785932160606]
	TIME [epoch: 9.71 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.89271632967985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.89271632967985 | validation: 4.892370688625109]
	TIME [epoch: 9.71 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.75304183376179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.75304183376179 | validation: 5.558725425512882]
	TIME [epoch: 9.69 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.393700526147632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.393700526147632 | validation: 3.8422202540957118]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.549468602053712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.549468602053712 | validation: 4.371369834089063]
	TIME [epoch: 9.7 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.47567915897763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.47567915897763 | validation: 5.506443469128067]
	TIME [epoch: 9.71 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.377118307741345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.377118307741345 | validation: 4.098941646117506]
	TIME [epoch: 9.7 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.937657829771618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.937657829771618 | validation: 4.543866543860112]
	TIME [epoch: 9.69 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.856598464171047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.856598464171047 | validation: 3.845215757741903]
	TIME [epoch: 9.68 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.631476507348201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.631476507348201 | validation: 3.7032987901823233]
	TIME [epoch: 9.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.514039444693265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.514039444693265 | validation: 4.074758287046024]
	TIME [epoch: 9.72 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.470699430228965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.470699430228965 | validation: 3.759506482946256]
	TIME [epoch: 9.71 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.50405702929676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.50405702929676 | validation: 4.166104823103365]
	TIME [epoch: 9.69 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.561248246872881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.561248246872881 | validation: 6.304920251609558]
	TIME [epoch: 9.71 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1193850969938435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1193850969938435 | validation: 4.385031923409107]
	TIME [epoch: 9.69 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.594377277291837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.594377277291837 | validation: 3.613428830946308]
	TIME [epoch: 9.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.611490947265139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.611490947265139 | validation: 4.5701108859499024]
	TIME [epoch: 9.7 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.40123644738479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.40123644738479 | validation: 3.918461178345988]
	TIME [epoch: 9.71 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2911287284376005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2911287284376005 | validation: 3.7851429342552723]
	TIME [epoch: 9.7 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.489806254023007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.489806254023007 | validation: 3.6262253573725407]
	TIME [epoch: 9.69 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.5098333922269855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5098333922269855 | validation: 3.791340863472391]
	TIME [epoch: 9.73 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.262237066350626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.262237066350626 | validation: 4.09202300435537]
	TIME [epoch: 9.69 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.363870718249551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.363870718249551 | validation: 3.737198595833846]
	TIME [epoch: 9.69 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.338295924888304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.338295924888304 | validation: 4.039627362627592]
	TIME [epoch: 9.7 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.355766808063136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.355766808063136 | validation: 3.69156323445836]
	TIME [epoch: 9.7 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3424501637205815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3424501637205815 | validation: 4.615198428306018]
	TIME [epoch: 9.7 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.392383776952356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.392383776952356 | validation: 3.7223176694864497]
	TIME [epoch: 9.69 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.316735447625982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.316735447625982 | validation: 4.295438164822065]
	TIME [epoch: 9.73 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.368658958822941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.368658958822941 | validation: 3.6674548257534982]
	TIME [epoch: 9.69 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.440011875940041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.440011875940041 | validation: 4.169518356567614]
	TIME [epoch: 9.7 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.377107504975708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.377107504975708 | validation: 3.75999199326256]
	TIME [epoch: 9.7 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.312866499221909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.312866499221909 | validation: 3.985544237636398]
	TIME [epoch: 9.7 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.340091726821321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.340091726821321 | validation: 3.8015052796722784]
	TIME [epoch: 9.69 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.286666027440775		[learning rate: 0.009971]
	Learning Rate: 0.00997096
	LOSS [training: 4.286666027440775 | validation: 4.123663059047451]
	TIME [epoch: 9.71 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.742427669678352		[learning rate: 0.0099348]
	Learning Rate: 0.00993477
	LOSS [training: 4.742427669678352 | validation: 3.7351840943342416]
	TIME [epoch: 9.72 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.753687933155928		[learning rate: 0.0098987]
	Learning Rate: 0.00989872
	LOSS [training: 4.753687933155928 | validation: 4.624717488516228]
	TIME [epoch: 9.68 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.653167755421748		[learning rate: 0.0098628]
	Learning Rate: 0.00986279
	LOSS [training: 4.653167755421748 | validation: 3.7478153445853453]
	TIME [epoch: 9.68 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.495910398953092		[learning rate: 0.009827]
	Learning Rate: 0.009827
	LOSS [training: 4.495910398953092 | validation: 3.8141874484288576]
	TIME [epoch: 9.7 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.542223994538782		[learning rate: 0.0097913]
	Learning Rate: 0.00979134
	LOSS [training: 4.542223994538782 | validation: 3.7468853212285174]
	TIME [epoch: 9.7 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.244679432684103		[learning rate: 0.0097558]
	Learning Rate: 0.00975581
	LOSS [training: 4.244679432684103 | validation: 3.7892797886407186]
	TIME [epoch: 9.7 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.310707888072751		[learning rate: 0.0097204]
	Learning Rate: 0.0097204
	LOSS [training: 4.310707888072751 | validation: 3.759036137498759]
	TIME [epoch: 9.71 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.234876914916521		[learning rate: 0.0096851]
	Learning Rate: 0.00968513
	LOSS [training: 4.234876914916521 | validation: 3.947686556606351]
	TIME [epoch: 9.7 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.333330737192807		[learning rate: 0.00965]
	Learning Rate: 0.00964998
	LOSS [training: 4.333330737192807 | validation: 3.9032421702975104]
	TIME [epoch: 9.69 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.35530571743146		[learning rate: 0.009615]
	Learning Rate: 0.00961496
	LOSS [training: 4.35530571743146 | validation: 3.666608487906354]
	TIME [epoch: 9.68 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.206610779480244		[learning rate: 0.0095801]
	Learning Rate: 0.00958006
	LOSS [training: 4.206610779480244 | validation: 3.473101935961763]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.421288611865171		[learning rate: 0.0095453]
	Learning Rate: 0.0095453
	LOSS [training: 4.421288611865171 | validation: 3.598846315377474]
	TIME [epoch: 9.7 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.354754893904529		[learning rate: 0.0095107]
	Learning Rate: 0.00951066
	LOSS [training: 4.354754893904529 | validation: 3.505148581818968]
	TIME [epoch: 9.72 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.4760028517189285		[learning rate: 0.0094761]
	Learning Rate: 0.00947614
	LOSS [training: 4.4760028517189285 | validation: 4.071042233991174]
	TIME [epoch: 9.73 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.387113920757219		[learning rate: 0.0094418]
	Learning Rate: 0.00944175
	LOSS [training: 4.387113920757219 | validation: 3.866913104161292]
	TIME [epoch: 9.71 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.121071756169394		[learning rate: 0.0094075]
	Learning Rate: 0.00940749
	LOSS [training: 4.121071756169394 | validation: 4.320486907412402]
	TIME [epoch: 9.7 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.4830868648030116		[learning rate: 0.0093733]
	Learning Rate: 0.00937335
	LOSS [training: 4.4830868648030116 | validation: 3.514257014570929]
	TIME [epoch: 9.71 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2532665871386515		[learning rate: 0.0093393]
	Learning Rate: 0.00933933
	LOSS [training: 4.2532665871386515 | validation: 4.952222747948607]
	TIME [epoch: 9.72 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.934154295606376		[learning rate: 0.0093054]
	Learning Rate: 0.00930544
	LOSS [training: 4.934154295606376 | validation: 3.8318993239645853]
	TIME [epoch: 9.7 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.728646099215755		[learning rate: 0.0092717]
	Learning Rate: 0.00927167
	LOSS [training: 4.728646099215755 | validation: 4.1669177181741865]
	TIME [epoch: 9.71 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.342018166291782		[learning rate: 0.009238]
	Learning Rate: 0.00923802
	LOSS [training: 4.342018166291782 | validation: 4.003315064239891]
	TIME [epoch: 9.73 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.208341291106487		[learning rate: 0.0092045]
	Learning Rate: 0.0092045
	LOSS [training: 4.208341291106487 | validation: 4.242131114188153]
	TIME [epoch: 9.71 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1821899969583844		[learning rate: 0.0091711]
	Learning Rate: 0.00917109
	LOSS [training: 4.1821899969583844 | validation: 3.8400827776123307]
	TIME [epoch: 9.71 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.399347648997617		[learning rate: 0.0091378]
	Learning Rate: 0.00913781
	LOSS [training: 4.399347648997617 | validation: 3.4814743081431483]
	TIME [epoch: 9.72 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.231352829477847		[learning rate: 0.0091046]
	Learning Rate: 0.00910465
	LOSS [training: 4.231352829477847 | validation: 3.937746857852864]
	TIME [epoch: 9.71 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.40642366826308		[learning rate: 0.0090716]
	Learning Rate: 0.00907161
	LOSS [training: 4.40642366826308 | validation: 3.8673440900064486]
	TIME [epoch: 9.7 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.138123146267475		[learning rate: 0.0090387]
	Learning Rate: 0.00903868
	LOSS [training: 4.138123146267475 | validation: 3.4469947972764503]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.057087780115987		[learning rate: 0.0090059]
	Learning Rate: 0.00900588
	LOSS [training: 4.057087780115987 | validation: 3.538147450299986]
	TIME [epoch: 9.72 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.608689029155909		[learning rate: 0.0089732]
	Learning Rate: 0.0089732
	LOSS [training: 5.608689029155909 | validation: 3.4238766958861175]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.119837897937288		[learning rate: 0.0089406]
	Learning Rate: 0.00894064
	LOSS [training: 4.119837897937288 | validation: 3.4783149940869147]
	TIME [epoch: 9.7 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.130916350086251		[learning rate: 0.0089082]
	Learning Rate: 0.00890819
	LOSS [training: 4.130916350086251 | validation: 3.5588942711747813]
	TIME [epoch: 9.74 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0510631275759135		[learning rate: 0.0088759]
	Learning Rate: 0.00887586
	LOSS [training: 4.0510631275759135 | validation: 3.867360972022882]
	TIME [epoch: 9.72 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.323300379746092		[learning rate: 0.0088437]
	Learning Rate: 0.00884365
	LOSS [training: 4.323300379746092 | validation: 3.460069615911966]
	TIME [epoch: 9.71 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0460944187472		[learning rate: 0.0088116]
	Learning Rate: 0.00881156
	LOSS [training: 4.0460944187472 | validation: 3.336621925025345]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.150214548590036		[learning rate: 0.0087796]
	Learning Rate: 0.00877958
	LOSS [training: 4.150214548590036 | validation: 4.575849498877358]
	TIME [epoch: 9.74 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.312539192855607		[learning rate: 0.0087477]
	Learning Rate: 0.00874772
	LOSS [training: 4.312539192855607 | validation: 3.295301453052174]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.283649311036763		[learning rate: 0.008716]
	Learning Rate: 0.00871597
	LOSS [training: 4.283649311036763 | validation: 3.573332405440414]
	TIME [epoch: 9.7 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9896404229240856		[learning rate: 0.0086843]
	Learning Rate: 0.00868434
	LOSS [training: 3.9896404229240856 | validation: 3.372146844513511]
	TIME [epoch: 9.72 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.047820593502399		[learning rate: 0.0086528]
	Learning Rate: 0.00865282
	LOSS [training: 4.047820593502399 | validation: 3.3781037663300175]
	TIME [epoch: 9.71 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.984794479636359		[learning rate: 0.0086214]
	Learning Rate: 0.00862142
	LOSS [training: 3.984794479636359 | validation: 3.54833356981392]
	TIME [epoch: 9.7 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.007559677096873		[learning rate: 0.0085901]
	Learning Rate: 0.00859013
	LOSS [training: 4.007559677096873 | validation: 3.8027404467971313]
	TIME [epoch: 9.73 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0355051460647235		[learning rate: 0.008559]
	Learning Rate: 0.00855896
	LOSS [training: 4.0355051460647235 | validation: 3.3663058475417222]
	TIME [epoch: 9.72 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.206187523423157		[learning rate: 0.0085279]
	Learning Rate: 0.0085279
	LOSS [training: 4.206187523423157 | validation: 3.39507534217269]
	TIME [epoch: 9.71 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.291665776302954		[learning rate: 0.008497]
	Learning Rate: 0.00849695
	LOSS [training: 4.291665776302954 | validation: 4.308108061822796]
	TIME [epoch: 9.71 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.505872503729913		[learning rate: 0.0084661]
	Learning Rate: 0.00846612
	LOSS [training: 4.505872503729913 | validation: 3.4007677320397964]
	TIME [epoch: 9.74 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.228053825620007		[learning rate: 0.0084354]
	Learning Rate: 0.00843539
	LOSS [training: 4.228053825620007 | validation: 4.262773333183174]
	TIME [epoch: 9.7 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.381024822060843		[learning rate: 0.0084048]
	Learning Rate: 0.00840478
	LOSS [training: 4.381024822060843 | validation: 3.39775869390254]
	TIME [epoch: 9.71 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.453970143979131		[learning rate: 0.0083743]
	Learning Rate: 0.00837428
	LOSS [training: 4.453970143979131 | validation: 3.9904496317459985]
	TIME [epoch: 9.73 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3226059924250455		[learning rate: 0.0083439]
	Learning Rate: 0.00834389
	LOSS [training: 4.3226059924250455 | validation: 3.3633037224092868]
	TIME [epoch: 9.71 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.073689260749177		[learning rate: 0.0083136]
	Learning Rate: 0.00831361
	LOSS [training: 4.073689260749177 | validation: 4.083620736036272]
	TIME [epoch: 9.71 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.24760491915155		[learning rate: 0.0082834]
	Learning Rate: 0.00828344
	LOSS [training: 4.24760491915155 | validation: 3.307407273109817]
	TIME [epoch: 9.71 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.370850506289075		[learning rate: 0.0082534]
	Learning Rate: 0.00825338
	LOSS [training: 4.370850506289075 | validation: 4.741211821832469]
	TIME [epoch: 9.73 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.335680395237938		[learning rate: 0.0082234]
	Learning Rate: 0.00822342
	LOSS [training: 4.335680395237938 | validation: 3.2473347180414796]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9699945523819986		[learning rate: 0.0081936]
	Learning Rate: 0.00819358
	LOSS [training: 3.9699945523819986 | validation: 3.9502719486241986]
	TIME [epoch: 9.71 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.160192229942014		[learning rate: 0.0081638]
	Learning Rate: 0.00816384
	LOSS [training: 4.160192229942014 | validation: 3.4183044239504885]
	TIME [epoch: 9.74 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.377855613789587		[learning rate: 0.0081342]
	Learning Rate: 0.00813422
	LOSS [training: 4.377855613789587 | validation: 4.069808000547258]
	TIME [epoch: 9.71 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.386228914065461		[learning rate: 0.0081047]
	Learning Rate: 0.0081047
	LOSS [training: 4.386228914065461 | validation: 3.4256406407922464]
	TIME [epoch: 9.7 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.147594716428756		[learning rate: 0.0080753]
	Learning Rate: 0.00807529
	LOSS [training: 4.147594716428756 | validation: 3.9205200120184633]
	TIME [epoch: 9.72 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.210814351319948		[learning rate: 0.008046]
	Learning Rate: 0.00804598
	LOSS [training: 4.210814351319948 | validation: 4.053700977958216]
	TIME [epoch: 9.71 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.282475881108555		[learning rate: 0.0080168]
	Learning Rate: 0.00801678
	LOSS [training: 4.282475881108555 | validation: 3.2592408506025228]
	TIME [epoch: 9.71 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.105927461862052		[learning rate: 0.0079877]
	Learning Rate: 0.00798769
	LOSS [training: 4.105927461862052 | validation: 3.2290601080238828]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8189830577653696		[learning rate: 0.0079587]
	Learning Rate: 0.0079587
	LOSS [training: 3.8189830577653696 | validation: 3.8445660315061]
	TIME [epoch: 9.74 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9950462144623264		[learning rate: 0.0079298]
	Learning Rate: 0.00792982
	LOSS [training: 3.9950462144623264 | validation: 3.3371571096021997]
	TIME [epoch: 9.71 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.010331088023761		[learning rate: 0.007901]
	Learning Rate: 0.00790104
	LOSS [training: 4.010331088023761 | validation: 3.230757425464616]
	TIME [epoch: 9.7 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.981212243535162		[learning rate: 0.0078724]
	Learning Rate: 0.00787237
	LOSS [training: 3.981212243535162 | validation: 3.3566283868340285]
	TIME [epoch: 9.74 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.833785844590255		[learning rate: 0.0078438]
	Learning Rate: 0.0078438
	LOSS [training: 3.833785844590255 | validation: 3.685857340359567]
	TIME [epoch: 9.71 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.961146564333962		[learning rate: 0.0078153]
	Learning Rate: 0.00781533
	LOSS [training: 3.961146564333962 | validation: 4.525417469654126]
	TIME [epoch: 9.71 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2905015493254925		[learning rate: 0.007787]
	Learning Rate: 0.00778697
	LOSS [training: 4.2905015493254925 | validation: 3.200034648825867]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.084333121480596		[learning rate: 0.0077587]
	Learning Rate: 0.00775871
	LOSS [training: 4.084333121480596 | validation: 3.6660479357038493]
	TIME [epoch: 9.73 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.268273774473643		[learning rate: 0.0077306]
	Learning Rate: 0.00773055
	LOSS [training: 4.268273774473643 | validation: 3.456916802806727]
	TIME [epoch: 9.7 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.096620320164299		[learning rate: 0.0077025]
	Learning Rate: 0.0077025
	LOSS [training: 4.096620320164299 | validation: 3.7450583176539762]
	TIME [epoch: 9.7 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.045651614067323		[learning rate: 0.0076745]
	Learning Rate: 0.00767455
	LOSS [training: 4.045651614067323 | validation: 3.2404825024231685]
	TIME [epoch: 9.73 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.012012456913775		[learning rate: 0.0076467]
	Learning Rate: 0.00764669
	LOSS [training: 4.012012456913775 | validation: 3.768284999767449]
	TIME [epoch: 9.7 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9004641647181977		[learning rate: 0.0076189]
	Learning Rate: 0.00761894
	LOSS [training: 3.9004641647181977 | validation: 3.449210100627513]
	TIME [epoch: 9.7 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8079430578516273		[learning rate: 0.0075913]
	Learning Rate: 0.00759129
	LOSS [training: 3.8079430578516273 | validation: 3.9229258442829567]
	TIME [epoch: 9.72 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.14037873872054		[learning rate: 0.0075637]
	Learning Rate: 0.00756374
	LOSS [training: 4.14037873872054 | validation: 3.4324908049509526]
	TIME [epoch: 9.69 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.050740737354096		[learning rate: 0.0075363]
	Learning Rate: 0.00753629
	LOSS [training: 4.050740737354096 | validation: 3.6045768542531538]
	TIME [epoch: 9.7 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.964784747906293		[learning rate: 0.0075089]
	Learning Rate: 0.00750895
	LOSS [training: 3.964784747906293 | validation: 3.286086589226844]
	TIME [epoch: 9.69 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8332905213261244		[learning rate: 0.0074817]
	Learning Rate: 0.00748169
	LOSS [training: 3.8332905213261244 | validation: 3.1807973939029797]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.186267544990726		[learning rate: 0.0074545]
	Learning Rate: 0.00745454
	LOSS [training: 4.186267544990726 | validation: 3.462094435930558]
	TIME [epoch: 9.7 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9435489801147803		[learning rate: 0.0074275]
	Learning Rate: 0.00742749
	LOSS [training: 3.9435489801147803 | validation: 3.256787275191537]
	TIME [epoch: 9.69 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.114673197483748		[learning rate: 0.0074005]
	Learning Rate: 0.00740054
	LOSS [training: 4.114673197483748 | validation: 4.130807988659603]
	TIME [epoch: 9.71 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.247637958338404		[learning rate: 0.0073737]
	Learning Rate: 0.00737368
	LOSS [training: 4.247637958338404 | validation: 3.3172107770771015]
	TIME [epoch: 9.7 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.987951589074394		[learning rate: 0.0073469]
	Learning Rate: 0.00734692
	LOSS [training: 3.987951589074394 | validation: 3.4829152012114744]
	TIME [epoch: 9.7 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9497056832409774		[learning rate: 0.0073203]
	Learning Rate: 0.00732026
	LOSS [training: 3.9497056832409774 | validation: 3.277377765787507]
	TIME [epoch: 9.72 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9179725796603755		[learning rate: 0.0072937]
	Learning Rate: 0.00729369
	LOSS [training: 3.9179725796603755 | validation: 3.323363962426303]
	TIME [epoch: 9.7 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.861693893476695		[learning rate: 0.0072672]
	Learning Rate: 0.00726722
	LOSS [training: 3.861693893476695 | validation: 3.4082806154758356]
	TIME [epoch: 9.69 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8620886977037605		[learning rate: 0.0072408]
	Learning Rate: 0.00724085
	LOSS [training: 3.8620886977037605 | validation: 3.3886136581935484]
	TIME [epoch: 9.69 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8552070748350866		[learning rate: 0.0072146]
	Learning Rate: 0.00721457
	LOSS [training: 3.8552070748350866 | validation: 3.2149138820051495]
	TIME [epoch: 9.71 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.866180579257763		[learning rate: 0.0071884]
	Learning Rate: 0.00718839
	LOSS [training: 3.866180579257763 | validation: 3.292781646037215]
	TIME [epoch: 9.68 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.905391993494276		[learning rate: 0.0071623]
	Learning Rate: 0.0071623
	LOSS [training: 3.905391993494276 | validation: 3.1866209717173355]
	TIME [epoch: 9.68 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0072416208383626		[learning rate: 0.0071363]
	Learning Rate: 0.00713631
	LOSS [training: 4.0072416208383626 | validation: 3.604105046406862]
	TIME [epoch: 9.7 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.831068169790492		[learning rate: 0.0071104]
	Learning Rate: 0.00711041
	LOSS [training: 3.831068169790492 | validation: 3.9347178788679087]
	TIME [epoch: 9.7 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.109027557092429		[learning rate: 0.0070846]
	Learning Rate: 0.00708461
	LOSS [training: 4.109027557092429 | validation: 3.172290935278712]
	TIME [epoch: 9.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7882698004503963		[learning rate: 0.0070589]
	Learning Rate: 0.0070589
	LOSS [training: 3.7882698004503963 | validation: 3.30988598161397]
	TIME [epoch: 9.7 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.833994876148907		[learning rate: 0.0070333]
	Learning Rate: 0.00703328
	LOSS [training: 3.833994876148907 | validation: 3.7221009288868423]
	TIME [epoch: 9.72 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.192974697689088		[learning rate: 0.0070078]
	Learning Rate: 0.00700776
	LOSS [training: 4.192974697689088 | validation: 3.2822744402516175]
	TIME [epoch: 9.7 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.13217663348376		[learning rate: 0.0069823]
	Learning Rate: 0.00698232
	LOSS [training: 4.13217663348376 | validation: 3.3718988005344417]
	TIME [epoch: 9.7 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.91703620557574		[learning rate: 0.006957]
	Learning Rate: 0.00695698
	LOSS [training: 3.91703620557574 | validation: 3.887709744020138]
	TIME [epoch: 9.73 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.021711435556977		[learning rate: 0.0069317]
	Learning Rate: 0.00693174
	LOSS [training: 4.021711435556977 | validation: 3.236658103808428]
	TIME [epoch: 9.69 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9194598108105354		[learning rate: 0.0069066]
	Learning Rate: 0.00690658
	LOSS [training: 3.9194598108105354 | validation: 3.566296785966248]
	TIME [epoch: 9.69 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8162772721650042		[learning rate: 0.0068815]
	Learning Rate: 0.00688152
	LOSS [training: 3.8162772721650042 | validation: 3.3905598544044095]
	TIME [epoch: 9.71 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.052126356505161		[learning rate: 0.0068565]
	Learning Rate: 0.00685654
	LOSS [training: 4.052126356505161 | validation: 3.2349078960579845]
	TIME [epoch: 9.7 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.124897436614272		[learning rate: 0.0068317]
	Learning Rate: 0.00683166
	LOSS [training: 4.124897436614272 | validation: 3.9049724009395983]
	TIME [epoch: 9.69 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.069668895084449		[learning rate: 0.0068069]
	Learning Rate: 0.00680687
	LOSS [training: 4.069668895084449 | validation: 3.27677904679995]
	TIME [epoch: 9.69 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.88440063808702		[learning rate: 0.0067822]
	Learning Rate: 0.00678217
	LOSS [training: 3.88440063808702 | validation: 3.6719027063983107]
	TIME [epoch: 9.72 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8835752461399773		[learning rate: 0.0067576]
	Learning Rate: 0.00675755
	LOSS [training: 3.8835752461399773 | validation: 3.1793032278333433]
	TIME [epoch: 9.69 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0830284596575614		[learning rate: 0.006733]
	Learning Rate: 0.00673303
	LOSS [training: 4.0830284596575614 | validation: 3.721030429692789]
	TIME [epoch: 9.68 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.010459076496825		[learning rate: 0.0067086]
	Learning Rate: 0.00670859
	LOSS [training: 4.010459076496825 | validation: 3.1869969474310613]
	TIME [epoch: 9.72 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7556943295382843		[learning rate: 0.0066842]
	Learning Rate: 0.00668425
	LOSS [training: 3.7556943295382843 | validation: 3.3059993697975814]
	TIME [epoch: 9.7 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.769714880103551		[learning rate: 0.00666]
	Learning Rate: 0.00665999
	LOSS [training: 3.769714880103551 | validation: 3.5399670006104937]
	TIME [epoch: 9.69 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8178994447502186		[learning rate: 0.0066358]
	Learning Rate: 0.00663582
	LOSS [training: 3.8178994447502186 | validation: 3.261026501448802]
	TIME [epoch: 9.69 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.057977555194223		[learning rate: 0.0066117]
	Learning Rate: 0.00661174
	LOSS [training: 4.057977555194223 | validation: 3.4093002450476315]
	TIME [epoch: 9.71 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.128556743936139		[learning rate: 0.0065877]
	Learning Rate: 0.00658775
	LOSS [training: 4.128556743936139 | validation: 3.899470495932474]
	TIME [epoch: 9.69 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.990511212182169		[learning rate: 0.0065638]
	Learning Rate: 0.00656384
	LOSS [training: 3.990511212182169 | validation: 4.589985621580289]
	TIME [epoch: 9.71 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2134195295543035		[learning rate: 0.00654]
	Learning Rate: 0.00654002
	LOSS [training: 4.2134195295543035 | validation: 3.1844165199278915]
	TIME [epoch: 9.72 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8740535241823517		[learning rate: 0.0065163]
	Learning Rate: 0.00651628
	LOSS [training: 3.8740535241823517 | validation: 3.95601783816662]
	TIME [epoch: 9.7 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.107811282151468		[learning rate: 0.0064926]
	Learning Rate: 0.00649264
	LOSS [training: 4.107811282151468 | validation: 3.2731976788398858]
	TIME [epoch: 9.7 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7946123224638058		[learning rate: 0.0064691]
	Learning Rate: 0.00646907
	LOSS [training: 3.7946123224638058 | validation: 3.3037446004703397]
	TIME [epoch: 9.71 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8099012234085636		[learning rate: 0.0064456]
	Learning Rate: 0.0064456
	LOSS [training: 3.8099012234085636 | validation: 3.8685524711540267]
	TIME [epoch: 9.71 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.064908251362051		[learning rate: 0.0064222]
	Learning Rate: 0.00642221
	LOSS [training: 4.064908251362051 | validation: 3.337867963828809]
	TIME [epoch: 9.69 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.884880173699399		[learning rate: 0.0063989]
	Learning Rate: 0.0063989
	LOSS [training: 3.884880173699399 | validation: 3.66576697865263]
	TIME [epoch: 9.7 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.034828242927743		[learning rate: 0.0063757]
	Learning Rate: 0.00637568
	LOSS [training: 4.034828242927743 | validation: 3.2265358892245413]
	TIME [epoch: 9.72 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8988202609589138		[learning rate: 0.0063525]
	Learning Rate: 0.00635254
	LOSS [training: 3.8988202609589138 | validation: 3.866941204042869]
	TIME [epoch: 9.69 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.957589380974722		[learning rate: 0.0063295]
	Learning Rate: 0.00632949
	LOSS [training: 3.957589380974722 | validation: 3.17418430997592]
	TIME [epoch: 9.71 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.788325217412531		[learning rate: 0.0063065]
	Learning Rate: 0.00630652
	LOSS [training: 3.788325217412531 | validation: 3.1513584623612405]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.79268329127594		[learning rate: 0.0062836]
	Learning Rate: 0.00628363
	LOSS [training: 3.79268329127594 | validation: 3.140895241801816]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7423673994925637		[learning rate: 0.0062608]
	Learning Rate: 0.00626082
	LOSS [training: 3.7423673994925637 | validation: 3.2412144465747295]
	TIME [epoch: 9.71 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.78695580547224		[learning rate: 0.0062381]
	Learning Rate: 0.0062381
	LOSS [training: 3.78695580547224 | validation: 3.138010289956775]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8158598590263573		[learning rate: 0.0062155]
	Learning Rate: 0.00621547
	LOSS [training: 3.8158598590263573 | validation: 3.1229122893833887]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.954657791036145		[learning rate: 0.0061929]
	Learning Rate: 0.00619291
	LOSS [training: 3.954657791036145 | validation: 3.6625854464945986]
	TIME [epoch: 9.72 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.86234053095705		[learning rate: 0.0061704]
	Learning Rate: 0.00617043
	LOSS [training: 3.86234053095705 | validation: 3.3813847689975454]
	TIME [epoch: 9.72 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.743995204235752		[learning rate: 0.006148]
	Learning Rate: 0.00614804
	LOSS [training: 3.743995204235752 | validation: 3.217516576743692]
	TIME [epoch: 9.74 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7795154051534654		[learning rate: 0.0061257]
	Learning Rate: 0.00612573
	LOSS [training: 3.7795154051534654 | validation: 3.2443014285927405]
	TIME [epoch: 9.72 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.878254053438819		[learning rate: 0.0061035]
	Learning Rate: 0.0061035
	LOSS [training: 3.878254053438819 | validation: 3.292013398540088]
	TIME [epoch: 9.72 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8549496987082086		[learning rate: 0.0060814]
	Learning Rate: 0.00608135
	LOSS [training: 3.8549496987082086 | validation: 3.235528287736847]
	TIME [epoch: 9.73 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7664510111406044		[learning rate: 0.0060593]
	Learning Rate: 0.00605928
	LOSS [training: 3.7664510111406044 | validation: 3.689418155545748]
	TIME [epoch: 9.72 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.929075320182986		[learning rate: 0.0060373]
	Learning Rate: 0.00603729
	LOSS [training: 3.929075320182986 | validation: 3.180552353314547]
	TIME [epoch: 9.72 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7701418201857875		[learning rate: 0.0060154]
	Learning Rate: 0.00601538
	LOSS [training: 3.7701418201857875 | validation: 3.2270005444525625]
	TIME [epoch: 9.71 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771730906876198		[learning rate: 0.0059936]
	Learning Rate: 0.00599355
	LOSS [training: 3.771730906876198 | validation: 3.3747010044292494]
	TIME [epoch: 9.75 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9945629123649438		[learning rate: 0.0059718]
	Learning Rate: 0.0059718
	LOSS [training: 3.9945629123649438 | validation: 3.3582318624937275]
	TIME [epoch: 9.71 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.91529442644851		[learning rate: 0.0059501]
	Learning Rate: 0.00595013
	LOSS [training: 3.91529442644851 | validation: 3.2578750010668194]
	TIME [epoch: 9.71 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8533702826730343		[learning rate: 0.0059285]
	Learning Rate: 0.00592853
	LOSS [training: 3.8533702826730343 | validation: 3.6567897865995675]
	TIME [epoch: 9.74 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.061282381695688		[learning rate: 0.005907]
	Learning Rate: 0.00590702
	LOSS [training: 4.061282381695688 | validation: 3.2957104613769905]
	TIME [epoch: 9.72 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7993566530359226		[learning rate: 0.0058856]
	Learning Rate: 0.00588558
	LOSS [training: 3.7993566530359226 | validation: 3.1682243654379953]
	TIME [epoch: 9.71 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.829833265797075		[learning rate: 0.0058642]
	Learning Rate: 0.00586422
	LOSS [training: 3.829833265797075 | validation: 3.4718600969872377]
	TIME [epoch: 9.72 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.823682537294059		[learning rate: 0.0058429]
	Learning Rate: 0.00584294
	LOSS [training: 3.823682537294059 | validation: 3.4173087850463957]
	TIME [epoch: 9.73 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8861327175615337		[learning rate: 0.0058217]
	Learning Rate: 0.00582174
	LOSS [training: 3.8861327175615337 | validation: 3.353860184484641]
	TIME [epoch: 9.71 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.792133247246423		[learning rate: 0.0058006]
	Learning Rate: 0.00580061
	LOSS [training: 3.792133247246423 | validation: 3.1914338723486346]
	TIME [epoch: 9.72 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.013304716190231		[learning rate: 0.0057796]
	Learning Rate: 0.00577956
	LOSS [training: 4.013304716190231 | validation: 3.5844286891600157]
	TIME [epoch: 9.74 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8840821749764642		[learning rate: 0.0057586]
	Learning Rate: 0.00575859
	LOSS [training: 3.8840821749764642 | validation: 3.315194586017749]
	TIME [epoch: 9.72 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7360659891620016		[learning rate: 0.0057377]
	Learning Rate: 0.00573769
	LOSS [training: 3.7360659891620016 | validation: 3.4808146210385633]
	TIME [epoch: 9.72 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9881759682910802		[learning rate: 0.0057169]
	Learning Rate: 0.00571686
	LOSS [training: 3.9881759682910802 | validation: 3.372206298402698]
	TIME [epoch: 9.73 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8171479009244265		[learning rate: 0.0056961]
	Learning Rate: 0.00569612
	LOSS [training: 3.8171479009244265 | validation: 3.371794330329678]
	TIME [epoch: 9.72 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.798314222031416		[learning rate: 0.0056754]
	Learning Rate: 0.00567545
	LOSS [training: 3.798314222031416 | validation: 3.1315371354150647]
	TIME [epoch: 9.72 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780370443956309		[learning rate: 0.0056548]
	Learning Rate: 0.00565485
	LOSS [training: 3.780370443956309 | validation: 3.1480932774745582]
	TIME [epoch: 9.71 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8229463694266728		[learning rate: 0.0056343]
	Learning Rate: 0.00563433
	LOSS [training: 3.8229463694266728 | validation: 3.1589655384145363]
	TIME [epoch: 9.74 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.718780119132602		[learning rate: 0.0056139]
	Learning Rate: 0.00561388
	LOSS [training: 3.718780119132602 | validation: 3.2160889123619896]
	TIME [epoch: 9.72 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8079454210376555		[learning rate: 0.0055935]
	Learning Rate: 0.00559351
	LOSS [training: 3.8079454210376555 | validation: 3.2629924997863373]
	TIME [epoch: 9.72 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.771055030464617		[learning rate: 0.0055732]
	Learning Rate: 0.00557321
	LOSS [training: 3.771055030464617 | validation: 3.2686940064246484]
	TIME [epoch: 9.74 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780317478619736		[learning rate: 0.005553]
	Learning Rate: 0.00555298
	LOSS [training: 3.780317478619736 | validation: 3.1773507223938293]
	TIME [epoch: 9.72 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.833769015276566		[learning rate: 0.0055328]
	Learning Rate: 0.00553283
	LOSS [training: 3.833769015276566 | validation: 3.200938046257221]
	TIME [epoch: 9.72 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.904369157331898		[learning rate: 0.0055128]
	Learning Rate: 0.00551275
	LOSS [training: 3.904369157331898 | validation: 3.2164160330799008]
	TIME [epoch: 9.72 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8602606836968136		[learning rate: 0.0054927]
	Learning Rate: 0.00549274
	LOSS [training: 3.8602606836968136 | validation: 3.262271457094118]
	TIME [epoch: 9.72 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.803393285671042		[learning rate: 0.0054728]
	Learning Rate: 0.00547281
	LOSS [training: 3.803393285671042 | validation: 3.195820019714237]
	TIME [epoch: 9.71 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7826814645679576		[learning rate: 0.005453]
	Learning Rate: 0.00545295
	LOSS [training: 3.7826814645679576 | validation: 3.1055766963443365]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8888029975735328		[learning rate: 0.0054332]
	Learning Rate: 0.00543316
	LOSS [training: 3.8888029975735328 | validation: 3.2804282380307828]
	TIME [epoch: 9.74 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7811118756624276		[learning rate: 0.0054134]
	Learning Rate: 0.00541344
	LOSS [training: 3.7811118756624276 | validation: 3.1652862912896627]
	TIME [epoch: 9.71 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.975415157601809		[learning rate: 0.0053938]
	Learning Rate: 0.0053938
	LOSS [training: 3.975415157601809 | validation: 3.2705804453026373]
	TIME [epoch: 9.71 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.82682604610705		[learning rate: 0.0053742]
	Learning Rate: 0.00537422
	LOSS [training: 3.82682604610705 | validation: 3.2217029728337296]
	TIME [epoch: 9.73 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7172528710225707		[learning rate: 0.0053547]
	Learning Rate: 0.00535472
	LOSS [training: 3.7172528710225707 | validation: 3.3923891012937553]
	TIME [epoch: 9.72 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9346198518861213		[learning rate: 0.0053353]
	Learning Rate: 0.00533529
	LOSS [training: 3.9346198518861213 | validation: 3.1259748438086694]
	TIME [epoch: 9.72 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.855219624846265		[learning rate: 0.0053159]
	Learning Rate: 0.00531593
	LOSS [training: 3.855219624846265 | validation: 3.903162435802202]
	TIME [epoch: 9.71 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9545101778907807		[learning rate: 0.0052966]
	Learning Rate: 0.00529663
	LOSS [training: 3.9545101778907807 | validation: 3.1604211141960508]
	TIME [epoch: 9.74 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8406628615059235		[learning rate: 0.0052774]
	Learning Rate: 0.00527741
	LOSS [training: 3.8406628615059235 | validation: 3.352077702380324]
	TIME [epoch: 9.71 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8178776110112596		[learning rate: 0.0052583]
	Learning Rate: 0.00525826
	LOSS [training: 3.8178776110112596 | validation: 3.1320227632701347]
	TIME [epoch: 9.71 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.793383659769925		[learning rate: 0.0052392]
	Learning Rate: 0.00523918
	LOSS [training: 3.793383659769925 | validation: 3.451451456225866]
	TIME [epoch: 9.73 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.058975678512539		[learning rate: 0.0052202]
	Learning Rate: 0.00522017
	LOSS [training: 4.058975678512539 | validation: 3.3308068901009826]
	TIME [epoch: 9.72 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.850085856444168		[learning rate: 0.0052012]
	Learning Rate: 0.00520122
	LOSS [training: 3.850085856444168 | validation: 3.541602623832472]
	TIME [epoch: 9.71 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8979124367190265		[learning rate: 0.0051823]
	Learning Rate: 0.00518234
	LOSS [training: 3.8979124367190265 | validation: 3.5813722195561297]
	TIME [epoch: 9.72 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.804718004770213		[learning rate: 0.0051635]
	Learning Rate: 0.00516354
	LOSS [training: 3.804718004770213 | validation: 3.4389235104987486]
	TIME [epoch: 9.73 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.831069842201211		[learning rate: 0.0051448]
	Learning Rate: 0.0051448
	LOSS [training: 3.831069842201211 | validation: 3.125063915195886]
	TIME [epoch: 9.71 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.744061935170547		[learning rate: 0.0051261]
	Learning Rate: 0.00512613
	LOSS [training: 3.744061935170547 | validation: 3.123046940011713]
	TIME [epoch: 9.7 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9129916701678837		[learning rate: 0.0051075]
	Learning Rate: 0.00510753
	LOSS [training: 3.9129916701678837 | validation: 3.486682053273963]
	TIME [epoch: 9.73 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7678468810794987		[learning rate: 0.005089]
	Learning Rate: 0.00508899
	LOSS [training: 3.7678468810794987 | validation: 3.1830223014559165]
	TIME [epoch: 9.71 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8217942263182914		[learning rate: 0.0050705]
	Learning Rate: 0.00507052
	LOSS [training: 3.8217942263182914 | validation: 3.1180304813701953]
	TIME [epoch: 9.7 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.812533565362126		[learning rate: 0.0050521]
	Learning Rate: 0.00505212
	LOSS [training: 3.812533565362126 | validation: 3.139433488970036]
	TIME [epoch: 9.71 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7051652976823894		[learning rate: 0.0050338]
	Learning Rate: 0.00503379
	LOSS [training: 3.7051652976823894 | validation: 3.1583782521306967]
	TIME [epoch: 9.73 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8785259047239458		[learning rate: 0.0050155]
	Learning Rate: 0.00501552
	LOSS [training: 3.8785259047239458 | validation: 3.1831232087338606]
	TIME [epoch: 9.71 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7139236318366216		[learning rate: 0.0049973]
	Learning Rate: 0.00499732
	LOSS [training: 3.7139236318366216 | validation: 3.350351025303938]
	TIME [epoch: 9.7 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.730221756573571		[learning rate: 0.0049792]
	Learning Rate: 0.00497918
	LOSS [training: 3.730221756573571 | validation: 3.159567497371851]
	TIME [epoch: 9.73 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7885714095260368		[learning rate: 0.0049611]
	Learning Rate: 0.00496111
	LOSS [training: 3.7885714095260368 | validation: 3.298738147846855]
	TIME [epoch: 9.71 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.792661947756475		[learning rate: 0.0049431]
	Learning Rate: 0.00494311
	LOSS [training: 3.792661947756475 | validation: 3.12914763455385]
	TIME [epoch: 9.71 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.68766299601002		[learning rate: 0.0049252]
	Learning Rate: 0.00492517
	LOSS [training: 3.68766299601002 | validation: 3.5522131929249943]
	TIME [epoch: 9.73 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.814766377471863		[learning rate: 0.0049073]
	Learning Rate: 0.00490729
	LOSS [training: 3.814766377471863 | validation: 3.239910191862597]
	TIME [epoch: 9.72 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.747567613525927		[learning rate: 0.0048895]
	Learning Rate: 0.00488948
	LOSS [training: 3.747567613525927 | validation: 3.379741624924899]
	TIME [epoch: 9.71 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.798930326890717		[learning rate: 0.0048717]
	Learning Rate: 0.00487174
	LOSS [training: 3.798930326890717 | validation: 3.132422359596818]
	TIME [epoch: 9.71 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.945365925969017		[learning rate: 0.0048541]
	Learning Rate: 0.00485406
	LOSS [training: 3.945365925969017 | validation: 3.4311702974387392]
	TIME [epoch: 9.72 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8032214243108484		[learning rate: 0.0048364]
	Learning Rate: 0.00483645
	LOSS [training: 3.8032214243108484 | validation: 3.1322838060236973]
	TIME [epoch: 9.71 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7113805903283867		[learning rate: 0.0048189]
	Learning Rate: 0.00481889
	LOSS [training: 3.7113805903283867 | validation: 3.1108162250691307]
	TIME [epoch: 9.7 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.699541053752138		[learning rate: 0.0048014]
	Learning Rate: 0.00480141
	LOSS [training: 3.699541053752138 | validation: 3.6448078230882373]
	TIME [epoch: 9.73 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8481788936341403		[learning rate: 0.004784]
	Learning Rate: 0.00478398
	LOSS [training: 3.8481788936341403 | validation: 3.1403388554239484]
	TIME [epoch: 9.7 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.706440232955812		[learning rate: 0.0047666]
	Learning Rate: 0.00476662
	LOSS [training: 3.706440232955812 | validation: 3.140132262179857]
	TIME [epoch: 9.71 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7701995424778176		[learning rate: 0.0047493]
	Learning Rate: 0.00474932
	LOSS [training: 3.7701995424778176 | validation: 3.2217411634976587]
	TIME [epoch: 9.71 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6908790490799155		[learning rate: 0.0047321]
	Learning Rate: 0.00473209
	LOSS [training: 3.6908790490799155 | validation: 3.2982319220122576]
	TIME [epoch: 9.73 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7829482101826257		[learning rate: 0.0047149]
	Learning Rate: 0.00471491
	LOSS [training: 3.7829482101826257 | validation: 3.1458723372761654]
	TIME [epoch: 9.71 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.736709421073484		[learning rate: 0.0046978]
	Learning Rate: 0.0046978
	LOSS [training: 3.736709421073484 | validation: 3.174930753019952]
	TIME [epoch: 9.71 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7053520358684415		[learning rate: 0.0046808]
	Learning Rate: 0.00468075
	LOSS [training: 3.7053520358684415 | validation: 3.298682406785129]
	TIME [epoch: 9.73 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7753047369056154		[learning rate: 0.0046638]
	Learning Rate: 0.00466377
	LOSS [training: 3.7753047369056154 | validation: 3.1292737489030458]
	TIME [epoch: 9.71 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8492441097250447		[learning rate: 0.0046468]
	Learning Rate: 0.00464684
	LOSS [training: 3.8492441097250447 | validation: 3.1139488003616034]
	TIME [epoch: 9.7 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.807909739607394		[learning rate: 0.00463]
	Learning Rate: 0.00462998
	LOSS [training: 3.807909739607394 | validation: 3.466199480586429]
	TIME [epoch: 9.73 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.76319671931398		[learning rate: 0.0046132]
	Learning Rate: 0.00461318
	LOSS [training: 3.76319671931398 | validation: 3.0899474930766666]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.681461571846262		[learning rate: 0.0045964]
	Learning Rate: 0.00459643
	LOSS [training: 3.681461571846262 | validation: 3.1771218203320544]
	TIME [epoch: 9.71 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.726021781693607		[learning rate: 0.0045798]
	Learning Rate: 0.00457975
	LOSS [training: 3.726021781693607 | validation: 3.221528165273351]
	TIME [epoch: 9.71 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7083133496244214		[learning rate: 0.0045631]
	Learning Rate: 0.00456313
	LOSS [training: 3.7083133496244214 | validation: 3.343795937992114]
	TIME [epoch: 9.73 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8000897574563752		[learning rate: 0.0045466]
	Learning Rate: 0.00454657
	LOSS [training: 3.8000897574563752 | validation: 3.139501246207657]
	TIME [epoch: 9.71 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.754567282265524		[learning rate: 0.0045301]
	Learning Rate: 0.00453007
	LOSS [training: 3.754567282265524 | validation: 3.4811045410125576]
	TIME [epoch: 9.71 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7697747113664826		[learning rate: 0.0045136]
	Learning Rate: 0.00451363
	LOSS [training: 3.7697747113664826 | validation: 3.125835277752094]
	TIME [epoch: 9.73 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.783762689433857		[learning rate: 0.0044973]
	Learning Rate: 0.00449725
	LOSS [training: 3.783762689433857 | validation: 3.1739966384829508]
	TIME [epoch: 9.72 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6846777102957766		[learning rate: 0.0044809]
	Learning Rate: 0.00448093
	LOSS [training: 3.6846777102957766 | validation: 3.4223200605652933]
	TIME [epoch: 9.71 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8318319276858714		[learning rate: 0.0044647]
	Learning Rate: 0.00446467
	LOSS [training: 3.8318319276858714 | validation: 3.151787314647511]
	TIME [epoch: 9.72 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.728410039963336		[learning rate: 0.0044485]
	Learning Rate: 0.00444847
	LOSS [training: 3.728410039963336 | validation: 3.3379461025072636]
	TIME [epoch: 9.73 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.797263226921335		[learning rate: 0.0044323]
	Learning Rate: 0.00443232
	LOSS [training: 3.797263226921335 | validation: 3.2011872487487403]
	TIME [epoch: 9.71 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6592100400691363		[learning rate: 0.0044162]
	Learning Rate: 0.00441624
	LOSS [training: 3.6592100400691363 | validation: 3.5384314752971058]
	TIME [epoch: 9.71 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.748759227286498		[learning rate: 0.0044002]
	Learning Rate: 0.00440021
	LOSS [training: 3.748759227286498 | validation: 3.1370708253189377]
	TIME [epoch: 9.73 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6495145711251618		[learning rate: 0.0043842]
	Learning Rate: 0.00438424
	LOSS [training: 3.6495145711251618 | validation: 3.404188390021236]
	TIME [epoch: 9.72 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6903332359921692		[learning rate: 0.0043683]
	Learning Rate: 0.00436833
	LOSS [training: 3.6903332359921692 | validation: 3.3877902285520882]
	TIME [epoch: 9.71 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.996492479221677		[learning rate: 0.0043525]
	Learning Rate: 0.00435248
	LOSS [training: 3.996492479221677 | validation: 3.190566059420736]
	TIME [epoch: 9.72 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7599629936869077		[learning rate: 0.0043367]
	Learning Rate: 0.00433668
	LOSS [training: 3.7599629936869077 | validation: 3.253304374512436]
	TIME [epoch: 9.72 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7944883815047006		[learning rate: 0.0043209]
	Learning Rate: 0.00432095
	LOSS [training: 3.7944883815047006 | validation: 3.576601307435483]
	TIME [epoch: 9.71 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.855122131556685		[learning rate: 0.0043053]
	Learning Rate: 0.00430527
	LOSS [training: 3.855122131556685 | validation: 3.1735760878319117]
	TIME [epoch: 9.71 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6772583268820442		[learning rate: 0.0042896]
	Learning Rate: 0.00428964
	LOSS [training: 3.6772583268820442 | validation: 3.7665330714879315]
	TIME [epoch: 9.74 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8452003408070974		[learning rate: 0.0042741]
	Learning Rate: 0.00427407
	LOSS [training: 3.8452003408070974 | validation: 3.210710122159176]
	TIME [epoch: 9.71 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.716161901878144		[learning rate: 0.0042586]
	Learning Rate: 0.00425856
	LOSS [training: 3.716161901878144 | validation: 3.150331859558743]
	TIME [epoch: 9.71 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6633713881454986		[learning rate: 0.0042431]
	Learning Rate: 0.00424311
	LOSS [training: 3.6633713881454986 | validation: 3.189845403518777]
	TIME [epoch: 9.73 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.713359107566551		[learning rate: 0.0042277]
	Learning Rate: 0.00422771
	LOSS [training: 3.713359107566551 | validation: 3.1065957103246853]
	TIME [epoch: 9.71 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7960275599748363		[learning rate: 0.0042124]
	Learning Rate: 0.00421237
	LOSS [training: 3.7960275599748363 | validation: 3.1858110971072584]
	TIME [epoch: 9.7 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.69536248042781		[learning rate: 0.0041971]
	Learning Rate: 0.00419708
	LOSS [training: 3.69536248042781 | validation: 3.313638417610314]
	TIME [epoch: 9.71 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.76360125288289		[learning rate: 0.0041818]
	Learning Rate: 0.00418185
	LOSS [training: 3.76360125288289 | validation: 3.3334722912595582]
	TIME [epoch: 9.7 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7389885232630418		[learning rate: 0.0041667]
	Learning Rate: 0.00416667
	LOSS [training: 3.7389885232630418 | validation: 3.249546359826526]
	TIME [epoch: 9.7 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.703044109397007		[learning rate: 0.0041516]
	Learning Rate: 0.00415155
	LOSS [training: 3.703044109397007 | validation: 3.2368650437802775]
	TIME [epoch: 9.7 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8369901955261208		[learning rate: 0.0041365]
	Learning Rate: 0.00413649
	LOSS [training: 3.8369901955261208 | validation: 3.1631408547705133]
	TIME [epoch: 9.74 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.72404013668442		[learning rate: 0.0041215]
	Learning Rate: 0.00412147
	LOSS [training: 3.72404013668442 | validation: 3.114235073052737]
	TIME [epoch: 9.72 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.840619235295976		[learning rate: 0.0041065]
	Learning Rate: 0.00410652
	LOSS [training: 3.840619235295976 | validation: 3.1567820024720334]
	TIME [epoch: 9.71 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.763867671162097		[learning rate: 0.0040916]
	Learning Rate: 0.00409161
	LOSS [training: 3.763867671162097 | validation: 3.1623771832707135]
	TIME [epoch: 9.72 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.666788318788716		[learning rate: 0.0040768]
	Learning Rate: 0.00407677
	LOSS [training: 3.666788318788716 | validation: 3.550065070175931]
	TIME [epoch: 9.71 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8691263943795846		[learning rate: 0.004062]
	Learning Rate: 0.00406197
	LOSS [training: 3.8691263943795846 | validation: 3.5767055457520907]
	TIME [epoch: 9.69 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.820936345666497		[learning rate: 0.0040472]
	Learning Rate: 0.00404723
	LOSS [training: 3.820936345666497 | validation: 3.13062584449721]
	TIME [epoch: 9.7 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7534343933025154		[learning rate: 0.0040325]
	Learning Rate: 0.00403254
	LOSS [training: 3.7534343933025154 | validation: 3.2043472251151868]
	TIME [epoch: 9.74 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.800435090272411		[learning rate: 0.0040179]
	Learning Rate: 0.00401791
	LOSS [training: 3.800435090272411 | validation: 3.1601171136567365]
	TIME [epoch: 9.71 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.746555309568179		[learning rate: 0.0040033]
	Learning Rate: 0.00400333
	LOSS [training: 3.746555309568179 | validation: 3.201998761282593]
	TIME [epoch: 9.7 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.727384538602095		[learning rate: 0.0039888]
	Learning Rate: 0.0039888
	LOSS [training: 3.727384538602095 | validation: 3.2430295810793472]
	TIME [epoch: 9.72 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7420334674199944		[learning rate: 0.0039743]
	Learning Rate: 0.00397432
	LOSS [training: 3.7420334674199944 | validation: 3.1399202817860714]
	TIME [epoch: 9.69 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6738271203275494		[learning rate: 0.0039599]
	Learning Rate: 0.0039599
	LOSS [training: 3.6738271203275494 | validation: 3.162055339112184]
	TIME [epoch: 9.7 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.687829748297525		[learning rate: 0.0039455]
	Learning Rate: 0.00394553
	LOSS [training: 3.687829748297525 | validation: 3.1771695387075103]
	TIME [epoch: 9.71 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.758784826543213		[learning rate: 0.0039312]
	Learning Rate: 0.00393121
	LOSS [training: 3.758784826543213 | validation: 3.1555465213708187]
	TIME [epoch: 9.71 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.826384471013662		[learning rate: 0.0039169]
	Learning Rate: 0.00391694
	LOSS [training: 3.826384471013662 | validation: 3.347220436549737]
	TIME [epoch: 9.7 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.021560698289374		[learning rate: 0.0039027]
	Learning Rate: 0.00390273
	LOSS [training: 4.021560698289374 | validation: 3.117607655093153]
	TIME [epoch: 9.71 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.681321422481312		[learning rate: 0.0038886]
	Learning Rate: 0.00388857
	LOSS [training: 3.681321422481312 | validation: 3.1614624505782762]
	TIME [epoch: 9.72 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6879924255497896		[learning rate: 0.0038745]
	Learning Rate: 0.00387445
	LOSS [training: 3.6879924255497896 | validation: 3.126957828870943]
	TIME [epoch: 9.69 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.692628621257021		[learning rate: 0.0038604]
	Learning Rate: 0.00386039
	LOSS [training: 3.692628621257021 | validation: 3.4181324357808713]
	TIME [epoch: 9.7 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77578018098337		[learning rate: 0.0038464]
	Learning Rate: 0.00384638
	LOSS [training: 3.77578018098337 | validation: 3.1262241096206287]
	TIME [epoch: 9.7 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7084348749836473		[learning rate: 0.0038324]
	Learning Rate: 0.00383242
	LOSS [training: 3.7084348749836473 | validation: 3.2807755127794382]
	TIME [epoch: 9.71 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7919217179637066		[learning rate: 0.0038185]
	Learning Rate: 0.00381852
	LOSS [training: 3.7919217179637066 | validation: 3.200552766677258]
	TIME [epoch: 9.69 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.664929492017478		[learning rate: 0.0038047]
	Learning Rate: 0.00380466
	LOSS [training: 3.664929492017478 | validation: 3.1971956548621177]
	TIME [epoch: 9.69 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7411314477764734		[learning rate: 0.0037909]
	Learning Rate: 0.00379085
	LOSS [training: 3.7411314477764734 | validation: 3.110530058600954]
	TIME [epoch: 9.72 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.734652789511845		[learning rate: 0.0037771]
	Learning Rate: 0.00377709
	LOSS [training: 3.734652789511845 | validation: 3.117902040908076]
	TIME [epoch: 9.7 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.731080447156166		[learning rate: 0.0037634]
	Learning Rate: 0.00376339
	LOSS [training: 3.731080447156166 | validation: 3.190802137149232]
	TIME [epoch: 9.7 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.694469945858016		[learning rate: 0.0037497]
	Learning Rate: 0.00374973
	LOSS [training: 3.694469945858016 | validation: 3.133227098345368]
	TIME [epoch: 9.72 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.645959516974448		[learning rate: 0.0037361]
	Learning Rate: 0.00373612
	LOSS [training: 3.645959516974448 | validation: 3.2100992079967505]
	TIME [epoch: 9.7 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7788519006910755		[learning rate: 0.0037226]
	Learning Rate: 0.00372256
	LOSS [training: 3.7788519006910755 | validation: 3.18451570999299]
	TIME [epoch: 9.7 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6962006213918093		[learning rate: 0.0037091]
	Learning Rate: 0.00370905
	LOSS [training: 3.6962006213918093 | validation: 3.1717759077099243]
	TIME [epoch: 9.71 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.703622220113887		[learning rate: 0.0036956]
	Learning Rate: 0.00369559
	LOSS [training: 3.703622220113887 | validation: 3.4001090143465365]
	TIME [epoch: 9.72 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.723121156970662		[learning rate: 0.0036822]
	Learning Rate: 0.00368218
	LOSS [training: 3.723121156970662 | validation: 3.2263258515139377]
	TIME [epoch: 9.7 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7470290971906115		[learning rate: 0.0036688]
	Learning Rate: 0.00366882
	LOSS [training: 3.7470290971906115 | validation: 3.1439363217870278]
	TIME [epoch: 9.69 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.717719758073322		[learning rate: 0.0036555]
	Learning Rate: 0.0036555
	LOSS [training: 3.717719758073322 | validation: 3.0985314192771773]
	TIME [epoch: 9.72 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.698255665983683		[learning rate: 0.0036422]
	Learning Rate: 0.00364224
	LOSS [training: 3.698255665983683 | validation: 3.395117093748672]
	TIME [epoch: 9.71 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.729938653093137		[learning rate: 0.003629]
	Learning Rate: 0.00362902
	LOSS [training: 3.729938653093137 | validation: 3.1672111894945316]
	TIME [epoch: 9.69 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.670274073631453		[learning rate: 0.0036159]
	Learning Rate: 0.00361585
	LOSS [training: 3.670274073631453 | validation: 3.267548059735401]
	TIME [epoch: 9.72 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.755748820180366		[learning rate: 0.0036027]
	Learning Rate: 0.00360273
	LOSS [training: 3.755748820180366 | validation: 3.1376514912419307]
	TIME [epoch: 9.73 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6881542136756913		[learning rate: 0.0035897]
	Learning Rate: 0.00358965
	LOSS [training: 3.6881542136756913 | validation: 3.4731584642509086]
	TIME [epoch: 9.69 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.821173413472768		[learning rate: 0.0035766]
	Learning Rate: 0.00357663
	LOSS [training: 3.821173413472768 | validation: 3.1836369834730194]
	TIME [epoch: 9.71 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7508405591915697		[learning rate: 0.0035636]
	Learning Rate: 0.00356365
	LOSS [training: 3.7508405591915697 | validation: 3.1760900285189053]
	TIME [epoch: 9.72 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6708061783767576		[learning rate: 0.0035507]
	Learning Rate: 0.00355072
	LOSS [training: 3.6708061783767576 | validation: 3.152240554442087]
	TIME [epoch: 9.7 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7317119162077326		[learning rate: 0.0035378]
	Learning Rate: 0.00353783
	LOSS [training: 3.7317119162077326 | validation: 3.2588456906453263]
	TIME [epoch: 9.7 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6962172123743144		[learning rate: 0.003525]
	Learning Rate: 0.00352499
	LOSS [training: 3.6962172123743144 | validation: 3.1106524192403753]
	TIME [epoch: 9.71 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.667150808530998		[learning rate: 0.0035122]
	Learning Rate: 0.0035122
	LOSS [training: 3.667150808530998 | validation: 3.11360145350336]
	TIME [epoch: 9.7 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7143964365682516		[learning rate: 0.0034995]
	Learning Rate: 0.00349945
	LOSS [training: 3.7143964365682516 | validation: 3.2958547334437562]
	TIME [epoch: 9.69 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7227010518928942		[learning rate: 0.0034868]
	Learning Rate: 0.00348675
	LOSS [training: 3.7227010518928942 | validation: 3.2710526143902054]
	TIME [epoch: 9.69 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.757720372048829		[learning rate: 0.0034741]
	Learning Rate: 0.0034741
	LOSS [training: 3.757720372048829 | validation: 3.0997309381957177]
	TIME [epoch: 9.72 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.670140281378662		[learning rate: 0.0034615]
	Learning Rate: 0.00346149
	LOSS [training: 3.670140281378662 | validation: 3.2598488314890264]
	TIME [epoch: 9.7 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7167029813187398		[learning rate: 0.0034489]
	Learning Rate: 0.00344893
	LOSS [training: 3.7167029813187398 | validation: 3.175383905254947]
	TIME [epoch: 9.69 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6848988599077765		[learning rate: 0.0034364]
	Learning Rate: 0.00343641
	LOSS [training: 3.6848988599077765 | validation: 3.3182512435364555]
	TIME [epoch: 9.72 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.730245072294707		[learning rate: 0.0034239]
	Learning Rate: 0.00342394
	LOSS [training: 3.730245072294707 | validation: 3.1177936042726238]
	TIME [epoch: 9.71 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6627728112667057		[learning rate: 0.0034115]
	Learning Rate: 0.00341152
	LOSS [training: 3.6627728112667057 | validation: 3.24988261447026]
	TIME [epoch: 9.69 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.848549150481854		[learning rate: 0.0033991]
	Learning Rate: 0.00339914
	LOSS [training: 3.848549150481854 | validation: 3.1109589045115844]
	TIME [epoch: 9.71 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6349830890555226		[learning rate: 0.0033868]
	Learning Rate: 0.0033868
	LOSS [training: 3.6349830890555226 | validation: 3.2392671231210204]
	TIME [epoch: 9.71 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.659195316700683		[learning rate: 0.0033745]
	Learning Rate: 0.00337451
	LOSS [training: 3.659195316700683 | validation: 3.121217397498053]
	TIME [epoch: 9.7 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.670890108511395		[learning rate: 0.0033623]
	Learning Rate: 0.00336226
	LOSS [training: 3.670890108511395 | validation: 3.1417779674079727]
	TIME [epoch: 9.69 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.700176833230221		[learning rate: 0.0033501]
	Learning Rate: 0.00335006
	LOSS [training: 3.700176833230221 | validation: 3.192547898727773]
	TIME [epoch: 9.71 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.706653104140412		[learning rate: 0.0033379]
	Learning Rate: 0.0033379
	LOSS [training: 3.706653104140412 | validation: 3.345245263393564]
	TIME [epoch: 9.69 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.792048752347136		[learning rate: 0.0033258]
	Learning Rate: 0.00332579
	LOSS [training: 3.792048752347136 | validation: 3.195367977571068]
	TIME [epoch: 9.7 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7290946558435714		[learning rate: 0.0033137]
	Learning Rate: 0.00331372
	LOSS [training: 3.7290946558435714 | validation: 3.1163034505796436]
	TIME [epoch: 9.71 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.681850926638311		[learning rate: 0.0033017]
	Learning Rate: 0.00330169
	LOSS [training: 3.681850926638311 | validation: 3.117985559324334]
	TIME [epoch: 9.7 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7133989914789938		[learning rate: 0.0032897]
	Learning Rate: 0.00328971
	LOSS [training: 3.7133989914789938 | validation: 3.2171919672254923]
	TIME [epoch: 9.7 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.834438248866334		[learning rate: 0.0032778]
	Learning Rate: 0.00327777
	LOSS [training: 3.834438248866334 | validation: 3.1189962006823184]
	TIME [epoch: 9.7 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6635979059787736		[learning rate: 0.0032659]
	Learning Rate: 0.00326588
	LOSS [training: 3.6635979059787736 | validation: 3.2666931197336466]
	TIME [epoch: 9.72 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6823910068439845		[learning rate: 0.003254]
	Learning Rate: 0.00325403
	LOSS [training: 3.6823910068439845 | validation: 3.1517734436130556]
	TIME [epoch: 9.7 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7169300034336326		[learning rate: 0.0032422]
	Learning Rate: 0.00324222
	LOSS [training: 3.7169300034336326 | validation: 3.109616595747364]
	TIME [epoch: 9.69 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7131262928049935		[learning rate: 0.0032305]
	Learning Rate: 0.00323045
	LOSS [training: 3.7131262928049935 | validation: 3.292573912925466]
	TIME [epoch: 9.72 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7143960590494394		[learning rate: 0.0032187]
	Learning Rate: 0.00321873
	LOSS [training: 3.7143960590494394 | validation: 3.2250690148244714]
	TIME [epoch: 9.7 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.761656057940953		[learning rate: 0.003207]
	Learning Rate: 0.00320705
	LOSS [training: 3.761656057940953 | validation: 3.1345325287238346]
	TIME [epoch: 9.7 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6518063951046864		[learning rate: 0.0031954]
	Learning Rate: 0.00319541
	LOSS [training: 3.6518063951046864 | validation: 3.144212398987372]
	TIME [epoch: 9.69 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.644324169735248		[learning rate: 0.0031838]
	Learning Rate: 0.00318381
	LOSS [training: 3.644324169735248 | validation: 3.1435548795253454]
	TIME [epoch: 9.72 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6458716065266756		[learning rate: 0.0031723]
	Learning Rate: 0.00317226
	LOSS [training: 3.6458716065266756 | validation: 3.2381731799092877]
	TIME [epoch: 9.7 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.741493238156221		[learning rate: 0.0031607]
	Learning Rate: 0.00316075
	LOSS [training: 3.741493238156221 | validation: 3.1335577939106005]
	TIME [epoch: 9.7 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.714748933735149		[learning rate: 0.0031493]
	Learning Rate: 0.00314927
	LOSS [training: 3.714748933735149 | validation: 3.2063688203199674]
	TIME [epoch: 9.72 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6689212785630922		[learning rate: 0.0031378]
	Learning Rate: 0.00313785
	LOSS [training: 3.6689212785630922 | validation: 3.1979287465289103]
	TIME [epoch: 9.7 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7535474541712426		[learning rate: 0.0031265]
	Learning Rate: 0.00312646
	LOSS [training: 3.7535474541712426 | validation: 3.1767970657579214]
	TIME [epoch: 9.7 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.669183438517335		[learning rate: 0.0031151]
	Learning Rate: 0.00311511
	LOSS [training: 3.669183438517335 | validation: 3.155769430661747]
	TIME [epoch: 9.71 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6789762553546184		[learning rate: 0.0031038]
	Learning Rate: 0.00310381
	LOSS [training: 3.6789762553546184 | validation: 3.233053642639303]
	TIME [epoch: 9.71 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.714212112974218		[learning rate: 0.0030925]
	Learning Rate: 0.00309254
	LOSS [training: 3.714212112974218 | validation: 3.145732514314726]
	TIME [epoch: 9.7 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6976483773495645		[learning rate: 0.0030813]
	Learning Rate: 0.00308132
	LOSS [training: 3.6976483773495645 | validation: 3.1183898820269538]
	TIME [epoch: 9.7 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6634942928696206		[learning rate: 0.0030701]
	Learning Rate: 0.00307014
	LOSS [training: 3.6634942928696206 | validation: 3.1471428935423047]
	TIME [epoch: 9.74 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6966485058111758		[learning rate: 0.003059]
	Learning Rate: 0.003059
	LOSS [training: 3.6966485058111758 | validation: 3.160473786745024]
	TIME [epoch: 9.71 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6610481999009785		[learning rate: 0.0030479]
	Learning Rate: 0.0030479
	LOSS [training: 3.6610481999009785 | validation: 3.4178577913179224]
	TIME [epoch: 9.69 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7457999374025945		[learning rate: 0.0030368]
	Learning Rate: 0.00303683
	LOSS [training: 3.7457999374025945 | validation: 3.108842336238412]
	TIME [epoch: 9.72 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.727850601859963		[learning rate: 0.0030258]
	Learning Rate: 0.00302581
	LOSS [training: 3.727850601859963 | validation: 3.1773979327399484]
	TIME [epoch: 9.7 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.662400408229977		[learning rate: 0.0030148]
	Learning Rate: 0.00301483
	LOSS [training: 3.662400408229977 | validation: 3.2244007151733873]
	TIME [epoch: 9.7 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.676996202143991		[learning rate: 0.0030039]
	Learning Rate: 0.00300389
	LOSS [training: 3.676996202143991 | validation: 3.1699069969603904]
	TIME [epoch: 9.71 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6694407604529493		[learning rate: 0.002993]
	Learning Rate: 0.00299299
	LOSS [training: 3.6694407604529493 | validation: 3.1756605090484413]
	TIME [epoch: 9.72 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6507281430691476		[learning rate: 0.0029821]
	Learning Rate: 0.00298213
	LOSS [training: 3.6507281430691476 | validation: 3.1059711039569833]
	TIME [epoch: 9.7 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7367487801686385		[learning rate: 0.0029713]
	Learning Rate: 0.00297131
	LOSS [training: 3.7367487801686385 | validation: 3.1929269999275447]
	TIME [epoch: 9.71 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.697947219282873		[learning rate: 0.0029605]
	Learning Rate: 0.00296052
	LOSS [training: 3.697947219282873 | validation: 3.350302579958797]
	TIME [epoch: 9.74 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.761916111498042		[learning rate: 0.0029498]
	Learning Rate: 0.00294978
	LOSS [training: 3.761916111498042 | validation: 3.343061258753611]
	TIME [epoch: 9.7 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.743191045094241		[learning rate: 0.0029391]
	Learning Rate: 0.00293907
	LOSS [training: 3.743191045094241 | validation: 3.117308412817266]
	TIME [epoch: 9.7 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6958904509365054		[learning rate: 0.0029284]
	Learning Rate: 0.00292841
	LOSS [training: 3.6958904509365054 | validation: 3.1180187709519265]
	TIME [epoch: 9.7 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.661465807143192		[learning rate: 0.0029178]
	Learning Rate: 0.00291778
	LOSS [training: 3.661465807143192 | validation: 3.104381366403931]
	TIME [epoch: 9.72 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.653675864578008		[learning rate: 0.0029072]
	Learning Rate: 0.00290719
	LOSS [training: 3.653675864578008 | validation: 3.2627234345816207]
	TIME [epoch: 9.69 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6767872388489393		[learning rate: 0.0028966]
	Learning Rate: 0.00289664
	LOSS [training: 3.6767872388489393 | validation: 3.2127926736531585]
	TIME [epoch: 9.69 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7005209414017997		[learning rate: 0.0028861]
	Learning Rate: 0.00288613
	LOSS [training: 3.7005209414017997 | validation: 3.1002895882701806]
	TIME [epoch: 9.71 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6591343492079176		[learning rate: 0.0028757]
	Learning Rate: 0.00287566
	LOSS [training: 3.6591343492079176 | validation: 3.133533851875886]
	TIME [epoch: 9.68 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.652846225144087		[learning rate: 0.0028652]
	Learning Rate: 0.00286522
	LOSS [training: 3.652846225144087 | validation: 3.2105648944668146]
	TIME [epoch: 9.69 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.768179347565456		[learning rate: 0.0028548]
	Learning Rate: 0.00285482
	LOSS [training: 3.768179347565456 | validation: 3.1536262636383072]
	TIME [epoch: 9.71 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.683627695630264		[learning rate: 0.0028445]
	Learning Rate: 0.00284446
	LOSS [training: 3.683627695630264 | validation: 3.4292084613497127]
	TIME [epoch: 9.7 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7300667103698446		[learning rate: 0.0028341]
	Learning Rate: 0.00283414
	LOSS [training: 3.7300667103698446 | validation: 3.150848245101031]
	TIME [epoch: 9.69 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.668461769115676		[learning rate: 0.0028239]
	Learning Rate: 0.00282385
	LOSS [training: 3.668461769115676 | validation: 3.106043401229993]
	TIME [epoch: 9.69 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.691850502729399		[learning rate: 0.0028136]
	Learning Rate: 0.00281361
	LOSS [training: 3.691850502729399 | validation: 3.1550008373421856]
	TIME [epoch: 9.72 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.688035578261135		[learning rate: 0.0028034]
	Learning Rate: 0.00280339
	LOSS [training: 3.688035578261135 | validation: 3.258946743173065]
	TIME [epoch: 9.7 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.672298029824532		[learning rate: 0.0027932]
	Learning Rate: 0.00279322
	LOSS [training: 3.672298029824532 | validation: 3.1420113728406536]
	TIME [epoch: 9.69 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6496915873820703		[learning rate: 0.0027831]
	Learning Rate: 0.00278308
	LOSS [training: 3.6496915873820703 | validation: 3.160889343771105]
	TIME [epoch: 9.72 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6806386425562545		[learning rate: 0.002773]
	Learning Rate: 0.00277298
	LOSS [training: 3.6806386425562545 | validation: 3.2073463344646744]
	TIME [epoch: 9.69 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.655245055849835		[learning rate: 0.0027629]
	Learning Rate: 0.00276292
	LOSS [training: 3.655245055849835 | validation: 3.212527212994421]
	TIME [epoch: 9.72 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6845080585070313		[learning rate: 0.0027529]
	Learning Rate: 0.00275289
	LOSS [training: 3.6845080585070313 | validation: 3.097867663039912]
	TIME [epoch: 9.71 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.679613577227658		[learning rate: 0.0027429]
	Learning Rate: 0.0027429
	LOSS [training: 3.679613577227658 | validation: 3.168749310394667]
	TIME [epoch: 9.72 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6556274598477616		[learning rate: 0.0027329]
	Learning Rate: 0.00273295
	LOSS [training: 3.6556274598477616 | validation: 3.2187435930960224]
	TIME [epoch: 9.69 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.657847609384343		[learning rate: 0.002723]
	Learning Rate: 0.00272303
	LOSS [training: 3.657847609384343 | validation: 3.1720397970207648]
	TIME [epoch: 9.69 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.743478004643715		[learning rate: 0.0027131]
	Learning Rate: 0.00271315
	LOSS [training: 3.743478004643715 | validation: 3.1656169893523702]
	TIME [epoch: 9.71 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.669878053369718		[learning rate: 0.0027033]
	Learning Rate: 0.0027033
	LOSS [training: 3.669878053369718 | validation: 3.127419385243399]
	TIME [epoch: 9.71 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6697886650928964		[learning rate: 0.0026935]
	Learning Rate: 0.00269349
	LOSS [training: 3.6697886650928964 | validation: 3.1406302699033084]
	TIME [epoch: 9.69 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.697360268555903		[learning rate: 0.0026837]
	Learning Rate: 0.00268372
	LOSS [training: 3.697360268555903 | validation: 3.1648460900079147]
	TIME [epoch: 9.7 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6586119177046053		[learning rate: 0.002674]
	Learning Rate: 0.00267398
	LOSS [training: 3.6586119177046053 | validation: 3.1250414685132264]
	TIME [epoch: 9.71 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6544242840747927		[learning rate: 0.0026643]
	Learning Rate: 0.00266427
	LOSS [training: 3.6544242840747927 | validation: 3.1121546021327275]
	TIME [epoch: 9.69 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.633678015004552		[learning rate: 0.0026546]
	Learning Rate: 0.00265461
	LOSS [training: 3.633678015004552 | validation: 3.3166766997483745]
	TIME [epoch: 9.68 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.719196943252874		[learning rate: 0.002645]
	Learning Rate: 0.00264497
	LOSS [training: 3.719196943252874 | validation: 3.2834486676817525]
	TIME [epoch: 9.71 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.684948538256317		[learning rate: 0.0026354]
	Learning Rate: 0.00263537
	LOSS [training: 3.684948538256317 | validation: 3.170357961954145]
	TIME [epoch: 9.69 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.646459395509994		[learning rate: 0.0026258]
	Learning Rate: 0.00262581
	LOSS [training: 3.646459395509994 | validation: 3.195563436680248]
	TIME [epoch: 9.7 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7121807102545645		[learning rate: 0.0026163]
	Learning Rate: 0.00261628
	LOSS [training: 3.7121807102545645 | validation: 3.2241637812734054]
	TIME [epoch: 9.7 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6903098793746936		[learning rate: 0.0026068]
	Learning Rate: 0.00260679
	LOSS [training: 3.6903098793746936 | validation: 3.1013478711813547]
	TIME [epoch: 9.7 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7347030304207847		[learning rate: 0.0025973]
	Learning Rate: 0.00259733
	LOSS [training: 3.7347030304207847 | validation: 3.1462118789121627]
	TIME [epoch: 9.69 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6510459903403722		[learning rate: 0.0025879]
	Learning Rate: 0.0025879
	LOSS [training: 3.6510459903403722 | validation: 3.2492783265032403]
	TIME [epoch: 9.69 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.69776260721216		[learning rate: 0.0025785]
	Learning Rate: 0.00257851
	LOSS [training: 3.69776260721216 | validation: 3.198437614116769]
	TIME [epoch: 9.71 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.634640182067013		[learning rate: 0.0025691]
	Learning Rate: 0.00256915
	LOSS [training: 3.634640182067013 | validation: 3.1088272681424427]
	TIME [epoch: 9.7 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.673247655226959		[learning rate: 0.0025598]
	Learning Rate: 0.00255983
	LOSS [training: 3.673247655226959 | validation: 3.25271813597193]
	TIME [epoch: 9.7 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.660242757185132		[learning rate: 0.0025505]
	Learning Rate: 0.00255054
	LOSS [training: 3.660242757185132 | validation: 3.193533387530373]
	TIME [epoch: 9.71 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6553904876839134		[learning rate: 0.0025413]
	Learning Rate: 0.00254128
	LOSS [training: 3.6553904876839134 | validation: 3.125208017658141]
	TIME [epoch: 9.69 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.647745007137061		[learning rate: 0.0025321]
	Learning Rate: 0.00253206
	LOSS [training: 3.647745007137061 | validation: 3.2482220424636394]
	TIME [epoch: 9.7 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.679065132763319		[learning rate: 0.0025229]
	Learning Rate: 0.00252287
	LOSS [training: 3.679065132763319 | validation: 3.111364337981878]
	TIME [epoch: 9.7 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.656593010391058		[learning rate: 0.0025137]
	Learning Rate: 0.00251371
	LOSS [training: 3.656593010391058 | validation: 3.12728653137008]
	TIME [epoch: 9.72 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6666081349542714		[learning rate: 0.0025046]
	Learning Rate: 0.00250459
	LOSS [training: 3.6666081349542714 | validation: 3.207690655438551]
	TIME [epoch: 9.69 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.671266737142831		[learning rate: 0.0024955]
	Learning Rate: 0.0024955
	LOSS [training: 3.671266737142831 | validation: 3.18469927641457]
	TIME [epoch: 9.7 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.658430429344007		[learning rate: 0.0024864]
	Learning Rate: 0.00248645
	LOSS [training: 3.658430429344007 | validation: 3.186425155183343]
	TIME [epoch: 9.73 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.654535972672387		[learning rate: 0.0024774]
	Learning Rate: 0.00247742
	LOSS [training: 3.654535972672387 | validation: 3.1416093738556237]
	TIME [epoch: 9.7 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6535691960157273		[learning rate: 0.0024684]
	Learning Rate: 0.00246843
	LOSS [training: 3.6535691960157273 | validation: 3.1127646806109484]
	TIME [epoch: 9.7 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.706123771140202		[learning rate: 0.0024595]
	Learning Rate: 0.00245947
	LOSS [training: 3.706123771140202 | validation: 3.1268402266109194]
	TIME [epoch: 9.72 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.657734061100652		[learning rate: 0.0024505]
	Learning Rate: 0.00245055
	LOSS [training: 3.657734061100652 | validation: 3.2539370587790923]
	TIME [epoch: 9.71 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.678245623242238		[learning rate: 0.0024417]
	Learning Rate: 0.00244165
	LOSS [training: 3.678245623242238 | validation: 3.232135945511473]
	TIME [epoch: 9.7 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.687823796840078		[learning rate: 0.0024328]
	Learning Rate: 0.00243279
	LOSS [training: 3.687823796840078 | validation: 3.1797072372252355]
	TIME [epoch: 9.69 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.667277176579799		[learning rate: 0.002424]
	Learning Rate: 0.00242396
	LOSS [training: 3.667277176579799 | validation: 3.152051546651689]
	TIME [epoch: 9.72 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6715636639427487		[learning rate: 0.0024152]
	Learning Rate: 0.00241517
	LOSS [training: 3.6715636639427487 | validation: 3.121074489674914]
	TIME [epoch: 9.69 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6572197348841917		[learning rate: 0.0024064]
	Learning Rate: 0.0024064
	LOSS [training: 3.6572197348841917 | validation: 3.140818666570158]
	TIME [epoch: 9.71 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6649410700201708		[learning rate: 0.0023977]
	Learning Rate: 0.00239767
	LOSS [training: 3.6649410700201708 | validation: 3.171458881876642]
	TIME [epoch: 9.72 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6600141553055665		[learning rate: 0.002389]
	Learning Rate: 0.00238897
	LOSS [training: 3.6600141553055665 | validation: 3.1125425778115368]
	TIME [epoch: 9.71 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.68289780707482		[learning rate: 0.0023803]
	Learning Rate: 0.0023803
	LOSS [training: 3.68289780707482 | validation: 3.211190935275737]
	TIME [epoch: 9.7 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6747143023510973		[learning rate: 0.0023717]
	Learning Rate: 0.00237166
	LOSS [training: 3.6747143023510973 | validation: 3.1545204640583036]
	TIME [epoch: 9.69 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.639858264375357		[learning rate: 0.0023631]
	Learning Rate: 0.00236305
	LOSS [training: 3.639858264375357 | validation: 3.1766682855935118]
	TIME [epoch: 9.71 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.701949918298669		[learning rate: 0.0023545]
	Learning Rate: 0.00235448
	LOSS [training: 3.701949918298669 | validation: 3.1114974935069757]
	TIME [epoch: 9.7 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6610195566441677		[learning rate: 0.0023459]
	Learning Rate: 0.00234593
	LOSS [training: 3.6610195566441677 | validation: 3.109582841184517]
	TIME [epoch: 9.7 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6938201433854765		[learning rate: 0.0023374]
	Learning Rate: 0.00233742
	LOSS [training: 3.6938201433854765 | validation: 3.15158940699914]
	TIME [epoch: 9.72 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.690421762281937		[learning rate: 0.0023289]
	Learning Rate: 0.00232894
	LOSS [training: 3.690421762281937 | validation: 3.124540507863352]
	TIME [epoch: 9.7 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7168821099659852		[learning rate: 0.0023205]
	Learning Rate: 0.00232049
	LOSS [training: 3.7168821099659852 | validation: 3.2525990119771007]
	TIME [epoch: 9.69 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7094352314418764		[learning rate: 0.0023121]
	Learning Rate: 0.00231206
	LOSS [training: 3.7094352314418764 | validation: 3.240363275373973]
	TIME [epoch: 9.72 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.652741481260301		[learning rate: 0.0023037]
	Learning Rate: 0.00230367
	LOSS [training: 3.652741481260301 | validation: 3.2651364726393086]
	TIME [epoch: 9.7 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.74740467852878		[learning rate: 0.0022953]
	Learning Rate: 0.00229531
	LOSS [training: 3.74740467852878 | validation: 3.141554531070692]
	TIME [epoch: 9.7 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.647070926247264		[learning rate: 0.002287]
	Learning Rate: 0.00228698
	LOSS [training: 3.647070926247264 | validation: 3.2467152276310327]
	TIME [epoch: 9.69 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6557475936358124		[learning rate: 0.0022787]
	Learning Rate: 0.00227868
	LOSS [training: 3.6557475936358124 | validation: 3.120765139622769]
	TIME [epoch: 9.72 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6556757829889177		[learning rate: 0.0022704]
	Learning Rate: 0.00227042
	LOSS [training: 3.6556757829889177 | validation: 3.189309281234947]
	TIME [epoch: 9.7 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.649133480894386		[learning rate: 0.0022622]
	Learning Rate: 0.00226218
	LOSS [training: 3.649133480894386 | validation: 3.331356915751936]
	TIME [epoch: 9.69 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6965661028870924		[learning rate: 0.002254]
	Learning Rate: 0.00225397
	LOSS [training: 3.6965661028870924 | validation: 3.147604557190121]
	TIME [epoch: 9.72 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.653182939539409		[learning rate: 0.0022458]
	Learning Rate: 0.00224579
	LOSS [training: 3.653182939539409 | validation: 3.1216469323363256]
	TIME [epoch: 9.72 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6618141150922177		[learning rate: 0.0022376]
	Learning Rate: 0.00223764
	LOSS [training: 3.6618141150922177 | validation: 3.136592051568262]
	TIME [epoch: 9.7 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6618193529561		[learning rate: 0.0022295]
	Learning Rate: 0.00222952
	LOSS [training: 3.6618193529561 | validation: 3.139150622278249]
	TIME [epoch: 9.7 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.660542479145228		[learning rate: 0.0022214]
	Learning Rate: 0.00222142
	LOSS [training: 3.660542479145228 | validation: 3.2086625396942146]
	TIME [epoch: 9.72 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.648585895694341		[learning rate: 0.0022134]
	Learning Rate: 0.00221336
	LOSS [training: 3.648585895694341 | validation: 3.1128885395337886]
	TIME [epoch: 9.71 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7256001373259258		[learning rate: 0.0022053]
	Learning Rate: 0.00220533
	LOSS [training: 3.7256001373259258 | validation: 3.2600228161421616]
	TIME [epoch: 9.7 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6754486383859053		[learning rate: 0.0021973]
	Learning Rate: 0.00219733
	LOSS [training: 3.6754486383859053 | validation: 3.1215858705449313]
	TIME [epoch: 9.72 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.651160712481527		[learning rate: 0.0021894]
	Learning Rate: 0.00218935
	LOSS [training: 3.651160712481527 | validation: 3.122897744694013]
	TIME [epoch: 9.7 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.648170693317146		[learning rate: 0.0021814]
	Learning Rate: 0.00218141
	LOSS [training: 3.648170693317146 | validation: 3.2411433347977905]
	TIME [epoch: 9.67 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6713427380882835		[learning rate: 0.0021735]
	Learning Rate: 0.00217349
	LOSS [training: 3.6713427380882835 | validation: 3.1117251591877095]
	TIME [epoch: 9.69 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.635634382990125		[learning rate: 0.0021656]
	Learning Rate: 0.0021656
	LOSS [training: 3.635634382990125 | validation: 3.1392510882597775]
	TIME [epoch: 9.7 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6547331013697635		[learning rate: 0.0021577]
	Learning Rate: 0.00215774
	LOSS [training: 3.6547331013697635 | validation: 3.1231443166747095]
	TIME [epoch: 9.68 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.638079886897624		[learning rate: 0.0021499]
	Learning Rate: 0.00214991
	LOSS [training: 3.638079886897624 | validation: 3.180383795546756]
	TIME [epoch: 9.68 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6438590601471317		[learning rate: 0.0021421]
	Learning Rate: 0.00214211
	LOSS [training: 3.6438590601471317 | validation: 3.4773435327171263]
	TIME [epoch: 9.71 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7721359262050327		[learning rate: 0.0021343]
	Learning Rate: 0.00213434
	LOSS [training: 3.7721359262050327 | validation: 3.1312751542999706]
	TIME [epoch: 9.69 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6615162576170333		[learning rate: 0.0021266]
	Learning Rate: 0.00212659
	LOSS [training: 3.6615162576170333 | validation: 3.1477915980372466]
	TIME [epoch: 9.69 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6900538390574944		[learning rate: 0.0021189]
	Learning Rate: 0.00211887
	LOSS [training: 3.6900538390574944 | validation: 3.1750661447293114]
	TIME [epoch: 9.72 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.676504189060453		[learning rate: 0.0021112]
	Learning Rate: 0.00211119
	LOSS [training: 3.676504189060453 | validation: 3.1766743117503764]
	TIME [epoch: 9.7 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.655414341751026		[learning rate: 0.0021035]
	Learning Rate: 0.00210352
	LOSS [training: 3.655414341751026 | validation: 3.1146910043163825]
	TIME [epoch: 9.69 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.679783199682813		[learning rate: 0.0020959]
	Learning Rate: 0.00209589
	LOSS [training: 3.679783199682813 | validation: 3.1380007516585375]
	TIME [epoch: 9.7 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6585528743684845		[learning rate: 0.0020883]
	Learning Rate: 0.00208828
	LOSS [training: 3.6585528743684845 | validation: 3.1070785831396464]
	TIME [epoch: 9.72 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6476613322533034		[learning rate: 0.0020807]
	Learning Rate: 0.00208071
	LOSS [training: 3.6476613322533034 | validation: 3.168455312214221]
	TIME [epoch: 9.69 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6513784486273466		[learning rate: 0.0020732]
	Learning Rate: 0.00207315
	LOSS [training: 3.6513784486273466 | validation: 3.1343637182020605]
	TIME [epoch: 9.69 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.662463478599379		[learning rate: 0.0020656]
	Learning Rate: 0.00206563
	LOSS [training: 3.662463478599379 | validation: 3.288055119893669]
	TIME [epoch: 9.71 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.691047801664362		[learning rate: 0.0020581]
	Learning Rate: 0.00205813
	LOSS [training: 3.691047801664362 | validation: 3.157328064421999]
	TIME [epoch: 9.7 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6423831025726323		[learning rate: 0.0020507]
	Learning Rate: 0.00205067
	LOSS [training: 3.6423831025726323 | validation: 3.1153906714461193]
	TIME [epoch: 9.69 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6501820832475724		[learning rate: 0.0020432]
	Learning Rate: 0.00204322
	LOSS [training: 3.6501820832475724 | validation: 3.2742325273964776]
	TIME [epoch: 9.69 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.692936647771165		[learning rate: 0.0020358]
	Learning Rate: 0.00203581
	LOSS [training: 3.692936647771165 | validation: 3.147088621398193]
	TIME [epoch: 9.72 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.677282676366834		[learning rate: 0.0020284]
	Learning Rate: 0.00202842
	LOSS [training: 3.677282676366834 | validation: 3.14177548916663]
	TIME [epoch: 9.69 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6324185125067494		[learning rate: 0.0020211]
	Learning Rate: 0.00202106
	LOSS [training: 3.6324185125067494 | validation: 3.165505196958562]
	TIME [epoch: 9.69 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6454734824622292		[learning rate: 0.0020137]
	Learning Rate: 0.00201372
	LOSS [training: 3.6454734824622292 | validation: 3.1753430862050926]
	TIME [epoch: 9.71 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6669637410345066		[learning rate: 0.0020064]
	Learning Rate: 0.00200642
	LOSS [training: 3.6669637410345066 | validation: 3.1449540428088474]
	TIME [epoch: 9.68 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.677630922970111		[learning rate: 0.0019991]
	Learning Rate: 0.00199913
	LOSS [training: 3.677630922970111 | validation: 3.12733455932776]
	TIME [epoch: 9.7 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6720618878811835		[learning rate: 0.0019919]
	Learning Rate: 0.00199188
	LOSS [training: 3.6720618878811835 | validation: 3.1174038457376456]
	TIME [epoch: 9.71 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.653443737717488		[learning rate: 0.0019847]
	Learning Rate: 0.00198465
	LOSS [training: 3.653443737717488 | validation: 3.1999894982878834]
	TIME [epoch: 9.7 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.640447446169797		[learning rate: 0.0019774]
	Learning Rate: 0.00197745
	LOSS [training: 3.640447446169797 | validation: 3.1672867341082394]
	TIME [epoch: 9.69 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.651500629658792		[learning rate: 0.0019703]
	Learning Rate: 0.00197027
	LOSS [training: 3.651500629658792 | validation: 3.1448923649658727]
	TIME [epoch: 9.69 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6416981626003158		[learning rate: 0.0019631]
	Learning Rate: 0.00196312
	LOSS [training: 3.6416981626003158 | validation: 3.128199831709707]
	TIME [epoch: 9.71 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.649755568809526		[learning rate: 0.001956]
	Learning Rate: 0.001956
	LOSS [training: 3.649755568809526 | validation: 3.1864144643392196]
	TIME [epoch: 9.7 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.633400024754855		[learning rate: 0.0019489]
	Learning Rate: 0.0019489
	LOSS [training: 3.633400024754855 | validation: 3.1910093680073244]
	TIME [epoch: 9.68 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.671455167532483		[learning rate: 0.0019418]
	Learning Rate: 0.00194183
	LOSS [training: 3.671455167532483 | validation: 3.21834046269798]
	TIME [epoch: 9.71 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.662093831004809		[learning rate: 0.0019348]
	Learning Rate: 0.00193478
	LOSS [training: 3.662093831004809 | validation: 3.100971935647985]
	TIME [epoch: 9.7 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6304510371288514		[learning rate: 0.0019278]
	Learning Rate: 0.00192776
	LOSS [training: 3.6304510371288514 | validation: 3.1490369087099097]
	TIME [epoch: 9.69 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.644312327892668		[learning rate: 0.0019208]
	Learning Rate: 0.00192076
	LOSS [training: 3.644312327892668 | validation: 3.1299955490996205]
	TIME [epoch: 9.69 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.645409703820895		[learning rate: 0.0019138]
	Learning Rate: 0.00191379
	LOSS [training: 3.645409703820895 | validation: 3.1934572655728726]
	TIME [epoch: 9.72 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6861307505944767		[learning rate: 0.0019068]
	Learning Rate: 0.00190685
	LOSS [training: 3.6861307505944767 | validation: 3.1817354317444053]
	TIME [epoch: 9.7 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6567797711892984		[learning rate: 0.0018999]
	Learning Rate: 0.00189993
	LOSS [training: 3.6567797711892984 | validation: 3.116076530715901]
	TIME [epoch: 9.69 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.641732695980984		[learning rate: 0.001893]
	Learning Rate: 0.00189303
	LOSS [training: 3.641732695980984 | validation: 3.130318693297833]
	TIME [epoch: 9.72 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6911148772337463		[learning rate: 0.0018862]
	Learning Rate: 0.00188616
	LOSS [training: 3.6911148772337463 | validation: 3.1690979949076534]
	TIME [epoch: 9.69 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.631988779508743		[learning rate: 0.0018793]
	Learning Rate: 0.00187932
	LOSS [training: 3.631988779508743 | validation: 3.110480034719111]
	TIME [epoch: 9.7 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.665472470916175		[learning rate: 0.0018725]
	Learning Rate: 0.0018725
	LOSS [training: 3.665472470916175 | validation: 3.180606781429079]
	TIME [epoch: 9.7 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6398217016341663		[learning rate: 0.0018657]
	Learning Rate: 0.0018657
	LOSS [training: 3.6398217016341663 | validation: 3.1216876225190253]
	TIME [epoch: 9.71 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.646646888188043		[learning rate: 0.0018589]
	Learning Rate: 0.00185893
	LOSS [training: 3.646646888188043 | validation: 3.1178290669788105]
	TIME [epoch: 9.69 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.635618867144963		[learning rate: 0.0018522]
	Learning Rate: 0.00185218
	LOSS [training: 3.635618867144963 | validation: 3.1219778954818906]
	TIME [epoch: 9.69 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6442683169979007		[learning rate: 0.0018455]
	Learning Rate: 0.00184546
	LOSS [training: 3.6442683169979007 | validation: 3.161028818703243]
	TIME [epoch: 9.72 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7321191551560524		[learning rate: 0.0018388]
	Learning Rate: 0.00183877
	LOSS [training: 3.7321191551560524 | validation: 3.1623287966552995]
	TIME [epoch: 9.7 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6425599194056475		[learning rate: 0.0018321]
	Learning Rate: 0.00183209
	LOSS [training: 3.6425599194056475 | validation: 3.1185820631041468]
	TIME [epoch: 9.7 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.65222555743169		[learning rate: 0.0018254]
	Learning Rate: 0.00182544
	LOSS [training: 3.65222555743169 | validation: 3.2850126647331646]
	TIME [epoch: 9.71 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7087168653861915		[learning rate: 0.0018188]
	Learning Rate: 0.00181882
	LOSS [training: 3.7087168653861915 | validation: 3.1242287667322515]
	TIME [epoch: 9.7 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6349338702242213		[learning rate: 0.0018122]
	Learning Rate: 0.00181222
	LOSS [training: 3.6349338702242213 | validation: 3.1649210174972007]
	TIME [epoch: 9.7 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.629181487051904		[learning rate: 0.0018056]
	Learning Rate: 0.00180564
	LOSS [training: 3.629181487051904 | validation: 3.1100987359612953]
	TIME [epoch: 9.7 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6671986962007614		[learning rate: 0.0017991]
	Learning Rate: 0.00179909
	LOSS [training: 3.6671986962007614 | validation: 3.2149281891229915]
	TIME [epoch: 9.72 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.695647810943165		[learning rate: 0.0017926]
	Learning Rate: 0.00179256
	LOSS [training: 3.695647810943165 | validation: 3.121750730692954]
	TIME [epoch: 9.7 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.629760314250828		[learning rate: 0.0017861]
	Learning Rate: 0.00178605
	LOSS [training: 3.629760314250828 | validation: 3.1107129503504463]
	TIME [epoch: 9.7 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6328412169840227		[learning rate: 0.0017796]
	Learning Rate: 0.00177957
	LOSS [training: 3.6328412169840227 | validation: 3.1404464630672346]
	TIME [epoch: 9.71 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6312680930315104		[learning rate: 0.0017731]
	Learning Rate: 0.00177311
	LOSS [training: 3.6312680930315104 | validation: 3.1678061596046247]
	TIME [epoch: 9.7 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.651423355165303		[learning rate: 0.0017667]
	Learning Rate: 0.00176668
	LOSS [training: 3.651423355165303 | validation: 3.1158192632827584]
	TIME [epoch: 9.7 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.634865065286313		[learning rate: 0.0017603]
	Learning Rate: 0.00176027
	LOSS [training: 3.634865065286313 | validation: 3.1393288323409103]
	TIME [epoch: 9.7 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6253845898952077		[learning rate: 0.0017539]
	Learning Rate: 0.00175388
	LOSS [training: 3.6253845898952077 | validation: 3.114999543546761]
	TIME [epoch: 9.71 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.619135098906324		[learning rate: 0.0017475]
	Learning Rate: 0.00174752
	LOSS [training: 3.619135098906324 | validation: 3.106876579465766]
	TIME [epoch: 9.69 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6439961768527716		[learning rate: 0.0017412]
	Learning Rate: 0.00174117
	LOSS [training: 3.6439961768527716 | validation: 3.1332765496829076]
	TIME [epoch: 9.7 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.639032527639872		[learning rate: 0.0017349]
	Learning Rate: 0.00173486
	LOSS [training: 3.639032527639872 | validation: 3.1150169809993553]
	TIME [epoch: 9.72 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.637749006730266		[learning rate: 0.0017286]
	Learning Rate: 0.00172856
	LOSS [training: 3.637749006730266 | validation: 3.1970374451549013]
	TIME [epoch: 9.7 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.66680332151959		[learning rate: 0.0017223]
	Learning Rate: 0.00172229
	LOSS [training: 3.66680332151959 | validation: 3.1072692143825864]
	TIME [epoch: 9.7 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.636331854761214		[learning rate: 0.001716]
	Learning Rate: 0.00171604
	LOSS [training: 3.636331854761214 | validation: 3.2038543958388903]
	TIME [epoch: 9.71 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.658059912418018		[learning rate: 0.0017098]
	Learning Rate: 0.00170981
	LOSS [training: 3.658059912418018 | validation: 3.1952268440009837]
	TIME [epoch: 9.71 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.67633767365311		[learning rate: 0.0017036]
	Learning Rate: 0.0017036
	LOSS [training: 3.67633767365311 | validation: 3.1391204351757422]
	TIME [epoch: 9.7 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6411632766176547		[learning rate: 0.0016974]
	Learning Rate: 0.00169742
	LOSS [training: 3.6411632766176547 | validation: 3.1107352520180562]
	TIME [epoch: 9.7 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6252929207827167		[learning rate: 0.0016913]
	Learning Rate: 0.00169126
	LOSS [training: 3.6252929207827167 | validation: 3.1391005852591634]
	TIME [epoch: 9.73 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6193812227547584		[learning rate: 0.0016851]
	Learning Rate: 0.00168512
	LOSS [training: 3.6193812227547584 | validation: 3.1092561259310845]
	TIME [epoch: 9.71 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.615946856142995		[learning rate: 0.001679]
	Learning Rate: 0.00167901
	LOSS [training: 3.615946856142995 | validation: 3.1721138270953664]
	TIME [epoch: 9.71 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6412000133695		[learning rate: 0.0016729]
	Learning Rate: 0.00167291
	LOSS [training: 3.6412000133695 | validation: 3.091191924121133]
	TIME [epoch: 9.73 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6468169390192022		[learning rate: 0.0016668]
	Learning Rate: 0.00166684
	LOSS [training: 3.6468169390192022 | validation: 3.1160825796277596]
	TIME [epoch: 9.71 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6241833151163774		[learning rate: 0.0016608]
	Learning Rate: 0.00166079
	LOSS [training: 3.6241833151163774 | validation: 3.131710279958022]
	TIME [epoch: 9.72 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6437994332901056		[learning rate: 0.0016548]
	Learning Rate: 0.00165477
	LOSS [training: 3.6437994332901056 | validation: 3.0975807970752727]
	TIME [epoch: 9.71 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6267784888604178		[learning rate: 0.0016488]
	Learning Rate: 0.00164876
	LOSS [training: 3.6267784888604178 | validation: 3.149475688632933]
	TIME [epoch: 9.73 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.671561403908804		[learning rate: 0.0016428]
	Learning Rate: 0.00164278
	LOSS [training: 3.671561403908804 | validation: 3.2057958651783514]
	TIME [epoch: 9.72 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.663986718446845		[learning rate: 0.0016368]
	Learning Rate: 0.00163682
	LOSS [training: 3.663986718446845 | validation: 3.129526354902378]
	TIME [epoch: 9.71 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.659681174634702		[learning rate: 0.0016309]
	Learning Rate: 0.00163088
	LOSS [training: 3.659681174634702 | validation: 3.2663817046365184]
	TIME [epoch: 9.74 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.660029542939013		[learning rate: 0.001625]
	Learning Rate: 0.00162496
	LOSS [training: 3.660029542939013 | validation: 3.1577054758160386]
	TIME [epoch: 9.72 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6475541290553166		[learning rate: 0.0016191]
	Learning Rate: 0.00161906
	LOSS [training: 3.6475541290553166 | validation: 3.1327455887416376]
	TIME [epoch: 9.71 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.660220154924422		[learning rate: 0.0016132]
	Learning Rate: 0.00161319
	LOSS [training: 3.660220154924422 | validation: 3.1326317808934125]
	TIME [epoch: 9.73 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6513850046859546		[learning rate: 0.0016073]
	Learning Rate: 0.00160733
	LOSS [training: 3.6513850046859546 | validation: 3.104287194048262]
	TIME [epoch: 9.72 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.627106455617753		[learning rate: 0.0016015]
	Learning Rate: 0.0016015
	LOSS [training: 3.627106455617753 | validation: 3.1401867193523247]
	TIME [epoch: 9.71 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.646382614943805		[learning rate: 0.0015957]
	Learning Rate: 0.00159569
	LOSS [training: 3.646382614943805 | validation: 3.1347184950050475]
	TIME [epoch: 9.72 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.639212859493731		[learning rate: 0.0015899]
	Learning Rate: 0.00158989
	LOSS [training: 3.639212859493731 | validation: 3.114246117076041]
	TIME [epoch: 9.74 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6290630812172457		[learning rate: 0.0015841]
	Learning Rate: 0.00158413
	LOSS [training: 3.6290630812172457 | validation: 3.122450649363543]
	TIME [epoch: 9.71 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6556475355182023		[learning rate: 0.0015784]
	Learning Rate: 0.00157838
	LOSS [training: 3.6556475355182023 | validation: 3.1182228662590155]
	TIME [epoch: 9.71 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6330535298511712		[learning rate: 0.0015726]
	Learning Rate: 0.00157265
	LOSS [training: 3.6330535298511712 | validation: 3.163897438394365]
	TIME [epoch: 9.73 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6562618770550137		[learning rate: 0.0015669]
	Learning Rate: 0.00156694
	LOSS [training: 3.6562618770550137 | validation: 3.1281649438913166]
	TIME [epoch: 9.72 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.63729649430947		[learning rate: 0.0015613]
	Learning Rate: 0.00156125
	LOSS [training: 3.63729649430947 | validation: 3.1221991189905256]
	TIME [epoch: 9.72 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.640052027904084		[learning rate: 0.0015556]
	Learning Rate: 0.00155559
	LOSS [training: 3.640052027904084 | validation: 3.1379157050658306]
	TIME [epoch: 9.72 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6249207479481043		[learning rate: 0.0015499]
	Learning Rate: 0.00154994
	LOSS [training: 3.6249207479481043 | validation: 3.1798221951861603]
	TIME [epoch: 9.74 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6574710651471647		[learning rate: 0.0015443]
	Learning Rate: 0.00154432
	LOSS [training: 3.6574710651471647 | validation: 3.1033505427847956]
	TIME [epoch: 9.72 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.638749355566504		[learning rate: 0.0015387]
	Learning Rate: 0.00153871
	LOSS [training: 3.638749355566504 | validation: 3.213748765267244]
	TIME [epoch: 9.71 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6562395454157652		[learning rate: 0.0015331]
	Learning Rate: 0.00153313
	LOSS [training: 3.6562395454157652 | validation: 3.108059846332586]
	TIME [epoch: 9.74 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6314137347214115		[learning rate: 0.0015276]
	Learning Rate: 0.00152757
	LOSS [training: 3.6314137347214115 | validation: 3.156312619274043]
	TIME [epoch: 9.72 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6371293363381376		[learning rate: 0.001522]
	Learning Rate: 0.00152202
	LOSS [training: 3.6371293363381376 | validation: 3.1072144850240218]
	TIME [epoch: 9.71 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6227676914754054		[learning rate: 0.0015165]
	Learning Rate: 0.0015165
	LOSS [training: 3.6227676914754054 | validation: 3.215130640147247]
	TIME [epoch: 9.73 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6447557741506067		[learning rate: 0.001511]
	Learning Rate: 0.001511
	LOSS [training: 3.6447557741506067 | validation: 3.1553073306008286]
	TIME [epoch: 9.72 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.658370628852988		[learning rate: 0.0015055]
	Learning Rate: 0.00150551
	LOSS [training: 3.658370628852988 | validation: 3.1363812123598174]
	TIME [epoch: 9.71 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6270718487773683		[learning rate: 0.0015]
	Learning Rate: 0.00150005
	LOSS [training: 3.6270718487773683 | validation: 3.138315726566708]
	TIME [epoch: 9.71 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6714705148331293		[learning rate: 0.0014946]
	Learning Rate: 0.0014946
	LOSS [training: 3.6714705148331293 | validation: 3.1054509572509312]
	TIME [epoch: 9.74 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.640764357128801		[learning rate: 0.0014892]
	Learning Rate: 0.00148918
	LOSS [training: 3.640764357128801 | validation: 3.134881484678682]
	TIME [epoch: 9.72 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.61867361934871		[learning rate: 0.0014838]
	Learning Rate: 0.00148378
	LOSS [training: 3.61867361934871 | validation: 3.1180335187745984]
	TIME [epoch: 9.71 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6399217653497615		[learning rate: 0.0014784]
	Learning Rate: 0.00147839
	LOSS [training: 3.6399217653497615 | validation: 3.152029423411197]
	TIME [epoch: 9.73 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6560213540306585		[learning rate: 0.001473]
	Learning Rate: 0.00147303
	LOSS [training: 3.6560213540306585 | validation: 3.143811966233272]
	TIME [epoch: 9.71 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6490141323439444		[learning rate: 0.0014677]
	Learning Rate: 0.00146768
	LOSS [training: 3.6490141323439444 | validation: 3.2043303685456097]
	TIME [epoch: 9.71 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6446231564189637		[learning rate: 0.0014624]
	Learning Rate: 0.00146235
	LOSS [training: 3.6446231564189637 | validation: 3.120781847502998]
	TIME [epoch: 9.71 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6751579958806055		[learning rate: 0.001457]
	Learning Rate: 0.00145705
	LOSS [training: 3.6751579958806055 | validation: 3.096407142835527]
	TIME [epoch: 9.73 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6178371102112266		[learning rate: 0.0014518]
	Learning Rate: 0.00145176
	LOSS [training: 3.6178371102112266 | validation: 3.156253345023072]
	TIME [epoch: 9.71 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7154418326077305		[learning rate: 0.0014465]
	Learning Rate: 0.00144649
	LOSS [training: 3.7154418326077305 | validation: 3.150746592818265]
	TIME [epoch: 9.71 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6523194933122034		[learning rate: 0.0014412]
	Learning Rate: 0.00144124
	LOSS [training: 3.6523194933122034 | validation: 3.140386650882715]
	TIME [epoch: 9.73 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6291111476859905		[learning rate: 0.001436]
	Learning Rate: 0.00143601
	LOSS [training: 3.6291111476859905 | validation: 3.1273175776816893]
	TIME [epoch: 9.71 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.633235517069474		[learning rate: 0.0014308]
	Learning Rate: 0.0014308
	LOSS [training: 3.633235517069474 | validation: 3.105453541815448]
	TIME [epoch: 9.7 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6269790965147948		[learning rate: 0.0014256]
	Learning Rate: 0.00142561
	LOSS [training: 3.6269790965147948 | validation: 3.212327142489132]
	TIME [epoch: 9.71 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6707976907225834		[learning rate: 0.0014204]
	Learning Rate: 0.00142043
	LOSS [training: 3.6707976907225834 | validation: 3.1007133576895427]
	TIME [epoch: 9.73 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.624865059502843		[learning rate: 0.0014153]
	Learning Rate: 0.00141528
	LOSS [training: 3.624865059502843 | validation: 3.1467831922968332]
	TIME [epoch: 9.69 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.635986006428201		[learning rate: 0.0014101]
	Learning Rate: 0.00141014
	LOSS [training: 3.635986006428201 | validation: 3.1517486184887016]
	TIME [epoch: 9.71 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6435842616248637		[learning rate: 0.001405]
	Learning Rate: 0.00140503
	LOSS [training: 3.6435842616248637 | validation: 3.118588609049679]
	TIME [epoch: 9.72 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6731039636719287		[learning rate: 0.0013999]
	Learning Rate: 0.00139993
	LOSS [training: 3.6731039636719287 | validation: 3.1037618662138007]
	TIME [epoch: 9.71 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6195924351334177		[learning rate: 0.0013948]
	Learning Rate: 0.00139485
	LOSS [training: 3.6195924351334177 | validation: 3.0993678290979934]
	TIME [epoch: 9.72 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.631669276387812		[learning rate: 0.0013898]
	Learning Rate: 0.00138978
	LOSS [training: 3.631669276387812 | validation: 3.1023638508348426]
	TIME [epoch: 9.72 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.625493073448294		[learning rate: 0.0013847]
	Learning Rate: 0.00138474
	LOSS [training: 3.625493073448294 | validation: 3.1291392967058096]
	TIME [epoch: 9.71 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6245240435094126		[learning rate: 0.0013797]
	Learning Rate: 0.00137972
	LOSS [training: 3.6245240435094126 | validation: 3.109983541675345]
	TIME [epoch: 9.7 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.612338400763574		[learning rate: 0.0013747]
	Learning Rate: 0.00137471
	LOSS [training: 3.612338400763574 | validation: 3.11418659033012]
	TIME [epoch: 9.71 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.629926370677952		[learning rate: 0.0013697]
	Learning Rate: 0.00136972
	LOSS [training: 3.629926370677952 | validation: 3.172750042940172]
	TIME [epoch: 9.73 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6674030601069254		[learning rate: 0.0013647]
	Learning Rate: 0.00136475
	LOSS [training: 3.6674030601069254 | validation: 3.1351991594842668]
	TIME [epoch: 9.7 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6714855302090745		[learning rate: 0.0013598]
	Learning Rate: 0.0013598
	LOSS [training: 3.6714855302090745 | validation: 3.1532242722536385]
	TIME [epoch: 9.69 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.646192927415238		[learning rate: 0.0013549]
	Learning Rate: 0.00135486
	LOSS [training: 3.646192927415238 | validation: 3.1022414303274166]
	TIME [epoch: 9.72 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6724758461005598		[learning rate: 0.0013499]
	Learning Rate: 0.00134994
	LOSS [training: 3.6724758461005598 | validation: 3.0925669909066675]
	TIME [epoch: 9.71 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.640421756644158		[learning rate: 0.001345]
	Learning Rate: 0.00134505
	LOSS [training: 3.640421756644158 | validation: 3.1709168248890593]
	TIME [epoch: 9.69 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6898741228668754		[learning rate: 0.0013402]
	Learning Rate: 0.00134016
	LOSS [training: 3.6898741228668754 | validation: 3.111306797366708]
	TIME [epoch: 9.71 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6440308036662588		[learning rate: 0.0013353]
	Learning Rate: 0.0013353
	LOSS [training: 3.6440308036662588 | validation: 3.107492762957504]
	TIME [epoch: 9.71 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6136388635041667		[learning rate: 0.0013305]
	Learning Rate: 0.00133045
	LOSS [training: 3.6136388635041667 | validation: 3.0924072149915713]
	TIME [epoch: 9.71 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6235788179631605		[learning rate: 0.0013256]
	Learning Rate: 0.00132563
	LOSS [training: 3.6235788179631605 | validation: 3.1318257624256036]
	TIME [epoch: 9.7 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6408629879659307		[learning rate: 0.0013208]
	Learning Rate: 0.00132082
	LOSS [training: 3.6408629879659307 | validation: 3.209398814491811]
	TIME [epoch: 9.72 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6685101572308483		[learning rate: 0.001316]
	Learning Rate: 0.00131602
	LOSS [training: 3.6685101572308483 | validation: 3.1185698390856875]
	TIME [epoch: 9.7 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6268923687842394		[learning rate: 0.0013112]
	Learning Rate: 0.00131125
	LOSS [training: 3.6268923687842394 | validation: 3.1533386581111227]
	TIME [epoch: 9.7 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.643625382141584		[learning rate: 0.0013065]
	Learning Rate: 0.00130649
	LOSS [training: 3.643625382141584 | validation: 3.119680891571264]
	TIME [epoch: 9.72 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6264974101609178		[learning rate: 0.0013017]
	Learning Rate: 0.00130175
	LOSS [training: 3.6264974101609178 | validation: 3.0957741944422765]
	TIME [epoch: 9.71 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6250689721923286		[learning rate: 0.001297]
	Learning Rate: 0.00129702
	LOSS [training: 3.6250689721923286 | validation: 3.098107822685673]
	TIME [epoch: 9.7 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6548378471554805		[learning rate: 0.0012923]
	Learning Rate: 0.00129232
	LOSS [training: 3.6548378471554805 | validation: 3.112140857360878]
	TIME [epoch: 9.7 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.621266547175547		[learning rate: 0.0012876]
	Learning Rate: 0.00128763
	LOSS [training: 3.621266547175547 | validation: 3.1179314580411637]
	TIME [epoch: 9.72 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.640992974101498		[learning rate: 0.001283]
	Learning Rate: 0.00128295
	LOSS [training: 3.640992974101498 | validation: 3.1755303319199584]
	TIME [epoch: 9.7 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6424328408141173		[learning rate: 0.0012783]
	Learning Rate: 0.0012783
	LOSS [training: 3.6424328408141173 | validation: 3.1245971333565574]
	TIME [epoch: 9.7 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6107692622550855		[learning rate: 0.0012737]
	Learning Rate: 0.00127366
	LOSS [training: 3.6107692622550855 | validation: 3.1402995304937225]
	TIME [epoch: 9.72 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6441107562833563		[learning rate: 0.001269]
	Learning Rate: 0.00126904
	LOSS [training: 3.6441107562833563 | validation: 3.092079345360125]
	TIME [epoch: 9.71 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6100498893300483		[learning rate: 0.0012644]
	Learning Rate: 0.00126443
	LOSS [training: 3.6100498893300483 | validation: 3.092164863511454]
	TIME [epoch: 9.7 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.614514447599876		[learning rate: 0.0012598]
	Learning Rate: 0.00125984
	LOSS [training: 3.614514447599876 | validation: 3.0935511697617857]
	TIME [epoch: 9.71 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6202067252878023		[learning rate: 0.0012553]
	Learning Rate: 0.00125527
	LOSS [training: 3.6202067252878023 | validation: 3.1066064815855463]
	TIME [epoch: 9.71 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.618626843892829		[learning rate: 0.0012507]
	Learning Rate: 0.00125071
	LOSS [training: 3.618626843892829 | validation: 3.1129820591247377]
	TIME [epoch: 9.69 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6330538284605347		[learning rate: 0.0012462]
	Learning Rate: 0.00124617
	LOSS [training: 3.6330538284605347 | validation: 3.177053795663157]
	TIME [epoch: 9.7 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6255893506052694		[learning rate: 0.0012417]
	Learning Rate: 0.00124165
	LOSS [training: 3.6255893506052694 | validation: 3.1292597485140323]
	TIME [epoch: 9.72 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.64690137251428		[learning rate: 0.0012371]
	Learning Rate: 0.00123715
	LOSS [training: 3.64690137251428 | validation: 3.0897227736139956]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_675.pth
	Model improved!!!
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.64866115134282		[learning rate: 0.0012327]
	Learning Rate: 0.00123266
	LOSS [training: 3.64866115134282 | validation: 3.1117422671505586]
	TIME [epoch: 9.69 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.631398699085692		[learning rate: 0.0012282]
	Learning Rate: 0.00122818
	LOSS [training: 3.631398699085692 | validation: 3.100213753022232]
	TIME [epoch: 9.7 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6332466439434383		[learning rate: 0.0012237]
	Learning Rate: 0.00122373
	LOSS [training: 3.6332466439434383 | validation: 3.1125200653623417]
	TIME [epoch: 9.7 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6154499235684647		[learning rate: 0.0012193]
	Learning Rate: 0.00121929
	LOSS [training: 3.6154499235684647 | validation: 3.074894914450598]
	TIME [epoch: 9.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_679.pth
	Model improved!!!
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6476473578414628		[learning rate: 0.0012149]
	Learning Rate: 0.00121486
	LOSS [training: 3.6476473578414628 | validation: 3.099779949076532]
	TIME [epoch: 9.7 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.654348381353368		[learning rate: 0.0012105]
	Learning Rate: 0.00121045
	LOSS [training: 3.654348381353368 | validation: 3.100543662369393]
	TIME [epoch: 9.72 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6263695210647753		[learning rate: 0.0012061]
	Learning Rate: 0.00120606
	LOSS [training: 3.6263695210647753 | validation: 3.0996494178571075]
	TIME [epoch: 9.69 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.629908820983328		[learning rate: 0.0012017]
	Learning Rate: 0.00120168
	LOSS [training: 3.629908820983328 | validation: 3.1593415205633906]
	TIME [epoch: 9.7 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.658942900456205		[learning rate: 0.0011973]
	Learning Rate: 0.00119732
	LOSS [training: 3.658942900456205 | validation: 3.138805687552318]
	TIME [epoch: 9.72 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6173131320086513		[learning rate: 0.001193]
	Learning Rate: 0.00119298
	LOSS [training: 3.6173131320086513 | validation: 3.1202429828900984]
	TIME [epoch: 9.7 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.637015384517107		[learning rate: 0.0011886]
	Learning Rate: 0.00118865
	LOSS [training: 3.637015384517107 | validation: 3.1030980294363486]
	TIME [epoch: 9.7 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6355148743608074		[learning rate: 0.0011843]
	Learning Rate: 0.00118433
	LOSS [training: 3.6355148743608074 | validation: 3.1260743250484677]
	TIME [epoch: 9.7 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6203896211008475		[learning rate: 0.00118]
	Learning Rate: 0.00118003
	LOSS [training: 3.6203896211008475 | validation: 3.1083415285291554]
	TIME [epoch: 9.72 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6261792312006316		[learning rate: 0.0011758]
	Learning Rate: 0.00117575
	LOSS [training: 3.6261792312006316 | validation: 3.1210869871609237]
	TIME [epoch: 9.7 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6221836668984806		[learning rate: 0.0011715]
	Learning Rate: 0.00117149
	LOSS [training: 3.6221836668984806 | validation: 3.09396534499407]
	TIME [epoch: 9.7 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.626845921418732		[learning rate: 0.0011672]
	Learning Rate: 0.00116723
	LOSS [training: 3.626845921418732 | validation: 3.0929415930007567]
	TIME [epoch: 9.72 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6463508202796673		[learning rate: 0.001163]
	Learning Rate: 0.001163
	LOSS [training: 3.6463508202796673 | validation: 3.086756472795372]
	TIME [epoch: 9.69 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6293806978818823		[learning rate: 0.0011588]
	Learning Rate: 0.00115878
	LOSS [training: 3.6293806978818823 | validation: 3.104210865104582]
	TIME [epoch: 9.7 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6445236244843215		[learning rate: 0.0011546]
	Learning Rate: 0.00115457
	LOSS [training: 3.6445236244843215 | validation: 3.180390618647223]
	TIME [epoch: 9.72 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6490852226799326		[learning rate: 0.0011504]
	Learning Rate: 0.00115038
	LOSS [training: 3.6490852226799326 | validation: 3.081290046966657]
	TIME [epoch: 9.71 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6259460141318813		[learning rate: 0.0011462]
	Learning Rate: 0.00114621
	LOSS [training: 3.6259460141318813 | validation: 3.085062133311447]
	TIME [epoch: 9.7 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.63372532952517		[learning rate: 0.001142]
	Learning Rate: 0.00114205
	LOSS [training: 3.63372532952517 | validation: 3.1811471914228453]
	TIME [epoch: 9.69 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.635003509900146		[learning rate: 0.0011379]
	Learning Rate: 0.0011379
	LOSS [training: 3.635003509900146 | validation: 3.0984565892399836]
	TIME [epoch: 9.71 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.622142350085819		[learning rate: 0.0011338]
	Learning Rate: 0.00113377
	LOSS [training: 3.622142350085819 | validation: 3.106226490379244]
	TIME [epoch: 9.69 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6294105019086045		[learning rate: 0.0011297]
	Learning Rate: 0.00112966
	LOSS [training: 3.6294105019086045 | validation: 3.1303451700795892]
	TIME [epoch: 9.69 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6194327061523905		[learning rate: 0.0011256]
	Learning Rate: 0.00112556
	LOSS [training: 3.6194327061523905 | validation: 3.092289111678878]
	TIME [epoch: 9.71 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.607808286623521		[learning rate: 0.0011215]
	Learning Rate: 0.00112147
	LOSS [training: 3.607808286623521 | validation: 3.099233719211259]
	TIME [epoch: 9.7 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.624141352390981		[learning rate: 0.0011174]
	Learning Rate: 0.0011174
	LOSS [training: 3.624141352390981 | validation: 3.096965662122274]
	TIME [epoch: 9.7 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.617224511133256		[learning rate: 0.0011133]
	Learning Rate: 0.00111335
	LOSS [training: 3.617224511133256 | validation: 3.0994375056828254]
	TIME [epoch: 9.71 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.620504719492196		[learning rate: 0.0011093]
	Learning Rate: 0.00110931
	LOSS [training: 3.620504719492196 | validation: 3.1161761607862672]
	TIME [epoch: 9.71 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.626621050780318		[learning rate: 0.0011053]
	Learning Rate: 0.00110528
	LOSS [training: 3.626621050780318 | validation: 3.093540369398182]
	TIME [epoch: 9.69 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.638202584212171		[learning rate: 0.0011013]
	Learning Rate: 0.00110127
	LOSS [training: 3.638202584212171 | validation: 3.13922948147269]
	TIME [epoch: 9.7 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.627835437551089		[learning rate: 0.0010973]
	Learning Rate: 0.00109728
	LOSS [training: 3.627835437551089 | validation: 3.139453351087549]
	TIME [epoch: 9.71 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.619957165075737		[learning rate: 0.0010933]
	Learning Rate: 0.00109329
	LOSS [training: 3.619957165075737 | validation: 3.125507000113936]
	TIME [epoch: 9.69 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6219322375840868		[learning rate: 0.0010893]
	Learning Rate: 0.00108933
	LOSS [training: 3.6219322375840868 | validation: 3.1188240709998922]
	TIME [epoch: 9.69 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6340630684720936		[learning rate: 0.0010854]
	Learning Rate: 0.00108537
	LOSS [training: 3.6340630684720936 | validation: 3.1050362803806064]
	TIME [epoch: 9.7 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6366756325989336		[learning rate: 0.0010814]
	Learning Rate: 0.00108143
	LOSS [training: 3.6366756325989336 | validation: 3.1160997668675807]
	TIME [epoch: 9.69 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.623255650639327		[learning rate: 0.0010775]
	Learning Rate: 0.00107751
	LOSS [training: 3.623255650639327 | validation: 3.0962658584155167]
	TIME [epoch: 9.69 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.632088397461446		[learning rate: 0.0010736]
	Learning Rate: 0.0010736
	LOSS [training: 3.632088397461446 | validation: 3.140370201785734]
	TIME [epoch: 9.69 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.664434819719838		[learning rate: 0.0010697]
	Learning Rate: 0.0010697
	LOSS [training: 3.664434819719838 | validation: 3.1686925092274194]
	TIME [epoch: 9.71 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6329640373034735		[learning rate: 0.0010658]
	Learning Rate: 0.00106582
	LOSS [training: 3.6329640373034735 | validation: 3.1299825723331005]
	TIME [epoch: 9.69 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.663213993912952		[learning rate: 0.001062]
	Learning Rate: 0.00106195
	LOSS [training: 3.663213993912952 | validation: 3.1802287838968915]
	TIME [epoch: 9.69 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6262206594857815		[learning rate: 0.0010581]
	Learning Rate: 0.0010581
	LOSS [training: 3.6262206594857815 | validation: 3.1238714545421646]
	TIME [epoch: 9.71 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.609154465173919		[learning rate: 0.0010543]
	Learning Rate: 0.00105426
	LOSS [training: 3.609154465173919 | validation: 3.1065028308777825]
	TIME [epoch: 9.7 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.625558981055041		[learning rate: 0.0010504]
	Learning Rate: 0.00105043
	LOSS [training: 3.625558981055041 | validation: 3.0933330427586725]
	TIME [epoch: 9.69 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6116899800839533		[learning rate: 0.0010466]
	Learning Rate: 0.00104662
	LOSS [training: 3.6116899800839533 | validation: 3.0810245012498036]
	TIME [epoch: 9.69 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6228725574246448		[learning rate: 0.0010428]
	Learning Rate: 0.00104282
	LOSS [training: 3.6228725574246448 | validation: 3.0858608907130116]
	TIME [epoch: 9.72 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6683512986801032		[learning rate: 0.001039]
	Learning Rate: 0.00103904
	LOSS [training: 3.6683512986801032 | validation: 3.093213802106455]
	TIME [epoch: 9.69 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6326322730864513		[learning rate: 0.0010353]
	Learning Rate: 0.00103527
	LOSS [training: 3.6326322730864513 | validation: 3.091167960023896]
	TIME [epoch: 9.69 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6220926209990645		[learning rate: 0.0010315]
	Learning Rate: 0.00103151
	LOSS [training: 3.6220926209990645 | validation: 3.091734382828937]
	TIME [epoch: 9.72 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6241094461249936		[learning rate: 0.0010278]
	Learning Rate: 0.00102777
	LOSS [training: 3.6241094461249936 | validation: 3.116166977837902]
	TIME [epoch: 9.69 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6604064921051007		[learning rate: 0.001024]
	Learning Rate: 0.00102404
	LOSS [training: 3.6604064921051007 | validation: 3.1367673545011643]
	TIME [epoch: 9.69 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60853275773216		[learning rate: 0.0010203]
	Learning Rate: 0.00102032
	LOSS [training: 3.60853275773216 | validation: 3.095239339992192]
	TIME [epoch: 9.7 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.622964328192185		[learning rate: 0.0010166]
	Learning Rate: 0.00101662
	LOSS [training: 3.622964328192185 | validation: 3.0939990656987484]
	TIME [epoch: 9.71 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6698296800840624		[learning rate: 0.0010129]
	Learning Rate: 0.00101293
	LOSS [training: 3.6698296800840624 | validation: 3.139263668017802]
	TIME [epoch: 9.69 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6399766493203236		[learning rate: 0.0010093]
	Learning Rate: 0.00100925
	LOSS [training: 3.6399766493203236 | validation: 3.1515941990162144]
	TIME [epoch: 9.69 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6308789600723985		[learning rate: 0.0010056]
	Learning Rate: 0.00100559
	LOSS [training: 3.6308789600723985 | validation: 3.2488407770768344]
	TIME [epoch: 9.71 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6615975757875447		[learning rate: 0.0010019]
	Learning Rate: 0.00100194
	LOSS [training: 3.6615975757875447 | validation: 3.1229008564385556]
	TIME [epoch: 9.69 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6241681881073546		[learning rate: 0.0009983]
	Learning Rate: 0.000998305
	LOSS [training: 3.6241681881073546 | validation: 3.0891068398402264]
	TIME [epoch: 9.69 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6223958876314755		[learning rate: 0.00099468]
	Learning Rate: 0.000994682
	LOSS [training: 3.6223958876314755 | validation: 3.1518790360739106]
	TIME [epoch: 9.71 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6166631278466026		[learning rate: 0.00099107]
	Learning Rate: 0.000991072
	LOSS [training: 3.6166631278466026 | validation: 3.1116576832713587]
	TIME [epoch: 9.69 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.61514045137788		[learning rate: 0.00098748]
	Learning Rate: 0.000987475
	LOSS [training: 3.61514045137788 | validation: 3.1397954087994298]
	TIME [epoch: 9.69 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.628208599662625		[learning rate: 0.00098389]
	Learning Rate: 0.000983892
	LOSS [training: 3.628208599662625 | validation: 3.1522906674544227]
	TIME [epoch: 9.69 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.621359443197239		[learning rate: 0.00098032]
	Learning Rate: 0.000980321
	LOSS [training: 3.621359443197239 | validation: 3.092375559856421]
	TIME [epoch: 9.71 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6180722618534773		[learning rate: 0.00097676]
	Learning Rate: 0.000976764
	LOSS [training: 3.6180722618534773 | validation: 3.103689480955746]
	TIME [epoch: 9.69 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.630451933185637		[learning rate: 0.00097322]
	Learning Rate: 0.000973219
	LOSS [training: 3.630451933185637 | validation: 3.125016069878621]
	TIME [epoch: 9.69 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6130955612462463		[learning rate: 0.00096969]
	Learning Rate: 0.000969687
	LOSS [training: 3.6130955612462463 | validation: 3.106475333617551]
	TIME [epoch: 9.71 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6168110901158643		[learning rate: 0.00096617]
	Learning Rate: 0.000966168
	LOSS [training: 3.6168110901158643 | validation: 3.1383158546398593]
	TIME [epoch: 9.7 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6369507338314646		[learning rate: 0.00096266]
	Learning Rate: 0.000962662
	LOSS [training: 3.6369507338314646 | validation: 3.094695535137614]
	TIME [epoch: 9.69 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.629294446134318		[learning rate: 0.00095917]
	Learning Rate: 0.000959168
	LOSS [training: 3.629294446134318 | validation: 3.1761264167385064]
	TIME [epoch: 9.7 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.628382229523811		[learning rate: 0.00095569]
	Learning Rate: 0.000955687
	LOSS [training: 3.628382229523811 | validation: 3.0937129541958037]
	TIME [epoch: 9.71 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6182077468109584		[learning rate: 0.00095222]
	Learning Rate: 0.000952219
	LOSS [training: 3.6182077468109584 | validation: 3.129181919994287]
	TIME [epoch: 9.69 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6570007938352598		[learning rate: 0.00094876]
	Learning Rate: 0.000948763
	LOSS [training: 3.6570007938352598 | validation: 3.096627446224186]
	TIME [epoch: 9.69 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6282144362682844		[learning rate: 0.00094532]
	Learning Rate: 0.00094532
	LOSS [training: 3.6282144362682844 | validation: 3.089505681409219]
	TIME [epoch: 9.72 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60592659738381		[learning rate: 0.00094189]
	Learning Rate: 0.000941889
	LOSS [training: 3.60592659738381 | validation: 3.098414366743772]
	TIME [epoch: 9.69 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.615023804037521		[learning rate: 0.00093847]
	Learning Rate: 0.000938471
	LOSS [training: 3.615023804037521 | validation: 3.119315522067085]
	TIME [epoch: 9.69 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6299486858451133		[learning rate: 0.00093507]
	Learning Rate: 0.000935066
	LOSS [training: 3.6299486858451133 | validation: 3.0939840036266366]
	TIME [epoch: 9.7 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6383918516889486		[learning rate: 0.00093167]
	Learning Rate: 0.000931672
	LOSS [training: 3.6383918516889486 | validation: 3.1403610734143443]
	TIME [epoch: 9.7 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6151930110302573		[learning rate: 0.00092829]
	Learning Rate: 0.000928291
	LOSS [training: 3.6151930110302573 | validation: 3.1141369332568383]
	TIME [epoch: 9.69 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6184768720246376		[learning rate: 0.00092492]
	Learning Rate: 0.000924922
	LOSS [training: 3.6184768720246376 | validation: 3.114999823958334]
	TIME [epoch: 9.7 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.68582076999339		[learning rate: 0.00092157]
	Learning Rate: 0.000921566
	LOSS [training: 3.68582076999339 | validation: 3.167507743793004]
	TIME [epoch: 9.72 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.639487675361994		[learning rate: 0.00091822]
	Learning Rate: 0.000918221
	LOSS [training: 3.639487675361994 | validation: 3.0897506424414387]
	TIME [epoch: 9.7 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6743260187151945		[learning rate: 0.00091489]
	Learning Rate: 0.000914889
	LOSS [training: 3.6743260187151945 | validation: 3.1141488047711654]
	TIME [epoch: 9.69 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6198331717665004		[learning rate: 0.00091157]
	Learning Rate: 0.000911569
	LOSS [training: 3.6198331717665004 | validation: 3.1118472392574903]
	TIME [epoch: 9.72 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6069282208526774		[learning rate: 0.00090826]
	Learning Rate: 0.000908261
	LOSS [training: 3.6069282208526774 | validation: 3.1016983982808344]
	TIME [epoch: 9.7 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6148670073004743		[learning rate: 0.00090496]
	Learning Rate: 0.000904965
	LOSS [training: 3.6148670073004743 | validation: 3.0821410644704277]
	TIME [epoch: 9.7 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.618974120668932		[learning rate: 0.00090168]
	Learning Rate: 0.00090168
	LOSS [training: 3.618974120668932 | validation: 3.100475791127473]
	TIME [epoch: 9.7 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.619831012144489		[learning rate: 0.00089841]
	Learning Rate: 0.000898408
	LOSS [training: 3.619831012144489 | validation: 3.1111519951548554]
	TIME [epoch: 9.71 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.619006449471128		[learning rate: 0.00089515]
	Learning Rate: 0.000895148
	LOSS [training: 3.619006449471128 | validation: 3.1105375199382355]
	TIME [epoch: 9.69 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.624522894868987		[learning rate: 0.0008919]
	Learning Rate: 0.000891899
	LOSS [training: 3.624522894868987 | validation: 3.107271073128876]
	TIME [epoch: 9.7 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6102074520153975		[learning rate: 0.00088866]
	Learning Rate: 0.000888663
	LOSS [training: 3.6102074520153975 | validation: 3.120572748052904]
	TIME [epoch: 9.73 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.628706961281136		[learning rate: 0.00088544]
	Learning Rate: 0.000885438
	LOSS [training: 3.628706961281136 | validation: 3.1466127425932324]
	TIME [epoch: 9.7 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6263875063861306		[learning rate: 0.00088222]
	Learning Rate: 0.000882224
	LOSS [training: 3.6263875063861306 | validation: 3.1213128568217816]
	TIME [epoch: 9.7 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6394784360297883		[learning rate: 0.00087902]
	Learning Rate: 0.000879022
	LOSS [training: 3.6394784360297883 | validation: 3.0985402986482815]
	TIME [epoch: 9.72 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.622829580410118		[learning rate: 0.00087583]
	Learning Rate: 0.000875833
	LOSS [training: 3.622829580410118 | validation: 3.1309550348645123]
	TIME [epoch: 9.71 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6496163873871934		[learning rate: 0.00087265]
	Learning Rate: 0.000872654
	LOSS [training: 3.6496163873871934 | validation: 3.1055841964970154]
	TIME [epoch: 9.7 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.619685148355007		[learning rate: 0.00086949]
	Learning Rate: 0.000869487
	LOSS [training: 3.619685148355007 | validation: 3.1367168610818137]
	TIME [epoch: 9.69 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6175560233596307		[learning rate: 0.00086633]
	Learning Rate: 0.000866332
	LOSS [training: 3.6175560233596307 | validation: 3.1054558823077305]
	TIME [epoch: 9.73 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.608734582920436		[learning rate: 0.00086319]
	Learning Rate: 0.000863188
	LOSS [training: 3.608734582920436 | validation: 3.0815665836875654]
	TIME [epoch: 9.7 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.617671260485077		[learning rate: 0.00086006]
	Learning Rate: 0.000860055
	LOSS [training: 3.617671260485077 | validation: 3.1268731616481795]
	TIME [epoch: 9.69 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6206926144091853		[learning rate: 0.00085693]
	Learning Rate: 0.000856934
	LOSS [training: 3.6206926144091853 | validation: 3.0907345875051373]
	TIME [epoch: 9.72 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6307283411263844		[learning rate: 0.00085382]
	Learning Rate: 0.000853824
	LOSS [training: 3.6307283411263844 | validation: 3.115296965394923]
	TIME [epoch: 9.7 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.612089254572104		[learning rate: 0.00085073]
	Learning Rate: 0.000850726
	LOSS [training: 3.612089254572104 | validation: 3.115470537854167]
	TIME [epoch: 9.7 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.634292005394079		[learning rate: 0.00084764]
	Learning Rate: 0.000847638
	LOSS [training: 3.634292005394079 | validation: 3.1049876655016164]
	TIME [epoch: 9.7 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6072704291450313		[learning rate: 0.00084456]
	Learning Rate: 0.000844562
	LOSS [training: 3.6072704291450313 | validation: 3.1029327733034644]
	TIME [epoch: 9.73 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.638010902736057		[learning rate: 0.0008415]
	Learning Rate: 0.000841497
	LOSS [training: 3.638010902736057 | validation: 3.1097156340965744]
	TIME [epoch: 9.7 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.62431030349524		[learning rate: 0.00083844]
	Learning Rate: 0.000838443
	LOSS [training: 3.62431030349524 | validation: 3.130912338295742]
	TIME [epoch: 9.7 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.638833327773842		[learning rate: 0.0008354]
	Learning Rate: 0.000835401
	LOSS [training: 3.638833327773842 | validation: 3.135819995019219]
	TIME [epoch: 9.72 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.61759988210313		[learning rate: 0.00083237]
	Learning Rate: 0.000832369
	LOSS [training: 3.61759988210313 | validation: 3.093452758815753]
	TIME [epoch: 9.71 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6306031807282926		[learning rate: 0.00082935]
	Learning Rate: 0.000829348
	LOSS [training: 3.6306031807282926 | validation: 3.11482579677253]
	TIME [epoch: 9.7 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6155339935821402		[learning rate: 0.00082634]
	Learning Rate: 0.000826338
	LOSS [training: 3.6155339935821402 | validation: 3.0974301380541966]
	TIME [epoch: 9.7 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.612232005526839		[learning rate: 0.00082334]
	Learning Rate: 0.00082334
	LOSS [training: 3.612232005526839 | validation: 3.0978753552147884]
	TIME [epoch: 9.72 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6243714422751494		[learning rate: 0.00082035]
	Learning Rate: 0.000820352
	LOSS [training: 3.6243714422751494 | validation: 3.0960228359029145]
	TIME [epoch: 9.7 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.618121213808847		[learning rate: 0.00081737]
	Learning Rate: 0.000817375
	LOSS [training: 3.618121213808847 | validation: 3.095387365623808]
	TIME [epoch: 9.7 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6045166443603875		[learning rate: 0.00081441]
	Learning Rate: 0.000814408
	LOSS [training: 3.6045166443603875 | validation: 3.117708812223231]
	TIME [epoch: 9.72 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6181839605840933		[learning rate: 0.00081145]
	Learning Rate: 0.000811453
	LOSS [training: 3.6181839605840933 | validation: 3.1011628087495837]
	TIME [epoch: 9.71 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.624285807948181		[learning rate: 0.00080851]
	Learning Rate: 0.000808508
	LOSS [training: 3.624285807948181 | validation: 3.090320040667782]
	TIME [epoch: 9.7 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6154903595428656		[learning rate: 0.00080557]
	Learning Rate: 0.000805574
	LOSS [training: 3.6154903595428656 | validation: 3.1269318192073805]
	TIME [epoch: 9.73 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6382500327414		[learning rate: 0.00080265]
	Learning Rate: 0.00080265
	LOSS [training: 3.6382500327414 | validation: 3.094124439086538]
	TIME [epoch: 9.71 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6119588171262316		[learning rate: 0.00079974]
	Learning Rate: 0.000799737
	LOSS [training: 3.6119588171262316 | validation: 3.1076525585681636]
	TIME [epoch: 9.7 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6152980168586417		[learning rate: 0.00079684]
	Learning Rate: 0.000796835
	LOSS [training: 3.6152980168586417 | validation: 3.0845566606399757]
	TIME [epoch: 9.7 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6024598931841014		[learning rate: 0.00079394]
	Learning Rate: 0.000793943
	LOSS [training: 3.6024598931841014 | validation: 3.110145226751133]
	TIME [epoch: 9.72 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6171347206240796		[learning rate: 0.00079106]
	Learning Rate: 0.000791062
	LOSS [training: 3.6171347206240796 | validation: 3.114610326652289]
	TIME [epoch: 9.7 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6246189970975458		[learning rate: 0.00078819]
	Learning Rate: 0.000788191
	LOSS [training: 3.6246189970975458 | validation: 3.1225045622297687]
	TIME [epoch: 9.7 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6353085023794494		[learning rate: 0.00078533]
	Learning Rate: 0.000785331
	LOSS [training: 3.6353085023794494 | validation: 3.1255081849271598]
	TIME [epoch: 9.72 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.632510205823535		[learning rate: 0.00078248]
	Learning Rate: 0.000782481
	LOSS [training: 3.632510205823535 | validation: 3.1137890708786995]
	TIME [epoch: 9.7 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.614894786526206		[learning rate: 0.00077964]
	Learning Rate: 0.000779641
	LOSS [training: 3.614894786526206 | validation: 3.092840327419558]
	TIME [epoch: 9.71 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6080949428699376		[learning rate: 0.00077681]
	Learning Rate: 0.000776812
	LOSS [training: 3.6080949428699376 | validation: 3.095253805031073]
	TIME [epoch: 9.72 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.610953409929131		[learning rate: 0.00077399]
	Learning Rate: 0.000773993
	LOSS [training: 3.610953409929131 | validation: 3.1109640351866505]
	TIME [epoch: 9.72 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6164717864634355		[learning rate: 0.00077118]
	Learning Rate: 0.000771184
	LOSS [training: 3.6164717864634355 | validation: 3.102221831361136]
	TIME [epoch: 9.71 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6297449221037636		[learning rate: 0.00076839]
	Learning Rate: 0.000768385
	LOSS [training: 3.6297449221037636 | validation: 3.1409753448247786]
	TIME [epoch: 9.7 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.640055216231736		[learning rate: 0.0007656]
	Learning Rate: 0.000765597
	LOSS [training: 3.640055216231736 | validation: 3.128417591945842]
	TIME [epoch: 9.73 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6158791561064247		[learning rate: 0.00076282]
	Learning Rate: 0.000762818
	LOSS [training: 3.6158791561064247 | validation: 3.0951497958711585]
	TIME [epoch: 9.7 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.622694610109913		[learning rate: 0.00076005]
	Learning Rate: 0.00076005
	LOSS [training: 3.622694610109913 | validation: 3.1004971591107235]
	TIME [epoch: 9.7 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6162934108023306		[learning rate: 0.00075729]
	Learning Rate: 0.000757292
	LOSS [training: 3.6162934108023306 | validation: 3.097430648343674]
	TIME [epoch: 9.72 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6198150588034084		[learning rate: 0.00075454]
	Learning Rate: 0.000754543
	LOSS [training: 3.6198150588034084 | validation: 3.100753351146236]
	TIME [epoch: 9.72 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6321266133164065		[learning rate: 0.00075181]
	Learning Rate: 0.000751805
	LOSS [training: 3.6321266133164065 | validation: 3.1016771415242346]
	TIME [epoch: 9.71 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.624073887023625		[learning rate: 0.00074908]
	Learning Rate: 0.000749077
	LOSS [training: 3.624073887023625 | validation: 3.105611815796376]
	TIME [epoch: 9.72 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6172387620255733		[learning rate: 0.00074636]
	Learning Rate: 0.000746358
	LOSS [training: 3.6172387620255733 | validation: 3.140503038903428]
	TIME [epoch: 9.73 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6218651356696254		[learning rate: 0.00074365]
	Learning Rate: 0.00074365
	LOSS [training: 3.6218651356696254 | validation: 3.0901045187812803]
	TIME [epoch: 9.71 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6103888260006967		[learning rate: 0.00074095]
	Learning Rate: 0.000740951
	LOSS [training: 3.6103888260006967 | validation: 3.094643604017097]
	TIME [epoch: 9.7 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.612575817152152		[learning rate: 0.00073826]
	Learning Rate: 0.000738262
	LOSS [training: 3.612575817152152 | validation: 3.111979786563025]
	TIME [epoch: 9.73 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6113676203810443		[learning rate: 0.00073558]
	Learning Rate: 0.000735583
	LOSS [training: 3.6113676203810443 | validation: 3.108235373525631]
	TIME [epoch: 9.71 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6152553996018377		[learning rate: 0.00073291]
	Learning Rate: 0.000732913
	LOSS [training: 3.6152553996018377 | validation: 3.111566958563283]
	TIME [epoch: 9.71 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.630846125890053		[learning rate: 0.00073025]
	Learning Rate: 0.000730254
	LOSS [training: 3.630846125890053 | validation: 3.096253771873395]
	TIME [epoch: 9.71 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.612214945333141		[learning rate: 0.0007276]
	Learning Rate: 0.000727603
	LOSS [training: 3.612214945333141 | validation: 3.1093598629520747]
	TIME [epoch: 9.73 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6165007296245433		[learning rate: 0.00072496]
	Learning Rate: 0.000724963
	LOSS [training: 3.6165007296245433 | validation: 3.1059660521101082]
	TIME [epoch: 9.7 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6118651078102753		[learning rate: 0.00072233]
	Learning Rate: 0.000722332
	LOSS [training: 3.6118651078102753 | validation: 3.0875429839213293]
	TIME [epoch: 9.7 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.612535008172346		[learning rate: 0.00071971]
	Learning Rate: 0.000719711
	LOSS [training: 3.612535008172346 | validation: 3.1092046292982984]
	TIME [epoch: 9.72 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6381821962244443		[learning rate: 0.0007171]
	Learning Rate: 0.000717099
	LOSS [training: 3.6381821962244443 | validation: 3.105742751443289]
	TIME [epoch: 9.71 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6157742497734646		[learning rate: 0.0007145]
	Learning Rate: 0.000714496
	LOSS [training: 3.6157742497734646 | validation: 3.1218406722273504]
	TIME [epoch: 9.7 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.616471274831573		[learning rate: 0.0007119]
	Learning Rate: 0.000711903
	LOSS [training: 3.616471274831573 | validation: 3.091086534369825]
	TIME [epoch: 9.72 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6127742584399245		[learning rate: 0.00070932]
	Learning Rate: 0.00070932
	LOSS [training: 3.6127742584399245 | validation: 3.1465223113084826]
	TIME [epoch: 9.71 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.645518723464319		[learning rate: 0.00070675]
	Learning Rate: 0.000706746
	LOSS [training: 3.645518723464319 | validation: 3.104064051281828]
	TIME [epoch: 9.71 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.619532519222635		[learning rate: 0.00070418]
	Learning Rate: 0.000704181
	LOSS [training: 3.619532519222635 | validation: 3.092802070311184]
	TIME [epoch: 9.7 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6220555789697357		[learning rate: 0.00070163]
	Learning Rate: 0.000701625
	LOSS [training: 3.6220555789697357 | validation: 3.100408328765707]
	TIME [epoch: 9.73 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6074069145427066		[learning rate: 0.00069908]
	Learning Rate: 0.000699079
	LOSS [training: 3.6074069145427066 | validation: 3.0905483248292533]
	TIME [epoch: 9.71 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.621393339437675		[learning rate: 0.00069654]
	Learning Rate: 0.000696542
	LOSS [training: 3.621393339437675 | validation: 3.146661760979745]
	TIME [epoch: 9.7 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.614733224772233		[learning rate: 0.00069401]
	Learning Rate: 0.000694014
	LOSS [training: 3.614733224772233 | validation: 3.092821299644951]
	TIME [epoch: 9.72 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.615370327784926		[learning rate: 0.0006915]
	Learning Rate: 0.000691496
	LOSS [training: 3.615370327784926 | validation: 3.0936155703456665]
	TIME [epoch: 9.71 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6325513092173396		[learning rate: 0.00068899]
	Learning Rate: 0.000688986
	LOSS [training: 3.6325513092173396 | validation: 3.161034256825662]
	TIME [epoch: 9.7 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.632938871725514		[learning rate: 0.00068649]
	Learning Rate: 0.000686486
	LOSS [training: 3.632938871725514 | validation: 3.0955744759272874]
	TIME [epoch: 9.7 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6319674170626093		[learning rate: 0.00068399]
	Learning Rate: 0.000683994
	LOSS [training: 3.6319674170626093 | validation: 3.100635018944837]
	TIME [epoch: 9.71 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6317548084229956		[learning rate: 0.00068151]
	Learning Rate: 0.000681512
	LOSS [training: 3.6317548084229956 | validation: 3.0902797147431293]
	TIME [epoch: 9.7 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.615768446433987		[learning rate: 0.00067904]
	Learning Rate: 0.000679039
	LOSS [training: 3.615768446433987 | validation: 3.102866790225355]
	TIME [epoch: 9.69 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6055848772303642		[learning rate: 0.00067657]
	Learning Rate: 0.000676575
	LOSS [training: 3.6055848772303642 | validation: 3.0886969835100184]
	TIME [epoch: 9.73 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6167367416422764		[learning rate: 0.00067412]
	Learning Rate: 0.00067412
	LOSS [training: 3.6167367416422764 | validation: 3.1024679084209925]
	TIME [epoch: 9.7 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6117554409083303		[learning rate: 0.00067167]
	Learning Rate: 0.000671673
	LOSS [training: 3.6117554409083303 | validation: 3.107758902914468]
	TIME [epoch: 9.7 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.620909635644151		[learning rate: 0.00066924]
	Learning Rate: 0.000669235
	LOSS [training: 3.620909635644151 | validation: 3.094375361775354]
	TIME [epoch: 9.71 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.615996236447714		[learning rate: 0.00066681]
	Learning Rate: 0.000666807
	LOSS [training: 3.615996236447714 | validation: 3.0919621751567132]
	TIME [epoch: 9.72 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6169864825246827		[learning rate: 0.00066439]
	Learning Rate: 0.000664387
	LOSS [training: 3.6169864825246827 | validation: 3.1010271012109736]
	TIME [epoch: 9.69 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6196928163964595		[learning rate: 0.00066198]
	Learning Rate: 0.000661976
	LOSS [training: 3.6196928163964595 | validation: 3.1134708543325207]
	TIME [epoch: 9.7 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.620352365912967		[learning rate: 0.00065957]
	Learning Rate: 0.000659573
	LOSS [training: 3.620352365912967 | validation: 3.0732407720673587]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_848.pth
	Model improved!!!
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.605617221866443		[learning rate: 0.00065718]
	Learning Rate: 0.00065718
	LOSS [training: 3.605617221866443 | validation: 3.1062669250312203]
	TIME [epoch: 9.69 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.610953372082902		[learning rate: 0.00065479]
	Learning Rate: 0.000654795
	LOSS [training: 3.610953372082902 | validation: 3.109849860188553]
	TIME [epoch: 9.69 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6410457835906733		[learning rate: 0.00065242]
	Learning Rate: 0.000652419
	LOSS [training: 3.6410457835906733 | validation: 3.0930887513692573]
	TIME [epoch: 9.71 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6168724768438225		[learning rate: 0.00065005]
	Learning Rate: 0.000650051
	LOSS [training: 3.6168724768438225 | validation: 3.1005071300356826]
	TIME [epoch: 9.69 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.604160285687287		[learning rate: 0.00064769]
	Learning Rate: 0.000647692
	LOSS [training: 3.604160285687287 | validation: 3.110894899649494]
	TIME [epoch: 9.7 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6196484394121136		[learning rate: 0.00064534]
	Learning Rate: 0.000645341
	LOSS [training: 3.6196484394121136 | validation: 3.112296072800542]
	TIME [epoch: 9.71 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6148851632349084		[learning rate: 0.000643]
	Learning Rate: 0.000642999
	LOSS [training: 3.6148851632349084 | validation: 3.130185159204528]
	TIME [epoch: 9.72 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6139713999339547		[learning rate: 0.00064067]
	Learning Rate: 0.000640666
	LOSS [training: 3.6139713999339547 | validation: 3.0912757545429272]
	TIME [epoch: 9.69 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60332955463515		[learning rate: 0.00063834]
	Learning Rate: 0.000638341
	LOSS [training: 3.60332955463515 | validation: 3.149703251874669]
	TIME [epoch: 9.69 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.613097573366413		[learning rate: 0.00063602]
	Learning Rate: 0.000636024
	LOSS [training: 3.613097573366413 | validation: 3.09260032545983]
	TIME [epoch: 9.72 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603210633278651		[learning rate: 0.00063372]
	Learning Rate: 0.000633716
	LOSS [training: 3.603210633278651 | validation: 3.096417869836657]
	TIME [epoch: 9.7 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599927628500789		[learning rate: 0.00063142]
	Learning Rate: 0.000631416
	LOSS [training: 3.599927628500789 | validation: 3.0890740243269046]
	TIME [epoch: 9.69 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6040179712285414		[learning rate: 0.00062912]
	Learning Rate: 0.000629125
	LOSS [training: 3.6040179712285414 | validation: 3.100752629352677]
	TIME [epoch: 9.7 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6316997878928134		[learning rate: 0.00062684]
	Learning Rate: 0.000626842
	LOSS [training: 3.6316997878928134 | validation: 3.1082480435021766]
	TIME [epoch: 9.72 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6144344151455114		[learning rate: 0.00062457]
	Learning Rate: 0.000624567
	LOSS [training: 3.6144344151455114 | validation: 3.1002622538662594]
	TIME [epoch: 9.7 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6071773212432587		[learning rate: 0.0006223]
	Learning Rate: 0.0006223
	LOSS [training: 3.6071773212432587 | validation: 3.1354158154287655]
	TIME [epoch: 9.7 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6133692918053755		[learning rate: 0.00062004]
	Learning Rate: 0.000620042
	LOSS [training: 3.6133692918053755 | validation: 3.0992669650705005]
	TIME [epoch: 9.72 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.616424271602373		[learning rate: 0.00061779]
	Learning Rate: 0.000617792
	LOSS [training: 3.616424271602373 | validation: 3.095043301113973]
	TIME [epoch: 9.7 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.606198121599629		[learning rate: 0.00061555]
	Learning Rate: 0.00061555
	LOSS [training: 3.606198121599629 | validation: 3.0867573411204354]
	TIME [epoch: 9.7 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6048159617626725		[learning rate: 0.00061332]
	Learning Rate: 0.000613316
	LOSS [training: 3.6048159617626725 | validation: 3.127600366672782]
	TIME [epoch: 9.72 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.633997031566578		[learning rate: 0.00061109]
	Learning Rate: 0.00061109
	LOSS [training: 3.633997031566578 | validation: 3.108150892068162]
	TIME [epoch: 9.7 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.607936550964131		[learning rate: 0.00060887]
	Learning Rate: 0.000608872
	LOSS [training: 3.607936550964131 | validation: 3.1000658446082263]
	TIME [epoch: 9.7 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6188902832297076		[learning rate: 0.00060666]
	Learning Rate: 0.000606663
	LOSS [training: 3.6188902832297076 | validation: 3.0889940756169354]
	TIME [epoch: 9.69 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6041664612026665		[learning rate: 0.00060446]
	Learning Rate: 0.000604461
	LOSS [training: 3.6041664612026665 | validation: 3.090990595770394]
	TIME [epoch: 9.72 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601559152087327		[learning rate: 0.00060227]
	Learning Rate: 0.000602268
	LOSS [training: 3.601559152087327 | validation: 3.1050888199818742]
	TIME [epoch: 9.69 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6105643786649453		[learning rate: 0.00060008]
	Learning Rate: 0.000600082
	LOSS [training: 3.6105643786649453 | validation: 3.10552385244701]
	TIME [epoch: 9.69 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.604154196277258		[learning rate: 0.0005979]
	Learning Rate: 0.000597904
	LOSS [training: 3.604154196277258 | validation: 3.1086580580156635]
	TIME [epoch: 9.72 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.619740334899704		[learning rate: 0.00059573]
	Learning Rate: 0.000595734
	LOSS [training: 3.619740334899704 | validation: 3.0806662445524093]
	TIME [epoch: 9.7 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6202536064392112		[learning rate: 0.00059357]
	Learning Rate: 0.000593572
	LOSS [training: 3.6202536064392112 | validation: 3.1190850970756583]
	TIME [epoch: 9.7 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6287035319778402		[learning rate: 0.00059142]
	Learning Rate: 0.000591418
	LOSS [training: 3.6287035319778402 | validation: 3.07558952066934]
	TIME [epoch: 9.7 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6064576548784215		[learning rate: 0.00058927]
	Learning Rate: 0.000589272
	LOSS [training: 3.6064576548784215 | validation: 3.09091646400021]
	TIME [epoch: 9.7 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6077684410282345		[learning rate: 0.00058713]
	Learning Rate: 0.000587133
	LOSS [training: 3.6077684410282345 | validation: 3.0977640143296536]
	TIME [epoch: 9.69 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6087566817003767		[learning rate: 0.000585]
	Learning Rate: 0.000585003
	LOSS [training: 3.6087566817003767 | validation: 3.0812489243778707]
	TIME [epoch: 9.69 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6078138861101485		[learning rate: 0.00058288]
	Learning Rate: 0.00058288
	LOSS [training: 3.6078138861101485 | validation: 3.1252950919027818]
	TIME [epoch: 9.73 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.618355978598102		[learning rate: 0.00058076]
	Learning Rate: 0.000580764
	LOSS [training: 3.618355978598102 | validation: 3.079291243212811]
	TIME [epoch: 9.69 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6074494660987755		[learning rate: 0.00057866]
	Learning Rate: 0.000578657
	LOSS [training: 3.6074494660987755 | validation: 3.114591935677744]
	TIME [epoch: 9.69 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.612418572585767		[learning rate: 0.00057656]
	Learning Rate: 0.000576557
	LOSS [training: 3.612418572585767 | validation: 3.0897008279040894]
	TIME [epoch: 9.7 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.633514873764379		[learning rate: 0.00057446]
	Learning Rate: 0.000574465
	LOSS [training: 3.633514873764379 | validation: 3.1412239229386434]
	TIME [epoch: 9.7 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.62466311782888		[learning rate: 0.00057238]
	Learning Rate: 0.00057238
	LOSS [training: 3.62466311782888 | validation: 3.0970481224310924]
	TIME [epoch: 9.69 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.612780446288216		[learning rate: 0.0005703]
	Learning Rate: 0.000570303
	LOSS [training: 3.612780446288216 | validation: 3.1467092704716646]
	TIME [epoch: 9.69 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.622982616487586		[learning rate: 0.00056823]
	Learning Rate: 0.000568233
	LOSS [training: 3.622982616487586 | validation: 3.1126014217068656]
	TIME [epoch: 9.72 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.615282698735278		[learning rate: 0.00056617]
	Learning Rate: 0.000566171
	LOSS [training: 3.615282698735278 | validation: 3.0861158328003166]
	TIME [epoch: 9.69 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.608246435900879		[learning rate: 0.00056412]
	Learning Rate: 0.000564116
	LOSS [training: 3.608246435900879 | validation: 3.0957249325582814]
	TIME [epoch: 9.69 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6054325096915436		[learning rate: 0.00056207]
	Learning Rate: 0.000562069
	LOSS [training: 3.6054325096915436 | validation: 3.1046741755732583]
	TIME [epoch: 9.71 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.611259792864078		[learning rate: 0.00056003]
	Learning Rate: 0.000560029
	LOSS [training: 3.611259792864078 | validation: 3.1374963021670994]
	TIME [epoch: 9.7 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.650473159579166		[learning rate: 0.000558]
	Learning Rate: 0.000557997
	LOSS [training: 3.650473159579166 | validation: 3.1153299948011672]
	TIME [epoch: 9.69 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.610368840189718		[learning rate: 0.00055597]
	Learning Rate: 0.000555972
	LOSS [training: 3.610368840189718 | validation: 3.096017107182212]
	TIME [epoch: 9.7 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6200148587706407		[learning rate: 0.00055395]
	Learning Rate: 0.000553954
	LOSS [training: 3.6200148587706407 | validation: 3.1324114570919677]
	TIME [epoch: 9.71 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.609908012285446		[learning rate: 0.00055194]
	Learning Rate: 0.000551944
	LOSS [training: 3.609908012285446 | validation: 3.086981490627265]
	TIME [epoch: 9.69 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.608086306948259		[learning rate: 0.00054994]
	Learning Rate: 0.000549941
	LOSS [training: 3.608086306948259 | validation: 3.1123302663839536]
	TIME [epoch: 9.7 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.623151029936435		[learning rate: 0.00054794]
	Learning Rate: 0.000547945
	LOSS [training: 3.623151029936435 | validation: 3.136678951732373]
	TIME [epoch: 9.71 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6294197140426063		[learning rate: 0.00054596]
	Learning Rate: 0.000545956
	LOSS [training: 3.6294197140426063 | validation: 3.0827826439866635]
	TIME [epoch: 9.69 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6101976566238143		[learning rate: 0.00054397]
	Learning Rate: 0.000543975
	LOSS [training: 3.6101976566238143 | validation: 3.1236485632758497]
	TIME [epoch: 9.68 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6154096109492535		[learning rate: 0.000542]
	Learning Rate: 0.000542001
	LOSS [training: 3.6154096109492535 | validation: 3.1083986397481937]
	TIME [epoch: 9.7 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.607875419419165		[learning rate: 0.00054003]
	Learning Rate: 0.000540034
	LOSS [training: 3.607875419419165 | validation: 3.094876350632459]
	TIME [epoch: 9.69 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.617832374893468		[learning rate: 0.00053807]
	Learning Rate: 0.000538074
	LOSS [training: 3.617832374893468 | validation: 3.074094663793845]
	TIME [epoch: 9.69 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6157067775743563		[learning rate: 0.00053612]
	Learning Rate: 0.000536121
	LOSS [training: 3.6157067775743563 | validation: 3.097806974552792]
	TIME [epoch: 9.68 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.605659887965919		[learning rate: 0.00053418]
	Learning Rate: 0.000534176
	LOSS [training: 3.605659887965919 | validation: 3.105169184278299]
	TIME [epoch: 9.71 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6125567429035312		[learning rate: 0.00053224]
	Learning Rate: 0.000532237
	LOSS [training: 3.6125567429035312 | validation: 3.12845433296404]
	TIME [epoch: 9.69 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6161626310541144		[learning rate: 0.00053031]
	Learning Rate: 0.000530306
	LOSS [training: 3.6161626310541144 | validation: 3.0929645643690624]
	TIME [epoch: 9.69 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.61140484132234		[learning rate: 0.00052838]
	Learning Rate: 0.000528381
	LOSS [training: 3.61140484132234 | validation: 3.100063922026078]
	TIME [epoch: 9.71 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6442368236609455		[learning rate: 0.00052646]
	Learning Rate: 0.000526464
	LOSS [training: 3.6442368236609455 | validation: 3.1550193357945244]
	TIME [epoch: 9.7 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.629533573181446		[learning rate: 0.00052455]
	Learning Rate: 0.000524553
	LOSS [training: 3.629533573181446 | validation: 3.0987866260127697]
	TIME [epoch: 9.68 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.61050847464871		[learning rate: 0.00052265]
	Learning Rate: 0.000522649
	LOSS [training: 3.61050847464871 | validation: 3.104163791960268]
	TIME [epoch: 9.69 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.607084613808426		[learning rate: 0.00052075]
	Learning Rate: 0.000520753
	LOSS [training: 3.607084613808426 | validation: 3.1021214288568872]
	TIME [epoch: 9.71 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603589823518462		[learning rate: 0.00051886]
	Learning Rate: 0.000518863
	LOSS [training: 3.603589823518462 | validation: 3.0924613690558047]
	TIME [epoch: 9.68 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.612225458265962		[learning rate: 0.00051698]
	Learning Rate: 0.00051698
	LOSS [training: 3.612225458265962 | validation: 3.0929711024462234]
	TIME [epoch: 9.68 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602723364935529		[learning rate: 0.0005151]
	Learning Rate: 0.000515104
	LOSS [training: 3.602723364935529 | validation: 3.095155409812976]
	TIME [epoch: 9.71 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.61922057714989		[learning rate: 0.00051323]
	Learning Rate: 0.000513235
	LOSS [training: 3.61922057714989 | validation: 3.1126238702179556]
	TIME [epoch: 9.69 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.614566571016198		[learning rate: 0.00051137]
	Learning Rate: 0.000511372
	LOSS [training: 3.614566571016198 | validation: 3.120603813710499]
	TIME [epoch: 9.69 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603511386918055		[learning rate: 0.00050952]
	Learning Rate: 0.000509516
	LOSS [training: 3.603511386918055 | validation: 3.1018164428862254]
	TIME [epoch: 9.7 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.607137813954772		[learning rate: 0.00050767]
	Learning Rate: 0.000507667
	LOSS [training: 3.607137813954772 | validation: 3.1038629301901954]
	TIME [epoch: 9.71 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.628393852724373		[learning rate: 0.00050582]
	Learning Rate: 0.000505825
	LOSS [training: 3.628393852724373 | validation: 3.0931417806791774]
	TIME [epoch: 9.7 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.608116555969887		[learning rate: 0.00050399]
	Learning Rate: 0.000503989
	LOSS [training: 3.608116555969887 | validation: 3.097937218165281]
	TIME [epoch: 9.69 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6212398595145485		[learning rate: 0.00050216]
	Learning Rate: 0.00050216
	LOSS [training: 3.6212398595145485 | validation: 3.1202406015690736]
	TIME [epoch: 9.71 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6123534872133014		[learning rate: 0.00050034]
	Learning Rate: 0.000500338
	LOSS [training: 3.6123534872133014 | validation: 3.0813790760655615]
	TIME [epoch: 9.7 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.620524624959809		[learning rate: 0.00049852]
	Learning Rate: 0.000498522
	LOSS [training: 3.620524624959809 | validation: 3.077264031864209]
	TIME [epoch: 9.69 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6270052862109807		[learning rate: 0.00049671]
	Learning Rate: 0.000496713
	LOSS [training: 3.6270052862109807 | validation: 3.1370033840094096]
	TIME [epoch: 9.71 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.617051379600791		[learning rate: 0.00049491]
	Learning Rate: 0.00049491
	LOSS [training: 3.617051379600791 | validation: 3.100735435159261]
	TIME [epoch: 9.7 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6159101647791423		[learning rate: 0.00049311]
	Learning Rate: 0.000493114
	LOSS [training: 3.6159101647791423 | validation: 3.079345267988472]
	TIME [epoch: 9.69 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6079944817666814		[learning rate: 0.00049132]
	Learning Rate: 0.000491325
	LOSS [training: 3.6079944817666814 | validation: 3.1064070672670625]
	TIME [epoch: 9.69 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6180270243502664		[learning rate: 0.00048954]
	Learning Rate: 0.000489542
	LOSS [training: 3.6180270243502664 | validation: 3.1169359782603476]
	TIME [epoch: 9.72 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.617338414746076		[learning rate: 0.00048776]
	Learning Rate: 0.000487765
	LOSS [training: 3.617338414746076 | validation: 3.0986605346816187]
	TIME [epoch: 9.69 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6189037929749306		[learning rate: 0.00048599]
	Learning Rate: 0.000485995
	LOSS [training: 3.6189037929749306 | validation: 3.089685877980969]
	TIME [epoch: 9.69 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6145584202539647		[learning rate: 0.00048423]
	Learning Rate: 0.000484231
	LOSS [training: 3.6145584202539647 | validation: 3.089883029050799]
	TIME [epoch: 9.71 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.613302426525641		[learning rate: 0.00048247]
	Learning Rate: 0.000482474
	LOSS [training: 3.613302426525641 | validation: 3.085385726118921]
	TIME [epoch: 9.7 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600066544042698		[learning rate: 0.00048072]
	Learning Rate: 0.000480723
	LOSS [training: 3.600066544042698 | validation: 3.0870255313201005]
	TIME [epoch: 9.69 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6002520056112943		[learning rate: 0.00047898]
	Learning Rate: 0.000478978
	LOSS [training: 3.6002520056112943 | validation: 3.084754173445806]
	TIME [epoch: 9.7 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.608023686725315		[learning rate: 0.00047724]
	Learning Rate: 0.00047724
	LOSS [training: 3.608023686725315 | validation: 3.0897669006638138]
	TIME [epoch: 9.7 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6109534252656283		[learning rate: 0.00047551]
	Learning Rate: 0.000475508
	LOSS [training: 3.6109534252656283 | validation: 3.102770524276063]
	TIME [epoch: 9.69 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.619745706925446		[learning rate: 0.00047378]
	Learning Rate: 0.000473782
	LOSS [training: 3.619745706925446 | validation: 3.101708782815843]
	TIME [epoch: 9.69 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602495356367276		[learning rate: 0.00047206]
	Learning Rate: 0.000472063
	LOSS [training: 3.602495356367276 | validation: 3.0882638360029966]
	TIME [epoch: 9.72 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60552353118076		[learning rate: 0.00047035]
	Learning Rate: 0.00047035
	LOSS [training: 3.60552353118076 | validation: 3.110779721798924]
	TIME [epoch: 9.69 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.61458708621725		[learning rate: 0.00046864]
	Learning Rate: 0.000468643
	LOSS [training: 3.61458708621725 | validation: 3.090951788183018]
	TIME [epoch: 9.69 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6207358155050193		[learning rate: 0.00046694]
	Learning Rate: 0.000466942
	LOSS [training: 3.6207358155050193 | validation: 3.088809999216425]
	TIME [epoch: 9.7 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6076634377406136		[learning rate: 0.00046525]
	Learning Rate: 0.000465248
	LOSS [training: 3.6076634377406136 | validation: 3.101528685827372]
	TIME [epoch: 9.7 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.609936317173966		[learning rate: 0.00046356]
	Learning Rate: 0.000463559
	LOSS [training: 3.609936317173966 | validation: 3.1445173155900354]
	TIME [epoch: 9.69 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.61887985452996		[learning rate: 0.00046188]
	Learning Rate: 0.000461877
	LOSS [training: 3.61887985452996 | validation: 3.127122530849034]
	TIME [epoch: 9.7 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6102652687342776		[learning rate: 0.0004602]
	Learning Rate: 0.000460201
	LOSS [training: 3.6102652687342776 | validation: 3.086330681309338]
	TIME [epoch: 9.72 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.611913085533307		[learning rate: 0.00045853]
	Learning Rate: 0.000458531
	LOSS [training: 3.611913085533307 | validation: 3.090834864494866]
	TIME [epoch: 9.7 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6180599947152365		[learning rate: 0.00045687]
	Learning Rate: 0.000456867
	LOSS [training: 3.6180599947152365 | validation: 3.0858241204434083]
	TIME [epoch: 9.69 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6106664745886063		[learning rate: 0.00045521]
	Learning Rate: 0.000455209
	LOSS [training: 3.6106664745886063 | validation: 3.124367325702909]
	TIME [epoch: 9.71 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.622095474938136		[learning rate: 0.00045356]
	Learning Rate: 0.000453557
	LOSS [training: 3.622095474938136 | validation: 3.076688905056683]
	TIME [epoch: 9.7 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.613763203059817		[learning rate: 0.00045191]
	Learning Rate: 0.000451911
	LOSS [training: 3.613763203059817 | validation: 3.0965449256200204]
	TIME [epoch: 9.7 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6033612200814416		[learning rate: 0.00045027]
	Learning Rate: 0.000450271
	LOSS [training: 3.6033612200814416 | validation: 3.08691928241538]
	TIME [epoch: 9.71 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6044375078793522		[learning rate: 0.00044864]
	Learning Rate: 0.000448637
	LOSS [training: 3.6044375078793522 | validation: 3.0984723849326845]
	TIME [epoch: 9.71 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.625692395801418		[learning rate: 0.00044701]
	Learning Rate: 0.000447009
	LOSS [training: 3.625692395801418 | validation: 3.1722631983632983]
	TIME [epoch: 9.69 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.628138306489763		[learning rate: 0.00044539]
	Learning Rate: 0.000445386
	LOSS [training: 3.628138306489763 | validation: 3.105901233201969]
	TIME [epoch: 9.69 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.617805802392705		[learning rate: 0.00044377]
	Learning Rate: 0.00044377
	LOSS [training: 3.617805802392705 | validation: 3.08194980616401]
	TIME [epoch: 9.72 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.609512273035732		[learning rate: 0.00044216]
	Learning Rate: 0.00044216
	LOSS [training: 3.609512273035732 | validation: 3.104884062692454]
	TIME [epoch: 9.69 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6093938684191174		[learning rate: 0.00044055]
	Learning Rate: 0.000440555
	LOSS [training: 3.6093938684191174 | validation: 3.0991582747855997]
	TIME [epoch: 9.69 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6044855481762794		[learning rate: 0.00043896]
	Learning Rate: 0.000438956
	LOSS [training: 3.6044855481762794 | validation: 3.1119586201000073]
	TIME [epoch: 9.71 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6114528009422955		[learning rate: 0.00043736]
	Learning Rate: 0.000437363
	LOSS [training: 3.6114528009422955 | validation: 3.091568860219287]
	TIME [epoch: 9.7 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599621649812179		[learning rate: 0.00043578]
	Learning Rate: 0.000435776
	LOSS [training: 3.599621649812179 | validation: 3.0915432002288608]
	TIME [epoch: 9.69 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.606449795876813		[learning rate: 0.00043419]
	Learning Rate: 0.000434194
	LOSS [training: 3.606449795876813 | validation: 3.106014658562178]
	TIME [epoch: 9.69 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.609927991314012		[learning rate: 0.00043262]
	Learning Rate: 0.000432619
	LOSS [training: 3.609927991314012 | validation: 3.0868193427078188]
	TIME [epoch: 9.72 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.610299449440629		[learning rate: 0.00043105]
	Learning Rate: 0.000431049
	LOSS [training: 3.610299449440629 | validation: 3.0910915411224478]
	TIME [epoch: 9.69 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603636166803033		[learning rate: 0.00042948]
	Learning Rate: 0.000429484
	LOSS [training: 3.603636166803033 | validation: 3.0962461300383803]
	TIME [epoch: 9.7 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6033203892041703		[learning rate: 0.00042793]
	Learning Rate: 0.000427926
	LOSS [training: 3.6033203892041703 | validation: 3.091818609663692]
	TIME [epoch: 9.71 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.612862844867012		[learning rate: 0.00042637]
	Learning Rate: 0.000426373
	LOSS [training: 3.612862844867012 | validation: 3.0935390242936878]
	TIME [epoch: 9.7 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6168110855522206		[learning rate: 0.00042483]
	Learning Rate: 0.000424825
	LOSS [training: 3.6168110855522206 | validation: 3.1039898564936608]
	TIME [epoch: 9.7 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6027543885876616		[learning rate: 0.00042328]
	Learning Rate: 0.000423284
	LOSS [training: 3.6027543885876616 | validation: 3.0970522104860674]
	TIME [epoch: 9.7 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6054879337658092		[learning rate: 0.00042175]
	Learning Rate: 0.000421748
	LOSS [training: 3.6054879337658092 | validation: 3.0958187315548673]
	TIME [epoch: 9.72 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6037037670553134		[learning rate: 0.00042022]
	Learning Rate: 0.000420217
	LOSS [training: 3.6037037670553134 | validation: 3.1182992055913803]
	TIME [epoch: 9.69 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6102310006486933		[learning rate: 0.00041869]
	Learning Rate: 0.000418692
	LOSS [training: 3.6102310006486933 | validation: 3.087899909790352]
	TIME [epoch: 9.69 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6061616741914646		[learning rate: 0.00041717]
	Learning Rate: 0.000417173
	LOSS [training: 3.6061616741914646 | validation: 3.1142696170045205]
	TIME [epoch: 9.71 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.62448176672954		[learning rate: 0.00041566]
	Learning Rate: 0.000415659
	LOSS [training: 3.62448176672954 | validation: 3.104481312556601]
	TIME [epoch: 9.69 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601440257301744		[learning rate: 0.00041415]
	Learning Rate: 0.00041415
	LOSS [training: 3.601440257301744 | validation: 3.0870856337444614]
	TIME [epoch: 9.69 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6103889042772055		[learning rate: 0.00041265]
	Learning Rate: 0.000412647
	LOSS [training: 3.6103889042772055 | validation: 3.0845296590744478]
	TIME [epoch: 9.7 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6015529569898694		[learning rate: 0.00041115]
	Learning Rate: 0.00041115
	LOSS [training: 3.6015529569898694 | validation: 3.0810206233670603]
	TIME [epoch: 9.71 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5996529709099745		[learning rate: 0.00040966]
	Learning Rate: 0.000409658
	LOSS [training: 3.5996529709099745 | validation: 3.0806257551821954]
	TIME [epoch: 9.69 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5973919996339645		[learning rate: 0.00040817]
	Learning Rate: 0.000408171
	LOSS [training: 3.5973919996339645 | validation: 3.0894095256812313]
	TIME [epoch: 9.69 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.604292632965784		[learning rate: 0.00040669]
	Learning Rate: 0.00040669
	LOSS [training: 3.604292632965784 | validation: 3.089927770701461]
	TIME [epoch: 9.72 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.610744549285698		[learning rate: 0.00040521]
	Learning Rate: 0.000405214
	LOSS [training: 3.610744549285698 | validation: 3.117140266204763]
	TIME [epoch: 9.68 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.612330362736506		[learning rate: 0.00040374]
	Learning Rate: 0.000403743
	LOSS [training: 3.612330362736506 | validation: 3.087061606503241]
	TIME [epoch: 9.69 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.608058013267555		[learning rate: 0.00040228]
	Learning Rate: 0.000402278
	LOSS [training: 3.608058013267555 | validation: 3.1183984927543067]
	TIME [epoch: 9.71 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.648861955582446		[learning rate: 0.00040082]
	Learning Rate: 0.000400818
	LOSS [training: 3.648861955582446 | validation: 3.0933673154530514]
	TIME [epoch: 9.69 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6169951940469653		[learning rate: 0.00039936]
	Learning Rate: 0.000399364
	LOSS [training: 3.6169951940469653 | validation: 3.1020259293259063]
	TIME [epoch: 9.69 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.608293952454568		[learning rate: 0.00039791]
	Learning Rate: 0.000397914
	LOSS [training: 3.608293952454568 | validation: 3.0721290962250754]
	TIME [epoch: 9.68 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_987.pth
	Model improved!!!
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6045099127441254		[learning rate: 0.00039647]
	Learning Rate: 0.00039647
	LOSS [training: 3.6045099127441254 | validation: 3.087699144258061]
	TIME [epoch: 9.72 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.611052705038283		[learning rate: 0.00039503]
	Learning Rate: 0.000395031
	LOSS [training: 3.611052705038283 | validation: 3.079971214046576]
	TIME [epoch: 9.69 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6021364739878647		[learning rate: 0.0003936]
	Learning Rate: 0.000393598
	LOSS [training: 3.6021364739878647 | validation: 3.082980419486462]
	TIME [epoch: 9.7 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.608633407904329		[learning rate: 0.00039217]
	Learning Rate: 0.000392169
	LOSS [training: 3.608633407904329 | validation: 3.083254378054679]
	TIME [epoch: 9.71 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6050327606815307		[learning rate: 0.00039075]
	Learning Rate: 0.000390746
	LOSS [training: 3.6050327606815307 | validation: 3.0988798272227265]
	TIME [epoch: 9.69 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6066625431244397		[learning rate: 0.00038933]
	Learning Rate: 0.000389328
	LOSS [training: 3.6066625431244397 | validation: 3.10100920640062]
	TIME [epoch: 9.7 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6106287621878033		[learning rate: 0.00038792]
	Learning Rate: 0.000387915
	LOSS [training: 3.6106287621878033 | validation: 3.114698611393619]
	TIME [epoch: 9.71 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6136108133837688		[learning rate: 0.00038651]
	Learning Rate: 0.000386508
	LOSS [training: 3.6136108133837688 | validation: 3.1129882290358877]
	TIME [epoch: 9.73 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60452092491892		[learning rate: 0.0003851]
	Learning Rate: 0.000385105
	LOSS [training: 3.60452092491892 | validation: 3.1008525593419414]
	TIME [epoch: 9.7 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6059616181854595		[learning rate: 0.00038371]
	Learning Rate: 0.000383707
	LOSS [training: 3.6059616181854595 | validation: 3.0859502602266593]
	TIME [epoch: 9.7 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6059486227252338		[learning rate: 0.00038231]
	Learning Rate: 0.000382315
	LOSS [training: 3.6059486227252338 | validation: 3.125806252594963]
	TIME [epoch: 9.72 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6125000337183963		[learning rate: 0.00038093]
	Learning Rate: 0.000380927
	LOSS [training: 3.6125000337183963 | validation: 3.110112333671909]
	TIME [epoch: 9.7 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6238797110635814		[learning rate: 0.00037954]
	Learning Rate: 0.000379545
	LOSS [training: 3.6238797110635814 | validation: 3.087259221954278]
	TIME [epoch: 9.69 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6068332459755608		[learning rate: 0.00037817]
	Learning Rate: 0.000378167
	LOSS [training: 3.6068332459755608 | validation: 3.0810150399595946]
	TIME [epoch: 9.72 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597390424577393		[learning rate: 0.0003768]
	Learning Rate: 0.000376795
	LOSS [training: 3.597390424577393 | validation: 3.11847913838507]
	TIME [epoch: 9.7 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6089029581051193		[learning rate: 0.00037543]
	Learning Rate: 0.000375428
	LOSS [training: 3.6089029581051193 | validation: 3.117947099660075]
	TIME [epoch: 9.7 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6130811841102295		[learning rate: 0.00037407]
	Learning Rate: 0.000374065
	LOSS [training: 3.6130811841102295 | validation: 3.1002880830439508]
	TIME [epoch: 9.7 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6052986397408295		[learning rate: 0.00037271]
	Learning Rate: 0.000372708
	LOSS [training: 3.6052986397408295 | validation: 3.0892000929963945]
	TIME [epoch: 9.72 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603855911688877		[learning rate: 0.00037136]
	Learning Rate: 0.000371355
	LOSS [training: 3.603855911688877 | validation: 3.096526611242115]
	TIME [epoch: 9.7 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6207349766764		[learning rate: 0.00037001]
	Learning Rate: 0.000370008
	LOSS [training: 3.6207349766764 | validation: 3.088832019795634]
	TIME [epoch: 9.7 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6096535977643427		[learning rate: 0.00036866]
	Learning Rate: 0.000368665
	LOSS [training: 3.6096535977643427 | validation: 3.0857026613975247]
	TIME [epoch: 9.72 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6151333267209913		[learning rate: 0.00036733]
	Learning Rate: 0.000367327
	LOSS [training: 3.6151333267209913 | validation: 3.096827876185736]
	TIME [epoch: 9.7 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598790145125451		[learning rate: 0.00036599]
	Learning Rate: 0.000365994
	LOSS [training: 3.598790145125451 | validation: 3.107208516316415]
	TIME [epoch: 9.7 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6155783780315773		[learning rate: 0.00036467]
	Learning Rate: 0.000364666
	LOSS [training: 3.6155783780315773 | validation: 3.0993748229290943]
	TIME [epoch: 9.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.612754424319566		[learning rate: 0.00036334]
	Learning Rate: 0.000363342
	LOSS [training: 3.612754424319566 | validation: 3.096519520125473]
	TIME [epoch: 9.71 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6072729079209864		[learning rate: 0.00036202]
	Learning Rate: 0.000362024
	LOSS [training: 3.6072729079209864 | validation: 3.0988575089629733]
	TIME [epoch: 9.7 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6072133958271877		[learning rate: 0.00036071]
	Learning Rate: 0.00036071
	LOSS [training: 3.6072133958271877 | validation: 3.0969867522041454]
	TIME [epoch: 9.69 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.605059747734714		[learning rate: 0.0003594]
	Learning Rate: 0.000359401
	LOSS [training: 3.605059747734714 | validation: 3.0947997903298883]
	TIME [epoch: 9.72 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.609676566415763		[learning rate: 0.0003581]
	Learning Rate: 0.000358096
	LOSS [training: 3.609676566415763 | validation: 3.078365087276723]
	TIME [epoch: 9.7 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6049234722679047		[learning rate: 0.0003568]
	Learning Rate: 0.000356797
	LOSS [training: 3.6049234722679047 | validation: 3.108557794051991]
	TIME [epoch: 9.69 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60307845717416		[learning rate: 0.0003555]
	Learning Rate: 0.000355502
	LOSS [training: 3.60307845717416 | validation: 3.090411391986512]
	TIME [epoch: 9.71 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602504238031158		[learning rate: 0.00035421]
	Learning Rate: 0.000354212
	LOSS [training: 3.602504238031158 | validation: 3.1210960306179825]
	TIME [epoch: 9.71 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6042968956713137		[learning rate: 0.00035293]
	Learning Rate: 0.000352926
	LOSS [training: 3.6042968956713137 | validation: 3.108114176840984]
	TIME [epoch: 9.7 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602883535221191		[learning rate: 0.00035165]
	Learning Rate: 0.000351646
	LOSS [training: 3.602883535221191 | validation: 3.083526420875779]
	TIME [epoch: 9.71 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5983868342885117		[learning rate: 0.00035037]
	Learning Rate: 0.00035037
	LOSS [training: 3.5983868342885117 | validation: 3.0963091231957534]
	TIME [epoch: 9.72 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6016388034556774		[learning rate: 0.0003491]
	Learning Rate: 0.000349098
	LOSS [training: 3.6016388034556774 | validation: 3.097134897875845]
	TIME [epoch: 9.7 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6080448763877357		[learning rate: 0.00034783]
	Learning Rate: 0.000347831
	LOSS [training: 3.6080448763877357 | validation: 3.069303792357378]
	TIME [epoch: 9.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2_fix_noise/tr_study206/model_tr_study206_r5_20240220_000152/states/model_tr_study206_1024.pth
	Model improved!!!
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6022205708831594		[learning rate: 0.00034657]
	Learning Rate: 0.000346569
	LOSS [training: 3.6022205708831594 | validation: 3.11559071135413]
	TIME [epoch: 9.72 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.618629407947806		[learning rate: 0.00034531]
	Learning Rate: 0.000345311
	LOSS [training: 3.618629407947806 | validation: 3.0866644298389354]
	TIME [epoch: 9.7 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6004315181353794		[learning rate: 0.00034406]
	Learning Rate: 0.000344058
	LOSS [training: 3.6004315181353794 | validation: 3.095647577958437]
	TIME [epoch: 9.69 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603037449480653		[learning rate: 0.00034281]
	Learning Rate: 0.000342809
	LOSS [training: 3.603037449480653 | validation: 3.101510064098367]
	TIME [epoch: 9.69 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6203062163172275		[learning rate: 0.00034157]
	Learning Rate: 0.000341565
	LOSS [training: 3.6203062163172275 | validation: 3.113566102741521]
	TIME [epoch: 9.71 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.620398470658754		[learning rate: 0.00034033]
	Learning Rate: 0.000340326
	LOSS [training: 3.620398470658754 | validation: 3.084595199251371]
	TIME [epoch: 9.69 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600863589587491		[learning rate: 0.00033909]
	Learning Rate: 0.000339091
	LOSS [training: 3.600863589587491 | validation: 3.0848116640450427]
	TIME [epoch: 9.69 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6040466567274416		[learning rate: 0.00033786]
	Learning Rate: 0.00033786
	LOSS [training: 3.6040466567274416 | validation: 3.1000343426774535]
	TIME [epoch: 9.71 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6113733491211164		[learning rate: 0.00033663]
	Learning Rate: 0.000336634
	LOSS [training: 3.6113733491211164 | validation: 3.103520348859736]
	TIME [epoch: 9.69 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6077806506643304		[learning rate: 0.00033541]
	Learning Rate: 0.000335412
	LOSS [training: 3.6077806506643304 | validation: 3.1058733141697985]
	TIME [epoch: 9.69 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.607743981285657		[learning rate: 0.0003342]
	Learning Rate: 0.000334195
	LOSS [training: 3.607743981285657 | validation: 3.075126565700689]
	TIME [epoch: 9.7 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6032794179254664		[learning rate: 0.00033298]
	Learning Rate: 0.000332982
	LOSS [training: 3.6032794179254664 | validation: 3.0894208115958284]
	TIME [epoch: 9.69 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603341133409536		[learning rate: 0.00033177]
	Learning Rate: 0.000331774
	LOSS [training: 3.603341133409536 | validation: 3.119284678378775]
	TIME [epoch: 9.69 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.609625794113046		[learning rate: 0.00033057]
	Learning Rate: 0.00033057
	LOSS [training: 3.609625794113046 | validation: 3.088598757894873]
	TIME [epoch: 9.68 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601599409415539		[learning rate: 0.00032937]
	Learning Rate: 0.00032937
	LOSS [training: 3.601599409415539 | validation: 3.093942485651512]
	TIME [epoch: 9.7 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.607274045577858		[learning rate: 0.00032817]
	Learning Rate: 0.000328175
	LOSS [training: 3.607274045577858 | validation: 3.0914675573389423]
	TIME [epoch: 9.68 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6005893774802034		[learning rate: 0.00032698]
	Learning Rate: 0.000326984
	LOSS [training: 3.6005893774802034 | validation: 3.0823885324732823]
	TIME [epoch: 9.69 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5998152118294287		[learning rate: 0.0003258]
	Learning Rate: 0.000325797
	LOSS [training: 3.5998152118294287 | validation: 3.0807897480568034]
	TIME [epoch: 9.7 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601572935420451		[learning rate: 0.00032461]
	Learning Rate: 0.000324615
	LOSS [training: 3.601572935420451 | validation: 3.0818330252557895]
	TIME [epoch: 9.69 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.615852111808441		[learning rate: 0.00032344]
	Learning Rate: 0.000323437
	LOSS [training: 3.615852111808441 | validation: 3.0862871242909016]
	TIME [epoch: 9.69 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6008346996277667		[learning rate: 0.00032226]
	Learning Rate: 0.000322263
	LOSS [training: 3.6008346996277667 | validation: 3.0812531661619733]
	TIME [epoch: 9.69 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6067291153436627		[learning rate: 0.00032109]
	Learning Rate: 0.000321094
	LOSS [training: 3.6067291153436627 | validation: 3.079172351184513]
	TIME [epoch: 9.7 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6009214813229216		[learning rate: 0.00031993]
	Learning Rate: 0.000319928
	LOSS [training: 3.6009214813229216 | validation: 3.094392302146504]
	TIME [epoch: 9.69 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6006251165401877		[learning rate: 0.00031877]
	Learning Rate: 0.000318767
	LOSS [training: 3.6006251165401877 | validation: 3.088698907391443]
	TIME [epoch: 9.69 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6048157648456005		[learning rate: 0.00031761]
	Learning Rate: 0.000317611
	LOSS [training: 3.6048157648456005 | validation: 3.089928735159996]
	TIME [epoch: 9.71 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6119877229205883		[learning rate: 0.00031646]
	Learning Rate: 0.000316458
	LOSS [training: 3.6119877229205883 | validation: 3.0843801622564495]
	TIME [epoch: 9.69 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6076670068648915		[learning rate: 0.00031531]
	Learning Rate: 0.000315309
	LOSS [training: 3.6076670068648915 | validation: 3.0986848363169868]
	TIME [epoch: 9.69 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6050354768352753		[learning rate: 0.00031417]
	Learning Rate: 0.000314165
	LOSS [training: 3.6050354768352753 | validation: 3.1486553255090404]
	TIME [epoch: 9.7 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.643255808416702		[learning rate: 0.00031302]
	Learning Rate: 0.000313025
	LOSS [training: 3.643255808416702 | validation: 3.096245876359806]
	TIME [epoch: 9.7 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6038890065577234		[learning rate: 0.00031189]
	Learning Rate: 0.000311889
	LOSS [training: 3.6038890065577234 | validation: 3.092532447469756]
	TIME [epoch: 9.69 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.605356534242148		[learning rate: 0.00031076]
	Learning Rate: 0.000310757
	LOSS [training: 3.605356534242148 | validation: 3.084875210946439]
	TIME [epoch: 9.69 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596208740526353		[learning rate: 0.00030963]
	Learning Rate: 0.000309629
	LOSS [training: 3.596208740526353 | validation: 3.0807771289694292]
	TIME [epoch: 9.71 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6031143806709904		[learning rate: 0.00030851]
	Learning Rate: 0.000308506
	LOSS [training: 3.6031143806709904 | validation: 3.0852505753266275]
	TIME [epoch: 9.69 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.606898967773044		[learning rate: 0.00030739]
	Learning Rate: 0.000307386
	LOSS [training: 3.606898967773044 | validation: 3.121020828340192]
	TIME [epoch: 9.69 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.607650666987665		[learning rate: 0.00030627]
	Learning Rate: 0.000306271
	LOSS [training: 3.607650666987665 | validation: 3.096210052557284]
	TIME [epoch: 9.71 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603025914651101		[learning rate: 0.00030516]
	Learning Rate: 0.000305159
	LOSS [training: 3.603025914651101 | validation: 3.082697495484418]
	TIME [epoch: 9.69 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597447939005061		[learning rate: 0.00030405]
	Learning Rate: 0.000304052
	LOSS [training: 3.597447939005061 | validation: 3.1186192668054677]
	TIME [epoch: 9.69 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6012301048874313		[learning rate: 0.00030295]
	Learning Rate: 0.000302948
	LOSS [training: 3.6012301048874313 | validation: 3.094064338771527]
	TIME [epoch: 9.7 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6077517935107304		[learning rate: 0.00030185]
	Learning Rate: 0.000301849
	LOSS [training: 3.6077517935107304 | validation: 3.1041327632306057]
	TIME [epoch: 9.71 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.606582242826479		[learning rate: 0.00030075]
	Learning Rate: 0.000300753
	LOSS [training: 3.606582242826479 | validation: 3.089137935157166]
	TIME [epoch: 9.7 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5937244283858925		[learning rate: 0.00029966]
	Learning Rate: 0.000299662
	LOSS [training: 3.5937244283858925 | validation: 3.0936013231435573]
	TIME [epoch: 9.69 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603495537331733		[learning rate: 0.00029857]
	Learning Rate: 0.000298574
	LOSS [training: 3.603495537331733 | validation: 3.0934279305843577]
	TIME [epoch: 9.71 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6039931614138387		[learning rate: 0.00029749]
	Learning Rate: 0.000297491
	LOSS [training: 3.6039931614138387 | validation: 3.0934352623853996]
	TIME [epoch: 9.71 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6035884689840616		[learning rate: 0.00029641]
	Learning Rate: 0.000296411
	LOSS [training: 3.6035884689840616 | validation: 3.1006403275561554]
	TIME [epoch: 9.69 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.605103859695911		[learning rate: 0.00029534]
	Learning Rate: 0.000295336
	LOSS [training: 3.605103859695911 | validation: 3.098935413039511]
	TIME [epoch: 9.7 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599888972726151		[learning rate: 0.00029426]
	Learning Rate: 0.000294264
	LOSS [training: 3.599888972726151 | validation: 3.0843252406487442]
	TIME [epoch: 9.71 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.606828706067722		[learning rate: 0.0002932]
	Learning Rate: 0.000293196
	LOSS [training: 3.606828706067722 | validation: 3.086709034563985]
	TIME [epoch: 9.69 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6064473272039863		[learning rate: 0.00029213]
	Learning Rate: 0.000292132
	LOSS [training: 3.6064473272039863 | validation: 3.0917392232830183]
	TIME [epoch: 9.69 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6067340892133637		[learning rate: 0.00029107]
	Learning Rate: 0.000291072
	LOSS [training: 3.6067340892133637 | validation: 3.083818100661838]
	TIME [epoch: 9.72 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6014450655892247		[learning rate: 0.00029002]
	Learning Rate: 0.000290015
	LOSS [training: 3.6014450655892247 | validation: 3.078963069939687]
	TIME [epoch: 9.69 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6047603773512797		[learning rate: 0.00028896]
	Learning Rate: 0.000288963
	LOSS [training: 3.6047603773512797 | validation: 3.090135282545619]
	TIME [epoch: 9.69 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6015199761913466		[learning rate: 0.00028791]
	Learning Rate: 0.000287914
	LOSS [training: 3.6015199761913466 | validation: 3.0953641860845336]
	TIME [epoch: 9.71 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.607658162994748		[learning rate: 0.00028687]
	Learning Rate: 0.000286869
	LOSS [training: 3.607658162994748 | validation: 3.0824857240986865]
	TIME [epoch: 9.69 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60511930135662		[learning rate: 0.00028583]
	Learning Rate: 0.000285828
	LOSS [training: 3.60511930135662 | validation: 3.107436706401181]
	TIME [epoch: 9.69 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6074852468371112		[learning rate: 0.00028479]
	Learning Rate: 0.000284791
	LOSS [training: 3.6074852468371112 | validation: 3.083537128238514]
	TIME [epoch: 9.69 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.610955308001563		[learning rate: 0.00028376]
	Learning Rate: 0.000283758
	LOSS [training: 3.610955308001563 | validation: 3.088725689136471]
	TIME [epoch: 9.71 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5992490873646963		[learning rate: 0.00028273]
	Learning Rate: 0.000282728
	LOSS [training: 3.5992490873646963 | validation: 3.0895405481550693]
	TIME [epoch: 9.69 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603916271158444		[learning rate: 0.0002817]
	Learning Rate: 0.000281702
	LOSS [training: 3.603916271158444 | validation: 3.081266736891121]
	TIME [epoch: 9.69 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6006333187296518		[learning rate: 0.00028068]
	Learning Rate: 0.000280679
	LOSS [training: 3.6006333187296518 | validation: 3.100940027538348]
	TIME [epoch: 9.72 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6005579855575904		[learning rate: 0.00027966]
	Learning Rate: 0.000279661
	LOSS [training: 3.6005579855575904 | validation: 3.1063467631310266]
	TIME [epoch: 9.7 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5999754661961907		[learning rate: 0.00027865]
	Learning Rate: 0.000278646
	LOSS [training: 3.5999754661961907 | validation: 3.0836620195016953]
	TIME [epoch: 9.68 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6060258362616864		[learning rate: 0.00027763]
	Learning Rate: 0.000277635
	LOSS [training: 3.6060258362616864 | validation: 3.0976884360688817]
	TIME [epoch: 9.69 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.605221874701256		[learning rate: 0.00027663]
	Learning Rate: 0.000276627
	LOSS [training: 3.605221874701256 | validation: 3.082799272603073]
	TIME [epoch: 9.7 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6023652204181786		[learning rate: 0.00027562]
	Learning Rate: 0.000275623
	LOSS [training: 3.6023652204181786 | validation: 3.086968110508692]
	TIME [epoch: 9.69 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.605247378603443		[learning rate: 0.00027462]
	Learning Rate: 0.000274623
	LOSS [training: 3.605247378603443 | validation: 3.104539624348529]
	TIME [epoch: 9.69 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6012343095025416		[learning rate: 0.00027363]
	Learning Rate: 0.000273626
	LOSS [training: 3.6012343095025416 | validation: 3.103430579397027]
	TIME [epoch: 9.71 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6032007147767713		[learning rate: 0.00027263]
	Learning Rate: 0.000272633
	LOSS [training: 3.6032007147767713 | validation: 3.0940645523386014]
	TIME [epoch: 9.69 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603305203331692		[learning rate: 0.00027164]
	Learning Rate: 0.000271644
	LOSS [training: 3.603305203331692 | validation: 3.1012395714737035]
	TIME [epoch: 9.69 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6151020380860928		[learning rate: 0.00027066]
	Learning Rate: 0.000270658
	LOSS [training: 3.6151020380860928 | validation: 3.088546988184237]
	TIME [epoch: 9.7 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6032664756252686		[learning rate: 0.00026968]
	Learning Rate: 0.000269676
	LOSS [training: 3.6032664756252686 | validation: 3.084848341957494]
	TIME [epoch: 9.7 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6030657019643657		[learning rate: 0.0002687]
	Learning Rate: 0.000268697
	LOSS [training: 3.6030657019643657 | validation: 3.096763576939818]
	TIME [epoch: 9.69 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600320430758451		[learning rate: 0.00026772]
	Learning Rate: 0.000267722
	LOSS [training: 3.600320430758451 | validation: 3.0897011012146143]
	TIME [epoch: 9.69 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5998809871611988		[learning rate: 0.00026675]
	Learning Rate: 0.000266751
	LOSS [training: 3.5998809871611988 | validation: 3.0807565737560028]
	TIME [epoch: 9.71 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6177026478371572		[learning rate: 0.00026578]
	Learning Rate: 0.000265782
	LOSS [training: 3.6177026478371572 | validation: 3.09887182631698]
	TIME [epoch: 9.69 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6299501129659553		[learning rate: 0.00026482]
	Learning Rate: 0.000264818
	LOSS [training: 3.6299501129659553 | validation: 3.0996491168011664]
	TIME [epoch: 9.69 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6063330455408478		[learning rate: 0.00026386]
	Learning Rate: 0.000263857
	LOSS [training: 3.6063330455408478 | validation: 3.08562466282984]
	TIME [epoch: 9.71 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5975917890267852		[learning rate: 0.0002629]
	Learning Rate: 0.000262899
	LOSS [training: 3.5975917890267852 | validation: 3.089092473580196]
	TIME [epoch: 9.7 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6024701233208396		[learning rate: 0.00026195]
	Learning Rate: 0.000261945
	LOSS [training: 3.6024701233208396 | validation: 3.094288484277052]
	TIME [epoch: 9.69 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.610011670958696		[learning rate: 0.00026099]
	Learning Rate: 0.000260995
	LOSS [training: 3.610011670958696 | validation: 3.1323778956172896]
	TIME [epoch: 9.69 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6109781768295846		[learning rate: 0.00026005]
	Learning Rate: 0.000260047
	LOSS [training: 3.6109781768295846 | validation: 3.0896677677442415]
	TIME [epoch: 9.71 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6027828119013137		[learning rate: 0.0002591]
	Learning Rate: 0.000259104
	LOSS [training: 3.6027828119013137 | validation: 3.1058018398029823]
	TIME [epoch: 9.69 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6053060196076743		[learning rate: 0.00025816]
	Learning Rate: 0.000258163
	LOSS [training: 3.6053060196076743 | validation: 3.10517094472165]
	TIME [epoch: 9.69 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597777243956796		[learning rate: 0.00025723]
	Learning Rate: 0.000257227
	LOSS [training: 3.597777243956796 | validation: 3.085415870830825]
	TIME [epoch: 9.71 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5980649240866747		[learning rate: 0.00025629]
	Learning Rate: 0.000256293
	LOSS [training: 3.5980649240866747 | validation: 3.076434585238944]
	TIME [epoch: 9.68 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6023690126881123		[learning rate: 0.00025536]
	Learning Rate: 0.000255363
	LOSS [training: 3.6023690126881123 | validation: 3.0943203919328375]
	TIME [epoch: 9.69 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.607898919990967		[learning rate: 0.00025444]
	Learning Rate: 0.000254436
	LOSS [training: 3.607898919990967 | validation: 3.101440963420879]
	TIME [epoch: 9.69 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6053107992510105		[learning rate: 0.00025351]
	Learning Rate: 0.000253513
	LOSS [training: 3.6053107992510105 | validation: 3.093556881046287]
	TIME [epoch: 9.69 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.605599269324202		[learning rate: 0.00025259]
	Learning Rate: 0.000252593
	LOSS [training: 3.605599269324202 | validation: 3.1074382811833665]
	TIME [epoch: 9.68 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603839637508706		[learning rate: 0.00025168]
	Learning Rate: 0.000251676
	LOSS [training: 3.603839637508706 | validation: 3.1067672425132726]
	TIME [epoch: 9.68 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6091010450014407		[learning rate: 0.00025076]
	Learning Rate: 0.000250763
	LOSS [training: 3.6091010450014407 | validation: 3.0940160090975404]
	TIME [epoch: 9.7 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.609122102216341		[learning rate: 0.00024985]
	Learning Rate: 0.000249853
	LOSS [training: 3.609122102216341 | validation: 3.081274436288126]
	TIME [epoch: 9.68 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6000718050205647		[learning rate: 0.00024895]
	Learning Rate: 0.000248946
	LOSS [training: 3.6000718050205647 | validation: 3.106162097390218]
	TIME [epoch: 9.68 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6101278781407315		[learning rate: 0.00024804]
	Learning Rate: 0.000248043
	LOSS [training: 3.6101278781407315 | validation: 3.090586642990936]
	TIME [epoch: 9.7 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6006785787163365		[learning rate: 0.00024714]
	Learning Rate: 0.000247142
	LOSS [training: 3.6006785787163365 | validation: 3.099786266784702]
	TIME [epoch: 9.69 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599666993827791		[learning rate: 0.00024625]
	Learning Rate: 0.000246246
	LOSS [training: 3.599666993827791 | validation: 3.10421771193808]
	TIME [epoch: 9.69 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5987166887834796		[learning rate: 0.00024535]
	Learning Rate: 0.000245352
	LOSS [training: 3.5987166887834796 | validation: 3.0918513466188866]
	TIME [epoch: 9.69 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603016718396779		[learning rate: 0.00024446]
	Learning Rate: 0.000244462
	LOSS [training: 3.603016718396779 | validation: 3.084064330863242]
	TIME [epoch: 9.7 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599079029371069		[learning rate: 0.00024357]
	Learning Rate: 0.000243574
	LOSS [training: 3.599079029371069 | validation: 3.0910044016445823]
	TIME [epoch: 9.68 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599008371375131		[learning rate: 0.00024269]
	Learning Rate: 0.00024269
	LOSS [training: 3.599008371375131 | validation: 3.0973023151748373]
	TIME [epoch: 9.68 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602497047497407		[learning rate: 0.00024181]
	Learning Rate: 0.00024181
	LOSS [training: 3.602497047497407 | validation: 3.13805695465427]
	TIME [epoch: 9.7 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.620804918555838		[learning rate: 0.00024093]
	Learning Rate: 0.000240932
	LOSS [training: 3.620804918555838 | validation: 3.0890799446036024]
	TIME [epoch: 9.68 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.606613576355456		[learning rate: 0.00024006]
	Learning Rate: 0.000240058
	LOSS [training: 3.606613576355456 | validation: 3.0903345149708152]
	TIME [epoch: 9.68 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599136355895728		[learning rate: 0.00023919]
	Learning Rate: 0.000239187
	LOSS [training: 3.599136355895728 | validation: 3.0925359602492257]
	TIME [epoch: 9.68 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6055236828980703		[learning rate: 0.00023832]
	Learning Rate: 0.000238319
	LOSS [training: 3.6055236828980703 | validation: 3.112825482514919]
	TIME [epoch: 9.69 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6048129120834167		[learning rate: 0.00023745]
	Learning Rate: 0.000237454
	LOSS [training: 3.6048129120834167 | validation: 3.1022165946852023]
	TIME [epoch: 9.68 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6143549815907017		[learning rate: 0.00023659]
	Learning Rate: 0.000236592
	LOSS [training: 3.6143549815907017 | validation: 3.098353067833798]
	TIME [epoch: 9.68 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.606996737075921		[learning rate: 0.00023573]
	Learning Rate: 0.000235733
	LOSS [training: 3.606996737075921 | validation: 3.0924810529106996]
	TIME [epoch: 9.7 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5992165188050413		[learning rate: 0.00023488]
	Learning Rate: 0.000234878
	LOSS [training: 3.5992165188050413 | validation: 3.103007366471849]
	TIME [epoch: 9.68 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599454277891867		[learning rate: 0.00023403]
	Learning Rate: 0.000234026
	LOSS [training: 3.599454277891867 | validation: 3.0961430651707533]
	TIME [epoch: 9.68 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599362183985069		[learning rate: 0.00023318]
	Learning Rate: 0.000233176
	LOSS [training: 3.599362183985069 | validation: 3.0901576743768717]
	TIME [epoch: 9.7 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6069889049224875		[learning rate: 0.00023233]
	Learning Rate: 0.00023233
	LOSS [training: 3.6069889049224875 | validation: 3.091733164619186]
	TIME [epoch: 9.68 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6005181465142044		[learning rate: 0.00023149]
	Learning Rate: 0.000231487
	LOSS [training: 3.6005181465142044 | validation: 3.0933345307910094]
	TIME [epoch: 9.68 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6014759699449224		[learning rate: 0.00023065]
	Learning Rate: 0.000230647
	LOSS [training: 3.6014759699449224 | validation: 3.0968447202651386]
	TIME [epoch: 9.68 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6071298200435535		[learning rate: 0.00022981]
	Learning Rate: 0.00022981
	LOSS [training: 3.6071298200435535 | validation: 3.0913524250069355]
	TIME [epoch: 9.7 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.604339002624793		[learning rate: 0.00022898]
	Learning Rate: 0.000228976
	LOSS [training: 3.604339002624793 | validation: 3.093256165660564]
	TIME [epoch: 9.68 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6051879805632994		[learning rate: 0.00022814]
	Learning Rate: 0.000228145
	LOSS [training: 3.6051879805632994 | validation: 3.0924027594428822]
	TIME [epoch: 9.68 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.610936202264296		[learning rate: 0.00022732]
	Learning Rate: 0.000227317
	LOSS [training: 3.610936202264296 | validation: 3.0938842955138006]
	TIME [epoch: 9.7 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6101121152916136		[learning rate: 0.00022649]
	Learning Rate: 0.000226492
	LOSS [training: 3.6101121152916136 | validation: 3.0907452078633013]
	TIME [epoch: 9.68 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596156158315229		[learning rate: 0.00022567]
	Learning Rate: 0.00022567
	LOSS [training: 3.596156158315229 | validation: 3.0870268686814097]
	TIME [epoch: 9.68 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60668004870747		[learning rate: 0.00022485]
	Learning Rate: 0.000224851
	LOSS [training: 3.60668004870747 | validation: 3.1373052998625575]
	TIME [epoch: 9.68 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.615142447165771		[learning rate: 0.00022403]
	Learning Rate: 0.000224035
	LOSS [training: 3.615142447165771 | validation: 3.082791040147601]
	TIME [epoch: 9.7 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600668737041765		[learning rate: 0.00022322]
	Learning Rate: 0.000223222
	LOSS [training: 3.600668737041765 | validation: 3.085304162309824]
	TIME [epoch: 9.67 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6020315168345816		[learning rate: 0.00022241]
	Learning Rate: 0.000222412
	LOSS [training: 3.6020315168345816 | validation: 3.094173661977745]
	TIME [epoch: 9.68 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5952174673353787		[learning rate: 0.0002216]
	Learning Rate: 0.000221605
	LOSS [training: 3.5952174673353787 | validation: 3.0912377282072403]
	TIME [epoch: 9.7 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6038214767132226		[learning rate: 0.0002208]
	Learning Rate: 0.0002208
	LOSS [training: 3.6038214767132226 | validation: 3.094980533294306]
	TIME [epoch: 9.68 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6037073483427804		[learning rate: 0.00022]
	Learning Rate: 0.000219999
	LOSS [training: 3.6037073483427804 | validation: 3.0934418061770237]
	TIME [epoch: 9.68 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6148806049602356		[learning rate: 0.0002192]
	Learning Rate: 0.000219201
	LOSS [training: 3.6148806049602356 | validation: 3.1030303313754986]
	TIME [epoch: 9.69 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6054537667091267		[learning rate: 0.00021841]
	Learning Rate: 0.000218405
	LOSS [training: 3.6054537667091267 | validation: 3.092673459042273]
	TIME [epoch: 9.69 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.610526179648147		[learning rate: 0.00021761]
	Learning Rate: 0.000217613
	LOSS [training: 3.610526179648147 | validation: 3.094986449593817]
	TIME [epoch: 9.67 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6069018458009383		[learning rate: 0.00021682]
	Learning Rate: 0.000216823
	LOSS [training: 3.6069018458009383 | validation: 3.084177995441172]
	TIME [epoch: 9.68 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6061604070653646		[learning rate: 0.00021604]
	Learning Rate: 0.000216036
	LOSS [training: 3.6061604070653646 | validation: 3.0986548372359226]
	TIME [epoch: 9.7 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.612147550986782		[learning rate: 0.00021525]
	Learning Rate: 0.000215252
	LOSS [training: 3.612147550986782 | validation: 3.0796290244360023]
	TIME [epoch: 9.68 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602309957123404		[learning rate: 0.00021447]
	Learning Rate: 0.000214471
	LOSS [training: 3.602309957123404 | validation: 3.0883306819894596]
	TIME [epoch: 9.67 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.608835049677885		[learning rate: 0.00021369]
	Learning Rate: 0.000213693
	LOSS [training: 3.608835049677885 | validation: 3.0901199647195488]
	TIME [epoch: 9.69 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6019459597400547		[learning rate: 0.00021292]
	Learning Rate: 0.000212917
	LOSS [training: 3.6019459597400547 | validation: 3.0843946383740093]
	TIME [epoch: 9.68 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602266865598473		[learning rate: 0.00021214]
	Learning Rate: 0.000212144
	LOSS [training: 3.602266865598473 | validation: 3.0815396217616478]
	TIME [epoch: 9.67 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6035577894706057		[learning rate: 0.00021137]
	Learning Rate: 0.000211375
	LOSS [training: 3.6035577894706057 | validation: 3.0808616058997234]
	TIME [epoch: 9.68 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5988252537483048		[learning rate: 0.00021061]
	Learning Rate: 0.000210607
	LOSS [training: 3.5988252537483048 | validation: 3.0822136982934545]
	TIME [epoch: 9.69 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598793734396339		[learning rate: 0.00020984]
	Learning Rate: 0.000209843
	LOSS [training: 3.598793734396339 | validation: 3.090046493232709]
	TIME [epoch: 9.68 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6065476622211916		[learning rate: 0.00020908]
	Learning Rate: 0.000209082
	LOSS [training: 3.6065476622211916 | validation: 3.080901789205047]
	TIME [epoch: 9.67 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598030368783668		[learning rate: 0.00020832]
	Learning Rate: 0.000208323
	LOSS [training: 3.598030368783668 | validation: 3.1007174057080427]
	TIME [epoch: 9.7 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.61053554017201		[learning rate: 0.00020757]
	Learning Rate: 0.000207567
	LOSS [training: 3.61053554017201 | validation: 3.0951348056916506]
	TIME [epoch: 9.68 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.604136584928805		[learning rate: 0.00020681]
	Learning Rate: 0.000206814
	LOSS [training: 3.604136584928805 | validation: 3.1143044318505737]
	TIME [epoch: 9.68 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6043650609242825		[learning rate: 0.00020606]
	Learning Rate: 0.000206063
	LOSS [training: 3.6043650609242825 | validation: 3.100314895764412]
	TIME [epoch: 9.69 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6042778939968607		[learning rate: 0.00020532]
	Learning Rate: 0.000205315
	LOSS [training: 3.6042778939968607 | validation: 3.097468607260687]
	TIME [epoch: 9.69 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602795609766736		[learning rate: 0.00020457]
	Learning Rate: 0.00020457
	LOSS [training: 3.602795609766736 | validation: 3.090494466441195]
	TIME [epoch: 9.67 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5968689732568135		[learning rate: 0.00020383]
	Learning Rate: 0.000203828
	LOSS [training: 3.5968689732568135 | validation: 3.0869609342606057]
	TIME [epoch: 9.68 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5969858363440794		[learning rate: 0.00020309]
	Learning Rate: 0.000203088
	LOSS [training: 3.5969858363440794 | validation: 3.0884318124055383]
	TIME [epoch: 9.7 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6018504200599395		[learning rate: 0.00020235]
	Learning Rate: 0.000202351
	LOSS [training: 3.6018504200599395 | validation: 3.1004040747373036]
	TIME [epoch: 9.68 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602368004279731		[learning rate: 0.00020162]
	Learning Rate: 0.000201617
	LOSS [training: 3.602368004279731 | validation: 3.0967943947014094]
	TIME [epoch: 9.68 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6012338569018367		[learning rate: 0.00020088]
	Learning Rate: 0.000200885
	LOSS [training: 3.6012338569018367 | validation: 3.1009372315074164]
	TIME [epoch: 9.7 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598913929576505		[learning rate: 0.00020016]
	Learning Rate: 0.000200156
	LOSS [training: 3.598913929576505 | validation: 3.0928441479532442]
	TIME [epoch: 9.68 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595326370768133		[learning rate: 0.00019943]
	Learning Rate: 0.00019943
	LOSS [training: 3.595326370768133 | validation: 3.092866330013988]
	TIME [epoch: 9.68 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6134630132396977		[learning rate: 0.00019871]
	Learning Rate: 0.000198706
	LOSS [training: 3.6134630132396977 | validation: 3.1276719444208574]
	TIME [epoch: 9.68 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.609579496626432		[learning rate: 0.00019798]
	Learning Rate: 0.000197985
	LOSS [training: 3.609579496626432 | validation: 3.092059272927928]
	TIME [epoch: 9.7 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5986609791688577		[learning rate: 0.00019727]
	Learning Rate: 0.000197266
	LOSS [training: 3.5986609791688577 | validation: 3.1026335569250736]
	TIME [epoch: 9.67 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599353738147387		[learning rate: 0.00019655]
	Learning Rate: 0.00019655
	LOSS [training: 3.599353738147387 | validation: 3.0964253768755614]
	TIME [epoch: 9.68 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601671921768316		[learning rate: 0.00019584]
	Learning Rate: 0.000195837
	LOSS [training: 3.601671921768316 | validation: 3.088443534154326]
	TIME [epoch: 9.7 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6004190659175763		[learning rate: 0.00019513]
	Learning Rate: 0.000195126
	LOSS [training: 3.6004190659175763 | validation: 3.0949504290913854]
	TIME [epoch: 9.68 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597667016228981		[learning rate: 0.00019442]
	Learning Rate: 0.000194418
	LOSS [training: 3.597667016228981 | validation: 3.0854182408038775]
	TIME [epoch: 9.68 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597553863006257		[learning rate: 0.00019371]
	Learning Rate: 0.000193713
	LOSS [training: 3.597553863006257 | validation: 3.0899388668878727]
	TIME [epoch: 9.68 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6085236275430157		[learning rate: 0.00019301]
	Learning Rate: 0.00019301
	LOSS [training: 3.6085236275430157 | validation: 3.0952344110422576]
	TIME [epoch: 9.69 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.607063374421898		[learning rate: 0.00019231]
	Learning Rate: 0.000192309
	LOSS [training: 3.607063374421898 | validation: 3.079606202204945]
	TIME [epoch: 9.68 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603563774646517		[learning rate: 0.00019161]
	Learning Rate: 0.000191611
	LOSS [training: 3.603563774646517 | validation: 3.101162333927845]
	TIME [epoch: 9.68 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6015438225841536		[learning rate: 0.00019092]
	Learning Rate: 0.000190916
	LOSS [training: 3.6015438225841536 | validation: 3.0796987684117845]
	TIME [epoch: 9.7 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.594632705289746		[learning rate: 0.00019022]
	Learning Rate: 0.000190223
	LOSS [training: 3.594632705289746 | validation: 3.093768947950792]
	TIME [epoch: 9.68 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.607100854570736		[learning rate: 0.00018953]
	Learning Rate: 0.000189533
	LOSS [training: 3.607100854570736 | validation: 3.0917100660051244]
	TIME [epoch: 9.68 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.608417531524796		[learning rate: 0.00018884]
	Learning Rate: 0.000188845
	LOSS [training: 3.608417531524796 | validation: 3.107088605244109]
	TIME [epoch: 9.69 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6091656044073757		[learning rate: 0.00018816]
	Learning Rate: 0.00018816
	LOSS [training: 3.6091656044073757 | validation: 3.10010139341405]
	TIME [epoch: 9.69 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6079453865604996		[learning rate: 0.00018748]
	Learning Rate: 0.000187477
	LOSS [training: 3.6079453865604996 | validation: 3.0858520934825346]
	TIME [epoch: 9.67 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6089093470855027		[learning rate: 0.0001868]
	Learning Rate: 0.000186796
	LOSS [training: 3.6089093470855027 | validation: 3.0861115538954347]
	TIME [epoch: 9.68 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6018207305606103		[learning rate: 0.00018612]
	Learning Rate: 0.000186119
	LOSS [training: 3.6018207305606103 | validation: 3.0854770761255983]
	TIME [epoch: 9.7 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.615726618152953		[learning rate: 0.00018544]
	Learning Rate: 0.000185443
	LOSS [training: 3.615726618152953 | validation: 3.0842297154961686]
	TIME [epoch: 9.69 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.613465634539235		[learning rate: 0.00018477]
	Learning Rate: 0.00018477
	LOSS [training: 3.613465634539235 | validation: 3.111635776050637]
	TIME [epoch: 9.68 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.610976969907123		[learning rate: 0.0001841]
	Learning Rate: 0.000184099
	LOSS [training: 3.610976969907123 | validation: 3.1048855446237607]
	TIME [epoch: 9.7 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5960049622370733		[learning rate: 0.00018343]
	Learning Rate: 0.000183431
	LOSS [training: 3.5960049622370733 | validation: 3.1019698900022843]
	TIME [epoch: 9.68 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5969399127041894		[learning rate: 0.00018277]
	Learning Rate: 0.000182766
	LOSS [training: 3.5969399127041894 | validation: 3.099348576762831]
	TIME [epoch: 9.68 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5978018396743088		[learning rate: 0.0001821]
	Learning Rate: 0.000182102
	LOSS [training: 3.5978018396743088 | validation: 3.090125959863375]
	TIME [epoch: 9.68 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6084177173950875		[learning rate: 0.00018144]
	Learning Rate: 0.000181442
	LOSS [training: 3.6084177173950875 | validation: 3.0847001799942917]
	TIME [epoch: 9.7 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6023575530053664		[learning rate: 0.00018078]
	Learning Rate: 0.000180783
	LOSS [training: 3.6023575530053664 | validation: 3.103279753017229]
	TIME [epoch: 9.68 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6010557988943743		[learning rate: 0.00018013]
	Learning Rate: 0.000180127
	LOSS [training: 3.6010557988943743 | validation: 3.102553598273861]
	TIME [epoch: 9.68 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.593289893175423		[learning rate: 0.00017947]
	Learning Rate: 0.000179473
	LOSS [training: 3.593289893175423 | validation: 3.0915816585106377]
	TIME [epoch: 9.7 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60591800662363		[learning rate: 0.00017882]
	Learning Rate: 0.000178822
	LOSS [training: 3.60591800662363 | validation: 3.111238895658153]
	TIME [epoch: 9.69 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.609919715032004		[learning rate: 0.00017817]
	Learning Rate: 0.000178173
	LOSS [training: 3.609919715032004 | validation: 3.1235310749003746]
	TIME [epoch: 9.68 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6069798475664756		[learning rate: 0.00017753]
	Learning Rate: 0.000177526
	LOSS [training: 3.6069798475664756 | validation: 3.1018162759891768]
	TIME [epoch: 9.7 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6009618743208756		[learning rate: 0.00017688]
	Learning Rate: 0.000176882
	LOSS [training: 3.6009618743208756 | validation: 3.0848501021303463]
	TIME [epoch: 9.69 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598861151577112		[learning rate: 0.00017624]
	Learning Rate: 0.00017624
	LOSS [training: 3.598861151577112 | validation: 3.1043021294260416]
	TIME [epoch: 9.68 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6023947052527845		[learning rate: 0.0001756]
	Learning Rate: 0.000175601
	LOSS [training: 3.6023947052527845 | validation: 3.0853576882732434]
	TIME [epoch: 9.68 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60558190019419		[learning rate: 0.00017496]
	Learning Rate: 0.000174964
	LOSS [training: 3.60558190019419 | validation: 3.1130907686751046]
	TIME [epoch: 9.7 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596706699814009		[learning rate: 0.00017433]
	Learning Rate: 0.000174329
	LOSS [training: 3.596706699814009 | validation: 3.1033994847974733]
	TIME [epoch: 9.68 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5980466652474745		[learning rate: 0.0001737]
	Learning Rate: 0.000173696
	LOSS [training: 3.5980466652474745 | validation: 3.1005585962287445]
	TIME [epoch: 9.68 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6008481271777804		[learning rate: 0.00017307]
	Learning Rate: 0.000173066
	LOSS [training: 3.6008481271777804 | validation: 3.081940174365733]
	TIME [epoch: 9.7 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6013692864950615		[learning rate: 0.00017244]
	Learning Rate: 0.000172437
	LOSS [training: 3.6013692864950615 | validation: 3.081290355614449]
	TIME [epoch: 9.68 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6060530656233487		[learning rate: 0.00017181]
	Learning Rate: 0.000171812
	LOSS [training: 3.6060530656233487 | validation: 3.0869563461659255]
	TIME [epoch: 9.68 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5983481773231447		[learning rate: 0.00017119]
	Learning Rate: 0.000171188
	LOSS [training: 3.5983481773231447 | validation: 3.1011838792058177]
	TIME [epoch: 9.68 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597842207451226		[learning rate: 0.00017057]
	Learning Rate: 0.000170567
	LOSS [training: 3.597842207451226 | validation: 3.09199863407796]
	TIME [epoch: 9.7 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5989574641247146		[learning rate: 0.00016995]
	Learning Rate: 0.000169948
	LOSS [training: 3.5989574641247146 | validation: 3.0953197740414238]
	TIME [epoch: 9.68 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6065420819188945		[learning rate: 0.00016933]
	Learning Rate: 0.000169331
	LOSS [training: 3.6065420819188945 | validation: 3.092078177118699]
	TIME [epoch: 9.68 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.611464627784682		[learning rate: 0.00016872]
	Learning Rate: 0.000168717
	LOSS [training: 3.611464627784682 | validation: 3.1029798719812614]
	TIME [epoch: 9.69 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600276040838975		[learning rate: 0.0001681]
	Learning Rate: 0.000168104
	LOSS [training: 3.600276040838975 | validation: 3.097717684106236]
	TIME [epoch: 9.67 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6031803990779885		[learning rate: 0.00016749]
	Learning Rate: 0.000167494
	LOSS [training: 3.6031803990779885 | validation: 3.082120527261277]
	TIME [epoch: 9.68 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5976303100086944		[learning rate: 0.00016689]
	Learning Rate: 0.000166886
	LOSS [training: 3.5976303100086944 | validation: 3.0921003211752147]
	TIME [epoch: 9.69 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6035273298008845		[learning rate: 0.00016628]
	Learning Rate: 0.000166281
	LOSS [training: 3.6035273298008845 | validation: 3.0970384842266383]
	TIME [epoch: 9.7 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6152252245714216		[learning rate: 0.00016568]
	Learning Rate: 0.000165677
	LOSS [training: 3.6152252245714216 | validation: 3.09003237601103]
	TIME [epoch: 9.67 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6090121594598323		[learning rate: 0.00016508]
	Learning Rate: 0.000165076
	LOSS [training: 3.6090121594598323 | validation: 3.0974517562452566]
	TIME [epoch: 9.67 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6020001948110165		[learning rate: 0.00016448]
	Learning Rate: 0.000164477
	LOSS [training: 3.6020001948110165 | validation: 3.0847738495037085]
	TIME [epoch: 9.7 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5982759676040432		[learning rate: 0.00016388]
	Learning Rate: 0.00016388
	LOSS [training: 3.5982759676040432 | validation: 3.0901204235362503]
	TIME [epoch: 9.68 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597362453162174		[learning rate: 0.00016329]
	Learning Rate: 0.000163285
	LOSS [training: 3.597362453162174 | validation: 3.1020701980304297]
	TIME [epoch: 9.67 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6050806705626663		[learning rate: 0.00016269]
	Learning Rate: 0.000162693
	LOSS [training: 3.6050806705626663 | validation: 3.09462802693414]
	TIME [epoch: 9.7 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5980181843138688		[learning rate: 0.0001621]
	Learning Rate: 0.000162102
	LOSS [training: 3.5980181843138688 | validation: 3.0968505352502156]
	TIME [epoch: 9.68 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6017022836883		[learning rate: 0.00016151]
	Learning Rate: 0.000161514
	LOSS [training: 3.6017022836883 | validation: 3.0998959014913696]
	TIME [epoch: 9.68 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6030720371308105		[learning rate: 0.00016093]
	Learning Rate: 0.000160928
	LOSS [training: 3.6030720371308105 | validation: 3.105927122803497]
	TIME [epoch: 9.68 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6037310699657708		[learning rate: 0.00016034]
	Learning Rate: 0.000160344
	LOSS [training: 3.6037310699657708 | validation: 3.0768305701000362]
	TIME [epoch: 9.7 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6068983289056127		[learning rate: 0.00015976]
	Learning Rate: 0.000159762
	LOSS [training: 3.6068983289056127 | validation: 3.0883383828251114]
	TIME [epoch: 9.68 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6057008622457274		[learning rate: 0.00015918]
	Learning Rate: 0.000159182
	LOSS [training: 3.6057008622457274 | validation: 3.0815407052064963]
	TIME [epoch: 9.68 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601800206722435		[learning rate: 0.0001586]
	Learning Rate: 0.000158605
	LOSS [training: 3.601800206722435 | validation: 3.1034330291737264]
	TIME [epoch: 9.7 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6001412383186535		[learning rate: 0.00015803]
	Learning Rate: 0.000158029
	LOSS [training: 3.6001412383186535 | validation: 3.0952768731483946]
	TIME [epoch: 9.68 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597935930882406		[learning rate: 0.00015746]
	Learning Rate: 0.000157456
	LOSS [training: 3.597935930882406 | validation: 3.095754474080891]
	TIME [epoch: 9.68 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5975799055954125		[learning rate: 0.00015688]
	Learning Rate: 0.000156884
	LOSS [training: 3.5975799055954125 | validation: 3.104669481540588]
	TIME [epoch: 9.69 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5986037122462156		[learning rate: 0.00015631]
	Learning Rate: 0.000156315
	LOSS [training: 3.5986037122462156 | validation: 3.099459122544885]
	TIME [epoch: 9.7 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600678990412214		[learning rate: 0.00015575]
	Learning Rate: 0.000155748
	LOSS [training: 3.600678990412214 | validation: 3.1048503600214192]
	TIME [epoch: 9.68 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.606270218643169		[learning rate: 0.00015518]
	Learning Rate: 0.000155182
	LOSS [training: 3.606270218643169 | validation: 3.097355141933306]
	TIME [epoch: 9.68 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5964622924615015		[learning rate: 0.00015462]
	Learning Rate: 0.000154619
	LOSS [training: 3.5964622924615015 | validation: 3.090404277456494]
	TIME [epoch: 9.7 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601197680466028		[learning rate: 0.00015406]
	Learning Rate: 0.000154058
	LOSS [training: 3.601197680466028 | validation: 3.0962724415902008]
	TIME [epoch: 9.68 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5999013673531834		[learning rate: 0.0001535]
	Learning Rate: 0.000153499
	LOSS [training: 3.5999013673531834 | validation: 3.0866396845096236]
	TIME [epoch: 9.68 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6023450307244276		[learning rate: 0.00015294]
	Learning Rate: 0.000152942
	LOSS [training: 3.6023450307244276 | validation: 3.100800713268751]
	TIME [epoch: 9.69 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60093332150788		[learning rate: 0.00015239]
	Learning Rate: 0.000152387
	LOSS [training: 3.60093332150788 | validation: 3.0841685597397577]
	TIME [epoch: 9.69 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5995965884934904		[learning rate: 0.00015183]
	Learning Rate: 0.000151834
	LOSS [training: 3.5995965884934904 | validation: 3.101964835656922]
	TIME [epoch: 9.67 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597938419426389		[learning rate: 0.00015128]
	Learning Rate: 0.000151283
	LOSS [training: 3.597938419426389 | validation: 3.0878482844240267]
	TIME [epoch: 9.68 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603485798918084		[learning rate: 0.00015073]
	Learning Rate: 0.000150734
	LOSS [training: 3.603485798918084 | validation: 3.1135597604439718]
	TIME [epoch: 9.7 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.606248389375653		[learning rate: 0.00015019]
	Learning Rate: 0.000150187
	LOSS [training: 3.606248389375653 | validation: 3.0972367800057947]
	TIME [epoch: 9.69 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600728655943064		[learning rate: 0.00014964]
	Learning Rate: 0.000149642
	LOSS [training: 3.600728655943064 | validation: 3.084339171856139]
	TIME [epoch: 9.68 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59717918000577		[learning rate: 0.0001491]
	Learning Rate: 0.000149099
	LOSS [training: 3.59717918000577 | validation: 3.0866824062355103]
	TIME [epoch: 9.7 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6011792750673957		[learning rate: 0.00014856]
	Learning Rate: 0.000148558
	LOSS [training: 3.6011792750673957 | validation: 3.082965963398028]
	TIME [epoch: 9.69 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599451032005377		[learning rate: 0.00014802]
	Learning Rate: 0.000148018
	LOSS [training: 3.599451032005377 | validation: 3.0879109472804966]
	TIME [epoch: 9.68 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6027449168247307		[learning rate: 0.00014748]
	Learning Rate: 0.000147481
	LOSS [training: 3.6027449168247307 | validation: 3.086417952933009]
	TIME [epoch: 9.69 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5997275013065355		[learning rate: 0.00014695]
	Learning Rate: 0.000146946
	LOSS [training: 3.5997275013065355 | validation: 3.081691632304615]
	TIME [epoch: 9.69 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6013107899635295		[learning rate: 0.00014641]
	Learning Rate: 0.000146413
	LOSS [training: 3.6013107899635295 | validation: 3.0987160721132536]
	TIME [epoch: 9.68 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5982029828175603		[learning rate: 0.00014588]
	Learning Rate: 0.000145881
	LOSS [training: 3.5982029828175603 | validation: 3.092209115877097]
	TIME [epoch: 9.68 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602390800757084		[learning rate: 0.00014535]
	Learning Rate: 0.000145352
	LOSS [training: 3.602390800757084 | validation: 3.086755969813858]
	TIME [epoch: 9.7 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5976122256463356		[learning rate: 0.00014482]
	Learning Rate: 0.000144825
	LOSS [training: 3.5976122256463356 | validation: 3.0916025506457743]
	TIME [epoch: 9.68 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5980750481523756		[learning rate: 0.0001443]
	Learning Rate: 0.000144299
	LOSS [training: 3.5980750481523756 | validation: 3.0912770561210063]
	TIME [epoch: 9.68 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598110904805915		[learning rate: 0.00014378]
	Learning Rate: 0.000143775
	LOSS [training: 3.598110904805915 | validation: 3.085303748408378]
	TIME [epoch: 9.68 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5988083170141225		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 3.5988083170141225 | validation: 3.0823522431411865]
	TIME [epoch: 9.7 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5969146017773554		[learning rate: 0.00014273]
	Learning Rate: 0.000142734
	LOSS [training: 3.5969146017773554 | validation: 3.0805840709135395]
	TIME [epoch: 9.69 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603440410727555		[learning rate: 0.00014222]
	Learning Rate: 0.000142216
	LOSS [training: 3.603440410727555 | validation: 3.0894931275316857]
	TIME [epoch: 9.68 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601295782634934		[learning rate: 0.0001417]
	Learning Rate: 0.0001417
	LOSS [training: 3.601295782634934 | validation: 3.102224797356929]
	TIME [epoch: 9.71 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6018631900619553		[learning rate: 0.00014119]
	Learning Rate: 0.000141185
	LOSS [training: 3.6018631900619553 | validation: 3.0732044617746044]
	TIME [epoch: 9.68 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596004413914738		[learning rate: 0.00014067]
	Learning Rate: 0.000140673
	LOSS [training: 3.596004413914738 | validation: 3.0843451486171705]
	TIME [epoch: 9.68 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6035233324794556		[learning rate: 0.00014016]
	Learning Rate: 0.000140162
	LOSS [training: 3.6035233324794556 | validation: 3.0763760123595962]
	TIME [epoch: 9.7 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597726966972023		[learning rate: 0.00013965]
	Learning Rate: 0.000139654
	LOSS [training: 3.597726966972023 | validation: 3.0926109657684777]
	TIME [epoch: 9.68 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6045312223196895		[learning rate: 0.00013915]
	Learning Rate: 0.000139147
	LOSS [training: 3.6045312223196895 | validation: 3.1067919639979413]
	TIME [epoch: 9.68 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.608027275168923		[learning rate: 0.00013864]
	Learning Rate: 0.000138642
	LOSS [training: 3.608027275168923 | validation: 3.1059588400224865]
	TIME [epoch: 9.68 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6065864369049607		[learning rate: 0.00013814]
	Learning Rate: 0.000138139
	LOSS [training: 3.6065864369049607 | validation: 3.079093600897709]
	TIME [epoch: 9.7 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6005508235778314		[learning rate: 0.00013764]
	Learning Rate: 0.000137638
	LOSS [training: 3.6005508235778314 | validation: 3.0734850692002316]
	TIME [epoch: 9.68 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.605129366457131		[learning rate: 0.00013714]
	Learning Rate: 0.000137138
	LOSS [training: 3.605129366457131 | validation: 3.1064551661957673]
	TIME [epoch: 9.68 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598759951202177		[learning rate: 0.00013664]
	Learning Rate: 0.00013664
	LOSS [training: 3.598759951202177 | validation: 3.0888433225215186]
	TIME [epoch: 9.7 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6004143103144806		[learning rate: 0.00013614]
	Learning Rate: 0.000136144
	LOSS [training: 3.6004143103144806 | validation: 3.109884579372267]
	TIME [epoch: 9.69 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6066152981877764		[learning rate: 0.00013565]
	Learning Rate: 0.00013565
	LOSS [training: 3.6066152981877764 | validation: 3.1027509191872635]
	TIME [epoch: 9.68 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6096836091995463		[learning rate: 0.00013516]
	Learning Rate: 0.000135158
	LOSS [training: 3.6096836091995463 | validation: 3.1144426505148917]
	TIME [epoch: 9.69 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6024166906180484		[learning rate: 0.00013467]
	Learning Rate: 0.000134668
	LOSS [training: 3.6024166906180484 | validation: 3.0938341321007155]
	TIME [epoch: 9.7 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5986564784042665		[learning rate: 0.00013418]
	Learning Rate: 0.000134179
	LOSS [training: 3.5986564784042665 | validation: 3.0915587986010102]
	TIME [epoch: 9.68 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.609560446036775		[learning rate: 0.00013369]
	Learning Rate: 0.000133692
	LOSS [training: 3.609560446036775 | validation: 3.091477248548076]
	TIME [epoch: 9.68 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60532821537834		[learning rate: 0.00013321]
	Learning Rate: 0.000133207
	LOSS [training: 3.60532821537834 | validation: 3.098040093217423]
	TIME [epoch: 9.71 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6073638130947563		[learning rate: 0.00013272]
	Learning Rate: 0.000132723
	LOSS [training: 3.6073638130947563 | validation: 3.09249122592969]
	TIME [epoch: 9.68 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6057529722380757		[learning rate: 0.00013224]
	Learning Rate: 0.000132242
	LOSS [training: 3.6057529722380757 | validation: 3.0902819362657112]
	TIME [epoch: 9.68 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6062716981644973		[learning rate: 0.00013176]
	Learning Rate: 0.000131762
	LOSS [training: 3.6062716981644973 | validation: 3.086057435959919]
	TIME [epoch: 9.7 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5991257065807956		[learning rate: 0.00013128]
	Learning Rate: 0.000131284
	LOSS [training: 3.5991257065807956 | validation: 3.085291276720568]
	TIME [epoch: 9.7 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5994462330074186		[learning rate: 0.00013081]
	Learning Rate: 0.000130807
	LOSS [training: 3.5994462330074186 | validation: 3.078146155941807]
	TIME [epoch: 9.68 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602251686250149		[learning rate: 0.00013033]
	Learning Rate: 0.000130332
	LOSS [training: 3.602251686250149 | validation: 3.0776888962657836]
	TIME [epoch: 9.68 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6006927707945664		[learning rate: 0.00012986]
	Learning Rate: 0.000129859
	LOSS [training: 3.6006927707945664 | validation: 3.0780794920270376]
	TIME [epoch: 9.71 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603586114552993		[learning rate: 0.00012939]
	Learning Rate: 0.000129388
	LOSS [training: 3.603586114552993 | validation: 3.094651274874077]
	TIME [epoch: 9.69 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598738971485685		[learning rate: 0.00012892]
	Learning Rate: 0.000128919
	LOSS [training: 3.598738971485685 | validation: 3.0896116036520676]
	TIME [epoch: 9.68 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5971767390898384		[learning rate: 0.00012845]
	Learning Rate: 0.000128451
	LOSS [training: 3.5971767390898384 | validation: 3.087617949470922]
	TIME [epoch: 9.7 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6038424568802965		[learning rate: 0.00012798]
	Learning Rate: 0.000127985
	LOSS [training: 3.6038424568802965 | validation: 3.08077066656138]
	TIME [epoch: 9.68 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6018720486521523		[learning rate: 0.00012752]
	Learning Rate: 0.00012752
	LOSS [training: 3.6018720486521523 | validation: 3.1052583096581494]
	TIME [epoch: 9.68 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6025863803574842		[learning rate: 0.00012706]
	Learning Rate: 0.000127057
	LOSS [training: 3.6025863803574842 | validation: 3.0920994027089574]
	TIME [epoch: 9.69 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599617557923687		[learning rate: 0.0001266]
	Learning Rate: 0.000126596
	LOSS [training: 3.599617557923687 | validation: 3.090153855866199]
	TIME [epoch: 9.7 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6046969035172056		[learning rate: 0.00012614]
	Learning Rate: 0.000126137
	LOSS [training: 3.6046969035172056 | validation: 3.0828979329208734]
	TIME [epoch: 9.68 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6024642178040454		[learning rate: 0.00012568]
	Learning Rate: 0.000125679
	LOSS [training: 3.6024642178040454 | validation: 3.0910127832797496]
	TIME [epoch: 9.68 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5991837908002857		[learning rate: 0.00012522]
	Learning Rate: 0.000125223
	LOSS [training: 3.5991837908002857 | validation: 3.0883115208333356]
	TIME [epoch: 9.7 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6054680234918095		[learning rate: 0.00012477]
	Learning Rate: 0.000124769
	LOSS [training: 3.6054680234918095 | validation: 3.100952539026545]
	TIME [epoch: 9.68 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6060131877269277		[learning rate: 0.00012432]
	Learning Rate: 0.000124316
	LOSS [training: 3.6060131877269277 | validation: 3.081661439068038]
	TIME [epoch: 9.68 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6011002443935127		[learning rate: 0.00012386]
	Learning Rate: 0.000123865
	LOSS [training: 3.6011002443935127 | validation: 3.092167344783852]
	TIME [epoch: 9.69 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603981321807294		[learning rate: 0.00012342]
	Learning Rate: 0.000123415
	LOSS [training: 3.603981321807294 | validation: 3.1071727250499057]
	TIME [epoch: 9.69 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602162213644591		[learning rate: 0.00012297]
	Learning Rate: 0.000122967
	LOSS [training: 3.602162213644591 | validation: 3.0823664669959245]
	TIME [epoch: 9.68 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5979554087502095		[learning rate: 0.00012252]
	Learning Rate: 0.000122521
	LOSS [training: 3.5979554087502095 | validation: 3.0977103052633583]
	TIME [epoch: 9.68 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5943991614198687		[learning rate: 0.00012208]
	Learning Rate: 0.000122076
	LOSS [training: 3.5943991614198687 | validation: 3.0940613671392874]
	TIME [epoch: 9.7 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603695079452578		[learning rate: 0.00012163]
	Learning Rate: 0.000121633
	LOSS [training: 3.603695079452578 | validation: 3.1050247951121173]
	TIME [epoch: 9.68 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.610037797740926		[learning rate: 0.00012119]
	Learning Rate: 0.000121192
	LOSS [training: 3.610037797740926 | validation: 3.104029733513308]
	TIME [epoch: 9.68 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.604486469099276		[learning rate: 0.00012075]
	Learning Rate: 0.000120752
	LOSS [training: 3.604486469099276 | validation: 3.0959280058950247]
	TIME [epoch: 9.71 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5990897574179863		[learning rate: 0.00012031]
	Learning Rate: 0.000120314
	LOSS [training: 3.5990897574179863 | validation: 3.0868735854104727]
	TIME [epoch: 9.69 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5974196584858857		[learning rate: 0.00011988]
	Learning Rate: 0.000119877
	LOSS [training: 3.5974196584858857 | validation: 3.097047163579224]
	TIME [epoch: 9.68 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595387846533595		[learning rate: 0.00011944]
	Learning Rate: 0.000119442
	LOSS [training: 3.595387846533595 | validation: 3.085253297043589]
	TIME [epoch: 9.68 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5974666818377594		[learning rate: 0.00011901]
	Learning Rate: 0.000119009
	LOSS [training: 3.5974666818377594 | validation: 3.0791531758602173]
	TIME [epoch: 9.71 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599122746146395		[learning rate: 0.00011858]
	Learning Rate: 0.000118577
	LOSS [training: 3.599122746146395 | validation: 3.0869502036702743]
	TIME [epoch: 9.69 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601094820637633		[learning rate: 0.00011815]
	Learning Rate: 0.000118147
	LOSS [training: 3.601094820637633 | validation: 3.1107292476250796]
	TIME [epoch: 9.69 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603547544336652		[learning rate: 0.00011772]
	Learning Rate: 0.000117718
	LOSS [training: 3.603547544336652 | validation: 3.087386542503839]
	TIME [epoch: 9.7 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59473662977132		[learning rate: 0.00011729]
	Learning Rate: 0.000117291
	LOSS [training: 3.59473662977132 | validation: 3.0832976833009647]
	TIME [epoch: 9.68 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603839661400116		[learning rate: 0.00011686]
	Learning Rate: 0.000116865
	LOSS [training: 3.603839661400116 | validation: 3.082431814051497]
	TIME [epoch: 9.68 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6035434473186485		[learning rate: 0.00011644]
	Learning Rate: 0.000116441
	LOSS [training: 3.6035434473186485 | validation: 3.085335196463491]
	TIME [epoch: 9.69 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5986748232017547		[learning rate: 0.00011602]
	Learning Rate: 0.000116018
	LOSS [training: 3.5986748232017547 | validation: 3.08846173298385]
	TIME [epoch: 9.69 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5957804433903093		[learning rate: 0.0001156]
	Learning Rate: 0.000115597
	LOSS [training: 3.5957804433903093 | validation: 3.0947865692410743]
	TIME [epoch: 9.68 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596768208239979		[learning rate: 0.00011518]
	Learning Rate: 0.000115178
	LOSS [training: 3.596768208239979 | validation: 3.07749688139835]
	TIME [epoch: 9.68 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5944440943479075		[learning rate: 0.00011476]
	Learning Rate: 0.00011476
	LOSS [training: 3.5944440943479075 | validation: 3.0793391070689076]
	TIME [epoch: 9.71 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5981027111119253		[learning rate: 0.00011434]
	Learning Rate: 0.000114343
	LOSS [training: 3.5981027111119253 | validation: 3.087827300945197]
	TIME [epoch: 9.69 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6029015591788713		[learning rate: 0.00011393]
	Learning Rate: 0.000113928
	LOSS [training: 3.6029015591788713 | validation: 3.099854088851531]
	TIME [epoch: 9.69 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601886379315313		[learning rate: 0.00011351]
	Learning Rate: 0.000113515
	LOSS [training: 3.601886379315313 | validation: 3.0826828025513033]
	TIME [epoch: 9.7 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600139634162429		[learning rate: 0.0001131]
	Learning Rate: 0.000113103
	LOSS [training: 3.600139634162429 | validation: 3.0810553185820657]
	TIME [epoch: 9.69 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6031092431508562		[learning rate: 0.00011269]
	Learning Rate: 0.000112692
	LOSS [training: 3.6031092431508562 | validation: 3.093485248931864]
	TIME [epoch: 9.69 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597827721399253		[learning rate: 0.00011228]
	Learning Rate: 0.000112283
	LOSS [training: 3.597827721399253 | validation: 3.0828663611579716]
	TIME [epoch: 9.68 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59818259331385		[learning rate: 0.00011188]
	Learning Rate: 0.000111876
	LOSS [training: 3.59818259331385 | validation: 3.0838041086600407]
	TIME [epoch: 9.7 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600327063808849		[learning rate: 0.00011147]
	Learning Rate: 0.00011147
	LOSS [training: 3.600327063808849 | validation: 3.0833542043529065]
	TIME [epoch: 9.68 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6012169717244618		[learning rate: 0.00011107]
	Learning Rate: 0.000111065
	LOSS [training: 3.6012169717244618 | validation: 3.090721381360845]
	TIME [epoch: 9.69 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.604894719716102		[learning rate: 0.00011066]
	Learning Rate: 0.000110662
	LOSS [training: 3.604894719716102 | validation: 3.095252268604672]
	TIME [epoch: 9.7 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5922745960936977		[learning rate: 0.00011026]
	Learning Rate: 0.000110261
	LOSS [training: 3.5922745960936977 | validation: 3.0787219765914053]
	TIME [epoch: 9.69 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6043836348400546		[learning rate: 0.00010986]
	Learning Rate: 0.000109861
	LOSS [training: 3.6043836348400546 | validation: 3.0984184274241957]
	TIME [epoch: 9.68 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595759476430609		[learning rate: 0.00010946]
	Learning Rate: 0.000109462
	LOSS [training: 3.595759476430609 | validation: 3.087170136643711]
	TIME [epoch: 9.69 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5961569318979927		[learning rate: 0.00010906]
	Learning Rate: 0.000109065
	LOSS [training: 3.5961569318979927 | validation: 3.088005487605459]
	TIME [epoch: 9.7 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599113087942311		[learning rate: 0.00010867]
	Learning Rate: 0.000108669
	LOSS [training: 3.599113087942311 | validation: 3.1072770740914657]
	TIME [epoch: 9.69 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.604549201016976		[learning rate: 0.00010827]
	Learning Rate: 0.000108275
	LOSS [training: 3.604549201016976 | validation: 3.0923026601278902]
	TIME [epoch: 9.68 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5928682457499397		[learning rate: 0.00010788]
	Learning Rate: 0.000107882
	LOSS [training: 3.5928682457499397 | validation: 3.0869093328207318]
	TIME [epoch: 9.71 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.605994659711714		[learning rate: 0.00010749]
	Learning Rate: 0.00010749
	LOSS [training: 3.605994659711714 | validation: 3.08271180113289]
	TIME [epoch: 9.68 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5973420179369127		[learning rate: 0.0001071]
	Learning Rate: 0.0001071
	LOSS [training: 3.5973420179369127 | validation: 3.090840060700806]
	TIME [epoch: 9.69 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6011116637981453		[learning rate: 0.00010671]
	Learning Rate: 0.000106711
	LOSS [training: 3.6011116637981453 | validation: 3.0823890714401445]
	TIME [epoch: 9.71 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599475315171168		[learning rate: 0.00010632]
	Learning Rate: 0.000106324
	LOSS [training: 3.599475315171168 | validation: 3.0982744832649236]
	TIME [epoch: 9.69 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6035036387944728		[learning rate: 0.00010594]
	Learning Rate: 0.000105938
	LOSS [training: 3.6035036387944728 | validation: 3.084789569755957]
	TIME [epoch: 9.68 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6018982654768754		[learning rate: 0.00010555]
	Learning Rate: 0.000105554
	LOSS [training: 3.6018982654768754 | validation: 3.0808589025365745]
	TIME [epoch: 9.69 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5986090984183114		[learning rate: 0.00010517]
	Learning Rate: 0.000105171
	LOSS [training: 3.5986090984183114 | validation: 3.1005200536508206]
	TIME [epoch: 9.71 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5962674173098828		[learning rate: 0.00010479]
	Learning Rate: 0.000104789
	LOSS [training: 3.5962674173098828 | validation: 3.091708992485885]
	TIME [epoch: 9.7 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.604776993030085		[learning rate: 0.00010441]
	Learning Rate: 0.000104409
	LOSS [training: 3.604776993030085 | validation: 3.1042338863779326]
	TIME [epoch: 9.68 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6041671836215796		[learning rate: 0.00010403]
	Learning Rate: 0.00010403
	LOSS [training: 3.6041671836215796 | validation: 3.097647636904778]
	TIME [epoch: 9.7 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6000198037863784		[learning rate: 0.00010365]
	Learning Rate: 0.000103652
	LOSS [training: 3.6000198037863784 | validation: 3.0930863380083236]
	TIME [epoch: 9.69 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.607549567515851		[learning rate: 0.00010328]
	Learning Rate: 0.000103276
	LOSS [training: 3.607549567515851 | validation: 3.0920159769671685]
	TIME [epoch: 9.69 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59693358972453		[learning rate: 0.0001029]
	Learning Rate: 0.000102901
	LOSS [training: 3.59693358972453 | validation: 3.0876273222982378]
	TIME [epoch: 9.69 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6024924451184575		[learning rate: 0.00010253]
	Learning Rate: 0.000102528
	LOSS [training: 3.6024924451184575 | validation: 3.0945448437952665]
	TIME [epoch: 9.71 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.604815668320776		[learning rate: 0.00010216]
	Learning Rate: 0.000102156
	LOSS [training: 3.604815668320776 | validation: 3.0886889383078664]
	TIME [epoch: 9.69 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597000583825392		[learning rate: 0.00010179]
	Learning Rate: 0.000101785
	LOSS [training: 3.597000583825392 | validation: 3.090943904712788]
	TIME [epoch: 9.68 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5969771193465476		[learning rate: 0.00010142]
	Learning Rate: 0.000101416
	LOSS [training: 3.5969771193465476 | validation: 3.0880001503764527]
	TIME [epoch: 9.71 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.592986252157277		[learning rate: 0.00010105]
	Learning Rate: 0.000101048
	LOSS [training: 3.592986252157277 | validation: 3.0893015418641903]
	TIME [epoch: 9.68 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59814711010007		[learning rate: 0.00010068]
	Learning Rate: 0.000100681
	LOSS [training: 3.59814711010007 | validation: 3.0859772199125644]
	TIME [epoch: 9.68 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6013065335654213		[learning rate: 0.00010032]
	Learning Rate: 0.000100316
	LOSS [training: 3.6013065335654213 | validation: 3.0957100801040403]
	TIME [epoch: 9.7 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.593589331609709		[learning rate: 9.9952e-05]
	Learning Rate: 9.99515e-05
	LOSS [training: 3.593589331609709 | validation: 3.1019144423116707]
	TIME [epoch: 9.69 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5999812535777287		[learning rate: 9.9589e-05]
	Learning Rate: 9.95888e-05
	LOSS [training: 3.5999812535777287 | validation: 3.0764053425221336]
	TIME [epoch: 9.69 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5992075539520494		[learning rate: 9.9227e-05]
	Learning Rate: 9.92274e-05
	LOSS [training: 3.5992075539520494 | validation: 3.079077549034037]
	TIME [epoch: 9.69 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5962003968306413		[learning rate: 9.8867e-05]
	Learning Rate: 9.88673e-05
	LOSS [training: 3.5962003968306413 | validation: 3.084180052729737]
	TIME [epoch: 9.71 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596424389864855		[learning rate: 9.8509e-05]
	Learning Rate: 9.85085e-05
	LOSS [training: 3.596424389864855 | validation: 3.0913115712430215]
	TIME [epoch: 9.69 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598397555054499		[learning rate: 9.8151e-05]
	Learning Rate: 9.8151e-05
	LOSS [training: 3.598397555054499 | validation: 3.09235578554157]
	TIME [epoch: 9.7 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598204968716831		[learning rate: 9.7795e-05]
	Learning Rate: 9.77948e-05
	LOSS [training: 3.598204968716831 | validation: 3.093368204061684]
	TIME [epoch: 9.7 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596403626933784		[learning rate: 9.744e-05]
	Learning Rate: 9.74399e-05
	LOSS [training: 3.596403626933784 | validation: 3.0919321710271]
	TIME [epoch: 9.69 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6058626506683327		[learning rate: 9.7086e-05]
	Learning Rate: 9.70863e-05
	LOSS [training: 3.6058626506683327 | validation: 3.091271977022732]
	TIME [epoch: 9.69 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599668221372453		[learning rate: 9.6734e-05]
	Learning Rate: 9.6734e-05
	LOSS [training: 3.599668221372453 | validation: 3.09326669936667]
	TIME [epoch: 9.69 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5972998293776897		[learning rate: 9.6383e-05]
	Learning Rate: 9.63829e-05
	LOSS [training: 3.5972998293776897 | validation: 3.084643313873548]
	TIME [epoch: 9.71 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5943752924637167		[learning rate: 9.6033e-05]
	Learning Rate: 9.60331e-05
	LOSS [training: 3.5943752924637167 | validation: 3.0987030300277385]
	TIME [epoch: 9.69 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59734535666128		[learning rate: 9.5685e-05]
	Learning Rate: 9.56846e-05
	LOSS [training: 3.59734535666128 | validation: 3.085467539518439]
	TIME [epoch: 9.68 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598730812570487		[learning rate: 9.5337e-05]
	Learning Rate: 9.53374e-05
	LOSS [training: 3.598730812570487 | validation: 3.083232745455955]
	TIME [epoch: 9.71 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6020241904683274		[learning rate: 9.4991e-05]
	Learning Rate: 9.49914e-05
	LOSS [training: 3.6020241904683274 | validation: 3.0794241116007988]
	TIME [epoch: 9.68 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6044217619967767		[learning rate: 9.4647e-05]
	Learning Rate: 9.46466e-05
	LOSS [training: 3.6044217619967767 | validation: 3.092296677527961]
	TIME [epoch: 9.68 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600819004172137		[learning rate: 9.4303e-05]
	Learning Rate: 9.43032e-05
	LOSS [training: 3.600819004172137 | validation: 3.102605212242128]
	TIME [epoch: 9.69 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5994472920735725		[learning rate: 9.3961e-05]
	Learning Rate: 9.39609e-05
	LOSS [training: 3.5994472920735725 | validation: 3.092048765399783]
	TIME [epoch: 9.7 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5988609576745887		[learning rate: 9.362e-05]
	Learning Rate: 9.362e-05
	LOSS [training: 3.5988609576745887 | validation: 3.0918279499150176]
	TIME [epoch: 9.69 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6035449936625845		[learning rate: 9.328e-05]
	Learning Rate: 9.32802e-05
	LOSS [training: 3.6035449936625845 | validation: 3.094792721982659]
	TIME [epoch: 9.68 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597093859272271		[learning rate: 9.2942e-05]
	Learning Rate: 9.29417e-05
	LOSS [training: 3.597093859272271 | validation: 3.088380798828991]
	TIME [epoch: 9.72 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598895656527991		[learning rate: 9.2604e-05]
	Learning Rate: 9.26044e-05
	LOSS [training: 3.598895656527991 | validation: 3.099741201038717]
	TIME [epoch: 9.69 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602736935038723		[learning rate: 9.2268e-05]
	Learning Rate: 9.22683e-05
	LOSS [training: 3.602736935038723 | validation: 3.0910966124320187]
	TIME [epoch: 9.68 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5965574943316474		[learning rate: 9.1933e-05]
	Learning Rate: 9.19335e-05
	LOSS [training: 3.5965574943316474 | validation: 3.0956404555781813]
	TIME [epoch: 9.71 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5986863623860423		[learning rate: 9.16e-05]
	Learning Rate: 9.15998e-05
	LOSS [training: 3.5986863623860423 | validation: 3.0759667523009306]
	TIME [epoch: 9.69 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5971895440125143		[learning rate: 9.1267e-05]
	Learning Rate: 9.12674e-05
	LOSS [training: 3.5971895440125143 | validation: 3.0933270577556846]
	TIME [epoch: 9.68 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596132374634365		[learning rate: 9.0936e-05]
	Learning Rate: 9.09362e-05
	LOSS [training: 3.596132374634365 | validation: 3.0817497303391987]
	TIME [epoch: 9.69 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6012537868865757		[learning rate: 9.0606e-05]
	Learning Rate: 9.06062e-05
	LOSS [training: 3.6012537868865757 | validation: 3.0810723854290565]
	TIME [epoch: 9.71 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5983328436316215		[learning rate: 9.0277e-05]
	Learning Rate: 9.02774e-05
	LOSS [training: 3.5983328436316215 | validation: 3.101823955575988]
	TIME [epoch: 9.68 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5958245200777528		[learning rate: 8.995e-05]
	Learning Rate: 8.99498e-05
	LOSS [training: 3.5958245200777528 | validation: 3.0924749509210927]
	TIME [epoch: 9.68 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603472917410849		[learning rate: 8.9623e-05]
	Learning Rate: 8.96233e-05
	LOSS [training: 3.603472917410849 | validation: 3.09535379725962]
	TIME [epoch: 9.7 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.606432944111414		[learning rate: 8.9298e-05]
	Learning Rate: 8.92981e-05
	LOSS [training: 3.606432944111414 | validation: 3.0985409180289434]
	TIME [epoch: 9.69 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6021330547151935		[learning rate: 8.8974e-05]
	Learning Rate: 8.8974e-05
	LOSS [training: 3.6021330547151935 | validation: 3.0833690042079427]
	TIME [epoch: 9.69 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5964259975945865		[learning rate: 8.8651e-05]
	Learning Rate: 8.86511e-05
	LOSS [training: 3.5964259975945865 | validation: 3.084046589106557]
	TIME [epoch: 9.69 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.609480628723378		[learning rate: 8.8329e-05]
	Learning Rate: 8.83294e-05
	LOSS [training: 3.609480628723378 | validation: 3.0952151821986864]
	TIME [epoch: 9.7 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5998507035836256		[learning rate: 8.8009e-05]
	Learning Rate: 8.80088e-05
	LOSS [training: 3.5998507035836256 | validation: 3.0871154196433905]
	TIME [epoch: 9.69 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5975427368938804		[learning rate: 8.7689e-05]
	Learning Rate: 8.76895e-05
	LOSS [training: 3.5975427368938804 | validation: 3.1077910106202458]
	TIME [epoch: 9.69 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602187169934825		[learning rate: 8.7371e-05]
	Learning Rate: 8.73712e-05
	LOSS [training: 3.602187169934825 | validation: 3.0955187622427673]
	TIME [epoch: 9.71 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5971802772813732		[learning rate: 8.7054e-05]
	Learning Rate: 8.70542e-05
	LOSS [training: 3.5971802772813732 | validation: 3.0945729990514153]
	TIME [epoch: 9.69 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5975457921614535		[learning rate: 8.6738e-05]
	Learning Rate: 8.67382e-05
	LOSS [training: 3.5975457921614535 | validation: 3.09449378400981]
	TIME [epoch: 9.68 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6006147673240294		[learning rate: 8.6423e-05]
	Learning Rate: 8.64235e-05
	LOSS [training: 3.6006147673240294 | validation: 3.1144502482926066]
	TIME [epoch: 9.7 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.607193004047231		[learning rate: 8.611e-05]
	Learning Rate: 8.61098e-05
	LOSS [training: 3.607193004047231 | validation: 3.109601309748037]
	TIME [epoch: 9.69 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600587698258818		[learning rate: 8.5797e-05]
	Learning Rate: 8.57973e-05
	LOSS [training: 3.600587698258818 | validation: 3.0978761275222313]
	TIME [epoch: 9.68 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601568875877277		[learning rate: 8.5486e-05]
	Learning Rate: 8.54859e-05
	LOSS [training: 3.601568875877277 | validation: 3.0902201080854876]
	TIME [epoch: 9.68 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5995695029617165		[learning rate: 8.5176e-05]
	Learning Rate: 8.51757e-05
	LOSS [training: 3.5995695029617165 | validation: 3.0882705146626166]
	TIME [epoch: 9.71 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5979610400286277		[learning rate: 8.4867e-05]
	Learning Rate: 8.48666e-05
	LOSS [training: 3.5979610400286277 | validation: 3.0948555012182637]
	TIME [epoch: 9.68 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5988672945461233		[learning rate: 8.4559e-05]
	Learning Rate: 8.45586e-05
	LOSS [training: 3.5988672945461233 | validation: 3.0909634715707135]
	TIME [epoch: 9.69 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598046116941861		[learning rate: 8.4252e-05]
	Learning Rate: 8.42518e-05
	LOSS [training: 3.598046116941861 | validation: 3.085671880290144]
	TIME [epoch: 9.7 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595820978678848		[learning rate: 8.3946e-05]
	Learning Rate: 8.3946e-05
	LOSS [training: 3.595820978678848 | validation: 3.0846201118496626]
	TIME [epoch: 9.69 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599833235068889		[learning rate: 8.3641e-05]
	Learning Rate: 8.36414e-05
	LOSS [training: 3.599833235068889 | validation: 3.092757112044123]
	TIME [epoch: 9.69 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6030276461266766		[learning rate: 8.3338e-05]
	Learning Rate: 8.33378e-05
	LOSS [training: 3.6030276461266766 | validation: 3.085752807065189]
	TIME [epoch: 9.7 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6001942836665983		[learning rate: 8.3035e-05]
	Learning Rate: 8.30354e-05
	LOSS [training: 3.6001942836665983 | validation: 3.0751651924845973]
	TIME [epoch: 9.7 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600770616365665		[learning rate: 8.2734e-05]
	Learning Rate: 8.2734e-05
	LOSS [training: 3.600770616365665 | validation: 3.09382530465796]
	TIME [epoch: 9.68 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5990086440523106		[learning rate: 8.2434e-05]
	Learning Rate: 8.24338e-05
	LOSS [training: 3.5990086440523106 | validation: 3.087131919767186]
	TIME [epoch: 9.68 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5995666871011727		[learning rate: 8.2135e-05]
	Learning Rate: 8.21346e-05
	LOSS [training: 3.5995666871011727 | validation: 3.089066739554581]
	TIME [epoch: 9.71 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5965353987735424		[learning rate: 8.1837e-05]
	Learning Rate: 8.18366e-05
	LOSS [training: 3.5965353987735424 | validation: 3.0917574154359575]
	TIME [epoch: 9.69 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.593013389702037		[learning rate: 8.154e-05]
	Learning Rate: 8.15396e-05
	LOSS [training: 3.593013389702037 | validation: 3.096259587823719]
	TIME [epoch: 9.69 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5949495112008285		[learning rate: 8.1244e-05]
	Learning Rate: 8.12437e-05
	LOSS [training: 3.5949495112008285 | validation: 3.078385894024823]
	TIME [epoch: 9.7 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5973770106385956		[learning rate: 8.0949e-05]
	Learning Rate: 8.09488e-05
	LOSS [training: 3.5973770106385956 | validation: 3.0922335958993354]
	TIME [epoch: 9.69 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599636479334733		[learning rate: 8.0655e-05]
	Learning Rate: 8.0655e-05
	LOSS [training: 3.599636479334733 | validation: 3.0925776947929706]
	TIME [epoch: 9.68 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601355025668847		[learning rate: 8.0362e-05]
	Learning Rate: 8.03623e-05
	LOSS [training: 3.601355025668847 | validation: 3.0944339483508534]
	TIME [epoch: 9.68 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601807243142153		[learning rate: 8.0071e-05]
	Learning Rate: 8.00707e-05
	LOSS [training: 3.601807243142153 | validation: 3.0759162891502627]
	TIME [epoch: 9.69 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6006287931479433		[learning rate: 7.978e-05]
	Learning Rate: 7.97801e-05
	LOSS [training: 3.6006287931479433 | validation: 3.0795875891147024]
	TIME [epoch: 9.69 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6046376374462605		[learning rate: 7.9491e-05]
	Learning Rate: 7.94906e-05
	LOSS [training: 3.6046376374462605 | validation: 3.093046531348587]
	TIME [epoch: 9.68 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5967813899934207		[learning rate: 7.9202e-05]
	Learning Rate: 7.92021e-05
	LOSS [training: 3.5967813899934207 | validation: 3.0853414539199178]
	TIME [epoch: 9.7 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601676895660179		[learning rate: 7.8915e-05]
	Learning Rate: 7.89147e-05
	LOSS [training: 3.601676895660179 | validation: 3.071081805926015]
	TIME [epoch: 9.7 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6036582193036475		[learning rate: 7.8628e-05]
	Learning Rate: 7.86283e-05
	LOSS [training: 3.6036582193036475 | validation: 3.0784902422302522]
	TIME [epoch: 9.68 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6003620815052706		[learning rate: 7.8343e-05]
	Learning Rate: 7.8343e-05
	LOSS [training: 3.6003620815052706 | validation: 3.0828373580427395]
	TIME [epoch: 9.69 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6015041097794884		[learning rate: 7.8059e-05]
	Learning Rate: 7.80586e-05
	LOSS [training: 3.6015041097794884 | validation: 3.1002624520925472]
	TIME [epoch: 9.7 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599871061895211		[learning rate: 7.7775e-05]
	Learning Rate: 7.77754e-05
	LOSS [training: 3.599871061895211 | validation: 3.077490487029587]
	TIME [epoch: 9.69 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5998889875903695		[learning rate: 7.7493e-05]
	Learning Rate: 7.74931e-05
	LOSS [training: 3.5998889875903695 | validation: 3.0891339435379566]
	TIME [epoch: 9.68 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599133680235505		[learning rate: 7.7212e-05]
	Learning Rate: 7.72119e-05
	LOSS [training: 3.599133680235505 | validation: 3.092819191540979]
	TIME [epoch: 9.71 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6000311776606706		[learning rate: 7.6932e-05]
	Learning Rate: 7.69317e-05
	LOSS [training: 3.6000311776606706 | validation: 3.102581422041372]
	TIME [epoch: 9.69 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6061182165557097		[learning rate: 7.6653e-05]
	Learning Rate: 7.66525e-05
	LOSS [training: 3.6061182165557097 | validation: 3.0958584730718797]
	TIME [epoch: 9.69 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.594943010871634		[learning rate: 7.6374e-05]
	Learning Rate: 7.63743e-05
	LOSS [training: 3.594943010871634 | validation: 3.0970554607743845]
	TIME [epoch: 9.69 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5985553291181844		[learning rate: 7.6097e-05]
	Learning Rate: 7.60972e-05
	LOSS [training: 3.5985553291181844 | validation: 3.0933693173973418]
	TIME [epoch: 9.7 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5981979244207323		[learning rate: 7.5821e-05]
	Learning Rate: 7.5821e-05
	LOSS [training: 3.5981979244207323 | validation: 3.085226264541039]
	TIME [epoch: 9.69 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.594218134225598		[learning rate: 7.5546e-05]
	Learning Rate: 7.55458e-05
	LOSS [training: 3.594218134225598 | validation: 3.08850479358125]
	TIME [epoch: 9.69 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596757208538153		[learning rate: 7.5272e-05]
	Learning Rate: 7.52717e-05
	LOSS [training: 3.596757208538153 | validation: 3.087086878704863]
	TIME [epoch: 9.71 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5974553372200617		[learning rate: 7.4999e-05]
	Learning Rate: 7.49985e-05
	LOSS [training: 3.5974553372200617 | validation: 3.0770346332545433]
	TIME [epoch: 9.69 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5984515727985147		[learning rate: 7.4726e-05]
	Learning Rate: 7.47263e-05
	LOSS [training: 3.5984515727985147 | validation: 3.0886389534826004]
	TIME [epoch: 9.69 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599069346163		[learning rate: 7.4455e-05]
	Learning Rate: 7.44552e-05
	LOSS [training: 3.599069346163 | validation: 3.0883528282330905]
	TIME [epoch: 9.71 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5953411002380173		[learning rate: 7.4185e-05]
	Learning Rate: 7.4185e-05
	LOSS [training: 3.5953411002380173 | validation: 3.096955743828716]
	TIME [epoch: 9.68 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5997926369341458		[learning rate: 7.3916e-05]
	Learning Rate: 7.39157e-05
	LOSS [training: 3.5997926369341458 | validation: 3.078992752153989]
	TIME [epoch: 9.7 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5973255248332547		[learning rate: 7.3647e-05]
	Learning Rate: 7.36475e-05
	LOSS [training: 3.5973255248332547 | validation: 3.0811784818210493]
	TIME [epoch: 9.68 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601219394393586		[learning rate: 7.338e-05]
	Learning Rate: 7.33802e-05
	LOSS [training: 3.601219394393586 | validation: 3.089513516037738]
	TIME [epoch: 9.71 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6055270555420877		[learning rate: 7.3114e-05]
	Learning Rate: 7.31139e-05
	LOSS [training: 3.6055270555420877 | validation: 3.081525922848724]
	TIME [epoch: 9.68 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5983324480445815		[learning rate: 7.2849e-05]
	Learning Rate: 7.28486e-05
	LOSS [training: 3.5983324480445815 | validation: 3.082921444585296]
	TIME [epoch: 9.69 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6003634139900926		[learning rate: 7.2584e-05]
	Learning Rate: 7.25842e-05
	LOSS [training: 3.6003634139900926 | validation: 3.0796902210424153]
	TIME [epoch: 9.7 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600090068176622		[learning rate: 7.2321e-05]
	Learning Rate: 7.23208e-05
	LOSS [training: 3.600090068176622 | validation: 3.0835202108986506]
	TIME [epoch: 9.69 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6001340792389636		[learning rate: 7.2058e-05]
	Learning Rate: 7.20583e-05
	LOSS [training: 3.6001340792389636 | validation: 3.0908528020189654]
	TIME [epoch: 9.68 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6006388283803803		[learning rate: 7.1797e-05]
	Learning Rate: 7.17968e-05
	LOSS [training: 3.6006388283803803 | validation: 3.0821853556285124]
	TIME [epoch: 9.69 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5993899254991057		[learning rate: 7.1536e-05]
	Learning Rate: 7.15363e-05
	LOSS [training: 3.5993899254991057 | validation: 3.086850621720168]
	TIME [epoch: 9.69 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5991695686541556		[learning rate: 7.1277e-05]
	Learning Rate: 7.12767e-05
	LOSS [training: 3.5991695686541556 | validation: 3.0855153825254953]
	TIME [epoch: 9.69 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59422069855621		[learning rate: 7.1018e-05]
	Learning Rate: 7.1018e-05
	LOSS [training: 3.59422069855621 | validation: 3.0872660474072724]
	TIME [epoch: 9.69 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598824368028832		[learning rate: 7.076e-05]
	Learning Rate: 7.07603e-05
	LOSS [training: 3.598824368028832 | validation: 3.0869812193514146]
	TIME [epoch: 9.7 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600336160244468		[learning rate: 7.0503e-05]
	Learning Rate: 7.05035e-05
	LOSS [training: 3.600336160244468 | validation: 3.087403818409721]
	TIME [epoch: 9.68 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5953788509864824		[learning rate: 7.0248e-05]
	Learning Rate: 7.02476e-05
	LOSS [training: 3.5953788509864824 | validation: 3.089875557358445]
	TIME [epoch: 9.7 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6005874868194274		[learning rate: 6.9993e-05]
	Learning Rate: 6.99927e-05
	LOSS [training: 3.6005874868194274 | validation: 3.0914073742384445]
	TIME [epoch: 9.7 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.594548815380191		[learning rate: 6.9739e-05]
	Learning Rate: 6.97387e-05
	LOSS [training: 3.594548815380191 | validation: 3.091031195486559]
	TIME [epoch: 9.73 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596359864064786		[learning rate: 6.9486e-05]
	Learning Rate: 6.94856e-05
	LOSS [training: 3.596359864064786 | validation: 3.089138373807677]
	TIME [epoch: 9.69 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5931626632617473		[learning rate: 6.9233e-05]
	Learning Rate: 6.92334e-05
	LOSS [training: 3.5931626632617473 | validation: 3.0883784739978175]
	TIME [epoch: 9.69 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595940184852451		[learning rate: 6.8982e-05]
	Learning Rate: 6.89822e-05
	LOSS [training: 3.595940184852451 | validation: 3.08357004122208]
	TIME [epoch: 9.71 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6021897846369497		[learning rate: 6.8732e-05]
	Learning Rate: 6.87318e-05
	LOSS [training: 3.6021897846369497 | validation: 3.08658458363421]
	TIME [epoch: 9.69 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5946715478492477		[learning rate: 6.8482e-05]
	Learning Rate: 6.84824e-05
	LOSS [training: 3.5946715478492477 | validation: 3.086740643485241]
	TIME [epoch: 9.69 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6004055718076087		[learning rate: 6.8234e-05]
	Learning Rate: 6.82339e-05
	LOSS [training: 3.6004055718076087 | validation: 3.0850637886424055]
	TIME [epoch: 9.71 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597482245706452		[learning rate: 6.7986e-05]
	Learning Rate: 6.79862e-05
	LOSS [training: 3.597482245706452 | validation: 3.085142325042499]
	TIME [epoch: 9.7 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5919224689593356		[learning rate: 6.774e-05]
	Learning Rate: 6.77395e-05
	LOSS [training: 3.5919224689593356 | validation: 3.0751993010211702]
	TIME [epoch: 9.7 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5940374844979095		[learning rate: 6.7494e-05]
	Learning Rate: 6.74937e-05
	LOSS [training: 3.5940374844979095 | validation: 3.088687889971348]
	TIME [epoch: 9.7 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5936585730057558		[learning rate: 6.7249e-05]
	Learning Rate: 6.72488e-05
	LOSS [training: 3.5936585730057558 | validation: 3.081155357489475]
	TIME [epoch: 9.7 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.593521162722444		[learning rate: 6.7005e-05]
	Learning Rate: 6.70047e-05
	LOSS [training: 3.593521162722444 | validation: 3.094229271350358]
	TIME [epoch: 9.68 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595107753924186		[learning rate: 6.6762e-05]
	Learning Rate: 6.67616e-05
	LOSS [training: 3.595107753924186 | validation: 3.089749090362699]
	TIME [epoch: 9.69 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5968436328867184		[learning rate: 6.6519e-05]
	Learning Rate: 6.65192e-05
	LOSS [training: 3.5968436328867184 | validation: 3.0911767911346555]
	TIME [epoch: 9.71 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601939084338246		[learning rate: 6.6278e-05]
	Learning Rate: 6.62778e-05
	LOSS [training: 3.601939084338246 | validation: 3.0881833647401518]
	TIME [epoch: 9.7 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598013824035513		[learning rate: 6.6037e-05]
	Learning Rate: 6.60373e-05
	LOSS [training: 3.598013824035513 | validation: 3.08821639292652]
	TIME [epoch: 9.69 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6045287612953034		[learning rate: 6.5798e-05]
	Learning Rate: 6.57977e-05
	LOSS [training: 3.6045287612953034 | validation: 3.098077618800065]
	TIME [epoch: 9.71 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599724087660223		[learning rate: 6.5559e-05]
	Learning Rate: 6.55589e-05
	LOSS [training: 3.599724087660223 | validation: 3.0919454284362446]
	TIME [epoch: 9.69 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5951494522604728		[learning rate: 6.5321e-05]
	Learning Rate: 6.5321e-05
	LOSS [training: 3.5951494522604728 | validation: 3.0903750554553895]
	TIME [epoch: 9.69 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60158194870277		[learning rate: 6.5084e-05]
	Learning Rate: 6.50839e-05
	LOSS [training: 3.60158194870277 | validation: 3.0908225728147762]
	TIME [epoch: 9.68 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5978918430580555		[learning rate: 6.4848e-05]
	Learning Rate: 6.48477e-05
	LOSS [training: 3.5978918430580555 | validation: 3.0888106538915516]
	TIME [epoch: 9.71 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60003421677932		[learning rate: 6.4612e-05]
	Learning Rate: 6.46124e-05
	LOSS [training: 3.60003421677932 | validation: 3.078866003283705]
	TIME [epoch: 9.68 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600573507757276		[learning rate: 6.4378e-05]
	Learning Rate: 6.43779e-05
	LOSS [training: 3.600573507757276 | validation: 3.082367180274937]
	TIME [epoch: 9.69 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5957827355684		[learning rate: 6.4144e-05]
	Learning Rate: 6.41443e-05
	LOSS [training: 3.5957827355684 | validation: 3.0963779452873723]
	TIME [epoch: 9.71 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6055477227462758		[learning rate: 6.3911e-05]
	Learning Rate: 6.39115e-05
	LOSS [training: 3.6055477227462758 | validation: 3.0861238935732604]
	TIME [epoch: 9.69 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6020420073508235		[learning rate: 6.368e-05]
	Learning Rate: 6.36795e-05
	LOSS [training: 3.6020420073508235 | validation: 3.0806486964525504]
	TIME [epoch: 9.69 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600258141521908		[learning rate: 6.3448e-05]
	Learning Rate: 6.34485e-05
	LOSS [training: 3.600258141521908 | validation: 3.0919908882572704]
	TIME [epoch: 9.69 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6003738663337947		[learning rate: 6.3218e-05]
	Learning Rate: 6.32182e-05
	LOSS [training: 3.6003738663337947 | validation: 3.094996431935209]
	TIME [epoch: 9.71 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599594078166652		[learning rate: 6.2989e-05]
	Learning Rate: 6.29888e-05
	LOSS [training: 3.599594078166652 | validation: 3.0796148878634586]
	TIME [epoch: 9.69 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598458828968682		[learning rate: 6.276e-05]
	Learning Rate: 6.27602e-05
	LOSS [training: 3.598458828968682 | validation: 3.0821121218484695]
	TIME [epoch: 9.68 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598741797057129		[learning rate: 6.2532e-05]
	Learning Rate: 6.25324e-05
	LOSS [training: 3.598741797057129 | validation: 3.09111100498776]
	TIME [epoch: 9.71 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5965286706670367		[learning rate: 6.2305e-05]
	Learning Rate: 6.23055e-05
	LOSS [training: 3.5965286706670367 | validation: 3.0833348715097713]
	TIME [epoch: 9.69 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5965520714714088		[learning rate: 6.2079e-05]
	Learning Rate: 6.20794e-05
	LOSS [training: 3.5965520714714088 | validation: 3.091131387804219]
	TIME [epoch: 9.69 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5965225269385828		[learning rate: 6.1854e-05]
	Learning Rate: 6.18541e-05
	LOSS [training: 3.5965225269385828 | validation: 3.0909628476822766]
	TIME [epoch: 9.7 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5995424015412		[learning rate: 6.163e-05]
	Learning Rate: 6.16296e-05
	LOSS [training: 3.5995424015412 | validation: 3.091115897903327]
	TIME [epoch: 9.7 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6014498891479505		[learning rate: 6.1406e-05]
	Learning Rate: 6.1406e-05
	LOSS [training: 3.6014498891479505 | validation: 3.088918313833592]
	TIME [epoch: 9.68 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6016416606204573		[learning rate: 6.1183e-05]
	Learning Rate: 6.11831e-05
	LOSS [training: 3.6016416606204573 | validation: 3.0841303190980445]
	TIME [epoch: 9.69 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5981585869170667		[learning rate: 6.0961e-05]
	Learning Rate: 6.09611e-05
	LOSS [training: 3.5981585869170667 | validation: 3.0941460025894663]
	TIME [epoch: 9.71 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598312201309149		[learning rate: 6.074e-05]
	Learning Rate: 6.07399e-05
	LOSS [training: 3.598312201309149 | validation: 3.088856361756124]
	TIME [epoch: 9.7 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6002886378625236		[learning rate: 6.0519e-05]
	Learning Rate: 6.05194e-05
	LOSS [training: 3.6002886378625236 | validation: 3.084880033064963]
	TIME [epoch: 9.69 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595614544494741		[learning rate: 6.03e-05]
	Learning Rate: 6.02998e-05
	LOSS [training: 3.595614544494741 | validation: 3.092728458343469]
	TIME [epoch: 9.71 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6022371626249		[learning rate: 6.0081e-05]
	Learning Rate: 6.0081e-05
	LOSS [training: 3.6022371626249 | validation: 3.083111538373977]
	TIME [epoch: 9.69 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597360382047113		[learning rate: 5.9863e-05]
	Learning Rate: 5.98629e-05
	LOSS [training: 3.597360382047113 | validation: 3.092836047145671]
	TIME [epoch: 9.69 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598111224835108		[learning rate: 5.9646e-05]
	Learning Rate: 5.96457e-05
	LOSS [training: 3.598111224835108 | validation: 3.0786693508524854]
	TIME [epoch: 9.69 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598102723058554		[learning rate: 5.9429e-05]
	Learning Rate: 5.94292e-05
	LOSS [training: 3.598102723058554 | validation: 3.0842237088824227]
	TIME [epoch: 9.71 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603934222864294		[learning rate: 5.9214e-05]
	Learning Rate: 5.92136e-05
	LOSS [training: 3.603934222864294 | validation: 3.079465327140668]
	TIME [epoch: 9.69 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602134713828131		[learning rate: 5.8999e-05]
	Learning Rate: 5.89987e-05
	LOSS [training: 3.602134713828131 | validation: 3.0833748338102738]
	TIME [epoch: 9.69 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601457539808473		[learning rate: 5.8785e-05]
	Learning Rate: 5.87846e-05
	LOSS [training: 3.601457539808473 | validation: 3.083160612105516]
	TIME [epoch: 9.71 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5982856163355272		[learning rate: 5.8571e-05]
	Learning Rate: 5.85712e-05
	LOSS [training: 3.5982856163355272 | validation: 3.090184240072947]
	TIME [epoch: 9.7 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597518939325571		[learning rate: 5.8359e-05]
	Learning Rate: 5.83586e-05
	LOSS [training: 3.597518939325571 | validation: 3.092119893487344]
	TIME [epoch: 9.69 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597776160030665		[learning rate: 5.8147e-05]
	Learning Rate: 5.81469e-05
	LOSS [training: 3.597776160030665 | validation: 3.0919570671079133]
	TIME [epoch: 9.69 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6006464110741887		[learning rate: 5.7936e-05]
	Learning Rate: 5.79358e-05
	LOSS [training: 3.6006464110741887 | validation: 3.089169787201736]
	TIME [epoch: 9.7 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6027643486111645		[learning rate: 5.7726e-05]
	Learning Rate: 5.77256e-05
	LOSS [training: 3.6027643486111645 | validation: 3.0933207461160657]
	TIME [epoch: 9.69 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598030554748603		[learning rate: 5.7516e-05]
	Learning Rate: 5.75161e-05
	LOSS [training: 3.598030554748603 | validation: 3.0943353657714145]
	TIME [epoch: 9.69 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5997043857782955		[learning rate: 5.7307e-05]
	Learning Rate: 5.73074e-05
	LOSS [training: 3.5997043857782955 | validation: 3.0785059447007543]
	TIME [epoch: 9.71 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597526007366009		[learning rate: 5.7099e-05]
	Learning Rate: 5.70994e-05
	LOSS [training: 3.597526007366009 | validation: 3.0934445390452527]
	TIME [epoch: 9.68 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59749504843003		[learning rate: 5.6892e-05]
	Learning Rate: 5.68922e-05
	LOSS [training: 3.59749504843003 | validation: 3.0763705003166253]
	TIME [epoch: 9.69 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5938647508962434		[learning rate: 5.6686e-05]
	Learning Rate: 5.66857e-05
	LOSS [training: 3.5938647508962434 | validation: 3.0943579211451855]
	TIME [epoch: 9.7 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5984888422366175		[learning rate: 5.648e-05]
	Learning Rate: 5.648e-05
	LOSS [training: 3.5984888422366175 | validation: 3.07810520883207]
	TIME [epoch: 9.69 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601185109212038		[learning rate: 5.6275e-05]
	Learning Rate: 5.6275e-05
	LOSS [training: 3.601185109212038 | validation: 3.088666769134164]
	TIME [epoch: 9.68 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6055008692903967		[learning rate: 5.6071e-05]
	Learning Rate: 5.60708e-05
	LOSS [training: 3.6055008692903967 | validation: 3.0996888747326627]
	TIME [epoch: 9.69 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6045784615166725		[learning rate: 5.5867e-05]
	Learning Rate: 5.58673e-05
	LOSS [training: 3.6045784615166725 | validation: 3.0985601944833627]
	TIME [epoch: 9.71 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6006113283455314		[learning rate: 5.5665e-05]
	Learning Rate: 5.56646e-05
	LOSS [training: 3.6006113283455314 | validation: 3.0835843500644664]
	TIME [epoch: 9.69 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5931002054798684		[learning rate: 5.5463e-05]
	Learning Rate: 5.54626e-05
	LOSS [training: 3.5931002054798684 | validation: 3.082601829248446]
	TIME [epoch: 9.69 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6017854109575205		[learning rate: 5.5261e-05]
	Learning Rate: 5.52613e-05
	LOSS [training: 3.6017854109575205 | validation: 3.0821255635459828]
	TIME [epoch: 9.7 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598902735521777		[learning rate: 5.5061e-05]
	Learning Rate: 5.50608e-05
	LOSS [training: 3.598902735521777 | validation: 3.1030107799126228]
	TIME [epoch: 9.69 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5996427283427463		[learning rate: 5.4861e-05]
	Learning Rate: 5.48609e-05
	LOSS [training: 3.5996427283427463 | validation: 3.0982287193323375]
	TIME [epoch: 9.69 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59856636409602		[learning rate: 5.4662e-05]
	Learning Rate: 5.46618e-05
	LOSS [training: 3.59856636409602 | validation: 3.0993276685558455]
	TIME [epoch: 9.69 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600405617866776		[learning rate: 5.4463e-05]
	Learning Rate: 5.44635e-05
	LOSS [training: 3.600405617866776 | validation: 3.093554145224971]
	TIME [epoch: 9.69 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5975641001023213		[learning rate: 5.4266e-05]
	Learning Rate: 5.42658e-05
	LOSS [training: 3.5975641001023213 | validation: 3.071752161518596]
	TIME [epoch: 9.69 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5966375104319632		[learning rate: 5.4069e-05]
	Learning Rate: 5.40689e-05
	LOSS [training: 3.5966375104319632 | validation: 3.087992220150078]
	TIME [epoch: 9.69 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6002221511163506		[learning rate: 5.3873e-05]
	Learning Rate: 5.38727e-05
	LOSS [training: 3.6002221511163506 | validation: 3.0852296248492266]
	TIME [epoch: 9.71 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600154396924514		[learning rate: 5.3677e-05]
	Learning Rate: 5.36772e-05
	LOSS [training: 3.600154396924514 | validation: 3.0899354252169395]
	TIME [epoch: 9.69 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5992544191717726		[learning rate: 5.3482e-05]
	Learning Rate: 5.34824e-05
	LOSS [training: 3.5992544191717726 | validation: 3.090667594412806]
	TIME [epoch: 9.69 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5977348566777736		[learning rate: 5.3288e-05]
	Learning Rate: 5.32883e-05
	LOSS [training: 3.5977348566777736 | validation: 3.094792525611475]
	TIME [epoch: 9.71 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598021630477791		[learning rate: 5.3095e-05]
	Learning Rate: 5.30949e-05
	LOSS [training: 3.598021630477791 | validation: 3.08178555852542]
	TIME [epoch: 9.69 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5979026727453323		[learning rate: 5.2902e-05]
	Learning Rate: 5.29022e-05
	LOSS [training: 3.5979026727453323 | validation: 3.0817658334641282]
	TIME [epoch: 9.68 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6018300257776494		[learning rate: 5.271e-05]
	Learning Rate: 5.27102e-05
	LOSS [training: 3.6018300257776494 | validation: 3.090318637861595]
	TIME [epoch: 9.69 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6008519193527446		[learning rate: 5.2519e-05]
	Learning Rate: 5.25189e-05
	LOSS [training: 3.6008519193527446 | validation: 3.096055395329944]
	TIME [epoch: 9.71 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5997679969798724		[learning rate: 5.2328e-05]
	Learning Rate: 5.23283e-05
	LOSS [training: 3.5997679969798724 | validation: 3.0892827564056744]
	TIME [epoch: 9.69 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596783735784409		[learning rate: 5.2138e-05]
	Learning Rate: 5.21384e-05
	LOSS [training: 3.596783735784409 | validation: 3.0905660793715715]
	TIME [epoch: 9.68 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5990759006910955		[learning rate: 5.1949e-05]
	Learning Rate: 5.19492e-05
	LOSS [training: 3.5990759006910955 | validation: 3.077373567624918]
	TIME [epoch: 9.7 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5975787046852177		[learning rate: 5.1761e-05]
	Learning Rate: 5.17607e-05
	LOSS [training: 3.5975787046852177 | validation: 3.0934747493073647]
	TIME [epoch: 9.69 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5982216107787415		[learning rate: 5.1573e-05]
	Learning Rate: 5.15729e-05
	LOSS [training: 3.5982216107787415 | validation: 3.089425265150967]
	TIME [epoch: 9.69 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5954858956517897		[learning rate: 5.1386e-05]
	Learning Rate: 5.13857e-05
	LOSS [training: 3.5954858956517897 | validation: 3.0919924557830054]
	TIME [epoch: 9.7 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6010506502814996		[learning rate: 5.1199e-05]
	Learning Rate: 5.11992e-05
	LOSS [training: 3.6010506502814996 | validation: 3.089482074518454]
	TIME [epoch: 9.71 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5949432949984583		[learning rate: 5.1013e-05]
	Learning Rate: 5.10134e-05
	LOSS [training: 3.5949432949984583 | validation: 3.085773653780917]
	TIME [epoch: 9.69 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5977821430793453		[learning rate: 5.0828e-05]
	Learning Rate: 5.08283e-05
	LOSS [training: 3.5977821430793453 | validation: 3.080098815152264]
	TIME [epoch: 9.69 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5985907928477294		[learning rate: 5.0644e-05]
	Learning Rate: 5.06438e-05
	LOSS [training: 3.5985907928477294 | validation: 3.0935329776151765]
	TIME [epoch: 9.71 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5996002751333087		[learning rate: 5.046e-05]
	Learning Rate: 5.046e-05
	LOSS [training: 3.5996002751333087 | validation: 3.087487084063048]
	TIME [epoch: 9.69 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6002958226325603		[learning rate: 5.0277e-05]
	Learning Rate: 5.02769e-05
	LOSS [training: 3.6002958226325603 | validation: 3.0916631656389404]
	TIME [epoch: 9.69 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5936928724673427		[learning rate: 5.0094e-05]
	Learning Rate: 5.00944e-05
	LOSS [training: 3.5936928724673427 | validation: 3.0819743966538513]
	TIME [epoch: 9.7 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5974569740740874		[learning rate: 4.9913e-05]
	Learning Rate: 4.99127e-05
	LOSS [training: 3.5974569740740874 | validation: 3.0887902534636273]
	TIME [epoch: 9.7 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6011243235951724		[learning rate: 4.9732e-05]
	Learning Rate: 4.97315e-05
	LOSS [training: 3.6011243235951724 | validation: 3.0713657079779857]
	TIME [epoch: 9.71 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6016944080764164		[learning rate: 4.9551e-05]
	Learning Rate: 4.9551e-05
	LOSS [training: 3.6016944080764164 | validation: 3.081029442637488]
	TIME [epoch: 9.69 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5918448630372266		[learning rate: 4.9371e-05]
	Learning Rate: 4.93712e-05
	LOSS [training: 3.5918448630372266 | validation: 3.086239392645932]
	TIME [epoch: 9.71 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5959663951734315		[learning rate: 4.9192e-05]
	Learning Rate: 4.9192e-05
	LOSS [training: 3.5959663951734315 | validation: 3.0885944143319612]
	TIME [epoch: 9.69 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598788903606736		[learning rate: 4.9014e-05]
	Learning Rate: 4.90135e-05
	LOSS [training: 3.598788903606736 | validation: 3.081680913475008]
	TIME [epoch: 9.68 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5952988400751194		[learning rate: 4.8836e-05]
	Learning Rate: 4.88356e-05
	LOSS [training: 3.5952988400751194 | validation: 3.1029795188399043]
	TIME [epoch: 9.7 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602472574436186		[learning rate: 4.8658e-05]
	Learning Rate: 4.86584e-05
	LOSS [training: 3.602472574436186 | validation: 3.0944496101557126]
	TIME [epoch: 9.68 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6011453189772467		[learning rate: 4.8482e-05]
	Learning Rate: 4.84818e-05
	LOSS [training: 3.6011453189772467 | validation: 3.084433643621896]
	TIME [epoch: 9.68 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5968254125303667		[learning rate: 4.8306e-05]
	Learning Rate: 4.83059e-05
	LOSS [training: 3.5968254125303667 | validation: 3.0889902849760085]
	TIME [epoch: 9.69 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5985897624845675		[learning rate: 4.8131e-05]
	Learning Rate: 4.81306e-05
	LOSS [training: 3.5985897624845675 | validation: 3.090599188878232]
	TIME [epoch: 9.7 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6032488228061355		[learning rate: 4.7956e-05]
	Learning Rate: 4.79559e-05
	LOSS [training: 3.6032488228061355 | validation: 3.0885785924697036]
	TIME [epoch: 9.69 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5992272893040536		[learning rate: 4.7782e-05]
	Learning Rate: 4.77819e-05
	LOSS [training: 3.5992272893040536 | validation: 3.087656965851014]
	TIME [epoch: 9.68 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598358104061881		[learning rate: 4.7608e-05]
	Learning Rate: 4.76085e-05
	LOSS [training: 3.598358104061881 | validation: 3.0958063807213705]
	TIME [epoch: 9.7 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5977553861340126		[learning rate: 4.7436e-05]
	Learning Rate: 4.74357e-05
	LOSS [training: 3.5977553861340126 | validation: 3.1004877188872753]
	TIME [epoch: 9.69 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6016385893756335		[learning rate: 4.7264e-05]
	Learning Rate: 4.72636e-05
	LOSS [training: 3.6016385893756335 | validation: 3.093467133198386]
	TIME [epoch: 9.68 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5973929824860362		[learning rate: 4.7092e-05]
	Learning Rate: 4.7092e-05
	LOSS [training: 3.5973929824860362 | validation: 3.0828817522610974]
	TIME [epoch: 9.7 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597793826483511		[learning rate: 4.6921e-05]
	Learning Rate: 4.69211e-05
	LOSS [training: 3.597793826483511 | validation: 3.083619057887448]
	TIME [epoch: 9.7 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6006685756302064		[learning rate: 4.6751e-05]
	Learning Rate: 4.67508e-05
	LOSS [training: 3.6006685756302064 | validation: 3.084674773893413]
	TIME [epoch: 9.68 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598145824099312		[learning rate: 4.6581e-05]
	Learning Rate: 4.65812e-05
	LOSS [training: 3.598145824099312 | validation: 3.0902745466839314]
	TIME [epoch: 9.69 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59980296392035		[learning rate: 4.6412e-05]
	Learning Rate: 4.64121e-05
	LOSS [training: 3.59980296392035 | validation: 3.1017832869015667]
	TIME [epoch: 9.71 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5976244385488725		[learning rate: 4.6244e-05]
	Learning Rate: 4.62437e-05
	LOSS [training: 3.5976244385488725 | validation: 3.089747976938879]
	TIME [epoch: 9.69 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599233281814083		[learning rate: 4.6076e-05]
	Learning Rate: 4.60759e-05
	LOSS [training: 3.599233281814083 | validation: 3.0732219424829066]
	TIME [epoch: 9.68 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5975047007561374		[learning rate: 4.5909e-05]
	Learning Rate: 4.59087e-05
	LOSS [training: 3.5975047007561374 | validation: 3.0957858794661024]
	TIME [epoch: 9.7 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599781558072485		[learning rate: 4.5742e-05]
	Learning Rate: 4.57421e-05
	LOSS [training: 3.599781558072485 | validation: 3.092865420683527]
	TIME [epoch: 9.68 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59530680392682		[learning rate: 4.5576e-05]
	Learning Rate: 4.55761e-05
	LOSS [training: 3.59530680392682 | validation: 3.095710507488635]
	TIME [epoch: 9.68 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5983683571285283		[learning rate: 4.5411e-05]
	Learning Rate: 4.54107e-05
	LOSS [training: 3.5983683571285283 | validation: 3.091616394034553]
	TIME [epoch: 9.68 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596364509829852		[learning rate: 4.5246e-05]
	Learning Rate: 4.52459e-05
	LOSS [training: 3.596364509829852 | validation: 3.0859598587363735]
	TIME [epoch: 9.71 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595029915354008		[learning rate: 4.5082e-05]
	Learning Rate: 4.50817e-05
	LOSS [training: 3.595029915354008 | validation: 3.0906418875247965]
	TIME [epoch: 9.68 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595768843673376		[learning rate: 4.4918e-05]
	Learning Rate: 4.49181e-05
	LOSS [training: 3.595768843673376 | validation: 3.0905719457308045]
	TIME [epoch: 9.68 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6046625488422093		[learning rate: 4.4755e-05]
	Learning Rate: 4.47551e-05
	LOSS [training: 3.6046625488422093 | validation: 3.080023304425329]
	TIME [epoch: 9.7 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601074209441779		[learning rate: 4.4593e-05]
	Learning Rate: 4.45926e-05
	LOSS [training: 3.601074209441779 | validation: 3.093770868187072]
	TIME [epoch: 9.69 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.594951244088334		[learning rate: 4.4431e-05]
	Learning Rate: 4.44308e-05
	LOSS [training: 3.594951244088334 | validation: 3.075912958951185]
	TIME [epoch: 9.68 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598438333997058		[learning rate: 4.427e-05]
	Learning Rate: 4.42696e-05
	LOSS [training: 3.598438333997058 | validation: 3.0884513052671556]
	TIME [epoch: 9.69 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6002245916011795		[learning rate: 4.4109e-05]
	Learning Rate: 4.41089e-05
	LOSS [training: 3.6002245916011795 | validation: 3.078686904629379]
	TIME [epoch: 9.7 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595575957163993		[learning rate: 4.3949e-05]
	Learning Rate: 4.39489e-05
	LOSS [training: 3.595575957163993 | validation: 3.090809580779853]
	TIME [epoch: 9.69 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5944663311967298		[learning rate: 4.3789e-05]
	Learning Rate: 4.37893e-05
	LOSS [training: 3.5944663311967298 | validation: 3.0831710412369935]
	TIME [epoch: 9.68 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5988325792318405		[learning rate: 4.363e-05]
	Learning Rate: 4.36304e-05
	LOSS [training: 3.5988325792318405 | validation: 3.088502329640278]
	TIME [epoch: 9.71 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597526962476908		[learning rate: 4.3472e-05]
	Learning Rate: 4.34721e-05
	LOSS [training: 3.597526962476908 | validation: 3.0906195244919235]
	TIME [epoch: 9.69 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601000350663264		[learning rate: 4.3314e-05]
	Learning Rate: 4.33143e-05
	LOSS [training: 3.601000350663264 | validation: 3.0988122011950954]
	TIME [epoch: 9.68 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602575370330128		[learning rate: 4.3157e-05]
	Learning Rate: 4.31571e-05
	LOSS [training: 3.602575370330128 | validation: 3.0908091614652933]
	TIME [epoch: 9.71 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6023744958949444		[learning rate: 4.3001e-05]
	Learning Rate: 4.30005e-05
	LOSS [training: 3.6023744958949444 | validation: 3.0891677375250772]
	TIME [epoch: 9.69 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5971280213897914		[learning rate: 4.2844e-05]
	Learning Rate: 4.28445e-05
	LOSS [training: 3.5971280213897914 | validation: 3.0903009060298716]
	TIME [epoch: 9.68 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5957636256303864		[learning rate: 4.2689e-05]
	Learning Rate: 4.2689e-05
	LOSS [training: 3.5957636256303864 | validation: 3.0917960524877572]
	TIME [epoch: 9.69 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596318608094523		[learning rate: 4.2534e-05]
	Learning Rate: 4.25341e-05
	LOSS [training: 3.596318608094523 | validation: 3.075874874285688]
	TIME [epoch: 9.71 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5971460241580133		[learning rate: 4.238e-05]
	Learning Rate: 4.23797e-05
	LOSS [training: 3.5971460241580133 | validation: 3.0951767994497494]
	TIME [epoch: 9.69 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5980586299465984		[learning rate: 4.2226e-05]
	Learning Rate: 4.22259e-05
	LOSS [training: 3.5980586299465984 | validation: 3.090018688255776]
	TIME [epoch: 9.68 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596388749686433		[learning rate: 4.2073e-05]
	Learning Rate: 4.20727e-05
	LOSS [training: 3.596388749686433 | validation: 3.0983562142048404]
	TIME [epoch: 9.71 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601360982982265		[learning rate: 4.192e-05]
	Learning Rate: 4.192e-05
	LOSS [training: 3.601360982982265 | validation: 3.09411850339984]
	TIME [epoch: 9.69 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6009290687085995		[learning rate: 4.1768e-05]
	Learning Rate: 4.17679e-05
	LOSS [training: 3.6009290687085995 | validation: 3.099095022002227]
	TIME [epoch: 9.69 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5942695936955102		[learning rate: 4.1616e-05]
	Learning Rate: 4.16163e-05
	LOSS [training: 3.5942695936955102 | validation: 3.079876708276938]
	TIME [epoch: 9.68 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5991642932619867		[learning rate: 4.1465e-05]
	Learning Rate: 4.14652e-05
	LOSS [training: 3.5991642932619867 | validation: 3.0860002357666083]
	TIME [epoch: 9.7 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596121522427596		[learning rate: 4.1315e-05]
	Learning Rate: 4.13148e-05
	LOSS [training: 3.596121522427596 | validation: 3.09019152644942]
	TIME [epoch: 9.69 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6001477304387195		[learning rate: 4.1165e-05]
	Learning Rate: 4.11648e-05
	LOSS [training: 3.6001477304387195 | validation: 3.083360355100723]
	TIME [epoch: 9.69 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5971419848692276		[learning rate: 4.1015e-05]
	Learning Rate: 4.10154e-05
	LOSS [training: 3.5971419848692276 | validation: 3.0861812362408534]
	TIME [epoch: 9.71 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5985265654968877		[learning rate: 4.0867e-05]
	Learning Rate: 4.08666e-05
	LOSS [training: 3.5985265654968877 | validation: 3.0822936892385666]
	TIME [epoch: 9.69 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595736299553974		[learning rate: 4.0718e-05]
	Learning Rate: 4.07183e-05
	LOSS [training: 3.595736299553974 | validation: 3.0752459648444392]
	TIME [epoch: 9.69 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601063848891659		[learning rate: 4.0571e-05]
	Learning Rate: 4.05705e-05
	LOSS [training: 3.601063848891659 | validation: 3.0718712351105557]
	TIME [epoch: 9.7 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5977570949147677		[learning rate: 4.0423e-05]
	Learning Rate: 4.04233e-05
	LOSS [training: 3.5977570949147677 | validation: 3.0880580790574186]
	TIME [epoch: 9.69 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6026271560464416		[learning rate: 4.0277e-05]
	Learning Rate: 4.02766e-05
	LOSS [training: 3.6026271560464416 | validation: 3.075899426105335]
	TIME [epoch: 9.68 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6014715632473178		[learning rate: 4.013e-05]
	Learning Rate: 4.01304e-05
	LOSS [training: 3.6014715632473178 | validation: 3.080987347170614]
	TIME [epoch: 9.68 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595494802526381		[learning rate: 3.9985e-05]
	Learning Rate: 3.99848e-05
	LOSS [training: 3.595494802526381 | validation: 3.0966890424387237]
	TIME [epoch: 9.7 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597473792311115		[learning rate: 3.984e-05]
	Learning Rate: 3.98397e-05
	LOSS [training: 3.597473792311115 | validation: 3.0973590889949283]
	TIME [epoch: 9.68 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602527348139622		[learning rate: 3.9695e-05]
	Learning Rate: 3.96951e-05
	LOSS [training: 3.602527348139622 | validation: 3.092512204818289]
	TIME [epoch: 9.68 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5968633721715646		[learning rate: 3.9551e-05]
	Learning Rate: 3.9551e-05
	LOSS [training: 3.5968633721715646 | validation: 3.0787174899637986]
	TIME [epoch: 9.7 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5992927897577602		[learning rate: 3.9408e-05]
	Learning Rate: 3.94075e-05
	LOSS [training: 3.5992927897577602 | validation: 3.0792619502817438]
	TIME [epoch: 9.69 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59716127810963		[learning rate: 3.9264e-05]
	Learning Rate: 3.92645e-05
	LOSS [training: 3.59716127810963 | validation: 3.081017603373039]
	TIME [epoch: 9.68 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6006733367930686		[learning rate: 3.9122e-05]
	Learning Rate: 3.9122e-05
	LOSS [training: 3.6006733367930686 | validation: 3.091773625182725]
	TIME [epoch: 9.69 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598136105239392		[learning rate: 3.898e-05]
	Learning Rate: 3.898e-05
	LOSS [training: 3.598136105239392 | validation: 3.0825907567134996]
	TIME [epoch: 9.7 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597518497024905		[learning rate: 3.8839e-05]
	Learning Rate: 3.88386e-05
	LOSS [training: 3.597518497024905 | validation: 3.089963205442034]
	TIME [epoch: 9.68 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598116717600501		[learning rate: 3.8698e-05]
	Learning Rate: 3.86976e-05
	LOSS [training: 3.598116717600501 | validation: 3.093839687344271]
	TIME [epoch: 9.68 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6004834045520306		[learning rate: 3.8557e-05]
	Learning Rate: 3.85572e-05
	LOSS [training: 3.6004834045520306 | validation: 3.0979776794387632]
	TIME [epoch: 9.7 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6002238731491607		[learning rate: 3.8417e-05]
	Learning Rate: 3.84173e-05
	LOSS [training: 3.6002238731491607 | validation: 3.086065540894723]
	TIME [epoch: 9.68 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.593517630521183		[learning rate: 3.8278e-05]
	Learning Rate: 3.82778e-05
	LOSS [training: 3.593517630521183 | validation: 3.086185446015253]
	TIME [epoch: 9.68 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602641271855925		[learning rate: 3.8139e-05]
	Learning Rate: 3.81389e-05
	LOSS [training: 3.602641271855925 | validation: 3.088517215371649]
	TIME [epoch: 9.7 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5983415712982185		[learning rate: 3.8001e-05]
	Learning Rate: 3.80005e-05
	LOSS [training: 3.5983415712982185 | validation: 3.1011999625791566]
	TIME [epoch: 9.69 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596351836790414		[learning rate: 3.7863e-05]
	Learning Rate: 3.78626e-05
	LOSS [training: 3.596351836790414 | validation: 3.08919452510202]
	TIME [epoch: 9.68 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596501908876614		[learning rate: 3.7725e-05]
	Learning Rate: 3.77252e-05
	LOSS [training: 3.596501908876614 | validation: 3.0892984636738188]
	TIME [epoch: 9.68 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596200411454416		[learning rate: 3.7588e-05]
	Learning Rate: 3.75883e-05
	LOSS [training: 3.596200411454416 | validation: 3.0820100327217315]
	TIME [epoch: 9.71 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5918089562261732		[learning rate: 3.7452e-05]
	Learning Rate: 3.74519e-05
	LOSS [training: 3.5918089562261732 | validation: 3.079300356574045]
	TIME [epoch: 9.68 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5957798340924456		[learning rate: 3.7316e-05]
	Learning Rate: 3.7316e-05
	LOSS [training: 3.5957798340924456 | validation: 3.0855238691447555]
	TIME [epoch: 9.68 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599406705408265		[learning rate: 3.7181e-05]
	Learning Rate: 3.71805e-05
	LOSS [training: 3.599406705408265 | validation: 3.075298504829192]
	TIME [epoch: 9.7 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5981852394892826		[learning rate: 3.7046e-05]
	Learning Rate: 3.70456e-05
	LOSS [training: 3.5981852394892826 | validation: 3.088856222799633]
	TIME [epoch: 9.69 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5968970905714266		[learning rate: 3.6911e-05]
	Learning Rate: 3.69112e-05
	LOSS [training: 3.5968970905714266 | validation: 3.083743820893642]
	TIME [epoch: 9.68 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5974747452128377		[learning rate: 3.6777e-05]
	Learning Rate: 3.67772e-05
	LOSS [training: 3.5974747452128377 | validation: 3.0912334811305655]
	TIME [epoch: 9.68 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5989795973057364		[learning rate: 3.6644e-05]
	Learning Rate: 3.66438e-05
	LOSS [training: 3.5989795973057364 | validation: 3.074296646299034]
	TIME [epoch: 9.71 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598580681736864		[learning rate: 3.6511e-05]
	Learning Rate: 3.65108e-05
	LOSS [training: 3.598580681736864 | validation: 3.0950034236242296]
	TIME [epoch: 9.68 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.604576836231827		[learning rate: 3.6378e-05]
	Learning Rate: 3.63783e-05
	LOSS [training: 3.604576836231827 | validation: 3.089350078345147]
	TIME [epoch: 9.68 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5986390443585785		[learning rate: 3.6246e-05]
	Learning Rate: 3.62463e-05
	LOSS [training: 3.5986390443585785 | validation: 3.1012366320232063]
	TIME [epoch: 9.71 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600643455183422		[learning rate: 3.6115e-05]
	Learning Rate: 3.61147e-05
	LOSS [training: 3.600643455183422 | validation: 3.0940500999592917]
	TIME [epoch: 9.69 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5978931589055874		[learning rate: 3.5984e-05]
	Learning Rate: 3.59837e-05
	LOSS [training: 3.5978931589055874 | validation: 3.098447097679304]
	TIME [epoch: 9.69 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.594508744490313		[learning rate: 3.5853e-05]
	Learning Rate: 3.58531e-05
	LOSS [training: 3.594508744490313 | validation: 3.080681299750221]
	TIME [epoch: 9.69 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.591457008531255		[learning rate: 3.5723e-05]
	Learning Rate: 3.5723e-05
	LOSS [training: 3.591457008531255 | validation: 3.070029478402529]
	TIME [epoch: 9.7 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5949508291401577		[learning rate: 3.5593e-05]
	Learning Rate: 3.55933e-05
	LOSS [training: 3.5949508291401577 | validation: 3.093626739266749]
	TIME [epoch: 9.68 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5959399811235295		[learning rate: 3.5464e-05]
	Learning Rate: 3.54641e-05
	LOSS [training: 3.5959399811235295 | validation: 3.091314642515133]
	TIME [epoch: 9.68 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6002556083722963		[learning rate: 3.5335e-05]
	Learning Rate: 3.53354e-05
	LOSS [training: 3.6002556083722963 | validation: 3.0960684640307328]
	TIME [epoch: 9.7 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5992617102274145		[learning rate: 3.5207e-05]
	Learning Rate: 3.52072e-05
	LOSS [training: 3.5992617102274145 | validation: 3.0927805489171023]
	TIME [epoch: 9.69 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596951569473261		[learning rate: 3.5079e-05]
	Learning Rate: 3.50794e-05
	LOSS [training: 3.596951569473261 | validation: 3.0862570286252846]
	TIME [epoch: 9.69 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6023538154614796		[learning rate: 3.4952e-05]
	Learning Rate: 3.49521e-05
	LOSS [training: 3.6023538154614796 | validation: 3.0971607647654977]
	TIME [epoch: 9.7 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5980199546806224		[learning rate: 3.4825e-05]
	Learning Rate: 3.48253e-05
	LOSS [training: 3.5980199546806224 | validation: 3.084670526542051]
	TIME [epoch: 9.68 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5937812386111654		[learning rate: 3.4699e-05]
	Learning Rate: 3.46989e-05
	LOSS [training: 3.5937812386111654 | validation: 3.0833989543958844]
	TIME [epoch: 9.68 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5956972901054796		[learning rate: 3.4573e-05]
	Learning Rate: 3.4573e-05
	LOSS [training: 3.5956972901054796 | validation: 3.0854390034977817]
	TIME [epoch: 9.68 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602121029431905		[learning rate: 3.4448e-05]
	Learning Rate: 3.44475e-05
	LOSS [training: 3.602121029431905 | validation: 3.0908742871885297]
	TIME [epoch: 9.7 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600804332169692		[learning rate: 3.4323e-05]
	Learning Rate: 3.43225e-05
	LOSS [training: 3.600804332169692 | validation: 3.087194897892355]
	TIME [epoch: 9.68 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6009520134146897		[learning rate: 3.4198e-05]
	Learning Rate: 3.4198e-05
	LOSS [training: 3.6009520134146897 | validation: 3.086227991451036]
	TIME [epoch: 9.69 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5970592961795242		[learning rate: 3.4074e-05]
	Learning Rate: 3.40738e-05
	LOSS [training: 3.5970592961795242 | validation: 3.0832599743595024]
	TIME [epoch: 9.71 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5981090005727543		[learning rate: 3.395e-05]
	Learning Rate: 3.39502e-05
	LOSS [training: 3.5981090005727543 | validation: 3.074591384738747]
	TIME [epoch: 9.68 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6017802967481884		[learning rate: 3.3827e-05]
	Learning Rate: 3.3827e-05
	LOSS [training: 3.6017802967481884 | validation: 3.088279735648057]
	TIME [epoch: 9.69 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603392407156349		[learning rate: 3.3704e-05]
	Learning Rate: 3.37042e-05
	LOSS [training: 3.603392407156349 | validation: 3.0827950336521908]
	TIME [epoch: 9.69 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6063025870540253		[learning rate: 3.3582e-05]
	Learning Rate: 3.35819e-05
	LOSS [training: 3.6063025870540253 | validation: 3.0937615779509002]
	TIME [epoch: 9.7 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601572137171175		[learning rate: 3.346e-05]
	Learning Rate: 3.346e-05
	LOSS [training: 3.601572137171175 | validation: 3.085139178461505]
	TIME [epoch: 9.69 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598217487066713		[learning rate: 3.3339e-05]
	Learning Rate: 3.33386e-05
	LOSS [training: 3.598217487066713 | validation: 3.0824244762737583]
	TIME [epoch: 9.68 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599481132401923		[learning rate: 3.3218e-05]
	Learning Rate: 3.32176e-05
	LOSS [training: 3.599481132401923 | validation: 3.101558564716956]
	TIME [epoch: 9.7 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5997264314756188		[learning rate: 3.3097e-05]
	Learning Rate: 3.30971e-05
	LOSS [training: 3.5997264314756188 | validation: 3.083107923303416]
	TIME [epoch: 9.68 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5962729427133646		[learning rate: 3.2977e-05]
	Learning Rate: 3.2977e-05
	LOSS [training: 3.5962729427133646 | validation: 3.1000446808703392]
	TIME [epoch: 9.68 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597381353380103		[learning rate: 3.2857e-05]
	Learning Rate: 3.28573e-05
	LOSS [training: 3.597381353380103 | validation: 3.10486560908762]
	TIME [epoch: 9.71 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598454221725647		[learning rate: 3.2738e-05]
	Learning Rate: 3.2738e-05
	LOSS [training: 3.598454221725647 | validation: 3.0874242052110152]
	TIME [epoch: 9.69 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5999526890142817		[learning rate: 3.2619e-05]
	Learning Rate: 3.26192e-05
	LOSS [training: 3.5999526890142817 | validation: 3.0840168996790727]
	TIME [epoch: 9.69 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600000627853432		[learning rate: 3.2501e-05]
	Learning Rate: 3.25009e-05
	LOSS [training: 3.600000627853432 | validation: 3.0795150358492838]
	TIME [epoch: 9.68 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5978238880166855		[learning rate: 3.2383e-05]
	Learning Rate: 3.23829e-05
	LOSS [training: 3.5978238880166855 | validation: 3.0882586281989957]
	TIME [epoch: 9.71 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601171804281062		[learning rate: 3.2265e-05]
	Learning Rate: 3.22654e-05
	LOSS [training: 3.601171804281062 | validation: 3.0825528292434674]
	TIME [epoch: 9.68 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5949777466703745		[learning rate: 3.2148e-05]
	Learning Rate: 3.21483e-05
	LOSS [training: 3.5949777466703745 | validation: 3.084796690899874]
	TIME [epoch: 9.68 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5981757375840644		[learning rate: 3.2032e-05]
	Learning Rate: 3.20316e-05
	LOSS [training: 3.5981757375840644 | validation: 3.082782977296779]
	TIME [epoch: 9.71 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595477776316316		[learning rate: 3.1915e-05]
	Learning Rate: 3.19154e-05
	LOSS [training: 3.595477776316316 | validation: 3.087114695486003]
	TIME [epoch: 9.69 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5982532069522897		[learning rate: 3.18e-05]
	Learning Rate: 3.17996e-05
	LOSS [training: 3.5982532069522897 | validation: 3.085244373718002]
	TIME [epoch: 9.68 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5958644491730496		[learning rate: 3.1684e-05]
	Learning Rate: 3.16842e-05
	LOSS [training: 3.5958644491730496 | validation: 3.083593622934834]
	TIME [epoch: 9.68 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598974570763912		[learning rate: 3.1569e-05]
	Learning Rate: 3.15692e-05
	LOSS [training: 3.598974570763912 | validation: 3.0813375220969523]
	TIME [epoch: 9.69 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5953091365678462		[learning rate: 3.1455e-05]
	Learning Rate: 3.14546e-05
	LOSS [training: 3.5953091365678462 | validation: 3.085436675882304]
	TIME [epoch: 9.68 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60275988535802		[learning rate: 3.134e-05]
	Learning Rate: 3.13405e-05
	LOSS [training: 3.60275988535802 | validation: 3.0810246450512366]
	TIME [epoch: 9.68 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595974478779941		[learning rate: 3.1227e-05]
	Learning Rate: 3.12267e-05
	LOSS [training: 3.595974478779941 | validation: 3.0775494526220086]
	TIME [epoch: 9.71 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59711039994225		[learning rate: 3.1113e-05]
	Learning Rate: 3.11134e-05
	LOSS [training: 3.59711039994225 | validation: 3.0818741587153995]
	TIME [epoch: 9.68 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6000323446060043		[learning rate: 3.1e-05]
	Learning Rate: 3.10005e-05
	LOSS [training: 3.6000323446060043 | validation: 3.07859864503576]
	TIME [epoch: 9.68 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6001509226566384		[learning rate: 3.0888e-05]
	Learning Rate: 3.0888e-05
	LOSS [training: 3.6001509226566384 | validation: 3.0828165007388852]
	TIME [epoch: 9.69 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601460433704827		[learning rate: 3.0776e-05]
	Learning Rate: 3.07759e-05
	LOSS [training: 3.601460433704827 | validation: 3.0848529033843834]
	TIME [epoch: 9.69 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5975632617885913		[learning rate: 3.0664e-05]
	Learning Rate: 3.06642e-05
	LOSS [training: 3.5975632617885913 | validation: 3.0922169727588695]
	TIME [epoch: 9.68 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5989996787451246		[learning rate: 3.0553e-05]
	Learning Rate: 3.05529e-05
	LOSS [training: 3.5989996787451246 | validation: 3.0696968047746056]
	TIME [epoch: 9.68 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596513016661987		[learning rate: 3.0442e-05]
	Learning Rate: 3.0442e-05
	LOSS [training: 3.596513016661987 | validation: 3.088120897252909]
	TIME [epoch: 9.71 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5991110449398755		[learning rate: 3.0332e-05]
	Learning Rate: 3.03316e-05
	LOSS [training: 3.5991110449398755 | validation: 3.0712510065371017]
	TIME [epoch: 9.69 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597712074888844		[learning rate: 3.0221e-05]
	Learning Rate: 3.02215e-05
	LOSS [training: 3.597712074888844 | validation: 3.092018427348343]
	TIME [epoch: 9.68 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602776805241085		[learning rate: 3.0112e-05]
	Learning Rate: 3.01118e-05
	LOSS [training: 3.602776805241085 | validation: 3.088010578738255]
	TIME [epoch: 9.71 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595395129124733		[learning rate: 3.0003e-05]
	Learning Rate: 3.00025e-05
	LOSS [training: 3.595395129124733 | validation: 3.078329686173041]
	TIME [epoch: 9.69 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.604974262703906		[learning rate: 2.9894e-05]
	Learning Rate: 2.98936e-05
	LOSS [training: 3.604974262703906 | validation: 3.0699401123990206]
	TIME [epoch: 9.68 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5975110159041224		[learning rate: 2.9785e-05]
	Learning Rate: 2.97852e-05
	LOSS [training: 3.5975110159041224 | validation: 3.081011586630101]
	TIME [epoch: 9.69 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597247420462412		[learning rate: 2.9677e-05]
	Learning Rate: 2.96771e-05
	LOSS [training: 3.597247420462412 | validation: 3.079126174943566]
	TIME [epoch: 9.69 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5994973250669107		[learning rate: 2.9569e-05]
	Learning Rate: 2.95694e-05
	LOSS [training: 3.5994973250669107 | validation: 3.082278481317469]
	TIME [epoch: 9.68 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5986462383653786		[learning rate: 2.9462e-05]
	Learning Rate: 2.94621e-05
	LOSS [training: 3.5986462383653786 | validation: 3.08777885913118]
	TIME [epoch: 9.68 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59734515159439		[learning rate: 2.9355e-05]
	Learning Rate: 2.93551e-05
	LOSS [training: 3.59734515159439 | validation: 3.0782274072608447]
	TIME [epoch: 9.7 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5990488100845526		[learning rate: 2.9249e-05]
	Learning Rate: 2.92486e-05
	LOSS [training: 3.5990488100845526 | validation: 3.086883152609392]
	TIME [epoch: 9.68 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.593881793079722		[learning rate: 2.9142e-05]
	Learning Rate: 2.91425e-05
	LOSS [training: 3.593881793079722 | validation: 3.101720711722326]
	TIME [epoch: 9.68 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5957749663101892		[learning rate: 2.9037e-05]
	Learning Rate: 2.90367e-05
	LOSS [training: 3.5957749663101892 | validation: 3.0874928224574423]
	TIME [epoch: 9.69 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6004161113826854		[learning rate: 2.8931e-05]
	Learning Rate: 2.89313e-05
	LOSS [training: 3.6004161113826854 | validation: 3.092620799941164]
	TIME [epoch: 9.69 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5971094630766416		[learning rate: 2.8826e-05]
	Learning Rate: 2.88263e-05
	LOSS [training: 3.5971094630766416 | validation: 3.0826481671416777]
	TIME [epoch: 9.68 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595561325765439		[learning rate: 2.8722e-05]
	Learning Rate: 2.87217e-05
	LOSS [training: 3.595561325765439 | validation: 3.0872301725287166]
	TIME [epoch: 9.68 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.593279484320591		[learning rate: 2.8617e-05]
	Learning Rate: 2.86175e-05
	LOSS [training: 3.593279484320591 | validation: 3.097950093187038]
	TIME [epoch: 9.7 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.591635543957281		[learning rate: 2.8514e-05]
	Learning Rate: 2.85136e-05
	LOSS [training: 3.591635543957281 | validation: 3.093648443256526]
	TIME [epoch: 9.68 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598937923221098		[learning rate: 2.841e-05]
	Learning Rate: 2.84102e-05
	LOSS [training: 3.598937923221098 | validation: 3.084847867332245]
	TIME [epoch: 9.68 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5944186771700517		[learning rate: 2.8307e-05]
	Learning Rate: 2.83071e-05
	LOSS [training: 3.5944186771700517 | validation: 3.09332970092887]
	TIME [epoch: 9.7 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5951799494074606		[learning rate: 2.8204e-05]
	Learning Rate: 2.82043e-05
	LOSS [training: 3.5951799494074606 | validation: 3.093219432702]
	TIME [epoch: 9.69 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600175952099131		[learning rate: 2.8102e-05]
	Learning Rate: 2.8102e-05
	LOSS [training: 3.600175952099131 | validation: 3.083070475477176]
	TIME [epoch: 9.68 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5960414496422466		[learning rate: 2.8e-05]
	Learning Rate: 2.8e-05
	LOSS [training: 3.5960414496422466 | validation: 3.0880553466492455]
	TIME [epoch: 9.68 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599164691821714		[learning rate: 2.7898e-05]
	Learning Rate: 2.78984e-05
	LOSS [training: 3.599164691821714 | validation: 3.084156907340412]
	TIME [epoch: 9.7 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5973147756218964		[learning rate: 2.7797e-05]
	Learning Rate: 2.77971e-05
	LOSS [training: 3.5973147756218964 | validation: 3.1012696424098394]
	TIME [epoch: 9.68 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5991454257052617		[learning rate: 2.7696e-05]
	Learning Rate: 2.76963e-05
	LOSS [training: 3.5991454257052617 | validation: 3.091003728243387]
	TIME [epoch: 9.68 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5959361319435486		[learning rate: 2.7596e-05]
	Learning Rate: 2.75957e-05
	LOSS [training: 3.5959361319435486 | validation: 3.0821103375458434]
	TIME [epoch: 9.7 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5947076102647584		[learning rate: 2.7496e-05]
	Learning Rate: 2.74956e-05
	LOSS [training: 3.5947076102647584 | validation: 3.0865340849839904]
	TIME [epoch: 9.68 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600681560925642		[learning rate: 2.7396e-05]
	Learning Rate: 2.73958e-05
	LOSS [training: 3.600681560925642 | validation: 3.0851923230008094]
	TIME [epoch: 9.68 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5986471307480956		[learning rate: 2.7296e-05]
	Learning Rate: 2.72964e-05
	LOSS [training: 3.5986471307480956 | validation: 3.0929817758760123]
	TIME [epoch: 9.69 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5981836945607766		[learning rate: 2.7197e-05]
	Learning Rate: 2.71973e-05
	LOSS [training: 3.5981836945607766 | validation: 3.0884389808472648]
	TIME [epoch: 9.7 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601473678804183		[learning rate: 2.7099e-05]
	Learning Rate: 2.70986e-05
	LOSS [training: 3.601473678804183 | validation: 3.0783244915397634]
	TIME [epoch: 9.69 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595310977267682		[learning rate: 2.7e-05]
	Learning Rate: 2.70003e-05
	LOSS [training: 3.595310977267682 | validation: 3.09114556898332]
	TIME [epoch: 9.68 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600147804200047		[learning rate: 2.6902e-05]
	Learning Rate: 2.69023e-05
	LOSS [training: 3.600147804200047 | validation: 3.0985759303907456]
	TIME [epoch: 9.7 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6002171736792428		[learning rate: 2.6805e-05]
	Learning Rate: 2.68047e-05
	LOSS [training: 3.6002171736792428 | validation: 3.0831932105947204]
	TIME [epoch: 9.68 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5945184517535083		[learning rate: 2.6707e-05]
	Learning Rate: 2.67074e-05
	LOSS [training: 3.5945184517535083 | validation: 3.088761985916639]
	TIME [epoch: 9.68 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5944635050186555		[learning rate: 2.661e-05]
	Learning Rate: 2.66105e-05
	LOSS [training: 3.5944635050186555 | validation: 3.0777488628952114]
	TIME [epoch: 9.7 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595024971910692		[learning rate: 2.6514e-05]
	Learning Rate: 2.65139e-05
	LOSS [training: 3.595024971910692 | validation: 3.081089590486329]
	TIME [epoch: 9.69 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.594325675995813		[learning rate: 2.6418e-05]
	Learning Rate: 2.64177e-05
	LOSS [training: 3.594325675995813 | validation: 3.0851266733579736]
	TIME [epoch: 9.67 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5969685947612193		[learning rate: 2.6322e-05]
	Learning Rate: 2.63218e-05
	LOSS [training: 3.5969685947612193 | validation: 3.0793658202142598]
	TIME [epoch: 9.68 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5987199641821577		[learning rate: 2.6226e-05]
	Learning Rate: 2.62263e-05
	LOSS [training: 3.5987199641821577 | validation: 3.0941888890272127]
	TIME [epoch: 9.7 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.589654262959993		[learning rate: 2.6131e-05]
	Learning Rate: 2.61311e-05
	LOSS [training: 3.589654262959993 | validation: 3.0794721973320747]
	TIME [epoch: 9.68 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5969502963001987		[learning rate: 2.6036e-05]
	Learning Rate: 2.60363e-05
	LOSS [training: 3.5969502963001987 | validation: 3.0967414646228457]
	TIME [epoch: 9.68 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.591065271805129		[learning rate: 2.5942e-05]
	Learning Rate: 2.59418e-05
	LOSS [training: 3.591065271805129 | validation: 3.0860688326833747]
	TIME [epoch: 9.7 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5946432010090357		[learning rate: 2.5848e-05]
	Learning Rate: 2.58477e-05
	LOSS [training: 3.5946432010090357 | validation: 3.1044317147747575]
	TIME [epoch: 9.68 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5930571302435284		[learning rate: 2.5754e-05]
	Learning Rate: 2.57539e-05
	LOSS [training: 3.5930571302435284 | validation: 3.0775942652427366]
	TIME [epoch: 9.68 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5966544767289683		[learning rate: 2.566e-05]
	Learning Rate: 2.56604e-05
	LOSS [training: 3.5966544767289683 | validation: 3.099768463693599]
	TIME [epoch: 9.68 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5967607414907343		[learning rate: 2.5567e-05]
	Learning Rate: 2.55673e-05
	LOSS [training: 3.5967607414907343 | validation: 3.0740270803601915]
	TIME [epoch: 9.69 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5982728871632546		[learning rate: 2.5474e-05]
	Learning Rate: 2.54745e-05
	LOSS [training: 3.5982728871632546 | validation: 3.0880445534912395]
	TIME [epoch: 9.68 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597167120990421		[learning rate: 2.5382e-05]
	Learning Rate: 2.5382e-05
	LOSS [training: 3.597167120990421 | validation: 3.0927672324936]
	TIME [epoch: 9.68 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6001902025977985		[learning rate: 2.529e-05]
	Learning Rate: 2.52899e-05
	LOSS [training: 3.6001902025977985 | validation: 3.0865577470421464]
	TIME [epoch: 9.7 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5973059008903805		[learning rate: 2.5198e-05]
	Learning Rate: 2.51981e-05
	LOSS [training: 3.5973059008903805 | validation: 3.096091139157412]
	TIME [epoch: 9.68 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598996644277688		[learning rate: 2.5107e-05]
	Learning Rate: 2.51067e-05
	LOSS [training: 3.598996644277688 | validation: 3.0954465606938313]
	TIME [epoch: 9.68 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597302026570611		[learning rate: 2.5016e-05]
	Learning Rate: 2.50156e-05
	LOSS [training: 3.597302026570611 | validation: 3.092904688563532]
	TIME [epoch: 9.69 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59625383978402		[learning rate: 2.4925e-05]
	Learning Rate: 2.49248e-05
	LOSS [training: 3.59625383978402 | validation: 3.0889467432713156]
	TIME [epoch: 9.69 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5993763324197587		[learning rate: 2.4834e-05]
	Learning Rate: 2.48343e-05
	LOSS [training: 3.5993763324197587 | validation: 3.0939026012256567]
	TIME [epoch: 9.68 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595707653885141		[learning rate: 2.4744e-05]
	Learning Rate: 2.47442e-05
	LOSS [training: 3.595707653885141 | validation: 3.101009118954369]
	TIME [epoch: 9.68 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5970202331310475		[learning rate: 2.4654e-05]
	Learning Rate: 2.46544e-05
	LOSS [training: 3.5970202331310475 | validation: 3.0855803840915916]
	TIME [epoch: 9.7 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598020245610872		[learning rate: 2.4565e-05]
	Learning Rate: 2.45649e-05
	LOSS [training: 3.598020245610872 | validation: 3.0805890885139364]
	TIME [epoch: 9.68 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598776221492834		[learning rate: 2.4476e-05]
	Learning Rate: 2.44758e-05
	LOSS [training: 3.598776221492834 | validation: 3.1014395821050256]
	TIME [epoch: 9.68 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5969062082734866		[learning rate: 2.4387e-05]
	Learning Rate: 2.4387e-05
	LOSS [training: 3.5969062082734866 | validation: 3.0852833786319116]
	TIME [epoch: 9.7 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5957293749902774		[learning rate: 2.4298e-05]
	Learning Rate: 2.42985e-05
	LOSS [training: 3.5957293749902774 | validation: 3.0903544314043003]
	TIME [epoch: 9.68 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.593707894227491		[learning rate: 2.421e-05]
	Learning Rate: 2.42103e-05
	LOSS [training: 3.593707894227491 | validation: 3.0807205995575395]
	TIME [epoch: 9.68 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6003408254866023		[learning rate: 2.4122e-05]
	Learning Rate: 2.41224e-05
	LOSS [training: 3.6003408254866023 | validation: 3.0876263449486943]
	TIME [epoch: 9.69 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597084890275476		[learning rate: 2.4035e-05]
	Learning Rate: 2.40349e-05
	LOSS [training: 3.597084890275476 | validation: 3.0798844889416244]
	TIME [epoch: 9.69 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596701060168074		[learning rate: 2.3948e-05]
	Learning Rate: 2.39477e-05
	LOSS [training: 3.596701060168074 | validation: 3.084193005575362]
	TIME [epoch: 9.68 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5980372614272893		[learning rate: 2.3861e-05]
	Learning Rate: 2.38608e-05
	LOSS [training: 3.5980372614272893 | validation: 3.083863740287007]
	TIME [epoch: 9.68 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6016543744440996		[learning rate: 2.3774e-05]
	Learning Rate: 2.37742e-05
	LOSS [training: 3.6016543744440996 | validation: 3.092473968785397]
	TIME [epoch: 9.7 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5953890409239633		[learning rate: 2.3688e-05]
	Learning Rate: 2.36879e-05
	LOSS [training: 3.5953890409239633 | validation: 3.0904223212409327]
	TIME [epoch: 9.68 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.594773939593499		[learning rate: 2.3602e-05]
	Learning Rate: 2.36019e-05
	LOSS [training: 3.594773939593499 | validation: 3.086554885009186]
	TIME [epoch: 9.68 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597810716147786		[learning rate: 2.3516e-05]
	Learning Rate: 2.35163e-05
	LOSS [training: 3.597810716147786 | validation: 3.0809576248028665]
	TIME [epoch: 9.69 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5938467456050645		[learning rate: 2.3431e-05]
	Learning Rate: 2.34309e-05
	LOSS [training: 3.5938467456050645 | validation: 3.087072356145832]
	TIME [epoch: 9.69 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5981787131303236		[learning rate: 2.3346e-05]
	Learning Rate: 2.33459e-05
	LOSS [training: 3.5981787131303236 | validation: 3.084272867792736]
	TIME [epoch: 9.68 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597263400870591		[learning rate: 2.3261e-05]
	Learning Rate: 2.32612e-05
	LOSS [training: 3.597263400870591 | validation: 3.089082739740376]
	TIME [epoch: 9.67 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5952754918900474		[learning rate: 2.3177e-05]
	Learning Rate: 2.31768e-05
	LOSS [training: 3.5952754918900474 | validation: 3.092760349271354]
	TIME [epoch: 9.7 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600657047837977		[learning rate: 2.3093e-05]
	Learning Rate: 2.30926e-05
	LOSS [training: 3.600657047837977 | validation: 3.0820070992399917]
	TIME [epoch: 9.68 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598176730427288		[learning rate: 2.3009e-05]
	Learning Rate: 2.30088e-05
	LOSS [training: 3.598176730427288 | validation: 3.080355812533968]
	TIME [epoch: 9.68 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597716261163465		[learning rate: 2.2925e-05]
	Learning Rate: 2.29253e-05
	LOSS [training: 3.597716261163465 | validation: 3.0937840774726304]
	TIME [epoch: 9.7 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5966731037316473		[learning rate: 2.2842e-05]
	Learning Rate: 2.28421e-05
	LOSS [training: 3.5966731037316473 | validation: 3.0859275399211663]
	TIME [epoch: 9.68 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5959406944497276		[learning rate: 2.2759e-05]
	Learning Rate: 2.27592e-05
	LOSS [training: 3.5959406944497276 | validation: 3.0782349231588455]
	TIME [epoch: 9.68 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6014096130566093		[learning rate: 2.2677e-05]
	Learning Rate: 2.26767e-05
	LOSS [training: 3.6014096130566093 | validation: 3.0817377466721627]
	TIME [epoch: 9.67 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5935384872095333		[learning rate: 2.2594e-05]
	Learning Rate: 2.25944e-05
	LOSS [training: 3.5935384872095333 | validation: 3.0845796427443775]
	TIME [epoch: 9.69 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5982194159883703		[learning rate: 2.2512e-05]
	Learning Rate: 2.25124e-05
	LOSS [training: 3.5982194159883703 | validation: 3.084731374586908]
	TIME [epoch: 9.68 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.594841515858792		[learning rate: 2.2431e-05]
	Learning Rate: 2.24307e-05
	LOSS [training: 3.594841515858792 | validation: 3.0861973770406457]
	TIME [epoch: 9.68 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5959960438810477		[learning rate: 2.2349e-05]
	Learning Rate: 2.23493e-05
	LOSS [training: 3.5959960438810477 | validation: 3.096069074359061]
	TIME [epoch: 9.7 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.592668151540458		[learning rate: 2.2268e-05]
	Learning Rate: 2.22682e-05
	LOSS [training: 3.592668151540458 | validation: 3.093413090321578]
	TIME [epoch: 9.68 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59848675189786		[learning rate: 2.2187e-05]
	Learning Rate: 2.21873e-05
	LOSS [training: 3.59848675189786 | validation: 3.0935353472883462]
	TIME [epoch: 9.68 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600340036201449		[learning rate: 2.2107e-05]
	Learning Rate: 2.21068e-05
	LOSS [training: 3.600340036201449 | validation: 3.0884963006544686]
	TIME [epoch: 9.68 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599840221255886		[learning rate: 2.2027e-05]
	Learning Rate: 2.20266e-05
	LOSS [training: 3.599840221255886 | validation: 3.0969100681961192]
	TIME [epoch: 9.69 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597480168579177		[learning rate: 2.1947e-05]
	Learning Rate: 2.19467e-05
	LOSS [training: 3.597480168579177 | validation: 3.1040781241205226]
	TIME [epoch: 9.68 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5948463046374357		[learning rate: 2.1867e-05]
	Learning Rate: 2.1867e-05
	LOSS [training: 3.5948463046374357 | validation: 3.0866777182940677]
	TIME [epoch: 9.68 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597991319783651		[learning rate: 2.1788e-05]
	Learning Rate: 2.17877e-05
	LOSS [training: 3.597991319783651 | validation: 3.0890322675059583]
	TIME [epoch: 9.7 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5990693305614405		[learning rate: 2.1709e-05]
	Learning Rate: 2.17086e-05
	LOSS [training: 3.5990693305614405 | validation: 3.0860692033849446]
	TIME [epoch: 9.67 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.592484646919216		[learning rate: 2.163e-05]
	Learning Rate: 2.16298e-05
	LOSS [training: 3.592484646919216 | validation: 3.0834295404466787]
	TIME [epoch: 9.67 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600101647250111		[learning rate: 2.1551e-05]
	Learning Rate: 2.15513e-05
	LOSS [training: 3.600101647250111 | validation: 3.0803280421765304]
	TIME [epoch: 9.7 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5971138511727845		[learning rate: 2.1473e-05]
	Learning Rate: 2.14731e-05
	LOSS [training: 3.5971138511727845 | validation: 3.0795128712924957]
	TIME [epoch: 9.69 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599181090516976		[learning rate: 2.1395e-05]
	Learning Rate: 2.13952e-05
	LOSS [training: 3.599181090516976 | validation: 3.0866351439573023]
	TIME [epoch: 9.69 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.593589780972451		[learning rate: 2.1318e-05]
	Learning Rate: 2.13175e-05
	LOSS [training: 3.593589780972451 | validation: 3.07484062189937]
	TIME [epoch: 9.67 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5943487957879747		[learning rate: 2.124e-05]
	Learning Rate: 2.12402e-05
	LOSS [training: 3.5943487957879747 | validation: 3.083014426808203]
	TIME [epoch: 9.7 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5960630146274397		[learning rate: 2.1163e-05]
	Learning Rate: 2.11631e-05
	LOSS [training: 3.5960630146274397 | validation: 3.0794213923766462]
	TIME [epoch: 9.68 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5959812372466926		[learning rate: 2.1086e-05]
	Learning Rate: 2.10863e-05
	LOSS [training: 3.5959812372466926 | validation: 3.089515088063509]
	TIME [epoch: 9.67 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5937214265233677		[learning rate: 2.101e-05]
	Learning Rate: 2.10098e-05
	LOSS [training: 3.5937214265233677 | validation: 3.0833244319212625]
	TIME [epoch: 9.7 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.593927841811575		[learning rate: 2.0934e-05]
	Learning Rate: 2.09335e-05
	LOSS [training: 3.593927841811575 | validation: 3.0962852148358615]
	TIME [epoch: 9.67 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5941687803189515		[learning rate: 2.0858e-05]
	Learning Rate: 2.08575e-05
	LOSS [training: 3.5941687803189515 | validation: 3.0873222632540034]
	TIME [epoch: 9.68 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599948514237449		[learning rate: 2.0782e-05]
	Learning Rate: 2.07819e-05
	LOSS [training: 3.599948514237449 | validation: 3.0873990028538225]
	TIME [epoch: 9.68 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5976247475064485		[learning rate: 2.0706e-05]
	Learning Rate: 2.07064e-05
	LOSS [training: 3.5976247475064485 | validation: 3.0787183842316677]
	TIME [epoch: 9.7 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6003951597737385		[learning rate: 2.0631e-05]
	Learning Rate: 2.06313e-05
	LOSS [training: 3.6003951597737385 | validation: 3.0827487665250577]
	TIME [epoch: 9.68 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597764610015928		[learning rate: 2.0556e-05]
	Learning Rate: 2.05564e-05
	LOSS [training: 3.597764610015928 | validation: 3.0836941835351968]
	TIME [epoch: 9.68 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5957629077996978		[learning rate: 2.0482e-05]
	Learning Rate: 2.04818e-05
	LOSS [training: 3.5957629077996978 | validation: 3.0941455553824753]
	TIME [epoch: 9.71 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5992606706494525		[learning rate: 2.0407e-05]
	Learning Rate: 2.04075e-05
	LOSS [training: 3.5992606706494525 | validation: 3.093166210765505]
	TIME [epoch: 9.68 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5989773493091364		[learning rate: 2.0333e-05]
	Learning Rate: 2.03334e-05
	LOSS [training: 3.5989773493091364 | validation: 3.0841731124473006]
	TIME [epoch: 9.68 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5954658057807776		[learning rate: 2.026e-05]
	Learning Rate: 2.02596e-05
	LOSS [training: 3.5954658057807776 | validation: 3.089707453849156]
	TIME [epoch: 9.69 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598526185915398		[learning rate: 2.0186e-05]
	Learning Rate: 2.01861e-05
	LOSS [training: 3.598526185915398 | validation: 3.084866351164054]
	TIME [epoch: 9.68 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601590363507068		[learning rate: 2.0113e-05]
	Learning Rate: 2.01129e-05
	LOSS [training: 3.601590363507068 | validation: 3.078631364012357]
	TIME [epoch: 9.68 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6004337007961467		[learning rate: 2.004e-05]
	Learning Rate: 2.00399e-05
	LOSS [training: 3.6004337007961467 | validation: 3.09356117999458]
	TIME [epoch: 9.67 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5975379043044917		[learning rate: 1.9967e-05]
	Learning Rate: 1.99671e-05
	LOSS [training: 3.5975379043044917 | validation: 3.0835118009595943]
	TIME [epoch: 9.7 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5970193466901796		[learning rate: 1.9895e-05]
	Learning Rate: 1.98947e-05
	LOSS [training: 3.5970193466901796 | validation: 3.083709163100175]
	TIME [epoch: 9.68 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599692828603608		[learning rate: 1.9822e-05]
	Learning Rate: 1.98225e-05
	LOSS [training: 3.599692828603608 | validation: 3.081627459418067]
	TIME [epoch: 9.68 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.594660492391752		[learning rate: 1.9751e-05]
	Learning Rate: 1.97505e-05
	LOSS [training: 3.594660492391752 | validation: 3.090865568709972]
	TIME [epoch: 9.69 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5984811185143926		[learning rate: 1.9679e-05]
	Learning Rate: 1.96789e-05
	LOSS [training: 3.5984811185143926 | validation: 3.0851579187066647]
	TIME [epoch: 9.68 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603619704058597		[learning rate: 1.9607e-05]
	Learning Rate: 1.96074e-05
	LOSS [training: 3.603619704058597 | validation: 3.097572095447672]
	TIME [epoch: 9.68 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5923258679335524		[learning rate: 1.9536e-05]
	Learning Rate: 1.95363e-05
	LOSS [training: 3.5923258679335524 | validation: 3.0902241678663507]
	TIME [epoch: 9.69 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5953277866679505		[learning rate: 1.9465e-05]
	Learning Rate: 1.94654e-05
	LOSS [training: 3.5953277866679505 | validation: 3.0906617793572786]
	TIME [epoch: 9.7 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599628236419403		[learning rate: 1.9395e-05]
	Learning Rate: 1.93948e-05
	LOSS [training: 3.599628236419403 | validation: 3.0922483840554027]
	TIME [epoch: 9.68 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5943989050848635		[learning rate: 1.9324e-05]
	Learning Rate: 1.93244e-05
	LOSS [training: 3.5943989050848635 | validation: 3.0942916662610367]
	TIME [epoch: 9.68 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5985278770879296		[learning rate: 1.9254e-05]
	Learning Rate: 1.92542e-05
	LOSS [training: 3.5985278770879296 | validation: 3.093174420274181]
	TIME [epoch: 9.71 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597101459395342		[learning rate: 1.9184e-05]
	Learning Rate: 1.91844e-05
	LOSS [training: 3.597101459395342 | validation: 3.0856744658985527]
	TIME [epoch: 9.68 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5971925834792353		[learning rate: 1.9115e-05]
	Learning Rate: 1.91147e-05
	LOSS [training: 3.5971925834792353 | validation: 3.0719931120136996]
	TIME [epoch: 9.69 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5969405830074925		[learning rate: 1.9045e-05]
	Learning Rate: 1.90454e-05
	LOSS [training: 3.5969405830074925 | validation: 3.096980146379268]
	TIME [epoch: 9.69 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599183713388883		[learning rate: 1.8976e-05]
	Learning Rate: 1.89763e-05
	LOSS [training: 3.599183713388883 | validation: 3.094519905847808]
	TIME [epoch: 9.68 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596065120115334		[learning rate: 1.8907e-05]
	Learning Rate: 1.89074e-05
	LOSS [training: 3.596065120115334 | validation: 3.0907640169219133]
	TIME [epoch: 9.67 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5971985238731476		[learning rate: 1.8839e-05]
	Learning Rate: 1.88388e-05
	LOSS [training: 3.5971985238731476 | validation: 3.0906021616078325]
	TIME [epoch: 9.67 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597463203798623		[learning rate: 1.877e-05]
	Learning Rate: 1.87704e-05
	LOSS [training: 3.597463203798623 | validation: 3.09710965985939]
	TIME [epoch: 9.69 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597203926069247		[learning rate: 1.8702e-05]
	Learning Rate: 1.87023e-05
	LOSS [training: 3.597203926069247 | validation: 3.1001645283748993]
	TIME [epoch: 9.67 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6011877725082235		[learning rate: 1.8634e-05]
	Learning Rate: 1.86344e-05
	LOSS [training: 3.6011877725082235 | validation: 3.091423756049369]
	TIME [epoch: 9.68 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596229384786109		[learning rate: 1.8567e-05]
	Learning Rate: 1.85668e-05
	LOSS [training: 3.596229384786109 | validation: 3.0871569589359966]
	TIME [epoch: 9.69 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6035585451930587		[learning rate: 1.8499e-05]
	Learning Rate: 1.84994e-05
	LOSS [training: 3.6035585451930587 | validation: 3.1038935425119694]
	TIME [epoch: 9.68 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5997709250967262		[learning rate: 1.8432e-05]
	Learning Rate: 1.84323e-05
	LOSS [training: 3.5997709250967262 | validation: 3.088114072419841]
	TIME [epoch: 9.69 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597898689647388		[learning rate: 1.8365e-05]
	Learning Rate: 1.83654e-05
	LOSS [training: 3.597898689647388 | validation: 3.0884902279142574]
	TIME [epoch: 9.69 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5976762152471053		[learning rate: 1.8299e-05]
	Learning Rate: 1.82987e-05
	LOSS [training: 3.5976762152471053 | validation: 3.084241956864562]
	TIME [epoch: 9.7 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.604038988897443		[learning rate: 1.8232e-05]
	Learning Rate: 1.82323e-05
	LOSS [training: 3.604038988897443 | validation: 3.100717715041649]
	TIME [epoch: 9.68 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598923920397806		[learning rate: 1.8166e-05]
	Learning Rate: 1.81662e-05
	LOSS [training: 3.598923920397806 | validation: 3.0941362101509915]
	TIME [epoch: 9.68 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595837858809946		[learning rate: 1.81e-05]
	Learning Rate: 1.81002e-05
	LOSS [training: 3.595837858809946 | validation: 3.1030474385095093]
	TIME [epoch: 9.7 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.594676274106668		[learning rate: 1.8035e-05]
	Learning Rate: 1.80346e-05
	LOSS [training: 3.594676274106668 | validation: 3.092475546850478]
	TIME [epoch: 9.68 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596151941013125		[learning rate: 1.7969e-05]
	Learning Rate: 1.79691e-05
	LOSS [training: 3.596151941013125 | validation: 3.096870926164491]
	TIME [epoch: 9.67 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597419847097388		[learning rate: 1.7904e-05]
	Learning Rate: 1.79039e-05
	LOSS [training: 3.597419847097388 | validation: 3.0890512162016512]
	TIME [epoch: 9.7 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596886551270296		[learning rate: 1.7839e-05]
	Learning Rate: 1.78389e-05
	LOSS [training: 3.596886551270296 | validation: 3.092273699015766]
	TIME [epoch: 9.7 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597331668686637		[learning rate: 1.7774e-05]
	Learning Rate: 1.77742e-05
	LOSS [training: 3.597331668686637 | validation: 3.0995603172291966]
	TIME [epoch: 9.68 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602859493294866		[learning rate: 1.771e-05]
	Learning Rate: 1.77097e-05
	LOSS [training: 3.602859493294866 | validation: 3.1024694002000195]
	TIME [epoch: 9.68 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5965119724874364		[learning rate: 1.7645e-05]
	Learning Rate: 1.76454e-05
	LOSS [training: 3.5965119724874364 | validation: 3.089348049808299]
	TIME [epoch: 9.69 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5964777638418077		[learning rate: 1.7581e-05]
	Learning Rate: 1.75814e-05
	LOSS [training: 3.5964777638418077 | validation: 3.086440683155595]
	TIME [epoch: 9.66 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5929646614325605		[learning rate: 1.7518e-05]
	Learning Rate: 1.75176e-05
	LOSS [training: 3.5929646614325605 | validation: 3.0831796761915915]
	TIME [epoch: 9.68 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5958357203999207		[learning rate: 1.7454e-05]
	Learning Rate: 1.7454e-05
	LOSS [training: 3.5958357203999207 | validation: 3.091148821678023]
	TIME [epoch: 9.69 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6013316103194315		[learning rate: 1.7391e-05]
	Learning Rate: 1.73907e-05
	LOSS [training: 3.6013316103194315 | validation: 3.0969456197396013]
	TIME [epoch: 9.69 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59883081549986		[learning rate: 1.7328e-05]
	Learning Rate: 1.73275e-05
	LOSS [training: 3.59883081549986 | validation: 3.0952967872848594]
	TIME [epoch: 9.68 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.593126586724005		[learning rate: 1.7265e-05]
	Learning Rate: 1.72647e-05
	LOSS [training: 3.593126586724005 | validation: 3.088567091981508]
	TIME [epoch: 9.68 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5973862011302		[learning rate: 1.7202e-05]
	Learning Rate: 1.7202e-05
	LOSS [training: 3.5973862011302 | validation: 3.074917564127091]
	TIME [epoch: 9.7 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5978569038876573		[learning rate: 1.714e-05]
	Learning Rate: 1.71396e-05
	LOSS [training: 3.5978569038876573 | validation: 3.0829923136510438]
	TIME [epoch: 9.68 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596523977472659		[learning rate: 1.7077e-05]
	Learning Rate: 1.70774e-05
	LOSS [training: 3.596523977472659 | validation: 3.080966809633293]
	TIME [epoch: 9.68 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595407273644105		[learning rate: 1.7015e-05]
	Learning Rate: 1.70154e-05
	LOSS [training: 3.595407273644105 | validation: 3.082758964622331]
	TIME [epoch: 9.71 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5949946009133464		[learning rate: 1.6954e-05]
	Learning Rate: 1.69537e-05
	LOSS [training: 3.5949946009133464 | validation: 3.0927505637285977]
	TIME [epoch: 9.67 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5955408365709367		[learning rate: 1.6892e-05]
	Learning Rate: 1.68921e-05
	LOSS [training: 3.5955408365709367 | validation: 3.077136519423051]
	TIME [epoch: 9.68 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599055039014503		[learning rate: 1.6831e-05]
	Learning Rate: 1.68308e-05
	LOSS [training: 3.599055039014503 | validation: 3.078274371498957]
	TIME [epoch: 9.69 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5990599327276747		[learning rate: 1.677e-05]
	Learning Rate: 1.67697e-05
	LOSS [training: 3.5990599327276747 | validation: 3.0844391915254232]
	TIME [epoch: 9.7 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5959637081622504		[learning rate: 1.6709e-05]
	Learning Rate: 1.67089e-05
	LOSS [training: 3.5959637081622504 | validation: 3.0910342085450218]
	TIME [epoch: 9.67 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59306521818057		[learning rate: 1.6648e-05]
	Learning Rate: 1.66482e-05
	LOSS [training: 3.59306521818057 | validation: 3.09358924552674]
	TIME [epoch: 9.68 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6012538548871107		[learning rate: 1.6588e-05]
	Learning Rate: 1.65878e-05
	LOSS [training: 3.6012538548871107 | validation: 3.0816809479698124]
	TIME [epoch: 9.7 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5973570541074755		[learning rate: 1.6528e-05]
	Learning Rate: 1.65276e-05
	LOSS [training: 3.5973570541074755 | validation: 3.0896100940890556]
	TIME [epoch: 9.69 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5964680056499274		[learning rate: 1.6468e-05]
	Learning Rate: 1.64677e-05
	LOSS [training: 3.5964680056499274 | validation: 3.0841625774126418]
	TIME [epoch: 9.68 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5997230440406964		[learning rate: 1.6408e-05]
	Learning Rate: 1.64079e-05
	LOSS [training: 3.5997230440406964 | validation: 3.0937841154060117]
	TIME [epoch: 9.7 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5960141094522386		[learning rate: 1.6348e-05]
	Learning Rate: 1.63483e-05
	LOSS [training: 3.5960141094522386 | validation: 3.08865739621191]
	TIME [epoch: 9.68 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6003109468397754		[learning rate: 1.6289e-05]
	Learning Rate: 1.6289e-05
	LOSS [training: 3.6003109468397754 | validation: 3.0900713843267305]
	TIME [epoch: 9.68 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600058856709185		[learning rate: 1.623e-05]
	Learning Rate: 1.62299e-05
	LOSS [training: 3.600058856709185 | validation: 3.0841424976227474]
	TIME [epoch: 9.68 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60008492512858		[learning rate: 1.6171e-05]
	Learning Rate: 1.6171e-05
	LOSS [training: 3.60008492512858 | validation: 3.093919019808352]
	TIME [epoch: 9.71 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5981788956638274		[learning rate: 1.6112e-05]
	Learning Rate: 1.61123e-05
	LOSS [training: 3.5981788956638274 | validation: 3.0828421767206]
	TIME [epoch: 9.69 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599271819839528		[learning rate: 1.6054e-05]
	Learning Rate: 1.60538e-05
	LOSS [training: 3.599271819839528 | validation: 3.0873060191070762]
	TIME [epoch: 9.68 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5950328390288773		[learning rate: 1.5996e-05]
	Learning Rate: 1.59956e-05
	LOSS [training: 3.5950328390288773 | validation: 3.082048728567283]
	TIME [epoch: 9.69 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595531191370412		[learning rate: 1.5938e-05]
	Learning Rate: 1.59375e-05
	LOSS [training: 3.595531191370412 | validation: 3.0852584884824052]
	TIME [epoch: 9.67 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603319886178371		[learning rate: 1.588e-05]
	Learning Rate: 1.58797e-05
	LOSS [training: 3.603319886178371 | validation: 3.0980050273801574]
	TIME [epoch: 9.68 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600796196492604		[learning rate: 1.5822e-05]
	Learning Rate: 1.58221e-05
	LOSS [training: 3.600796196492604 | validation: 3.087231318800911]
	TIME [epoch: 9.69 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598262321782439		[learning rate: 1.5765e-05]
	Learning Rate: 1.57647e-05
	LOSS [training: 3.598262321782439 | validation: 3.090271184916338]
	TIME [epoch: 9.7 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601063083927656		[learning rate: 1.5707e-05]
	Learning Rate: 1.57074e-05
	LOSS [training: 3.601063083927656 | validation: 3.0836453548212375]
	TIME [epoch: 9.68 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598714900794824		[learning rate: 1.565e-05]
	Learning Rate: 1.56504e-05
	LOSS [training: 3.598714900794824 | validation: 3.0913269374626617]
	TIME [epoch: 9.67 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603601996437271		[learning rate: 1.5594e-05]
	Learning Rate: 1.55936e-05
	LOSS [training: 3.603601996437271 | validation: 3.0933797608873466]
	TIME [epoch: 9.7 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598498343483693		[learning rate: 1.5537e-05]
	Learning Rate: 1.5537e-05
	LOSS [training: 3.598498343483693 | validation: 3.082450378838313]
	TIME [epoch: 9.69 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5891789763184105		[learning rate: 1.5481e-05]
	Learning Rate: 1.54807e-05
	LOSS [training: 3.5891789763184105 | validation: 3.087777709469194]
	TIME [epoch: 9.69 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.594525483834962		[learning rate: 1.5424e-05]
	Learning Rate: 1.54245e-05
	LOSS [training: 3.594525483834962 | validation: 3.0911630926104285]
	TIME [epoch: 9.7 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5992854979498388		[learning rate: 1.5369e-05]
	Learning Rate: 1.53685e-05
	LOSS [training: 3.5992854979498388 | validation: 3.0848585294918633]
	TIME [epoch: 9.69 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599491659397482		[learning rate: 1.5313e-05]
	Learning Rate: 1.53127e-05
	LOSS [training: 3.599491659397482 | validation: 3.0855271129319632]
	TIME [epoch: 9.68 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5971580809664294		[learning rate: 1.5257e-05]
	Learning Rate: 1.52572e-05
	LOSS [training: 3.5971580809664294 | validation: 3.090216906720006]
	TIME [epoch: 9.69 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5981220873158266		[learning rate: 1.5202e-05]
	Learning Rate: 1.52018e-05
	LOSS [training: 3.5981220873158266 | validation: 3.088479395116034]
	TIME [epoch: 9.7 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5960252539092736		[learning rate: 1.5147e-05]
	Learning Rate: 1.51466e-05
	LOSS [training: 3.5960252539092736 | validation: 3.0864388252503807]
	TIME [epoch: 9.7 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5974068746105807		[learning rate: 1.5092e-05]
	Learning Rate: 1.50917e-05
	LOSS [training: 3.5974068746105807 | validation: 3.0855591442353147]
	TIME [epoch: 9.68 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599702863485484		[learning rate: 1.5037e-05]
	Learning Rate: 1.50369e-05
	LOSS [training: 3.599702863485484 | validation: 3.076995887020697]
	TIME [epoch: 9.72 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599137596097344		[learning rate: 1.4982e-05]
	Learning Rate: 1.49823e-05
	LOSS [training: 3.599137596097344 | validation: 3.084960694149572]
	TIME [epoch: 9.7 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5942759048672768		[learning rate: 1.4928e-05]
	Learning Rate: 1.49279e-05
	LOSS [training: 3.5942759048672768 | validation: 3.0844589864003176]
	TIME [epoch: 9.69 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602027669051104		[learning rate: 1.4874e-05]
	Learning Rate: 1.48738e-05
	LOSS [training: 3.602027669051104 | validation: 3.0734152507600103]
	TIME [epoch: 9.71 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5964460907489064		[learning rate: 1.482e-05]
	Learning Rate: 1.48198e-05
	LOSS [training: 3.5964460907489064 | validation: 3.088445407557726]
	TIME [epoch: 9.71 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5971315759387172		[learning rate: 1.4766e-05]
	Learning Rate: 1.4766e-05
	LOSS [training: 3.5971315759387172 | validation: 3.0945325615062855]
	TIME [epoch: 9.7 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5997004993932777		[learning rate: 1.4712e-05]
	Learning Rate: 1.47124e-05
	LOSS [training: 3.5997004993932777 | validation: 3.086669436557007]
	TIME [epoch: 9.69 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.593021191112526		[learning rate: 1.4659e-05]
	Learning Rate: 1.4659e-05
	LOSS [training: 3.593021191112526 | validation: 3.081292992856238]
	TIME [epoch: 9.71 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595450577121304		[learning rate: 1.4606e-05]
	Learning Rate: 1.46058e-05
	LOSS [training: 3.595450577121304 | validation: 3.0778605336279132]
	TIME [epoch: 9.69 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596012756806516		[learning rate: 1.4553e-05]
	Learning Rate: 1.45528e-05
	LOSS [training: 3.596012756806516 | validation: 3.0911100834850043]
	TIME [epoch: 9.69 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5996652883764453		[learning rate: 1.45e-05]
	Learning Rate: 1.45e-05
	LOSS [training: 3.5996652883764453 | validation: 3.0860592252871575]
	TIME [epoch: 9.71 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5984881089912846		[learning rate: 1.4447e-05]
	Learning Rate: 1.44474e-05
	LOSS [training: 3.5984881089912846 | validation: 3.091975116909107]
	TIME [epoch: 9.7 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.593416622443448		[learning rate: 1.4395e-05]
	Learning Rate: 1.4395e-05
	LOSS [training: 3.593416622443448 | validation: 3.0904780418072004]
	TIME [epoch: 9.7 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602163483340796		[learning rate: 1.4343e-05]
	Learning Rate: 1.43427e-05
	LOSS [training: 3.602163483340796 | validation: 3.0786578560830895]
	TIME [epoch: 9.7 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59648862891619		[learning rate: 1.4291e-05]
	Learning Rate: 1.42907e-05
	LOSS [training: 3.59648862891619 | validation: 3.0816055870440904]
	TIME [epoch: 9.71 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599462873592895		[learning rate: 1.4239e-05]
	Learning Rate: 1.42388e-05
	LOSS [training: 3.599462873592895 | validation: 3.0814460280077527]
	TIME [epoch: 9.7 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5967320647577226		[learning rate: 1.4187e-05]
	Learning Rate: 1.41871e-05
	LOSS [training: 3.5967320647577226 | validation: 3.0825461497804985]
	TIME [epoch: 9.69 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5949176006070864		[learning rate: 1.4136e-05]
	Learning Rate: 1.41357e-05
	LOSS [training: 3.5949176006070864 | validation: 3.0758250901712816]
	TIME [epoch: 9.71 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5939372792677524		[learning rate: 1.4084e-05]
	Learning Rate: 1.40844e-05
	LOSS [training: 3.5939372792677524 | validation: 3.084008921760013]
	TIME [epoch: 9.7 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597099951564548		[learning rate: 1.4033e-05]
	Learning Rate: 1.40332e-05
	LOSS [training: 3.597099951564548 | validation: 3.081297715946777]
	TIME [epoch: 9.71 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5965834976623334		[learning rate: 1.3982e-05]
	Learning Rate: 1.39823e-05
	LOSS [training: 3.5965834976623334 | validation: 3.0942760776749036]
	TIME [epoch: 9.7 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599086092066411		[learning rate: 1.3932e-05]
	Learning Rate: 1.39316e-05
	LOSS [training: 3.599086092066411 | validation: 3.0908799847694937]
	TIME [epoch: 9.71 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598141942204318		[learning rate: 1.3881e-05]
	Learning Rate: 1.3881e-05
	LOSS [training: 3.598141942204318 | validation: 3.0790794954994944]
	TIME [epoch: 9.7 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5991376649084095		[learning rate: 1.3831e-05]
	Learning Rate: 1.38306e-05
	LOSS [training: 3.5991376649084095 | validation: 3.093978371326162]
	TIME [epoch: 9.7 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5960276304946888		[learning rate: 1.378e-05]
	Learning Rate: 1.37804e-05
	LOSS [training: 3.5960276304946888 | validation: 3.083684728055404]
	TIME [epoch: 9.71 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5961369334682614		[learning rate: 1.373e-05]
	Learning Rate: 1.37304e-05
	LOSS [training: 3.5961369334682614 | validation: 3.0965274497705857]
	TIME [epoch: 9.69 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5975627378626895		[learning rate: 1.3681e-05]
	Learning Rate: 1.36806e-05
	LOSS [training: 3.5975627378626895 | validation: 3.0895657959274554]
	TIME [epoch: 9.7 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5953692194443554		[learning rate: 1.3631e-05]
	Learning Rate: 1.3631e-05
	LOSS [training: 3.5953692194443554 | validation: 3.094358413042841]
	TIME [epoch: 9.7 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5956552297133926		[learning rate: 1.3581e-05]
	Learning Rate: 1.35815e-05
	LOSS [training: 3.5956552297133926 | validation: 3.0779800951305747]
	TIME [epoch: 9.7 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599842362111559		[learning rate: 1.3532e-05]
	Learning Rate: 1.35322e-05
	LOSS [training: 3.599842362111559 | validation: 3.084495986128351]
	TIME [epoch: 9.69 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5964627706998717		[learning rate: 1.3483e-05]
	Learning Rate: 1.34831e-05
	LOSS [training: 3.5964627706998717 | validation: 3.078936134091216]
	TIME [epoch: 9.7 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5960180792655327		[learning rate: 1.3434e-05]
	Learning Rate: 1.34342e-05
	LOSS [training: 3.5960180792655327 | validation: 3.0878021682384986]
	TIME [epoch: 9.71 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600054823337865		[learning rate: 1.3385e-05]
	Learning Rate: 1.33854e-05
	LOSS [training: 3.600054823337865 | validation: 3.0870307046670087]
	TIME [epoch: 9.69 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596423629022999		[learning rate: 1.3337e-05]
	Learning Rate: 1.33368e-05
	LOSS [training: 3.596423629022999 | validation: 3.0744789806570645]
	TIME [epoch: 9.69 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600320529074628		[learning rate: 1.3288e-05]
	Learning Rate: 1.32884e-05
	LOSS [training: 3.600320529074628 | validation: 3.0826846355586257]
	TIME [epoch: 9.72 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.605013514658765		[learning rate: 1.324e-05]
	Learning Rate: 1.32402e-05
	LOSS [training: 3.605013514658765 | validation: 3.073769685807355]
	TIME [epoch: 9.69 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5975275262907855		[learning rate: 1.3192e-05]
	Learning Rate: 1.31922e-05
	LOSS [training: 3.5975275262907855 | validation: 3.086431010167569]
	TIME [epoch: 9.68 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5952096345600233		[learning rate: 1.3144e-05]
	Learning Rate: 1.31443e-05
	LOSS [training: 3.5952096345600233 | validation: 3.086570606444626]
	TIME [epoch: 9.69 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5969023218797367		[learning rate: 1.3097e-05]
	Learning Rate: 1.30966e-05
	LOSS [training: 3.5969023218797367 | validation: 3.070579972626208]
	TIME [epoch: 9.7 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5973201185847805		[learning rate: 1.3049e-05]
	Learning Rate: 1.30491e-05
	LOSS [training: 3.5973201185847805 | validation: 3.077692822431036]
	TIME [epoch: 9.68 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596017353560197		[learning rate: 1.3002e-05]
	Learning Rate: 1.30017e-05
	LOSS [training: 3.596017353560197 | validation: 3.0819347317804646]
	TIME [epoch: 9.68 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599765263468292		[learning rate: 1.2955e-05]
	Learning Rate: 1.29545e-05
	LOSS [training: 3.599765263468292 | validation: 3.082201324136646]
	TIME [epoch: 9.71 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6000419922853943		[learning rate: 1.2907e-05]
	Learning Rate: 1.29075e-05
	LOSS [training: 3.6000419922853943 | validation: 3.081781070432885]
	TIME [epoch: 9.69 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5920581680808157		[learning rate: 1.2861e-05]
	Learning Rate: 1.28607e-05
	LOSS [training: 3.5920581680808157 | validation: 3.0894269031687007]
	TIME [epoch: 9.66 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6006170690379378		[learning rate: 1.2814e-05]
	Learning Rate: 1.2814e-05
	LOSS [training: 3.6006170690379378 | validation: 3.094298699743068]
	TIME [epoch: 9.68 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.593523573811292		[learning rate: 1.2767e-05]
	Learning Rate: 1.27675e-05
	LOSS [training: 3.593523573811292 | validation: 3.088297849583696]
	TIME [epoch: 9.7 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5972775456108175		[learning rate: 1.2721e-05]
	Learning Rate: 1.27212e-05
	LOSS [training: 3.5972775456108175 | validation: 3.088854994062245]
	TIME [epoch: 9.67 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6026445480931186		[learning rate: 1.2675e-05]
	Learning Rate: 1.2675e-05
	LOSS [training: 3.6026445480931186 | validation: 3.079382825981317]
	TIME [epoch: 9.68 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5925672813439915		[learning rate: 1.2629e-05]
	Learning Rate: 1.2629e-05
	LOSS [training: 3.5925672813439915 | validation: 3.093752088516893]
	TIME [epoch: 9.71 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5930028648752255		[learning rate: 1.2583e-05]
	Learning Rate: 1.25832e-05
	LOSS [training: 3.5930028648752255 | validation: 3.0864090387591463]
	TIME [epoch: 9.69 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599388088386381		[learning rate: 1.2537e-05]
	Learning Rate: 1.25375e-05
	LOSS [training: 3.599388088386381 | validation: 3.0846492304599624]
	TIME [epoch: 9.68 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59010840914834		[learning rate: 1.2492e-05]
	Learning Rate: 1.2492e-05
	LOSS [training: 3.59010840914834 | validation: 3.0792436966564094]
	TIME [epoch: 9.7 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5987968424188246		[learning rate: 1.2447e-05]
	Learning Rate: 1.24467e-05
	LOSS [training: 3.5987968424188246 | validation: 3.089884888252989]
	TIME [epoch: 9.68 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6002682885904704		[learning rate: 1.2401e-05]
	Learning Rate: 1.24015e-05
	LOSS [training: 3.6002682885904704 | validation: 3.091639957034476]
	TIME [epoch: 9.67 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5938456799768828		[learning rate: 1.2356e-05]
	Learning Rate: 1.23565e-05
	LOSS [training: 3.5938456799768828 | validation: 3.0890222989312917]
	TIME [epoch: 9.68 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5988966802302116		[learning rate: 1.2312e-05]
	Learning Rate: 1.23116e-05
	LOSS [training: 3.5988966802302116 | validation: 3.081627421375821]
	TIME [epoch: 9.7 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597354355337695		[learning rate: 1.2267e-05]
	Learning Rate: 1.2267e-05
	LOSS [training: 3.597354355337695 | validation: 3.0917605800396895]
	TIME [epoch: 9.69 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597695862470496		[learning rate: 1.2222e-05]
	Learning Rate: 1.22224e-05
	LOSS [training: 3.597695862470496 | validation: 3.077719332021687]
	TIME [epoch: 9.68 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5981120094203876		[learning rate: 1.2178e-05]
	Learning Rate: 1.21781e-05
	LOSS [training: 3.5981120094203876 | validation: 3.0828470919971487]
	TIME [epoch: 9.69 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5966182959073136		[learning rate: 1.2134e-05]
	Learning Rate: 1.21339e-05
	LOSS [training: 3.5966182959073136 | validation: 3.085022215526984]
	TIME [epoch: 9.68 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599088466183298		[learning rate: 1.209e-05]
	Learning Rate: 1.20899e-05
	LOSS [training: 3.599088466183298 | validation: 3.0848262010583474]
	TIME [epoch: 9.67 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600432177157837		[learning rate: 1.2046e-05]
	Learning Rate: 1.2046e-05
	LOSS [training: 3.600432177157837 | validation: 3.0858645084351277]
	TIME [epoch: 9.67 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59983305964858		[learning rate: 1.2002e-05]
	Learning Rate: 1.20023e-05
	LOSS [training: 3.59983305964858 | validation: 3.0837865727429654]
	TIME [epoch: 9.68 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5996152663040077		[learning rate: 1.1959e-05]
	Learning Rate: 1.19587e-05
	LOSS [training: 3.5996152663040077 | validation: 3.086276284710455]
	TIME [epoch: 9.68 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597337044145609		[learning rate: 1.1915e-05]
	Learning Rate: 1.19153e-05
	LOSS [training: 3.597337044145609 | validation: 3.0787493339392498]
	TIME [epoch: 9.67 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597187988624649		[learning rate: 1.1872e-05]
	Learning Rate: 1.18721e-05
	LOSS [training: 3.597187988624649 | validation: 3.0922719763798128]
	TIME [epoch: 9.71 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.606012652296627		[learning rate: 1.1829e-05]
	Learning Rate: 1.1829e-05
	LOSS [training: 3.606012652296627 | validation: 3.0891571071709008]
	TIME [epoch: 9.68 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5998119094987935		[learning rate: 1.1786e-05]
	Learning Rate: 1.17861e-05
	LOSS [training: 3.5998119094987935 | validation: 3.0875545795904906]
	TIME [epoch: 9.66 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.594823480427027		[learning rate: 1.1743e-05]
	Learning Rate: 1.17433e-05
	LOSS [training: 3.594823480427027 | validation: 3.084748062817776]
	TIME [epoch: 9.7 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59478124525274		[learning rate: 1.1701e-05]
	Learning Rate: 1.17007e-05
	LOSS [training: 3.59478124525274 | validation: 3.087049543720175]
	TIME [epoch: 9.69 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5950627169593226		[learning rate: 1.1658e-05]
	Learning Rate: 1.16582e-05
	LOSS [training: 3.5950627169593226 | validation: 3.084628448703047]
	TIME [epoch: 9.68 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5950369744806494		[learning rate: 1.1616e-05]
	Learning Rate: 1.16159e-05
	LOSS [training: 3.5950369744806494 | validation: 3.0964114736186232]
	TIME [epoch: 9.68 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5992049391615994		[learning rate: 1.1574e-05]
	Learning Rate: 1.15737e-05
	LOSS [training: 3.5992049391615994 | validation: 3.086676596068604]
	TIME [epoch: 9.71 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5998687425475304		[learning rate: 1.1532e-05]
	Learning Rate: 1.15317e-05
	LOSS [training: 3.5998687425475304 | validation: 3.0800452140383103]
	TIME [epoch: 9.69 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5984021873264114		[learning rate: 1.149e-05]
	Learning Rate: 1.14899e-05
	LOSS [training: 3.5984021873264114 | validation: 3.084897668236231]
	TIME [epoch: 9.69 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5984085781132684		[learning rate: 1.1448e-05]
	Learning Rate: 1.14482e-05
	LOSS [training: 3.5984085781132684 | validation: 3.087155322854475]
	TIME [epoch: 9.71 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598942099673266		[learning rate: 1.1407e-05]
	Learning Rate: 1.14066e-05
	LOSS [training: 3.598942099673266 | validation: 3.0834890272660456]
	TIME [epoch: 9.69 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5958802589933456		[learning rate: 1.1365e-05]
	Learning Rate: 1.13652e-05
	LOSS [training: 3.5958802589933456 | validation: 3.084588555704925]
	TIME [epoch: 9.68 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.594070395453167		[learning rate: 1.1324e-05]
	Learning Rate: 1.1324e-05
	LOSS [training: 3.594070395453167 | validation: 3.088217053466969]
	TIME [epoch: 9.68 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597009442272705		[learning rate: 1.1283e-05]
	Learning Rate: 1.12829e-05
	LOSS [training: 3.597009442272705 | validation: 3.0917002114804815]
	TIME [epoch: 9.69 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595014823063208		[learning rate: 1.1242e-05]
	Learning Rate: 1.1242e-05
	LOSS [training: 3.595014823063208 | validation: 3.0786172575263406]
	TIME [epoch: 9.68 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597153395798918		[learning rate: 1.1201e-05]
	Learning Rate: 1.12012e-05
	LOSS [training: 3.597153395798918 | validation: 3.0743168474645337]
	TIME [epoch: 9.68 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5991889723270347		[learning rate: 1.1161e-05]
	Learning Rate: 1.11605e-05
	LOSS [training: 3.5991889723270347 | validation: 3.0856546443819086]
	TIME [epoch: 9.7 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599917788771819		[learning rate: 1.112e-05]
	Learning Rate: 1.112e-05
	LOSS [training: 3.599917788771819 | validation: 3.0772557209923197]
	TIME [epoch: 9.67 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602339189026665		[learning rate: 1.108e-05]
	Learning Rate: 1.10797e-05
	LOSS [training: 3.602339189026665 | validation: 3.0868561384735287]
	TIME [epoch: 9.66 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5938282927752683		[learning rate: 1.1039e-05]
	Learning Rate: 1.10394e-05
	LOSS [training: 3.5938282927752683 | validation: 3.086305055131343]
	TIME [epoch: 9.69 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5976397382277083		[learning rate: 1.0999e-05]
	Learning Rate: 1.09994e-05
	LOSS [training: 3.5976397382277083 | validation: 3.0880827354485536]
	TIME [epoch: 9.7 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601199157368343		[learning rate: 1.0959e-05]
	Learning Rate: 1.09595e-05
	LOSS [training: 3.601199157368343 | validation: 3.0784114324124854]
	TIME [epoch: 9.67 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5968864586948293		[learning rate: 1.092e-05]
	Learning Rate: 1.09197e-05
	LOSS [training: 3.5968864586948293 | validation: 3.081145995664582]
	TIME [epoch: 9.68 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5958822418781886		[learning rate: 1.088e-05]
	Learning Rate: 1.08801e-05
	LOSS [training: 3.5958822418781886 | validation: 3.082879804507823]
	TIME [epoch: 9.71 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5974789723986382		[learning rate: 1.0841e-05]
	Learning Rate: 1.08406e-05
	LOSS [training: 3.5974789723986382 | validation: 3.093140440175356]
	TIME [epoch: 9.67 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5984806586280866		[learning rate: 1.0801e-05]
	Learning Rate: 1.08012e-05
	LOSS [training: 3.5984806586280866 | validation: 3.0959966716137455]
	TIME [epoch: 9.67 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5961735823451897		[learning rate: 1.0762e-05]
	Learning Rate: 1.0762e-05
	LOSS [training: 3.5961735823451897 | validation: 3.079014117050856]
	TIME [epoch: 9.7 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596705206510466		[learning rate: 1.0723e-05]
	Learning Rate: 1.0723e-05
	LOSS [training: 3.596705206510466 | validation: 3.0793972854215426]
	TIME [epoch: 9.69 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5921324589724177		[learning rate: 1.0684e-05]
	Learning Rate: 1.06841e-05
	LOSS [training: 3.5921324589724177 | validation: 3.0990796186255953]
	TIME [epoch: 9.67 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59862257875844		[learning rate: 1.0645e-05]
	Learning Rate: 1.06453e-05
	LOSS [training: 3.59862257875844 | validation: 3.0921220404764713]
	TIME [epoch: 9.68 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595976600276701		[learning rate: 1.0607e-05]
	Learning Rate: 1.06067e-05
	LOSS [training: 3.595976600276701 | validation: 3.093278264953858]
	TIME [epoch: 9.69 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6006423769461073		[learning rate: 1.0568e-05]
	Learning Rate: 1.05682e-05
	LOSS [training: 3.6006423769461073 | validation: 3.084926216637296]
	TIME [epoch: 9.68 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602115416031226		[learning rate: 1.053e-05]
	Learning Rate: 1.05298e-05
	LOSS [training: 3.602115416031226 | validation: 3.0879381860294544]
	TIME [epoch: 9.68 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5942068905978717		[learning rate: 1.0492e-05]
	Learning Rate: 1.04916e-05
	LOSS [training: 3.5942068905978717 | validation: 3.0820014484614457]
	TIME [epoch: 9.7 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5926022206797463		[learning rate: 1.0454e-05]
	Learning Rate: 1.04535e-05
	LOSS [training: 3.5926022206797463 | validation: 3.082426502104143]
	TIME [epoch: 9.68 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5944304418940476		[learning rate: 1.0416e-05]
	Learning Rate: 1.04156e-05
	LOSS [training: 3.5944304418940476 | validation: 3.0816119003154347]
	TIME [epoch: 9.68 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6005266871179		[learning rate: 1.0378e-05]
	Learning Rate: 1.03778e-05
	LOSS [training: 3.6005266871179 | validation: 3.08338030294864]
	TIME [epoch: 9.7 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599115469347889		[learning rate: 1.034e-05]
	Learning Rate: 1.03401e-05
	LOSS [training: 3.599115469347889 | validation: 3.081894068112605]
	TIME [epoch: 9.69 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.594893419768865		[learning rate: 1.0303e-05]
	Learning Rate: 1.03026e-05
	LOSS [training: 3.594893419768865 | validation: 3.0954401149286777]
	TIME [epoch: 9.67 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5970889170068836		[learning rate: 1.0265e-05]
	Learning Rate: 1.02652e-05
	LOSS [training: 3.5970889170068836 | validation: 3.09603848075453]
	TIME [epoch: 9.68 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5988702176715903		[learning rate: 1.0228e-05]
	Learning Rate: 1.0228e-05
	LOSS [training: 3.5988702176715903 | validation: 3.092044836506259]
	TIME [epoch: 9.71 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5956687795399462		[learning rate: 1.0191e-05]
	Learning Rate: 1.01909e-05
	LOSS [training: 3.5956687795399462 | validation: 3.084482658918146]
	TIME [epoch: 9.68 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6003851889163223		[learning rate: 1.0154e-05]
	Learning Rate: 1.01539e-05
	LOSS [training: 3.6003851889163223 | validation: 3.0844726220699092]
	TIME [epoch: 9.67 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5996258008233766		[learning rate: 1.0117e-05]
	Learning Rate: 1.0117e-05
	LOSS [training: 3.5996258008233766 | validation: 3.0841734649743144]
	TIME [epoch: 9.7 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596191073541251		[learning rate: 1.008e-05]
	Learning Rate: 1.00803e-05
	LOSS [training: 3.596191073541251 | validation: 3.08455362676557]
	TIME [epoch: 9.68 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5946756973177036		[learning rate: 1.0044e-05]
	Learning Rate: 1.00437e-05
	LOSS [training: 3.5946756973177036 | validation: 3.0902901673326197]
	TIME [epoch: 9.67 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5972267626202252		[learning rate: 1.0007e-05]
	Learning Rate: 1.00073e-05
	LOSS [training: 3.5972267626202252 | validation: 3.089647275332031]
	TIME [epoch: 9.67 sec]
Finished training in 19537.603 seconds.
