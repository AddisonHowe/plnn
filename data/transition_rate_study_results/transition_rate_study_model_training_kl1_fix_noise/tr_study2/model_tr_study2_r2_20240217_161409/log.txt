Args:
Namespace(name='model_tr_study2', outdir='out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2', training_data='data/transition_rate_studies/tr_study2/tr_study2_training/r2', validation_data='data/transition_rate_studies/tr_study2/tr_study2_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.1, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 569351578

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 10/20] avg loss: 8.410998431687654		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.285271715627597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.848135073657625 | validation: 7.618463823347102]
	TIME [epoch: 78.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.638129683261466		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.275688625163888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.956909154212677 | validation: 5.299044643155277]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_2.pth
	Model improved!!!
EPOCH 3/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.111365010432109		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8072412629602708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.95930313669619 | validation: 4.083442392929067]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_3.pth
	Model improved!!!
EPOCH 4/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.4511089483995754		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.224102236692546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3376055925460597 | validation: 3.8306175755179934]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_4.pth
	Model improved!!!
EPOCH 5/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.8721686765127905		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1774654972495835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5248170868811872 | validation: 1.0797318135774394]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_5.pth
	Model improved!!!
EPOCH 6/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5091531556391575		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3580969188480405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.433625037243599 | validation: 1.29724223699734]
	TIME [epoch: 8.15 sec]
EPOCH 7/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3691995323963488		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1377234851793854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.253461508787867 | validation: 1.2307927992095657]
	TIME [epoch: 8.14 sec]
EPOCH 8/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2147137053646204		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0937490870760143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1542313962203168 | validation: 0.7042180742837465]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_8.pth
	Model improved!!!
EPOCH 9/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0882275638914114		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.052430449308083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0703290065997473 | validation: 1.0129357584280554]
	TIME [epoch: 8.17 sec]
EPOCH 10/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.308136930606554		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2686629193920824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7883999249993177 | validation: 0.7012712491588924]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_10.pth
	Model improved!!!
EPOCH 11/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9688131603021534		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.028873247762671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9988432040324122 | validation: 0.7525373934363615]
	TIME [epoch: 8.14 sec]
EPOCH 12/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.853888668563742		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8775946165633302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8657416425635361 | validation: 1.806725917564965]
	TIME [epoch: 8.15 sec]
EPOCH 13/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0728900460389295		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7599189386263691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9164044923326491 | validation: 1.2037935821388634]
	TIME [epoch: 8.15 sec]
EPOCH 14/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9836534518896387		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7077404063828877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8456969291362633 | validation: 0.8568591375984609]
	TIME [epoch: 8.14 sec]
EPOCH 15/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8344756937532498		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2730686130976472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0537721534254485 | validation: 0.5691589193563826]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_15.pth
	Model improved!!!
EPOCH 16/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7152601040042749		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8408367273112557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7780484156577654 | validation: 0.9006725050445464]
	TIME [epoch: 8.15 sec]
EPOCH 17/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6976270947907194		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9448654139298099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8212462543602648 | validation: 0.8746955488361546]
	TIME [epoch: 8.16 sec]
EPOCH 18/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6750849125585009		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6980920846513831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.686588498604942 | validation: 0.5638127498843181]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_18.pth
	Model improved!!!
EPOCH 19/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.074739020511856		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6430801838480747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8589096021799655 | validation: 0.8167230972029809]
	TIME [epoch: 8.14 sec]
EPOCH 20/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7242780555432425		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7078862118290594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.716082133686151 | validation: 0.6114668242729764]
	TIME [epoch: 8.15 sec]
EPOCH 21/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6761074359487883		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9700964524858326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8231019442173105 | validation: 0.649353415131946]
	TIME [epoch: 8.15 sec]
EPOCH 22/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6047990579240177		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7057390072388443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.655269032581431 | validation: 0.5057681549498051]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_22.pth
	Model improved!!!
EPOCH 23/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5899640203238213		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7317761881354797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6608701042296504 | validation: 0.8082770744574163]
	TIME [epoch: 8.13 sec]
EPOCH 24/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7059556185481413		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6482379113592114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6770967649536763 | validation: 1.030589631788048]
	TIME [epoch: 8.13 sec]
EPOCH 25/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6550364940392733		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.664622951186454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6598297226128637 | validation: 1.0944849359077953]
	TIME [epoch: 8.16 sec]
EPOCH 26/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6920017881174223		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6465062240662535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6692540060918379 | validation: 0.7659294894287214]
	TIME [epoch: 8.13 sec]
EPOCH 27/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7649441478854503		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6775167487161303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7212304483007903 | validation: 0.6967297672566446]
	TIME [epoch: 8.13 sec]
EPOCH 28/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6951790095526768		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6219196008373853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6585493051950311 | validation: 0.7685822845047268]
	TIME [epoch: 8.13 sec]
EPOCH 29/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6655737078893782		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6442930741036523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6549333909965154 | validation: 0.5746583863976517]
	TIME [epoch: 8.15 sec]
EPOCH 30/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6793378430508004		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6261278665785461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6527328548146732 | validation: 0.9496519099125736]
	TIME [epoch: 8.12 sec]
EPOCH 31/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6901612788855054		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6643685441828191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6772649115341622 | validation: 0.6940292126889901]
	TIME [epoch: 8.13 sec]
EPOCH 32/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5901498908475873		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6905168300386882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6403333604431378 | validation: 1.3453878545592528]
	TIME [epoch: 8.13 sec]
EPOCH 33/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7073202718549645		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5750657878010222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6411930298279933 | validation: 0.504825235165465]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_33.pth
	Model improved!!!
EPOCH 34/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6537518756210271		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5440925326844013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.598922204152714 | validation: 0.7179223447933953]
	TIME [epoch: 8.13 sec]
EPOCH 35/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6118143906338385		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7204114589844485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6661129248091434 | validation: 0.48296237244154683]
	TIME [epoch: 8.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_35.pth
	Model improved!!!
EPOCH 36/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.544248597238905		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5527758368053058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5485122170221054 | validation: 0.8049264598371162]
	TIME [epoch: 8.13 sec]
EPOCH 37/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6058621775589981		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5874980487435166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5966801131512574 | validation: 0.42073891751198556]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_37.pth
	Model improved!!!
EPOCH 38/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6869677994652683		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5651681191194444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6260679592923564 | validation: 0.45720591617365347]
	TIME [epoch: 8.14 sec]
EPOCH 39/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8522182323346745		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8694496450377092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8608339386861917 | validation: 1.4338801229294524]
	TIME [epoch: 8.13 sec]
EPOCH 40/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6150463056743405		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6572316554145097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6361389805444253 | validation: 0.6716202513437668]
	TIME [epoch: 8.13 sec]
EPOCH 41/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6450069865612507		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.628428582292466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6367177844268582 | validation: 0.5097884511696978]
	TIME [epoch: 8.45 sec]
EPOCH 42/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5387680552567705		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.675259284840423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6070136700485967 | validation: 0.47766474378240104]
	TIME [epoch: 8.15 sec]
EPOCH 43/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6205036394841684		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6092741163051901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.614888877894679 | validation: 0.6848496160045081]
	TIME [epoch: 8.15 sec]
EPOCH 44/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5830119395448773		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5958325138441231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5894222266945002 | validation: 0.5577618913205152]
	TIME [epoch: 8.15 sec]
EPOCH 45/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5507206778651992		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6952520750988034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6229863764820014 | validation: 0.6924655465067686]
	TIME [epoch: 8.18 sec]
EPOCH 46/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6340720042804712		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5541009129832318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5940864586318515 | validation: 0.5729420509150647]
	TIME [epoch: 8.15 sec]
EPOCH 47/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6149833566283719		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5631160364644116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5890496965463916 | validation: 0.5989463725794497]
	TIME [epoch: 8.15 sec]
EPOCH 48/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6342730064668375		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6810134656559436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6576432360613905 | validation: 0.511328732480457]
	TIME [epoch: 8.15 sec]
EPOCH 49/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5082187554938422		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5810118755983108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5446153155460765 | validation: 0.6245076694467554]
	TIME [epoch: 8.17 sec]
EPOCH 50/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6297779694756483		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5433405962029292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5865592828392886 | validation: 0.5689484671083721]
	TIME [epoch: 8.15 sec]
EPOCH 51/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5077718207094631		[learning rate: 0.0099782]
		[batch 20/20] avg loss: 0.5200888106212643		[learning rate: 0.0099541]
	Learning Rate: 0.00995405
	LOSS [training: 0.5139303156653636 | validation: 0.4413930317566849]
	TIME [epoch: 8.15 sec]
EPOCH 52/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.577036794866408		[learning rate: 0.00993]
		[batch 20/20] avg loss: 0.5310452636106454		[learning rate: 0.0099059]
	Learning Rate: 0.00990592
	LOSS [training: 0.5540410292385267 | validation: 1.185861091764093]
	TIME [epoch: 8.15 sec]
EPOCH 53/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6788121358852865		[learning rate: 0.0098819]
		[batch 20/20] avg loss: 0.524713614649068		[learning rate: 0.009858]
	Learning Rate: 0.00985801
	LOSS [training: 0.6017628752671772 | validation: 0.42561272568974456]
	TIME [epoch: 8.18 sec]
EPOCH 54/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.499110075812592		[learning rate: 0.0098341]
		[batch 20/20] avg loss: 0.5297219168693517		[learning rate: 0.0098103]
	Learning Rate: 0.00981034
	LOSS [training: 0.5144159963409719 | validation: 0.5117817185523384]
	TIME [epoch: 8.15 sec]
EPOCH 55/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5216846839567836		[learning rate: 0.0097866]
		[batch 20/20] avg loss: 0.4938221267481084		[learning rate: 0.0097629]
	Learning Rate: 0.0097629
	LOSS [training: 0.5077534053524461 | validation: 0.3988086813537987]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_55.pth
	Model improved!!!
EPOCH 56/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9248638687488828		[learning rate: 0.0097393]
		[batch 20/20] avg loss: 0.6043993142272279		[learning rate: 0.0097157]
	Learning Rate: 0.00971569
	LOSS [training: 0.7646315914880553 | validation: 0.34832763975820813]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_56.pth
	Model improved!!!
EPOCH 57/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5384076562972957		[learning rate: 0.0096922]
		[batch 20/20] avg loss: 0.5399537646075899		[learning rate: 0.0096687]
	Learning Rate: 0.00966871
	LOSS [training: 0.5391807104524429 | validation: 0.4640351429581573]
	TIME [epoch: 8.17 sec]
EPOCH 58/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5402390435248295		[learning rate: 0.0096453]
		[batch 20/20] avg loss: 0.5007175908278304		[learning rate: 0.009622]
	Learning Rate: 0.00962195
	LOSS [training: 0.5204783171763299 | validation: 0.5607864952713558]
	TIME [epoch: 8.15 sec]
EPOCH 59/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.776066224904883		[learning rate: 0.0095987]
		[batch 20/20] avg loss: 0.49368073253189715		[learning rate: 0.0095754]
	Learning Rate: 0.00957542
	LOSS [training: 0.6348734787183901 | validation: 0.630980127990334]
	TIME [epoch: 8.13 sec]
EPOCH 60/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5091578050500297		[learning rate: 0.0095522]
		[batch 20/20] avg loss: 0.6013693801415207		[learning rate: 0.0095291]
	Learning Rate: 0.00952912
	LOSS [training: 0.555263592595775 | validation: 0.6607555596081567]
	TIME [epoch: 8.14 sec]
EPOCH 61/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.469881986941842		[learning rate: 0.009506]
		[batch 20/20] avg loss: 0.48727836904145716		[learning rate: 0.009483]
	Learning Rate: 0.00948303
	LOSS [training: 0.4785801779916497 | validation: 0.5675526659985729]
	TIME [epoch: 8.16 sec]
EPOCH 62/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.48487634525584467		[learning rate: 0.0094601]
		[batch 20/20] avg loss: 0.4679512652010459		[learning rate: 0.0094372]
	Learning Rate: 0.00943718
	LOSS [training: 0.47641380522844523 | validation: 0.6418381669747523]
	TIME [epoch: 8.15 sec]
EPOCH 63/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5404379871678775		[learning rate: 0.0094143]
		[batch 20/20] avg loss: 0.40908899654919717		[learning rate: 0.0093915]
	Learning Rate: 0.00939154
	LOSS [training: 0.4747634918585373 | validation: 0.4708294237812978]
	TIME [epoch: 8.14 sec]
EPOCH 64/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.47697278877524435		[learning rate: 0.0093688]
		[batch 20/20] avg loss: 0.7460426694910332		[learning rate: 0.0093461]
	Learning Rate: 0.00934612
	LOSS [training: 0.6115077291331389 | validation: 0.3974323323776334]
	TIME [epoch: 8.14 sec]
EPOCH 65/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4412744452778739		[learning rate: 0.0093235]
		[batch 20/20] avg loss: 0.46316766058376696		[learning rate: 0.0093009]
	Learning Rate: 0.00930093
	LOSS [training: 0.4522210529308205 | validation: 0.5016752286947929]
	TIME [epoch: 8.16 sec]
EPOCH 66/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.46754378964174614		[learning rate: 0.0092784]
		[batch 20/20] avg loss: 0.6115286312370747		[learning rate: 0.009256]
	Learning Rate: 0.00925595
	LOSS [training: 0.5395362104394104 | validation: 0.3778556412586166]
	TIME [epoch: 8.15 sec]
EPOCH 67/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.44109757683274164		[learning rate: 0.0092335]
		[batch 20/20] avg loss: 0.4696858575486572		[learning rate: 0.0092112]
	Learning Rate: 0.00921119
	LOSS [training: 0.4553917171906994 | validation: 0.47432795561003055]
	TIME [epoch: 8.14 sec]
EPOCH 68/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6377989245837491		[learning rate: 0.0091889]
		[batch 20/20] avg loss: 0.46706963640272486		[learning rate: 0.0091666]
	Learning Rate: 0.00916665
	LOSS [training: 0.5524342804932372 | validation: 0.5557188959938094]
	TIME [epoch: 8.14 sec]
EPOCH 69/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5043994200424423		[learning rate: 0.0091445]
		[batch 20/20] avg loss: 0.44570184258263695		[learning rate: 0.0091223]
	Learning Rate: 0.00912232
	LOSS [training: 0.47505063131253983 | validation: 0.4285193434318587]
	TIME [epoch: 8.16 sec]
EPOCH 70/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5198153858159918		[learning rate: 0.0091002]
		[batch 20/20] avg loss: 0.4347149252349147		[learning rate: 0.0090782]
	Learning Rate: 0.0090782
	LOSS [training: 0.4772651555254533 | validation: 0.3511126685735341]
	TIME [epoch: 8.15 sec]
EPOCH 71/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4438945552874246		[learning rate: 0.0090562]
		[batch 20/20] avg loss: 0.4306717782072897		[learning rate: 0.0090343]
	Learning Rate: 0.0090343
	LOSS [training: 0.4372831667473571 | validation: 0.4105250431394554]
	TIME [epoch: 8.14 sec]
EPOCH 72/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4270552591989981		[learning rate: 0.0090124]
		[batch 20/20] avg loss: 0.39863156156402846		[learning rate: 0.0089906]
	Learning Rate: 0.00899062
	LOSS [training: 0.4128434103815134 | validation: 0.3886367049859703]
	TIME [epoch: 8.14 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5043350154207614		[learning rate: 0.0089689]
		[batch 20/20] avg loss: 0.39002204073526975		[learning rate: 0.0089471]
	Learning Rate: 0.00894714
	LOSS [training: 0.4471785280780156 | validation: 0.7476000951144275]
	TIME [epoch: 8.16 sec]
EPOCH 74/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.43268955251836455		[learning rate: 0.0089255]
		[batch 20/20] avg loss: 0.3963958320273327		[learning rate: 0.0089039]
	Learning Rate: 0.00890387
	LOSS [training: 0.4145426922728485 | validation: 0.2886244811862711]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_74.pth
	Model improved!!!
EPOCH 75/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3790018886103499		[learning rate: 0.0088823]
		[batch 20/20] avg loss: 0.5335835425824418		[learning rate: 0.0088608]
	Learning Rate: 0.00886082
	LOSS [training: 0.45629271559639595 | validation: 0.35360555081819844]
	TIME [epoch: 8.14 sec]
EPOCH 76/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.48599626112210303		[learning rate: 0.0088394]
		[batch 20/20] avg loss: 0.37175576822816425		[learning rate: 0.008818]
	Learning Rate: 0.00881797
	LOSS [training: 0.4288760146751337 | validation: 0.6796791622302503]
	TIME [epoch: 8.14 sec]
EPOCH 77/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.45775250247789084		[learning rate: 0.0087966]
		[batch 20/20] avg loss: 0.41692920663827177		[learning rate: 0.0087753]
	Learning Rate: 0.00877532
	LOSS [training: 0.43734085455808136 | validation: 0.3198875407275561]
	TIME [epoch: 8.16 sec]
EPOCH 78/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36108114140248315		[learning rate: 0.0087541]
		[batch 20/20] avg loss: 0.33789600525868146		[learning rate: 0.0087329]
	Learning Rate: 0.00873289
	LOSS [training: 0.34948857333058236 | validation: 0.5044164688746281]
	TIME [epoch: 8.15 sec]
EPOCH 79/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3907409307743751		[learning rate: 0.0087117]
		[batch 20/20] avg loss: 0.3924274279779857		[learning rate: 0.0086907]
	Learning Rate: 0.00869066
	LOSS [training: 0.39158417937618034 | validation: 0.3421457063948856]
	TIME [epoch: 8.13 sec]
EPOCH 80/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.43323042210493634		[learning rate: 0.0086696]
		[batch 20/20] avg loss: 0.4275952854404676		[learning rate: 0.0086486]
	Learning Rate: 0.00864863
	LOSS [training: 0.430412853772702 | validation: 0.4027693213997335]
	TIME [epoch: 8.13 sec]
EPOCH 81/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4083815176334376		[learning rate: 0.0086277]
		[batch 20/20] avg loss: 0.35895527390371595		[learning rate: 0.0086068]
	Learning Rate: 0.00860681
	LOSS [training: 0.3836683957685768 | validation: 0.22027882601347795]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_81.pth
	Model improved!!!
EPOCH 82/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3709593639150769		[learning rate: 0.008586]
		[batch 20/20] avg loss: 0.3622258526785469		[learning rate: 0.0085652]
	Learning Rate: 0.00856519
	LOSS [training: 0.36659260829681195 | validation: 0.2924154699180064]
	TIME [epoch: 8.15 sec]
EPOCH 83/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.35878700814502346		[learning rate: 0.0085445]
		[batch 20/20] avg loss: 0.4644344124406866		[learning rate: 0.0085238]
	Learning Rate: 0.00852377
	LOSS [training: 0.41161071029285506 | validation: 0.43294987702456167]
	TIME [epoch: 8.13 sec]
EPOCH 84/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4850098898859689		[learning rate: 0.0085031]
		[batch 20/20] avg loss: 0.30099127890752675		[learning rate: 0.0084825]
	Learning Rate: 0.00848255
	LOSS [training: 0.3930005843967478 | validation: 0.33577529193508754]
	TIME [epoch: 8.14 sec]
EPOCH 85/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.375025818874984		[learning rate: 0.008462]
		[batch 20/20] avg loss: 0.4253785984093151		[learning rate: 0.0084415]
	Learning Rate: 0.00844153
	LOSS [training: 0.4002022086421495 | validation: 0.2732512798797851]
	TIME [epoch: 8.15 sec]
EPOCH 86/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5180363674263563		[learning rate: 0.0084211]
		[batch 20/20] avg loss: 0.41915532114339255		[learning rate: 0.0084007]
	Learning Rate: 0.00840071
	LOSS [training: 0.4685958442848744 | validation: 0.28817942320867346]
	TIME [epoch: 8.16 sec]
EPOCH 87/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3009988845833673		[learning rate: 0.0083804]
		[batch 20/20] avg loss: 0.41928435058941965		[learning rate: 0.0083601]
	Learning Rate: 0.00836008
	LOSS [training: 0.3601416175863935 | validation: 0.49337226465821116]
	TIME [epoch: 8.14 sec]
EPOCH 88/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.32260941150668987		[learning rate: 0.0083398]
		[batch 20/20] avg loss: 0.3827460975871367		[learning rate: 0.0083197]
	Learning Rate: 0.00831965
	LOSS [training: 0.3526777545469133 | validation: 0.2741363209642709]
	TIME [epoch: 8.14 sec]
EPOCH 89/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30727901314638795		[learning rate: 0.0082995]
		[batch 20/20] avg loss: 0.3361073192867968		[learning rate: 0.0082794]
	Learning Rate: 0.00827942
	LOSS [training: 0.3216931662165924 | validation: 0.3900356638812488]
	TIME [epoch: 8.15 sec]
EPOCH 90/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4152039441987723		[learning rate: 0.0082594]
		[batch 20/20] avg loss: 0.3391188802830668		[learning rate: 0.0082394]
	Learning Rate: 0.00823938
	LOSS [training: 0.37716141224091954 | validation: 0.47433715650683794]
	TIME [epoch: 8.15 sec]
EPOCH 91/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36674698972061837		[learning rate: 0.0082194]
		[batch 20/20] avg loss: 0.43063210453745454		[learning rate: 0.0081995]
	Learning Rate: 0.00819954
	LOSS [training: 0.39868954712903654 | validation: 0.3482997732177455]
	TIME [epoch: 8.13 sec]
EPOCH 92/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.40302894536210393		[learning rate: 0.0081797]
		[batch 20/20] avg loss: 0.334604278484621		[learning rate: 0.0081599]
	Learning Rate: 0.00815989
	LOSS [training: 0.3688166119233624 | validation: 0.42961640753191116]
	TIME [epoch: 8.13 sec]
EPOCH 93/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2747000954257909		[learning rate: 0.0081401]
		[batch 20/20] avg loss: 0.4222421462680421		[learning rate: 0.0081204]
	Learning Rate: 0.00812043
	LOSS [training: 0.34847112084691656 | validation: 0.37440829306606227]
	TIME [epoch: 8.14 sec]
EPOCH 94/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28983991179168844		[learning rate: 0.0081008]
		[batch 20/20] avg loss: 0.403324730261711		[learning rate: 0.0080812]
	Learning Rate: 0.00808116
	LOSS [training: 0.3465823210266997 | validation: 0.3240015450130729]
	TIME [epoch: 8.16 sec]
EPOCH 95/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3608644247788699		[learning rate: 0.0080616]
		[batch 20/20] avg loss: 0.3658251862640379		[learning rate: 0.0080421]
	Learning Rate: 0.00804208
	LOSS [training: 0.3633448055214539 | validation: 0.18836755750008302]
	TIME [epoch: 8.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_95.pth
	Model improved!!!
EPOCH 96/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30620457375264654		[learning rate: 0.0080226]
		[batch 20/20] avg loss: 0.27546718803337245		[learning rate: 0.0080032]
	Learning Rate: 0.00800319
	LOSS [training: 0.29083588089300955 | validation: 0.5388349566511764]
	TIME [epoch: 8.14 sec]
EPOCH 97/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3933277893293012		[learning rate: 0.0079838]
		[batch 20/20] avg loss: 0.28624771506505403		[learning rate: 0.0079645]
	Learning Rate: 0.00796449
	LOSS [training: 0.3397877521971776 | validation: 0.2157617885690587]
	TIME [epoch: 8.14 sec]
EPOCH 98/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3650407769109961		[learning rate: 0.0079452]
		[batch 20/20] avg loss: 0.318882945515706		[learning rate: 0.007926]
	Learning Rate: 0.00792597
	LOSS [training: 0.3419618612133511 | validation: 0.21008611581423497]
	TIME [epoch: 8.16 sec]
EPOCH 99/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.396278811407958		[learning rate: 0.0079068]
		[batch 20/20] avg loss: 0.33600414860469197		[learning rate: 0.0078876]
	Learning Rate: 0.00788765
	LOSS [training: 0.366141480006325 | validation: 0.23407998765751603]
	TIME [epoch: 8.14 sec]
EPOCH 100/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3315525750873982		[learning rate: 0.0078685]
		[batch 20/20] avg loss: 0.26909587336705987		[learning rate: 0.0078495]
	Learning Rate: 0.0078495
	LOSS [training: 0.30032422422722904 | validation: 0.2989169494471708]
	TIME [epoch: 8.14 sec]
EPOCH 101/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3400965043227884		[learning rate: 0.0078305]
		[batch 20/20] avg loss: 0.3547519265330898		[learning rate: 0.0078115]
	Learning Rate: 0.00781154
	LOSS [training: 0.34742421542793916 | validation: 0.48807005285383365]
	TIME [epoch: 8.14 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2691113072087464		[learning rate: 0.0077926]
		[batch 20/20] avg loss: 0.3045023654895974		[learning rate: 0.0077738]
	Learning Rate: 0.00777377
	LOSS [training: 0.2868068363491719 | validation: 0.4266103210196218]
	TIME [epoch: 8.16 sec]
EPOCH 103/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.35549380914088846		[learning rate: 0.0077549]
		[batch 20/20] avg loss: 0.42258500576607994		[learning rate: 0.0077362]
	Learning Rate: 0.00773618
	LOSS [training: 0.38903940745348425 | validation: 0.2615855484608988]
	TIME [epoch: 8.14 sec]
EPOCH 104/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3016279629307543		[learning rate: 0.0077174]
		[batch 20/20] avg loss: 0.3438131535131963		[learning rate: 0.0076988]
	Learning Rate: 0.00769876
	LOSS [training: 0.3227205582219753 | validation: 0.24227166413834153]
	TIME [epoch: 8.14 sec]
EPOCH 105/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.37542575528556876		[learning rate: 0.0076801]
		[batch 20/20] avg loss: 0.3261648290882881		[learning rate: 0.0076615]
	Learning Rate: 0.00766153
	LOSS [training: 0.3507952921869284 | validation: 0.5925254518138523]
	TIME [epoch: 8.14 sec]
EPOCH 106/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3494413682561687		[learning rate: 0.007643]
		[batch 20/20] avg loss: 0.2677663296991291		[learning rate: 0.0076245]
	Learning Rate: 0.00762449
	LOSS [training: 0.30860384897764886 | validation: 0.29272843563877576]
	TIME [epoch: 8.16 sec]
EPOCH 107/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2754860226709314		[learning rate: 0.007606]
		[batch 20/20] avg loss: 0.3062270958469242		[learning rate: 0.0075876]
	Learning Rate: 0.00758761
	LOSS [training: 0.2908565592589279 | validation: 0.21996620081546758]
	TIME [epoch: 8.14 sec]
EPOCH 108/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27084068672175504		[learning rate: 0.0075692]
		[batch 20/20] avg loss: 0.3248768114155193		[learning rate: 0.0075509]
	Learning Rate: 0.00755092
	LOSS [training: 0.29785874906863724 | validation: 0.4536736838356861]
	TIME [epoch: 8.13 sec]
EPOCH 109/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29606690426568444		[learning rate: 0.0075326]
		[batch 20/20] avg loss: 0.2831073081397951		[learning rate: 0.0075144]
	Learning Rate: 0.00751441
	LOSS [training: 0.28958710620273986 | validation: 0.3244920581876978]
	TIME [epoch: 8.13 sec]
EPOCH 110/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29423408316524524		[learning rate: 0.0074962]
		[batch 20/20] avg loss: 0.38494768046711786		[learning rate: 0.0074781]
	Learning Rate: 0.00747807
	LOSS [training: 0.3395908818161816 | validation: 0.26012672038332146]
	TIME [epoch: 8.16 sec]
EPOCH 111/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2886449086289348		[learning rate: 0.00746]
		[batch 20/20] avg loss: 0.271516608876265		[learning rate: 0.0074419]
	Learning Rate: 0.00744191
	LOSS [training: 0.28008075875259986 | validation: 0.5973253420940426]
	TIME [epoch: 8.13 sec]
EPOCH 112/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30557608496335076		[learning rate: 0.0074239]
		[batch 20/20] avg loss: 0.2848917323537622		[learning rate: 0.0074059]
	Learning Rate: 0.00740592
	LOSS [training: 0.2952339086585565 | validation: 0.2033509506533585]
	TIME [epoch: 8.13 sec]
EPOCH 113/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4212259554928415		[learning rate: 0.007388]
		[batch 20/20] avg loss: 0.2616021174045021		[learning rate: 0.0073701]
	Learning Rate: 0.00737011
	LOSS [training: 0.34141403644867174 | validation: 0.37465331878395025]
	TIME [epoch: 8.12 sec]
EPOCH 114/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.34487723172098406		[learning rate: 0.0073523]
		[batch 20/20] avg loss: 0.3310898043952891		[learning rate: 0.0073345]
	Learning Rate: 0.00733446
	LOSS [training: 0.3379835180581366 | validation: 0.32657753072613604]
	TIME [epoch: 8.15 sec]
EPOCH 115/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4055461164399614		[learning rate: 0.0073167]
		[batch 20/20] avg loss: 0.28457280640916605		[learning rate: 0.007299]
	Learning Rate: 0.007299
	LOSS [training: 0.34505946142456373 | validation: 0.44018298720378135]
	TIME [epoch: 8.13 sec]
EPOCH 116/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2887636573325364		[learning rate: 0.0072813]
		[batch 20/20] avg loss: 0.27225852852755333		[learning rate: 0.0072637]
	Learning Rate: 0.0072637
	LOSS [training: 0.28051109293004484 | validation: 0.5001020311884253]
	TIME [epoch: 8.13 sec]
EPOCH 117/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3033167830913229		[learning rate: 0.0072461]
		[batch 20/20] avg loss: 0.2905866424881852		[learning rate: 0.0072286]
	Learning Rate: 0.00722857
	LOSS [training: 0.296951712789754 | validation: 0.21954086910776008]
	TIME [epoch: 8.13 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26838623779909143		[learning rate: 0.0072111]
		[batch 20/20] avg loss: 0.26571832992745514		[learning rate: 0.0071936]
	Learning Rate: 0.00719362
	LOSS [training: 0.2670522838632733 | validation: 0.24975250051919845]
	TIME [epoch: 8.16 sec]
EPOCH 119/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30460614587242896		[learning rate: 0.0071762]
		[batch 20/20] avg loss: 0.29770097560640413		[learning rate: 0.0071588]
	Learning Rate: 0.00715883
	LOSS [training: 0.3011535607394165 | validation: 0.15996421944522346]
	TIME [epoch: 8.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_119.pth
	Model improved!!!
EPOCH 120/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2627769240877599		[learning rate: 0.0071415]
		[batch 20/20] avg loss: 0.21417462906110094		[learning rate: 0.0071242]
	Learning Rate: 0.00712421
	LOSS [training: 0.2384757765744304 | validation: 0.5650331736448476]
	TIME [epoch: 8.13 sec]
EPOCH 121/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3712544098993116		[learning rate: 0.007107]
		[batch 20/20] avg loss: 0.23428556230715497		[learning rate: 0.0070898]
	Learning Rate: 0.00708976
	LOSS [training: 0.30276998610323325 | validation: 0.196904571569287]
	TIME [epoch: 8.13 sec]
EPOCH 122/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2460044481059321		[learning rate: 0.0070726]
		[batch 20/20] avg loss: 0.3589907926125622		[learning rate: 0.0070555]
	Learning Rate: 0.00705548
	LOSS [training: 0.3024976203592472 | validation: 0.22473112992025335]
	TIME [epoch: 8.15 sec]
EPOCH 123/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23410735542792632		[learning rate: 0.0070384]
		[batch 20/20] avg loss: 0.30760710345776127		[learning rate: 0.0070214]
	Learning Rate: 0.00702136
	LOSS [training: 0.2708572294428438 | validation: 0.26101196295901613]
	TIME [epoch: 8.13 sec]
EPOCH 124/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30774305154719606		[learning rate: 0.0070044]
		[batch 20/20] avg loss: 0.2446138752890362		[learning rate: 0.0069874]
	Learning Rate: 0.0069874
	LOSS [training: 0.27617846341811614 | validation: 0.14801077227850337]
	TIME [epoch: 8.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_124.pth
	Model improved!!!
EPOCH 125/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2138113669854406		[learning rate: 0.0069705]
		[batch 20/20] avg loss: 0.26389760142645663		[learning rate: 0.0069536]
	Learning Rate: 0.00695361
	LOSS [training: 0.23885448420594865 | validation: 0.13947607712194715]
	TIME [epoch: 8.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_125.pth
	Model improved!!!
EPOCH 126/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20295143271012556		[learning rate: 0.0069368]
		[batch 20/20] avg loss: 0.2002442263128688		[learning rate: 0.00692]
	Learning Rate: 0.00691999
	LOSS [training: 0.20159782951149716 | validation: 0.1958933253811378]
	TIME [epoch: 8.15 sec]
EPOCH 127/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2358054081830986		[learning rate: 0.0069032]
		[batch 20/20] avg loss: 0.37185664226066006		[learning rate: 0.0068865]
	Learning Rate: 0.00688652
	LOSS [training: 0.3038310252218793 | validation: 0.1617008344902154]
	TIME [epoch: 8.13 sec]
EPOCH 128/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2520393774598089		[learning rate: 0.0068699]
		[batch 20/20] avg loss: 0.20303994229992012		[learning rate: 0.0068532]
	Learning Rate: 0.00685322
	LOSS [training: 0.22753965987986446 | validation: 0.14791300750025582]
	TIME [epoch: 8.13 sec]
EPOCH 129/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25539795933950227		[learning rate: 0.0068366]
		[batch 20/20] avg loss: 0.26825687966884254		[learning rate: 0.0068201]
	Learning Rate: 0.00682008
	LOSS [training: 0.26182741950417243 | validation: 0.6324358265389824]
	TIME [epoch: 8.13 sec]
EPOCH 130/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3225794604522886		[learning rate: 0.0068036]
		[batch 20/20] avg loss: 0.3710405561591682		[learning rate: 0.0067871]
	Learning Rate: 0.0067871
	LOSS [training: 0.3468100083057283 | validation: 0.11313147179758348]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_130.pth
	Model improved!!!
EPOCH 131/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2568152898620433		[learning rate: 0.0067707]
		[batch 20/20] avg loss: 0.21760545253694757		[learning rate: 0.0067543]
	Learning Rate: 0.00675428
	LOSS [training: 0.23721037119949542 | validation: 0.29462619197524936]
	TIME [epoch: 8.15 sec]
EPOCH 132/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25285111316510117		[learning rate: 0.0067379]
		[batch 20/20] avg loss: 0.23647785461386356		[learning rate: 0.0067216]
	Learning Rate: 0.00672162
	LOSS [training: 0.2446644838894824 | validation: 0.36155784478338143]
	TIME [epoch: 8.14 sec]
EPOCH 133/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25881295077162425		[learning rate: 0.0067053]
		[batch 20/20] avg loss: 0.319045939159715		[learning rate: 0.0066891]
	Learning Rate: 0.00668911
	LOSS [training: 0.2889294449656696 | validation: 0.13908264611520796]
	TIME [epoch: 8.12 sec]
EPOCH 134/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2462589161775141		[learning rate: 0.0066729]
		[batch 20/20] avg loss: 0.23335705827009745		[learning rate: 0.0066568]
	Learning Rate: 0.00665676
	LOSS [training: 0.23980798722380578 | validation: 0.49699465422319483]
	TIME [epoch: 8.15 sec]
EPOCH 135/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22309006933408476		[learning rate: 0.0066406]
		[batch 20/20] avg loss: 0.23308023642061163		[learning rate: 0.0066246]
	Learning Rate: 0.00662457
	LOSS [training: 0.22808515287734812 | validation: 0.24942819294842566]
	TIME [epoch: 8.14 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2668056768331104		[learning rate: 0.0066085]
		[batch 20/20] avg loss: 0.2846007590771057		[learning rate: 0.0065925]
	Learning Rate: 0.00659254
	LOSS [training: 0.2757032179551081 | validation: 0.26040988475747096]
	TIME [epoch: 8.14 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.38748338329157717		[learning rate: 0.0065766]
		[batch 20/20] avg loss: 0.29296423103323227		[learning rate: 0.0065607]
	Learning Rate: 0.00656066
	LOSS [training: 0.34022380716240463 | validation: 0.47936997840488155]
	TIME [epoch: 8.13 sec]
EPOCH 138/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31799093911368825		[learning rate: 0.0065448]
		[batch 20/20] avg loss: 0.22237220659583987		[learning rate: 0.0065289]
	Learning Rate: 0.00652893
	LOSS [training: 0.2701815728547641 | validation: 0.9155838289571635]
	TIME [epoch: 8.16 sec]
EPOCH 139/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.37101211357596		[learning rate: 0.0065131]
		[batch 20/20] avg loss: 0.2855291188111492		[learning rate: 0.0064974]
	Learning Rate: 0.00649736
	LOSS [training: 0.3282706161935546 | validation: 0.12323865963427089]
	TIME [epoch: 8.14 sec]
EPOCH 140/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2696138292323924		[learning rate: 0.0064816]
		[batch 20/20] avg loss: 0.24627758995274535		[learning rate: 0.0064659]
	Learning Rate: 0.00646594
	LOSS [training: 0.2579457095925689 | validation: 0.5473441222289274]
	TIME [epoch: 8.14 sec]
EPOCH 141/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31314228090416546		[learning rate: 0.0064503]
		[batch 20/20] avg loss: 0.209621178063695		[learning rate: 0.0064347]
	Learning Rate: 0.00643467
	LOSS [training: 0.26138172948393024 | validation: 0.2919446756735437]
	TIME [epoch: 8.13 sec]
EPOCH 142/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17827617919548283		[learning rate: 0.0064191]
		[batch 20/20] avg loss: 0.24146147321394543		[learning rate: 0.0064036]
	Learning Rate: 0.00640355
	LOSS [training: 0.2098688262047141 | validation: 0.3614112385210057]
	TIME [epoch: 8.16 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2383066073498684		[learning rate: 0.0063881]
		[batch 20/20] avg loss: 0.24893164336740647		[learning rate: 0.0063726]
	Learning Rate: 0.00637259
	LOSS [training: 0.24361912535863745 | validation: 0.14039045667696498]
	TIME [epoch: 8.15 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2620964054738298		[learning rate: 0.0063572]
		[batch 20/20] avg loss: 0.30432941667030666		[learning rate: 0.0063418]
	Learning Rate: 0.00634177
	LOSS [training: 0.2832129110720683 | validation: 0.1565358928900622]
	TIME [epoch: 8.14 sec]
EPOCH 145/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25830313248349407		[learning rate: 0.0063264]
		[batch 20/20] avg loss: 0.2855746942602918		[learning rate: 0.0063111]
	Learning Rate: 0.0063111
	LOSS [training: 0.271938913371893 | validation: 0.1571677519685368]
	TIME [epoch: 8.14 sec]
EPOCH 146/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2098402238141654		[learning rate: 0.0062958]
		[batch 20/20] avg loss: 0.3233088665225152		[learning rate: 0.0062806]
	Learning Rate: 0.00628058
	LOSS [training: 0.2665745451683403 | validation: 0.4361459627499794]
	TIME [epoch: 8.15 sec]
EPOCH 147/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28614247653412955		[learning rate: 0.0062654]
		[batch 20/20] avg loss: 0.28841967767764837		[learning rate: 0.0062502]
	Learning Rate: 0.00625021
	LOSS [training: 0.28728107710588896 | validation: 0.21572552992760835]
	TIME [epoch: 8.15 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2582459593755184		[learning rate: 0.0062351]
		[batch 20/20] avg loss: 0.2131701141223777		[learning rate: 0.00622]
	Learning Rate: 0.00621999
	LOSS [training: 0.23570803674894805 | validation: 0.6008597500778577]
	TIME [epoch: 8.13 sec]
EPOCH 149/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29123180525415315		[learning rate: 0.0062049]
		[batch 20/20] avg loss: 0.22936571412139153		[learning rate: 0.0061899]
	Learning Rate: 0.00618991
	LOSS [training: 0.26029875968777233 | validation: 0.07782281255034772]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_149.pth
	Model improved!!!
EPOCH 150/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2881731712114159		[learning rate: 0.0061749]
		[batch 20/20] avg loss: 0.2770111747314158		[learning rate: 0.00616]
	Learning Rate: 0.00615997
	LOSS [training: 0.28259217297141587 | validation: 0.5984842568352663]
	TIME [epoch: 8.15 sec]
EPOCH 151/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2934349898359717		[learning rate: 0.0061451]
		[batch 20/20] avg loss: 0.3232109041400669		[learning rate: 0.0061302]
	Learning Rate: 0.00613019
	LOSS [training: 0.3083229469880193 | validation: 0.22549310818936588]
	TIME [epoch: 8.15 sec]
EPOCH 152/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21004733921697424		[learning rate: 0.0061153]
		[batch 20/20] avg loss: 0.21010044092310257		[learning rate: 0.0061005]
	Learning Rate: 0.00610054
	LOSS [training: 0.21007389007003838 | validation: 0.16412652459851837]
	TIME [epoch: 8.14 sec]
EPOCH 153/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25729030869572006		[learning rate: 0.0060858]
		[batch 20/20] avg loss: 0.21444793477850102		[learning rate: 0.006071]
	Learning Rate: 0.00607104
	LOSS [training: 0.2358691217371105 | validation: 0.09758760198182756]
	TIME [epoch: 8.14 sec]
EPOCH 154/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.47915820106851187		[learning rate: 0.0060563]
		[batch 20/20] avg loss: 0.2271297281303976		[learning rate: 0.0060417]
	Learning Rate: 0.00604168
	LOSS [training: 0.3531439645994548 | validation: 0.2808778120425002]
	TIME [epoch: 8.14 sec]
EPOCH 155/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3354548886473829		[learning rate: 0.0060271]
		[batch 20/20] avg loss: 0.21325249619366424		[learning rate: 0.0060125]
	Learning Rate: 0.00601247
	LOSS [training: 0.27435369242052354 | validation: 0.15267888367642996]
	TIME [epoch: 8.17 sec]
EPOCH 156/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20980354408171023		[learning rate: 0.0059979]
		[batch 20/20] avg loss: 0.20343521383936686		[learning rate: 0.0059834]
	Learning Rate: 0.00598339
	LOSS [training: 0.20661937896053856 | validation: 0.07400205649033599]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_156.pth
	Model improved!!!
EPOCH 157/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26404280720929163		[learning rate: 0.0059689]
		[batch 20/20] avg loss: 0.2511830653687943		[learning rate: 0.0059545]
	Learning Rate: 0.00595446
	LOSS [training: 0.25761293628904297 | validation: 0.23043711851195126]
	TIME [epoch: 8.13 sec]
EPOCH 158/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.32051354372923285		[learning rate: 0.00594]
		[batch 20/20] avg loss: 0.20887948751825264		[learning rate: 0.0059257]
	Learning Rate: 0.00592566
	LOSS [training: 0.26469651562374275 | validation: 0.23173690969612332]
	TIME [epoch: 8.13 sec]
EPOCH 159/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2532088546107448		[learning rate: 0.0059113]
		[batch 20/20] avg loss: 0.27616566669412096		[learning rate: 0.005897]
	Learning Rate: 0.00589701
	LOSS [training: 0.26468726065243287 | validation: 0.11262252593848583]
	TIME [epoch: 8.16 sec]
EPOCH 160/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20113321324571629		[learning rate: 0.0058827]
		[batch 20/20] avg loss: 0.23254966400838217		[learning rate: 0.0058685]
	Learning Rate: 0.00586849
	LOSS [training: 0.21684143862704924 | validation: 0.14615797654406873]
	TIME [epoch: 8.15 sec]
EPOCH 161/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25087911953234227		[learning rate: 0.0058543]
		[batch 20/20] avg loss: 0.3146370408540763		[learning rate: 0.0058401]
	Learning Rate: 0.00584011
	LOSS [training: 0.28275808019320936 | validation: 0.3995456567098692]
	TIME [epoch: 8.13 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.38601776807694127		[learning rate: 0.005826]
		[batch 20/20] avg loss: 0.19327846216215422		[learning rate: 0.0058119]
	Learning Rate: 0.00581187
	LOSS [training: 0.2896481151195477 | validation: 0.12902421565999175]
	TIME [epoch: 8.13 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28520722153632		[learning rate: 0.0057978]
		[batch 20/20] avg loss: 0.2704749124385505		[learning rate: 0.0057838]
	Learning Rate: 0.00578376
	LOSS [training: 0.27784106698743527 | validation: 0.17833905790817586]
	TIME [epoch: 8.16 sec]
EPOCH 164/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22940543439084782		[learning rate: 0.0057698]
		[batch 20/20] avg loss: 0.24718120049134001		[learning rate: 0.0057558]
	Learning Rate: 0.00575579
	LOSS [training: 0.23829331744109394 | validation: 0.7647206884937783]
	TIME [epoch: 8.13 sec]
EPOCH 165/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3243997894592406		[learning rate: 0.0057419]
		[batch 20/20] avg loss: 0.19103197219625753		[learning rate: 0.005728]
	Learning Rate: 0.00572796
	LOSS [training: 0.25771588082774904 | validation: 0.22896225203283463]
	TIME [epoch: 8.13 sec]
EPOCH 166/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22802102674338837		[learning rate: 0.0057141]
		[batch 20/20] avg loss: 0.36371365093638064		[learning rate: 0.0057003]
	Learning Rate: 0.00570026
	LOSS [training: 0.2958673388398846 | validation: 0.23304565365253097]
	TIME [epoch: 8.13 sec]
EPOCH 167/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1973370031125839		[learning rate: 0.0056865]
		[batch 20/20] avg loss: 0.279135766584326		[learning rate: 0.0056727]
	Learning Rate: 0.0056727
	LOSS [training: 0.23823638484845494 | validation: 0.19243961742513443]
	TIME [epoch: 8.16 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20730218604055667		[learning rate: 0.005659]
		[batch 20/20] avg loss: 0.39404994574138197		[learning rate: 0.0056453]
	Learning Rate: 0.00564526
	LOSS [training: 0.3006760658909693 | validation: 0.19285339460282638]
	TIME [epoch: 8.13 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25569988052017073		[learning rate: 0.0056316]
		[batch 20/20] avg loss: 0.3667854405771491		[learning rate: 0.005618]
	Learning Rate: 0.00561796
	LOSS [training: 0.3112426605486599 | validation: 0.24480091628749953]
	TIME [epoch: 8.13 sec]
EPOCH 170/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2663617858105771		[learning rate: 0.0056044]
		[batch 20/20] avg loss: 0.2504208044609685		[learning rate: 0.0055908]
	Learning Rate: 0.0055908
	LOSS [training: 0.25839129513577286 | validation: 0.4088937433148888]
	TIME [epoch: 8.13 sec]
EPOCH 171/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.32063410489819244		[learning rate: 0.0055773]
		[batch 20/20] avg loss: 0.2078839349544645		[learning rate: 0.0055638]
	Learning Rate: 0.00556376
	LOSS [training: 0.2642590199263285 | validation: 0.33596198384747283]
	TIME [epoch: 8.16 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2935019649304714		[learning rate: 0.0055503]
		[batch 20/20] avg loss: 0.24435080600466225		[learning rate: 0.0055369]
	Learning Rate: 0.00553685
	LOSS [training: 0.26892638546756686 | validation: 0.5953567285403069]
	TIME [epoch: 8.13 sec]
EPOCH 173/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3575927682076121		[learning rate: 0.0055235]
		[batch 20/20] avg loss: 0.3271695248800976		[learning rate: 0.0055101]
	Learning Rate: 0.00551008
	LOSS [training: 0.34238114654385476 | validation: 0.12195498609229345]
	TIME [epoch: 8.13 sec]
EPOCH 174/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3374835212107997		[learning rate: 0.0054967]
		[batch 20/20] avg loss: 0.2733326670320339		[learning rate: 0.0054834]
	Learning Rate: 0.00548343
	LOSS [training: 0.30540809412141684 | validation: 0.12655960861683285]
	TIME [epoch: 8.13 sec]
EPOCH 175/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1856063070548717		[learning rate: 0.0054702]
		[batch 20/20] avg loss: 0.2008163897307861		[learning rate: 0.0054569]
	Learning Rate: 0.00545692
	LOSS [training: 0.19321134839282889 | validation: 0.17840036512847066]
	TIME [epoch: 8.15 sec]
EPOCH 176/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16746413052445025		[learning rate: 0.0054437]
		[batch 20/20] avg loss: 0.20731496568661748		[learning rate: 0.0054305]
	Learning Rate: 0.00543053
	LOSS [training: 0.18738954810553385 | validation: 0.1317382543010962]
	TIME [epoch: 8.13 sec]
EPOCH 177/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.317457310712865		[learning rate: 0.0054174]
		[batch 20/20] avg loss: 0.2659192652281887		[learning rate: 0.0054043]
	Learning Rate: 0.00540427
	LOSS [training: 0.29168828797052687 | validation: 0.12719692162285426]
	TIME [epoch: 8.13 sec]
EPOCH 178/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26242694921673193		[learning rate: 0.0053912]
		[batch 20/20] avg loss: 0.27836132731914665		[learning rate: 0.0053781]
	Learning Rate: 0.00537813
	LOSS [training: 0.27039413826793934 | validation: 0.31961598831461785]
	TIME [epoch: 8.13 sec]
EPOCH 179/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2651344599032254		[learning rate: 0.0053651]
		[batch 20/20] avg loss: 0.20027532093422332		[learning rate: 0.0053521]
	Learning Rate: 0.00535213
	LOSS [training: 0.23270489041872439 | validation: 0.1396887606468674]
	TIME [epoch: 8.16 sec]
EPOCH 180/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20831602147679842		[learning rate: 0.0053392]
		[batch 20/20] avg loss: 0.18615020222196338		[learning rate: 0.0053262]
	Learning Rate: 0.00532624
	LOSS [training: 0.19723311184938092 | validation: 0.10924504284013418]
	TIME [epoch: 8.14 sec]
EPOCH 181/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21942076816781073		[learning rate: 0.0053133]
		[batch 20/20] avg loss: 0.24854959385200476		[learning rate: 0.0053005]
	Learning Rate: 0.00530049
	LOSS [training: 0.2339851810099077 | validation: 0.13786049452719978]
	TIME [epoch: 8.13 sec]
EPOCH 182/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2603867093844998		[learning rate: 0.0052877]
		[batch 20/20] avg loss: 0.2774660036059605		[learning rate: 0.0052749]
	Learning Rate: 0.00527485
	LOSS [training: 0.2689263564952302 | validation: 0.15761768985473598]
	TIME [epoch: 8.13 sec]
EPOCH 183/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4020279001697424		[learning rate: 0.0052621]
		[batch 20/20] avg loss: 0.22208908119035234		[learning rate: 0.0052493]
	Learning Rate: 0.00524935
	LOSS [training: 0.3120584906800473 | validation: 0.15519198360875253]
	TIME [epoch: 8.16 sec]
EPOCH 184/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2239914791720036		[learning rate: 0.0052366]
		[batch 20/20] avg loss: 0.23655711762569406		[learning rate: 0.005224]
	Learning Rate: 0.00522396
	LOSS [training: 0.23027429839884883 | validation: 0.4466505714897712]
	TIME [epoch: 8.13 sec]
EPOCH 185/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2625321076905651		[learning rate: 0.0052113]
		[batch 20/20] avg loss: 0.22882401800063906		[learning rate: 0.0051987]
	Learning Rate: 0.0051987
	LOSS [training: 0.24567806284560212 | validation: 0.5523076175303654]
	TIME [epoch: 8.13 sec]
EPOCH 186/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29433666264406216		[learning rate: 0.0051861]
		[batch 20/20] avg loss: 0.1870004953167148		[learning rate: 0.0051736]
	Learning Rate: 0.00517356
	LOSS [training: 0.24066857898038846 | validation: 0.134534799816325]
	TIME [epoch: 8.13 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.33332953064618354		[learning rate: 0.005161]
		[batch 20/20] avg loss: 0.24976875972027818		[learning rate: 0.0051485]
	Learning Rate: 0.00514854
	LOSS [training: 0.2915491451832308 | validation: 0.13411943842435495]
	TIME [epoch: 8.14 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28247940141171646		[learning rate: 0.0051361]
		[batch 20/20] avg loss: 0.24043787114365173		[learning rate: 0.0051236]
	Learning Rate: 0.00512364
	LOSS [training: 0.261458636277684 | validation: 0.4741972375391834]
	TIME [epoch: 8.15 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19244740698193313		[learning rate: 0.0051112]
		[batch 20/20] avg loss: 0.25131342168005294		[learning rate: 0.0050989]
	Learning Rate: 0.00509887
	LOSS [training: 0.22188041433099306 | validation: 0.13206919037814446]
	TIME [epoch: 8.19 sec]
EPOCH 190/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18033157610624406		[learning rate: 0.0050865]
		[batch 20/20] avg loss: 0.23383453884749325		[learning rate: 0.0050742]
	Learning Rate: 0.00507421
	LOSS [training: 0.20708305747686864 | validation: 0.12462379806757429]
	TIME [epoch: 8.14 sec]
EPOCH 191/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2500632517971996		[learning rate: 0.0050619]
		[batch 20/20] avg loss: 0.28837703056478753		[learning rate: 0.0050497]
	Learning Rate: 0.00504967
	LOSS [training: 0.2692201411809936 | validation: 0.10831684489630973]
	TIME [epoch: 8.16 sec]
EPOCH 192/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22823129600678427		[learning rate: 0.0050374]
		[batch 20/20] avg loss: 0.2334734210944335		[learning rate: 0.0050253]
	Learning Rate: 0.00502525
	LOSS [training: 0.2308523585506089 | validation: 0.3040283503831909]
	TIME [epoch: 8.15 sec]
EPOCH 193/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2270745784646035		[learning rate: 0.0050131]
		[batch 20/20] avg loss: 0.33872196494388546		[learning rate: 0.005001]
	Learning Rate: 0.00500095
	LOSS [training: 0.2828982717042445 | validation: 0.4002261712675528]
	TIME [epoch: 8.13 sec]
EPOCH 194/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23388704378606912		[learning rate: 0.0049888]
		[batch 20/20] avg loss: 0.2409797935051871		[learning rate: 0.0049768]
	Learning Rate: 0.00497677
	LOSS [training: 0.2374334186456281 | validation: 0.1539429853872442]
	TIME [epoch: 8.14 sec]
EPOCH 195/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1863128419312612		[learning rate: 0.0049647]
		[batch 20/20] avg loss: 0.2032278747069836		[learning rate: 0.0049527]
	Learning Rate: 0.0049527
	LOSS [training: 0.19477035831912234 | validation: 0.19299713251363199]
	TIME [epoch: 8.15 sec]
EPOCH 196/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17483019946496473		[learning rate: 0.0049407]
		[batch 20/20] avg loss: 0.27050420758543936		[learning rate: 0.0049288]
	Learning Rate: 0.00492875
	LOSS [training: 0.22266720352520206 | validation: 0.289180319885055]
	TIME [epoch: 8.15 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23905140396461372		[learning rate: 0.0049168]
		[batch 20/20] avg loss: 0.19672477623797202		[learning rate: 0.0049049]
	Learning Rate: 0.00490492
	LOSS [training: 0.21788809010129281 | validation: 0.10161222944965664]
	TIME [epoch: 8.12 sec]
EPOCH 198/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27844553072539785		[learning rate: 0.004893]
		[batch 20/20] avg loss: 0.18255429403548176		[learning rate: 0.0048812]
	Learning Rate: 0.0048812
	LOSS [training: 0.23049991238043974 | validation: 0.2513262214995911]
	TIME [epoch: 8.14 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.244631222787969		[learning rate: 0.0048694]
		[batch 20/20] avg loss: 0.21879728661437295		[learning rate: 0.0048576]
	Learning Rate: 0.00485759
	LOSS [training: 0.23171425470117096 | validation: 0.2710835323558099]
	TIME [epoch: 8.15 sec]
EPOCH 200/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2743126367033654		[learning rate: 0.0048458]
		[batch 20/20] avg loss: 0.29110713806326977		[learning rate: 0.0048341]
	Learning Rate: 0.0048341
	LOSS [training: 0.2827098873833176 | validation: 0.41568785804073316]
	TIME [epoch: 8.15 sec]
EPOCH 201/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2827371837452587		[learning rate: 0.0048224]
		[batch 20/20] avg loss: 0.1993288286649315		[learning rate: 0.0048107]
	Learning Rate: 0.00481072
	LOSS [training: 0.24103300620509502 | validation: 0.1433471106011076]
	TIME [epoch: 8.14 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19248026081082573		[learning rate: 0.0047991]
		[batch 20/20] avg loss: 0.23716402120136934		[learning rate: 0.0047875]
	Learning Rate: 0.00478746
	LOSS [training: 0.21482214100609748 | validation: 0.1370418549499525]
	TIME [epoch: 8.13 sec]
EPOCH 203/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17684613341578842		[learning rate: 0.0047759]
		[batch 20/20] avg loss: 0.25732675248353964		[learning rate: 0.0047643]
	Learning Rate: 0.00476431
	LOSS [training: 0.21708644294966403 | validation: 0.17482510539739662]
	TIME [epoch: 8.14 sec]
EPOCH 204/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1872298289085454		[learning rate: 0.0047528]
		[batch 20/20] avg loss: 0.24878394546039878		[learning rate: 0.0047413]
	Learning Rate: 0.00474127
	LOSS [training: 0.2180068871844721 | validation: 0.3517037610777523]
	TIME [epoch: 8.15 sec]
EPOCH 205/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25081957285031253		[learning rate: 0.0047298]
		[batch 20/20] avg loss: 0.18322044604756876		[learning rate: 0.0047183]
	Learning Rate: 0.00471834
	LOSS [training: 0.21702000944894065 | validation: 0.19694552424606857]
	TIME [epoch: 8.14 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2286704196254718		[learning rate: 0.0047069]
		[batch 20/20] avg loss: 0.2052062826003261		[learning rate: 0.0046955]
	Learning Rate: 0.00469553
	LOSS [training: 0.21693835111289897 | validation: 0.14064448344063174]
	TIME [epoch: 8.14 sec]
EPOCH 207/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27623811402801957		[learning rate: 0.0046842]
		[batch 20/20] avg loss: 0.210056476072298		[learning rate: 0.0046728]
	Learning Rate: 0.00467282
	LOSS [training: 0.2431472950501588 | validation: 0.14629041964514414]
	TIME [epoch: 8.15 sec]
EPOCH 208/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21346031290618703		[learning rate: 0.0046615]
		[batch 20/20] avg loss: 0.2564443048471175		[learning rate: 0.0046502]
	Learning Rate: 0.00465022
	LOSS [training: 0.2349523088766523 | validation: 0.2972931492428964]
	TIME [epoch: 8.16 sec]
EPOCH 209/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17072084678212376		[learning rate: 0.004639]
		[batch 20/20] avg loss: 0.21914963552068273		[learning rate: 0.0046277]
	Learning Rate: 0.00462773
	LOSS [training: 0.19493524115140323 | validation: 0.173601408808075]
	TIME [epoch: 8.14 sec]
EPOCH 210/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2501717233647029		[learning rate: 0.0046165]
		[batch 20/20] avg loss: 0.25968578864744407		[learning rate: 0.0046054]
	Learning Rate: 0.00460536
	LOSS [training: 0.25492875600607345 | validation: 0.3997189285499929]
	TIME [epoch: 8.14 sec]
EPOCH 211/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19867583515331616		[learning rate: 0.0045942]
		[batch 20/20] avg loss: 0.2583996332505088		[learning rate: 0.0045831]
	Learning Rate: 0.00458308
	LOSS [training: 0.2285377342019125 | validation: 0.1735383587060475]
	TIME [epoch: 8.14 sec]
EPOCH 212/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20594110299651006		[learning rate: 0.004572]
		[batch 20/20] avg loss: 0.19568242975733222		[learning rate: 0.0045609]
	Learning Rate: 0.00456092
	LOSS [training: 0.2008117663769211 | validation: 0.11201036591506422]
	TIME [epoch: 8.16 sec]
EPOCH 213/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2130039488134448		[learning rate: 0.0045499]
		[batch 20/20] avg loss: 0.2255025152949181		[learning rate: 0.0045389]
	Learning Rate: 0.00453887
	LOSS [training: 0.2192532320541815 | validation: 0.30507666064931066]
	TIME [epoch: 8.14 sec]
EPOCH 214/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2045728918957249		[learning rate: 0.0045279]
		[batch 20/20] avg loss: 0.15171932556019302		[learning rate: 0.0045169]
	Learning Rate: 0.00451692
	LOSS [training: 0.17814610872795894 | validation: 0.34586569445612014]
	TIME [epoch: 8.13 sec]
EPOCH 215/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22044438260057192		[learning rate: 0.004506]
		[batch 20/20] avg loss: 0.1607050994306221		[learning rate: 0.0044951]
	Learning Rate: 0.00449507
	LOSS [training: 0.19057474101559704 | validation: 0.11623887322873139]
	TIME [epoch: 8.14 sec]
EPOCH 216/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2502183157234624		[learning rate: 0.0044842]
		[batch 20/20] avg loss: 0.1905770715042641		[learning rate: 0.0044733]
	Learning Rate: 0.00447334
	LOSS [training: 0.22039769361386327 | validation: 0.1857141996996518]
	TIME [epoch: 8.16 sec]
EPOCH 217/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19623308133497147		[learning rate: 0.0044625]
		[batch 20/20] avg loss: 0.21306064186954837		[learning rate: 0.0044517]
	Learning Rate: 0.0044517
	LOSS [training: 0.20464686160225992 | validation: 0.17597992611558907]
	TIME [epoch: 8.14 sec]
EPOCH 218/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16724341317100772		[learning rate: 0.0044409]
		[batch 20/20] avg loss: 0.1816400502313577		[learning rate: 0.0044302]
	Learning Rate: 0.00443018
	LOSS [training: 0.1744417317011827 | validation: 0.15534302607888262]
	TIME [epoch: 8.14 sec]
EPOCH 219/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3565955266185318		[learning rate: 0.0044195]
		[batch 20/20] avg loss: 0.1596291896458864		[learning rate: 0.0044088]
	Learning Rate: 0.00440875
	LOSS [training: 0.2581123581322091 | validation: 0.18076191888666568]
	TIME [epoch: 8.14 sec]
EPOCH 220/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20924173083300684		[learning rate: 0.0043981]
		[batch 20/20] avg loss: 0.2133733088369091		[learning rate: 0.0043874]
	Learning Rate: 0.00438743
	LOSS [training: 0.21130751983495796 | validation: 0.20110006264268404]
	TIME [epoch: 8.17 sec]
EPOCH 221/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19717761271287815		[learning rate: 0.0043768]
		[batch 20/20] avg loss: 0.25666423204283917		[learning rate: 0.0043662]
	Learning Rate: 0.00436622
	LOSS [training: 0.22692092237785863 | validation: 0.2295535440448228]
	TIME [epoch: 8.15 sec]
EPOCH 222/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18002281369461826		[learning rate: 0.0043556]
		[batch 20/20] avg loss: 0.23958371799428554		[learning rate: 0.0043451]
	Learning Rate: 0.0043451
	LOSS [training: 0.2098032658444519 | validation: 0.37219713709214386]
	TIME [epoch: 8.14 sec]
EPOCH 223/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3149699702113893		[learning rate: 0.0043346]
		[batch 20/20] avg loss: 0.18699376643133908		[learning rate: 0.0043241]
	Learning Rate: 0.00432409
	LOSS [training: 0.25098186832136415 | validation: 0.11002248765765182]
	TIME [epoch: 8.14 sec]
EPOCH 224/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1812808489175734		[learning rate: 0.0043136]
		[batch 20/20] avg loss: 0.22538744663409022		[learning rate: 0.0043032]
	Learning Rate: 0.00430318
	LOSS [training: 0.20333414777583175 | validation: 0.17340399653901484]
	TIME [epoch: 8.17 sec]
EPOCH 225/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19557222158268628		[learning rate: 0.0042928]
		[batch 20/20] avg loss: 0.2624166475711022		[learning rate: 0.0042824]
	Learning Rate: 0.00428237
	LOSS [training: 0.22899443457689425 | validation: 0.16034420122955162]
	TIME [epoch: 8.14 sec]
EPOCH 226/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17327341294450108		[learning rate: 0.004272]
		[batch 20/20] avg loss: 0.25475452844575136		[learning rate: 0.0042617]
	Learning Rate: 0.00426166
	LOSS [training: 0.2140139706951262 | validation: 0.19264095649946358]
	TIME [epoch: 8.13 sec]
EPOCH 227/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18284186725941098		[learning rate: 0.0042513]
		[batch 20/20] avg loss: 0.26803577268703516		[learning rate: 0.0042411]
	Learning Rate: 0.00424105
	LOSS [training: 0.22543881997322304 | validation: 0.34154164994133573]
	TIME [epoch: 8.13 sec]
EPOCH 228/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3219373698702092		[learning rate: 0.0042308]
		[batch 20/20] avg loss: 0.2529114087018839		[learning rate: 0.0042205]
	Learning Rate: 0.00422054
	LOSS [training: 0.2874243892860465 | validation: 0.18872175823394827]
	TIME [epoch: 8.16 sec]
EPOCH 229/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22375926155866083		[learning rate: 0.0042103]
		[batch 20/20] avg loss: 0.21786066799050202		[learning rate: 0.0042001]
	Learning Rate: 0.00420013
	LOSS [training: 0.22080996477458142 | validation: 0.3482552749187326]
	TIME [epoch: 8.14 sec]
EPOCH 230/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1957744400310006		[learning rate: 0.00419]
		[batch 20/20] avg loss: 0.21196140871387917		[learning rate: 0.0041798]
	Learning Rate: 0.00417982
	LOSS [training: 0.2038679243724399 | validation: 0.12844109695799824]
	TIME [epoch: 8.14 sec]
EPOCH 231/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27402940547564425		[learning rate: 0.0041697]
		[batch 20/20] avg loss: 0.1802466790129598		[learning rate: 0.0041596]
	Learning Rate: 0.00415961
	LOSS [training: 0.22713804224430195 | validation: 0.23085320614638227]
	TIME [epoch: 8.14 sec]
EPOCH 232/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21530307188151934		[learning rate: 0.0041495]
		[batch 20/20] avg loss: 0.16418358323730303		[learning rate: 0.0041395]
	Learning Rate: 0.0041395
	LOSS [training: 0.18974332755941115 | validation: 0.1952287663386387]
	TIME [epoch: 8.16 sec]
EPOCH 233/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23872591447481417		[learning rate: 0.0041295]
		[batch 20/20] avg loss: 0.20841087156431595		[learning rate: 0.0041195]
	Learning Rate: 0.00411948
	LOSS [training: 0.22356839301956505 | validation: 0.21723744803359962]
	TIME [epoch: 8.14 sec]
EPOCH 234/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20426158007863648		[learning rate: 0.0041095]
		[batch 20/20] avg loss: 0.19271918623547		[learning rate: 0.0040996]
	Learning Rate: 0.00409956
	LOSS [training: 0.19849038315705322 | validation: 0.12559801070536572]
	TIME [epoch: 8.13 sec]
EPOCH 235/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21433443705446958		[learning rate: 0.0040896]
		[batch 20/20] avg loss: 0.21120659407800169		[learning rate: 0.0040797]
	Learning Rate: 0.00407973
	LOSS [training: 0.21277051556623566 | validation: 0.4353395320770686]
	TIME [epoch: 8.15 sec]
EPOCH 236/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24277242458023168		[learning rate: 0.0040699]
		[batch 20/20] avg loss: 0.1755777231082724		[learning rate: 0.00406]
	Learning Rate: 0.00406
	LOSS [training: 0.209175073844252 | validation: 0.47668314964511954]
	TIME [epoch: 8.17 sec]
EPOCH 237/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24958568970261785		[learning rate: 0.0040502]
		[batch 20/20] avg loss: 0.21217175956243567		[learning rate: 0.0040404]
	Learning Rate: 0.00404037
	LOSS [training: 0.2308787246325268 | validation: 0.12913831820076976]
	TIME [epoch: 8.14 sec]
EPOCH 238/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20653542985057474		[learning rate: 0.0040306]
		[batch 20/20] avg loss: 0.18844960644449493		[learning rate: 0.0040208]
	Learning Rate: 0.00402083
	LOSS [training: 0.19749251814753482 | validation: 0.12691458651843213]
	TIME [epoch: 8.13 sec]
EPOCH 239/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1498012534875146		[learning rate: 0.0040111]
		[batch 20/20] avg loss: 0.14907529893576288		[learning rate: 0.0040014]
	Learning Rate: 0.00400139
	LOSS [training: 0.14943827621163874 | validation: 0.1827312777451107]
	TIME [epoch: 8.15 sec]
EPOCH 240/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26992742028470873		[learning rate: 0.0039917]
		[batch 20/20] avg loss: 0.17362614166855725		[learning rate: 0.003982]
	Learning Rate: 0.00398204
	LOSS [training: 0.22177678097663303 | validation: 0.250215301491212]
	TIME [epoch: 8.17 sec]
EPOCH 241/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19359808772083653		[learning rate: 0.0039724]
		[batch 20/20] avg loss: 0.19056018090808569		[learning rate: 0.0039628]
	Learning Rate: 0.00396278
	LOSS [training: 0.1920791343144611 | validation: 0.09203764526433234]
	TIME [epoch: 8.15 sec]
EPOCH 242/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13410378376933518		[learning rate: 0.0039532]
		[batch 20/20] avg loss: 0.17798810477280913		[learning rate: 0.0039436]
	Learning Rate: 0.00394362
	LOSS [training: 0.15604594427107216 | validation: 0.10060305327400193]
	TIME [epoch: 8.15 sec]
EPOCH 243/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27078881484470096		[learning rate: 0.0039341]
		[batch 20/20] avg loss: 0.1945960271420321		[learning rate: 0.0039245]
	Learning Rate: 0.00392455
	LOSS [training: 0.23269242099336665 | validation: 0.19494424594076865]
	TIME [epoch: 8.14 sec]
EPOCH 244/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15391796770821012		[learning rate: 0.003915]
		[batch 20/20] avg loss: 0.19646762637079748		[learning rate: 0.0039056]
	Learning Rate: 0.00390557
	LOSS [training: 0.1751927970395038 | validation: 0.09849433289798337]
	TIME [epoch: 8.17 sec]
EPOCH 245/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19240217891546693		[learning rate: 0.0038961]
		[batch 20/20] avg loss: 0.17482010019367356		[learning rate: 0.0038867]
	Learning Rate: 0.00388668
	LOSS [training: 0.18361113955457026 | validation: 0.20577834086284474]
	TIME [epoch: 8.15 sec]
EPOCH 246/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1281475251539425		[learning rate: 0.0038773]
		[batch 20/20] avg loss: 0.16641983276679298		[learning rate: 0.0038679]
	Learning Rate: 0.00386789
	LOSS [training: 0.14728367896036776 | validation: 0.18196975980849833]
	TIME [epoch: 8.15 sec]
EPOCH 247/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3439171736525279		[learning rate: 0.0038585]
		[batch 20/20] avg loss: 0.16187985013542264		[learning rate: 0.0038492]
	Learning Rate: 0.00384918
	LOSS [training: 0.2528985118939752 | validation: 0.11411288433119504]
	TIME [epoch: 8.12 sec]
EPOCH 248/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.142120797287424		[learning rate: 0.0038399]
		[batch 20/20] avg loss: 0.17698649650746562		[learning rate: 0.0038306]
	Learning Rate: 0.00383057
	LOSS [training: 0.15955364689744478 | validation: 0.08982765074635439]
	TIME [epoch: 8.15 sec]
EPOCH 249/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11357248900276921		[learning rate: 0.0038213]
		[batch 20/20] avg loss: 0.21993222568307952		[learning rate: 0.003812]
	Learning Rate: 0.00381204
	LOSS [training: 0.16675235734292435 | validation: 0.2246957050615793]
	TIME [epoch: 8.13 sec]
EPOCH 250/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15774305645685174		[learning rate: 0.0038028]
		[batch 20/20] avg loss: 0.37777811180907694		[learning rate: 0.0037936]
	Learning Rate: 0.00379361
	LOSS [training: 0.26776058413296433 | validation: 0.1521693517701323]
	TIME [epoch: 8.13 sec]
EPOCH 251/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1750961268322197		[learning rate: 0.0037844]
		[batch 20/20] avg loss: 0.1857360438006475		[learning rate: 0.0037753]
	Learning Rate: 0.00377526
	LOSS [training: 0.1804160853164336 | validation: 0.2300808392032419]
	TIME [epoch: 8.13 sec]
EPOCH 252/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14025155215909502		[learning rate: 0.0037661]
		[batch 20/20] avg loss: 0.10692679964187395		[learning rate: 0.003757]
	Learning Rate: 0.00375701
	LOSS [training: 0.12358917590048446 | validation: 0.09156959388444795]
	TIME [epoch: 8.15 sec]
EPOCH 253/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14272058755143147		[learning rate: 0.0037479]
		[batch 20/20] avg loss: 0.18371927904451132		[learning rate: 0.0037388]
	Learning Rate: 0.00373884
	LOSS [training: 0.1632199332979714 | validation: 0.144393615724134]
	TIME [epoch: 8.13 sec]
EPOCH 254/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27837123200569475		[learning rate: 0.0037298]
		[batch 20/20] avg loss: 0.18969764485916557		[learning rate: 0.0037208]
	Learning Rate: 0.00372076
	LOSS [training: 0.23403443843243013 | validation: 0.13929435226156503]
	TIME [epoch: 8.12 sec]
EPOCH 255/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17015113508460716		[learning rate: 0.0037118]
		[batch 20/20] avg loss: 0.2512094377309501		[learning rate: 0.0037028]
	Learning Rate: 0.00370277
	LOSS [training: 0.21068028640777858 | validation: 0.1567504704102283]
	TIME [epoch: 8.12 sec]
EPOCH 256/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13462856674271104		[learning rate: 0.0036938]
		[batch 20/20] avg loss: 0.1450744095864917		[learning rate: 0.0036849]
	Learning Rate: 0.00368486
	LOSS [training: 0.13985148816460136 | validation: 0.09120283128655317]
	TIME [epoch: 8.12 sec]
EPOCH 257/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12096766997212818		[learning rate: 0.0036759]
		[batch 20/20] avg loss: 0.11552974474116773		[learning rate: 0.003667]
	Learning Rate: 0.00366704
	LOSS [training: 0.11824870735664797 | validation: 0.11225452027629236]
	TIME [epoch: 8.16 sec]
EPOCH 258/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20641732685890307		[learning rate: 0.0036582]
		[batch 20/20] avg loss: 0.18440226025374779		[learning rate: 0.0036493]
	Learning Rate: 0.00364931
	LOSS [training: 0.1954097935563254 | validation: 0.24495793862912735]
	TIME [epoch: 8.11 sec]
EPOCH 259/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21153788255650938		[learning rate: 0.0036405]
		[batch 20/20] avg loss: 0.1682644435871989		[learning rate: 0.0036317]
	Learning Rate: 0.00363166
	LOSS [training: 0.18990116307185412 | validation: 0.12168548518417019]
	TIME [epoch: 8.12 sec]
EPOCH 260/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16757806593235564		[learning rate: 0.0036229]
		[batch 20/20] avg loss: 0.21030511811336172		[learning rate: 0.0036141]
	Learning Rate: 0.0036141
	LOSS [training: 0.1889415920228587 | validation: 0.1598235271804903]
	TIME [epoch: 8.12 sec]
EPOCH 261/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14857056706231617		[learning rate: 0.0036053]
		[batch 20/20] avg loss: 0.1361084569928899		[learning rate: 0.0035966]
	Learning Rate: 0.00359662
	LOSS [training: 0.14233951202760303 | validation: 0.4005006762089386]
	TIME [epoch: 8.14 sec]
EPOCH 262/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17165811800247938		[learning rate: 0.0035879]
		[batch 20/20] avg loss: 0.14717341863124966		[learning rate: 0.0035792]
	Learning Rate: 0.00357923
	LOSS [training: 0.1594157683168645 | validation: 0.2409078562457303]
	TIME [epoch: 8.11 sec]
EPOCH 263/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14246869095451173		[learning rate: 0.0035706]
		[batch 20/20] avg loss: 0.2385298099686833		[learning rate: 0.0035619]
	Learning Rate: 0.00356192
	LOSS [training: 0.1904992504615975 | validation: 0.13340447645281556]
	TIME [epoch: 8.11 sec]
EPOCH 264/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12102445103331794		[learning rate: 0.0035533]
		[batch 20/20] avg loss: 0.19551543958674655		[learning rate: 0.0035447]
	Learning Rate: 0.0035447
	LOSS [training: 0.15826994531003225 | validation: 0.15137345794934423]
	TIME [epoch: 8.11 sec]
EPOCH 265/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15794396696215968		[learning rate: 0.0035361]
		[batch 20/20] avg loss: 0.18738110704391125		[learning rate: 0.0035276]
	Learning Rate: 0.00352755
	LOSS [training: 0.17266253700303547 | validation: 0.14001834099174315]
	TIME [epoch: 8.14 sec]
EPOCH 266/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15473764736771503		[learning rate: 0.003519]
		[batch 20/20] avg loss: 0.33782704130220625		[learning rate: 0.0035105]
	Learning Rate: 0.0035105
	LOSS [training: 0.2462823443349606 | validation: 0.1429364506518593]
	TIME [epoch: 8.12 sec]
EPOCH 267/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18730662987148367		[learning rate: 0.003502]
		[batch 20/20] avg loss: 0.13204663350061357		[learning rate: 0.0034935]
	Learning Rate: 0.00349352
	LOSS [training: 0.15967663168604862 | validation: 0.42614102755936994]
	TIME [epoch: 8.11 sec]
EPOCH 268/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19771368564977074		[learning rate: 0.0034851]
		[batch 20/20] avg loss: 0.23460279373208018		[learning rate: 0.0034766]
	Learning Rate: 0.00347663
	LOSS [training: 0.21615823969092546 | validation: 0.13559036840298308]
	TIME [epoch: 8.12 sec]
EPOCH 269/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19158802424959226		[learning rate: 0.0034682]
		[batch 20/20] avg loss: 0.14485737889302347		[learning rate: 0.0034598]
	Learning Rate: 0.00345981
	LOSS [training: 0.1682227015713079 | validation: 0.10428597781267072]
	TIME [epoch: 8.14 sec]
EPOCH 270/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1927418696819342		[learning rate: 0.0034514]
		[batch 20/20] avg loss: 0.16266212190605817		[learning rate: 0.0034431]
	Learning Rate: 0.00344308
	LOSS [training: 0.1777019957939962 | validation: 0.24647744005698272]
	TIME [epoch: 8.12 sec]
EPOCH 271/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15428805659561742		[learning rate: 0.0034347]
		[batch 20/20] avg loss: 0.1201011842705189		[learning rate: 0.0034264]
	Learning Rate: 0.00342643
	LOSS [training: 0.13719462043306818 | validation: 0.16273450935589645]
	TIME [epoch: 8.11 sec]
EPOCH 272/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19985284030128486		[learning rate: 0.0034181]
		[batch 20/20] avg loss: 0.14013265271989944		[learning rate: 0.0034099]
	Learning Rate: 0.00340986
	LOSS [training: 0.16999274651059215 | validation: 0.19807273520920626]
	TIME [epoch: 8.11 sec]
EPOCH 273/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19457396750715011		[learning rate: 0.0034016]
		[batch 20/20] avg loss: 0.16617016297573878		[learning rate: 0.0033934]
	Learning Rate: 0.00339337
	LOSS [training: 0.1803720652414444 | validation: 0.1640866367933049]
	TIME [epoch: 8.14 sec]
EPOCH 274/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18997485346927295		[learning rate: 0.0033852]
		[batch 20/20] avg loss: 0.1739831565771211		[learning rate: 0.003377]
	Learning Rate: 0.00337696
	LOSS [training: 0.18197900502319705 | validation: 0.2948790718879716]
	TIME [epoch: 8.13 sec]
EPOCH 275/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11935473296338103		[learning rate: 0.0033688]
		[batch 20/20] avg loss: 0.1416302955518711		[learning rate: 0.0033606]
	Learning Rate: 0.00336063
	LOSS [training: 0.1304925142576261 | validation: 0.35526548208959896]
	TIME [epoch: 8.12 sec]
EPOCH 276/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20987580253483032		[learning rate: 0.0033525]
		[batch 20/20] avg loss: 0.1371684157016067		[learning rate: 0.0033444]
	Learning Rate: 0.00334438
	LOSS [training: 0.17352210911821853 | validation: 0.09457685245442107]
	TIME [epoch: 8.12 sec]
EPOCH 277/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1630694555125989		[learning rate: 0.0033363]
		[batch 20/20] avg loss: 0.19375101303122952		[learning rate: 0.0033282]
	Learning Rate: 0.00332821
	LOSS [training: 0.17841023427191421 | validation: 0.08320665468649976]
	TIME [epoch: 8.15 sec]
EPOCH 278/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1686057030463422		[learning rate: 0.0033202]
		[batch 20/20] avg loss: 0.15431915144373903		[learning rate: 0.0033121]
	Learning Rate: 0.00331211
	LOSS [training: 0.16146242724504065 | validation: 0.32349443704567726]
	TIME [epoch: 8.11 sec]
EPOCH 279/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1303815966089266		[learning rate: 0.0033041]
		[batch 20/20] avg loss: 0.12380539034495822		[learning rate: 0.0032961]
	Learning Rate: 0.0032961
	LOSS [training: 0.12709349347694238 | validation: 0.46537549329775174]
	TIME [epoch: 8.11 sec]
EPOCH 280/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20211237594860082		[learning rate: 0.0032881]
		[batch 20/20] avg loss: 0.14355853540916347		[learning rate: 0.0032802]
	Learning Rate: 0.00328016
	LOSS [training: 0.17283545567888214 | validation: 0.12417278008710639]
	TIME [epoch: 8.11 sec]
EPOCH 281/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16381422609613455		[learning rate: 0.0032722]
		[batch 20/20] avg loss: 0.15814839421498753		[learning rate: 0.0032643]
	Learning Rate: 0.0032643
	LOSS [training: 0.160981310155561 | validation: 0.10493811628617468]
	TIME [epoch: 8.14 sec]
EPOCH 282/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14799258944287233		[learning rate: 0.0032564]
		[batch 20/20] avg loss: 0.14304077970825374		[learning rate: 0.0032485]
	Learning Rate: 0.00324851
	LOSS [training: 0.14551668457556305 | validation: 0.20902265865608044]
	TIME [epoch: 8.11 sec]
EPOCH 283/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15365688638770653		[learning rate: 0.0032406]
		[batch 20/20] avg loss: 0.1360884886096211		[learning rate: 0.0032328]
	Learning Rate: 0.0032328
	LOSS [training: 0.14487268749866383 | validation: 0.20889411220312737]
	TIME [epoch: 8.12 sec]
EPOCH 284/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13760812734045097		[learning rate: 0.003225]
		[batch 20/20] avg loss: 0.13747439817898677		[learning rate: 0.0032172]
	Learning Rate: 0.00321717
	LOSS [training: 0.1375412627597189 | validation: 0.14792814644074362]
	TIME [epoch: 8.12 sec]
EPOCH 285/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13609349481670732		[learning rate: 0.0032094]
		[batch 20/20] avg loss: 0.12052617605797292		[learning rate: 0.0032016]
	Learning Rate: 0.00320161
	LOSS [training: 0.12830983543734015 | validation: 0.1014025024548734]
	TIME [epoch: 8.14 sec]
EPOCH 286/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15667290609892456		[learning rate: 0.0031939]
		[batch 20/20] avg loss: 0.1391699455254403		[learning rate: 0.0031861]
	Learning Rate: 0.00318613
	LOSS [training: 0.14792142581218243 | validation: 0.08217929593714891]
	TIME [epoch: 8.11 sec]
EPOCH 287/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23240612443646044		[learning rate: 0.0031784]
		[batch 20/20] avg loss: 0.170027491604964		[learning rate: 0.0031707]
	Learning Rate: 0.00317072
	LOSS [training: 0.20121680802071223 | validation: 0.13379656651655641]
	TIME [epoch: 8.11 sec]
EPOCH 288/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12325495694633781		[learning rate: 0.003163]
		[batch 20/20] avg loss: 0.12795987373747325		[learning rate: 0.0031554]
	Learning Rate: 0.00315539
	LOSS [training: 0.1256074153419055 | validation: 0.04970170676270172]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_288.pth
	Model improved!!!
EPOCH 289/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12209257274659854		[learning rate: 0.0031477]
		[batch 20/20] avg loss: 0.19231730054415067		[learning rate: 0.0031401]
	Learning Rate: 0.00314013
	LOSS [training: 0.1572049366453746 | validation: 0.18682755830793968]
	TIME [epoch: 8.14 sec]
EPOCH 290/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1547034835423564		[learning rate: 0.0031325]
		[batch 20/20] avg loss: 0.12487502118760796		[learning rate: 0.0031249]
	Learning Rate: 0.00312494
	LOSS [training: 0.13978925236498216 | validation: 0.17525757919417298]
	TIME [epoch: 8.13 sec]
EPOCH 291/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17416844005419269		[learning rate: 0.0031174]
		[batch 20/20] avg loss: 0.13595294954729037		[learning rate: 0.0031098]
	Learning Rate: 0.00310983
	LOSS [training: 0.15506069480074156 | validation: 0.1219810741610175]
	TIME [epoch: 8.11 sec]
EPOCH 292/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11715612984052033		[learning rate: 0.0031023]
		[batch 20/20] avg loss: 0.18396533184799718		[learning rate: 0.0030948]
	Learning Rate: 0.00309479
	LOSS [training: 0.15056073084425875 | validation: 0.12392728540381583]
	TIME [epoch: 8.12 sec]
EPOCH 293/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21047201141791644		[learning rate: 0.0030873]
		[batch 20/20] avg loss: 0.14067285395347817		[learning rate: 0.0030798]
	Learning Rate: 0.00307983
	LOSS [training: 0.17557243268569728 | validation: 0.28394454806537484]
	TIME [epoch: 8.13 sec]
EPOCH 294/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20126620972187936		[learning rate: 0.0030724]
		[batch 20/20] avg loss: 0.12372157777804875		[learning rate: 0.0030649]
	Learning Rate: 0.00306493
	LOSS [training: 0.16249389374996404 | validation: 0.06726865859693981]
	TIME [epoch: 8.13 sec]
EPOCH 295/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14316237187431807		[learning rate: 0.0030575]
		[batch 20/20] avg loss: 0.1664600265552792		[learning rate: 0.0030501]
	Learning Rate: 0.00305011
	LOSS [training: 0.15481119921479863 | validation: 0.1563158918995337]
	TIME [epoch: 8.11 sec]
EPOCH 296/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1403495983885974		[learning rate: 0.0030427]
		[batch 20/20] avg loss: 0.12914438150887503		[learning rate: 0.0030354]
	Learning Rate: 0.00303536
	LOSS [training: 0.13474698994873618 | validation: 0.11448640769498777]
	TIME [epoch: 8.12 sec]
EPOCH 297/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1099007617377857		[learning rate: 0.003028]
		[batch 20/20] avg loss: 0.14597468541598052		[learning rate: 0.0030207]
	Learning Rate: 0.00302068
	LOSS [training: 0.1279377235768831 | validation: 0.42113339906685543]
	TIME [epoch: 8.15 sec]
EPOCH 298/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.171386446073541		[learning rate: 0.0030134]
		[batch 20/20] avg loss: 0.13634947425099891		[learning rate: 0.0030061]
	Learning Rate: 0.00300608
	LOSS [training: 0.15386796016226995 | validation: 0.11783977691614839]
	TIME [epoch: 8.13 sec]
EPOCH 299/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09886870474244731		[learning rate: 0.0029988]
		[batch 20/20] avg loss: 0.15064721290031008		[learning rate: 0.0029915]
	Learning Rate: 0.00299154
	LOSS [training: 0.1247579588213787 | validation: 0.08057661371504256]
	TIME [epoch: 8.12 sec]
EPOCH 300/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18212188126089376		[learning rate: 0.0029843]
		[batch 20/20] avg loss: 0.11627576733436804		[learning rate: 0.0029771]
	Learning Rate: 0.00297707
	LOSS [training: 0.1491988242976309 | validation: 0.11222502807392187]
	TIME [epoch: 8.12 sec]
EPOCH 301/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13510691478783626		[learning rate: 0.0029699]
		[batch 20/20] avg loss: 0.15662239641892883		[learning rate: 0.0029627]
	Learning Rate: 0.00296268
	LOSS [training: 0.14586465560338255 | validation: 0.18294302873303364]
	TIME [epoch: 8.14 sec]
EPOCH 302/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11530735414716657		[learning rate: 0.0029555]
		[batch 20/20] avg loss: 0.2113754994937156		[learning rate: 0.0029483]
	Learning Rate: 0.00294835
	LOSS [training: 0.16334142682044112 | validation: 0.21148532799361372]
	TIME [epoch: 8.12 sec]
EPOCH 303/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12859174342625704		[learning rate: 0.0029412]
		[batch 20/20] avg loss: 0.12049972968273408		[learning rate: 0.0029341]
	Learning Rate: 0.00293409
	LOSS [training: 0.12454573655449556 | validation: 0.29632428605238925]
	TIME [epoch: 8.11 sec]
EPOCH 304/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1660888739215334		[learning rate: 0.002927]
		[batch 20/20] avg loss: 0.13531963832381735		[learning rate: 0.0029199]
	Learning Rate: 0.0029199
	LOSS [training: 0.15070425612267538 | validation: 0.11316347171545416]
	TIME [epoch: 8.12 sec]
EPOCH 305/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1349205067875886		[learning rate: 0.0029128]
		[batch 20/20] avg loss: 0.16309139945235207		[learning rate: 0.0029058]
	Learning Rate: 0.00290578
	LOSS [training: 0.14900595311997034 | validation: 0.1010811548110798]
	TIME [epoch: 8.13 sec]
EPOCH 306/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14771860321417676		[learning rate: 0.0028987]
		[batch 20/20] avg loss: 0.12824979414799403		[learning rate: 0.0028917]
	Learning Rate: 0.00289173
	LOSS [training: 0.1379841986810854 | validation: 0.3887146121755798]
	TIME [epoch: 8.13 sec]
EPOCH 307/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21513194857217774		[learning rate: 0.0028847]
		[batch 20/20] avg loss: 0.13714388957423357		[learning rate: 0.0028777]
	Learning Rate: 0.00287775
	LOSS [training: 0.17613791907320564 | validation: 0.16184079813337277]
	TIME [epoch: 8.13 sec]
EPOCH 308/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13860770795183736		[learning rate: 0.0028708]
		[batch 20/20] avg loss: 0.12246819711455266		[learning rate: 0.0028638]
	Learning Rate: 0.00286383
	LOSS [training: 0.130537952533195 | validation: 0.11781354181002268]
	TIME [epoch: 8.11 sec]
EPOCH 309/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1683492149270888		[learning rate: 0.0028569]
		[batch 20/20] avg loss: 0.13523919261886236		[learning rate: 0.00285]
	Learning Rate: 0.00284998
	LOSS [training: 0.1517942037729756 | validation: 0.10700792082499277]
	TIME [epoch: 8.12 sec]
EPOCH 310/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10900266988431542		[learning rate: 0.0028431]
		[batch 20/20] avg loss: 0.1469297427579729		[learning rate: 0.0028362]
	Learning Rate: 0.0028362
	LOSS [training: 0.12796620632114414 | validation: 0.10786248189386664]
	TIME [epoch: 8.22 sec]
EPOCH 311/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12299504263165624		[learning rate: 0.0028293]
		[batch 20/20] avg loss: 0.11057197056872896		[learning rate: 0.0028225]
	Learning Rate: 0.00282248
	LOSS [training: 0.11678350660019257 | validation: 0.12409888966449135]
	TIME [epoch: 8.12 sec]
EPOCH 312/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17498553839501071		[learning rate: 0.0028157]
		[batch 20/20] avg loss: 0.14839953332562816		[learning rate: 0.0028088]
	Learning Rate: 0.00280884
	LOSS [training: 0.16169253586031943 | validation: 0.07777887455433921]
	TIME [epoch: 8.11 sec]
EPOCH 313/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13443462800636913		[learning rate: 0.002802]
		[batch 20/20] avg loss: 0.14649277148547854		[learning rate: 0.0027953]
	Learning Rate: 0.00279525
	LOSS [training: 0.14046369974592385 | validation: 0.1265217790395324]
	TIME [epoch: 8.11 sec]
EPOCH 314/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11851381564544287		[learning rate: 0.0027885]
		[batch 20/20] avg loss: 0.1482861331977377		[learning rate: 0.0027817]
	Learning Rate: 0.00278174
	LOSS [training: 0.1333999744215903 | validation: 0.072570003342535]
	TIME [epoch: 8.13 sec]
EPOCH 315/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13424560163625143		[learning rate: 0.002775]
		[batch 20/20] avg loss: 0.18031691525504018		[learning rate: 0.0027683]
	Learning Rate: 0.00276828
	LOSS [training: 0.1572812584456458 | validation: 0.14173474048248624]
	TIME [epoch: 8.11 sec]
EPOCH 316/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11867121260129085		[learning rate: 0.0027616]
		[batch 20/20] avg loss: 0.13137220256221888		[learning rate: 0.0027549]
	Learning Rate: 0.0027549
	LOSS [training: 0.12502170758175485 | validation: 0.5441428340629093]
	TIME [epoch: 8.11 sec]
EPOCH 317/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2242747270410356		[learning rate: 0.0027482]
		[batch 20/20] avg loss: 0.19778852486467288		[learning rate: 0.0027416]
	Learning Rate: 0.00274157
	LOSS [training: 0.21103162595285424 | validation: 0.1366749095256109]
	TIME [epoch: 8.11 sec]
EPOCH 318/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1948211522029111		[learning rate: 0.0027349]
		[batch 20/20] avg loss: 0.14202273179014066		[learning rate: 0.0027283]
	Learning Rate: 0.00272832
	LOSS [training: 0.16842194199652588 | validation: 0.15668027690091857]
	TIME [epoch: 8.14 sec]
EPOCH 319/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13184050042097858		[learning rate: 0.0027217]
		[batch 20/20] avg loss: 0.19762773067262818		[learning rate: 0.0027151]
	Learning Rate: 0.00271512
	LOSS [training: 0.16473411554680337 | validation: 0.06831539727142648]
	TIME [epoch: 8.11 sec]
EPOCH 320/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13802244551528448		[learning rate: 0.0027086]
		[batch 20/20] avg loss: 0.21521942350282278		[learning rate: 0.002702]
	Learning Rate: 0.00270199
	LOSS [training: 0.17662093450905364 | validation: 0.22477779142826376]
	TIME [epoch: 8.12 sec]
EPOCH 321/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12846192935900652		[learning rate: 0.0026955]
		[batch 20/20] avg loss: 0.15221778616555703		[learning rate: 0.0026889]
	Learning Rate: 0.00268893
	LOSS [training: 0.1403398577622818 | validation: 0.21332200135120594]
	TIME [epoch: 8.12 sec]
EPOCH 322/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14818465039459322		[learning rate: 0.0026824]
		[batch 20/20] avg loss: 0.17011492485014718		[learning rate: 0.0026759]
	Learning Rate: 0.00267592
	LOSS [training: 0.15914978762237017 | validation: 0.3567407080853296]
	TIME [epoch: 8.14 sec]
EPOCH 323/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19355100305767506		[learning rate: 0.0026694]
		[batch 20/20] avg loss: 0.1664015215892958		[learning rate: 0.002663]
	Learning Rate: 0.00266298
	LOSS [training: 0.17997626232348546 | validation: 0.16595730337683604]
	TIME [epoch: 8.12 sec]
EPOCH 324/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13169881233218472		[learning rate: 0.0026565]
		[batch 20/20] avg loss: 0.14335558214706967		[learning rate: 0.0026501]
	Learning Rate: 0.00265011
	LOSS [training: 0.1375271972396272 | validation: 0.1525812478216451]
	TIME [epoch: 8.11 sec]
EPOCH 325/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14063813336777908		[learning rate: 0.0026437]
		[batch 20/20] avg loss: 0.11687834095951277		[learning rate: 0.0026373]
	Learning Rate: 0.00263729
	LOSS [training: 0.1287582371636459 | validation: 0.0716496047833363]
	TIME [epoch: 8.12 sec]
EPOCH 326/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1394712757516021		[learning rate: 0.0026309]
		[batch 20/20] avg loss: 0.11416727308570898		[learning rate: 0.0026245]
	Learning Rate: 0.00262454
	LOSS [training: 0.12681927441865554 | validation: 0.10457394389053462]
	TIME [epoch: 8.14 sec]
EPOCH 327/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1590511041084346		[learning rate: 0.0026182]
		[batch 20/20] avg loss: 0.1188341907695655		[learning rate: 0.0026118]
	Learning Rate: 0.00261184
	LOSS [training: 0.1389426474390001 | validation: 0.0938887663055005]
	TIME [epoch: 8.13 sec]
EPOCH 328/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13734946179594693		[learning rate: 0.0026055]
		[batch 20/20] avg loss: 0.18839665420710405		[learning rate: 0.0025992]
	Learning Rate: 0.00259921
	LOSS [training: 0.16287305800152546 | validation: 0.09350934401772268]
	TIME [epoch: 8.11 sec]
EPOCH 329/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1059477191704586		[learning rate: 0.0025929]
		[batch 20/20] avg loss: 0.11135219163002692		[learning rate: 0.0025866]
	Learning Rate: 0.00258664
	LOSS [training: 0.10864995540024276 | validation: 0.08227701813400337]
	TIME [epoch: 8.12 sec]
EPOCH 330/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.096009608031974		[learning rate: 0.0025804]
		[batch 20/20] avg loss: 0.14981498195722467		[learning rate: 0.0025741]
	Learning Rate: 0.00257414
	LOSS [training: 0.12291229499459932 | validation: 0.10053787090540434]
	TIME [epoch: 8.14 sec]
EPOCH 331/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11331603629174733		[learning rate: 0.0025679]
		[batch 20/20] avg loss: 0.08034484485434126		[learning rate: 0.0025617]
	Learning Rate: 0.00256169
	LOSS [training: 0.0968304405730443 | validation: 0.07721886817899]
	TIME [epoch: 8.12 sec]
EPOCH 332/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09911111855898784		[learning rate: 0.0025555]
		[batch 20/20] avg loss: 0.08648247494186653		[learning rate: 0.0025493]
	Learning Rate: 0.0025493
	LOSS [training: 0.0927967967504272 | validation: 0.0675114574478872]
	TIME [epoch: 8.11 sec]
EPOCH 333/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1413181727202391		[learning rate: 0.0025431]
		[batch 20/20] avg loss: 0.14318573035250975		[learning rate: 0.002537]
	Learning Rate: 0.00253697
	LOSS [training: 0.14225195153637446 | validation: 0.15897235952952424]
	TIME [epoch: 8.12 sec]
EPOCH 334/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13394244411282377		[learning rate: 0.0025308]
		[batch 20/20] avg loss: 0.12397874921755372		[learning rate: 0.0025247]
	Learning Rate: 0.0025247
	LOSS [training: 0.12896059666518878 | validation: 0.19873449189763093]
	TIME [epoch: 8.13 sec]
EPOCH 335/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1128062525176123		[learning rate: 0.0025186]
		[batch 20/20] avg loss: 0.1039935818795702		[learning rate: 0.0025125]
	Learning Rate: 0.0025125
	LOSS [training: 0.10839991719859124 | validation: 0.06599878155736653]
	TIME [epoch: 8.12 sec]
EPOCH 336/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11335048218530056		[learning rate: 0.0025064]
		[batch 20/20] avg loss: 0.1153458784486812		[learning rate: 0.0025003]
	Learning Rate: 0.00250035
	LOSS [training: 0.11434818031699086 | validation: 0.09463241074060781]
	TIME [epoch: 8.11 sec]
EPOCH 337/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15628682303030045		[learning rate: 0.0024943]
		[batch 20/20] avg loss: 0.10908824565208293		[learning rate: 0.0024883]
	Learning Rate: 0.00248825
	LOSS [training: 0.13268753434119168 | validation: 0.06791673526664972]
	TIME [epoch: 8.12 sec]
EPOCH 338/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10392367054122167		[learning rate: 0.0024822]
		[batch 20/20] avg loss: 0.1543363343733964		[learning rate: 0.0024762]
	Learning Rate: 0.00247622
	LOSS [training: 0.12913000245730905 | validation: 0.08968405576315676]
	TIME [epoch: 8.14 sec]
EPOCH 339/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11524224665842672		[learning rate: 0.0024702]
		[batch 20/20] avg loss: 0.06331778605449527		[learning rate: 0.0024642]
	Learning Rate: 0.00246425
	LOSS [training: 0.089280016356461 | validation: 0.05265783052192018]
	TIME [epoch: 8.17 sec]
EPOCH 340/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09596434143130937		[learning rate: 0.0024583]
		[batch 20/20] avg loss: 0.09387413628391815		[learning rate: 0.0024523]
	Learning Rate: 0.00245233
	LOSS [training: 0.09491923885761377 | validation: 0.1046276502812135]
	TIME [epoch: 8.11 sec]
EPOCH 341/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11981823834192466		[learning rate: 0.0024464]
		[batch 20/20] avg loss: 0.0963336179767931		[learning rate: 0.0024405]
	Learning Rate: 0.00244047
	LOSS [training: 0.10807592815935887 | validation: 0.07798708149651642]
	TIME [epoch: 8.11 sec]
EPOCH 342/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09829126269839131		[learning rate: 0.0024346]
		[batch 20/20] avg loss: 0.10221089411625375		[learning rate: 0.0024287]
	Learning Rate: 0.00242867
	LOSS [training: 0.10025107840732253 | validation: 0.10501457021972924]
	TIME [epoch: 8.13 sec]
EPOCH 343/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12257326605324337		[learning rate: 0.0024228]
		[batch 20/20] avg loss: 0.09476191648845579		[learning rate: 0.0024169]
	Learning Rate: 0.00241693
	LOSS [training: 0.1086675912708496 | validation: 0.07390102997278615]
	TIME [epoch: 8.12 sec]
EPOCH 344/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13562464466181953		[learning rate: 0.0024111]
		[batch 20/20] avg loss: 0.09478031839100036		[learning rate: 0.0024052]
	Learning Rate: 0.00240524
	LOSS [training: 0.11520248152640993 | validation: 0.052890596646272464]
	TIME [epoch: 8.12 sec]
EPOCH 345/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12391815671149582		[learning rate: 0.0023994]
		[batch 20/20] avg loss: 0.11231111538356739		[learning rate: 0.0023936]
	Learning Rate: 0.00239361
	LOSS [training: 0.11811463604753161 | validation: 0.12155648380094196]
	TIME [epoch: 8.1 sec]
EPOCH 346/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08332150877771285		[learning rate: 0.0023878]
		[batch 20/20] avg loss: 0.13298764899183843		[learning rate: 0.002382]
	Learning Rate: 0.00238203
	LOSS [training: 0.10815457888477564 | validation: 0.11411819925068806]
	TIME [epoch: 8.13 sec]
EPOCH 347/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22671430153135805		[learning rate: 0.0023763]
		[batch 20/20] avg loss: 0.06958851026030598		[learning rate: 0.0023705]
	Learning Rate: 0.00237051
	LOSS [training: 0.14815140589583203 | validation: 0.0757313324353768]
	TIME [epoch: 8.12 sec]
EPOCH 348/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06429463062691788		[learning rate: 0.0023648]
		[batch 20/20] avg loss: 0.13234518752242458		[learning rate: 0.002359]
	Learning Rate: 0.00235905
	LOSS [training: 0.09831990907467122 | validation: 0.11498546999034567]
	TIME [epoch: 8.11 sec]
EPOCH 349/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15421700885055634		[learning rate: 0.0023533]
		[batch 20/20] avg loss: 0.08725233916741723		[learning rate: 0.0023476]
	Learning Rate: 0.00234764
	LOSS [training: 0.12073467400898677 | validation: 0.1046185073571728]
	TIME [epoch: 8.11 sec]
EPOCH 350/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12174237179993619		[learning rate: 0.002342]
		[batch 20/20] avg loss: 0.14128931726584965		[learning rate: 0.0023363]
	Learning Rate: 0.00233629
	LOSS [training: 0.13151584453289292 | validation: 0.16108029998250933]
	TIME [epoch: 8.12 sec]
EPOCH 351/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11490005494667302		[learning rate: 0.0023306]
		[batch 20/20] avg loss: 0.08134177065263534		[learning rate: 0.002325]
	Learning Rate: 0.00232499
	LOSS [training: 0.09812091279965415 | validation: 0.21984739777283396]
	TIME [epoch: 8.12 sec]
EPOCH 352/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16834613974754967		[learning rate: 0.0023194]
		[batch 20/20] avg loss: 0.11378989730724853		[learning rate: 0.0023137]
	Learning Rate: 0.00231375
	LOSS [training: 0.14106801852739909 | validation: 0.08851715906918012]
	TIME [epoch: 8.1 sec]
EPOCH 353/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10605593034177732		[learning rate: 0.0023081]
		[batch 20/20] avg loss: 0.13319723289286134		[learning rate: 0.0023026]
	Learning Rate: 0.00230256
	LOSS [training: 0.11962658161731932 | validation: 0.12204718565378919]
	TIME [epoch: 8.1 sec]
EPOCH 354/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10737314609527582		[learning rate: 0.002297]
		[batch 20/20] avg loss: 0.0886503169423781		[learning rate: 0.0022914]
	Learning Rate: 0.00229142
	LOSS [training: 0.09801173151882696 | validation: 0.11547797612603493]
	TIME [epoch: 8.1 sec]
EPOCH 355/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07794023522090969		[learning rate: 0.0022859]
		[batch 20/20] avg loss: 0.12740252306354666		[learning rate: 0.0022803]
	Learning Rate: 0.00228034
	LOSS [training: 0.10267137914222817 | validation: 0.1528729319595616]
	TIME [epoch: 8.14 sec]
EPOCH 356/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09650776535647725		[learning rate: 0.0022748]
		[batch 20/20] avg loss: 0.12473506663802203		[learning rate: 0.0022693]
	Learning Rate: 0.00226931
	LOSS [training: 0.11062141599724964 | validation: 0.14063562541378932]
	TIME [epoch: 8.11 sec]
EPOCH 357/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13632427667394922		[learning rate: 0.0022638]
		[batch 20/20] avg loss: 0.1307110068468968		[learning rate: 0.0022583]
	Learning Rate: 0.00225834
	LOSS [training: 0.13351764176042302 | validation: 0.11578695055850065]
	TIME [epoch: 8.1 sec]
EPOCH 358/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09612625448221743		[learning rate: 0.0022529]
		[batch 20/20] avg loss: 0.10246580911702903		[learning rate: 0.0022474]
	Learning Rate: 0.00224742
	LOSS [training: 0.09929603179962324 | validation: 0.0654296215907818]
	TIME [epoch: 8.1 sec]
EPOCH 359/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08128747809702054		[learning rate: 0.002242]
		[batch 20/20] avg loss: 0.11035835772177452		[learning rate: 0.0022366]
	Learning Rate: 0.00223655
	LOSS [training: 0.09582291790939754 | validation: 0.04821309927628998]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_359.pth
	Model improved!!!
EPOCH 360/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08961338089298478		[learning rate: 0.0022311]
		[batch 20/20] avg loss: 0.09209651090137322		[learning rate: 0.0022257]
	Learning Rate: 0.00222574
	LOSS [training: 0.09085494589717899 | validation: 0.12993984726595614]
	TIME [epoch: 8.11 sec]
EPOCH 361/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09362933205013008		[learning rate: 0.0022203]
		[batch 20/20] avg loss: 0.09650849810036519		[learning rate: 0.002215]
	Learning Rate: 0.00221497
	LOSS [training: 0.09506891507524763 | validation: 0.09712240411298673]
	TIME [epoch: 8.11 sec]
EPOCH 362/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1835895308745839		[learning rate: 0.0022096]
		[batch 20/20] avg loss: 0.12798532182850283		[learning rate: 0.0022043]
	Learning Rate: 0.00220426
	LOSS [training: 0.15578742635154336 | validation: 0.09390359543404543]
	TIME [epoch: 8.11 sec]
EPOCH 363/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12918345915631685		[learning rate: 0.0021989]
		[batch 20/20] avg loss: 0.1490349868191991		[learning rate: 0.0021936]
	Learning Rate: 0.0021936
	LOSS [training: 0.139109222987758 | validation: 0.26762795731446937]
	TIME [epoch: 8.12 sec]
EPOCH 364/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12370178175954702		[learning rate: 0.0021883]
		[batch 20/20] avg loss: 0.13544520598350446		[learning rate: 0.002183]
	Learning Rate: 0.00218299
	LOSS [training: 0.12957349387152572 | validation: 0.09903819722682258]
	TIME [epoch: 8.1 sec]
EPOCH 365/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10453858383048018		[learning rate: 0.0021777]
		[batch 20/20] avg loss: 0.08433714543317168		[learning rate: 0.0021724]
	Learning Rate: 0.00217244
	LOSS [training: 0.09443786463182594 | validation: 0.10857216514593654]
	TIME [epoch: 8.1 sec]
EPOCH 366/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09098475568546967		[learning rate: 0.0021672]
		[batch 20/20] avg loss: 0.10565431806425248		[learning rate: 0.0021619]
	Learning Rate: 0.00216193
	LOSS [training: 0.09831953687486109 | validation: 0.054538600611795246]
	TIME [epoch: 8.1 sec]
EPOCH 367/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09702221103382522		[learning rate: 0.0021567]
		[batch 20/20] avg loss: 0.1602618061247571		[learning rate: 0.0021515]
	Learning Rate: 0.00215148
	LOSS [training: 0.12864200857929114 | validation: 0.10134006288840966]
	TIME [epoch: 8.13 sec]
EPOCH 368/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10068453522635037		[learning rate: 0.0021463]
		[batch 20/20] avg loss: 0.10160890675845784		[learning rate: 0.0021411]
	Learning Rate: 0.00214107
	LOSS [training: 0.10114672099240411 | validation: 0.13472468972950294]
	TIME [epoch: 8.11 sec]
EPOCH 369/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13306176189522306		[learning rate: 0.0021359]
		[batch 20/20] avg loss: 0.11115747055430385		[learning rate: 0.0021307]
	Learning Rate: 0.00213072
	LOSS [training: 0.12210961622476346 | validation: 0.05308781128047503]
	TIME [epoch: 8.1 sec]
EPOCH 370/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08591331033975155		[learning rate: 0.0021256]
		[batch 20/20] avg loss: 0.1434427948930345		[learning rate: 0.0021204]
	Learning Rate: 0.00212042
	LOSS [training: 0.11467805261639301 | validation: 0.21205227462728474]
	TIME [epoch: 8.11 sec]
EPOCH 371/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13348375487403913		[learning rate: 0.0021153]
		[batch 20/20] avg loss: 0.1171878996817367		[learning rate: 0.0021102]
	Learning Rate: 0.00211016
	LOSS [training: 0.12533582727788792 | validation: 0.15313303985334156]
	TIME [epoch: 8.13 sec]
EPOCH 372/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08806945565211162		[learning rate: 0.0021051]
		[batch 20/20] avg loss: 0.14071623778518325		[learning rate: 0.0021]
	Learning Rate: 0.00209996
	LOSS [training: 0.11439284671864743 | validation: 0.17681101114961056]
	TIME [epoch: 8.11 sec]
EPOCH 373/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12063611205602942		[learning rate: 0.0020949]
		[batch 20/20] avg loss: 0.08845367217864412		[learning rate: 0.0020898]
	Learning Rate: 0.0020898
	LOSS [training: 0.10454489211733678 | validation: 0.05744777929401882]
	TIME [epoch: 8.1 sec]
EPOCH 374/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12636050412637348		[learning rate: 0.0020847]
		[batch 20/20] avg loss: 0.12232451735466718		[learning rate: 0.0020797]
	Learning Rate: 0.0020797
	LOSS [training: 0.12434251074052034 | validation: 0.06309685516263475]
	TIME [epoch: 8.11 sec]
EPOCH 375/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09333844333810577		[learning rate: 0.0020747]
		[batch 20/20] avg loss: 0.09757016487686672		[learning rate: 0.0020696]
	Learning Rate: 0.00206964
	LOSS [training: 0.09545430410748622 | validation: 0.02891107116545093]
	TIME [epoch: 8.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_375.pth
	Model improved!!!
EPOCH 376/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0853947175277834		[learning rate: 0.0020646]
		[batch 20/20] avg loss: 0.12950005552053412		[learning rate: 0.0020596]
	Learning Rate: 0.00205963
	LOSS [training: 0.10744738652415875 | validation: 0.045051221016001376]
	TIME [epoch: 8.11 sec]
EPOCH 377/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0758525182444341		[learning rate: 0.0020546]
		[batch 20/20] avg loss: 0.0866527717040448		[learning rate: 0.0020497]
	Learning Rate: 0.00204967
	LOSS [training: 0.08125264497423944 | validation: 0.07739089870954148]
	TIME [epoch: 8.11 sec]
EPOCH 378/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13386605533839718		[learning rate: 0.0020447]
		[batch 20/20] avg loss: 0.10387812574675544		[learning rate: 0.0020398]
	Learning Rate: 0.00203976
	LOSS [training: 0.1188720905425763 | validation: 0.044823144726196334]
	TIME [epoch: 8.11 sec]
EPOCH 379/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11030566012460363		[learning rate: 0.0020348]
		[batch 20/20] avg loss: 0.12445868218056573		[learning rate: 0.0020299]
	Learning Rate: 0.0020299
	LOSS [training: 0.11738217115258469 | validation: 0.045568133779613634]
	TIME [epoch: 8.13 sec]
EPOCH 380/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08124461251171716		[learning rate: 0.002025]
		[batch 20/20] avg loss: 0.07409160807700427		[learning rate: 0.0020201]
	Learning Rate: 0.00202008
	LOSS [training: 0.07766811029436071 | validation: 0.0951021708696735]
	TIME [epoch: 8.11 sec]
EPOCH 381/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08975953023147946		[learning rate: 0.0020152]
		[batch 20/20] avg loss: 0.14594516786642564		[learning rate: 0.0020103]
	Learning Rate: 0.00201031
	LOSS [training: 0.11785234904895255 | validation: 0.06892001023644848]
	TIME [epoch: 8.1 sec]
EPOCH 382/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06316755236739482		[learning rate: 0.0020054]
		[batch 20/20] avg loss: 0.1660550253816099		[learning rate: 0.0020006]
	Learning Rate: 0.00200059
	LOSS [training: 0.11461128887450237 | validation: 0.10695749096486909]
	TIME [epoch: 8.1 sec]
EPOCH 383/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10196232048311675		[learning rate: 0.0019957]
		[batch 20/20] avg loss: 0.05915068176070282		[learning rate: 0.0019909]
	Learning Rate: 0.00199091
	LOSS [training: 0.08055650112190979 | validation: 0.04789480409867316]
	TIME [epoch: 8.13 sec]
EPOCH 384/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08713087676753044		[learning rate: 0.0019861]
		[batch 20/20] avg loss: 0.08974741213744794		[learning rate: 0.0019813]
	Learning Rate: 0.00198129
	LOSS [training: 0.0884391444524892 | validation: 0.04950812862744603]
	TIME [epoch: 8.12 sec]
EPOCH 385/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12288685972747868		[learning rate: 0.0019765]
		[batch 20/20] avg loss: 0.12398035268042813		[learning rate: 0.0019717]
	Learning Rate: 0.00197171
	LOSS [training: 0.12343360620395341 | validation: 0.048814052290982735]
	TIME [epoch: 8.1 sec]
EPOCH 386/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11331398143677193		[learning rate: 0.0019669]
		[batch 20/20] avg loss: 0.08493820028982094		[learning rate: 0.0019622]
	Learning Rate: 0.00196217
	LOSS [training: 0.09912609086329643 | validation: 0.04068323182991926]
	TIME [epoch: 8.1 sec]
EPOCH 387/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10150946875723266		[learning rate: 0.0019574]
		[batch 20/20] avg loss: 0.09437220262437483		[learning rate: 0.0019527]
	Learning Rate: 0.00195268
	LOSS [training: 0.09794083569080374 | validation: 0.09323102473075977]
	TIME [epoch: 8.12 sec]
EPOCH 388/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09432303041427947		[learning rate: 0.001948]
		[batch 20/20] avg loss: 0.10564987577792775		[learning rate: 0.0019432]
	Learning Rate: 0.00194324
	LOSS [training: 0.09998645309610359 | validation: 0.12156598159977343]
	TIME [epoch: 8.11 sec]
EPOCH 389/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18962002096939595		[learning rate: 0.0019385]
		[batch 20/20] avg loss: 0.11360534423234032		[learning rate: 0.0019338]
	Learning Rate: 0.00193384
	LOSS [training: 0.15161268260086816 | validation: 0.055091417962763956]
	TIME [epoch: 8.11 sec]
EPOCH 390/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08704044511776418		[learning rate: 0.0019292]
		[batch 20/20] avg loss: 0.11224770718886719		[learning rate: 0.0019245]
	Learning Rate: 0.00192449
	LOSS [training: 0.09964407615331568 | validation: 0.12635009309803316]
	TIME [epoch: 8.1 sec]
EPOCH 391/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07059838126642756		[learning rate: 0.0019198]
		[batch 20/20] avg loss: 0.07710442972233708		[learning rate: 0.0019152]
	Learning Rate: 0.00191518
	LOSS [training: 0.07385140549438232 | validation: 0.06089869515081832]
	TIME [epoch: 8.11 sec]
EPOCH 392/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07881722659003894		[learning rate: 0.0019105]
		[batch 20/20] avg loss: 0.15350619901048207		[learning rate: 0.0019059]
	Learning Rate: 0.00190592
	LOSS [training: 0.11616171280026051 | validation: 0.14637017298554394]
	TIME [epoch: 8.12 sec]
EPOCH 393/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08976226260050908		[learning rate: 0.0019013]
		[batch 20/20] avg loss: 0.1480865029895152		[learning rate: 0.0018967]
	Learning Rate: 0.00189671
	LOSS [training: 0.11892438279501213 | validation: 0.16601026988439843]
	TIME [epoch: 8.11 sec]
EPOCH 394/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0850926305886969		[learning rate: 0.0018921]
		[batch 20/20] avg loss: 0.10391802824583632		[learning rate: 0.0018875]
	Learning Rate: 0.00188753
	LOSS [training: 0.09450532941726661 | validation: 0.1925764646183501]
	TIME [epoch: 8.1 sec]
EPOCH 395/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13819503385365567		[learning rate: 0.001883]
		[batch 20/20] avg loss: 0.08529155122266627		[learning rate: 0.0018784]
	Learning Rate: 0.00187841
	LOSS [training: 0.11174329253816095 | validation: 0.11544615882667303]
	TIME [epoch: 8.12 sec]
EPOCH 396/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11252243918983998		[learning rate: 0.0018739]
		[batch 20/20] avg loss: 0.10897249826961013		[learning rate: 0.0018693]
	Learning Rate: 0.00186932
	LOSS [training: 0.11074746872972507 | validation: 0.08598520342352055]
	TIME [epoch: 8.12 sec]
EPOCH 397/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07600470644827595		[learning rate: 0.0018648]
		[batch 20/20] avg loss: 0.09262035639634217		[learning rate: 0.0018603]
	Learning Rate: 0.00186028
	LOSS [training: 0.08431253142230907 | validation: 0.09267088899404255]
	TIME [epoch: 8.11 sec]
EPOCH 398/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08678409575322507		[learning rate: 0.0018558]
		[batch 20/20] avg loss: 0.0784841075625395		[learning rate: 0.0018513]
	Learning Rate: 0.00185129
	LOSS [training: 0.08263410165788228 | validation: 0.056377209998024654]
	TIME [epoch: 8.11 sec]
EPOCH 399/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10936878215314631		[learning rate: 0.0018468]
		[batch 20/20] avg loss: 0.10671125196417641		[learning rate: 0.0018423]
	Learning Rate: 0.00184233
	LOSS [training: 0.10804001705866137 | validation: 0.051046699148522875]
	TIME [epoch: 8.11 sec]
EPOCH 400/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08687438005165281		[learning rate: 0.0018379]
		[batch 20/20] avg loss: 0.08546892562802631		[learning rate: 0.0018334]
	Learning Rate: 0.00183343
	LOSS [training: 0.08617165283983956 | validation: 0.3019886485348953]
	TIME [epoch: 8.12 sec]
EPOCH 401/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15202748449317718		[learning rate: 0.001829]
		[batch 20/20] avg loss: 0.10039597273024277		[learning rate: 0.0018246]
	Learning Rate: 0.00182456
	LOSS [training: 0.12621172861170998 | validation: 0.06827642915645078]
	TIME [epoch: 8.11 sec]
EPOCH 402/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12134282141913463		[learning rate: 0.0018201]
		[batch 20/20] avg loss: 0.06579673485137263		[learning rate: 0.0018157]
	Learning Rate: 0.00181574
	LOSS [training: 0.09356977813525363 | validation: 0.06260177498746625]
	TIME [epoch: 8.1 sec]
EPOCH 403/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07620027546907716		[learning rate: 0.0018113]
		[batch 20/20] avg loss: 0.08222463977952507		[learning rate: 0.001807]
	Learning Rate: 0.00180696
	LOSS [training: 0.07921245762430111 | validation: 0.11061325600281183]
	TIME [epoch: 8.11 sec]
EPOCH 404/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09455782140831659		[learning rate: 0.0018026]
		[batch 20/20] avg loss: 0.10210249897250057		[learning rate: 0.0017982]
	Learning Rate: 0.00179822
	LOSS [training: 0.09833016019040859 | validation: 0.12013704762989255]
	TIME [epoch: 8.13 sec]
EPOCH 405/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08249218883776203		[learning rate: 0.0017939]
		[batch 20/20] avg loss: 0.08611396595445373		[learning rate: 0.0017895]
	Learning Rate: 0.00178952
	LOSS [training: 0.08430307739610786 | validation: 0.13939318940568332]
	TIME [epoch: 8.11 sec]
EPOCH 406/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09116268746817785		[learning rate: 0.0017852]
		[batch 20/20] avg loss: 0.10973863415914606		[learning rate: 0.0017809]
	Learning Rate: 0.00178087
	LOSS [training: 0.10045066081366194 | validation: 0.11974097277555534]
	TIME [epoch: 8.11 sec]
EPOCH 407/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09081692228665751		[learning rate: 0.0017766]
		[batch 20/20] avg loss: 0.07926019470655209		[learning rate: 0.0017723]
	Learning Rate: 0.00177226
	LOSS [training: 0.0850385584966048 | validation: 0.048452744205664054]
	TIME [epoch: 8.11 sec]
EPOCH 408/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12276120521819185		[learning rate: 0.001768]
		[batch 20/20] avg loss: 0.08113626430122739		[learning rate: 0.0017637]
	Learning Rate: 0.00176369
	LOSS [training: 0.10194873475970963 | validation: 0.06594841767074419]
	TIME [epoch: 8.13 sec]
EPOCH 409/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10784671052164085		[learning rate: 0.0017594]
		[batch 20/20] avg loss: 0.0994737360938468		[learning rate: 0.0017552]
	Learning Rate: 0.00175516
	LOSS [training: 0.1036602233077438 | validation: 0.08535308168643305]
	TIME [epoch: 8.1 sec]
EPOCH 410/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0818561374170246		[learning rate: 0.0017509]
		[batch 20/20] avg loss: 0.11852492445306329		[learning rate: 0.0017467]
	Learning Rate: 0.00174667
	LOSS [training: 0.10019053093504396 | validation: 0.09158885933640931]
	TIME [epoch: 8.11 sec]
EPOCH 411/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15212379654405675		[learning rate: 0.0017424]
		[batch 20/20] avg loss: 0.126016897408404		[learning rate: 0.0017382]
	Learning Rate: 0.00173822
	LOSS [training: 0.13907034697623033 | validation: 0.04275466618278066]
	TIME [epoch: 8.11 sec]
EPOCH 412/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06406216942711968		[learning rate: 0.001734]
		[batch 20/20] avg loss: 0.10350734481085606		[learning rate: 0.0017298]
	Learning Rate: 0.00172982
	LOSS [training: 0.08378475711898788 | validation: 0.07250424836234176]
	TIME [epoch: 8.13 sec]
EPOCH 413/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07509052776187808		[learning rate: 0.0017256]
		[batch 20/20] avg loss: 0.09212906090513992		[learning rate: 0.0017215]
	Learning Rate: 0.00172145
	LOSS [training: 0.08360979433350899 | validation: 0.15837326643309088]
	TIME [epoch: 8.11 sec]
EPOCH 414/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11958753916352165		[learning rate: 0.0017173]
		[batch 20/20] avg loss: 0.08796852188245943		[learning rate: 0.0017131]
	Learning Rate: 0.00171313
	LOSS [training: 0.10377803052299053 | validation: 0.13765064103816932]
	TIME [epoch: 8.11 sec]
EPOCH 415/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07347490944216314		[learning rate: 0.001709]
		[batch 20/20] avg loss: 0.08558036746532177		[learning rate: 0.0017048]
	Learning Rate: 0.00170484
	LOSS [training: 0.07952763845374247 | validation: 0.1582767838040723]
	TIME [epoch: 8.11 sec]
EPOCH 416/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0883720249137556		[learning rate: 0.0017007]
		[batch 20/20] avg loss: 0.12150534278180936		[learning rate: 0.0016966]
	Learning Rate: 0.0016966
	LOSS [training: 0.10493868384778246 | validation: 0.03326511866751699]
	TIME [epoch: 8.14 sec]
EPOCH 417/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07812869107187176		[learning rate: 0.0016925]
		[batch 20/20] avg loss: 0.09259526812498427		[learning rate: 0.0016884]
	Learning Rate: 0.00168839
	LOSS [training: 0.085361979598428 | validation: 0.096682884552457]
	TIME [epoch: 8.11 sec]
EPOCH 418/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07965373854427635		[learning rate: 0.0016843]
		[batch 20/20] avg loss: 0.10363852464920506		[learning rate: 0.0016802]
	Learning Rate: 0.00168023
	LOSS [training: 0.0916461315967407 | validation: 0.07872717621633486]
	TIME [epoch: 8.11 sec]
EPOCH 419/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10524880498820277		[learning rate: 0.0016762]
		[batch 20/20] avg loss: 0.05623527086227753		[learning rate: 0.0016721]
	Learning Rate: 0.0016721
	LOSS [training: 0.08074203792524016 | validation: 0.19042534593138316]
	TIME [epoch: 8.11 sec]
EPOCH 420/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.126319928349031		[learning rate: 0.0016681]
		[batch 20/20] avg loss: 0.07347579479409257		[learning rate: 0.001664]
	Learning Rate: 0.00166402
	LOSS [training: 0.09989786157156179 | validation: 0.04348089136008271]
	TIME [epoch: 8.13 sec]
EPOCH 421/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11419953034922894		[learning rate: 0.00166]
		[batch 20/20] avg loss: 0.08428853320476583		[learning rate: 0.001656]
	Learning Rate: 0.00165597
	LOSS [training: 0.0992440317769974 | validation: 0.04600566023914237]
	TIME [epoch: 8.1 sec]
EPOCH 422/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04977855342132831		[learning rate: 0.001652]
		[batch 20/20] avg loss: 0.05338017721851908		[learning rate: 0.001648]
	Learning Rate: 0.00164796
	LOSS [training: 0.0515793653199237 | validation: 0.05907875587713758]
	TIME [epoch: 8.11 sec]
EPOCH 423/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0729953409667595		[learning rate: 0.001644]
		[batch 20/20] avg loss: 0.0940596801086871		[learning rate: 0.00164]
	Learning Rate: 0.00163999
	LOSS [training: 0.08352751053772331 | validation: 0.030397122922224955]
	TIME [epoch: 8.11 sec]
EPOCH 424/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12573570965753628		[learning rate: 0.001636]
		[batch 20/20] avg loss: 0.10217809409794007		[learning rate: 0.0016321]
	Learning Rate: 0.00163206
	LOSS [training: 0.11395690187773817 | validation: 0.05883070402226336]
	TIME [epoch: 8.13 sec]
EPOCH 425/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21096757196710075		[learning rate: 0.0016281]
		[batch 20/20] avg loss: 0.08220684754718242		[learning rate: 0.0016242]
	Learning Rate: 0.00162417
	LOSS [training: 0.1465872097571416 | validation: 0.07869291685829845]
	TIME [epoch: 8.11 sec]
EPOCH 426/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09087625578960605		[learning rate: 0.0016202]
		[batch 20/20] avg loss: 0.08210648615758642		[learning rate: 0.0016163]
	Learning Rate: 0.00161632
	LOSS [training: 0.08649137097359624 | validation: 0.07696944590428033]
	TIME [epoch: 8.11 sec]
EPOCH 427/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08342438964675025		[learning rate: 0.0016124]
		[batch 20/20] avg loss: 0.05960561683172132		[learning rate: 0.0016085]
	Learning Rate: 0.0016085
	LOSS [training: 0.07151500323923579 | validation: 0.10941834565604822]
	TIME [epoch: 8.11 sec]
EPOCH 428/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06460781168283627		[learning rate: 0.0016046]
		[batch 20/20] avg loss: 0.0975046703878871		[learning rate: 0.0016007]
	Learning Rate: 0.00160072
	LOSS [training: 0.08105624103536167 | validation: 0.035455729014672144]
	TIME [epoch: 8.13 sec]
EPOCH 429/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08133994090021099		[learning rate: 0.0015968]
		[batch 20/20] avg loss: 0.07827812511722736		[learning rate: 0.001593]
	Learning Rate: 0.00159298
	LOSS [training: 0.0798090330087192 | validation: 0.08286973733933349]
	TIME [epoch: 8.12 sec]
EPOCH 430/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09015495098017774		[learning rate: 0.0015891]
		[batch 20/20] avg loss: 0.1045458659867388		[learning rate: 0.0015853]
	Learning Rate: 0.00158528
	LOSS [training: 0.09735040848345825 | validation: 0.07223577911279583]
	TIME [epoch: 8.11 sec]
EPOCH 431/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07950994833747319		[learning rate: 0.0015814]
		[batch 20/20] avg loss: 0.07493258925267074		[learning rate: 0.0015776]
	Learning Rate: 0.00157761
	LOSS [training: 0.07722126879507196 | validation: 0.06858340352336953]
	TIME [epoch: 8.11 sec]
EPOCH 432/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07845147034549657		[learning rate: 0.0015738]
		[batch 20/20] avg loss: 0.06605387793406739		[learning rate: 0.00157]
	Learning Rate: 0.00156998
	LOSS [training: 0.07225267413978198 | validation: 0.059706558078666344]
	TIME [epoch: 8.13 sec]
EPOCH 433/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07183961738435282		[learning rate: 0.0015662]
		[batch 20/20] avg loss: 0.08116528494859691		[learning rate: 0.0015624]
	Learning Rate: 0.00156239
	LOSS [training: 0.07650245116647487 | validation: 0.04907939940675706]
	TIME [epoch: 8.12 sec]
EPOCH 434/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08734127649023035		[learning rate: 0.0015586]
		[batch 20/20] avg loss: 0.09445059763677752		[learning rate: 0.0015548]
	Learning Rate: 0.00155483
	LOSS [training: 0.09089593706350393 | validation: 0.05073761550862526]
	TIME [epoch: 8.12 sec]
EPOCH 435/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06813046293648159		[learning rate: 0.0015511]
		[batch 20/20] avg loss: 0.0758147286023874		[learning rate: 0.0015473]
	Learning Rate: 0.00154732
	LOSS [training: 0.07197259576943452 | validation: 0.031075270359911103]
	TIME [epoch: 8.11 sec]
EPOCH 436/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09735457467379173		[learning rate: 0.0015436]
		[batch 20/20] avg loss: 0.08196658347176901		[learning rate: 0.0015398]
	Learning Rate: 0.00153983
	LOSS [training: 0.08966057907278037 | validation: 0.12505385814239103]
	TIME [epoch: 8.14 sec]
EPOCH 437/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10389652940935765		[learning rate: 0.0015361]
		[batch 20/20] avg loss: 0.06362985441170385		[learning rate: 0.0015324]
	Learning Rate: 0.00153239
	LOSS [training: 0.08376319191053076 | validation: 0.08051224375342697]
	TIME [epoch: 8.12 sec]
EPOCH 438/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.057137174807435555		[learning rate: 0.0015287]
		[batch 20/20] avg loss: 0.05788911031704743		[learning rate: 0.001525]
	Learning Rate: 0.00152498
	LOSS [training: 0.05751314256224147 | validation: 0.05274225444525445]
	TIME [epoch: 8.11 sec]
EPOCH 439/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07094988118277368		[learning rate: 0.0015213]
		[batch 20/20] avg loss: 0.09278934692091118		[learning rate: 0.0015176]
	Learning Rate: 0.0015176
	LOSS [training: 0.08186961405184243 | validation: 0.11317257274589931]
	TIME [epoch: 8.11 sec]
EPOCH 440/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09997762066985727		[learning rate: 0.0015139]
		[batch 20/20] avg loss: 0.09566269822162836		[learning rate: 0.0015103]
	Learning Rate: 0.00151026
	LOSS [training: 0.09782015944574282 | validation: 0.05805291123592902]
	TIME [epoch: 8.12 sec]
EPOCH 441/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07765015101381698		[learning rate: 0.0015066]
		[batch 20/20] avg loss: 0.0757912990772556		[learning rate: 0.001503]
	Learning Rate: 0.00150296
	LOSS [training: 0.07672072504553629 | validation: 0.0521793790669302]
	TIME [epoch: 8.11 sec]
EPOCH 442/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06414289603313339		[learning rate: 0.0014993]
		[batch 20/20] avg loss: 0.08584840959458427		[learning rate: 0.0014957]
	Learning Rate: 0.00149569
	LOSS [training: 0.07499565281385884 | validation: 0.05277050413380061]
	TIME [epoch: 8.11 sec]
EPOCH 443/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09875854978536111		[learning rate: 0.0014921]
		[batch 20/20] avg loss: 0.07687584200014808		[learning rate: 0.0014885]
	Learning Rate: 0.00148846
	LOSS [training: 0.0878171958927546 | validation: 0.06641897825928567]
	TIME [epoch: 8.11 sec]
EPOCH 444/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05789577801365381		[learning rate: 0.0014849]
		[batch 20/20] avg loss: 0.061872269977869855		[learning rate: 0.0014813]
	Learning Rate: 0.00148126
	LOSS [training: 0.05988402399576185 | validation: 0.1038030470107561]
	TIME [epoch: 8.11 sec]
EPOCH 445/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09796887557774678		[learning rate: 0.0014777]
		[batch 20/20] avg loss: 0.11080751226776173		[learning rate: 0.0014741]
	Learning Rate: 0.0014741
	LOSS [training: 0.10438819392275427 | validation: 0.11280506156747479]
	TIME [epoch: 8.13 sec]
EPOCH 446/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09500531287449651		[learning rate: 0.0014705]
		[batch 20/20] avg loss: 0.051366234086190274		[learning rate: 0.001467]
	Learning Rate: 0.00146697
	LOSS [training: 0.0731857734803434 | validation: 0.07383978976216149]
	TIME [epoch: 8.1 sec]
EPOCH 447/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07670805301096299		[learning rate: 0.0014634]
		[batch 20/20] avg loss: 0.11648696553303602		[learning rate: 0.0014599]
	Learning Rate: 0.00145988
	LOSS [training: 0.09659750927199948 | validation: 0.14389274618537526]
	TIME [epoch: 8.1 sec]
EPOCH 448/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07375506344002603		[learning rate: 0.0014563]
		[batch 20/20] avg loss: 0.07572014557878001		[learning rate: 0.0014528]
	Learning Rate: 0.00145282
	LOSS [training: 0.07473760450940302 | validation: 0.06920238800862948]
	TIME [epoch: 8.1 sec]
EPOCH 449/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07868469203968706		[learning rate: 0.0014493]
		[batch 20/20] avg loss: 0.07586327251309047		[learning rate: 0.0014458]
	Learning Rate: 0.00144579
	LOSS [training: 0.07727398227638878 | validation: 0.08830647210734067]
	TIME [epoch: 8.13 sec]
EPOCH 450/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0961541120648692		[learning rate: 0.0014423]
		[batch 20/20] avg loss: 0.1343978246266923		[learning rate: 0.0014388]
	Learning Rate: 0.0014388
	LOSS [training: 0.11527596834578076 | validation: 0.04385694744832437]
	TIME [epoch: 8.1 sec]
EPOCH 451/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0550983534093065		[learning rate: 0.0014353]
		[batch 20/20] avg loss: 0.06706978697065244		[learning rate: 0.0014318]
	Learning Rate: 0.00143184
	LOSS [training: 0.06108407018997948 | validation: 0.09988860755447815]
	TIME [epoch: 8.1 sec]
EPOCH 452/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.082534391259238		[learning rate: 0.0014284]
		[batch 20/20] avg loss: 0.06455662742372056		[learning rate: 0.0014249]
	Learning Rate: 0.00142492
	LOSS [training: 0.07354550934147928 | validation: 0.06091916079122196]
	TIME [epoch: 8.1 sec]
EPOCH 453/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0720451575641274		[learning rate: 0.0014215]
		[batch 20/20] avg loss: 0.11620044128481419		[learning rate: 0.001418]
	Learning Rate: 0.00141803
	LOSS [training: 0.0941227994244708 | validation: 0.08911268041683792]
	TIME [epoch: 8.13 sec]
EPOCH 454/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08245125234447655		[learning rate: 0.0014146]
		[batch 20/20] avg loss: 0.08416856113425414		[learning rate: 0.0014112]
	Learning Rate: 0.00141117
	LOSS [training: 0.08330990673936534 | validation: 0.06567986095434265]
	TIME [epoch: 8.11 sec]
EPOCH 455/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10095559840952695		[learning rate: 0.0014078]
		[batch 20/20] avg loss: 0.07269890318729477		[learning rate: 0.0014043]
	Learning Rate: 0.00140434
	LOSS [training: 0.08682725079841087 | validation: 0.07824781928567917]
	TIME [epoch: 8.1 sec]
EPOCH 456/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07901815840661938		[learning rate: 0.0014009]
		[batch 20/20] avg loss: 0.08262553855151974		[learning rate: 0.0013976]
	Learning Rate: 0.00139755
	LOSS [training: 0.08082184847906956 | validation: 0.14394704694783952]
	TIME [epoch: 8.11 sec]
EPOCH 457/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10860897871758055		[learning rate: 0.0013942]
		[batch 20/20] avg loss: 0.07726026088676		[learning rate: 0.0013908]
	Learning Rate: 0.0013908
	LOSS [training: 0.09293461980217027 | validation: 0.04464362561290218]
	TIME [epoch: 8.12 sec]
EPOCH 458/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06622587914130558		[learning rate: 0.0013874]
		[batch 20/20] avg loss: 0.06940003574111368		[learning rate: 0.0013841]
	Learning Rate: 0.00138407
	LOSS [training: 0.06781295744120963 | validation: 0.1571492492420497]
	TIME [epoch: 8.11 sec]
EPOCH 459/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10997444474946454		[learning rate: 0.0013807]
		[batch 20/20] avg loss: 0.09102878697801917		[learning rate: 0.0013774]
	Learning Rate: 0.00137738
	LOSS [training: 0.10050161586374184 | validation: 0.10592803043407968]
	TIME [epoch: 8.11 sec]
EPOCH 460/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06731301725969631		[learning rate: 0.001374]
		[batch 20/20] avg loss: 0.08676298292389702		[learning rate: 0.0013707]
	Learning Rate: 0.00137072
	LOSS [training: 0.07703800009179665 | validation: 0.10240091362588746]
	TIME [epoch: 8.11 sec]
EPOCH 461/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0852198990649189		[learning rate: 0.0013674]
		[batch 20/20] avg loss: 0.09536578272492868		[learning rate: 0.0013641]
	Learning Rate: 0.00136409
	LOSS [training: 0.09029284089492377 | validation: 0.08317989333035554]
	TIME [epoch: 8.13 sec]
EPOCH 462/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07598155682932325		[learning rate: 0.0013608]
		[batch 20/20] avg loss: 0.11272575485316667		[learning rate: 0.0013575]
	Learning Rate: 0.00135749
	LOSS [training: 0.09435365584124496 | validation: 0.22098188947536915]
	TIME [epoch: 8.11 sec]
EPOCH 463/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09210550699234224		[learning rate: 0.0013542]
		[batch 20/20] avg loss: 0.07634786372989258		[learning rate: 0.0013509]
	Learning Rate: 0.00135093
	LOSS [training: 0.0842266853611174 | validation: 0.09780148924437827]
	TIME [epoch: 8.11 sec]
EPOCH 464/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08127859358213443		[learning rate: 0.0013477]
		[batch 20/20] avg loss: 0.09492011376327514		[learning rate: 0.0013444]
	Learning Rate: 0.00134439
	LOSS [training: 0.08809935367270479 | validation: 0.05861541223376974]
	TIME [epoch: 8.1 sec]
EPOCH 465/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06413235561897121		[learning rate: 0.0013411]
		[batch 20/20] avg loss: 0.0661018123496733		[learning rate: 0.0013379]
	Learning Rate: 0.00133789
	LOSS [training: 0.06511708398432224 | validation: 0.06662920643340363]
	TIME [epoch: 8.13 sec]
EPOCH 466/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09359004908387158		[learning rate: 0.0013347]
		[batch 20/20] avg loss: 0.05074000186966974		[learning rate: 0.0013314]
	Learning Rate: 0.00133142
	LOSS [training: 0.07216502547677067 | validation: 0.08434996462256103]
	TIME [epoch: 8.11 sec]
EPOCH 467/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0587664178597064		[learning rate: 0.0013282]
		[batch 20/20] avg loss: 0.08834727278920075		[learning rate: 0.001325]
	Learning Rate: 0.00132498
	LOSS [training: 0.07355684532445357 | validation: 0.06060758210936894]
	TIME [epoch: 8.1 sec]
EPOCH 468/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0603204238783049		[learning rate: 0.0013218]
		[batch 20/20] avg loss: 0.0589738216387705		[learning rate: 0.0013186]
	Learning Rate: 0.00131858
	LOSS [training: 0.059647122758537695 | validation: 0.19004064632289971]
	TIME [epoch: 8.1 sec]
EPOCH 469/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11622441502467303		[learning rate: 0.0013154]
		[batch 20/20] avg loss: 0.10295672681048584		[learning rate: 0.0013122]
	Learning Rate: 0.0013122
	LOSS [training: 0.10959057091757944 | validation: 0.07172699437625352]
	TIME [epoch: 8.13 sec]
EPOCH 470/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07231872680779403		[learning rate: 0.001309]
		[batch 20/20] avg loss: 0.057428768041149415		[learning rate: 0.0013059]
	Learning Rate: 0.00130585
	LOSS [training: 0.06487374742447173 | validation: 0.17260224182891137]
	TIME [epoch: 8.1 sec]
EPOCH 471/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08286264350786801		[learning rate: 0.0013027]
		[batch 20/20] avg loss: 0.07506646495056599		[learning rate: 0.0012995]
	Learning Rate: 0.00129954
	LOSS [training: 0.07896455422921701 | validation: 0.04522463450285558]
	TIME [epoch: 8.11 sec]
EPOCH 472/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06637447279804506		[learning rate: 0.0012964]
		[batch 20/20] avg loss: 0.08702261361069492		[learning rate: 0.0012933]
	Learning Rate: 0.00129326
	LOSS [training: 0.07669854320437 | validation: 0.0547342846545579]
	TIME [epoch: 8.1 sec]
EPOCH 473/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06783722845239758		[learning rate: 0.0012901]
		[batch 20/20] avg loss: 0.06317036939397193		[learning rate: 0.001287]
	Learning Rate: 0.001287
	LOSS [training: 0.06550379892318477 | validation: 0.04642217826018108]
	TIME [epoch: 8.13 sec]
EPOCH 474/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0637596140425469		[learning rate: 0.0012839]
		[batch 20/20] avg loss: 0.06277758065221865		[learning rate: 0.0012808]
	Learning Rate: 0.00128078
	LOSS [training: 0.06326859734738277 | validation: 0.08785180257474313]
	TIME [epoch: 8.11 sec]
EPOCH 475/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05731630155277121		[learning rate: 0.0012777]
		[batch 20/20] avg loss: 0.10445380909596032		[learning rate: 0.0012746]
	Learning Rate: 0.00127458
	LOSS [training: 0.08088505532436577 | validation: 0.07373035284006678]
	TIME [epoch: 8.1 sec]
EPOCH 476/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07525891744817706		[learning rate: 0.0012715]
		[batch 20/20] avg loss: 0.12354542007422695		[learning rate: 0.0012684]
	Learning Rate: 0.00126842
	LOSS [training: 0.099402168761202 | validation: 0.06553536768499624]
	TIME [epoch: 8.11 sec]
EPOCH 477/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06137124197593001		[learning rate: 0.0012653]
		[batch 20/20] avg loss: 0.08431461563312109		[learning rate: 0.0012623]
	Learning Rate: 0.00126229
	LOSS [training: 0.07284292880452556 | validation: 0.05200081106353687]
	TIME [epoch: 8.13 sec]
EPOCH 478/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06751719528318866		[learning rate: 0.0012592]
		[batch 20/20] avg loss: 0.057378651154138496		[learning rate: 0.0012562]
	Learning Rate: 0.00125618
	LOSS [training: 0.06244792321866357 | validation: 0.09082899053910681]
	TIME [epoch: 8.11 sec]
EPOCH 479/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07465708669298621		[learning rate: 0.0012531]
		[batch 20/20] avg loss: 0.08862412243824118		[learning rate: 0.0012501]
	Learning Rate: 0.00125011
	LOSS [training: 0.0816406045656137 | validation: 0.08975651828722189]
	TIME [epoch: 8.1 sec]
EPOCH 480/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07629064692650561		[learning rate: 0.0012471]
		[batch 20/20] avg loss: 0.0809251392537271		[learning rate: 0.0012441]
	Learning Rate: 0.00124406
	LOSS [training: 0.07860789309011636 | validation: 0.033634591448045245]
	TIME [epoch: 8.11 sec]
EPOCH 481/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05940561719925156		[learning rate: 0.0012411]
		[batch 20/20] avg loss: 0.09437498608981834		[learning rate: 0.001238]
	Learning Rate: 0.00123805
	LOSS [training: 0.07689030164453495 | validation: 0.06275597799852795]
	TIME [epoch: 8.12 sec]
EPOCH 482/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06578091145736278		[learning rate: 0.001235]
		[batch 20/20] avg loss: 0.061378038625445774		[learning rate: 0.0012321]
	Learning Rate: 0.00123206
	LOSS [training: 0.0635794750414043 | validation: 0.06572431788563195]
	TIME [epoch: 8.12 sec]
EPOCH 483/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08143484053868091		[learning rate: 0.0012291]
		[batch 20/20] avg loss: 0.07558198451287704		[learning rate: 0.0012261]
	Learning Rate: 0.0012261
	LOSS [training: 0.07850841252577898 | validation: 0.05372712500825178]
	TIME [epoch: 8.11 sec]
EPOCH 484/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08008068610044848		[learning rate: 0.0012231]
		[batch 20/20] avg loss: 0.12191584372032824		[learning rate: 0.0012202]
	Learning Rate: 0.00122017
	LOSS [training: 0.10099826491038837 | validation: 0.04243206652853849]
	TIME [epoch: 8.12 sec]
EPOCH 485/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07186202843084355		[learning rate: 0.0012172]
		[batch 20/20] avg loss: 0.07019713235802014		[learning rate: 0.0012143]
	Learning Rate: 0.00121427
	LOSS [training: 0.07102958039443184 | validation: 0.07375141059465261]
	TIME [epoch: 8.11 sec]
EPOCH 486/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05169711151731006		[learning rate: 0.0012113]
		[batch 20/20] avg loss: 0.05726504318133039		[learning rate: 0.0012084]
	Learning Rate: 0.0012084
	LOSS [training: 0.054481077349320214 | validation: 0.048403216312484384]
	TIME [epoch: 8.11 sec]
EPOCH 487/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06009811320350865		[learning rate: 0.0012055]
		[batch 20/20] avg loss: 0.09091434572749686		[learning rate: 0.0012026]
	Learning Rate: 0.00120256
	LOSS [training: 0.07550622946550277 | validation: 0.07401912096111347]
	TIME [epoch: 8.1 sec]
EPOCH 488/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07276188380996865		[learning rate: 0.0011996]
		[batch 20/20] avg loss: 0.053053273436250145		[learning rate: 0.0011967]
	Learning Rate: 0.00119674
	LOSS [training: 0.06290757862310939 | validation: 0.05104685206039344]
	TIME [epoch: 8.12 sec]
EPOCH 489/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07844061374593596		[learning rate: 0.0011938]
		[batch 20/20] avg loss: 0.0655444123951252		[learning rate: 0.001191]
	Learning Rate: 0.00119095
	LOSS [training: 0.07199251307053059 | validation: 0.06897708914214651]
	TIME [epoch: 8.13 sec]
EPOCH 490/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09444069506528927		[learning rate: 0.0011881]
		[batch 20/20] avg loss: 0.07114568921280114		[learning rate: 0.0011852]
	Learning Rate: 0.00118519
	LOSS [training: 0.0827931921390452 | validation: 0.04329603154377392]
	TIME [epoch: 8.13 sec]
EPOCH 491/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07978192592391176		[learning rate: 0.0011823]
		[batch 20/20] avg loss: 0.08266207624157391		[learning rate: 0.0011795]
	Learning Rate: 0.00117946
	LOSS [training: 0.08122200108274283 | validation: 0.06812389867918771]
	TIME [epoch: 8.1 sec]
EPOCH 492/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.049900656350929974		[learning rate: 0.0011766]
		[batch 20/20] avg loss: 0.08171314713144154		[learning rate: 0.0011738]
	Learning Rate: 0.00117376
	LOSS [training: 0.06580690174118575 | validation: 0.09753687222450308]
	TIME [epoch: 8.1 sec]
EPOCH 493/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08273018862314069		[learning rate: 0.0011709]
		[batch 20/20] avg loss: 0.0514359718659943		[learning rate: 0.0011681]
	Learning Rate: 0.00116808
	LOSS [training: 0.06708308024456748 | validation: 0.0972478534505518]
	TIME [epoch: 8.11 sec]
EPOCH 494/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07928534613147581		[learning rate: 0.0011653]
		[batch 20/20] avg loss: 0.06778700829221608		[learning rate: 0.0011624]
	Learning Rate: 0.00116243
	LOSS [training: 0.07353617721184594 | validation: 0.08916906844546134]
	TIME [epoch: 8.13 sec]
EPOCH 495/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06516472971831985		[learning rate: 0.0011596]
		[batch 20/20] avg loss: 0.06361663329696995		[learning rate: 0.0011568]
	Learning Rate: 0.00115681
	LOSS [training: 0.0643906815076449 | validation: 0.03535906480162167]
	TIME [epoch: 8.12 sec]
EPOCH 496/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07359169770083891		[learning rate: 0.001154]
		[batch 20/20] avg loss: 0.06119975513044325		[learning rate: 0.0011512]
	Learning Rate: 0.00115122
	LOSS [training: 0.06739572641564107 | validation: 0.05547782445846136]
	TIME [epoch: 8.11 sec]
EPOCH 497/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05119577325293019		[learning rate: 0.0011484]
		[batch 20/20] avg loss: 0.09901665261108183		[learning rate: 0.0011457]
	Learning Rate: 0.00114565
	LOSS [training: 0.075106212932006 | validation: 0.1251919462036779]
	TIME [epoch: 8.12 sec]
EPOCH 498/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07724683090084286		[learning rate: 0.0011429]
		[batch 20/20] avg loss: 0.06848661794430126		[learning rate: 0.0011401]
	Learning Rate: 0.00114011
	LOSS [training: 0.07286672442257205 | validation: 0.08039880894976853]
	TIME [epoch: 8.14 sec]
EPOCH 499/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05745493789559203		[learning rate: 0.0011374]
		[batch 20/20] avg loss: 0.0825554054326462		[learning rate: 0.0011346]
	Learning Rate: 0.0011346
	LOSS [training: 0.0700051716641191 | validation: 0.07261439702628901]
	TIME [epoch: 8.11 sec]
EPOCH 500/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07779944275537762		[learning rate: 0.0011319]
		[batch 20/20] avg loss: 0.054094430660574636		[learning rate: 0.0011291]
	Learning Rate: 0.00112911
	LOSS [training: 0.06594693670797612 | validation: 0.0797786777514426]
	TIME [epoch: 8.1 sec]
EPOCH 501/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07083588536853068		[learning rate: 0.0011264]
		[batch 20/20] avg loss: 0.08925564687531436		[learning rate: 0.0011237]
	Learning Rate: 0.00112365
	LOSS [training: 0.08004576612192252 | validation: 0.14455424028149724]
	TIME [epoch: 8.11 sec]
EPOCH 502/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07214273014457528		[learning rate: 0.0011209]
		[batch 20/20] avg loss: 0.056599158877501575		[learning rate: 0.0011182]
	Learning Rate: 0.00111822
	LOSS [training: 0.06437094451103845 | validation: 0.060048656717784143]
	TIME [epoch: 8.13 sec]
EPOCH 503/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06531366901044869		[learning rate: 0.0011155]
		[batch 20/20] avg loss: 0.05085378041158658		[learning rate: 0.0011128]
	Learning Rate: 0.00111281
	LOSS [training: 0.05808372471101764 | validation: 0.05477186815219315]
	TIME [epoch: 8.1 sec]
EPOCH 504/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07212763983471952		[learning rate: 0.0011101]
		[batch 20/20] avg loss: 0.05833718912657415		[learning rate: 0.0011074]
	Learning Rate: 0.00110743
	LOSS [training: 0.06523241448064684 | validation: 0.042090477991541135]
	TIME [epoch: 8.11 sec]
EPOCH 505/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07704086470763778		[learning rate: 0.0011047]
		[batch 20/20] avg loss: 0.08650784148976275		[learning rate: 0.0011021]
	Learning Rate: 0.00110207
	LOSS [training: 0.08177435309870026 | validation: 0.11448207572788305]
	TIME [epoch: 8.1 sec]
EPOCH 506/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06148609264533048		[learning rate: 0.0010994]
		[batch 20/20] avg loss: 0.12282279473414956		[learning rate: 0.0010967]
	Learning Rate: 0.00109674
	LOSS [training: 0.09215444368974002 | validation: 0.10193283783064315]
	TIME [epoch: 8.14 sec]
EPOCH 507/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07553030434255029		[learning rate: 0.0010941]
		[batch 20/20] avg loss: 0.08885498628041141		[learning rate: 0.0010914]
	Learning Rate: 0.00109144
	LOSS [training: 0.08219264531148085 | validation: 0.05446663031591297]
	TIME [epoch: 8.12 sec]
EPOCH 508/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05387532869785118		[learning rate: 0.0010888]
		[batch 20/20] avg loss: 0.0719538968593018		[learning rate: 0.0010862]
	Learning Rate: 0.00108616
	LOSS [training: 0.06291461277857649 | validation: 0.08173511963534985]
	TIME [epoch: 8.12 sec]
EPOCH 509/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07131434461917112		[learning rate: 0.0010835]
		[batch 20/20] avg loss: 0.08221904023249921		[learning rate: 0.0010809]
	Learning Rate: 0.00108091
	LOSS [training: 0.07676669242583516 | validation: 0.052862063504283845]
	TIME [epoch: 8.11 sec]
EPOCH 510/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05440281219281016		[learning rate: 0.0010783]
		[batch 20/20] avg loss: 0.0726517499866066		[learning rate: 0.0010757]
	Learning Rate: 0.00107568
	LOSS [training: 0.06352728108970837 | validation: 0.06093184301201535]
	TIME [epoch: 8.15 sec]
EPOCH 511/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06584588267399101		[learning rate: 0.0010731]
		[batch 20/20] avg loss: 0.055698817146267364		[learning rate: 0.0010705]
	Learning Rate: 0.00107048
	LOSS [training: 0.060772349910129184 | validation: 0.03577642429225225]
	TIME [epoch: 8.12 sec]
EPOCH 512/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06573256999845037		[learning rate: 0.0010679]
		[batch 20/20] avg loss: 0.05106758628026844		[learning rate: 0.0010653]
	Learning Rate: 0.0010653
	LOSS [training: 0.0584000781393594 | validation: 0.05482785586907042]
	TIME [epoch: 8.12 sec]
EPOCH 513/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.073122960536677		[learning rate: 0.0010627]
		[batch 20/20] avg loss: 0.05289336982336298		[learning rate: 0.0010602]
	Learning Rate: 0.00106015
	LOSS [training: 0.06300816518001998 | validation: 0.10509462737019487]
	TIME [epoch: 8.12 sec]
EPOCH 514/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.054365072015720474		[learning rate: 0.0010576]
		[batch 20/20] avg loss: 0.06506992007807529		[learning rate: 0.001055]
	Learning Rate: 0.00105503
	LOSS [training: 0.05971749604689789 | validation: 0.041394881634740026]
	TIME [epoch: 8.14 sec]
EPOCH 515/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0821706644008839		[learning rate: 0.0010525]
		[batch 20/20] avg loss: 0.062062569259309086		[learning rate: 0.0010499]
	Learning Rate: 0.00104992
	LOSS [training: 0.0721166168300965 | validation: 0.039336509207274134]
	TIME [epoch: 8.13 sec]
EPOCH 516/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06348290154321597		[learning rate: 0.0010474]
		[batch 20/20] avg loss: 0.08721289550304255		[learning rate: 0.0010448]
	Learning Rate: 0.00104485
	LOSS [training: 0.07534789852312926 | validation: 0.03533747309407291]
	TIME [epoch: 8.12 sec]
EPOCH 517/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04622736865720822		[learning rate: 0.0010423]
		[batch 20/20] avg loss: 0.056340373729146485		[learning rate: 0.0010398]
	Learning Rate: 0.00103979
	LOSS [training: 0.05128387119317736 | validation: 0.025355507643330118]
	TIME [epoch: 8.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_517.pth
	Model improved!!!
EPOCH 518/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.053249317992001964		[learning rate: 0.0010373]
		[batch 20/20] avg loss: 0.06760636610589105		[learning rate: 0.0010348]
	Learning Rate: 0.00103477
	LOSS [training: 0.0604278420489465 | validation: 0.06254370290354612]
	TIME [epoch: 8.13 sec]
EPOCH 519/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06523440050409668		[learning rate: 0.0010323]
		[batch 20/20] avg loss: 0.08035441730835968		[learning rate: 0.0010298]
	Learning Rate: 0.00102976
	LOSS [training: 0.07279440890622817 | validation: 0.10268688970326918]
	TIME [epoch: 8.11 sec]
EPOCH 520/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06389338219518502		[learning rate: 0.0010273]
		[batch 20/20] avg loss: 0.09015781379758017		[learning rate: 0.0010248]
	Learning Rate: 0.00102478
	LOSS [training: 0.0770255979963826 | validation: 0.06934811504139961]
	TIME [epoch: 8.1 sec]
EPOCH 521/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04681505304170354		[learning rate: 0.0010223]
		[batch 20/20] avg loss: 0.03975514476540658		[learning rate: 0.0010198]
	Learning Rate: 0.00101983
	LOSS [training: 0.04328509890355505 | validation: 0.040074978524620136]
	TIME [epoch: 8.11 sec]
EPOCH 522/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05223204242260847		[learning rate: 0.0010174]
		[batch 20/20] avg loss: 0.1210782610188135		[learning rate: 0.0010149]
	Learning Rate: 0.00101489
	LOSS [training: 0.08665515172071099 | validation: 0.040645382654305125]
	TIME [epoch: 8.12 sec]
EPOCH 523/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05717198016026216		[learning rate: 0.0010124]
		[batch 20/20] avg loss: 0.051028063025823786		[learning rate: 0.00101]
	Learning Rate: 0.00100999
	LOSS [training: 0.05410002159304298 | validation: 0.05868064321476855]
	TIME [epoch: 8.12 sec]
EPOCH 524/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07882603497951045		[learning rate: 0.0010075]
		[batch 20/20] avg loss: 0.09415203200797934		[learning rate: 0.0010051]
	Learning Rate: 0.0010051
	LOSS [training: 0.08648903349374489 | validation: 0.030426478855254353]
	TIME [epoch: 8.1 sec]
EPOCH 525/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07436085441919864		[learning rate: 0.0010027]
		[batch 20/20] avg loss: 0.06825351341690199		[learning rate: 0.0010002]
	Learning Rate: 0.00100024
	LOSS [training: 0.07130718391805031 | validation: 0.04466790730694025]
	TIME [epoch: 8.1 sec]
EPOCH 526/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0719258491300822		[learning rate: 0.00099782]
		[batch 20/20] avg loss: 0.059843024465049535		[learning rate: 0.00099541]
	Learning Rate: 0.000995405
	LOSS [training: 0.06588443679756587 | validation: 0.048372836915685925]
	TIME [epoch: 8.12 sec]
EPOCH 527/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07286759553476616		[learning rate: 0.000993]
		[batch 20/20] avg loss: 0.04084513616241395		[learning rate: 0.00099059]
	Learning Rate: 0.000990592
	LOSS [training: 0.05685636584859005 | validation: 0.11977092321611367]
	TIME [epoch: 8.11 sec]
EPOCH 528/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08496094119719085		[learning rate: 0.00098819]
		[batch 20/20] avg loss: 0.07772647611628773		[learning rate: 0.0009858]
	Learning Rate: 0.000985801
	LOSS [training: 0.0813437086567393 | validation: 0.07628547215510167]
	TIME [epoch: 8.11 sec]
EPOCH 529/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0644343274565953		[learning rate: 0.00098341]
		[batch 20/20] avg loss: 0.07325840297988225		[learning rate: 0.00098103]
	Learning Rate: 0.000981034
	LOSS [training: 0.06884636521823878 | validation: 0.11688312766262422]
	TIME [epoch: 8.1 sec]
EPOCH 530/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05390347941154452		[learning rate: 0.00097866]
		[batch 20/20] avg loss: 0.06785778707182373		[learning rate: 0.00097629]
	Learning Rate: 0.00097629
	LOSS [training: 0.06088063324168412 | validation: 0.04889391531277289]
	TIME [epoch: 8.12 sec]
EPOCH 531/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09006308706409266		[learning rate: 0.00097393]
		[batch 20/20] avg loss: 0.07876973843040594		[learning rate: 0.00097157]
	Learning Rate: 0.000971569
	LOSS [training: 0.0844164127472493 | validation: 0.06458650520679786]
	TIME [epoch: 8.11 sec]
EPOCH 532/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07375743169952073		[learning rate: 0.00096922]
		[batch 20/20] avg loss: 0.04809248743650625		[learning rate: 0.00096687]
	Learning Rate: 0.000966871
	LOSS [training: 0.06092495956801349 | validation: 0.041827042764982385]
	TIME [epoch: 8.1 sec]
EPOCH 533/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.044956779763934175		[learning rate: 0.00096453]
		[batch 20/20] avg loss: 0.055224563412127246		[learning rate: 0.00096219]
	Learning Rate: 0.000962195
	LOSS [training: 0.05009067158803071 | validation: 0.0676834949322187]
	TIME [epoch: 8.1 sec]
EPOCH 534/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06546994099394418		[learning rate: 0.00095987]
		[batch 20/20] avg loss: 0.07792270929711317		[learning rate: 0.00095754]
	Learning Rate: 0.000957542
	LOSS [training: 0.07169632514552868 | validation: 0.09789275571942442]
	TIME [epoch: 8.13 sec]
EPOCH 535/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07098009306490786		[learning rate: 0.00095522]
		[batch 20/20] avg loss: 0.05047190295342428		[learning rate: 0.00095291]
	Learning Rate: 0.000952912
	LOSS [training: 0.06072599800916608 | validation: 0.034958068242514115]
	TIME [epoch: 8.12 sec]
EPOCH 536/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08682812995336835		[learning rate: 0.0009506]
		[batch 20/20] avg loss: 0.0500299254308252		[learning rate: 0.0009483]
	Learning Rate: 0.000948304
	LOSS [training: 0.06842902769209677 | validation: 0.0231187155895213]
	TIME [epoch: 8.11 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_536.pth
	Model improved!!!
EPOCH 537/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.054691305439399075		[learning rate: 0.00094601]
		[batch 20/20] avg loss: 0.04967031405538013		[learning rate: 0.00094372]
	Learning Rate: 0.000943718
	LOSS [training: 0.0521808097473896 | validation: 0.03410797123845627]
	TIME [epoch: 8.12 sec]
EPOCH 538/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04266091751578566		[learning rate: 0.00094143]
		[batch 20/20] avg loss: 0.0633487267322999		[learning rate: 0.00093915]
	Learning Rate: 0.000939154
	LOSS [training: 0.05300482212404278 | validation: 0.0935012092584381]
	TIME [epoch: 8.12 sec]
EPOCH 539/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05318863148037935		[learning rate: 0.00093688]
		[batch 20/20] avg loss: 0.0555134349979679		[learning rate: 0.00093461]
	Learning Rate: 0.000934613
	LOSS [training: 0.05435103323917362 | validation: 0.0609075834269559]
	TIME [epoch: 8.13 sec]
EPOCH 540/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08121102647562653		[learning rate: 0.00093235]
		[batch 20/20] avg loss: 0.0882669638917487		[learning rate: 0.00093009]
	Learning Rate: 0.000930093
	LOSS [training: 0.08473899518368763 | validation: 0.04464194273570715]
	TIME [epoch: 8.12 sec]
EPOCH 541/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05625061682566761		[learning rate: 0.00092784]
		[batch 20/20] avg loss: 0.06044221107668851		[learning rate: 0.0009256]
	Learning Rate: 0.000925595
	LOSS [training: 0.05834641395117805 | validation: 0.06076524013635802]
	TIME [epoch: 8.12 sec]
EPOCH 542/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06374652988844681		[learning rate: 0.00092335]
		[batch 20/20] avg loss: 0.04531201472865763		[learning rate: 0.00092112]
	Learning Rate: 0.000921119
	LOSS [training: 0.05452927230855222 | validation: 0.051562415914618694]
	TIME [epoch: 8.11 sec]
EPOCH 543/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.047618677089514574		[learning rate: 0.00091889]
		[batch 20/20] avg loss: 0.05070392456727195		[learning rate: 0.00091666]
	Learning Rate: 0.000916665
	LOSS [training: 0.04916130082839326 | validation: 0.08660148996538916]
	TIME [epoch: 8.14 sec]
EPOCH 544/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04966629991290013		[learning rate: 0.00091445]
		[batch 20/20] avg loss: 0.0617553491804286		[learning rate: 0.00091223]
	Learning Rate: 0.000912232
	LOSS [training: 0.05571082454666435 | validation: 0.1116825815012592]
	TIME [epoch: 8.12 sec]
EPOCH 545/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0703714967568286		[learning rate: 0.00091002]
		[batch 20/20] avg loss: 0.06533539057811252		[learning rate: 0.00090782]
	Learning Rate: 0.000907821
	LOSS [training: 0.06785344366747056 | validation: 0.10300319480208635]
	TIME [epoch: 8.11 sec]
EPOCH 546/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07674572075328526		[learning rate: 0.00090562]
		[batch 20/20] avg loss: 0.08863716278457374		[learning rate: 0.00090343]
	Learning Rate: 0.00090343
	LOSS [training: 0.0826914417689295 | validation: 0.031238553517647545]
	TIME [epoch: 8.11 sec]
EPOCH 547/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05162332531789082		[learning rate: 0.00090124]
		[batch 20/20] avg loss: 0.06635534986717224		[learning rate: 0.00089906]
	Learning Rate: 0.000899062
	LOSS [training: 0.05898933759253154 | validation: 0.07595936563150696]
	TIME [epoch: 8.13 sec]
EPOCH 548/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06393252535488186		[learning rate: 0.00089689]
		[batch 20/20] avg loss: 0.0705429484996105		[learning rate: 0.00089471]
	Learning Rate: 0.000894714
	LOSS [training: 0.06723773692724618 | validation: 0.08148667390197452]
	TIME [epoch: 8.11 sec]
EPOCH 549/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04937167721795002		[learning rate: 0.00089255]
		[batch 20/20] avg loss: 0.07245633466122507		[learning rate: 0.00089039]
	Learning Rate: 0.000890387
	LOSS [training: 0.06091400593958755 | validation: 0.03882045763833915]
	TIME [epoch: 8.11 sec]
EPOCH 550/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04967888634610411		[learning rate: 0.00088823]
		[batch 20/20] avg loss: 0.06471484942203053		[learning rate: 0.00088608]
	Learning Rate: 0.000886081
	LOSS [training: 0.05719686788406732 | validation: 0.05346355108550925]
	TIME [epoch: 8.12 sec]
EPOCH 551/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07685093091710703		[learning rate: 0.00088394]
		[batch 20/20] avg loss: 0.0785995727485465		[learning rate: 0.0008818]
	Learning Rate: 0.000881797
	LOSS [training: 0.07772525183282676 | validation: 0.03767688561177086]
	TIME [epoch: 8.14 sec]
EPOCH 552/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04704218534467557		[learning rate: 0.00087966]
		[batch 20/20] avg loss: 0.05177073739317325		[learning rate: 0.00087753]
	Learning Rate: 0.000877532
	LOSS [training: 0.04940646136892441 | validation: 0.08410605576584292]
	TIME [epoch: 8.12 sec]
EPOCH 553/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06051135407483765		[learning rate: 0.00087541]
		[batch 20/20] avg loss: 0.04708463413155464		[learning rate: 0.00087329]
	Learning Rate: 0.000873289
	LOSS [training: 0.05379799410319615 | validation: 0.03965429355523612]
	TIME [epoch: 8.11 sec]
EPOCH 554/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06860228262290656		[learning rate: 0.00087117]
		[batch 20/20] avg loss: 0.04171811566573226		[learning rate: 0.00086907]
	Learning Rate: 0.000869066
	LOSS [training: 0.05516019914431941 | validation: 0.06501875726989555]
	TIME [epoch: 8.11 sec]
EPOCH 555/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.054415548000320765		[learning rate: 0.00086696]
		[batch 20/20] avg loss: 0.07403555940391601		[learning rate: 0.00086486]
	Learning Rate: 0.000864863
	LOSS [training: 0.0642255537021184 | validation: 0.029638545615101864]
	TIME [epoch: 8.14 sec]
EPOCH 556/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05385307552927158		[learning rate: 0.00086277]
		[batch 20/20] avg loss: 0.06832821745500264		[learning rate: 0.00086068]
	Learning Rate: 0.000860681
	LOSS [training: 0.061090646492137114 | validation: 0.03328141020560221]
	TIME [epoch: 8.11 sec]
EPOCH 557/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04298156990140305		[learning rate: 0.0008586]
		[batch 20/20] avg loss: 0.05037781712711067		[learning rate: 0.00085652]
	Learning Rate: 0.000856519
	LOSS [training: 0.04667969351425685 | validation: 0.028752558272490455]
	TIME [epoch: 8.1 sec]
EPOCH 558/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04978658681483573		[learning rate: 0.00085445]
		[batch 20/20] avg loss: 0.05285608500918455		[learning rate: 0.00085238]
	Learning Rate: 0.000852377
	LOSS [training: 0.05132133591201014 | validation: 0.06169363178357508]
	TIME [epoch: 8.11 sec]
EPOCH 559/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.045716681981618		[learning rate: 0.00085031]
		[batch 20/20] avg loss: 0.0666554727111778		[learning rate: 0.00084825]
	Learning Rate: 0.000848255
	LOSS [training: 0.05618607734639789 | validation: 0.07732592877979887]
	TIME [epoch: 8.13 sec]
EPOCH 560/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06185402393231937		[learning rate: 0.0008462]
		[batch 20/20] avg loss: 0.04546234472393224		[learning rate: 0.00084415]
	Learning Rate: 0.000844153
	LOSS [training: 0.05365818432812579 | validation: 0.03959943813691269]
	TIME [epoch: 8.11 sec]
EPOCH 561/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04382264656019726		[learning rate: 0.00084211]
		[batch 20/20] avg loss: 0.06559919975909244		[learning rate: 0.00084007]
	Learning Rate: 0.000840071
	LOSS [training: 0.054710923159644854 | validation: 0.02827060036443568]
	TIME [epoch: 8.11 sec]
EPOCH 562/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03780728591878942		[learning rate: 0.00083804]
		[batch 20/20] avg loss: 0.05780173201338647		[learning rate: 0.00083601]
	Learning Rate: 0.000836008
	LOSS [training: 0.04780450896608795 | validation: 0.027806201789323335]
	TIME [epoch: 8.11 sec]
EPOCH 563/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.047652372547421634		[learning rate: 0.00083398]
		[batch 20/20] avg loss: 0.05045478834486762		[learning rate: 0.00083197]
	Learning Rate: 0.000831965
	LOSS [training: 0.04905358044614462 | validation: 0.037950302641747785]
	TIME [epoch: 8.14 sec]
EPOCH 564/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.050635302811041616		[learning rate: 0.00082995]
		[batch 20/20] avg loss: 0.06556369422531116		[learning rate: 0.00082794]
	Learning Rate: 0.000827942
	LOSS [training: 0.0580994985181764 | validation: 0.07666521899100992]
	TIME [epoch: 8.11 sec]
EPOCH 565/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.055809081974869466		[learning rate: 0.00082594]
		[batch 20/20] avg loss: 0.0673407076964348		[learning rate: 0.00082394]
	Learning Rate: 0.000823938
	LOSS [training: 0.061574894835652115 | validation: 0.04360650391486044]
	TIME [epoch: 8.12 sec]
EPOCH 566/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04343511102927931		[learning rate: 0.00082194]
		[batch 20/20] avg loss: 0.06518716065541294		[learning rate: 0.00081995]
	Learning Rate: 0.000819954
	LOSS [training: 0.05431113584234611 | validation: 0.042679709950932995]
	TIME [epoch: 8.11 sec]
EPOCH 567/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.048068524943753105		[learning rate: 0.00081797]
		[batch 20/20] avg loss: 0.031192323184470006		[learning rate: 0.00081599]
	Learning Rate: 0.000815989
	LOSS [training: 0.03963042406411156 | validation: 0.06750718533136113]
	TIME [epoch: 8.13 sec]
EPOCH 568/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04806875493076744		[learning rate: 0.00081401]
		[batch 20/20] avg loss: 0.07441958323873218		[learning rate: 0.00081204]
	Learning Rate: 0.000812043
	LOSS [training: 0.06124416908474981 | validation: 0.07789107276906806]
	TIME [epoch: 8.11 sec]
EPOCH 569/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05616854583666786		[learning rate: 0.00081008]
		[batch 20/20] avg loss: 0.0403366489761137		[learning rate: 0.00080812]
	Learning Rate: 0.000808116
	LOSS [training: 0.048252597406390776 | validation: 0.0870103140113017]
	TIME [epoch: 8.11 sec]
EPOCH 570/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0672857140623882		[learning rate: 0.00080616]
		[batch 20/20] avg loss: 0.07972593727957082		[learning rate: 0.00080421]
	Learning Rate: 0.000804208
	LOSS [training: 0.07350582567097952 | validation: 0.053001402985363585]
	TIME [epoch: 8.11 sec]
EPOCH 571/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07557370407231784		[learning rate: 0.00080226]
		[batch 20/20] avg loss: 0.045066856039769024		[learning rate: 0.00080032]
	Learning Rate: 0.000800319
	LOSS [training: 0.06032028005604342 | validation: 0.033611317542396546]
	TIME [epoch: 8.13 sec]
EPOCH 572/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05036521495644579		[learning rate: 0.00079838]
		[batch 20/20] avg loss: 0.06007374668081121		[learning rate: 0.00079645]
	Learning Rate: 0.000796449
	LOSS [training: 0.05521948081862851 | validation: 0.047489948501138975]
	TIME [epoch: 8.12 sec]
EPOCH 573/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05733066711565056		[learning rate: 0.00079452]
		[batch 20/20] avg loss: 0.04248065197865021		[learning rate: 0.0007926]
	Learning Rate: 0.000792597
	LOSS [training: 0.04990565954715039 | validation: 0.03210525605482681]
	TIME [epoch: 8.11 sec]
EPOCH 574/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05007648603321927		[learning rate: 0.00079068]
		[batch 20/20] avg loss: 0.07148728236241891		[learning rate: 0.00078876]
	Learning Rate: 0.000788765
	LOSS [training: 0.06078188419781909 | validation: 0.03377395284480145]
	TIME [epoch: 8.11 sec]
EPOCH 575/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.058835670031640794		[learning rate: 0.00078686]
		[batch 20/20] avg loss: 0.03944733227385348		[learning rate: 0.00078495]
	Learning Rate: 0.00078495
	LOSS [training: 0.049141501152747145 | validation: 0.02702524931524489]
	TIME [epoch: 8.13 sec]
EPOCH 576/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03496984269383337		[learning rate: 0.00078305]
		[batch 20/20] avg loss: 0.04754328837135828		[learning rate: 0.00078115]
	Learning Rate: 0.000781154
	LOSS [training: 0.04125656553259582 | validation: 0.08843229041137711]
	TIME [epoch: 8.12 sec]
EPOCH 577/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08632507446133497		[learning rate: 0.00077926]
		[batch 20/20] avg loss: 0.09155719759258674		[learning rate: 0.00077738]
	Learning Rate: 0.000777377
	LOSS [training: 0.08894113602696085 | validation: 0.03577372948357321]
	TIME [epoch: 8.11 sec]
EPOCH 578/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04242611389200522		[learning rate: 0.00077549]
		[batch 20/20] avg loss: 0.05319751750625844		[learning rate: 0.00077362]
	Learning Rate: 0.000773618
	LOSS [training: 0.04781181569913182 | validation: 0.1324227616383311]
	TIME [epoch: 8.12 sec]
EPOCH 579/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05337829469617537		[learning rate: 0.00077174]
		[batch 20/20] avg loss: 0.06473588612901597		[learning rate: 0.00076988]
	Learning Rate: 0.000769877
	LOSS [training: 0.05905709041259568 | validation: 0.05460585993574356]
	TIME [epoch: 8.13 sec]
EPOCH 580/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08166693338849097		[learning rate: 0.00076801]
		[batch 20/20] avg loss: 0.05274530486425818		[learning rate: 0.00076615]
	Learning Rate: 0.000766154
	LOSS [training: 0.06720611912637459 | validation: 0.04630925931874426]
	TIME [epoch: 8.13 sec]
EPOCH 581/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.048708023301662864		[learning rate: 0.0007643]
		[batch 20/20] avg loss: 0.059293118282214785		[learning rate: 0.00076245]
	Learning Rate: 0.000762448
	LOSS [training: 0.05400057079193883 | validation: 0.06775073917778887]
	TIME [epoch: 8.11 sec]
EPOCH 582/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.043732320548112324		[learning rate: 0.0007606]
		[batch 20/20] avg loss: 0.052461397696467314		[learning rate: 0.00075876]
	Learning Rate: 0.000758761
	LOSS [training: 0.04809685912228981 | validation: 0.039550378794091824]
	TIME [epoch: 8.1 sec]
EPOCH 583/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03849420952086357		[learning rate: 0.00075692]
		[batch 20/20] avg loss: 0.05005204091397388		[learning rate: 0.00075509]
	Learning Rate: 0.000755092
	LOSS [training: 0.04427312521741874 | validation: 0.07346822778945877]
	TIME [epoch: 8.11 sec]
EPOCH 584/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06584101581399057		[learning rate: 0.00075326]
		[batch 20/20] avg loss: 0.04421476630556888		[learning rate: 0.00075144]
	Learning Rate: 0.000751441
	LOSS [training: 0.05502789105977971 | validation: 0.030593118509229418]
	TIME [epoch: 8.12 sec]
EPOCH 585/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.045998450811107316		[learning rate: 0.00074962]
		[batch 20/20] avg loss: 0.04676622233815202		[learning rate: 0.00074781]
	Learning Rate: 0.000747807
	LOSS [training: 0.04638233657462968 | validation: 0.04582959908079616]
	TIME [epoch: 8.11 sec]
EPOCH 586/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05243675887931702		[learning rate: 0.000746]
		[batch 20/20] avg loss: 0.05982312553930215		[learning rate: 0.00074419]
	Learning Rate: 0.000744191
	LOSS [training: 0.05612994220930959 | validation: 0.13315604412949397]
	TIME [epoch: 8.1 sec]
EPOCH 587/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.050282070346693686		[learning rate: 0.00074239]
		[batch 20/20] avg loss: 0.0400805829537611		[learning rate: 0.00074059]
	Learning Rate: 0.000740592
	LOSS [training: 0.045181326650227394 | validation: 0.02831813597100767]
	TIME [epoch: 8.11 sec]
EPOCH 588/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05275979741145866		[learning rate: 0.0007388]
		[batch 20/20] avg loss: 0.05360407190565093		[learning rate: 0.00073701]
	Learning Rate: 0.000737011
	LOSS [training: 0.05318193465855479 | validation: 0.07526710316094305]
	TIME [epoch: 8.13 sec]
EPOCH 589/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0664519261086945		[learning rate: 0.00073523]
		[batch 20/20] avg loss: 0.05333936237308634		[learning rate: 0.00073345]
	Learning Rate: 0.000733446
	LOSS [training: 0.05989564424089041 | validation: 0.05433196658092722]
	TIME [epoch: 8.11 sec]
EPOCH 590/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.040075406392742464		[learning rate: 0.00073167]
		[batch 20/20] avg loss: 0.04529714142252291		[learning rate: 0.0007299]
	Learning Rate: 0.0007299
	LOSS [training: 0.04268627390763269 | validation: 0.04629623448213708]
	TIME [epoch: 8.12 sec]
EPOCH 591/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.055377497708881396		[learning rate: 0.00072813]
		[batch 20/20] avg loss: 0.0447521965919534		[learning rate: 0.00072637]
	Learning Rate: 0.00072637
	LOSS [training: 0.0500648471504174 | validation: 0.034301230412757684]
	TIME [epoch: 8.1 sec]
EPOCH 592/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04618381934926351		[learning rate: 0.00072461]
		[batch 20/20] avg loss: 0.052109402492394465		[learning rate: 0.00072286]
	Learning Rate: 0.000722857
	LOSS [training: 0.04914661092082898 | validation: 0.04904244566047075]
	TIME [epoch: 8.13 sec]
EPOCH 593/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04091161954446011		[learning rate: 0.00072111]
		[batch 20/20] avg loss: 0.058938594926215414		[learning rate: 0.00071936]
	Learning Rate: 0.000719362
	LOSS [training: 0.04992510723533776 | validation: 0.06915724125152253]
	TIME [epoch: 8.11 sec]
EPOCH 594/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05949844606430392		[learning rate: 0.00071762]
		[batch 20/20] avg loss: 0.07888514533329775		[learning rate: 0.00071588]
	Learning Rate: 0.000715883
	LOSS [training: 0.06919179569880084 | validation: 0.03184895795471619]
	TIME [epoch: 8.1 sec]
EPOCH 595/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0372549302031722		[learning rate: 0.00071415]
		[batch 20/20] avg loss: 0.05114017332975781		[learning rate: 0.00071242]
	Learning Rate: 0.000712421
	LOSS [training: 0.044197551766465004 | validation: 0.09387907532424766]
	TIME [epoch: 8.11 sec]
EPOCH 596/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07183175215944869		[learning rate: 0.0007107]
		[batch 20/20] avg loss: 0.046656077715197		[learning rate: 0.00070898]
	Learning Rate: 0.000708976
	LOSS [training: 0.05924391493732285 | validation: 0.038461893910406726]
	TIME [epoch: 8.13 sec]
EPOCH 597/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.041571366176351494		[learning rate: 0.00070726]
		[batch 20/20] avg loss: 0.05073565138933025		[learning rate: 0.00070555]
	Learning Rate: 0.000705548
	LOSS [training: 0.046153508782840864 | validation: 0.047769988187136465]
	TIME [epoch: 8.11 sec]
EPOCH 598/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05601920313923152		[learning rate: 0.00070384]
		[batch 20/20] avg loss: 0.05918166836266368		[learning rate: 0.00070214]
	Learning Rate: 0.000702136
	LOSS [training: 0.0576004357509476 | validation: 0.03665737098596884]
	TIME [epoch: 8.1 sec]
EPOCH 599/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.034023081621792815		[learning rate: 0.00070044]
		[batch 20/20] avg loss: 0.03944905257588459		[learning rate: 0.00069874]
	Learning Rate: 0.00069874
	LOSS [training: 0.036736067098838705 | validation: 0.04817703819455209]
	TIME [epoch: 8.1 sec]
EPOCH 600/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09755208404351842		[learning rate: 0.00069705]
		[batch 20/20] avg loss: 0.04283545194858397		[learning rate: 0.00069536]
	Learning Rate: 0.000695361
	LOSS [training: 0.07019376799605118 | validation: 0.042108030968756024]
	TIME [epoch: 8.13 sec]
EPOCH 601/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05480811678088526		[learning rate: 0.00069368]
		[batch 20/20] avg loss: 0.03759429145449893		[learning rate: 0.000692]
	Learning Rate: 0.000691999
	LOSS [training: 0.046201204117692095 | validation: 0.0321927759116194]
	TIME [epoch: 8.11 sec]
EPOCH 602/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03916618010221973		[learning rate: 0.00069032]
		[batch 20/20] avg loss: 0.03882503655876581		[learning rate: 0.00068865]
	Learning Rate: 0.000688652
	LOSS [training: 0.03899560833049277 | validation: 0.045167525403646144]
	TIME [epoch: 8.11 sec]
EPOCH 603/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05175503553177126		[learning rate: 0.00068699]
		[batch 20/20] avg loss: 0.050364331589426437		[learning rate: 0.00068532]
	Learning Rate: 0.000685322
	LOSS [training: 0.05105968356059885 | validation: 0.05761795809193434]
	TIME [epoch: 8.11 sec]
EPOCH 604/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04614112421547566		[learning rate: 0.00068366]
		[batch 20/20] avg loss: 0.035909762996686856		[learning rate: 0.00068201]
	Learning Rate: 0.000682008
	LOSS [training: 0.041025443606081256 | validation: 0.028743312060887115]
	TIME [epoch: 8.13 sec]
EPOCH 605/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.038086201859778054		[learning rate: 0.00068036]
		[batch 20/20] avg loss: 0.05084823160127042		[learning rate: 0.00067871]
	Learning Rate: 0.00067871
	LOSS [training: 0.044467216730524235 | validation: 0.06181672772558427]
	TIME [epoch: 8.11 sec]
EPOCH 606/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05321335202556647		[learning rate: 0.00067707]
		[batch 20/20] avg loss: 0.05011761870287497		[learning rate: 0.00067543]
	Learning Rate: 0.000675428
	LOSS [training: 0.051665485364220734 | validation: 0.042171279327754815]
	TIME [epoch: 8.1 sec]
EPOCH 607/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05799631027076728		[learning rate: 0.00067379]
		[batch 20/20] avg loss: 0.03184886528384878		[learning rate: 0.00067216]
	Learning Rate: 0.000672162
	LOSS [training: 0.04492258777730804 | validation: 0.03467365597667587]
	TIME [epoch: 8.1 sec]
EPOCH 608/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04084674446690974		[learning rate: 0.00067053]
		[batch 20/20] avg loss: 0.04213368934889611		[learning rate: 0.00066891]
	Learning Rate: 0.000668911
	LOSS [training: 0.04149021690790292 | validation: 0.03913430917041769]
	TIME [epoch: 8.13 sec]
EPOCH 609/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04545495786883429		[learning rate: 0.00066729]
		[batch 20/20] avg loss: 0.04031908683978215		[learning rate: 0.00066568]
	Learning Rate: 0.000665676
	LOSS [training: 0.04288702235430822 | validation: 0.040328851172814685]
	TIME [epoch: 8.11 sec]
EPOCH 610/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04376765823968649		[learning rate: 0.00066406]
		[batch 20/20] avg loss: 0.05055527603784508		[learning rate: 0.00066246]
	Learning Rate: 0.000662457
	LOSS [training: 0.04716146713876578 | validation: 0.04843084259427628]
	TIME [epoch: 8.1 sec]
EPOCH 611/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06114551225250579		[learning rate: 0.00066085]
		[batch 20/20] avg loss: 0.07094776031030649		[learning rate: 0.00065925]
	Learning Rate: 0.000659254
	LOSS [training: 0.06604663628140614 | validation: 0.05770616699212012]
	TIME [epoch: 8.11 sec]
EPOCH 612/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07169918114141975		[learning rate: 0.00065766]
		[batch 20/20] avg loss: 0.05063865635365673		[learning rate: 0.00065607]
	Learning Rate: 0.000656066
	LOSS [training: 0.061168918747538235 | validation: 0.07560765217871974]
	TIME [epoch: 8.13 sec]
EPOCH 613/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08315670309980658		[learning rate: 0.00065448]
		[batch 20/20] avg loss: 0.0544000670792403		[learning rate: 0.00065289]
	Learning Rate: 0.000652893
	LOSS [training: 0.06877838508952344 | validation: 0.03774173292709465]
	TIME [epoch: 8.11 sec]
EPOCH 614/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04916713637296912		[learning rate: 0.00065131]
		[batch 20/20] avg loss: 0.06635043054528546		[learning rate: 0.00064974]
	Learning Rate: 0.000649736
	LOSS [training: 0.05775878345912729 | validation: 0.04706225798916962]
	TIME [epoch: 8.11 sec]
EPOCH 615/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0564283639206649		[learning rate: 0.00064816]
		[batch 20/20] avg loss: 0.03389301172344227		[learning rate: 0.00064659]
	Learning Rate: 0.000646594
	LOSS [training: 0.04516068782205359 | validation: 0.031214457359506013]
	TIME [epoch: 8.11 sec]
EPOCH 616/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.039791462028087005		[learning rate: 0.00064503]
		[batch 20/20] avg loss: 0.03690860371731784		[learning rate: 0.00064347]
	Learning Rate: 0.000643467
	LOSS [training: 0.03835003287270242 | validation: 0.03545483897497895]
	TIME [epoch: 8.13 sec]
EPOCH 617/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.041189413576868436		[learning rate: 0.00064191]
		[batch 20/20] avg loss: 0.06597930403880184		[learning rate: 0.00064036]
	Learning Rate: 0.000640355
	LOSS [training: 0.05358435880783513 | validation: 0.041008383653363074]
	TIME [epoch: 8.12 sec]
EPOCH 618/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.048822433925150246		[learning rate: 0.00063881]
		[batch 20/20] avg loss: 0.04715449004082863		[learning rate: 0.00063726]
	Learning Rate: 0.000637259
	LOSS [training: 0.04798846198298944 | validation: 0.07474426944129606]
	TIME [epoch: 8.11 sec]
EPOCH 619/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05714077085149073		[learning rate: 0.00063572]
		[batch 20/20] avg loss: 0.05531703925443436		[learning rate: 0.00063418]
	Learning Rate: 0.000634177
	LOSS [training: 0.05622890505296254 | validation: 0.03961216259903297]
	TIME [epoch: 8.11 sec]
EPOCH 620/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05625796390386554		[learning rate: 0.00063264]
		[batch 20/20] avg loss: 0.04582303604538021		[learning rate: 0.00063111]
	Learning Rate: 0.00063111
	LOSS [training: 0.051040499974622865 | validation: 0.03015135538347073]
	TIME [epoch: 8.13 sec]
EPOCH 621/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.029515742759324677		[learning rate: 0.00062958]
		[batch 20/20] avg loss: 0.051483169231407166		[learning rate: 0.00062806]
	Learning Rate: 0.000628058
	LOSS [training: 0.04049945599536592 | validation: 0.08152315160888243]
	TIME [epoch: 8.12 sec]
EPOCH 622/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04723329454241869		[learning rate: 0.00062654]
		[batch 20/20] avg loss: 0.07672826292524527		[learning rate: 0.00062502]
	Learning Rate: 0.000625021
	LOSS [training: 0.061980778733831975 | validation: 0.05927682585734767]
	TIME [epoch: 8.12 sec]
EPOCH 623/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05344048370208608		[learning rate: 0.00062351]
		[batch 20/20] avg loss: 0.05529586876456864		[learning rate: 0.000622]
	Learning Rate: 0.000621999
	LOSS [training: 0.05436817623332737 | validation: 0.05074971494948161]
	TIME [epoch: 8.12 sec]
EPOCH 624/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04792952443653045		[learning rate: 0.00062049]
		[batch 20/20] avg loss: 0.06984211188393177		[learning rate: 0.00061899]
	Learning Rate: 0.000618991
	LOSS [training: 0.0588858181602311 | validation: 0.06983620195493005]
	TIME [epoch: 8.14 sec]
EPOCH 625/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04900266959100824		[learning rate: 0.00061749]
		[batch 20/20] avg loss: 0.039930444107537544		[learning rate: 0.000616]
	Learning Rate: 0.000615997
	LOSS [training: 0.0444665568492729 | validation: 0.03980463040430821]
	TIME [epoch: 8.13 sec]
EPOCH 626/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05671674550406821		[learning rate: 0.00061451]
		[batch 20/20] avg loss: 0.04204430277518902		[learning rate: 0.00061302]
	Learning Rate: 0.000613019
	LOSS [training: 0.04938052413962861 | validation: 0.03945631843038651]
	TIME [epoch: 8.12 sec]
EPOCH 627/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04365673709775176		[learning rate: 0.00061153]
		[batch 20/20] avg loss: 0.06155009836630062		[learning rate: 0.00061005]
	Learning Rate: 0.000610054
	LOSS [training: 0.05260341773202618 | validation: 0.07633941295275029]
	TIME [epoch: 8.11 sec]
EPOCH 628/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05767121533500238		[learning rate: 0.00060858]
		[batch 20/20] avg loss: 0.047786668373957966		[learning rate: 0.0006071]
	Learning Rate: 0.000607104
	LOSS [training: 0.05272894185448018 | validation: 0.13890541112375182]
	TIME [epoch: 8.13 sec]
EPOCH 629/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06840898940586129		[learning rate: 0.00060563]
		[batch 20/20] avg loss: 0.06315136667260297		[learning rate: 0.00060417]
	Learning Rate: 0.000604168
	LOSS [training: 0.06578017803923211 | validation: 0.0386842056187304]
	TIME [epoch: 8.12 sec]
EPOCH 630/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03466447941210178		[learning rate: 0.00060271]
		[batch 20/20] avg loss: 0.04574593394909163		[learning rate: 0.00060125]
	Learning Rate: 0.000601247
	LOSS [training: 0.040205206680596704 | validation: 0.04246476283569475]
	TIME [epoch: 8.12 sec]
EPOCH 631/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.052519353197429085		[learning rate: 0.00059979]
		[batch 20/20] avg loss: 0.05317373448299405		[learning rate: 0.00059834]
	Learning Rate: 0.000598339
	LOSS [training: 0.052846543840211556 | validation: 0.08164785986612089]
	TIME [epoch: 8.12 sec]
EPOCH 632/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06495910957825221		[learning rate: 0.00059689]
		[batch 20/20] avg loss: 0.03503651418705582		[learning rate: 0.00059545]
	Learning Rate: 0.000595446
	LOSS [training: 0.049997811882654015 | validation: 0.05210408134830455]
	TIME [epoch: 8.12 sec]
EPOCH 633/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.040536791972293276		[learning rate: 0.000594]
		[batch 20/20] avg loss: 0.05433456505793307		[learning rate: 0.00059257]
	Learning Rate: 0.000592566
	LOSS [training: 0.04743567851511317 | validation: 0.03503774065696061]
	TIME [epoch: 8.14 sec]
EPOCH 634/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.036685544927246905		[learning rate: 0.00059113]
		[batch 20/20] avg loss: 0.05094389778357248		[learning rate: 0.0005897]
	Learning Rate: 0.000589701
	LOSS [training: 0.043814721355409696 | validation: 0.03858955322860428]
	TIME [epoch: 8.12 sec]
EPOCH 635/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06430739616378382		[learning rate: 0.00058827]
		[batch 20/20] avg loss: 0.05167914816138761		[learning rate: 0.00058685]
	Learning Rate: 0.000586849
	LOSS [training: 0.05799327216258573 | validation: 0.06452346552150345]
	TIME [epoch: 8.12 sec]
EPOCH 636/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04527707423803799		[learning rate: 0.00058543]
		[batch 20/20] avg loss: 0.04865901475311375		[learning rate: 0.00058401]
	Learning Rate: 0.000584011
	LOSS [training: 0.04696804449557587 | validation: 0.04470149156686243]
	TIME [epoch: 8.12 sec]
EPOCH 637/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0422690673637697		[learning rate: 0.0005826]
		[batch 20/20] avg loss: 0.043499032184342507		[learning rate: 0.00058119]
	Learning Rate: 0.000581187
	LOSS [training: 0.0428840497740561 | validation: 0.024337654518210936]
	TIME [epoch: 8.15 sec]
EPOCH 638/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04883681450135145		[learning rate: 0.00057978]
		[batch 20/20] avg loss: 0.045791188121209324		[learning rate: 0.00057838]
	Learning Rate: 0.000578376
	LOSS [training: 0.0473140013112804 | validation: 0.03409113475088994]
	TIME [epoch: 8.12 sec]
EPOCH 639/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03505371320928244		[learning rate: 0.00057698]
		[batch 20/20] avg loss: 0.03814902076853838		[learning rate: 0.00057558]
	Learning Rate: 0.000575579
	LOSS [training: 0.03660136698891041 | validation: 0.0287849924933265]
	TIME [epoch: 8.12 sec]
EPOCH 640/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.057234442843317676		[learning rate: 0.00057419]
		[batch 20/20] avg loss: 0.06124703244650368		[learning rate: 0.0005728]
	Learning Rate: 0.000572796
	LOSS [training: 0.05924073764491067 | validation: 0.05564293747229748]
	TIME [epoch: 8.12 sec]
EPOCH 641/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.044050372476842126		[learning rate: 0.00057141]
		[batch 20/20] avg loss: 0.04310366059448077		[learning rate: 0.00057003]
	Learning Rate: 0.000570026
	LOSS [training: 0.04357701653566145 | validation: 0.05571372859268206]
	TIME [epoch: 8.14 sec]
EPOCH 642/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04332313650158346		[learning rate: 0.00056865]
		[batch 20/20] avg loss: 0.040345809850203224		[learning rate: 0.00056727]
	Learning Rate: 0.00056727
	LOSS [training: 0.04183447317589333 | validation: 0.043067924349294975]
	TIME [epoch: 8.11 sec]
EPOCH 643/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05402381810648464		[learning rate: 0.0005659]
		[batch 20/20] avg loss: 0.053252493782153555		[learning rate: 0.00056453]
	Learning Rate: 0.000564526
	LOSS [training: 0.05363815594431911 | validation: 0.051197903023543934]
	TIME [epoch: 8.12 sec]
EPOCH 644/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04011918597378102		[learning rate: 0.00056316]
		[batch 20/20] avg loss: 0.05628715388777879		[learning rate: 0.0005618]
	Learning Rate: 0.000561796
	LOSS [training: 0.04820316993077991 | validation: 0.03160540793739072]
	TIME [epoch: 8.11 sec]
EPOCH 645/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03621117927611771		[learning rate: 0.00056044]
		[batch 20/20] avg loss: 0.05029078672076457		[learning rate: 0.00055908]
	Learning Rate: 0.00055908
	LOSS [training: 0.04325098299844114 | validation: 0.046508122251692374]
	TIME [epoch: 8.13 sec]
EPOCH 646/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04206996457963187		[learning rate: 0.00055773]
		[batch 20/20] avg loss: 0.06406298330350847		[learning rate: 0.00055638]
	Learning Rate: 0.000556376
	LOSS [training: 0.05306647394157016 | validation: 0.03250322765186728]
	TIME [epoch: 8.12 sec]
EPOCH 647/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06948566866045111		[learning rate: 0.00055503]
		[batch 20/20] avg loss: 0.06063956943477724		[learning rate: 0.00055369]
	Learning Rate: 0.000553685
	LOSS [training: 0.06506261904761416 | validation: 0.06144933752570377]
	TIME [epoch: 8.12 sec]
EPOCH 648/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.036384826468639914		[learning rate: 0.00055235]
		[batch 20/20] avg loss: 0.08106732441352849		[learning rate: 0.00055101]
	Learning Rate: 0.000551008
	LOSS [training: 0.0587260754410842 | validation: 0.04325345858749066]
	TIME [epoch: 8.12 sec]
EPOCH 649/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.039439290657331		[learning rate: 0.00054967]
		[batch 20/20] avg loss: 0.038808027526413505		[learning rate: 0.00054834]
	Learning Rate: 0.000548343
	LOSS [training: 0.039123659091872244 | validation: 0.0473703333630697]
	TIME [epoch: 8.14 sec]
EPOCH 650/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03739938822523886		[learning rate: 0.00054702]
		[batch 20/20] avg loss: 0.03999324307717474		[learning rate: 0.00054569]
	Learning Rate: 0.000545692
	LOSS [training: 0.0386963156512068 | validation: 0.03291467198168153]
	TIME [epoch: 8.12 sec]
EPOCH 651/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04339833076156281		[learning rate: 0.00054437]
		[batch 20/20] avg loss: 0.03634716176831982		[learning rate: 0.00054305]
	Learning Rate: 0.000543053
	LOSS [training: 0.039872746264941306 | validation: 0.041900713267491056]
	TIME [epoch: 8.12 sec]
EPOCH 652/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06082145206608852		[learning rate: 0.00054174]
		[batch 20/20] avg loss: 0.050592865582442303		[learning rate: 0.00054043]
	Learning Rate: 0.000540427
	LOSS [training: 0.055707158824265426 | validation: 0.0457234119149766]
	TIME [epoch: 8.12 sec]
EPOCH 653/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.056206868423935505		[learning rate: 0.00053912]
		[batch 20/20] avg loss: 0.0508969343930598		[learning rate: 0.00053781]
	Learning Rate: 0.000537813
	LOSS [training: 0.05355190140849766 | validation: 0.023279024472839714]
	TIME [epoch: 8.14 sec]
EPOCH 654/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.041551470855227875		[learning rate: 0.00053651]
		[batch 20/20] avg loss: 0.059497001355354		[learning rate: 0.00053521]
	Learning Rate: 0.000535213
	LOSS [training: 0.05052423610529092 | validation: 0.032432981392885044]
	TIME [epoch: 8.12 sec]
EPOCH 655/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.051968975607152065		[learning rate: 0.00053392]
		[batch 20/20] avg loss: 0.03252545940680341		[learning rate: 0.00053262]
	Learning Rate: 0.000532624
	LOSS [training: 0.04224721750697774 | validation: 0.024395356038096398]
	TIME [epoch: 8.12 sec]
EPOCH 656/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03679710393127562		[learning rate: 0.00053134]
		[batch 20/20] avg loss: 0.05928026247960163		[learning rate: 0.00053005]
	Learning Rate: 0.000530049
	LOSS [training: 0.04803868320543862 | validation: 0.04676658178047595]
	TIME [epoch: 8.11 sec]
EPOCH 657/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06833028001488542		[learning rate: 0.00052877]
		[batch 20/20] avg loss: 0.034954852190647		[learning rate: 0.00052749]
	Learning Rate: 0.000527485
	LOSS [training: 0.051642566102766206 | validation: 0.04415101542384044]
	TIME [epoch: 8.14 sec]
EPOCH 658/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04372980786982819		[learning rate: 0.00052621]
		[batch 20/20] avg loss: 0.05428876600018081		[learning rate: 0.00052493]
	Learning Rate: 0.000524935
	LOSS [training: 0.049009286935004504 | validation: 0.06859760623200648]
	TIME [epoch: 8.12 sec]
EPOCH 659/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04280731193966568		[learning rate: 0.00052366]
		[batch 20/20] avg loss: 0.05198470957613963		[learning rate: 0.0005224]
	Learning Rate: 0.000522396
	LOSS [training: 0.04739601075790266 | validation: 0.04192436957881722]
	TIME [epoch: 8.12 sec]
EPOCH 660/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03752994931351233		[learning rate: 0.00052113]
		[batch 20/20] avg loss: 0.05529590271182157		[learning rate: 0.00051987]
	Learning Rate: 0.00051987
	LOSS [training: 0.04641292601266696 | validation: 0.030911654070660936]
	TIME [epoch: 8.12 sec]
EPOCH 661/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03672546830238682		[learning rate: 0.00051861]
		[batch 20/20] avg loss: 0.044633432940885474		[learning rate: 0.00051736]
	Learning Rate: 0.000517356
	LOSS [training: 0.04067945062163615 | validation: 0.031532550051486564]
	TIME [epoch: 8.14 sec]
EPOCH 662/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05136806065353239		[learning rate: 0.0005161]
		[batch 20/20] avg loss: 0.03988852063611411		[learning rate: 0.00051485]
	Learning Rate: 0.000514854
	LOSS [training: 0.045628290644823245 | validation: 0.060310069923168105]
	TIME [epoch: 8.12 sec]
EPOCH 663/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03510151524167474		[learning rate: 0.00051361]
		[batch 20/20] avg loss: 0.025774668991803373		[learning rate: 0.00051236]
	Learning Rate: 0.000512364
	LOSS [training: 0.030438092116739056 | validation: 0.036373356279016555]
	TIME [epoch: 8.11 sec]
EPOCH 664/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04070652707779661		[learning rate: 0.00051112]
		[batch 20/20] avg loss: 0.04329947722045973		[learning rate: 0.00050989]
	Learning Rate: 0.000509887
	LOSS [training: 0.04200300214912816 | validation: 0.0826954403748801]
	TIME [epoch: 8.12 sec]
EPOCH 665/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05866538480971193		[learning rate: 0.00050865]
		[batch 20/20] avg loss: 0.03296929428853039		[learning rate: 0.00050742]
	Learning Rate: 0.000507421
	LOSS [training: 0.045817339549121164 | validation: 0.0452118460885168]
	TIME [epoch: 8.13 sec]
EPOCH 666/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030443651272461642		[learning rate: 0.00050619]
		[batch 20/20] avg loss: 0.04263351530999617		[learning rate: 0.00050497]
	Learning Rate: 0.000504967
	LOSS [training: 0.0365385832912289 | validation: 0.08953104576109902]
	TIME [epoch: 8.12 sec]
EPOCH 667/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04723958830934151		[learning rate: 0.00050374]
		[batch 20/20] avg loss: 0.03673101129972832		[learning rate: 0.00050253]
	Learning Rate: 0.000502525
	LOSS [training: 0.04198529980453491 | validation: 0.04429967527611159]
	TIME [epoch: 8.11 sec]
EPOCH 668/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03972895536616993		[learning rate: 0.00050131]
		[batch 20/20] avg loss: 0.04413023248769791		[learning rate: 0.0005001]
	Learning Rate: 0.000500095
	LOSS [training: 0.04192959392693392 | validation: 0.05012631780752828]
	TIME [epoch: 8.11 sec]
EPOCH 669/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.040298540959533585		[learning rate: 0.00049888]
		[batch 20/20] avg loss: 0.04022261126707098		[learning rate: 0.00049768]
	Learning Rate: 0.000497677
	LOSS [training: 0.04026057611330229 | validation: 0.03379166674926867]
	TIME [epoch: 8.13 sec]
EPOCH 670/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0398281944395309		[learning rate: 0.00049647]
		[batch 20/20] avg loss: 0.04894273402545795		[learning rate: 0.00049527]
	Learning Rate: 0.00049527
	LOSS [training: 0.04438546423249443 | validation: 0.08388626304103983]
	TIME [epoch: 8.12 sec]
EPOCH 671/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05252306400479677		[learning rate: 0.00049407]
		[batch 20/20] avg loss: 0.03608431265157068		[learning rate: 0.00049288]
	Learning Rate: 0.000492875
	LOSS [training: 0.044303688328183724 | validation: 0.02856437120058227]
	TIME [epoch: 8.11 sec]
EPOCH 672/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.031514526057440984		[learning rate: 0.00049168]
		[batch 20/20] avg loss: 0.033418620447816565		[learning rate: 0.00049049]
	Learning Rate: 0.000490492
	LOSS [training: 0.03246657325262878 | validation: 0.04654352781729694]
	TIME [epoch: 8.12 sec]
EPOCH 673/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02780513553269528		[learning rate: 0.0004893]
		[batch 20/20] avg loss: 0.04141046830999621		[learning rate: 0.00048812]
	Learning Rate: 0.00048812
	LOSS [training: 0.03460780192134574 | validation: 0.05410942130686405]
	TIME [epoch: 8.14 sec]
EPOCH 674/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04285687457937078		[learning rate: 0.00048694]
		[batch 20/20] avg loss: 0.03197602919443819		[learning rate: 0.00048576]
	Learning Rate: 0.000485759
	LOSS [training: 0.03741645188690449 | validation: 0.03721805377682511]
	TIME [epoch: 8.13 sec]
EPOCH 675/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03958344247507541		[learning rate: 0.00048458]
		[batch 20/20] avg loss: 0.05120172394108178		[learning rate: 0.00048341]
	Learning Rate: 0.00048341
	LOSS [training: 0.045392583208078596 | validation: 0.038874497096239614]
	TIME [epoch: 8.11 sec]
EPOCH 676/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03116567683344565		[learning rate: 0.00048224]
		[batch 20/20] avg loss: 0.032064096596237955		[learning rate: 0.00048107]
	Learning Rate: 0.000481072
	LOSS [training: 0.0316148867148418 | validation: 0.043785173599356554]
	TIME [epoch: 8.12 sec]
EPOCH 677/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04266394622194013		[learning rate: 0.00047991]
		[batch 20/20] avg loss: 0.05159715197660934		[learning rate: 0.00047875]
	Learning Rate: 0.000478746
	LOSS [training: 0.04713054909927472 | validation: 0.09895340867238828]
	TIME [epoch: 8.13 sec]
EPOCH 678/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04885015090920718		[learning rate: 0.00047759]
		[batch 20/20] avg loss: 0.05042841870017374		[learning rate: 0.00047643]
	Learning Rate: 0.000476431
	LOSS [training: 0.049639284804690466 | validation: 0.031673497504702275]
	TIME [epoch: 8.13 sec]
EPOCH 679/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04403075061710322		[learning rate: 0.00047528]
		[batch 20/20] avg loss: 0.048994473186331614		[learning rate: 0.00047413]
	Learning Rate: 0.000474127
	LOSS [training: 0.04651261190171742 | validation: 0.0421100880778181]
	TIME [epoch: 8.11 sec]
EPOCH 680/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.042315239729099494		[learning rate: 0.00047298]
		[batch 20/20] avg loss: 0.05422859660003955		[learning rate: 0.00047183]
	Learning Rate: 0.000471834
	LOSS [training: 0.048271918164569536 | validation: 0.027125856833173655]
	TIME [epoch: 8.11 sec]
EPOCH 681/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0354453542307905		[learning rate: 0.00047069]
		[batch 20/20] avg loss: 0.04516911495584504		[learning rate: 0.00046955]
	Learning Rate: 0.000469553
	LOSS [training: 0.040307234593317774 | validation: 0.08451735258997743]
	TIME [epoch: 8.13 sec]
EPOCH 682/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.039888877144899595		[learning rate: 0.00046842]
		[batch 20/20] avg loss: 0.042709454504673845		[learning rate: 0.00046728]
	Learning Rate: 0.000467282
	LOSS [training: 0.04129916582478672 | validation: 0.029553841240469038]
	TIME [epoch: 8.13 sec]
EPOCH 683/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02693023149886179		[learning rate: 0.00046615]
		[batch 20/20] avg loss: 0.029194978899993302		[learning rate: 0.00046502]
	Learning Rate: 0.000465022
	LOSS [training: 0.02806260519942754 | validation: 0.024107856618348474]
	TIME [epoch: 8.12 sec]
EPOCH 684/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.044921299666245425		[learning rate: 0.0004639]
		[batch 20/20] avg loss: 0.04434651148084928		[learning rate: 0.00046277]
	Learning Rate: 0.000462773
	LOSS [training: 0.04463390557354736 | validation: 0.03802106263375101]
	TIME [epoch: 8.11 sec]
EPOCH 685/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04264552885947522		[learning rate: 0.00046165]
		[batch 20/20] avg loss: 0.03202974620901729		[learning rate: 0.00046054]
	Learning Rate: 0.000460536
	LOSS [training: 0.03733763753424625 | validation: 0.03383601373725794]
	TIME [epoch: 8.11 sec]
EPOCH 686/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03636404323156668		[learning rate: 0.00045942]
		[batch 20/20] avg loss: 0.038358507791388226		[learning rate: 0.00045831]
	Learning Rate: 0.000458309
	LOSS [training: 0.03736127551147746 | validation: 0.027379875679594014]
	TIME [epoch: 8.14 sec]
EPOCH 687/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03686264366367249		[learning rate: 0.0004572]
		[batch 20/20] avg loss: 0.0382315468917782		[learning rate: 0.00045609]
	Learning Rate: 0.000456092
	LOSS [training: 0.03754709527772535 | validation: 0.02140280530099322]
	TIME [epoch: 8.12 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_687.pth
	Model improved!!!
EPOCH 688/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.042075574839099183		[learning rate: 0.00045499]
		[batch 20/20] avg loss: 0.03983633114659231		[learning rate: 0.00045389]
	Learning Rate: 0.000453887
	LOSS [training: 0.04095595299284575 | validation: 0.026610881388905664]
	TIME [epoch: 8.12 sec]
EPOCH 689/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02308814174530904		[learning rate: 0.00045279]
		[batch 20/20] avg loss: 0.06298218109295875		[learning rate: 0.00045169]
	Learning Rate: 0.000451692
	LOSS [training: 0.0430351614191339 | validation: 0.035708216497889876]
	TIME [epoch: 8.11 sec]
EPOCH 690/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03345452334580725		[learning rate: 0.0004506]
		[batch 20/20] avg loss: 0.04300724889554647		[learning rate: 0.00044951]
	Learning Rate: 0.000449507
	LOSS [training: 0.03823088612067686 | validation: 0.02562460929902859]
	TIME [epoch: 8.14 sec]
EPOCH 691/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03296249786588201		[learning rate: 0.00044842]
		[batch 20/20] avg loss: 0.05284641361461503		[learning rate: 0.00044733]
	Learning Rate: 0.000447334
	LOSS [training: 0.042904455740248516 | validation: 0.02470821787422236]
	TIME [epoch: 8.11 sec]
EPOCH 692/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024274999292585024		[learning rate: 0.00044625]
		[batch 20/20] avg loss: 0.04916481324815302		[learning rate: 0.00044517]
	Learning Rate: 0.00044517
	LOSS [training: 0.03671990627036903 | validation: 0.03426328023914536]
	TIME [epoch: 8.12 sec]
EPOCH 693/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.021874872659402765		[learning rate: 0.00044409]
		[batch 20/20] avg loss: 0.03639975310636754		[learning rate: 0.00044302]
	Learning Rate: 0.000443018
	LOSS [training: 0.029137312882885145 | validation: 0.023429758492483476]
	TIME [epoch: 8.11 sec]
EPOCH 694/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03833962274572682		[learning rate: 0.00044195]
		[batch 20/20] avg loss: 0.030145830056485724		[learning rate: 0.00044088]
	Learning Rate: 0.000440875
	LOSS [training: 0.034242726401106274 | validation: 0.03554816643422275]
	TIME [epoch: 8.14 sec]
EPOCH 695/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04636345214917727		[learning rate: 0.00043981]
		[batch 20/20] avg loss: 0.02907559397938253		[learning rate: 0.00043874]
	Learning Rate: 0.000438743
	LOSS [training: 0.0377195230642799 | validation: 0.033820014218834404]
	TIME [epoch: 8.11 sec]
EPOCH 696/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.036484702178873604		[learning rate: 0.00043768]
		[batch 20/20] avg loss: 0.035783515047507056		[learning rate: 0.00043662]
	Learning Rate: 0.000436622
	LOSS [training: 0.03613410861319033 | validation: 0.026033972088918462]
	TIME [epoch: 8.11 sec]
EPOCH 697/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03947371261456359		[learning rate: 0.00043556]
		[batch 20/20] avg loss: 0.03129657019737482		[learning rate: 0.00043451]
	Learning Rate: 0.00043451
	LOSS [training: 0.035385141405969205 | validation: 0.025980886606661605]
	TIME [epoch: 8.11 sec]
EPOCH 698/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.040103293494336056		[learning rate: 0.00043346]
		[batch 20/20] avg loss: 0.03816247059402472		[learning rate: 0.00043241]
	Learning Rate: 0.000432409
	LOSS [training: 0.0391328820441804 | validation: 0.06691747643661485]
	TIME [epoch: 8.15 sec]
EPOCH 699/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.048366499652935685		[learning rate: 0.00043136]
		[batch 20/20] avg loss: 0.04458896549529205		[learning rate: 0.00043032]
	Learning Rate: 0.000430318
	LOSS [training: 0.046477732574113875 | validation: 0.030953874214636763]
	TIME [epoch: 8.11 sec]
EPOCH 700/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030190570058489952		[learning rate: 0.00042928]
		[batch 20/20] avg loss: 0.027436865529951192		[learning rate: 0.00042824]
	Learning Rate: 0.000428237
	LOSS [training: 0.028813717794220577 | validation: 0.07464675555712827]
	TIME [epoch: 8.11 sec]
EPOCH 701/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04154826218387369		[learning rate: 0.0004272]
		[batch 20/20] avg loss: 0.03578926761815361		[learning rate: 0.00042617]
	Learning Rate: 0.000426166
	LOSS [training: 0.038668764901013654 | validation: 0.026842673558782285]
	TIME [epoch: 8.11 sec]
EPOCH 702/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07413628701002044		[learning rate: 0.00042513]
		[batch 20/20] avg loss: 0.033400584945256995		[learning rate: 0.00042411]
	Learning Rate: 0.000424105
	LOSS [training: 0.053768435977638716 | validation: 0.020171017079015804]
	TIME [epoch: 8.13 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_702.pth
	Model improved!!!
EPOCH 703/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03319582415930631		[learning rate: 0.00042308]
		[batch 20/20] avg loss: 0.031664692433388916		[learning rate: 0.00042205]
	Learning Rate: 0.000422054
	LOSS [training: 0.03243025829634762 | validation: 0.025765532653626938]
	TIME [epoch: 8.11 sec]
EPOCH 704/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03724037739185663		[learning rate: 0.00042103]
		[batch 20/20] avg loss: 0.04110027292976007		[learning rate: 0.00042001]
	Learning Rate: 0.000420013
	LOSS [training: 0.03917032516080835 | validation: 0.02958990890617155]
	TIME [epoch: 8.09 sec]
EPOCH 705/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.027553578019689696		[learning rate: 0.000419]
		[batch 20/20] avg loss: 0.03430827507431597		[learning rate: 0.00041798]
	Learning Rate: 0.000417982
	LOSS [training: 0.030930926547002822 | validation: 0.029334859932474713]
	TIME [epoch: 8.1 sec]
EPOCH 706/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04767277175364124		[learning rate: 0.00041697]
		[batch 20/20] avg loss: 0.042161783493997004		[learning rate: 0.00041596]
	Learning Rate: 0.000415961
	LOSS [training: 0.04491727762381912 | validation: 0.047521384541516776]
	TIME [epoch: 8.12 sec]
EPOCH 707/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.040324833447387146		[learning rate: 0.00041495]
		[batch 20/20] avg loss: 0.027420590474441692		[learning rate: 0.00041395]
	Learning Rate: 0.00041395
	LOSS [training: 0.03387271196091442 | validation: 0.02270866403167097]
	TIME [epoch: 8.11 sec]
EPOCH 708/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.033433768860348975		[learning rate: 0.00041295]
		[batch 20/20] avg loss: 0.04204514115529311		[learning rate: 0.00041195]
	Learning Rate: 0.000411948
	LOSS [training: 0.03773945500782105 | validation: 0.0329673425964242]
	TIME [epoch: 8.09 sec]
EPOCH 709/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024755728765254965		[learning rate: 0.00041095]
		[batch 20/20] avg loss: 0.042209748797054115		[learning rate: 0.00040996]
	Learning Rate: 0.000409956
	LOSS [training: 0.03348273878115453 | validation: 0.025618990582662027]
	TIME [epoch: 8.1 sec]
EPOCH 710/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.031466098758967956		[learning rate: 0.00040896]
		[batch 20/20] avg loss: 0.03755132222923026		[learning rate: 0.00040797]
	Learning Rate: 0.000407973
	LOSS [training: 0.03450871049409911 | validation: 0.028926133636667326]
	TIME [epoch: 8.12 sec]
EPOCH 711/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.040918329708615885		[learning rate: 0.00040699]
		[batch 20/20] avg loss: 0.044299145907638816		[learning rate: 0.000406]
	Learning Rate: 0.000406
	LOSS [training: 0.04260873780812735 | validation: 0.043150356905490315]
	TIME [epoch: 8.1 sec]
EPOCH 712/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03865044903882087		[learning rate: 0.00040502]
		[batch 20/20] avg loss: 0.06720705035707907		[learning rate: 0.00040404]
	Learning Rate: 0.000404037
	LOSS [training: 0.05292874969794996 | validation: 0.053729803169032045]
	TIME [epoch: 8.1 sec]
EPOCH 713/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028697518499719233		[learning rate: 0.00040306]
		[batch 20/20] avg loss: 0.04126776395755593		[learning rate: 0.00040208]
	Learning Rate: 0.000402083
	LOSS [training: 0.034982641228637576 | validation: 0.05059542132393144]
	TIME [epoch: 8.1 sec]
EPOCH 714/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05374025541388966		[learning rate: 0.00040111]
		[batch 20/20] avg loss: 0.04997629358189803		[learning rate: 0.00040014]
	Learning Rate: 0.000400139
	LOSS [training: 0.05185827449789384 | validation: 0.052141761671886416]
	TIME [epoch: 8.12 sec]
EPOCH 715/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03247646454397479		[learning rate: 0.00039917]
		[batch 20/20] avg loss: 0.03790251701557394		[learning rate: 0.0003982]
	Learning Rate: 0.000398204
	LOSS [training: 0.03518949077977436 | validation: 0.0579073901127604]
	TIME [epoch: 8.1 sec]
EPOCH 716/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05109372643840464		[learning rate: 0.00039724]
		[batch 20/20] avg loss: 0.03686972559408615		[learning rate: 0.00039628]
	Learning Rate: 0.000396278
	LOSS [training: 0.0439817260162454 | validation: 0.027508342666968957]
	TIME [epoch: 8.1 sec]
EPOCH 717/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02976687461629012		[learning rate: 0.00039532]
		[batch 20/20] avg loss: 0.04253606406000427		[learning rate: 0.00039436]
	Learning Rate: 0.000394362
	LOSS [training: 0.03615146933814719 | validation: 0.037632211151858885]
	TIME [epoch: 8.09 sec]
EPOCH 718/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04080021959260403		[learning rate: 0.00039341]
		[batch 20/20] avg loss: 0.038584124679759066		[learning rate: 0.00039245]
	Learning Rate: 0.000392455
	LOSS [training: 0.03969217213618155 | validation: 0.02837082233733867]
	TIME [epoch: 8.12 sec]
EPOCH 719/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03933242295093073		[learning rate: 0.0003915]
		[batch 20/20] avg loss: 0.0447022637399377		[learning rate: 0.00039056]
	Learning Rate: 0.000390557
	LOSS [training: 0.042017343345434215 | validation: 0.05710097329703101]
	TIME [epoch: 8.1 sec]
EPOCH 720/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03533182261161465		[learning rate: 0.00038961]
		[batch 20/20] avg loss: 0.04074183686759782		[learning rate: 0.00038867]
	Learning Rate: 0.000388668
	LOSS [training: 0.03803682973960624 | validation: 0.04149398434922466]
	TIME [epoch: 8.1 sec]
EPOCH 721/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.031183935493293048		[learning rate: 0.00038773]
		[batch 20/20] avg loss: 0.04406454221275056		[learning rate: 0.00038679]
	Learning Rate: 0.000386789
	LOSS [training: 0.0376242388530218 | validation: 0.05750238897534512]
	TIME [epoch: 8.1 sec]
EPOCH 722/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.044230686447167114		[learning rate: 0.00038585]
		[batch 20/20] avg loss: 0.03743106656667693		[learning rate: 0.00038492]
	Learning Rate: 0.000384918
	LOSS [training: 0.04083087650692202 | validation: 0.04378797909893348]
	TIME [epoch: 8.12 sec]
EPOCH 723/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04060989453844861		[learning rate: 0.00038399]
		[batch 20/20] avg loss: 0.0491803812949147		[learning rate: 0.00038306]
	Learning Rate: 0.000383057
	LOSS [training: 0.04489513791668166 | validation: 0.04709442029754079]
	TIME [epoch: 8.1 sec]
EPOCH 724/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03274274442137497		[learning rate: 0.00038213]
		[batch 20/20] avg loss: 0.039176523019788335		[learning rate: 0.0003812]
	Learning Rate: 0.000381204
	LOSS [training: 0.03595963372058165 | validation: 0.01762757761023258]
	TIME [epoch: 8.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_724.pth
	Model improved!!!
EPOCH 725/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02311139663849299		[learning rate: 0.00038028]
		[batch 20/20] avg loss: 0.04334277145535821		[learning rate: 0.00037936]
	Learning Rate: 0.000379361
	LOSS [training: 0.0332270840469256 | validation: 0.04807492573096071]
	TIME [epoch: 8.11 sec]
EPOCH 726/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.047345150722045565		[learning rate: 0.00037844]
		[batch 20/20] avg loss: 0.027054399352997017		[learning rate: 0.00037753]
	Learning Rate: 0.000377526
	LOSS [training: 0.03719977503752128 | validation: 0.03422262352841947]
	TIME [epoch: 8.12 sec]
EPOCH 727/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03448443117854548		[learning rate: 0.00037661]
		[batch 20/20] avg loss: 0.03711882094656667		[learning rate: 0.0003757]
	Learning Rate: 0.000375701
	LOSS [training: 0.03580162606255608 | validation: 0.03686667817030706]
	TIME [epoch: 8.11 sec]
EPOCH 728/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03012893944573975		[learning rate: 0.00037479]
		[batch 20/20] avg loss: 0.0344065464282155		[learning rate: 0.00037388]
	Learning Rate: 0.000373884
	LOSS [training: 0.03226774293697763 | validation: 0.02342025328747034]
	TIME [epoch: 8.09 sec]
EPOCH 729/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.044247681919271205		[learning rate: 0.00037298]
		[batch 20/20] avg loss: 0.024245903307281345		[learning rate: 0.00037208]
	Learning Rate: 0.000372076
	LOSS [training: 0.034246792613276275 | validation: 0.03520476157040808]
	TIME [epoch: 8.1 sec]
EPOCH 730/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.033794256573772295		[learning rate: 0.00037118]
		[batch 20/20] avg loss: 0.03756543126431413		[learning rate: 0.00037028]
	Learning Rate: 0.000370277
	LOSS [training: 0.03567984391904321 | validation: 0.03337768539300628]
	TIME [epoch: 8.1 sec]
EPOCH 731/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03087163828587247		[learning rate: 0.00036938]
		[batch 20/20] avg loss: 0.03260625312625562		[learning rate: 0.00036849]
	Learning Rate: 0.000368486
	LOSS [training: 0.031738945706064045 | validation: 0.02509057268795739]
	TIME [epoch: 8.12 sec]
EPOCH 732/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.027677488701741648		[learning rate: 0.00036759]
		[batch 20/20] avg loss: 0.045517015743086586		[learning rate: 0.0003667]
	Learning Rate: 0.000366704
	LOSS [training: 0.03659725222241411 | validation: 0.017644898124336915]
	TIME [epoch: 8.1 sec]
EPOCH 733/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04936487627339815		[learning rate: 0.00036582]
		[batch 20/20] avg loss: 0.0543134800455131		[learning rate: 0.00036493]
	Learning Rate: 0.000364931
	LOSS [training: 0.05183917815945562 | validation: 0.025851808021341405]
	TIME [epoch: 8.1 sec]
EPOCH 734/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025590183902985657		[learning rate: 0.00036405]
		[batch 20/20] avg loss: 0.04114047644703151		[learning rate: 0.00036317]
	Learning Rate: 0.000363166
	LOSS [training: 0.033365330175008576 | validation: 0.044878691918824806]
	TIME [epoch: 8.1 sec]
EPOCH 735/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03091716688556061		[learning rate: 0.00036229]
		[batch 20/20] avg loss: 0.036041625384718654		[learning rate: 0.00036141]
	Learning Rate: 0.00036141
	LOSS [training: 0.03347939613513963 | validation: 0.0438639386637872]
	TIME [epoch: 8.13 sec]
EPOCH 736/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.036363156804906184		[learning rate: 0.00036053]
		[batch 20/20] avg loss: 0.02680191665823422		[learning rate: 0.00035966]
	Learning Rate: 0.000359662
	LOSS [training: 0.031582536731570195 | validation: 0.030905494345762057]
	TIME [epoch: 8.1 sec]
EPOCH 737/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.044577518581779434		[learning rate: 0.00035879]
		[batch 20/20] avg loss: 0.038070791207787516		[learning rate: 0.00035792]
	Learning Rate: 0.000357923
	LOSS [training: 0.041324154894783464 | validation: 0.052036683077001815]
	TIME [epoch: 8.1 sec]
EPOCH 738/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.038242952605896094		[learning rate: 0.00035706]
		[batch 20/20] avg loss: 0.029551687774087017		[learning rate: 0.00035619]
	Learning Rate: 0.000356192
	LOSS [training: 0.03389732018999156 | validation: 0.037887026090722135]
	TIME [epoch: 8.1 sec]
EPOCH 739/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.032898319997328486		[learning rate: 0.00035533]
		[batch 20/20] avg loss: 0.03158521275598013		[learning rate: 0.00035447]
	Learning Rate: 0.00035447
	LOSS [training: 0.03224176637665431 | validation: 0.03398245900751776]
	TIME [epoch: 8.12 sec]
EPOCH 740/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03614837656272575		[learning rate: 0.00035361]
		[batch 20/20] avg loss: 0.022254147584344297		[learning rate: 0.00035276]
	Learning Rate: 0.000352755
	LOSS [training: 0.029201262073535024 | validation: 0.04048228113462023]
	TIME [epoch: 8.1 sec]
EPOCH 741/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02971817387817923		[learning rate: 0.0003519]
		[batch 20/20] avg loss: 0.04947831717409214		[learning rate: 0.00035105]
	Learning Rate: 0.00035105
	LOSS [training: 0.03959824552613569 | validation: 0.023565509933487896]
	TIME [epoch: 8.1 sec]
EPOCH 742/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03306532573671306		[learning rate: 0.0003502]
		[batch 20/20] avg loss: 0.029355462703666214		[learning rate: 0.00034935]
	Learning Rate: 0.000349352
	LOSS [training: 0.031210394220189636 | validation: 0.030822201633240304]
	TIME [epoch: 8.1 sec]
EPOCH 743/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.027186761809701043		[learning rate: 0.00034851]
		[batch 20/20] avg loss: 0.02804858960249098		[learning rate: 0.00034766]
	Learning Rate: 0.000347663
	LOSS [training: 0.027617675706096013 | validation: 0.03551075358073103]
	TIME [epoch: 8.12 sec]
EPOCH 744/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03433161774658593		[learning rate: 0.00034682]
		[batch 20/20] avg loss: 0.04095303439782417		[learning rate: 0.00034598]
	Learning Rate: 0.000345981
	LOSS [training: 0.037642326072205044 | validation: 0.047884629750393856]
	TIME [epoch: 8.1 sec]
EPOCH 745/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03125657159371359		[learning rate: 0.00034514]
		[batch 20/20] avg loss: 0.034474171805040615		[learning rate: 0.00034431]
	Learning Rate: 0.000344308
	LOSS [training: 0.0328653716993771 | validation: 0.03432898967904406]
	TIME [epoch: 8.16 sec]
EPOCH 746/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03351112230812464		[learning rate: 0.00034347]
		[batch 20/20] avg loss: 0.025730811796518083		[learning rate: 0.00034264]
	Learning Rate: 0.000342643
	LOSS [training: 0.02962096705232136 | validation: 0.03733404776930958]
	TIME [epoch: 8.11 sec]
EPOCH 747/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04581077900525227		[learning rate: 0.00034181]
		[batch 20/20] avg loss: 0.04480467587335533		[learning rate: 0.00034099]
	Learning Rate: 0.000340986
	LOSS [training: 0.045307727439303794 | validation: 0.019337755464628018]
	TIME [epoch: 8.13 sec]
EPOCH 748/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02464987267656215		[learning rate: 0.00034016]
		[batch 20/20] avg loss: 0.035581703054963644		[learning rate: 0.00033934]
	Learning Rate: 0.000339337
	LOSS [training: 0.030115787865762893 | validation: 0.021463671266638524]
	TIME [epoch: 8.11 sec]
EPOCH 749/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.029176220644183164		[learning rate: 0.00033852]
		[batch 20/20] avg loss: 0.029627705253887793		[learning rate: 0.0003377]
	Learning Rate: 0.000337696
	LOSS [training: 0.029401962949035482 | validation: 0.04253019019327797]
	TIME [epoch: 8.11 sec]
EPOCH 750/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.041503617153747396		[learning rate: 0.00033688]
		[batch 20/20] avg loss: 0.025269907276553916		[learning rate: 0.00033606]
	Learning Rate: 0.000336063
	LOSS [training: 0.033386762215150655 | validation: 0.034910862676583033]
	TIME [epoch: 8.1 sec]
EPOCH 751/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.045687060497066466		[learning rate: 0.00033525]
		[batch 20/20] avg loss: 0.031543561636726024		[learning rate: 0.00033444]
	Learning Rate: 0.000334438
	LOSS [training: 0.03861531106689624 | validation: 0.04168155486187386]
	TIME [epoch: 8.13 sec]
EPOCH 752/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02609192997734469		[learning rate: 0.00033363]
		[batch 20/20] avg loss: 0.030924999982650254		[learning rate: 0.00033282]
	Learning Rate: 0.000332821
	LOSS [training: 0.028508464979997473 | validation: 0.020695209929693727]
	TIME [epoch: 8.11 sec]
EPOCH 753/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02782388021771366		[learning rate: 0.00033202]
		[batch 20/20] avg loss: 0.03294881681665775		[learning rate: 0.00033121]
	Learning Rate: 0.000331211
	LOSS [training: 0.0303863485171857 | validation: 0.021138451308574654]
	TIME [epoch: 8.11 sec]
EPOCH 754/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.033010879244880095		[learning rate: 0.00033041]
		[batch 20/20] avg loss: 0.0476144673237546		[learning rate: 0.00032961]
	Learning Rate: 0.00032961
	LOSS [training: 0.04031267328431735 | validation: 0.03140659067823446]
	TIME [epoch: 8.1 sec]
EPOCH 755/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.031596448256646835		[learning rate: 0.00032881]
		[batch 20/20] avg loss: 0.034371820528829436		[learning rate: 0.00032802]
	Learning Rate: 0.000328016
	LOSS [training: 0.03298413439273813 | validation: 0.02678325424996648]
	TIME [epoch: 8.14 sec]
EPOCH 756/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03613740180341264		[learning rate: 0.00032722]
		[batch 20/20] avg loss: 0.034983686189914334		[learning rate: 0.00032643]
	Learning Rate: 0.00032643
	LOSS [training: 0.03556054399666349 | validation: 0.04929598122359541]
	TIME [epoch: 8.11 sec]
EPOCH 757/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030758543655518243		[learning rate: 0.00032564]
		[batch 20/20] avg loss: 0.03617023107090848		[learning rate: 0.00032485]
	Learning Rate: 0.000324851
	LOSS [training: 0.03346438736321336 | validation: 0.0345229139188714]
	TIME [epoch: 8.11 sec]
EPOCH 758/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028750215420865383		[learning rate: 0.00032406]
		[batch 20/20] avg loss: 0.05305768663590188		[learning rate: 0.00032328]
	Learning Rate: 0.00032328
	LOSS [training: 0.04090395102838364 | validation: 0.03131081071688736]
	TIME [epoch: 8.11 sec]
EPOCH 759/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03883084825187404		[learning rate: 0.0003225]
		[batch 20/20] avg loss: 0.03964671779737469		[learning rate: 0.00032172]
	Learning Rate: 0.000321717
	LOSS [training: 0.039238783024624364 | validation: 0.02539632835198688]
	TIME [epoch: 8.13 sec]
EPOCH 760/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.029080668213018374		[learning rate: 0.00032094]
		[batch 20/20] avg loss: 0.039000044789384375		[learning rate: 0.00032016]
	Learning Rate: 0.000320161
	LOSS [training: 0.03404035650120137 | validation: 0.05520130770968948]
	TIME [epoch: 8.11 sec]
EPOCH 761/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.043641150776179194		[learning rate: 0.00031939]
		[batch 20/20] avg loss: 0.03511352781934224		[learning rate: 0.00031861]
	Learning Rate: 0.000318613
	LOSS [training: 0.039377339297760716 | validation: 0.026291400986899676]
	TIME [epoch: 8.11 sec]
EPOCH 762/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04322428193415687		[learning rate: 0.00031784]
		[batch 20/20] avg loss: 0.050761288615147074		[learning rate: 0.00031707]
	Learning Rate: 0.000317072
	LOSS [training: 0.04699278527465197 | validation: 0.038472623899175315]
	TIME [epoch: 8.11 sec]
EPOCH 763/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.029613885704787946		[learning rate: 0.0003163]
		[batch 20/20] avg loss: 0.026966808554513368		[learning rate: 0.00031554]
	Learning Rate: 0.000315539
	LOSS [training: 0.028290347129650652 | validation: 0.04228889710820068]
	TIME [epoch: 8.12 sec]
EPOCH 764/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03410052306810511		[learning rate: 0.00031477]
		[batch 20/20] avg loss: 0.027876515563879645		[learning rate: 0.00031401]
	Learning Rate: 0.000314013
	LOSS [training: 0.03098851931599238 | validation: 0.03191438483318202]
	TIME [epoch: 8.12 sec]
EPOCH 765/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0368443701426783		[learning rate: 0.00031325]
		[batch 20/20] avg loss: 0.037750472482395714		[learning rate: 0.00031249]
	Learning Rate: 0.000312494
	LOSS [training: 0.03729742131253701 | validation: 0.03926515133324343]
	TIME [epoch: 8.11 sec]
EPOCH 766/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03704820038214365		[learning rate: 0.00031174]
		[batch 20/20] avg loss: 0.03430777194746276		[learning rate: 0.00031098]
	Learning Rate: 0.000310983
	LOSS [training: 0.03567798616480321 | validation: 0.021927760744468165]
	TIME [epoch: 8.11 sec]
EPOCH 767/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03195553123356499		[learning rate: 0.00031023]
		[batch 20/20] avg loss: 0.026425533913726106		[learning rate: 0.00030948]
	Learning Rate: 0.000309479
	LOSS [training: 0.029190532573645546 | validation: 0.02280228332761656]
	TIME [epoch: 8.12 sec]
EPOCH 768/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03154984177633041		[learning rate: 0.00030873]
		[batch 20/20] avg loss: 0.03012426415878814		[learning rate: 0.00030798]
	Learning Rate: 0.000307983
	LOSS [training: 0.03083705296755927 | validation: 0.027249411788369877]
	TIME [epoch: 8.12 sec]
EPOCH 769/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04989649889593821		[learning rate: 0.00030724]
		[batch 20/20] avg loss: 0.03326193953261897		[learning rate: 0.00030649]
	Learning Rate: 0.000306493
	LOSS [training: 0.04157921921427859 | validation: 0.040461518210691855]
	TIME [epoch: 8.11 sec]
EPOCH 770/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.037342362751763544		[learning rate: 0.00030575]
		[batch 20/20] avg loss: 0.021508305309031774		[learning rate: 0.00030501]
	Learning Rate: 0.000305011
	LOSS [training: 0.029425334030397654 | validation: 0.036329742590828856]
	TIME [epoch: 8.11 sec]
EPOCH 771/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.035854981110727234		[learning rate: 0.00030427]
		[batch 20/20] avg loss: 0.027772646382993406		[learning rate: 0.00030354]
	Learning Rate: 0.000303536
	LOSS [training: 0.03181381374686031 | validation: 0.03793756044770025]
	TIME [epoch: 8.12 sec]
EPOCH 772/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028093909453959183		[learning rate: 0.0003028]
		[batch 20/20] avg loss: 0.03777988714534295		[learning rate: 0.00030207]
	Learning Rate: 0.000302068
	LOSS [training: 0.03293689829965106 | validation: 0.03874510211304615]
	TIME [epoch: 8.12 sec]
EPOCH 773/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.037100399042712125		[learning rate: 0.00030134]
		[batch 20/20] avg loss: 0.03792414797063854		[learning rate: 0.00030061]
	Learning Rate: 0.000300608
	LOSS [training: 0.03751227350667533 | validation: 0.04023135162031751]
	TIME [epoch: 8.11 sec]
EPOCH 774/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02977081217294119		[learning rate: 0.00029988]
		[batch 20/20] avg loss: 0.042122135338466524		[learning rate: 0.00029915]
	Learning Rate: 0.000299154
	LOSS [training: 0.03594647375570385 | validation: 0.013141298141514057]
	TIME [epoch: 8.11 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240217_161409/states/model_tr_study2_774.pth
	Model improved!!!
EPOCH 775/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023603282161817028		[learning rate: 0.00029843]
		[batch 20/20] avg loss: 0.03183324620286852		[learning rate: 0.00029771]
	Learning Rate: 0.000297707
	LOSS [training: 0.02771826418234278 | validation: 0.024635927202166532]
	TIME [epoch: 8.12 sec]
EPOCH 776/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02996262243818528		[learning rate: 0.00029699]
		[batch 20/20] avg loss: 0.03600338770674621		[learning rate: 0.00029627]
	Learning Rate: 0.000296268
	LOSS [training: 0.03298300507246575 | validation: 0.03741106500617584]
	TIME [epoch: 8.1 sec]
EPOCH 777/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03291733220986719		[learning rate: 0.00029555]
		[batch 20/20] avg loss: 0.030016599814527346		[learning rate: 0.00029483]
	Learning Rate: 0.000294835
	LOSS [training: 0.03146696601219726 | validation: 0.037021301046429304]
	TIME [epoch: 8.1 sec]
EPOCH 778/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.018359873456123086		[learning rate: 0.00029412]
		[batch 20/20] avg loss: 0.02546566461775246		[learning rate: 0.00029341]
	Learning Rate: 0.000293409
	LOSS [training: 0.021912769036937767 | validation: 0.030336950587730557]
	TIME [epoch: 8.09 sec]
EPOCH 779/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026118565305164976		[learning rate: 0.0002927]
		[batch 20/20] avg loss: 0.025258847782762005		[learning rate: 0.00029199]
	Learning Rate: 0.00029199
	LOSS [training: 0.02568870654396349 | validation: 0.02501274671664075]
	TIME [epoch: 8.11 sec]
EPOCH 780/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02443242455940861		[learning rate: 0.00029128]
		[batch 20/20] avg loss: 0.04135544039986023		[learning rate: 0.00029058]
	Learning Rate: 0.000290578
	LOSS [training: 0.03289393247963442 | validation: 0.023514043764120227]
	TIME [epoch: 8.12 sec]
EPOCH 781/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023085202262843142		[learning rate: 0.00028987]
		[batch 20/20] avg loss: 0.02836769623822074		[learning rate: 0.00028917]
	Learning Rate: 0.000289173
	LOSS [training: 0.02572644925053194 | validation: 0.04666530946315753]
	TIME [epoch: 8.1 sec]
EPOCH 782/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02943409378383034		[learning rate: 0.00028847]
		[batch 20/20] avg loss: 0.032033350657314566		[learning rate: 0.00028777]
	Learning Rate: 0.000287775
	LOSS [training: 0.03073372222057245 | validation: 0.027346710239566106]
	TIME [epoch: 8.1 sec]
EPOCH 783/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.035654931565718015		[learning rate: 0.00028708]
		[batch 20/20] avg loss: 0.024658368163476486		[learning rate: 0.00028638]
	Learning Rate: 0.000286383
	LOSS [training: 0.030156649864597245 | validation: 0.018807173770538532]
	TIME [epoch: 8.1 sec]
EPOCH 784/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02710617194151043		[learning rate: 0.00028569]
		[batch 20/20] avg loss: 0.05010091922806888		[learning rate: 0.000285]
	Learning Rate: 0.000284998
	LOSS [training: 0.03860354558478965 | validation: 0.025346225555503137]
	TIME [epoch: 8.12 sec]
EPOCH 785/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02283719487424895		[learning rate: 0.00028431]
		[batch 20/20] avg loss: 0.02557282932267662		[learning rate: 0.00028362]
	Learning Rate: 0.00028362
	LOSS [training: 0.024205012098462785 | validation: 0.028397476307844702]
	TIME [epoch: 8.09 sec]
EPOCH 786/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02844568929062527		[learning rate: 0.00028293]
		[batch 20/20] avg loss: 0.03513844375325507		[learning rate: 0.00028225]
	Learning Rate: 0.000282248
	LOSS [training: 0.03179206652194017 | validation: 0.03311859581866979]
	TIME [epoch: 8.13 sec]
EPOCH 787/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03559121366977555		[learning rate: 0.00028157]
		[batch 20/20] avg loss: 0.03682089968820764		[learning rate: 0.00028088]
	Learning Rate: 0.000280884
	LOSS [training: 0.036206056678991586 | validation: 0.023875491483815837]
	TIME [epoch: 8.1 sec]
EPOCH 788/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0335197759536001		[learning rate: 0.0002802]
		[batch 20/20] avg loss: 0.06010571689136268		[learning rate: 0.00027953]
	Learning Rate: 0.000279525
	LOSS [training: 0.04681274642248139 | validation: 0.034810059294778546]
	TIME [epoch: 8.13 sec]
EPOCH 789/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030852851652975632		[learning rate: 0.00027885]
		[batch 20/20] avg loss: 0.048793051754542474		[learning rate: 0.00027817]
	Learning Rate: 0.000278173
	LOSS [training: 0.039822951703759046 | validation: 0.04674661480843381]
	TIME [epoch: 8.1 sec]
EPOCH 790/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03948150023200583		[learning rate: 0.0002775]
		[batch 20/20] avg loss: 0.035887176617339624		[learning rate: 0.00027683]
	Learning Rate: 0.000276828
	LOSS [training: 0.037684338424672725 | validation: 0.03595204809774001]
	TIME [epoch: 8.11 sec]
EPOCH 791/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.031970325695878925		[learning rate: 0.00027616]
		[batch 20/20] avg loss: 0.03993030355306826		[learning rate: 0.00027549]
	Learning Rate: 0.00027549
	LOSS [training: 0.03595031462447359 | validation: 0.03161438780050909]
	TIME [epoch: 8.1 sec]
EPOCH 792/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03206084669423834		[learning rate: 0.00027482]
		[batch 20/20] avg loss: 0.02805301104291391		[learning rate: 0.00027416]
	Learning Rate: 0.000274157
	LOSS [training: 0.030056928868576128 | validation: 0.04570647049778866]
	TIME [epoch: 8.13 sec]
EPOCH 793/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03532284502490868		[learning rate: 0.00027349]
		[batch 20/20] avg loss: 0.033505412930430876		[learning rate: 0.00027283]
	Learning Rate: 0.000272832
	LOSS [training: 0.03441412897766978 | validation: 0.01968741056631524]
	TIME [epoch: 8.1 sec]
EPOCH 794/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.044400906009752564		[learning rate: 0.00027217]
		[batch 20/20] avg loss: 0.035983400845016424		[learning rate: 0.00027151]
	Learning Rate: 0.000271512
	LOSS [training: 0.04019215342738448 | validation: 0.026959106389887157]
	TIME [epoch: 8.1 sec]
EPOCH 795/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030591300666662796		[learning rate: 0.00027086]
		[batch 20/20] avg loss: 0.0338232055673913		[learning rate: 0.0002702]
	Learning Rate: 0.000270199
	LOSS [training: 0.032207253117027046 | validation: 0.01930490614641211]
	TIME [epoch: 8.11 sec]
EPOCH 796/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.029150797128700993		[learning rate: 0.00026955]
		[batch 20/20] avg loss: 0.03326603056547211		[learning rate: 0.00026889]
	Learning Rate: 0.000268893
	LOSS [training: 0.03120841384708655 | validation: 0.0458811338250724]
	TIME [epoch: 8.13 sec]
EPOCH 797/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03675294310296495		[learning rate: 0.00026824]
		[batch 20/20] avg loss: 0.03340897435176374		[learning rate: 0.00026759]
	Learning Rate: 0.000267592
	LOSS [training: 0.035080958727364345 | validation: 0.024567323535144632]
	TIME [epoch: 8.11 sec]
EPOCH 798/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0287931554730194		[learning rate: 0.00026694]
		[batch 20/20] avg loss: 0.02174554763053741		[learning rate: 0.0002663]
	Learning Rate: 0.000266298
	LOSS [training: 0.025269351551778407 | validation: 0.020247934539841514]
	TIME [epoch: 8.1 sec]
EPOCH 799/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.029728355309197273		[learning rate: 0.00026565]
		[batch 20/20] avg loss: 0.03470590768977526		[learning rate: 0.00026501]
	Learning Rate: 0.000265011
	LOSS [training: 0.03221713149948626 | validation: 0.04009960644096465]
	TIME [epoch: 8.1 sec]
EPOCH 800/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025324235483173495		[learning rate: 0.00026437]
		[batch 20/20] avg loss: 0.025102582807736522		[learning rate: 0.00026373]
	Learning Rate: 0.000263729
	LOSS [training: 0.025213409145455012 | validation: 0.031391381094334876]
	TIME [epoch: 8.12 sec]
EPOCH 801/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04467141899336152		[learning rate: 0.00026309]
		[batch 20/20] avg loss: 0.020511079750716317		[learning rate: 0.00026245]
	Learning Rate: 0.000262454
	LOSS [training: 0.03259124937203893 | validation: 0.018836832617765403]
	TIME [epoch: 8.11 sec]
EPOCH 802/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024231308667540148		[learning rate: 0.00026182]
		[batch 20/20] avg loss: 0.023748521349248847		[learning rate: 0.00026118]
	Learning Rate: 0.000261184
	LOSS [training: 0.023989915008394494 | validation: 0.0295000778867737]
	TIME [epoch: 8.1 sec]
EPOCH 803/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025621282527246676		[learning rate: 0.00026055]
		[batch 20/20] avg loss: 0.028348597627417278		[learning rate: 0.00025992]
	Learning Rate: 0.000259921
	LOSS [training: 0.02698494007733197 | validation: 0.030983088965068677]
	TIME [epoch: 8.11 sec]
EPOCH 804/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023123731821522452		[learning rate: 0.00025929]
		[batch 20/20] avg loss: 0.026370497538171313		[learning rate: 0.00025866]
	Learning Rate: 0.000258665
	LOSS [training: 0.024747114679846888 | validation: 0.025840295385508374]
	TIME [epoch: 8.12 sec]
EPOCH 805/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030973629758693923		[learning rate: 0.00025804]
		[batch 20/20] avg loss: 0.020164619121688106		[learning rate: 0.00025741]
	Learning Rate: 0.000257414
	LOSS [training: 0.025569124440191014 | validation: 0.029814686778959952]
	TIME [epoch: 8.1 sec]
EPOCH 806/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.040289981426685675		[learning rate: 0.00025679]
		[batch 20/20] avg loss: 0.021202657953147623		[learning rate: 0.00025617]
	Learning Rate: 0.000256169
	LOSS [training: 0.030746319689916653 | validation: 0.023725773884820528]
	TIME [epoch: 8.1 sec]
EPOCH 807/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024525215621911123		[learning rate: 0.00025555]
		[batch 20/20] avg loss: 0.030809812171135997		[learning rate: 0.00025493]
	Learning Rate: 0.00025493
	LOSS [training: 0.02766751389652356 | validation: 0.025366306139527443]
	TIME [epoch: 8.1 sec]
EPOCH 808/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.031710586612475625		[learning rate: 0.00025431]
		[batch 20/20] avg loss: 0.02333649462178481		[learning rate: 0.0002537]
	Learning Rate: 0.000253697
	LOSS [training: 0.027523540617130214 | validation: 0.023368925026960332]
	TIME [epoch: 8.12 sec]
EPOCH 809/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028448330309890312		[learning rate: 0.00025308]
		[batch 20/20] avg loss: 0.026252859119187198		[learning rate: 0.00025247]
	Learning Rate: 0.00025247
	LOSS [training: 0.027350594714538757 | validation: 0.033067293885441065]
	TIME [epoch: 8.1 sec]
EPOCH 810/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03901415664286402		[learning rate: 0.00025186]
		[batch 20/20] avg loss: 0.028412454512824592		[learning rate: 0.00025125]
	Learning Rate: 0.00025125
	LOSS [training: 0.033713305577844306 | validation: 0.02186992100534927]
	TIME [epoch: 8.1 sec]
EPOCH 811/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02336256418481858		[learning rate: 0.00025064]
		[batch 20/20] avg loss: 0.027428652908062658		[learning rate: 0.00025003]
	Learning Rate: 0.000250035
	LOSS [training: 0.02539560854644062 | validation: 0.03769696650984772]
	TIME [epoch: 8.1 sec]
EPOCH 812/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.029276829048315828		[learning rate: 0.00024943]
		[batch 20/20] avg loss: 0.03430147723175		[learning rate: 0.00024883]
	Learning Rate: 0.000248825
	LOSS [training: 0.03178915314003291 | validation: 0.04794276195996523]
	TIME [epoch: 8.12 sec]
EPOCH 813/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.032778590134637284		[learning rate: 0.00024822]
		[batch 20/20] avg loss: 0.022654011067141717		[learning rate: 0.00024762]
	Learning Rate: 0.000247622
	LOSS [training: 0.027716300600889497 | validation: 0.03894280456605083]
	TIME [epoch: 8.11 sec]
EPOCH 814/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02884977492673268		[learning rate: 0.00024702]
		[batch 20/20] avg loss: 0.031062245634868924		[learning rate: 0.00024642]
	Learning Rate: 0.000246425
	LOSS [training: 0.029956010280800787 | validation: 0.02792514252294429]
	TIME [epoch: 8.1 sec]
EPOCH 815/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02653719986416233		[learning rate: 0.00024583]
		[batch 20/20] avg loss: 0.02946302778575008		[learning rate: 0.00024523]
	Learning Rate: 0.000245233
	LOSS [training: 0.0280001138249562 | validation: 0.03161688524792028]
	TIME [epoch: 8.1 sec]
EPOCH 816/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.029396001169440873		[learning rate: 0.00024464]
		[batch 20/20] avg loss: 0.02697756583211567		[learning rate: 0.00024405]
	Learning Rate: 0.000244047
	LOSS [training: 0.028186783500778273 | validation: 0.02634719835541731]
	TIME [epoch: 8.12 sec]
EPOCH 817/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024654160290279465		[learning rate: 0.00024346]
		[batch 20/20] avg loss: 0.04896961919389795		[learning rate: 0.00024287]
	Learning Rate: 0.000242867
	LOSS [training: 0.036811889742088696 | validation: 0.0360819526883617]
	TIME [epoch: 8.11 sec]
EPOCH 818/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03294984648028419		[learning rate: 0.00024228]
		[batch 20/20] avg loss: 0.03090635228140376		[learning rate: 0.00024169]
	Learning Rate: 0.000241693
	LOSS [training: 0.031928099380843976 | validation: 0.018129910732334133]
	TIME [epoch: 8.1 sec]
EPOCH 819/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028424794220508864		[learning rate: 0.00024111]
		[batch 20/20] avg loss: 0.03311723794099854		[learning rate: 0.00024052]
	Learning Rate: 0.000240524
	LOSS [training: 0.030771016080753704 | validation: 0.027006876338408266]
	TIME [epoch: 8.1 sec]
EPOCH 820/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030573583928996624		[learning rate: 0.00023994]
		[batch 20/20] avg loss: 0.03333778749220129		[learning rate: 0.00023936]
	Learning Rate: 0.000239361
	LOSS [training: 0.031955685710598945 | validation: 0.02432690050143848]
	TIME [epoch: 8.12 sec]
EPOCH 821/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03823729088683261		[learning rate: 0.00023878]
		[batch 20/20] avg loss: 0.020364814509548808		[learning rate: 0.0002382]
	Learning Rate: 0.000238203
	LOSS [training: 0.02930105269819071 | validation: 0.029125278067808045]
	TIME [epoch: 8.11 sec]
EPOCH 822/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0279086755771648		[learning rate: 0.00023763]
		[batch 20/20] avg loss: 0.03540878143649464		[learning rate: 0.00023705]
	Learning Rate: 0.000237051
	LOSS [training: 0.03165872850682973 | validation: 0.03245448068200268]
	TIME [epoch: 8.09 sec]
EPOCH 823/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04294581412411646		[learning rate: 0.00023648]
		[batch 20/20] avg loss: 0.026524345182959902		[learning rate: 0.0002359]
	Learning Rate: 0.000235905
	LOSS [training: 0.034735079653538184 | validation: 0.02753832838704848]
	TIME [epoch: 8.1 sec]
EPOCH 824/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.033529900862010034		[learning rate: 0.00023533]
		[batch 20/20] avg loss: 0.03261450519087168		[learning rate: 0.00023476]
	Learning Rate: 0.000234764
	LOSS [training: 0.03307220302644086 | validation: 0.023645594383986387]
	TIME [epoch: 8.1 sec]
EPOCH 825/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026900885705545625		[learning rate: 0.0002342]
		[batch 20/20] avg loss: 0.030562628333022018		[learning rate: 0.00023363]
	Learning Rate: 0.000233629
	LOSS [training: 0.028731757019283814 | validation: 0.022284822778674822]
	TIME [epoch: 8.12 sec]
EPOCH 826/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03287966036595862		[learning rate: 0.00023306]
		[batch 20/20] avg loss: 0.024138715028500173		[learning rate: 0.0002325]
	Learning Rate: 0.000232499
	LOSS [training: 0.028509187697229398 | validation: 0.023504381743473003]
	TIME [epoch: 8.1 sec]
EPOCH 827/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02324391153420158		[learning rate: 0.00023194]
		[batch 20/20] avg loss: 0.029939212473899045		[learning rate: 0.00023137]
	Learning Rate: 0.000231375
	LOSS [training: 0.02659156200405031 | validation: 0.022737622526349595]
	TIME [epoch: 8.1 sec]
EPOCH 828/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.021972090041253924		[learning rate: 0.00023081]
		[batch 20/20] avg loss: 0.04045513656666222		[learning rate: 0.00023026]
	Learning Rate: 0.000230256
	LOSS [training: 0.03121361330395807 | validation: 0.03691331280544755]
	TIME [epoch: 8.1 sec]
EPOCH 829/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026718131646655586		[learning rate: 0.0002297]
		[batch 20/20] avg loss: 0.03718038701982958		[learning rate: 0.00022914]
	Learning Rate: 0.000229142
	LOSS [training: 0.03194925933324258 | validation: 0.022085922070419903]
	TIME [epoch: 8.13 sec]
EPOCH 830/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030896983681604883		[learning rate: 0.00022859]
		[batch 20/20] avg loss: 0.0209927371322841		[learning rate: 0.00022803]
	Learning Rate: 0.000228034
	LOSS [training: 0.025944860406944492 | validation: 0.02963750164314227]
	TIME [epoch: 8.1 sec]
EPOCH 831/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02609686654351286		[learning rate: 0.00022748]
		[batch 20/20] avg loss: 0.03320997591163881		[learning rate: 0.00022693]
	Learning Rate: 0.000226931
	LOSS [training: 0.02965342122757584 | validation: 0.029811449900494967]
	TIME [epoch: 8.1 sec]
EPOCH 832/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.031032472500660258		[learning rate: 0.00022638]
		[batch 20/20] avg loss: 0.020783488107063174		[learning rate: 0.00022583]
	Learning Rate: 0.000225834
	LOSS [training: 0.025907980303861718 | validation: 0.017856705037048615]
	TIME [epoch: 8.11 sec]
EPOCH 833/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.020330786630784296		[learning rate: 0.00022529]
		[batch 20/20] avg loss: 0.025057462300646243		[learning rate: 0.00022474]
	Learning Rate: 0.000224742
	LOSS [training: 0.02269412446571527 | validation: 0.05126041733966033]
	TIME [epoch: 8.13 sec]
EPOCH 834/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03668522289991		[learning rate: 0.0002242]
		[batch 20/20] avg loss: 0.029702774905880275		[learning rate: 0.00022366]
	Learning Rate: 0.000223655
	LOSS [training: 0.033193998902895136 | validation: 0.023577191761192107]
	TIME [epoch: 8.1 sec]
EPOCH 835/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03694404849290618		[learning rate: 0.00022311]
		[batch 20/20] avg loss: 0.032498636853086335		[learning rate: 0.00022257]
	Learning Rate: 0.000222574
	LOSS [training: 0.03472134267299626 | validation: 0.013963285854770293]
	TIME [epoch: 8.11 sec]
EPOCH 836/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022439692855325953		[learning rate: 0.00022203]
		[batch 20/20] avg loss: 0.03226929410266443		[learning rate: 0.0002215]
	Learning Rate: 0.000221497
	LOSS [training: 0.0273544934789952 | validation: 0.020571181127565678]
	TIME [epoch: 8.1 sec]
EPOCH 837/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024886322279385458		[learning rate: 0.00022096]
		[batch 20/20] avg loss: 0.02689484261927555		[learning rate: 0.00022043]
	Learning Rate: 0.000220426
	LOSS [training: 0.025890582449330502 | validation: 0.03386647667693814]
	TIME [epoch: 8.12 sec]
EPOCH 838/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03253915314914141		[learning rate: 0.00021989]
		[batch 20/20] avg loss: 0.02600489421681469		[learning rate: 0.00021936]
	Learning Rate: 0.00021936
	LOSS [training: 0.02927202368297805 | validation: 0.02508702194097046]
	TIME [epoch: 8.1 sec]
EPOCH 839/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.029784086341485598		[learning rate: 0.00021883]
		[batch 20/20] avg loss: 0.028313145774013414		[learning rate: 0.0002183]
	Learning Rate: 0.000218299
	LOSS [training: 0.029048616057749506 | validation: 0.03044838805480516]
	TIME [epoch: 8.1 sec]
EPOCH 840/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025695440761796084		[learning rate: 0.00021777]
		[batch 20/20] avg loss: 0.03285267558802748		[learning rate: 0.00021724]
	Learning Rate: 0.000217244
	LOSS [training: 0.02927405817491179 | validation: 0.03785409805022969]
	TIME [epoch: 8.1 sec]
EPOCH 841/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02724749298808703		[learning rate: 0.00021672]
		[batch 20/20] avg loss: 0.026268125432387924		[learning rate: 0.00021619]
	Learning Rate: 0.000216193
	LOSS [training: 0.026757809210237477 | validation: 0.025696116159199513]
	TIME [epoch: 8.12 sec]
EPOCH 842/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0263083846069761		[learning rate: 0.00021567]
		[batch 20/20] avg loss: 0.03247597977676326		[learning rate: 0.00021515]
	Learning Rate: 0.000215148
	LOSS [training: 0.029392182191869676 | validation: 0.04230816911790179]
	TIME [epoch: 8.11 sec]
EPOCH 843/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028134567820883798		[learning rate: 0.00021463]
		[batch 20/20] avg loss: 0.02158038422360633		[learning rate: 0.00021411]
	Learning Rate: 0.000214107
	LOSS [training: 0.024857476022245063 | validation: 0.0312591137441443]
	TIME [epoch: 8.1 sec]
EPOCH 844/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03736115778528583		[learning rate: 0.00021359]
		[batch 20/20] avg loss: 0.024631637524884217		[learning rate: 0.00021307]
	Learning Rate: 0.000213072
	LOSS [training: 0.030996397655085023 | validation: 0.025358859936169542]
	TIME [epoch: 8.1 sec]
EPOCH 845/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030317313689443397		[learning rate: 0.00021256]
		[batch 20/20] avg loss: 0.024750542154594456		[learning rate: 0.00021204]
	Learning Rate: 0.000212042
	LOSS [training: 0.02753392792201893 | validation: 0.03586741897676675]
	TIME [epoch: 8.12 sec]
EPOCH 846/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030153633690698955		[learning rate: 0.00021153]
		[batch 20/20] avg loss: 0.02645638525473002		[learning rate: 0.00021102]
	Learning Rate: 0.000211016
	LOSS [training: 0.028305009472714488 | validation: 0.024050770750596667]
	TIME [epoch: 8.1 sec]
EPOCH 847/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03446190602100631		[learning rate: 0.00021051]
		[batch 20/20] avg loss: 0.02562912738841005		[learning rate: 0.00021]
	Learning Rate: 0.000209996
	LOSS [training: 0.030045516704708175 | validation: 0.025411765979560408]
	TIME [epoch: 8.11 sec]
EPOCH 848/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02666193084111206		[learning rate: 0.00020949]
		[batch 20/20] avg loss: 0.04135453212443852		[learning rate: 0.00020898]
	Learning Rate: 0.00020898
	LOSS [training: 0.03400823148277528 | validation: 0.019892092436423037]
	TIME [epoch: 8.09 sec]
EPOCH 849/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0293518013462524		[learning rate: 0.00020847]
		[batch 20/20] avg loss: 0.019577574466717682		[learning rate: 0.00020797]
	Learning Rate: 0.00020797
	LOSS [training: 0.02446468790648504 | validation: 0.02432574399777181]
	TIME [epoch: 8.12 sec]
EPOCH 850/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028292464911757114		[learning rate: 0.00020747]
		[batch 20/20] avg loss: 0.03201383715055313		[learning rate: 0.00020696]
	Learning Rate: 0.000206964
	LOSS [training: 0.030153151031155123 | validation: 0.03962465261943712]
	TIME [epoch: 8.1 sec]
EPOCH 851/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028847545010299725		[learning rate: 0.00020646]
		[batch 20/20] avg loss: 0.032183452850188574		[learning rate: 0.00020596]
	Learning Rate: 0.000205963
	LOSS [training: 0.03051549893024414 | validation: 0.023188397960371993]
	TIME [epoch: 8.11 sec]
EPOCH 852/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03174009969481299		[learning rate: 0.00020546]
		[batch 20/20] avg loss: 0.024251224707163045		[learning rate: 0.00020497]
	Learning Rate: 0.000204967
	LOSS [training: 0.02799566220098802 | validation: 0.026996728424190798]
	TIME [epoch: 8.09 sec]
EPOCH 853/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.019719827599396892		[learning rate: 0.00020447]
		[batch 20/20] avg loss: 0.02922414098106422		[learning rate: 0.00020398]
	Learning Rate: 0.000203976
	LOSS [training: 0.024471984290230554 | validation: 0.016303732447078632]
	TIME [epoch: 8.12 sec]
EPOCH 854/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023780383488153		[learning rate: 0.00020348]
		[batch 20/20] avg loss: 0.028054106650324023		[learning rate: 0.00020299]
	Learning Rate: 0.00020299
	LOSS [training: 0.02591724506923852 | validation: 0.022957897389160414]
	TIME [epoch: 8.1 sec]
EPOCH 855/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02197859337331806		[learning rate: 0.0002025]
		[batch 20/20] avg loss: 0.023412155352664187		[learning rate: 0.00020201]
	Learning Rate: 0.000202008
	LOSS [training: 0.022695374362991124 | validation: 0.03226485902721858]
	TIME [epoch: 8.1 sec]
EPOCH 856/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030652566612388493		[learning rate: 0.00020152]
		[batch 20/20] avg loss: 0.04171313473782514		[learning rate: 0.00020103]
	Learning Rate: 0.000201031
	LOSS [training: 0.036182850675106806 | validation: 0.02617165988143155]
	TIME [epoch: 8.1 sec]
EPOCH 857/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0323557900612813		[learning rate: 0.00020054]
		[batch 20/20] avg loss: 0.031049800138565592		[learning rate: 0.00020006]
	Learning Rate: 0.000200059
	LOSS [training: 0.03170279509992345 | validation: 0.034799435215392714]
	TIME [epoch: 8.12 sec]
EPOCH 858/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03255928960740396		[learning rate: 0.00019957]
		[batch 20/20] avg loss: 0.02562478703941366		[learning rate: 0.00019909]
	Learning Rate: 0.000199091
	LOSS [training: 0.029092038323408807 | validation: 0.042717132572262916]
	TIME [epoch: 8.12 sec]
EPOCH 859/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02596461593164273		[learning rate: 0.00019861]
		[batch 20/20] avg loss: 0.028597213158172867		[learning rate: 0.00019813]
	Learning Rate: 0.000198129
	LOSS [training: 0.027280914544907796 | validation: 0.0349965544574914]
	TIME [epoch: 8.1 sec]
EPOCH 860/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028129125797573346		[learning rate: 0.00019765]
		[batch 20/20] avg loss: 0.032702888856304665		[learning rate: 0.00019717]
	Learning Rate: 0.000197171
	LOSS [training: 0.030416007326939005 | validation: 0.01693944445576068]
	TIME [epoch: 8.1 sec]
EPOCH 861/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02088881652352672		[learning rate: 0.00019669]
		[batch 20/20] avg loss: 0.026455620153627575		[learning rate: 0.00019622]
	Learning Rate: 0.000196217
	LOSS [training: 0.023672218338577147 | validation: 0.04592193800243184]
	TIME [epoch: 8.11 sec]
EPOCH 862/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02609239731780011		[learning rate: 0.00019574]
		[batch 20/20] avg loss: 0.02913354575837267		[learning rate: 0.00019527]
	Learning Rate: 0.000195268
	LOSS [training: 0.027612971538086388 | validation: 0.02943421090870719]
	TIME [epoch: 8.11 sec]
EPOCH 863/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02639823969950065		[learning rate: 0.0001948]
		[batch 20/20] avg loss: 0.0265514913247429		[learning rate: 0.00019432]
	Learning Rate: 0.000194324
	LOSS [training: 0.02647486551212177 | validation: 0.021526313773260808]
	TIME [epoch: 8.1 sec]
EPOCH 864/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028186037152303937		[learning rate: 0.00019385]
		[batch 20/20] avg loss: 0.0175471444072619		[learning rate: 0.00019338]
	Learning Rate: 0.000193384
	LOSS [training: 0.022866590779782918 | validation: 0.022576721970669313]
	TIME [epoch: 8.1 sec]
EPOCH 865/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030816394360199877		[learning rate: 0.00019292]
		[batch 20/20] avg loss: 0.02224435935018721		[learning rate: 0.00019245]
	Learning Rate: 0.000192449
	LOSS [training: 0.02653037685519355 | validation: 0.02735446339044046]
	TIME [epoch: 8.11 sec]
EPOCH 866/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03592836549692953		[learning rate: 0.00019198]
		[batch 20/20] avg loss: 0.022503846544220656		[learning rate: 0.00019152]
	Learning Rate: 0.000191518
	LOSS [training: 0.02921610602057509 | validation: 0.0216477172918405]
	TIME [epoch: 8.11 sec]
EPOCH 867/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030147752365742474		[learning rate: 0.00019105]
		[batch 20/20] avg loss: 0.03126519001886313		[learning rate: 0.00019059]
	Learning Rate: 0.000190592
	LOSS [training: 0.030706471192302808 | validation: 0.037587936845690534]
	TIME [epoch: 8.1 sec]
EPOCH 868/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.029691374195073433		[learning rate: 0.00019013]
		[batch 20/20] avg loss: 0.019469611809846132		[learning rate: 0.00018967]
	Learning Rate: 0.000189671
	LOSS [training: 0.024580493002459784 | validation: 0.025367003871212213]
	TIME [epoch: 8.1 sec]
EPOCH 869/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022288948531744218		[learning rate: 0.00018921]
		[batch 20/20] avg loss: 0.027182919064462425		[learning rate: 0.00018875]
	Learning Rate: 0.000188753
	LOSS [training: 0.024735933798103318 | validation: 0.03154256637931954]
	TIME [epoch: 8.11 sec]
EPOCH 870/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022890140260521264		[learning rate: 0.0001883]
		[batch 20/20] avg loss: 0.02736384092653642		[learning rate: 0.00018784]
	Learning Rate: 0.000187841
	LOSS [training: 0.025126990593528846 | validation: 0.027408310893274557]
	TIME [epoch: 8.11 sec]
EPOCH 871/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0241020180027351		[learning rate: 0.00018739]
		[batch 20/20] avg loss: 0.019572380456573425		[learning rate: 0.00018693]
	Learning Rate: 0.000186932
	LOSS [training: 0.021837199229654262 | validation: 0.029824898826095503]
	TIME [epoch: 8.1 sec]
EPOCH 872/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.019237716785961083		[learning rate: 0.00018648]
		[batch 20/20] avg loss: 0.028900396467392543		[learning rate: 0.00018603]
	Learning Rate: 0.000186028
	LOSS [training: 0.024069056626676816 | validation: 0.03361972145505324]
	TIME [epoch: 8.09 sec]
EPOCH 873/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03183557553414197		[learning rate: 0.00018558]
		[batch 20/20] avg loss: 0.0226113133266096		[learning rate: 0.00018513]
	Learning Rate: 0.000185129
	LOSS [training: 0.027223444430375787 | validation: 0.029382554280707532]
	TIME [epoch: 8.1 sec]
EPOCH 874/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03222471465555243		[learning rate: 0.00018468]
		[batch 20/20] avg loss: 0.019859565823975266		[learning rate: 0.00018423]
	Learning Rate: 0.000184233
	LOSS [training: 0.02604214023976385 | validation: 0.022690128627335843]
	TIME [epoch: 8.12 sec]
EPOCH 875/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024660326655281173		[learning rate: 0.00018379]
		[batch 20/20] avg loss: 0.0315444701212137		[learning rate: 0.00018334]
	Learning Rate: 0.000183343
	LOSS [training: 0.028102398388247442 | validation: 0.015698137125049177]
	TIME [epoch: 8.11 sec]
EPOCH 876/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026173372538481694		[learning rate: 0.0001829]
		[batch 20/20] avg loss: 0.025468190924957577		[learning rate: 0.00018246]
	Learning Rate: 0.000182456
	LOSS [training: 0.025820781731719634 | validation: 0.02628690541022681]
	TIME [epoch: 8.1 sec]
EPOCH 877/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01902355227707169		[learning rate: 0.00018201]
		[batch 20/20] avg loss: 0.030736806100708468		[learning rate: 0.00018157]
	Learning Rate: 0.000181574
	LOSS [training: 0.02488017918889008 | validation: 0.01943352832951933]
	TIME [epoch: 8.1 sec]
EPOCH 878/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02734943762481894		[learning rate: 0.00018113]
		[batch 20/20] avg loss: 0.021320564477835605		[learning rate: 0.0001807]
	Learning Rate: 0.000180696
	LOSS [training: 0.02433500105132727 | validation: 0.030659311264437255]
	TIME [epoch: 8.12 sec]
EPOCH 879/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028945250136623496		[learning rate: 0.00018026]
		[batch 20/20] avg loss: 0.020103149406612026		[learning rate: 0.00017982]
	Learning Rate: 0.000179822
	LOSS [training: 0.02452419977161776 | validation: 0.026716630066405112]
	TIME [epoch: 8.09 sec]
EPOCH 880/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03636709600020599		[learning rate: 0.00017939]
		[batch 20/20] avg loss: 0.024050272318892858		[learning rate: 0.00017895]
	Learning Rate: 0.000178952
	LOSS [training: 0.030208684159549432 | validation: 0.02095745157613241]
	TIME [epoch: 8.1 sec]
EPOCH 881/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023590795120599355		[learning rate: 0.00017852]
		[batch 20/20] avg loss: 0.025341204487969882		[learning rate: 0.00017809]
	Learning Rate: 0.000178087
	LOSS [training: 0.02446599980428462 | validation: 0.024404203007350883]
	TIME [epoch: 8.09 sec]
EPOCH 882/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.027021989307302548		[learning rate: 0.00017766]
		[batch 20/20] avg loss: 0.026476965120433276		[learning rate: 0.00017723]
	Learning Rate: 0.000177226
	LOSS [training: 0.02674947721386791 | validation: 0.03021214297278785]
	TIME [epoch: 8.12 sec]
EPOCH 883/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02247075358270795		[learning rate: 0.0001768]
		[batch 20/20] avg loss: 0.03488222741496113		[learning rate: 0.00017637]
	Learning Rate: 0.000176369
	LOSS [training: 0.028676490498834546 | validation: 0.03753769178584391]
	TIME [epoch: 8.09 sec]
EPOCH 884/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025906148042370604		[learning rate: 0.00017594]
		[batch 20/20] avg loss: 0.03095310595667356		[learning rate: 0.00017552]
	Learning Rate: 0.000175516
	LOSS [training: 0.028429626999522072 | validation: 0.03370470923858906]
	TIME [epoch: 8.1 sec]
EPOCH 885/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023285388016742972		[learning rate: 0.00017509]
		[batch 20/20] avg loss: 0.019959975470479743		[learning rate: 0.00017467]
	Learning Rate: 0.000174667
	LOSS [training: 0.021622681743611354 | validation: 0.026833273861329356]
	TIME [epoch: 8.09 sec]
EPOCH 886/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.020348425905674986		[learning rate: 0.00017424]
		[batch 20/20] avg loss: 0.025315942261441026		[learning rate: 0.00017382]
	Learning Rate: 0.000173822
	LOSS [training: 0.022832184083558006 | validation: 0.021931392975034016]
	TIME [epoch: 8.12 sec]
EPOCH 887/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022870067103139408		[learning rate: 0.0001734]
		[batch 20/20] avg loss: 0.023756773240033842		[learning rate: 0.00017298]
	Learning Rate: 0.000172982
	LOSS [training: 0.023313420171586618 | validation: 0.019828737857754516]
	TIME [epoch: 8.09 sec]
EPOCH 888/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.027151536953699113		[learning rate: 0.00017256]
		[batch 20/20] avg loss: 0.029880473506142934		[learning rate: 0.00017215]
	Learning Rate: 0.000172145
	LOSS [training: 0.028516005229921027 | validation: 0.02033811397313659]
	TIME [epoch: 8.1 sec]
EPOCH 889/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023177630136859088		[learning rate: 0.00017173]
		[batch 20/20] avg loss: 0.0248041773790059		[learning rate: 0.00017131]
	Learning Rate: 0.000171313
	LOSS [training: 0.023990903757932495 | validation: 0.02858092703881157]
	TIME [epoch: 8.09 sec]
EPOCH 890/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.018628758847163605		[learning rate: 0.0001709]
		[batch 20/20] avg loss: 0.025779058429740216		[learning rate: 0.00017048]
	Learning Rate: 0.000170484
	LOSS [training: 0.022203908638451918 | validation: 0.02242823510082413]
	TIME [epoch: 8.12 sec]
EPOCH 891/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026208760816578636		[learning rate: 0.00017007]
		[batch 20/20] avg loss: 0.019130718927176573		[learning rate: 0.00016966]
	Learning Rate: 0.00016966
	LOSS [training: 0.022669739871877606 | validation: 0.026273355058197723]
	TIME [epoch: 8.1 sec]
EPOCH 892/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.027253250611853814		[learning rate: 0.00016925]
		[batch 20/20] avg loss: 0.028809450712384575		[learning rate: 0.00016884]
	Learning Rate: 0.000168839
	LOSS [training: 0.028031350662119203 | validation: 0.02793673587013061]
	TIME [epoch: 8.1 sec]
EPOCH 893/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01914768626061348		[learning rate: 0.00016843]
		[batch 20/20] avg loss: 0.04607784241904948		[learning rate: 0.00016802]
	Learning Rate: 0.000168023
	LOSS [training: 0.03261276433983148 | validation: 0.020822023126788287]
	TIME [epoch: 8.1 sec]
EPOCH 894/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023250377214844804		[learning rate: 0.00016762]
		[batch 20/20] avg loss: 0.021483423223150742		[learning rate: 0.00016721]
	Learning Rate: 0.00016721
	LOSS [training: 0.022366900218997766 | validation: 0.031959386108967064]
	TIME [epoch: 8.12 sec]
EPOCH 895/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023555501862563454		[learning rate: 0.00016681]
		[batch 20/20] avg loss: 0.021610655101586794		[learning rate: 0.0001664]
	Learning Rate: 0.000166402
	LOSS [training: 0.02258307848207513 | validation: 0.02689231122561742]
	TIME [epoch: 8.1 sec]
EPOCH 896/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.018477044139490643		[learning rate: 0.000166]
		[batch 20/20] avg loss: 0.0344255310907885		[learning rate: 0.0001656]
	Learning Rate: 0.000165597
	LOSS [training: 0.026451287615139575 | validation: 0.02183492975199094]
	TIME [epoch: 8.1 sec]
EPOCH 897/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.027721509796527887		[learning rate: 0.0001652]
		[batch 20/20] avg loss: 0.02959311473992755		[learning rate: 0.0001648]
	Learning Rate: 0.000164796
	LOSS [training: 0.02865731226822772 | validation: 0.015483379189594961]
	TIME [epoch: 8.1 sec]
EPOCH 898/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.020945680439110395		[learning rate: 0.0001644]
		[batch 20/20] avg loss: 0.023133356882337825		[learning rate: 0.000164]
	Learning Rate: 0.000163999
	LOSS [training: 0.02203951866072411 | validation: 0.04264654404401975]
	TIME [epoch: 8.12 sec]
EPOCH 899/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0383611152789079		[learning rate: 0.0001636]
		[batch 20/20] avg loss: 0.02297844219747843		[learning rate: 0.00016321]
	Learning Rate: 0.000163206
	LOSS [training: 0.03066977873819316 | validation: 0.025510741303816045]
	TIME [epoch: 8.1 sec]
EPOCH 900/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03279578479121006		[learning rate: 0.00016281]
		[batch 20/20] avg loss: 0.02607675108128611		[learning rate: 0.00016242]
	Learning Rate: 0.000162417
	LOSS [training: 0.029436267936248088 | validation: 0.02512754775567648]
	TIME [epoch: 8.1 sec]
EPOCH 901/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024730085050673933		[learning rate: 0.00016202]
		[batch 20/20] avg loss: 0.020523266967488917		[learning rate: 0.00016163]
	Learning Rate: 0.000161632
	LOSS [training: 0.022626676009081425 | validation: 0.022406066118560303]
	TIME [epoch: 8.1 sec]
EPOCH 902/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.021600665768886994		[learning rate: 0.00016124]
		[batch 20/20] avg loss: 0.024839037319891396		[learning rate: 0.00016085]
	Learning Rate: 0.00016085
	LOSS [training: 0.023219851544389197 | validation: 0.02239999231726659]
	TIME [epoch: 8.12 sec]
EPOCH 903/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02381551797523944		[learning rate: 0.00016046]
		[batch 20/20] avg loss: 0.021711039129829995		[learning rate: 0.00016007]
	Learning Rate: 0.000160072
	LOSS [training: 0.022763278552534717 | validation: 0.024244526567127316]
	TIME [epoch: 8.11 sec]
EPOCH 904/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026263384900791942		[learning rate: 0.00015968]
		[batch 20/20] avg loss: 0.022855146774374308		[learning rate: 0.0001593]
	Learning Rate: 0.000159298
	LOSS [training: 0.02455926583758312 | validation: 0.016844896509337196]
	TIME [epoch: 8.11 sec]
EPOCH 905/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.020141697034907122		[learning rate: 0.00015891]
		[batch 20/20] avg loss: 0.029741400664212676		[learning rate: 0.00015853]
	Learning Rate: 0.000158528
	LOSS [training: 0.0249415488495599 | validation: 0.025532465687111358]
	TIME [epoch: 8.1 sec]
EPOCH 906/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02975289657175582		[learning rate: 0.00015814]
		[batch 20/20] avg loss: 0.024197062557114492		[learning rate: 0.00015776]
	Learning Rate: 0.000157761
	LOSS [training: 0.026974979564435158 | validation: 0.02602833223158632]
	TIME [epoch: 8.12 sec]
EPOCH 907/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0271088333781023		[learning rate: 0.00015738]
		[batch 20/20] avg loss: 0.0236530828783355		[learning rate: 0.000157]
	Learning Rate: 0.000156998
	LOSS [training: 0.025380958128218896 | validation: 0.023544888075677384]
	TIME [epoch: 8.11 sec]
EPOCH 908/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0245280177496623		[learning rate: 0.00015662]
		[batch 20/20] avg loss: 0.022714499759620782		[learning rate: 0.00015624]
	Learning Rate: 0.000156239
	LOSS [training: 0.023621258754641543 | validation: 0.03939650582043873]
	TIME [epoch: 8.1 sec]
EPOCH 909/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03250733069005577		[learning rate: 0.00015586]
		[batch 20/20] avg loss: 0.02504384641030834		[learning rate: 0.00015548]
	Learning Rate: 0.000155483
	LOSS [training: 0.028775588550182057 | validation: 0.02346913060932279]
	TIME [epoch: 8.09 sec]
EPOCH 910/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01749688989481205		[learning rate: 0.00015511]
		[batch 20/20] avg loss: 0.022772073539180794		[learning rate: 0.00015473]
	Learning Rate: 0.000154732
	LOSS [training: 0.020134481716996423 | validation: 0.02388780177131917]
	TIME [epoch: 8.11 sec]
EPOCH 911/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.027609405900711675		[learning rate: 0.00015436]
		[batch 20/20] avg loss: 0.02722134763464603		[learning rate: 0.00015398]
	Learning Rate: 0.000153983
	LOSS [training: 0.02741537676767885 | validation: 0.025974982279722413]
	TIME [epoch: 8.1 sec]
EPOCH 912/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024501603067399756		[learning rate: 0.00015361]
		[batch 20/20] avg loss: 0.026319950243188527		[learning rate: 0.00015324]
	Learning Rate: 0.000153239
	LOSS [training: 0.02541077665529414 | validation: 0.027147873544867935]
	TIME [epoch: 8.1 sec]
EPOCH 913/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02307935844427809		[learning rate: 0.00015287]
		[batch 20/20] avg loss: 0.02435345459426898		[learning rate: 0.0001525]
	Learning Rate: 0.000152498
	LOSS [training: 0.023716406519273533 | validation: 0.02673850244544556]
	TIME [epoch: 8.1 sec]
EPOCH 914/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.012352481771349571		[learning rate: 0.00015213]
		[batch 20/20] avg loss: 0.02847245577768744		[learning rate: 0.00015176]
	Learning Rate: 0.00015176
	LOSS [training: 0.020412468774518504 | validation: 0.023816590219160695]
	TIME [epoch: 8.1 sec]
EPOCH 915/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03881357527017253		[learning rate: 0.00015139]
		[batch 20/20] avg loss: 0.02269002769181974		[learning rate: 0.00015103]
	Learning Rate: 0.000151026
	LOSS [training: 0.030751801480996134 | validation: 0.022038262392688718]
	TIME [epoch: 8.11 sec]
EPOCH 916/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02240561428444603		[learning rate: 0.00015066]
		[batch 20/20] avg loss: 0.028650409303065515		[learning rate: 0.0001503]
	Learning Rate: 0.000150296
	LOSS [training: 0.025528011793755773 | validation: 0.021451935763291963]
	TIME [epoch: 8.09 sec]
EPOCH 917/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.027598992676080313		[learning rate: 0.00014993]
		[batch 20/20] avg loss: 0.018139981564035436		[learning rate: 0.00014957]
	Learning Rate: 0.000149569
	LOSS [training: 0.02286948712005787 | validation: 0.019500241046146557]
	TIME [epoch: 8.1 sec]
EPOCH 918/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025309787394638218		[learning rate: 0.00014921]
		[batch 20/20] avg loss: 0.029286841168836092		[learning rate: 0.00014885]
	Learning Rate: 0.000148846
	LOSS [training: 0.02729831428173715 | validation: 0.034048836821998454]
	TIME [epoch: 8.11 sec]
EPOCH 919/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026675841495847424		[learning rate: 0.00014849]
		[batch 20/20] avg loss: 0.02083129945954412		[learning rate: 0.00014813]
	Learning Rate: 0.000148126
	LOSS [training: 0.023753570477695773 | validation: 0.017505976399272226]
	TIME [epoch: 8.13 sec]
EPOCH 920/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.029258868957122697		[learning rate: 0.00014777]
		[batch 20/20] avg loss: 0.020091413203244445		[learning rate: 0.00014741]
	Learning Rate: 0.00014741
	LOSS [training: 0.024675141080183573 | validation: 0.02175255531500757]
	TIME [epoch: 8.1 sec]
EPOCH 921/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03050529491686983		[learning rate: 0.00014705]
		[batch 20/20] avg loss: 0.017174589098781064		[learning rate: 0.0001467]
	Learning Rate: 0.000146697
	LOSS [training: 0.02383994200782545 | validation: 0.02910994633929243]
	TIME [epoch: 8.1 sec]
EPOCH 922/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023794006448195824		[learning rate: 0.00014634]
		[batch 20/20] avg loss: 0.02260851164386308		[learning rate: 0.00014599]
	Learning Rate: 0.000145988
	LOSS [training: 0.023201259046029453 | validation: 0.024551756250244604]
	TIME [epoch: 8.1 sec]
EPOCH 923/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025876432699964348		[learning rate: 0.00014563]
		[batch 20/20] avg loss: 0.022996243406841754		[learning rate: 0.00014528]
	Learning Rate: 0.000145282
	LOSS [training: 0.02443633805340305 | validation: 0.021354201693824345]
	TIME [epoch: 8.12 sec]
EPOCH 924/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02912510594118615		[learning rate: 0.00014493]
		[batch 20/20] avg loss: 0.018464509372158797		[learning rate: 0.00014458]
	Learning Rate: 0.000144579
	LOSS [training: 0.023794807656672474 | validation: 0.0325832594689134]
	TIME [epoch: 8.1 sec]
EPOCH 925/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.027142585877031243		[learning rate: 0.00014423]
		[batch 20/20] avg loss: 0.02466822641087153		[learning rate: 0.00014388]
	Learning Rate: 0.00014388
	LOSS [training: 0.025905406143951388 | validation: 0.02314022697364418]
	TIME [epoch: 8.1 sec]
EPOCH 926/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022449772483206676		[learning rate: 0.00014353]
		[batch 20/20] avg loss: 0.021877474524897605		[learning rate: 0.00014318]
	Learning Rate: 0.000143184
	LOSS [training: 0.02216362350405214 | validation: 0.023170663508027763]
	TIME [epoch: 8.1 sec]
EPOCH 927/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025533207987931256		[learning rate: 0.00014284]
		[batch 20/20] avg loss: 0.023811316821623733		[learning rate: 0.00014249]
	Learning Rate: 0.000142492
	LOSS [training: 0.024672262404777497 | validation: 0.021367240482577564]
	TIME [epoch: 8.12 sec]
EPOCH 928/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.020512912614414628		[learning rate: 0.00014215]
		[batch 20/20] avg loss: 0.021334930765544935		[learning rate: 0.0001418]
	Learning Rate: 0.000141803
	LOSS [training: 0.020923921689979787 | validation: 0.029640452698838266]
	TIME [epoch: 8.1 sec]
EPOCH 929/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026619842218183344		[learning rate: 0.00014146]
		[batch 20/20] avg loss: 0.03501019458814812		[learning rate: 0.00014112]
	Learning Rate: 0.000141117
	LOSS [training: 0.030815018403165733 | validation: 0.026747403583973123]
	TIME [epoch: 8.1 sec]
EPOCH 930/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023298992075171315		[learning rate: 0.00014078]
		[batch 20/20] avg loss: 0.02601640395169168		[learning rate: 0.00014043]
	Learning Rate: 0.000140434
	LOSS [training: 0.024657698013431498 | validation: 0.02277321772395654]
	TIME [epoch: 8.1 sec]
EPOCH 931/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022564236817511872		[learning rate: 0.00014009]
		[batch 20/20] avg loss: 0.029695859967076688		[learning rate: 0.00013976]
	Learning Rate: 0.000139755
	LOSS [training: 0.026130048392294276 | validation: 0.03402196196502196]
	TIME [epoch: 8.13 sec]
EPOCH 932/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02854831459648056		[learning rate: 0.00013942]
		[batch 20/20] avg loss: 0.029117043134676383		[learning rate: 0.00013908]
	Learning Rate: 0.00013908
	LOSS [training: 0.02883267886557848 | validation: 0.025657520151658193]
	TIME [epoch: 8.11 sec]
EPOCH 933/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023677839092313233		[learning rate: 0.00013874]
		[batch 20/20] avg loss: 0.020460722043878492		[learning rate: 0.00013841]
	Learning Rate: 0.000138407
	LOSS [training: 0.022069280568095866 | validation: 0.024672718457724126]
	TIME [epoch: 8.1 sec]
EPOCH 934/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.017184457697198237		[learning rate: 0.00013807]
		[batch 20/20] avg loss: 0.02715041572316803		[learning rate: 0.00013774]
	Learning Rate: 0.000137738
	LOSS [training: 0.022167436710183133 | validation: 0.038011246544096614]
	TIME [epoch: 8.11 sec]
EPOCH 935/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024722804431060154		[learning rate: 0.0001374]
		[batch 20/20] avg loss: 0.024077070804593415		[learning rate: 0.00013707]
	Learning Rate: 0.000137072
	LOSS [training: 0.024399937617826786 | validation: 0.0261993479982129]
	TIME [epoch: 8.12 sec]
EPOCH 936/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02179466196489265		[learning rate: 0.00013674]
		[batch 20/20] avg loss: 0.020019744162468732		[learning rate: 0.00013641]
	Learning Rate: 0.000136409
	LOSS [training: 0.020907203063680685 | validation: 0.02519477766160079]
	TIME [epoch: 8.11 sec]
EPOCH 937/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02262689802983161		[learning rate: 0.00013608]
		[batch 20/20] avg loss: 0.02698345515459948		[learning rate: 0.00013575]
	Learning Rate: 0.000135749
	LOSS [training: 0.024805176592215542 | validation: 0.021722610782954357]
	TIME [epoch: 8.1 sec]
EPOCH 938/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.020771977577718113		[learning rate: 0.00013542]
		[batch 20/20] avg loss: 0.028822330302004588		[learning rate: 0.00013509]
	Learning Rate: 0.000135093
	LOSS [training: 0.024797153939861354 | validation: 0.025999674495706757]
	TIME [epoch: 8.1 sec]
EPOCH 939/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030485325884060627		[learning rate: 0.00013477]
		[batch 20/20] avg loss: 0.027021036204102407		[learning rate: 0.00013444]
	Learning Rate: 0.000134439
	LOSS [training: 0.028753181044081515 | validation: 0.032183474837810484]
	TIME [epoch: 8.12 sec]
EPOCH 940/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02644237475233186		[learning rate: 0.00013411]
		[batch 20/20] avg loss: 0.018761118480084823		[learning rate: 0.00013379]
	Learning Rate: 0.000133789
	LOSS [training: 0.022601746616208344 | validation: 0.029501723429856762]
	TIME [epoch: 8.1 sec]
EPOCH 941/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01642860607253393		[learning rate: 0.00013347]
		[batch 20/20] avg loss: 0.02962248485193642		[learning rate: 0.00013314]
	Learning Rate: 0.000133142
	LOSS [training: 0.02302554546223517 | validation: 0.020870270000933878]
	TIME [epoch: 8.1 sec]
EPOCH 942/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.020581921471803995		[learning rate: 0.00013282]
		[batch 20/20] avg loss: 0.02576551739267266		[learning rate: 0.0001325]
	Learning Rate: 0.000132498
	LOSS [training: 0.02317371943223832 | validation: 0.02470277468611716]
	TIME [epoch: 8.1 sec]
EPOCH 943/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02753331541986336		[learning rate: 0.00013218]
		[batch 20/20] avg loss: 0.02454716295165688		[learning rate: 0.00013186]
	Learning Rate: 0.000131858
	LOSS [training: 0.026040239185760122 | validation: 0.029154083208053145]
	TIME [epoch: 8.13 sec]
EPOCH 944/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023041083522678155		[learning rate: 0.00013154]
		[batch 20/20] avg loss: 0.03289572393177821		[learning rate: 0.00013122]
	Learning Rate: 0.00013122
	LOSS [training: 0.02796840372722818 | validation: 0.02391518914115519]
	TIME [epoch: 8.1 sec]
EPOCH 945/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026008141704721717		[learning rate: 0.0001309]
		[batch 20/20] avg loss: 0.01988105429513381		[learning rate: 0.00013059]
	Learning Rate: 0.000130585
	LOSS [training: 0.02294459799992776 | validation: 0.020991096128191798]
	TIME [epoch: 8.1 sec]
EPOCH 946/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.031722052848277754		[learning rate: 0.00013027]
		[batch 20/20] avg loss: 0.024670745962018452		[learning rate: 0.00012995]
	Learning Rate: 0.000129954
	LOSS [training: 0.028196399405148103 | validation: 0.019885392759845664]
	TIME [epoch: 8.1 sec]
EPOCH 947/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02220353165462914		[learning rate: 0.00012964]
		[batch 20/20] avg loss: 0.023774145502009156		[learning rate: 0.00012933]
	Learning Rate: 0.000129326
	LOSS [training: 0.022988838578319147 | validation: 0.02764882439966588]
	TIME [epoch: 8.12 sec]
EPOCH 948/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026864858463493308		[learning rate: 0.00012901]
		[batch 20/20] avg loss: 0.028755501724819506		[learning rate: 0.0001287]
	Learning Rate: 0.0001287
	LOSS [training: 0.027810180094156402 | validation: 0.021598006970809724]
	TIME [epoch: 8.11 sec]
EPOCH 949/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01869412746802072		[learning rate: 0.00012839]
		[batch 20/20] avg loss: 0.028861614990022683		[learning rate: 0.00012808]
	Learning Rate: 0.000128078
	LOSS [training: 0.023777871229021703 | validation: 0.023551157287729687]
	TIME [epoch: 8.1 sec]
EPOCH 950/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03373169419562664		[learning rate: 0.00012777]
		[batch 20/20] avg loss: 0.018733814364118945		[learning rate: 0.00012746]
	Learning Rate: 0.000127458
	LOSS [training: 0.026232754279872796 | validation: 0.0248148313649897]
	TIME [epoch: 8.1 sec]
EPOCH 951/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022152133159593516		[learning rate: 0.00012715]
		[batch 20/20] avg loss: 0.021619494756577776		[learning rate: 0.00012684]
	Learning Rate: 0.000126842
	LOSS [training: 0.021885813958085646 | validation: 0.026336640645030118]
	TIME [epoch: 8.12 sec]
EPOCH 952/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025104897407302672		[learning rate: 0.00012653]
		[batch 20/20] avg loss: 0.024487122765526226		[learning rate: 0.00012623]
	Learning Rate: 0.000126229
	LOSS [training: 0.02479601008641445 | validation: 0.02670492313114565]
	TIME [epoch: 8.11 sec]
EPOCH 953/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02601870276895542		[learning rate: 0.00012592]
		[batch 20/20] avg loss: 0.01895291950458823		[learning rate: 0.00012562]
	Learning Rate: 0.000125618
	LOSS [training: 0.022485811136771828 | validation: 0.029500780923579994]
	TIME [epoch: 8.1 sec]
EPOCH 954/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026528593766648782		[learning rate: 0.00012531]
		[batch 20/20] avg loss: 0.022929395763905968		[learning rate: 0.00012501]
	Learning Rate: 0.000125011
	LOSS [training: 0.024728994765277373 | validation: 0.025898137603547894]
	TIME [epoch: 8.1 sec]
EPOCH 955/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028073459608439426		[learning rate: 0.00012471]
		[batch 20/20] avg loss: 0.01528601827604217		[learning rate: 0.00012441]
	Learning Rate: 0.000124406
	LOSS [training: 0.0216797389422408 | validation: 0.024342469612332364]
	TIME [epoch: 8.11 sec]
EPOCH 956/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02871623447987366		[learning rate: 0.00012411]
		[batch 20/20] avg loss: 0.022458202309644536		[learning rate: 0.0001238]
	Learning Rate: 0.000123805
	LOSS [training: 0.025587218394759102 | validation: 0.024620498811735014]
	TIME [epoch: 8.11 sec]
EPOCH 957/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02195041301527807		[learning rate: 0.0001235]
		[batch 20/20] avg loss: 0.028298626575918917		[learning rate: 0.00012321]
	Learning Rate: 0.000123206
	LOSS [training: 0.025124519795598495 | validation: 0.024358445755984105]
	TIME [epoch: 8.1 sec]
EPOCH 958/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.019325420375330558		[learning rate: 0.00012291]
		[batch 20/20] avg loss: 0.02089937008451124		[learning rate: 0.00012261]
	Learning Rate: 0.00012261
	LOSS [training: 0.020112395229920893 | validation: 0.018808343794889132]
	TIME [epoch: 8.1 sec]
EPOCH 959/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023273100066815773		[learning rate: 0.00012231]
		[batch 20/20] avg loss: 0.01990925560314168		[learning rate: 0.00012202]
	Learning Rate: 0.000122017
	LOSS [training: 0.02159117783497873 | validation: 0.029063915394576433]
	TIME [epoch: 8.11 sec]
EPOCH 960/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.020007409515215988		[learning rate: 0.00012172]
		[batch 20/20] avg loss: 0.02022305441482285		[learning rate: 0.00012143]
	Learning Rate: 0.000121427
	LOSS [training: 0.02011523196501942 | validation: 0.021917473225634705]
	TIME [epoch: 8.12 sec]
EPOCH 961/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028527076870287105		[learning rate: 0.00012113]
		[batch 20/20] avg loss: 0.019012380308190687		[learning rate: 0.00012084]
	Learning Rate: 0.00012084
	LOSS [training: 0.02376972858923889 | validation: 0.025415921080215488]
	TIME [epoch: 8.1 sec]
EPOCH 962/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.019836953642247843		[learning rate: 0.00012055]
		[batch 20/20] avg loss: 0.020938581215298215		[learning rate: 0.00012026]
	Learning Rate: 0.000120256
	LOSS [training: 0.020387767428773025 | validation: 0.02299502020916565]
	TIME [epoch: 8.1 sec]
EPOCH 963/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.016365311954587826		[learning rate: 0.00011996]
		[batch 20/20] avg loss: 0.02470454859011536		[learning rate: 0.00011967]
	Learning Rate: 0.000119674
	LOSS [training: 0.020534930272351593 | validation: 0.030120000057465762]
	TIME [epoch: 8.11 sec]
EPOCH 964/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026711634752483247		[learning rate: 0.00011938]
		[batch 20/20] avg loss: 0.01663581352908569		[learning rate: 0.0001191]
	Learning Rate: 0.000119095
	LOSS [training: 0.021673724140784466 | validation: 0.015293042246276357]
	TIME [epoch: 8.12 sec]
EPOCH 965/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025434305530564062		[learning rate: 0.00011881]
		[batch 20/20] avg loss: 0.01626627113212597		[learning rate: 0.00011852]
	Learning Rate: 0.000118519
	LOSS [training: 0.02085028833134501 | validation: 0.02586251159301736]
	TIME [epoch: 8.1 sec]
EPOCH 966/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023160344132206762		[learning rate: 0.00011823]
		[batch 20/20] avg loss: 0.026464166539023493		[learning rate: 0.00011795]
	Learning Rate: 0.000117946
	LOSS [training: 0.024812255335615124 | validation: 0.023436478485147662]
	TIME [epoch: 8.1 sec]
EPOCH 967/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0271673484456084		[learning rate: 0.00011766]
		[batch 20/20] avg loss: 0.022555269687170056		[learning rate: 0.00011738]
	Learning Rate: 0.000117376
	LOSS [training: 0.02486130906638923 | validation: 0.02401587057009179]
	TIME [epoch: 8.1 sec]
EPOCH 968/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0211676786253387		[learning rate: 0.00011709]
		[batch 20/20] avg loss: 0.027080357935959826		[learning rate: 0.00011681]
	Learning Rate: 0.000116808
	LOSS [training: 0.024124018280649258 | validation: 0.01744223574198923]
	TIME [epoch: 8.12 sec]
EPOCH 969/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.017032430078216275		[learning rate: 0.00011653]
		[batch 20/20] avg loss: 0.021573346683152167		[learning rate: 0.00011624]
	Learning Rate: 0.000116243
	LOSS [training: 0.01930288838068422 | validation: 0.01943370563082968]
	TIME [epoch: 8.11 sec]
EPOCH 970/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02381673997367977		[learning rate: 0.00011596]
		[batch 20/20] avg loss: 0.021177603348160694		[learning rate: 0.00011568]
	Learning Rate: 0.000115681
	LOSS [training: 0.02249717166092023 | validation: 0.02338715447166765]
	TIME [epoch: 8.1 sec]
EPOCH 971/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02001354083302782		[learning rate: 0.0001154]
		[batch 20/20] avg loss: 0.02279232593700114		[learning rate: 0.00011512]
	Learning Rate: 0.000115122
	LOSS [training: 0.021402933385014478 | validation: 0.026907990466705483]
	TIME [epoch: 8.1 sec]
EPOCH 972/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03398993428408935		[learning rate: 0.00011484]
		[batch 20/20] avg loss: 0.029541701169555297		[learning rate: 0.00011457]
	Learning Rate: 0.000114565
	LOSS [training: 0.031765817726822326 | validation: 0.023931641424719416]
	TIME [epoch: 8.12 sec]
EPOCH 973/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03304512237175392		[learning rate: 0.00011429]
		[batch 20/20] avg loss: 0.024356055677833853		[learning rate: 0.00011401]
	Learning Rate: 0.000114011
	LOSS [training: 0.028700589024793895 | validation: 0.019652519761123383]
	TIME [epoch: 8.1 sec]
EPOCH 974/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02207344130536365		[learning rate: 0.00011374]
		[batch 20/20] avg loss: 0.030864508279095648		[learning rate: 0.00011346]
	Learning Rate: 0.00011346
	LOSS [training: 0.02646897479222965 | validation: 0.030113802585571947]
	TIME [epoch: 8.11 sec]
EPOCH 975/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025259642198300243		[learning rate: 0.00011319]
		[batch 20/20] avg loss: 0.029143919630803365		[learning rate: 0.00011291]
	Learning Rate: 0.000112911
	LOSS [training: 0.027201780914551804 | validation: 0.02367706374185967]
	TIME [epoch: 8.1 sec]
EPOCH 976/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023957774879859926		[learning rate: 0.00011264]
		[batch 20/20] avg loss: 0.028820112714412056		[learning rate: 0.00011237]
	Learning Rate: 0.000112365
	LOSS [training: 0.026388943797135984 | validation: 0.01810234166750755]
	TIME [epoch: 8.13 sec]
EPOCH 977/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.020691169155229613		[learning rate: 0.00011209]
		[batch 20/20] avg loss: 0.025286927984858248		[learning rate: 0.00011182]
	Learning Rate: 0.000111822
	LOSS [training: 0.02298904857004393 | validation: 0.0190256019232404]
	TIME [epoch: 8.1 sec]
EPOCH 978/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0195112805139079		[learning rate: 0.00011155]
		[batch 20/20] avg loss: 0.01744515475807352		[learning rate: 0.00011128]
	Learning Rate: 0.000111281
	LOSS [training: 0.018478217635990713 | validation: 0.024607008632638545]
	TIME [epoch: 8.11 sec]
EPOCH 979/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.032653543910486915		[learning rate: 0.00011101]
		[batch 20/20] avg loss: 0.01581157511303725		[learning rate: 0.00011074]
	Learning Rate: 0.000110743
	LOSS [training: 0.024232559511762085 | validation: 0.021961326074208726]
	TIME [epoch: 8.1 sec]
EPOCH 980/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02586223605146497		[learning rate: 0.00011047]
		[batch 20/20] avg loss: 0.03573502020060497		[learning rate: 0.00011021]
	Learning Rate: 0.000110207
	LOSS [training: 0.030798628126034965 | validation: 0.0217560159832367]
	TIME [epoch: 8.12 sec]
EPOCH 981/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.016448237905672593		[learning rate: 0.00010994]
		[batch 20/20] avg loss: 0.029033332727489713		[learning rate: 0.00010967]
	Learning Rate: 0.000109674
	LOSS [training: 0.022740785316581157 | validation: 0.02824498483857217]
	TIME [epoch: 8.1 sec]
EPOCH 982/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02214109013235971		[learning rate: 0.00010941]
		[batch 20/20] avg loss: 0.025602959206389005		[learning rate: 0.00010914]
	Learning Rate: 0.000109144
	LOSS [training: 0.023872024669374353 | validation: 0.021118234655256935]
	TIME [epoch: 8.1 sec]
EPOCH 983/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0225787966571338		[learning rate: 0.00010888]
		[batch 20/20] avg loss: 0.020526855268476887		[learning rate: 0.00010862]
	Learning Rate: 0.000108616
	LOSS [training: 0.021552825962805344 | validation: 0.019051810214340617]
	TIME [epoch: 8.1 sec]
EPOCH 984/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01903666223930819		[learning rate: 0.00010835]
		[batch 20/20] avg loss: 0.025757853984351587		[learning rate: 0.00010809]
	Learning Rate: 0.000108091
	LOSS [training: 0.02239725811182989 | validation: 0.024195037049168887]
	TIME [epoch: 8.12 sec]
EPOCH 985/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.029816324798137245		[learning rate: 0.00010783]
		[batch 20/20] avg loss: 0.020220426086372083		[learning rate: 0.00010757]
	Learning Rate: 0.000107568
	LOSS [training: 0.025018375442254666 | validation: 0.018095042534624746]
	TIME [epoch: 8.1 sec]
EPOCH 986/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.027295465807983903		[learning rate: 0.00010731]
		[batch 20/20] avg loss: 0.02402330748286428		[learning rate: 0.00010705]
	Learning Rate: 0.000107048
	LOSS [training: 0.02565938664542409 | validation: 0.034162965599854885]
	TIME [epoch: 8.1 sec]
EPOCH 987/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025885846678016085		[learning rate: 0.00010679]
		[batch 20/20] avg loss: 0.022258825151159463		[learning rate: 0.00010653]
	Learning Rate: 0.00010653
	LOSS [training: 0.02407233591458777 | validation: 0.01991431556400853]
	TIME [epoch: 8.1 sec]
EPOCH 988/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02451198787378802		[learning rate: 0.00010627]
		[batch 20/20] avg loss: 0.02698920660575622		[learning rate: 0.00010602]
	Learning Rate: 0.000106015
	LOSS [training: 0.025750597239772122 | validation: 0.01702017657726692]
	TIME [epoch: 8.12 sec]
EPOCH 989/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.018148723297103007		[learning rate: 0.00010576]
		[batch 20/20] avg loss: 0.020812917915094434		[learning rate: 0.0001055]
	Learning Rate: 0.000105503
	LOSS [training: 0.01948082060609872 | validation: 0.030033465387318513]
	TIME [epoch: 8.11 sec]
EPOCH 990/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.021185044415991113		[learning rate: 0.00010525]
		[batch 20/20] avg loss: 0.02399192458219821		[learning rate: 0.00010499]
	Learning Rate: 0.000104992
	LOSS [training: 0.022588484499094657 | validation: 0.025661774534962666]
	TIME [epoch: 8.1 sec]
EPOCH 991/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023664982524923592		[learning rate: 0.00010474]
		[batch 20/20] avg loss: 0.021651267442395993		[learning rate: 0.00010448]
	Learning Rate: 0.000104485
	LOSS [training: 0.022658124983659794 | validation: 0.027319359051601468]
	TIME [epoch: 8.1 sec]
EPOCH 992/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022138794974389708		[learning rate: 0.00010423]
		[batch 20/20] avg loss: 0.02504586474403253		[learning rate: 0.00010398]
	Learning Rate: 0.000103979
	LOSS [training: 0.023592329859211114 | validation: 0.027005061317717245]
	TIME [epoch: 8.12 sec]
EPOCH 993/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02413338554535111		[learning rate: 0.00010373]
		[batch 20/20] avg loss: 0.0245534567741372		[learning rate: 0.00010348]
	Learning Rate: 0.000103477
	LOSS [training: 0.024343421159744156 | validation: 0.020643568817687103]
	TIME [epoch: 8.11 sec]
EPOCH 994/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026998514871727088		[learning rate: 0.00010323]
		[batch 20/20] avg loss: 0.021745528994913742		[learning rate: 0.00010298]
	Learning Rate: 0.000102976
	LOSS [training: 0.024372021933320408 | validation: 0.030227614911231552]
	TIME [epoch: 8.1 sec]
EPOCH 995/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01649966197230516		[learning rate: 0.00010273]
		[batch 20/20] avg loss: 0.03742135731174304		[learning rate: 0.00010248]
	Learning Rate: 0.000102478
	LOSS [training: 0.0269605096420241 | validation: 0.029750925777788415]
	TIME [epoch: 8.1 sec]
EPOCH 996/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0256697242997869		[learning rate: 0.00010223]
		[batch 20/20] avg loss: 0.019064479027155672		[learning rate: 0.00010198]
	Learning Rate: 0.000101983
	LOSS [training: 0.022367101663471285 | validation: 0.025495619678941435]
	TIME [epoch: 8.12 sec]
EPOCH 997/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02104980954916351		[learning rate: 0.00010174]
		[batch 20/20] avg loss: 0.021371660878020834		[learning rate: 0.00010149]
	Learning Rate: 0.000101489
	LOSS [training: 0.02121073521359217 | validation: 0.021900195045964143]
	TIME [epoch: 8.11 sec]
EPOCH 998/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02251573389381852		[learning rate: 0.00010124]
		[batch 20/20] avg loss: 0.015908633566123036		[learning rate: 0.000101]
	Learning Rate: 0.000100999
	LOSS [training: 0.019212183729970776 | validation: 0.01907895243355769]
	TIME [epoch: 8.1 sec]
EPOCH 999/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023468931887922838		[learning rate: 0.00010075]
		[batch 20/20] avg loss: 0.02248054544836951		[learning rate: 0.00010051]
	Learning Rate: 0.00010051
	LOSS [training: 0.022974738668146175 | validation: 0.023507620174459364]
	TIME [epoch: 8.1 sec]
EPOCH 1000/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023484340070980526		[learning rate: 0.00010027]
		[batch 20/20] avg loss: 0.01967145967951471		[learning rate: 0.00010002]
	Learning Rate: 0.000100024
	LOSS [training: 0.021577899875247622 | validation: 0.0333967341926925]
	TIME [epoch: 8.12 sec]
Finished training in 8218.888 seconds.
