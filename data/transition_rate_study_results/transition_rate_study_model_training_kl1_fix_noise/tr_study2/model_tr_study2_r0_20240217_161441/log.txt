Args:
Namespace(name='model_tr_study2', outdir='out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0', training_data='data/transition_rate_studies/tr_study2/tr_study2_training/r0', validation_data='data/transition_rate_studies/tr_study2/tr_study2_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.1, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4268891807

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 10/20] avg loss: 7.718010058040106		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.596044830990456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.157027444515282 | validation: 5.6796971389621005]
	TIME [epoch: 67.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.3723185396242785		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.493920118620421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.933119329122349 | validation: 3.9708520128460165]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_2.pth
	Model improved!!!
EPOCH 3/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.925283634319034		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.469121658679884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6972026464994583 | validation: 3.627312398070721]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_3.pth
	Model improved!!!
EPOCH 4/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.7400619178781067		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.909023685537793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8245428017079504 | validation: 2.983152959731709]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_4.pth
	Model improved!!!
EPOCH 5/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.192989749566937		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1839015747184294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.188445662142683 | validation: 3.0646653703747497]
	TIME [epoch: 8.8 sec]
EPOCH 6/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7196283853986494		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6641780589853563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6919032221920027 | validation: 1.4124738235305192]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_6.pth
	Model improved!!!
EPOCH 7/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.888575885134156		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.618239151536472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7534075183353142 | validation: 1.5238296962145101]
	TIME [epoch: 8.78 sec]
EPOCH 8/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7643208160751744		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.243310737580228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5038157768277012 | validation: 1.0017442263542993]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_8.pth
	Model improved!!!
EPOCH 9/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.220893644107816		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0727691784303623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1468314112690892 | validation: 1.203121343965769]
	TIME [epoch: 8.8 sec]
EPOCH 10/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6104763949605965		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0900542508435016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3502653229020491 | validation: 0.48567908504099433]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_10.pth
	Model improved!!!
EPOCH 11/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9502557726057453		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2747418077863735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1124987901960597 | validation: 0.5322203071394862]
	TIME [epoch: 8.79 sec]
EPOCH 12/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9586991491635886		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.805910149784245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8823046494739166 | validation: 1.0687088724344997]
	TIME [epoch: 8.78 sec]
EPOCH 13/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8652533678580626		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8706759674803946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8679646676692286 | validation: 0.5436966703258075]
	TIME [epoch: 8.79 sec]
EPOCH 14/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6534387488545741		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7667909416931563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7101148452738651 | validation: 0.6648630064855156]
	TIME [epoch: 8.81 sec]
EPOCH 15/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5704421594248987		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6646431669503541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6175426631876263 | validation: 0.5429544649072021]
	TIME [epoch: 8.8 sec]
EPOCH 16/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7052535596366527		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8219605713953317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7636070655159922 | validation: 0.721142191963889]
	TIME [epoch: 8.79 sec]
EPOCH 17/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6496068810563821		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.589966214820031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6197865479382066 | validation: 0.5696879343239855]
	TIME [epoch: 8.78 sec]
EPOCH 18/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.588243900453924		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6959288669383907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6420863836961576 | validation: 0.595569634770275]
	TIME [epoch: 8.78 sec]
EPOCH 19/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6844865643004757		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5932611183601981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.638873841330337 | validation: 0.36340443475597306]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_19.pth
	Model improved!!!
EPOCH 20/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7216326008996528		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.541893306779538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6317629538395954 | validation: 0.41025038770759625]
	TIME [epoch: 8.79 sec]
EPOCH 21/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.48459596056189086		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6831097618128127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5838528611873519 | validation: 0.5876655768760836]
	TIME [epoch: 8.78 sec]
EPOCH 22/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5076635268844133		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6332667504553191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.570465138669866 | validation: 0.5750453736286945]
	TIME [epoch: 8.79 sec]
EPOCH 23/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5455191560075756		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5582709599409078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5518950579742417 | validation: 0.47585567736450185]
	TIME [epoch: 8.78 sec]
EPOCH 24/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5797017595543605		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4968032714337115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5382525154940361 | validation: 0.34573910537168717]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_24.pth
	Model improved!!!
EPOCH 25/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8178318433277705		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4628595957284686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6403457195281194 | validation: 0.5680434303207714]
	TIME [epoch: 8.78 sec]
EPOCH 26/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5215107940024334		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6143613075020705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5679360507522518 | validation: 0.49360919342502857]
	TIME [epoch: 8.76 sec]
EPOCH 27/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.567557812678576		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5618297308148207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5646937717466982 | validation: 0.5369491617938511]
	TIME [epoch: 8.79 sec]
EPOCH 28/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.49021005847705945		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5774942028651713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5338521306711154 | validation: 0.3045629651617875]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_28.pth
	Model improved!!!
EPOCH 29/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5075051550832786		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.49557915364548394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5015421543643813 | validation: 1.5528467934656556]
	TIME [epoch: 8.8 sec]
EPOCH 30/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6415130210497906		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4794228938973676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5604679574735792 | validation: 0.48973880766072286]
	TIME [epoch: 8.81 sec]
EPOCH 31/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5312536778377978		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5228136929561444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.527033685396971 | validation: 0.3393447216756853]
	TIME [epoch: 8.77 sec]
EPOCH 32/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5104724432401954		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.546544582036487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5285085126383411 | validation: 0.6076759471281291]
	TIME [epoch: 8.77 sec]
EPOCH 33/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5204679687719491		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.556753464402682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5386107165873154 | validation: 0.37244656548727373]
	TIME [epoch: 8.78 sec]
EPOCH 34/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5605683139143018		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5121294386700805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5363488762921912 | validation: 0.36338630556942075]
	TIME [epoch: 8.76 sec]
EPOCH 35/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5100148177247126		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.525291121404243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5176529695644778 | validation: 0.654570639037345]
	TIME [epoch: 8.8 sec]
EPOCH 36/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.608781760938611		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5302780898502749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.569529925394443 | validation: 0.7260507251571173]
	TIME [epoch: 8.76 sec]
EPOCH 37/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5234504880292823		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6179155247070657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5706830063681739 | validation: 0.4735316700927615]
	TIME [epoch: 8.78 sec]
EPOCH 38/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7257713633934292		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4155094126546685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5706403880240487 | validation: 0.6792398817495158]
	TIME [epoch: 8.76 sec]
EPOCH 39/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4436359822891488		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5577158868025582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5006759345458534 | validation: 0.5301670200832272]
	TIME [epoch: 8.77 sec]
EPOCH 40/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4988323650652102		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5340198613583673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5164261132117887 | validation: 0.4226241963023014]
	TIME [epoch: 8.79 sec]
EPOCH 41/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5243558684015193		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4808446643201406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5026002663608298 | validation: 0.38737351584780777]
	TIME [epoch: 8.76 sec]
EPOCH 42/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4591523983256266		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5138582912673952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4865053447965108 | validation: 0.3808093843892071]
	TIME [epoch: 8.78 sec]
EPOCH 43/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5064333068838526		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45942385581052747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48292858134719 | validation: 0.7039729345359064]
	TIME [epoch: 8.77 sec]
EPOCH 44/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5024056219146311		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5925645855275363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5474851037210838 | validation: 0.3290051400390403]
	TIME [epoch: 8.78 sec]
EPOCH 45/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5137621009801914		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.704035928232548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6088990146063697 | validation: 0.4130288395696898]
	TIME [epoch: 8.79 sec]
EPOCH 46/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.44032409002605916		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5920372695362973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5161806797811782 | validation: 0.38796391369493166]
	TIME [epoch: 8.76 sec]
EPOCH 47/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5226640461645083		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5353025210523384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5289832836084234 | validation: 0.5276298159440365]
	TIME [epoch: 8.77 sec]
EPOCH 48/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5003746527695904		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.46580495365812197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48308980321385625 | validation: 0.41221344373519425]
	TIME [epoch: 8.76 sec]
EPOCH 49/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.47637541556064233		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4257833135313076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.451079364545975 | validation: 1.2608059633243462]
	TIME [epoch: 8.77 sec]
EPOCH 50/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6042390122478828		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4984737884059558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5513564003269193 | validation: 0.6238083050129749]
	TIME [epoch: 8.79 sec]
EPOCH 51/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5495524602803201		[learning rate: 0.0099782]
		[batch 20/20] avg loss: 0.5133510668614847		[learning rate: 0.0099541]
	Learning Rate: 0.00995405
	LOSS [training: 0.5314517635709024 | validation: 0.3050192513297405]
	TIME [epoch: 8.78 sec]
EPOCH 52/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5299638101962004		[learning rate: 0.00993]
		[batch 20/20] avg loss: 0.583016431662455		[learning rate: 0.0099059]
	Learning Rate: 0.00990592
	LOSS [training: 0.5564901209293277 | validation: 0.4916838431509021]
	TIME [epoch: 8.77 sec]
EPOCH 53/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.48424762275980343		[learning rate: 0.0098819]
		[batch 20/20] avg loss: 0.4653295593927515		[learning rate: 0.009858]
	Learning Rate: 0.00985801
	LOSS [training: 0.47478859107627763 | validation: 0.38562318481038504]
	TIME [epoch: 8.76 sec]
EPOCH 54/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4735709571566026		[learning rate: 0.0098341]
		[batch 20/20] avg loss: 0.5938191002900183		[learning rate: 0.0098103]
	Learning Rate: 0.00981034
	LOSS [training: 0.5336950287233104 | validation: 0.43124145967465005]
	TIME [epoch: 8.76 sec]
EPOCH 55/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.49478962006677546		[learning rate: 0.0097866]
		[batch 20/20] avg loss: 0.454435936321142		[learning rate: 0.0097629]
	Learning Rate: 0.0097629
	LOSS [training: 0.4746127781939588 | validation: 0.33340992657986596]
	TIME [epoch: 8.79 sec]
EPOCH 56/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.472304456665518		[learning rate: 0.0097393]
		[batch 20/20] avg loss: 0.4455480908568338		[learning rate: 0.0097157]
	Learning Rate: 0.00971569
	LOSS [training: 0.458926273761176 | validation: 0.3669025891620707]
	TIME [epoch: 8.79 sec]
EPOCH 57/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.40473523896620023		[learning rate: 0.0096922]
		[batch 20/20] avg loss: 0.4576069554876653		[learning rate: 0.0096687]
	Learning Rate: 0.00966871
	LOSS [training: 0.4311710972269327 | validation: 0.34914233644850723]
	TIME [epoch: 8.79 sec]
EPOCH 58/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4884648802770112		[learning rate: 0.0096453]
		[batch 20/20] avg loss: 0.3897054899956651		[learning rate: 0.009622]
	Learning Rate: 0.00962195
	LOSS [training: 0.4390851851363382 | validation: 0.8052460818238729]
	TIME [epoch: 8.77 sec]
EPOCH 59/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5043508032704319		[learning rate: 0.0095987]
		[batch 20/20] avg loss: 0.4503932315052902		[learning rate: 0.0095754]
	Learning Rate: 0.00957542
	LOSS [training: 0.477372017387861 | validation: 0.3228349496411772]
	TIME [epoch: 8.77 sec]
EPOCH 60/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4829098460657287		[learning rate: 0.0095522]
		[batch 20/20] avg loss: 0.47458213269768257		[learning rate: 0.0095291]
	Learning Rate: 0.00952912
	LOSS [training: 0.47874598938170554 | validation: 0.3639154569016704]
	TIME [epoch: 8.78 sec]
EPOCH 61/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4676276381465983		[learning rate: 0.009506]
		[batch 20/20] avg loss: 0.4105313923073256		[learning rate: 0.009483]
	Learning Rate: 0.00948303
	LOSS [training: 0.43907951522696187 | validation: 0.22530657614418703]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_61.pth
	Model improved!!!
EPOCH 62/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4528908467578625		[learning rate: 0.0094601]
		[batch 20/20] avg loss: 0.4367236311068382		[learning rate: 0.0094372]
	Learning Rate: 0.00943718
	LOSS [training: 0.4448072389323504 | validation: 0.45033170655322535]
	TIME [epoch: 8.77 sec]
EPOCH 63/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.426744335925991		[learning rate: 0.0094143]
		[batch 20/20] avg loss: 0.4432177586718371		[learning rate: 0.0093915]
	Learning Rate: 0.00939154
	LOSS [training: 0.43498104729891407 | validation: 0.6458831690051641]
	TIME [epoch: 8.77 sec]
EPOCH 64/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4281936436704624		[learning rate: 0.0093688]
		[batch 20/20] avg loss: 0.4957917684092523		[learning rate: 0.0093461]
	Learning Rate: 0.00934612
	LOSS [training: 0.46199270603985737 | validation: 0.428666586089146]
	TIME [epoch: 8.78 sec]
EPOCH 65/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5652711666923295		[learning rate: 0.0093235]
		[batch 20/20] avg loss: 0.3699461474805289		[learning rate: 0.0093009]
	Learning Rate: 0.00930093
	LOSS [training: 0.4676086570864292 | validation: 0.3057736133692565]
	TIME [epoch: 8.77 sec]
EPOCH 66/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.42466586086805436		[learning rate: 0.0092784]
		[batch 20/20] avg loss: 0.6663798133564722		[learning rate: 0.009256]
	Learning Rate: 0.00925595
	LOSS [training: 0.5455228371122631 | validation: 0.2640226084603843]
	TIME [epoch: 8.78 sec]
EPOCH 67/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.40094178052196494		[learning rate: 0.0092335]
		[batch 20/20] avg loss: 0.39757931663523405		[learning rate: 0.0092112]
	Learning Rate: 0.00921119
	LOSS [training: 0.39926054857859944 | validation: 0.41368786603068486]
	TIME [epoch: 8.76 sec]
EPOCH 68/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4813962707162777		[learning rate: 0.0091889]
		[batch 20/20] avg loss: 0.5436601081370042		[learning rate: 0.0091666]
	Learning Rate: 0.00916665
	LOSS [training: 0.512528189426641 | validation: 0.2645524696917726]
	TIME [epoch: 8.77 sec]
EPOCH 69/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5621599033635805		[learning rate: 0.0091445]
		[batch 20/20] avg loss: 0.42899408957808116		[learning rate: 0.0091223]
	Learning Rate: 0.00912232
	LOSS [training: 0.49557699647083087 | validation: 0.2303103712709291]
	TIME [epoch: 8.77 sec]
EPOCH 70/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.38123323757246436		[learning rate: 0.0091002]
		[batch 20/20] avg loss: 0.4143567951485392		[learning rate: 0.0090782]
	Learning Rate: 0.0090782
	LOSS [training: 0.39779501636050185 | validation: 0.364218301578157]
	TIME [epoch: 8.76 sec]
EPOCH 71/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5394246519457113		[learning rate: 0.0090562]
		[batch 20/20] avg loss: 0.4638108416624619		[learning rate: 0.0090343]
	Learning Rate: 0.0090343
	LOSS [training: 0.5016177468040867 | validation: 0.2778429819231134]
	TIME [epoch: 8.8 sec]
EPOCH 72/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4100315951744804		[learning rate: 0.0090124]
		[batch 20/20] avg loss: 0.39752985898341164		[learning rate: 0.0089906]
	Learning Rate: 0.00899062
	LOSS [training: 0.403780727078946 | validation: 0.27101471823214596]
	TIME [epoch: 8.76 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.41942342704114		[learning rate: 0.0089689]
		[batch 20/20] avg loss: 0.4442393333433041		[learning rate: 0.0089471]
	Learning Rate: 0.00894714
	LOSS [training: 0.4318313801922221 | validation: 0.5553254096013159]
	TIME [epoch: 8.77 sec]
EPOCH 74/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36527175943439416		[learning rate: 0.0089255]
		[batch 20/20] avg loss: 0.43730990222839533		[learning rate: 0.0089039]
	Learning Rate: 0.00890387
	LOSS [training: 0.4012908308313947 | validation: 0.26610626299693246]
	TIME [epoch: 8.77 sec]
EPOCH 75/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3998282804323544		[learning rate: 0.0088823]
		[batch 20/20] avg loss: 0.5096867649959719		[learning rate: 0.0088608]
	Learning Rate: 0.00886082
	LOSS [training: 0.4547575227141632 | validation: 0.2776259340730568]
	TIME [epoch: 8.76 sec]
EPOCH 76/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.41055513579394143		[learning rate: 0.0088394]
		[batch 20/20] avg loss: 0.4558009933404873		[learning rate: 0.008818]
	Learning Rate: 0.00881797
	LOSS [training: 0.43317806456721436 | validation: 0.2848742618083821]
	TIME [epoch: 8.8 sec]
EPOCH 77/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5311686423417138		[learning rate: 0.0087966]
		[batch 20/20] avg loss: 0.37268675042873345		[learning rate: 0.0087753]
	Learning Rate: 0.00877532
	LOSS [training: 0.45192769638522357 | validation: 0.27487390306281695]
	TIME [epoch: 8.76 sec]
EPOCH 78/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3727971533221571		[learning rate: 0.0087541]
		[batch 20/20] avg loss: 0.435737732533072		[learning rate: 0.0087329]
	Learning Rate: 0.00873289
	LOSS [training: 0.40426744292761463 | validation: 0.3045460060114572]
	TIME [epoch: 8.76 sec]
EPOCH 79/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.39641786534189255		[learning rate: 0.0087117]
		[batch 20/20] avg loss: 0.37327358951833306		[learning rate: 0.0086907]
	Learning Rate: 0.00869066
	LOSS [training: 0.3848457274301128 | validation: 0.18749447291163163]
	TIME [epoch: 8.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_79.pth
	Model improved!!!
EPOCH 80/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.39952048100369053		[learning rate: 0.0086696]
		[batch 20/20] avg loss: 0.47014212849291737		[learning rate: 0.0086486]
	Learning Rate: 0.00864863
	LOSS [training: 0.4348313047483039 | validation: 0.3689601987197287]
	TIME [epoch: 8.76 sec]
EPOCH 81/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.39749820472814956		[learning rate: 0.0086277]
		[batch 20/20] avg loss: 0.3890500841702534		[learning rate: 0.0086068]
	Learning Rate: 0.00860681
	LOSS [training: 0.3932741444492015 | validation: 0.6479569357373972]
	TIME [epoch: 8.79 sec]
EPOCH 82/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5365338073840913		[learning rate: 0.008586]
		[batch 20/20] avg loss: 0.43140358856753547		[learning rate: 0.0085652]
	Learning Rate: 0.00856519
	LOSS [training: 0.4839686979758132 | validation: 0.37594684610648565]
	TIME [epoch: 8.77 sec]
EPOCH 83/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.35539338062969805		[learning rate: 0.0085445]
		[batch 20/20] avg loss: 0.42390191709915437		[learning rate: 0.0085238]
	Learning Rate: 0.00852377
	LOSS [training: 0.38964764886442615 | validation: 0.381153396754033]
	TIME [epoch: 8.78 sec]
EPOCH 84/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.37649218713389565		[learning rate: 0.0085031]
		[batch 20/20] avg loss: 0.42341127311262633		[learning rate: 0.0084825]
	Learning Rate: 0.00848255
	LOSS [training: 0.39995173012326096 | validation: 0.40123426085614433]
	TIME [epoch: 8.76 sec]
EPOCH 85/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3842884861996994		[learning rate: 0.008462]
		[batch 20/20] avg loss: 0.3379879880137334		[learning rate: 0.0084415]
	Learning Rate: 0.00844153
	LOSS [training: 0.3611382371067164 | validation: 0.4718939264795853]
	TIME [epoch: 8.77 sec]
EPOCH 86/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.497506765882134		[learning rate: 0.0084211]
		[batch 20/20] avg loss: 0.37878360276859435		[learning rate: 0.0084007]
	Learning Rate: 0.00840071
	LOSS [training: 0.4381451843253642 | validation: 0.4170390995828848]
	TIME [epoch: 8.79 sec]
EPOCH 87/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5196951416535034		[learning rate: 0.0083804]
		[batch 20/20] avg loss: 0.38147196889258617		[learning rate: 0.0083601]
	Learning Rate: 0.00836008
	LOSS [training: 0.4505835552730447 | validation: 0.23623841889280361]
	TIME [epoch: 8.76 sec]
EPOCH 88/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.41011536003252225		[learning rate: 0.0083398]
		[batch 20/20] avg loss: 0.3046525642022737		[learning rate: 0.0083197]
	Learning Rate: 0.00831965
	LOSS [training: 0.35738396211739804 | validation: 0.5912610765008802]
	TIME [epoch: 8.77 sec]
EPOCH 89/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4306722385233286		[learning rate: 0.0082995]
		[batch 20/20] avg loss: 0.41206936963555724		[learning rate: 0.0082794]
	Learning Rate: 0.00827942
	LOSS [training: 0.42137080407944294 | validation: 0.40390457200846913]
	TIME [epoch: 8.76 sec]
EPOCH 90/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3455573954385801		[learning rate: 0.0082594]
		[batch 20/20] avg loss: 0.429305815340734		[learning rate: 0.0082394]
	Learning Rate: 0.00823938
	LOSS [training: 0.38743160538965704 | validation: 0.23739540922568175]
	TIME [epoch: 8.77 sec]
EPOCH 91/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3259267013544046		[learning rate: 0.0082194]
		[batch 20/20] avg loss: 0.3640898658235988		[learning rate: 0.0081995]
	Learning Rate: 0.00819954
	LOSS [training: 0.3450082835890017 | validation: 0.5481967179963326]
	TIME [epoch: 8.78 sec]
EPOCH 92/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.44286192903181154		[learning rate: 0.0081797]
		[batch 20/20] avg loss: 0.29462361833210343		[learning rate: 0.0081599]
	Learning Rate: 0.00815989
	LOSS [training: 0.3687427736819575 | validation: 0.3031580022984536]
	TIME [epoch: 8.78 sec]
EPOCH 93/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4110407603936526		[learning rate: 0.0081401]
		[batch 20/20] avg loss: 0.32435907218008214		[learning rate: 0.0081204]
	Learning Rate: 0.00812043
	LOSS [training: 0.3676999162868674 | validation: 0.4115121174645083]
	TIME [epoch: 8.76 sec]
EPOCH 94/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3807802912390541		[learning rate: 0.0081008]
		[batch 20/20] avg loss: 0.45843970460784245		[learning rate: 0.0080812]
	Learning Rate: 0.00808116
	LOSS [training: 0.4196099979234482 | validation: 0.27208200933548066]
	TIME [epoch: 8.76 sec]
EPOCH 95/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3112833195328405		[learning rate: 0.0080616]
		[batch 20/20] avg loss: 0.32951072010682936		[learning rate: 0.0080421]
	Learning Rate: 0.00804208
	LOSS [training: 0.3203970198198349 | validation: 0.3095994761296345]
	TIME [epoch: 8.77 sec]
EPOCH 96/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.35873547809827727		[learning rate: 0.0080226]
		[batch 20/20] avg loss: 0.3923545510528872		[learning rate: 0.0080032]
	Learning Rate: 0.00800319
	LOSS [training: 0.3755450145755822 | validation: 0.33290759646578283]
	TIME [epoch: 8.79 sec]
EPOCH 97/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36112897323006604		[learning rate: 0.0079838]
		[batch 20/20] avg loss: 0.31968583736153783		[learning rate: 0.0079645]
	Learning Rate: 0.00796449
	LOSS [training: 0.34040740529580193 | validation: 0.30408421556085413]
	TIME [epoch: 8.81 sec]
EPOCH 98/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3197996332389797		[learning rate: 0.0079452]
		[batch 20/20] avg loss: 0.3149227606892214		[learning rate: 0.007926]
	Learning Rate: 0.00792597
	LOSS [training: 0.3173611969641006 | validation: 0.5964807518733364]
	TIME [epoch: 8.79 sec]
EPOCH 99/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.40858898241717245		[learning rate: 0.0079068]
		[batch 20/20] avg loss: 0.3136062064691606		[learning rate: 0.0078876]
	Learning Rate: 0.00788765
	LOSS [training: 0.36109759444316647 | validation: 0.4902045570785458]
	TIME [epoch: 8.77 sec]
EPOCH 100/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3752344656023556		[learning rate: 0.0078685]
		[batch 20/20] avg loss: 0.32739156784447865		[learning rate: 0.0078495]
	Learning Rate: 0.0078495
	LOSS [training: 0.35131301672341714 | validation: 0.37004696287696415]
	TIME [epoch: 8.78 sec]
EPOCH 101/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4463493037361578		[learning rate: 0.0078305]
		[batch 20/20] avg loss: 0.3929568987947313		[learning rate: 0.0078115]
	Learning Rate: 0.00781154
	LOSS [training: 0.4196531012654446 | validation: 0.19047597099329572]
	TIME [epoch: 8.77 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3605969559148939		[learning rate: 0.0077926]
		[batch 20/20] avg loss: 0.39623115311574747		[learning rate: 0.0077738]
	Learning Rate: 0.00777377
	LOSS [training: 0.3784140545153206 | validation: 0.27224386604680617]
	TIME [epoch: 8.81 sec]
EPOCH 103/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3172592101709529		[learning rate: 0.0077549]
		[batch 20/20] avg loss: 0.4096613697914548		[learning rate: 0.0077362]
	Learning Rate: 0.00773618
	LOSS [training: 0.36346028998120383 | validation: 0.3562882526215939]
	TIME [epoch: 8.77 sec]
EPOCH 104/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.367872298946233		[learning rate: 0.0077174]
		[batch 20/20] avg loss: 0.3427024425047902		[learning rate: 0.0076988]
	Learning Rate: 0.00769876
	LOSS [training: 0.35528737072551164 | validation: 0.17331505204131498]
	TIME [epoch: 8.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_104.pth
	Model improved!!!
EPOCH 105/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3228800758448503		[learning rate: 0.0076801]
		[batch 20/20] avg loss: 0.3475703565612548		[learning rate: 0.0076615]
	Learning Rate: 0.00766153
	LOSS [training: 0.33522521620305257 | validation: 0.7043203527523818]
	TIME [epoch: 8.78 sec]
EPOCH 106/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3559892823915171		[learning rate: 0.007643]
		[batch 20/20] avg loss: 0.29626551859671074		[learning rate: 0.0076245]
	Learning Rate: 0.00762449
	LOSS [training: 0.32612740049411393 | validation: 0.8082185295404086]
	TIME [epoch: 8.78 sec]
EPOCH 107/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.33815888184105175		[learning rate: 0.007606]
		[batch 20/20] avg loss: 0.3257875834550471		[learning rate: 0.0075876]
	Learning Rate: 0.00758761
	LOSS [training: 0.3319732326480494 | validation: 0.3461234505031411]
	TIME [epoch: 8.81 sec]
EPOCH 108/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.32941491270576506		[learning rate: 0.0075692]
		[batch 20/20] avg loss: 0.34704500431190693		[learning rate: 0.0075509]
	Learning Rate: 0.00755092
	LOSS [training: 0.338229958508836 | validation: 0.4400291905535652]
	TIME [epoch: 8.79 sec]
EPOCH 109/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3679447392230067		[learning rate: 0.0075326]
		[batch 20/20] avg loss: 0.29898069037802427		[learning rate: 0.0075144]
	Learning Rate: 0.00751441
	LOSS [training: 0.3334627148005155 | validation: 0.15838429969622048]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_109.pth
	Model improved!!!
EPOCH 110/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.34550177463210996		[learning rate: 0.0074962]
		[batch 20/20] avg loss: 0.2555606390782292		[learning rate: 0.0074781]
	Learning Rate: 0.00747807
	LOSS [training: 0.3005312068551696 | validation: 0.2818209329342549]
	TIME [epoch: 8.8 sec]
EPOCH 111/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28371036326640087		[learning rate: 0.00746]
		[batch 20/20] avg loss: 0.2832271553218663		[learning rate: 0.0074419]
	Learning Rate: 0.00744191
	LOSS [training: 0.2834687592941336 | validation: 0.18678530709452235]
	TIME [epoch: 8.78 sec]
EPOCH 112/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3352103436802042		[learning rate: 0.0074239]
		[batch 20/20] avg loss: 0.2566031504673779		[learning rate: 0.0074059]
	Learning Rate: 0.00740592
	LOSS [training: 0.2959067470737911 | validation: 0.21415967568361277]
	TIME [epoch: 8.82 sec]
EPOCH 113/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2674526510745069		[learning rate: 0.007388]
		[batch 20/20] avg loss: 0.4225936312821192		[learning rate: 0.0073701]
	Learning Rate: 0.00737011
	LOSS [training: 0.3450231411783129 | validation: 0.2716258038681382]
	TIME [epoch: 8.79 sec]
EPOCH 114/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3128601618741185		[learning rate: 0.0073523]
		[batch 20/20] avg loss: 0.32267812308524196		[learning rate: 0.0073345]
	Learning Rate: 0.00733446
	LOSS [training: 0.3177691424796802 | validation: 0.5140733776033544]
	TIME [epoch: 8.78 sec]
EPOCH 115/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3017343771001684		[learning rate: 0.0073167]
		[batch 20/20] avg loss: 0.2735955562463555		[learning rate: 0.007299]
	Learning Rate: 0.007299
	LOSS [training: 0.2876649666732619 | validation: 0.20749790885209698]
	TIME [epoch: 8.78 sec]
EPOCH 116/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.32556785140958394		[learning rate: 0.0072813]
		[batch 20/20] avg loss: 0.3083821141709655		[learning rate: 0.0072637]
	Learning Rate: 0.0072637
	LOSS [training: 0.31697498279027475 | validation: 0.5100702344889031]
	TIME [epoch: 8.77 sec]
EPOCH 117/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.40502991987925796		[learning rate: 0.0072461]
		[batch 20/20] avg loss: 0.30546180203153367		[learning rate: 0.0072286]
	Learning Rate: 0.00722857
	LOSS [training: 0.3552458609553957 | validation: 0.1684402545186773]
	TIME [epoch: 8.81 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2556767216614249		[learning rate: 0.0072111]
		[batch 20/20] avg loss: 0.28204737981311545		[learning rate: 0.0071936]
	Learning Rate: 0.00719362
	LOSS [training: 0.2688620507372702 | validation: 0.15762721948692102]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_118.pth
	Model improved!!!
EPOCH 119/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3476699925173569		[learning rate: 0.0071762]
		[batch 20/20] avg loss: 0.25257594760693414		[learning rate: 0.0071588]
	Learning Rate: 0.00715883
	LOSS [training: 0.3001229700621455 | validation: 0.3415151905222297]
	TIME [epoch: 8.78 sec]
EPOCH 120/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2798813003469515		[learning rate: 0.0071415]
		[batch 20/20] avg loss: 0.26853459256182394		[learning rate: 0.0071242]
	Learning Rate: 0.00712421
	LOSS [training: 0.2742079464543877 | validation: 0.3239958803140435]
	TIME [epoch: 8.78 sec]
EPOCH 121/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23691919260840674		[learning rate: 0.007107]
		[batch 20/20] avg loss: 0.2547106080453679		[learning rate: 0.0070898]
	Learning Rate: 0.00708976
	LOSS [training: 0.24581490032688733 | validation: 0.4933746790372421]
	TIME [epoch: 8.77 sec]
EPOCH 122/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3406220183429153		[learning rate: 0.0070726]
		[batch 20/20] avg loss: 0.24881455134695915		[learning rate: 0.0070555]
	Learning Rate: 0.00705548
	LOSS [training: 0.29471828484493723 | validation: 0.4043101732828505]
	TIME [epoch: 8.8 sec]
EPOCH 123/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3228324855504783		[learning rate: 0.0070384]
		[batch 20/20] avg loss: 0.322145816556867		[learning rate: 0.0070214]
	Learning Rate: 0.00702136
	LOSS [training: 0.3224891510536726 | validation: 0.30254993922559154]
	TIME [epoch: 8.79 sec]
EPOCH 124/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2684843160139617		[learning rate: 0.0070044]
		[batch 20/20] avg loss: 0.2923974601392126		[learning rate: 0.0069874]
	Learning Rate: 0.0069874
	LOSS [training: 0.2804408880765873 | validation: 0.19885970792890534]
	TIME [epoch: 8.79 sec]
EPOCH 125/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24959898289939786		[learning rate: 0.0069705]
		[batch 20/20] avg loss: 0.25535363878527506		[learning rate: 0.0069536]
	Learning Rate: 0.00695361
	LOSS [training: 0.2524763108423365 | validation: 0.2081959492632187]
	TIME [epoch: 8.79 sec]
EPOCH 126/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2543103754369223		[learning rate: 0.0069368]
		[batch 20/20] avg loss: 0.3361202815048307		[learning rate: 0.00692]
	Learning Rate: 0.00691999
	LOSS [training: 0.2952153284708765 | validation: 0.2968947000209759]
	TIME [epoch: 8.78 sec]
EPOCH 127/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2653409077509526		[learning rate: 0.0069032]
		[batch 20/20] avg loss: 0.3827893117250579		[learning rate: 0.0068865]
	Learning Rate: 0.00688652
	LOSS [training: 0.3240651097380053 | validation: 0.30052529018659935]
	TIME [epoch: 8.79 sec]
EPOCH 128/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2559575875106967		[learning rate: 0.0068699]
		[batch 20/20] avg loss: 0.3168781957686751		[learning rate: 0.0068532]
	Learning Rate: 0.00685322
	LOSS [training: 0.2864178916396859 | validation: 0.19654717768952013]
	TIME [epoch: 8.79 sec]
EPOCH 129/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21701351688361087		[learning rate: 0.0068366]
		[batch 20/20] avg loss: 0.26654797825228527		[learning rate: 0.0068201]
	Learning Rate: 0.00682008
	LOSS [training: 0.24178074756794815 | validation: 0.20333004906240384]
	TIME [epoch: 8.77 sec]
EPOCH 130/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22887678858484453		[learning rate: 0.0068036]
		[batch 20/20] avg loss: 0.25141291762016077		[learning rate: 0.0067871]
	Learning Rate: 0.0067871
	LOSS [training: 0.2401448531025026 | validation: 0.2505482158834468]
	TIME [epoch: 8.78 sec]
EPOCH 131/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2769757389662217		[learning rate: 0.0067707]
		[batch 20/20] avg loss: 0.29585152281203303		[learning rate: 0.0067543]
	Learning Rate: 0.00675428
	LOSS [training: 0.2864136308891273 | validation: 0.11211559321180029]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_131.pth
	Model improved!!!
EPOCH 132/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28720332991243447		[learning rate: 0.0067379]
		[batch 20/20] avg loss: 0.2543120532077128		[learning rate: 0.0067216]
	Learning Rate: 0.00672162
	LOSS [training: 0.2707576915600737 | validation: 0.15283746424903694]
	TIME [epoch: 8.79 sec]
EPOCH 133/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2569234668801625		[learning rate: 0.0067053]
		[batch 20/20] avg loss: 0.21938169116984643		[learning rate: 0.0066891]
	Learning Rate: 0.00668911
	LOSS [training: 0.23815257902500445 | validation: 0.13685005811469034]
	TIME [epoch: 8.79 sec]
EPOCH 134/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30631071990606074		[learning rate: 0.0066729]
		[batch 20/20] avg loss: 0.40445278518245054		[learning rate: 0.0066568]
	Learning Rate: 0.00665676
	LOSS [training: 0.35538175254425564 | validation: 0.3691162748196593]
	TIME [epoch: 8.77 sec]
EPOCH 135/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2113840772957934		[learning rate: 0.0066406]
		[batch 20/20] avg loss: 0.308970185347489		[learning rate: 0.0066246]
	Learning Rate: 0.00662457
	LOSS [training: 0.26017713132164116 | validation: 0.42295330662495284]
	TIME [epoch: 8.77 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21258634098935553		[learning rate: 0.0066085]
		[batch 20/20] avg loss: 0.2038698483968399		[learning rate: 0.0065925]
	Learning Rate: 0.00659254
	LOSS [training: 0.20822809469309772 | validation: 0.3608597340409683]
	TIME [epoch: 8.77 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3166131811269502		[learning rate: 0.0065766]
		[batch 20/20] avg loss: 0.24360985755448325		[learning rate: 0.0065607]
	Learning Rate: 0.00656066
	LOSS [training: 0.2801115193407167 | validation: 0.11484467142146794]
	TIME [epoch: 8.78 sec]
EPOCH 138/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19007388769521766		[learning rate: 0.0065448]
		[batch 20/20] avg loss: 0.20845107151474934		[learning rate: 0.0065289]
	Learning Rate: 0.00652893
	LOSS [training: 0.19926247960498353 | validation: 0.08903488645770705]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_138.pth
	Model improved!!!
EPOCH 139/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21091879243320405		[learning rate: 0.0065131]
		[batch 20/20] avg loss: 0.2674940039134128		[learning rate: 0.0064974]
	Learning Rate: 0.00649736
	LOSS [training: 0.23920639817330844 | validation: 0.16749096853849804]
	TIME [epoch: 8.79 sec]
EPOCH 140/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1939885979119635		[learning rate: 0.0064816]
		[batch 20/20] avg loss: 0.38704606819901555		[learning rate: 0.0064659]
	Learning Rate: 0.00646594
	LOSS [training: 0.2905173330554895 | validation: 0.35357150503417045]
	TIME [epoch: 8.77 sec]
EPOCH 141/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2364591408714498		[learning rate: 0.0064503]
		[batch 20/20] avg loss: 0.2756658510437232		[learning rate: 0.0064347]
	Learning Rate: 0.00643467
	LOSS [training: 0.2560624959575865 | validation: 0.16470456065244948]
	TIME [epoch: 8.78 sec]
EPOCH 142/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18917194966813672		[learning rate: 0.0064191]
		[batch 20/20] avg loss: 0.20065659560810278		[learning rate: 0.0064036]
	Learning Rate: 0.00640355
	LOSS [training: 0.19491427263811972 | validation: 0.09316204816776942]
	TIME [epoch: 8.77 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22048451775998618		[learning rate: 0.0063881]
		[batch 20/20] avg loss: 0.320972049571924		[learning rate: 0.0063726]
	Learning Rate: 0.00637259
	LOSS [training: 0.270728283665955 | validation: 0.38534293006796694]
	TIME [epoch: 8.8 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24844750715918673		[learning rate: 0.0063572]
		[batch 20/20] avg loss: 0.31966384745671006		[learning rate: 0.0063418]
	Learning Rate: 0.00634177
	LOSS [training: 0.28405567730794845 | validation: 0.12258994380356097]
	TIME [epoch: 8.78 sec]
EPOCH 145/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22022858352176203		[learning rate: 0.0063264]
		[batch 20/20] avg loss: 0.22674398994954398		[learning rate: 0.0063111]
	Learning Rate: 0.0063111
	LOSS [training: 0.22348628673565302 | validation: 0.253878850778746]
	TIME [epoch: 8.77 sec]
EPOCH 146/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3171580779330106		[learning rate: 0.0062958]
		[batch 20/20] avg loss: 0.2779841171182563		[learning rate: 0.0062806]
	Learning Rate: 0.00628058
	LOSS [training: 0.2975710975256335 | validation: 0.07952459746245638]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_146.pth
	Model improved!!!
EPOCH 147/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24629875188063774		[learning rate: 0.0062654]
		[batch 20/20] avg loss: 0.22495463745202976		[learning rate: 0.0062502]
	Learning Rate: 0.00625021
	LOSS [training: 0.23562669466633382 | validation: 0.18260000790818523]
	TIME [epoch: 8.78 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1921311616158414		[learning rate: 0.0062351]
		[batch 20/20] avg loss: 0.23825547051063284		[learning rate: 0.00622]
	Learning Rate: 0.00621999
	LOSS [training: 0.2151933160632371 | validation: 0.4389169868112155]
	TIME [epoch: 8.81 sec]
EPOCH 149/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19906592412770022		[learning rate: 0.0062049]
		[batch 20/20] avg loss: 0.27064478253406554		[learning rate: 0.0061899]
	Learning Rate: 0.00618991
	LOSS [training: 0.2348553533308829 | validation: 0.16301475798953116]
	TIME [epoch: 8.79 sec]
EPOCH 150/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2991225947508564		[learning rate: 0.0061749]
		[batch 20/20] avg loss: 0.24108730841049247		[learning rate: 0.00616]
	Learning Rate: 0.00615997
	LOSS [training: 0.2701049515806745 | validation: 0.10027854694155483]
	TIME [epoch: 8.78 sec]
EPOCH 151/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17041434770199962		[learning rate: 0.0061451]
		[batch 20/20] avg loss: 0.18562783271947408		[learning rate: 0.0061302]
	Learning Rate: 0.00613019
	LOSS [training: 0.17802109021073684 | validation: 0.09728284474504026]
	TIME [epoch: 8.79 sec]
EPOCH 152/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2343963312060124		[learning rate: 0.0061153]
		[batch 20/20] avg loss: 0.22133494717281016		[learning rate: 0.0061005]
	Learning Rate: 0.00610054
	LOSS [training: 0.2278656391894113 | validation: 0.2601571374294218]
	TIME [epoch: 8.78 sec]
EPOCH 153/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2502697119702396		[learning rate: 0.0060858]
		[batch 20/20] avg loss: 0.23472063495630194		[learning rate: 0.006071]
	Learning Rate: 0.00607104
	LOSS [training: 0.24249517346327082 | validation: 0.4489470107871949]
	TIME [epoch: 8.81 sec]
EPOCH 154/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3044161181944649		[learning rate: 0.0060563]
		[batch 20/20] avg loss: 0.32916981957334546		[learning rate: 0.0060417]
	Learning Rate: 0.00604168
	LOSS [training: 0.3167929688839052 | validation: 0.1396662837281945]
	TIME [epoch: 8.79 sec]
EPOCH 155/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21533615153496305		[learning rate: 0.0060271]
		[batch 20/20] avg loss: 0.19954089942298084		[learning rate: 0.0060125]
	Learning Rate: 0.00601247
	LOSS [training: 0.20743852547897196 | validation: 0.12260155619708245]
	TIME [epoch: 8.78 sec]
EPOCH 156/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24688826929508262		[learning rate: 0.0059979]
		[batch 20/20] avg loss: 0.2695133941269546		[learning rate: 0.0059834]
	Learning Rate: 0.00598339
	LOSS [training: 0.25820083171101865 | validation: 0.1584620787115461]
	TIME [epoch: 8.78 sec]
EPOCH 157/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1605441468381439		[learning rate: 0.0059689]
		[batch 20/20] avg loss: 0.25901156653594276		[learning rate: 0.0059545]
	Learning Rate: 0.00595446
	LOSS [training: 0.20977785668704332 | validation: 0.4137308546756524]
	TIME [epoch: 8.78 sec]
EPOCH 158/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30860905460761373		[learning rate: 0.00594]
		[batch 20/20] avg loss: 0.23070966925120512		[learning rate: 0.0059257]
	Learning Rate: 0.00592566
	LOSS [training: 0.2696593619294095 | validation: 0.15140880471804666]
	TIME [epoch: 8.79 sec]
EPOCH 159/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2672168405174084		[learning rate: 0.0059113]
		[batch 20/20] avg loss: 0.22699362529689554		[learning rate: 0.005897]
	Learning Rate: 0.00589701
	LOSS [training: 0.247105232907152 | validation: 0.6951442064259046]
	TIME [epoch: 8.8 sec]
EPOCH 160/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25360457752471655		[learning rate: 0.0058827]
		[batch 20/20] avg loss: 0.22165274374840158		[learning rate: 0.0058685]
	Learning Rate: 0.00586849
	LOSS [training: 0.2376286606365591 | validation: 0.17985112033605982]
	TIME [epoch: 8.78 sec]
EPOCH 161/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20453394738546823		[learning rate: 0.0058543]
		[batch 20/20] avg loss: 0.2851080799933233		[learning rate: 0.0058401]
	Learning Rate: 0.00584011
	LOSS [training: 0.24482101368939574 | validation: 0.08046645066712978]
	TIME [epoch: 8.78 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3503502080177967		[learning rate: 0.005826]
		[batch 20/20] avg loss: 0.2749701063704141		[learning rate: 0.0058119]
	Learning Rate: 0.00581187
	LOSS [training: 0.3126601571941054 | validation: 0.11796830361109577]
	TIME [epoch: 8.78 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3583068365963143		[learning rate: 0.0057978]
		[batch 20/20] avg loss: 0.21735957484440496		[learning rate: 0.0057838]
	Learning Rate: 0.00578376
	LOSS [training: 0.2878332057203596 | validation: 0.08983535770388112]
	TIME [epoch: 8.78 sec]
EPOCH 164/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2446892854568259		[learning rate: 0.0057698]
		[batch 20/20] avg loss: 0.23106243458309855		[learning rate: 0.0057558]
	Learning Rate: 0.00575579
	LOSS [training: 0.23787586001996225 | validation: 0.10648849950183667]
	TIME [epoch: 8.8 sec]
EPOCH 165/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21374621388509177		[learning rate: 0.0057419]
		[batch 20/20] avg loss: 0.3039373918601831		[learning rate: 0.005728]
	Learning Rate: 0.00572796
	LOSS [training: 0.25884180287263747 | validation: 0.18354428829162628]
	TIME [epoch: 8.79 sec]
EPOCH 166/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1843340847461497		[learning rate: 0.0057141]
		[batch 20/20] avg loss: 0.20817382426739323		[learning rate: 0.0057003]
	Learning Rate: 0.00570026
	LOSS [training: 0.19625395450677147 | validation: 0.10235605206947476]
	TIME [epoch: 8.79 sec]
EPOCH 167/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2706484586288455		[learning rate: 0.0056865]
		[batch 20/20] avg loss: 0.21079558569265786		[learning rate: 0.0056727]
	Learning Rate: 0.0056727
	LOSS [training: 0.2407220221607517 | validation: 0.12040358909830745]
	TIME [epoch: 8.78 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21950606439719217		[learning rate: 0.005659]
		[batch 20/20] avg loss: 0.22739354048943303		[learning rate: 0.0056453]
	Learning Rate: 0.00564526
	LOSS [training: 0.22344980244331264 | validation: 0.36958122367516966]
	TIME [epoch: 8.78 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2729514541272574		[learning rate: 0.0056316]
		[batch 20/20] avg loss: 0.16272095075181153		[learning rate: 0.005618]
	Learning Rate: 0.00561796
	LOSS [training: 0.21783620243953447 | validation: 0.13729748464488933]
	TIME [epoch: 8.81 sec]
EPOCH 170/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17929788017129095		[learning rate: 0.0056044]
		[batch 20/20] avg loss: 0.30129153664875247		[learning rate: 0.0055908]
	Learning Rate: 0.0055908
	LOSS [training: 0.24029470841002168 | validation: 0.13494333861033955]
	TIME [epoch: 8.78 sec]
EPOCH 171/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24467239543392635		[learning rate: 0.0055773]
		[batch 20/20] avg loss: 0.21698837002607144		[learning rate: 0.0055638]
	Learning Rate: 0.00556376
	LOSS [training: 0.23083038272999884 | validation: 0.14058338097228743]
	TIME [epoch: 8.78 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20740106174590572		[learning rate: 0.0055503]
		[batch 20/20] avg loss: 0.20683585316939418		[learning rate: 0.0055369]
	Learning Rate: 0.00553685
	LOSS [training: 0.2071184574576499 | validation: 0.3184418034077127]
	TIME [epoch: 8.78 sec]
EPOCH 173/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19231019423571422		[learning rate: 0.0055235]
		[batch 20/20] avg loss: 0.1798521805209931		[learning rate: 0.0055101]
	Learning Rate: 0.00551008
	LOSS [training: 0.18608118737835372 | validation: 0.11413787645178218]
	TIME [epoch: 8.78 sec]
EPOCH 174/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24820161168033197		[learning rate: 0.0054967]
		[batch 20/20] avg loss: 0.14832854865483822		[learning rate: 0.0054834]
	Learning Rate: 0.00548343
	LOSS [training: 0.1982650801675851 | validation: 0.10108741510886673]
	TIME [epoch: 8.8 sec]
EPOCH 175/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19181075990937288		[learning rate: 0.0054702]
		[batch 20/20] avg loss: 0.32447235258740903		[learning rate: 0.0054569]
	Learning Rate: 0.00545692
	LOSS [training: 0.25814155624839097 | validation: 0.17293915952244368]
	TIME [epoch: 8.78 sec]
EPOCH 176/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16423117915626248		[learning rate: 0.0054437]
		[batch 20/20] avg loss: 0.27175495796527654		[learning rate: 0.0054305]
	Learning Rate: 0.00543053
	LOSS [training: 0.21799306856076947 | validation: 0.34917353276180496]
	TIME [epoch: 8.77 sec]
EPOCH 177/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19109232438595058		[learning rate: 0.0054174]
		[batch 20/20] avg loss: 0.21514060978714483		[learning rate: 0.0054043]
	Learning Rate: 0.00540427
	LOSS [training: 0.20311646708654774 | validation: 0.12241590218224283]
	TIME [epoch: 8.78 sec]
EPOCH 178/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20899654020577074		[learning rate: 0.0053912]
		[batch 20/20] avg loss: 0.2917214183954832		[learning rate: 0.0053781]
	Learning Rate: 0.00537813
	LOSS [training: 0.25035897930062695 | validation: 0.1408059913194425]
	TIME [epoch: 8.79 sec]
EPOCH 179/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19160783183620073		[learning rate: 0.0053651]
		[batch 20/20] avg loss: 0.214402879704806		[learning rate: 0.0053521]
	Learning Rate: 0.00535213
	LOSS [training: 0.20300535577050338 | validation: 0.1285059164545857]
	TIME [epoch: 8.8 sec]
EPOCH 180/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24977826788541413		[learning rate: 0.0053392]
		[batch 20/20] avg loss: 0.17261027728376485		[learning rate: 0.0053262]
	Learning Rate: 0.00532624
	LOSS [training: 0.2111942725845895 | validation: 0.10954280005792413]
	TIME [epoch: 8.79 sec]
EPOCH 181/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2198121637063354		[learning rate: 0.0053133]
		[batch 20/20] avg loss: 0.18792568111389568		[learning rate: 0.0053005]
	Learning Rate: 0.00530049
	LOSS [training: 0.20386892241011556 | validation: 0.23004300310365006]
	TIME [epoch: 8.78 sec]
EPOCH 182/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2691551184322741		[learning rate: 0.0052877]
		[batch 20/20] avg loss: 0.2370565151492099		[learning rate: 0.0052749]
	Learning Rate: 0.00527485
	LOSS [training: 0.25310581679074196 | validation: 0.21405734175536323]
	TIME [epoch: 8.78 sec]
EPOCH 183/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15369165153098957		[learning rate: 0.0052621]
		[batch 20/20] avg loss: 0.18792668301436968		[learning rate: 0.0052493]
	Learning Rate: 0.00524935
	LOSS [training: 0.17080916727267964 | validation: 0.14299648043428148]
	TIME [epoch: 8.78 sec]
EPOCH 184/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17176458234593575		[learning rate: 0.0052366]
		[batch 20/20] avg loss: 0.2043109132963381		[learning rate: 0.005224]
	Learning Rate: 0.00522396
	LOSS [training: 0.18803774782113689 | validation: 0.14503641849037474]
	TIME [epoch: 8.79 sec]
EPOCH 185/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1871896400002106		[learning rate: 0.0052113]
		[batch 20/20] avg loss: 0.14667050969094897		[learning rate: 0.0051987]
	Learning Rate: 0.0051987
	LOSS [training: 0.16693007484557976 | validation: 0.20378048262832588]
	TIME [epoch: 8.77 sec]
EPOCH 186/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16398399599791744		[learning rate: 0.0051861]
		[batch 20/20] avg loss: 0.19522546465959675		[learning rate: 0.0051736]
	Learning Rate: 0.00517356
	LOSS [training: 0.17960473032875707 | validation: 0.14232015772382622]
	TIME [epoch: 8.77 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2338381907738059		[learning rate: 0.005161]
		[batch 20/20] avg loss: 0.1789209719608858		[learning rate: 0.0051485]
	Learning Rate: 0.00514854
	LOSS [training: 0.20637958136734585 | validation: 0.1592858701142567]
	TIME [epoch: 8.78 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1251712773065477		[learning rate: 0.0051361]
		[batch 20/20] avg loss: 0.2083402684025108		[learning rate: 0.0051236]
	Learning Rate: 0.00512364
	LOSS [training: 0.1667557728545293 | validation: 0.16408474955565117]
	TIME [epoch: 8.77 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2606879033145567		[learning rate: 0.0051112]
		[batch 20/20] avg loss: 0.29228317369232876		[learning rate: 0.0050989]
	Learning Rate: 0.00509887
	LOSS [training: 0.2764855385034427 | validation: 0.25375051655072345]
	TIME [epoch: 8.78 sec]
EPOCH 190/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22873246814325993		[learning rate: 0.0050865]
		[batch 20/20] avg loss: 0.17266490686060382		[learning rate: 0.0050742]
	Learning Rate: 0.00507421
	LOSS [training: 0.20069868750193182 | validation: 0.09398359537399634]
	TIME [epoch: 8.79 sec]
EPOCH 191/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12299254069443796		[learning rate: 0.0050619]
		[batch 20/20] avg loss: 0.1868639964987545		[learning rate: 0.0050497]
	Learning Rate: 0.00504967
	LOSS [training: 0.15492826859659625 | validation: 0.07013260444757762]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_191.pth
	Model improved!!!
EPOCH 192/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14229364812807416		[learning rate: 0.0050374]
		[batch 20/20] avg loss: 0.19391109134716145		[learning rate: 0.0050253]
	Learning Rate: 0.00502525
	LOSS [training: 0.16810236973761777 | validation: 0.19513671841454674]
	TIME [epoch: 8.79 sec]
EPOCH 193/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15725968529209639		[learning rate: 0.0050131]
		[batch 20/20] avg loss: 0.21288160508870502		[learning rate: 0.005001]
	Learning Rate: 0.00500095
	LOSS [training: 0.1850706451904007 | validation: 0.06999688138351055]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_193.pth
	Model improved!!!
EPOCH 194/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15130008999586797		[learning rate: 0.0049888]
		[batch 20/20] avg loss: 0.15096140044247108		[learning rate: 0.0049768]
	Learning Rate: 0.00497677
	LOSS [training: 0.15113074521916955 | validation: 0.1464715120526225]
	TIME [epoch: 8.79 sec]
EPOCH 195/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2013550271764643		[learning rate: 0.0049647]
		[batch 20/20] avg loss: 0.19720427183837713		[learning rate: 0.0049527]
	Learning Rate: 0.0049527
	LOSS [training: 0.19927964950742072 | validation: 0.20218084587581758]
	TIME [epoch: 8.79 sec]
EPOCH 196/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14361655408201368		[learning rate: 0.0049407]
		[batch 20/20] avg loss: 0.1473610484791496		[learning rate: 0.0049288]
	Learning Rate: 0.00492875
	LOSS [training: 0.14548880128058164 | validation: 0.4279707877418265]
	TIME [epoch: 8.76 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18572655333786178		[learning rate: 0.0049168]
		[batch 20/20] avg loss: 0.3084958864702513		[learning rate: 0.0049049]
	Learning Rate: 0.00490492
	LOSS [training: 0.24711121990405652 | validation: 0.15959387573203349]
	TIME [epoch: 8.77 sec]
EPOCH 198/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23165139874081547		[learning rate: 0.004893]
		[batch 20/20] avg loss: 0.17647733869713844		[learning rate: 0.0048812]
	Learning Rate: 0.0048812
	LOSS [training: 0.20406436871897693 | validation: 0.24435664451227546]
	TIME [epoch: 8.77 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18684848074065546		[learning rate: 0.0048694]
		[batch 20/20] avg loss: 0.13836627820923578		[learning rate: 0.0048576]
	Learning Rate: 0.00485759
	LOSS [training: 0.1626073794749456 | validation: 0.07601115560919657]
	TIME [epoch: 8.77 sec]
EPOCH 200/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15638342156701796		[learning rate: 0.0048458]
		[batch 20/20] avg loss: 0.20389187151688395		[learning rate: 0.0048341]
	Learning Rate: 0.0048341
	LOSS [training: 0.18013764654195094 | validation: 0.4130815806079413]
	TIME [epoch: 8.8 sec]
EPOCH 201/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.32559720529119035		[learning rate: 0.0048224]
		[batch 20/20] avg loss: 0.1714797651821653		[learning rate: 0.0048107]
	Learning Rate: 0.00481072
	LOSS [training: 0.24853848523667782 | validation: 0.24492224474262983]
	TIME [epoch: 8.76 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2068971891532973		[learning rate: 0.0047991]
		[batch 20/20] avg loss: 0.14933921712270257		[learning rate: 0.0047875]
	Learning Rate: 0.00478746
	LOSS [training: 0.17811820313799992 | validation: 0.13666177180713912]
	TIME [epoch: 8.77 sec]
EPOCH 203/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1405578853516221		[learning rate: 0.0047759]
		[batch 20/20] avg loss: 0.2939255287717961		[learning rate: 0.0047643]
	Learning Rate: 0.00476431
	LOSS [training: 0.21724170706170906 | validation: 0.7292088778168765]
	TIME [epoch: 8.76 sec]
EPOCH 204/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2217295773612365		[learning rate: 0.0047528]
		[batch 20/20] avg loss: 0.20588871306497944		[learning rate: 0.0047413]
	Learning Rate: 0.00474127
	LOSS [training: 0.21380914521310795 | validation: 0.05481655314136865]
	TIME [epoch: 8.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_204.pth
	Model improved!!!
EPOCH 205/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18090637042378566		[learning rate: 0.0047298]
		[batch 20/20] avg loss: 0.15335080592067563		[learning rate: 0.0047183]
	Learning Rate: 0.00471834
	LOSS [training: 0.16712858817223059 | validation: 0.18239215061939157]
	TIME [epoch: 8.81 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1372213143134704		[learning rate: 0.0047069]
		[batch 20/20] avg loss: 0.16631679323786053		[learning rate: 0.0046955]
	Learning Rate: 0.00469553
	LOSS [training: 0.15176905377566544 | validation: 0.40478573079368124]
	TIME [epoch: 8.78 sec]
EPOCH 207/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19006835285128462		[learning rate: 0.0046842]
		[batch 20/20] avg loss: 0.20306025698420654		[learning rate: 0.0046728]
	Learning Rate: 0.00467282
	LOSS [training: 0.19656430491774562 | validation: 0.03956951362647173]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_207.pth
	Model improved!!!
EPOCH 208/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11376925203961405		[learning rate: 0.0046615]
		[batch 20/20] avg loss: 0.17894506569118432		[learning rate: 0.0046502]
	Learning Rate: 0.00465022
	LOSS [training: 0.14635715886539918 | validation: 0.32492984340073783]
	TIME [epoch: 8.78 sec]
EPOCH 209/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2180823755705558		[learning rate: 0.004639]
		[batch 20/20] avg loss: 0.15183231081447296		[learning rate: 0.0046277]
	Learning Rate: 0.00462773
	LOSS [training: 0.1849573431925144 | validation: 0.670750409462223]
	TIME [epoch: 8.78 sec]
EPOCH 210/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2420918453514552		[learning rate: 0.0046165]
		[batch 20/20] avg loss: 0.1585529029708043		[learning rate: 0.0046054]
	Learning Rate: 0.00460536
	LOSS [training: 0.20032237416112975 | validation: 0.17045480902716853]
	TIME [epoch: 8.8 sec]
EPOCH 211/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1906105758440695		[learning rate: 0.0045942]
		[batch 20/20] avg loss: 0.15274834073633725		[learning rate: 0.0045831]
	Learning Rate: 0.00458308
	LOSS [training: 0.17167945829020337 | validation: 0.14700891212544898]
	TIME [epoch: 8.77 sec]
EPOCH 212/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20346878896147041		[learning rate: 0.004572]
		[batch 20/20] avg loss: 0.19276043302350065		[learning rate: 0.0045609]
	Learning Rate: 0.00456092
	LOSS [training: 0.19811461099248556 | validation: 0.06541047282080237]
	TIME [epoch: 8.77 sec]
EPOCH 213/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12317992787248311		[learning rate: 0.0045499]
		[batch 20/20] avg loss: 0.1718824310557875		[learning rate: 0.0045389]
	Learning Rate: 0.00453887
	LOSS [training: 0.14753117946413533 | validation: 0.25136223763550414]
	TIME [epoch: 8.77 sec]
EPOCH 214/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14715852321100448		[learning rate: 0.0045279]
		[batch 20/20] avg loss: 0.22214045185757286		[learning rate: 0.0045169]
	Learning Rate: 0.00451692
	LOSS [training: 0.1846494875342887 | validation: 0.23045213131340803]
	TIME [epoch: 8.78 sec]
EPOCH 215/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14455453947312064		[learning rate: 0.004506]
		[batch 20/20] avg loss: 0.118921664851715		[learning rate: 0.0044951]
	Learning Rate: 0.00449507
	LOSS [training: 0.13173810216241785 | validation: 0.0986257131895385]
	TIME [epoch: 8.79 sec]
EPOCH 216/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1825092069162969		[learning rate: 0.0044842]
		[batch 20/20] avg loss: 0.21866797047136668		[learning rate: 0.0044733]
	Learning Rate: 0.00447334
	LOSS [training: 0.20058858869383175 | validation: 0.11484889724517156]
	TIME [epoch: 8.78 sec]
EPOCH 217/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11157304346140011		[learning rate: 0.0044625]
		[batch 20/20] avg loss: 0.15679719709530734		[learning rate: 0.0044517]
	Learning Rate: 0.0044517
	LOSS [training: 0.13418512027835372 | validation: 0.2179419902741811]
	TIME [epoch: 8.77 sec]
EPOCH 218/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1640425425228694		[learning rate: 0.0044409]
		[batch 20/20] avg loss: 0.12682638240246732		[learning rate: 0.0044302]
	Learning Rate: 0.00443018
	LOSS [training: 0.1454344624626684 | validation: 0.20250632352485295]
	TIME [epoch: 8.78 sec]
EPOCH 219/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14279956406028987		[learning rate: 0.0044195]
		[batch 20/20] avg loss: 0.17583028353100932		[learning rate: 0.0044088]
	Learning Rate: 0.00440875
	LOSS [training: 0.15931492379564957 | validation: 0.06245111274029755]
	TIME [epoch: 8.78 sec]
EPOCH 220/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14631071265595227		[learning rate: 0.0043981]
		[batch 20/20] avg loss: 0.10994026707512639		[learning rate: 0.0043874]
	Learning Rate: 0.00438743
	LOSS [training: 0.12812548986553934 | validation: 0.06417593953559114]
	TIME [epoch: 8.79 sec]
EPOCH 221/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0986631009161755		[learning rate: 0.0043768]
		[batch 20/20] avg loss: 0.13376696102437208		[learning rate: 0.0043662]
	Learning Rate: 0.00436622
	LOSS [training: 0.11621503097027377 | validation: 0.3371395110908615]
	TIME [epoch: 8.79 sec]
EPOCH 222/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19075572794899148		[learning rate: 0.0043556]
		[batch 20/20] avg loss: 0.1825688891633382		[learning rate: 0.0043451]
	Learning Rate: 0.0043451
	LOSS [training: 0.18666230855616484 | validation: 0.14885409297301916]
	TIME [epoch: 8.78 sec]
EPOCH 223/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1878972344790601		[learning rate: 0.0043346]
		[batch 20/20] avg loss: 0.19547802865845876		[learning rate: 0.0043241]
	Learning Rate: 0.00432409
	LOSS [training: 0.19168763156875943 | validation: 0.22948526418881093]
	TIME [epoch: 8.77 sec]
EPOCH 224/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1780372645979989		[learning rate: 0.0043136]
		[batch 20/20] avg loss: 0.15978320576618696		[learning rate: 0.0043032]
	Learning Rate: 0.00430318
	LOSS [training: 0.16891023518209294 | validation: 0.257904828238828]
	TIME [epoch: 8.78 sec]
EPOCH 225/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11980034901860434		[learning rate: 0.0042928]
		[batch 20/20] avg loss: 0.2026809813002694		[learning rate: 0.0042824]
	Learning Rate: 0.00428237
	LOSS [training: 0.1612406651594369 | validation: 0.07135644902572576]
	TIME [epoch: 8.78 sec]
EPOCH 226/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14503739721178383		[learning rate: 0.004272]
		[batch 20/20] avg loss: 0.12610225400018693		[learning rate: 0.0042617]
	Learning Rate: 0.00426166
	LOSS [training: 0.13556982560598535 | validation: 0.2517489725580537]
	TIME [epoch: 8.8 sec]
EPOCH 227/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17538484199644308		[learning rate: 0.0042513]
		[batch 20/20] avg loss: 0.16743061213877702		[learning rate: 0.0042411]
	Learning Rate: 0.00424105
	LOSS [training: 0.17140772706761004 | validation: 0.14647527435029029]
	TIME [epoch: 8.77 sec]
EPOCH 228/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16360212443576344		[learning rate: 0.0042308]
		[batch 20/20] avg loss: 0.31782028314462374		[learning rate: 0.0042205]
	Learning Rate: 0.00422054
	LOSS [training: 0.24071120379019356 | validation: 0.5497979060428864]
	TIME [epoch: 8.78 sec]
EPOCH 229/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1941306265791391		[learning rate: 0.0042103]
		[batch 20/20] avg loss: 0.16204665476859795		[learning rate: 0.0042001]
	Learning Rate: 0.00420013
	LOSS [training: 0.17808864067386856 | validation: 0.17188062188341702]
	TIME [epoch: 8.77 sec]
EPOCH 230/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14214771946359608		[learning rate: 0.00419]
		[batch 20/20] avg loss: 0.1400100131802059		[learning rate: 0.0041798]
	Learning Rate: 0.00417982
	LOSS [training: 0.14107886632190098 | validation: 0.7464131137085349]
	TIME [epoch: 8.78 sec]
EPOCH 231/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23161502201688505		[learning rate: 0.0041697]
		[batch 20/20] avg loss: 0.15331824234673874		[learning rate: 0.0041596]
	Learning Rate: 0.00415961
	LOSS [training: 0.1924666321818119 | validation: 0.1233364292225777]
	TIME [epoch: 8.79 sec]
EPOCH 232/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10597300624925195		[learning rate: 0.0041495]
		[batch 20/20] avg loss: 0.1781449108242518		[learning rate: 0.0041395]
	Learning Rate: 0.0041395
	LOSS [training: 0.14205895853675185 | validation: 0.23563118752733964]
	TIME [epoch: 8.78 sec]
EPOCH 233/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13942161045538967		[learning rate: 0.0041295]
		[batch 20/20] avg loss: 0.2492850108125034		[learning rate: 0.0041195]
	Learning Rate: 0.00411948
	LOSS [training: 0.1943533106339465 | validation: 0.133966183106264]
	TIME [epoch: 8.79 sec]
EPOCH 234/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14392323888170563		[learning rate: 0.0041095]
		[batch 20/20] avg loss: 0.12680863998602115		[learning rate: 0.0040996]
	Learning Rate: 0.00409956
	LOSS [training: 0.13536593943386338 | validation: 0.05349953562017538]
	TIME [epoch: 8.78 sec]
EPOCH 235/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14589042006142122		[learning rate: 0.0040896]
		[batch 20/20] avg loss: 0.17029444958508838		[learning rate: 0.0040797]
	Learning Rate: 0.00407973
	LOSS [training: 0.1580924348232548 | validation: 0.14125054191051245]
	TIME [epoch: 8.77 sec]
EPOCH 236/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1592397722975316		[learning rate: 0.0040699]
		[batch 20/20] avg loss: 0.12499904647141466		[learning rate: 0.00406]
	Learning Rate: 0.00406
	LOSS [training: 0.1421194093844731 | validation: 0.2103972992723299]
	TIME [epoch: 8.8 sec]
EPOCH 237/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21038007499126507		[learning rate: 0.0040502]
		[batch 20/20] avg loss: 0.1079703910972559		[learning rate: 0.0040404]
	Learning Rate: 0.00404037
	LOSS [training: 0.15917523304426046 | validation: 0.1704456117989709]
	TIME [epoch: 8.77 sec]
EPOCH 238/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11962432547304624		[learning rate: 0.0040306]
		[batch 20/20] avg loss: 0.18354685997452488		[learning rate: 0.0040208]
	Learning Rate: 0.00402083
	LOSS [training: 0.15158559272378558 | validation: 0.13178437804909238]
	TIME [epoch: 8.78 sec]
EPOCH 239/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15203394371813678		[learning rate: 0.0040111]
		[batch 20/20] avg loss: 0.13326475417521455		[learning rate: 0.0040014]
	Learning Rate: 0.00400139
	LOSS [training: 0.14264934894667564 | validation: 0.019566680821430725]
	TIME [epoch: 8.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_239.pth
	Model improved!!!
EPOCH 240/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15405423064572746		[learning rate: 0.0039917]
		[batch 20/20] avg loss: 0.16139946288849322		[learning rate: 0.003982]
	Learning Rate: 0.00398204
	LOSS [training: 0.1577268467671103 | validation: 0.13924434631942056]
	TIME [epoch: 8.77 sec]
EPOCH 241/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.146648548831616		[learning rate: 0.0039724]
		[batch 20/20] avg loss: 0.10367385763720646		[learning rate: 0.0039628]
	Learning Rate: 0.00396278
	LOSS [training: 0.12516120323441124 | validation: 0.09304611636392922]
	TIME [epoch: 8.79 sec]
EPOCH 242/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10215645673439493		[learning rate: 0.0039532]
		[batch 20/20] avg loss: 0.0999843748886044		[learning rate: 0.0039436]
	Learning Rate: 0.00394362
	LOSS [training: 0.10107041581149967 | validation: 0.056869130486530056]
	TIME [epoch: 8.77 sec]
EPOCH 243/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13188440694673514		[learning rate: 0.0039341]
		[batch 20/20] avg loss: 0.1782831035510457		[learning rate: 0.0039245]
	Learning Rate: 0.00392455
	LOSS [training: 0.1550837552488904 | validation: 0.1706261808555219]
	TIME [epoch: 8.77 sec]
EPOCH 244/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13953505632900506		[learning rate: 0.003915]
		[batch 20/20] avg loss: 0.14034699226900207		[learning rate: 0.0039056]
	Learning Rate: 0.00390557
	LOSS [training: 0.13994102429900357 | validation: 0.07334933069343591]
	TIME [epoch: 8.76 sec]
EPOCH 245/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14193385201113898		[learning rate: 0.0038961]
		[batch 20/20] avg loss: 0.10917754768166092		[learning rate: 0.0038867]
	Learning Rate: 0.00388668
	LOSS [training: 0.12555569984639997 | validation: 0.14588558135976976]
	TIME [epoch: 8.77 sec]
EPOCH 246/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13325130546006136		[learning rate: 0.0038773]
		[batch 20/20] avg loss: 0.1330097541174773		[learning rate: 0.0038679]
	Learning Rate: 0.00386789
	LOSS [training: 0.13313052978876933 | validation: 0.06714301062089692]
	TIME [epoch: 8.8 sec]
EPOCH 247/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1215415057152431		[learning rate: 0.0038585]
		[batch 20/20] avg loss: 0.09083727159732202		[learning rate: 0.0038492]
	Learning Rate: 0.00384918
	LOSS [training: 0.10618938865628258 | validation: 0.19516240930541717]
	TIME [epoch: 8.77 sec]
EPOCH 248/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16681334436701417		[learning rate: 0.0038399]
		[batch 20/20] avg loss: 0.15725541418779515		[learning rate: 0.0038306]
	Learning Rate: 0.00383057
	LOSS [training: 0.16203437927740463 | validation: 0.10570670218876214]
	TIME [epoch: 8.78 sec]
EPOCH 249/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12235305327453601		[learning rate: 0.0038213]
		[batch 20/20] avg loss: 0.1783586019413958		[learning rate: 0.003812]
	Learning Rate: 0.00381204
	LOSS [training: 0.1503558276079659 | validation: 0.11160790814134208]
	TIME [epoch: 8.76 sec]
EPOCH 250/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.271375113560871		[learning rate: 0.0038028]
		[batch 20/20] avg loss: 0.20705084414728106		[learning rate: 0.0037936]
	Learning Rate: 0.00379361
	LOSS [training: 0.239212978854076 | validation: 0.06867637022244176]
	TIME [epoch: 8.77 sec]
EPOCH 251/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08868977568195009		[learning rate: 0.0037844]
		[batch 20/20] avg loss: 0.1790688255789823		[learning rate: 0.0037753]
	Learning Rate: 0.00377526
	LOSS [training: 0.13387930063046619 | validation: 0.14866687916227417]
	TIME [epoch: 8.79 sec]
EPOCH 252/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12341060999983197		[learning rate: 0.0037661]
		[batch 20/20] avg loss: 0.12254371050100818		[learning rate: 0.003757]
	Learning Rate: 0.00375701
	LOSS [training: 0.12297716025042009 | validation: 0.4207928132510331]
	TIME [epoch: 8.77 sec]
EPOCH 253/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1859196683051892		[learning rate: 0.0037479]
		[batch 20/20] avg loss: 0.14848216752245863		[learning rate: 0.0037388]
	Learning Rate: 0.00373884
	LOSS [training: 0.16720091791382385 | validation: 0.1289924031929747]
	TIME [epoch: 8.77 sec]
EPOCH 254/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16958193003157834		[learning rate: 0.0037298]
		[batch 20/20] avg loss: 0.11001253644459234		[learning rate: 0.0037208]
	Learning Rate: 0.00372076
	LOSS [training: 0.13979723323808532 | validation: 0.13318632353927662]
	TIME [epoch: 8.77 sec]
EPOCH 255/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11729933166145785		[learning rate: 0.0037118]
		[batch 20/20] avg loss: 0.19735093584847668		[learning rate: 0.0037028]
	Learning Rate: 0.00370277
	LOSS [training: 0.1573251337549673 | validation: 0.05106327892037125]
	TIME [epoch: 8.77 sec]
EPOCH 256/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12845418336843148		[learning rate: 0.0036938]
		[batch 20/20] avg loss: 0.147682947240007		[learning rate: 0.0036849]
	Learning Rate: 0.00368486
	LOSS [training: 0.13806856530421924 | validation: 0.19106229368434158]
	TIME [epoch: 8.77 sec]
EPOCH 257/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13981030314135262		[learning rate: 0.0036759]
		[batch 20/20] avg loss: 0.1093013679902354		[learning rate: 0.003667]
	Learning Rate: 0.00366704
	LOSS [training: 0.12455583556579397 | validation: 0.0721656742707073]
	TIME [epoch: 8.78 sec]
EPOCH 258/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09212488955673097		[learning rate: 0.0036582]
		[batch 20/20] avg loss: 0.12781050773399083		[learning rate: 0.0036493]
	Learning Rate: 0.00364931
	LOSS [training: 0.1099676986453609 | validation: 0.0931516364447681]
	TIME [epoch: 8.77 sec]
EPOCH 259/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0919522916274199		[learning rate: 0.0036405]
		[batch 20/20] avg loss: 0.14772016839464036		[learning rate: 0.0036317]
	Learning Rate: 0.00363166
	LOSS [training: 0.11983623001103012 | validation: 0.12810981884386174]
	TIME [epoch: 8.77 sec]
EPOCH 260/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11407024037222686		[learning rate: 0.0036229]
		[batch 20/20] avg loss: 0.12897785540552284		[learning rate: 0.0036141]
	Learning Rate: 0.0036141
	LOSS [training: 0.12152404788887487 | validation: 0.15609465087163493]
	TIME [epoch: 8.79 sec]
EPOCH 261/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09721510538751069		[learning rate: 0.0036053]
		[batch 20/20] avg loss: 0.11543049961413229		[learning rate: 0.0035966]
	Learning Rate: 0.00359662
	LOSS [training: 0.10632280250082152 | validation: 0.08890297531264442]
	TIME [epoch: 8.79 sec]
EPOCH 262/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08428910194556778		[learning rate: 0.0035879]
		[batch 20/20] avg loss: 0.22478847900983875		[learning rate: 0.0035792]
	Learning Rate: 0.00357923
	LOSS [training: 0.15453879047770322 | validation: 0.21623836442894007]
	TIME [epoch: 8.8 sec]
EPOCH 263/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15929752337468778		[learning rate: 0.0035706]
		[batch 20/20] avg loss: 0.08789808577666684		[learning rate: 0.0035619]
	Learning Rate: 0.00356192
	LOSS [training: 0.12359780457567733 | validation: 0.021816838558025034]
	TIME [epoch: 8.77 sec]
EPOCH 264/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09011309799132831		[learning rate: 0.0035533]
		[batch 20/20] avg loss: 0.09982030050789178		[learning rate: 0.0035447]
	Learning Rate: 0.0035447
	LOSS [training: 0.09496669924961006 | validation: 0.05890091282228766]
	TIME [epoch: 8.77 sec]
EPOCH 265/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17428690321271342		[learning rate: 0.0035361]
		[batch 20/20] avg loss: 0.08473338668695056		[learning rate: 0.0035276]
	Learning Rate: 0.00352755
	LOSS [training: 0.12951014494983198 | validation: 0.05928890888965479]
	TIME [epoch: 8.78 sec]
EPOCH 266/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1395954561039465		[learning rate: 0.003519]
		[batch 20/20] avg loss: 0.10335066984273725		[learning rate: 0.0035105]
	Learning Rate: 0.0035105
	LOSS [training: 0.12147306297334186 | validation: 0.0649400544304276]
	TIME [epoch: 8.78 sec]
EPOCH 267/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09956810402820002		[learning rate: 0.003502]
		[batch 20/20] avg loss: 0.10779190754639714		[learning rate: 0.0034935]
	Learning Rate: 0.00349352
	LOSS [training: 0.1036800057872986 | validation: 0.17067208251567692]
	TIME [epoch: 8.81 sec]
EPOCH 268/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10696571365412337		[learning rate: 0.0034851]
		[batch 20/20] avg loss: 0.120252090601167		[learning rate: 0.0034766]
	Learning Rate: 0.00347663
	LOSS [training: 0.11360890212764518 | validation: 0.2201308695058522]
	TIME [epoch: 8.77 sec]
EPOCH 269/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1106691118157412		[learning rate: 0.0034682]
		[batch 20/20] avg loss: 0.07901936055001071		[learning rate: 0.0034598]
	Learning Rate: 0.00345981
	LOSS [training: 0.09484423618287595 | validation: 0.31118338569840065]
	TIME [epoch: 8.78 sec]
EPOCH 270/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17515019199379583		[learning rate: 0.0034514]
		[batch 20/20] avg loss: 0.07277718367892973		[learning rate: 0.0034431]
	Learning Rate: 0.00344308
	LOSS [training: 0.12396368783636276 | validation: 0.18828817535208192]
	TIME [epoch: 8.77 sec]
EPOCH 271/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09874826640767549		[learning rate: 0.0034347]
		[batch 20/20] avg loss: 0.08173141753938362		[learning rate: 0.0034264]
	Learning Rate: 0.00342643
	LOSS [training: 0.09023984197352955 | validation: 0.05927380190131959]
	TIME [epoch: 8.77 sec]
EPOCH 272/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15622231144534665		[learning rate: 0.0034181]
		[batch 20/20] avg loss: 0.12443034307739276		[learning rate: 0.0034099]
	Learning Rate: 0.00340986
	LOSS [training: 0.14032632726136968 | validation: 0.3118332310698855]
	TIME [epoch: 8.81 sec]
EPOCH 273/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17379521534366005		[learning rate: 0.0034016]
		[batch 20/20] avg loss: 0.1486000202126016		[learning rate: 0.0033934]
	Learning Rate: 0.00339337
	LOSS [training: 0.16119761777813085 | validation: 0.07054183444719644]
	TIME [epoch: 8.78 sec]
EPOCH 274/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1470245479139824		[learning rate: 0.0033852]
		[batch 20/20] avg loss: 0.39049093944027		[learning rate: 0.003377]
	Learning Rate: 0.00337696
	LOSS [training: 0.26875774367712624 | validation: 0.10895870252575271]
	TIME [epoch: 8.79 sec]
EPOCH 275/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15007939741348722		[learning rate: 0.0033688]
		[batch 20/20] avg loss: 0.06450585687108293		[learning rate: 0.0033606]
	Learning Rate: 0.00336063
	LOSS [training: 0.10729262714228509 | validation: 0.036942009860596446]
	TIME [epoch: 8.78 sec]
EPOCH 276/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15082793811924672		[learning rate: 0.0033525]
		[batch 20/20] avg loss: 0.14934046882041985		[learning rate: 0.0033444]
	Learning Rate: 0.00334438
	LOSS [training: 0.15008420346983325 | validation: 0.1805670770107064]
	TIME [epoch: 8.78 sec]
EPOCH 277/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1884258265130549		[learning rate: 0.0033363]
		[batch 20/20] avg loss: 0.1655732665508171		[learning rate: 0.0033282]
	Learning Rate: 0.00332821
	LOSS [training: 0.176999546531936 | validation: 0.040268329620148854]
	TIME [epoch: 8.8 sec]
EPOCH 278/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09435136272271075		[learning rate: 0.0033202]
		[batch 20/20] avg loss: 0.09635926553659976		[learning rate: 0.0033121]
	Learning Rate: 0.00331211
	LOSS [training: 0.09535531412965524 | validation: 0.13005662543713353]
	TIME [epoch: 8.78 sec]
EPOCH 279/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0951100925987814		[learning rate: 0.0033041]
		[batch 20/20] avg loss: 0.08588557496817112		[learning rate: 0.0032961]
	Learning Rate: 0.0032961
	LOSS [training: 0.09049783378347627 | validation: 0.10080837916913771]
	TIME [epoch: 8.78 sec]
EPOCH 280/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09801562204652092		[learning rate: 0.0032881]
		[batch 20/20] avg loss: 0.10301736750056005		[learning rate: 0.0032802]
	Learning Rate: 0.00328016
	LOSS [training: 0.10051649477354048 | validation: 0.12272294751774104]
	TIME [epoch: 8.79 sec]
EPOCH 281/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11221801869644821		[learning rate: 0.0032722]
		[batch 20/20] avg loss: 0.09697041060112666		[learning rate: 0.0032643]
	Learning Rate: 0.0032643
	LOSS [training: 0.10459421464878745 | validation: 0.28719774331189163]
	TIME [epoch: 8.78 sec]
EPOCH 282/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20659414848339042		[learning rate: 0.0032564]
		[batch 20/20] avg loss: 0.07473604457377857		[learning rate: 0.0032485]
	Learning Rate: 0.00324851
	LOSS [training: 0.14066509652858447 | validation: 0.32975086529465636]
	TIME [epoch: 8.8 sec]
EPOCH 283/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10284464721778766		[learning rate: 0.0032406]
		[batch 20/20] avg loss: 0.10359355301505249		[learning rate: 0.0032328]
	Learning Rate: 0.0032328
	LOSS [training: 0.10321910011642006 | validation: 0.18742072589996142]
	TIME [epoch: 8.79 sec]
EPOCH 284/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08216624181200449		[learning rate: 0.003225]
		[batch 20/20] avg loss: 0.09614777828177563		[learning rate: 0.0032172]
	Learning Rate: 0.00321717
	LOSS [training: 0.08915701004689006 | validation: 0.1278628930805556]
	TIME [epoch: 8.78 sec]
EPOCH 285/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1238453834286615		[learning rate: 0.0032094]
		[batch 20/20] avg loss: 0.10691325661599423		[learning rate: 0.0032016]
	Learning Rate: 0.00320161
	LOSS [training: 0.11537932002232787 | validation: 0.11471962731458762]
	TIME [epoch: 8.78 sec]
EPOCH 286/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07461893309433668		[learning rate: 0.0031939]
		[batch 20/20] avg loss: 0.09201917737394272		[learning rate: 0.0031861]
	Learning Rate: 0.00318613
	LOSS [training: 0.0833190552341397 | validation: 0.07624596272820944]
	TIME [epoch: 8.79 sec]
EPOCH 287/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09102574196803058		[learning rate: 0.0031784]
		[batch 20/20] avg loss: 0.09562673180907283		[learning rate: 0.0031707]
	Learning Rate: 0.00317072
	LOSS [training: 0.09332623688855171 | validation: 0.1219647156006681]
	TIME [epoch: 8.8 sec]
EPOCH 288/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10230583929061539		[learning rate: 0.003163]
		[batch 20/20] avg loss: 0.12367456519285092		[learning rate: 0.0031554]
	Learning Rate: 0.00315539
	LOSS [training: 0.11299020224173315 | validation: 0.20645433107907307]
	TIME [epoch: 8.8 sec]
EPOCH 289/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11135684397176697		[learning rate: 0.0031477]
		[batch 20/20] avg loss: 0.07375399152738041		[learning rate: 0.0031401]
	Learning Rate: 0.00314013
	LOSS [training: 0.09255541774957368 | validation: 0.12302732301110744]
	TIME [epoch: 8.79 sec]
EPOCH 290/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.061771128911581395		[learning rate: 0.0031325]
		[batch 20/20] avg loss: 0.10546960786551574		[learning rate: 0.0031249]
	Learning Rate: 0.00312494
	LOSS [training: 0.08362036838854855 | validation: 0.11722582105461118]
	TIME [epoch: 8.78 sec]
EPOCH 291/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13828297641317816		[learning rate: 0.0031174]
		[batch 20/20] avg loss: 0.31504883248219284		[learning rate: 0.0031098]
	Learning Rate: 0.00310983
	LOSS [training: 0.22666590444768553 | validation: 0.07021593591320047]
	TIME [epoch: 8.78 sec]
EPOCH 292/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08101310403257533		[learning rate: 0.0031023]
		[batch 20/20] avg loss: 0.13896755998182958		[learning rate: 0.0030948]
	Learning Rate: 0.00309479
	LOSS [training: 0.10999033200720246 | validation: 0.013245038274543281]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_292.pth
	Model improved!!!
EPOCH 293/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11473494693314432		[learning rate: 0.0030873]
		[batch 20/20] avg loss: 0.18196934174584853		[learning rate: 0.0030798]
	Learning Rate: 0.00307983
	LOSS [training: 0.14835214433949645 | validation: 0.017971436039783507]
	TIME [epoch: 8.78 sec]
EPOCH 294/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10257618671854019		[learning rate: 0.0030724]
		[batch 20/20] avg loss: 0.08595191444659055		[learning rate: 0.0030649]
	Learning Rate: 0.00306493
	LOSS [training: 0.09426405058256536 | validation: 0.03589584415262709]
	TIME [epoch: 8.77 sec]
EPOCH 295/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0745754609188914		[learning rate: 0.0030575]
		[batch 20/20] avg loss: 0.16044042139064535		[learning rate: 0.0030501]
	Learning Rate: 0.00305011
	LOSS [training: 0.11750794115476837 | validation: 0.07877881019121573]
	TIME [epoch: 8.77 sec]
EPOCH 296/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08088442307395408		[learning rate: 0.0030427]
		[batch 20/20] avg loss: 0.10824998381389173		[learning rate: 0.0030354]
	Learning Rate: 0.00303536
	LOSS [training: 0.0945672034439229 | validation: 0.07250038389236681]
	TIME [epoch: 8.78 sec]
EPOCH 297/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14428940348595595		[learning rate: 0.003028]
		[batch 20/20] avg loss: 0.1206476111554193		[learning rate: 0.0030207]
	Learning Rate: 0.00302068
	LOSS [training: 0.13246850732068763 | validation: 0.010756865009293494]
	TIME [epoch: 8.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_297.pth
	Model improved!!!
EPOCH 298/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09784354203937633		[learning rate: 0.0030134]
		[batch 20/20] avg loss: 0.08905241964659938		[learning rate: 0.0030061]
	Learning Rate: 0.00300608
	LOSS [training: 0.09344798084298785 | validation: 0.15009504379021885]
	TIME [epoch: 8.79 sec]
EPOCH 299/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08684922453473887		[learning rate: 0.0029988]
		[batch 20/20] avg loss: 0.09308527818055426		[learning rate: 0.0029915]
	Learning Rate: 0.00299154
	LOSS [training: 0.08996725135764656 | validation: 0.23534003407840132]
	TIME [epoch: 8.77 sec]
EPOCH 300/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16858663825831266		[learning rate: 0.0029843]
		[batch 20/20] avg loss: 0.09805396787930894		[learning rate: 0.0029771]
	Learning Rate: 0.00297707
	LOSS [training: 0.13332030306881082 | validation: 0.15975765599223052]
	TIME [epoch: 8.78 sec]
EPOCH 301/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13337285540759788		[learning rate: 0.0029699]
		[batch 20/20] avg loss: 0.10949765324759927		[learning rate: 0.0029627]
	Learning Rate: 0.00296268
	LOSS [training: 0.12143525432759858 | validation: 0.036935116164814145]
	TIME [epoch: 8.78 sec]
EPOCH 302/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13752140597619564		[learning rate: 0.0029555]
		[batch 20/20] avg loss: 0.08816880215644243		[learning rate: 0.0029483]
	Learning Rate: 0.00294835
	LOSS [training: 0.11284510406631905 | validation: 0.06807363010359693]
	TIME [epoch: 8.77 sec]
EPOCH 303/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0924477253902952		[learning rate: 0.0029412]
		[batch 20/20] avg loss: 0.0815207352160919		[learning rate: 0.0029341]
	Learning Rate: 0.00293409
	LOSS [training: 0.08698423030319355 | validation: 0.08106026027565039]
	TIME [epoch: 8.8 sec]
EPOCH 304/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07804972417102907		[learning rate: 0.002927]
		[batch 20/20] avg loss: 0.055490691242440195		[learning rate: 0.0029199]
	Learning Rate: 0.0029199
	LOSS [training: 0.06677020770673464 | validation: 0.16469217826532165]
	TIME [epoch: 8.78 sec]
EPOCH 305/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09680254558083137		[learning rate: 0.0029128]
		[batch 20/20] avg loss: 0.15485466864111647		[learning rate: 0.0029058]
	Learning Rate: 0.00290578
	LOSS [training: 0.1258286071109739 | validation: 0.026305773336861165]
	TIME [epoch: 8.77 sec]
EPOCH 306/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0897366273434461		[learning rate: 0.0028987]
		[batch 20/20] avg loss: 0.06584394247848827		[learning rate: 0.0028917]
	Learning Rate: 0.00289173
	LOSS [training: 0.07779028491096719 | validation: 0.19440763317482457]
	TIME [epoch: 8.78 sec]
EPOCH 307/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0911236519448668		[learning rate: 0.0028847]
		[batch 20/20] avg loss: 0.06286710018160005		[learning rate: 0.0028777]
	Learning Rate: 0.00287775
	LOSS [training: 0.07699537606323342 | validation: 0.14518462616576688]
	TIME [epoch: 8.76 sec]
EPOCH 308/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09300463055024616		[learning rate: 0.0028708]
		[batch 20/20] avg loss: 0.09014984897547054		[learning rate: 0.0028638]
	Learning Rate: 0.00286383
	LOSS [training: 0.09157723976285835 | validation: 0.10725956473727741]
	TIME [epoch: 8.79 sec]
EPOCH 309/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1162796768158165		[learning rate: 0.0028569]
		[batch 20/20] avg loss: 0.06862804197980493		[learning rate: 0.00285]
	Learning Rate: 0.00284998
	LOSS [training: 0.09245385939781071 | validation: 0.24136839868288923]
	TIME [epoch: 8.78 sec]
EPOCH 310/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08085055147703044		[learning rate: 0.0028431]
		[batch 20/20] avg loss: 0.09070281982360176		[learning rate: 0.0028362]
	Learning Rate: 0.0028362
	LOSS [training: 0.08577668565031611 | validation: 0.02447880807224431]
	TIME [epoch: 8.77 sec]
EPOCH 311/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08332935849785505		[learning rate: 0.0028293]
		[batch 20/20] avg loss: 0.08875213346000979		[learning rate: 0.0028225]
	Learning Rate: 0.00282248
	LOSS [training: 0.08604074597893241 | validation: 0.033711742870293146]
	TIME [epoch: 8.77 sec]
EPOCH 312/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0893156301187689		[learning rate: 0.0028157]
		[batch 20/20] avg loss: 0.08150107661684841		[learning rate: 0.0028088]
	Learning Rate: 0.00280884
	LOSS [training: 0.08540835336780865 | validation: 0.035928255894518105]
	TIME [epoch: 8.76 sec]
EPOCH 313/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11132400709453358		[learning rate: 0.002802]
		[batch 20/20] avg loss: 0.058156512848107134		[learning rate: 0.0027953]
	Learning Rate: 0.00279525
	LOSS [training: 0.08474025997132036 | validation: 0.11494529218504052]
	TIME [epoch: 8.79 sec]
EPOCH 314/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08125507095392845		[learning rate: 0.0027885]
		[batch 20/20] avg loss: 0.08380875570794974		[learning rate: 0.0027817]
	Learning Rate: 0.00278174
	LOSS [training: 0.08253191333093908 | validation: 0.09288449110151141]
	TIME [epoch: 8.78 sec]
EPOCH 315/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10381837698670504		[learning rate: 0.002775]
		[batch 20/20] avg loss: 0.12129408875176666		[learning rate: 0.0027683]
	Learning Rate: 0.00276828
	LOSS [training: 0.11255623286923586 | validation: 0.036233841105826725]
	TIME [epoch: 8.78 sec]
EPOCH 316/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09274055713263515		[learning rate: 0.0027616]
		[batch 20/20] avg loss: 0.09225744219392636		[learning rate: 0.0027549]
	Learning Rate: 0.0027549
	LOSS [training: 0.09249899966328076 | validation: 0.05076593790849951]
	TIME [epoch: 8.78 sec]
EPOCH 317/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10544058154392974		[learning rate: 0.0027482]
		[batch 20/20] avg loss: 0.08367078901816047		[learning rate: 0.0027416]
	Learning Rate: 0.00274157
	LOSS [training: 0.09455568528104509 | validation: 0.12383325701688061]
	TIME [epoch: 8.77 sec]
EPOCH 318/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09682102540270068		[learning rate: 0.0027349]
		[batch 20/20] avg loss: 0.09328703554396883		[learning rate: 0.0027283]
	Learning Rate: 0.00272832
	LOSS [training: 0.09505403047333479 | validation: 0.06572587769156255]
	TIME [epoch: 8.79 sec]
EPOCH 319/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0573946068671933		[learning rate: 0.0027217]
		[batch 20/20] avg loss: 0.10803926104641774		[learning rate: 0.0027151]
	Learning Rate: 0.00271512
	LOSS [training: 0.08271693395680552 | validation: 0.11652986740097362]
	TIME [epoch: 8.79 sec]
EPOCH 320/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06969270895522997		[learning rate: 0.0027086]
		[batch 20/20] avg loss: 0.08121655508167211		[learning rate: 0.002702]
	Learning Rate: 0.00270199
	LOSS [training: 0.07545463201845103 | validation: 0.06207867065535385]
	TIME [epoch: 8.76 sec]
EPOCH 321/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.061237040397272305		[learning rate: 0.0026955]
		[batch 20/20] avg loss: 0.06370320144856412		[learning rate: 0.0026889]
	Learning Rate: 0.00268893
	LOSS [training: 0.06247012092291822 | validation: 0.015022982194226842]
	TIME [epoch: 8.78 sec]
EPOCH 322/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07204709487839218		[learning rate: 0.0026824]
		[batch 20/20] avg loss: 0.07998514578192886		[learning rate: 0.0026759]
	Learning Rate: 0.00267592
	LOSS [training: 0.07601612033016052 | validation: 0.1863853839183193]
	TIME [epoch: 8.77 sec]
EPOCH 323/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10485474376269925		[learning rate: 0.0026694]
		[batch 20/20] avg loss: 0.11219697953275451		[learning rate: 0.002663]
	Learning Rate: 0.00266298
	LOSS [training: 0.1085258616477269 | validation: 0.11818849071708312]
	TIME [epoch: 8.79 sec]
EPOCH 324/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14256499097541586		[learning rate: 0.0026565]
		[batch 20/20] avg loss: 0.11591301144951675		[learning rate: 0.0026501]
	Learning Rate: 0.00265011
	LOSS [training: 0.12923900121246631 | validation: 0.04240831424135275]
	TIME [epoch: 8.82 sec]
EPOCH 325/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10954677665823329		[learning rate: 0.0026437]
		[batch 20/20] avg loss: 0.10023889617420341		[learning rate: 0.0026373]
	Learning Rate: 0.00263729
	LOSS [training: 0.10489283641621835 | validation: 0.04821324920530594]
	TIME [epoch: 8.77 sec]
EPOCH 326/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.050808629760132194		[learning rate: 0.0026309]
		[batch 20/20] avg loss: 0.07141717635115995		[learning rate: 0.0026245]
	Learning Rate: 0.00262454
	LOSS [training: 0.06111290305564606 | validation: 0.046837001589038765]
	TIME [epoch: 8.77 sec]
EPOCH 327/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08058155779522021		[learning rate: 0.0026182]
		[batch 20/20] avg loss: 0.06471481024665998		[learning rate: 0.0026118]
	Learning Rate: 0.00261184
	LOSS [training: 0.07264818402094012 | validation: 0.04221694695342192]
	TIME [epoch: 8.78 sec]
EPOCH 328/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0885783453982258		[learning rate: 0.0026055]
		[batch 20/20] avg loss: 0.1059166899601589		[learning rate: 0.0025992]
	Learning Rate: 0.00259921
	LOSS [training: 0.09724751767919235 | validation: 0.24073120398639483]
	TIME [epoch: 8.78 sec]
EPOCH 329/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0622218676886472		[learning rate: 0.0025929]
		[batch 20/20] avg loss: 0.06271724558441638		[learning rate: 0.0025866]
	Learning Rate: 0.00258664
	LOSS [training: 0.06246955663653179 | validation: 0.08106108043178714]
	TIME [epoch: 8.8 sec]
EPOCH 330/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07604803177463318		[learning rate: 0.0025804]
		[batch 20/20] avg loss: 0.04746123529383495		[learning rate: 0.0025741]
	Learning Rate: 0.00257414
	LOSS [training: 0.06175463353423406 | validation: 0.03288944956387611]
	TIME [epoch: 8.78 sec]
EPOCH 331/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07674408974165364		[learning rate: 0.0025679]
		[batch 20/20] avg loss: 0.1128017025809809		[learning rate: 0.0025617]
	Learning Rate: 0.00256169
	LOSS [training: 0.09477289616131726 | validation: 0.03204792716912756]
	TIME [epoch: 8.77 sec]
EPOCH 332/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10247258004576114		[learning rate: 0.0025555]
		[batch 20/20] avg loss: 0.10217002928545801		[learning rate: 0.0025493]
	Learning Rate: 0.0025493
	LOSS [training: 0.10232130466560958 | validation: 0.19253650076499157]
	TIME [epoch: 8.76 sec]
EPOCH 333/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08669440296143792		[learning rate: 0.0025431]
		[batch 20/20] avg loss: 0.10660212426688895		[learning rate: 0.002537]
	Learning Rate: 0.00253697
	LOSS [training: 0.09664826361416341 | validation: 0.24107884182685385]
	TIME [epoch: 8.77 sec]
EPOCH 334/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08605425403609755		[learning rate: 0.0025308]
		[batch 20/20] avg loss: 0.07003357098580248		[learning rate: 0.0025247]
	Learning Rate: 0.0025247
	LOSS [training: 0.07804391251095003 | validation: 0.07939933309999166]
	TIME [epoch: 8.79 sec]
EPOCH 335/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09187593885168088		[learning rate: 0.0025186]
		[batch 20/20] avg loss: 0.11972644458694184		[learning rate: 0.0025125]
	Learning Rate: 0.0025125
	LOSS [training: 0.10580119171931135 | validation: 0.06325919833028043]
	TIME [epoch: 8.78 sec]
EPOCH 336/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.064968871162804		[learning rate: 0.0025064]
		[batch 20/20] avg loss: 0.06715293318033784		[learning rate: 0.0025003]
	Learning Rate: 0.00250035
	LOSS [training: 0.06606090217157091 | validation: 0.053915239498705844]
	TIME [epoch: 8.76 sec]
EPOCH 337/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07234003745065061		[learning rate: 0.0024943]
		[batch 20/20] avg loss: 0.09169284060874278		[learning rate: 0.0024883]
	Learning Rate: 0.00248825
	LOSS [training: 0.08201643902969669 | validation: 0.0734283734866721]
	TIME [epoch: 8.77 sec]
EPOCH 338/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11457618123740661		[learning rate: 0.0024822]
		[batch 20/20] avg loss: 0.10076209777801784		[learning rate: 0.0024762]
	Learning Rate: 0.00247622
	LOSS [training: 0.1076691395077122 | validation: 0.06395320484289757]
	TIME [epoch: 8.78 sec]
EPOCH 339/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06284979984646484		[learning rate: 0.0024702]
		[batch 20/20] avg loss: 0.06748519333042621		[learning rate: 0.0024642]
	Learning Rate: 0.00246425
	LOSS [training: 0.06516749658844552 | validation: 0.049350179195032005]
	TIME [epoch: 8.79 sec]
EPOCH 340/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06347126531399094		[learning rate: 0.0024583]
		[batch 20/20] avg loss: 0.07766220917719546		[learning rate: 0.0024523]
	Learning Rate: 0.00245233
	LOSS [training: 0.07056673724559319 | validation: 0.02178613419480908]
	TIME [epoch: 8.78 sec]
EPOCH 341/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06342480218389801		[learning rate: 0.0024464]
		[batch 20/20] avg loss: 0.08051253424733595		[learning rate: 0.0024405]
	Learning Rate: 0.00244047
	LOSS [training: 0.07196866821561698 | validation: 0.024007773225121442]
	TIME [epoch: 8.77 sec]
EPOCH 342/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06133933673490997		[learning rate: 0.0024346]
		[batch 20/20] avg loss: 0.09297924630998829		[learning rate: 0.0024287]
	Learning Rate: 0.00242867
	LOSS [training: 0.07715929152244912 | validation: 0.1918786458226336]
	TIME [epoch: 8.78 sec]
EPOCH 343/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09359556302907782		[learning rate: 0.0024228]
		[batch 20/20] avg loss: 0.07552039910832346		[learning rate: 0.0024169]
	Learning Rate: 0.00241693
	LOSS [training: 0.08455798106870065 | validation: 0.12781024657733098]
	TIME [epoch: 8.78 sec]
EPOCH 344/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13597138288879876		[learning rate: 0.0024111]
		[batch 20/20] avg loss: 0.10062802893820558		[learning rate: 0.0024052]
	Learning Rate: 0.00240524
	LOSS [training: 0.11829970591350218 | validation: 0.01890609948835442]
	TIME [epoch: 8.8 sec]
EPOCH 345/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06124204780369132		[learning rate: 0.0023994]
		[batch 20/20] avg loss: 0.11757727516583712		[learning rate: 0.0023936]
	Learning Rate: 0.00239361
	LOSS [training: 0.0894096614847642 | validation: 0.16931938241540562]
	TIME [epoch: 8.78 sec]
EPOCH 346/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06756441338764428		[learning rate: 0.0023878]
		[batch 20/20] avg loss: 0.08085414916124292		[learning rate: 0.002382]
	Learning Rate: 0.00238203
	LOSS [training: 0.0742092812744436 | validation: 0.035500230516143975]
	TIME [epoch: 8.76 sec]
EPOCH 347/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09507408042493815		[learning rate: 0.0023763]
		[batch 20/20] avg loss: 0.07756925725447707		[learning rate: 0.0023705]
	Learning Rate: 0.00237051
	LOSS [training: 0.08632166883970761 | validation: 0.04315317286798498]
	TIME [epoch: 8.76 sec]
EPOCH 348/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11316587131841231		[learning rate: 0.0023648]
		[batch 20/20] avg loss: 0.08702928261866705		[learning rate: 0.002359]
	Learning Rate: 0.00235905
	LOSS [training: 0.10009757696853967 | validation: 0.08281617078120579]
	TIME [epoch: 8.77 sec]
EPOCH 349/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05707131201534717		[learning rate: 0.0023533]
		[batch 20/20] avg loss: 0.04811537786694565		[learning rate: 0.0023476]
	Learning Rate: 0.00234764
	LOSS [training: 0.052593344941146404 | validation: 0.03995534855376583]
	TIME [epoch: 8.78 sec]
EPOCH 350/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13494504550531464		[learning rate: 0.002342]
		[batch 20/20] avg loss: 0.05000699644182764		[learning rate: 0.0023363]
	Learning Rate: 0.00233629
	LOSS [training: 0.09247602097357113 | validation: 0.05953506999652304]
	TIME [epoch: 8.8 sec]
EPOCH 351/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.060569054928721756		[learning rate: 0.0023306]
		[batch 20/20] avg loss: 0.06398164122426368		[learning rate: 0.002325]
	Learning Rate: 0.00232499
	LOSS [training: 0.06227534807649272 | validation: 0.06032936834645563]
	TIME [epoch: 8.77 sec]
EPOCH 352/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08547711851590181		[learning rate: 0.0023194]
		[batch 20/20] avg loss: 0.10530183996317581		[learning rate: 0.0023137]
	Learning Rate: 0.00231375
	LOSS [training: 0.0953894792395388 | validation: 0.03859012793327543]
	TIME [epoch: 8.78 sec]
EPOCH 353/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06925417143647583		[learning rate: 0.0023081]
		[batch 20/20] avg loss: 0.1024846778764098		[learning rate: 0.0023026]
	Learning Rate: 0.00230256
	LOSS [training: 0.08586942465644282 | validation: 0.06671298702657523]
	TIME [epoch: 8.78 sec]
EPOCH 354/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06212448643978748		[learning rate: 0.002297]
		[batch 20/20] avg loss: 0.08512951317261189		[learning rate: 0.0022914]
	Learning Rate: 0.00229142
	LOSS [training: 0.07362699980619966 | validation: 0.006762578212569012]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_354.pth
	Model improved!!!
EPOCH 355/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06367693790068316		[learning rate: 0.0022859]
		[batch 20/20] avg loss: 0.06785688321270543		[learning rate: 0.0022803]
	Learning Rate: 0.00228034
	LOSS [training: 0.06576691055669429 | validation: 0.02433797608707057]
	TIME [epoch: 8.8 sec]
EPOCH 356/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12003710096893201		[learning rate: 0.0022748]
		[batch 20/20] avg loss: 0.08061851278328341		[learning rate: 0.0022693]
	Learning Rate: 0.00226931
	LOSS [training: 0.10032780687610772 | validation: 0.049892961651090396]
	TIME [epoch: 8.77 sec]
EPOCH 357/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09280847159439004		[learning rate: 0.0022638]
		[batch 20/20] avg loss: 0.07026721748686506		[learning rate: 0.0022583]
	Learning Rate: 0.00225834
	LOSS [training: 0.08153784454062754 | validation: 0.03182026067833858]
	TIME [epoch: 8.78 sec]
EPOCH 358/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07065852474769387		[learning rate: 0.0022529]
		[batch 20/20] avg loss: 0.07894706984341189		[learning rate: 0.0022474]
	Learning Rate: 0.00224742
	LOSS [training: 0.07480279729555288 | validation: 0.09939982794026594]
	TIME [epoch: 8.76 sec]
EPOCH 359/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0574486353952654		[learning rate: 0.002242]
		[batch 20/20] avg loss: 0.0721158072642547		[learning rate: 0.0022366]
	Learning Rate: 0.00223655
	LOSS [training: 0.06478222132976005 | validation: 0.019093595560396366]
	TIME [epoch: 8.77 sec]
EPOCH 360/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08009158697096863		[learning rate: 0.0022311]
		[batch 20/20] avg loss: 0.09285477522788561		[learning rate: 0.0022257]
	Learning Rate: 0.00222574
	LOSS [training: 0.08647318109942712 | validation: 0.025104019132685732]
	TIME [epoch: 8.79 sec]
EPOCH 361/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08939623688284241		[learning rate: 0.0022203]
		[batch 20/20] avg loss: 0.04834513115725236		[learning rate: 0.002215]
	Learning Rate: 0.00221497
	LOSS [training: 0.0688706840200474 | validation: 0.06913325249584697]
	TIME [epoch: 8.76 sec]
EPOCH 362/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0652874359889811		[learning rate: 0.0022096]
		[batch 20/20] avg loss: 0.08475936708502316		[learning rate: 0.0022043]
	Learning Rate: 0.00220426
	LOSS [training: 0.07502340153700213 | validation: 0.15094822272666605]
	TIME [epoch: 8.76 sec]
EPOCH 363/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08996108524102235		[learning rate: 0.0021989]
		[batch 20/20] avg loss: 0.08709679658430128		[learning rate: 0.0021936]
	Learning Rate: 0.0021936
	LOSS [training: 0.08852894091266182 | validation: 0.02501339071768134]
	TIME [epoch: 8.76 sec]
EPOCH 364/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06309202629618793		[learning rate: 0.0021883]
		[batch 20/20] avg loss: 0.05034111690485134		[learning rate: 0.002183]
	Learning Rate: 0.00218299
	LOSS [training: 0.05671657160051964 | validation: 0.0638470865459578]
	TIME [epoch: 8.76 sec]
EPOCH 365/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06210540392284712		[learning rate: 0.0021777]
		[batch 20/20] avg loss: 0.08065212504272194		[learning rate: 0.0021724]
	Learning Rate: 0.00217244
	LOSS [training: 0.07137876448278452 | validation: 0.07440580191991683]
	TIME [epoch: 8.8 sec]
EPOCH 366/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08617913231615615		[learning rate: 0.0021672]
		[batch 20/20] avg loss: 0.07629859044257561		[learning rate: 0.0021619]
	Learning Rate: 0.00216193
	LOSS [training: 0.0812388613793659 | validation: 0.0387216712485647]
	TIME [epoch: 8.77 sec]
EPOCH 367/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11810618057909092		[learning rate: 0.0021567]
		[batch 20/20] avg loss: 0.053601809798156784		[learning rate: 0.0021515]
	Learning Rate: 0.00215148
	LOSS [training: 0.08585399518862387 | validation: 0.02136718197853303]
	TIME [epoch: 8.77 sec]
EPOCH 368/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10121405564133153		[learning rate: 0.0021463]
		[batch 20/20] avg loss: 0.050203926278978704		[learning rate: 0.0021411]
	Learning Rate: 0.00214107
	LOSS [training: 0.07570899096015511 | validation: 0.020844785565855454]
	TIME [epoch: 8.77 sec]
EPOCH 369/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08472130465946798		[learning rate: 0.0021359]
		[batch 20/20] avg loss: 0.0463683107173573		[learning rate: 0.0021307]
	Learning Rate: 0.00213072
	LOSS [training: 0.06554480768841263 | validation: 0.04112260761134392]
	TIME [epoch: 8.77 sec]
EPOCH 370/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07322978533433723		[learning rate: 0.0021256]
		[batch 20/20] avg loss: 0.06003799406129694		[learning rate: 0.0021204]
	Learning Rate: 0.00212042
	LOSS [training: 0.06663388969781708 | validation: 0.11369938786920818]
	TIME [epoch: 8.8 sec]
EPOCH 371/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07194685117560225		[learning rate: 0.0021153]
		[batch 20/20] avg loss: 0.0730541106686259		[learning rate: 0.0021102]
	Learning Rate: 0.00211016
	LOSS [training: 0.07250048092211409 | validation: 0.05986919199201028]
	TIME [epoch: 8.78 sec]
EPOCH 372/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06771240551379752		[learning rate: 0.0021051]
		[batch 20/20] avg loss: 0.046634503601660986		[learning rate: 0.0021]
	Learning Rate: 0.00209996
	LOSS [training: 0.05717345455772925 | validation: 0.04504782102082429]
	TIME [epoch: 8.76 sec]
EPOCH 373/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.051336125257160595		[learning rate: 0.0020949]
		[batch 20/20] avg loss: 0.06633318512838035		[learning rate: 0.0020898]
	Learning Rate: 0.0020898
	LOSS [training: 0.058834655192770466 | validation: 0.04503623594770198]
	TIME [epoch: 8.77 sec]
EPOCH 374/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03724225634526845		[learning rate: 0.0020847]
		[batch 20/20] avg loss: 0.07631663564111933		[learning rate: 0.0020797]
	Learning Rate: 0.0020797
	LOSS [training: 0.0567794459931939 | validation: 0.054171105118711586]
	TIME [epoch: 8.77 sec]
EPOCH 375/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10837909030091675		[learning rate: 0.0020747]
		[batch 20/20] avg loss: 0.08242829959348456		[learning rate: 0.0020696]
	Learning Rate: 0.00206964
	LOSS [training: 0.09540369494720066 | validation: 0.0781362883373441]
	TIME [epoch: 8.79 sec]
EPOCH 376/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07160102571566937		[learning rate: 0.0020646]
		[batch 20/20] avg loss: 0.11567269383200487		[learning rate: 0.0020596]
	Learning Rate: 0.00205963
	LOSS [training: 0.09363685977383711 | validation: 0.030860722832227058]
	TIME [epoch: 8.79 sec]
EPOCH 377/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06174703963539276		[learning rate: 0.0020546]
		[batch 20/20] avg loss: 0.03568848606452761		[learning rate: 0.0020497]
	Learning Rate: 0.00204967
	LOSS [training: 0.0487177628499602 | validation: 0.03983641780841441]
	TIME [epoch: 8.77 sec]
EPOCH 378/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06389812622253377		[learning rate: 0.0020447]
		[batch 20/20] avg loss: 0.04620738781636254		[learning rate: 0.0020398]
	Learning Rate: 0.00203976
	LOSS [training: 0.05505275701944816 | validation: 0.01464513701412069]
	TIME [epoch: 8.78 sec]
EPOCH 379/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07440513875270821		[learning rate: 0.0020348]
		[batch 20/20] avg loss: 0.052730658675459285		[learning rate: 0.0020299]
	Learning Rate: 0.0020299
	LOSS [training: 0.06356789871408376 | validation: 0.014889526151162166]
	TIME [epoch: 8.77 sec]
EPOCH 380/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06360248535618071		[learning rate: 0.002025]
		[batch 20/20] avg loss: 0.0669447271060377		[learning rate: 0.0020201]
	Learning Rate: 0.00202008
	LOSS [training: 0.06527360623110921 | validation: 0.005476665277539004]
	TIME [epoch: 8.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_380.pth
	Model improved!!!
EPOCH 381/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05978435761685572		[learning rate: 0.0020152]
		[batch 20/20] avg loss: 0.059994633481996754		[learning rate: 0.0020103]
	Learning Rate: 0.00201031
	LOSS [training: 0.05988949554942623 | validation: 0.0815096577470509]
	TIME [epoch: 8.78 sec]
EPOCH 382/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0640636129787403		[learning rate: 0.0020054]
		[batch 20/20] avg loss: 0.08240433790885128		[learning rate: 0.0020006]
	Learning Rate: 0.00200059
	LOSS [training: 0.0732339754437958 | validation: 0.10117232665777316]
	TIME [epoch: 8.78 sec]
EPOCH 383/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06964625289321073		[learning rate: 0.0019957]
		[batch 20/20] avg loss: 0.05930607742712494		[learning rate: 0.0019909]
	Learning Rate: 0.00199091
	LOSS [training: 0.06447616516016783 | validation: 0.05375364606695814]
	TIME [epoch: 8.78 sec]
EPOCH 384/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08202118889119028		[learning rate: 0.0019861]
		[batch 20/20] avg loss: 0.05084506432028877		[learning rate: 0.0019813]
	Learning Rate: 0.00198129
	LOSS [training: 0.06643312660573952 | validation: 0.02077810335801584]
	TIME [epoch: 8.78 sec]
EPOCH 385/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05082950866470594		[learning rate: 0.0019765]
		[batch 20/20] avg loss: 0.08032229723551561		[learning rate: 0.0019717]
	Learning Rate: 0.00197171
	LOSS [training: 0.06557590295011079 | validation: 0.08160436882746198]
	TIME [epoch: 8.79 sec]
EPOCH 386/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06283018798549098		[learning rate: 0.0019669]
		[batch 20/20] avg loss: 0.08885864053103529		[learning rate: 0.0019622]
	Learning Rate: 0.00196217
	LOSS [training: 0.07584441425826313 | validation: 0.02388746508967354]
	TIME [epoch: 8.79 sec]
EPOCH 387/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04169407732854517		[learning rate: 0.0019574]
		[batch 20/20] avg loss: 0.0607967370736861		[learning rate: 0.0019527]
	Learning Rate: 0.00195268
	LOSS [training: 0.051245407201115645 | validation: 0.05081377297169146]
	TIME [epoch: 8.77 sec]
EPOCH 388/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09550825431692733		[learning rate: 0.001948]
		[batch 20/20] avg loss: 0.0494043801979999		[learning rate: 0.0019432]
	Learning Rate: 0.00194324
	LOSS [training: 0.07245631725746363 | validation: 0.15414009345177596]
	TIME [epoch: 8.78 sec]
EPOCH 389/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06976267924606816		[learning rate: 0.0019385]
		[batch 20/20] avg loss: 0.05477763293715401		[learning rate: 0.0019338]
	Learning Rate: 0.00193384
	LOSS [training: 0.06227015609161107 | validation: 0.06565458965325692]
	TIME [epoch: 8.76 sec]
EPOCH 390/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.061008672742675275		[learning rate: 0.0019292]
		[batch 20/20] avg loss: 0.06395606768879368		[learning rate: 0.0019245]
	Learning Rate: 0.00192449
	LOSS [training: 0.062482370215734495 | validation: 0.03191636918831327]
	TIME [epoch: 8.76 sec]
EPOCH 391/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.050740893101144005		[learning rate: 0.0019198]
		[batch 20/20] avg loss: 0.05061550854080963		[learning rate: 0.0019152]
	Learning Rate: 0.00191518
	LOSS [training: 0.05067820082097681 | validation: 0.0721928192761273]
	TIME [epoch: 8.81 sec]
EPOCH 392/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07957933716993232		[learning rate: 0.0019105]
		[batch 20/20] avg loss: 0.06166015257051581		[learning rate: 0.0019059]
	Learning Rate: 0.00190592
	LOSS [training: 0.07061974487022406 | validation: 0.03318398389699644]
	TIME [epoch: 8.76 sec]
EPOCH 393/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06369606091514556		[learning rate: 0.0019013]
		[batch 20/20] avg loss: 0.04692164872476271		[learning rate: 0.0018967]
	Learning Rate: 0.00189671
	LOSS [training: 0.05530885481995414 | validation: 0.06619143089213393]
	TIME [epoch: 8.77 sec]
EPOCH 394/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07158168302027793		[learning rate: 0.0018921]
		[batch 20/20] avg loss: 0.08238018873921238		[learning rate: 0.0018875]
	Learning Rate: 0.00188753
	LOSS [training: 0.07698093587974517 | validation: 0.32533675095166503]
	TIME [epoch: 8.76 sec]
EPOCH 395/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1285420678564944		[learning rate: 0.001883]
		[batch 20/20] avg loss: 0.053279739822310934		[learning rate: 0.0018784]
	Learning Rate: 0.00187841
	LOSS [training: 0.09091090383940267 | validation: 0.06945321432891376]
	TIME [epoch: 8.76 sec]
EPOCH 396/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08547441977356027		[learning rate: 0.0018739]
		[batch 20/20] avg loss: 0.04482634381177659		[learning rate: 0.0018693]
	Learning Rate: 0.00186932
	LOSS [training: 0.06515038179266842 | validation: 0.09978264793040974]
	TIME [epoch: 8.8 sec]
EPOCH 397/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10519087182096536		[learning rate: 0.0018648]
		[batch 20/20] avg loss: 0.07315810069847356		[learning rate: 0.0018603]
	Learning Rate: 0.00186028
	LOSS [training: 0.08917448625971945 | validation: 0.04690927968796748]
	TIME [epoch: 8.78 sec]
EPOCH 398/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.048888919978736824		[learning rate: 0.0018558]
		[batch 20/20] avg loss: 0.06087308001402464		[learning rate: 0.0018513]
	Learning Rate: 0.00185129
	LOSS [training: 0.05488099999638073 | validation: 0.1110208898649345]
	TIME [epoch: 8.78 sec]
EPOCH 399/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07339533375581751		[learning rate: 0.0018468]
		[batch 20/20] avg loss: 0.053302378255350266		[learning rate: 0.0018423]
	Learning Rate: 0.00184233
	LOSS [training: 0.06334885600558389 | validation: 0.01942660693579421]
	TIME [epoch: 8.77 sec]
EPOCH 400/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10217538972820925		[learning rate: 0.0018379]
		[batch 20/20] avg loss: 0.08471260025894276		[learning rate: 0.0018334]
	Learning Rate: 0.00183343
	LOSS [training: 0.093443994993576 | validation: 0.08927909394740062]
	TIME [epoch: 8.77 sec]
EPOCH 401/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04474168613404106		[learning rate: 0.001829]
		[batch 20/20] avg loss: 0.06475324286014464		[learning rate: 0.0018246]
	Learning Rate: 0.00182456
	LOSS [training: 0.05474746449709285 | validation: 0.019603925019190085]
	TIME [epoch: 8.78 sec]
EPOCH 402/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.046859715202533805		[learning rate: 0.0018201]
		[batch 20/20] avg loss: 0.04195427930358665		[learning rate: 0.0018157]
	Learning Rate: 0.00181574
	LOSS [training: 0.04440699725306023 | validation: 0.07325153587943077]
	TIME [epoch: 8.76 sec]
EPOCH 403/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06586190945697593		[learning rate: 0.0018113]
		[batch 20/20] avg loss: 0.07475862383023076		[learning rate: 0.001807]
	Learning Rate: 0.00180696
	LOSS [training: 0.07031026664360335 | validation: 0.08752733338465435]
	TIME [epoch: 8.78 sec]
EPOCH 404/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07770722103334922		[learning rate: 0.0018026]
		[batch 20/20] avg loss: 0.07653074516637352		[learning rate: 0.0017982]
	Learning Rate: 0.00179822
	LOSS [training: 0.07711898309986134 | validation: 0.01605153758978057]
	TIME [epoch: 8.76 sec]
EPOCH 405/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05505854724420837		[learning rate: 0.0017939]
		[batch 20/20] avg loss: 0.04414564660634916		[learning rate: 0.0017895]
	Learning Rate: 0.00178952
	LOSS [training: 0.04960209692527877 | validation: 0.03761337202054623]
	TIME [epoch: 8.76 sec]
EPOCH 406/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.061381365714906724		[learning rate: 0.0017852]
		[batch 20/20] avg loss: 0.04435640480525911		[learning rate: 0.0017809]
	Learning Rate: 0.00178087
	LOSS [training: 0.05286888526008293 | validation: 0.018059260531120527]
	TIME [epoch: 8.78 sec]
EPOCH 407/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05237560919444961		[learning rate: 0.0017766]
		[batch 20/20] avg loss: 0.07972472109254808		[learning rate: 0.0017723]
	Learning Rate: 0.00177226
	LOSS [training: 0.06605016514349885 | validation: 0.04352202539261555]
	TIME [epoch: 8.78 sec]
EPOCH 408/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04303862195163839		[learning rate: 0.001768]
		[batch 20/20] avg loss: 0.059082835397528656		[learning rate: 0.0017637]
	Learning Rate: 0.00176369
	LOSS [training: 0.051060728674583514 | validation: 0.021721949312690367]
	TIME [epoch: 8.76 sec]
EPOCH 409/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06174674087692816		[learning rate: 0.0017594]
		[batch 20/20] avg loss: 0.09419339458166501		[learning rate: 0.0017552]
	Learning Rate: 0.00175516
	LOSS [training: 0.07797006772929659 | validation: 0.2713140584927372]
	TIME [epoch: 8.77 sec]
EPOCH 410/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0974480933075686		[learning rate: 0.0017509]
		[batch 20/20] avg loss: 0.055207960102933704		[learning rate: 0.0017467]
	Learning Rate: 0.00174667
	LOSS [training: 0.07632802670525114 | validation: 0.07539820742928685]
	TIME [epoch: 8.77 sec]
EPOCH 411/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07606874627283214		[learning rate: 0.0017424]
		[batch 20/20] avg loss: 0.07975550010360558		[learning rate: 0.0017382]
	Learning Rate: 0.00173822
	LOSS [training: 0.07791212318821883 | validation: 0.030066599494168383]
	TIME [epoch: 8.78 sec]
EPOCH 412/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028108333344012793		[learning rate: 0.001734]
		[batch 20/20] avg loss: 0.03739185944514571		[learning rate: 0.0017298]
	Learning Rate: 0.00172982
	LOSS [training: 0.03275009639457926 | validation: 0.05198132386098372]
	TIME [epoch: 8.8 sec]
EPOCH 413/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04917889825290269		[learning rate: 0.0017256]
		[batch 20/20] avg loss: 0.05649394328034489		[learning rate: 0.0017215]
	Learning Rate: 0.00172145
	LOSS [training: 0.05283642076662378 | validation: 0.06864627280835864]
	TIME [epoch: 8.76 sec]
EPOCH 414/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05411678573521369		[learning rate: 0.0017173]
		[batch 20/20] avg loss: 0.07170217741761649		[learning rate: 0.0017131]
	Learning Rate: 0.00171313
	LOSS [training: 0.0629094815764151 | validation: 0.025412399241809387]
	TIME [epoch: 8.77 sec]
EPOCH 415/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.056234000282295124		[learning rate: 0.001709]
		[batch 20/20] avg loss: 0.04783138846917329		[learning rate: 0.0017048]
	Learning Rate: 0.00170484
	LOSS [training: 0.0520326943757342 | validation: 0.05951938953574094]
	TIME [epoch: 8.76 sec]
EPOCH 416/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07446580474712729		[learning rate: 0.0017007]
		[batch 20/20] avg loss: 0.06950059260982083		[learning rate: 0.0016966]
	Learning Rate: 0.0016966
	LOSS [training: 0.07198319867847407 | validation: 0.07434669913041336]
	TIME [epoch: 8.77 sec]
EPOCH 417/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04640472557150014		[learning rate: 0.0016925]
		[batch 20/20] avg loss: 0.05037229808198322		[learning rate: 0.0016884]
	Learning Rate: 0.00168839
	LOSS [training: 0.048388511826741684 | validation: 0.017076551665245396]
	TIME [epoch: 8.78 sec]
EPOCH 418/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04826548301100154		[learning rate: 0.0016843]
		[batch 20/20] avg loss: 0.05451045164330439		[learning rate: 0.0016802]
	Learning Rate: 0.00168023
	LOSS [training: 0.05138796732715297 | validation: 0.03459249377526691]
	TIME [epoch: 8.76 sec]
EPOCH 419/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.048819843269859636		[learning rate: 0.0016762]
		[batch 20/20] avg loss: 0.1179106660046231		[learning rate: 0.0016721]
	Learning Rate: 0.0016721
	LOSS [training: 0.08336525463724134 | validation: 0.09211186056030013]
	TIME [epoch: 8.77 sec]
EPOCH 420/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09155448513206683		[learning rate: 0.0016681]
		[batch 20/20] avg loss: 0.0434305391038796		[learning rate: 0.001664]
	Learning Rate: 0.00166402
	LOSS [training: 0.0674925121179732 | validation: 0.08161100055230398]
	TIME [epoch: 8.76 sec]
EPOCH 421/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0695155786700092		[learning rate: 0.00166]
		[batch 20/20] avg loss: 0.049332096055310214		[learning rate: 0.001656]
	Learning Rate: 0.00165597
	LOSS [training: 0.0594238373626597 | validation: 0.011390293274277829]
	TIME [epoch: 8.76 sec]
EPOCH 422/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04488762724074652		[learning rate: 0.001652]
		[batch 20/20] avg loss: 0.04872150172918787		[learning rate: 0.001648]
	Learning Rate: 0.00164796
	LOSS [training: 0.046804564484967184 | validation: 0.018748602266504523]
	TIME [epoch: 8.78 sec]
EPOCH 423/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07467182964558615		[learning rate: 0.001644]
		[batch 20/20] avg loss: 0.060094368425062494		[learning rate: 0.00164]
	Learning Rate: 0.00163999
	LOSS [training: 0.06738309903532433 | validation: 0.11465582548080708]
	TIME [epoch: 8.76 sec]
EPOCH 424/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08794668631310479		[learning rate: 0.001636]
		[batch 20/20] avg loss: 0.044965139293242626		[learning rate: 0.0016321]
	Learning Rate: 0.00163206
	LOSS [training: 0.06645591280317371 | validation: 0.012136150957099046]
	TIME [epoch: 8.77 sec]
EPOCH 425/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05121896855849226		[learning rate: 0.0016281]
		[batch 20/20] avg loss: 0.03825561212383123		[learning rate: 0.0016242]
	Learning Rate: 0.00162417
	LOSS [training: 0.044737290341161744 | validation: 0.017294425392776987]
	TIME [epoch: 8.77 sec]
EPOCH 426/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.047866673393902666		[learning rate: 0.0016202]
		[batch 20/20] avg loss: 0.03903377032729562		[learning rate: 0.0016163]
	Learning Rate: 0.00161632
	LOSS [training: 0.04345022186059914 | validation: 0.025568738951710288]
	TIME [epoch: 8.76 sec]
EPOCH 427/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07624666503017355		[learning rate: 0.0016124]
		[batch 20/20] avg loss: 0.05467097202607961		[learning rate: 0.0016085]
	Learning Rate: 0.0016085
	LOSS [training: 0.06545881852812659 | validation: 0.09206486056174495]
	TIME [epoch: 8.8 sec]
EPOCH 428/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.060557517309709576		[learning rate: 0.0016046]
		[batch 20/20] avg loss: 0.06199204803285195		[learning rate: 0.0016007]
	Learning Rate: 0.00160072
	LOSS [training: 0.06127478267128076 | validation: 0.02658233984907455]
	TIME [epoch: 8.76 sec]
EPOCH 429/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03561903060458051		[learning rate: 0.0015968]
		[batch 20/20] avg loss: 0.050170393465496545		[learning rate: 0.001593]
	Learning Rate: 0.00159298
	LOSS [training: 0.042894712035038515 | validation: 0.032656804196627955]
	TIME [epoch: 8.76 sec]
EPOCH 430/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0662074741126424		[learning rate: 0.0015891]
		[batch 20/20] avg loss: 0.03947035964948199		[learning rate: 0.0015853]
	Learning Rate: 0.00158528
	LOSS [training: 0.0528389168810622 | validation: 0.03221423137161528]
	TIME [epoch: 8.75 sec]
EPOCH 431/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05227052894938703		[learning rate: 0.0015814]
		[batch 20/20] avg loss: 0.09196351657051746		[learning rate: 0.0015776]
	Learning Rate: 0.00157761
	LOSS [training: 0.07211702275995226 | validation: 0.05652693467769645]
	TIME [epoch: 8.76 sec]
EPOCH 432/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.045838520762319326		[learning rate: 0.0015738]
		[batch 20/20] avg loss: 0.02827979171967973		[learning rate: 0.00157]
	Learning Rate: 0.00156998
	LOSS [training: 0.03705915624099953 | validation: 0.06480129067954055]
	TIME [epoch: 8.79 sec]
EPOCH 433/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.045698523946658465		[learning rate: 0.0015662]
		[batch 20/20] avg loss: 0.04030313412065828		[learning rate: 0.0015624]
	Learning Rate: 0.00156239
	LOSS [training: 0.043000829033658365 | validation: 0.008585344761502094]
	TIME [epoch: 8.76 sec]
EPOCH 434/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.036799066406942825		[learning rate: 0.0015586]
		[batch 20/20] avg loss: 0.05868379125905228		[learning rate: 0.0015548]
	Learning Rate: 0.00155483
	LOSS [training: 0.04774142883299755 | validation: 0.013021092508150794]
	TIME [epoch: 8.77 sec]
EPOCH 435/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04470499822200347		[learning rate: 0.0015511]
		[batch 20/20] avg loss: 0.0818949398159789		[learning rate: 0.0015473]
	Learning Rate: 0.00154732
	LOSS [training: 0.06329996901899118 | validation: 0.009853839114505014]
	TIME [epoch: 8.76 sec]
EPOCH 436/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04822646468745109		[learning rate: 0.0015436]
		[batch 20/20] avg loss: 0.054256738006491355		[learning rate: 0.0015398]
	Learning Rate: 0.00153983
	LOSS [training: 0.05124160134697122 | validation: 0.010198775011640984]
	TIME [epoch: 8.76 sec]
EPOCH 437/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.039984330562536904		[learning rate: 0.0015361]
		[batch 20/20] avg loss: 0.04381216324128743		[learning rate: 0.0015324]
	Learning Rate: 0.00153239
	LOSS [training: 0.04189824690191216 | validation: 0.01157552811803272]
	TIME [epoch: 8.78 sec]
EPOCH 438/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06016606666815307		[learning rate: 0.0015287]
		[batch 20/20] avg loss: 0.04280142323129945		[learning rate: 0.001525]
	Learning Rate: 0.00152498
	LOSS [training: 0.05148374494972626 | validation: 0.02269030360882618]
	TIME [epoch: 8.79 sec]
EPOCH 439/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.033539162275183686		[learning rate: 0.0015213]
		[batch 20/20] avg loss: 0.03877182546338775		[learning rate: 0.0015176]
	Learning Rate: 0.0015176
	LOSS [training: 0.036155493869285724 | validation: 0.020302703066072784]
	TIME [epoch: 8.77 sec]
EPOCH 440/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0416339788930274		[learning rate: 0.0015139]
		[batch 20/20] avg loss: 0.057825120230001456		[learning rate: 0.0015103]
	Learning Rate: 0.00151026
	LOSS [training: 0.04972954956151443 | validation: 0.07435829971748101]
	TIME [epoch: 8.77 sec]
EPOCH 441/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05253861614283691		[learning rate: 0.0015066]
		[batch 20/20] avg loss: 0.04302655018365282		[learning rate: 0.001503]
	Learning Rate: 0.00150296
	LOSS [training: 0.047782583163244866 | validation: 0.03850212589358933]
	TIME [epoch: 8.76 sec]
EPOCH 442/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04209572867476943		[learning rate: 0.0014993]
		[batch 20/20] avg loss: 0.03949214730816529		[learning rate: 0.0014957]
	Learning Rate: 0.00149569
	LOSS [training: 0.04079393799146737 | validation: 0.025685917656695367]
	TIME [epoch: 8.76 sec]
EPOCH 443/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.040423687725553735		[learning rate: 0.0014921]
		[batch 20/20] avg loss: 0.03905368350084293		[learning rate: 0.0014885]
	Learning Rate: 0.00148846
	LOSS [training: 0.03973868561319833 | validation: 0.1031091626041224]
	TIME [epoch: 8.78 sec]
EPOCH 444/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.045883965343692275		[learning rate: 0.0014849]
		[batch 20/20] avg loss: 0.054513428361739294		[learning rate: 0.0014813]
	Learning Rate: 0.00148126
	LOSS [training: 0.05019869685271579 | validation: 0.042566208384226534]
	TIME [epoch: 8.76 sec]
EPOCH 445/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04772307580630719		[learning rate: 0.0014777]
		[batch 20/20] avg loss: 0.03533374558515854		[learning rate: 0.0014741]
	Learning Rate: 0.0014741
	LOSS [training: 0.04152841069573286 | validation: 0.01784190912233103]
	TIME [epoch: 8.76 sec]
EPOCH 446/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0368576354395173		[learning rate: 0.0014705]
		[batch 20/20] avg loss: 0.07096639577396595		[learning rate: 0.001467]
	Learning Rate: 0.00146697
	LOSS [training: 0.05391201560674162 | validation: 0.00909806630162122]
	TIME [epoch: 8.76 sec]
EPOCH 447/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03326366573432519		[learning rate: 0.0014634]
		[batch 20/20] avg loss: 0.03246870876360626		[learning rate: 0.0014599]
	Learning Rate: 0.00145988
	LOSS [training: 0.032866187248965714 | validation: 0.051255055719126044]
	TIME [epoch: 8.76 sec]
EPOCH 448/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04556631573863162		[learning rate: 0.0014563]
		[batch 20/20] avg loss: 0.05546687293341498		[learning rate: 0.0014528]
	Learning Rate: 0.00145282
	LOSS [training: 0.05051659433602329 | validation: 0.05368453017507753]
	TIME [epoch: 8.78 sec]
EPOCH 449/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04937167931068664		[learning rate: 0.0014493]
		[batch 20/20] avg loss: 0.02881146267243432		[learning rate: 0.0014458]
	Learning Rate: 0.00144579
	LOSS [training: 0.03909157099156048 | validation: 0.018339754092406013]
	TIME [epoch: 8.76 sec]
EPOCH 450/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.040657232415523645		[learning rate: 0.0014423]
		[batch 20/20] avg loss: 0.04411451752494623		[learning rate: 0.0014388]
	Learning Rate: 0.0014388
	LOSS [training: 0.04238587497023493 | validation: 0.06292373032701154]
	TIME [epoch: 8.76 sec]
EPOCH 451/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04667682889426712		[learning rate: 0.0014353]
		[batch 20/20] avg loss: 0.0638346695739728		[learning rate: 0.0014318]
	Learning Rate: 0.00143184
	LOSS [training: 0.055255749234119945 | validation: 0.020831303032138874]
	TIME [epoch: 8.77 sec]
EPOCH 452/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07694962133362915		[learning rate: 0.0014284]
		[batch 20/20] avg loss: 0.03972214592122919		[learning rate: 0.0014249]
	Learning Rate: 0.00142492
	LOSS [training: 0.058335883627429176 | validation: 0.13282277845998933]
	TIME [epoch: 8.76 sec]
EPOCH 453/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.042491750558664367		[learning rate: 0.0014215]
		[batch 20/20] avg loss: 0.04825270994253066		[learning rate: 0.001418]
	Learning Rate: 0.00141803
	LOSS [training: 0.045372230250597514 | validation: 0.007124217635198058]
	TIME [epoch: 8.8 sec]
EPOCH 454/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06047571778533227		[learning rate: 0.0014146]
		[batch 20/20] avg loss: 0.047382387982952065		[learning rate: 0.0014112]
	Learning Rate: 0.00141117
	LOSS [training: 0.05392905288414218 | validation: 0.009404688676809481]
	TIME [epoch: 8.76 sec]
EPOCH 455/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03726338506082903		[learning rate: 0.0014078]
		[batch 20/20] avg loss: 0.07792948761939812		[learning rate: 0.0014043]
	Learning Rate: 0.00140434
	LOSS [training: 0.05759643634011356 | validation: 0.06901234820326953]
	TIME [epoch: 8.77 sec]
EPOCH 456/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05416356286496715		[learning rate: 0.0014009]
		[batch 20/20] avg loss: 0.06321508316165723		[learning rate: 0.0013976]
	Learning Rate: 0.00139755
	LOSS [training: 0.058689323013312186 | validation: 0.0064922620720226235]
	TIME [epoch: 8.77 sec]
EPOCH 457/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04176398349776537		[learning rate: 0.0013942]
		[batch 20/20] avg loss: 0.03901097495831327		[learning rate: 0.0013908]
	Learning Rate: 0.0013908
	LOSS [training: 0.04038747922803933 | validation: 0.014083748425749224]
	TIME [epoch: 8.75 sec]
EPOCH 458/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04727290568984292		[learning rate: 0.0013874]
		[batch 20/20] avg loss: 0.061347242161184076		[learning rate: 0.0013841]
	Learning Rate: 0.00138407
	LOSS [training: 0.0543100739255135 | validation: 0.02449337890747454]
	TIME [epoch: 8.78 sec]
EPOCH 459/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.054617997801380255		[learning rate: 0.0013807]
		[batch 20/20] avg loss: 0.05504947713673858		[learning rate: 0.0013774]
	Learning Rate: 0.00137738
	LOSS [training: 0.05483373746905942 | validation: 0.024426693048742835]
	TIME [epoch: 8.75 sec]
EPOCH 460/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.057597915925313214		[learning rate: 0.001374]
		[batch 20/20] avg loss: 0.0361207006395287		[learning rate: 0.0013707]
	Learning Rate: 0.00137072
	LOSS [training: 0.04685930828242095 | validation: 0.020977347614116307]
	TIME [epoch: 8.76 sec]
EPOCH 461/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07518960013055746		[learning rate: 0.0013674]
		[batch 20/20] avg loss: 0.04528677724135981		[learning rate: 0.0013641]
	Learning Rate: 0.00136409
	LOSS [training: 0.06023818868595866 | validation: 0.07393736871712359]
	TIME [epoch: 8.75 sec]
EPOCH 462/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06888256449352147		[learning rate: 0.0013608]
		[batch 20/20] avg loss: 0.03765509414358171		[learning rate: 0.0013575]
	Learning Rate: 0.00135749
	LOSS [training: 0.05326882931855158 | validation: 0.06765606182937799]
	TIME [epoch: 8.76 sec]
EPOCH 463/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03750088029821201		[learning rate: 0.0013542]
		[batch 20/20] avg loss: 0.0619636679166494		[learning rate: 0.0013509]
	Learning Rate: 0.00135093
	LOSS [training: 0.04973227410743071 | validation: 0.03366783864215246]
	TIME [epoch: 8.78 sec]
EPOCH 464/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.018674479633827044		[learning rate: 0.0013477]
		[batch 20/20] avg loss: 0.0482099864699078		[learning rate: 0.0013444]
	Learning Rate: 0.00134439
	LOSS [training: 0.03344223305186742 | validation: 0.013398267258672198]
	TIME [epoch: 8.76 sec]
EPOCH 465/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04826874018988694		[learning rate: 0.0013411]
		[batch 20/20] avg loss: 0.03265032940084941		[learning rate: 0.0013379]
	Learning Rate: 0.00133789
	LOSS [training: 0.040459534795368166 | validation: 0.006329448426072437]
	TIME [epoch: 8.76 sec]
EPOCH 466/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.045707068501592285		[learning rate: 0.0013347]
		[batch 20/20] avg loss: 0.07300237362527354		[learning rate: 0.0013314]
	Learning Rate: 0.00133142
	LOSS [training: 0.05935472106343291 | validation: 0.05137967051829005]
	TIME [epoch: 8.77 sec]
EPOCH 467/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.059523841086791464		[learning rate: 0.0013282]
		[batch 20/20] avg loss: 0.07719051513013328		[learning rate: 0.001325]
	Learning Rate: 0.00132498
	LOSS [training: 0.06835717810846238 | validation: 0.08790837339321413]
	TIME [epoch: 8.77 sec]
EPOCH 468/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05235989636264726		[learning rate: 0.0013218]
		[batch 20/20] avg loss: 0.06460027515105002		[learning rate: 0.0013186]
	Learning Rate: 0.00131858
	LOSS [training: 0.05848008575684864 | validation: 0.05880380082167591]
	TIME [epoch: 8.77 sec]
EPOCH 469/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04897442780511182		[learning rate: 0.0013154]
		[batch 20/20] avg loss: 0.04001510499577442		[learning rate: 0.0013122]
	Learning Rate: 0.0013122
	LOSS [training: 0.04449476640044312 | validation: 0.04543659201788018]
	TIME [epoch: 8.77 sec]
EPOCH 470/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0176968988502541		[learning rate: 0.001309]
		[batch 20/20] avg loss: 0.039165941759004286		[learning rate: 0.0013059]
	Learning Rate: 0.00130585
	LOSS [training: 0.02843142030462919 | validation: 0.01653174011812811]
	TIME [epoch: 8.76 sec]
EPOCH 471/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04543425899161648		[learning rate: 0.0013027]
		[batch 20/20] avg loss: 0.04365602503689862		[learning rate: 0.0012995]
	Learning Rate: 0.00129954
	LOSS [training: 0.04454514201425754 | validation: 0.016266960440482422]
	TIME [epoch: 8.76 sec]
EPOCH 472/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024790501422124703		[learning rate: 0.0012964]
		[batch 20/20] avg loss: 0.03717796584299511		[learning rate: 0.0012933]
	Learning Rate: 0.00129326
	LOSS [training: 0.030984233632559904 | validation: 0.09904529718987376]
	TIME [epoch: 8.76 sec]
EPOCH 473/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05154336754402474		[learning rate: 0.0012901]
		[batch 20/20] avg loss: 0.039562866406432105		[learning rate: 0.001287]
	Learning Rate: 0.001287
	LOSS [training: 0.04555311697522842 | validation: 0.009775833376473655]
	TIME [epoch: 8.78 sec]
EPOCH 474/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07991364127599501		[learning rate: 0.0012839]
		[batch 20/20] avg loss: 0.028733161112441397		[learning rate: 0.0012808]
	Learning Rate: 0.00128078
	LOSS [training: 0.05432340119421821 | validation: 0.008508320291374712]
	TIME [epoch: 8.77 sec]
EPOCH 475/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03423109880942159		[learning rate: 0.0012777]
		[batch 20/20] avg loss: 0.03699025469421479		[learning rate: 0.0012746]
	Learning Rate: 0.00127458
	LOSS [training: 0.03561067675181818 | validation: 0.060747476118154176]
	TIME [epoch: 8.75 sec]
EPOCH 476/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0415451696055541		[learning rate: 0.0012715]
		[batch 20/20] avg loss: 0.02725477332413555		[learning rate: 0.0012684]
	Learning Rate: 0.00126842
	LOSS [training: 0.034399971464844824 | validation: 0.006589011481975029]
	TIME [epoch: 8.75 sec]
EPOCH 477/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03089738164074792		[learning rate: 0.0012653]
		[batch 20/20] avg loss: 0.04920092213227799		[learning rate: 0.0012623]
	Learning Rate: 0.00126229
	LOSS [training: 0.04004915188651296 | validation: 0.01582129791647713]
	TIME [epoch: 8.75 sec]
EPOCH 478/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06597093353297934		[learning rate: 0.0012592]
		[batch 20/20] avg loss: 0.041387816350516984		[learning rate: 0.0012562]
	Learning Rate: 0.00125618
	LOSS [training: 0.05367937494174815 | validation: 0.014391481949200048]
	TIME [epoch: 8.76 sec]
EPOCH 479/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04465062418979916		[learning rate: 0.0012531]
		[batch 20/20] avg loss: 0.034884293233261934		[learning rate: 0.0012501]
	Learning Rate: 0.00125011
	LOSS [training: 0.03976745871153055 | validation: 0.0562463440459712]
	TIME [epoch: 8.78 sec]
EPOCH 480/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.035145268244240226		[learning rate: 0.0012471]
		[batch 20/20] avg loss: 0.08976315869774579		[learning rate: 0.0012441]
	Learning Rate: 0.00124406
	LOSS [training: 0.06245421347099302 | validation: 0.013490780039786954]
	TIME [epoch: 8.76 sec]
EPOCH 481/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0489289524319937		[learning rate: 0.0012411]
		[batch 20/20] avg loss: 0.04593564048209077		[learning rate: 0.001238]
	Learning Rate: 0.00123805
	LOSS [training: 0.047432296457042236 | validation: 0.0323360559427408]
	TIME [epoch: 8.74 sec]
EPOCH 482/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03740612649488651		[learning rate: 0.001235]
		[batch 20/20] avg loss: 0.06129765882409255		[learning rate: 0.0012321]
	Learning Rate: 0.00123206
	LOSS [training: 0.049351892659489534 | validation: 0.011450298483284127]
	TIME [epoch: 8.75 sec]
EPOCH 483/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030193995601966877		[learning rate: 0.0012291]
		[batch 20/20] avg loss: 0.06624207855899614		[learning rate: 0.0012261]
	Learning Rate: 0.0012261
	LOSS [training: 0.04821803708048151 | validation: 0.08272090378887328]
	TIME [epoch: 8.74 sec]
EPOCH 484/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03248252153687645		[learning rate: 0.0012231]
		[batch 20/20] avg loss: 0.0433850158383153		[learning rate: 0.0012202]
	Learning Rate: 0.00122017
	LOSS [training: 0.037933768687595876 | validation: 0.03910455691347425]
	TIME [epoch: 8.78 sec]
EPOCH 485/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028216844294832018		[learning rate: 0.0012172]
		[batch 20/20] avg loss: 0.04204360602616409		[learning rate: 0.0012143]
	Learning Rate: 0.00121427
	LOSS [training: 0.03513022516049806 | validation: 0.02803600651040413]
	TIME [epoch: 8.76 sec]
EPOCH 486/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02323104046860496		[learning rate: 0.0012113]
		[batch 20/20] avg loss: 0.05583702069853822		[learning rate: 0.0012084]
	Learning Rate: 0.0012084
	LOSS [training: 0.03953403058357159 | validation: 0.04777641033343826]
	TIME [epoch: 8.75 sec]
EPOCH 487/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03489592256127618		[learning rate: 0.0012055]
		[batch 20/20] avg loss: 0.03786207029078896		[learning rate: 0.0012026]
	Learning Rate: 0.00120256
	LOSS [training: 0.03637899642603258 | validation: 0.010671440262553776]
	TIME [epoch: 8.76 sec]
EPOCH 488/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028165855133844224		[learning rate: 0.0011996]
		[batch 20/20] avg loss: 0.04550945425069674		[learning rate: 0.0011967]
	Learning Rate: 0.00119674
	LOSS [training: 0.03683765469227048 | validation: 0.07217280454322457]
	TIME [epoch: 8.75 sec]
EPOCH 489/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.057001239141077166		[learning rate: 0.0011938]
		[batch 20/20] avg loss: 0.03055987007184784		[learning rate: 0.001191]
	Learning Rate: 0.00119095
	LOSS [training: 0.043780554606462505 | validation: 0.06898533320322431]
	TIME [epoch: 8.78 sec]
EPOCH 490/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04743519101077831		[learning rate: 0.0011881]
		[batch 20/20] avg loss: 0.03399989604064771		[learning rate: 0.0011852]
	Learning Rate: 0.00118519
	LOSS [training: 0.04071754352571301 | validation: 0.03126186619727928]
	TIME [epoch: 8.76 sec]
EPOCH 491/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05134690642033078		[learning rate: 0.0011823]
		[batch 20/20] avg loss: 0.03678612961064751		[learning rate: 0.0011795]
	Learning Rate: 0.00117946
	LOSS [training: 0.04406651801548916 | validation: 0.01964503666881567]
	TIME [epoch: 8.76 sec]
EPOCH 492/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030206226587132456		[learning rate: 0.0011766]
		[batch 20/20] avg loss: 0.03107044555671497		[learning rate: 0.0011738]
	Learning Rate: 0.00117376
	LOSS [training: 0.030638336071923712 | validation: 0.01613856389268547]
	TIME [epoch: 8.77 sec]
EPOCH 493/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05797688451914755		[learning rate: 0.0011709]
		[batch 20/20] avg loss: 0.021936223589153137		[learning rate: 0.0011681]
	Learning Rate: 0.00116808
	LOSS [training: 0.03995655405415034 | validation: 0.008224290701561644]
	TIME [epoch: 8.76 sec]
EPOCH 494/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.019307971976609363		[learning rate: 0.0011653]
		[batch 20/20] avg loss: 0.015124426791608617		[learning rate: 0.0011624]
	Learning Rate: 0.00116243
	LOSS [training: 0.01721619938410899 | validation: 0.009127703344331377]
	TIME [epoch: 8.78 sec]
EPOCH 495/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0432907108937632		[learning rate: 0.0011596]
		[batch 20/20] avg loss: 0.04834378457101411		[learning rate: 0.0011568]
	Learning Rate: 0.00115681
	LOSS [training: 0.045817247732388655 | validation: 0.02540825163940804]
	TIME [epoch: 8.76 sec]
EPOCH 496/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.050620005404018585		[learning rate: 0.001154]
		[batch 20/20] avg loss: 0.026472533520098797		[learning rate: 0.0011512]
	Learning Rate: 0.00115122
	LOSS [training: 0.03854626946205869 | validation: 0.01632693223986813]
	TIME [epoch: 8.76 sec]
EPOCH 497/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03944216456026994		[learning rate: 0.0011484]
		[batch 20/20] avg loss: 0.04108519949451376		[learning rate: 0.0011457]
	Learning Rate: 0.00114565
	LOSS [training: 0.04026368202739185 | validation: -0.0001325908957750296]
	TIME [epoch: 8.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_497.pth
	Model improved!!!
EPOCH 498/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02410565173884379		[learning rate: 0.0011429]
		[batch 20/20] avg loss: 0.038562798916865385		[learning rate: 0.0011401]
	Learning Rate: 0.00114011
	LOSS [training: 0.031334225327854584 | validation: 0.003272912315426451]
	TIME [epoch: 8.75 sec]
EPOCH 499/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02540459738556754		[learning rate: 0.0011374]
		[batch 20/20] avg loss: 0.027013795011292707		[learning rate: 0.0011346]
	Learning Rate: 0.0011346
	LOSS [training: 0.026209196198430124 | validation: 0.008575553124575114]
	TIME [epoch: 8.76 sec]
EPOCH 500/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05460240030681128		[learning rate: 0.0011319]
		[batch 20/20] avg loss: 0.03218691774884413		[learning rate: 0.0011291]
	Learning Rate: 0.00112911
	LOSS [training: 0.0433946590278277 | validation: 0.011895680035303854]
	TIME [epoch: 8.76 sec]
EPOCH 501/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026018491550353563		[learning rate: 0.0011264]
		[batch 20/20] avg loss: 0.05111898436461534		[learning rate: 0.0011237]
	Learning Rate: 0.00112365
	LOSS [training: 0.038568737957484445 | validation: 0.012167452756868598]
	TIME [epoch: 8.74 sec]
EPOCH 502/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03682088529551737		[learning rate: 0.0011209]
		[batch 20/20] avg loss: 0.030105588081387448		[learning rate: 0.0011182]
	Learning Rate: 0.00111822
	LOSS [training: 0.03346323668845241 | validation: 0.15751878788377624]
	TIME [epoch: 8.74 sec]
EPOCH 503/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0775710388312906		[learning rate: 0.0011155]
		[batch 20/20] avg loss: 0.016006970435086892		[learning rate: 0.0011128]
	Learning Rate: 0.00111281
	LOSS [training: 0.046789004633188754 | validation: 0.01065402040265545]
	TIME [epoch: 8.74 sec]
EPOCH 504/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.043367804614501466		[learning rate: 0.0011101]
		[batch 20/20] avg loss: 0.03686492234692801		[learning rate: 0.0011074]
	Learning Rate: 0.00110743
	LOSS [training: 0.04011636348071474 | validation: 0.00921059473159031]
	TIME [epoch: 8.76 sec]
EPOCH 505/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02627129442578006		[learning rate: 0.0011047]
		[batch 20/20] avg loss: 0.0466925699908136		[learning rate: 0.0011021]
	Learning Rate: 0.00110207
	LOSS [training: 0.03648193220829683 | validation: 0.01606395007839857]
	TIME [epoch: 8.76 sec]
EPOCH 506/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.029981118131362137		[learning rate: 0.0010994]
		[batch 20/20] avg loss: 0.02873653034202151		[learning rate: 0.0010967]
	Learning Rate: 0.00109674
	LOSS [training: 0.02935882423669183 | validation: 0.023388996984604664]
	TIME [epoch: 8.75 sec]
EPOCH 507/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03629590345417087		[learning rate: 0.0010941]
		[batch 20/20] avg loss: 0.027509589253060646		[learning rate: 0.0010914]
	Learning Rate: 0.00109144
	LOSS [training: 0.03190274635361576 | validation: 0.030699615896827105]
	TIME [epoch: 8.75 sec]
EPOCH 508/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.039800223016154865		[learning rate: 0.0010888]
		[batch 20/20] avg loss: 0.056375476413687085		[learning rate: 0.0010862]
	Learning Rate: 0.00108616
	LOSS [training: 0.04808784971492098 | validation: -0.001709259249531212]
	TIME [epoch: 8.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_508.pth
	Model improved!!!
EPOCH 509/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02109042619182383		[learning rate: 0.0010835]
		[batch 20/20] avg loss: 0.037330240968470624		[learning rate: 0.0010809]
	Learning Rate: 0.00108091
	LOSS [training: 0.02921033358014722 | validation: 0.06663431654818014]
	TIME [epoch: 8.76 sec]
EPOCH 510/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.032706606214302145		[learning rate: 0.0010783]
		[batch 20/20] avg loss: 0.037244558433462846		[learning rate: 0.0010757]
	Learning Rate: 0.00107568
	LOSS [training: 0.03497558232388249 | validation: 0.010250138558728774]
	TIME [epoch: 8.77 sec]
EPOCH 511/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02985079643712347		[learning rate: 0.0010731]
		[batch 20/20] avg loss: 0.0421758799118674		[learning rate: 0.0010705]
	Learning Rate: 0.00107048
	LOSS [training: 0.03601333817449544 | validation: 0.11661487620898109]
	TIME [epoch: 8.75 sec]
EPOCH 512/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.042032438765114394		[learning rate: 0.0010679]
		[batch 20/20] avg loss: 0.031391882855603494		[learning rate: 0.0010653]
	Learning Rate: 0.0010653
	LOSS [training: 0.03671216081035894 | validation: 0.0030025358036063843]
	TIME [epoch: 8.75 sec]
EPOCH 513/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022331447175120114		[learning rate: 0.0010627]
		[batch 20/20] avg loss: 0.031962945363679114		[learning rate: 0.0010602]
	Learning Rate: 0.00106015
	LOSS [training: 0.027147196269399616 | validation: 0.0002082534849738593]
	TIME [epoch: 8.76 sec]
EPOCH 514/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022764578515685568		[learning rate: 0.0010576]
		[batch 20/20] avg loss: 0.029249147422903958		[learning rate: 0.001055]
	Learning Rate: 0.00105503
	LOSS [training: 0.026006862969294768 | validation: 0.042179744462102084]
	TIME [epoch: 8.75 sec]
EPOCH 515/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06681443229400581		[learning rate: 0.0010525]
		[batch 20/20] avg loss: 0.08206648580624615		[learning rate: 0.0010499]
	Learning Rate: 0.00104992
	LOSS [training: 0.07444045905012599 | validation: 0.032344508325010346]
	TIME [epoch: 8.78 sec]
EPOCH 516/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06940344654374653		[learning rate: 0.0010474]
		[batch 20/20] avg loss: 0.027733400875087078		[learning rate: 0.0010448]
	Learning Rate: 0.00104485
	LOSS [training: 0.04856842370941681 | validation: 0.003670084369689352]
	TIME [epoch: 8.76 sec]
EPOCH 517/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023835892340617233		[learning rate: 0.0010423]
		[batch 20/20] avg loss: 0.04597869969875075		[learning rate: 0.0010398]
	Learning Rate: 0.00103979
	LOSS [training: 0.034907296019683995 | validation: 0.0030407771255599758]
	TIME [epoch: 8.76 sec]
EPOCH 518/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03321020096443107		[learning rate: 0.0010373]
		[batch 20/20] avg loss: 0.03245146565619149		[learning rate: 0.0010348]
	Learning Rate: 0.00103477
	LOSS [training: 0.03283083331031129 | validation: 0.0022703785441648334]
	TIME [epoch: 8.75 sec]
EPOCH 519/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023500392440469635		[learning rate: 0.0010323]
		[batch 20/20] avg loss: 0.039680691734638576		[learning rate: 0.0010298]
	Learning Rate: 0.00102976
	LOSS [training: 0.03159054208755411 | validation: -0.0027577514704987707]
	TIME [epoch: 8.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_519.pth
	Model improved!!!
EPOCH 520/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04445958891498665		[learning rate: 0.0010273]
		[batch 20/20] avg loss: 0.02928824670804563		[learning rate: 0.0010248]
	Learning Rate: 0.00102478
	LOSS [training: 0.036873917811516145 | validation: 0.028137105075588257]
	TIME [epoch: 8.79 sec]
EPOCH 521/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024312780658370295		[learning rate: 0.0010223]
		[batch 20/20] avg loss: 0.029380383965689927		[learning rate: 0.0010198]
	Learning Rate: 0.00101983
	LOSS [training: 0.02684658231203011 | validation: 0.14510366941222774]
	TIME [epoch: 8.76 sec]
EPOCH 522/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05554528520045269		[learning rate: 0.0010174]
		[batch 20/20] avg loss: 0.04013398795946638		[learning rate: 0.0010149]
	Learning Rate: 0.00101489
	LOSS [training: 0.04783963657995953 | validation: 0.05923231363863755]
	TIME [epoch: 8.75 sec]
EPOCH 523/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04403096371127084		[learning rate: 0.0010124]
		[batch 20/20] avg loss: 0.03395773417986607		[learning rate: 0.00101]
	Learning Rate: 0.00100999
	LOSS [training: 0.038994348945568455 | validation: 0.016519784117165964]
	TIME [epoch: 8.75 sec]
EPOCH 524/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03412869177750586		[learning rate: 0.0010075]
		[batch 20/20] avg loss: 0.03598028659782991		[learning rate: 0.0010051]
	Learning Rate: 0.0010051
	LOSS [training: 0.03505448918766788 | validation: 0.02893062559757655]
	TIME [epoch: 8.75 sec]
EPOCH 525/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04126244103886109		[learning rate: 0.0010027]
		[batch 20/20] avg loss: 0.030891419801201296		[learning rate: 0.0010002]
	Learning Rate: 0.00100024
	LOSS [training: 0.03607693042003119 | validation: 0.002899029469364193]
	TIME [epoch: 8.77 sec]
EPOCH 526/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.020697343447901088		[learning rate: 0.00099782]
		[batch 20/20] avg loss: 0.0576093809322354		[learning rate: 0.00099541]
	Learning Rate: 0.000995405
	LOSS [training: 0.03915336219006825 | validation: -0.0012400868425080985]
	TIME [epoch: 8.76 sec]
EPOCH 527/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02922891138619112		[learning rate: 0.000993]
		[batch 20/20] avg loss: 0.024254042475177123		[learning rate: 0.00099059]
	Learning Rate: 0.000990592
	LOSS [training: 0.026741476930684117 | validation: 0.014664612108660081]
	TIME [epoch: 8.75 sec]
EPOCH 528/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.020926225086161838		[learning rate: 0.00098819]
		[batch 20/20] avg loss: 0.026286545281323386		[learning rate: 0.0009858]
	Learning Rate: 0.000985801
	LOSS [training: 0.023606385183742612 | validation: 0.007982743274555282]
	TIME [epoch: 8.76 sec]
EPOCH 529/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02417800298122979		[learning rate: 0.00098341]
		[batch 20/20] avg loss: 0.037379579142030765		[learning rate: 0.00098103]
	Learning Rate: 0.000981034
	LOSS [training: 0.030778791061630277 | validation: 0.0037207523829136356]
	TIME [epoch: 8.75 sec]
EPOCH 530/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024837813429454698		[learning rate: 0.00097866]
		[batch 20/20] avg loss: 0.026466239803065415		[learning rate: 0.00097629]
	Learning Rate: 0.00097629
	LOSS [training: 0.025652026616260058 | validation: 0.06243343392355355]
	TIME [epoch: 8.76 sec]
EPOCH 531/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030992500279904533		[learning rate: 0.00097393]
		[batch 20/20] avg loss: 0.022586807380373355		[learning rate: 0.00097157]
	Learning Rate: 0.000971569
	LOSS [training: 0.026789653830138944 | validation: 0.01301172528225482]
	TIME [epoch: 8.77 sec]
EPOCH 532/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025627012253866564		[learning rate: 0.00096922]
		[batch 20/20] avg loss: 0.02793204276017177		[learning rate: 0.00096687]
	Learning Rate: 0.000966871
	LOSS [training: 0.026779527507019167 | validation: 0.01013688636782045]
	TIME [epoch: 8.75 sec]
EPOCH 533/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028721934495645102		[learning rate: 0.00096453]
		[batch 20/20] avg loss: 0.022143805185381914		[learning rate: 0.00096219]
	Learning Rate: 0.000962195
	LOSS [training: 0.02543286984051351 | validation: 0.039529712061645295]
	TIME [epoch: 8.76 sec]
EPOCH 534/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03841031700393671		[learning rate: 0.00095987]
		[batch 20/20] avg loss: 0.038050708994210314		[learning rate: 0.00095754]
	Learning Rate: 0.000957542
	LOSS [training: 0.038230512999073506 | validation: 0.02311003283315829]
	TIME [epoch: 8.76 sec]
EPOCH 535/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04205091453237512		[learning rate: 0.00095522]
		[batch 20/20] avg loss: 0.03906944506061159		[learning rate: 0.00095291]
	Learning Rate: 0.000952912
	LOSS [training: 0.04056017979649336 | validation: 0.0175633441855955]
	TIME [epoch: 8.78 sec]
EPOCH 536/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02952818038968285		[learning rate: 0.0009506]
		[batch 20/20] avg loss: 0.04050991991250098		[learning rate: 0.0009483]
	Learning Rate: 0.000948304
	LOSS [training: 0.03501905015109192 | validation: -0.00010706611585592843]
	TIME [epoch: 8.78 sec]
EPOCH 537/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02582968630914124		[learning rate: 0.00094601]
		[batch 20/20] avg loss: 0.035712932744926124		[learning rate: 0.00094372]
	Learning Rate: 0.000943718
	LOSS [training: 0.03077130952703368 | validation: 0.00793952967860529]
	TIME [epoch: 8.75 sec]
EPOCH 538/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02937203118240155		[learning rate: 0.00094143]
		[batch 20/20] avg loss: 0.039431381242715766		[learning rate: 0.00093915]
	Learning Rate: 0.000939154
	LOSS [training: 0.03440170621255866 | validation: 0.044518094179723536]
	TIME [epoch: 8.75 sec]
EPOCH 539/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04119730279498733		[learning rate: 0.00093688]
		[batch 20/20] avg loss: 0.03474400606696806		[learning rate: 0.00093461]
	Learning Rate: 0.000934613
	LOSS [training: 0.0379706544309777 | validation: -0.0025210970575819665]
	TIME [epoch: 8.75 sec]
EPOCH 540/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.052155899437683485		[learning rate: 0.00093235]
		[batch 20/20] avg loss: 0.05186858492955859		[learning rate: 0.00093009]
	Learning Rate: 0.000930093
	LOSS [training: 0.052012242183621035 | validation: 0.07352456997580847]
	TIME [epoch: 8.75 sec]
EPOCH 541/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.032064244000808		[learning rate: 0.00092784]
		[batch 20/20] avg loss: 0.03928250444775465		[learning rate: 0.0009256]
	Learning Rate: 0.000925595
	LOSS [training: 0.03567337422428132 | validation: 0.004893163446511762]
	TIME [epoch: 8.78 sec]
EPOCH 542/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.013101540908543171		[learning rate: 0.00092335]
		[batch 20/20] avg loss: 0.03292613172719396		[learning rate: 0.00092112]
	Learning Rate: 0.000921119
	LOSS [training: 0.023013836317868568 | validation: 0.014719853431236399]
	TIME [epoch: 8.76 sec]
EPOCH 543/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.027895872482329515		[learning rate: 0.00091889]
		[batch 20/20] avg loss: 0.0444330870405733		[learning rate: 0.00091666]
	Learning Rate: 0.000916665
	LOSS [training: 0.03616447976145141 | validation: 0.007216624943060471]
	TIME [epoch: 8.75 sec]
EPOCH 544/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024590031356006924		[learning rate: 0.00091445]
		[batch 20/20] avg loss: 0.029823312300765577		[learning rate: 0.00091223]
	Learning Rate: 0.000912232
	LOSS [training: 0.027206671828386254 | validation: 0.02898271904669922]
	TIME [epoch: 8.74 sec]
EPOCH 545/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028858997201373075		[learning rate: 0.00091002]
		[batch 20/20] avg loss: 0.024557051700291106		[learning rate: 0.00090782]
	Learning Rate: 0.000907821
	LOSS [training: 0.02670802445083209 | validation: 0.021020077237965014]
	TIME [epoch: 8.75 sec]
EPOCH 546/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03628987947808017		[learning rate: 0.00090562]
		[batch 20/20] avg loss: 0.035073740509769545		[learning rate: 0.00090343]
	Learning Rate: 0.00090343
	LOSS [training: 0.03568180999392486 | validation: 0.04464709231679626]
	TIME [epoch: 8.77 sec]
EPOCH 547/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02594677823527374		[learning rate: 0.00090124]
		[batch 20/20] avg loss: 0.023382870483405985		[learning rate: 0.00089906]
	Learning Rate: 0.000899062
	LOSS [training: 0.02466482435933986 | validation: 0.019996872825621632]
	TIME [epoch: 8.76 sec]
EPOCH 548/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022781062522464462		[learning rate: 0.00089689]
		[batch 20/20] avg loss: 0.02993400810456206		[learning rate: 0.00089471]
	Learning Rate: 0.000894714
	LOSS [training: 0.02635753531351326 | validation: 0.023229768989304754]
	TIME [epoch: 8.76 sec]
EPOCH 549/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.018706573368374734		[learning rate: 0.00089255]
		[batch 20/20] avg loss: 0.03599259768823309		[learning rate: 0.00089039]
	Learning Rate: 0.000890387
	LOSS [training: 0.027349585528303917 | validation: 0.1491135483140385]
	TIME [epoch: 8.75 sec]
EPOCH 550/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.051553801284824785		[learning rate: 0.00088823]
		[batch 20/20] avg loss: 0.020199066875423253		[learning rate: 0.00088608]
	Learning Rate: 0.000886081
	LOSS [training: 0.035876434080124024 | validation: 0.10237304892666473]
	TIME [epoch: 8.75 sec]
EPOCH 551/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0505412610823748		[learning rate: 0.00088394]
		[batch 20/20] avg loss: 0.03214803866927139		[learning rate: 0.0008818]
	Learning Rate: 0.000881797
	LOSS [training: 0.041344649875823106 | validation: 0.009176289292077588]
	TIME [epoch: 8.77 sec]
EPOCH 552/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0285852709178916		[learning rate: 0.00087966]
		[batch 20/20] avg loss: 0.02753085482010741		[learning rate: 0.00087753]
	Learning Rate: 0.000877532
	LOSS [training: 0.0280580628689995 | validation: 0.01785767987786387]
	TIME [epoch: 8.76 sec]
EPOCH 553/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03470401809728682		[learning rate: 0.00087541]
		[batch 20/20] avg loss: 0.030062704641337527		[learning rate: 0.00087329]
	Learning Rate: 0.000873289
	LOSS [training: 0.03238336136931218 | validation: 0.03229586272031864]
	TIME [epoch: 8.76 sec]
EPOCH 554/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.021443417559609705		[learning rate: 0.00087117]
		[batch 20/20] avg loss: 0.0315334826430386		[learning rate: 0.00086907]
	Learning Rate: 0.000869066
	LOSS [training: 0.026488450101324158 | validation: 0.03984361302814449]
	TIME [epoch: 8.76 sec]
EPOCH 555/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.033664513087329394		[learning rate: 0.00086696]
		[batch 20/20] avg loss: 0.02836592680923534		[learning rate: 0.00086486]
	Learning Rate: 0.000864863
	LOSS [training: 0.03101521994828237 | validation: 0.010649093343226453]
	TIME [epoch: 8.76 sec]
EPOCH 556/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03525933299482699		[learning rate: 0.00086277]
		[batch 20/20] avg loss: 0.030335905938879266		[learning rate: 0.00086068]
	Learning Rate: 0.000860681
	LOSS [training: 0.03279761946685312 | validation: 0.004457162406010899]
	TIME [epoch: 8.77 sec]
EPOCH 557/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03675141518427798		[learning rate: 0.0008586]
		[batch 20/20] avg loss: 0.03177485221160878		[learning rate: 0.00085652]
	Learning Rate: 0.000856519
	LOSS [training: 0.034263133697943385 | validation: 0.005958583837541423]
	TIME [epoch: 8.77 sec]
EPOCH 558/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.020269481921681886		[learning rate: 0.00085445]
		[batch 20/20] avg loss: 0.030439304618250795		[learning rate: 0.00085238]
	Learning Rate: 0.000852377
	LOSS [training: 0.025354393269966337 | validation: 0.02887688260267338]
	TIME [epoch: 8.75 sec]
EPOCH 559/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0348830571206796		[learning rate: 0.00085031]
		[batch 20/20] avg loss: 0.04124354589225969		[learning rate: 0.00084825]
	Learning Rate: 0.000848255
	LOSS [training: 0.038063301506469645 | validation: 0.0010476172360454304]
	TIME [epoch: 8.76 sec]
EPOCH 560/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02268295883200578		[learning rate: 0.0008462]
		[batch 20/20] avg loss: 0.052653834703555016		[learning rate: 0.00084415]
	Learning Rate: 0.000844153
	LOSS [training: 0.0376683967677804 | validation: 0.08359464864047798]
	TIME [epoch: 8.77 sec]
EPOCH 561/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.037750197617994044		[learning rate: 0.00084211]
		[batch 20/20] avg loss: 0.038836322528575384		[learning rate: 0.00084007]
	Learning Rate: 0.000840071
	LOSS [training: 0.03829326007328471 | validation: 0.00502339884659644]
	TIME [epoch: 8.77 sec]
EPOCH 562/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.034061805006024265		[learning rate: 0.00083804]
		[batch 20/20] avg loss: 0.020773497942919127		[learning rate: 0.00083601]
	Learning Rate: 0.000836008
	LOSS [training: 0.027417651474471692 | validation: 0.014631644609394288]
	TIME [epoch: 8.78 sec]
EPOCH 563/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.058452019769827626		[learning rate: 0.00083398]
		[batch 20/20] avg loss: 0.02949953855739369		[learning rate: 0.00083197]
	Learning Rate: 0.000831965
	LOSS [training: 0.04397577916361065 | validation: 0.003564803479700042]
	TIME [epoch: 8.76 sec]
EPOCH 564/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.019874819430412676		[learning rate: 0.00082995]
		[batch 20/20] avg loss: 0.039891742946676		[learning rate: 0.00082794]
	Learning Rate: 0.000827942
	LOSS [training: 0.02988328118854433 | validation: 0.010734475567074544]
	TIME [epoch: 8.76 sec]
EPOCH 565/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.020176453053160584		[learning rate: 0.00082594]
		[batch 20/20] avg loss: 0.04922645663619707		[learning rate: 0.00082394]
	Learning Rate: 0.000823938
	LOSS [training: 0.03470145484467882 | validation: 0.013701303582687592]
	TIME [epoch: 8.76 sec]
EPOCH 566/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.015552290238794106		[learning rate: 0.00082194]
		[batch 20/20] avg loss: 0.02140857743066522		[learning rate: 0.00081995]
	Learning Rate: 0.000819954
	LOSS [training: 0.018480433834729665 | validation: 0.023963533842697355]
	TIME [epoch: 8.76 sec]
EPOCH 567/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03512768733554274		[learning rate: 0.00081797]
		[batch 20/20] avg loss: 0.041861469741709886		[learning rate: 0.00081599]
	Learning Rate: 0.000815989
	LOSS [training: 0.0384945785386263 | validation: 0.03966139457768089]
	TIME [epoch: 8.79 sec]
EPOCH 568/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03876044780331368		[learning rate: 0.00081401]
		[batch 20/20] avg loss: 0.03698167395054526		[learning rate: 0.00081204]
	Learning Rate: 0.000812043
	LOSS [training: 0.037871060876929465 | validation: 0.04012120612447355]
	TIME [epoch: 8.75 sec]
EPOCH 569/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.05622485198167426		[learning rate: 0.00081008]
		[batch 20/20] avg loss: 0.02761642494737146		[learning rate: 0.00080812]
	Learning Rate: 0.000808116
	LOSS [training: 0.041920638464522854 | validation: 0.02823664415968575]
	TIME [epoch: 8.76 sec]
EPOCH 570/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04427301821144138		[learning rate: 0.00080616]
		[batch 20/20] avg loss: 0.052401881195255406		[learning rate: 0.00080421]
	Learning Rate: 0.000804208
	LOSS [training: 0.048337449703348405 | validation: 0.054063894238598906]
	TIME [epoch: 8.75 sec]
EPOCH 571/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03413559475216496		[learning rate: 0.00080226]
		[batch 20/20] avg loss: 0.0172129709755391		[learning rate: 0.00080032]
	Learning Rate: 0.000800319
	LOSS [training: 0.02567428286385203 | validation: 0.0315550560492197]
	TIME [epoch: 8.76 sec]
EPOCH 572/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025557178282349387		[learning rate: 0.00079838]
		[batch 20/20] avg loss: 0.028623166419446962		[learning rate: 0.00079645]
	Learning Rate: 0.000796449
	LOSS [training: 0.027090172350898167 | validation: -0.00026227805306901697]
	TIME [epoch: 8.78 sec]
EPOCH 573/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026992326898596992		[learning rate: 0.00079452]
		[batch 20/20] avg loss: 0.0136934870180003		[learning rate: 0.0007926]
	Learning Rate: 0.000792597
	LOSS [training: 0.020342906958298646 | validation: 0.028108560401267146]
	TIME [epoch: 8.77 sec]
EPOCH 574/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03324858773048632		[learning rate: 0.00079068]
		[batch 20/20] avg loss: 0.0177455354522266		[learning rate: 0.00078876]
	Learning Rate: 0.000788765
	LOSS [training: 0.025497061591356462 | validation: 0.034809707124921055]
	TIME [epoch: 8.77 sec]
EPOCH 575/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.035185082509524145		[learning rate: 0.00078686]
		[batch 20/20] avg loss: 0.01958121975255237		[learning rate: 0.00078495]
	Learning Rate: 0.00078495
	LOSS [training: 0.02738315113103825 | validation: 0.009958344542164795]
	TIME [epoch: 8.77 sec]
EPOCH 576/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03682477681678033		[learning rate: 0.00078305]
		[batch 20/20] avg loss: 0.027766430959768613		[learning rate: 0.00078115]
	Learning Rate: 0.000781154
	LOSS [training: 0.032295603888274474 | validation: 0.0020013484430067424]
	TIME [epoch: 8.76 sec]
EPOCH 577/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010416877790416763		[learning rate: 0.00077926]
		[batch 20/20] avg loss: 0.02519599282820616		[learning rate: 0.00077738]
	Learning Rate: 0.000777377
	LOSS [training: 0.01780643530931146 | validation: 0.008625456263240574]
	TIME [epoch: 8.78 sec]
EPOCH 578/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02278060128753449		[learning rate: 0.00077549]
		[batch 20/20] avg loss: 0.03063189701184636		[learning rate: 0.00077362]
	Learning Rate: 0.000773618
	LOSS [training: 0.02670624914969043 | validation: 0.00334152994616196]
	TIME [epoch: 8.76 sec]
EPOCH 579/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.036981255790123804		[learning rate: 0.00077174]
		[batch 20/20] avg loss: 0.031573273070328624		[learning rate: 0.00076988]
	Learning Rate: 0.000769877
	LOSS [training: 0.03427726443022622 | validation: -0.0011113938432798806]
	TIME [epoch: 8.77 sec]
EPOCH 580/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024122877102382936		[learning rate: 0.00076801]
		[batch 20/20] avg loss: 0.03126032061898083		[learning rate: 0.00076615]
	Learning Rate: 0.000766154
	LOSS [training: 0.027691598860681888 | validation: 0.03631261530304451]
	TIME [epoch: 8.75 sec]
EPOCH 581/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.038076650790197605		[learning rate: 0.0007643]
		[batch 20/20] avg loss: 0.024705241729989252		[learning rate: 0.00076245]
	Learning Rate: 0.000762448
	LOSS [training: 0.031390946260093436 | validation: 0.014249535365405583]
	TIME [epoch: 8.77 sec]
EPOCH 582/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.047916682313806365		[learning rate: 0.0007606]
		[batch 20/20] avg loss: 0.03638225306015998		[learning rate: 0.00075876]
	Learning Rate: 0.000758761
	LOSS [training: 0.04214946768698317 | validation: 0.05991211499002817]
	TIME [epoch: 8.76 sec]
EPOCH 583/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02116141261342392		[learning rate: 0.00075692]
		[batch 20/20] avg loss: 0.02222763106003674		[learning rate: 0.00075509]
	Learning Rate: 0.000755092
	LOSS [training: 0.021694521836730332 | validation: -0.0012416942893164484]
	TIME [epoch: 8.77 sec]
EPOCH 584/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01957842856946466		[learning rate: 0.00075326]
		[batch 20/20] avg loss: 0.016651399881612683		[learning rate: 0.00075144]
	Learning Rate: 0.000751441
	LOSS [training: 0.018114914225538667 | validation: 0.005933529276531536]
	TIME [epoch: 8.76 sec]
EPOCH 585/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02517684963319407		[learning rate: 0.00074962]
		[batch 20/20] avg loss: 0.028199068067743683		[learning rate: 0.00074781]
	Learning Rate: 0.000747807
	LOSS [training: 0.026687958850468874 | validation: 0.04020604902162157]
	TIME [epoch: 8.75 sec]
EPOCH 586/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024981433397788473		[learning rate: 0.000746]
		[batch 20/20] avg loss: 0.026952844642635993		[learning rate: 0.00074419]
	Learning Rate: 0.000744191
	LOSS [training: 0.02596713902021224 | validation: 0.025583211565320535]
	TIME [epoch: 8.75 sec]
EPOCH 587/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026237439120746598		[learning rate: 0.00074239]
		[batch 20/20] avg loss: 0.015458117680585432		[learning rate: 0.00074059]
	Learning Rate: 0.000740592
	LOSS [training: 0.02084777840066601 | validation: 0.01117180041263667]
	TIME [epoch: 8.76 sec]
EPOCH 588/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0338133646211468		[learning rate: 0.0007388]
		[batch 20/20] avg loss: 0.04364517701525276		[learning rate: 0.00073701]
	Learning Rate: 0.000737011
	LOSS [training: 0.038729270818199775 | validation: 0.00643793390851082]
	TIME [epoch: 8.78 sec]
EPOCH 589/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.039634817823096684		[learning rate: 0.00073523]
		[batch 20/20] avg loss: 0.03562535161023285		[learning rate: 0.00073345]
	Learning Rate: 0.000733446
	LOSS [training: 0.03763008471666477 | validation: 0.02059292452026708]
	TIME [epoch: 8.76 sec]
EPOCH 590/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03615163243346485		[learning rate: 0.00073167]
		[batch 20/20] avg loss: 0.01961239494442401		[learning rate: 0.0007299]
	Learning Rate: 0.0007299
	LOSS [training: 0.027882013688944423 | validation: 0.012701197318322964]
	TIME [epoch: 8.75 sec]
EPOCH 591/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03943814837135223		[learning rate: 0.00072813]
		[batch 20/20] avg loss: 0.012085600646090915		[learning rate: 0.00072637]
	Learning Rate: 0.00072637
	LOSS [training: 0.025761874508721576 | validation: 0.009455968390425666]
	TIME [epoch: 8.76 sec]
EPOCH 592/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.016291063539021867		[learning rate: 0.00072461]
		[batch 20/20] avg loss: 0.020674803996988546		[learning rate: 0.00072286]
	Learning Rate: 0.000722857
	LOSS [training: 0.018482933768005205 | validation: 0.0060459165328355415]
	TIME [epoch: 8.76 sec]
EPOCH 593/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.018415470877780078		[learning rate: 0.00072111]
		[batch 20/20] avg loss: 0.041829176944355825		[learning rate: 0.00071936]
	Learning Rate: 0.000719362
	LOSS [training: 0.030122323911067957 | validation: -5.709873705469744e-06]
	TIME [epoch: 8.77 sec]
EPOCH 594/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01869201410431531		[learning rate: 0.00071762]
		[batch 20/20] avg loss: 0.03024693365293002		[learning rate: 0.00071588]
	Learning Rate: 0.000715883
	LOSS [training: 0.024469473878622666 | validation: 0.023437164103400264]
	TIME [epoch: 8.75 sec]
EPOCH 595/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.029229325611659935		[learning rate: 0.00071415]
		[batch 20/20] avg loss: 0.040075893436177165		[learning rate: 0.00071242]
	Learning Rate: 0.000712421
	LOSS [training: 0.03465260952391854 | validation: 0.004529591245913496]
	TIME [epoch: 8.76 sec]
EPOCH 596/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.013872752093521049		[learning rate: 0.0007107]
		[batch 20/20] avg loss: 0.017197134163825498		[learning rate: 0.00070898]
	Learning Rate: 0.000708976
	LOSS [training: 0.015534943128673272 | validation: 0.04502368715843848]
	TIME [epoch: 8.76 sec]
EPOCH 597/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03175838156143908		[learning rate: 0.00070726]
		[batch 20/20] avg loss: 0.01637867609717157		[learning rate: 0.00070555]
	Learning Rate: 0.000705548
	LOSS [training: 0.024068528829305325 | validation: 0.013313230400685773]
	TIME [epoch: 8.76 sec]
EPOCH 598/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.019089265715736575		[learning rate: 0.00070384]
		[batch 20/20] avg loss: 0.022339706339536892		[learning rate: 0.00070214]
	Learning Rate: 0.000702136
	LOSS [training: 0.020714486027636737 | validation: 0.04460468154912379]
	TIME [epoch: 8.79 sec]
EPOCH 599/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026744100430045176		[learning rate: 0.00070044]
		[batch 20/20] avg loss: 0.022224175225172404		[learning rate: 0.00069874]
	Learning Rate: 0.00069874
	LOSS [training: 0.024484137827608787 | validation: -0.0019297498117034394]
	TIME [epoch: 8.75 sec]
EPOCH 600/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024721997679231333		[learning rate: 0.00069705]
		[batch 20/20] avg loss: 0.02637700462749412		[learning rate: 0.00069536]
	Learning Rate: 0.000695361
	LOSS [training: 0.02554950115336272 | validation: -0.0037339348675294497]
	TIME [epoch: 8.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_600.pth
	Model improved!!!
EPOCH 601/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.021997946842522893		[learning rate: 0.00069368]
		[batch 20/20] avg loss: 0.019335278988464242		[learning rate: 0.000692]
	Learning Rate: 0.000691999
	LOSS [training: 0.02066661291549357 | validation: 0.02572838090277217]
	TIME [epoch: 8.77 sec]
EPOCH 602/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.029917405446668887		[learning rate: 0.00069032]
		[batch 20/20] avg loss: 0.028166523146257373		[learning rate: 0.00068865]
	Learning Rate: 0.000688652
	LOSS [training: 0.02904196429646313 | validation: 0.017207269871683924]
	TIME [epoch: 8.77 sec]
EPOCH 603/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022540511643703694		[learning rate: 0.00068699]
		[batch 20/20] avg loss: 0.02359454877426439		[learning rate: 0.00068532]
	Learning Rate: 0.000685322
	LOSS [training: 0.023067530208984038 | validation: 0.009142528274167686]
	TIME [epoch: 8.8 sec]
EPOCH 604/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028227165259659837		[learning rate: 0.00068366]
		[batch 20/20] avg loss: 0.004189239403232304		[learning rate: 0.00068201]
	Learning Rate: 0.000682008
	LOSS [training: 0.016208202331446073 | validation: 0.013597165943686075]
	TIME [epoch: 8.76 sec]
EPOCH 605/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023501448420650516		[learning rate: 0.00068036]
		[batch 20/20] avg loss: 0.006498075210022494		[learning rate: 0.00067871]
	Learning Rate: 0.00067871
	LOSS [training: 0.014999761815336507 | validation: 0.01746520417026366]
	TIME [epoch: 8.75 sec]
EPOCH 606/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.031103427980011568		[learning rate: 0.00067707]
		[batch 20/20] avg loss: 0.03995886343968376		[learning rate: 0.00067543]
	Learning Rate: 0.000675428
	LOSS [training: 0.03553114570984766 | validation: 0.02334896650968432]
	TIME [epoch: 8.74 sec]
EPOCH 607/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026981251191531865		[learning rate: 0.00067379]
		[batch 20/20] avg loss: 0.016703164619599316		[learning rate: 0.00067216]
	Learning Rate: 0.000672162
	LOSS [training: 0.02184220790556559 | validation: -0.0060583688449431464]
	TIME [epoch: 8.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_607.pth
	Model improved!!!
EPOCH 608/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.015417965503508302		[learning rate: 0.00067053]
		[batch 20/20] avg loss: 0.020913506690576483		[learning rate: 0.00066891]
	Learning Rate: 0.000668911
	LOSS [training: 0.018165736097042397 | validation: -0.0028449683961581478]
	TIME [epoch: 8.79 sec]
EPOCH 609/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.015542478442369964		[learning rate: 0.00066729]
		[batch 20/20] avg loss: 0.0190938330328372		[learning rate: 0.00066568]
	Learning Rate: 0.000665676
	LOSS [training: 0.017318155737603584 | validation: 0.00940508322317845]
	TIME [epoch: 8.76 sec]
EPOCH 610/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03502595198846899		[learning rate: 0.00066406]
		[batch 20/20] avg loss: 0.02028534071163583		[learning rate: 0.00066246]
	Learning Rate: 0.000662457
	LOSS [training: 0.027655646350052403 | validation: 0.03608071457639503]
	TIME [epoch: 8.78 sec]
EPOCH 611/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02984030200075598		[learning rate: 0.00066085]
		[batch 20/20] avg loss: 0.031179841631807682		[learning rate: 0.00065925]
	Learning Rate: 0.000659254
	LOSS [training: 0.03051007181628182 | validation: 0.00024455606435495273]
	TIME [epoch: 8.76 sec]
EPOCH 612/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.011201719296825336		[learning rate: 0.00065766]
		[batch 20/20] avg loss: 0.02933171477375362		[learning rate: 0.00065607]
	Learning Rate: 0.000656066
	LOSS [training: 0.020266717035289476 | validation: 0.00404061244804757]
	TIME [epoch: 8.76 sec]
EPOCH 613/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01646866933960909		[learning rate: 0.00065448]
		[batch 20/20] avg loss: 0.025786741542032598		[learning rate: 0.00065289]
	Learning Rate: 0.000652893
	LOSS [training: 0.02112770544082084 | validation: 0.010056813627363739]
	TIME [epoch: 8.76 sec]
EPOCH 614/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.015072842261960772		[learning rate: 0.00065131]
		[batch 20/20] avg loss: 0.020979375914170244		[learning rate: 0.00064974]
	Learning Rate: 0.000649736
	LOSS [training: 0.01802610908806551 | validation: 0.03211966026399091]
	TIME [epoch: 8.78 sec]
EPOCH 615/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025179349950473124		[learning rate: 0.00064816]
		[batch 20/20] avg loss: 0.022700038905069275		[learning rate: 0.00064659]
	Learning Rate: 0.000646594
	LOSS [training: 0.023939694427771194 | validation: 0.045888364091908565]
	TIME [epoch: 8.79 sec]
EPOCH 616/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023615954106538238		[learning rate: 0.00064503]
		[batch 20/20] avg loss: 0.02251038752672571		[learning rate: 0.00064347]
	Learning Rate: 0.000643467
	LOSS [training: 0.023063170816631978 | validation: 0.02256415583718805]
	TIME [epoch: 8.77 sec]
EPOCH 617/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02941784857788334		[learning rate: 0.00064191]
		[batch 20/20] avg loss: 0.01267819751510584		[learning rate: 0.00064036]
	Learning Rate: 0.000640355
	LOSS [training: 0.02104802304649459 | validation: 0.017136995225968524]
	TIME [epoch: 8.77 sec]
EPOCH 618/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07091807232872545		[learning rate: 0.00063881]
		[batch 20/20] avg loss: 0.014861895635812448		[learning rate: 0.00063726]
	Learning Rate: 0.000637259
	LOSS [training: 0.042889983982268946 | validation: 0.02459189820159555]
	TIME [epoch: 8.76 sec]
EPOCH 619/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02035408857060551		[learning rate: 0.00063572]
		[batch 20/20] avg loss: 0.019910836563190946		[learning rate: 0.00063418]
	Learning Rate: 0.000634177
	LOSS [training: 0.02013246256689823 | validation: -0.00322133832709051]
	TIME [epoch: 8.77 sec]
EPOCH 620/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.019791817548118512		[learning rate: 0.00063264]
		[batch 20/20] avg loss: 0.009998810828790117		[learning rate: 0.00063111]
	Learning Rate: 0.00063111
	LOSS [training: 0.014895314188454314 | validation: 0.03802407278265861]
	TIME [epoch: 8.77 sec]
EPOCH 621/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02804147705775381		[learning rate: 0.00062958]
		[batch 20/20] avg loss: 0.009783834180419265		[learning rate: 0.00062806]
	Learning Rate: 0.000628058
	LOSS [training: 0.018912655619086532 | validation: 0.00034619054916402775]
	TIME [epoch: 8.75 sec]
EPOCH 622/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01524760315884568		[learning rate: 0.00062654]
		[batch 20/20] avg loss: 0.03541851087767735		[learning rate: 0.00062502]
	Learning Rate: 0.000625021
	LOSS [training: 0.025333057018261516 | validation: 0.004036026910638555]
	TIME [epoch: 8.77 sec]
EPOCH 623/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.015253325029265182		[learning rate: 0.00062351]
		[batch 20/20] avg loss: 0.01903329465243964		[learning rate: 0.000622]
	Learning Rate: 0.000621999
	LOSS [training: 0.01714330984085241 | validation: 0.03342334904171873]
	TIME [epoch: 8.76 sec]
EPOCH 624/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.030426918573638596		[learning rate: 0.00062049]
		[batch 20/20] avg loss: 0.02651785729495108		[learning rate: 0.00061899]
	Learning Rate: 0.000618991
	LOSS [training: 0.028472387934294834 | validation: 0.03335768277858464]
	TIME [epoch: 8.78 sec]
EPOCH 625/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.016337303026424645		[learning rate: 0.00061749]
		[batch 20/20] avg loss: 0.010488677568839578		[learning rate: 0.000616]
	Learning Rate: 0.000615997
	LOSS [training: 0.01341299029763211 | validation: 0.017726318025072012]
	TIME [epoch: 8.78 sec]
EPOCH 626/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03196519155965051		[learning rate: 0.00061451]
		[batch 20/20] avg loss: 0.011853309679872465		[learning rate: 0.00061302]
	Learning Rate: 0.000613019
	LOSS [training: 0.02190925061976149 | validation: 0.024216067424588747]
	TIME [epoch: 8.74 sec]
EPOCH 627/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.013881338120637474		[learning rate: 0.00061153]
		[batch 20/20] avg loss: 0.03508739443201367		[learning rate: 0.00061005]
	Learning Rate: 0.000610054
	LOSS [training: 0.02448436627632557 | validation: 0.006588806360725523]
	TIME [epoch: 8.76 sec]
EPOCH 628/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.014251844222162083		[learning rate: 0.00060858]
		[batch 20/20] avg loss: 0.02247533356656497		[learning rate: 0.0006071]
	Learning Rate: 0.000607104
	LOSS [training: 0.01836358889436352 | validation: 0.04673622905226704]
	TIME [epoch: 8.76 sec]
EPOCH 629/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02922649895214574		[learning rate: 0.00060563]
		[batch 20/20] avg loss: 0.015895415689314907		[learning rate: 0.00060417]
	Learning Rate: 0.000604168
	LOSS [training: 0.022560957320730324 | validation: 0.02091825233695783]
	TIME [epoch: 8.77 sec]
EPOCH 630/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.015623431872556956		[learning rate: 0.00060271]
		[batch 20/20] avg loss: 0.044379205123092944		[learning rate: 0.00060125]
	Learning Rate: 0.000601247
	LOSS [training: 0.030001318497824947 | validation: 0.0440595324114339]
	TIME [epoch: 8.78 sec]
EPOCH 631/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.032436020718774716		[learning rate: 0.00059979]
		[batch 20/20] avg loss: 0.038141379831840266		[learning rate: 0.00059834]
	Learning Rate: 0.000598339
	LOSS [training: 0.03528870027530749 | validation: 0.00952060532710047]
	TIME [epoch: 8.76 sec]
EPOCH 632/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007523378388226316		[learning rate: 0.00059689]
		[batch 20/20] avg loss: 0.010079767128585096		[learning rate: 0.00059545]
	Learning Rate: 0.000595446
	LOSS [training: 0.008801572758405709 | validation: 0.04646527597783883]
	TIME [epoch: 8.76 sec]
EPOCH 633/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04559589194724421		[learning rate: 0.000594]
		[batch 20/20] avg loss: 0.02448085753503667		[learning rate: 0.00059257]
	Learning Rate: 0.000592566
	LOSS [training: 0.035038374741140445 | validation: 0.007569762561762236]
	TIME [epoch: 8.74 sec]
EPOCH 634/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02552080763341093		[learning rate: 0.00059113]
		[batch 20/20] avg loss: 0.018622007967740807		[learning rate: 0.0005897]
	Learning Rate: 0.000589701
	LOSS [training: 0.022071407800575867 | validation: 0.006417142947241308]
	TIME [epoch: 8.76 sec]
EPOCH 635/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02193485707782188		[learning rate: 0.00058827]
		[batch 20/20] avg loss: 0.012701246152072371		[learning rate: 0.00058685]
	Learning Rate: 0.000586849
	LOSS [training: 0.01731805161494713 | validation: 0.023848212939068504]
	TIME [epoch: 8.78 sec]
EPOCH 636/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028681015883769208		[learning rate: 0.00058543]
		[batch 20/20] avg loss: 0.01663095676430932		[learning rate: 0.00058401]
	Learning Rate: 0.000584011
	LOSS [training: 0.022655986324039257 | validation: 0.015242330527508698]
	TIME [epoch: 8.76 sec]
EPOCH 637/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.019342515264781496		[learning rate: 0.0005826]
		[batch 20/20] avg loss: 0.019602813898292975		[learning rate: 0.00058119]
	Learning Rate: 0.000581187
	LOSS [training: 0.019472664581537236 | validation: -0.00018120144391577456]
	TIME [epoch: 8.77 sec]
EPOCH 638/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.023070986859018476		[learning rate: 0.00057978]
		[batch 20/20] avg loss: 0.021577628797199416		[learning rate: 0.00057838]
	Learning Rate: 0.000578376
	LOSS [training: 0.022324307828108948 | validation: 0.02563054005763956]
	TIME [epoch: 8.75 sec]
EPOCH 639/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.017342560195037635		[learning rate: 0.00057698]
		[batch 20/20] avg loss: 0.018706565672019605		[learning rate: 0.00057558]
	Learning Rate: 0.000575579
	LOSS [training: 0.01802456293352862 | validation: 0.03180503666606215]
	TIME [epoch: 8.76 sec]
EPOCH 640/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03248205332045813		[learning rate: 0.00057419]
		[batch 20/20] avg loss: 0.016926817189575817		[learning rate: 0.0005728]
	Learning Rate: 0.000572796
	LOSS [training: 0.024704435255016977 | validation: 0.012690140819783619]
	TIME [epoch: 8.78 sec]
EPOCH 641/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010140705884520528		[learning rate: 0.00057141]
		[batch 20/20] avg loss: 0.014366929554505383		[learning rate: 0.00057003]
	Learning Rate: 0.000570026
	LOSS [training: 0.012253817719512956 | validation: 0.0006315369603736953]
	TIME [epoch: 8.77 sec]
EPOCH 642/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025843531343797705		[learning rate: 0.00056865]
		[batch 20/20] avg loss: 0.02836408705560619		[learning rate: 0.00056727]
	Learning Rate: 0.00056727
	LOSS [training: 0.027103809199701945 | validation: 0.00838819522899193]
	TIME [epoch: 8.77 sec]
EPOCH 643/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.018407238219806523		[learning rate: 0.0005659]
		[batch 20/20] avg loss: 0.035643668133178694		[learning rate: 0.00056453]
	Learning Rate: 0.000564526
	LOSS [training: 0.02702545317649262 | validation: 0.012797177793952803]
	TIME [epoch: 8.76 sec]
EPOCH 644/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03799133755326624		[learning rate: 0.00056316]
		[batch 20/20] avg loss: 0.012307131046318398		[learning rate: 0.0005618]
	Learning Rate: 0.000561796
	LOSS [training: 0.025149234299792315 | validation: 0.011022038679480086]
	TIME [epoch: 8.76 sec]
EPOCH 645/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.021431697745315493		[learning rate: 0.00056044]
		[batch 20/20] avg loss: 0.02220277611122761		[learning rate: 0.00055908]
	Learning Rate: 0.00055908
	LOSS [training: 0.021817236928271554 | validation: 0.008203404719829001]
	TIME [epoch: 8.76 sec]
EPOCH 646/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025187155549431454		[learning rate: 0.00055773]
		[batch 20/20] avg loss: 0.024349778697649132		[learning rate: 0.00055638]
	Learning Rate: 0.000556376
	LOSS [training: 0.024768467123540295 | validation: 0.006809281248764132]
	TIME [epoch: 8.76 sec]
EPOCH 647/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.021983325099499938		[learning rate: 0.00055503]
		[batch 20/20] avg loss: 0.010769570663664659		[learning rate: 0.00055369]
	Learning Rate: 0.000553685
	LOSS [training: 0.0163764478815823 | validation: 0.0031940826294950815]
	TIME [epoch: 8.75 sec]
EPOCH 648/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01777670658536132		[learning rate: 0.00055235]
		[batch 20/20] avg loss: 0.007117474433116841		[learning rate: 0.00055101]
	Learning Rate: 0.000551008
	LOSS [training: 0.012447090509239082 | validation: 0.00361753191808133]
	TIME [epoch: 8.75 sec]
EPOCH 649/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02787120654370016		[learning rate: 0.00054967]
		[batch 20/20] avg loss: 0.015906220730219987		[learning rate: 0.00054834]
	Learning Rate: 0.000548343
	LOSS [training: 0.021888713636960076 | validation: 0.005464303386635375]
	TIME [epoch: 8.76 sec]
EPOCH 650/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00977565585455083		[learning rate: 0.00054702]
		[batch 20/20] avg loss: 0.010845310847229273		[learning rate: 0.00054569]
	Learning Rate: 0.000545692
	LOSS [training: 0.010310483350890052 | validation: 0.0033273862636849756]
	TIME [epoch: 8.76 sec]
EPOCH 651/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010468076233694867		[learning rate: 0.00054437]
		[batch 20/20] avg loss: 0.021465494274368523		[learning rate: 0.00054305]
	Learning Rate: 0.000543053
	LOSS [training: 0.015966785254031696 | validation: 0.04085626075577106]
	TIME [epoch: 8.79 sec]
EPOCH 652/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.044474218338489926		[learning rate: 0.00054174]
		[batch 20/20] avg loss: 0.03365537011597032		[learning rate: 0.00054043]
	Learning Rate: 0.000540427
	LOSS [training: 0.039064794227230124 | validation: 0.02842024755365636]
	TIME [epoch: 8.75 sec]
EPOCH 653/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.042367960184297226		[learning rate: 0.00053912]
		[batch 20/20] avg loss: 0.02689701744514999		[learning rate: 0.00053781]
	Learning Rate: 0.000537813
	LOSS [training: 0.034632488814723604 | validation: 0.008491366760736021]
	TIME [epoch: 8.76 sec]
EPOCH 654/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022274545415409183		[learning rate: 0.00053651]
		[batch 20/20] avg loss: 0.021114390092316776		[learning rate: 0.00053521]
	Learning Rate: 0.000535213
	LOSS [training: 0.02169446775386298 | validation: 0.019823513914712702]
	TIME [epoch: 8.75 sec]
EPOCH 655/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02622284324232128		[learning rate: 0.00053392]
		[batch 20/20] avg loss: 0.023474887328117665		[learning rate: 0.00053262]
	Learning Rate: 0.000532624
	LOSS [training: 0.02484886528521947 | validation: 0.010128863595571925]
	TIME [epoch: 8.77 sec]
EPOCH 656/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01884366335545515		[learning rate: 0.00053134]
		[batch 20/20] avg loss: 0.014214489154021685		[learning rate: 0.00053005]
	Learning Rate: 0.000530049
	LOSS [training: 0.016529076254738418 | validation: 0.00863731917808842]
	TIME [epoch: 8.78 sec]
EPOCH 657/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01394695215092295		[learning rate: 0.00052877]
		[batch 20/20] avg loss: 0.021134633892341506		[learning rate: 0.00052749]
	Learning Rate: 0.000527485
	LOSS [training: 0.01754079302163223 | validation: 0.002964946452955997]
	TIME [epoch: 8.77 sec]
EPOCH 658/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028764095630201846		[learning rate: 0.00052621]
		[batch 20/20] avg loss: 0.01733544770584091		[learning rate: 0.00052493]
	Learning Rate: 0.000524935
	LOSS [training: 0.010105928634430546 | validation: 0.01069715122957653]
	TIME [epoch: 8.77 sec]
EPOCH 659/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.028403016245164642		[learning rate: 0.00052366]
		[batch 20/20] avg loss: 0.00889988180936578		[learning rate: 0.0005224]
	Learning Rate: 0.000522396
	LOSS [training: 0.018651449027265208 | validation: 0.010015854083400883]
	TIME [epoch: 8.76 sec]
EPOCH 660/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.021334576537128745		[learning rate: 0.00052113]
		[batch 20/20] avg loss: 0.008070402291339814		[learning rate: 0.00051987]
	Learning Rate: 0.00051987
	LOSS [training: 0.014702489414234281 | validation: 0.00436091104875627]
	TIME [epoch: 8.75 sec]
EPOCH 661/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.013936523333700962		[learning rate: 0.00051861]
		[batch 20/20] avg loss: 0.014542067432843553		[learning rate: 0.00051736]
	Learning Rate: 0.000517356
	LOSS [training: 0.014239295383272257 | validation: 0.0058217123455192215]
	TIME [epoch: 8.78 sec]
EPOCH 662/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.014657031497431161		[learning rate: 0.0005161]
		[batch 20/20] avg loss: 0.01325221854457304		[learning rate: 0.00051485]
	Learning Rate: 0.000514854
	LOSS [training: 0.013954625021002101 | validation: 0.014750211488818442]
	TIME [epoch: 8.75 sec]
EPOCH 663/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.011076042529587198		[learning rate: 0.00051361]
		[batch 20/20] avg loss: 0.02332199252430462		[learning rate: 0.00051236]
	Learning Rate: 0.000512364
	LOSS [training: 0.017199017526945908 | validation: 0.01894402113177214]
	TIME [epoch: 8.76 sec]
EPOCH 664/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.017841973258577955		[learning rate: 0.00051112]
		[batch 20/20] avg loss: 0.01175339190387616		[learning rate: 0.00050989]
	Learning Rate: 0.000509887
	LOSS [training: 0.014797682581227059 | validation: 0.029618606781664093]
	TIME [epoch: 8.75 sec]
EPOCH 665/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.016183387077500857		[learning rate: 0.00050865]
		[batch 20/20] avg loss: 0.020516940863791244		[learning rate: 0.00050742]
	Learning Rate: 0.000507421
	LOSS [training: 0.01835016397064605 | validation: 0.011703520908476197]
	TIME [epoch: 8.76 sec]
EPOCH 666/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01213983804793672		[learning rate: 0.00050619]
		[batch 20/20] avg loss: 0.026519000261123256		[learning rate: 0.00050497]
	Learning Rate: 0.000504967
	LOSS [training: 0.01932941915452999 | validation: 0.00617652876686735]
	TIME [epoch: 8.78 sec]
EPOCH 667/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02272480692526942		[learning rate: 0.00050374]
		[batch 20/20] avg loss: 0.009484787470133413		[learning rate: 0.00050253]
	Learning Rate: 0.000502525
	LOSS [training: 0.01610479719770142 | validation: -0.0021462475315481436]
	TIME [epoch: 8.74 sec]
EPOCH 668/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0067045023808230296		[learning rate: 0.00050131]
		[batch 20/20] avg loss: 0.015520099004500059		[learning rate: 0.0005001]
	Learning Rate: 0.000500095
	LOSS [training: 0.011112300692661545 | validation: 0.03260529998596545]
	TIME [epoch: 8.76 sec]
EPOCH 669/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010455180533855546		[learning rate: 0.00049888]
		[batch 20/20] avg loss: 0.021980887411272843		[learning rate: 0.00049768]
	Learning Rate: 0.000497677
	LOSS [training: 0.016218033972564196 | validation: 0.022602657678924575]
	TIME [epoch: 8.76 sec]
EPOCH 670/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.018950699286042073		[learning rate: 0.00049647]
		[batch 20/20] avg loss: 0.010199246614781676		[learning rate: 0.00049527]
	Learning Rate: 0.00049527
	LOSS [training: 0.014574972950411874 | validation: 0.00656327584093819]
	TIME [epoch: 8.77 sec]
EPOCH 671/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01684641761615989		[learning rate: 0.00049407]
		[batch 20/20] avg loss: 0.012591216600234684		[learning rate: 0.00049288]
	Learning Rate: 0.000492875
	LOSS [training: 0.014718817108197288 | validation: 0.005981208283862969]
	TIME [epoch: 8.79 sec]
EPOCH 672/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007246717356481615		[learning rate: 0.00049168]
		[batch 20/20] avg loss: 0.016626251917962902		[learning rate: 0.00049049]
	Learning Rate: 0.000490492
	LOSS [training: 0.011936484637222262 | validation: 0.0642784312804016]
	TIME [epoch: 8.76 sec]
EPOCH 673/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0422675259888686		[learning rate: 0.0004893]
		[batch 20/20] avg loss: 0.01493260907566055		[learning rate: 0.00048812]
	Learning Rate: 0.00048812
	LOSS [training: 0.02860006753226458 | validation: 0.0018613775439112082]
	TIME [epoch: 8.77 sec]
EPOCH 674/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.018087302395480606		[learning rate: 0.00048694]
		[batch 20/20] avg loss: 0.017630349582621215		[learning rate: 0.00048576]
	Learning Rate: 0.000485759
	LOSS [training: 0.017858825989050912 | validation: 0.002609031570050824]
	TIME [epoch: 8.75 sec]
EPOCH 675/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.025874699873724667		[learning rate: 0.00048458]
		[batch 20/20] avg loss: 0.015491272042068963		[learning rate: 0.00048341]
	Learning Rate: 0.00048341
	LOSS [training: 0.02068298595789681 | validation: 0.0008524932505195757]
	TIME [epoch: 8.76 sec]
EPOCH 676/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.011292122268868894		[learning rate: 0.00048224]
		[batch 20/20] avg loss: 0.020689186821519705		[learning rate: 0.00048107]
	Learning Rate: 0.000481072
	LOSS [training: 0.0159906545451943 | validation: 0.04502514252521035]
	TIME [epoch: 8.79 sec]
EPOCH 677/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.029533914312634975		[learning rate: 0.00047991]
		[batch 20/20] avg loss: 0.011448490953404444		[learning rate: 0.00047875]
	Learning Rate: 0.000478746
	LOSS [training: 0.020491202633019713 | validation: 0.012871025082317966]
	TIME [epoch: 8.76 sec]
EPOCH 678/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.012122234598220587		[learning rate: 0.00047759]
		[batch 20/20] avg loss: 0.015065884578801914		[learning rate: 0.00047643]
	Learning Rate: 0.000476431
	LOSS [training: 0.013594059588511249 | validation: -0.002902533455362478]
	TIME [epoch: 8.76 sec]
EPOCH 679/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.009109066624590064		[learning rate: 0.00047528]
		[batch 20/20] avg loss: 0.03121555033344687		[learning rate: 0.00047413]
	Learning Rate: 0.000474127
	LOSS [training: 0.020162308479018464 | validation: 0.002292460374170353]
	TIME [epoch: 8.76 sec]
EPOCH 680/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.014978866278771397		[learning rate: 0.00047298]
		[batch 20/20] avg loss: 0.0116704399439647		[learning rate: 0.00047183]
	Learning Rate: 0.000471834
	LOSS [training: 0.013324653111368049 | validation: -0.0018162392767025722]
	TIME [epoch: 8.75 sec]
EPOCH 681/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.020566426929167693		[learning rate: 0.00047069]
		[batch 20/20] avg loss: 0.02034099549716768		[learning rate: 0.00046955]
	Learning Rate: 0.000469553
	LOSS [training: 0.020453711213167684 | validation: 0.023932230176628294]
	TIME [epoch: 8.76 sec]
EPOCH 682/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.020271998476948626		[learning rate: 0.00046842]
		[batch 20/20] avg loss: 0.011763217997909543		[learning rate: 0.00046728]
	Learning Rate: 0.000467282
	LOSS [training: 0.016017608237429086 | validation: -0.002691104368361205]
	TIME [epoch: 8.77 sec]
EPOCH 683/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01249874447942175		[learning rate: 0.00046615]
		[batch 20/20] avg loss: 0.008511133898099545		[learning rate: 0.00046502]
	Learning Rate: 0.000465022
	LOSS [training: 0.010504939188760647 | validation: 0.004527333890910452]
	TIME [epoch: 8.76 sec]
EPOCH 684/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02712072220206843		[learning rate: 0.0004639]
		[batch 20/20] avg loss: 0.007885647178296682		[learning rate: 0.00046277]
	Learning Rate: 0.000462773
	LOSS [training: 0.017503184690182562 | validation: -0.0039221910838355455]
	TIME [epoch: 8.76 sec]
EPOCH 685/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01875482195586405		[learning rate: 0.00046165]
		[batch 20/20] avg loss: 0.013956635356218789		[learning rate: 0.00046054]
	Learning Rate: 0.000460536
	LOSS [training: 0.01635572865604142 | validation: 0.008430268792350086]
	TIME [epoch: 8.76 sec]
EPOCH 686/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01105860054336583		[learning rate: 0.00045942]
		[batch 20/20] avg loss: 0.03421907635005945		[learning rate: 0.00045831]
	Learning Rate: 0.000458309
	LOSS [training: 0.02263883844671264 | validation: 0.0021400435840404778]
	TIME [epoch: 8.76 sec]
EPOCH 687/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.017832710369313755		[learning rate: 0.0004572]
		[batch 20/20] avg loss: 0.01717062607027711		[learning rate: 0.00045609]
	Learning Rate: 0.000456092
	LOSS [training: 0.017501668219795437 | validation: -0.0009464841061688923]
	TIME [epoch: 8.77 sec]
EPOCH 688/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.018839640588878652		[learning rate: 0.00045499]
		[batch 20/20] avg loss: 0.006000595395209845		[learning rate: 0.00045389]
	Learning Rate: 0.000453887
	LOSS [training: 0.01242011799204425 | validation: 0.032094650374048135]
	TIME [epoch: 8.76 sec]
EPOCH 689/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03433634950141619		[learning rate: 0.00045279]
		[batch 20/20] avg loss: 0.015819581681024063		[learning rate: 0.00045169]
	Learning Rate: 0.000451692
	LOSS [training: 0.02507796559122013 | validation: 0.014475163219891174]
	TIME [epoch: 8.75 sec]
EPOCH 690/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.019649028331289557		[learning rate: 0.0004506]
		[batch 20/20] avg loss: 0.011082305712122114		[learning rate: 0.00044951]
	Learning Rate: 0.000449507
	LOSS [training: 0.015365667021705836 | validation: 0.013385652432460194]
	TIME [epoch: 8.75 sec]
EPOCH 691/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022646686159929694		[learning rate: 0.00044842]
		[batch 20/20] avg loss: 0.024005515656251898		[learning rate: 0.00044733]
	Learning Rate: 0.000447334
	LOSS [training: 0.023326100908090796 | validation: 0.010386098375194542]
	TIME [epoch: 8.74 sec]
EPOCH 692/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0056932221594117385		[learning rate: 0.00044625]
		[batch 20/20] avg loss: 0.03226524878478422		[learning rate: 0.00044517]
	Learning Rate: 0.00044517
	LOSS [training: 0.018979235472097976 | validation: 0.010705492250997433]
	TIME [epoch: 8.78 sec]
EPOCH 693/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.011509864411228556		[learning rate: 0.00044409]
		[batch 20/20] avg loss: 0.017855867232724962		[learning rate: 0.00044302]
	Learning Rate: 0.000443018
	LOSS [training: 0.014682865821976759 | validation: 0.00415485107015943]
	TIME [epoch: 8.75 sec]
EPOCH 694/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.014925274394548693		[learning rate: 0.00044195]
		[batch 20/20] avg loss: 0.01555666369383733		[learning rate: 0.00044088]
	Learning Rate: 0.000440875
	LOSS [training: 0.015240969044193014 | validation: -0.0033641911358495494]
	TIME [epoch: 8.75 sec]
EPOCH 695/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.013163110897802616		[learning rate: 0.00043981]
		[batch 20/20] avg loss: 0.015304419508927882		[learning rate: 0.00043874]
	Learning Rate: 0.000438743
	LOSS [training: 0.014233765203365247 | validation: 0.02228980658416943]
	TIME [epoch: 8.75 sec]
EPOCH 696/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.029159075265581386		[learning rate: 0.00043768]
		[batch 20/20] avg loss: 0.01117526209858183		[learning rate: 0.00043662]
	Learning Rate: 0.000436622
	LOSS [training: 0.020167168682081606 | validation: 0.06617901710499213]
	TIME [epoch: 8.74 sec]
EPOCH 697/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02856008150684706		[learning rate: 0.00043556]
		[batch 20/20] avg loss: 0.04716372961664521		[learning rate: 0.00043451]
	Learning Rate: 0.00043451
	LOSS [training: 0.03786190556174614 | validation: 0.036814532265050624]
	TIME [epoch: 8.78 sec]
EPOCH 698/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.018035808589223525		[learning rate: 0.00043346]
		[batch 20/20] avg loss: 0.00739659867084391		[learning rate: 0.00043241]
	Learning Rate: 0.000432409
	LOSS [training: 0.012716203630033715 | validation: 0.013527883904948055]
	TIME [epoch: 8.76 sec]
EPOCH 699/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.024152308384106595		[learning rate: 0.00043136]
		[batch 20/20] avg loss: 0.017028992985638126		[learning rate: 0.00043032]
	Learning Rate: 0.000430318
	LOSS [training: 0.02059065068487236 | validation: 0.0016464863360178192]
	TIME [epoch: 8.76 sec]
EPOCH 700/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005360832278032852		[learning rate: 0.00042928]
		[batch 20/20] avg loss: 0.02776116684145554		[learning rate: 0.00042824]
	Learning Rate: 0.000428237
	LOSS [training: 0.016560999559744195 | validation: 0.02244866334739226]
	TIME [epoch: 8.75 sec]
EPOCH 701/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.011273179687994518		[learning rate: 0.0004272]
		[batch 20/20] avg loss: 0.010753830405972515		[learning rate: 0.00042617]
	Learning Rate: 0.000426166
	LOSS [training: 0.011013505046983518 | validation: -0.005287363993698347]
	TIME [epoch: 8.74 sec]
EPOCH 702/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.03168489179468326		[learning rate: 0.00042513]
		[batch 20/20] avg loss: 0.00926854178526904		[learning rate: 0.00042411]
	Learning Rate: 0.000424105
	LOSS [training: 0.020476716789976156 | validation: 0.008539397507188367]
	TIME [epoch: 8.77 sec]
EPOCH 703/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01766386232468985		[learning rate: 0.00042308]
		[batch 20/20] avg loss: 0.026444672812548774		[learning rate: 0.00042205]
	Learning Rate: 0.000422054
	LOSS [training: 0.022054267568619314 | validation: 0.0032478269837877497]
	TIME [epoch: 8.76 sec]
EPOCH 704/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.009746874339757617		[learning rate: 0.00042103]
		[batch 20/20] avg loss: 0.008428229203862993		[learning rate: 0.00042001]
	Learning Rate: 0.000420013
	LOSS [training: 0.009087551771810306 | validation: 0.009119716782619164]
	TIME [epoch: 8.75 sec]
EPOCH 705/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01379300239424506		[learning rate: 0.000419]
		[batch 20/20] avg loss: 0.014229443417046603		[learning rate: 0.00041798]
	Learning Rate: 0.000417982
	LOSS [training: 0.014011222905645831 | validation: 0.005120108440992916]
	TIME [epoch: 8.74 sec]
EPOCH 706/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.017970790151878267		[learning rate: 0.00041697]
		[batch 20/20] avg loss: 0.003609307487879117		[learning rate: 0.00041596]
	Learning Rate: 0.000415961
	LOSS [training: 0.010790048819878693 | validation: 0.0020505015749221352]
	TIME [epoch: 8.75 sec]
EPOCH 707/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.015494361993352251		[learning rate: 0.00041495]
		[batch 20/20] avg loss: 0.014150384875306269		[learning rate: 0.00041395]
	Learning Rate: 0.00041395
	LOSS [training: 0.014822373434329258 | validation: 0.014905520760333864]
	TIME [epoch: 8.77 sec]
EPOCH 708/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02390058217626784		[learning rate: 0.00041295]
		[batch 20/20] avg loss: 0.009721905120167093		[learning rate: 0.00041195]
	Learning Rate: 0.000411948
	LOSS [training: 0.016811243648217466 | validation: 0.006177362009005458]
	TIME [epoch: 8.75 sec]
EPOCH 709/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008010364802512302		[learning rate: 0.00041095]
		[batch 20/20] avg loss: 0.014726428821211857		[learning rate: 0.00040996]
	Learning Rate: 0.000409956
	LOSS [training: 0.011368396811862077 | validation: 0.011530199797075853]
	TIME [epoch: 8.74 sec]
EPOCH 710/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.016467491476632068		[learning rate: 0.00040896]
		[batch 20/20] avg loss: 0.01361088628758637		[learning rate: 0.00040797]
	Learning Rate: 0.000407973
	LOSS [training: 0.01503918888210922 | validation: 0.00838194197204503]
	TIME [epoch: 8.75 sec]
EPOCH 711/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.013047865731897309		[learning rate: 0.00040699]
		[batch 20/20] avg loss: 0.016255517135694085		[learning rate: 0.000406]
	Learning Rate: 0.000406
	LOSS [training: 0.014651691433795699 | validation: 8.221382622770239e-05]
	TIME [epoch: 8.77 sec]
EPOCH 712/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010535972838517163		[learning rate: 0.00040502]
		[batch 20/20] avg loss: 0.009263943977057019		[learning rate: 0.00040404]
	Learning Rate: 0.000404037
	LOSS [training: 0.009899958407787091 | validation: 2.801062459053638e-05]
	TIME [epoch: 8.77 sec]
EPOCH 713/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.04941368199669616		[learning rate: 0.00040306]
		[batch 20/20] avg loss: 0.006098783269852024		[learning rate: 0.00040208]
	Learning Rate: 0.000402083
	LOSS [training: 0.0277562326332741 | validation: -0.004353961202528391]
	TIME [epoch: 8.78 sec]
EPOCH 714/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008454704092058556		[learning rate: 0.00040111]
		[batch 20/20] avg loss: 0.009924815774336284		[learning rate: 0.00040014]
	Learning Rate: 0.000400139
	LOSS [training: 0.00918975993319742 | validation: 0.004093482758290774]
	TIME [epoch: 8.76 sec]
EPOCH 715/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008995151564166356		[learning rate: 0.00039917]
		[batch 20/20] avg loss: 0.017738413746891033		[learning rate: 0.0003982]
	Learning Rate: 0.000398204
	LOSS [training: 0.013366782655528694 | validation: -0.004624314583727142]
	TIME [epoch: 8.75 sec]
EPOCH 716/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.013344913155301192		[learning rate: 0.00039724]
		[batch 20/20] avg loss: 0.007449977553608387		[learning rate: 0.00039628]
	Learning Rate: 0.000396278
	LOSS [training: 0.010397445354454788 | validation: 0.005957068920964965]
	TIME [epoch: 8.75 sec]
EPOCH 717/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.019424872792529895		[learning rate: 0.00039532]
		[batch 20/20] avg loss: 0.013747265491971306		[learning rate: 0.00039436]
	Learning Rate: 0.000394362
	LOSS [training: 0.0165860691422506 | validation: 0.0003353403956915733]
	TIME [epoch: 8.76 sec]
EPOCH 718/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.017412808872397174		[learning rate: 0.00039341]
		[batch 20/20] avg loss: 0.013116507656470452		[learning rate: 0.00039245]
	Learning Rate: 0.000392455
	LOSS [training: 0.015264658264433812 | validation: -0.003181446275589414]
	TIME [epoch: 8.77 sec]
EPOCH 719/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01315873323358226		[learning rate: 0.0003915]
		[batch 20/20] avg loss: 0.013669950977204331		[learning rate: 0.00039056]
	Learning Rate: 0.000390557
	LOSS [training: 0.013414342105393296 | validation: 0.030889143702383236]
	TIME [epoch: 8.75 sec]
EPOCH 720/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.013941147572077312		[learning rate: 0.00038961]
		[batch 20/20] avg loss: 0.010005126497789807		[learning rate: 0.00038867]
	Learning Rate: 0.000388668
	LOSS [training: 0.01197313703493356 | validation: 0.024351685769007633]
	TIME [epoch: 8.75 sec]
EPOCH 721/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.013430474544603702		[learning rate: 0.00038773]
		[batch 20/20] avg loss: 0.02260842714603661		[learning rate: 0.00038679]
	Learning Rate: 0.000386789
	LOSS [training: 0.01801945084532015 | validation: -4.7753550502859865e-06]
	TIME [epoch: 8.76 sec]
EPOCH 722/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005992672796197816		[learning rate: 0.00038585]
		[batch 20/20] avg loss: 0.013995194400911687		[learning rate: 0.00038492]
	Learning Rate: 0.000384918
	LOSS [training: 0.009993933598554753 | validation: 0.002179530274895149]
	TIME [epoch: 8.75 sec]
EPOCH 723/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.014171494932803019		[learning rate: 0.00038399]
		[batch 20/20] avg loss: 0.0051772578932477884		[learning rate: 0.00038306]
	Learning Rate: 0.000383057
	LOSS [training: 0.009674376413025404 | validation: 0.0014625783024165347]
	TIME [epoch: 8.79 sec]
EPOCH 724/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00960358913163239		[learning rate: 0.00038213]
		[batch 20/20] avg loss: 0.009414501098152527		[learning rate: 0.0003812]
	Learning Rate: 0.000381204
	LOSS [training: 0.009509045114892457 | validation: 0.008626429935304937]
	TIME [epoch: 8.77 sec]
EPOCH 725/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.006650475560055767		[learning rate: 0.00038028]
		[batch 20/20] avg loss: 0.013551817305455929		[learning rate: 0.00037936]
	Learning Rate: 0.000379361
	LOSS [training: 0.010101146432755849 | validation: 0.02627769777506031]
	TIME [epoch: 8.76 sec]
EPOCH 726/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00962281812814583		[learning rate: 0.00037844]
		[batch 20/20] avg loss: 0.012638685657443176		[learning rate: 0.00037753]
	Learning Rate: 0.000377526
	LOSS [training: 0.011130751892794504 | validation: 0.0012594203146760962]
	TIME [epoch: 8.77 sec]
EPOCH 727/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.020564494403464077		[learning rate: 0.00037661]
		[batch 20/20] avg loss: 0.01951028737819572		[learning rate: 0.0003757]
	Learning Rate: 0.000375701
	LOSS [training: 0.020037390890829895 | validation: 0.024401888048293973]
	TIME [epoch: 8.75 sec]
EPOCH 728/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01221758553159678		[learning rate: 0.00037479]
		[batch 20/20] avg loss: 0.006982489545684975		[learning rate: 0.00037388]
	Learning Rate: 0.000373884
	LOSS [training: 0.009600037538640877 | validation: 0.02163851876098858]
	TIME [epoch: 8.77 sec]
EPOCH 729/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.026888696637756164		[learning rate: 0.00037298]
		[batch 20/20] avg loss: 0.0056512768698601485		[learning rate: 0.00037208]
	Learning Rate: 0.000372076
	LOSS [training: 0.01626998675380816 | validation: -0.004518710453742884]
	TIME [epoch: 8.76 sec]
EPOCH 730/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.014886686725276797		[learning rate: 0.00037118]
		[batch 20/20] avg loss: 0.020339707801263124		[learning rate: 0.00037028]
	Learning Rate: 0.000370277
	LOSS [training: 0.01761319726326996 | validation: 0.019388787903736018]
	TIME [epoch: 8.76 sec]
EPOCH 731/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01581954798308522		[learning rate: 0.00036938]
		[batch 20/20] avg loss: 0.008768761928578018		[learning rate: 0.00036849]
	Learning Rate: 0.000368486
	LOSS [training: 0.01229415495583162 | validation: -0.0012247743840493185]
	TIME [epoch: 8.76 sec]
EPOCH 732/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.011069218529160894		[learning rate: 0.00036759]
		[batch 20/20] avg loss: 0.011455080127510345		[learning rate: 0.0003667]
	Learning Rate: 0.000366704
	LOSS [training: 0.011262149328335618 | validation: -0.002931481154749305]
	TIME [epoch: 8.75 sec]
EPOCH 733/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.012596788401591194		[learning rate: 0.00036582]
		[batch 20/20] avg loss: 0.01697209922987914		[learning rate: 0.00036493]
	Learning Rate: 0.000364931
	LOSS [training: 0.014784443815735168 | validation: -0.0026362014075228903]
	TIME [epoch: 8.78 sec]
EPOCH 734/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0032191866607089656		[learning rate: 0.00036405]
		[batch 20/20] avg loss: 0.013131330802855206		[learning rate: 0.00036317]
	Learning Rate: 0.000363166
	LOSS [training: 0.008175258731782088 | validation: 0.029736066404993877]
	TIME [epoch: 8.75 sec]
EPOCH 735/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.013126975019254607		[learning rate: 0.00036229]
		[batch 20/20] avg loss: 0.013257856938657627		[learning rate: 0.00036141]
	Learning Rate: 0.00036141
	LOSS [training: 0.01319241597895612 | validation: 0.01159681450718739]
	TIME [epoch: 8.75 sec]
EPOCH 736/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0077233272715078365		[learning rate: 0.00036053]
		[batch 20/20] avg loss: 0.017363726541127167		[learning rate: 0.00035966]
	Learning Rate: 0.000359662
	LOSS [training: 0.012543526906317501 | validation: 0.03958463218406353]
	TIME [epoch: 8.75 sec]
EPOCH 737/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.017176548746192385		[learning rate: 0.00035879]
		[batch 20/20] avg loss: 0.014733808492309286		[learning rate: 0.00035792]
	Learning Rate: 0.000357923
	LOSS [training: 0.015955178619250835 | validation: 0.001136399289164015]
	TIME [epoch: 8.75 sec]
EPOCH 738/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004061389051765819		[learning rate: 0.00035706]
		[batch 20/20] avg loss: 0.020083157565236147		[learning rate: 0.00035619]
	Learning Rate: 0.000356192
	LOSS [training: 0.012072273308500985 | validation: 0.0031541319765337845]
	TIME [epoch: 8.78 sec]
EPOCH 739/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.012821481668769305		[learning rate: 0.00035533]
		[batch 20/20] avg loss: 0.01168016059736211		[learning rate: 0.00035447]
	Learning Rate: 0.00035447
	LOSS [training: 0.01225082113306571 | validation: -0.0029363267453487206]
	TIME [epoch: 8.79 sec]
EPOCH 740/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008894738703692178		[learning rate: 0.00035361]
		[batch 20/20] avg loss: 0.008711306359408762		[learning rate: 0.00035276]
	Learning Rate: 0.000352755
	LOSS [training: 0.00880302253155047 | validation: 0.011595464806832274]
	TIME [epoch: 8.77 sec]
EPOCH 741/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008455074505164592		[learning rate: 0.0003519]
		[batch 20/20] avg loss: 0.01682823641806006		[learning rate: 0.00035105]
	Learning Rate: 0.00035105
	LOSS [training: 0.012641655461612328 | validation: 0.014085237049618006]
	TIME [epoch: 8.76 sec]
EPOCH 742/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010072097455577826		[learning rate: 0.0003502]
		[batch 20/20] avg loss: 0.014619091464495432		[learning rate: 0.00034935]
	Learning Rate: 0.000349352
	LOSS [training: 0.012345594460036627 | validation: -0.0033373137514868412]
	TIME [epoch: 8.74 sec]
EPOCH 743/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022042938304644447		[learning rate: 0.00034851]
		[batch 20/20] avg loss: 0.012670168374981028		[learning rate: 0.00034766]
	Learning Rate: 0.000347663
	LOSS [training: 0.017356553339812737 | validation: 0.008854291395065422]
	TIME [epoch: 8.77 sec]
EPOCH 744/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008632643323019792		[learning rate: 0.00034682]
		[batch 20/20] avg loss: 0.007244273635029172		[learning rate: 0.00034598]
	Learning Rate: 0.000345981
	LOSS [training: 0.007938458479024482 | validation: 0.007063552535266518]
	TIME [epoch: 8.77 sec]
EPOCH 745/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.013519706584975727		[learning rate: 0.00034514]
		[batch 20/20] avg loss: 0.02484775513957327		[learning rate: 0.00034431]
	Learning Rate: 0.000344308
	LOSS [training: 0.0191837308622745 | validation: 0.037026116619132715]
	TIME [epoch: 8.76 sec]
EPOCH 746/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008662968767756424		[learning rate: 0.00034347]
		[batch 20/20] avg loss: 0.006261677626980808		[learning rate: 0.00034264]
	Learning Rate: 0.000342643
	LOSS [training: 0.007462323197368616 | validation: -0.007200029988626334]
	TIME [epoch: 8.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_746.pth
	Model improved!!!
EPOCH 747/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.017703997686351333		[learning rate: 0.00034181]
		[batch 20/20] avg loss: 0.023622031392422815		[learning rate: 0.00034099]
	Learning Rate: 0.000340986
	LOSS [training: 0.020663014539387074 | validation: 0.025201203731517845]
	TIME [epoch: 8.76 sec]
EPOCH 748/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004907854262857759		[learning rate: 0.00034016]
		[batch 20/20] avg loss: 0.008210026048441243		[learning rate: 0.00033934]
	Learning Rate: 0.000339337
	LOSS [training: 0.0065589401556494994 | validation: 0.020297146168174563]
	TIME [epoch: 8.75 sec]
EPOCH 749/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022072828913576086		[learning rate: 0.00033852]
		[batch 20/20] avg loss: 0.006706393548875564		[learning rate: 0.0003377]
	Learning Rate: 0.000337696
	LOSS [training: 0.014389611231225823 | validation: -0.0007179974702726266]
	TIME [epoch: 8.76 sec]
EPOCH 750/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.012279894407655732		[learning rate: 0.00033688]
		[batch 20/20] avg loss: 0.008978583414721584		[learning rate: 0.00033606]
	Learning Rate: 0.000336063
	LOSS [training: 0.010629238911188655 | validation: -0.008465928431414728]
	TIME [epoch: 8.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_750.pth
	Model improved!!!
EPOCH 751/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.003098038332093281		[learning rate: 0.00033525]
		[batch 20/20] avg loss: 0.003136241567961943		[learning rate: 0.00033444]
	Learning Rate: 0.000334438
	LOSS [training: 0.003117139950027611 | validation: -0.006747443449733539]
	TIME [epoch: 8.77 sec]
EPOCH 752/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01015956141710401		[learning rate: 0.00033363]
		[batch 20/20] avg loss: 0.0076579090854926755		[learning rate: 0.00033282]
	Learning Rate: 0.000332821
	LOSS [training: 0.008908735251298342 | validation: 0.0011362423110377053]
	TIME [epoch: 8.76 sec]
EPOCH 753/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007370280067616371		[learning rate: 0.00033202]
		[batch 20/20] avg loss: 0.015198207285730162		[learning rate: 0.00033121]
	Learning Rate: 0.000331211
	LOSS [training: 0.011284243676673265 | validation: -0.0005434871831455524]
	TIME [epoch: 8.75 sec]
EPOCH 754/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.011397342507656134		[learning rate: 0.00033041]
		[batch 20/20] avg loss: 0.016365578831918685		[learning rate: 0.00032961]
	Learning Rate: 0.00032961
	LOSS [training: 0.013881460669787413 | validation: 0.005144578403962255]
	TIME [epoch: 8.78 sec]
EPOCH 755/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.009403396159054864		[learning rate: 0.00032881]
		[batch 20/20] avg loss: 0.014253567732581643		[learning rate: 0.00032802]
	Learning Rate: 0.000328016
	LOSS [training: 0.011828481945818254 | validation: 0.010087696698243817]
	TIME [epoch: 8.76 sec]
EPOCH 756/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02244236374196324		[learning rate: 0.00032722]
		[batch 20/20] avg loss: 0.004019612481938689		[learning rate: 0.00032643]
	Learning Rate: 0.00032643
	LOSS [training: 0.013230988111950967 | validation: 7.462100739570252e-05]
	TIME [epoch: 8.75 sec]
EPOCH 757/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004446828259274091		[learning rate: 0.00032564]
		[batch 20/20] avg loss: 0.01878517307891362		[learning rate: 0.00032485]
	Learning Rate: 0.000324851
	LOSS [training: 0.011616000669093855 | validation: -0.003624678583388319]
	TIME [epoch: 8.76 sec]
EPOCH 758/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01790589985325588		[learning rate: 0.00032406]
		[batch 20/20] avg loss: 0.002634297922154287		[learning rate: 0.00032328]
	Learning Rate: 0.00032328
	LOSS [training: 0.010270098887705083 | validation: 0.0019071378098931176]
	TIME [epoch: 8.76 sec]
EPOCH 759/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.012113557850961085		[learning rate: 0.0003225]
		[batch 20/20] avg loss: 0.014379811137340307		[learning rate: 0.00032172]
	Learning Rate: 0.000321717
	LOSS [training: 0.013246684494150696 | validation: 0.008506446160727411]
	TIME [epoch: 8.76 sec]
EPOCH 760/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007851950500016787		[learning rate: 0.00032094]
		[batch 20/20] avg loss: 0.02548905785190102		[learning rate: 0.00032016]
	Learning Rate: 0.000320161
	LOSS [training: 0.0166705041759589 | validation: 0.004636824288315759]
	TIME [epoch: 8.77 sec]
EPOCH 761/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007497731998373783		[learning rate: 0.00031939]
		[batch 20/20] avg loss: 0.01936789321002542		[learning rate: 0.00031861]
	Learning Rate: 0.000318613
	LOSS [training: 0.0134328126041996 | validation: 0.011318123279559604]
	TIME [epoch: 8.75 sec]
EPOCH 762/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008807794118036876		[learning rate: 0.00031784]
		[batch 20/20] avg loss: 0.010739728658645632		[learning rate: 0.00031707]
	Learning Rate: 0.000317072
	LOSS [training: 0.009773761388341254 | validation: -0.004855680033439606]
	TIME [epoch: 8.75 sec]
EPOCH 763/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010617028918512707		[learning rate: 0.0003163]
		[batch 20/20] avg loss: 0.014686045363050102		[learning rate: 0.00031554]
	Learning Rate: 0.000315539
	LOSS [training: 0.012651537140781405 | validation: 0.0322403741496709]
	TIME [epoch: 8.74 sec]
EPOCH 764/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.009025063455020677		[learning rate: 0.00031477]
		[batch 20/20] avg loss: 0.010865229317701836		[learning rate: 0.00031401]
	Learning Rate: 0.000314013
	LOSS [training: 0.009945146386361256 | validation: 0.03367454994504132]
	TIME [epoch: 8.74 sec]
EPOCH 765/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.020604379276157183		[learning rate: 0.00031325]
		[batch 20/20] avg loss: 0.006099613562605119		[learning rate: 0.00031249]
	Learning Rate: 0.000312494
	LOSS [training: 0.013351996419381151 | validation: 0.006365866093665526]
	TIME [epoch: 8.78 sec]
EPOCH 766/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008194309444219814		[learning rate: 0.00031174]
		[batch 20/20] avg loss: 0.007238451118186023		[learning rate: 0.00031098]
	Learning Rate: 0.000310983
	LOSS [training: 0.00771638028120292 | validation: -0.0003612638164058545]
	TIME [epoch: 8.75 sec]
EPOCH 767/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007559468850264954		[learning rate: 0.00031023]
		[batch 20/20] avg loss: 0.013374009415932076		[learning rate: 0.00030948]
	Learning Rate: 0.000309479
	LOSS [training: 0.010466739133098512 | validation: 0.002445740117823104]
	TIME [epoch: 8.76 sec]
EPOCH 768/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.009462349429712556		[learning rate: 0.00030873]
		[batch 20/20] avg loss: 0.01392406167735753		[learning rate: 0.00030798]
	Learning Rate: 0.000307983
	LOSS [training: 0.011693205553535042 | validation: -0.0014322588424786407]
	TIME [epoch: 8.74 sec]
EPOCH 769/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.006500934499636938		[learning rate: 0.00030724]
		[batch 20/20] avg loss: 0.010901237252476092		[learning rate: 0.00030649]
	Learning Rate: 0.000306493
	LOSS [training: 0.008701085876056515 | validation: -0.003649905978729777]
	TIME [epoch: 8.74 sec]
EPOCH 770/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.013839906438272606		[learning rate: 0.00030575]
		[batch 20/20] avg loss: 0.004840632084337153		[learning rate: 0.00030501]
	Learning Rate: 0.000305011
	LOSS [training: 0.009340269261304877 | validation: -0.0037417432853913486]
	TIME [epoch: 8.75 sec]
EPOCH 771/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.011441609327186028		[learning rate: 0.00030427]
		[batch 20/20] avg loss: 0.008884999683308374		[learning rate: 0.00030354]
	Learning Rate: 0.000303536
	LOSS [training: 0.010163304505247203 | validation: -0.005903279276630198]
	TIME [epoch: 8.77 sec]
EPOCH 772/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.012349951294353496		[learning rate: 0.0003028]
		[batch 20/20] avg loss: 0.003361460171941362		[learning rate: 0.00030207]
	Learning Rate: 0.000302068
	LOSS [training: 0.007855705733147427 | validation: 0.002993665099170229]
	TIME [epoch: 8.75 sec]
EPOCH 773/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004777705454073422		[learning rate: 0.00030134]
		[batch 20/20] avg loss: 0.005916075150567596		[learning rate: 0.00030061]
	Learning Rate: 0.000300608
	LOSS [training: 0.005346890302320509 | validation: -0.003046087921384072]
	TIME [epoch: 8.74 sec]
EPOCH 774/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00924503982652076		[learning rate: 0.00029988]
		[batch 20/20] avg loss: 0.014168406815151746		[learning rate: 0.00029915]
	Learning Rate: 0.000299154
	LOSS [training: 0.011706723320836252 | validation: -0.004686556396647195]
	TIME [epoch: 8.75 sec]
EPOCH 775/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.014683953933029159		[learning rate: 0.00029843]
		[batch 20/20] avg loss: 0.012635117247780636		[learning rate: 0.00029771]
	Learning Rate: 0.000297707
	LOSS [training: 0.0136595355904049 | validation: -0.00578295396454766]
	TIME [epoch: 8.74 sec]
EPOCH 776/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025745222902477665		[learning rate: 0.00029699]
		[batch 20/20] avg loss: 0.006827408511398079		[learning rate: 0.00029627]
	Learning Rate: 0.000296268
	LOSS [training: 0.004700965400822923 | validation: -0.004098003860239138]
	TIME [epoch: 8.76 sec]
EPOCH 777/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007340796646846437		[learning rate: 0.00029555]
		[batch 20/20] avg loss: 0.004072761703967537		[learning rate: 0.00029483]
	Learning Rate: 0.000294835
	LOSS [training: 0.005706779175406986 | validation: -0.0038669530977384456]
	TIME [epoch: 8.75 sec]
EPOCH 778/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0036576841539292656		[learning rate: 0.00029412]
		[batch 20/20] avg loss: 0.013391016293508617		[learning rate: 0.00029341]
	Learning Rate: 0.000293409
	LOSS [training: 0.008524350223718943 | validation: 0.0027334986275873935]
	TIME [epoch: 8.74 sec]
EPOCH 779/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.016081740959430067		[learning rate: 0.0002927]
		[batch 20/20] avg loss: 0.011056457991444505		[learning rate: 0.00029199]
	Learning Rate: 0.00029199
	LOSS [training: 0.013569099475437284 | validation: -0.0020772145569971775]
	TIME [epoch: 8.75 sec]
EPOCH 780/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.003460862644814691		[learning rate: 0.00029128]
		[batch 20/20] avg loss: 0.013112120549413108		[learning rate: 0.00029058]
	Learning Rate: 0.000290578
	LOSS [training: 0.0082864915971139 | validation: 0.010667748215505255]
	TIME [epoch: 8.74 sec]
EPOCH 781/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.013487656491130188		[learning rate: 0.00028987]
		[batch 20/20] avg loss: 0.01165022371066259		[learning rate: 0.00028917]
	Learning Rate: 0.000289173
	LOSS [training: 0.012568940100896386 | validation: -0.00131885878464362]
	TIME [epoch: 8.74 sec]
EPOCH 782/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.011503622161679907		[learning rate: 0.00028847]
		[batch 20/20] avg loss: 0.02042482993183194		[learning rate: 0.00028777]
	Learning Rate: 0.000287775
	LOSS [training: 0.015964226046755924 | validation: 0.004071988260204603]
	TIME [epoch: 8.76 sec]
EPOCH 783/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.013562913160777188		[learning rate: 0.00028708]
		[batch 20/20] avg loss: 0.01073323328657513		[learning rate: 0.00028638]
	Learning Rate: 0.000286383
	LOSS [training: 0.012148073223676156 | validation: -0.002018947904467397]
	TIME [epoch: 8.74 sec]
EPOCH 784/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0057421616283672506		[learning rate: 0.00028569]
		[batch 20/20] avg loss: 0.01167138094092269		[learning rate: 0.000285]
	Learning Rate: 0.000284998
	LOSS [training: 0.008706771284644971 | validation: 0.020978377460419733]
	TIME [epoch: 8.74 sec]
EPOCH 785/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01961909526291392		[learning rate: 0.00028431]
		[batch 20/20] avg loss: 0.005834848446324261		[learning rate: 0.00028362]
	Learning Rate: 0.00028362
	LOSS [training: 0.012726971854619091 | validation: 0.0002606536214947821]
	TIME [epoch: 8.74 sec]
EPOCH 786/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004065588268457001		[learning rate: 0.00028293]
		[batch 20/20] avg loss: 0.0120123015449034		[learning rate: 0.00028225]
	Learning Rate: 0.000282248
	LOSS [training: 0.0080389449066802 | validation: 0.023868613422428426]
	TIME [epoch: 8.75 sec]
EPOCH 787/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.034024762180381495		[learning rate: 0.00028157]
		[batch 20/20] avg loss: 0.025064296344433894		[learning rate: 0.00028088]
	Learning Rate: 0.000280884
	LOSS [training: 0.0295445292624077 | validation: 0.0033364575829641007]
	TIME [epoch: 8.75 sec]
EPOCH 788/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00782166734674614		[learning rate: 0.0002802]
		[batch 20/20] avg loss: 0.016055606924422153		[learning rate: 0.00027953]
	Learning Rate: 0.000279525
	LOSS [training: 0.011938637135584147 | validation: -0.00350434089266333]
	TIME [epoch: 8.76 sec]
EPOCH 789/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.015473914429322203		[learning rate: 0.00027885]
		[batch 20/20] avg loss: 0.008464831912094942		[learning rate: 0.00027817]
	Learning Rate: 0.000278173
	LOSS [training: 0.011969373170708572 | validation: 0.0009551299977019804]
	TIME [epoch: 8.75 sec]
EPOCH 790/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01068507282859026		[learning rate: 0.0002775]
		[batch 20/20] avg loss: 0.007937635735224217		[learning rate: 0.00027683]
	Learning Rate: 0.000276828
	LOSS [training: 0.009311354281907238 | validation: -0.0011590091331392916]
	TIME [epoch: 8.74 sec]
EPOCH 791/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014127287150506856		[learning rate: 0.00027616]
		[batch 20/20] avg loss: 0.021198362171394237		[learning rate: 0.00027549]
	Learning Rate: 0.00027549
	LOSS [training: 0.011305545443222462 | validation: 0.004526849398798431]
	TIME [epoch: 8.74 sec]
EPOCH 792/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.006179896532614521		[learning rate: 0.00027482]
		[batch 20/20] avg loss: 0.004878901033667988		[learning rate: 0.00027416]
	Learning Rate: 0.000274157
	LOSS [training: 0.005529398783141255 | validation: -0.0018533801966870694]
	TIME [epoch: 8.75 sec]
EPOCH 793/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01277557187697377		[learning rate: 0.00027349]
		[batch 20/20] avg loss: 0.012335151594400757		[learning rate: 0.00027283]
	Learning Rate: 0.000272832
	LOSS [training: 0.012555361735687266 | validation: 0.0008956333137822321]
	TIME [epoch: 8.78 sec]
EPOCH 794/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.011323403659320724		[learning rate: 0.00027217]
		[batch 20/20] avg loss: 0.004458994337124429		[learning rate: 0.00027151]
	Learning Rate: 0.000271512
	LOSS [training: 0.007891198998222578 | validation: -0.00042188945235969695]
	TIME [epoch: 8.75 sec]
EPOCH 795/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0074274941823069636		[learning rate: 0.00027086]
		[batch 20/20] avg loss: 0.015376255215096932		[learning rate: 0.0002702]
	Learning Rate: 0.000270199
	LOSS [training: 0.01140187469870195 | validation: 0.00837537436214194]
	TIME [epoch: 8.74 sec]
EPOCH 796/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.003255185840425087		[learning rate: 0.00026955]
		[batch 20/20] avg loss: 0.012506980089109923		[learning rate: 0.00026889]
	Learning Rate: 0.000268893
	LOSS [training: 0.007881082964767505 | validation: -0.0025575159180456983]
	TIME [epoch: 8.76 sec]
EPOCH 797/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02721666596611034		[learning rate: 0.00026824]
		[batch 20/20] avg loss: 0.009226528049934289		[learning rate: 0.00026759]
	Learning Rate: 0.000267592
	LOSS [training: 0.018221597008022315 | validation: 0.0032819312319392543]
	TIME [epoch: 8.74 sec]
EPOCH 798/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0077948729734550245		[learning rate: 0.00026694]
		[batch 20/20] avg loss: 0.009473210301500843		[learning rate: 0.0002663]
	Learning Rate: 0.000266298
	LOSS [training: 0.00863404163747793 | validation: -0.0067352003550471586]
	TIME [epoch: 8.75 sec]
EPOCH 799/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00754813627763491		[learning rate: 0.00026565]
		[batch 20/20] avg loss: 0.004421103783177556		[learning rate: 0.00026501]
	Learning Rate: 0.000265011
	LOSS [training: 0.0059846200304062335 | validation: 0.001151497977147425]
	TIME [epoch: 8.77 sec]
EPOCH 800/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0037332172882770824		[learning rate: 0.00026437]
		[batch 20/20] avg loss: 0.02152966964248151		[learning rate: 0.00026373]
	Learning Rate: 0.000263729
	LOSS [training: 0.012631443465379293 | validation: 0.01572085367974732]
	TIME [epoch: 8.75 sec]
EPOCH 801/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008640817951560891		[learning rate: 0.00026309]
		[batch 20/20] avg loss: 0.00802838465331009		[learning rate: 0.00026245]
	Learning Rate: 0.000262454
	LOSS [training: 0.00833460130243549 | validation: 0.0016646512749014832]
	TIME [epoch: 8.75 sec]
EPOCH 802/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010083095135019796		[learning rate: 0.00026182]
		[batch 20/20] avg loss: 0.006606160730455074		[learning rate: 0.00026118]
	Learning Rate: 0.000261184
	LOSS [training: 0.008344627932737433 | validation: -0.004790740560273286]
	TIME [epoch: 8.74 sec]
EPOCH 803/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008480730927847615		[learning rate: 0.00026055]
		[batch 20/20] avg loss: 0.003972633655535268		[learning rate: 0.00025992]
	Learning Rate: 0.000259921
	LOSS [training: 0.006226682291691442 | validation: 0.006032316002170923]
	TIME [epoch: 8.75 sec]
EPOCH 804/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010592198562172904		[learning rate: 0.00025929]
		[batch 20/20] avg loss: 0.008125950727573206		[learning rate: 0.00025866]
	Learning Rate: 0.000258665
	LOSS [training: 0.009359074644873054 | validation: 0.007029827797620812]
	TIME [epoch: 8.75 sec]
EPOCH 805/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004193335802932918		[learning rate: 0.00025804]
		[batch 20/20] avg loss: 0.012174876546284847		[learning rate: 0.00025741]
	Learning Rate: 0.000257414
	LOSS [training: 0.008184106174608883 | validation: -0.0015063224068171226]
	TIME [epoch: 8.77 sec]
EPOCH 806/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004851418399496715		[learning rate: 0.00025679]
		[batch 20/20] avg loss: 0.008244395385014466		[learning rate: 0.00025617]
	Learning Rate: 0.000256169
	LOSS [training: 0.00654790689225559 | validation: 0.02174539024125608]
	TIME [epoch: 8.77 sec]
EPOCH 807/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.021756848578635055		[learning rate: 0.00025555]
		[batch 20/20] avg loss: 0.00980874545852951		[learning rate: 0.00025493]
	Learning Rate: 0.00025493
	LOSS [training: 0.015782797018582283 | validation: -0.00035283669560596463]
	TIME [epoch: 8.76 sec]
EPOCH 808/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.009128705772394363		[learning rate: 0.00025431]
		[batch 20/20] avg loss: 0.0042488666202053745		[learning rate: 0.0002537]
	Learning Rate: 0.000253697
	LOSS [training: 0.006688786196299869 | validation: 0.0018172788605201073]
	TIME [epoch: 8.77 sec]
EPOCH 809/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005716090784641273		[learning rate: 0.00025308]
		[batch 20/20] avg loss: 0.014258582727123043		[learning rate: 0.00025247]
	Learning Rate: 0.00025247
	LOSS [training: 0.009987336755882158 | validation: -0.000994201499457859]
	TIME [epoch: 8.75 sec]
EPOCH 810/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005273433352775391		[learning rate: 0.00025186]
		[batch 20/20] avg loss: 0.00664684338232608		[learning rate: 0.00025125]
	Learning Rate: 0.00025125
	LOSS [training: 0.005960138367550737 | validation: 0.000262231162417928]
	TIME [epoch: 8.78 sec]
EPOCH 811/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.015492354966720523		[learning rate: 0.00025064]
		[batch 20/20] avg loss: 0.02045520303428286		[learning rate: 0.00025003]
	Learning Rate: 0.000250035
	LOSS [training: 0.017973779000501695 | validation: 0.000591399980454871]
	TIME [epoch: 8.76 sec]
EPOCH 812/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007656435514082756		[learning rate: 0.00024943]
		[batch 20/20] avg loss: 0.005115974330548096		[learning rate: 0.00024883]
	Learning Rate: 0.000248825
	LOSS [training: 0.006386204922315427 | validation: 0.025102119149938942]
	TIME [epoch: 8.76 sec]
EPOCH 813/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.013615520176387308		[learning rate: 0.00024822]
		[batch 20/20] avg loss: 0.005220119316311688		[learning rate: 0.00024762]
	Learning Rate: 0.000247622
	LOSS [training: 0.009417819746349499 | validation: -0.0011583686411114715]
	TIME [epoch: 8.76 sec]
EPOCH 814/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005435620155366061		[learning rate: 0.00024702]
		[batch 20/20] avg loss: 0.015508814109352079		[learning rate: 0.00024642]
	Learning Rate: 0.000246425
	LOSS [training: 0.01047221713235907 | validation: -0.003754808859169898]
	TIME [epoch: 8.75 sec]
EPOCH 815/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00223215585828871		[learning rate: 0.00024583]
		[batch 20/20] avg loss: 0.011578732288210967		[learning rate: 0.00024523]
	Learning Rate: 0.000245233
	LOSS [training: 0.006905444073249842 | validation: 4.7263809293802625e-05]
	TIME [epoch: 8.76 sec]
EPOCH 816/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004109971310972576		[learning rate: 0.00024464]
		[batch 20/20] avg loss: 0.023127131666599854		[learning rate: 0.00024405]
	Learning Rate: 0.000244047
	LOSS [training: 0.013618551488786216 | validation: 0.012119422327948574]
	TIME [epoch: 8.76 sec]
EPOCH 817/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01833597601601162		[learning rate: 0.00024346]
		[batch 20/20] avg loss: 0.01274679929554962		[learning rate: 0.00024287]
	Learning Rate: 0.000242867
	LOSS [training: 0.01554138765578062 | validation: 0.0019596364786309594]
	TIME [epoch: 8.76 sec]
EPOCH 818/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.002452958070474864		[learning rate: 0.00024228]
		[batch 20/20] avg loss: 0.014969425069388077		[learning rate: 0.00024169]
	Learning Rate: 0.000241693
	LOSS [training: 0.008711191569931472 | validation: 0.011500693826380769]
	TIME [epoch: 8.75 sec]
EPOCH 819/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010825769245115401		[learning rate: 0.00024111]
		[batch 20/20] avg loss: 0.014558025014177836		[learning rate: 0.00024052]
	Learning Rate: 0.000240524
	LOSS [training: 0.012691897129646617 | validation: -0.0026403222975947757]
	TIME [epoch: 8.75 sec]
EPOCH 820/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004393563442583676		[learning rate: 0.00023994]
		[batch 20/20] avg loss: 0.010176054666626954		[learning rate: 0.00023936]
	Learning Rate: 0.000239361
	LOSS [training: 0.007284809054605315 | validation: -0.00317348967930794]
	TIME [epoch: 8.77 sec]
EPOCH 821/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004953396684522228		[learning rate: 0.00023878]
		[batch 20/20] avg loss: 0.004921075452431446		[learning rate: 0.0002382]
	Learning Rate: 0.000238203
	LOSS [training: 0.004937236068476836 | validation: 0.0016836784559112234]
	TIME [epoch: 8.78 sec]
EPOCH 822/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008748090169505413		[learning rate: 0.00023763]
		[batch 20/20] avg loss: 0.008270515057348379		[learning rate: 0.00023705]
	Learning Rate: 0.000237051
	LOSS [training: 0.008509302613426898 | validation: 0.00020184439965820417]
	TIME [epoch: 8.77 sec]
EPOCH 823/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007105023415691506		[learning rate: 0.00023648]
		[batch 20/20] avg loss: 0.0017402897635911713		[learning rate: 0.0002359]
	Learning Rate: 0.000235905
	LOSS [training: 0.004422656589641338 | validation: 0.0028419806071298156]
	TIME [epoch: 8.75 sec]
EPOCH 824/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010143208489017909		[learning rate: 0.00023533]
		[batch 20/20] avg loss: 0.008109196006189933		[learning rate: 0.00023476]
	Learning Rate: 0.000234764
	LOSS [training: 0.009126202247603922 | validation: 4.942045752428354e-05]
	TIME [epoch: 8.75 sec]
EPOCH 825/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007559782583832289		[learning rate: 0.0002342]
		[batch 20/20] avg loss: 0.010052251604215544		[learning rate: 0.00023363]
	Learning Rate: 0.000233629
	LOSS [training: 0.008806017094023916 | validation: 0.009120392477579217]
	TIME [epoch: 8.76 sec]
EPOCH 826/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.009114438644149256		[learning rate: 0.00023306]
		[batch 20/20] avg loss: 0.007020193152387543		[learning rate: 0.0002325]
	Learning Rate: 0.000232499
	LOSS [training: 0.008067315898268401 | validation: -0.004512899986938105]
	TIME [epoch: 8.75 sec]
EPOCH 827/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.015920276904374087		[learning rate: 0.00023194]
		[batch 20/20] avg loss: 0.009652590341893013		[learning rate: 0.00023137]
	Learning Rate: 0.000231375
	LOSS [training: 0.012786433623133548 | validation: -0.005927924675352276]
	TIME [epoch: 8.79 sec]
EPOCH 828/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.012622569795801678		[learning rate: 0.00023081]
		[batch 20/20] avg loss: 0.007870061823746295		[learning rate: 0.00023026]
	Learning Rate: 0.000230256
	LOSS [training: 0.010246315809773986 | validation: -0.005390795687120486]
	TIME [epoch: 8.75 sec]
EPOCH 829/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005378565040285517		[learning rate: 0.0002297]
		[batch 20/20] avg loss: 0.009305166380018524		[learning rate: 0.00022914]
	Learning Rate: 0.000229142
	LOSS [training: 0.007341865710152021 | validation: 0.0012242059818012635]
	TIME [epoch: 8.75 sec]
EPOCH 830/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0030788134030944633		[learning rate: 0.00022859]
		[batch 20/20] avg loss: 0.004573335937045729		[learning rate: 0.00022803]
	Learning Rate: 0.000228034
	LOSS [training: 0.0038260746700700963 | validation: -0.002429836780026115]
	TIME [epoch: 8.75 sec]
EPOCH 831/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.009015454539218822		[learning rate: 0.00022748]
		[batch 20/20] avg loss: 0.006765348075494426		[learning rate: 0.00022693]
	Learning Rate: 0.000226931
	LOSS [training: 0.007890401307356623 | validation: 0.018245276125086307]
	TIME [epoch: 8.74 sec]
EPOCH 832/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00825005232744645		[learning rate: 0.00022638]
		[batch 20/20] avg loss: 0.0024040308889088195		[learning rate: 0.00022583]
	Learning Rate: 0.000225834
	LOSS [training: 0.005327041608177634 | validation: 0.0023034544168864745]
	TIME [epoch: 8.76 sec]
EPOCH 833/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.017478444303400266		[learning rate: 0.00022529]
		[batch 20/20] avg loss: 0.007057310147652172		[learning rate: 0.00022474]
	Learning Rate: 0.000224742
	LOSS [training: 0.012267877225526217 | validation: -0.001639378597060484]
	TIME [epoch: 8.77 sec]
EPOCH 834/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.02016989146984432		[learning rate: 0.0002242]
		[batch 20/20] avg loss: 0.011974710461901763		[learning rate: 0.00022366]
	Learning Rate: 0.000223655
	LOSS [training: 0.01607230096587304 | validation: -0.002812986247841981]
	TIME [epoch: 8.76 sec]
EPOCH 835/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010945191829921342		[learning rate: 0.00022311]
		[batch 20/20] avg loss: 0.00963903356528355		[learning rate: 0.00022257]
	Learning Rate: 0.000222574
	LOSS [training: 0.010292112697602445 | validation: 0.007432975897188318]
	TIME [epoch: 8.76 sec]
EPOCH 836/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010786622200713542		[learning rate: 0.00022203]
		[batch 20/20] avg loss: 0.0004994124374480156		[learning rate: 0.0002215]
	Learning Rate: 0.000221497
	LOSS [training: 0.005643017319080779 | validation: -0.002395515035296989]
	TIME [epoch: 8.76 sec]
EPOCH 837/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.006387685365239796		[learning rate: 0.00022096]
		[batch 20/20] avg loss: 0.01971307853621506		[learning rate: 0.00022043]
	Learning Rate: 0.000220426
	LOSS [training: 0.013050381950727427 | validation: 0.0033572535419157633]
	TIME [epoch: 8.75 sec]
EPOCH 838/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008503380279905812		[learning rate: 0.00021989]
		[batch 20/20] avg loss: 0.00545700524791706		[learning rate: 0.00021936]
	Learning Rate: 0.00021936
	LOSS [training: 0.0069801927639114355 | validation: -0.007562680693753572]
	TIME [epoch: 8.77 sec]
EPOCH 839/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008412144510325507		[learning rate: 0.00021883]
		[batch 20/20] avg loss: 0.0050725569597210775		[learning rate: 0.0002183]
	Learning Rate: 0.000218299
	LOSS [training: 0.006742350735023293 | validation: 0.0013948763635460916]
	TIME [epoch: 8.76 sec]
EPOCH 840/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.006180600199329406		[learning rate: 0.00021777]
		[batch 20/20] avg loss: 0.006576748102532523		[learning rate: 0.00021724]
	Learning Rate: 0.000217244
	LOSS [training: 0.006378674150930965 | validation: -0.0009538043741976454]
	TIME [epoch: 8.74 sec]
EPOCH 841/1000:
	Training over batches...
		[batch 10/20] avg loss: -0.000760688738917382		[learning rate: 0.00021672]
		[batch 20/20] avg loss: 0.01837387775429052		[learning rate: 0.00021619]
	Learning Rate: 0.000216193
	LOSS [training: 0.008806594507686567 | validation: 0.010228323285889985]
	TIME [epoch: 8.76 sec]
EPOCH 842/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.006316156549818061		[learning rate: 0.00021567]
		[batch 20/20] avg loss: 0.004287986557239262		[learning rate: 0.00021515]
	Learning Rate: 0.000215148
	LOSS [training: 0.0053020715535286615 | validation: -0.0017947999755434847]
	TIME [epoch: 8.75 sec]
EPOCH 843/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007738517963522274		[learning rate: 0.00021463]
		[batch 20/20] avg loss: 0.009655178388170616		[learning rate: 0.00021411]
	Learning Rate: 0.000214107
	LOSS [training: 0.008696848175846445 | validation: 0.015311297134813587]
	TIME [epoch: 8.76 sec]
EPOCH 844/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010049077527689732		[learning rate: 0.00021359]
		[batch 20/20] avg loss: 0.0064314415493884405		[learning rate: 0.00021307]
	Learning Rate: 0.000213072
	LOSS [training: 0.008240259538539086 | validation: -0.0015967075110908306]
	TIME [epoch: 8.77 sec]
EPOCH 845/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011848550765872594		[learning rate: 0.00021256]
		[batch 20/20] avg loss: 0.009864069873694157		[learning rate: 0.00021204]
	Learning Rate: 0.000212042
	LOSS [training: 0.005524462475140708 | validation: 0.0019744406057615803]
	TIME [epoch: 8.75 sec]
EPOCH 846/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.009576247631552673		[learning rate: 0.00021153]
		[batch 20/20] avg loss: 0.01108042976971953		[learning rate: 0.00021102]
	Learning Rate: 0.000211016
	LOSS [training: 0.010328338700636102 | validation: 0.0028486194733949974]
	TIME [epoch: 8.74 sec]
EPOCH 847/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005705971694140692		[learning rate: 0.00021051]
		[batch 20/20] avg loss: 0.0016099462194147161		[learning rate: 0.00021]
	Learning Rate: 0.000209996
	LOSS [training: 0.003657958956777705 | validation: -0.006292072269302583]
	TIME [epoch: 8.78 sec]
EPOCH 848/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006279794857269723		[learning rate: 0.00020949]
		[batch 20/20] avg loss: 0.009323648202194171		[learning rate: 0.00020898]
	Learning Rate: 0.00020898
	LOSS [training: 0.004975813843960572 | validation: 0.015704679389420793]
	TIME [epoch: 8.76 sec]
EPOCH 849/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022238828263172025		[learning rate: 0.00020847]
		[batch 20/20] avg loss: 0.004796023280287344		[learning rate: 0.00020797]
	Learning Rate: 0.00020797
	LOSS [training: 0.013517425771729683 | validation: -0.0016379779345565254]
	TIME [epoch: 8.79 sec]
EPOCH 850/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005506242126961458		[learning rate: 0.00020747]
		[batch 20/20] avg loss: 0.004549679598419767		[learning rate: 0.00020696]
	Learning Rate: 0.000206964
	LOSS [training: 0.005027960862690611 | validation: -0.002585181098435379]
	TIME [epoch: 8.74 sec]
EPOCH 851/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008913009818482188		[learning rate: 0.00020646]
		[batch 20/20] avg loss: 0.013665510572389078		[learning rate: 0.00020596]
	Learning Rate: 0.000205963
	LOSS [training: 0.011289260195435632 | validation: -0.00179652753271727]
	TIME [epoch: 8.75 sec]
EPOCH 852/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.006707691076411393		[learning rate: 0.00020546]
		[batch 20/20] avg loss: 0.012579695299506177		[learning rate: 0.00020497]
	Learning Rate: 0.000204967
	LOSS [training: 0.009643693187958786 | validation: -0.00019277691517220367]
	TIME [epoch: 8.75 sec]
EPOCH 853/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027465476904042713		[learning rate: 0.00020447]
		[batch 20/20] avg loss: 0.013463644177168119		[learning rate: 0.00020398]
	Learning Rate: 0.000203976
	LOSS [training: 0.008105095933786196 | validation: -0.001023532847907184]
	TIME [epoch: 8.76 sec]
EPOCH 854/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00463334448713631		[learning rate: 0.00020348]
		[batch 20/20] avg loss: 0.006037428566181778		[learning rate: 0.00020299]
	Learning Rate: 0.00020299
	LOSS [training: 0.005335386526659042 | validation: 0.011519151335230876]
	TIME [epoch: 8.75 sec]
EPOCH 855/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0069888743961760095		[learning rate: 0.0002025]
		[batch 20/20] avg loss: 0.008587307551722084		[learning rate: 0.00020201]
	Learning Rate: 0.000202008
	LOSS [training: 0.0077880909739490465 | validation: 0.0057051821669419885]
	TIME [epoch: 8.77 sec]
EPOCH 856/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010546703412970747		[learning rate: 0.00020152]
		[batch 20/20] avg loss: 0.007184624428044409		[learning rate: 0.00020103]
	Learning Rate: 0.000201031
	LOSS [training: 0.008865663920507577 | validation: -0.0046994166958731235]
	TIME [epoch: 8.76 sec]
EPOCH 857/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0005199358063866585		[learning rate: 0.00020054]
		[batch 20/20] avg loss: 0.016053804696923506		[learning rate: 0.00020006]
	Learning Rate: 0.000200059
	LOSS [training: 0.008286870251655084 | validation: 0.014614633719924788]
	TIME [epoch: 8.76 sec]
EPOCH 858/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004610752782286317		[learning rate: 0.00019957]
		[batch 20/20] avg loss: 0.004962233674941854		[learning rate: 0.00019909]
	Learning Rate: 0.000199091
	LOSS [training: 0.004786493228614087 | validation: 0.0011363051518069492]
	TIME [epoch: 8.76 sec]
EPOCH 859/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008744629375106667		[learning rate: 0.00019861]
		[batch 20/20] avg loss: 0.0027140928383305304		[learning rate: 0.00019813]
	Learning Rate: 0.000198129
	LOSS [training: 0.005729361106718598 | validation: 0.001344245132517662]
	TIME [epoch: 8.75 sec]
EPOCH 860/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004641446099619012		[learning rate: 0.00019765]
		[batch 20/20] avg loss: 0.004229007763783434		[learning rate: 0.00019717]
	Learning Rate: 0.000197171
	LOSS [training: 0.0044352269317012236 | validation: -0.0018010023578163498]
	TIME [epoch: 8.76 sec]
EPOCH 861/1000:
	Training over batches...
		[batch 10/20] avg loss: -0.0001930640642836292		[learning rate: 0.00019669]
		[batch 20/20] avg loss: 0.012792155804564914		[learning rate: 0.00019622]
	Learning Rate: 0.000196217
	LOSS [training: 0.006299545870140642 | validation: -0.00248011051358122]
	TIME [epoch: 8.78 sec]
EPOCH 862/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.012274136668294102		[learning rate: 0.00019574]
		[batch 20/20] avg loss: 0.00489340252096375		[learning rate: 0.00019527]
	Learning Rate: 0.000195268
	LOSS [training: 0.008583769594628926 | validation: -0.009557617010941353]
	TIME [epoch: 8.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_862.pth
	Model improved!!!
EPOCH 863/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.006212946969238813		[learning rate: 0.0001948]
		[batch 20/20] avg loss: 0.007915578072181012		[learning rate: 0.00019432]
	Learning Rate: 0.000194324
	LOSS [training: 0.007064262520709914 | validation: -0.004896479603992133]
	TIME [epoch: 8.77 sec]
EPOCH 864/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.009541895045879918		[learning rate: 0.00019385]
		[batch 20/20] avg loss: 0.003865511111110993		[learning rate: 0.00019338]
	Learning Rate: 0.000193384
	LOSS [training: 0.006703703078495456 | validation: 0.00022028106561060225]
	TIME [epoch: 8.74 sec]
EPOCH 865/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.014240792777323527		[learning rate: 0.00019292]
		[batch 20/20] avg loss: 0.00556656644016666		[learning rate: 0.00019245]
	Learning Rate: 0.000192449
	LOSS [training: 0.009903679608745096 | validation: 0.0039217139580298155]
	TIME [epoch: 8.75 sec]
EPOCH 866/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00649170584797072		[learning rate: 0.00019198]
		[batch 20/20] avg loss: 0.01128406251156483		[learning rate: 0.00019152]
	Learning Rate: 0.000191518
	LOSS [training: 0.008887884179767774 | validation: 0.012657985697912759]
	TIME [epoch: 8.78 sec]
EPOCH 867/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00964288762140542		[learning rate: 0.00019105]
		[batch 20/20] avg loss: 0.005578553017480838		[learning rate: 0.00019059]
	Learning Rate: 0.000190592
	LOSS [training: 0.007610720319443128 | validation: -0.007346150916048605]
	TIME [epoch: 8.77 sec]
EPOCH 868/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.002587954073597282		[learning rate: 0.00019013]
		[batch 20/20] avg loss: 0.007346304616485349		[learning rate: 0.00018967]
	Learning Rate: 0.000189671
	LOSS [training: 0.004967129345041316 | validation: -0.0015642707699771168]
	TIME [epoch: 8.75 sec]
EPOCH 869/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008460078741491947		[learning rate: 0.00018921]
		[batch 20/20] avg loss: 0.011694762694923538		[learning rate: 0.00018875]
	Learning Rate: 0.000188753
	LOSS [training: 0.010077420718207743 | validation: -0.009770062591902502]
	TIME [epoch: 8.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_869.pth
	Model improved!!!
EPOCH 870/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01188745295865171		[learning rate: 0.0001883]
		[batch 20/20] avg loss: 0.00554270953043013		[learning rate: 0.00018784]
	Learning Rate: 0.000187841
	LOSS [training: 0.008715081244540918 | validation: 0.005785667989673768]
	TIME [epoch: 8.76 sec]
EPOCH 871/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01148874078823614		[learning rate: 0.00018739]
		[batch 20/20] avg loss: 0.0014384774059420964		[learning rate: 0.00018693]
	Learning Rate: 0.000186932
	LOSS [training: 0.006463609097089118 | validation: -0.00057207272613335]
	TIME [epoch: 8.75 sec]
EPOCH 872/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.006015968743970078		[learning rate: 0.00018648]
		[batch 20/20] avg loss: 0.007873606664842886		[learning rate: 0.00018603]
	Learning Rate: 0.000186028
	LOSS [training: 0.006944787704406484 | validation: -0.0006611733363494799]
	TIME [epoch: 8.76 sec]
EPOCH 873/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008900879819646348		[learning rate: 0.00018558]
		[batch 20/20] avg loss: 0.00778871726325783		[learning rate: 0.00018513]
	Learning Rate: 0.000185129
	LOSS [training: 0.008344798541452089 | validation: -0.002408733440929251]
	TIME [epoch: 8.76 sec]
EPOCH 874/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005807009470051529		[learning rate: 0.00018468]
		[batch 20/20] avg loss: 0.006019728378922294		[learning rate: 0.00018423]
	Learning Rate: 0.000184233
	LOSS [training: 0.005913368924486914 | validation: 0.0019482770151085915]
	TIME [epoch: 8.76 sec]
EPOCH 875/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.022303747141406403		[learning rate: 0.00018379]
		[batch 20/20] avg loss: 0.00889429276574966		[learning rate: 0.00018334]
	Learning Rate: 0.000183343
	LOSS [training: 0.015599019953578033 | validation: -0.004021781437469417]
	TIME [epoch: 8.77 sec]
EPOCH 876/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.003649203173181074		[learning rate: 0.0001829]
		[batch 20/20] avg loss: 0.004897259183052278		[learning rate: 0.00018246]
	Learning Rate: 0.000182456
	LOSS [training: 0.004273231178116675 | validation: 0.0013686994479839057]
	TIME [epoch: 8.77 sec]
EPOCH 877/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.011844647145150754		[learning rate: 0.00018201]
		[batch 20/20] avg loss: 0.011773994297051938		[learning rate: 0.00018157]
	Learning Rate: 0.000181574
	LOSS [training: 0.011809320721101343 | validation: -0.0024997951809142254]
	TIME [epoch: 8.78 sec]
EPOCH 878/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008487076178831088		[learning rate: 0.00018113]
		[batch 20/20] avg loss: 0.007017382344098999		[learning rate: 0.0001807]
	Learning Rate: 0.000180696
	LOSS [training: 0.007752229261465042 | validation: 0.0020697746757418656]
	TIME [epoch: 8.76 sec]
EPOCH 879/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010255725141754298		[learning rate: 0.00018026]
		[batch 20/20] avg loss: 0.006402041196247233		[learning rate: 0.00017982]
	Learning Rate: 0.000179822
	LOSS [training: 0.008328883169000764 | validation: 0.0019745133968799063]
	TIME [epoch: 8.75 sec]
EPOCH 880/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00940884486647323		[learning rate: 0.00017939]
		[batch 20/20] avg loss: 0.007324152308139058		[learning rate: 0.00017895]
	Learning Rate: 0.000178952
	LOSS [training: 0.008366498587306145 | validation: 0.005990491078333054]
	TIME [epoch: 8.76 sec]
EPOCH 881/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004721135001754017		[learning rate: 0.00017852]
		[batch 20/20] avg loss: 0.015787144837239662		[learning rate: 0.00017809]
	Learning Rate: 0.000178087
	LOSS [training: 0.010254139919496839 | validation: 0.007916971842125517]
	TIME [epoch: 8.76 sec]
EPOCH 882/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00526173794562381		[learning rate: 0.00017766]
		[batch 20/20] avg loss: 0.004853194067468234		[learning rate: 0.00017723]
	Learning Rate: 0.000177226
	LOSS [training: 0.005057466006546021 | validation: 0.007601891218749997]
	TIME [epoch: 8.76 sec]
EPOCH 883/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029602006018864114		[learning rate: 0.0001768]
		[batch 20/20] avg loss: 0.004806211576437756		[learning rate: 0.00017637]
	Learning Rate: 0.000176369
	LOSS [training: 0.003883206089162084 | validation: 0.0006606707385090117]
	TIME [epoch: 8.78 sec]
EPOCH 884/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007087388556461335		[learning rate: 0.00017594]
		[batch 20/20] avg loss: 0.009010957665794912		[learning rate: 0.00017552]
	Learning Rate: 0.000175516
	LOSS [training: 0.008049173111128124 | validation: 0.001446870513205544]
	TIME [epoch: 8.76 sec]
EPOCH 885/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005894416532352899		[learning rate: 0.00017509]
		[batch 20/20] avg loss: 0.001895945856699889		[learning rate: 0.00017467]
	Learning Rate: 0.000174667
	LOSS [training: 0.0038951811945263947 | validation: 0.004864410907656794]
	TIME [epoch: 8.75 sec]
EPOCH 886/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007988425425590362		[learning rate: 0.00017424]
		[batch 20/20] avg loss: 0.006728806484778775		[learning rate: 0.00017382]
	Learning Rate: 0.000173822
	LOSS [training: 0.007358615955184567 | validation: 0.0015750090267438175]
	TIME [epoch: 8.76 sec]
EPOCH 887/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.012464219190598232		[learning rate: 0.0001734]
		[batch 20/20] avg loss: 0.012098896977405446		[learning rate: 0.00017298]
	Learning Rate: 0.000172982
	LOSS [training: 0.012281558084001836 | validation: -0.0012643686166289594]
	TIME [epoch: 8.76 sec]
EPOCH 888/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007266481992412799		[learning rate: 0.00017256]
		[batch 20/20] avg loss: 0.0019741799733617796		[learning rate: 0.00017215]
	Learning Rate: 0.000172145
	LOSS [training: 0.004620330982887289 | validation: 0.004372937911221361]
	TIME [epoch: 8.78 sec]
EPOCH 889/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.014843572444736688		[learning rate: 0.00017173]
		[batch 20/20] avg loss: 0.000669737407919209		[learning rate: 0.00017131]
	Learning Rate: 0.000171313
	LOSS [training: 0.007756654926327948 | validation: -0.00339009252196473]
	TIME [epoch: 8.77 sec]
EPOCH 890/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004097118788015665		[learning rate: 0.0001709]
		[batch 20/20] avg loss: 0.0037811196436356		[learning rate: 0.00017048]
	Learning Rate: 0.000170484
	LOSS [training: 0.003939119215825633 | validation: 0.0006394647889040482]
	TIME [epoch: 8.76 sec]
EPOCH 891/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004471332578245726		[learning rate: 0.00017007]
		[batch 20/20] avg loss: 0.00351023530835938		[learning rate: 0.00016966]
	Learning Rate: 0.00016966
	LOSS [training: 0.003990783943302554 | validation: -0.0017356333255171804]
	TIME [epoch: 8.74 sec]
EPOCH 892/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008982508329579331		[learning rate: 0.00016925]
		[batch 20/20] avg loss: 0.0057378130772121495		[learning rate: 0.00016884]
	Learning Rate: 0.000168839
	LOSS [training: 0.00736016070339574 | validation: 0.001287047552059739]
	TIME [epoch: 8.75 sec]
EPOCH 893/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005980939218567058		[learning rate: 0.00016843]
		[batch 20/20] avg loss: 0.019407175399967676		[learning rate: 0.00016802]
	Learning Rate: 0.000168023
	LOSS [training: 0.012694057309267365 | validation: 0.013415818264618419]
	TIME [epoch: 8.74 sec]
EPOCH 894/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010622747080941528		[learning rate: 0.00016762]
		[batch 20/20] avg loss: 0.020597433496455916		[learning rate: 0.00016721]
	Learning Rate: 0.00016721
	LOSS [training: 0.015610090288698721 | validation: 0.004028154109630202]
	TIME [epoch: 8.77 sec]
EPOCH 895/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.006117460299192199		[learning rate: 0.00016681]
		[batch 20/20] avg loss: 0.008552874717917238		[learning rate: 0.0001664]
	Learning Rate: 0.000166402
	LOSS [training: 0.0073351675085547175 | validation: 0.00694740962838049]
	TIME [epoch: 8.75 sec]
EPOCH 896/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.012423843991402252		[learning rate: 0.000166]
		[batch 20/20] avg loss: 0.016017290881021044		[learning rate: 0.0001656]
	Learning Rate: 0.000165597
	LOSS [training: 0.01422056743621165 | validation: 0.0020850695762916994]
	TIME [epoch: 8.74 sec]
EPOCH 897/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.014645025707017884		[learning rate: 0.0001652]
		[batch 20/20] avg loss: 0.011200540962762594		[learning rate: 0.0001648]
	Learning Rate: 0.000164796
	LOSS [training: 0.012922783334890239 | validation: -0.001616473133275057]
	TIME [epoch: 8.75 sec]
EPOCH 898/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008177500478525954		[learning rate: 0.0001644]
		[batch 20/20] avg loss: 0.003096869881621428		[learning rate: 0.000164]
	Learning Rate: 0.000163999
	LOSS [training: 0.005637185180073692 | validation: -0.007277461501396671]
	TIME [epoch: 8.74 sec]
EPOCH 899/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0033985287383017314		[learning rate: 0.0001636]
		[batch 20/20] avg loss: 0.004636702165240811		[learning rate: 0.00016321]
	Learning Rate: 0.000163206
	LOSS [training: 0.00401761545177127 | validation: -0.0026100947917912284]
	TIME [epoch: 8.77 sec]
EPOCH 900/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.001444819723411428		[learning rate: 0.00016281]
		[batch 20/20] avg loss: 0.008975173668745789		[learning rate: 0.00016242]
	Learning Rate: 0.000162417
	LOSS [training: 0.005209996696078608 | validation: -0.002683874268718674]
	TIME [epoch: 8.77 sec]
EPOCH 901/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008627792744915045		[learning rate: 0.00016202]
		[batch 20/20] avg loss: 0.005357263632599555		[learning rate: 0.00016163]
	Learning Rate: 0.000161632
	LOSS [training: 0.006992528188757301 | validation: 0.008332880812515027]
	TIME [epoch: 8.76 sec]
EPOCH 902/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01260322852738592		[learning rate: 0.00016124]
		[batch 20/20] avg loss: -0.0011031916979831239		[learning rate: 0.00016085]
	Learning Rate: 0.00016085
	LOSS [training: 0.005750018414701399 | validation: -0.00044290448127742757]
	TIME [epoch: 8.77 sec]
EPOCH 903/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.002051738875561305		[learning rate: 0.00016046]
		[batch 20/20] avg loss: 0.00261404769614371		[learning rate: 0.00016007]
	Learning Rate: 0.000160072
	LOSS [training: 0.002332893285852507 | validation: -0.002973116109962212]
	TIME [epoch: 8.75 sec]
EPOCH 904/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005137406577711433		[learning rate: 0.00015968]
		[batch 20/20] avg loss: 0.006091444225533604		[learning rate: 0.0001593]
	Learning Rate: 0.000159298
	LOSS [training: 0.0056144254016225175 | validation: 0.0035367173149272848]
	TIME [epoch: 8.76 sec]
EPOCH 905/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005672270148378715		[learning rate: 0.00015891]
		[batch 20/20] avg loss: 0.006693028546374838		[learning rate: 0.00015853]
	Learning Rate: 0.000158528
	LOSS [training: 0.006182649347376775 | validation: -0.007769549281127682]
	TIME [epoch: 8.78 sec]
EPOCH 906/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.002673068139208833		[learning rate: 0.00015814]
		[batch 20/20] avg loss: 0.010323084275264627		[learning rate: 0.00015776]
	Learning Rate: 0.000157761
	LOSS [training: 0.006498076207236729 | validation: -0.0005773796597631315]
	TIME [epoch: 8.76 sec]
EPOCH 907/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004383251576222084		[learning rate: 0.00015738]
		[batch 20/20] avg loss: 0.0071941325150409255		[learning rate: 0.000157]
	Learning Rate: 0.000156998
	LOSS [training: 0.005788692045631505 | validation: 0.0022938011790413063]
	TIME [epoch: 8.75 sec]
EPOCH 908/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00862797796730137		[learning rate: 0.00015662]
		[batch 20/20] avg loss: 0.006315945048069575		[learning rate: 0.00015624]
	Learning Rate: 0.000156239
	LOSS [training: 0.0074719615076854744 | validation: -0.0013266383124943325]
	TIME [epoch: 8.75 sec]
EPOCH 909/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005126625634518876		[learning rate: 0.00015586]
		[batch 20/20] avg loss: 0.005495259307035488		[learning rate: 0.00015548]
	Learning Rate: 0.000155483
	LOSS [training: 0.005310942470777182 | validation: 0.006976524863758106]
	TIME [epoch: 8.76 sec]
EPOCH 910/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.009188349943292724		[learning rate: 0.00015511]
		[batch 20/20] avg loss: 0.00987295416953814		[learning rate: 0.00015473]
	Learning Rate: 0.000154732
	LOSS [training: 0.009530652056415432 | validation: 0.005199402106089255]
	TIME [epoch: 8.75 sec]
EPOCH 911/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01294946173546135		[learning rate: 0.00015436]
		[batch 20/20] avg loss: 0.0032341392320074753		[learning rate: 0.00015398]
	Learning Rate: 0.000153983
	LOSS [training: 0.008091800483734413 | validation: 0.010403214312569572]
	TIME [epoch: 8.79 sec]
EPOCH 912/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.011578863500569447		[learning rate: 0.00015361]
		[batch 20/20] avg loss: 0.0029556593261537505		[learning rate: 0.00015324]
	Learning Rate: 0.000153239
	LOSS [training: 0.007267261413361601 | validation: 0.0037351333323394622]
	TIME [epoch: 8.77 sec]
EPOCH 913/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.011682714016028608		[learning rate: 0.00015287]
		[batch 20/20] avg loss: 0.006089336106841896		[learning rate: 0.0001525]
	Learning Rate: 0.000152498
	LOSS [training: 0.008886025061435253 | validation: -0.0020088236466145382]
	TIME [epoch: 8.75 sec]
EPOCH 914/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00431138771903277		[learning rate: 0.00015213]
		[batch 20/20] avg loss: 0.00082433067287189		[learning rate: 0.00015176]
	Learning Rate: 0.00015176
	LOSS [training: 0.00256785919595233 | validation: -0.005669508521427252]
	TIME [epoch: 8.76 sec]
EPOCH 915/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.009335192039847172		[learning rate: 0.00015139]
		[batch 20/20] avg loss: 0.005774265495005539		[learning rate: 0.00015103]
	Learning Rate: 0.000151026
	LOSS [training: 0.0075547287674263535 | validation: -0.0005231799225309825]
	TIME [epoch: 8.76 sec]
EPOCH 916/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.014497376988513246		[learning rate: 0.00015066]
		[batch 20/20] avg loss: 0.0037544533875674318		[learning rate: 0.0001503]
	Learning Rate: 0.000150296
	LOSS [training: 0.00912591518804034 | validation: 0.00020184571485401103]
	TIME [epoch: 8.77 sec]
EPOCH 917/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.006000786207833853		[learning rate: 0.00014993]
		[batch 20/20] avg loss: 0.005078364430060308		[learning rate: 0.00014957]
	Learning Rate: 0.000149569
	LOSS [training: 0.00553957531894708 | validation: 0.0009896652254016772]
	TIME [epoch: 8.79 sec]
EPOCH 918/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.011342868207982939		[learning rate: 0.00014921]
		[batch 20/20] avg loss: 0.005543699924424155		[learning rate: 0.00014885]
	Learning Rate: 0.000148846
	LOSS [training: 0.008443284066203548 | validation: -0.0034285974427460744]
	TIME [epoch: 8.76 sec]
EPOCH 919/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.006060938641650559		[learning rate: 0.00014849]
		[batch 20/20] avg loss: 0.018138538683318096		[learning rate: 0.00014813]
	Learning Rate: 0.000148126
	LOSS [training: 0.012099738662484327 | validation: 0.004271970240846025]
	TIME [epoch: 8.75 sec]
EPOCH 920/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.012495877567164798		[learning rate: 0.00014777]
		[batch 20/20] avg loss: 0.007760390293467723		[learning rate: 0.00014741]
	Learning Rate: 0.00014741
	LOSS [training: 0.01012813393031626 | validation: 0.012823467508970942]
	TIME [epoch: 8.75 sec]
EPOCH 921/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.017468981294238118		[learning rate: 0.00014705]
		[batch 20/20] avg loss: 0.010060517762670584		[learning rate: 0.0001467]
	Learning Rate: 0.000146697
	LOSS [training: 0.013764749528454351 | validation: -0.008440826904963557]
	TIME [epoch: 8.76 sec]
EPOCH 922/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0011062126908301787		[learning rate: 0.00014634]
		[batch 20/20] avg loss: 0.0072576371628649645		[learning rate: 0.00014599]
	Learning Rate: 0.000145988
	LOSS [training: 0.004181924926847573 | validation: 0.003881575188740247]
	TIME [epoch: 8.79 sec]
EPOCH 923/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00022060875819377156		[learning rate: 0.00014563]
		[batch 20/20] avg loss: 0.006970815814606656		[learning rate: 0.00014528]
	Learning Rate: 0.000145282
	LOSS [training: 0.0035957122864002143 | validation: -0.004417095238593085]
	TIME [epoch: 8.76 sec]
EPOCH 924/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006595553673183393		[learning rate: 0.00014493]
		[batch 20/20] avg loss: 0.006757629384951701		[learning rate: 0.00014458]
	Learning Rate: 0.000144579
	LOSS [training: 0.0037085923761350204 | validation: -0.004046234774148463]
	TIME [epoch: 8.76 sec]
EPOCH 925/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.001094900674249675		[learning rate: 0.00014423]
		[batch 20/20] avg loss: 0.0018721200253768798		[learning rate: 0.00014388]
	Learning Rate: 0.00014388
	LOSS [training: 0.0014835103498132773 | validation: -0.008086072161035554]
	TIME [epoch: 8.75 sec]
EPOCH 926/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.011200155417480099		[learning rate: 0.00014353]
		[batch 20/20] avg loss: 0.010119062902656158		[learning rate: 0.00014318]
	Learning Rate: 0.000143184
	LOSS [training: 0.010659609160068128 | validation: -0.0034337483217159815]
	TIME [epoch: 8.76 sec]
EPOCH 927/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005023332745664773		[learning rate: 0.00014284]
		[batch 20/20] avg loss: 0.0017459643933802871		[learning rate: 0.00014249]
	Learning Rate: 0.000142492
	LOSS [training: 0.0033846485695225295 | validation: -0.0012795439535359305]
	TIME [epoch: 8.76 sec]
EPOCH 928/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004012353726606353		[learning rate: 0.00014215]
		[batch 20/20] avg loss: 0.006693450633272253		[learning rate: 0.0001418]
	Learning Rate: 0.000141803
	LOSS [training: 0.005352902179939303 | validation: -0.002669180919204493]
	TIME [epoch: 8.77 sec]
EPOCH 929/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.01284383285771436		[learning rate: 0.00014146]
		[batch 20/20] avg loss: 0.003111469473385058		[learning rate: 0.00014112]
	Learning Rate: 0.000141117
	LOSS [training: 0.007977651165549708 | validation: -0.008499208254626984]
	TIME [epoch: 8.76 sec]
EPOCH 930/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010943621047622882		[learning rate: 0.00014078]
		[batch 20/20] avg loss: 0.012378665171209516		[learning rate: 0.00014043]
	Learning Rate: 0.000140434
	LOSS [training: 0.0116611431094162 | validation: 0.0007149905026425053]
	TIME [epoch: 8.77 sec]
EPOCH 931/1000:
	Training over batches...
		[batch 10/20] avg loss: -0.0010376507963356905		[learning rate: 0.00014009]
		[batch 20/20] avg loss: 0.006761637314383373		[learning rate: 0.00013976]
	Learning Rate: 0.000139755
	LOSS [training: 0.0028619932590238416 | validation: 0.008976811545558119]
	TIME [epoch: 8.76 sec]
EPOCH 932/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007174249475085412		[learning rate: 0.00013942]
		[batch 20/20] avg loss: 0.0009126787009355434		[learning rate: 0.00013908]
	Learning Rate: 0.00013908
	LOSS [training: 0.004043464088010477 | validation: 0.005501490607691649]
	TIME [epoch: 8.75 sec]
EPOCH 933/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010665638136678356		[learning rate: 0.00013874]
		[batch 20/20] avg loss: 0.00396326723582369		[learning rate: 0.00013841]
	Learning Rate: 0.000138407
	LOSS [training: 0.007314452686251025 | validation: -0.00407251611868411]
	TIME [epoch: 8.78 sec]
EPOCH 934/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.003166503963207769		[learning rate: 0.00013807]
		[batch 20/20] avg loss: 0.0035480466031825803		[learning rate: 0.00013774]
	Learning Rate: 0.000137738
	LOSS [training: 0.003357275283195174 | validation: -0.0017319683449739268]
	TIME [epoch: 8.77 sec]
EPOCH 935/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.009546166293270679		[learning rate: 0.0001374]
		[batch 20/20] avg loss: 0.001971642866991206		[learning rate: 0.00013707]
	Learning Rate: 0.000137072
	LOSS [training: 0.005758904580130942 | validation: 0.004594633091575594]
	TIME [epoch: 8.75 sec]
EPOCH 936/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.003117259269938091		[learning rate: 0.00013674]
		[batch 20/20] avg loss: 0.009663873435142729		[learning rate: 0.00013641]
	Learning Rate: 0.000136409
	LOSS [training: 0.0063905663525404095 | validation: 0.0029334193879387107]
	TIME [epoch: 8.76 sec]
EPOCH 937/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.019027668362719824		[learning rate: 0.00013608]
		[batch 20/20] avg loss: 0.008021212519466598		[learning rate: 0.00013575]
	Learning Rate: 0.000135749
	LOSS [training: 0.013524440441093211 | validation: 0.005555965585942765]
	TIME [epoch: 8.75 sec]
EPOCH 938/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028617168263266783		[learning rate: 0.00013542]
		[batch 20/20] avg loss: 0.009794439430365778		[learning rate: 0.00013509]
	Learning Rate: 0.000135093
	LOSS [training: 0.00632807812834623 | validation: -0.01291869466183807]
	TIME [epoch: 8.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r0_20240217_161441/states/model_tr_study2_938.pth
	Model improved!!!
EPOCH 939/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0015209233954953006		[learning rate: 0.00013477]
		[batch 20/20] avg loss: 0.01085177099468733		[learning rate: 0.00013444]
	Learning Rate: 0.000134439
	LOSS [training: 0.0061863471950913155 | validation: 0.0013319077113403098]
	TIME [epoch: 8.78 sec]
EPOCH 940/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.003635237995554174		[learning rate: 0.00013411]
		[batch 20/20] avg loss: 0.01037678767435571		[learning rate: 0.00013379]
	Learning Rate: 0.000133789
	LOSS [training: 0.007006012834954943 | validation: 0.009978646914245756]
	TIME [epoch: 8.76 sec]
EPOCH 941/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010078129502218076		[learning rate: 0.00013347]
		[batch 20/20] avg loss: 0.0038323324404240925		[learning rate: 0.00013314]
	Learning Rate: 0.000133142
	LOSS [training: 0.006955230971321084 | validation: 0.003412450596569841]
	TIME [epoch: 8.75 sec]
EPOCH 942/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004639414129403596		[learning rate: 0.00013282]
		[batch 20/20] avg loss: 0.0016541294521614952		[learning rate: 0.0001325]
	Learning Rate: 0.000132498
	LOSS [training: 0.003146771790782546 | validation: -0.005395679058561561]
	TIME [epoch: 8.76 sec]
EPOCH 943/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007170280026116173		[learning rate: 0.00013218]
		[batch 20/20] avg loss: 0.0009015184359180428		[learning rate: 0.00013186]
	Learning Rate: 0.000131858
	LOSS [training: 0.004035899231017108 | validation: -0.0013931192151971396]
	TIME [epoch: 8.78 sec]
EPOCH 944/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.006644337780137087		[learning rate: 0.00013154]
		[batch 20/20] avg loss: 0.006986910645275122		[learning rate: 0.00013122]
	Learning Rate: 0.00013122
	LOSS [training: 0.006815624212706105 | validation: 0.005316032710364775]
	TIME [epoch: 8.78 sec]
EPOCH 945/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0028816514111699573		[learning rate: 0.0001309]
		[batch 20/20] avg loss: -0.002545266757970157		[learning rate: 0.00013059]
	Learning Rate: 0.000130585
	LOSS [training: 0.00016819232659990007 | validation: -0.006460946792923895]
	TIME [epoch: 8.79 sec]
EPOCH 946/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0027752058236713215		[learning rate: 0.00013027]
		[batch 20/20] avg loss: 0.00359187616856727		[learning rate: 0.00012995]
	Learning Rate: 0.000129954
	LOSS [training: 0.003183540996119296 | validation: -0.0022866473656794097]
	TIME [epoch: 8.75 sec]
EPOCH 947/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.003960294900269122		[learning rate: 0.00012964]
		[batch 20/20] avg loss: 0.008189629239736453		[learning rate: 0.00012933]
	Learning Rate: 0.000129326
	LOSS [training: 0.006074962070002787 | validation: -0.003017403322637025]
	TIME [epoch: 8.75 sec]
EPOCH 948/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007587272452384819		[learning rate: 0.00012901]
		[batch 20/20] avg loss: 0.0004052660360390573		[learning rate: 0.0001287]
	Learning Rate: 0.0001287
	LOSS [training: 0.003996269244211938 | validation: -0.002556217581589798]
	TIME [epoch: 8.75 sec]
EPOCH 949/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00450048642259389		[learning rate: 0.00012839]
		[batch 20/20] avg loss: 0.00878147753985557		[learning rate: 0.00012808]
	Learning Rate: 0.000128078
	LOSS [training: 0.006640981981224731 | validation: -0.0030871681334404765]
	TIME [epoch: 8.76 sec]
EPOCH 950/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0006264318043695883		[learning rate: 0.00012777]
		[batch 20/20] avg loss: 0.005413740304046305		[learning rate: 0.00012746]
	Learning Rate: 0.000127458
	LOSS [training: 0.0030200860542079462 | validation: 0.011675588814793338]
	TIME [epoch: 8.78 sec]
EPOCH 951/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.012162546490250961		[learning rate: 0.00012715]
		[batch 20/20] avg loss: 0.0007969903165599066		[learning rate: 0.00012684]
	Learning Rate: 0.000126842
	LOSS [training: 0.0064797684034054335 | validation: 0.005130660925147105]
	TIME [epoch: 8.76 sec]
EPOCH 952/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.009655213507282835		[learning rate: 0.00012653]
		[batch 20/20] avg loss: 0.0011152504847451931		[learning rate: 0.00012623]
	Learning Rate: 0.000126229
	LOSS [training: 0.005385231996014014 | validation: -0.0011869607395130835]
	TIME [epoch: 8.76 sec]
EPOCH 953/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0019462603276179484		[learning rate: 0.00012592]
		[batch 20/20] avg loss: 0.009222852453081692		[learning rate: 0.00012562]
	Learning Rate: 0.000125618
	LOSS [training: 0.005584556390349821 | validation: 0.0022098308736091398]
	TIME [epoch: 8.75 sec]
EPOCH 954/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.006048395136828892		[learning rate: 0.00012531]
		[batch 20/20] avg loss: 0.006056992886878598		[learning rate: 0.00012501]
	Learning Rate: 0.000125011
	LOSS [training: 0.0060526940118537445 | validation: -0.0017018903591323176]
	TIME [epoch: 8.75 sec]
EPOCH 955/1000:
	Training over batches...
		[batch 10/20] avg loss: -0.00018238019145720314		[learning rate: 0.00012471]
		[batch 20/20] avg loss: 0.008692532371595138		[learning rate: 0.00012441]
	Learning Rate: 0.000124406
	LOSS [training: 0.004255076090068968 | validation: -0.002306172845579064]
	TIME [epoch: 8.76 sec]
EPOCH 956/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008199443890446568		[learning rate: 0.00012411]
		[batch 20/20] avg loss: 0.007507674327571804		[learning rate: 0.0001238]
	Learning Rate: 0.000123805
	LOSS [training: 0.007853559109009188 | validation: -0.007226377552423403]
	TIME [epoch: 8.77 sec]
EPOCH 957/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004135486211410783		[learning rate: 0.0001235]
		[batch 20/20] avg loss: 0.0060724608834696065		[learning rate: 0.00012321]
	Learning Rate: 0.000123206
	LOSS [training: 0.005103973547440196 | validation: -0.0018770817526402234]
	TIME [epoch: 8.78 sec]
EPOCH 958/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.003193962565297416		[learning rate: 0.00012291]
		[batch 20/20] avg loss: 0.0037102885871020583		[learning rate: 0.00012261]
	Learning Rate: 0.00012261
	LOSS [training: 0.0034521255761997367 | validation: 0.0016882521303908048]
	TIME [epoch: 8.78 sec]
EPOCH 959/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.002905021899775308		[learning rate: 0.00012231]
		[batch 20/20] avg loss: 0.00663333012104249		[learning rate: 0.00012202]
	Learning Rate: 0.000122017
	LOSS [training: 0.004769176010408899 | validation: -0.005015992910722192]
	TIME [epoch: 8.76 sec]
EPOCH 960/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.001334396778641943		[learning rate: 0.00012172]
		[batch 20/20] avg loss: 0.0042213704605848406		[learning rate: 0.00012143]
	Learning Rate: 0.000121427
	LOSS [training: 0.0027778836196133924 | validation: -0.00749560868987133]
	TIME [epoch: 8.76 sec]
EPOCH 961/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.010488822451075104		[learning rate: 0.00012113]
		[batch 20/20] avg loss: 0.0022005104183411457		[learning rate: 0.00012084]
	Learning Rate: 0.00012084
	LOSS [training: 0.006344666434708125 | validation: -0.002124317093907556]
	TIME [epoch: 8.77 sec]
EPOCH 962/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0087932896730482		[learning rate: 0.00012055]
		[batch 20/20] avg loss: 0.00030173249299986687		[learning rate: 0.00012026]
	Learning Rate: 0.000120256
	LOSS [training: 0.004547511083024034 | validation: -0.003245861632174547]
	TIME [epoch: 8.76 sec]
EPOCH 963/1000:
	Training over batches...
		[batch 10/20] avg loss: -0.0008148746172959773		[learning rate: 0.00011996]
		[batch 20/20] avg loss: 0.007650402056023917		[learning rate: 0.00011967]
	Learning Rate: 0.000119674
	LOSS [training: 0.0034177637193639697 | validation: -0.0027713035304976585]
	TIME [epoch: 8.75 sec]
EPOCH 964/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0007278499625770925		[learning rate: 0.00011938]
		[batch 20/20] avg loss: 0.004720190482428916		[learning rate: 0.0001191]
	Learning Rate: 0.000119095
	LOSS [training: 0.0027240202225030036 | validation: 0.0009256418989147157]
	TIME [epoch: 8.75 sec]
EPOCH 965/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0048039337350523886		[learning rate: 0.00011881]
		[batch 20/20] avg loss: 0.008012388474410916		[learning rate: 0.00011852]
	Learning Rate: 0.000118519
	LOSS [training: 0.006408161104731652 | validation: -0.0031814909290812992]
	TIME [epoch: 8.76 sec]
EPOCH 966/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008193689934367209		[learning rate: 0.00011823]
		[batch 20/20] avg loss: 0.006380488985395361		[learning rate: 0.00011795]
	Learning Rate: 0.000117946
	LOSS [training: 0.007287089459881285 | validation: 0.0002877404152687524]
	TIME [epoch: 8.74 sec]
EPOCH 967/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007569204614420027		[learning rate: 0.00011766]
		[batch 20/20] avg loss: 0.0020893904996896113		[learning rate: 0.00011738]
	Learning Rate: 0.000117376
	LOSS [training: 0.00482929755705482 | validation: -0.0026901467658964197]
	TIME [epoch: 8.79 sec]
EPOCH 968/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.007784200321234276		[learning rate: 0.00011709]
		[batch 20/20] avg loss: 0.0003200308861665337		[learning rate: 0.00011681]
	Learning Rate: 0.000116808
	LOSS [training: 0.004052115603700405 | validation: -0.008720941236593829]
	TIME [epoch: 8.75 sec]
EPOCH 969/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.002412920271610905		[learning rate: 0.00011653]
		[batch 20/20] avg loss: 0.004148080270545066		[learning rate: 0.00011624]
	Learning Rate: 0.000116243
	LOSS [training: 0.003280500271077985 | validation: 0.0016244176806288944]
	TIME [epoch: 8.77 sec]
EPOCH 970/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004191714965008052		[learning rate: 0.00011596]
		[batch 20/20] avg loss: 0.007245693455719432		[learning rate: 0.00011568]
	Learning Rate: 0.000115681
	LOSS [training: 0.005718704210363741 | validation: -0.007114685297623437]
	TIME [epoch: 8.76 sec]
EPOCH 971/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029590527661035103		[learning rate: 0.0001154]
		[batch 20/20] avg loss: 0.003404023607661022		[learning rate: 0.00011512]
	Learning Rate: 0.000115122
	LOSS [training: 0.0031815381868822656 | validation: 0.0035050632605227723]
	TIME [epoch: 8.76 sec]
EPOCH 972/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0014960769242280585		[learning rate: 0.00011484]
		[batch 20/20] avg loss: 0.002647583306867696		[learning rate: 0.00011457]
	Learning Rate: 0.000114565
	LOSS [training: 0.002071830115547878 | validation: 0.008117130397404419]
	TIME [epoch: 8.77 sec]
EPOCH 973/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.009049618916159189		[learning rate: 0.00011429]
		[batch 20/20] avg loss: 0.010142319389152294		[learning rate: 0.00011401]
	Learning Rate: 0.000114011
	LOSS [training: 0.009595969152655742 | validation: 0.006355886506663214]
	TIME [epoch: 8.78 sec]
EPOCH 974/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004354971320604355		[learning rate: 0.00011374]
		[batch 20/20] avg loss: 0.0037198760583255787		[learning rate: 0.00011346]
	Learning Rate: 0.00011346
	LOSS [training: 0.004037423689464966 | validation: -0.0009794744593503548]
	TIME [epoch: 8.75 sec]
EPOCH 975/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.004828206309905003		[learning rate: 0.00011319]
		[batch 20/20] avg loss: 0.008769665743078463		[learning rate: 0.00011291]
	Learning Rate: 0.000112911
	LOSS [training: 0.006798936026491733 | validation: -0.0019580855603923476]
	TIME [epoch: 8.75 sec]
EPOCH 976/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.008672813408422422		[learning rate: 0.00011264]
		[batch 20/20] avg loss: 0.0063600575025054935		[learning rate: 0.00011237]
	Learning Rate: 0.000112365
	LOSS [training: 0.007516435455463958 | validation: -0.000261267700119678]
	TIME [epoch: 8.76 sec]
EPOCH 977/1000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006414399642303518		[learning rate: 0.00011209]
		[batch 20/20] avg loss: 0.006102008771122641		[learning rate: 0.00011182]
	Learning Rate: 0.000111822
	LOSS [training: 0.002730284403446145 | validation: -0.00595093662073121]
	TIME [epoch: 8.75 sec]
EPOCH 978/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.012772195017554156		[learning rate: 0.00011155]
		[batch 20/20] avg loss: 0.008561947645433284		[learning rate: 0.00011128]
	Learning Rate: 0.000111281
	LOSS [training: 0.010667071331493716 | validation: -0.007202705988086686]
	TIME [epoch: 8.77 sec]
EPOCH 979/1000:
	Training over batches...
		[batch 10/20] avg loss: -0.0029342076425571163		[learning rate: 0.00011101]
		[batch 20/20] avg loss: 0.004333190118884818		[learning rate: 0.00011074]
	Learning Rate: 0.000110743
	LOSS [training: 0.0006994912381638501 | validation: -0.0001865468444335965]
	TIME [epoch: 8.76 sec]
EPOCH 980/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0009019726504181357		[learning rate: 0.00011047]
		[batch 20/20] avg loss: 0.011688764186737536		[learning rate: 0.00011021]
	Learning Rate: 0.000110207
	LOSS [training: 0.006295368418577835 | validation: -0.0020494010851487975]
	TIME [epoch: 8.76 sec]
EPOCH 981/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005842999014373349		[learning rate: 0.00010994]
		[batch 20/20] avg loss: 0.0007728891476139477		[learning rate: 0.00010967]
	Learning Rate: 0.000109674
	LOSS [training: 0.003307944080993648 | validation: -0.004742671016035536]
	TIME [epoch: 8.75 sec]
EPOCH 982/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0063341997504953755		[learning rate: 0.00010941]
		[batch 20/20] avg loss: 0.011070288927090591		[learning rate: 0.00010914]
	Learning Rate: 0.000109144
	LOSS [training: 0.008702244338792983 | validation: -0.0008511947174476859]
	TIME [epoch: 8.76 sec]
EPOCH 983/1000:
	Training over batches...
		[batch 10/20] avg loss: -0.001097623767506629		[learning rate: 0.00010888]
		[batch 20/20] avg loss: 0.008672374614022479		[learning rate: 0.00010862]
	Learning Rate: 0.000108616
	LOSS [training: 0.0037873754232579256 | validation: -0.000963597297529719]
	TIME [epoch: 8.76 sec]
EPOCH 984/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005025746984092705		[learning rate: 0.00010835]
		[batch 20/20] avg loss: 0.004239300562445477		[learning rate: 0.00010809]
	Learning Rate: 0.000108091
	LOSS [training: 0.004632523773269091 | validation: -0.002768454885836325]
	TIME [epoch: 8.79 sec]
EPOCH 985/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.003554557304987186		[learning rate: 0.00010783]
		[batch 20/20] avg loss: 0.005189709390707978		[learning rate: 0.00010757]
	Learning Rate: 0.000107568
	LOSS [training: 0.004372133347847582 | validation: -0.005093210123937797]
	TIME [epoch: 8.76 sec]
EPOCH 986/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0025271226825447435		[learning rate: 0.00010731]
		[batch 20/20] avg loss: 0.003649514161450705		[learning rate: 0.00010705]
	Learning Rate: 0.000107048
	LOSS [training: 0.0030883184219977237 | validation: -0.003891666714743148]
	TIME [epoch: 8.76 sec]
EPOCH 987/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0010788713768600231		[learning rate: 0.00010679]
		[batch 20/20] avg loss: 0.0010395513280410805		[learning rate: 0.00010653]
	Learning Rate: 0.00010653
	LOSS [training: 0.0010592113524505518 | validation: -0.00784283834739177]
	TIME [epoch: 8.75 sec]
EPOCH 988/1000:
	Training over batches...
		[batch 10/20] avg loss: -0.002308384161845918		[learning rate: 0.00010627]
		[batch 20/20] avg loss: 0.0061359856248269585		[learning rate: 0.00010602]
	Learning Rate: 0.000106015
	LOSS [training: 0.0019138007314905199 | validation: -0.0038497121397871583]
	TIME [epoch: 8.75 sec]
EPOCH 989/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0050511018711231195		[learning rate: 0.00010576]
		[batch 20/20] avg loss: 0.0030665334759228734		[learning rate: 0.0001055]
	Learning Rate: 0.000105503
	LOSS [training: 0.004058817673522996 | validation: 0.0005632011281290359]
	TIME [epoch: 8.77 sec]
EPOCH 990/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005499315837227301		[learning rate: 0.00010525]
		[batch 20/20] avg loss: 0.0011721109243093073		[learning rate: 0.00010499]
	Learning Rate: 0.000104992
	LOSS [training: 0.003335713380768304 | validation: -0.001740710667401199]
	TIME [epoch: 8.75 sec]
EPOCH 991/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.005122261292298951		[learning rate: 0.00010474]
		[batch 20/20] avg loss: 0.009149840684115222		[learning rate: 0.00010448]
	Learning Rate: 0.000104485
	LOSS [training: 0.007136050988207086 | validation: 0.006852359884658921]
	TIME [epoch: 8.76 sec]
EPOCH 992/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.00790484323247871		[learning rate: 0.00010423]
		[batch 20/20] avg loss: 0.0029467070282836413		[learning rate: 0.00010398]
	Learning Rate: 0.000103979
	LOSS [training: 0.005425775130381176 | validation: -0.0010973333406697474]
	TIME [epoch: 8.75 sec]
EPOCH 993/1000:
	Training over batches...
		[batch 10/20] avg loss: -0.0006676732740012603		[learning rate: 0.00010373]
		[batch 20/20] avg loss: 0.005113475721607988		[learning rate: 0.00010348]
	Learning Rate: 0.000103477
	LOSS [training: 0.0022229012238033636 | validation: 0.000216847320616333]
	TIME [epoch: 8.75 sec]
EPOCH 994/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.006927609099495223		[learning rate: 0.00010323]
		[batch 20/20] avg loss: 0.0013774413817346843		[learning rate: 0.00010298]
	Learning Rate: 0.000102976
	LOSS [training: 0.004152525240614954 | validation: -0.0003837593963188486]
	TIME [epoch: 8.75 sec]
EPOCH 995/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0029831820971743437		[learning rate: 0.00010273]
		[batch 20/20] avg loss: 0.006982802906586498		[learning rate: 0.00010248]
	Learning Rate: 0.000102478
	LOSS [training: 0.00498299250188042 | validation: -0.006834072041422913]
	TIME [epoch: 8.77 sec]
EPOCH 996/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.612744750846828e-05		[learning rate: 0.00010223]
		[batch 20/20] avg loss: 0.004977708953435617		[learning rate: 0.00010198]
	Learning Rate: 0.000101983
	LOSS [training: 0.0025119182004720427 | validation: -0.002656073185437196]
	TIME [epoch: 8.75 sec]
EPOCH 997/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0026807087337781914		[learning rate: 0.00010174]
		[batch 20/20] avg loss: 0.0020270923977452416		[learning rate: 0.00010149]
	Learning Rate: 0.000101489
	LOSS [training: 0.0023539005657617165 | validation: 0.004220836123724522]
	TIME [epoch: 8.76 sec]
EPOCH 998/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.003711934597991099		[learning rate: 0.00010124]
		[batch 20/20] avg loss: 0.005141614330354586		[learning rate: 0.000101]
	Learning Rate: 0.000100999
	LOSS [training: 0.004426774464172842 | validation: -0.005855266902545083]
	TIME [epoch: 8.76 sec]
EPOCH 999/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0042414579688815676		[learning rate: 0.00010075]
		[batch 20/20] avg loss: 0.009765070441063638		[learning rate: 0.00010051]
	Learning Rate: 0.00010051
	LOSS [training: 0.007003264204972602 | validation: 0.007819487459366175]
	TIME [epoch: 8.76 sec]
EPOCH 1000/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0044226138187806446		[learning rate: 0.00010027]
		[batch 20/20] avg loss: 0.004380594754702769		[learning rate: 0.00010002]
	Learning Rate: 0.000100024
	LOSS [training: 0.004401604286741708 | validation: -0.00010257492120003034]
	TIME [epoch: 8.76 sec]
Finished training in 8853.867 seconds.
