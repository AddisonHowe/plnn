Args:
Namespace(name='model_tr_study2', outdir='out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2', training_data='data/transition_rate_studies/tr_study2/tr_study2_training/r2', validation_data='data/transition_rate_studies/tr_study2/tr_study2_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.1, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1529257589

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.636936490746077		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.96477030432553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.300853397535803 | validation: 8.840116357183211]
	TIME [epoch: 79.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.239589125512586		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.59725755185883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.418423338685709 | validation: 4.782618155590994]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.697646460526104		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.6288837598940917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.163265110210098 | validation: 2.840978692418588]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.312542345333269		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6089951194628185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9607687323980434 | validation: 2.3812653596408655]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.237590529179311		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8275348083761702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.032562668777741 | validation: 2.0574356450405316]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9916336149663452		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8028061656011545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.89721989028375 | validation: 2.9449283168339617]
	TIME [epoch: 8.34 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9191289602624177		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.579704454160272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7494167072113445 | validation: 2.8372301650914404]
	TIME [epoch: 8.33 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7469547861440478		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4943862336469942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6206705098955208 | validation: 1.3533741768554104]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.278425521835698		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2221556996977854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2502906107667413 | validation: 0.9562760682290328]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.277932899779406		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2313408243766113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2546368620780088 | validation: 1.4803559964248951]
	TIME [epoch: 8.34 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3579673383908462		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9328321655332632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1453997519620547 | validation: 1.0343063125965937]
	TIME [epoch: 8.35 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9572081010649571		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.028084619785177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9926463604250669 | validation: 0.7927943899477286]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.923643192209199		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8046350551402218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8641391236747105 | validation: 1.039627745225999]
	TIME [epoch: 8.35 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8805057757226402		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7568878874407512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8186968315816957 | validation: 0.47238430033467654]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7897626219648036		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7536145577300964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7716885898474503 | validation: 0.7273117680773028]
	TIME [epoch: 8.34 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7616524742089439		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7027489298081949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7322007020085695 | validation: 0.5168085966406187]
	TIME [epoch: 8.37 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7289618689844855		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5999262343994799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6644440516919825 | validation: 0.6352684624500757]
	TIME [epoch: 8.34 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9162893211232923		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6659561929933557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7911227570583241 | validation: 0.9097086146387889]
	TIME [epoch: 8.34 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7266043545255865		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7485296803148995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7375670174202431 | validation: 0.6166831439781975]
	TIME [epoch: 8.35 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9103481281626513		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6628258791917532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7865870036772021 | validation: 0.4910331013345696]
	TIME [epoch: 8.36 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7993417392406142		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.748841309376629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7740915243086215 | validation: 0.5602309854449576]
	TIME [epoch: 8.34 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6727448775096022		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.701663306901768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.687204092205685 | validation: 0.9751036337050185]
	TIME [epoch: 8.33 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8909377402892303		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.579644875929448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7352913081093393 | validation: 0.46437121216630695]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7485101564761123		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7838232309032664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7661666936896893 | validation: 0.5306992030769444]
	TIME [epoch: 8.37 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6862220885184472		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6059310961084597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6460765923134533 | validation: 0.7250852128856803]
	TIME [epoch: 8.34 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7349820973961757		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8022278149469102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7686049561715429 | validation: 0.7238738229644168]
	TIME [epoch: 8.33 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6650852330689323		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8112495326624758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7381673828657038 | validation: 0.5975524049698525]
	TIME [epoch: 8.33 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7563922288692579		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6232430513674991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6898176401183785 | validation: 0.5825844933601938]
	TIME [epoch: 8.36 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.841257982203043		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.687700531093764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7644792566484034 | validation: 0.5990077388424453]
	TIME [epoch: 8.33 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6093403827372279		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6218217805264988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6155810816318633 | validation: 0.43369192070003637]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5840661281925099		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6005867519283516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5923264400604307 | validation: 0.5848578604662522]
	TIME [epoch: 8.32 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6209952279431397		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7792228517910376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7001090398670886 | validation: 1.1812504347527095]
	TIME [epoch: 8.35 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8041487463465383		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5807879346241726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6924683404853553 | validation: 0.7281750444301164]
	TIME [epoch: 8.32 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7581948269695051		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.712432607023083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7353137169962941 | validation: 0.505910724966994]
	TIME [epoch: 8.32 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5857535177850459		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6885042121565061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6371288649707761 | validation: 0.5081727779648113]
	TIME [epoch: 8.32 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5811117156849254		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.637099547157753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6091056314213393 | validation: 0.5702743134683491]
	TIME [epoch: 8.35 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6143176668032495		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5645572061016353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5894374364524423 | validation: 0.7477398989540369]
	TIME [epoch: 8.33 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7646638715610268		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6013043943456265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6829841329533267 | validation: 0.7217718365835935]
	TIME [epoch: 8.33 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6486779836361877		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7402178432276181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6944479134319029 | validation: 0.5912139730909354]
	TIME [epoch: 8.33 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7494824716439994		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6720256455341052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7107540585890524 | validation: 0.5041455497299066]
	TIME [epoch: 8.35 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5880269028715805		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5808778721105952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.584452387491088 | validation: 0.6571840549744384]
	TIME [epoch: 8.34 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5899230479103063		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7024530935955423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6461880707529242 | validation: 0.7462150234016927]
	TIME [epoch: 8.33 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6883141362986215		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7200468647924366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7041805005455292 | validation: 1.0168808478526552]
	TIME [epoch: 8.33 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6193469592935203		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6813793620451392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6503631606693296 | validation: 0.5659911557170736]
	TIME [epoch: 8.36 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.68269389617244		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5860216516485262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6343577739104831 | validation: 0.5603866889855309]
	TIME [epoch: 8.34 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.732540694240803		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6802057131217933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7063732036812981 | validation: 0.9509683834907195]
	TIME [epoch: 8.32 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6348545715538549		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5355866219227701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5852205967383125 | validation: 0.6332327919779257]
	TIME [epoch: 8.33 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8342096460457785		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.573934739545772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7040721927957754 | validation: 0.4047995196012996]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6056427053253133		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6055682813751169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.605605493350215 | validation: 0.7110002344175024]
	TIME [epoch: 8.33 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6252964825194886		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6443112402536716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.63480386138658 | validation: 0.7605978979938309]
	TIME [epoch: 8.33 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6767797565290928		[learning rate: 0.0099894]
		[batch 20/20] avg loss: 0.5963202768752521		[learning rate: 0.0099776]
	Learning Rate: 0.00997759
	LOSS [training: 0.6365500167021725 | validation: 0.44540401154040415]
	TIME [epoch: 8.33 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8166877559672445		[learning rate: 0.0099658]
		[batch 20/20] avg loss: 0.6907666174112864		[learning rate: 0.0099541]
	Learning Rate: 0.00995405
	LOSS [training: 0.7537271866892654 | validation: 0.5359377489501049]
	TIME [epoch: 8.35 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5522695175517466		[learning rate: 0.0099423]
		[batch 20/20] avg loss: 0.6550219020591423		[learning rate: 0.0099306]
	Learning Rate: 0.00993057
	LOSS [training: 0.6036457098054444 | validation: 0.5237652203098007]
	TIME [epoch: 8.32 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5722201635206359		[learning rate: 0.0099189]
		[batch 20/20] avg loss: 0.6159201775014711		[learning rate: 0.0099071]
	Learning Rate: 0.00990715
	LOSS [training: 0.5940701705110534 | validation: 0.5802841771898476]
	TIME [epoch: 8.32 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5590253677554247		[learning rate: 0.0098955]
		[batch 20/20] avg loss: 0.6515247251483193		[learning rate: 0.0098838]
	Learning Rate: 0.00988378
	LOSS [training: 0.6052750464518719 | validation: 0.7335226562446907]
	TIME [epoch: 8.34 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5497283810620676		[learning rate: 0.0098721]
		[batch 20/20] avg loss: 0.5524223415210331		[learning rate: 0.0098605]
	Learning Rate: 0.00986047
	LOSS [training: 0.5510753612915502 | validation: 0.47333961665797375]
	TIME [epoch: 8.36 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4877192945448926		[learning rate: 0.0098488]
		[batch 20/20] avg loss: 0.5563186047393576		[learning rate: 0.0098372]
	Learning Rate: 0.00983721
	LOSS [training: 0.5220189496421253 | validation: 0.547146886472391]
	TIME [epoch: 8.33 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5228977177296621		[learning rate: 0.0098256]
		[batch 20/20] avg loss: 0.4901765807696824		[learning rate: 0.009814]
	Learning Rate: 0.009814
	LOSS [training: 0.5065371492496723 | validation: 0.4114388238937914]
	TIME [epoch: 8.33 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4853332145669856		[learning rate: 0.0098024]
		[batch 20/20] avg loss: 0.6707949776315341		[learning rate: 0.0097909]
	Learning Rate: 0.00979085
	LOSS [training: 0.5780640960992598 | validation: 0.5610539151091242]
	TIME [epoch: 8.33 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4657609009234863		[learning rate: 0.0097793]
		[batch 20/20] avg loss: 0.7332264345113613		[learning rate: 0.0097678]
	Learning Rate: 0.00976776
	LOSS [training: 0.5994936677174236 | validation: 0.3657143006978332]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4721054814723237		[learning rate: 0.0097562]
		[batch 20/20] avg loss: 0.5086113142330186		[learning rate: 0.0097447]
	Learning Rate: 0.00974472
	LOSS [training: 0.49035839785267116 | validation: 0.4857507672036653]
	TIME [epoch: 8.34 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48497562840654496		[learning rate: 0.0097332]
		[batch 20/20] avg loss: 0.4693391441099746		[learning rate: 0.0097217]
	Learning Rate: 0.00972173
	LOSS [training: 0.47715738625825976 | validation: 0.9127104145785679]
	TIME [epoch: 8.33 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5567751593319044		[learning rate: 0.0097103]
		[batch 20/20] avg loss: 0.5249749174868883		[learning rate: 0.0096988]
	Learning Rate: 0.0096988
	LOSS [training: 0.5408750384093962 | validation: 0.47068260919971505]
	TIME [epoch: 8.34 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.509942054249693		[learning rate: 0.0096874]
		[batch 20/20] avg loss: 0.6054264474453962		[learning rate: 0.0096759]
	Learning Rate: 0.00967592
	LOSS [training: 0.5576842508475446 | validation: 0.7175650837622989]
	TIME [epoch: 8.36 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5328016720157942		[learning rate: 0.0096645]
		[batch 20/20] avg loss: 0.48344938825515005		[learning rate: 0.0096531]
	Learning Rate: 0.0096531
	LOSS [training: 0.508125530135472 | validation: 0.4136562158899747]
	TIME [epoch: 8.33 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4766081776331121		[learning rate: 0.0096417]
		[batch 20/20] avg loss: 0.5372081941938645		[learning rate: 0.0096303]
	Learning Rate: 0.00963033
	LOSS [training: 0.5069081859134883 | validation: 0.5558475264079382]
	TIME [epoch: 8.33 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5972843142001575		[learning rate: 0.009619]
		[batch 20/20] avg loss: 0.5189117051426066		[learning rate: 0.0096076]
	Learning Rate: 0.00960761
	LOSS [training: 0.558098009671382 | validation: 0.3476800039274041]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5389579566145664		[learning rate: 0.0095963]
		[batch 20/20] avg loss: 0.578748593137963		[learning rate: 0.0095849]
	Learning Rate: 0.00958495
	LOSS [training: 0.5588532748762647 | validation: 0.3531317888494103]
	TIME [epoch: 8.37 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4912225513310757		[learning rate: 0.0095736]
		[batch 20/20] avg loss: 0.4828169400333306		[learning rate: 0.0095623]
	Learning Rate: 0.00956234
	LOSS [training: 0.4870197456822033 | validation: 0.4171772272314889]
	TIME [epoch: 8.34 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4862781823968202		[learning rate: 0.0095511]
		[batch 20/20] avg loss: 0.5087280469315119		[learning rate: 0.0095398]
	Learning Rate: 0.00953978
	LOSS [training: 0.4975031146641659 | validation: 0.3213830169137966]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6755022720698961		[learning rate: 0.0095285]
		[batch 20/20] avg loss: 0.6101779901631595		[learning rate: 0.0095173]
	Learning Rate: 0.00951728
	LOSS [training: 0.6428401311165277 | validation: 0.37414704139155014]
	TIME [epoch: 8.34 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48702974454784853		[learning rate: 0.009506]
		[batch 20/20] avg loss: 0.47968236193667335		[learning rate: 0.0094948]
	Learning Rate: 0.00949483
	LOSS [training: 0.48335605324226083 | validation: 0.40229598171965064]
	TIME [epoch: 8.36 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5417677909794312		[learning rate: 0.0094836]
		[batch 20/20] avg loss: 0.44633792666661887		[learning rate: 0.0094724]
	Learning Rate: 0.00947243
	LOSS [training: 0.49405285882302497 | validation: 0.3707197377232614]
	TIME [epoch: 8.34 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4216125051177261		[learning rate: 0.0094613]
		[batch 20/20] avg loss: 0.4103385031179358		[learning rate: 0.0094501]
	Learning Rate: 0.00945009
	LOSS [training: 0.4159755041178309 | validation: 0.8460404940913173]
	TIME [epoch: 8.34 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5671742814659513		[learning rate: 0.0094389]
		[batch 20/20] avg loss: 0.5454515282329055		[learning rate: 0.0094278]
	Learning Rate: 0.0094278
	LOSS [training: 0.5563129048494285 | validation: 0.3888658313409712]
	TIME [epoch: 8.35 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5363877622463745		[learning rate: 0.0094167]
		[batch 20/20] avg loss: 0.4290575345466913		[learning rate: 0.0094056]
	Learning Rate: 0.00940556
	LOSS [training: 0.48272264839653295 | validation: 0.6776632378699122]
	TIME [epoch: 8.35 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43711486442428316		[learning rate: 0.0093945]
		[batch 20/20] avg loss: 0.48320954966434454		[learning rate: 0.0093834]
	Learning Rate: 0.00938337
	LOSS [training: 0.4601622070443138 | validation: 0.365242893443743]
	TIME [epoch: 8.33 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48427077552203857		[learning rate: 0.0093723]
		[batch 20/20] avg loss: 0.4795658252768501		[learning rate: 0.0093612]
	Learning Rate: 0.00936124
	LOSS [training: 0.4819183003994444 | validation: 0.45874434293663113]
	TIME [epoch: 8.33 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5362734012349589		[learning rate: 0.0093502]
		[batch 20/20] avg loss: 0.43580729647778427		[learning rate: 0.0093392]
	Learning Rate: 0.00933916
	LOSS [training: 0.48604034885637165 | validation: 0.4845544894485132]
	TIME [epoch: 8.35 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5783525094211568		[learning rate: 0.0093281]
		[batch 20/20] avg loss: 0.5155164340760391		[learning rate: 0.0093171]
	Learning Rate: 0.00931713
	LOSS [training: 0.5469344717485978 | validation: 0.5574734501438413]
	TIME [epoch: 8.36 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4272115225626731		[learning rate: 0.0093061]
		[batch 20/20] avg loss: 0.5334507537356762		[learning rate: 0.0092951]
	Learning Rate: 0.00929515
	LOSS [training: 0.4803311381491747 | validation: 0.6004969462392429]
	TIME [epoch: 8.35 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48074358186943505		[learning rate: 0.0092842]
		[batch 20/20] avg loss: 0.3981914630449818		[learning rate: 0.0092732]
	Learning Rate: 0.00927322
	LOSS [training: 0.4394675224572084 | validation: 0.37202642247807666]
	TIME [epoch: 8.34 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43513567321586466		[learning rate: 0.0092623]
		[batch 20/20] avg loss: 0.5161509903233055		[learning rate: 0.0092514]
	Learning Rate: 0.00925135
	LOSS [training: 0.4756433317695851 | validation: 0.383846130354368]
	TIME [epoch: 8.36 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42468702254994906		[learning rate: 0.0092404]
		[batch 20/20] avg loss: 0.4752735032619738		[learning rate: 0.0092295]
	Learning Rate: 0.00922953
	LOSS [training: 0.4499802629059613 | validation: 0.3761284244336857]
	TIME [epoch: 8.34 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4544784764177452		[learning rate: 0.0092186]
		[batch 20/20] avg loss: 0.5572562089035167		[learning rate: 0.0092078]
	Learning Rate: 0.00920776
	LOSS [training: 0.505867342660631 | validation: 0.5550844034477564]
	TIME [epoch: 8.33 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4928837268067632		[learning rate: 0.0091969]
		[batch 20/20] avg loss: 0.4548302530133955		[learning rate: 0.009186]
	Learning Rate: 0.00918604
	LOSS [training: 0.47385698991007946 | validation: 0.4004718548677919]
	TIME [epoch: 8.33 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5159773323552478		[learning rate: 0.0091752]
		[batch 20/20] avg loss: 0.4700564281347094		[learning rate: 0.0091644]
	Learning Rate: 0.00916437
	LOSS [training: 0.4930168802449787 | validation: 0.4483088074155942]
	TIME [epoch: 8.35 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4108573575461715		[learning rate: 0.0091536]
		[batch 20/20] avg loss: 0.5288263411499086		[learning rate: 0.0091428]
	Learning Rate: 0.00914275
	LOSS [training: 0.4698418493480402 | validation: 0.4494052271065164]
	TIME [epoch: 8.34 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4303959598256847		[learning rate: 0.009132]
		[batch 20/20] avg loss: 0.5119641737713798		[learning rate: 0.0091212]
	Learning Rate: 0.00912119
	LOSS [training: 0.47118006679853225 | validation: 0.5069345959582665]
	TIME [epoch: 8.33 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4109679955507188		[learning rate: 0.0091104]
		[batch 20/20] avg loss: 0.48503635641036064		[learning rate: 0.0090997]
	Learning Rate: 0.00909967
	LOSS [training: 0.4480021759805398 | validation: 0.4333995261333089]
	TIME [epoch: 8.33 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42958574458918275		[learning rate: 0.0090889]
		[batch 20/20] avg loss: 0.36804330425476295		[learning rate: 0.0090782]
	Learning Rate: 0.0090782
	LOSS [training: 0.39881452442197285 | validation: 0.8790573583483094]
	TIME [epoch: 8.35 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45631440105150983		[learning rate: 0.0090675]
		[batch 20/20] avg loss: 0.4385202071272386		[learning rate: 0.0090568]
	Learning Rate: 0.00905679
	LOSS [training: 0.44741730408937413 | validation: 0.552835488150203]
	TIME [epoch: 8.34 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37882887787250974		[learning rate: 0.0090461]
		[batch 20/20] avg loss: 0.5159648003265647		[learning rate: 0.0090354]
	Learning Rate: 0.00903543
	LOSS [training: 0.44739683909953715 | validation: 0.23986468320786256]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3798939293432551		[learning rate: 0.0090248]
		[batch 20/20] avg loss: 0.48226040858035735		[learning rate: 0.0090141]
	Learning Rate: 0.00901411
	LOSS [training: 0.4310771689618062 | validation: 0.4079381540493419]
	TIME [epoch: 8.33 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4402005308620961		[learning rate: 0.0090035]
		[batch 20/20] avg loss: 0.3679162087613551		[learning rate: 0.0089929]
	Learning Rate: 0.00899285
	LOSS [training: 0.4040583698117257 | validation: 0.40927534307971886]
	TIME [epoch: 8.35 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5044605123962465		[learning rate: 0.0089822]
		[batch 20/20] avg loss: 0.5285615269664607		[learning rate: 0.0089716]
	Learning Rate: 0.00897164
	LOSS [training: 0.5165110196813536 | validation: 0.42530958374741407]
	TIME [epoch: 8.35 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6863777320186746		[learning rate: 0.0089611]
		[batch 20/20] avg loss: 0.5856141611441055		[learning rate: 0.0089505]
	Learning Rate: 0.00895048
	LOSS [training: 0.63599594658139 | validation: 0.3838252764614036]
	TIME [epoch: 8.34 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4361287081560225		[learning rate: 0.0089399]
		[batch 20/20] avg loss: 0.4651388656006376		[learning rate: 0.0089294]
	Learning Rate: 0.00892936
	LOSS [training: 0.45063378687833006 | validation: 0.4441724663410205]
	TIME [epoch: 8.34 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5294391394205883		[learning rate: 0.0089188]
		[batch 20/20] avg loss: 0.5050026064656375		[learning rate: 0.0089083]
	Learning Rate: 0.0089083
	LOSS [training: 0.5172208729431128 | validation: 0.4661715426064331]
	TIME [epoch: 8.36 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3527482056002986		[learning rate: 0.0088978]
		[batch 20/20] avg loss: 0.5078184300979248		[learning rate: 0.0088873]
	Learning Rate: 0.00888729
	LOSS [training: 0.43028331784911167 | validation: 0.27803009178289106]
	TIME [epoch: 8.35 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38525987876560697		[learning rate: 0.0088768]
		[batch 20/20] avg loss: 0.3653869575863793		[learning rate: 0.0088663]
	Learning Rate: 0.00886632
	LOSS [training: 0.3753234181759932 | validation: 0.3210467934066086]
	TIME [epoch: 8.33 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4023390275943407		[learning rate: 0.0088559]
		[batch 20/20] avg loss: 0.4369290021128978		[learning rate: 0.0088454]
	Learning Rate: 0.00884541
	LOSS [training: 0.41963401485361923 | validation: 0.2731155374454822]
	TIME [epoch: 8.33 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.359342463285712		[learning rate: 0.008835]
		[batch 20/20] avg loss: 0.38296702708379654		[learning rate: 0.0088245]
	Learning Rate: 0.00882454
	LOSS [training: 0.3711547451847543 | validation: 0.27195763709150933]
	TIME [epoch: 8.35 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4558069538853319		[learning rate: 0.0088141]
		[batch 20/20] avg loss: 0.5402292349496814		[learning rate: 0.0088037]
	Learning Rate: 0.00880373
	LOSS [training: 0.49801809441750666 | validation: 0.2626612871134412]
	TIME [epoch: 8.34 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40388128669275203		[learning rate: 0.0087933]
		[batch 20/20] avg loss: 0.5702561981060705		[learning rate: 0.008783]
	Learning Rate: 0.00878296
	LOSS [training: 0.4870687423994114 | validation: 0.22059594676775235]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42454075125659785		[learning rate: 0.0087726]
		[batch 20/20] avg loss: 0.41038319647791266		[learning rate: 0.0087622]
	Learning Rate: 0.00876225
	LOSS [training: 0.41746197386725525 | validation: 0.28310233611532704]
	TIME [epoch: 8.33 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37724470036782765		[learning rate: 0.0087519]
		[batch 20/20] avg loss: 0.358883713584116		[learning rate: 0.0087416]
	Learning Rate: 0.00874158
	LOSS [training: 0.3680642069759718 | validation: 0.4096484417200521]
	TIME [epoch: 8.35 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3699504182850152		[learning rate: 0.0087313]
		[batch 20/20] avg loss: 0.4236306354461564		[learning rate: 0.008721]
	Learning Rate: 0.00872096
	LOSS [training: 0.3967905268655858 | validation: 0.2658456872288376]
	TIME [epoch: 8.34 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27989526827865874		[learning rate: 0.0087107]
		[batch 20/20] avg loss: 0.45411756003619275		[learning rate: 0.0087004]
	Learning Rate: 0.00870038
	LOSS [training: 0.3670064141574257 | validation: 0.3060670357289783]
	TIME [epoch: 8.33 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43290333808956233		[learning rate: 0.0086901]
		[batch 20/20] avg loss: 0.4092410784580826		[learning rate: 0.0086799]
	Learning Rate: 0.00867986
	LOSS [training: 0.42107220827382247 | validation: 0.2860149380333667]
	TIME [epoch: 8.33 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4915163488153313		[learning rate: 0.0086696]
		[batch 20/20] avg loss: 0.39777959787716893		[learning rate: 0.0086594]
	Learning Rate: 0.00865939
	LOSS [training: 0.44464797334625006 | validation: 0.3454450676497851]
	TIME [epoch: 8.35 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4056296609951259		[learning rate: 0.0086492]
		[batch 20/20] avg loss: 0.3609026122906879		[learning rate: 0.008639]
	Learning Rate: 0.00863896
	LOSS [training: 0.38326613664290676 | validation: 0.6672827148081596]
	TIME [epoch: 8.33 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.436186140964345		[learning rate: 0.0086288]
		[batch 20/20] avg loss: 0.4014967993570531		[learning rate: 0.0086186]
	Learning Rate: 0.00861858
	LOSS [training: 0.41884147016069917 | validation: 0.27600857199156964]
	TIME [epoch: 8.32 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3723250972426011		[learning rate: 0.0086084]
		[batch 20/20] avg loss: 0.3756214069788918		[learning rate: 0.0085983]
	Learning Rate: 0.00859825
	LOSS [training: 0.37397325211074645 | validation: 0.6968530285786648]
	TIME [epoch: 8.32 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44546347457327845		[learning rate: 0.0085881]
		[batch 20/20] avg loss: 0.3988701946157346		[learning rate: 0.008578]
	Learning Rate: 0.00857797
	LOSS [training: 0.4221668345945065 | validation: 0.2792521335763958]
	TIME [epoch: 8.34 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3254723743677032		[learning rate: 0.0085678]
		[batch 20/20] avg loss: 0.30657323021348576		[learning rate: 0.0085577]
	Learning Rate: 0.00855774
	LOSS [training: 0.31602280229059454 | validation: 0.3543745610069231]
	TIME [epoch: 8.34 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.464202251656867		[learning rate: 0.0085476]
		[batch 20/20] avg loss: 0.6076283373772969		[learning rate: 0.0085376]
	Learning Rate: 0.00853755
	LOSS [training: 0.5359152945170821 | validation: 0.31415218415918567]
	TIME [epoch: 8.33 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2646071900953098		[learning rate: 0.0085275]
		[batch 20/20] avg loss: 0.3099602105228231		[learning rate: 0.0085174]
	Learning Rate: 0.00851741
	LOSS [training: 0.2872837003090665 | validation: 0.17513250406817626]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4983245371535749		[learning rate: 0.0085074]
		[batch 20/20] avg loss: 0.27100083879063436		[learning rate: 0.0084973]
	Learning Rate: 0.00849732
	LOSS [training: 0.38466268797210457 | validation: 0.23864089827156737]
	TIME [epoch: 8.37 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6856243607614362		[learning rate: 0.0084873]
		[batch 20/20] avg loss: 0.4322483479114457		[learning rate: 0.0084773]
	Learning Rate: 0.00847728
	LOSS [training: 0.558936354336441 | validation: 0.4042726903632918]
	TIME [epoch: 8.33 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4087932291171428		[learning rate: 0.0084673]
		[batch 20/20] avg loss: 0.31838014976917905		[learning rate: 0.0084573]
	Learning Rate: 0.00845728
	LOSS [training: 0.3635866894431609 | validation: 0.7215331563741244]
	TIME [epoch: 8.33 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31094757503601644		[learning rate: 0.0084473]
		[batch 20/20] avg loss: 0.4159674792875709		[learning rate: 0.0084373]
	Learning Rate: 0.00843733
	LOSS [training: 0.36345752716179364 | validation: 0.42348318954128994]
	TIME [epoch: 8.33 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30550304624332864		[learning rate: 0.0084274]
		[batch 20/20] avg loss: 0.31632619474446505		[learning rate: 0.0084174]
	Learning Rate: 0.00841743
	LOSS [training: 0.31091462049389684 | validation: 0.3872789984009807]
	TIME [epoch: 8.36 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35706105717146686		[learning rate: 0.0084075]
		[batch 20/20] avg loss: 0.3044691671157388		[learning rate: 0.0083976]
	Learning Rate: 0.00839757
	LOSS [training: 0.3307651121436029 | validation: 0.40781236460801673]
	TIME [epoch: 8.34 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4108000056502502		[learning rate: 0.0083877]
		[batch 20/20] avg loss: 0.4150905588499456		[learning rate: 0.0083778]
	Learning Rate: 0.00837777
	LOSS [training: 0.4129452822500979 | validation: 0.27282473341405267]
	TIME [epoch: 8.33 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31475975874311424		[learning rate: 0.0083679]
		[batch 20/20] avg loss: 0.3459538989392966		[learning rate: 0.008358]
	Learning Rate: 0.008358
	LOSS [training: 0.33035682884120543 | validation: 0.2630677128815204]
	TIME [epoch: 8.33 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27839088480273216		[learning rate: 0.0083481]
		[batch 20/20] avg loss: 0.3100190009059703		[learning rate: 0.0083383]
	Learning Rate: 0.00833829
	LOSS [training: 0.29420494285435117 | validation: 0.5756725924059968]
	TIME [epoch: 8.36 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43085173523402087		[learning rate: 0.0083284]
		[batch 20/20] avg loss: 0.3419637390686961		[learning rate: 0.0083186]
	Learning Rate: 0.00831862
	LOSS [training: 0.38640773715135845 | validation: 0.15094681941560578]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34215368737864676		[learning rate: 0.0083088]
		[batch 20/20] avg loss: 0.3924709398027755		[learning rate: 0.008299]
	Learning Rate: 0.008299
	LOSS [training: 0.3673123135907111 | validation: 0.2757018070827979]
	TIME [epoch: 8.36 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3551469717751467		[learning rate: 0.0082892]
		[batch 20/20] avg loss: 0.350171592142252		[learning rate: 0.0082794]
	Learning Rate: 0.00827942
	LOSS [training: 0.35265928195869944 | validation: 0.27426082413299446]
	TIME [epoch: 8.34 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27999889832790953		[learning rate: 0.0082697]
		[batch 20/20] avg loss: 0.2730842320330522		[learning rate: 0.0082599]
	Learning Rate: 0.00825989
	LOSS [training: 0.27654156518048084 | validation: 0.3923595504681499]
	TIME [epoch: 8.37 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3803298116125091		[learning rate: 0.0082501]
		[batch 20/20] avg loss: 0.3608978564112004		[learning rate: 0.0082404]
	Learning Rate: 0.00824041
	LOSS [training: 0.37061383401185477 | validation: 0.22279812518901082]
	TIME [epoch: 8.35 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3838356564148727		[learning rate: 0.0082307]
		[batch 20/20] avg loss: 0.49701676227369707		[learning rate: 0.008221]
	Learning Rate: 0.00822097
	LOSS [training: 0.4404262093442849 | validation: 0.22671617660863747]
	TIME [epoch: 8.34 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43206068751330806		[learning rate: 0.0082113]
		[batch 20/20] avg loss: 0.34647068511942797		[learning rate: 0.0082016]
	Learning Rate: 0.00820158
	LOSS [training: 0.38926568631636804 | validation: 0.26462284274431636]
	TIME [epoch: 8.34 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4012214357885808		[learning rate: 0.0081919]
		[batch 20/20] avg loss: 0.41990202373680574		[learning rate: 0.0081822]
	Learning Rate: 0.00818223
	LOSS [training: 0.41056172976269334 | validation: 0.7571226292837405]
	TIME [epoch: 8.37 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4460712696172294		[learning rate: 0.0081726]
		[batch 20/20] avg loss: 0.4321231639776303		[learning rate: 0.0081629]
	Learning Rate: 0.00816293
	LOSS [training: 0.4390972167974298 | validation: 0.5601318301050779]
	TIME [epoch: 8.35 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3215952247234549		[learning rate: 0.0081533]
		[batch 20/20] avg loss: 0.34224677384386093		[learning rate: 0.0081437]
	Learning Rate: 0.00814368
	LOSS [training: 0.331920999283658 | validation: 0.45945233222787984]
	TIME [epoch: 8.34 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3420371337916596		[learning rate: 0.0081341]
		[batch 20/20] avg loss: 0.3747010766821974		[learning rate: 0.0081245]
	Learning Rate: 0.00812447
	LOSS [training: 0.3583691052369285 | validation: 0.3819760777062493]
	TIME [epoch: 8.35 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3611508757746043		[learning rate: 0.0081149]
		[batch 20/20] avg loss: 0.3819993152534183		[learning rate: 0.0081053]
	Learning Rate: 0.0081053
	LOSS [training: 0.37157509551401124 | validation: 0.39273525068845017]
	TIME [epoch: 8.37 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36681131709979753		[learning rate: 0.0080957]
		[batch 20/20] avg loss: 0.40869347116962357		[learning rate: 0.0080862]
	Learning Rate: 0.00808618
	LOSS [training: 0.3877523941347106 | validation: 0.9444203018317296]
	TIME [epoch: 8.35 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34094995630616054		[learning rate: 0.0080766]
		[batch 20/20] avg loss: 0.5101325201936792		[learning rate: 0.0080671]
	Learning Rate: 0.00806711
	LOSS [training: 0.42554123824991985 | validation: 0.23541188446737638]
	TIME [epoch: 8.34 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34840870668785917		[learning rate: 0.0080576]
		[batch 20/20] avg loss: 0.32484355133773263		[learning rate: 0.0080481]
	Learning Rate: 0.00804808
	LOSS [training: 0.3366261290127959 | validation: 0.6817676991440722]
	TIME [epoch: 8.34 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5192981347128718		[learning rate: 0.0080386]
		[batch 20/20] avg loss: 0.284919142678449		[learning rate: 0.0080291]
	Learning Rate: 0.0080291
	LOSS [training: 0.4021086386956604 | validation: 0.24122883339061962]
	TIME [epoch: 8.37 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41363835485059547		[learning rate: 0.0080196]
		[batch 20/20] avg loss: 0.41686920238684505		[learning rate: 0.0080102]
	Learning Rate: 0.00801016
	LOSS [training: 0.4152537786187203 | validation: 0.18143927589746087]
	TIME [epoch: 8.35 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28860873928579406		[learning rate: 0.0080007]
		[batch 20/20] avg loss: 0.3282591550263574		[learning rate: 0.0079913]
	Learning Rate: 0.00799126
	LOSS [training: 0.30843394715607586 | validation: 0.582733593653335]
	TIME [epoch: 8.35 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5001612331599457		[learning rate: 0.0079818]
		[batch 20/20] avg loss: 0.36417831622339725		[learning rate: 0.0079724]
	Learning Rate: 0.00797241
	LOSS [training: 0.43216977469167145 | validation: 0.38166104684429636]
	TIME [epoch: 8.35 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.344279327446486		[learning rate: 0.007963]
		[batch 20/20] avg loss: 0.3417275627629749		[learning rate: 0.0079536]
	Learning Rate: 0.00795361
	LOSS [training: 0.34300344510473046 | validation: 0.35783866689176114]
	TIME [epoch: 8.37 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3480748170379088		[learning rate: 0.0079442]
		[batch 20/20] avg loss: 0.3681048687448812		[learning rate: 0.0079348]
	Learning Rate: 0.00793485
	LOSS [training: 0.358089842891395 | validation: 0.6762967176220624]
	TIME [epoch: 8.35 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35712196432962384		[learning rate: 0.0079255]
		[batch 20/20] avg loss: 0.4284645176294151		[learning rate: 0.0079161]
	Learning Rate: 0.00791613
	LOSS [training: 0.3927932409795195 | validation: 0.4875285556092486]
	TIME [epoch: 8.34 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3815523294846236		[learning rate: 0.0079068]
		[batch 20/20] avg loss: 0.385785637758039		[learning rate: 0.0078975]
	Learning Rate: 0.00789746
	LOSS [training: 0.3836689836213313 | validation: 0.2788804342694703]
	TIME [epoch: 8.34 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4170043126122051		[learning rate: 0.0078881]
		[batch 20/20] avg loss: 0.2607148908412878		[learning rate: 0.0078788]
	Learning Rate: 0.00787883
	LOSS [training: 0.3388596017267464 | validation: 0.5360515464572456]
	TIME [epoch: 8.37 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2941572600245775		[learning rate: 0.0078695]
		[batch 20/20] avg loss: 0.3754334551552946		[learning rate: 0.0078602]
	Learning Rate: 0.00786024
	LOSS [training: 0.334795357589936 | validation: 0.17993866293920036]
	TIME [epoch: 8.35 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27587640758204973		[learning rate: 0.007851]
		[batch 20/20] avg loss: 0.5418366581245534		[learning rate: 0.0078417]
	Learning Rate: 0.0078417
	LOSS [training: 0.4088565328533016 | validation: 0.14328115739341216]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39071140972799456		[learning rate: 0.0078324]
		[batch 20/20] avg loss: 0.32928094184147944		[learning rate: 0.0078232]
	Learning Rate: 0.0078232
	LOSS [training: 0.3599961757847369 | validation: 0.16481043322548203]
	TIME [epoch: 8.35 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25319557165336765		[learning rate: 0.007814]
		[batch 20/20] avg loss: 0.25843369385139436		[learning rate: 0.0078047]
	Learning Rate: 0.00780475
	LOSS [training: 0.25581463275238103 | validation: 0.23555041897421483]
	TIME [epoch: 8.37 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28143969942219876		[learning rate: 0.0077955]
		[batch 20/20] avg loss: 0.46848524508713574		[learning rate: 0.0077863]
	Learning Rate: 0.00778634
	LOSS [training: 0.3749624722546672 | validation: 0.30308855649369965]
	TIME [epoch: 8.35 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30688805272770303		[learning rate: 0.0077772]
		[batch 20/20] avg loss: 0.39271969678251767		[learning rate: 0.007768]
	Learning Rate: 0.00776797
	LOSS [training: 0.3498038747551103 | validation: 0.2797604772481269]
	TIME [epoch: 8.34 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2262490920004157		[learning rate: 0.0077588]
		[batch 20/20] avg loss: 0.3449949976783148		[learning rate: 0.0077496]
	Learning Rate: 0.00774965
	LOSS [training: 0.28562204483936526 | validation: 0.5154440763348141]
	TIME [epoch: 8.34 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39248270624595427		[learning rate: 0.0077405]
		[batch 20/20] avg loss: 0.4119218187511219		[learning rate: 0.0077314]
	Learning Rate: 0.00773137
	LOSS [training: 0.4022022624985381 | validation: 0.3090587733340461]
	TIME [epoch: 8.37 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31146157315803463		[learning rate: 0.0077222]
		[batch 20/20] avg loss: 0.3758043844677917		[learning rate: 0.0077131]
	Learning Rate: 0.00771313
	LOSS [training: 0.34363297881291316 | validation: 0.35927289797217665]
	TIME [epoch: 8.35 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2813087383366134		[learning rate: 0.007704]
		[batch 20/20] avg loss: 0.3479944725058524		[learning rate: 0.0076949]
	Learning Rate: 0.00769494
	LOSS [training: 0.3146516054212328 | validation: 0.18666570354913464]
	TIME [epoch: 8.35 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25624921826237357		[learning rate: 0.0076859]
		[batch 20/20] avg loss: 0.2896593900768945		[learning rate: 0.0076768]
	Learning Rate: 0.00767679
	LOSS [training: 0.2729543041696341 | validation: 0.44498901418783077]
	TIME [epoch: 8.35 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4005355156845941		[learning rate: 0.0076677]
		[batch 20/20] avg loss: 0.26314505187362625		[learning rate: 0.0076587]
	Learning Rate: 0.00765868
	LOSS [training: 0.3318402837791102 | validation: 0.32514501712727617]
	TIME [epoch: 8.38 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2576717800373146		[learning rate: 0.0076496]
		[batch 20/20] avg loss: 0.2920802347589263		[learning rate: 0.0076406]
	Learning Rate: 0.00764061
	LOSS [training: 0.27487600739812046 | validation: 0.29682664329432434]
	TIME [epoch: 8.35 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26706213865052203		[learning rate: 0.0076316]
		[batch 20/20] avg loss: 0.3065300191402369		[learning rate: 0.0076226]
	Learning Rate: 0.00762259
	LOSS [training: 0.28679607889537945 | validation: 0.1355224353248561]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3820167207250795		[learning rate: 0.0076136]
		[batch 20/20] avg loss: 0.34253425711267405		[learning rate: 0.0076046]
	Learning Rate: 0.00760461
	LOSS [training: 0.36227548891887673 | validation: 0.3374188986560214]
	TIME [epoch: 8.33 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4037371818297711		[learning rate: 0.0075956]
		[batch 20/20] avg loss: 0.27781489722593605		[learning rate: 0.0075867]
	Learning Rate: 0.00758667
	LOSS [training: 0.3407760395278535 | validation: 0.46724670485535824]
	TIME [epoch: 8.35 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3253748785348516		[learning rate: 0.0075777]
		[batch 20/20] avg loss: 0.29770770416635606		[learning rate: 0.0075688]
	Learning Rate: 0.00756878
	LOSS [training: 0.3115412913506039 | validation: 0.29207765531400254]
	TIME [epoch: 8.33 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4421236825087094		[learning rate: 0.0075598]
		[batch 20/20] avg loss: 0.33434061636428875		[learning rate: 0.0075509]
	Learning Rate: 0.00755092
	LOSS [training: 0.38823214943649914 | validation: 0.16905569696135983]
	TIME [epoch: 8.33 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3697255679985406		[learning rate: 0.007542]
		[batch 20/20] avg loss: 0.273293440125541		[learning rate: 0.0075331]
	Learning Rate: 0.00753311
	LOSS [training: 0.3215095040620408 | validation: 0.1226062139159853]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2515351270948133		[learning rate: 0.0075242]
		[batch 20/20] avg loss: 0.4490207760322426		[learning rate: 0.0075153]
	Learning Rate: 0.00751534
	LOSS [training: 0.3502779515635279 | validation: 0.3427020321435088]
	TIME [epoch: 8.36 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2714123291255687		[learning rate: 0.0075065]
		[batch 20/20] avg loss: 0.24975176193810156		[learning rate: 0.0074976]
	Learning Rate: 0.00749761
	LOSS [training: 0.26058204553183506 | validation: 0.16899270825916274]
	TIME [epoch: 8.33 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32897889785736517		[learning rate: 0.0074888]
		[batch 20/20] avg loss: 0.2339825612966439		[learning rate: 0.0074799]
	Learning Rate: 0.00747993
	LOSS [training: 0.2814807295770045 | validation: 0.190489348938694]
	TIME [epoch: 8.33 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3493214146984954		[learning rate: 0.0074711]
		[batch 20/20] avg loss: 0.24980009862948543		[learning rate: 0.0074623]
	Learning Rate: 0.00746228
	LOSS [training: 0.29956075666399046 | validation: 0.19913492484350842]
	TIME [epoch: 8.33 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22179820968293168		[learning rate: 0.0074535]
		[batch 20/20] avg loss: 0.3125695641106303		[learning rate: 0.0074447]
	Learning Rate: 0.00744468
	LOSS [training: 0.26718388689678096 | validation: 0.4460403475976852]
	TIME [epoch: 8.36 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2989448286982343		[learning rate: 0.0074359]
		[batch 20/20] avg loss: 0.2822924234004746		[learning rate: 0.0074271]
	Learning Rate: 0.00742712
	LOSS [training: 0.2906186260493545 | validation: 0.24135255453429447]
	TIME [epoch: 8.33 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2681611941667731		[learning rate: 0.0074184]
		[batch 20/20] avg loss: 0.2687457704096683		[learning rate: 0.0074096]
	Learning Rate: 0.0074096
	LOSS [training: 0.2684534822882207 | validation: 0.3149777484941862]
	TIME [epoch: 8.33 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2819638647499291		[learning rate: 0.0074009]
		[batch 20/20] avg loss: 0.26171031560953933		[learning rate: 0.0073921]
	Learning Rate: 0.00739212
	LOSS [training: 0.2718370901797342 | validation: 0.18961257603151377]
	TIME [epoch: 8.33 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29275196652582053		[learning rate: 0.0073834]
		[batch 20/20] avg loss: 0.32316227883597815		[learning rate: 0.0073747]
	Learning Rate: 0.00737469
	LOSS [training: 0.30795712268089936 | validation: 0.1439143915637316]
	TIME [epoch: 8.35 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24741214620213717		[learning rate: 0.007366]
		[batch 20/20] avg loss: 0.2907117030903062		[learning rate: 0.0073573]
	Learning Rate: 0.00735729
	LOSS [training: 0.26906192464622164 | validation: 0.2760838149663154]
	TIME [epoch: 8.33 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2522523049848787		[learning rate: 0.0073486]
		[batch 20/20] avg loss: 0.32969628773871695		[learning rate: 0.0073399]
	Learning Rate: 0.00733994
	LOSS [training: 0.29097429636179784 | validation: 0.28025867322680287]
	TIME [epoch: 8.33 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2895513391171577		[learning rate: 0.0073313]
		[batch 20/20] avg loss: 0.2696026989480941		[learning rate: 0.0073226]
	Learning Rate: 0.00732262
	LOSS [training: 0.2795770190326259 | validation: 0.7183137166075969]
	TIME [epoch: 8.33 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3508180345738512		[learning rate: 0.007314]
		[batch 20/20] avg loss: 0.25911628550651505		[learning rate: 0.0073053]
	Learning Rate: 0.00730535
	LOSS [training: 0.30496716004018315 | validation: 0.4018831340423401]
	TIME [epoch: 8.35 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2738024072954278		[learning rate: 0.0072967]
		[batch 20/20] avg loss: 0.22544890782206592		[learning rate: 0.0072881]
	Learning Rate: 0.00728812
	LOSS [training: 0.24962565755874683 | validation: 0.15351937455290438]
	TIME [epoch: 8.34 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2436761073396058		[learning rate: 0.0072795]
		[batch 20/20] avg loss: 0.27946585358813464		[learning rate: 0.0072709]
	Learning Rate: 0.00727093
	LOSS [training: 0.2615709804638702 | validation: 0.23274934542215797]
	TIME [epoch: 8.32 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2987265042883163		[learning rate: 0.0072623]
		[batch 20/20] avg loss: 0.29420596207006283		[learning rate: 0.0072538]
	Learning Rate: 0.00725377
	LOSS [training: 0.2964662331791896 | validation: 0.2881417260527743]
	TIME [epoch: 8.32 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20204075089029988		[learning rate: 0.0072452]
		[batch 20/20] avg loss: 0.5024022335972045		[learning rate: 0.0072367]
	Learning Rate: 0.00723666
	LOSS [training: 0.3522214922437522 | validation: 0.08980886341946964]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2785307264276588		[learning rate: 0.0072281]
		[batch 20/20] avg loss: 0.23609539013422548		[learning rate: 0.0072196]
	Learning Rate: 0.00721959
	LOSS [training: 0.25731305828094214 | validation: 0.14829122154397267]
	TIME [epoch: 8.33 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.295091617793212		[learning rate: 0.0072111]
		[batch 20/20] avg loss: 0.24169824139843438		[learning rate: 0.0072026]
	Learning Rate: 0.00720256
	LOSS [training: 0.2683949295958231 | validation: 0.1504756468969295]
	TIME [epoch: 8.32 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24356937779224636		[learning rate: 0.0071941]
		[batch 20/20] avg loss: 0.37322692451269324		[learning rate: 0.0071856]
	Learning Rate: 0.00718558
	LOSS [training: 0.30839815115246977 | validation: 0.15989659757689748]
	TIME [epoch: 8.33 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2512954565670428		[learning rate: 0.0071771]
		[batch 20/20] avg loss: 0.2484583034418275		[learning rate: 0.0071686]
	Learning Rate: 0.00716863
	LOSS [training: 0.24987688000443514 | validation: 0.12144080390023512]
	TIME [epoch: 8.34 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20359062176225087		[learning rate: 0.0071602]
		[batch 20/20] avg loss: 0.27201180447432727		[learning rate: 0.0071517]
	Learning Rate: 0.00715172
	LOSS [training: 0.23780121311828903 | validation: 0.27870136972549064]
	TIME [epoch: 8.32 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3001151491519406		[learning rate: 0.0071433]
		[batch 20/20] avg loss: 0.3081255596010073		[learning rate: 0.0071348]
	Learning Rate: 0.00713485
	LOSS [training: 0.304120354376474 | validation: 0.1827525596659657]
	TIME [epoch: 8.32 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.270669421129227		[learning rate: 0.0071264]
		[batch 20/20] avg loss: 0.1758647798253639		[learning rate: 0.007118]
	Learning Rate: 0.00711802
	LOSS [training: 0.22326710047729542 | validation: 0.24603282335871515]
	TIME [epoch: 8.33 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37563520130430544		[learning rate: 0.0071096]
		[batch 20/20] avg loss: 0.2147778642447234		[learning rate: 0.0071012]
	Learning Rate: 0.00710123
	LOSS [training: 0.2952065327745145 | validation: 0.16118453618011477]
	TIME [epoch: 8.34 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4155724655020645		[learning rate: 0.0070928]
		[batch 20/20] avg loss: 0.2851644393727203		[learning rate: 0.0070845]
	Learning Rate: 0.00708447
	LOSS [training: 0.35036845243739234 | validation: 0.12265409253307674]
	TIME [epoch: 8.32 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.284897270650782		[learning rate: 0.0070761]
		[batch 20/20] avg loss: 0.33442944968349325		[learning rate: 0.0070678]
	Learning Rate: 0.00706776
	LOSS [training: 0.30966336016713764 | validation: 0.7751634699355398]
	TIME [epoch: 8.32 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32977448593352293		[learning rate: 0.0070594]
		[batch 20/20] avg loss: 0.3496767306039231		[learning rate: 0.0070511]
	Learning Rate: 0.00705109
	LOSS [training: 0.339725608268723 | validation: 0.4350360472330915]
	TIME [epoch: 8.33 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2547436721698255		[learning rate: 0.0070428]
		[batch 20/20] avg loss: 0.3200468700694169		[learning rate: 0.0070345]
	Learning Rate: 0.00703446
	LOSS [training: 0.28739527111962115 | validation: 0.20321133950220766]
	TIME [epoch: 8.34 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2332029945618555		[learning rate: 0.0070262]
		[batch 20/20] avg loss: 0.28756717877630555		[learning rate: 0.0070179]
	Learning Rate: 0.00701787
	LOSS [training: 0.26038508666908056 | validation: 0.4273082603618975]
	TIME [epoch: 8.32 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22921582814804292		[learning rate: 0.0070096]
		[batch 20/20] avg loss: 0.2975420437838448		[learning rate: 0.0070013]
	Learning Rate: 0.00700131
	LOSS [training: 0.26337893596594386 | validation: 0.2882485591185217]
	TIME [epoch: 8.32 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32722629238985945		[learning rate: 0.006993]
		[batch 20/20] avg loss: 0.34292958203018886		[learning rate: 0.0069848]
	Learning Rate: 0.0069848
	LOSS [training: 0.3350779372100242 | validation: 0.13696552269711362]
	TIME [epoch: 8.34 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22431537443889682		[learning rate: 0.0069766]
		[batch 20/20] avg loss: 0.20181117653296482		[learning rate: 0.0069683]
	Learning Rate: 0.00696832
	LOSS [training: 0.21306327548593082 | validation: 0.1963724027862464]
	TIME [epoch: 8.33 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.289601040209537		[learning rate: 0.0069601]
		[batch 20/20] avg loss: 0.21659193116501946		[learning rate: 0.0069519]
	Learning Rate: 0.00695188
	LOSS [training: 0.2530964856872782 | validation: 0.28385532247291917]
	TIME [epoch: 8.31 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25991155409609507		[learning rate: 0.0069437]
		[batch 20/20] avg loss: 0.22082037206237648		[learning rate: 0.0069355]
	Learning Rate: 0.00693549
	LOSS [training: 0.2403659630792358 | validation: 0.26719172890014575]
	TIME [epoch: 8.31 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2017427439401573		[learning rate: 0.0069273]
		[batch 20/20] avg loss: 0.29171925918464625		[learning rate: 0.0069191]
	Learning Rate: 0.00691913
	LOSS [training: 0.2467310015624018 | validation: 0.37243393357425175]
	TIME [epoch: 8.33 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19427328164518393		[learning rate: 0.006911]
		[batch 20/20] avg loss: 0.27328359628923105		[learning rate: 0.0069028]
	Learning Rate: 0.00690281
	LOSS [training: 0.2337784389672075 | validation: 0.17684184153524485]
	TIME [epoch: 8.33 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3266442243815826		[learning rate: 0.0068947]
		[batch 20/20] avg loss: 0.30998761589071433		[learning rate: 0.0068865]
	Learning Rate: 0.00688652
	LOSS [training: 0.3183159201361484 | validation: 0.19149996919045087]
	TIME [epoch: 8.32 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.276677011665232		[learning rate: 0.0068784]
		[batch 20/20] avg loss: 0.2994018368720729		[learning rate: 0.0068703]
	Learning Rate: 0.00687028
	LOSS [training: 0.2880394242686525 | validation: 0.5555624078226253]
	TIME [epoch: 8.32 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28206125876412697		[learning rate: 0.0068622]
		[batch 20/20] avg loss: 0.21855945525572412		[learning rate: 0.0068541]
	Learning Rate: 0.00685407
	LOSS [training: 0.25031035700992554 | validation: 0.1354768774514182]
	TIME [epoch: 8.34 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22303804083149684		[learning rate: 0.006846]
		[batch 20/20] avg loss: 0.323204645336663		[learning rate: 0.0068379]
	Learning Rate: 0.0068379
	LOSS [training: 0.2731213430840799 | validation: 0.3159879688368524]
	TIME [epoch: 8.33 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2547545082441324		[learning rate: 0.0068298]
		[batch 20/20] avg loss: 0.20842817425741605		[learning rate: 0.0068218]
	Learning Rate: 0.00682178
	LOSS [training: 0.23159134125077419 | validation: 0.11875611332056865]
	TIME [epoch: 8.31 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24209375997359567		[learning rate: 0.0068137]
		[batch 20/20] avg loss: 0.31590689142443307		[learning rate: 0.0068057]
	Learning Rate: 0.00680568
	LOSS [training: 0.27900032569901434 | validation: 0.44580445395811397]
	TIME [epoch: 8.32 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3384670965744764		[learning rate: 0.0067977]
		[batch 20/20] avg loss: 0.298650013226638		[learning rate: 0.0067896]
	Learning Rate: 0.00678963
	LOSS [training: 0.3185585549005573 | validation: 0.19866850824061247]
	TIME [epoch: 8.33 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29047454815341733		[learning rate: 0.0067816]
		[batch 20/20] avg loss: 0.20374504649444258		[learning rate: 0.0067736]
	Learning Rate: 0.00677361
	LOSS [training: 0.24710979732392996 | validation: 0.12508877263784812]
	TIME [epoch: 8.33 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.226630017430607		[learning rate: 0.0067656]
		[batch 20/20] avg loss: 0.3031476289405373		[learning rate: 0.0067576]
	Learning Rate: 0.00675764
	LOSS [training: 0.26488882318557216 | validation: 0.1325500877582973]
	TIME [epoch: 8.31 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24564685034230327		[learning rate: 0.0067497]
		[batch 20/20] avg loss: 0.2407474383157838		[learning rate: 0.0067417]
	Learning Rate: 0.0067417
	LOSS [training: 0.24319714432904363 | validation: 0.2648270035989008]
	TIME [epoch: 8.32 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2169437410459357		[learning rate: 0.0067337]
		[batch 20/20] avg loss: 0.17730071540555709		[learning rate: 0.0067258]
	Learning Rate: 0.00672579
	LOSS [training: 0.19712222822574638 | validation: 0.08852194411688563]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22115197215568932		[learning rate: 0.0067179]
		[batch 20/20] avg loss: 0.2845538087236594		[learning rate: 0.0067099]
	Learning Rate: 0.00670993
	LOSS [training: 0.25285289043967435 | validation: 0.11418595390071071]
	TIME [epoch: 8.34 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26231644313588354		[learning rate: 0.006702]
		[batch 20/20] avg loss: 0.30269255584320176		[learning rate: 0.0066941]
	Learning Rate: 0.0066941
	LOSS [training: 0.2825044994895426 | validation: 0.14394508685882376]
	TIME [epoch: 8.32 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19443666797606113		[learning rate: 0.0066862]
		[batch 20/20] avg loss: 0.20855291180011437		[learning rate: 0.0066783]
	Learning Rate: 0.00667831
	LOSS [training: 0.20149478988808775 | validation: 0.13567901046029582]
	TIME [epoch: 8.32 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23207748960091648		[learning rate: 0.0066704]
		[batch 20/20] avg loss: 0.21339350038461852		[learning rate: 0.0066626]
	Learning Rate: 0.00666256
	LOSS [training: 0.2227354949927675 | validation: 0.2900187934729646]
	TIME [epoch: 8.34 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.344469163841033		[learning rate: 0.0066547]
		[batch 20/20] avg loss: 0.19577992749942158		[learning rate: 0.0066468]
	Learning Rate: 0.00664684
	LOSS [training: 0.27012454567022726 | validation: 0.09819220097840661]
	TIME [epoch: 8.33 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1626266963313799		[learning rate: 0.006639]
		[batch 20/20] avg loss: 0.23566082524291604		[learning rate: 0.0066312]
	Learning Rate: 0.00663116
	LOSS [training: 0.199143760787148 | validation: 0.1367409662344887]
	TIME [epoch: 8.32 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23455130137676328		[learning rate: 0.0066233]
		[batch 20/20] avg loss: 0.2199631535673418		[learning rate: 0.0066155]
	Learning Rate: 0.00661552
	LOSS [training: 0.22725722747205257 | validation: 0.17159976495294407]
	TIME [epoch: 8.32 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2156707491497294		[learning rate: 0.0066077]
		[batch 20/20] avg loss: 0.2170017727955535		[learning rate: 0.0065999]
	Learning Rate: 0.00659992
	LOSS [training: 0.2163362609726415 | validation: 0.13979315919955754]
	TIME [epoch: 8.34 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2238266901091412		[learning rate: 0.0065921]
		[batch 20/20] avg loss: 0.217294740679651		[learning rate: 0.0065843]
	Learning Rate: 0.00658435
	LOSS [training: 0.2205607153943961 | validation: 0.1425967501449137]
	TIME [epoch: 8.33 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17891414696422392		[learning rate: 0.0065766]
		[batch 20/20] avg loss: 0.3327414386881624		[learning rate: 0.0065688]
	Learning Rate: 0.00656882
	LOSS [training: 0.25582779282619317 | validation: 0.26416413542080297]
	TIME [epoch: 8.32 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23754697055249463		[learning rate: 0.0065611]
		[batch 20/20] avg loss: 0.198374948588723		[learning rate: 0.0065533]
	Learning Rate: 0.00655332
	LOSS [training: 0.21796095957060882 | validation: 0.30878332951031146]
	TIME [epoch: 8.32 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21875610851862876		[learning rate: 0.0065456]
		[batch 20/20] avg loss: 0.19444055689214923		[learning rate: 0.0065379]
	Learning Rate: 0.00653786
	LOSS [training: 0.206598332705389 | validation: 0.08552184616571065]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23805188503102376		[learning rate: 0.0065301]
		[batch 20/20] avg loss: 0.17977844906264306		[learning rate: 0.0065224]
	Learning Rate: 0.00652244
	LOSS [training: 0.20891516704683344 | validation: 0.19371240159317704]
	TIME [epoch: 8.34 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3223203511605426		[learning rate: 0.0065147]
		[batch 20/20] avg loss: 0.22955399466575263		[learning rate: 0.0065071]
	Learning Rate: 0.00650706
	LOSS [training: 0.27593717291314757 | validation: 0.11292091168264413]
	TIME [epoch: 8.32 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21129667160751625		[learning rate: 0.0064994]
		[batch 20/20] avg loss: 0.21489528774867975		[learning rate: 0.0064917]
	Learning Rate: 0.00649171
	LOSS [training: 0.213095979678098 | validation: 0.2898101922309452]
	TIME [epoch: 8.31 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2765252224206377		[learning rate: 0.006484]
		[batch 20/20] avg loss: 0.39894040682145687		[learning rate: 0.0064764]
	Learning Rate: 0.00647639
	LOSS [training: 0.3377328146210473 | validation: 0.2213781009563969]
	TIME [epoch: 8.33 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17878197102538979		[learning rate: 0.0064688]
		[batch 20/20] avg loss: 0.18637974128434726		[learning rate: 0.0064611]
	Learning Rate: 0.00646112
	LOSS [training: 0.18258085615486852 | validation: 0.5184896988720885]
	TIME [epoch: 8.33 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2741540450798981		[learning rate: 0.0064535]
		[batch 20/20] avg loss: 0.247222746196414		[learning rate: 0.0064459]
	Learning Rate: 0.00644588
	LOSS [training: 0.26068839563815605 | validation: 0.0793467300973429]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17269710708595265		[learning rate: 0.0064383]
		[batch 20/20] avg loss: 0.2595559096439447		[learning rate: 0.0064307]
	Learning Rate: 0.00643067
	LOSS [training: 0.21612650836494868 | validation: 0.21750205355477714]
	TIME [epoch: 8.34 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2503626226974243		[learning rate: 0.0064231]
		[batch 20/20] avg loss: 0.26471432135376305		[learning rate: 0.0064155]
	Learning Rate: 0.0064155
	LOSS [training: 0.25753847202559366 | validation: 0.2344772550760893]
	TIME [epoch: 8.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20311903673452983		[learning rate: 0.0064079]
		[batch 20/20] avg loss: 0.2703850611120917		[learning rate: 0.0064004]
	Learning Rate: 0.00640037
	LOSS [training: 0.23675204892331075 | validation: 0.16947211108838237]
	TIME [epoch: 8.33 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28377284850752954		[learning rate: 0.0063928]
		[batch 20/20] avg loss: 0.2296692255140747		[learning rate: 0.0063853]
	Learning Rate: 0.00638527
	LOSS [training: 0.25672103701080207 | validation: 0.2242985034511584]
	TIME [epoch: 8.33 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21737696400880746		[learning rate: 0.0063777]
		[batch 20/20] avg loss: 0.2793298057178037		[learning rate: 0.0063702]
	Learning Rate: 0.00637021
	LOSS [training: 0.24835338486330563 | validation: 0.392889164262494]
	TIME [epoch: 8.33 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3002313056237315		[learning rate: 0.0063627]
		[batch 20/20] avg loss: 0.17193783507247523		[learning rate: 0.0063552]
	Learning Rate: 0.00635518
	LOSS [training: 0.23608457034810337 | validation: 0.29091684802544693]
	TIME [epoch: 8.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2505484167568873		[learning rate: 0.0063477]
		[batch 20/20] avg loss: 0.20997984264446412		[learning rate: 0.0063402]
	Learning Rate: 0.00634019
	LOSS [training: 0.2302641297006757 | validation: 0.1595970501250678]
	TIME [epoch: 8.34 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22066944985032794		[learning rate: 0.0063327]
		[batch 20/20] avg loss: 0.18725185747105616		[learning rate: 0.0063252]
	Learning Rate: 0.00632524
	LOSS [training: 0.20396065366069202 | validation: 0.46830714673228935]
	TIME [epoch: 8.33 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3017689192068823		[learning rate: 0.0063178]
		[batch 20/20] avg loss: 0.3007741992743044		[learning rate: 0.0063103]
	Learning Rate: 0.00631032
	LOSS [training: 0.30127155924059335 | validation: 0.12397085633602455]
	TIME [epoch: 8.34 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14244002411202236		[learning rate: 0.0063029]
		[batch 20/20] avg loss: 0.22855795166831286		[learning rate: 0.0062954]
	Learning Rate: 0.00629543
	LOSS [training: 0.1854989878901676 | validation: 0.1523575018388733]
	TIME [epoch: 8.37 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2729277156267215		[learning rate: 0.006288]
		[batch 20/20] avg loss: 0.2186739344371011		[learning rate: 0.0062806]
	Learning Rate: 0.00628058
	LOSS [training: 0.24580082503191134 | validation: 0.2332533560555325]
	TIME [epoch: 8.34 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18974107497821907		[learning rate: 0.0062732]
		[batch 20/20] avg loss: 0.1793709544873611		[learning rate: 0.0062658]
	Learning Rate: 0.00626577
	LOSS [training: 0.1845560147327901 | validation: 0.11886964392886762]
	TIME [epoch: 8.33 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2657190656057162		[learning rate: 0.0062584]
		[batch 20/20] avg loss: 0.20565731913074167		[learning rate: 0.006251]
	Learning Rate: 0.00625099
	LOSS [training: 0.23568819236822894 | validation: 0.2612149488307479]
	TIME [epoch: 8.31 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2406760777948894		[learning rate: 0.0062436]
		[batch 20/20] avg loss: 0.25709184285805653		[learning rate: 0.0062362]
	Learning Rate: 0.00623624
	LOSS [training: 0.24888396032647292 | validation: 0.12276067524801274]
	TIME [epoch: 8.32 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27198894626744013		[learning rate: 0.0062289]
		[batch 20/20] avg loss: 0.22507937424274949		[learning rate: 0.0062215]
	Learning Rate: 0.00622153
	LOSS [training: 0.24853416025509484 | validation: 0.45170975574721184]
	TIME [epoch: 8.34 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19982596158703905		[learning rate: 0.0062142]
		[batch 20/20] avg loss: 0.2363715106238696		[learning rate: 0.0062069]
	Learning Rate: 0.00620686
	LOSS [training: 0.21809873610545435 | validation: 0.31217835126491345]
	TIME [epoch: 8.35 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1849188442835363		[learning rate: 0.0061995]
		[batch 20/20] avg loss: 0.16303198609100253		[learning rate: 0.0061922]
	Learning Rate: 0.00619222
	LOSS [training: 0.1739754151872694 | validation: 0.3161356688507162]
	TIME [epoch: 8.34 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19318049171098967		[learning rate: 0.0061849]
		[batch 20/20] avg loss: 0.28710114507574325		[learning rate: 0.0061776]
	Learning Rate: 0.00617761
	LOSS [training: 0.24014081839336637 | validation: 0.21722467762123984]
	TIME [epoch: 8.37 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23214835950973808		[learning rate: 0.0061703]
		[batch 20/20] avg loss: 0.21314774006315865		[learning rate: 0.006163]
	Learning Rate: 0.00616304
	LOSS [training: 0.22264804978644834 | validation: 0.11397523509456611]
	TIME [epoch: 8.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2469814250686877		[learning rate: 0.0061558]
		[batch 20/20] avg loss: 0.21539397977760566		[learning rate: 0.0061485]
	Learning Rate: 0.0061485
	LOSS [training: 0.23118770242314665 | validation: 0.21741354637235302]
	TIME [epoch: 8.32 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24981422894725508		[learning rate: 0.0061412]
		[batch 20/20] avg loss: 0.2798085619729377		[learning rate: 0.006134]
	Learning Rate: 0.006134
	LOSS [training: 0.26481139546009635 | validation: 0.18798127796890027]
	TIME [epoch: 8.33 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22475680250650543		[learning rate: 0.0061268]
		[batch 20/20] avg loss: 0.1836469562757304		[learning rate: 0.0061195]
	Learning Rate: 0.00611953
	LOSS [training: 0.20420187939111792 | validation: 0.15583695851519683]
	TIME [epoch: 8.36 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34287348750399904		[learning rate: 0.0061123]
		[batch 20/20] avg loss: 0.20507824859461712		[learning rate: 0.0061051]
	Learning Rate: 0.00610509
	LOSS [training: 0.2739758680493081 | validation: 0.09008810379336943]
	TIME [epoch: 8.33 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22348894453299722		[learning rate: 0.0060979]
		[batch 20/20] avg loss: 0.23046824209946698		[learning rate: 0.0060907]
	Learning Rate: 0.00609069
	LOSS [training: 0.2269785933162321 | validation: 0.0809068866063466]
	TIME [epoch: 8.32 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.187911344426468		[learning rate: 0.0060835]
		[batch 20/20] avg loss: 0.25600316800885625		[learning rate: 0.0060763]
	Learning Rate: 0.00607633
	LOSS [training: 0.2219572562176621 | validation: 0.3291835764926878]
	TIME [epoch: 8.32 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.297571871397113		[learning rate: 0.0060692]
		[batch 20/20] avg loss: 0.19225630260509224		[learning rate: 0.006062]
	Learning Rate: 0.00606199
	LOSS [training: 0.24491408700110262 | validation: 0.21334012908516725]
	TIME [epoch: 8.34 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.204946124447216		[learning rate: 0.0060548]
		[batch 20/20] avg loss: 0.18413737204779565		[learning rate: 0.0060477]
	Learning Rate: 0.00604769
	LOSS [training: 0.1945417482475058 | validation: 0.15236333705149532]
	TIME [epoch: 8.33 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25744573486643746		[learning rate: 0.0060406]
		[batch 20/20] avg loss: 0.19057507545415783		[learning rate: 0.0060334]
	Learning Rate: 0.00603343
	LOSS [training: 0.22401040516029763 | validation: 0.11217843346393075]
	TIME [epoch: 8.32 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18183727381731413		[learning rate: 0.0060263]
		[batch 20/20] avg loss: 0.1995554944294284		[learning rate: 0.0060192]
	Learning Rate: 0.0060192
	LOSS [training: 0.19069638412337125 | validation: 0.17563185723824976]
	TIME [epoch: 8.32 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20881513420363698		[learning rate: 0.0060121]
		[batch 20/20] avg loss: 0.236749857147303		[learning rate: 0.006005]
	Learning Rate: 0.006005
	LOSS [training: 0.22278249567546995 | validation: 0.12122990873949233]
	TIME [epoch: 8.34 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22888860362844343		[learning rate: 0.0059979]
		[batch 20/20] avg loss: 0.17324178816318012		[learning rate: 0.0059908]
	Learning Rate: 0.00599083
	LOSS [training: 0.20106519589581176 | validation: 0.23218365800440555]
	TIME [epoch: 8.33 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18907266736358225		[learning rate: 0.0059838]
		[batch 20/20] avg loss: 0.21611902973475844		[learning rate: 0.0059767]
	Learning Rate: 0.0059767
	LOSS [training: 0.2025958485491703 | validation: 0.10023192724905416]
	TIME [epoch: 8.32 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25232856138968074		[learning rate: 0.0059696]
		[batch 20/20] avg loss: 0.2397465003842389		[learning rate: 0.0059626]
	Learning Rate: 0.0059626
	LOSS [training: 0.24603753088695984 | validation: 0.2041165094718462]
	TIME [epoch: 8.32 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1760363798879961		[learning rate: 0.0059556]
		[batch 20/20] avg loss: 0.2112361019602924		[learning rate: 0.0059485]
	Learning Rate: 0.00594854
	LOSS [training: 0.19363624092414425 | validation: 0.7421127308861559]
	TIME [epoch: 8.35 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2443605931817455		[learning rate: 0.0059415]
		[batch 20/20] avg loss: 0.24335260401846828		[learning rate: 0.0059345]
	Learning Rate: 0.00593451
	LOSS [training: 0.24385659860010694 | validation: 0.4496261850393771]
	TIME [epoch: 8.33 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2665861170488179		[learning rate: 0.0059275]
		[batch 20/20] avg loss: 0.25436450917835174		[learning rate: 0.0059205]
	Learning Rate: 0.00592051
	LOSS [training: 0.2604753131135848 | validation: 0.12430188066011114]
	TIME [epoch: 8.32 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22284857988358092		[learning rate: 0.0059135]
		[batch 20/20] avg loss: 0.21672313104004787		[learning rate: 0.0059065]
	Learning Rate: 0.00590654
	LOSS [training: 0.21978585546181434 | validation: 0.10627807489857392]
	TIME [epoch: 8.32 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23459115715417256		[learning rate: 0.0058996]
		[batch 20/20] avg loss: 0.20762339757380902		[learning rate: 0.0058926]
	Learning Rate: 0.00589261
	LOSS [training: 0.22110727736399075 | validation: 0.10062392726555754]
	TIME [epoch: 8.35 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2254913105139981		[learning rate: 0.0058857]
		[batch 20/20] avg loss: 0.23667950455504405		[learning rate: 0.0058787]
	Learning Rate: 0.00587871
	LOSS [training: 0.2310854075345211 | validation: 0.10815939555859307]
	TIME [epoch: 8.33 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22913989803909202		[learning rate: 0.0058718]
		[batch 20/20] avg loss: 0.3809410722437029		[learning rate: 0.0058648]
	Learning Rate: 0.00586484
	LOSS [training: 0.30504048514139737 | validation: 0.42153579829994753]
	TIME [epoch: 8.33 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25676320384567275		[learning rate: 0.0058579]
		[batch 20/20] avg loss: 0.29823500179923357		[learning rate: 0.005851]
	Learning Rate: 0.00585101
	LOSS [training: 0.27749910282245316 | validation: 0.20046000525423038]
	TIME [epoch: 8.33 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23484811741827044		[learning rate: 0.0058441]
		[batch 20/20] avg loss: 0.20571503827021012		[learning rate: 0.0058372]
	Learning Rate: 0.00583721
	LOSS [training: 0.22028157784424027 | validation: 0.27262546055910764]
	TIME [epoch: 8.35 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2300033404445243		[learning rate: 0.0058303]
		[batch 20/20] avg loss: 0.21593602455550806		[learning rate: 0.0058234]
	Learning Rate: 0.00582344
	LOSS [training: 0.2229696825000162 | validation: 0.3037261619994947]
	TIME [epoch: 8.33 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19564070197747935		[learning rate: 0.0058166]
		[batch 20/20] avg loss: 0.24495699270676097		[learning rate: 0.0058097]
	Learning Rate: 0.0058097
	LOSS [training: 0.22029884734212019 | validation: 0.15447478805337328]
	TIME [epoch: 8.32 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1742932024980044		[learning rate: 0.0058028]
		[batch 20/20] avg loss: 0.23805666192610012		[learning rate: 0.005796]
	Learning Rate: 0.005796
	LOSS [training: 0.20617493221205224 | validation: 0.14163148070525589]
	TIME [epoch: 8.32 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16712106055843498		[learning rate: 0.0057892]
		[batch 20/20] avg loss: 0.25743344694385106		[learning rate: 0.0057823]
	Learning Rate: 0.00578233
	LOSS [training: 0.21227725375114304 | validation: 0.20070948076399542]
	TIME [epoch: 8.34 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2112143745460553		[learning rate: 0.0057755]
		[batch 20/20] avg loss: 0.14907664860138287		[learning rate: 0.0057687]
	Learning Rate: 0.00576869
	LOSS [training: 0.1801455115737191 | validation: 0.09979871906336199]
	TIME [epoch: 8.33 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24268230736934884		[learning rate: 0.0057619]
		[batch 20/20] avg loss: 0.14106944908570312		[learning rate: 0.0057551]
	Learning Rate: 0.00575508
	LOSS [training: 0.19187587822752594 | validation: 0.1000209341371329]
	TIME [epoch: 8.32 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.175603412466793		[learning rate: 0.0057483]
		[batch 20/20] avg loss: 0.21271715158827117		[learning rate: 0.0057415]
	Learning Rate: 0.0057415
	LOSS [training: 0.19416028202753213 | validation: 0.11708501814322031]
	TIME [epoch: 8.32 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16765180245311712		[learning rate: 0.0057347]
		[batch 20/20] avg loss: 0.19424871089018653		[learning rate: 0.005728]
	Learning Rate: 0.00572796
	LOSS [training: 0.18095025667165188 | validation: 0.12320808557716323]
	TIME [epoch: 8.35 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1641905128602152		[learning rate: 0.0057212]
		[batch 20/20] avg loss: 0.2085457299219276		[learning rate: 0.0057144]
	Learning Rate: 0.00571445
	LOSS [training: 0.1863681213910714 | validation: 0.2441260512657057]
	TIME [epoch: 8.33 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.196511336187913		[learning rate: 0.0057077]
		[batch 20/20] avg loss: 0.22050940243203637		[learning rate: 0.005701]
	Learning Rate: 0.00570097
	LOSS [training: 0.2085103693099747 | validation: 0.16584187130576114]
	TIME [epoch: 8.32 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19714047084994363		[learning rate: 0.0056942]
		[batch 20/20] avg loss: 0.19614347755419353		[learning rate: 0.0056875]
	Learning Rate: 0.00568752
	LOSS [training: 0.19664197420206864 | validation: 0.3135773864972159]
	TIME [epoch: 8.32 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27154648695963923		[learning rate: 0.0056808]
		[batch 20/20] avg loss: 0.16998361084994065		[learning rate: 0.0056741]
	Learning Rate: 0.00567411
	LOSS [training: 0.22076504890478996 | validation: 0.18740146754606835]
	TIME [epoch: 8.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20284401536150312		[learning rate: 0.0056674]
		[batch 20/20] avg loss: 0.171812122915476		[learning rate: 0.0056607]
	Learning Rate: 0.00566072
	LOSS [training: 0.18732806913848957 | validation: 0.504933664947879]
	TIME [epoch: 8.33 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20927457476165592		[learning rate: 0.005654]
		[batch 20/20] avg loss: 0.23547784444783307		[learning rate: 0.0056474]
	Learning Rate: 0.00564737
	LOSS [training: 0.2223762096047445 | validation: 0.23416185828418695]
	TIME [epoch: 8.32 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21370140200612395		[learning rate: 0.0056407]
		[batch 20/20] avg loss: 0.14753466609972674		[learning rate: 0.005634]
	Learning Rate: 0.00563405
	LOSS [training: 0.1806180340529254 | validation: 0.1349458042047087]
	TIME [epoch: 8.32 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17056771211852		[learning rate: 0.0056274]
		[batch 20/20] avg loss: 0.22802281588461906		[learning rate: 0.0056208]
	Learning Rate: 0.00562076
	LOSS [training: 0.19929526400156955 | validation: 0.08211457864176917]
	TIME [epoch: 8.35 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18347082706082565		[learning rate: 0.0056141]
		[batch 20/20] avg loss: 0.17631752049370028		[learning rate: 0.0056075]
	Learning Rate: 0.0056075
	LOSS [training: 0.179894173777263 | validation: 0.22599121785044543]
	TIME [epoch: 8.33 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2643067765523176		[learning rate: 0.0056009]
		[batch 20/20] avg loss: 0.3131677472755092		[learning rate: 0.0055943]
	Learning Rate: 0.00559427
	LOSS [training: 0.28873726191391347 | validation: 0.2744846986515472]
	TIME [epoch: 8.33 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24679707161088466		[learning rate: 0.0055877]
		[batch 20/20] avg loss: 0.20640345810201577		[learning rate: 0.0055811]
	Learning Rate: 0.00558108
	LOSS [training: 0.22660026485645018 | validation: 0.24402996770899882]
	TIME [epoch: 8.32 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15338292902249018		[learning rate: 0.0055745]
		[batch 20/20] avg loss: 0.24756504443646307		[learning rate: 0.0055679]
	Learning Rate: 0.00556791
	LOSS [training: 0.20047398672947664 | validation: 0.20406237179515946]
	TIME [epoch: 8.35 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2252582665395953		[learning rate: 0.0055613]
		[batch 20/20] avg loss: 0.22879817324263252		[learning rate: 0.0055548]
	Learning Rate: 0.00555478
	LOSS [training: 0.2270282198911139 | validation: 0.19461812716816268]
	TIME [epoch: 8.33 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14840589456381856		[learning rate: 0.0055482]
		[batch 20/20] avg loss: 0.15774767533104964		[learning rate: 0.0055417]
	Learning Rate: 0.00554167
	LOSS [training: 0.1530767849474341 | validation: 0.07397125452531772]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22617033526959465		[learning rate: 0.0055351]
		[batch 20/20] avg loss: 0.18528799254757308		[learning rate: 0.0055286]
	Learning Rate: 0.0055286
	LOSS [training: 0.2057291639085838 | validation: 0.2663243632658486]
	TIME [epoch: 8.31 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22598456537275124		[learning rate: 0.0055221]
		[batch 20/20] avg loss: 0.26484561433998544		[learning rate: 0.0055156]
	Learning Rate: 0.00551556
	LOSS [training: 0.24541508985636828 | validation: 0.31256886596167466]
	TIME [epoch: 8.33 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2172466577747282		[learning rate: 0.0055091]
		[batch 20/20] avg loss: 0.2726662627799742		[learning rate: 0.0055026]
	Learning Rate: 0.00550255
	LOSS [training: 0.2449564602773512 | validation: 0.8329016663512545]
	TIME [epoch: 8.32 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2830890180971962		[learning rate: 0.0054961]
		[batch 20/20] avg loss: 0.24643476110367818		[learning rate: 0.0054896]
	Learning Rate: 0.00548957
	LOSS [training: 0.26476188960043723 | validation: 0.11982385024912445]
	TIME [epoch: 8.31 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14749297608923356		[learning rate: 0.0054831]
		[batch 20/20] avg loss: 0.2369031729545205		[learning rate: 0.0054766]
	Learning Rate: 0.00547662
	LOSS [training: 0.19219807452187704 | validation: 0.23828240424760666]
	TIME [epoch: 8.31 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.224148468418995		[learning rate: 0.0054702]
		[batch 20/20] avg loss: 0.247507074072448		[learning rate: 0.0054637]
	Learning Rate: 0.0054637
	LOSS [training: 0.23582777124572152 | validation: 0.11106661260869352]
	TIME [epoch: 8.34 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12944836850459662		[learning rate: 0.0054573]
		[batch 20/20] avg loss: 0.322359415027938		[learning rate: 0.0054508]
	Learning Rate: 0.00545082
	LOSS [training: 0.22590389176626732 | validation: 0.15471281223271668]
	TIME [epoch: 8.31 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17783883119600102		[learning rate: 0.0054444]
		[batch 20/20] avg loss: 0.16510112974582722		[learning rate: 0.005438]
	Learning Rate: 0.00543796
	LOSS [training: 0.17146998047091414 | validation: 0.2296735693191894]
	TIME [epoch: 8.31 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1541068521802913		[learning rate: 0.0054315]
		[batch 20/20] avg loss: 0.1435968451836916		[learning rate: 0.0054251]
	Learning Rate: 0.00542513
	LOSS [training: 0.14885184868199147 | validation: 0.18204344358052393]
	TIME [epoch: 8.31 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20060228669197736		[learning rate: 0.0054187]
		[batch 20/20] avg loss: 0.2294532548404848		[learning rate: 0.0054123]
	Learning Rate: 0.00541233
	LOSS [training: 0.21502777076623106 | validation: 0.295158402171399]
	TIME [epoch: 8.33 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1803500188404777		[learning rate: 0.0054059]
		[batch 20/20] avg loss: 0.21431435158340548		[learning rate: 0.0053996]
	Learning Rate: 0.00539957
	LOSS [training: 0.1973321852119416 | validation: 0.07278358253323233]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13303519012828335		[learning rate: 0.0053932]
		[batch 20/20] avg loss: 0.18217467947618454		[learning rate: 0.0053868]
	Learning Rate: 0.00538683
	LOSS [training: 0.15760493480223392 | validation: 0.053946459758882626]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20257285239910114		[learning rate: 0.0053805]
		[batch 20/20] avg loss: 0.18451884416788736		[learning rate: 0.0053741]
	Learning Rate: 0.00537412
	LOSS [training: 0.19354584828349425 | validation: 0.09324097529291081]
	TIME [epoch: 8.32 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17935810209814806		[learning rate: 0.0053678]
		[batch 20/20] avg loss: 0.24571566989270638		[learning rate: 0.0053614]
	Learning Rate: 0.00536145
	LOSS [training: 0.21253688599542722 | validation: 0.09081865518936409]
	TIME [epoch: 8.34 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.140049221730774		[learning rate: 0.0053551]
		[batch 20/20] avg loss: 0.17095663478937087		[learning rate: 0.0053488]
	Learning Rate: 0.0053488
	LOSS [training: 0.15550292826007245 | validation: 0.15365871664865796]
	TIME [epoch: 8.33 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.216136353125899		[learning rate: 0.0053425]
		[batch 20/20] avg loss: 0.18057951170934228		[learning rate: 0.0053362]
	Learning Rate: 0.00533618
	LOSS [training: 0.1983579324176206 | validation: 0.15511519197112586]
	TIME [epoch: 8.3 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19170607849020832		[learning rate: 0.0053299]
		[batch 20/20] avg loss: 0.23446816075113106		[learning rate: 0.0053236]
	Learning Rate: 0.0053236
	LOSS [training: 0.21308711962066967 | validation: 0.0612646931276171]
	TIME [epoch: 8.32 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.163845837076361		[learning rate: 0.0053173]
		[batch 20/20] avg loss: 0.17317058283078243		[learning rate: 0.005311]
	Learning Rate: 0.00531104
	LOSS [training: 0.16850820995357174 | validation: 0.2283341908179471]
	TIME [epoch: 8.35 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18281198111539113		[learning rate: 0.0053048]
		[batch 20/20] avg loss: 0.28145098935230817		[learning rate: 0.0052985]
	Learning Rate: 0.00529851
	LOSS [training: 0.23213148523384963 | validation: 0.10993722244834797]
	TIME [epoch: 8.32 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1733960424710942		[learning rate: 0.0052923]
		[batch 20/20] avg loss: 0.19172611849420806		[learning rate: 0.005286]
	Learning Rate: 0.00528601
	LOSS [training: 0.18256108048265113 | validation: 0.5376460221377382]
	TIME [epoch: 8.32 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18424943450087625		[learning rate: 0.0052798]
		[batch 20/20] avg loss: 0.2632328539514656		[learning rate: 0.0052735]
	Learning Rate: 0.00527354
	LOSS [training: 0.22374114422617092 | validation: 0.49423604171691693]
	TIME [epoch: 8.32 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2364802617609605		[learning rate: 0.0052673]
		[batch 20/20] avg loss: 0.2172230813696699		[learning rate: 0.0052611]
	Learning Rate: 0.0052611
	LOSS [training: 0.22685167156531513 | validation: 0.08176900445791449]
	TIME [epoch: 8.34 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20152072753320213		[learning rate: 0.0052549]
		[batch 20/20] avg loss: 0.1568012076977347		[learning rate: 0.0052487]
	Learning Rate: 0.00524869
	LOSS [training: 0.1791609676154684 | validation: 0.12191613461094755]
	TIME [epoch: 8.32 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23469862141638567		[learning rate: 0.0052425]
		[batch 20/20] avg loss: 0.27006726672997466		[learning rate: 0.0052363]
	Learning Rate: 0.00523631
	LOSS [training: 0.2523829440731802 | validation: 0.055858357491876495]
	TIME [epoch: 8.32 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16390278115122467		[learning rate: 0.0052301]
		[batch 20/20] avg loss: 0.20651462721517383		[learning rate: 0.005224]
	Learning Rate: 0.00522396
	LOSS [training: 0.18520870418319924 | validation: 0.11442841409442805]
	TIME [epoch: 8.33 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1814890799991736		[learning rate: 0.0052178]
		[batch 20/20] avg loss: 0.19431021909604293		[learning rate: 0.0052116]
	Learning Rate: 0.00521164
	LOSS [training: 0.1878996495476083 | validation: 0.21437916356866824]
	TIME [epoch: 8.34 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1333244619724367		[learning rate: 0.0052055]
		[batch 20/20] avg loss: 0.15490719613541448		[learning rate: 0.0051993]
	Learning Rate: 0.00519935
	LOSS [training: 0.1441158290539256 | validation: 0.21023893421021753]
	TIME [epoch: 8.31 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23769068898160334		[learning rate: 0.0051932]
		[batch 20/20] avg loss: 0.21941598164815437		[learning rate: 0.0051871]
	Learning Rate: 0.00518708
	LOSS [training: 0.22855333531487884 | validation: 0.19285676663501056]
	TIME [epoch: 8.32 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18565378072276068		[learning rate: 0.005181]
		[batch 20/20] avg loss: 0.196469689823896		[learning rate: 0.0051748]
	Learning Rate: 0.00517485
	LOSS [training: 0.1910617352733284 | validation: 0.14137520837289289]
	TIME [epoch: 8.31 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1849896516118309		[learning rate: 0.0051687]
		[batch 20/20] avg loss: 0.23348956968629692		[learning rate: 0.0051626]
	Learning Rate: 0.00516264
	LOSS [training: 0.20923961064906393 | validation: 0.07751789945638013]
	TIME [epoch: 8.34 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13910855779894235		[learning rate: 0.0051565]
		[batch 20/20] avg loss: 0.22340755657784164		[learning rate: 0.0051505]
	Learning Rate: 0.00515046
	LOSS [training: 0.181258057188392 | validation: 0.3831398713330104]
	TIME [epoch: 8.32 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23795798979800448		[learning rate: 0.0051444]
		[batch 20/20] avg loss: 0.1956454411958622		[learning rate: 0.0051383]
	Learning Rate: 0.00513831
	LOSS [training: 0.21680171549693333 | validation: 0.07735823319463739]
	TIME [epoch: 8.31 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20076510050529944		[learning rate: 0.0051322]
		[batch 20/20] avg loss: 0.20721025951158234		[learning rate: 0.0051262]
	Learning Rate: 0.00512619
	LOSS [training: 0.20398768000844086 | validation: 0.10093790638939272]
	TIME [epoch: 8.31 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15851440849852547		[learning rate: 0.0051201]
		[batch 20/20] avg loss: 0.3244262773165777		[learning rate: 0.0051141]
	Learning Rate: 0.0051141
	LOSS [training: 0.2414703429075516 | validation: 0.19425423042281817]
	TIME [epoch: 8.34 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1732902866139138		[learning rate: 0.0051081]
		[batch 20/20] avg loss: 0.17022859888047628		[learning rate: 0.005102]
	Learning Rate: 0.00510204
	LOSS [training: 0.17175944274719504 | validation: 0.3497457977066808]
	TIME [epoch: 8.31 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1787567825586535		[learning rate: 0.005096]
		[batch 20/20] avg loss: 0.19460018554221636		[learning rate: 0.00509]
	Learning Rate: 0.00509
	LOSS [training: 0.1866784840504349 | validation: 0.3239902667378375]
	TIME [epoch: 8.33 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2147086554832392		[learning rate: 0.005084]
		[batch 20/20] avg loss: 0.2127849987412787		[learning rate: 0.005078]
	Learning Rate: 0.00507799
	LOSS [training: 0.21374682711225895 | validation: 0.1419379052372329]
	TIME [epoch: 8.32 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2718957290488997		[learning rate: 0.005072]
		[batch 20/20] avg loss: 0.23060489567132728		[learning rate: 0.005066]
	Learning Rate: 0.00506602
	LOSS [training: 0.25125031236011347 | validation: 0.1666917925825273]
	TIME [epoch: 8.35 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18859487779676676		[learning rate: 0.00506]
		[batch 20/20] avg loss: 0.22391246798783512		[learning rate: 0.0050541]
	Learning Rate: 0.00505407
	LOSS [training: 0.20625367289230095 | validation: 0.14503810736582934]
	TIME [epoch: 8.33 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21055922988736656		[learning rate: 0.0050481]
		[batch 20/20] avg loss: 0.1781354504967695		[learning rate: 0.0050421]
	Learning Rate: 0.00504215
	LOSS [training: 0.19434734019206798 | validation: 0.10223867290452604]
	TIME [epoch: 8.32 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14786171660312328		[learning rate: 0.0050362]
		[batch 20/20] avg loss: 0.1590861319351045		[learning rate: 0.0050303]
	Learning Rate: 0.00503025
	LOSS [training: 0.15347392426911394 | validation: 0.19749386409225791]
	TIME [epoch: 8.32 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25226562036535		[learning rate: 0.0050243]
		[batch 20/20] avg loss: 0.17550635142632456		[learning rate: 0.0050184]
	Learning Rate: 0.00501839
	LOSS [training: 0.2138859858958373 | validation: 0.2771509043151109]
	TIME [epoch: 8.34 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14923317688022852		[learning rate: 0.0050125]
		[batch 20/20] avg loss: 0.1717091521314981		[learning rate: 0.0050065]
	Learning Rate: 0.00500655
	LOSS [training: 0.16047116450586335 | validation: 0.10947575868512259]
	TIME [epoch: 8.32 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29876939545718234		[learning rate: 0.0050006]
		[batch 20/20] avg loss: 0.2218223848894226		[learning rate: 0.0049947]
	Learning Rate: 0.00499474
	LOSS [training: 0.2602958901733025 | validation: 0.07598611073035719]
	TIME [epoch: 8.32 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1647589084529027		[learning rate: 0.0049888]
		[batch 20/20] avg loss: 0.2010033646160863		[learning rate: 0.004983]
	Learning Rate: 0.00498296
	LOSS [training: 0.1828811365344945 | validation: 0.2222484131432864]
	TIME [epoch: 8.32 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19473348525471004		[learning rate: 0.0049771]
		[batch 20/20] avg loss: 0.2330615483510175		[learning rate: 0.0049712]
	Learning Rate: 0.0049712
	LOSS [training: 0.2138975168028637 | validation: 0.31036193355941893]
	TIME [epoch: 8.35 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1424762831586744		[learning rate: 0.0049653]
		[batch 20/20] avg loss: 0.2190725529094179		[learning rate: 0.0049595]
	Learning Rate: 0.00495948
	LOSS [training: 0.18077441803404615 | validation: 0.6208354932031382]
	TIME [epoch: 8.32 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2688036012158529		[learning rate: 0.0049536]
		[batch 20/20] avg loss: 0.19618395203104308		[learning rate: 0.0049478]
	Learning Rate: 0.00494778
	LOSS [training: 0.23249377662344797 | validation: 0.06733410464337805]
	TIME [epoch: 8.33 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20615832273225926		[learning rate: 0.0049419]
		[batch 20/20] avg loss: 0.14430728625315487		[learning rate: 0.0049361]
	Learning Rate: 0.00493611
	LOSS [training: 0.175232804492707 | validation: 0.14741253560910295]
	TIME [epoch: 8.32 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14371194957063768		[learning rate: 0.0049303]
		[batch 20/20] avg loss: 0.2775857843881534		[learning rate: 0.0049245]
	Learning Rate: 0.00492446
	LOSS [training: 0.21064886697939555 | validation: 0.15850180309504883]
	TIME [epoch: 8.34 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18573033456087518		[learning rate: 0.0049187]
		[batch 20/20] avg loss: 0.17216492907446893		[learning rate: 0.0049128]
	Learning Rate: 0.00491285
	LOSS [training: 0.17894763181767206 | validation: 0.26012756268360604]
	TIME [epoch: 8.32 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13047030370534846		[learning rate: 0.0049071]
		[batch 20/20] avg loss: 0.21941500458439203		[learning rate: 0.0049013]
	Learning Rate: 0.00490126
	LOSS [training: 0.17494265414487026 | validation: 0.17663330434394903]
	TIME [epoch: 8.32 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18244655750101668		[learning rate: 0.0048955]
		[batch 20/20] avg loss: 0.19176989500783653		[learning rate: 0.0048897]
	Learning Rate: 0.0048897
	LOSS [training: 0.18710822625442664 | validation: 0.1983147791133467]
	TIME [epoch: 8.33 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1934569846934541		[learning rate: 0.0048839]
		[batch 20/20] avg loss: 0.18092092104199992		[learning rate: 0.0048782]
	Learning Rate: 0.00487816
	LOSS [training: 0.18718895286772702 | validation: 0.08547645853844835]
	TIME [epoch: 8.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17237218773250468		[learning rate: 0.0048724]
		[batch 20/20] avg loss: 0.2900444074727908		[learning rate: 0.0048667]
	Learning Rate: 0.00486666
	LOSS [training: 0.23120829760264777 | validation: 0.20071756840453295]
	TIME [epoch: 8.32 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21120472468417678		[learning rate: 0.0048609]
		[batch 20/20] avg loss: 0.22683119393769618		[learning rate: 0.0048552]
	Learning Rate: 0.00485518
	LOSS [training: 0.21901795931093648 | validation: 0.1016748926437431]
	TIME [epoch: 8.31 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1758551391549801		[learning rate: 0.0048494]
		[batch 20/20] avg loss: 0.14128369082078085		[learning rate: 0.0048437]
	Learning Rate: 0.00484372
	LOSS [training: 0.15856941498788046 | validation: 0.08177863772453231]
	TIME [epoch: 8.32 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22709623822954977		[learning rate: 0.004838]
		[batch 20/20] avg loss: 0.2019217682140982		[learning rate: 0.0048323]
	Learning Rate: 0.0048323
	LOSS [training: 0.21450900322182398 | validation: 0.08007881648654647]
	TIME [epoch: 8.34 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20047675226069955		[learning rate: 0.0048266]
		[batch 20/20] avg loss: 0.172973266825784		[learning rate: 0.0048209]
	Learning Rate: 0.0048209
	LOSS [training: 0.1867250095432418 | validation: 0.23395002102765267]
	TIME [epoch: 8.31 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24877173739774547		[learning rate: 0.0048152]
		[batch 20/20] avg loss: 0.18959551538050848		[learning rate: 0.0048095]
	Learning Rate: 0.00480953
	LOSS [training: 0.21918362638912697 | validation: 0.2971776886489881]
	TIME [epoch: 8.32 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20019314239358704		[learning rate: 0.0048039]
		[batch 20/20] avg loss: 0.1940393078520481		[learning rate: 0.0047982]
	Learning Rate: 0.00479818
	LOSS [training: 0.19711622512281754 | validation: 0.0948785310795304]
	TIME [epoch: 8.32 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1603663720031413		[learning rate: 0.0047925]
		[batch 20/20] avg loss: 0.1468539768609723		[learning rate: 0.0047869]
	Learning Rate: 0.00478687
	LOSS [training: 0.15361017443205677 | validation: 0.13892301680655433]
	TIME [epoch: 8.34 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2535080392304805		[learning rate: 0.0047812]
		[batch 20/20] avg loss: 0.1850328004326331		[learning rate: 0.0047756]
	Learning Rate: 0.00477557
	LOSS [training: 0.21927041983155676 | validation: 0.09460811330485705]
	TIME [epoch: 8.32 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2682505922469357		[learning rate: 0.0047699]
		[batch 20/20] avg loss: 0.25478800005442254		[learning rate: 0.0047643]
	Learning Rate: 0.00476431
	LOSS [training: 0.26151929615067904 | validation: 0.468001603850681]
	TIME [epoch: 8.32 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21717756641549718		[learning rate: 0.0047587]
		[batch 20/20] avg loss: 0.21713357180530352		[learning rate: 0.0047531]
	Learning Rate: 0.00475307
	LOSS [training: 0.21715556911040035 | validation: 0.5392025836708985]
	TIME [epoch: 8.32 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26032880715617146		[learning rate: 0.0047475]
		[batch 20/20] avg loss: 0.2038798938862043		[learning rate: 0.0047419]
	Learning Rate: 0.00474186
	LOSS [training: 0.23210435052118786 | validation: 0.23111232113887203]
	TIME [epoch: 8.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26367718209146196		[learning rate: 0.0047363]
		[batch 20/20] avg loss: 0.250134358102638		[learning rate: 0.0047307]
	Learning Rate: 0.00473067
	LOSS [training: 0.25690577009704996 | validation: 0.1910507438023382]
	TIME [epoch: 8.32 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21224474093850598		[learning rate: 0.0047251]
		[batch 20/20] avg loss: 0.21554617517612656		[learning rate: 0.0047195]
	Learning Rate: 0.00471952
	LOSS [training: 0.2138954580573163 | validation: 0.22734236833178154]
	TIME [epoch: 8.32 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2567274578119173		[learning rate: 0.0047139]
		[batch 20/20] avg loss: 0.17053791932185197		[learning rate: 0.0047084]
	Learning Rate: 0.00470838
	LOSS [training: 0.2136326885668846 | validation: 0.09008224025921827]
	TIME [epoch: 8.32 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24317518304101648		[learning rate: 0.0047028]
		[batch 20/20] avg loss: 0.17508167915130401		[learning rate: 0.0046973]
	Learning Rate: 0.00469728
	LOSS [training: 0.20912843109616025 | validation: 0.21810617643883207]
	TIME [epoch: 8.33 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1897496077322356		[learning rate: 0.0046917]
		[batch 20/20] avg loss: 0.22017363159910017		[learning rate: 0.0046862]
	Learning Rate: 0.0046862
	LOSS [training: 0.2049616196656679 | validation: 0.07511964256362293]
	TIME [epoch: 8.32 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2313610127566716		[learning rate: 0.0046807]
		[batch 20/20] avg loss: 0.2856757031419806		[learning rate: 0.0046751]
	Learning Rate: 0.00467514
	LOSS [training: 0.2585183579493261 | validation: 0.46141178494317914]
	TIME [epoch: 8.32 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29281605427768287		[learning rate: 0.0046696]
		[batch 20/20] avg loss: 0.26116781807829786		[learning rate: 0.0046641]
	Learning Rate: 0.00466411
	LOSS [training: 0.2769919361779904 | validation: 0.31625495873652587]
	TIME [epoch: 8.32 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19920698510379484		[learning rate: 0.0046586]
		[batch 20/20] avg loss: 0.2192772984547886		[learning rate: 0.0046531]
	Learning Rate: 0.00465311
	LOSS [training: 0.2092421417792918 | validation: 0.29784039869339535]
	TIME [epoch: 8.34 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24056112748572125		[learning rate: 0.0046476]
		[batch 20/20] avg loss: 0.33482307367190833		[learning rate: 0.0046421]
	Learning Rate: 0.00464214
	LOSS [training: 0.2876921005788148 | validation: 0.4684338493091247]
	TIME [epoch: 8.32 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2386965224259295		[learning rate: 0.0046367]
		[batch 20/20] avg loss: 0.20837562717261204		[learning rate: 0.0046312]
	Learning Rate: 0.00463119
	LOSS [training: 0.22353607479927082 | validation: 0.11023692087285003]
	TIME [epoch: 8.32 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2244625413972133		[learning rate: 0.0046257]
		[batch 20/20] avg loss: 0.14954817733978204		[learning rate: 0.0046203]
	Learning Rate: 0.00462026
	LOSS [training: 0.1870053593684977 | validation: 0.11371267676030325]
	TIME [epoch: 8.32 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18465916909416452		[learning rate: 0.0046148]
		[batch 20/20] avg loss: 0.3518440397597031		[learning rate: 0.0046094]
	Learning Rate: 0.00460936
	LOSS [training: 0.2682516044269338 | validation: 0.184934772096366]
	TIME [epoch: 8.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27944102419028727		[learning rate: 0.0046039]
		[batch 20/20] avg loss: 0.1894787433264881		[learning rate: 0.0045985]
	Learning Rate: 0.00459849
	LOSS [training: 0.2344598837583877 | validation: 0.40047174888438586]
	TIME [epoch: 8.32 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2640350813313271		[learning rate: 0.0045931]
		[batch 20/20] avg loss: 0.27026655192303656		[learning rate: 0.0045876]
	Learning Rate: 0.00458764
	LOSS [training: 0.26715081662718176 | validation: 0.2986736841874058]
	TIME [epoch: 8.32 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20562079861436663		[learning rate: 0.0045822]
		[batch 20/20] avg loss: 0.15508850082795186		[learning rate: 0.0045768]
	Learning Rate: 0.00457682
	LOSS [training: 0.18035464972115928 | validation: 0.2378621334093659]
	TIME [epoch: 8.33 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17460786769077036		[learning rate: 0.0045714]
		[batch 20/20] avg loss: 0.2823752847460772		[learning rate: 0.004566]
	Learning Rate: 0.00456603
	LOSS [training: 0.2284915762184238 | validation: 0.12402677477917616]
	TIME [epoch: 8.34 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13993384400419612		[learning rate: 0.0045606]
		[batch 20/20] avg loss: 0.2066300034790313		[learning rate: 0.0045553]
	Learning Rate: 0.00455526
	LOSS [training: 0.17328192374161372 | validation: 0.30495275093996366]
	TIME [epoch: 8.33 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16704027571473604		[learning rate: 0.0045499]
		[batch 20/20] avg loss: 0.18481816284000274		[learning rate: 0.0045445]
	Learning Rate: 0.00454451
	LOSS [training: 0.17592921927736938 | validation: 0.17261348808379773]
	TIME [epoch: 8.32 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16854243591732482		[learning rate: 0.0045391]
		[batch 20/20] avg loss: 0.20912452768568737		[learning rate: 0.0045338]
	Learning Rate: 0.00453379
	LOSS [training: 0.1888334818015061 | validation: 0.17139294845184688]
	TIME [epoch: 8.33 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18956788897166596		[learning rate: 0.0045284]
		[batch 20/20] avg loss: 0.21563581308729277		[learning rate: 0.0045231]
	Learning Rate: 0.0045231
	LOSS [training: 0.20260185102947936 | validation: 0.15490237636111823]
	TIME [epoch: 8.34 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21319065703601972		[learning rate: 0.0045178]
		[batch 20/20] avg loss: 0.2511140705834915		[learning rate: 0.0045124]
	Learning Rate: 0.00451243
	LOSS [training: 0.23215236380975557 | validation: 0.10392175849513757]
	TIME [epoch: 8.33 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20005139667011726		[learning rate: 0.0045071]
		[batch 20/20] avg loss: 0.18888553528372923		[learning rate: 0.0045018]
	Learning Rate: 0.00450178
	LOSS [training: 0.19446846597692324 | validation: 0.10933029064751015]
	TIME [epoch: 8.32 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3944247302342746		[learning rate: 0.0044965]
		[batch 20/20] avg loss: 0.4904780504240926		[learning rate: 0.0044912]
	Learning Rate: 0.00449116
	LOSS [training: 0.4424513903291837 | validation: 0.08778756950603009]
	TIME [epoch: 8.34 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20119640588172963		[learning rate: 0.0044859]
		[batch 20/20] avg loss: 0.1768853022143187		[learning rate: 0.0044806]
	Learning Rate: 0.00448057
	LOSS [training: 0.18904085404802415 | validation: 0.14431530624090821]
	TIME [epoch: 8.36 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2617613489463991		[learning rate: 0.0044753]
		[batch 20/20] avg loss: 0.19347364875893325		[learning rate: 0.00447]
	Learning Rate: 0.00447
	LOSS [training: 0.2276174988526661 | validation: 0.40388478714551024]
	TIME [epoch: 8.32 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22922602689017837		[learning rate: 0.0044647]
		[batch 20/20] avg loss: 0.1643940057984111		[learning rate: 0.0044595]
	Learning Rate: 0.00445946
	LOSS [training: 0.19681001634429474 | validation: 0.22318350366348186]
	TIME [epoch: 8.32 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18103320362994962		[learning rate: 0.0044542]
		[batch 20/20] avg loss: 0.18927634401279456		[learning rate: 0.0044489]
	Learning Rate: 0.00444894
	LOSS [training: 0.18515477382137208 | validation: 0.1390973218971638]
	TIME [epoch: 8.33 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14357011022020322		[learning rate: 0.0044437]
		[batch 20/20] avg loss: 0.1485726942975638		[learning rate: 0.0044384]
	Learning Rate: 0.00443844
	LOSS [training: 0.14607140225888351 | validation: 0.18802039490825612]
	TIME [epoch: 8.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24588993204960768		[learning rate: 0.0044332]
		[batch 20/20] avg loss: 0.2300172063801753		[learning rate: 0.004428]
	Learning Rate: 0.00442797
	LOSS [training: 0.23795356921489152 | validation: 0.30554036508355475]
	TIME [epoch: 8.33 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1936373720807362		[learning rate: 0.0044227]
		[batch 20/20] avg loss: 0.20183587625759425		[learning rate: 0.0044175]
	Learning Rate: 0.00441753
	LOSS [training: 0.1977366241691652 | validation: 0.12481042531830465]
	TIME [epoch: 8.33 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24305684008265188		[learning rate: 0.0044123]
		[batch 20/20] avg loss: 0.2627776885968747		[learning rate: 0.0044071]
	Learning Rate: 0.00440711
	LOSS [training: 0.2529172643397633 | validation: 0.14334397629494838]
	TIME [epoch: 8.34 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2012076302582912		[learning rate: 0.0044019]
		[batch 20/20] avg loss: 0.21015809023401233		[learning rate: 0.0043967]
	Learning Rate: 0.00439671
	LOSS [training: 0.2056828602461518 | validation: 0.04492314104772489]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22332620883165366		[learning rate: 0.0043915]
		[batch 20/20] avg loss: 0.16522032249565138		[learning rate: 0.0043863]
	Learning Rate: 0.00438634
	LOSS [training: 0.1942732656636525 | validation: 0.15070293087069775]
	TIME [epoch: 8.33 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13801616146242926		[learning rate: 0.0043812]
		[batch 20/20] avg loss: 0.3093425599888871		[learning rate: 0.004376]
	Learning Rate: 0.004376
	LOSS [training: 0.22367936072565814 | validation: 0.15204156208923614]
	TIME [epoch: 8.33 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23146929738794236		[learning rate: 0.0043708]
		[batch 20/20] avg loss: 0.22659420964397597		[learning rate: 0.0043657]
	Learning Rate: 0.00436567
	LOSS [training: 0.22903175351595917 | validation: 0.14768962567873894]
	TIME [epoch: 8.35 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24972454750610149		[learning rate: 0.0043605]
		[batch 20/20] avg loss: 0.15890321259802628		[learning rate: 0.0043554]
	Learning Rate: 0.00435538
	LOSS [training: 0.2043138800520639 | validation: 0.14370713193664264]
	TIME [epoch: 8.35 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21574329797499847		[learning rate: 0.0043502]
		[batch 20/20] avg loss: 0.1613289808067026		[learning rate: 0.0043451]
	Learning Rate: 0.0043451
	LOSS [training: 0.18853613939085054 | validation: 0.12772571241004047]
	TIME [epoch: 8.33 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14810653159314152		[learning rate: 0.00434]
		[batch 20/20] avg loss: 0.17351605337963366		[learning rate: 0.0043349]
	Learning Rate: 0.00433485
	LOSS [training: 0.16081129248638765 | validation: 0.4277390568737622]
	TIME [epoch: 8.33 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19545035739388608		[learning rate: 0.0043297]
		[batch 20/20] avg loss: 0.17431465080897038		[learning rate: 0.0043246]
	Learning Rate: 0.00432463
	LOSS [training: 0.1848825041014282 | validation: 0.11428418193358086]
	TIME [epoch: 8.35 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17370195562170343		[learning rate: 0.0043195]
		[batch 20/20] avg loss: 0.20490886235108968		[learning rate: 0.0043144]
	Learning Rate: 0.00431443
	LOSS [training: 0.18930540898639658 | validation: 0.22343255052671457]
	TIME [epoch: 8.34 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1835242721032912		[learning rate: 0.0043093]
		[batch 20/20] avg loss: 0.1931210559848534		[learning rate: 0.0043042]
	Learning Rate: 0.00430425
	LOSS [training: 0.1883226640440723 | validation: 0.08530060676343135]
	TIME [epoch: 8.33 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22234248095558096		[learning rate: 0.0042992]
		[batch 20/20] avg loss: 0.18880266131261708		[learning rate: 0.0042941]
	Learning Rate: 0.0042941
	LOSS [training: 0.20557257113409907 | validation: 0.15688975504272992]
	TIME [epoch: 8.33 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.264804274136465		[learning rate: 0.004289]
		[batch 20/20] avg loss: 0.13918307016346418		[learning rate: 0.004284]
	Learning Rate: 0.00428397
	LOSS [training: 0.20199367214996458 | validation: 0.3107309413546157]
	TIME [epoch: 8.35 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16658843293625614		[learning rate: 0.0042789]
		[batch 20/20] avg loss: 0.24113064799351047		[learning rate: 0.0042739]
	Learning Rate: 0.00427386
	LOSS [training: 0.2038595404648833 | validation: 0.09716834340972713]
	TIME [epoch: 8.34 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13680813401515182		[learning rate: 0.0042688]
		[batch 20/20] avg loss: 0.28320007758119836		[learning rate: 0.0042638]
	Learning Rate: 0.00426378
	LOSS [training: 0.21000410579817505 | validation: 0.11921927337459201]
	TIME [epoch: 8.34 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18277270235950802		[learning rate: 0.0042587]
		[batch 20/20] avg loss: 0.2028791625588891		[learning rate: 0.0042537]
	Learning Rate: 0.00425372
	LOSS [training: 0.1928259324591986 | validation: 0.1637037524141256]
	TIME [epoch: 8.33 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15952363317125243		[learning rate: 0.0042487]
		[batch 20/20] avg loss: 0.17286577720245375		[learning rate: 0.0042437]
	Learning Rate: 0.00424369
	LOSS [training: 0.16619470518685306 | validation: 0.27840396472503004]
	TIME [epoch: 8.35 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32194933217372074		[learning rate: 0.0042387]
		[batch 20/20] avg loss: 0.18489600201623008		[learning rate: 0.0042337]
	Learning Rate: 0.00423368
	LOSS [training: 0.2534226670949754 | validation: 0.0922249814673774]
	TIME [epoch: 8.35 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28227521800058286		[learning rate: 0.0042287]
		[batch 20/20] avg loss: 0.23525374199512056		[learning rate: 0.0042237]
	Learning Rate: 0.00422369
	LOSS [training: 0.25876447999785174 | validation: 0.2048535509779097]
	TIME [epoch: 8.33 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17688577763919727		[learning rate: 0.0042187]
		[batch 20/20] avg loss: 0.18401751901641417		[learning rate: 0.0042137]
	Learning Rate: 0.00421373
	LOSS [training: 0.1804516483278057 | validation: 0.0971748844519504]
	TIME [epoch: 8.33 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17259100908316755		[learning rate: 0.0042088]
		[batch 20/20] avg loss: 0.21118129066587193		[learning rate: 0.0042038]
	Learning Rate: 0.00420379
	LOSS [training: 0.1918861498745197 | validation: 0.07862749831030344]
	TIME [epoch: 8.35 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1997157337408266		[learning rate: 0.0041988]
		[batch 20/20] avg loss: 0.1799267699632277		[learning rate: 0.0041939]
	Learning Rate: 0.00419387
	LOSS [training: 0.1898212518520271 | validation: 0.31969680742292933]
	TIME [epoch: 8.35 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21032988517852025		[learning rate: 0.0041889]
		[batch 20/20] avg loss: 0.2940854366844781		[learning rate: 0.004184]
	Learning Rate: 0.00418398
	LOSS [training: 0.2522076609314992 | validation: 0.18924216922240936]
	TIME [epoch: 8.33 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1649376601670796		[learning rate: 0.004179]
		[batch 20/20] avg loss: 0.19702983262953708		[learning rate: 0.0041741]
	Learning Rate: 0.00417411
	LOSS [training: 0.18098374639830833 | validation: 0.3115152896275772]
	TIME [epoch: 8.34 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16866461584839923		[learning rate: 0.0041692]
		[batch 20/20] avg loss: 0.1686565571965985		[learning rate: 0.0041643]
	Learning Rate: 0.00416427
	LOSS [training: 0.16866058652249885 | validation: 0.2631725890957567]
	TIME [epoch: 8.35 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15810406308096517		[learning rate: 0.0041594]
		[batch 20/20] avg loss: 0.18794591421740248		[learning rate: 0.0041544]
	Learning Rate: 0.00415444
	LOSS [training: 0.1730249886491838 | validation: 0.09928530617229646]
	TIME [epoch: 8.35 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1398328249716434		[learning rate: 0.0041495]
		[batch 20/20] avg loss: 0.20186199985226322		[learning rate: 0.0041446]
	Learning Rate: 0.00414464
	LOSS [training: 0.1708474124119533 | validation: 0.08051296620624585]
	TIME [epoch: 8.33 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15010708994360938		[learning rate: 0.0041398]
		[batch 20/20] avg loss: 0.16416345058775364		[learning rate: 0.0041349]
	Learning Rate: 0.00413487
	LOSS [training: 0.1571352702656815 | validation: 0.0867148115029843]
	TIME [epoch: 8.33 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2656317146082426		[learning rate: 0.00413]
		[batch 20/20] avg loss: 0.21547324630687625		[learning rate: 0.0041251]
	Learning Rate: 0.00412511
	LOSS [training: 0.24055248045755945 | validation: 0.060277810125855896]
	TIME [epoch: 8.35 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17922583787235213		[learning rate: 0.0041202]
		[batch 20/20] avg loss: 0.11206082377159114		[learning rate: 0.0041154]
	Learning Rate: 0.00411538
	LOSS [training: 0.14564333082197164 | validation: 0.4823968469702725]
	TIME [epoch: 8.35 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21523694397430998		[learning rate: 0.0041105]
		[batch 20/20] avg loss: 0.125758000025865		[learning rate: 0.0041057]
	Learning Rate: 0.00410568
	LOSS [training: 0.17049747200008752 | validation: 0.19489772138812472]
	TIME [epoch: 8.33 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15460591681454725		[learning rate: 0.0041008]
		[batch 20/20] avg loss: 0.24741628161533008		[learning rate: 0.004096]
	Learning Rate: 0.00409599
	LOSS [training: 0.20101109921493862 | validation: 0.10066195392680483]
	TIME [epoch: 8.34 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1842611868553156		[learning rate: 0.0040912]
		[batch 20/20] avg loss: 0.16031520628813117		[learning rate: 0.0040863]
	Learning Rate: 0.00408633
	LOSS [training: 0.17228819657172337 | validation: 0.09447499646626885]
	TIME [epoch: 8.35 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2382500671227164		[learning rate: 0.0040815]
		[batch 20/20] avg loss: 0.15762245144711412		[learning rate: 0.0040767]
	Learning Rate: 0.00407669
	LOSS [training: 0.19793625928491526 | validation: 0.09030840846174483]
	TIME [epoch: 8.35 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12287575954367724		[learning rate: 0.0040719]
		[batch 20/20] avg loss: 0.17716950186687175		[learning rate: 0.0040671]
	Learning Rate: 0.00406707
	LOSS [training: 0.1500226307052745 | validation: 0.2411414937422717]
	TIME [epoch: 8.33 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2060120199908349		[learning rate: 0.0040623]
		[batch 20/20] avg loss: 0.1600812360723079		[learning rate: 0.0040575]
	Learning Rate: 0.00405748
	LOSS [training: 0.18304662803157137 | validation: 0.11598588234086467]
	TIME [epoch: 8.34 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19795203283886154		[learning rate: 0.0040527]
		[batch 20/20] avg loss: 0.1578439911712067		[learning rate: 0.0040479]
	Learning Rate: 0.00404791
	LOSS [training: 0.17789801200503413 | validation: 0.14034809879905596]
	TIME [epoch: 8.35 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15976854802042256		[learning rate: 0.0040431]
		[batch 20/20] avg loss: 0.23203749155008854		[learning rate: 0.0040384]
	Learning Rate: 0.00403836
	LOSS [training: 0.19590301978525557 | validation: 0.07802851106454044]
	TIME [epoch: 8.35 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1303994340422901		[learning rate: 0.0040336]
		[batch 20/20] avg loss: 0.20502206907574294		[learning rate: 0.0040288]
	Learning Rate: 0.00402883
	LOSS [training: 0.16771075155901655 | validation: 0.26047812911803636]
	TIME [epoch: 8.33 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16311502200790146		[learning rate: 0.0040241]
		[batch 20/20] avg loss: 0.2507515245413203		[learning rate: 0.0040193]
	Learning Rate: 0.00401933
	LOSS [training: 0.20693327327461083 | validation: 0.08388221774816565]
	TIME [epoch: 8.34 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19738226318490065		[learning rate: 0.0040146]
		[batch 20/20] avg loss: 0.13608460327310407		[learning rate: 0.0040099]
	Learning Rate: 0.00400985
	LOSS [training: 0.1667334332290024 | validation: 0.0751033745885453]
	TIME [epoch: 8.35 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17868113629698926		[learning rate: 0.0040051]
		[batch 20/20] avg loss: 0.17962543584706953		[learning rate: 0.0040004]
	Learning Rate: 0.00400039
	LOSS [training: 0.1791532860720294 | validation: 0.16772018990341975]
	TIME [epoch: 8.33 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1680548776690065		[learning rate: 0.0039957]
		[batch 20/20] avg loss: 0.19147095284689444		[learning rate: 0.003991]
	Learning Rate: 0.00399096
	LOSS [training: 0.17976291525795046 | validation: 0.11139459094987125]
	TIME [epoch: 8.33 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17154976020261029		[learning rate: 0.0039862]
		[batch 20/20] avg loss: 0.16783322863813369		[learning rate: 0.0039815]
	Learning Rate: 0.00398154
	LOSS [training: 0.16969149442037199 | validation: 0.1192526766454508]
	TIME [epoch: 8.33 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18686450652971884		[learning rate: 0.0039768]
		[batch 20/20] avg loss: 0.1901906310756991		[learning rate: 0.0039721]
	Learning Rate: 0.00397215
	LOSS [training: 0.18852756880270893 | validation: 0.07473137792860106]
	TIME [epoch: 8.35 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.164758024898014		[learning rate: 0.0039675]
		[batch 20/20] avg loss: 0.14980031291375143		[learning rate: 0.0039628]
	Learning Rate: 0.00396278
	LOSS [training: 0.1572791689058827 | validation: 0.08829984110887514]
	TIME [epoch: 8.34 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31541764268839884		[learning rate: 0.0039581]
		[batch 20/20] avg loss: 0.12276180526360496		[learning rate: 0.0039534]
	Learning Rate: 0.00395343
	LOSS [training: 0.21908972397600196 | validation: 0.053545858116373676]
	TIME [epoch: 8.33 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1558252390306182		[learning rate: 0.0039488]
		[batch 20/20] avg loss: 0.1328250228690199		[learning rate: 0.0039441]
	Learning Rate: 0.00394411
	LOSS [training: 0.14432513094981908 | validation: 0.38173321365403956]
	TIME [epoch: 8.33 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1937214644479841		[learning rate: 0.0039395]
		[batch 20/20] avg loss: 0.1587794300367596		[learning rate: 0.0039348]
	Learning Rate: 0.0039348
	LOSS [training: 0.17625044724237188 | validation: 0.044154625593014064]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_445.pth
	Model improved!!!
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14832636621696088		[learning rate: 0.0039302]
		[batch 20/20] avg loss: 0.13185364031931662		[learning rate: 0.0039255]
	Learning Rate: 0.00392552
	LOSS [training: 0.14009000326813875 | validation: 0.07958839457380965]
	TIME [epoch: 8.34 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23598415713078413		[learning rate: 0.0039209]
		[batch 20/20] avg loss: 0.19713850409151862		[learning rate: 0.0039163]
	Learning Rate: 0.00391626
	LOSS [training: 0.21656133061115135 | validation: 0.1458637871805447]
	TIME [epoch: 8.33 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25906604414684875		[learning rate: 0.0039116]
		[batch 20/20] avg loss: 0.15314574326196512		[learning rate: 0.003907]
	Learning Rate: 0.00390702
	LOSS [training: 0.20610589370440696 | validation: 0.19401963659301844]
	TIME [epoch: 8.33 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12796940155946937		[learning rate: 0.0039024]
		[batch 20/20] avg loss: 0.16156655922819632		[learning rate: 0.0038978]
	Learning Rate: 0.00389781
	LOSS [training: 0.14476798039383282 | validation: 0.2594241010484103]
	TIME [epoch: 8.35 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18066877101075376		[learning rate: 0.0038932]
		[batch 20/20] avg loss: 0.18765295025263365		[learning rate: 0.0038886]
	Learning Rate: 0.00388861
	LOSS [training: 0.1841608606316937 | validation: 0.06534216985924962]
	TIME [epoch: 8.34 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.130622251523623		[learning rate: 0.003884]
		[batch 20/20] avg loss: 0.2168938188923561		[learning rate: 0.0038794]
	Learning Rate: 0.00387944
	LOSS [training: 0.17375803520798955 | validation: 0.09403696846430734]
	TIME [epoch: 8.33 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15872717047478946		[learning rate: 0.0038749]
		[batch 20/20] avg loss: 0.22827097261790294		[learning rate: 0.0038703]
	Learning Rate: 0.00387029
	LOSS [training: 0.1934990715463462 | validation: 0.1623308461667179]
	TIME [epoch: 8.33 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16317382926594964		[learning rate: 0.0038657]
		[batch 20/20] avg loss: 0.1254633731426384		[learning rate: 0.0038612]
	Learning Rate: 0.00386116
	LOSS [training: 0.144318601204294 | validation: 0.11281011744067843]
	TIME [epoch: 8.36 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18628713444688996		[learning rate: 0.0038566]
		[batch 20/20] avg loss: 0.1852734446432392		[learning rate: 0.0038521]
	Learning Rate: 0.00385205
	LOSS [training: 0.1857802895450646 | validation: 0.07864749100773616]
	TIME [epoch: 8.33 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18232924908961273		[learning rate: 0.0038475]
		[batch 20/20] avg loss: 0.16210425364123512		[learning rate: 0.003843]
	Learning Rate: 0.00384297
	LOSS [training: 0.17221675136542394 | validation: 0.06804177653505702]
	TIME [epoch: 8.33 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16502126729083394		[learning rate: 0.0038384]
		[batch 20/20] avg loss: 0.14545283740251153		[learning rate: 0.0038339]
	Learning Rate: 0.0038339
	LOSS [training: 0.1552370523466727 | validation: 0.08367733387373086]
	TIME [epoch: 8.34 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1812878841064653		[learning rate: 0.0038294]
		[batch 20/20] avg loss: 0.16944954172610513		[learning rate: 0.0038249]
	Learning Rate: 0.00382486
	LOSS [training: 0.17536871291628522 | validation: 0.09649367358948177]
	TIME [epoch: 8.36 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14107297511273698		[learning rate: 0.0038203]
		[batch 20/20] avg loss: 0.18870579439809143		[learning rate: 0.0038158]
	Learning Rate: 0.00381584
	LOSS [training: 0.1648893847554142 | validation: 0.11047050239336724]
	TIME [epoch: 8.33 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17780692291028463		[learning rate: 0.0038113]
		[batch 20/20] avg loss: 0.12136727503052414		[learning rate: 0.0038068]
	Learning Rate: 0.00380684
	LOSS [training: 0.14958709897040437 | validation: 0.09417728835695527]
	TIME [epoch: 8.33 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15799345926678446		[learning rate: 0.0038023]
		[batch 20/20] avg loss: 0.1279538490775344		[learning rate: 0.0037979]
	Learning Rate: 0.00379786
	LOSS [training: 0.14297365417215943 | validation: 0.04342828785628257]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_460.pth
	Model improved!!!
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14679176460815918		[learning rate: 0.0037934]
		[batch 20/20] avg loss: 0.13312330413878543		[learning rate: 0.0037889]
	Learning Rate: 0.0037889
	LOSS [training: 0.13995753437347228 | validation: 0.06437256586199583]
	TIME [epoch: 8.36 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1698572844516088		[learning rate: 0.0037844]
		[batch 20/20] avg loss: 0.19827239602989827		[learning rate: 0.00378]
	Learning Rate: 0.00377996
	LOSS [training: 0.18406484024075354 | validation: 0.06020115859524652]
	TIME [epoch: 8.33 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10302784625556964		[learning rate: 0.0037755]
		[batch 20/20] avg loss: 0.1671616619859933		[learning rate: 0.003771]
	Learning Rate: 0.00377104
	LOSS [training: 0.13509475412078145 | validation: 0.7503573012087178]
	TIME [epoch: 8.33 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24601234610219005		[learning rate: 0.0037666]
		[batch 20/20] avg loss: 0.18472058302839353		[learning rate: 0.0037621]
	Learning Rate: 0.00376215
	LOSS [training: 0.2153664645652918 | validation: 0.2719614593954462]
	TIME [epoch: 8.33 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13700620279777126		[learning rate: 0.0037577]
		[batch 20/20] avg loss: 0.16918537733870379		[learning rate: 0.0037533]
	Learning Rate: 0.00375327
	LOSS [training: 0.15309579006823748 | validation: 0.1435081081718681]
	TIME [epoch: 8.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1397720104695343		[learning rate: 0.0037488]
		[batch 20/20] avg loss: 0.146367509327841		[learning rate: 0.0037444]
	Learning Rate: 0.00374442
	LOSS [training: 0.14306975989868767 | validation: 0.36447256691060415]
	TIME [epoch: 8.33 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14512025933122508		[learning rate: 0.00374]
		[batch 20/20] avg loss: 0.13258329878918074		[learning rate: 0.0037356]
	Learning Rate: 0.00373559
	LOSS [training: 0.1388517790602029 | validation: 0.3229992518995956]
	TIME [epoch: 8.33 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19485885995777447		[learning rate: 0.0037312]
		[batch 20/20] avg loss: 0.17325982697886783		[learning rate: 0.0037268]
	Learning Rate: 0.00372678
	LOSS [training: 0.18405934346832117 | validation: 0.23922244283471766]
	TIME [epoch: 8.33 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19473075910888116		[learning rate: 0.0037224]
		[batch 20/20] avg loss: 0.16848618167941432		[learning rate: 0.003718]
	Learning Rate: 0.00371799
	LOSS [training: 0.18160847039414776 | validation: 0.16563271544026612]
	TIME [epoch: 8.35 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20710725134948701		[learning rate: 0.0037136]
		[batch 20/20] avg loss: 0.1560877339945747		[learning rate: 0.0037092]
	Learning Rate: 0.00370922
	LOSS [training: 0.18159749267203087 | validation: 0.11846483042265038]
	TIME [epoch: 8.33 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20617154395910794		[learning rate: 0.0037048]
		[batch 20/20] avg loss: 0.1486328872075323		[learning rate: 0.0037005]
	Learning Rate: 0.00370047
	LOSS [training: 0.1774022155833201 | validation: 0.06248679004334561]
	TIME [epoch: 8.33 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18155668155788945		[learning rate: 0.0036961]
		[batch 20/20] avg loss: 0.2118886455806323		[learning rate: 0.0036917]
	Learning Rate: 0.00369174
	LOSS [training: 0.19672266356926088 | validation: 0.06821956191439914]
	TIME [epoch: 8.33 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1535676015766069		[learning rate: 0.0036874]
		[batch 20/20] avg loss: 0.14833140873846412		[learning rate: 0.003683]
	Learning Rate: 0.00368303
	LOSS [training: 0.15094950515753547 | validation: 0.2512236713053039]
	TIME [epoch: 8.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20210569726462704		[learning rate: 0.0036787]
		[batch 20/20] avg loss: 0.27215340467834764		[learning rate: 0.0036743]
	Learning Rate: 0.00367434
	LOSS [training: 0.23712955097148733 | validation: 0.1368090161965097]
	TIME [epoch: 8.33 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14723832448870533		[learning rate: 0.00367]
		[batch 20/20] avg loss: 0.16288807267152552		[learning rate: 0.0036657]
	Learning Rate: 0.00366567
	LOSS [training: 0.15506319858011539 | validation: 0.21696544581288724]
	TIME [epoch: 8.32 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13779388101607454		[learning rate: 0.0036613]
		[batch 20/20] avg loss: 0.17217888391253677		[learning rate: 0.003657]
	Learning Rate: 0.00365703
	LOSS [training: 0.15498638246430566 | validation: 0.20064944339577676]
	TIME [epoch: 8.33 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14359384640075518		[learning rate: 0.0036527]
		[batch 20/20] avg loss: 0.1767529672528257		[learning rate: 0.0036484]
	Learning Rate: 0.0036484
	LOSS [training: 0.16017340682679043 | validation: 0.1363121066887313]
	TIME [epoch: 8.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18509251268265042		[learning rate: 0.0036441]
		[batch 20/20] avg loss: 0.1891189393994513		[learning rate: 0.0036398]
	Learning Rate: 0.00363979
	LOSS [training: 0.18710572604105083 | validation: 0.09344894521880619]
	TIME [epoch: 8.34 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16228603963391328		[learning rate: 0.0036355]
		[batch 20/20] avg loss: 0.19699556207753463		[learning rate: 0.0036312]
	Learning Rate: 0.00363121
	LOSS [training: 0.17964080085572393 | validation: 0.161656735875995]
	TIME [epoch: 8.33 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1532794169636985		[learning rate: 0.0036269]
		[batch 20/20] avg loss: 0.15867048779855658		[learning rate: 0.0036226]
	Learning Rate: 0.00362264
	LOSS [training: 0.1559749523811276 | validation: 0.06750618364170909]
	TIME [epoch: 8.33 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1212635467444052		[learning rate: 0.0036184]
		[batch 20/20] avg loss: 0.1921335764191504		[learning rate: 0.0036141]
	Learning Rate: 0.0036141
	LOSS [training: 0.1566985615817778 | validation: 0.13087475638964724]
	TIME [epoch: 8.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1901947459233925		[learning rate: 0.0036098]
		[batch 20/20] avg loss: 0.16202141384715676		[learning rate: 0.0036056]
	Learning Rate: 0.00360557
	LOSS [training: 0.17610807988527463 | validation: 0.21310211757374523]
	TIME [epoch: 8.33 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18272982338057403		[learning rate: 0.0036013]
		[batch 20/20] avg loss: 0.19231187526467727		[learning rate: 0.0035971]
	Learning Rate: 0.00359707
	LOSS [training: 0.1875208493226256 | validation: 0.10837393141530552]
	TIME [epoch: 8.33 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11352200657580513		[learning rate: 0.0035928]
		[batch 20/20] avg loss: 0.1617000719868357		[learning rate: 0.0035886]
	Learning Rate: 0.00358858
	LOSS [training: 0.13761103928132037 | validation: 0.20358393469142422]
	TIME [epoch: 8.33 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15723288282285713		[learning rate: 0.0035843]
		[batch 20/20] avg loss: 0.15675558269485337		[learning rate: 0.0035801]
	Learning Rate: 0.00358012
	LOSS [training: 0.1569942327588553 | validation: 0.07383183477889127]
	TIME [epoch: 8.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1582026325733828		[learning rate: 0.0035759]
		[batch 20/20] avg loss: 0.15512543330031098		[learning rate: 0.0035717]
	Learning Rate: 0.00357167
	LOSS [training: 0.1566640329368469 | validation: 0.10411134891417195]
	TIME [epoch: 8.33 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12069407026332457		[learning rate: 0.0035675]
		[batch 20/20] avg loss: 0.194420217665276		[learning rate: 0.0035632]
	Learning Rate: 0.00356325
	LOSS [training: 0.1575571439643003 | validation: 0.3748417772886221]
	TIME [epoch: 8.32 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23984909676380378		[learning rate: 0.003559]
		[batch 20/20] avg loss: 0.2382842939930144		[learning rate: 0.0035548]
	Learning Rate: 0.00355484
	LOSS [training: 0.23906669537840913 | validation: 0.13216757029033835]
	TIME [epoch: 8.34 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.413150935751765		[learning rate: 0.0035506]
		[batch 20/20] avg loss: 0.21738772555720504		[learning rate: 0.0035465]
	Learning Rate: 0.00354646
	LOSS [training: 0.31526933065448504 | validation: 0.103031235213043]
	TIME [epoch: 8.35 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15549673012886514		[learning rate: 0.0035423]
		[batch 20/20] avg loss: 0.14639338457615608		[learning rate: 0.0035381]
	Learning Rate: 0.00353809
	LOSS [training: 0.15094505735251057 | validation: 0.22131639063123437]
	TIME [epoch: 8.33 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17646657295000248		[learning rate: 0.0035339]
		[batch 20/20] avg loss: 0.18576628923304123		[learning rate: 0.0035297]
	Learning Rate: 0.00352975
	LOSS [training: 0.18111643109152184 | validation: 0.08873477331362707]
	TIME [epoch: 8.33 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12425867071820107		[learning rate: 0.0035256]
		[batch 20/20] avg loss: 0.17856799903652545		[learning rate: 0.0035214]
	Learning Rate: 0.00352142
	LOSS [training: 0.15141333487736325 | validation: 0.14135934653429974]
	TIME [epoch: 8.33 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15329493646669612		[learning rate: 0.0035173]
		[batch 20/20] avg loss: 0.20957643644983243		[learning rate: 0.0035131]
	Learning Rate: 0.00351311
	LOSS [training: 0.1814356864582643 | validation: 0.07447432835283066]
	TIME [epoch: 8.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1272227679089325		[learning rate: 0.003509]
		[batch 20/20] avg loss: 0.13574167668451856		[learning rate: 0.0035048]
	Learning Rate: 0.00350483
	LOSS [training: 0.13148222229672554 | validation: 0.07655796620040026]
	TIME [epoch: 8.33 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14522719853653757		[learning rate: 0.0035007]
		[batch 20/20] avg loss: 0.14934567186015085		[learning rate: 0.0034966]
	Learning Rate: 0.00349656
	LOSS [training: 0.14728643519834422 | validation: 0.48530829649089663]
	TIME [epoch: 8.32 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1960891084141179		[learning rate: 0.0034924]
		[batch 20/20] avg loss: 0.23785048688188848		[learning rate: 0.0034883]
	Learning Rate: 0.00348831
	LOSS [training: 0.21696979764800317 | validation: 0.06650860853554968]
	TIME [epoch: 8.32 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12376005361109073		[learning rate: 0.0034842]
		[batch 20/20] avg loss: 0.20586564081905062		[learning rate: 0.0034801]
	Learning Rate: 0.00348008
	LOSS [training: 0.16481284721507072 | validation: 0.2164571273640937]
	TIME [epoch: 8.34 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15565558249894784		[learning rate: 0.003476]
		[batch 20/20] avg loss: 0.20677229161907434		[learning rate: 0.0034719]
	Learning Rate: 0.00347187
	LOSS [training: 0.18121393705901112 | validation: 0.10684601464876352]
	TIME [epoch: 8.32 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17477996917093758		[learning rate: 0.0034678]
		[batch 20/20] avg loss: 0.09107989286875026		[learning rate: 0.0034637]
	Learning Rate: 0.00346369
	LOSS [training: 0.1329299310198439 | validation: 0.0664361240111247]
	TIME [epoch: 8.32 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19864870997094147		[learning rate: 0.0034596]
		[batch 20/20] avg loss: 0.15040238757413932		[learning rate: 0.0034555]
	Learning Rate: 0.00345552
	LOSS [training: 0.1745255487725404 | validation: 0.22020298819566964]
	TIME [epoch: 8.32 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1930347528982017		[learning rate: 0.0034514]
		[batch 20/20] avg loss: 0.1290343102826447		[learning rate: 0.0034474]
	Learning Rate: 0.00344736
	LOSS [training: 0.16103453159042322 | validation: 0.29856969288522056]
	TIME [epoch: 8.35 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1445233789377804		[learning rate: 0.0034433]
		[batch 20/20] avg loss: 0.25618441631328986		[learning rate: 0.0034392]
	Learning Rate: 0.00343923
	LOSS [training: 0.2003538976255351 | validation: 0.22242101200644496]
	TIME [epoch: 8.32 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23833562684514903		[learning rate: 0.0034352]
		[batch 20/20] avg loss: 0.16904188181211505		[learning rate: 0.0034311]
	Learning Rate: 0.00343112
	LOSS [training: 0.2036887543286321 | validation: 0.06054927387133155]
	TIME [epoch: 8.33 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20900129880340584		[learning rate: 0.0034271]
		[batch 20/20] avg loss: 0.22276958221899273		[learning rate: 0.003423]
	Learning Rate: 0.00342303
	LOSS [training: 0.21588544051119926 | validation: 0.2731621358377148]
	TIME [epoch: 8.34 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19919345042528175		[learning rate: 0.003419]
		[batch 20/20] avg loss: 0.16547282185047685		[learning rate: 0.003415]
	Learning Rate: 0.00341495
	LOSS [training: 0.18233313613787933 | validation: 0.0724922536240234]
	TIME [epoch: 8.37 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19381044393665306		[learning rate: 0.0034109]
		[batch 20/20] avg loss: 0.15414394856391556		[learning rate: 0.0034069]
	Learning Rate: 0.0034069
	LOSS [training: 0.17397719625028435 | validation: 0.1472812337077437]
	TIME [epoch: 8.34 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30929774058913717		[learning rate: 0.0034029]
		[batch 20/20] avg loss: 0.2544508908612265		[learning rate: 0.0033989]
	Learning Rate: 0.00339886
	LOSS [training: 0.28187431572518185 | validation: 0.27356463416252547]
	TIME [epoch: 8.33 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15904592944495077		[learning rate: 0.0033948]
		[batch 20/20] avg loss: 0.18110181612431323		[learning rate: 0.0033908]
	Learning Rate: 0.00339084
	LOSS [training: 0.17007387278463199 | validation: 0.33134600888166904]
	TIME [epoch: 8.34 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16306836872606903		[learning rate: 0.0033868]
		[batch 20/20] avg loss: 0.16869287636732777		[learning rate: 0.0033828]
	Learning Rate: 0.00338284
	LOSS [training: 0.16588062254669841 | validation: 0.08354837462953801]
	TIME [epoch: 8.36 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10752622137621452		[learning rate: 0.0033789]
		[batch 20/20] avg loss: 0.17465286067223537		[learning rate: 0.0033749]
	Learning Rate: 0.00337487
	LOSS [training: 0.14108954102422494 | validation: 0.13262568871415442]
	TIME [epoch: 8.33 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16748692651286387		[learning rate: 0.0033709]
		[batch 20/20] avg loss: 0.2213069737563688		[learning rate: 0.0033669]
	Learning Rate: 0.0033669
	LOSS [training: 0.19439695013461633 | validation: 0.07357345845923062]
	TIME [epoch: 8.34 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14955220564450683		[learning rate: 0.0033629]
		[batch 20/20] avg loss: 0.15588355309089227		[learning rate: 0.003359]
	Learning Rate: 0.00335896
	LOSS [training: 0.15271787936769954 | validation: 0.281599347752325]
	TIME [epoch: 8.33 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14376781513219594		[learning rate: 0.003355]
		[batch 20/20] avg loss: 0.17187026723479037		[learning rate: 0.003351]
	Learning Rate: 0.00335104
	LOSS [training: 0.15781904118349319 | validation: 0.22676270037257626]
	TIME [epoch: 8.36 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19466468910730042		[learning rate: 0.0033471]
		[batch 20/20] avg loss: 0.16522401215808466		[learning rate: 0.0033431]
	Learning Rate: 0.00334313
	LOSS [training: 0.17994435063269257 | validation: 0.2483871577861598]
	TIME [epoch: 8.33 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13951879901409572		[learning rate: 0.0033392]
		[batch 20/20] avg loss: 0.26108242752947863		[learning rate: 0.0033352]
	Learning Rate: 0.00333525
	LOSS [training: 0.2003006132717872 | validation: 0.12860606330898272]
	TIME [epoch: 8.34 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18621952077653553		[learning rate: 0.0033313]
		[batch 20/20] avg loss: 0.22472301696142724		[learning rate: 0.0033274]
	Learning Rate: 0.00332738
	LOSS [training: 0.2054712688689814 | validation: 0.118297116719967]
	TIME [epoch: 8.33 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23882254578197734		[learning rate: 0.0033235]
		[batch 20/20] avg loss: 0.14428290834834823		[learning rate: 0.0033195]
	Learning Rate: 0.00331953
	LOSS [training: 0.19155272706516277 | validation: 0.062403312295624055]
	TIME [epoch: 8.33 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1769000861078041		[learning rate: 0.0033156]
		[batch 20/20] avg loss: 0.17931397889838732		[learning rate: 0.0033117]
	Learning Rate: 0.0033117
	LOSS [training: 0.17810703250309573 | validation: 0.16187326349510303]
	TIME [epoch: 8.3 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15323917226781886		[learning rate: 0.0033078]
		[batch 20/20] avg loss: 0.2535565695986778		[learning rate: 0.0033039]
	Learning Rate: 0.00330389
	LOSS [training: 0.20339787093324838 | validation: 0.1182908880758593]
	TIME [epoch: 8.3 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1587184724242566		[learning rate: 0.0033]
		[batch 20/20] avg loss: 0.160066682012788		[learning rate: 0.0032961]
	Learning Rate: 0.0032961
	LOSS [training: 0.1593925772185223 | validation: 0.38935793484758047]
	TIME [epoch: 8.35 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1823176392249569		[learning rate: 0.0032922]
		[batch 20/20] avg loss: 0.16406121672195534		[learning rate: 0.0032883]
	Learning Rate: 0.00328832
	LOSS [training: 0.1731894279734561 | validation: 0.1611818539495547]
	TIME [epoch: 8.38 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22152122792555842		[learning rate: 0.0032844]
		[batch 20/20] avg loss: 0.1718033697503098		[learning rate: 0.0032806]
	Learning Rate: 0.00328057
	LOSS [training: 0.19666229883793412 | validation: 0.09740512676755414]
	TIME [epoch: 8.33 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1560701668992149		[learning rate: 0.0032767]
		[batch 20/20] avg loss: 0.1337415988679958		[learning rate: 0.0032728]
	Learning Rate: 0.00327283
	LOSS [training: 0.14490588288360534 | validation: 0.10034393567145566]
	TIME [epoch: 8.34 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08843741904557614		[learning rate: 0.003269]
		[batch 20/20] avg loss: 0.1603487033893055		[learning rate: 0.0032651]
	Learning Rate: 0.00326511
	LOSS [training: 0.12439306121744083 | validation: 0.03076062504242772]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_524.pth
	Model improved!!!
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1421260103402815		[learning rate: 0.0032613]
		[batch 20/20] avg loss: 0.10687366623489107		[learning rate: 0.0032574]
	Learning Rate: 0.00325741
	LOSS [training: 0.12449983828758626 | validation: 0.17900489423322372]
	TIME [epoch: 8.33 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15029627815256827		[learning rate: 0.0032536]
		[batch 20/20] avg loss: 0.140374967061013		[learning rate: 0.0032497]
	Learning Rate: 0.00324972
	LOSS [training: 0.14533562260679064 | validation: 0.18224240716327567]
	TIME [epoch: 8.33 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14089266473868284		[learning rate: 0.0032459]
		[batch 20/20] avg loss: 0.14990976552043966		[learning rate: 0.0032421]
	Learning Rate: 0.00324206
	LOSS [training: 0.14540121512956122 | validation: 0.24900070228898558]
	TIME [epoch: 8.32 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14083550985256132		[learning rate: 0.0032382]
		[batch 20/20] avg loss: 0.16890253841651542		[learning rate: 0.0032344]
	Learning Rate: 0.00323441
	LOSS [training: 0.15486902413453837 | validation: 0.12379062475701641]
	TIME [epoch: 8.31 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17521540590520446		[learning rate: 0.0032306]
		[batch 20/20] avg loss: 0.18866518296761692		[learning rate: 0.0032268]
	Learning Rate: 0.00322678
	LOSS [training: 0.18194029443641066 | validation: 0.07794224695516348]
	TIME [epoch: 8.34 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14265423270794683		[learning rate: 0.003223]
		[batch 20/20] avg loss: 0.18893419313073503		[learning rate: 0.0032192]
	Learning Rate: 0.00321917
	LOSS [training: 0.16579421291934096 | validation: 0.07827622756216362]
	TIME [epoch: 8.31 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17706260053738493		[learning rate: 0.0032154]
		[batch 20/20] avg loss: 0.1493458901801323		[learning rate: 0.0032116]
	Learning Rate: 0.00321157
	LOSS [training: 0.1632042453587586 | validation: 0.07977881105735773]
	TIME [epoch: 8.32 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17516472929448007		[learning rate: 0.0032078]
		[batch 20/20] avg loss: 0.19001388162286964		[learning rate: 0.003204]
	Learning Rate: 0.003204
	LOSS [training: 0.18258930545867486 | validation: 0.127847520418848]
	TIME [epoch: 8.31 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14268601522619745		[learning rate: 0.0032002]
		[batch 20/20] avg loss: 0.2854930305844949		[learning rate: 0.0031964]
	Learning Rate: 0.00319644
	LOSS [training: 0.21408952290534616 | validation: 0.7518735795076299]
	TIME [epoch: 8.34 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23770808880571845		[learning rate: 0.0031927]
		[batch 20/20] avg loss: 0.14765669412652502		[learning rate: 0.0031889]
	Learning Rate: 0.0031889
	LOSS [training: 0.1926823914661217 | validation: 0.1628721861999774]
	TIME [epoch: 8.31 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11590087263145976		[learning rate: 0.0031851]
		[batch 20/20] avg loss: 0.1719937002270323		[learning rate: 0.0031814]
	Learning Rate: 0.00318138
	LOSS [training: 0.14394728642924604 | validation: 0.08831392374271584]
	TIME [epoch: 8.32 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20766020727290155		[learning rate: 0.0031776]
		[batch 20/20] avg loss: 0.14030552871425642		[learning rate: 0.0031739]
	Learning Rate: 0.00317387
	LOSS [training: 0.17398286799357898 | validation: 0.19675719522842616]
	TIME [epoch: 8.33 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17933283961922306		[learning rate: 0.0031701]
		[batch 20/20] avg loss: 0.12294356996338081		[learning rate: 0.0031664]
	Learning Rate: 0.00316639
	LOSS [training: 0.15113820479130197 | validation: 0.12684068843151458]
	TIME [epoch: 8.34 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15413835831749143		[learning rate: 0.0031627]
		[batch 20/20] avg loss: 0.12524387498983644		[learning rate: 0.0031589]
	Learning Rate: 0.00315892
	LOSS [training: 0.1396911166536639 | validation: 0.15697302203426733]
	TIME [epoch: 8.32 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10796948705162454		[learning rate: 0.0031552]
		[batch 20/20] avg loss: 0.1497175738082295		[learning rate: 0.0031515]
	Learning Rate: 0.00315147
	LOSS [training: 0.12884353042992702 | validation: 0.29023728338158467]
	TIME [epoch: 8.32 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17456371661963885		[learning rate: 0.0031477]
		[batch 20/20] avg loss: 0.14614959902174335		[learning rate: 0.003144]
	Learning Rate: 0.00314403
	LOSS [training: 0.16035665782069108 | validation: 0.06478407841057122]
	TIME [epoch: 8.31 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18111872213835298		[learning rate: 0.0031403]
		[batch 20/20] avg loss: 0.2314414626090857		[learning rate: 0.0031366]
	Learning Rate: 0.00313662
	LOSS [training: 0.2062800923737193 | validation: 0.07490795858440265]
	TIME [epoch: 8.35 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20706336994939595		[learning rate: 0.0031329]
		[batch 20/20] avg loss: 0.14040187550979022		[learning rate: 0.0031292]
	Learning Rate: 0.00312922
	LOSS [training: 0.17373262272959308 | validation: 0.1254411898663638]
	TIME [epoch: 8.31 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20549377471532		[learning rate: 0.0031255]
		[batch 20/20] avg loss: 0.14684759432212968		[learning rate: 0.0031218]
	Learning Rate: 0.00312184
	LOSS [training: 0.17617068451872486 | validation: 0.08038477206601703]
	TIME [epoch: 8.32 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17995409082705158		[learning rate: 0.0031182]
		[batch 20/20] avg loss: 0.12860614012238542		[learning rate: 0.0031145]
	Learning Rate: 0.00311447
	LOSS [training: 0.15428011547471848 | validation: 0.08751242953541206]
	TIME [epoch: 8.31 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1371245706819087		[learning rate: 0.0031108]
		[batch 20/20] avg loss: 0.14664880593724627		[learning rate: 0.0031071]
	Learning Rate: 0.00310713
	LOSS [training: 0.14188668830957749 | validation: 0.1819609673755283]
	TIME [epoch: 8.35 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21949726375672102		[learning rate: 0.0031035]
		[batch 20/20] avg loss: 0.20652260376649734		[learning rate: 0.0030998]
	Learning Rate: 0.0030998
	LOSS [training: 0.2130099337616092 | validation: 0.0430708826661478]
	TIME [epoch: 8.31 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20246734571145047		[learning rate: 0.0030961]
		[batch 20/20] avg loss: 0.17417690207831543		[learning rate: 0.0030925]
	Learning Rate: 0.00309249
	LOSS [training: 0.18832212389488295 | validation: 0.24873170933942657]
	TIME [epoch: 8.32 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15875603786140396		[learning rate: 0.0030888]
		[batch 20/20] avg loss: 0.14237135097257012		[learning rate: 0.0030852]
	Learning Rate: 0.00308519
	LOSS [training: 0.15056369441698705 | validation: 0.09297847188966503]
	TIME [epoch: 8.32 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1620288364097592		[learning rate: 0.0030815]
		[batch 20/20] avg loss: 0.22855205379623236		[learning rate: 0.0030779]
	Learning Rate: 0.00307791
	LOSS [training: 0.19529044510299576 | validation: 0.08851763393937348]
	TIME [epoch: 8.34 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14722520517981025		[learning rate: 0.0030743]
		[batch 20/20] avg loss: 0.16295716789238054		[learning rate: 0.0030707]
	Learning Rate: 0.00307065
	LOSS [training: 0.15509118653609535 | validation: 0.40441121781122813]
	TIME [epoch: 8.32 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1871256214847981		[learning rate: 0.003067]
		[batch 20/20] avg loss: 0.16844866553990429		[learning rate: 0.0030634]
	Learning Rate: 0.00306341
	LOSS [training: 0.17778714351235123 | validation: 0.40708963492572403]
	TIME [epoch: 8.32 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.157913576981926		[learning rate: 0.0030598]
		[batch 20/20] avg loss: 0.1726746178195238		[learning rate: 0.0030562]
	Learning Rate: 0.00305618
	LOSS [training: 0.16529409740072493 | validation: 0.1455338318066981]
	TIME [epoch: 8.33 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19439984528734922		[learning rate: 0.0030526]
		[batch 20/20] avg loss: 0.2142379872381306		[learning rate: 0.003049]
	Learning Rate: 0.00304897
	LOSS [training: 0.20431891626273996 | validation: 0.11681873916721902]
	TIME [epoch: 8.33 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1938363308209775		[learning rate: 0.0030454]
		[batch 20/20] avg loss: 0.1787351041776971		[learning rate: 0.0030418]
	Learning Rate: 0.00304178
	LOSS [training: 0.18628571749933734 | validation: 0.057750805582536556]
	TIME [epoch: 8.32 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1654225122927385		[learning rate: 0.0030382]
		[batch 20/20] avg loss: 0.11814905589716487		[learning rate: 0.0030346]
	Learning Rate: 0.00303461
	LOSS [training: 0.14178578409495166 | validation: 0.3904320448218102]
	TIME [epoch: 8.32 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14780006507630594		[learning rate: 0.003031]
		[batch 20/20] avg loss: 0.11011816598799773		[learning rate: 0.0030274]
	Learning Rate: 0.00302745
	LOSS [training: 0.12895911553215184 | validation: 0.08856987383869666]
	TIME [epoch: 8.33 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1839264455547462		[learning rate: 0.0030239]
		[batch 20/20] avg loss: 0.17753433586147577		[learning rate: 0.0030203]
	Learning Rate: 0.00302031
	LOSS [training: 0.18073039070811106 | validation: 0.06458245195167234]
	TIME [epoch: 8.34 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13198833616551367		[learning rate: 0.0030167]
		[batch 20/20] avg loss: 0.1530790534784658		[learning rate: 0.0030132]
	Learning Rate: 0.00301318
	LOSS [training: 0.14253369482198971 | validation: 0.06913338733410256]
	TIME [epoch: 8.31 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15408732562589209		[learning rate: 0.0030096]
		[batch 20/20] avg loss: 0.2681696355460471		[learning rate: 0.0030061]
	Learning Rate: 0.00300608
	LOSS [training: 0.21112848058596959 | validation: 0.21538289044333248]
	TIME [epoch: 8.32 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14359198155415998		[learning rate: 0.0030025]
		[batch 20/20] avg loss: 0.23095133192978062		[learning rate: 0.002999]
	Learning Rate: 0.00299899
	LOSS [training: 0.1872716567419703 | validation: 0.2041810446817669]
	TIME [epoch: 8.33 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13482876997146084		[learning rate: 0.0029954]
		[batch 20/20] avg loss: 0.1570948968022256		[learning rate: 0.0029919]
	Learning Rate: 0.00299191
	LOSS [training: 0.14596183338684326 | validation: 0.18281746845532862]
	TIME [epoch: 8.34 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14056197960116748		[learning rate: 0.0029884]
		[batch 20/20] avg loss: 0.24269778375045092		[learning rate: 0.0029849]
	Learning Rate: 0.00298485
	LOSS [training: 0.19162988167580916 | validation: 0.10864359399126311]
	TIME [epoch: 8.32 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1112671622909043		[learning rate: 0.0029813]
		[batch 20/20] avg loss: 0.14100044078361357		[learning rate: 0.0029778]
	Learning Rate: 0.00297781
	LOSS [training: 0.12613380153725898 | validation: 0.45138623747664897]
	TIME [epoch: 8.31 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18427705046599505		[learning rate: 0.0029743]
		[batch 20/20] avg loss: 0.15006536305057241		[learning rate: 0.0029708]
	Learning Rate: 0.00297079
	LOSS [training: 0.16717120675828373 | validation: 0.12756807089572117]
	TIME [epoch: 8.32 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1369193083813181		[learning rate: 0.0029673]
		[batch 20/20] avg loss: 0.14662923872349165		[learning rate: 0.0029638]
	Learning Rate: 0.00296378
	LOSS [training: 0.14177427355240488 | validation: 0.08547355272659156]
	TIME [epoch: 8.34 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1286220142584822		[learning rate: 0.0029603]
		[batch 20/20] avg loss: 0.14558909225088637		[learning rate: 0.0029568]
	Learning Rate: 0.00295679
	LOSS [training: 0.13710555325468426 | validation: 0.07796426621351775]
	TIME [epoch: 8.31 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14989646523580505		[learning rate: 0.0029533]
		[batch 20/20] avg loss: 0.10136621135171078		[learning rate: 0.0029498]
	Learning Rate: 0.00294982
	LOSS [training: 0.12563133829375792 | validation: 0.14461879465700572]
	TIME [epoch: 8.31 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16907129195955853		[learning rate: 0.0029463]
		[batch 20/20] avg loss: 0.1282649829397134		[learning rate: 0.0029429]
	Learning Rate: 0.00294286
	LOSS [training: 0.14866813744963597 | validation: 0.0904412140506187]
	TIME [epoch: 8.32 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11831871487659333		[learning rate: 0.0029394]
		[batch 20/20] avg loss: 0.29061140043823047		[learning rate: 0.0029359]
	Learning Rate: 0.00293592
	LOSS [training: 0.2044650576574119 | validation: 0.47584321526662227]
	TIME [epoch: 8.34 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1844142088868274		[learning rate: 0.0029325]
		[batch 20/20] avg loss: 0.16620264826028386		[learning rate: 0.002929]
	Learning Rate: 0.00292899
	LOSS [training: 0.1753084285735556 | validation: 0.05075902450354945]
	TIME [epoch: 8.32 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19030757734913703		[learning rate: 0.0029255]
		[batch 20/20] avg loss: 0.16242244851599225		[learning rate: 0.0029221]
	Learning Rate: 0.00292208
	LOSS [training: 0.17636501293256468 | validation: 0.1939880267313373]
	TIME [epoch: 8.32 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1388057853054497		[learning rate: 0.0029186]
		[batch 20/20] avg loss: 0.10787545501146831		[learning rate: 0.0029152]
	Learning Rate: 0.00291519
	LOSS [training: 0.123340620158459 | validation: 0.40183018405842685]
	TIME [epoch: 8.32 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2755030424002036		[learning rate: 0.0029117]
		[batch 20/20] avg loss: 0.12134469124637252		[learning rate: 0.0029083]
	Learning Rate: 0.00290831
	LOSS [training: 0.1984238668232881 | validation: 0.05040977017828802]
	TIME [epoch: 8.34 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.141583782697628		[learning rate: 0.0029049]
		[batch 20/20] avg loss: 0.16881907884908373		[learning rate: 0.0029015]
	Learning Rate: 0.00290145
	LOSS [training: 0.15520143077335585 | validation: 0.1498189736331132]
	TIME [epoch: 8.31 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13692748385303047		[learning rate: 0.002898]
		[batch 20/20] avg loss: 0.13395120890767184		[learning rate: 0.0028946]
	Learning Rate: 0.00289461
	LOSS [training: 0.1354393463803512 | validation: 0.06361073587067635]
	TIME [epoch: 8.33 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1744622459046704		[learning rate: 0.0028912]
		[batch 20/20] avg loss: 0.15768624758378885		[learning rate: 0.0028878]
	Learning Rate: 0.00288778
	LOSS [training: 0.16607424674422963 | validation: 0.18181345632558166]
	TIME [epoch: 8.34 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15581548716447324		[learning rate: 0.0028844]
		[batch 20/20] avg loss: 0.12178249348500217		[learning rate: 0.002881]
	Learning Rate: 0.00288097
	LOSS [training: 0.1387989903247377 | validation: 0.08471957276786404]
	TIME [epoch: 8.33 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10697957704442684		[learning rate: 0.0028776]
		[batch 20/20] avg loss: 0.11616724727733821		[learning rate: 0.0028742]
	Learning Rate: 0.00287417
	LOSS [training: 0.11157341216088253 | validation: 0.11302208133856971]
	TIME [epoch: 8.31 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15804379679243638		[learning rate: 0.0028708]
		[batch 20/20] avg loss: 0.10813345764310237		[learning rate: 0.0028674]
	Learning Rate: 0.00286739
	LOSS [training: 0.13308862721776937 | validation: 0.0813520155522955]
	TIME [epoch: 8.32 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10472752228881803		[learning rate: 0.002864]
		[batch 20/20] avg loss: 0.12267297975834664		[learning rate: 0.0028606]
	Learning Rate: 0.00286063
	LOSS [training: 0.11370025102358232 | validation: 0.09003911622538431]
	TIME [epoch: 8.33 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11909100033987804		[learning rate: 0.0028573]
		[batch 20/20] avg loss: 0.15354455493142372		[learning rate: 0.0028539]
	Learning Rate: 0.00285388
	LOSS [training: 0.13631777763565084 | validation: 0.11835118617535939]
	TIME [epoch: 8.33 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14171857439824448		[learning rate: 0.0028505]
		[batch 20/20] avg loss: 0.1734866445672309		[learning rate: 0.0028471]
	Learning Rate: 0.00284715
	LOSS [training: 0.15760260948273766 | validation: 0.0685177983217491]
	TIME [epoch: 8.31 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09519004442761031		[learning rate: 0.0028438]
		[batch 20/20] avg loss: 0.0970600286619136		[learning rate: 0.0028404]
	Learning Rate: 0.00284043
	LOSS [training: 0.09612503654476195 | validation: 0.19122719828363802]
	TIME [epoch: 8.31 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1324543953203667		[learning rate: 0.0028371]
		[batch 20/20] avg loss: 0.17478253758727538		[learning rate: 0.0028337]
	Learning Rate: 0.00283373
	LOSS [training: 0.15361846645382102 | validation: 0.11121209683640679]
	TIME [epoch: 8.34 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0919901151850703		[learning rate: 0.0028304]
		[batch 20/20] avg loss: 0.1819268878705101		[learning rate: 0.002827]
	Learning Rate: 0.00282705
	LOSS [training: 0.13695850152779016 | validation: 0.28937541234606023]
	TIME [epoch: 8.33 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18276492978546816		[learning rate: 0.0028237]
		[batch 20/20] avg loss: 0.1508985109257262		[learning rate: 0.0028204]
	Learning Rate: 0.00282038
	LOSS [training: 0.1668317203555972 | validation: 0.09592901968428372]
	TIME [epoch: 8.32 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11602287460358177		[learning rate: 0.0028171]
		[batch 20/20] avg loss: 0.12688159150193665		[learning rate: 0.0028137]
	Learning Rate: 0.00281373
	LOSS [training: 0.12145223305275923 | validation: 0.23231000984712996]
	TIME [epoch: 8.33 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15131174002322964		[learning rate: 0.0028104]
		[batch 20/20] avg loss: 0.17673184264068528		[learning rate: 0.0028071]
	Learning Rate: 0.00280709
	LOSS [training: 0.1640217913319575 | validation: 0.19254309807308012]
	TIME [epoch: 8.32 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15013644615315264		[learning rate: 0.0028038]
		[batch 20/20] avg loss: 0.11328797200087574		[learning rate: 0.0028005]
	Learning Rate: 0.00280047
	LOSS [training: 0.13171220907701423 | validation: 0.12102543135431001]
	TIME [epoch: 8.33 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09103236320498584		[learning rate: 0.0027972]
		[batch 20/20] avg loss: 0.1535615980795161		[learning rate: 0.0027939]
	Learning Rate: 0.00279386
	LOSS [training: 0.12229698064225099 | validation: 0.06583525630245374]
	TIME [epoch: 8.32 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0939450164609679		[learning rate: 0.0027906]
		[batch 20/20] avg loss: 0.11546262473478772		[learning rate: 0.0027873]
	Learning Rate: 0.00278727
	LOSS [training: 0.10470382059787782 | validation: 0.06329720523979339]
	TIME [epoch: 8.33 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1413909526414215		[learning rate: 0.002784]
		[batch 20/20] avg loss: 0.11915839898149265		[learning rate: 0.0027807]
	Learning Rate: 0.0027807
	LOSS [training: 0.1302746758114571 | validation: 0.14938216973281376]
	TIME [epoch: 8.32 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24934858734157062		[learning rate: 0.0027774]
		[batch 20/20] avg loss: 0.09990206551933623		[learning rate: 0.0027741]
	Learning Rate: 0.00277414
	LOSS [training: 0.17462532643045342 | validation: 0.0745604183160142]
	TIME [epoch: 8.34 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1673393433153325		[learning rate: 0.0027709]
		[batch 20/20] avg loss: 0.1458152912166935		[learning rate: 0.0027676]
	Learning Rate: 0.00276759
	LOSS [training: 0.15657731726601298 | validation: 0.09608796210150908]
	TIME [epoch: 8.33 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13036311235684547		[learning rate: 0.0027643]
		[batch 20/20] avg loss: 0.11218594350261304		[learning rate: 0.0027611]
	Learning Rate: 0.00276107
	LOSS [training: 0.12127452792972929 | validation: 0.05376773192071975]
	TIME [epoch: 8.32 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10940628152933451		[learning rate: 0.0027578]
		[batch 20/20] avg loss: 0.10893644991986244		[learning rate: 0.0027546]
	Learning Rate: 0.00275455
	LOSS [training: 0.10917136572459848 | validation: 0.10149744543137078]
	TIME [epoch: 8.33 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11676687314087728		[learning rate: 0.0027513]
		[batch 20/20] avg loss: 0.12182147547111052		[learning rate: 0.0027481]
	Learning Rate: 0.00274806
	LOSS [training: 0.1192941743059939 | validation: 0.09218308065548199]
	TIME [epoch: 8.33 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1867599745430653		[learning rate: 0.0027448]
		[batch 20/20] avg loss: 0.13434552663926286		[learning rate: 0.0027416]
	Learning Rate: 0.00274157
	LOSS [training: 0.16055275059116408 | validation: 0.2918591900773049]
	TIME [epoch: 8.31 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1185730705587132		[learning rate: 0.0027383]
		[batch 20/20] avg loss: 0.0912137484667322		[learning rate: 0.0027351]
	Learning Rate: 0.00273511
	LOSS [training: 0.10489340951272268 | validation: 0.16598418854498692]
	TIME [epoch: 8.32 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17611921152300666		[learning rate: 0.0027319]
		[batch 20/20] avg loss: 0.13347867751369258		[learning rate: 0.0027287]
	Learning Rate: 0.00272866
	LOSS [training: 0.1547989445183496 | validation: 0.19192856149922266]
	TIME [epoch: 8.33 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17367729549992864		[learning rate: 0.0027254]
		[batch 20/20] avg loss: 0.11679118417405816		[learning rate: 0.0027222]
	Learning Rate: 0.00272222
	LOSS [training: 0.1452342398369934 | validation: 0.09954998196520227]
	TIME [epoch: 8.32 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15016885155060983		[learning rate: 0.002719]
		[batch 20/20] avg loss: 0.12275222334750271		[learning rate: 0.0027158]
	Learning Rate: 0.0027158
	LOSS [training: 0.13646053744905626 | validation: 0.07136665308201849]
	TIME [epoch: 8.32 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.126825342442826		[learning rate: 0.0027126]
		[batch 20/20] avg loss: 0.11688744619173215		[learning rate: 0.0027094]
	Learning Rate: 0.00270939
	LOSS [training: 0.12185639431727906 | validation: 0.16218974122308155]
	TIME [epoch: 8.31 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11571121769753698		[learning rate: 0.0027062]
		[batch 20/20] avg loss: 0.10188155852390679		[learning rate: 0.002703]
	Learning Rate: 0.002703
	LOSS [training: 0.1087963881107219 | validation: 0.15574736263174513]
	TIME [epoch: 8.34 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1230894029102418		[learning rate: 0.0026998]
		[batch 20/20] avg loss: 0.1120977107975869		[learning rate: 0.0026966]
	Learning Rate: 0.00269662
	LOSS [training: 0.11759355685391434 | validation: 0.11194808471074572]
	TIME [epoch: 8.33 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18536692447001518		[learning rate: 0.0026934]
		[batch 20/20] avg loss: 0.11796130668031382		[learning rate: 0.0026903]
	Learning Rate: 0.00269026
	LOSS [training: 0.1516641155751645 | validation: 0.26783469686296535]
	TIME [epoch: 8.32 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16262559667559143		[learning rate: 0.0026871]
		[batch 20/20] avg loss: 0.13515981577495548		[learning rate: 0.0026839]
	Learning Rate: 0.00268392
	LOSS [training: 0.14889270622527345 | validation: 0.1020304954458378]
	TIME [epoch: 8.31 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10328823586618252		[learning rate: 0.0026808]
		[batch 20/20] avg loss: 0.13012619932174083		[learning rate: 0.0026776]
	Learning Rate: 0.00267759
	LOSS [training: 0.11670721759396167 | validation: 0.10108223493837912]
	TIME [epoch: 8.33 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11581110025741399		[learning rate: 0.0026744]
		[batch 20/20] avg loss: 0.12378617613989014		[learning rate: 0.0026713]
	Learning Rate: 0.00267127
	LOSS [training: 0.11979863819865204 | validation: 0.05189964270828959]
	TIME [epoch: 8.32 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1192319299630997		[learning rate: 0.0026681]
		[batch 20/20] avg loss: 0.11851465254584599		[learning rate: 0.002665]
	Learning Rate: 0.00266497
	LOSS [training: 0.11887329125447285 | validation: 0.06266477762900995]
	TIME [epoch: 8.32 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13934095220652085		[learning rate: 0.0026618]
		[batch 20/20] avg loss: 0.10951159868947276		[learning rate: 0.0026587]
	Learning Rate: 0.00265868
	LOSS [training: 0.12442627544799678 | validation: 0.05980476246323784]
	TIME [epoch: 8.31 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13629607503561758		[learning rate: 0.0026555]
		[batch 20/20] avg loss: 0.2058235990486883		[learning rate: 0.0026524]
	Learning Rate: 0.00265241
	LOSS [training: 0.17105983704215294 | validation: 0.06290189204312181]
	TIME [epoch: 8.34 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14115607160587978		[learning rate: 0.0026493]
		[batch 20/20] avg loss: 0.15997240448329048		[learning rate: 0.0026462]
	Learning Rate: 0.00264616
	LOSS [training: 0.15056423804458513 | validation: 0.22337166750381934]
	TIME [epoch: 8.33 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13270421238060323		[learning rate: 0.002643]
		[batch 20/20] avg loss: 0.11534661042174033		[learning rate: 0.0026399]
	Learning Rate: 0.00263991
	LOSS [training: 0.12402541140117178 | validation: 0.048990290981533395]
	TIME [epoch: 8.32 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13954327794779148		[learning rate: 0.0026368]
		[batch 20/20] avg loss: 0.14575586756185038		[learning rate: 0.0026337]
	Learning Rate: 0.00263369
	LOSS [training: 0.14264957275482096 | validation: 0.07064328640286252]
	TIME [epoch: 8.32 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12374968243081463		[learning rate: 0.0026306]
		[batch 20/20] avg loss: 0.17127358439893803		[learning rate: 0.0026275]
	Learning Rate: 0.00262747
	LOSS [training: 0.14751163341487633 | validation: 0.04313052617742816]
	TIME [epoch: 8.33 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07738576906682951		[learning rate: 0.0026244]
		[batch 20/20] avg loss: 0.15225805453508678		[learning rate: 0.0026213]
	Learning Rate: 0.00262128
	LOSS [training: 0.11482191180095813 | validation: 0.14633301037050517]
	TIME [epoch: 8.32 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1283376349470177		[learning rate: 0.0026182]
		[batch 20/20] avg loss: 0.11541543753271293		[learning rate: 0.0026151]
	Learning Rate: 0.00261509
	LOSS [training: 0.12187653623986532 | validation: 0.09680359245590903]
	TIME [epoch: 8.32 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1232008075132794		[learning rate: 0.002612]
		[batch 20/20] avg loss: 0.14595881284153506		[learning rate: 0.0026089]
	Learning Rate: 0.00260892
	LOSS [training: 0.13457981017740722 | validation: 0.08214009611048625]
	TIME [epoch: 8.31 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10547802741449522		[learning rate: 0.0026058]
		[batch 20/20] avg loss: 0.12845362063473026		[learning rate: 0.0026028]
	Learning Rate: 0.00260277
	LOSS [training: 0.11696582402461275 | validation: 0.17068352390986136]
	TIME [epoch: 8.33 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16463962084712291		[learning rate: 0.0025997]
		[batch 20/20] avg loss: 0.22011923431730343		[learning rate: 0.0025966]
	Learning Rate: 0.00259663
	LOSS [training: 0.19237942758221316 | validation: 0.15365715248639641]
	TIME [epoch: 8.32 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09248734220793915		[learning rate: 0.0025936]
		[batch 20/20] avg loss: 0.10188452663894369		[learning rate: 0.0025905]
	Learning Rate: 0.00259051
	LOSS [training: 0.09718593442344144 | validation: 0.039942151528746986]
	TIME [epoch: 8.32 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12570573588347708		[learning rate: 0.0025874]
		[batch 20/20] avg loss: 0.08803311796484155		[learning rate: 0.0025844]
	Learning Rate: 0.0025844
	LOSS [training: 0.10686942692415932 | validation: 0.07854728070260353]
	TIME [epoch: 8.31 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17300414745485737		[learning rate: 0.0025813]
		[batch 20/20] avg loss: 0.1579772982052857		[learning rate: 0.0025783]
	Learning Rate: 0.0025783
	LOSS [training: 0.16549072283007152 | validation: 0.05352350583616018]
	TIME [epoch: 8.34 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10160639625576721		[learning rate: 0.0025753]
		[batch 20/20] avg loss: 0.18926126324135933		[learning rate: 0.0025722]
	Learning Rate: 0.00257222
	LOSS [training: 0.14543382974856328 | validation: 0.06502448654305559]
	TIME [epoch: 8.32 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.098331056535508		[learning rate: 0.0025692]
		[batch 20/20] avg loss: 0.165839828052243		[learning rate: 0.0025662]
	Learning Rate: 0.00256615
	LOSS [training: 0.1320854422938755 | validation: 0.18654427018065672]
	TIME [epoch: 8.32 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20923229681499295		[learning rate: 0.0025631]
		[batch 20/20] avg loss: 0.2610657286498809		[learning rate: 0.0025601]
	Learning Rate: 0.0025601
	LOSS [training: 0.23514901273243694 | validation: 0.09763772562989316]
	TIME [epoch: 8.32 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10142903754655588		[learning rate: 0.0025571]
		[batch 20/20] avg loss: 0.23265281586883582		[learning rate: 0.0025541]
	Learning Rate: 0.00255406
	LOSS [training: 0.16704092670769582 | validation: 0.24659007742960964]
	TIME [epoch: 8.34 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1331884017043055		[learning rate: 0.002551]
		[batch 20/20] avg loss: 0.18909712536650863		[learning rate: 0.002548]
	Learning Rate: 0.00254803
	LOSS [training: 0.16114276353540705 | validation: 0.506726750544938]
	TIME [epoch: 8.32 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.139856737319299		[learning rate: 0.002545]
		[batch 20/20] avg loss: 0.12985494889236737		[learning rate: 0.002542]
	Learning Rate: 0.00254202
	LOSS [training: 0.13485584310583323 | validation: 0.15244993467352266]
	TIME [epoch: 8.33 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28336195143338466		[learning rate: 0.002539]
		[batch 20/20] avg loss: 0.11241626702380725		[learning rate: 0.002536]
	Learning Rate: 0.00253603
	LOSS [training: 0.19788910922859593 | validation: 0.05374879434846509]
	TIME [epoch: 8.32 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.131597716882899		[learning rate: 0.002533]
		[batch 20/20] avg loss: 0.155393984736625		[learning rate: 0.00253]
	Learning Rate: 0.00253004
	LOSS [training: 0.143495850809762 | validation: 0.16066189914656637]
	TIME [epoch: 8.33 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23379725243804733		[learning rate: 0.0025271]
		[batch 20/20] avg loss: 0.13842884100605768		[learning rate: 0.0025241]
	Learning Rate: 0.00252408
	LOSS [training: 0.1861130467220525 | validation: 0.059835853921409596]
	TIME [epoch: 8.32 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13118323200799395		[learning rate: 0.0025211]
		[batch 20/20] avg loss: 0.12311959210177122		[learning rate: 0.0025181]
	Learning Rate: 0.00251812
	LOSS [training: 0.1271514120548826 | validation: 0.19240051733830332]
	TIME [epoch: 8.32 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15642213595242366		[learning rate: 0.0025152]
		[batch 20/20] avg loss: 0.14676451099134324		[learning rate: 0.0025122]
	Learning Rate: 0.00251218
	LOSS [training: 0.15159332347188342 | validation: 0.09185933493422516]
	TIME [epoch: 8.31 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15931987725005442		[learning rate: 0.0025092]
		[batch 20/20] avg loss: 0.11637194993603366		[learning rate: 0.0025063]
	Learning Rate: 0.00250626
	LOSS [training: 0.13784591359304402 | validation: 0.043605222783423624]
	TIME [epoch: 8.34 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.092126442826684		[learning rate: 0.0025033]
		[batch 20/20] avg loss: 0.1716933037543489		[learning rate: 0.0025003]
	Learning Rate: 0.00250035
	LOSS [training: 0.13190987329051646 | validation: 0.20857687149144027]
	TIME [epoch: 8.33 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.317740307044152		[learning rate: 0.0024974]
		[batch 20/20] avg loss: 0.12125227971826338		[learning rate: 0.0024944]
	Learning Rate: 0.00249445
	LOSS [training: 0.21949629338120769 | validation: 0.34253712479410864]
	TIME [epoch: 8.32 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2478164941407277		[learning rate: 0.0024915]
		[batch 20/20] avg loss: 0.15020533636689262		[learning rate: 0.0024886]
	Learning Rate: 0.00248856
	LOSS [training: 0.19901091525381018 | validation: 0.15389899467275106]
	TIME [epoch: 8.32 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12838561865362833		[learning rate: 0.0024856]
		[batch 20/20] avg loss: 0.11971155709161796		[learning rate: 0.0024827]
	Learning Rate: 0.00248269
	LOSS [training: 0.12404858787262311 | validation: 0.0519206582661875]
	TIME [epoch: 8.33 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09202354099598692		[learning rate: 0.0024798]
		[batch 20/20] avg loss: 0.07837252047559096		[learning rate: 0.0024768]
	Learning Rate: 0.00247684
	LOSS [training: 0.08519803073578894 | validation: 0.10271972768252204]
	TIME [epoch: 8.33 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21145017100044616		[learning rate: 0.0024739]
		[batch 20/20] avg loss: 0.2243454090066201		[learning rate: 0.002471]
	Learning Rate: 0.00247099
	LOSS [training: 0.21789779000353313 | validation: 0.09490578364356043]
	TIME [epoch: 8.32 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1448952757936108		[learning rate: 0.0024681]
		[batch 20/20] avg loss: 0.20355106844170806		[learning rate: 0.0024652]
	Learning Rate: 0.00246517
	LOSS [training: 0.17422317211765945 | validation: 0.11948606187683626]
	TIME [epoch: 8.32 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1156613494802432		[learning rate: 0.0024623]
		[batch 20/20] avg loss: 0.14764318223942996		[learning rate: 0.0024594]
	Learning Rate: 0.00245935
	LOSS [training: 0.13165226585983658 | validation: 0.16117956106691408]
	TIME [epoch: 8.34 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18697023345784294		[learning rate: 0.0024564]
		[batch 20/20] avg loss: 0.20227920762842536		[learning rate: 0.0024535]
	Learning Rate: 0.00245355
	LOSS [training: 0.19462472054313412 | validation: 0.15340974193267237]
	TIME [epoch: 8.32 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14681827998577984		[learning rate: 0.0024507]
		[batch 20/20] avg loss: 0.11877417325657011		[learning rate: 0.0024478]
	Learning Rate: 0.00244776
	LOSS [training: 0.13279622662117496 | validation: 0.06225562114375354]
	TIME [epoch: 8.32 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1177642801786432		[learning rate: 0.0024449]
		[batch 20/20] avg loss: 0.12290683501741644		[learning rate: 0.002442]
	Learning Rate: 0.00244199
	LOSS [training: 0.1203355575980298 | validation: 0.134204062097726]
	TIME [epoch: 8.32 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11055355459942655		[learning rate: 0.0024391]
		[batch 20/20] avg loss: 0.11525903611770198		[learning rate: 0.0024362]
	Learning Rate: 0.00243623
	LOSS [training: 0.11290629535856427 | validation: 0.0788572391366532]
	TIME [epoch: 8.34 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12468235438096029		[learning rate: 0.0024334]
		[batch 20/20] avg loss: 0.13786194399262458		[learning rate: 0.0024305]
	Learning Rate: 0.00243048
	LOSS [training: 0.13127214918679245 | validation: 0.09455047899786877]
	TIME [epoch: 8.32 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1123805771692538		[learning rate: 0.0024276]
		[batch 20/20] avg loss: 0.10103257395178533		[learning rate: 0.0024247]
	Learning Rate: 0.00242475
	LOSS [training: 0.10670657556051959 | validation: 0.05971924424572418]
	TIME [epoch: 8.31 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1430402503325366		[learning rate: 0.0024219]
		[batch 20/20] avg loss: 0.11474296200969922		[learning rate: 0.002419]
	Learning Rate: 0.00241903
	LOSS [training: 0.1288916061711179 | validation: 0.15479994175084144]
	TIME [epoch: 8.31 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13887192985689645		[learning rate: 0.0024162]
		[batch 20/20] avg loss: 0.14075166681976095		[learning rate: 0.0024133]
	Learning Rate: 0.00241332
	LOSS [training: 0.13981179833832874 | validation: 0.06858881753729355]
	TIME [epoch: 8.33 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18004996378653082		[learning rate: 0.0024105]
		[batch 20/20] avg loss: 0.09274248235400809		[learning rate: 0.0024076]
	Learning Rate: 0.00240763
	LOSS [training: 0.13639622307026947 | validation: 0.041006531202806215]
	TIME [epoch: 8.32 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11366378127421976		[learning rate: 0.0024048]
		[batch 20/20] avg loss: 0.22608913305487494		[learning rate: 0.002402]
	Learning Rate: 0.00240195
	LOSS [training: 0.16987645716454736 | validation: 0.06713521156740637]
	TIME [epoch: 8.31 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10388454225157188		[learning rate: 0.0023991]
		[batch 20/20] avg loss: 0.13526660220245165		[learning rate: 0.0023963]
	Learning Rate: 0.00239628
	LOSS [training: 0.11957557222701179 | validation: 0.08818350504830433]
	TIME [epoch: 8.32 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15299940948248963		[learning rate: 0.0023935]
		[batch 20/20] avg loss: 0.11196027646820013		[learning rate: 0.0023906]
	Learning Rate: 0.00239063
	LOSS [training: 0.1324798429753449 | validation: 0.04176963589895786]
	TIME [epoch: 8.34 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11147024576125168		[learning rate: 0.0023878]
		[batch 20/20] avg loss: 0.08974120366643923		[learning rate: 0.002385]
	Learning Rate: 0.00238499
	LOSS [training: 0.10060572471384546 | validation: 0.07125992220789498]
	TIME [epoch: 8.33 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12075902288527529		[learning rate: 0.0023822]
		[batch 20/20] avg loss: 0.12898685055932077		[learning rate: 0.0023794]
	Learning Rate: 0.00237937
	LOSS [training: 0.124872936722298 | validation: 0.9495430228881646]
	TIME [epoch: 8.31 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26003956318308113		[learning rate: 0.0023766]
		[batch 20/20] avg loss: 0.14051689660150848		[learning rate: 0.0023738]
	Learning Rate: 0.00237375
	LOSS [training: 0.2002782298922948 | validation: 0.14737630029515877]
	TIME [epoch: 8.32 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10165816802745009		[learning rate: 0.002371]
		[batch 20/20] avg loss: 0.14270715778823256		[learning rate: 0.0023682]
	Learning Rate: 0.00236816
	LOSS [training: 0.12218266290784133 | validation: 0.14404252595174638]
	TIME [epoch: 8.34 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08359750270672743		[learning rate: 0.0023654]
		[batch 20/20] avg loss: 0.1737902714558906		[learning rate: 0.0023626]
	Learning Rate: 0.00236257
	LOSS [training: 0.128693887081309 | validation: 0.09189746457098569]
	TIME [epoch: 8.33 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1393638080784327		[learning rate: 0.0023598]
		[batch 20/20] avg loss: 0.12134359852700571		[learning rate: 0.002357]
	Learning Rate: 0.002357
	LOSS [training: 0.1303537033027192 | validation: 0.07870414010986504]
	TIME [epoch: 8.32 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1474303114467243		[learning rate: 0.0023542]
		[batch 20/20] avg loss: 0.0995580496106531		[learning rate: 0.0023514]
	Learning Rate: 0.00235144
	LOSS [training: 0.12349418052868866 | validation: 0.057631093230566685]
	TIME [epoch: 8.32 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19731401986560387		[learning rate: 0.0023487]
		[batch 20/20] avg loss: 0.15241060912485993		[learning rate: 0.0023459]
	Learning Rate: 0.00234589
	LOSS [training: 0.17486231449523193 | validation: 0.10936478020108285]
	TIME [epoch: 8.34 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11847707544070125		[learning rate: 0.0023431]
		[batch 20/20] avg loss: 0.0932830986978814		[learning rate: 0.0023404]
	Learning Rate: 0.00234036
	LOSS [training: 0.1058800870692913 | validation: 0.166003207479123]
	TIME [epoch: 8.32 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15764707524595617		[learning rate: 0.0023376]
		[batch 20/20] avg loss: 0.13580772073791716		[learning rate: 0.0023348]
	Learning Rate: 0.00233484
	LOSS [training: 0.1467273979919367 | validation: 0.13089055575343708]
	TIME [epoch: 8.32 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10098077653252377		[learning rate: 0.0023321]
		[batch 20/20] avg loss: 0.17123009536880668		[learning rate: 0.0023293]
	Learning Rate: 0.00232933
	LOSS [training: 0.1361054359506652 | validation: 0.1101054801958082]
	TIME [epoch: 8.32 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12064445826262385		[learning rate: 0.0023266]
		[batch 20/20] avg loss: 0.11338686587595954		[learning rate: 0.0023238]
	Learning Rate: 0.00232383
	LOSS [training: 0.1170156620692917 | validation: 0.09099204795794111]
	TIME [epoch: 8.33 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1396702047852459		[learning rate: 0.0023211]
		[batch 20/20] avg loss: 0.12263080128476171		[learning rate: 0.0023184]
	Learning Rate: 0.00231835
	LOSS [training: 0.1311505030350038 | validation: 0.08209917025773165]
	TIME [epoch: 8.33 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10292517169743406		[learning rate: 0.0023156]
		[batch 20/20] avg loss: 0.13610757859181527		[learning rate: 0.0023129]
	Learning Rate: 0.00231288
	LOSS [training: 0.11951637514462465 | validation: 0.19940426006302445]
	TIME [epoch: 8.32 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15706938926126904		[learning rate: 0.0023102]
		[batch 20/20] avg loss: 0.13595104755590845		[learning rate: 0.0023074]
	Learning Rate: 0.00230743
	LOSS [training: 0.14651021840858874 | validation: 0.08095749336981166]
	TIME [epoch: 8.32 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2140517390046961		[learning rate: 0.0023047]
		[batch 20/20] avg loss: 0.22551095544648128		[learning rate: 0.002302]
	Learning Rate: 0.00230199
	LOSS [training: 0.21978134722558865 | validation: 0.10630470130276179]
	TIME [epoch: 8.34 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0939881676753738		[learning rate: 0.0022993]
		[batch 20/20] avg loss: 0.11755379318509478		[learning rate: 0.0022966]
	Learning Rate: 0.00229656
	LOSS [training: 0.10577098043023425 | validation: 0.1920682756527531]
	TIME [epoch: 8.32 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09386749351875641		[learning rate: 0.0022938]
		[batch 20/20] avg loss: 0.16870117468264903		[learning rate: 0.0022911]
	Learning Rate: 0.00229114
	LOSS [training: 0.13128433410070273 | validation: 0.10695414184840131]
	TIME [epoch: 8.33 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17841154612808496		[learning rate: 0.0022884]
		[batch 20/20] avg loss: 0.17162577927824407		[learning rate: 0.0022857]
	Learning Rate: 0.00228573
	LOSS [training: 0.17501866270316455 | validation: 0.07818444237712749]
	TIME [epoch: 8.31 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13884656044244487		[learning rate: 0.002283]
		[batch 20/20] avg loss: 0.14514894948700668		[learning rate: 0.0022803]
	Learning Rate: 0.00228034
	LOSS [training: 0.14199775496472575 | validation: 0.045907389867803294]
	TIME [epoch: 8.34 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0883197681216845		[learning rate: 0.0022777]
		[batch 20/20] avg loss: 0.11955514568593004		[learning rate: 0.002275]
	Learning Rate: 0.00227496
	LOSS [training: 0.10393745690380726 | validation: 0.09023600682726843]
	TIME [epoch: 8.32 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12526165028343386		[learning rate: 0.0022723]
		[batch 20/20] avg loss: 0.07247764231713584		[learning rate: 0.0022696]
	Learning Rate: 0.0022696
	LOSS [training: 0.09886964630028486 | validation: 0.24140420010051594]
	TIME [epoch: 8.32 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17246021851406435		[learning rate: 0.0022669]
		[batch 20/20] avg loss: 0.114333195245289		[learning rate: 0.0022642]
	Learning Rate: 0.00226424
	LOSS [training: 0.1433967068796767 | validation: 0.06575988899798017]
	TIME [epoch: 8.31 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1224650145418205		[learning rate: 0.0022616]
		[batch 20/20] avg loss: 0.11447141574011366		[learning rate: 0.0022589]
	Learning Rate: 0.0022589
	LOSS [training: 0.11846821514096706 | validation: 0.06305248935758223]
	TIME [epoch: 8.34 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12029720833512633		[learning rate: 0.0022562]
		[batch 20/20] avg loss: 0.12194098620728325		[learning rate: 0.0022536]
	Learning Rate: 0.00225357
	LOSS [training: 0.1211190972712048 | validation: 0.11749420327317968]
	TIME [epoch: 8.32 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11984258663512551		[learning rate: 0.0022509]
		[batch 20/20] avg loss: 0.10483965233873191		[learning rate: 0.0022483]
	Learning Rate: 0.00224826
	LOSS [training: 0.11234111948692871 | validation: 0.20358810409920683]
	TIME [epoch: 8.32 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22630556917901573		[learning rate: 0.0022456]
		[batch 20/20] avg loss: 0.12849282907326243		[learning rate: 0.002243]
	Learning Rate: 0.00224295
	LOSS [training: 0.17739919912613905 | validation: 0.08456127398066345]
	TIME [epoch: 8.32 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11134417275626489		[learning rate: 0.0022403]
		[batch 20/20] avg loss: 0.14346320431723653		[learning rate: 0.0022377]
	Learning Rate: 0.00223766
	LOSS [training: 0.12740368853675071 | validation: 0.10453025532610007]
	TIME [epoch: 8.34 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24791258177582226		[learning rate: 0.002235]
		[batch 20/20] avg loss: 0.16492543148200628		[learning rate: 0.0022324]
	Learning Rate: 0.00223239
	LOSS [training: 0.20641900662891427 | validation: 0.0793613614313595]
	TIME [epoch: 8.33 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08841222641914974		[learning rate: 0.0022298]
		[batch 20/20] avg loss: 0.15617295949448617		[learning rate: 0.0022271]
	Learning Rate: 0.00222712
	LOSS [training: 0.12229259295681796 | validation: 0.09422041940447166]
	TIME [epoch: 8.31 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12282508921415321		[learning rate: 0.0022245]
		[batch 20/20] avg loss: 0.12698527618592778		[learning rate: 0.0022219]
	Learning Rate: 0.00222187
	LOSS [training: 0.12490518270004052 | validation: 0.1007334731504213]
	TIME [epoch: 8.32 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1673828585135916		[learning rate: 0.0022192]
		[batch 20/20] avg loss: 0.12471705642993465		[learning rate: 0.0022166]
	Learning Rate: 0.00221663
	LOSS [training: 0.14604995747176314 | validation: 0.09193620020414482]
	TIME [epoch: 8.34 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13754334435163057		[learning rate: 0.002214]
		[batch 20/20] avg loss: 0.10843644370238707		[learning rate: 0.0022114]
	Learning Rate: 0.0022114
	LOSS [training: 0.12298989402700881 | validation: 0.19272604333352397]
	TIME [epoch: 8.32 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13565803757516837		[learning rate: 0.0022088]
		[batch 20/20] avg loss: 0.09084671649165059		[learning rate: 0.0022062]
	Learning Rate: 0.00220618
	LOSS [training: 0.11325237703340948 | validation: 0.07020039210005508]
	TIME [epoch: 8.31 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12636000188023813		[learning rate: 0.0022036]
		[batch 20/20] avg loss: 0.24889545280359654		[learning rate: 0.002201]
	Learning Rate: 0.00220098
	LOSS [training: 0.18762772734191732 | validation: 0.1870096166756625]
	TIME [epoch: 8.31 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11348352042556539		[learning rate: 0.0021984]
		[batch 20/20] avg loss: 0.1378056706533006		[learning rate: 0.0021958]
	Learning Rate: 0.00219578
	LOSS [training: 0.12564459553943302 | validation: 0.2949741778356816]
	TIME [epoch: 8.34 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22093805833071153		[learning rate: 0.0021932]
		[batch 20/20] avg loss: 0.09553442822634449		[learning rate: 0.0021906]
	Learning Rate: 0.0021906
	LOSS [training: 0.15823624327852806 | validation: 0.2302092569256718]
	TIME [epoch: 8.32 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14300627972984972		[learning rate: 0.002188]
		[batch 20/20] avg loss: 0.1175450288540177		[learning rate: 0.0021854]
	Learning Rate: 0.00218544
	LOSS [training: 0.13027565429193372 | validation: 0.09874109598744225]
	TIME [epoch: 8.32 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07437734042082975		[learning rate: 0.0021829]
		[batch 20/20] avg loss: 0.1644407358580134		[learning rate: 0.0021803]
	Learning Rate: 0.00218028
	LOSS [training: 0.11940903813942154 | validation: 0.0559871799597837]
	TIME [epoch: 8.32 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1158199766949376		[learning rate: 0.0021777]
		[batch 20/20] avg loss: 0.12461623726947953		[learning rate: 0.0021751]
	Learning Rate: 0.00217514
	LOSS [training: 0.12021810698220854 | validation: 0.10974919486042575]
	TIME [epoch: 8.35 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08866182270394357		[learning rate: 0.0021726]
		[batch 20/20] avg loss: 0.09168077511715234		[learning rate: 0.00217]
	Learning Rate: 0.00217001
	LOSS [training: 0.09017129891054797 | validation: 0.1934322126713996]
	TIME [epoch: 8.32 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09746257423638381		[learning rate: 0.0021674]
		[batch 20/20] avg loss: 0.2258549014509515		[learning rate: 0.0021649]
	Learning Rate: 0.00216489
	LOSS [training: 0.16165873784366766 | validation: 0.12498251614806852]
	TIME [epoch: 8.32 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13627034138453234		[learning rate: 0.0021623]
		[batch 20/20] avg loss: 0.10050984261524842		[learning rate: 0.0021598]
	Learning Rate: 0.00215978
	LOSS [training: 0.11839009199989034 | validation: 0.05545699043132292]
	TIME [epoch: 8.32 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09102234878982063		[learning rate: 0.0021572]
		[batch 20/20] avg loss: 0.09019234814044129		[learning rate: 0.0021547]
	Learning Rate: 0.00215469
	LOSS [training: 0.09060734846513097 | validation: 0.0903316723175689]
	TIME [epoch: 8.35 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0910397274396106		[learning rate: 0.0021521]
		[batch 20/20] avg loss: 0.12077589214203155		[learning rate: 0.0021496]
	Learning Rate: 0.00214961
	LOSS [training: 0.10590780979082108 | validation: 0.32738850409783665]
	TIME [epoch: 8.32 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1264052331285114		[learning rate: 0.0021471]
		[batch 20/20] avg loss: 0.09835745868598347		[learning rate: 0.0021445]
	Learning Rate: 0.00214454
	LOSS [training: 0.11238134590724742 | validation: 0.1707864347624991]
	TIME [epoch: 8.32 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13582692648795153		[learning rate: 0.002142]
		[batch 20/20] avg loss: 0.10637650864832879		[learning rate: 0.0021395]
	Learning Rate: 0.00213948
	LOSS [training: 0.12110171756814014 | validation: 0.1439929962476083]
	TIME [epoch: 8.32 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1560143805111315		[learning rate: 0.002137]
		[batch 20/20] avg loss: 0.08293616646619219		[learning rate: 0.0021344]
	Learning Rate: 0.00213443
	LOSS [training: 0.11947527348866185 | validation: 0.09096231656933733]
	TIME [epoch: 8.35 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10494377555289906		[learning rate: 0.0021319]
		[batch 20/20] avg loss: 0.08222140653343737		[learning rate: 0.0021294]
	Learning Rate: 0.0021294
	LOSS [training: 0.09358259104316821 | validation: 0.03101980016869904]
	TIME [epoch: 8.32 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12095995043470775		[learning rate: 0.0021269]
		[batch 20/20] avg loss: 0.11605283136756836		[learning rate: 0.0021244]
	Learning Rate: 0.00212437
	LOSS [training: 0.11850639090113808 | validation: 0.10456573531649695]
	TIME [epoch: 8.32 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11026936033449056		[learning rate: 0.0021219]
		[batch 20/20] avg loss: 0.1242327076782342		[learning rate: 0.0021194]
	Learning Rate: 0.00211936
	LOSS [training: 0.11725103400636236 | validation: 0.15280330843332413]
	TIME [epoch: 8.32 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10546487641310565		[learning rate: 0.0021169]
		[batch 20/20] avg loss: 0.11806073107531674		[learning rate: 0.0021144]
	Learning Rate: 0.00211436
	LOSS [training: 0.11176280374421117 | validation: 0.035839668060203306]
	TIME [epoch: 8.34 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1035898820324335		[learning rate: 0.0021119]
		[batch 20/20] avg loss: 0.1553175943256131		[learning rate: 0.0021094]
	Learning Rate: 0.00210938
	LOSS [training: 0.1294537381790233 | validation: 0.049338741488804704]
	TIME [epoch: 8.33 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08470105686879084		[learning rate: 0.0021069]
		[batch 20/20] avg loss: 0.13792385017410502		[learning rate: 0.0021044]
	Learning Rate: 0.0021044
	LOSS [training: 0.11131245352144792 | validation: 0.0795578113531592]
	TIME [epoch: 8.32 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10837296391999415		[learning rate: 0.0021019]
		[batch 20/20] avg loss: 0.13777881990694013		[learning rate: 0.0020994]
	Learning Rate: 0.00209944
	LOSS [training: 0.12307589191346716 | validation: 0.12647559420504023]
	TIME [epoch: 8.31 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19775191927458785		[learning rate: 0.002097]
		[batch 20/20] avg loss: 0.11151975699563225		[learning rate: 0.0020945]
	Learning Rate: 0.00209448
	LOSS [training: 0.15463583813511003 | validation: 0.07563666811454067]
	TIME [epoch: 8.34 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0891012236613247		[learning rate: 0.002092]
		[batch 20/20] avg loss: 0.10579277907446549		[learning rate: 0.0020895]
	Learning Rate: 0.00208954
	LOSS [training: 0.0974470013678951 | validation: 0.18100636633790831]
	TIME [epoch: 8.32 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16567244401478726		[learning rate: 0.0020871]
		[batch 20/20] avg loss: 0.17507986180369137		[learning rate: 0.0020846]
	Learning Rate: 0.00208461
	LOSS [training: 0.17037615290923935 | validation: 0.04819145438074845]
	TIME [epoch: 8.31 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08999708127224229		[learning rate: 0.0020822]
		[batch 20/20] avg loss: 0.11673623278968939		[learning rate: 0.0020797]
	Learning Rate: 0.0020797
	LOSS [training: 0.10336665703096584 | validation: 0.058192156700087816]
	TIME [epoch: 8.32 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10380445357140027		[learning rate: 0.0020772]
		[batch 20/20] avg loss: 0.08789511679290517		[learning rate: 0.0020748]
	Learning Rate: 0.00207479
	LOSS [training: 0.09584978518215273 | validation: 0.0713321320480266]
	TIME [epoch: 8.34 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12309823614295087		[learning rate: 0.0020723]
		[batch 20/20] avg loss: 0.09910786149101788		[learning rate: 0.0020699]
	Learning Rate: 0.0020699
	LOSS [training: 0.11110304881698436 | validation: 0.08813714625665037]
	TIME [epoch: 8.32 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11345604439259986		[learning rate: 0.0020675]
		[batch 20/20] avg loss: 0.1301092901806787		[learning rate: 0.002065]
	Learning Rate: 0.00206501
	LOSS [training: 0.12178266728663929 | validation: 0.05048030505387535]
	TIME [epoch: 8.32 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09708013357645465		[learning rate: 0.0020626]
		[batch 20/20] avg loss: 0.07934581060636056		[learning rate: 0.0020601]
	Learning Rate: 0.00206014
	LOSS [training: 0.08821297209140762 | validation: 0.07529334864158735]
	TIME [epoch: 8.32 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1585034785281494		[learning rate: 0.0020577]
		[batch 20/20] avg loss: 0.11255972537268422		[learning rate: 0.0020553]
	Learning Rate: 0.00205528
	LOSS [training: 0.1355316019504168 | validation: 0.05297472327864057]
	TIME [epoch: 8.34 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15342996388928973		[learning rate: 0.0020529]
		[batch 20/20] avg loss: 0.10260390442612921		[learning rate: 0.0020504]
	Learning Rate: 0.00205044
	LOSS [training: 0.12801693415770948 | validation: 0.06335318963080214]
	TIME [epoch: 8.32 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10347527681211503		[learning rate: 0.002048]
		[batch 20/20] avg loss: 0.1369929646483149		[learning rate: 0.0020456]
	Learning Rate: 0.0020456
	LOSS [training: 0.12023412073021496 | validation: 0.10907231140110356]
	TIME [epoch: 8.31 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11871735965980464		[learning rate: 0.0020432]
		[batch 20/20] avg loss: 0.13908492973915426		[learning rate: 0.0020408]
	Learning Rate: 0.00204077
	LOSS [training: 0.1289011446994795 | validation: 0.22675548180509297]
	TIME [epoch: 8.32 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13338658532431374		[learning rate: 0.0020384]
		[batch 20/20] avg loss: 0.1423496333058501		[learning rate: 0.002036]
	Learning Rate: 0.00203596
	LOSS [training: 0.13786810931508192 | validation: 0.07876892681691573]
	TIME [epoch: 8.35 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07494858731907701		[learning rate: 0.0020336]
		[batch 20/20] avg loss: 0.08249447121848336		[learning rate: 0.0020312]
	Learning Rate: 0.00203116
	LOSS [training: 0.07872152926878016 | validation: 0.08824429522526984]
	TIME [epoch: 8.32 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08832309610674391		[learning rate: 0.0020288]
		[batch 20/20] avg loss: 0.10872146323626394		[learning rate: 0.0020264]
	Learning Rate: 0.00202637
	LOSS [training: 0.09852227967150394 | validation: 0.16902311607744358]
	TIME [epoch: 8.32 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1191412671105522		[learning rate: 0.002024]
		[batch 20/20] avg loss: 0.11025200707049283		[learning rate: 0.0020216]
	Learning Rate: 0.00202159
	LOSS [training: 0.11469663709052251 | validation: 0.049709367706985814]
	TIME [epoch: 8.32 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09086266388133843		[learning rate: 0.0020192]
		[batch 20/20] avg loss: 0.1209498473588565		[learning rate: 0.0020168]
	Learning Rate: 0.00201682
	LOSS [training: 0.10590625562009748 | validation: 0.21988060747649318]
	TIME [epoch: 8.34 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10654519822212098		[learning rate: 0.0020144]
		[batch 20/20] avg loss: 0.15552507271405716		[learning rate: 0.0020121]
	Learning Rate: 0.00201206
	LOSS [training: 0.13103513546808906 | validation: 0.11218310687194917]
	TIME [epoch: 8.32 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08699938723777653		[learning rate: 0.0020097]
		[batch 20/20] avg loss: 0.10122088069294655		[learning rate: 0.0020073]
	Learning Rate: 0.00200731
	LOSS [training: 0.09411013396536153 | validation: 0.09010695697273274]
	TIME [epoch: 8.32 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09470034094698558		[learning rate: 0.0020049]
		[batch 20/20] avg loss: 0.12543696937513713		[learning rate: 0.0020026]
	Learning Rate: 0.00200258
	LOSS [training: 0.11006865516106135 | validation: 0.2122303094308654]
	TIME [epoch: 8.31 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13401590102565827		[learning rate: 0.0020002]
		[batch 20/20] avg loss: 0.1584961500008119		[learning rate: 0.0019979]
	Learning Rate: 0.00199786
	LOSS [training: 0.1462560255132351 | validation: 0.07589948101812824]
	TIME [epoch: 8.34 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07524873211032154		[learning rate: 0.0019955]
		[batch 20/20] avg loss: 0.1196684679655939		[learning rate: 0.0019931]
	Learning Rate: 0.00199314
	LOSS [training: 0.09745860003795774 | validation: 0.12823453627792292]
	TIME [epoch: 8.32 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0991450437938016		[learning rate: 0.0019908]
		[batch 20/20] avg loss: 0.1168129757542385		[learning rate: 0.0019884]
	Learning Rate: 0.00198844
	LOSS [training: 0.10797900977402004 | validation: 0.04846034333443506]
	TIME [epoch: 8.32 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09451308977043964		[learning rate: 0.0019861]
		[batch 20/20] avg loss: 0.08740132744098641		[learning rate: 0.0019838]
	Learning Rate: 0.00198375
	LOSS [training: 0.09095720860571303 | validation: 0.0643777971303579]
	TIME [epoch: 8.32 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09448341423558032		[learning rate: 0.0019814]
		[batch 20/20] avg loss: 0.10149730742604848		[learning rate: 0.0019791]
	Learning Rate: 0.00197907
	LOSS [training: 0.09799036083081439 | validation: 0.07547433199377439]
	TIME [epoch: 8.34 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06848076212099109		[learning rate: 0.0019767]
		[batch 20/20] avg loss: 0.11515767601445481		[learning rate: 0.0019744]
	Learning Rate: 0.0019744
	LOSS [training: 0.09181921906772295 | validation: 0.17890640941480768]
	TIME [epoch: 8.33 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11092541693610203		[learning rate: 0.0019721]
		[batch 20/20] avg loss: 0.09464047991549603		[learning rate: 0.0019697]
	Learning Rate: 0.00196975
	LOSS [training: 0.10278294842579902 | validation: 0.10480750049359179]
	TIME [epoch: 8.31 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07560954486470603		[learning rate: 0.0019674]
		[batch 20/20] avg loss: 0.0729780733482293		[learning rate: 0.0019651]
	Learning Rate: 0.0019651
	LOSS [training: 0.07429380910646767 | validation: 0.21141816380310974]
	TIME [epoch: 8.32 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16183819959936188		[learning rate: 0.0019628]
		[batch 20/20] avg loss: 0.08295917332467714		[learning rate: 0.0019605]
	Learning Rate: 0.00196046
	LOSS [training: 0.1223986864620195 | validation: 0.12093017992441728]
	TIME [epoch: 8.34 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10079000782041043		[learning rate: 0.0019582]
		[batch 20/20] avg loss: 0.13849301800235383		[learning rate: 0.0019558]
	Learning Rate: 0.00195584
	LOSS [training: 0.11964151291138211 | validation: 0.09879179452211283]
	TIME [epoch: 8.33 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10444619567938265		[learning rate: 0.0019535]
		[batch 20/20] avg loss: 0.09311222469395762		[learning rate: 0.0019512]
	Learning Rate: 0.00195123
	LOSS [training: 0.09877921018667013 | validation: 0.054877112302940685]
	TIME [epoch: 8.31 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13143453185281048		[learning rate: 0.0019489]
		[batch 20/20] avg loss: 0.16295979223334667		[learning rate: 0.0019466]
	Learning Rate: 0.00194662
	LOSS [training: 0.1471971620430786 | validation: 0.05349327905577364]
	TIME [epoch: 8.33 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09509102135887175		[learning rate: 0.0019443]
		[batch 20/20] avg loss: 0.08465038773554406		[learning rate: 0.001942]
	Learning Rate: 0.00194203
	LOSS [training: 0.08987070454720789 | validation: 0.2186439504997319]
	TIME [epoch: 8.34 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12453502303841357		[learning rate: 0.0019397]
		[batch 20/20] avg loss: 0.1187889606943328		[learning rate: 0.0019375]
	Learning Rate: 0.00193745
	LOSS [training: 0.12166199186637319 | validation: 0.0525717302481224]
	TIME [epoch: 8.33 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08934489229799523		[learning rate: 0.0019352]
		[batch 20/20] avg loss: 0.0807177812980928		[learning rate: 0.0019329]
	Learning Rate: 0.00193288
	LOSS [training: 0.085031336798044 | validation: 0.10964962487049729]
	TIME [epoch: 8.31 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08069028371327358		[learning rate: 0.0019306]
		[batch 20/20] avg loss: 0.09121434965710148		[learning rate: 0.0019283]
	Learning Rate: 0.00192832
	LOSS [training: 0.08595231668518753 | validation: 0.13977521793841147]
	TIME [epoch: 8.32 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14114568327246707		[learning rate: 0.001926]
		[batch 20/20] avg loss: 0.09822405015994945		[learning rate: 0.0019238]
	Learning Rate: 0.00192377
	LOSS [training: 0.11968486671620826 | validation: 0.12578851558811951]
	TIME [epoch: 8.34 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0907973197868346		[learning rate: 0.0019215]
		[batch 20/20] avg loss: 0.13133430016532538		[learning rate: 0.0019192]
	Learning Rate: 0.00191924
	LOSS [training: 0.11106580997608 | validation: 0.09192739808914509]
	TIME [epoch: 8.32 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09745430924158571		[learning rate: 0.001917]
		[batch 20/20] avg loss: 0.1201838273185403		[learning rate: 0.0019147]
	Learning Rate: 0.00191471
	LOSS [training: 0.108819068280063 | validation: 0.10875104914089281]
	TIME [epoch: 8.32 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0644363881152139		[learning rate: 0.0019124]
		[batch 20/20] avg loss: 0.12264000382392592		[learning rate: 0.0019102]
	Learning Rate: 0.00191019
	LOSS [training: 0.09353819596956993 | validation: 0.08420167123422068]
	TIME [epoch: 8.33 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08213140268417121		[learning rate: 0.0019079]
		[batch 20/20] avg loss: 0.10936446557707302		[learning rate: 0.0019057]
	Learning Rate: 0.00190569
	LOSS [training: 0.0957479341306221 | validation: 0.07170954702627702]
	TIME [epoch: 8.34 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08175779628436666		[learning rate: 0.0019034]
		[batch 20/20] avg loss: 0.06649361442014347		[learning rate: 0.0019012]
	Learning Rate: 0.00190119
	LOSS [training: 0.07412570535225507 | validation: 0.16227925789724193]
	TIME [epoch: 8.32 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10030356824050998		[learning rate: 0.0018989]
		[batch 20/20] avg loss: 0.09345920777079998		[learning rate: 0.0018967]
	Learning Rate: 0.00189671
	LOSS [training: 0.09688138800565499 | validation: 0.0876822836697917]
	TIME [epoch: 8.32 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09839918835018685		[learning rate: 0.0018945]
		[batch 20/20] avg loss: 0.09243811412773753		[learning rate: 0.0018922]
	Learning Rate: 0.00189223
	LOSS [training: 0.09541865123896219 | validation: 0.0695862477311629]
	TIME [epoch: 8.31 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1059288585498072		[learning rate: 0.00189]
		[batch 20/20] avg loss: 0.12950197823562332		[learning rate: 0.0018878]
	Learning Rate: 0.00188777
	LOSS [training: 0.11771541839271524 | validation: 0.20134359422131665]
	TIME [epoch: 8.35 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08958907032635097		[learning rate: 0.0018855]
		[batch 20/20] avg loss: 0.11730929457868348		[learning rate: 0.0018833]
	Learning Rate: 0.00188332
	LOSS [training: 0.10344918245251725 | validation: 0.11654823958111293]
	TIME [epoch: 8.32 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09599227678778552		[learning rate: 0.0018811]
		[batch 20/20] avg loss: 0.06384677248837523		[learning rate: 0.0018789]
	Learning Rate: 0.00187887
	LOSS [training: 0.07991952463808039 | validation: 0.07761164870215707]
	TIME [epoch: 8.3 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05899965543168458		[learning rate: 0.0018767]
		[batch 20/20] avg loss: 0.11394318222740552		[learning rate: 0.0018744]
	Learning Rate: 0.00187444
	LOSS [training: 0.08647141882954504 | validation: 0.06590814659189083]
	TIME [epoch: 8.32 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06476807816983662		[learning rate: 0.0018722]
		[batch 20/20] avg loss: 0.06699443694999664		[learning rate: 0.00187]
	Learning Rate: 0.00187002
	LOSS [training: 0.06588125755991661 | validation: 0.03610342571400613]
	TIME [epoch: 8.34 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11746444662298365		[learning rate: 0.0018678]
		[batch 20/20] avg loss: 0.07085138585041065		[learning rate: 0.0018656]
	Learning Rate: 0.00186561
	LOSS [training: 0.09415791623669716 | validation: 0.03378449927111882]
	TIME [epoch: 8.32 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10047180184078017		[learning rate: 0.0018634]
		[batch 20/20] avg loss: 0.07757949668991544		[learning rate: 0.0018612]
	Learning Rate: 0.00186121
	LOSS [training: 0.08902564926534781 | validation: 0.04567118217911745]
	TIME [epoch: 8.31 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1095178276360794		[learning rate: 0.001859]
		[batch 20/20] avg loss: 0.10522039467447256		[learning rate: 0.0018568]
	Learning Rate: 0.00185682
	LOSS [training: 0.10736911115527596 | validation: 0.12124862898140926]
	TIME [epoch: 8.32 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10451923343484715		[learning rate: 0.0018546]
		[batch 20/20] avg loss: 0.07577094066333645		[learning rate: 0.0018524]
	Learning Rate: 0.00185244
	LOSS [training: 0.09014508704909178 | validation: 0.16274045002685614]
	TIME [epoch: 8.34 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11204827249660843		[learning rate: 0.0018503]
		[batch 20/20] avg loss: 0.08648119783198387		[learning rate: 0.0018481]
	Learning Rate: 0.00184807
	LOSS [training: 0.09926473516429614 | validation: 0.08253019848982387]
	TIME [epoch: 8.32 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09343889836209703		[learning rate: 0.0018459]
		[batch 20/20] avg loss: 0.09942748649042907		[learning rate: 0.0018437]
	Learning Rate: 0.00184371
	LOSS [training: 0.09643319242626307 | validation: 0.06789186031764807]
	TIME [epoch: 8.31 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09987548162301466		[learning rate: 0.0018415]
		[batch 20/20] avg loss: 0.09826573032468812		[learning rate: 0.0018394]
	Learning Rate: 0.00183936
	LOSS [training: 0.0990706059738514 | validation: 0.05643727001044782]
	TIME [epoch: 8.32 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12301003190042711		[learning rate: 0.0018372]
		[batch 20/20] avg loss: 0.16030819877645064		[learning rate: 0.001835]
	Learning Rate: 0.00183502
	LOSS [training: 0.14165911533843886 | validation: 0.0781164854309262]
	TIME [epoch: 8.34 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11267452370309783		[learning rate: 0.0018329]
		[batch 20/20] avg loss: 0.08763207811523398		[learning rate: 0.0018307]
	Learning Rate: 0.00183069
	LOSS [training: 0.1001533009091659 | validation: 0.06128044024095962]
	TIME [epoch: 8.32 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07928145385299995		[learning rate: 0.0018285]
		[batch 20/20] avg loss: 0.10929940184958258		[learning rate: 0.0018264]
	Learning Rate: 0.00182637
	LOSS [training: 0.09429042785129127 | validation: 0.07448371158480024]
	TIME [epoch: 8.31 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10002307154864218		[learning rate: 0.0018242]
		[batch 20/20] avg loss: 0.08596133159059782		[learning rate: 0.0018221]
	Learning Rate: 0.00182207
	LOSS [training: 0.09299220156961999 | validation: 0.09695697186680961]
	TIME [epoch: 8.32 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06402097383436609		[learning rate: 0.0018199]
		[batch 20/20] avg loss: 0.12498385209819655		[learning rate: 0.0018178]
	Learning Rate: 0.00181777
	LOSS [training: 0.09450241296628134 | validation: 0.10584828266920077]
	TIME [epoch: 8.34 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07041186419208922		[learning rate: 0.0018156]
		[batch 20/20] avg loss: 0.14747873830287095		[learning rate: 0.0018135]
	Learning Rate: 0.00181348
	LOSS [training: 0.1089453012474801 | validation: 0.035909513919685734]
	TIME [epoch: 8.33 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09014184815863024		[learning rate: 0.0018113]
		[batch 20/20] avg loss: 0.0960600481794633		[learning rate: 0.0018092]
	Learning Rate: 0.0018092
	LOSS [training: 0.0931009481690468 | validation: 0.09683723957290641]
	TIME [epoch: 8.31 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08840708605702101		[learning rate: 0.0018071]
		[batch 20/20] avg loss: 0.07319610332813503		[learning rate: 0.0018049]
	Learning Rate: 0.00180493
	LOSS [training: 0.080801594692578 | validation: 0.040782156733037964]
	TIME [epoch: 8.32 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0948304990497395		[learning rate: 0.0018028]
		[batch 20/20] avg loss: 0.12441488053986213		[learning rate: 0.0018007]
	Learning Rate: 0.00180068
	LOSS [training: 0.1096226897948008 | validation: 0.04079421923566234]
	TIME [epoch: 8.33 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11121657495884554		[learning rate: 0.0017986]
		[batch 20/20] avg loss: 0.06826160210171552		[learning rate: 0.0017964]
	Learning Rate: 0.00179643
	LOSS [training: 0.08973908853028054 | validation: 0.04176124370859499]
	TIME [epoch: 8.31 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06946831737386597		[learning rate: 0.0017943]
		[batch 20/20] avg loss: 0.10186651263660326		[learning rate: 0.0017922]
	Learning Rate: 0.00179219
	LOSS [training: 0.0856674150052346 | validation: 0.04166630783798003]
	TIME [epoch: 8.31 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09530689468157594		[learning rate: 0.0017901]
		[batch 20/20] avg loss: 0.09410191650303686		[learning rate: 0.001788]
	Learning Rate: 0.00178796
	LOSS [training: 0.0947044055923064 | validation: 0.1887059034185154]
	TIME [epoch: 8.31 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10187197602225954		[learning rate: 0.0017859]
		[batch 20/20] avg loss: 0.06807274860387416		[learning rate: 0.0017837]
	Learning Rate: 0.00178375
	LOSS [training: 0.08497236231306685 | validation: 0.037007332089894734]
	TIME [epoch: 8.34 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10055880837131512		[learning rate: 0.0017816]
		[batch 20/20] avg loss: 0.09073643923150314		[learning rate: 0.0017795]
	Learning Rate: 0.00177954
	LOSS [training: 0.09564762380140915 | validation: 0.09695623920422261]
	TIME [epoch: 8.32 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09248398762927937		[learning rate: 0.0017774]
		[batch 20/20] avg loss: 0.13078032577000234		[learning rate: 0.0017753]
	Learning Rate: 0.00177534
	LOSS [training: 0.11163215669964084 | validation: 0.2810918514341345]
	TIME [epoch: 8.33 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13091473106853396		[learning rate: 0.0017732]
		[batch 20/20] avg loss: 0.06742669694716999		[learning rate: 0.0017712]
	Learning Rate: 0.00177115
	LOSS [training: 0.09917071400785196 | validation: 0.07757369870476348]
	TIME [epoch: 8.32 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1311676151368204		[learning rate: 0.0017691]
		[batch 20/20] avg loss: 0.11171515540940688		[learning rate: 0.001767]
	Learning Rate: 0.00176698
	LOSS [training: 0.12144138527311363 | validation: 0.07451737411762845]
	TIME [epoch: 8.34 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0971246547531864		[learning rate: 0.0017649]
		[batch 20/20] avg loss: 0.09240379100369224		[learning rate: 0.0017628]
	Learning Rate: 0.00176281
	LOSS [training: 0.09476422287843932 | validation: 0.14628294008278783]
	TIME [epoch: 8.31 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08757392000494682		[learning rate: 0.0017607]
		[batch 20/20] avg loss: 0.13265985145889186		[learning rate: 0.0017587]
	Learning Rate: 0.00175865
	LOSS [training: 0.11011688573191933 | validation: 0.11310419964756964]
	TIME [epoch: 8.32 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0662130009524112		[learning rate: 0.0017566]
		[batch 20/20] avg loss: 0.08577003454137881		[learning rate: 0.0017545]
	Learning Rate: 0.0017545
	LOSS [training: 0.07599151774689503 | validation: 0.06021045979630785]
	TIME [epoch: 8.31 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05868345534331073		[learning rate: 0.0017524]
		[batch 20/20] avg loss: 0.08648750168394646		[learning rate: 0.0017504]
	Learning Rate: 0.00175036
	LOSS [training: 0.07258547851362859 | validation: 0.04949831958064708]
	TIME [epoch: 8.34 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08259096657273639		[learning rate: 0.0017483]
		[batch 20/20] avg loss: 0.09865943165085811		[learning rate: 0.0017462]
	Learning Rate: 0.00174623
	LOSS [training: 0.09062519911179726 | validation: 0.07569968314607363]
	TIME [epoch: 8.32 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07804751357536294		[learning rate: 0.0017442]
		[batch 20/20] avg loss: 0.0792062523889603		[learning rate: 0.0017421]
	Learning Rate: 0.00174212
	LOSS [training: 0.07862688298216161 | validation: 0.05627158990917314]
	TIME [epoch: 8.32 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062081298611694744		[learning rate: 0.0017401]
		[batch 20/20] avg loss: 0.07480804364896988		[learning rate: 0.001738]
	Learning Rate: 0.00173801
	LOSS [training: 0.06844467113033233 | validation: 0.07977907020469883]
	TIME [epoch: 8.32 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09106263129764226		[learning rate: 0.001736]
		[batch 20/20] avg loss: 0.09238553403106356		[learning rate: 0.0017339]
	Learning Rate: 0.00173391
	LOSS [training: 0.0917240826643529 | validation: 0.03471646972833014]
	TIME [epoch: 8.34 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08384578230996408		[learning rate: 0.0017319]
		[batch 20/20] avg loss: 0.07294110835249298		[learning rate: 0.0017298]
	Learning Rate: 0.00172982
	LOSS [training: 0.07839344533122854 | validation: 0.03155731430023168]
	TIME [epoch: 8.32 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07158009410526217		[learning rate: 0.0017278]
		[batch 20/20] avg loss: 0.08703768396017418		[learning rate: 0.0017257]
	Learning Rate: 0.00172574
	LOSS [training: 0.0793088890327182 | validation: 0.09632942714559359]
	TIME [epoch: 8.31 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0763522044738751		[learning rate: 0.0017237]
		[batch 20/20] avg loss: 0.08612413737674149		[learning rate: 0.0017217]
	Learning Rate: 0.00172167
	LOSS [training: 0.0812381709253083 | validation: 0.1444758640614077]
	TIME [epoch: 8.31 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06200011335771675		[learning rate: 0.0017196]
		[batch 20/20] avg loss: 0.09749394145956483		[learning rate: 0.0017176]
	Learning Rate: 0.0017176
	LOSS [training: 0.07974702740864079 | validation: 0.15279486703796905]
	TIME [epoch: 8.34 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08791854840270913		[learning rate: 0.0017156]
		[batch 20/20] avg loss: 0.1285409582010911		[learning rate: 0.0017136]
	Learning Rate: 0.00171355
	LOSS [training: 0.10822975330190014 | validation: 0.10736121439251378]
	TIME [epoch: 8.31 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07475062718056427		[learning rate: 0.0017115]
		[batch 20/20] avg loss: 0.08265387858344282		[learning rate: 0.0017095]
	Learning Rate: 0.00170951
	LOSS [training: 0.07870225288200354 | validation: 0.0815993941437656]
	TIME [epoch: 8.31 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10849799779409663		[learning rate: 0.0017075]
		[batch 20/20] avg loss: 0.08853114193013537		[learning rate: 0.0017055]
	Learning Rate: 0.00170548
	LOSS [training: 0.098514569862116 | validation: 0.0942316786555191]
	TIME [epoch: 8.31 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08667954458676466		[learning rate: 0.0017035]
		[batch 20/20] avg loss: 0.08040641694013703		[learning rate: 0.0017015]
	Learning Rate: 0.00170146
	LOSS [training: 0.08354298076345086 | validation: 0.07037190682735034]
	TIME [epoch: 8.34 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07185342931824121		[learning rate: 0.0016994]
		[batch 20/20] avg loss: 0.09987597696994854		[learning rate: 0.0016974]
	Learning Rate: 0.00169744
	LOSS [training: 0.08586470314409488 | validation: 0.17137064368785357]
	TIME [epoch: 8.3 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08544697449177245		[learning rate: 0.0016954]
		[batch 20/20] avg loss: 0.11695392511306997		[learning rate: 0.0016934]
	Learning Rate: 0.00169344
	LOSS [training: 0.10120044980242122 | validation: 0.10015250223855068]
	TIME [epoch: 8.32 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0633742629064695		[learning rate: 0.0016914]
		[batch 20/20] avg loss: 0.08186971954232727		[learning rate: 0.0016894]
	Learning Rate: 0.00168944
	LOSS [training: 0.07262199122439837 | validation: 0.10190793841812186]
	TIME [epoch: 8.31 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09157917205969551		[learning rate: 0.0016874]
		[batch 20/20] avg loss: 0.08169938579019291		[learning rate: 0.0016855]
	Learning Rate: 0.00168546
	LOSS [training: 0.0866392789249442 | validation: 0.06851406181047859]
	TIME [epoch: 8.34 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08191603475610396		[learning rate: 0.0016835]
		[batch 20/20] avg loss: 0.0834463403973238		[learning rate: 0.0016815]
	Learning Rate: 0.00168148
	LOSS [training: 0.08268118757671386 | validation: 0.04101863580762966]
	TIME [epoch: 8.32 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06657907517403108		[learning rate: 0.0016795]
		[batch 20/20] avg loss: 0.06547070941660346		[learning rate: 0.0016775]
	Learning Rate: 0.00167752
	LOSS [training: 0.06602489229531724 | validation: 0.0500334964985614]
	TIME [epoch: 8.32 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08400401298393019		[learning rate: 0.0016755]
		[batch 20/20] avg loss: 0.09620666834540981		[learning rate: 0.0016736]
	Learning Rate: 0.00167356
	LOSS [training: 0.09010534066466999 | validation: 0.10852813505898172]
	TIME [epoch: 8.31 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09084119147620573		[learning rate: 0.0016716]
		[batch 20/20] avg loss: 0.12463262349201229		[learning rate: 0.0016696]
	Learning Rate: 0.00166961
	LOSS [training: 0.10773690748410902 | validation: 0.27704484274091457]
	TIME [epoch: 8.35 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11061494940919284		[learning rate: 0.0016676]
		[batch 20/20] avg loss: 0.07278680003785701		[learning rate: 0.0016657]
	Learning Rate: 0.00166567
	LOSS [training: 0.09170087472352491 | validation: 0.07136968614705272]
	TIME [epoch: 8.31 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06279964870749849		[learning rate: 0.0016637]
		[batch 20/20] avg loss: 0.08776075153509362		[learning rate: 0.0016617]
	Learning Rate: 0.00166174
	LOSS [training: 0.07528020012129606 | validation: 0.044481292817161386]
	TIME [epoch: 8.31 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10186187580225417		[learning rate: 0.0016598]
		[batch 20/20] avg loss: 0.12307871805458385		[learning rate: 0.0016578]
	Learning Rate: 0.00165782
	LOSS [training: 0.112470296928419 | validation: 0.022600839785912968]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_811.pth
	Model improved!!!
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05864100105736171		[learning rate: 0.0016559]
		[batch 20/20] avg loss: 0.09977601500106414		[learning rate: 0.0016539]
	Learning Rate: 0.00165391
	LOSS [training: 0.07920850802921293 | validation: 0.04070768738261156]
	TIME [epoch: 8.34 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0706511711333547		[learning rate: 0.001652]
		[batch 20/20] avg loss: 0.08974283816499977		[learning rate: 0.00165]
	Learning Rate: 0.00165001
	LOSS [training: 0.08019700464917721 | validation: 0.07733777937451784]
	TIME [epoch: 8.31 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08159860694479186		[learning rate: 0.0016481]
		[batch 20/20] avg loss: 0.04969371455978917		[learning rate: 0.0016461]
	Learning Rate: 0.00164612
	LOSS [training: 0.06564616075229053 | validation: 0.07474299060157613]
	TIME [epoch: 8.31 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12057196064546319		[learning rate: 0.0016442]
		[batch 20/20] avg loss: 0.10473800056744018		[learning rate: 0.0016422]
	Learning Rate: 0.00164224
	LOSS [training: 0.1126549806064517 | validation: 0.05717200850483413]
	TIME [epoch: 8.32 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07504599145560897		[learning rate: 0.0016403]
		[batch 20/20] avg loss: 0.08303639489999862		[learning rate: 0.0016384]
	Learning Rate: 0.00163836
	LOSS [training: 0.0790411931778038 | validation: 0.08878923423162292]
	TIME [epoch: 8.33 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1019262064883207		[learning rate: 0.0016364]
		[batch 20/20] avg loss: 0.05631884348954005		[learning rate: 0.0016345]
	Learning Rate: 0.0016345
	LOSS [training: 0.07912252498893037 | validation: 0.05880452776565745]
	TIME [epoch: 8.31 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09090955794324222		[learning rate: 0.0016326]
		[batch 20/20] avg loss: 0.08883432163391437		[learning rate: 0.0016306]
	Learning Rate: 0.00163064
	LOSS [training: 0.0898719397885783 | validation: 0.021098824735135347]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_818.pth
	Model improved!!!
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07956601331935352		[learning rate: 0.0016287]
		[batch 20/20] avg loss: 0.096788196003756		[learning rate: 0.0016268]
	Learning Rate: 0.0016268
	LOSS [training: 0.08817710466155476 | validation: 0.16334288294335006]
	TIME [epoch: 8.31 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11785125148993507		[learning rate: 0.0016249]
		[batch 20/20] avg loss: 0.08191320082868761		[learning rate: 0.001623]
	Learning Rate: 0.00162296
	LOSS [training: 0.09988222615931135 | validation: 0.17425718806151552]
	TIME [epoch: 8.31 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08803640332888715		[learning rate: 0.001621]
		[batch 20/20] avg loss: 0.09710079920812692		[learning rate: 0.0016191]
	Learning Rate: 0.00161913
	LOSS [training: 0.09256860126850701 | validation: 0.37463762048280597]
	TIME [epoch: 8.31 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1056065628049709		[learning rate: 0.0016172]
		[batch 20/20] avg loss: 0.13314141913842004		[learning rate: 0.0016153]
	Learning Rate: 0.00161531
	LOSS [training: 0.11937399097169547 | validation: 0.1766866381949187]
	TIME [epoch: 8.31 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056870362327701526		[learning rate: 0.0016134]
		[batch 20/20] avg loss: 0.11305513182508502		[learning rate: 0.0016115]
	Learning Rate: 0.0016115
	LOSS [training: 0.08496274707639329 | validation: 0.11598136137600887]
	TIME [epoch: 8.31 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07312182290197136		[learning rate: 0.0016096]
		[batch 20/20] avg loss: 0.06281478590671438		[learning rate: 0.0016077]
	Learning Rate: 0.0016077
	LOSS [training: 0.06796830440434286 | validation: 0.05759989684767172]
	TIME [epoch: 8.33 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05579997210786793		[learning rate: 0.0016058]
		[batch 20/20] avg loss: 0.08601259503079425		[learning rate: 0.0016039]
	Learning Rate: 0.00160391
	LOSS [training: 0.0709062835693311 | validation: 0.03499218857737087]
	TIME [epoch: 8.31 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08147479927153778		[learning rate: 0.001602]
		[batch 20/20] avg loss: 0.06774918705378952		[learning rate: 0.0016001]
	Learning Rate: 0.00160012
	LOSS [training: 0.07461199316266363 | validation: 0.045326453827234825]
	TIME [epoch: 8.31 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20258661285222473		[learning rate: 0.0015982]
		[batch 20/20] avg loss: 0.13222400227618003		[learning rate: 0.0015964]
	Learning Rate: 0.00159635
	LOSS [training: 0.16740530756420233 | validation: 0.07137368074477216]
	TIME [epoch: 8.32 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08905828575225151		[learning rate: 0.0015945]
		[batch 20/20] avg loss: 0.08437793596593308		[learning rate: 0.0015926]
	Learning Rate: 0.00159258
	LOSS [training: 0.08671811085909228 | validation: 0.04130752001652089]
	TIME [epoch: 8.31 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10457326376824767		[learning rate: 0.0015907]
		[batch 20/20] avg loss: 0.08382684347863653		[learning rate: 0.0015888]
	Learning Rate: 0.00158883
	LOSS [training: 0.09420005362344211 | validation: 0.16278340623063756]
	TIME [epoch: 8.31 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08673000591602394		[learning rate: 0.001587]
		[batch 20/20] avg loss: 0.06920205351567241		[learning rate: 0.0015851]
	Learning Rate: 0.00158508
	LOSS [training: 0.07796602971584816 | validation: 0.07079152660112026]
	TIME [epoch: 8.3 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09258401727052286		[learning rate: 0.0015832]
		[batch 20/20] avg loss: 0.08983448705701108		[learning rate: 0.0015813]
	Learning Rate: 0.00158134
	LOSS [training: 0.09120925216376698 | validation: 0.09775381655224649]
	TIME [epoch: 8.32 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07244246129625591		[learning rate: 0.0015795]
		[batch 20/20] avg loss: 0.0792738367526963		[learning rate: 0.0015776]
	Learning Rate: 0.00157761
	LOSS [training: 0.07585814902447612 | validation: 0.08611204681296288]
	TIME [epoch: 8.31 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08699738866239895		[learning rate: 0.0015757]
		[batch 20/20] avg loss: 0.13455456103057098		[learning rate: 0.0015739]
	Learning Rate: 0.00157389
	LOSS [training: 0.11077597484648496 | validation: 0.10566924752606076]
	TIME [epoch: 8.3 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04420487404864897		[learning rate: 0.001572]
		[batch 20/20] avg loss: 0.053382672407616336		[learning rate: 0.0015702]
	Learning Rate: 0.00157018
	LOSS [training: 0.04879377322813266 | validation: 0.05774566883488906]
	TIME [epoch: 8.31 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05479219446886332		[learning rate: 0.0015683]
		[batch 20/20] avg loss: 0.17065963909952478		[learning rate: 0.0015665]
	Learning Rate: 0.00156647
	LOSS [training: 0.11272591678419404 | validation: 0.13696165070473834]
	TIME [epoch: 8.33 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13679602121790693		[learning rate: 0.0015646]
		[batch 20/20] avg loss: 0.0717021351727068		[learning rate: 0.0015628]
	Learning Rate: 0.00156278
	LOSS [training: 0.10424907819530685 | validation: 0.06702927293497642]
	TIME [epoch: 8.31 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0904255821899453		[learning rate: 0.0015609]
		[batch 20/20] avg loss: 0.07473349372196439		[learning rate: 0.0015591]
	Learning Rate: 0.00155909
	LOSS [training: 0.08257953795595484 | validation: 0.0669522702090729]
	TIME [epoch: 8.31 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05903542292727739		[learning rate: 0.0015573]
		[batch 20/20] avg loss: 0.12019137880565815		[learning rate: 0.0015554]
	Learning Rate: 0.00155541
	LOSS [training: 0.08961340086646777 | validation: 0.17167954410241834]
	TIME [epoch: 8.3 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0859658119385036		[learning rate: 0.0015536]
		[batch 20/20] avg loss: 0.09054826533849172		[learning rate: 0.0015517]
	Learning Rate: 0.00155175
	LOSS [training: 0.08825703863849767 | validation: 0.08888179361534179]
	TIME [epoch: 8.32 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10855101822948447		[learning rate: 0.0015499]
		[batch 20/20] avg loss: 0.0756555861017689		[learning rate: 0.0015481]
	Learning Rate: 0.00154809
	LOSS [training: 0.09210330216562668 | validation: 0.05940861591910422]
	TIME [epoch: 8.31 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09490217572669223		[learning rate: 0.0015463]
		[batch 20/20] avg loss: 0.10160612821223203		[learning rate: 0.0015444]
	Learning Rate: 0.00154443
	LOSS [training: 0.09825415196946212 | validation: 0.06811991423200671]
	TIME [epoch: 8.31 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07224263029785674		[learning rate: 0.0015426]
		[batch 20/20] avg loss: 0.07425068826058465		[learning rate: 0.0015408]
	Learning Rate: 0.00154079
	LOSS [training: 0.07324665927922068 | validation: 0.09400161218786279]
	TIME [epoch: 8.31 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07530843815534914		[learning rate: 0.001539]
		[batch 20/20] avg loss: 0.07634823069032196		[learning rate: 0.0015372]
	Learning Rate: 0.00153716
	LOSS [training: 0.07582833442283554 | validation: 0.06147063363840735]
	TIME [epoch: 8.32 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10775112113653176		[learning rate: 0.0015353]
		[batch 20/20] avg loss: 0.08283955430460807		[learning rate: 0.0015335]
	Learning Rate: 0.00153353
	LOSS [training: 0.09529533772056993 | validation: 0.10208515831596308]
	TIME [epoch: 8.32 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1058418816272435		[learning rate: 0.0015317]
		[batch 20/20] avg loss: 0.07879482442114212		[learning rate: 0.0015299]
	Learning Rate: 0.00152991
	LOSS [training: 0.0923183530241928 | validation: 0.10067185385092209]
	TIME [epoch: 8.31 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13050644965245925		[learning rate: 0.0015281]
		[batch 20/20] avg loss: 0.0802604002330691		[learning rate: 0.0015263]
	Learning Rate: 0.0015263
	LOSS [training: 0.10538342494276418 | validation: 0.03817415476387129]
	TIME [epoch: 8.3 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08350551113183712		[learning rate: 0.0015245]
		[batch 20/20] avg loss: 0.12713363652792528		[learning rate: 0.0015227]
	Learning Rate: 0.0015227
	LOSS [training: 0.10531957382988119 | validation: 0.06504859964553646]
	TIME [epoch: 8.31 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0755282061753725		[learning rate: 0.0015209]
		[batch 20/20] avg loss: 0.0657984938270351		[learning rate: 0.0015191]
	Learning Rate: 0.00151911
	LOSS [training: 0.0706633500012038 | validation: 0.05511565362120879]
	TIME [epoch: 8.31 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08232583465959714		[learning rate: 0.0015173]
		[batch 20/20] avg loss: 0.0977052196909024		[learning rate: 0.0015155]
	Learning Rate: 0.00151553
	LOSS [training: 0.09001552717524977 | validation: 0.07604653019228519]
	TIME [epoch: 8.31 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07522619864523664		[learning rate: 0.0015137]
		[batch 20/20] avg loss: 0.09790242348089906		[learning rate: 0.001512]
	Learning Rate: 0.00151195
	LOSS [training: 0.08656431106306783 | validation: 0.114914413176463]
	TIME [epoch: 8.3 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0746975693070445		[learning rate: 0.0015102]
		[batch 20/20] avg loss: 0.06625762797121307		[learning rate: 0.0015084]
	Learning Rate: 0.00150839
	LOSS [training: 0.0704775986391288 | validation: 0.03911408686979317]
	TIME [epoch: 8.31 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0706141020648954		[learning rate: 0.0015066]
		[batch 20/20] avg loss: 0.04953128146599351		[learning rate: 0.0015048]
	Learning Rate: 0.00150483
	LOSS [training: 0.060072691765444455 | validation: 0.1102156510819742]
	TIME [epoch: 8.32 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13582141028821387		[learning rate: 0.0015031]
		[batch 20/20] avg loss: 0.06278181602480498		[learning rate: 0.0015013]
	Learning Rate: 0.00150128
	LOSS [training: 0.09930161315650943 | validation: 0.040509405471208126]
	TIME [epoch: 8.3 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09642362907313204		[learning rate: 0.0014995]
		[batch 20/20] avg loss: 0.06657491747010238		[learning rate: 0.0014977]
	Learning Rate: 0.00149774
	LOSS [training: 0.08149927327161718 | validation: 0.03223095952170328]
	TIME [epoch: 8.31 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07675875511796712		[learning rate: 0.001496]
		[batch 20/20] avg loss: 0.06744393707805069		[learning rate: 0.0014942]
	Learning Rate: 0.0014942
	LOSS [training: 0.07210134609800892 | validation: 0.06993151907608487]
	TIME [epoch: 8.33 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0950751022375218		[learning rate: 0.0014924]
		[batch 20/20] avg loss: 0.08934243429139618		[learning rate: 0.0014907]
	Learning Rate: 0.00149068
	LOSS [training: 0.092208768264459 | validation: 0.11114221783004238]
	TIME [epoch: 8.31 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07471676397249495		[learning rate: 0.0014889]
		[batch 20/20] avg loss: 0.07329491882706186		[learning rate: 0.0014872]
	Learning Rate: 0.00148716
	LOSS [training: 0.07400584139977841 | validation: 0.11196516479197792]
	TIME [epoch: 8.31 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08241529757994864		[learning rate: 0.0014854]
		[batch 20/20] avg loss: 0.1299342995638444		[learning rate: 0.0014837]
	Learning Rate: 0.00148366
	LOSS [training: 0.10617479857189652 | validation: 0.025087828683239652]
	TIME [epoch: 8.31 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09793083158767799		[learning rate: 0.0014819]
		[batch 20/20] avg loss: 0.10530382471948822		[learning rate: 0.0014802]
	Learning Rate: 0.00148016
	LOSS [training: 0.10161732815358311 | validation: 0.0326071572744372]
	TIME [epoch: 8.32 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055729453977012136		[learning rate: 0.0014784]
		[batch 20/20] avg loss: 0.10870768433671125		[learning rate: 0.0014767]
	Learning Rate: 0.00147667
	LOSS [training: 0.0822185691568617 | validation: 0.05869361357129427]
	TIME [epoch: 8.31 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11871241897578436		[learning rate: 0.0014749]
		[batch 20/20] avg loss: 0.1471762675728951		[learning rate: 0.0014732]
	Learning Rate: 0.00147318
	LOSS [training: 0.13294434327433974 | validation: 0.07892679818513951]
	TIME [epoch: 8.31 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09256114114401903		[learning rate: 0.0014714]
		[batch 20/20] avg loss: 0.06698614194902015		[learning rate: 0.0014697]
	Learning Rate: 0.00146971
	LOSS [training: 0.07977364154651959 | validation: 0.06238462202202161]
	TIME [epoch: 8.31 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0654711553049797		[learning rate: 0.001468]
		[batch 20/20] avg loss: 0.0656691905181381		[learning rate: 0.0014662]
	Learning Rate: 0.00146624
	LOSS [training: 0.0655701729115589 | validation: 0.1502308801775377]
	TIME [epoch: 8.33 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07830864664183622		[learning rate: 0.0014645]
		[batch 20/20] avg loss: 0.0863349043476611		[learning rate: 0.0014628]
	Learning Rate: 0.00146278
	LOSS [training: 0.08232177549474867 | validation: 0.0849037026777798]
	TIME [epoch: 8.3 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06501012237624455		[learning rate: 0.0014611]
		[batch 20/20] avg loss: 0.11102758884567679		[learning rate: 0.0014593]
	Learning Rate: 0.00145933
	LOSS [training: 0.08801885561096068 | validation: 0.10904267915718305]
	TIME [epoch: 8.3 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07638776335362796		[learning rate: 0.0014576]
		[batch 20/20] avg loss: 0.0932612274260808		[learning rate: 0.0014559]
	Learning Rate: 0.00145589
	LOSS [training: 0.08482449538985437 | validation: 0.0681035754780281]
	TIME [epoch: 8.29 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09762233953861107		[learning rate: 0.0014542]
		[batch 20/20] avg loss: 0.07414061528637533		[learning rate: 0.0014525]
	Learning Rate: 0.00145245
	LOSS [training: 0.08588147741249319 | validation: 0.07765622118972226]
	TIME [epoch: 8.32 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07535863705019774		[learning rate: 0.0014507]
		[batch 20/20] avg loss: 0.08165794903243577		[learning rate: 0.001449]
	Learning Rate: 0.00144903
	LOSS [training: 0.07850829304131675 | validation: 0.13519937623492345]
	TIME [epoch: 8.3 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09590377610733432		[learning rate: 0.0014473]
		[batch 20/20] avg loss: 0.06677397532475117		[learning rate: 0.0014456]
	Learning Rate: 0.00144561
	LOSS [training: 0.08133887571604274 | validation: 0.039199880636560915]
	TIME [epoch: 8.32 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14517645675034035		[learning rate: 0.0014439]
		[batch 20/20] avg loss: 0.08220147297697498		[learning rate: 0.0014422]
	Learning Rate: 0.0014422
	LOSS [training: 0.11368896486365769 | validation: 0.05106043055912926]
	TIME [epoch: 8.35 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07293063623829066		[learning rate: 0.0014405]
		[batch 20/20] avg loss: 0.08763818339388905		[learning rate: 0.0014388]
	Learning Rate: 0.0014388
	LOSS [training: 0.08028440981608988 | validation: 0.04178429618530867]
	TIME [epoch: 8.35 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05566804019876729		[learning rate: 0.0014371]
		[batch 20/20] avg loss: 0.07106616974353032		[learning rate: 0.0014354]
	Learning Rate: 0.0014354
	LOSS [training: 0.06336710497114881 | validation: 0.07567852238806538]
	TIME [epoch: 8.33 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07885033164956616		[learning rate: 0.0014337]
		[batch 20/20] avg loss: 0.11836861877733029		[learning rate: 0.001432]
	Learning Rate: 0.00143202
	LOSS [training: 0.09860947521344822 | validation: 0.07852312732047036]
	TIME [epoch: 8.33 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0769817396939738		[learning rate: 0.0014303]
		[batch 20/20] avg loss: 0.06594886359618782		[learning rate: 0.0014286]
	Learning Rate: 0.00142864
	LOSS [training: 0.0714653016450808 | validation: 0.07075632893198591]
	TIME [epoch: 8.31 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11004782868495522		[learning rate: 0.001427]
		[batch 20/20] avg loss: 0.10684024500471925		[learning rate: 0.0014253]
	Learning Rate: 0.00142527
	LOSS [training: 0.10844403684483725 | validation: 0.05767933012689191]
	TIME [epoch: 8.32 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08832469191931672		[learning rate: 0.0014236]
		[batch 20/20] avg loss: 0.08528854524955899		[learning rate: 0.0014219]
	Learning Rate: 0.00142191
	LOSS [training: 0.08680661858443782 | validation: 0.05372752931995797]
	TIME [epoch: 8.33 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060995692157997085		[learning rate: 0.0014202]
		[batch 20/20] avg loss: 0.06506957130595468		[learning rate: 0.0014186]
	Learning Rate: 0.00141855
	LOSS [training: 0.06303263173197589 | validation: 0.0396476813996056]
	TIME [epoch: 8.31 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1149176016111771		[learning rate: 0.0014169]
		[batch 20/20] avg loss: 0.07568592941604707		[learning rate: 0.0014152]
	Learning Rate: 0.00141521
	LOSS [training: 0.09530176551361211 | validation: 0.11426403596430462]
	TIME [epoch: 8.3 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05859243955394722		[learning rate: 0.0014135]
		[batch 20/20] avg loss: 0.09314927350518555		[learning rate: 0.0014119]
	Learning Rate: 0.00141187
	LOSS [training: 0.07587085652956638 | validation: 0.10244597487864873]
	TIME [epoch: 8.32 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07107372258220271		[learning rate: 0.0014102]
		[batch 20/20] avg loss: 0.06880300780256945		[learning rate: 0.0014085]
	Learning Rate: 0.00140854
	LOSS [training: 0.06993836519238608 | validation: 0.056925387168150404]
	TIME [epoch: 8.31 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05429721678696196		[learning rate: 0.0014069]
		[batch 20/20] avg loss: 0.11460605140634916		[learning rate: 0.0014052]
	Learning Rate: 0.00140522
	LOSS [training: 0.08445163409665557 | validation: 0.06259566662499425]
	TIME [epoch: 8.3 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0832795722271335		[learning rate: 0.0014036]
		[batch 20/20] avg loss: 0.08460287374638051		[learning rate: 0.0014019]
	Learning Rate: 0.0014019
	LOSS [training: 0.083941222986757 | validation: 0.037638220905420405]
	TIME [epoch: 8.3 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08503281816030116		[learning rate: 0.0014002]
		[batch 20/20] avg loss: 0.1042559879057621		[learning rate: 0.0013986]
	Learning Rate: 0.0013986
	LOSS [training: 0.09464440303303163 | validation: 0.053520343149279115]
	TIME [epoch: 8.33 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06807034415119348		[learning rate: 0.0013969]
		[batch 20/20] avg loss: 0.0677823508641616		[learning rate: 0.0013953]
	Learning Rate: 0.0013953
	LOSS [training: 0.06792634750767754 | validation: 0.047783223035014165]
	TIME [epoch: 8.31 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06249766847977014		[learning rate: 0.0013937]
		[batch 20/20] avg loss: 0.07931407743067294		[learning rate: 0.001392]
	Learning Rate: 0.00139201
	LOSS [training: 0.07090587295522152 | validation: 0.12682259782857208]
	TIME [epoch: 8.3 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05533505969126486		[learning rate: 0.0013904]
		[batch 20/20] avg loss: 0.06362430269193362		[learning rate: 0.0013887]
	Learning Rate: 0.00138872
	LOSS [training: 0.05947968119159924 | validation: 0.04622194191829007]
	TIME [epoch: 8.29 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04774789930504429		[learning rate: 0.0013871]
		[batch 20/20] avg loss: 0.062342368037084074		[learning rate: 0.0013854]
	Learning Rate: 0.00138545
	LOSS [training: 0.055045133671064186 | validation: 0.06398291104578639]
	TIME [epoch: 8.32 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08100294255268499		[learning rate: 0.0013838]
		[batch 20/20] avg loss: 0.05864306918721516		[learning rate: 0.0013822]
	Learning Rate: 0.00138218
	LOSS [training: 0.06982300586995008 | validation: 0.04252243711907236]
	TIME [epoch: 8.3 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06334523848303045		[learning rate: 0.0013805]
		[batch 20/20] avg loss: 0.0707756254654904		[learning rate: 0.0013789]
	Learning Rate: 0.00137892
	LOSS [training: 0.06706043197426044 | validation: 0.04952747176439146]
	TIME [epoch: 8.3 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07397140724319515		[learning rate: 0.0013773]
		[batch 20/20] avg loss: 0.05891777871880018		[learning rate: 0.0013757]
	Learning Rate: 0.00137567
	LOSS [training: 0.06644459298099767 | validation: 0.03170130687386461]
	TIME [epoch: 8.29 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08211126582585557		[learning rate: 0.001374]
		[batch 20/20] avg loss: 0.12533928232934438		[learning rate: 0.0013724]
	Learning Rate: 0.00137242
	LOSS [training: 0.10372527407759997 | validation: 0.02475168241007067]
	TIME [epoch: 8.31 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09745308630598852		[learning rate: 0.0013708]
		[batch 20/20] avg loss: 0.10685039473495832		[learning rate: 0.0013692]
	Learning Rate: 0.00136918
	LOSS [training: 0.10215174052047345 | validation: 0.046798571906027875]
	TIME [epoch: 8.31 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04811713874742649		[learning rate: 0.0013676]
		[batch 20/20] avg loss: 0.053922155830195426		[learning rate: 0.001366]
	Learning Rate: 0.00136595
	LOSS [training: 0.051019647288810956 | validation: 0.029537535537907028]
	TIME [epoch: 8.3 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07876187919963588		[learning rate: 0.0013643]
		[batch 20/20] avg loss: 0.0801783953141857		[learning rate: 0.0013627]
	Learning Rate: 0.00136273
	LOSS [training: 0.0794701372569108 | validation: 0.04326590029873499]
	TIME [epoch: 8.3 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17725947493189625		[learning rate: 0.0013611]
		[batch 20/20] avg loss: 0.07220489821801447		[learning rate: 0.0013595]
	Learning Rate: 0.00135952
	LOSS [training: 0.12473218657495536 | validation: 0.03286158349576923]
	TIME [epoch: 8.31 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062193367233292954		[learning rate: 0.0013579]
		[batch 20/20] avg loss: 0.12003363957063688		[learning rate: 0.0013563]
	Learning Rate: 0.00135631
	LOSS [training: 0.09111350340196493 | validation: 0.23000557424773996]
	TIME [epoch: 8.32 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0984950548311582		[learning rate: 0.0013547]
		[batch 20/20] avg loss: 0.061013166651675534		[learning rate: 0.0013531]
	Learning Rate: 0.00135311
	LOSS [training: 0.07975411074141686 | validation: 0.030032633372907985]
	TIME [epoch: 8.31 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08419691169904639		[learning rate: 0.0013515]
		[batch 20/20] avg loss: 0.08063344320636945		[learning rate: 0.0013499]
	Learning Rate: 0.00134992
	LOSS [training: 0.08241517745270793 | validation: 0.1498745664547702]
	TIME [epoch: 8.29 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08754421366060902		[learning rate: 0.0013483]
		[batch 20/20] avg loss: 0.10131389797856985		[learning rate: 0.0013467]
	Learning Rate: 0.00134673
	LOSS [training: 0.09442905581958945 | validation: 0.07313399719264498]
	TIME [epoch: 8.32 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08487690371552412		[learning rate: 0.0013451]
		[batch 20/20] avg loss: 0.05511065528991539		[learning rate: 0.0013436]
	Learning Rate: 0.00134356
	LOSS [training: 0.06999377950271975 | validation: 0.04539158277351622]
	TIME [epoch: 8.31 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06254110986536171		[learning rate: 0.001342]
		[batch 20/20] avg loss: 0.1213978065214885		[learning rate: 0.0013404]
	Learning Rate: 0.00134039
	LOSS [training: 0.09196945819342509 | validation: 0.09429194227455215]
	TIME [epoch: 8.3 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08272771533340395		[learning rate: 0.0013388]
		[batch 20/20] avg loss: 0.06915356952844232		[learning rate: 0.0013372]
	Learning Rate: 0.00133723
	LOSS [training: 0.07594064243092313 | validation: 0.03865757034941033]
	TIME [epoch: 8.29 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05530574288372536		[learning rate: 0.0013356]
		[batch 20/20] avg loss: 0.06025768991582196		[learning rate: 0.0013341]
	Learning Rate: 0.00133407
	LOSS [training: 0.05778171639977366 | validation: 0.06282548865131228]
	TIME [epoch: 8.31 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06316654483340924		[learning rate: 0.0013325]
		[batch 20/20] avg loss: 0.1197227491235107		[learning rate: 0.0013309]
	Learning Rate: 0.00133093
	LOSS [training: 0.09144464697845997 | validation: 0.05794973973901789]
	TIME [epoch: 8.31 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13737380727200793		[learning rate: 0.0013294]
		[batch 20/20] avg loss: 0.07387782569792524		[learning rate: 0.0013278]
	Learning Rate: 0.00132779
	LOSS [training: 0.10562581648496658 | validation: 0.04543478400642563]
	TIME [epoch: 8.3 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07823339310425365		[learning rate: 0.0013262]
		[batch 20/20] avg loss: 0.052746105458954584		[learning rate: 0.0013247]
	Learning Rate: 0.00132465
	LOSS [training: 0.06548974928160411 | validation: 0.13825453628496254]
	TIME [epoch: 8.3 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07519470661628017		[learning rate: 0.0013231]
		[batch 20/20] avg loss: 0.09240121173060112		[learning rate: 0.0013215]
	Learning Rate: 0.00132153
	LOSS [training: 0.08379795917344064 | validation: 0.0930294997519496]
	TIME [epoch: 8.32 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06514645063177778		[learning rate: 0.00132]
		[batch 20/20] avg loss: 0.14116043864548183		[learning rate: 0.0013184]
	Learning Rate: 0.00131841
	LOSS [training: 0.10315344463862981 | validation: 0.04234758201481422]
	TIME [epoch: 8.31 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059189870998214524		[learning rate: 0.0013169]
		[batch 20/20] avg loss: 0.059003438875095446		[learning rate: 0.0013153]
	Learning Rate: 0.0013153
	LOSS [training: 0.05909665493665498 | validation: 0.3037047544098541]
	TIME [epoch: 8.3 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11297330127517272		[learning rate: 0.0013138]
		[batch 20/20] avg loss: 0.09197970137388135		[learning rate: 0.0013122]
	Learning Rate: 0.0013122
	LOSS [training: 0.10247650132452704 | validation: 0.05607519706411039]
	TIME [epoch: 8.3 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06225367728516564		[learning rate: 0.0013107]
		[batch 20/20] avg loss: 0.05012140010336412		[learning rate: 0.0013091]
	Learning Rate: 0.0013091
	LOSS [training: 0.05618753869426487 | validation: 0.05624636493292298]
	TIME [epoch: 8.31 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09932172066765504		[learning rate: 0.0013076]
		[batch 20/20] avg loss: 0.09301151992703058		[learning rate: 0.001306]
	Learning Rate: 0.00130602
	LOSS [training: 0.09616662029734283 | validation: 0.10562127541872923]
	TIME [epoch: 8.32 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06400165763442431		[learning rate: 0.0013045]
		[batch 20/20] avg loss: 0.08893658568675088		[learning rate: 0.0013029]
	Learning Rate: 0.00130294
	LOSS [training: 0.07646912166058761 | validation: 0.045013920987898845]
	TIME [epoch: 8.3 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06130509835740996		[learning rate: 0.0013014]
		[batch 20/20] avg loss: 0.0720431879285687		[learning rate: 0.0012999]
	Learning Rate: 0.00129986
	LOSS [training: 0.06667414314298935 | validation: 0.04202219002496359]
	TIME [epoch: 8.29 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06125397194911616		[learning rate: 0.0012983]
		[batch 20/20] avg loss: 0.0636628858120786		[learning rate: 0.0012968]
	Learning Rate: 0.0012968
	LOSS [training: 0.0624584288805974 | validation: 0.025642182100942268]
	TIME [epoch: 8.3 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06356957818606121		[learning rate: 0.0012953]
		[batch 20/20] avg loss: 0.05111234999506141		[learning rate: 0.0012937]
	Learning Rate: 0.00129374
	LOSS [training: 0.05734096409056131 | validation: 0.047730972120376566]
	TIME [epoch: 8.3 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062442748942398166		[learning rate: 0.0012922]
		[batch 20/20] avg loss: 0.05356775861656503		[learning rate: 0.0012907]
	Learning Rate: 0.00129069
	LOSS [training: 0.058005253779481604 | validation: 0.053437666192747695]
	TIME [epoch: 8.3 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04671550707932675		[learning rate: 0.0012892]
		[batch 20/20] avg loss: 0.10698384638058198		[learning rate: 0.0012876]
	Learning Rate: 0.00128764
	LOSS [training: 0.07684967672995438 | validation: 0.015985516273038278]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_918.pth
	Model improved!!!
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050835715695170036		[learning rate: 0.0012861]
		[batch 20/20] avg loss: 0.060748967071938255		[learning rate: 0.0012846]
	Learning Rate: 0.0012846
	LOSS [training: 0.05579234138355414 | validation: 0.03308607227536057]
	TIME [epoch: 8.32 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08436793543721326		[learning rate: 0.0012831]
		[batch 20/20] avg loss: 0.07170359430190758		[learning rate: 0.0012816]
	Learning Rate: 0.00128157
	LOSS [training: 0.0780357648695604 | validation: 0.05376917087392514]
	TIME [epoch: 8.3 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0689410592849545		[learning rate: 0.0012801]
		[batch 20/20] avg loss: 0.07119994244202957		[learning rate: 0.0012786]
	Learning Rate: 0.00127855
	LOSS [training: 0.07007050086349205 | validation: 0.05199059150073588]
	TIME [epoch: 8.29 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07944944739325052		[learning rate: 0.001277]
		[batch 20/20] avg loss: 0.057139078666842605		[learning rate: 0.0012755]
	Learning Rate: 0.00127553
	LOSS [training: 0.06829426303004657 | validation: 0.0853479333546743]
	TIME [epoch: 8.3 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09700413327770582		[learning rate: 0.001274]
		[batch 20/20] avg loss: 0.06651985239232672		[learning rate: 0.0012725]
	Learning Rate: 0.00127253
	LOSS [training: 0.08176199283501628 | validation: 0.03262723900802672]
	TIME [epoch: 8.31 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07152344194440437		[learning rate: 0.001271]
		[batch 20/20] avg loss: 0.05991253342459334		[learning rate: 0.0012695]
	Learning Rate: 0.00126952
	LOSS [training: 0.06571798768449885 | validation: 0.17554711630900174]
	TIME [epoch: 8.3 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08566373457055096		[learning rate: 0.001268]
		[batch 20/20] avg loss: 0.10026861123050206		[learning rate: 0.0012665]
	Learning Rate: 0.00126653
	LOSS [training: 0.09296617290052653 | validation: 0.1176079397755756]
	TIME [epoch: 8.29 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07415493323621393		[learning rate: 0.001265]
		[batch 20/20] avg loss: 0.04569126842941714		[learning rate: 0.0012635]
	Learning Rate: 0.00126354
	LOSS [training: 0.05992310083281553 | validation: 0.03254879339610429]
	TIME [epoch: 8.3 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044758565702086854		[learning rate: 0.0012621]
		[batch 20/20] avg loss: 0.09667241245267616		[learning rate: 0.0012606]
	Learning Rate: 0.00126056
	LOSS [training: 0.0707154890773815 | validation: 0.11843810417091268]
	TIME [epoch: 8.32 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15405067846019166		[learning rate: 0.0012591]
		[batch 20/20] avg loss: 0.08656841333380687		[learning rate: 0.0012576]
	Learning Rate: 0.00125759
	LOSS [training: 0.12030954589699924 | validation: 0.034401793202659875]
	TIME [epoch: 8.31 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11061119185658823		[learning rate: 0.0012561]
		[batch 20/20] avg loss: 0.0846510672434346		[learning rate: 0.0012546]
	Learning Rate: 0.00125462
	LOSS [training: 0.09763112955001142 | validation: 0.12377600230464707]
	TIME [epoch: 8.29 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07821267961412387		[learning rate: 0.0012531]
		[batch 20/20] avg loss: 0.04903156600815914		[learning rate: 0.0012517]
	Learning Rate: 0.00125166
	LOSS [training: 0.06362212281114152 | validation: 0.04914314410191556]
	TIME [epoch: 8.3 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0564579480636734		[learning rate: 0.0012502]
		[batch 20/20] avg loss: 0.07665335284212613		[learning rate: 0.0012487]
	Learning Rate: 0.00124871
	LOSS [training: 0.06655565045289975 | validation: 0.06382085208761126]
	TIME [epoch: 8.31 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08863822130610306		[learning rate: 0.0012472]
		[batch 20/20] avg loss: 0.07655117053689128		[learning rate: 0.0012458]
	Learning Rate: 0.00124576
	LOSS [training: 0.08259469592149717 | validation: 0.04165249406220764]
	TIME [epoch: 8.31 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07582370570601778		[learning rate: 0.0012443]
		[batch 20/20] avg loss: 0.05824955384232912		[learning rate: 0.0012428]
	Learning Rate: 0.00124283
	LOSS [training: 0.06703662977417343 | validation: 0.028044468717923376]
	TIME [epoch: 8.29 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08613968403293007		[learning rate: 0.0012414]
		[batch 20/20] avg loss: 0.0770034552220525		[learning rate: 0.0012399]
	Learning Rate: 0.00123989
	LOSS [training: 0.08157156962749128 | validation: 0.0619965921771574]
	TIME [epoch: 8.3 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0812049745178766		[learning rate: 0.0012384]
		[batch 20/20] avg loss: 0.06740429032916766		[learning rate: 0.001237]
	Learning Rate: 0.00123697
	LOSS [training: 0.07430463242352213 | validation: 0.05188494312161053]
	TIME [epoch: 8.32 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06855577010050949		[learning rate: 0.0012355]
		[batch 20/20] avg loss: 0.09151595898705693		[learning rate: 0.0012341]
	Learning Rate: 0.00123405
	LOSS [training: 0.08003586454378323 | validation: 0.08399467862878726]
	TIME [epoch: 8.3 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05963379479811855		[learning rate: 0.0012326]
		[batch 20/20] avg loss: 0.09339430920023388		[learning rate: 0.0012311]
	Learning Rate: 0.00123114
	LOSS [training: 0.07651405199917623 | validation: 0.05218171452301774]
	TIME [epoch: 8.3 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049041553747276714		[learning rate: 0.0012297]
		[batch 20/20] avg loss: 0.09439023886884726		[learning rate: 0.0012282]
	Learning Rate: 0.00122824
	LOSS [training: 0.071715896308062 | validation: 0.0421930165632037]
	TIME [epoch: 8.3 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0665366467835139		[learning rate: 0.0012268]
		[batch 20/20] avg loss: 0.08654222724645885		[learning rate: 0.0012253]
	Learning Rate: 0.00122534
	LOSS [training: 0.07653943701498639 | validation: 0.025959913396571575]
	TIME [epoch: 8.33 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046987943200361496		[learning rate: 0.0012239]
		[batch 20/20] avg loss: 0.0858137531759507		[learning rate: 0.0012224]
	Learning Rate: 0.00122245
	LOSS [training: 0.06640084818815609 | validation: 0.0584088495030837]
	TIME [epoch: 8.31 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051632844092212506		[learning rate: 0.001221]
		[batch 20/20] avg loss: 0.06833182005456463		[learning rate: 0.0012196]
	Learning Rate: 0.00121957
	LOSS [training: 0.05998233207338858 | validation: 0.16594370460117147]
	TIME [epoch: 8.3 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08555641948424302		[learning rate: 0.0012181]
		[batch 20/20] avg loss: 0.044615184551319256		[learning rate: 0.0012167]
	Learning Rate: 0.00121669
	LOSS [training: 0.06508580201778111 | validation: 0.056584149487764045]
	TIME [epoch: 8.3 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05839550869733998		[learning rate: 0.0012153]
		[batch 20/20] avg loss: 0.06379611065194557		[learning rate: 0.0012138]
	Learning Rate: 0.00121382
	LOSS [training: 0.06109580967464279 | validation: 0.09287253402542162]
	TIME [epoch: 8.32 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06132985565888018		[learning rate: 0.0012124]
		[batch 20/20] avg loss: 0.09218408764097201		[learning rate: 0.001211]
	Learning Rate: 0.00121096
	LOSS [training: 0.07675697164992609 | validation: 0.19312857338048456]
	TIME [epoch: 8.31 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09456202216703544		[learning rate: 0.0012095]
		[batch 20/20] avg loss: 0.06021492576464591		[learning rate: 0.0012081]
	Learning Rate: 0.0012081
	LOSS [training: 0.07738847396584067 | validation: 0.03857590300699333]
	TIME [epoch: 8.3 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05539420751131587		[learning rate: 0.0012067]
		[batch 20/20] avg loss: 0.08031054164976983		[learning rate: 0.0012052]
	Learning Rate: 0.00120525
	LOSS [training: 0.06785237458054286 | validation: 0.07581076293246013]
	TIME [epoch: 8.3 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08114385155875621		[learning rate: 0.0012038]
		[batch 20/20] avg loss: 0.05325295161747821		[learning rate: 0.0012024]
	Learning Rate: 0.00120241
	LOSS [training: 0.06719840158811721 | validation: 0.04101162742873477]
	TIME [epoch: 8.32 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06281175878134897		[learning rate: 0.001201]
		[batch 20/20] avg loss: 0.07866498804664025		[learning rate: 0.0011996]
	Learning Rate: 0.00119957
	LOSS [training: 0.0707383734139946 | validation: 0.07251037131274181]
	TIME [epoch: 8.3 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05781950588449704		[learning rate: 0.0011982]
		[batch 20/20] avg loss: 0.0769038088970999		[learning rate: 0.0011967]
	Learning Rate: 0.00119674
	LOSS [training: 0.06736165739079847 | validation: 0.1190549618538535]
	TIME [epoch: 8.3 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09304209817939144		[learning rate: 0.0011953]
		[batch 20/20] avg loss: 0.04761861826406998		[learning rate: 0.0011939]
	Learning Rate: 0.00119392
	LOSS [training: 0.0703303582217307 | validation: 0.023905631081602843]
	TIME [epoch: 8.3 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08916197809706977		[learning rate: 0.0011925]
		[batch 20/20] avg loss: 0.06604910693253235		[learning rate: 0.0011911]
	Learning Rate: 0.0011911
	LOSS [training: 0.07760554251480106 | validation: 0.10117699634739993]
	TIME [epoch: 8.33 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07533285796770117		[learning rate: 0.0011897]
		[batch 20/20] avg loss: 0.09378652624497022		[learning rate: 0.0011883]
	Learning Rate: 0.00118829
	LOSS [training: 0.0845596921063357 | validation: 0.04728134578176405]
	TIME [epoch: 8.3 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05369605830198752		[learning rate: 0.0011869]
		[batch 20/20] avg loss: 0.08149862516903711		[learning rate: 0.0011855]
	Learning Rate: 0.00118549
	LOSS [training: 0.06759734173551232 | validation: 0.14659402528329238]
	TIME [epoch: 8.29 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08139925787537826		[learning rate: 0.0011841]
		[batch 20/20] avg loss: 0.07234829826849329		[learning rate: 0.0011827]
	Learning Rate: 0.00118269
	LOSS [training: 0.07687377807193578 | validation: 0.11980153624264847]
	TIME [epoch: 8.3 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0712603113581957		[learning rate: 0.0011813]
		[batch 20/20] avg loss: 0.07159397084261196		[learning rate: 0.0011799]
	Learning Rate: 0.0011799
	LOSS [training: 0.07142714110040382 | validation: 0.08185503231182738]
	TIME [epoch: 8.33 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05718347816875585		[learning rate: 0.0011785]
		[batch 20/20] avg loss: 0.062675646725345		[learning rate: 0.0011771]
	Learning Rate: 0.00117712
	LOSS [training: 0.05992956244705041 | validation: 0.0303637042970556]
	TIME [epoch: 8.31 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08373850059646014		[learning rate: 0.0011757]
		[batch 20/20] avg loss: 0.07009774546553244		[learning rate: 0.0011743]
	Learning Rate: 0.00117434
	LOSS [training: 0.0769181230309963 | validation: 0.1024673839879374]
	TIME [epoch: 8.31 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06411919715077799		[learning rate: 0.001173]
		[batch 20/20] avg loss: 0.05579760280861158		[learning rate: 0.0011716]
	Learning Rate: 0.00117157
	LOSS [training: 0.0599583999796948 | validation: 0.048047535430111935]
	TIME [epoch: 8.3 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058248248171889824		[learning rate: 0.0011702]
		[batch 20/20] avg loss: 0.06034662185141		[learning rate: 0.0011688]
	Learning Rate: 0.00116881
	LOSS [training: 0.05929743501164991 | validation: 0.0465828885478255]
	TIME [epoch: 8.32 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06671432562704718		[learning rate: 0.0011674]
		[batch 20/20] avg loss: 0.061792286756647705		[learning rate: 0.0011661]
	Learning Rate: 0.00116605
	LOSS [training: 0.06425330619184746 | validation: 0.06277559850613643]
	TIME [epoch: 8.3 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06796913394915094		[learning rate: 0.0011647]
		[batch 20/20] avg loss: 0.0702765542159182		[learning rate: 0.0011633]
	Learning Rate: 0.0011633
	LOSS [training: 0.06912284408253458 | validation: 0.09094224490220361]
	TIME [epoch: 8.3 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06294719040203059		[learning rate: 0.0011619]
		[batch 20/20] avg loss: 0.06205825210405578		[learning rate: 0.0011606]
	Learning Rate: 0.00116056
	LOSS [training: 0.0625027212530432 | validation: 0.0363873442796908]
	TIME [epoch: 8.3 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06121587194168383		[learning rate: 0.0011592]
		[batch 20/20] avg loss: 0.09138065909622713		[learning rate: 0.0011578]
	Learning Rate: 0.00115782
	LOSS [training: 0.07629826551895548 | validation: 0.11090403547448684]
	TIME [epoch: 8.33 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061518489335030624		[learning rate: 0.0011565]
		[batch 20/20] avg loss: 0.05808131712475397		[learning rate: 0.0011551]
	Learning Rate: 0.00115509
	LOSS [training: 0.0597999032298923 | validation: 0.03945409358496156]
	TIME [epoch: 8.31 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07143248520002655		[learning rate: 0.0011537]
		[batch 20/20] avg loss: 0.06815059733713738		[learning rate: 0.0011524]
	Learning Rate: 0.00115236
	LOSS [training: 0.06979154126858197 | validation: 0.07610585674172664]
	TIME [epoch: 8.3 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05938051619537839		[learning rate: 0.001151]
		[batch 20/20] avg loss: 0.06059054405082477		[learning rate: 0.0011496]
	Learning Rate: 0.00114965
	LOSS [training: 0.05998553012310157 | validation: 0.061828550009993606]
	TIME [epoch: 8.3 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059790509353084276		[learning rate: 0.0011483]
		[batch 20/20] avg loss: 0.08265768758608895		[learning rate: 0.0011469]
	Learning Rate: 0.00114693
	LOSS [training: 0.07122409846958659 | validation: 0.13970508526253833]
	TIME [epoch: 8.32 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0730254227935599		[learning rate: 0.0011456]
		[batch 20/20] avg loss: 0.06937093023683996		[learning rate: 0.0011442]
	Learning Rate: 0.00114423
	LOSS [training: 0.07119817651519993 | validation: 0.14317488808370282]
	TIME [epoch: 8.31 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06118893375598285		[learning rate: 0.0011429]
		[batch 20/20] avg loss: 0.05616551372541395		[learning rate: 0.0011415]
	Learning Rate: 0.00114153
	LOSS [training: 0.058677223740698406 | validation: 0.13833580456178274]
	TIME [epoch: 8.3 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08792569529804553		[learning rate: 0.0011402]
		[batch 20/20] avg loss: 0.05193533438548248		[learning rate: 0.0011388]
	Learning Rate: 0.00113884
	LOSS [training: 0.069930514841764 | validation: 0.07634612807838204]
	TIME [epoch: 8.31 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05939480851664075		[learning rate: 0.0011375]
		[batch 20/20] avg loss: 0.08164101196384094		[learning rate: 0.0011362]
	Learning Rate: 0.00113615
	LOSS [training: 0.07051791024024084 | validation: 0.04588892576036932]
	TIME [epoch: 8.32 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06350617334675245		[learning rate: 0.0011348]
		[batch 20/20] avg loss: 0.08740987181980142		[learning rate: 0.0011335]
	Learning Rate: 0.00113347
	LOSS [training: 0.07545802258327693 | validation: 0.033064292984555065]
	TIME [epoch: 8.3 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060155425914845906		[learning rate: 0.0011321]
		[batch 20/20] avg loss: 0.07900381524219		[learning rate: 0.0011308]
	Learning Rate: 0.0011308
	LOSS [training: 0.06957962057851794 | validation: 0.0479380574126657]
	TIME [epoch: 8.3 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09885224118947085		[learning rate: 0.0011295]
		[batch 20/20] avg loss: 0.06588659842845342		[learning rate: 0.0011281]
	Learning Rate: 0.00112813
	LOSS [training: 0.08236941980896212 | validation: 0.06417237330109139]
	TIME [epoch: 8.3 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059640359412138286		[learning rate: 0.0011268]
		[batch 20/20] avg loss: 0.16073902425809217		[learning rate: 0.0011255]
	Learning Rate: 0.00112547
	LOSS [training: 0.1101896918351152 | validation: 0.03885333413102561]
	TIME [epoch: 8.32 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048598425045749634		[learning rate: 0.0011241]
		[batch 20/20] avg loss: 0.04668440569626565		[learning rate: 0.0011228]
	Learning Rate: 0.00112281
	LOSS [training: 0.04764141537100765 | validation: 0.02945841271842956]
	TIME [epoch: 8.3 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.065837235102671		[learning rate: 0.0011215]
		[batch 20/20] avg loss: 0.08062390485834006		[learning rate: 0.0011202]
	Learning Rate: 0.00112017
	LOSS [training: 0.07323056998050552 | validation: 0.03427549747102648]
	TIME [epoch: 8.29 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06352322874750617		[learning rate: 0.0011188]
		[batch 20/20] avg loss: 0.059220007615535177		[learning rate: 0.0011175]
	Learning Rate: 0.00111752
	LOSS [training: 0.06137161818152068 | validation: 0.15044312416472844]
	TIME [epoch: 8.3 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08226831921809444		[learning rate: 0.0011162]
		[batch 20/20] avg loss: 0.041981902338087976		[learning rate: 0.0011149]
	Learning Rate: 0.00111489
	LOSS [training: 0.06212511077809121 | validation: 0.033669277158212885]
	TIME [epoch: 8.32 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06048697607818203		[learning rate: 0.0011136]
		[batch 20/20] avg loss: 0.09944701348603804		[learning rate: 0.0011123]
	Learning Rate: 0.00111226
	LOSS [training: 0.07996699478211004 | validation: 0.0412246039718274]
	TIME [epoch: 8.31 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08002959258452255		[learning rate: 0.0011109]
		[batch 20/20] avg loss: 0.07833874339564176		[learning rate: 0.0011096]
	Learning Rate: 0.00110963
	LOSS [training: 0.07918416799008214 | validation: 0.0444382021100698]
	TIME [epoch: 8.29 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0531509187009482		[learning rate: 0.0011083]
		[batch 20/20] avg loss: 0.05449873604021301		[learning rate: 0.001107]
	Learning Rate: 0.00110702
	LOSS [training: 0.05382482737058061 | validation: 0.12341308466112887]
	TIME [epoch: 8.3 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0855657654880149		[learning rate: 0.0011057]
		[batch 20/20] avg loss: 0.08589220848089357		[learning rate: 0.0011044]
	Learning Rate: 0.0011044
	LOSS [training: 0.08572898698445423 | validation: 0.030875986295648996]
	TIME [epoch: 8.32 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049453150564314395		[learning rate: 0.0011031]
		[batch 20/20] avg loss: 0.08247042374543165		[learning rate: 0.0011018]
	Learning Rate: 0.0011018
	LOSS [training: 0.06596178715487303 | validation: 0.030278523547603093]
	TIME [epoch: 8.31 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09072070978242126		[learning rate: 0.0011005]
		[batch 20/20] avg loss: 0.04525504649777661		[learning rate: 0.0010992]
	Learning Rate: 0.0010992
	LOSS [training: 0.06798787814009895 | validation: 0.04022895486761635]
	TIME [epoch: 8.3 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08365069555709896		[learning rate: 0.0010979]
		[batch 20/20] avg loss: 0.05626284755164253		[learning rate: 0.0010966]
	Learning Rate: 0.00109661
	LOSS [training: 0.06995677155437076 | validation: 0.05217183383195752]
	TIME [epoch: 8.3 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07482423794752613		[learning rate: 0.0010953]
		[batch 20/20] avg loss: 0.09528433571247753		[learning rate: 0.001094]
	Learning Rate: 0.00109402
	LOSS [training: 0.08505428683000185 | validation: 0.051419875405980586]
	TIME [epoch: 8.33 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06934772334478616		[learning rate: 0.0010927]
		[batch 20/20] avg loss: 0.06406605225201589		[learning rate: 0.0010914]
	Learning Rate: 0.00109144
	LOSS [training: 0.06670688779840103 | validation: 0.076400803347247]
	TIME [epoch: 8.31 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05525336137706634		[learning rate: 0.0010902]
		[batch 20/20] avg loss: 0.09395826170967823		[learning rate: 0.0010889]
	Learning Rate: 0.00108887
	LOSS [training: 0.07460581154337229 | validation: 0.02471779145174416]
	TIME [epoch: 8.3 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06087920927028123		[learning rate: 0.0010876]
		[batch 20/20] avg loss: 0.06236982901025963		[learning rate: 0.0010863]
	Learning Rate: 0.0010863
	LOSS [training: 0.06162451914027043 | validation: 0.031840721015253134]
	TIME [epoch: 8.29 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055249379277445956		[learning rate: 0.001085]
		[batch 20/20] avg loss: 0.056127464987400635		[learning rate: 0.0010837]
	Learning Rate: 0.00108373
	LOSS [training: 0.05568842213242329 | validation: 0.06473851731810765]
	TIME [epoch: 8.32 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04530662760421382		[learning rate: 0.0010825]
		[batch 20/20] avg loss: 0.058888911288842195		[learning rate: 0.0010812]
	Learning Rate: 0.00108118
	LOSS [training: 0.052097769446528 | validation: 0.05593587888895787]
	TIME [epoch: 8.31 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0879143569319341		[learning rate: 0.0010799]
		[batch 20/20] avg loss: 0.05523343998728301		[learning rate: 0.0010786]
	Learning Rate: 0.00107863
	LOSS [training: 0.07157389845960856 | validation: 0.08924182837119188]
	TIME [epoch: 8.3 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06105980475813723		[learning rate: 0.0010774]
		[batch 20/20] avg loss: 0.0628192064157301		[learning rate: 0.0010761]
	Learning Rate: 0.00107608
	LOSS [training: 0.06193950558693366 | validation: 0.04304654216015938]
	TIME [epoch: 8.31 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05573267128446282		[learning rate: 0.0010748]
		[batch 20/20] avg loss: 0.0599177620689871		[learning rate: 0.0010735]
	Learning Rate: 0.00107355
	LOSS [training: 0.057825216676724946 | validation: 0.05146471960713946]
	TIME [epoch: 8.32 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08183078740953777		[learning rate: 0.0010723]
		[batch 20/20] avg loss: 0.12002895800339455		[learning rate: 0.001071]
	Learning Rate: 0.00107101
	LOSS [training: 0.10092987270646614 | validation: 0.06430207094762734]
	TIME [epoch: 8.31 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06355022995893155		[learning rate: 0.0010697]
		[batch 20/20] avg loss: 0.0648648047396008		[learning rate: 0.0010685]
	Learning Rate: 0.00106849
	LOSS [training: 0.06420751734926618 | validation: 0.06402282809942908]
	TIME [epoch: 8.3 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06025322705716045		[learning rate: 0.0010672]
		[batch 20/20] avg loss: 0.06399439995781986		[learning rate: 0.001066]
	Learning Rate: 0.00106597
	LOSS [training: 0.062123813507490154 | validation: 0.03198170623708137]
	TIME [epoch: 8.3 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04581287672428085		[learning rate: 0.0010647]
		[batch 20/20] avg loss: 0.06116102816289663		[learning rate: 0.0010635]
	Learning Rate: 0.00106345
	LOSS [training: 0.05348695244358875 | validation: 0.061884605583539615]
	TIME [epoch: 8.33 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07840452920961742		[learning rate: 0.0010622]
		[batch 20/20] avg loss: 0.07498591106168245		[learning rate: 0.0010609]
	Learning Rate: 0.00106094
	LOSS [training: 0.07669522013564994 | validation: 0.14952662420347754]
	TIME [epoch: 8.3 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0814894377315984		[learning rate: 0.0010597]
		[batch 20/20] avg loss: 0.048044387550144216		[learning rate: 0.0010584]
	Learning Rate: 0.00105844
	LOSS [training: 0.06476691264087131 | validation: 0.06020742526135114]
	TIME [epoch: 8.3 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06672126644120563		[learning rate: 0.0010572]
		[batch 20/20] avg loss: 0.048639338358525325		[learning rate: 0.0010559]
	Learning Rate: 0.00105594
	LOSS [training: 0.05768030239986547 | validation: 0.04911496017079843]
	TIME [epoch: 8.31 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07847743010993476		[learning rate: 0.0010547]
		[batch 20/20] avg loss: 0.06132409625660499		[learning rate: 0.0010535]
	Learning Rate: 0.00105345
	LOSS [training: 0.06990076318326985 | validation: 0.05552909572437169]
	TIME [epoch: 8.33 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07977710185243322		[learning rate: 0.0010522]
		[batch 20/20] avg loss: 0.0811278627487016		[learning rate: 0.001051]
	Learning Rate: 0.00105097
	LOSS [training: 0.08045248230056741 | validation: 0.09897169033786292]
	TIME [epoch: 8.31 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05529264277802953		[learning rate: 0.0010497]
		[batch 20/20] avg loss: 0.08806195886376295		[learning rate: 0.0010485]
	Learning Rate: 0.00104849
	LOSS [training: 0.07167730082089624 | validation: 0.044369004857404575]
	TIME [epoch: 8.31 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07946015593158814		[learning rate: 0.0010473]
		[batch 20/20] avg loss: 0.051211288974060444		[learning rate: 0.001046]
	Learning Rate: 0.00104602
	LOSS [training: 0.0653357224528243 | validation: 0.01989934156527929]
	TIME [epoch: 8.3 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038193378180313126		[learning rate: 0.0010448]
		[batch 20/20] avg loss: 0.08143254928897756		[learning rate: 0.0010435]
	Learning Rate: 0.00104355
	LOSS [training: 0.05981296373464534 | validation: 0.05859865637236482]
	TIME [epoch: 8.32 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08783933925065777		[learning rate: 0.0010423]
		[batch 20/20] avg loss: 0.08472713984053126		[learning rate: 0.0010411]
	Learning Rate: 0.00104109
	LOSS [training: 0.0862832395455945 | validation: 0.032024516681810195]
	TIME [epoch: 8.31 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05366274961148891		[learning rate: 0.0010399]
		[batch 20/20] avg loss: 0.06227161189118875		[learning rate: 0.0010386]
	Learning Rate: 0.00103863
	LOSS [training: 0.05796718075133883 | validation: 0.09791031535890048]
	TIME [epoch: 8.31 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06261929623262716		[learning rate: 0.0010374]
		[batch 20/20] avg loss: 0.05202676948607331		[learning rate: 0.0010362]
	Learning Rate: 0.00103618
	LOSS [training: 0.05732303285935024 | validation: 0.04256682128955336]
	TIME [epoch: 8.3 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04796709883329976		[learning rate: 0.001035]
		[batch 20/20] avg loss: 0.07208258939125128		[learning rate: 0.0010337]
	Learning Rate: 0.00103374
	LOSS [training: 0.06002484411227551 | validation: 0.02879911720347316]
	TIME [epoch: 8.33 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0683088441173721		[learning rate: 0.0010325]
		[batch 20/20] avg loss: 0.08686665278932487		[learning rate: 0.0010313]
	Learning Rate: 0.0010313
	LOSS [training: 0.07758774845334848 | validation: 0.0659698381087519]
	TIME [epoch: 8.31 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05483043290879171		[learning rate: 0.0010301]
		[batch 20/20] avg loss: 0.049981671183063583		[learning rate: 0.0010289]
	Learning Rate: 0.00102887
	LOSS [training: 0.052406052045927645 | validation: 0.029864248286933754]
	TIME [epoch: 8.3 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04091142595953457		[learning rate: 0.0010277]
		[batch 20/20] avg loss: 0.06008914710804415		[learning rate: 0.0010264]
	Learning Rate: 0.00102644
	LOSS [training: 0.05050028653378935 | validation: 0.03317246466962923]
	TIME [epoch: 8.31 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048794007961044564		[learning rate: 0.0010252]
		[batch 20/20] avg loss: 0.09222642889389245		[learning rate: 0.001024]
	Learning Rate: 0.00102402
	LOSS [training: 0.07051021842746852 | validation: 0.05272032776435687]
	TIME [epoch: 8.32 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06298304388570199		[learning rate: 0.0010228]
		[batch 20/20] avg loss: 0.06531743620472978		[learning rate: 0.0010216]
	Learning Rate: 0.0010216
	LOSS [training: 0.06415024004521588 | validation: 0.04896884601353905]
	TIME [epoch: 8.31 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045103896872895205		[learning rate: 0.0010204]
		[batch 20/20] avg loss: 0.07873116212587598		[learning rate: 0.0010192]
	Learning Rate: 0.00101919
	LOSS [training: 0.0619175294993856 | validation: 0.19170050881031686]
	TIME [epoch: 8.28 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08987879123931962		[learning rate: 0.001018]
		[batch 20/20] avg loss: 0.05805918076628128		[learning rate: 0.0010168]
	Learning Rate: 0.00101679
	LOSS [training: 0.07396898600280047 | validation: 0.12406199294558354]
	TIME [epoch: 8.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0781380166291653		[learning rate: 0.0010156]
		[batch 20/20] avg loss: 0.06394196911500807		[learning rate: 0.0010144]
	Learning Rate: 0.00101439
	LOSS [training: 0.0710399928720867 | validation: 0.028538591244072407]
	TIME [epoch: 8.32 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053440848387213344		[learning rate: 0.0010132]
		[batch 20/20] avg loss: 0.044331240570346084		[learning rate: 0.001012]
	Learning Rate: 0.001012
	LOSS [training: 0.048886044478779714 | validation: 0.03192673900864626]
	TIME [epoch: 8.3 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06470094526436224		[learning rate: 0.0010108]
		[batch 20/20] avg loss: 0.05734604796671883		[learning rate: 0.0010096]
	Learning Rate: 0.00100961
	LOSS [training: 0.06102349661554053 | validation: 0.030606913810085516]
	TIME [epoch: 8.3 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0628082414835546		[learning rate: 0.0010084]
		[batch 20/20] avg loss: 0.08436343399548588		[learning rate: 0.0010072]
	Learning Rate: 0.00100723
	LOSS [training: 0.07358583773952024 | validation: 0.04703935002182624]
	TIME [epoch: 8.3 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0571406634119076		[learning rate: 0.001006]
		[batch 20/20] avg loss: 0.06696234400072128		[learning rate: 0.0010049]
	Learning Rate: 0.00100485
	LOSS [training: 0.062051503706314445 | validation: 0.05265438826759497]
	TIME [epoch: 8.33 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07116271126335974		[learning rate: 0.0010037]
		[batch 20/20] avg loss: 0.05489714184659992		[learning rate: 0.0010025]
	Learning Rate: 0.00100248
	LOSS [training: 0.06302992655497983 | validation: 0.06736794638025509]
	TIME [epoch: 8.32 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05779701969913391		[learning rate: 0.0010013]
		[batch 20/20] avg loss: 0.055192151465679275		[learning rate: 0.0010001]
	Learning Rate: 0.00100012
	LOSS [training: 0.056494585582406595 | validation: 0.0966460300185952]
	TIME [epoch: 8.31 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07825365776297166		[learning rate: 0.00099894]
		[batch 20/20] avg loss: 0.09846308797868303		[learning rate: 0.00099776]
	Learning Rate: 0.000997759
	LOSS [training: 0.08835837287082733 | validation: 0.02061954363412554]
	TIME [epoch: 8.3 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047488783516413384		[learning rate: 0.00099658]
		[batch 20/20] avg loss: 0.07960422173531995		[learning rate: 0.00099541]
	Learning Rate: 0.000995405
	LOSS [training: 0.06354650262586668 | validation: 0.04991839203225559]
	TIME [epoch: 8.34 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05683062652270696		[learning rate: 0.00099423]
		[batch 20/20] avg loss: 0.1047175285609649		[learning rate: 0.00099306]
	Learning Rate: 0.000993057
	LOSS [training: 0.08077407754183592 | validation: 0.10830474800711616]
	TIME [epoch: 8.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0708807704311522		[learning rate: 0.00099189]
		[batch 20/20] avg loss: 0.07245684476223987		[learning rate: 0.00099071]
	Learning Rate: 0.000990715
	LOSS [training: 0.07166880759669605 | validation: 0.035376544076110954]
	TIME [epoch: 8.3 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04537918027925956		[learning rate: 0.00098955]
		[batch 20/20] avg loss: 0.05234425041178513		[learning rate: 0.00098838]
	Learning Rate: 0.000988378
	LOSS [training: 0.04886171534552235 | validation: 0.03426965267057818]
	TIME [epoch: 8.3 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07521029685736261		[learning rate: 0.00098721]
		[batch 20/20] avg loss: 0.05319083680411115		[learning rate: 0.00098605]
	Learning Rate: 0.000986047
	LOSS [training: 0.06420056683073685 | validation: 0.03392456621933017]
	TIME [epoch: 8.33 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047958139685034526		[learning rate: 0.00098488]
		[batch 20/20] avg loss: 0.0753132343577444		[learning rate: 0.00098372]
	Learning Rate: 0.000983721
	LOSS [training: 0.06163568702138946 | validation: 0.05465863992148932]
	TIME [epoch: 8.3 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07381557884728672		[learning rate: 0.00098256]
		[batch 20/20] avg loss: 0.0659134751726201		[learning rate: 0.0009814]
	Learning Rate: 0.0009814
	LOSS [training: 0.06986452700995341 | validation: 0.034033267767467526]
	TIME [epoch: 8.3 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059202779028819194		[learning rate: 0.00098024]
		[batch 20/20] avg loss: 0.06662997426828018		[learning rate: 0.00097909]
	Learning Rate: 0.000979085
	LOSS [training: 0.0629163766485497 | validation: 0.016677088243240858]
	TIME [epoch: 8.3 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06531371092343015		[learning rate: 0.00097793]
		[batch 20/20] avg loss: 0.04996211911226962		[learning rate: 0.00097678]
	Learning Rate: 0.000976776
	LOSS [training: 0.057637915017849886 | validation: 0.025767235828788912]
	TIME [epoch: 8.32 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04855720806683311		[learning rate: 0.00097562]
		[batch 20/20] avg loss: 0.04771176482817879		[learning rate: 0.00097447]
	Learning Rate: 0.000974472
	LOSS [training: 0.04813448644750595 | validation: 0.04125390677471452]
	TIME [epoch: 8.29 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08157689775328586		[learning rate: 0.00097332]
		[batch 20/20] avg loss: 0.045727916757165796		[learning rate: 0.00097217]
	Learning Rate: 0.000972173
	LOSS [training: 0.06365240725522583 | validation: 0.020369769045512045]
	TIME [epoch: 8.29 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05421019328369055		[learning rate: 0.00097103]
		[batch 20/20] avg loss: 0.048518805791885854		[learning rate: 0.00096988]
	Learning Rate: 0.00096988
	LOSS [training: 0.05136449953778821 | validation: 0.020769271594316175]
	TIME [epoch: 8.3 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07409428068433993		[learning rate: 0.00096874]
		[batch 20/20] avg loss: 0.06739586591917558		[learning rate: 0.00096759]
	Learning Rate: 0.000967592
	LOSS [training: 0.07074507330175775 | validation: 0.028527341683695126]
	TIME [epoch: 8.32 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04900607816345294		[learning rate: 0.00096645]
		[batch 20/20] avg loss: 0.05821045376089572		[learning rate: 0.00096531]
	Learning Rate: 0.00096531
	LOSS [training: 0.05360826596217434 | validation: 0.032748969605742646]
	TIME [epoch: 8.31 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0670999557366874		[learning rate: 0.00096417]
		[batch 20/20] avg loss: 0.06211699017899789		[learning rate: 0.00096303]
	Learning Rate: 0.000963033
	LOSS [training: 0.06460847295784265 | validation: 0.05893723138133441]
	TIME [epoch: 8.3 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08301065029962543		[learning rate: 0.0009619]
		[batch 20/20] avg loss: 0.07267625608602409		[learning rate: 0.00096076]
	Learning Rate: 0.000960761
	LOSS [training: 0.07784345319282478 | validation: 0.04896629573562882]
	TIME [epoch: 8.3 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10035945967629467		[learning rate: 0.00095963]
		[batch 20/20] avg loss: 0.07971858652604699		[learning rate: 0.00095849]
	Learning Rate: 0.000958495
	LOSS [training: 0.09003902310117082 | validation: 0.02948123251720243]
	TIME [epoch: 8.33 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03515445869384119		[learning rate: 0.00095736]
		[batch 20/20] avg loss: 0.09727828781824424		[learning rate: 0.00095623]
	Learning Rate: 0.000956234
	LOSS [training: 0.06621637325604271 | validation: 0.05400136092975604]
	TIME [epoch: 8.29 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05746449510487003		[learning rate: 0.00095511]
		[batch 20/20] avg loss: 0.06308965324129735		[learning rate: 0.00095398]
	Learning Rate: 0.000953978
	LOSS [training: 0.06027707417308368 | validation: 0.11302562904218247]
	TIME [epoch: 8.31 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06414974901637784		[learning rate: 0.00095285]
		[batch 20/20] avg loss: 0.06755439733321032		[learning rate: 0.00095173]
	Learning Rate: 0.000951728
	LOSS [training: 0.06585207317479408 | validation: 0.07471305066666104]
	TIME [epoch: 8.29 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045221615509445416		[learning rate: 0.0009506]
		[batch 20/20] avg loss: 0.05621968054784483		[learning rate: 0.00094948]
	Learning Rate: 0.000949483
	LOSS [training: 0.05072064802864512 | validation: 0.06120976959733059]
	TIME [epoch: 8.33 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057209187326087055		[learning rate: 0.00094836]
		[batch 20/20] avg loss: 0.044938279620748464		[learning rate: 0.00094724]
	Learning Rate: 0.000947243
	LOSS [training: 0.05107373347341776 | validation: 0.0266695103788552]
	TIME [epoch: 8.31 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029312478527039814		[learning rate: 0.00094613]
		[batch 20/20] avg loss: 0.05656658086889427		[learning rate: 0.00094501]
	Learning Rate: 0.000945009
	LOSS [training: 0.04293952969796704 | validation: 0.09953183911448586]
	TIME [epoch: 8.3 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05984328788645484		[learning rate: 0.00094389]
		[batch 20/20] avg loss: 0.05344632839126441		[learning rate: 0.00094278]
	Learning Rate: 0.00094278
	LOSS [training: 0.056644808138859616 | validation: 0.04596368674753835]
	TIME [epoch: 8.31 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04949541356885064		[learning rate: 0.00094167]
		[batch 20/20] avg loss: 0.045382243504454		[learning rate: 0.00094056]
	Learning Rate: 0.000940556
	LOSS [training: 0.04743882853665232 | validation: 0.027552790383687818]
	TIME [epoch: 8.33 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054832992684127925		[learning rate: 0.00093945]
		[batch 20/20] avg loss: 0.04795649511728206		[learning rate: 0.00093834]
	Learning Rate: 0.000938337
	LOSS [training: 0.05139474390070499 | validation: 0.061548675528245424]
	TIME [epoch: 8.31 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043949909451997095		[learning rate: 0.00093723]
		[batch 20/20] avg loss: 0.04149909478513135		[learning rate: 0.00093612]
	Learning Rate: 0.000936124
	LOSS [training: 0.042724502118564225 | validation: 0.10957817570058584]
	TIME [epoch: 8.3 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11531161105104167		[learning rate: 0.00093502]
		[batch 20/20] avg loss: 0.07336658018358373		[learning rate: 0.00093392]
	Learning Rate: 0.000933916
	LOSS [training: 0.0943390956173127 | validation: 0.050948909634405334]
	TIME [epoch: 8.31 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04853594215425887		[learning rate: 0.00093281]
		[batch 20/20] avg loss: 0.06251112179961632		[learning rate: 0.00093171]
	Learning Rate: 0.000931713
	LOSS [training: 0.0555235319769376 | validation: 0.02550679551068667]
	TIME [epoch: 8.32 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07068402261926345		[learning rate: 0.00093061]
		[batch 20/20] avg loss: 0.06743595237246022		[learning rate: 0.00092951]
	Learning Rate: 0.000929515
	LOSS [training: 0.06905998749586181 | validation: 0.05346710557852374]
	TIME [epoch: 8.31 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05239573537690854		[learning rate: 0.00092842]
		[batch 20/20] avg loss: 0.08894725984643756		[learning rate: 0.00092732]
	Learning Rate: 0.000927322
	LOSS [training: 0.07067149761167305 | validation: 0.059663373116527724]
	TIME [epoch: 8.3 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08848597316161297		[learning rate: 0.00092623]
		[batch 20/20] avg loss: 0.05808375504553777		[learning rate: 0.00092513]
	Learning Rate: 0.000925135
	LOSS [training: 0.07328486410357535 | validation: 0.03775514718454211]
	TIME [epoch: 8.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06604373706211529		[learning rate: 0.00092404]
		[batch 20/20] avg loss: 0.05000513143584693		[learning rate: 0.00092295]
	Learning Rate: 0.000922953
	LOSS [training: 0.058024434248981106 | validation: 0.03451057091665522]
	TIME [epoch: 8.32 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07386910653446788		[learning rate: 0.00092186]
		[batch 20/20] avg loss: 0.03682494532115273		[learning rate: 0.00092078]
	Learning Rate: 0.000920776
	LOSS [training: 0.05534702592781031 | validation: 0.04701359541776794]
	TIME [epoch: 8.3 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04953011343974196		[learning rate: 0.00091969]
		[batch 20/20] avg loss: 0.09108428744540162		[learning rate: 0.0009186]
	Learning Rate: 0.000918604
	LOSS [training: 0.0703072004425718 | validation: 0.059453464564522364]
	TIME [epoch: 8.3 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053925062769480794		[learning rate: 0.00091752]
		[batch 20/20] avg loss: 0.05669715859782291		[learning rate: 0.00091644]
	Learning Rate: 0.000916437
	LOSS [training: 0.05531111068365184 | validation: 0.037038966672463675]
	TIME [epoch: 8.3 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046578403210901634		[learning rate: 0.00091536]
		[batch 20/20] avg loss: 0.06281574475181372		[learning rate: 0.00091428]
	Learning Rate: 0.000914275
	LOSS [training: 0.05469707398135767 | validation: 0.04145337726651206]
	TIME [epoch: 8.33 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04819600433313061		[learning rate: 0.0009132]
		[batch 20/20] avg loss: 0.05310165372034057		[learning rate: 0.00091212]
	Learning Rate: 0.000912119
	LOSS [training: 0.050648829026735585 | validation: 0.0755053083454213]
	TIME [epoch: 8.31 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0714456713605489		[learning rate: 0.00091104]
		[batch 20/20] avg loss: 0.08580524305090624		[learning rate: 0.00090997]
	Learning Rate: 0.000909967
	LOSS [training: 0.07862545720572758 | validation: 0.03563751898502045]
	TIME [epoch: 8.3 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05394330355807937		[learning rate: 0.00090889]
		[batch 20/20] avg loss: 0.07257716032151594		[learning rate: 0.00090782]
	Learning Rate: 0.00090782
	LOSS [training: 0.06326023193979766 | validation: 0.04380311969068453]
	TIME [epoch: 8.29 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05868439620334075		[learning rate: 0.00090675]
		[batch 20/20] avg loss: 0.04235950210916954		[learning rate: 0.00090568]
	Learning Rate: 0.000905679
	LOSS [training: 0.05052194915625515 | validation: 0.042492627835767925]
	TIME [epoch: 8.32 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043771209297524816		[learning rate: 0.00090461]
		[batch 20/20] avg loss: 0.054709271252773396		[learning rate: 0.00090354]
	Learning Rate: 0.000903543
	LOSS [training: 0.0492402402751491 | validation: 0.05525261197625392]
	TIME [epoch: 8.3 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06796775535453665		[learning rate: 0.00090248]
		[batch 20/20] avg loss: 0.04676531960740662		[learning rate: 0.00090141]
	Learning Rate: 0.000901411
	LOSS [training: 0.057366537480971626 | validation: 0.023165525289710664]
	TIME [epoch: 8.3 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05912024205091335		[learning rate: 0.00090035]
		[batch 20/20] avg loss: 0.05888122357087735		[learning rate: 0.00089929]
	Learning Rate: 0.000899285
	LOSS [training: 0.05900073281089534 | validation: 0.014548293701079585]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_1070.pth
	Model improved!!!
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04492776313195733		[learning rate: 0.00089822]
		[batch 20/20] avg loss: 0.08538653242223528		[learning rate: 0.00089716]
	Learning Rate: 0.000897164
	LOSS [training: 0.06515714777709629 | validation: 0.06865215152157876]
	TIME [epoch: 8.33 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07397932490838807		[learning rate: 0.00089611]
		[batch 20/20] avg loss: 0.04961292779230856		[learning rate: 0.00089505]
	Learning Rate: 0.000895048
	LOSS [training: 0.06179612635034833 | validation: 0.02846059120778751]
	TIME [epoch: 8.3 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0516215088780129		[learning rate: 0.00089399]
		[batch 20/20] avg loss: 0.07019635556705683		[learning rate: 0.00089294]
	Learning Rate: 0.000892936
	LOSS [training: 0.060908932222534864 | validation: 0.0648838110621684]
	TIME [epoch: 8.3 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05695318190554551		[learning rate: 0.00089188]
		[batch 20/20] avg loss: 0.09865376726680754		[learning rate: 0.00089083]
	Learning Rate: 0.00089083
	LOSS [training: 0.07780347458617654 | validation: 0.048128323550412876]
	TIME [epoch: 8.31 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06819413557105845		[learning rate: 0.00088978]
		[batch 20/20] avg loss: 0.0733482898348701		[learning rate: 0.00088873]
	Learning Rate: 0.000888729
	LOSS [training: 0.0707712127029643 | validation: 0.04638733607949043]
	TIME [epoch: 8.33 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0745322514052687		[learning rate: 0.00088768]
		[batch 20/20] avg loss: 0.053591264616984505		[learning rate: 0.00088663]
	Learning Rate: 0.000886632
	LOSS [training: 0.0640617580111266 | validation: 0.09070804173215169]
	TIME [epoch: 8.31 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062026821091460106		[learning rate: 0.00088559]
		[batch 20/20] avg loss: 0.039823949643196084		[learning rate: 0.00088454]
	Learning Rate: 0.000884541
	LOSS [training: 0.05092538536732809 | validation: 0.03705987098493106]
	TIME [epoch: 8.31 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04941415859273794		[learning rate: 0.0008835]
		[batch 20/20] avg loss: 0.04953859164640249		[learning rate: 0.00088245]
	Learning Rate: 0.000882454
	LOSS [training: 0.04947637511957022 | validation: 0.042819560238212914]
	TIME [epoch: 8.31 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062381209133494674		[learning rate: 0.00088141]
		[batch 20/20] avg loss: 0.06449150671272505		[learning rate: 0.00088037]
	Learning Rate: 0.000880373
	LOSS [training: 0.06343635792310987 | validation: 0.07149235702314787]
	TIME [epoch: 8.33 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048069504769608605		[learning rate: 0.00087933]
		[batch 20/20] avg loss: 0.05777308132100171		[learning rate: 0.0008783]
	Learning Rate: 0.000878296
	LOSS [training: 0.052921293045305164 | validation: 0.04915370019928886]
	TIME [epoch: 8.31 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03743687022012114		[learning rate: 0.00087726]
		[batch 20/20] avg loss: 0.08604076373543926		[learning rate: 0.00087622]
	Learning Rate: 0.000876224
	LOSS [training: 0.0617388169777802 | validation: 0.06028180427174189]
	TIME [epoch: 8.3 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05230497376422536		[learning rate: 0.00087519]
		[batch 20/20] avg loss: 0.05314363637578555		[learning rate: 0.00087416]
	Learning Rate: 0.000874157
	LOSS [training: 0.05272430507000545 | validation: 0.030400260036108878]
	TIME [epoch: 8.31 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06043263167841295		[learning rate: 0.00087313]
		[batch 20/20] avg loss: 0.05703636389215104		[learning rate: 0.0008721]
	Learning Rate: 0.000872096
	LOSS [training: 0.05873449778528199 | validation: 0.0419948799749847]
	TIME [epoch: 8.32 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06078388447307529		[learning rate: 0.00087107]
		[batch 20/20] avg loss: 0.046125454203967874		[learning rate: 0.00087004]
	Learning Rate: 0.000870038
	LOSS [training: 0.05345466933852159 | validation: 0.05286989887547491]
	TIME [epoch: 8.31 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07520954997148263		[learning rate: 0.00086901]
		[batch 20/20] avg loss: 0.05110413959466579		[learning rate: 0.00086799]
	Learning Rate: 0.000867986
	LOSS [training: 0.06315684478307422 | validation: 0.069535699880367]
	TIME [epoch: 8.31 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06224017757234662		[learning rate: 0.00086696]
		[batch 20/20] avg loss: 0.0435931334875579		[learning rate: 0.00086594]
	Learning Rate: 0.000865939
	LOSS [training: 0.05291665552995226 | validation: 0.038308566246893876]
	TIME [epoch: 8.31 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05637500009710479		[learning rate: 0.00086492]
		[batch 20/20] avg loss: 0.05817987196733408		[learning rate: 0.0008639]
	Learning Rate: 0.000863896
	LOSS [training: 0.05727743603221944 | validation: 0.04359496484051924]
	TIME [epoch: 8.33 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04754777258932428		[learning rate: 0.00086288]
		[batch 20/20] avg loss: 0.05696530024029375		[learning rate: 0.00086186]
	Learning Rate: 0.000861858
	LOSS [training: 0.052256536414809006 | validation: 0.03511826105390771]
	TIME [epoch: 8.31 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.063258754590068		[learning rate: 0.00086084]
		[batch 20/20] avg loss: 0.048722917237644006		[learning rate: 0.00085983]
	Learning Rate: 0.000859825
	LOSS [training: 0.05599083591385601 | validation: 0.1047523139903443]
	TIME [epoch: 8.29 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08118820820617427		[learning rate: 0.00085881]
		[batch 20/20] avg loss: 0.04835974390498803		[learning rate: 0.0008578]
	Learning Rate: 0.000857797
	LOSS [training: 0.06477397605558115 | validation: 0.029911461234717492]
	TIME [epoch: 8.31 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04320251863616163		[learning rate: 0.00085678]
		[batch 20/20] avg loss: 0.05836635511407371		[learning rate: 0.00085577]
	Learning Rate: 0.000855774
	LOSS [training: 0.05078443687511767 | validation: 0.051126934610446664]
	TIME [epoch: 8.33 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06803150452876537		[learning rate: 0.00085476]
		[batch 20/20] avg loss: 0.056744133108557124		[learning rate: 0.00085376]
	Learning Rate: 0.000853755
	LOSS [training: 0.06238781881866125 | validation: 0.06658197236473393]
	TIME [epoch: 8.31 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07610072009517659		[learning rate: 0.00085275]
		[batch 20/20] avg loss: 0.06449051566296118		[learning rate: 0.00085174]
	Learning Rate: 0.000851741
	LOSS [training: 0.07029561787906889 | validation: 0.03684463279496523]
	TIME [epoch: 8.31 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04087663178440817		[learning rate: 0.00085074]
		[batch 20/20] avg loss: 0.04124080214168861		[learning rate: 0.00084973]
	Learning Rate: 0.000849732
	LOSS [training: 0.0410587169630484 | validation: 0.048280230221072154]
	TIME [epoch: 8.31 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03131011732223994		[learning rate: 0.00084873]
		[batch 20/20] avg loss: 0.07943136377001139		[learning rate: 0.00084773]
	Learning Rate: 0.000847728
	LOSS [training: 0.055370740546125674 | validation: 0.0341265673106244]
	TIME [epoch: 8.33 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04290311815093791		[learning rate: 0.00084673]
		[batch 20/20] avg loss: 0.042602112003562356		[learning rate: 0.00084573]
	Learning Rate: 0.000845728
	LOSS [training: 0.04275261507725013 | validation: 0.058076315333111625]
	TIME [epoch: 8.3 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06456416688620227		[learning rate: 0.00084473]
		[batch 20/20] avg loss: 0.05869880084379262		[learning rate: 0.00084373]
	Learning Rate: 0.000843733
	LOSS [training: 0.06163148386499745 | validation: 0.027024029859039535]
	TIME [epoch: 8.3 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051991495338439976		[learning rate: 0.00084274]
		[batch 20/20] avg loss: 0.07597038115897567		[learning rate: 0.00084174]
	Learning Rate: 0.000841743
	LOSS [training: 0.06398093824870782 | validation: 0.030053070487765055]
	TIME [epoch: 8.31 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06037842637977566		[learning rate: 0.00084075]
		[batch 20/20] avg loss: 0.04651295564219563		[learning rate: 0.00083976]
	Learning Rate: 0.000839757
	LOSS [training: 0.05344569101098564 | validation: 0.05278871081814419]
	TIME [epoch: 8.33 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04816296124315711		[learning rate: 0.00083877]
		[batch 20/20] avg loss: 0.06398691669193747		[learning rate: 0.00083778]
	Learning Rate: 0.000837777
	LOSS [training: 0.0560749389675473 | validation: 0.029028612660983753]
	TIME [epoch: 8.3 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04004183573676803		[learning rate: 0.00083679]
		[batch 20/20] avg loss: 0.09790739440633228		[learning rate: 0.0008358]
	Learning Rate: 0.0008358
	LOSS [training: 0.06897461507155016 | validation: 0.08106697009581948]
	TIME [epoch: 8.31 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05746785293437149		[learning rate: 0.00083481]
		[batch 20/20] avg loss: 0.07119750091086549		[learning rate: 0.00083383]
	Learning Rate: 0.000833829
	LOSS [training: 0.06433267692261849 | validation: 0.029308879059774282]
	TIME [epoch: 8.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042405682362184595		[learning rate: 0.00083284]
		[batch 20/20] avg loss: 0.05325161453197713		[learning rate: 0.00083186]
	Learning Rate: 0.000831862
	LOSS [training: 0.04782864844708087 | validation: 0.024571115276496077]
	TIME [epoch: 8.32 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05348019906562177		[learning rate: 0.00083088]
		[batch 20/20] avg loss: 0.058453479953636575		[learning rate: 0.0008299]
	Learning Rate: 0.0008299
	LOSS [training: 0.055966839509629177 | validation: 0.041798963677175005]
	TIME [epoch: 8.31 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04291893014306623		[learning rate: 0.00082892]
		[batch 20/20] avg loss: 0.031931151372169994		[learning rate: 0.00082794]
	Learning Rate: 0.000827942
	LOSS [training: 0.03742504075761811 | validation: 0.04221760768735694]
	TIME [epoch: 8.3 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06092243450737224		[learning rate: 0.00082697]
		[batch 20/20] avg loss: 0.05209380834259279		[learning rate: 0.00082599]
	Learning Rate: 0.000825989
	LOSS [training: 0.0565081214249825 | validation: 0.02369523342896745]
	TIME [epoch: 8.31 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049546382018233054		[learning rate: 0.00082501]
		[batch 20/20] avg loss: 0.052105626405682816		[learning rate: 0.00082404]
	Learning Rate: 0.000824041
	LOSS [training: 0.05082600421195793 | validation: 0.036648550034350164]
	TIME [epoch: 8.33 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04976668986186834		[learning rate: 0.00082307]
		[batch 20/20] avg loss: 0.04470766505098593		[learning rate: 0.0008221]
	Learning Rate: 0.000822097
	LOSS [training: 0.04723717745642714 | validation: 0.07503109410988634]
	TIME [epoch: 8.31 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06176161048762393		[learning rate: 0.00082113]
		[batch 20/20] avg loss: 0.05448449380558951		[learning rate: 0.00082016]
	Learning Rate: 0.000820158
	LOSS [training: 0.058123052146606714 | validation: 0.0581601144577733]
	TIME [epoch: 8.3 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04370401886177942		[learning rate: 0.00081919]
		[batch 20/20] avg loss: 0.10479434982781073		[learning rate: 0.00081822]
	Learning Rate: 0.000818223
	LOSS [training: 0.07424918434479508 | validation: 0.0432603240748788]
	TIME [epoch: 8.31 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06814069707696539		[learning rate: 0.00081726]
		[batch 20/20] avg loss: 0.046446967384889806		[learning rate: 0.00081629]
	Learning Rate: 0.000816293
	LOSS [training: 0.0572938322309276 | validation: 0.04261186400768363]
	TIME [epoch: 8.32 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03798228859697313		[learning rate: 0.00081533]
		[batch 20/20] avg loss: 0.050698273394947514		[learning rate: 0.00081437]
	Learning Rate: 0.000814368
	LOSS [training: 0.04434028099596031 | validation: 0.04735612154119475]
	TIME [epoch: 8.3 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03918606430743612		[learning rate: 0.00081341]
		[batch 20/20] avg loss: 0.04488931815686211		[learning rate: 0.00081245]
	Learning Rate: 0.000812447
	LOSS [training: 0.04203769123214912 | validation: 0.04741452179558933]
	TIME [epoch: 8.31 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07340400530887367		[learning rate: 0.00081149]
		[batch 20/20] avg loss: 0.05183673952089416		[learning rate: 0.00081053]
	Learning Rate: 0.00081053
	LOSS [training: 0.06262037241488391 | validation: 0.06011113546696523]
	TIME [epoch: 8.31 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04493558259245364		[learning rate: 0.00080957]
		[batch 20/20] avg loss: 0.05277182102825791		[learning rate: 0.00080862]
	Learning Rate: 0.000808618
	LOSS [training: 0.04885370181035577 | validation: 0.1277217213577182]
	TIME [epoch: 8.33 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08747889432158598		[learning rate: 0.00080766]
		[batch 20/20] avg loss: 0.06709137697455249		[learning rate: 0.00080671]
	Learning Rate: 0.000806711
	LOSS [training: 0.07728513564806924 | validation: 0.07282788305329604]
	TIME [epoch: 8.3 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05150482764808958		[learning rate: 0.00080576]
		[batch 20/20] avg loss: 0.06178582253266923		[learning rate: 0.00080481]
	Learning Rate: 0.000804808
	LOSS [training: 0.05664532509037941 | validation: 0.033712634690143316]
	TIME [epoch: 8.31 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061728097554276454		[learning rate: 0.00080386]
		[batch 20/20] avg loss: 0.06552410560037937		[learning rate: 0.00080291]
	Learning Rate: 0.00080291
	LOSS [training: 0.06362610157732791 | validation: 0.09325864516663748]
	TIME [epoch: 8.31 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06710565102536149		[learning rate: 0.00080196]
		[batch 20/20] avg loss: 0.05130974215516605		[learning rate: 0.00080102]
	Learning Rate: 0.000801016
	LOSS [training: 0.05920769659026377 | validation: 0.05373207593033272]
	TIME [epoch: 8.33 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08051744442521384		[learning rate: 0.00080007]
		[batch 20/20] avg loss: 0.05645484107847469		[learning rate: 0.00079913]
	Learning Rate: 0.000799126
	LOSS [training: 0.06848614275184427 | validation: 0.056426662657534876]
	TIME [epoch: 8.31 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08444513959627356		[learning rate: 0.00079818]
		[batch 20/20] avg loss: 0.05531371445551843		[learning rate: 0.00079724]
	Learning Rate: 0.000797241
	LOSS [training: 0.069879427025896 | validation: 0.1255373281760798]
	TIME [epoch: 8.31 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06601490655397493		[learning rate: 0.0007963]
		[batch 20/20] avg loss: 0.05470082988177625		[learning rate: 0.00079536]
	Learning Rate: 0.000795361
	LOSS [training: 0.0603578682178756 | validation: 0.05509580953866036]
	TIME [epoch: 8.31 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05589168078323594		[learning rate: 0.00079442]
		[batch 20/20] avg loss: 0.060576962395724555		[learning rate: 0.00079348]
	Learning Rate: 0.000793484
	LOSS [training: 0.05823432158948026 | validation: 0.05732527935511177]
	TIME [epoch: 8.32 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04775798829270562		[learning rate: 0.00079255]
		[batch 20/20] avg loss: 0.04465923574195601		[learning rate: 0.00079161]
	Learning Rate: 0.000791613
	LOSS [training: 0.04620861201733083 | validation: 0.03824079931197599]
	TIME [epoch: 8.31 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05377920157273837		[learning rate: 0.00079068]
		[batch 20/20] avg loss: 0.05401928578809284		[learning rate: 0.00078975]
	Learning Rate: 0.000789745
	LOSS [training: 0.0538992436804156 | validation: 0.103840023668283]
	TIME [epoch: 8.3 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08707750312107124		[learning rate: 0.00078881]
		[batch 20/20] avg loss: 0.07446299132587579		[learning rate: 0.00078788]
	Learning Rate: 0.000787882
	LOSS [training: 0.08077024722347352 | validation: 0.0412846075918378]
	TIME [epoch: 8.31 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057372648473577294		[learning rate: 0.00078695]
		[batch 20/20] avg loss: 0.03725160264113428		[learning rate: 0.00078602]
	Learning Rate: 0.000786024
	LOSS [training: 0.04731212555735579 | validation: 0.03816998469854219]
	TIME [epoch: 8.33 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05249447899911954		[learning rate: 0.0007851]
		[batch 20/20] avg loss: 0.06218802114136344		[learning rate: 0.00078417]
	Learning Rate: 0.00078417
	LOSS [training: 0.05734125007024149 | validation: 0.07988522345723204]
	TIME [epoch: 8.3 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054877798893697184		[learning rate: 0.00078324]
		[batch 20/20] avg loss: 0.05517454843899381		[learning rate: 0.00078232]
	Learning Rate: 0.00078232
	LOSS [training: 0.05502617366634551 | validation: 0.08096708878318648]
	TIME [epoch: 8.3 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054314473128615405		[learning rate: 0.0007814]
		[batch 20/20] avg loss: 0.04308552767118778		[learning rate: 0.00078047]
	Learning Rate: 0.000780475
	LOSS [training: 0.0487000003999016 | validation: 0.07037413875077819]
	TIME [epoch: 8.31 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06707218508911378		[learning rate: 0.00077955]
		[batch 20/20] avg loss: 0.0522516908998754		[learning rate: 0.00077863]
	Learning Rate: 0.000778634
	LOSS [training: 0.059661937994494596 | validation: 0.058424022335147645]
	TIME [epoch: 8.33 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04895463894636346		[learning rate: 0.00077772]
		[batch 20/20] avg loss: 0.06655774576758217		[learning rate: 0.0007768]
	Learning Rate: 0.000776797
	LOSS [training: 0.05775619235697281 | validation: 0.024116515564203904]
	TIME [epoch: 8.31 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056257964180684614		[learning rate: 0.00077588]
		[batch 20/20] avg loss: 0.033444193610368766		[learning rate: 0.00077496]
	Learning Rate: 0.000774965
	LOSS [training: 0.04485107889552669 | validation: 0.06483918705368227]
	TIME [epoch: 8.31 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045260444146435		[learning rate: 0.00077405]
		[batch 20/20] avg loss: 0.05887070962687389		[learning rate: 0.00077314]
	Learning Rate: 0.000773137
	LOSS [training: 0.052065576886654454 | validation: 0.09932829541388953]
	TIME [epoch: 8.3 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09927743038230938		[learning rate: 0.00077222]
		[batch 20/20] avg loss: 0.057999050140865274		[learning rate: 0.00077131]
	Learning Rate: 0.000771313
	LOSS [training: 0.0786382402615873 | validation: 0.06588793144535682]
	TIME [epoch: 8.33 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049728674885080155		[learning rate: 0.0007704]
		[batch 20/20] avg loss: 0.05328463628728612		[learning rate: 0.00076949]
	Learning Rate: 0.000769494
	LOSS [training: 0.051506655586183135 | validation: 0.04330773082641113]
	TIME [epoch: 8.3 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08183682238904258		[learning rate: 0.00076859]
		[batch 20/20] avg loss: 0.07273474388965524		[learning rate: 0.00076768]
	Learning Rate: 0.000767679
	LOSS [training: 0.07728578313934889 | validation: 0.03862999692143664]
	TIME [epoch: 8.3 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08047796921134029		[learning rate: 0.00076677]
		[batch 20/20] avg loss: 0.06049887723468319		[learning rate: 0.00076587]
	Learning Rate: 0.000765868
	LOSS [training: 0.07048842322301174 | validation: 0.041548576278264096]
	TIME [epoch: 8.3 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054352698426487954		[learning rate: 0.00076496]
		[batch 20/20] avg loss: 0.042619700842101864		[learning rate: 0.00076406]
	Learning Rate: 0.000764061
	LOSS [training: 0.0484861996342949 | validation: 0.019760525546809078]
	TIME [epoch: 8.33 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04888198978500435		[learning rate: 0.00076316]
		[batch 20/20] avg loss: 0.07060810068024723		[learning rate: 0.00076226]
	Learning Rate: 0.000762259
	LOSS [training: 0.059745045232625804 | validation: 0.0929254162185311]
	TIME [epoch: 8.3 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07504459478828897		[learning rate: 0.00076136]
		[batch 20/20] avg loss: 0.04475869446995691		[learning rate: 0.00076046]
	Learning Rate: 0.000760461
	LOSS [training: 0.05990164462912295 | validation: 0.01827403564863963]
	TIME [epoch: 8.3 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04150088761210666		[learning rate: 0.00075956]
		[batch 20/20] avg loss: 0.05604434915851649		[learning rate: 0.00075867]
	Learning Rate: 0.000758667
	LOSS [training: 0.04877261838531157 | validation: 0.0418292229570691]
	TIME [epoch: 8.31 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05124499430678897		[learning rate: 0.00075777]
		[batch 20/20] avg loss: 0.0565038018093321		[learning rate: 0.00075688]
	Learning Rate: 0.000756877
	LOSS [training: 0.053874398058060526 | validation: 0.10104945724225255]
	TIME [epoch: 8.32 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.065553173488983		[learning rate: 0.00075598]
		[batch 20/20] avg loss: 0.12010453716127097		[learning rate: 0.00075509]
	Learning Rate: 0.000755092
	LOSS [training: 0.09282885532512698 | validation: 0.04024559017382763]
	TIME [epoch: 8.31 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040874126863124284		[learning rate: 0.0007542]
		[batch 20/20] avg loss: 0.051162097036964324		[learning rate: 0.00075331]
	Learning Rate: 0.000753311
	LOSS [training: 0.046018111950044294 | validation: 0.043574102862728836]
	TIME [epoch: 8.31 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04414877533904392		[learning rate: 0.00075242]
		[batch 20/20] avg loss: 0.04336588779091722		[learning rate: 0.00075153]
	Learning Rate: 0.000751534
	LOSS [training: 0.043757331564980566 | validation: 0.05767287539054715]
	TIME [epoch: 8.31 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07486148521713057		[learning rate: 0.00075065]
		[batch 20/20] avg loss: 0.058113508169982495		[learning rate: 0.00074976]
	Learning Rate: 0.000749761
	LOSS [training: 0.06648749669355654 | validation: 0.03064846296734063]
	TIME [epoch: 8.32 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040219172204150076		[learning rate: 0.00074888]
		[batch 20/20] avg loss: 0.07949421708289447		[learning rate: 0.00074799]
	Learning Rate: 0.000747993
	LOSS [training: 0.059856694643522265 | validation: 0.018665969370207095]
	TIME [epoch: 8.3 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04565627972116473		[learning rate: 0.00074711]
		[batch 20/20] avg loss: 0.07184147857984616		[learning rate: 0.00074623]
	Learning Rate: 0.000746228
	LOSS [training: 0.05874887915050544 | validation: 0.05450827588985797]
	TIME [epoch: 8.3 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053936578397355786		[learning rate: 0.00074535]
		[batch 20/20] avg loss: 0.050999822265031435		[learning rate: 0.00074447]
	Learning Rate: 0.000744468
	LOSS [training: 0.05246820033119361 | validation: 0.10537221462838425]
	TIME [epoch: 8.31 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06472385412945081		[learning rate: 0.00074359]
		[batch 20/20] avg loss: 0.044013888374525266		[learning rate: 0.00074271]
	Learning Rate: 0.000742712
	LOSS [training: 0.05436887125198804 | validation: 0.06141949345066035]
	TIME [epoch: 8.33 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04589864459381206		[learning rate: 0.00074184]
		[batch 20/20] avg loss: 0.044273575189207964		[learning rate: 0.00074096]
	Learning Rate: 0.00074096
	LOSS [training: 0.04508610989151001 | validation: 0.022463779029366845]
	TIME [epoch: 8.3 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07571394562069822		[learning rate: 0.00074009]
		[batch 20/20] avg loss: 0.0434067114432106		[learning rate: 0.00073921]
	Learning Rate: 0.000739212
	LOSS [training: 0.05956032853195441 | validation: 0.019871832552041304]
	TIME [epoch: 8.3 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044949199414632414		[learning rate: 0.00073834]
		[batch 20/20] avg loss: 0.0459412099601486		[learning rate: 0.00073747]
	Learning Rate: 0.000737469
	LOSS [training: 0.04544520468739051 | validation: 0.06355671763197945]
	TIME [epoch: 8.31 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03850808881529673		[learning rate: 0.0007366]
		[batch 20/20] avg loss: 0.042845021042715134		[learning rate: 0.00073573]
	Learning Rate: 0.000735729
	LOSS [training: 0.040676554929005924 | validation: 0.04132613924711094]
	TIME [epoch: 8.32 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054248981242447106		[learning rate: 0.00073486]
		[batch 20/20] avg loss: 0.036899507301494425		[learning rate: 0.00073399]
	Learning Rate: 0.000733994
	LOSS [training: 0.04557424427197077 | validation: 0.03813861733059804]
	TIME [epoch: 8.3 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060291375022727335		[learning rate: 0.00073313]
		[batch 20/20] avg loss: 0.04142683491143267		[learning rate: 0.00073226]
	Learning Rate: 0.000732262
	LOSS [training: 0.050859104967079995 | validation: 0.025596763926443007]
	TIME [epoch: 8.3 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03645885571193884		[learning rate: 0.0007314]
		[batch 20/20] avg loss: 0.050033977952101086		[learning rate: 0.00073054]
	Learning Rate: 0.000730535
	LOSS [training: 0.04324641683201996 | validation: 0.04135162975515048]
	TIME [epoch: 8.31 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050325828661160976		[learning rate: 0.00072967]
		[batch 20/20] avg loss: 0.04876430097298425		[learning rate: 0.00072881]
	Learning Rate: 0.000728812
	LOSS [training: 0.04954506481707261 | validation: 0.032941342514039476]
	TIME [epoch: 8.32 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03946317502364508		[learning rate: 0.00072795]
		[batch 20/20] avg loss: 0.03887239916448311		[learning rate: 0.00072709]
	Learning Rate: 0.000727093
	LOSS [training: 0.03916778709406409 | validation: 0.07623870179434772]
	TIME [epoch: 8.3 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061293785969918566		[learning rate: 0.00072623]
		[batch 20/20] avg loss: 0.04221360778509146		[learning rate: 0.00072538]
	Learning Rate: 0.000725378
	LOSS [training: 0.051753696877505015 | validation: 0.030386604523443772]
	TIME [epoch: 8.3 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06264952683602701		[learning rate: 0.00072452]
		[batch 20/20] avg loss: 0.05744372445291818		[learning rate: 0.00072367]
	Learning Rate: 0.000723666
	LOSS [training: 0.060046625644472586 | validation: 0.02161150821069906]
	TIME [epoch: 8.31 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03435132746714271		[learning rate: 0.00072281]
		[batch 20/20] avg loss: 0.06666559948714065		[learning rate: 0.00072196]
	Learning Rate: 0.000721959
	LOSS [training: 0.050508463477141684 | validation: 0.017481063226255745]
	TIME [epoch: 8.32 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05175528795566059		[learning rate: 0.00072111]
		[batch 20/20] avg loss: 0.03914776827131718		[learning rate: 0.00072026]
	Learning Rate: 0.000720257
	LOSS [training: 0.04545152811348889 | validation: 0.04818634556094853]
	TIME [epoch: 8.3 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046235530985730175		[learning rate: 0.00071941]
		[batch 20/20] avg loss: 0.06454310445733913		[learning rate: 0.00071856]
	Learning Rate: 0.000718557
	LOSS [training: 0.055389317721534645 | validation: 0.028833532039678537]
	TIME [epoch: 8.32 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06119119811627347		[learning rate: 0.00071771]
		[batch 20/20] avg loss: 0.05516890042448304		[learning rate: 0.00071686]
	Learning Rate: 0.000716863
	LOSS [training: 0.05818004927037825 | validation: 0.06294032420089292]
	TIME [epoch: 8.31 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038634567678921664		[learning rate: 0.00071602]
		[batch 20/20] avg loss: 0.031228528748646796		[learning rate: 0.00071517]
	Learning Rate: 0.000715172
	LOSS [training: 0.03493154821378423 | validation: 0.10442803379013163]
	TIME [epoch: 8.32 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06360228939781232		[learning rate: 0.00071433]
		[batch 20/20] avg loss: 0.052523873432879876		[learning rate: 0.00071348]
	Learning Rate: 0.000713485
	LOSS [training: 0.05806308141534611 | validation: 0.10034367073928757]
	TIME [epoch: 8.31 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050350379650537694		[learning rate: 0.00071264]
		[batch 20/20] avg loss: 0.045578533232217		[learning rate: 0.0007118]
	Learning Rate: 0.000711802
	LOSS [training: 0.047964456441377354 | validation: 0.033707221220627775]
	TIME [epoch: 8.3 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051825221714261925		[learning rate: 0.00071096]
		[batch 20/20] avg loss: 0.04396726231912536		[learning rate: 0.00071012]
	Learning Rate: 0.000710123
	LOSS [training: 0.04789624201669364 | validation: 0.030959257954555203]
	TIME [epoch: 8.32 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.067495215594397		[learning rate: 0.00070928]
		[batch 20/20] avg loss: 0.04120008765860952		[learning rate: 0.00070845]
	Learning Rate: 0.000708448
	LOSS [training: 0.05434765162650325 | validation: 0.053132586893944206]
	TIME [epoch: 8.32 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05672394265034976		[learning rate: 0.00070761]
		[batch 20/20] avg loss: 0.04100016320214182		[learning rate: 0.00070678]
	Learning Rate: 0.000706776
	LOSS [training: 0.0488620529262458 | validation: 0.03907804279403673]
	TIME [epoch: 8.31 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06923539130921424		[learning rate: 0.00070594]
		[batch 20/20] avg loss: 0.05090260282839595		[learning rate: 0.00070511]
	Learning Rate: 0.000705109
	LOSS [training: 0.0600689970688051 | validation: 0.07472404932515037]
	TIME [epoch: 8.3 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06520044122218689		[learning rate: 0.00070428]
		[batch 20/20] avg loss: 0.06013724574579438		[learning rate: 0.00070345]
	Learning Rate: 0.000703446
	LOSS [training: 0.06266884348399061 | validation: 0.035813515287613173]
	TIME [epoch: 8.31 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047175514475759836		[learning rate: 0.00070262]
		[batch 20/20] avg loss: 0.054925792641077574		[learning rate: 0.00070179]
	Learning Rate: 0.000701787
	LOSS [training: 0.0510506535584187 | validation: 0.028601121329864747]
	TIME [epoch: 8.32 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05521111475985117		[learning rate: 0.00070096]
		[batch 20/20] avg loss: 0.048781460267339274		[learning rate: 0.00070013]
	Learning Rate: 0.000700131
	LOSS [training: 0.05199628751359521 | validation: 0.028244519050392655]
	TIME [epoch: 8.3 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03801616988144138		[learning rate: 0.00069931]
		[batch 20/20] avg loss: 0.059302347962797354		[learning rate: 0.00069848]
	Learning Rate: 0.00069848
	LOSS [training: 0.04865925892211938 | validation: 0.028232778374212158]
	TIME [epoch: 8.3 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06999636029713877		[learning rate: 0.00069766]
		[batch 20/20] avg loss: 0.05775388402929547		[learning rate: 0.00069683]
	Learning Rate: 0.000696832
	LOSS [training: 0.0638751221632171 | validation: 0.027347489996549165]
	TIME [epoch: 8.31 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05045278602148952		[learning rate: 0.00069601]
		[batch 20/20] avg loss: 0.05375929042961243		[learning rate: 0.00069519]
	Learning Rate: 0.000695188
	LOSS [training: 0.052106038225550974 | validation: 0.08187543296832818]
	TIME [epoch: 8.32 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058441108467660895		[learning rate: 0.00069437]
		[batch 20/20] avg loss: 0.03063893978671146		[learning rate: 0.00069355]
	Learning Rate: 0.000693549
	LOSS [training: 0.044540024127186176 | validation: 0.030386222797976172]
	TIME [epoch: 8.3 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026647734178700172		[learning rate: 0.00069273]
		[batch 20/20] avg loss: 0.06484096805376567		[learning rate: 0.00069191]
	Learning Rate: 0.000691913
	LOSS [training: 0.04574435111623292 | validation: 0.035840199121066146]
	TIME [epoch: 8.3 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04329007459587765		[learning rate: 0.0006911]
		[batch 20/20] avg loss: 0.062039501241114646		[learning rate: 0.00069028]
	Learning Rate: 0.00069028
	LOSS [training: 0.05266478791849616 | validation: 0.022289563685892147]
	TIME [epoch: 8.31 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042788625657708315		[learning rate: 0.00068947]
		[batch 20/20] avg loss: 0.04201071950718225		[learning rate: 0.00068865]
	Learning Rate: 0.000688652
	LOSS [training: 0.04239967258244528 | validation: 0.03854697097812818]
	TIME [epoch: 8.32 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03640282099799302		[learning rate: 0.00068784]
		[batch 20/20] avg loss: 0.06762445930378577		[learning rate: 0.00068703]
	Learning Rate: 0.000687028
	LOSS [training: 0.052013640150889405 | validation: 0.07902983069487352]
	TIME [epoch: 8.3 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06647478167046185		[learning rate: 0.00068622]
		[batch 20/20] avg loss: 0.05549753470977738		[learning rate: 0.00068541]
	Learning Rate: 0.000685407
	LOSS [training: 0.06098615819011961 | validation: 0.14294163307025862]
	TIME [epoch: 8.29 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0765048779043487		[learning rate: 0.0006846]
		[batch 20/20] avg loss: 0.04035394545569436		[learning rate: 0.00068379]
	Learning Rate: 0.00068379
	LOSS [training: 0.058429411680021526 | validation: 0.021792737122066544]
	TIME [epoch: 8.31 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048049387602212085		[learning rate: 0.00068298]
		[batch 20/20] avg loss: 0.05260867785757609		[learning rate: 0.00068218]
	Learning Rate: 0.000682178
	LOSS [training: 0.0503290327298941 | validation: 0.038019702786376584]
	TIME [epoch: 8.33 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033856369542913066		[learning rate: 0.00068137]
		[batch 20/20] avg loss: 0.04468130495040608		[learning rate: 0.00068057]
	Learning Rate: 0.000680568
	LOSS [training: 0.03926883724665957 | validation: 0.027342515972203826]
	TIME [epoch: 8.3 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062710732879446		[learning rate: 0.00067977]
		[batch 20/20] avg loss: 0.03370428744964426		[learning rate: 0.00067896]
	Learning Rate: 0.000678963
	LOSS [training: 0.04820751016454512 | validation: 0.09005102218757183]
	TIME [epoch: 8.29 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057239344799595006		[learning rate: 0.00067816]
		[batch 20/20] avg loss: 0.04347898304324103		[learning rate: 0.00067736]
	Learning Rate: 0.000677362
	LOSS [training: 0.05035916392141802 | validation: 0.03244429744136605]
	TIME [epoch: 8.32 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04654959673349957		[learning rate: 0.00067656]
		[batch 20/20] avg loss: 0.04225962907714988		[learning rate: 0.00067576]
	Learning Rate: 0.000675764
	LOSS [training: 0.04440461290532472 | validation: 0.03138219857452679]
	TIME [epoch: 8.31 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037110984003144915		[learning rate: 0.00067497]
		[batch 20/20] avg loss: 0.05335863200584669		[learning rate: 0.00067417]
	Learning Rate: 0.00067417
	LOSS [training: 0.045234808004495794 | validation: 0.05293907352711997]
	TIME [epoch: 8.3 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039578639476173495		[learning rate: 0.00067337]
		[batch 20/20] avg loss: 0.052880791633472445		[learning rate: 0.00067258]
	Learning Rate: 0.000672579
	LOSS [training: 0.046229715554822984 | validation: 0.033574794601902186]
	TIME [epoch: 8.29 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04745635370867217		[learning rate: 0.00067179]
		[batch 20/20] avg loss: 0.04254074137007538		[learning rate: 0.00067099]
	Learning Rate: 0.000670993
	LOSS [training: 0.04499854753937378 | validation: 0.055947709672702794]
	TIME [epoch: 8.31 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047802306944672544		[learning rate: 0.0006702]
		[batch 20/20] avg loss: 0.03626928502440942		[learning rate: 0.00066941]
	Learning Rate: 0.00066941
	LOSS [training: 0.04203579598454098 | validation: 0.03787833320782724]
	TIME [epoch: 8.31 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037732561565986714		[learning rate: 0.00066862]
		[batch 20/20] avg loss: 0.0526287269613271		[learning rate: 0.00066783]
	Learning Rate: 0.000667831
	LOSS [training: 0.04518064426365691 | validation: 0.12091387155648603]
	TIME [epoch: 8.3 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04825497213119438		[learning rate: 0.00066704]
		[batch 20/20] avg loss: 0.05625432391083598		[learning rate: 0.00066626]
	Learning Rate: 0.000666256
	LOSS [training: 0.05225464802101517 | validation: 0.03770873109415023]
	TIME [epoch: 8.3 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05168907631288335		[learning rate: 0.00066547]
		[batch 20/20] avg loss: 0.055175958692658324		[learning rate: 0.00066468]
	Learning Rate: 0.000664684
	LOSS [training: 0.053432517502770824 | validation: 0.07074331364243959]
	TIME [epoch: 8.32 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06409942646887626		[learning rate: 0.0006639]
		[batch 20/20] avg loss: 0.04129938495349689		[learning rate: 0.00066312]
	Learning Rate: 0.000663116
	LOSS [training: 0.05269940571118657 | validation: 0.09823342617609523]
	TIME [epoch: 8.31 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0567599324448958		[learning rate: 0.00066233]
		[batch 20/20] avg loss: 0.024499686352886324		[learning rate: 0.00066155]
	Learning Rate: 0.000661552
	LOSS [training: 0.040629809398891055 | validation: 0.026743723114918717]
	TIME [epoch: 8.3 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048730861575693896		[learning rate: 0.00066077]
		[batch 20/20] avg loss: 0.04879506653279015		[learning rate: 0.00065999]
	Learning Rate: 0.000659992
	LOSS [training: 0.048762964054242026 | validation: 0.028139215753467752]
	TIME [epoch: 8.3 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04295226310683757		[learning rate: 0.00065921]
		[batch 20/20] avg loss: 0.035232014904238815		[learning rate: 0.00065843]
	Learning Rate: 0.000658435
	LOSS [training: 0.03909213900553819 | validation: 0.026947477225878435]
	TIME [epoch: 8.31 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059521174821371384		[learning rate: 0.00065766]
		[batch 20/20] avg loss: 0.07140380890880474		[learning rate: 0.00065688]
	Learning Rate: 0.000656882
	LOSS [training: 0.06546249186508805 | validation: 0.03823568821204712]
	TIME [epoch: 8.31 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04630975455160419		[learning rate: 0.00065611]
		[batch 20/20] avg loss: 0.05397304324893071		[learning rate: 0.00065533]
	Learning Rate: 0.000655332
	LOSS [training: 0.050141398900267455 | validation: 0.04233578331435146]
	TIME [epoch: 8.3 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0624086375106536		[learning rate: 0.00065456]
		[batch 20/20] avg loss: 0.05811428408052878		[learning rate: 0.00065379]
	Learning Rate: 0.000653786
	LOSS [training: 0.06026146079559118 | validation: 0.03415594935989448]
	TIME [epoch: 8.3 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03599856421364631		[learning rate: 0.00065301]
		[batch 20/20] avg loss: 0.030510558557694446		[learning rate: 0.00065224]
	Learning Rate: 0.000652244
	LOSS [training: 0.03325456138567037 | validation: 0.03317517406209843]
	TIME [epoch: 8.32 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05546371744809455		[learning rate: 0.00065147]
		[batch 20/20] avg loss: 0.05249776132072265		[learning rate: 0.00065071]
	Learning Rate: 0.000650706
	LOSS [training: 0.0539807393844086 | validation: 0.06576086410209461]
	TIME [epoch: 8.31 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04892133402660276		[learning rate: 0.00064994]
		[batch 20/20] avg loss: 0.036636829214219124		[learning rate: 0.00064917]
	Learning Rate: 0.000649171
	LOSS [training: 0.04277908162041095 | validation: 0.031126441266669504]
	TIME [epoch: 8.3 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03497952308963369		[learning rate: 0.0006484]
		[batch 20/20] avg loss: 0.054108915546738		[learning rate: 0.00064764]
	Learning Rate: 0.000647639
	LOSS [training: 0.04454421931818585 | validation: 0.06727734163931656]
	TIME [epoch: 8.3 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046129578853315825		[learning rate: 0.00064688]
		[batch 20/20] avg loss: 0.03231291830587506		[learning rate: 0.00064611]
	Learning Rate: 0.000646112
	LOSS [training: 0.03922124857959545 | validation: 0.03971176792060985]
	TIME [epoch: 8.31 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044332595542801015		[learning rate: 0.00064535]
		[batch 20/20] avg loss: 0.05465539041230279		[learning rate: 0.00064459]
	Learning Rate: 0.000644588
	LOSS [training: 0.0494939929775519 | validation: 0.01778585237933436]
	TIME [epoch: 8.31 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0380747657243357		[learning rate: 0.00064383]
		[batch 20/20] avg loss: 0.04290907590638036		[learning rate: 0.00064307]
	Learning Rate: 0.000643067
	LOSS [training: 0.040491920815358025 | validation: 0.03732266932776446]
	TIME [epoch: 8.3 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060302111938658046		[learning rate: 0.00064231]
		[batch 20/20] avg loss: 0.049643991440000576		[learning rate: 0.00064155]
	Learning Rate: 0.00064155
	LOSS [training: 0.05497305168932931 | validation: 0.03602886750365206]
	TIME [epoch: 8.3 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03975999584263333		[learning rate: 0.00064079]
		[batch 20/20] avg loss: 0.04736482743363281		[learning rate: 0.00064004]
	Learning Rate: 0.000640037
	LOSS [training: 0.043562411638133076 | validation: 0.05454841824946312]
	TIME [epoch: 8.32 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05677815020353644		[learning rate: 0.00063928]
		[batch 20/20] avg loss: 0.048533458830105916		[learning rate: 0.00063853]
	Learning Rate: 0.000638527
	LOSS [training: 0.052655804516821184 | validation: 0.07051983544495762]
	TIME [epoch: 8.32 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052500194332834337		[learning rate: 0.00063777]
		[batch 20/20] avg loss: 0.03183325956721182		[learning rate: 0.00063702]
	Learning Rate: 0.000637021
	LOSS [training: 0.04216672695002308 | validation: 0.05852124631807068]
	TIME [epoch: 8.31 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06543426844777868		[learning rate: 0.00063627]
		[batch 20/20] avg loss: 0.044278455739260855		[learning rate: 0.00063552]
	Learning Rate: 0.000635519
	LOSS [training: 0.054856362093519774 | validation: 0.03976958126028749]
	TIME [epoch: 8.3 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06594856689463237		[learning rate: 0.00063477]
		[batch 20/20] avg loss: 0.04527462217051848		[learning rate: 0.00063402]
	Learning Rate: 0.000634019
	LOSS [training: 0.05561159453257543 | validation: 0.04282934338683249]
	TIME [epoch: 8.32 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04897613366470813		[learning rate: 0.00063327]
		[batch 20/20] avg loss: 0.06001862265868533		[learning rate: 0.00063252]
	Learning Rate: 0.000632524
	LOSS [training: 0.05449737816169673 | validation: 0.08874605639800828]
	TIME [epoch: 8.31 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04545439469738436		[learning rate: 0.00063178]
		[batch 20/20] avg loss: 0.040239910688970815		[learning rate: 0.00063103]
	Learning Rate: 0.000631032
	LOSS [training: 0.042847152693177584 | validation: 0.029139597946128543]
	TIME [epoch: 8.31 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04404345530039604		[learning rate: 0.00063029]
		[batch 20/20] avg loss: 0.04222369796120047		[learning rate: 0.00062954]
	Learning Rate: 0.000629543
	LOSS [training: 0.04313357663079824 | validation: 0.0645039394082198]
	TIME [epoch: 8.3 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052256646984814435		[learning rate: 0.0006288]
		[batch 20/20] avg loss: 0.03828960148879777		[learning rate: 0.00062806]
	Learning Rate: 0.000628058
	LOSS [training: 0.04527312423680611 | validation: 0.032739823523753195]
	TIME [epoch: 8.31 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030543239320686717		[learning rate: 0.00062732]
		[batch 20/20] avg loss: 0.03949661020951284		[learning rate: 0.00062658]
	Learning Rate: 0.000626577
	LOSS [training: 0.035019924765099776 | validation: 0.12713827623567298]
	TIME [epoch: 8.32 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07509083105759927		[learning rate: 0.00062584]
		[batch 20/20] avg loss: 0.080883991723072		[learning rate: 0.0006251]
	Learning Rate: 0.000625099
	LOSS [training: 0.07798741139033563 | validation: 0.047116609745145976]
	TIME [epoch: 8.29 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0547373155482043		[learning rate: 0.00062436]
		[batch 20/20] avg loss: 0.0579669151124836		[learning rate: 0.00062362]
	Learning Rate: 0.000623624
	LOSS [training: 0.056352115330343955 | validation: 0.06071964585902045]
	TIME [epoch: 8.3 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04542271761028177		[learning rate: 0.00062289]
		[batch 20/20] avg loss: 0.03842273028916195		[learning rate: 0.00062215]
	Learning Rate: 0.000622153
	LOSS [training: 0.04192272394972186 | validation: 0.022722734641393214]
	TIME [epoch: 8.31 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07099619438251989		[learning rate: 0.00062142]
		[batch 20/20] avg loss: 0.04845235699748457		[learning rate: 0.00062069]
	Learning Rate: 0.000620686
	LOSS [training: 0.059724275690002236 | validation: 0.028524360813773405]
	TIME [epoch: 8.31 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03917320589246188		[learning rate: 0.00061995]
		[batch 20/20] avg loss: 0.03912367371987647		[learning rate: 0.00061922]
	Learning Rate: 0.000619222
	LOSS [training: 0.03914843980616917 | validation: 0.018410565370744335]
	TIME [epoch: 8.29 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043443356813761644		[learning rate: 0.00061849]
		[batch 20/20] avg loss: 0.04669659521522335		[learning rate: 0.00061776]
	Learning Rate: 0.000617761
	LOSS [training: 0.04506997601449249 | validation: 0.03065946413296423]
	TIME [epoch: 8.31 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04208316479253084		[learning rate: 0.00061703]
		[batch 20/20] avg loss: 0.03586799150655263		[learning rate: 0.0006163]
	Learning Rate: 0.000616304
	LOSS [training: 0.03897557814954174 | validation: 0.068130222479717]
	TIME [epoch: 8.31 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06692170695599922		[learning rate: 0.00061558]
		[batch 20/20] avg loss: 0.04804385780850434		[learning rate: 0.00061485]
	Learning Rate: 0.00061485
	LOSS [training: 0.05748278238225178 | validation: 0.031598573146844514]
	TIME [epoch: 8.32 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047254658120117865		[learning rate: 0.00061412]
		[batch 20/20] avg loss: 0.046736427584600164		[learning rate: 0.0006134]
	Learning Rate: 0.0006134
	LOSS [training: 0.04699554285235902 | validation: 0.03933982527574809]
	TIME [epoch: 8.3 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04008201803722018		[learning rate: 0.00061268]
		[batch 20/20] avg loss: 0.03853255259700217		[learning rate: 0.00061195]
	Learning Rate: 0.000611953
	LOSS [training: 0.039307285317111175 | validation: 0.028389264583193692]
	TIME [epoch: 8.3 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04000416049914161		[learning rate: 0.00061123]
		[batch 20/20] avg loss: 0.04819994643192555		[learning rate: 0.00061051]
	Learning Rate: 0.000610509
	LOSS [training: 0.04410205346553358 | validation: 0.03305931924948819]
	TIME [epoch: 8.32 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05767838231570459		[learning rate: 0.00060979]
		[batch 20/20] avg loss: 0.03696307947270973		[learning rate: 0.00060907]
	Learning Rate: 0.000609069
	LOSS [training: 0.04732073089420716 | validation: 0.06723041108213221]
	TIME [epoch: 8.31 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04253322004754237		[learning rate: 0.00060835]
		[batch 20/20] avg loss: 0.022796999786208517		[learning rate: 0.00060763]
	Learning Rate: 0.000607632
	LOSS [training: 0.03266510991687544 | validation: 0.02516590107137733]
	TIME [epoch: 8.3 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041167461716045003		[learning rate: 0.00060692]
		[batch 20/20] avg loss: 0.06387506534782239		[learning rate: 0.0006062]
	Learning Rate: 0.000606199
	LOSS [training: 0.052521263531933704 | validation: 0.05736235330597855]
	TIME [epoch: 8.29 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04218299701917823		[learning rate: 0.00060548]
		[batch 20/20] avg loss: 0.060983515925570976		[learning rate: 0.00060477]
	Learning Rate: 0.000604769
	LOSS [training: 0.051583256472374606 | validation: 0.08170315335196994]
	TIME [epoch: 8.31 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03981860034522315		[learning rate: 0.00060406]
		[batch 20/20] avg loss: 0.04398358266490328		[learning rate: 0.00060334]
	Learning Rate: 0.000603343
	LOSS [training: 0.04190109150506322 | validation: 0.04741286020103884]
	TIME [epoch: 8.3 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039632444849270435		[learning rate: 0.00060263]
		[batch 20/20] avg loss: 0.026623588119891028		[learning rate: 0.00060192]
	Learning Rate: 0.00060192
	LOSS [training: 0.03312801648458073 | validation: 0.032158750857367094]
	TIME [epoch: 8.3 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03983904888718774		[learning rate: 0.00060121]
		[batch 20/20] avg loss: 0.05365399818074936		[learning rate: 0.0006005]
	Learning Rate: 0.0006005
	LOSS [training: 0.046746523533968555 | validation: 0.047752672660698964]
	TIME [epoch: 8.3 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03871118044926712		[learning rate: 0.00059979]
		[batch 20/20] avg loss: 0.054542003779710566		[learning rate: 0.00059908]
	Learning Rate: 0.000599083
	LOSS [training: 0.046626592114488837 | validation: 0.022090566399059475]
	TIME [epoch: 8.31 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041891789585372374		[learning rate: 0.00059838]
		[batch 20/20] avg loss: 0.042738277349706914		[learning rate: 0.00059767]
	Learning Rate: 0.00059767
	LOSS [training: 0.04231503346753965 | validation: 0.033659178736641905]
	TIME [epoch: 8.31 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04270363054580088		[learning rate: 0.00059696]
		[batch 20/20] avg loss: 0.06825993559653562		[learning rate: 0.00059626]
	Learning Rate: 0.00059626
	LOSS [training: 0.05548178307116826 | validation: 0.018321455599615448]
	TIME [epoch: 8.29 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03557575631159447		[learning rate: 0.00059556]
		[batch 20/20] avg loss: 0.031292456909568216		[learning rate: 0.00059485]
	Learning Rate: 0.000594854
	LOSS [training: 0.03343410661058135 | validation: 0.02262530134740604]
	TIME [epoch: 8.29 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03438119202931149		[learning rate: 0.00059415]
		[batch 20/20] avg loss: 0.032682237113904264		[learning rate: 0.00059345]
	Learning Rate: 0.000593451
	LOSS [training: 0.033531714571607876 | validation: 0.03047674263390506]
	TIME [epoch: 8.31 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03523832357807723		[learning rate: 0.00059275]
		[batch 20/20] avg loss: 0.04700494935716935		[learning rate: 0.00059205]
	Learning Rate: 0.000592051
	LOSS [training: 0.04112163646762329 | validation: 0.059899008871622667]
	TIME [epoch: 8.31 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060200668544359424		[learning rate: 0.00059135]
		[batch 20/20] avg loss: 0.05298423618194035		[learning rate: 0.00059065]
	Learning Rate: 0.000590654
	LOSS [training: 0.05659245236314988 | validation: 0.015939505234945178]
	TIME [epoch: 8.3 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03381603261832417		[learning rate: 0.00058996]
		[batch 20/20] avg loss: 0.04028984123170141		[learning rate: 0.00058926]
	Learning Rate: 0.000589261
	LOSS [training: 0.03705293692501278 | validation: 0.027942674236472537]
	TIME [epoch: 8.3 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03300017288897253		[learning rate: 0.00058857]
		[batch 20/20] avg loss: 0.05232067374989594		[learning rate: 0.00058787]
	Learning Rate: 0.000587871
	LOSS [training: 0.04266042331943423 | validation: 0.04485193098320568]
	TIME [epoch: 8.32 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057435197866937716		[learning rate: 0.00058718]
		[batch 20/20] avg loss: 0.03782136443768698		[learning rate: 0.00058648]
	Learning Rate: 0.000586484
	LOSS [training: 0.04762828115231235 | validation: 0.03202203831649623]
	TIME [epoch: 8.31 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03876484036944808		[learning rate: 0.00058579]
		[batch 20/20] avg loss: 0.03787822634161267		[learning rate: 0.0005851]
	Learning Rate: 0.000585101
	LOSS [training: 0.038321533355530374 | validation: 0.03510095618548338]
	TIME [epoch: 8.3 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03616665048155294		[learning rate: 0.00058441]
		[batch 20/20] avg loss: 0.046789325652937704		[learning rate: 0.00058372]
	Learning Rate: 0.000583721
	LOSS [training: 0.041477988067245314 | validation: 0.028765268459417195]
	TIME [epoch: 8.3 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05357031643147679		[learning rate: 0.00058303]
		[batch 20/20] avg loss: 0.03228975960075026		[learning rate: 0.00058234]
	Learning Rate: 0.000582344
	LOSS [training: 0.04293003801611354 | validation: 0.022446675755214036]
	TIME [epoch: 8.32 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0439147107375297		[learning rate: 0.00058166]
		[batch 20/20] avg loss: 0.04966811945063187		[learning rate: 0.00058097]
	Learning Rate: 0.00058097
	LOSS [training: 0.046791415094080795 | validation: 0.026542008484465373]
	TIME [epoch: 8.31 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04033607924384517		[learning rate: 0.00058028]
		[batch 20/20] avg loss: 0.061407556128439		[learning rate: 0.0005796]
	Learning Rate: 0.0005796
	LOSS [training: 0.05087181768614207 | validation: 0.09879924922585438]
	TIME [epoch: 8.31 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03983697474578625		[learning rate: 0.00057892]
		[batch 20/20] avg loss: 0.033841288710530916		[learning rate: 0.00057823]
	Learning Rate: 0.000578233
	LOSS [training: 0.03683913172815857 | validation: 0.02589746715053943]
	TIME [epoch: 8.29 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04043903742039985		[learning rate: 0.00057755]
		[batch 20/20] avg loss: 0.05126911973970949		[learning rate: 0.00057687]
	Learning Rate: 0.000576869
	LOSS [training: 0.045854078580054664 | validation: 0.030305535471640964]
	TIME [epoch: 8.31 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05653414258832815		[learning rate: 0.00057619]
		[batch 20/20] avg loss: 0.05212681455945759		[learning rate: 0.00057551]
	Learning Rate: 0.000575508
	LOSS [training: 0.05433047857389286 | validation: 0.035000288792729345]
	TIME [epoch: 8.3 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054341896578638305		[learning rate: 0.00057483]
		[batch 20/20] avg loss: 0.03784758244004728		[learning rate: 0.00057415]
	Learning Rate: 0.00057415
	LOSS [training: 0.046094739509342786 | validation: 0.027395960883591502]
	TIME [epoch: 8.29 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047615798496125246		[learning rate: 0.00057347]
		[batch 20/20] avg loss: 0.02484636303653325		[learning rate: 0.0005728]
	Learning Rate: 0.000572796
	LOSS [training: 0.036231080766329235 | validation: 0.04812081907262287]
	TIME [epoch: 8.3 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03759931259306698		[learning rate: 0.00057212]
		[batch 20/20] avg loss: 0.04300266168741962		[learning rate: 0.00057144]
	Learning Rate: 0.000571445
	LOSS [training: 0.0403009871402433 | validation: 0.06001298650978433]
	TIME [epoch: 8.32 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05523234939077824		[learning rate: 0.00057077]
		[batch 20/20] avg loss: 0.05242564238072498		[learning rate: 0.0005701]
	Learning Rate: 0.000570097
	LOSS [training: 0.053828995885751615 | validation: 0.04146690788871704]
	TIME [epoch: 8.31 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043340019539518305		[learning rate: 0.00056942]
		[batch 20/20] avg loss: 0.04213939187259615		[learning rate: 0.00056875]
	Learning Rate: 0.000568752
	LOSS [training: 0.04273970570605722 | validation: 0.04629124909344848]
	TIME [epoch: 8.3 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04498851979843626		[learning rate: 0.00056808]
		[batch 20/20] avg loss: 0.05244969965930866		[learning rate: 0.00056741]
	Learning Rate: 0.000567411
	LOSS [training: 0.04871910972887245 | validation: 0.022794308385660975]
	TIME [epoch: 8.3 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03672613222813066		[learning rate: 0.00056674]
		[batch 20/20] avg loss: 0.04408645335044366		[learning rate: 0.00056607]
	Learning Rate: 0.000566072
	LOSS [training: 0.04040629278928716 | validation: 0.028736163017580016]
	TIME [epoch: 8.31 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04630473621621002		[learning rate: 0.0005654]
		[batch 20/20] avg loss: 0.0463874111017535		[learning rate: 0.00056474]
	Learning Rate: 0.000564737
	LOSS [training: 0.046346073658981767 | validation: 0.03942193096900977]
	TIME [epoch: 8.3 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04902193953816434		[learning rate: 0.00056407]
		[batch 20/20] avg loss: 0.05854714518538371		[learning rate: 0.0005634]
	Learning Rate: 0.000563405
	LOSS [training: 0.053784542361774025 | validation: 0.023165005832441018]
	TIME [epoch: 8.29 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05016258725813327		[learning rate: 0.00056274]
		[batch 20/20] avg loss: 0.04915899512539835		[learning rate: 0.00056208]
	Learning Rate: 0.000562076
	LOSS [training: 0.0496607911917658 | validation: 0.027234287715434555]
	TIME [epoch: 8.28 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04500137141210127		[learning rate: 0.00056141]
		[batch 20/20] avg loss: 0.04461804937306245		[learning rate: 0.00056075]
	Learning Rate: 0.00056075
	LOSS [training: 0.04480971039258186 | validation: 0.03189564841091526]
	TIME [epoch: 8.3 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04427323450994995		[learning rate: 0.00056009]
		[batch 20/20] avg loss: 0.040703065316441536		[learning rate: 0.00055943]
	Learning Rate: 0.000559427
	LOSS [training: 0.04248814991319574 | validation: 0.03539823556731882]
	TIME [epoch: 8.31 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0511587915467729		[learning rate: 0.00055877]
		[batch 20/20] avg loss: 0.05527996439049163		[learning rate: 0.00055811]
	Learning Rate: 0.000558108
	LOSS [training: 0.05321937796863226 | validation: 0.0782096450566018]
	TIME [epoch: 8.3 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054162218368719525		[learning rate: 0.00055745]
		[batch 20/20] avg loss: 0.05396600358351104		[learning rate: 0.00055679]
	Learning Rate: 0.000556791
	LOSS [training: 0.05406411097611528 | validation: 0.0314768303552633]
	TIME [epoch: 8.29 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028342238739857584		[learning rate: 0.00055613]
		[batch 20/20] avg loss: 0.052356975069107345		[learning rate: 0.00055548]
	Learning Rate: 0.000555478
	LOSS [training: 0.04034960690448247 | validation: 0.05161822996235878]
	TIME [epoch: 8.31 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03894584465601556		[learning rate: 0.00055482]
		[batch 20/20] avg loss: 0.0480925524201455		[learning rate: 0.00055417]
	Learning Rate: 0.000554167
	LOSS [training: 0.04351919853808053 | validation: 0.04898670240763373]
	TIME [epoch: 8.3 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035635507378103004		[learning rate: 0.00055351]
		[batch 20/20] avg loss: 0.0325823521688573		[learning rate: 0.00055286]
	Learning Rate: 0.00055286
	LOSS [training: 0.03410892977348015 | validation: 0.028361542124682865]
	TIME [epoch: 8.3 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04528944697497774		[learning rate: 0.00055221]
		[batch 20/20] avg loss: 0.04044297796984135		[learning rate: 0.00055156]
	Learning Rate: 0.000551556
	LOSS [training: 0.04286621247240955 | validation: 0.03512203651157108]
	TIME [epoch: 8.3 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05341463693555053		[learning rate: 0.00055091]
		[batch 20/20] avg loss: 0.04644445975228868		[learning rate: 0.00055026]
	Learning Rate: 0.000550255
	LOSS [training: 0.0499295483439196 | validation: 0.024118772554384167]
	TIME [epoch: 8.31 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03893716458610268		[learning rate: 0.00054961]
		[batch 20/20] avg loss: 0.038834762477198206		[learning rate: 0.00054896]
	Learning Rate: 0.000548957
	LOSS [training: 0.03888596353165044 | validation: 0.03520843368828423]
	TIME [epoch: 8.31 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05073805936967287		[learning rate: 0.00054831]
		[batch 20/20] avg loss: 0.03525014405572849		[learning rate: 0.00054766]
	Learning Rate: 0.000547662
	LOSS [training: 0.04299410171270068 | validation: 0.024147147277479814]
	TIME [epoch: 8.3 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03341219304963281		[learning rate: 0.00054702]
		[batch 20/20] avg loss: 0.043096326298824136		[learning rate: 0.00054637]
	Learning Rate: 0.00054637
	LOSS [training: 0.03825425967422848 | validation: 0.026577075910819682]
	TIME [epoch: 8.3 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03244861747590559		[learning rate: 0.00054573]
		[batch 20/20] avg loss: 0.03265021616026991		[learning rate: 0.00054508]
	Learning Rate: 0.000545082
	LOSS [training: 0.03254941681808775 | validation: 0.03539473664989677]
	TIME [epoch: 8.32 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05852234094794082		[learning rate: 0.00054444]
		[batch 20/20] avg loss: 0.04114035603604927		[learning rate: 0.0005438]
	Learning Rate: 0.000543796
	LOSS [training: 0.049831348491995046 | validation: 0.062237373144181325]
	TIME [epoch: 8.32 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053791526846686186		[learning rate: 0.00054315]
		[batch 20/20] avg loss: 0.039259461646675174		[learning rate: 0.00054251]
	Learning Rate: 0.000542513
	LOSS [training: 0.04652549424668068 | validation: 0.03988405138576785]
	TIME [epoch: 8.3 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04593546215013117		[learning rate: 0.00054187]
		[batch 20/20] avg loss: 0.03617253388284671		[learning rate: 0.00054123]
	Learning Rate: 0.000541233
	LOSS [training: 0.04105399801648894 | validation: 0.037067596892931855]
	TIME [epoch: 8.3 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056973211434787306		[learning rate: 0.00054059]
		[batch 20/20] avg loss: 0.04995377509682588		[learning rate: 0.00053996]
	Learning Rate: 0.000539957
	LOSS [training: 0.05346349326580659 | validation: 0.02238529494192608]
	TIME [epoch: 8.32 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03059043806673656		[learning rate: 0.00053932]
		[batch 20/20] avg loss: 0.036843378510195965		[learning rate: 0.00053868]
	Learning Rate: 0.000538683
	LOSS [training: 0.03371690828846626 | validation: 0.033004800502515294]
	TIME [epoch: 8.31 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04156677350496796		[learning rate: 0.00053805]
		[batch 20/20] avg loss: 0.035253067320368814		[learning rate: 0.00053741]
	Learning Rate: 0.000537412
	LOSS [training: 0.038409920412668386 | validation: 0.040796239596065216]
	TIME [epoch: 8.3 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03376086936754333		[learning rate: 0.00053678]
		[batch 20/20] avg loss: 0.033271424503194244		[learning rate: 0.00053614]
	Learning Rate: 0.000536145
	LOSS [training: 0.033516146935368786 | validation: 0.01944798255009577]
	TIME [epoch: 8.3 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03336173912744966		[learning rate: 0.00053551]
		[batch 20/20] avg loss: 0.04984476505047112		[learning rate: 0.00053488]
	Learning Rate: 0.00053488
	LOSS [training: 0.041603252088960395 | validation: 0.024577499852941866]
	TIME [epoch: 8.32 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0305518598119069		[learning rate: 0.00053425]
		[batch 20/20] avg loss: 0.03997125156091817		[learning rate: 0.00053362]
	Learning Rate: 0.000533618
	LOSS [training: 0.03526155568641254 | validation: 0.045678028660545086]
	TIME [epoch: 8.31 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03610513043106521		[learning rate: 0.00053299]
		[batch 20/20] avg loss: 0.053478037850430836		[learning rate: 0.00053236]
	Learning Rate: 0.00053236
	LOSS [training: 0.04479158414074803 | validation: 0.026540150741805702]
	TIME [epoch: 8.3 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03445410011540138		[learning rate: 0.00053173]
		[batch 20/20] avg loss: 0.049122771642294207		[learning rate: 0.0005311]
	Learning Rate: 0.000531104
	LOSS [training: 0.0417884358788478 | validation: 0.020156305996164848]
	TIME [epoch: 8.3 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03762582683888775		[learning rate: 0.00053048]
		[batch 20/20] avg loss: 0.03701097897316545		[learning rate: 0.00052985]
	Learning Rate: 0.000529851
	LOSS [training: 0.0373184029060266 | validation: 0.018589794200595394]
	TIME [epoch: 8.31 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044258353036183376		[learning rate: 0.00052923]
		[batch 20/20] avg loss: 0.042333598879120804		[learning rate: 0.0005286]
	Learning Rate: 0.000528601
	LOSS [training: 0.0432959759576521 | validation: 0.03411620719709831]
	TIME [epoch: 8.31 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034498409024037786		[learning rate: 0.00052798]
		[batch 20/20] avg loss: 0.03513131803733742		[learning rate: 0.00052735]
	Learning Rate: 0.000527354
	LOSS [training: 0.0348148635306876 | validation: 0.034641548425419534]
	TIME [epoch: 8.3 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03624586191744637		[learning rate: 0.00052673]
		[batch 20/20] avg loss: 0.03803039223004034		[learning rate: 0.00052611]
	Learning Rate: 0.00052611
	LOSS [training: 0.03713812707374335 | validation: 0.021496182722288742]
	TIME [epoch: 8.31 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037768199125018255		[learning rate: 0.00052549]
		[batch 20/20] avg loss: 0.04736681491230087		[learning rate: 0.00052487]
	Learning Rate: 0.000524869
	LOSS [training: 0.04256750701865956 | validation: 0.025775993336638034]
	TIME [epoch: 8.32 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036945774622932914		[learning rate: 0.00052425]
		[batch 20/20] avg loss: 0.041374093442989895		[learning rate: 0.00052363]
	Learning Rate: 0.000523631
	LOSS [training: 0.039159934032961405 | validation: 0.020278718891020914]
	TIME [epoch: 8.31 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05865941127657466		[learning rate: 0.00052301]
		[batch 20/20] avg loss: 0.03546988795621428		[learning rate: 0.0005224]
	Learning Rate: 0.000522396
	LOSS [training: 0.04706464961639446 | validation: 0.02372967390895421]
	TIME [epoch: 8.3 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036686430954864256		[learning rate: 0.00052178]
		[batch 20/20] avg loss: 0.05808292824522706		[learning rate: 0.00052116]
	Learning Rate: 0.000521164
	LOSS [training: 0.047384679600045665 | validation: 0.031048492245195722]
	TIME [epoch: 8.3 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038782264364770704		[learning rate: 0.00052055]
		[batch 20/20] avg loss: 0.033595209748149994		[learning rate: 0.00051993]
	Learning Rate: 0.000519935
	LOSS [training: 0.036188737056460335 | validation: 0.04176652185582231]
	TIME [epoch: 8.32 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028289906699375755		[learning rate: 0.00051932]
		[batch 20/20] avg loss: 0.041510338591191055		[learning rate: 0.00051871]
	Learning Rate: 0.000518708
	LOSS [training: 0.03490012264528341 | validation: 0.029774058533962474]
	TIME [epoch: 8.3 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045949066007067545		[learning rate: 0.0005181]
		[batch 20/20] avg loss: 0.03387569151834817		[learning rate: 0.00051748]
	Learning Rate: 0.000517485
	LOSS [training: 0.039912378762707854 | validation: 0.024381360001186146]
	TIME [epoch: 8.3 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03933612874118098		[learning rate: 0.00051687]
		[batch 20/20] avg loss: 0.05852984869821505		[learning rate: 0.00051626]
	Learning Rate: 0.000516264
	LOSS [training: 0.04893298871969801 | validation: 0.01806688364573117]
	TIME [epoch: 8.3 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03625632314302081		[learning rate: 0.00051565]
		[batch 20/20] avg loss: 0.07087054201793867		[learning rate: 0.00051505]
	Learning Rate: 0.000515046
	LOSS [training: 0.05356343258047974 | validation: 0.04239054353938532]
	TIME [epoch: 8.31 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033580069777503885		[learning rate: 0.00051444]
		[batch 20/20] avg loss: 0.041620583629393415		[learning rate: 0.00051383]
	Learning Rate: 0.000513831
	LOSS [training: 0.037600326703448636 | validation: 0.09052540004125219]
	TIME [epoch: 8.31 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046229409891012936		[learning rate: 0.00051322]
		[batch 20/20] avg loss: 0.028743492030948104		[learning rate: 0.00051262]
	Learning Rate: 0.000512619
	LOSS [training: 0.037486450960980515 | validation: 0.02728366267206739]
	TIME [epoch: 8.3 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051639528504498836		[learning rate: 0.00051201]
		[batch 20/20] avg loss: 0.03242704730902242		[learning rate: 0.00051141]
	Learning Rate: 0.00051141
	LOSS [training: 0.04203328790676063 | validation: 0.0749005096201492]
	TIME [epoch: 8.3 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0675296893971678		[learning rate: 0.00051081]
		[batch 20/20] avg loss: 0.03784763700105516		[learning rate: 0.0005102]
	Learning Rate: 0.000510204
	LOSS [training: 0.05268866319911149 | validation: 0.020040337558021502]
	TIME [epoch: 8.32 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032734239282428516		[learning rate: 0.0005096]
		[batch 20/20] avg loss: 0.02972592357814613		[learning rate: 0.000509]
	Learning Rate: 0.000509
	LOSS [training: 0.031230081430287326 | validation: 0.023128458426557547]
	TIME [epoch: 8.3 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034773571692193694		[learning rate: 0.0005084]
		[batch 20/20] avg loss: 0.04859180499705519		[learning rate: 0.0005078]
	Learning Rate: 0.0005078
	LOSS [training: 0.041682688344624436 | validation: 0.032482662031487815]
	TIME [epoch: 8.3 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057487024562446545		[learning rate: 0.0005072]
		[batch 20/20] avg loss: 0.039994289330426175		[learning rate: 0.0005066]
	Learning Rate: 0.000506602
	LOSS [training: 0.04874065694643635 | validation: 0.05920107688748564]
	TIME [epoch: 8.3 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03368417876881315		[learning rate: 0.000506]
		[batch 20/20] avg loss: 0.07551129488060701		[learning rate: 0.00050541]
	Learning Rate: 0.000505407
	LOSS [training: 0.05459773682471007 | validation: 0.06677428517930328]
	TIME [epoch: 8.32 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03503714440964227		[learning rate: 0.00050481]
		[batch 20/20] avg loss: 0.04467042885614662		[learning rate: 0.00050421]
	Learning Rate: 0.000504215
	LOSS [training: 0.03985378663289445 | validation: 0.02127448501127692]
	TIME [epoch: 8.31 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03823772036384674		[learning rate: 0.00050362]
		[batch 20/20] avg loss: 0.049390201719361634		[learning rate: 0.00050303]
	Learning Rate: 0.000503025
	LOSS [training: 0.04381396104160419 | validation: 0.011762690355434365]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_1316.pth
	Model improved!!!
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03206118616150652		[learning rate: 0.00050243]
		[batch 20/20] avg loss: 0.030584661961949096		[learning rate: 0.00050184]
	Learning Rate: 0.000501839
	LOSS [training: 0.03132292406172781 | validation: 0.023692902015727307]
	TIME [epoch: 8.3 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050955330450709144		[learning rate: 0.00050125]
		[batch 20/20] avg loss: 0.04896292080413067		[learning rate: 0.00050065]
	Learning Rate: 0.000500655
	LOSS [training: 0.049959125627419915 | validation: 0.019626591369650104]
	TIME [epoch: 8.31 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03940251989665206		[learning rate: 0.00050006]
		[batch 20/20] avg loss: 0.0354495115245602		[learning rate: 0.00049947]
	Learning Rate: 0.000499474
	LOSS [training: 0.03742601571060612 | validation: 0.022260829737940576]
	TIME [epoch: 8.3 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02497247477284094		[learning rate: 0.00049888]
		[batch 20/20] avg loss: 0.0375205233848041		[learning rate: 0.0004983]
	Learning Rate: 0.000498296
	LOSS [training: 0.031246499078822522 | validation: 0.017589311681258123]
	TIME [epoch: 8.29 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025907009609812605		[learning rate: 0.00049771]
		[batch 20/20] avg loss: 0.031500757959017		[learning rate: 0.00049712]
	Learning Rate: 0.00049712
	LOSS [training: 0.028703883784414803 | validation: 0.02218787875941364]
	TIME [epoch: 8.28 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03336309615565285		[learning rate: 0.00049653]
		[batch 20/20] avg loss: 0.03676969963284655		[learning rate: 0.00049595]
	Learning Rate: 0.000495948
	LOSS [training: 0.03506639789424969 | validation: 0.02621465257955648]
	TIME [epoch: 8.32 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03326421006142648		[learning rate: 0.00049536]
		[batch 20/20] avg loss: 0.046153424622013675		[learning rate: 0.00049478]
	Learning Rate: 0.000494778
	LOSS [training: 0.03970881734172007 | validation: 0.03817658273626856]
	TIME [epoch: 8.3 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0533299642569906		[learning rate: 0.00049419]
		[batch 20/20] avg loss: 0.025839502787013607		[learning rate: 0.00049361]
	Learning Rate: 0.000493611
	LOSS [training: 0.039584733522002105 | validation: 0.019295844476476524]
	TIME [epoch: 8.29 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030597198207616926		[learning rate: 0.00049303]
		[batch 20/20] avg loss: 0.04106888279563205		[learning rate: 0.00049245]
	Learning Rate: 0.000492446
	LOSS [training: 0.03583304050162449 | validation: 0.027347001741049697]
	TIME [epoch: 8.29 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038937595508656234		[learning rate: 0.00049187]
		[batch 20/20] avg loss: 0.0396750387967802		[learning rate: 0.00049128]
	Learning Rate: 0.000491285
	LOSS [training: 0.039306317152718206 | validation: 0.027217240546651082]
	TIME [epoch: 8.32 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03864549851832592		[learning rate: 0.0004907]
		[batch 20/20] avg loss: 0.03113363703798331		[learning rate: 0.00049013]
	Learning Rate: 0.000490126
	LOSS [training: 0.034889567778154607 | validation: 0.03400453085945942]
	TIME [epoch: 8.3 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03686605942026357		[learning rate: 0.00048955]
		[batch 20/20] avg loss: 0.056218445458899514		[learning rate: 0.00048897]
	Learning Rate: 0.00048897
	LOSS [training: 0.04654225243958154 | validation: 0.020153505412150493]
	TIME [epoch: 8.29 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036988890495859836		[learning rate: 0.00048839]
		[batch 20/20] avg loss: 0.04176154550247983		[learning rate: 0.00048782]
	Learning Rate: 0.000487816
	LOSS [training: 0.039375217999169834 | validation: 0.023982778012457018]
	TIME [epoch: 8.3 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03420729272352075		[learning rate: 0.00048724]
		[batch 20/20] avg loss: 0.03525411012767301		[learning rate: 0.00048667]
	Learning Rate: 0.000486666
	LOSS [training: 0.03473070142559688 | validation: 0.02361173429156804]
	TIME [epoch: 8.32 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042303549151242736		[learning rate: 0.00048609]
		[batch 20/20] avg loss: 0.04698974540188714		[learning rate: 0.00048552]
	Learning Rate: 0.000485518
	LOSS [training: 0.044646647276564935 | validation: 0.020454723412417285]
	TIME [epoch: 8.3 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03260605805800001		[learning rate: 0.00048494]
		[batch 20/20] avg loss: 0.03608773889671235		[learning rate: 0.00048437]
	Learning Rate: 0.000484372
	LOSS [training: 0.03434689847735618 | validation: 0.03102243897442303]
	TIME [epoch: 8.29 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051553490734956055		[learning rate: 0.0004838]
		[batch 20/20] avg loss: 0.031070471674844658		[learning rate: 0.00048323]
	Learning Rate: 0.00048323
	LOSS [training: 0.04131198120490036 | validation: 0.0355024574276541]
	TIME [epoch: 8.29 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04932119994260487		[learning rate: 0.00048266]
		[batch 20/20] avg loss: 0.03271714101102145		[learning rate: 0.00048209]
	Learning Rate: 0.00048209
	LOSS [training: 0.04101917047681315 | validation: 0.02597261096481087]
	TIME [epoch: 8.32 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042836065543472704		[learning rate: 0.00048152]
		[batch 20/20] avg loss: 0.03140474616645838		[learning rate: 0.00048095]
	Learning Rate: 0.000480953
	LOSS [training: 0.037120405854965545 | validation: 0.020111589780393607]
	TIME [epoch: 8.3 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029617857813198416		[learning rate: 0.00048039]
		[batch 20/20] avg loss: 0.037835801513540455		[learning rate: 0.00047982]
	Learning Rate: 0.000479818
	LOSS [training: 0.033726829663369436 | validation: 0.08571905661185578]
	TIME [epoch: 8.29 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04119115263516039		[learning rate: 0.00047925]
		[batch 20/20] avg loss: 0.03487462268721883		[learning rate: 0.00047869]
	Learning Rate: 0.000478687
	LOSS [training: 0.03803288766118962 | validation: 0.02609109465751743]
	TIME [epoch: 8.3 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054475475117114446		[learning rate: 0.00047812]
		[batch 20/20] avg loss: 0.03156008777746819		[learning rate: 0.00047756]
	Learning Rate: 0.000477557
	LOSS [training: 0.04301778144729131 | validation: 0.03111124435392532]
	TIME [epoch: 8.32 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03996012285651364		[learning rate: 0.00047699]
		[batch 20/20] avg loss: 0.036489327646499564		[learning rate: 0.00047643]
	Learning Rate: 0.000476431
	LOSS [training: 0.0382247252515066 | validation: 0.06874195862922187]
	TIME [epoch: 8.29 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055838717354949496		[learning rate: 0.00047587]
		[batch 20/20] avg loss: 0.0525330756747999		[learning rate: 0.00047531]
	Learning Rate: 0.000475307
	LOSS [training: 0.054185896514874696 | validation: 0.03322865900500619]
	TIME [epoch: 8.31 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050743565758352024		[learning rate: 0.00047475]
		[batch 20/20] avg loss: 0.05007099518695527		[learning rate: 0.00047419]
	Learning Rate: 0.000474186
	LOSS [training: 0.05040728047265365 | validation: 0.031021730919142575]
	TIME [epoch: 8.3 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04317816858939227		[learning rate: 0.00047363]
		[batch 20/20] avg loss: 0.03637112712973373		[learning rate: 0.00047307]
	Learning Rate: 0.000473067
	LOSS [training: 0.03977464785956299 | validation: 0.07363620191272782]
	TIME [epoch: 8.32 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05980055946596161		[learning rate: 0.00047251]
		[batch 20/20] avg loss: 0.04251437381307001		[learning rate: 0.00047195]
	Learning Rate: 0.000471951
	LOSS [training: 0.0511574666395158 | validation: 0.050226582903593525]
	TIME [epoch: 8.29 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04329659393021623		[learning rate: 0.00047139]
		[batch 20/20] avg loss: 0.043358650995674514		[learning rate: 0.00047084]
	Learning Rate: 0.000470838
	LOSS [training: 0.04332762246294538 | validation: 0.0237739696940486]
	TIME [epoch: 8.29 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04319058211433582		[learning rate: 0.00047028]
		[batch 20/20] avg loss: 0.033158362950678456		[learning rate: 0.00046973]
	Learning Rate: 0.000469728
	LOSS [training: 0.038174472532507144 | validation: 0.024388914011130716]
	TIME [epoch: 8.29 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02939885147068469		[learning rate: 0.00046917]
		[batch 20/20] avg loss: 0.059112952147629697		[learning rate: 0.00046862]
	Learning Rate: 0.00046862
	LOSS [training: 0.04425590180915719 | validation: 0.13106339043130327]
	TIME [epoch: 8.31 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07451907919641643		[learning rate: 0.00046807]
		[batch 20/20] avg loss: 0.02320129560735217		[learning rate: 0.00046751]
	Learning Rate: 0.000467514
	LOSS [training: 0.04886018740188429 | validation: 0.016021552253099577]
	TIME [epoch: 8.3 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03284273467880549		[learning rate: 0.00046696]
		[batch 20/20] avg loss: 0.03666356502302724		[learning rate: 0.00046641]
	Learning Rate: 0.000466411
	LOSS [training: 0.03475314985091636 | validation: 0.05001930129999063]
	TIME [epoch: 8.28 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03593378143171656		[learning rate: 0.00046586]
		[batch 20/20] avg loss: 0.031604676639328505		[learning rate: 0.00046531]
	Learning Rate: 0.000465311
	LOSS [training: 0.03376922903552253 | validation: 0.022686018602793477]
	TIME [epoch: 8.29 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02674772783032207		[learning rate: 0.00046476]
		[batch 20/20] avg loss: 0.03743919421195119		[learning rate: 0.00046421]
	Learning Rate: 0.000464214
	LOSS [training: 0.032093461021136624 | validation: 0.025139447563248807]
	TIME [epoch: 8.31 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03454445296289409		[learning rate: 0.00046367]
		[batch 20/20] avg loss: 0.04160800191648554		[learning rate: 0.00046312]
	Learning Rate: 0.000463119
	LOSS [training: 0.03807622743968981 | validation: 0.027888864685860455]
	TIME [epoch: 8.29 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02929200149434113		[learning rate: 0.00046257]
		[batch 20/20] avg loss: 0.03737300251836647		[learning rate: 0.00046203]
	Learning Rate: 0.000462026
	LOSS [training: 0.0333325020063538 | validation: 0.06351350797110182]
	TIME [epoch: 8.29 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05390552959386875		[learning rate: 0.00046148]
		[batch 20/20] avg loss: 0.04969494107056705		[learning rate: 0.00046094]
	Learning Rate: 0.000460936
	LOSS [training: 0.051800235332217906 | validation: 0.02292830538694044]
	TIME [epoch: 8.29 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034187097439291415		[learning rate: 0.00046039]
		[batch 20/20] avg loss: 0.03994805163002425		[learning rate: 0.00045985]
	Learning Rate: 0.000459849
	LOSS [training: 0.03706757453465783 | validation: 0.027078860265590497]
	TIME [epoch: 8.32 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03097045369043237		[learning rate: 0.00045931]
		[batch 20/20] avg loss: 0.0446987435261912		[learning rate: 0.00045876]
	Learning Rate: 0.000458764
	LOSS [training: 0.03783459860831179 | validation: 0.033504418399156174]
	TIME [epoch: 8.29 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05368012968514457		[learning rate: 0.00045822]
		[batch 20/20] avg loss: 0.05301367334540262		[learning rate: 0.00045768]
	Learning Rate: 0.000457682
	LOSS [training: 0.05334690151527359 | validation: 0.028446853283410144]
	TIME [epoch: 8.3 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10044374851189986		[learning rate: 0.00045714]
		[batch 20/20] avg loss: 0.05043189065619817		[learning rate: 0.0004566]
	Learning Rate: 0.000456603
	LOSS [training: 0.075437819584049 | validation: 0.02451223023957254]
	TIME [epoch: 8.29 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03593960341985053		[learning rate: 0.00045606]
		[batch 20/20] avg loss: 0.03350098679270128		[learning rate: 0.00045553]
	Learning Rate: 0.000455526
	LOSS [training: 0.0347202951062759 | validation: 0.02974357821998728]
	TIME [epoch: 8.33 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04743611383007896		[learning rate: 0.00045499]
		[batch 20/20] avg loss: 0.03142763808226769		[learning rate: 0.00045445]
	Learning Rate: 0.000454451
	LOSS [training: 0.039431875956173326 | validation: 0.02559973405582623]
	TIME [epoch: 8.3 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031469973627209574		[learning rate: 0.00045391]
		[batch 20/20] avg loss: 0.038306547467569126		[learning rate: 0.00045338]
	Learning Rate: 0.000453379
	LOSS [training: 0.03488826054738935 | validation: 0.019604752754605246]
	TIME [epoch: 8.3 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027795273821231307		[learning rate: 0.00045284]
		[batch 20/20] avg loss: 0.03465013456008985		[learning rate: 0.00045231]
	Learning Rate: 0.00045231
	LOSS [training: 0.031222704190660578 | validation: 0.02549460438949269]
	TIME [epoch: 8.3 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03151893554629762		[learning rate: 0.00045178]
		[batch 20/20] avg loss: 0.048468936951253774		[learning rate: 0.00045124]
	Learning Rate: 0.000451243
	LOSS [training: 0.039993936248775695 | validation: 0.039030268062396886]
	TIME [epoch: 8.34 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05351109686177739		[learning rate: 0.00045071]
		[batch 20/20] avg loss: 0.04551037973453107		[learning rate: 0.00045018]
	Learning Rate: 0.000450178
	LOSS [training: 0.04951073829815422 | validation: 0.018787302825288985]
	TIME [epoch: 8.3 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04349841310069645		[learning rate: 0.00044965]
		[batch 20/20] avg loss: 0.035316481924984613		[learning rate: 0.00044912]
	Learning Rate: 0.000449116
	LOSS [training: 0.03940744751284054 | validation: 0.05169694208974942]
	TIME [epoch: 8.29 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05311710647540776		[learning rate: 0.00044859]
		[batch 20/20] avg loss: 0.04939783041204135		[learning rate: 0.00044806]
	Learning Rate: 0.000448057
	LOSS [training: 0.051257468443724565 | validation: 0.05379534148312756]
	TIME [epoch: 8.29 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05495082675223626		[learning rate: 0.00044753]
		[batch 20/20] avg loss: 0.039798942777963794		[learning rate: 0.000447]
	Learning Rate: 0.000447
	LOSS [training: 0.04737488476510003 | validation: 0.023939990611170348]
	TIME [epoch: 8.32 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036851484974325686		[learning rate: 0.00044647]
		[batch 20/20] avg loss: 0.0288762917390142		[learning rate: 0.00044595]
	Learning Rate: 0.000445946
	LOSS [training: 0.032863888356669944 | validation: 0.04824140654201205]
	TIME [epoch: 8.31 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036385874151385784		[learning rate: 0.00044542]
		[batch 20/20] avg loss: 0.03814597383900632		[learning rate: 0.00044489]
	Learning Rate: 0.000444894
	LOSS [training: 0.03726592399519606 | validation: 0.01695121716685713]
	TIME [epoch: 8.29 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03817839683406747		[learning rate: 0.00044437]
		[batch 20/20] avg loss: 0.024916981581768388		[learning rate: 0.00044384]
	Learning Rate: 0.000443844
	LOSS [training: 0.03154768920791794 | validation: 0.021782089974193154]
	TIME [epoch: 8.29 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03323013959877055		[learning rate: 0.00044332]
		[batch 20/20] avg loss: 0.045133963324981215		[learning rate: 0.0004428]
	Learning Rate: 0.000442797
	LOSS [training: 0.03918205146187588 | validation: 0.0411883240589098]
	TIME [epoch: 8.32 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0422826467601735		[learning rate: 0.00044227]
		[batch 20/20] avg loss: 0.059339078173721214		[learning rate: 0.00044175]
	Learning Rate: 0.000441753
	LOSS [training: 0.05081086246694737 | validation: 0.032340838652931345]
	TIME [epoch: 8.3 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04745133760567527		[learning rate: 0.00044123]
		[batch 20/20] avg loss: 0.0433275618485901		[learning rate: 0.00044071]
	Learning Rate: 0.000440711
	LOSS [training: 0.04538944972713269 | validation: 0.02282352928555447]
	TIME [epoch: 8.29 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038802517780140114		[learning rate: 0.00044019]
		[batch 20/20] avg loss: 0.044313022373406216		[learning rate: 0.00043967]
	Learning Rate: 0.000439671
	LOSS [training: 0.04155777007677316 | validation: 0.02091902799786248]
	TIME [epoch: 8.3 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044786344886171905		[learning rate: 0.00043915]
		[batch 20/20] avg loss: 0.03483868951821296		[learning rate: 0.00043863]
	Learning Rate: 0.000438634
	LOSS [training: 0.039812517202192435 | validation: 0.04505663222429223]
	TIME [epoch: 8.32 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04086079744943355		[learning rate: 0.00043812]
		[batch 20/20] avg loss: 0.039356734130789495		[learning rate: 0.0004376]
	Learning Rate: 0.0004376
	LOSS [training: 0.040108765790111536 | validation: 0.029742384084561854]
	TIME [epoch: 8.3 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03494894027031977		[learning rate: 0.00043708]
		[batch 20/20] avg loss: 0.04839211619478902		[learning rate: 0.00043657]
	Learning Rate: 0.000436567
	LOSS [training: 0.04167052823255439 | validation: 0.02984326934401752]
	TIME [epoch: 8.29 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03992090478048991		[learning rate: 0.00043605]
		[batch 20/20] avg loss: 0.035145494432582775		[learning rate: 0.00043554]
	Learning Rate: 0.000435538
	LOSS [training: 0.037533199606536345 | validation: 0.028162936615071816]
	TIME [epoch: 8.3 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03619124617942325		[learning rate: 0.00043502]
		[batch 20/20] avg loss: 0.0335052177852662		[learning rate: 0.00043451]
	Learning Rate: 0.00043451
	LOSS [training: 0.034848231982344734 | validation: 0.026663694265676274]
	TIME [epoch: 8.32 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039117563342509536		[learning rate: 0.000434]
		[batch 20/20] avg loss: 0.0448474054559804		[learning rate: 0.00043349]
	Learning Rate: 0.000433485
	LOSS [training: 0.041982484399244964 | validation: 0.026804926383501876]
	TIME [epoch: 8.3 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02789372071039699		[learning rate: 0.00043297]
		[batch 20/20] avg loss: 0.0447978724228279		[learning rate: 0.00043246]
	Learning Rate: 0.000432463
	LOSS [training: 0.03634579656661245 | validation: 0.0729220392228081]
	TIME [epoch: 8.3 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04793712471768924		[learning rate: 0.00043195]
		[batch 20/20] avg loss: 0.03660031244561836		[learning rate: 0.00043144]
	Learning Rate: 0.000431443
	LOSS [training: 0.0422687185816538 | validation: 0.09169554847357812]
	TIME [epoch: 8.29 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044885757015674446		[learning rate: 0.00043093]
		[batch 20/20] avg loss: 0.048923576343379384		[learning rate: 0.00043042]
	Learning Rate: 0.000430425
	LOSS [training: 0.04690466667952692 | validation: 0.032327071378254135]
	TIME [epoch: 8.32 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03397040086814403		[learning rate: 0.00042992]
		[batch 20/20] avg loss: 0.04985796057544074		[learning rate: 0.00042941]
	Learning Rate: 0.00042941
	LOSS [training: 0.04191418072179239 | validation: 0.04813273201256611]
	TIME [epoch: 8.3 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042889205817181364		[learning rate: 0.0004289]
		[batch 20/20] avg loss: 0.029561456341630467		[learning rate: 0.0004284]
	Learning Rate: 0.000428397
	LOSS [training: 0.03622533107940591 | validation: 0.03644366879212889]
	TIME [epoch: 8.3 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04337114256833642		[learning rate: 0.00042789]
		[batch 20/20] avg loss: 0.029181726558233236		[learning rate: 0.00042739]
	Learning Rate: 0.000427386
	LOSS [training: 0.036276434563284825 | validation: 0.03415886775130126]
	TIME [epoch: 8.3 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026932491548004017		[learning rate: 0.00042688]
		[batch 20/20] avg loss: 0.04129586224556546		[learning rate: 0.00042638]
	Learning Rate: 0.000426378
	LOSS [training: 0.03411417689678474 | validation: 0.03966992342017801]
	TIME [epoch: 8.31 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03737292841825176		[learning rate: 0.00042587]
		[batch 20/20] avg loss: 0.038200285826846		[learning rate: 0.00042537]
	Learning Rate: 0.000425372
	LOSS [training: 0.037786607122548874 | validation: 0.06366389921209407]
	TIME [epoch: 8.3 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05088626508514891		[learning rate: 0.00042487]
		[batch 20/20] avg loss: 0.02778524791799533		[learning rate: 0.00042437]
	Learning Rate: 0.000424369
	LOSS [training: 0.03933575650157211 | validation: 0.02274981047994324]
	TIME [epoch: 8.29 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028765504786235435		[learning rate: 0.00042387]
		[batch 20/20] avg loss: 0.03373457586523293		[learning rate: 0.00042337]
	Learning Rate: 0.000423368
	LOSS [training: 0.03125004032573418 | validation: 0.031160207547295524]
	TIME [epoch: 8.3 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03571679609286843		[learning rate: 0.00042287]
		[batch 20/20] avg loss: 0.03769409977366033		[learning rate: 0.00042237]
	Learning Rate: 0.000422369
	LOSS [training: 0.03670544793326439 | validation: 0.05433595360430374]
	TIME [epoch: 8.31 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04558454683685811		[learning rate: 0.00042187]
		[batch 20/20] avg loss: 0.03142346667627253		[learning rate: 0.00042137]
	Learning Rate: 0.000421373
	LOSS [training: 0.03850400675656531 | validation: 0.021880571457866108]
	TIME [epoch: 8.3 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0406910896537506		[learning rate: 0.00042088]
		[batch 20/20] avg loss: 0.04166380815978531		[learning rate: 0.00042038]
	Learning Rate: 0.000420379
	LOSS [training: 0.04117744890676795 | validation: 0.04173894833000211]
	TIME [epoch: 8.29 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03544675556860427		[learning rate: 0.00041988]
		[batch 20/20] avg loss: 0.046679380408039164		[learning rate: 0.00041939]
	Learning Rate: 0.000419387
	LOSS [training: 0.04106306798832172 | validation: 0.022489585433252825]
	TIME [epoch: 8.29 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028099155550901516		[learning rate: 0.00041889]
		[batch 20/20] avg loss: 0.03552238682157219		[learning rate: 0.0004184]
	Learning Rate: 0.000418398
	LOSS [training: 0.03181077118623684 | validation: 0.03130988404642313]
	TIME [epoch: 8.31 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027410876167701426		[learning rate: 0.0004179]
		[batch 20/20] avg loss: 0.04043270555098386		[learning rate: 0.00041741]
	Learning Rate: 0.000417411
	LOSS [training: 0.03392179085934264 | validation: 0.01603289818501679]
	TIME [epoch: 8.29 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034851341529657555		[learning rate: 0.00041692]
		[batch 20/20] avg loss: 0.033786821218686		[learning rate: 0.00041643]
	Learning Rate: 0.000416427
	LOSS [training: 0.03431908137417178 | validation: 0.02003497625209431]
	TIME [epoch: 8.29 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05847953509845712		[learning rate: 0.00041594]
		[batch 20/20] avg loss: 0.04768617703472584		[learning rate: 0.00041544]
	Learning Rate: 0.000415444
	LOSS [training: 0.053082856066591466 | validation: 0.01603241190938078]
	TIME [epoch: 8.29 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05607589817133187		[learning rate: 0.00041495]
		[batch 20/20] avg loss: 0.037809341094841485		[learning rate: 0.00041446]
	Learning Rate: 0.000414464
	LOSS [training: 0.04694261963308668 | validation: 0.034160121332573616]
	TIME [epoch: 8.32 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03486827806663772		[learning rate: 0.00041398]
		[batch 20/20] avg loss: 0.039230737613329486		[learning rate: 0.00041349]
	Learning Rate: 0.000413487
	LOSS [training: 0.0370495078399836 | validation: 0.03822157334404186]
	TIME [epoch: 8.3 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028852852023163976		[learning rate: 0.000413]
		[batch 20/20] avg loss: 0.05525143224680856		[learning rate: 0.00041251]
	Learning Rate: 0.000412511
	LOSS [training: 0.04205214213498627 | validation: 0.04213752319224638]
	TIME [epoch: 8.28 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03452159496842585		[learning rate: 0.00041202]
		[batch 20/20] avg loss: 0.03185720917989248		[learning rate: 0.00041154]
	Learning Rate: 0.000411538
	LOSS [training: 0.033189402074159165 | validation: 0.050311801353684606]
	TIME [epoch: 8.29 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05923706273358807		[learning rate: 0.00041105]
		[batch 20/20] avg loss: 0.03169795046543839		[learning rate: 0.00041057]
	Learning Rate: 0.000410568
	LOSS [training: 0.04546750659951324 | validation: 0.010887498404212768]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_1402.pth
	Model improved!!!
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03556130294627327		[learning rate: 0.00041008]
		[batch 20/20] avg loss: 0.036975907235387157		[learning rate: 0.0004096]
	Learning Rate: 0.000409599
	LOSS [training: 0.03626860509083022 | validation: 0.027887207260436024]
	TIME [epoch: 8.3 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029362038555664917		[learning rate: 0.00040912]
		[batch 20/20] avg loss: 0.040755575887518		[learning rate: 0.00040863]
	Learning Rate: 0.000408633
	LOSS [training: 0.03505880722159145 | validation: 0.027212866391641596]
	TIME [epoch: 8.29 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035251772512495914		[learning rate: 0.00040815]
		[batch 20/20] avg loss: 0.03313469913501748		[learning rate: 0.00040767]
	Learning Rate: 0.000407669
	LOSS [training: 0.03419323582375669 | validation: 0.030811276796451976]
	TIME [epoch: 8.29 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03329047077304605		[learning rate: 0.00040719]
		[batch 20/20] avg loss: 0.036741182379107666		[learning rate: 0.00040671]
	Learning Rate: 0.000406707
	LOSS [training: 0.03501582657607687 | validation: 0.03735304808427253]
	TIME [epoch: 8.31 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03351530247038244		[learning rate: 0.00040623]
		[batch 20/20] avg loss: 0.05657254402286024		[learning rate: 0.00040575]
	Learning Rate: 0.000405748
	LOSS [training: 0.045043923246621334 | validation: 0.03803527986369269]
	TIME [epoch: 8.29 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03262375938271659		[learning rate: 0.00040527]
		[batch 20/20] avg loss: 0.030077616141764518		[learning rate: 0.00040479]
	Learning Rate: 0.000404791
	LOSS [training: 0.03135068776224055 | validation: 0.02560825770009502]
	TIME [epoch: 8.29 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04295766034947144		[learning rate: 0.00040431]
		[batch 20/20] avg loss: 0.0272965505454699		[learning rate: 0.00040384]
	Learning Rate: 0.000403836
	LOSS [training: 0.03512710544747068 | validation: 0.03021861763033802]
	TIME [epoch: 8.29 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04065477697714778		[learning rate: 0.00040336]
		[batch 20/20] avg loss: 0.032170360634824945		[learning rate: 0.00040288]
	Learning Rate: 0.000402883
	LOSS [training: 0.03641256880598638 | validation: 0.028253984531037953]
	TIME [epoch: 8.32 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03815777295808917		[learning rate: 0.00040241]
		[batch 20/20] avg loss: 0.030089434784385548		[learning rate: 0.00040193]
	Learning Rate: 0.000401933
	LOSS [training: 0.034123603871237355 | validation: 0.024532859544181756]
	TIME [epoch: 8.29 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03078922745809275		[learning rate: 0.00040146]
		[batch 20/20] avg loss: 0.036152046999485446		[learning rate: 0.00040099]
	Learning Rate: 0.000400985
	LOSS [training: 0.0334706372287891 | validation: 0.01881866411867793]
	TIME [epoch: 8.28 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034221528584924686		[learning rate: 0.00040051]
		[batch 20/20] avg loss: 0.04071473717430891		[learning rate: 0.00040004]
	Learning Rate: 0.000400039
	LOSS [training: 0.0374681328796168 | validation: 0.04402090300453334]
	TIME [epoch: 8.29 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03182435441871141		[learning rate: 0.00039957]
		[batch 20/20] avg loss: 0.02886141801534382		[learning rate: 0.0003991]
	Learning Rate: 0.000399096
	LOSS [training: 0.030342886217027614 | validation: 0.02785119789434497]
	TIME [epoch: 8.32 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040466707170901936		[learning rate: 0.00039862]
		[batch 20/20] avg loss: 0.028786167142124775		[learning rate: 0.00039815]
	Learning Rate: 0.000398154
	LOSS [training: 0.034626437156513354 | validation: 0.04145682571254246]
	TIME [epoch: 8.3 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05550000132225724		[learning rate: 0.00039768]
		[batch 20/20] avg loss: 0.029536652138948006		[learning rate: 0.00039721]
	Learning Rate: 0.000397215
	LOSS [training: 0.04251832673060262 | validation: 0.04953748373992618]
	TIME [epoch: 8.3 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03029255068617464		[learning rate: 0.00039675]
		[batch 20/20] avg loss: 0.03699371481254845		[learning rate: 0.00039628]
	Learning Rate: 0.000396278
	LOSS [training: 0.033643132749361544 | validation: 0.021220135562636078]
	TIME [epoch: 8.29 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03709501479966291		[learning rate: 0.00039581]
		[batch 20/20] avg loss: 0.03283428420316249		[learning rate: 0.00039534]
	Learning Rate: 0.000395343
	LOSS [training: 0.03496464950141269 | validation: 0.02679053860606201]
	TIME [epoch: 8.31 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03411731364026691		[learning rate: 0.00039488]
		[batch 20/20] avg loss: 0.041601614592142296		[learning rate: 0.00039441]
	Learning Rate: 0.000394411
	LOSS [training: 0.037859464116204604 | validation: 0.018134154949451715]
	TIME [epoch: 8.29 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0351662491698818		[learning rate: 0.00039395]
		[batch 20/20] avg loss: 0.03792711947755101		[learning rate: 0.00039348]
	Learning Rate: 0.00039348
	LOSS [training: 0.036546684323716404 | validation: 0.028792528123779058]
	TIME [epoch: 8.3 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034010002420966794		[learning rate: 0.00039302]
		[batch 20/20] avg loss: 0.025887531255405032		[learning rate: 0.00039255]
	Learning Rate: 0.000392552
	LOSS [training: 0.029948766838185908 | validation: 0.018403319502897326]
	TIME [epoch: 8.3 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03410573775558776		[learning rate: 0.00039209]
		[batch 20/20] avg loss: 0.037769399407190346		[learning rate: 0.00039163]
	Learning Rate: 0.000391626
	LOSS [training: 0.03593756858138906 | validation: 0.024628775118960153]
	TIME [epoch: 8.32 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03511551558451467		[learning rate: 0.00039116]
		[batch 20/20] avg loss: 0.031438566913159924		[learning rate: 0.0003907]
	Learning Rate: 0.000390702
	LOSS [training: 0.03327704124883729 | validation: 0.01744344873114753]
	TIME [epoch: 8.3 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04208118240354015		[learning rate: 0.00039024]
		[batch 20/20] avg loss: 0.02935145734003882		[learning rate: 0.00038978]
	Learning Rate: 0.000389781
	LOSS [training: 0.035716319871789484 | validation: 0.026171535819011063]
	TIME [epoch: 8.3 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03473613647535601		[learning rate: 0.00038932]
		[batch 20/20] avg loss: 0.02840395249245118		[learning rate: 0.00038886]
	Learning Rate: 0.000388861
	LOSS [training: 0.031570044483903596 | validation: 0.019782654558481515]
	TIME [epoch: 8.29 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034381175060597395		[learning rate: 0.0003884]
		[batch 20/20] avg loss: 0.038012942523695595		[learning rate: 0.00038794]
	Learning Rate: 0.000387944
	LOSS [training: 0.0361970587921465 | validation: 0.0213288889853359]
	TIME [epoch: 8.32 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057494751460713055		[learning rate: 0.00038749]
		[batch 20/20] avg loss: 0.049682618805510934		[learning rate: 0.00038703]
	Learning Rate: 0.000387029
	LOSS [training: 0.05358868513311198 | validation: 0.043305453515869334]
	TIME [epoch: 8.29 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033276646141147656		[learning rate: 0.00038657]
		[batch 20/20] avg loss: 0.03916036769532251		[learning rate: 0.00038612]
	Learning Rate: 0.000386116
	LOSS [training: 0.036218506918235076 | validation: 0.03116196965363272]
	TIME [epoch: 8.3 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03139014646291467		[learning rate: 0.00038566]
		[batch 20/20] avg loss: 0.027489939728172576		[learning rate: 0.00038521]
	Learning Rate: 0.000385205
	LOSS [training: 0.029440043095543628 | validation: 0.01753814749491893]
	TIME [epoch: 8.29 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03399856458564925		[learning rate: 0.00038475]
		[batch 20/20] avg loss: 0.03373277581158965		[learning rate: 0.0003843]
	Learning Rate: 0.000384297
	LOSS [training: 0.033865670198619455 | validation: 0.018729110116473983]
	TIME [epoch: 8.33 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053239510053103824		[learning rate: 0.00038384]
		[batch 20/20] avg loss: 0.02934240166197887		[learning rate: 0.00038339]
	Learning Rate: 0.00038339
	LOSS [training: 0.04129095585754136 | validation: 0.020406382054346642]
	TIME [epoch: 8.29 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03013793927653986		[learning rate: 0.00038294]
		[batch 20/20] avg loss: 0.03475984912817682		[learning rate: 0.00038249]
	Learning Rate: 0.000382486
	LOSS [training: 0.032448894202358344 | validation: 0.013856006573812552]
	TIME [epoch: 8.3 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045729325403435314		[learning rate: 0.00038203]
		[batch 20/20] avg loss: 0.04301312904494842		[learning rate: 0.00038158]
	Learning Rate: 0.000381584
	LOSS [training: 0.044371227224191866 | validation: 0.024637040752040727]
	TIME [epoch: 8.29 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020705082221794353		[learning rate: 0.00038113]
		[batch 20/20] avg loss: 0.0412075783801173		[learning rate: 0.00038068]
	Learning Rate: 0.000380684
	LOSS [training: 0.030956330300955827 | validation: 0.03838353301408471]
	TIME [epoch: 8.32 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04242398246137609		[learning rate: 0.00038023]
		[batch 20/20] avg loss: 0.038598378158177016		[learning rate: 0.00037979]
	Learning Rate: 0.000379786
	LOSS [training: 0.04051118030977656 | validation: 0.02579738794606684]
	TIME [epoch: 8.29 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03939738957983026		[learning rate: 0.00037934]
		[batch 20/20] avg loss: 0.047512931494084365		[learning rate: 0.00037889]
	Learning Rate: 0.00037889
	LOSS [training: 0.043455160536957305 | validation: 0.0850079534752602]
	TIME [epoch: 8.3 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03916079788435217		[learning rate: 0.00037844]
		[batch 20/20] avg loss: 0.03700743151242762		[learning rate: 0.000378]
	Learning Rate: 0.000377996
	LOSS [training: 0.0380841146983899 | validation: 0.02184947252880662]
	TIME [epoch: 8.3 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03049051153280465		[learning rate: 0.00037755]
		[batch 20/20] avg loss: 0.036191794495177676		[learning rate: 0.0003771]
	Learning Rate: 0.000377104
	LOSS [training: 0.03334115301399116 | validation: 0.04177595043981718]
	TIME [epoch: 8.32 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03552260664350635		[learning rate: 0.00037666]
		[batch 20/20] avg loss: 0.03335105633831832		[learning rate: 0.00037621]
	Learning Rate: 0.000376215
	LOSS [training: 0.03443683149091233 | validation: 0.02821188867981835]
	TIME [epoch: 8.3 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03216443704903199		[learning rate: 0.00037577]
		[batch 20/20] avg loss: 0.03542127871861988		[learning rate: 0.00037533]
	Learning Rate: 0.000375327
	LOSS [training: 0.03379285788382594 | validation: 0.019591693185425266]
	TIME [epoch: 8.3 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03443108824059326		[learning rate: 0.00037488]
		[batch 20/20] avg loss: 0.029899173585598286		[learning rate: 0.00037444]
	Learning Rate: 0.000374442
	LOSS [training: 0.032165130913095766 | validation: 0.01984044059657818]
	TIME [epoch: 8.3 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031755772379790805		[learning rate: 0.000374]
		[batch 20/20] avg loss: 0.02839884267851774		[learning rate: 0.00037356]
	Learning Rate: 0.000373559
	LOSS [training: 0.030077307529154274 | validation: 0.017938395736176244]
	TIME [epoch: 8.32 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03098318207777621		[learning rate: 0.00037312]
		[batch 20/20] avg loss: 0.03286590978817079		[learning rate: 0.00037268]
	Learning Rate: 0.000372678
	LOSS [training: 0.0319245459329735 | validation: 0.018632831630060888]
	TIME [epoch: 8.3 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044309966130708214		[learning rate: 0.00037224]
		[batch 20/20] avg loss: 0.03381444230890235		[learning rate: 0.0003718]
	Learning Rate: 0.000371799
	LOSS [training: 0.039062204219805274 | validation: 0.014835859136364224]
	TIME [epoch: 8.29 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0521793513568519		[learning rate: 0.00037136]
		[batch 20/20] avg loss: 0.03201471972726787		[learning rate: 0.00037092]
	Learning Rate: 0.000370922
	LOSS [training: 0.04209703554205988 | validation: 0.022174038014277912]
	TIME [epoch: 8.29 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03283095749170077		[learning rate: 0.00037048]
		[batch 20/20] avg loss: 0.03190087491797269		[learning rate: 0.00037005]
	Learning Rate: 0.000370047
	LOSS [training: 0.03236591620483673 | validation: 0.02512961370406506]
	TIME [epoch: 8.31 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027001931909530163		[learning rate: 0.00036961]
		[batch 20/20] avg loss: 0.030413055930394246		[learning rate: 0.00036917]
	Learning Rate: 0.000369174
	LOSS [training: 0.0287074939199622 | validation: 0.021612354620031717]
	TIME [epoch: 8.29 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02486075504920178		[learning rate: 0.00036874]
		[batch 20/20] avg loss: 0.05700091432302436		[learning rate: 0.0003683]
	Learning Rate: 0.000368303
	LOSS [training: 0.04093083468611307 | validation: 0.013839964503416758]
	TIME [epoch: 8.3 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041622724255787745		[learning rate: 0.00036787]
		[batch 20/20] avg loss: 0.035636092423050394		[learning rate: 0.00036743]
	Learning Rate: 0.000367434
	LOSS [training: 0.038629408339419066 | validation: 0.029949405641257896]
	TIME [epoch: 8.3 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03818852238102754		[learning rate: 0.000367]
		[batch 20/20] avg loss: 0.04393558369514598		[learning rate: 0.00036657]
	Learning Rate: 0.000366567
	LOSS [training: 0.04106205303808676 | validation: 0.032624461787600166]
	TIME [epoch: 8.32 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03683185389793519		[learning rate: 0.00036613]
		[batch 20/20] avg loss: 0.03998692320089612		[learning rate: 0.0003657]
	Learning Rate: 0.000365703
	LOSS [training: 0.038409388549415655 | validation: 0.03146841868412622]
	TIME [epoch: 8.3 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028229648907595162		[learning rate: 0.00036527]
		[batch 20/20] avg loss: 0.056072428647463333		[learning rate: 0.00036484]
	Learning Rate: 0.00036484
	LOSS [training: 0.04215103877752925 | validation: 0.024919858294043223]
	TIME [epoch: 8.29 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046116652761063526		[learning rate: 0.00036441]
		[batch 20/20] avg loss: 0.05164486002718658		[learning rate: 0.00036398]
	Learning Rate: 0.000363979
	LOSS [training: 0.04888075639412505 | validation: 0.017166554352669974]
	TIME [epoch: 8.3 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02283888844775581		[learning rate: 0.00036355]
		[batch 20/20] avg loss: 0.02861918251768187		[learning rate: 0.00036312]
	Learning Rate: 0.000363121
	LOSS [training: 0.02572903548271884 | validation: 0.03613581740947869]
	TIME [epoch: 8.32 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03398560677559255		[learning rate: 0.00036269]
		[batch 20/20] avg loss: 0.032293638197189466		[learning rate: 0.00036226]
	Learning Rate: 0.000362264
	LOSS [training: 0.03313962248639101 | validation: 0.013606119078191897]
	TIME [epoch: 8.29 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03507431846829531		[learning rate: 0.00036184]
		[batch 20/20] avg loss: 0.03209712925065843		[learning rate: 0.00036141]
	Learning Rate: 0.00036141
	LOSS [training: 0.03358572385947687 | validation: 0.021721664965824783]
	TIME [epoch: 8.29 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0380078622585431		[learning rate: 0.00036098]
		[batch 20/20] avg loss: 0.024539945399264677		[learning rate: 0.00036056]
	Learning Rate: 0.000360557
	LOSS [training: 0.031273903828903896 | validation: 0.026580385962536025]
	TIME [epoch: 8.29 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034908011831189745		[learning rate: 0.00036013]
		[batch 20/20] avg loss: 0.0272219107758487		[learning rate: 0.00035971]
	Learning Rate: 0.000359707
	LOSS [training: 0.031064961303519212 | validation: 0.02488413241576605]
	TIME [epoch: 8.31 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026618678132956146		[learning rate: 0.00035928]
		[batch 20/20] avg loss: 0.033562023722428336		[learning rate: 0.00035886]
	Learning Rate: 0.000358858
	LOSS [training: 0.030090350927692237 | validation: 0.02084109578330587]
	TIME [epoch: 8.3 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032026549724354245		[learning rate: 0.00035843]
		[batch 20/20] avg loss: 0.04111225843668974		[learning rate: 0.00035801]
	Learning Rate: 0.000358012
	LOSS [training: 0.03656940408052199 | validation: 0.02070862339196329]
	TIME [epoch: 8.3 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0352689971090722		[learning rate: 0.00035759]
		[batch 20/20] avg loss: 0.029719281739534448		[learning rate: 0.00035717]
	Learning Rate: 0.000357167
	LOSS [training: 0.03249413942430332 | validation: 0.030821144030702557]
	TIME [epoch: 8.3 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033883211271229965		[learning rate: 0.00035675]
		[batch 20/20] avg loss: 0.030041921940804607		[learning rate: 0.00035632]
	Learning Rate: 0.000356325
	LOSS [training: 0.03196256660601729 | validation: 0.0252542822093303]
	TIME [epoch: 8.32 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03621880855054763		[learning rate: 0.0003559]
		[batch 20/20] avg loss: 0.027903974752306854		[learning rate: 0.00035548]
	Learning Rate: 0.000355484
	LOSS [training: 0.03206139165142725 | validation: 0.017740871657130983]
	TIME [epoch: 8.29 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031104071676592904		[learning rate: 0.00035506]
		[batch 20/20] avg loss: 0.036860160848495534		[learning rate: 0.00035465]
	Learning Rate: 0.000354646
	LOSS [training: 0.03398211626254423 | validation: 0.018493508171286796]
	TIME [epoch: 8.3 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03328446788926747		[learning rate: 0.00035423]
		[batch 20/20] avg loss: 0.035803839715083455		[learning rate: 0.00035381]
	Learning Rate: 0.000353809
	LOSS [training: 0.034544153802175466 | validation: 0.032225721136564155]
	TIME [epoch: 8.3 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03882477699486898		[learning rate: 0.00035339]
		[batch 20/20] avg loss: 0.027925176468286222		[learning rate: 0.00035297]
	Learning Rate: 0.000352975
	LOSS [training: 0.033374976731577605 | validation: 0.022282703371638984]
	TIME [epoch: 8.32 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029928845060991166		[learning rate: 0.00035256]
		[batch 20/20] avg loss: 0.032166749635626816		[learning rate: 0.00035214]
	Learning Rate: 0.000352142
	LOSS [training: 0.03104779734830899 | validation: 0.013823487243362432]
	TIME [epoch: 8.29 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03556695576060789		[learning rate: 0.00035173]
		[batch 20/20] avg loss: 0.037137624907883894		[learning rate: 0.00035131]
	Learning Rate: 0.000351311
	LOSS [training: 0.036352290334245904 | validation: 0.0237818363464649]
	TIME [epoch: 8.3 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035758735487733		[learning rate: 0.0003509]
		[batch 20/20] avg loss: 0.03623407917645084		[learning rate: 0.00035048]
	Learning Rate: 0.000350483
	LOSS [training: 0.03599640733209192 | validation: 0.01536445484159743]
	TIME [epoch: 8.3 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03449062877028786		[learning rate: 0.00035007]
		[batch 20/20] avg loss: 0.03093739356481904		[learning rate: 0.00034966]
	Learning Rate: 0.000349656
	LOSS [training: 0.032714011167553445 | validation: 0.019513465421545525]
	TIME [epoch: 8.32 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022272716196929727		[learning rate: 0.00034924]
		[batch 20/20] avg loss: 0.04107175797499033		[learning rate: 0.00034883]
	Learning Rate: 0.000348831
	LOSS [training: 0.03167223708596002 | validation: 0.02102238510136797]
	TIME [epoch: 8.3 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0336705679725388		[learning rate: 0.00034842]
		[batch 20/20] avg loss: 0.028881791267945968		[learning rate: 0.00034801]
	Learning Rate: 0.000348008
	LOSS [training: 0.03127617962024238 | validation: 0.024552928128468477]
	TIME [epoch: 8.29 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03408146115434228		[learning rate: 0.0003476]
		[batch 20/20] avg loss: 0.03684957583135119		[learning rate: 0.00034719]
	Learning Rate: 0.000347187
	LOSS [training: 0.03546551849284674 | validation: 0.01666165180040413]
	TIME [epoch: 8.3 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03147783953383819		[learning rate: 0.00034678]
		[batch 20/20] avg loss: 0.03360569499822458		[learning rate: 0.00034637]
	Learning Rate: 0.000346369
	LOSS [training: 0.03254176726603139 | validation: 0.05664785707449777]
	TIME [epoch: 8.31 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0353640942500225		[learning rate: 0.00034596]
		[batch 20/20] avg loss: 0.027832779275100868		[learning rate: 0.00034555]
	Learning Rate: 0.000345552
	LOSS [training: 0.03159843676256169 | validation: 0.019749862503522987]
	TIME [epoch: 8.3 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03415930367877083		[learning rate: 0.00034514]
		[batch 20/20] avg loss: 0.0418021735397715		[learning rate: 0.00034474]
	Learning Rate: 0.000344736
	LOSS [training: 0.03798073860927117 | validation: 0.02545732229434718]
	TIME [epoch: 8.3 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027912081782476155		[learning rate: 0.00034433]
		[batch 20/20] avg loss: 0.025269228659249816		[learning rate: 0.00034392]
	Learning Rate: 0.000343923
	LOSS [training: 0.026590655220862985 | validation: 0.0444260734840657]
	TIME [epoch: 8.3 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03630270224931592		[learning rate: 0.00034352]
		[batch 20/20] avg loss: 0.03317965819805507		[learning rate: 0.00034311]
	Learning Rate: 0.000343112
	LOSS [training: 0.03474118022368549 | validation: 0.03539494781985705]
	TIME [epoch: 8.31 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03630819585796623		[learning rate: 0.00034271]
		[batch 20/20] avg loss: 0.039020081767979345		[learning rate: 0.0003423]
	Learning Rate: 0.000342303
	LOSS [training: 0.03766413881297278 | validation: 0.03395748931133995]
	TIME [epoch: 8.31 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02808604300639179		[learning rate: 0.0003419]
		[batch 20/20] avg loss: 0.030632941221375932		[learning rate: 0.0003415]
	Learning Rate: 0.000341495
	LOSS [training: 0.02935949211388386 | validation: 0.023702402851940342]
	TIME [epoch: 8.29 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03164230657759608		[learning rate: 0.00034109]
		[batch 20/20] avg loss: 0.027590565846631908		[learning rate: 0.00034069]
	Learning Rate: 0.00034069
	LOSS [training: 0.029616436212113996 | validation: 0.023961493922885966]
	TIME [epoch: 8.31 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05352189463831312		[learning rate: 0.00034029]
		[batch 20/20] avg loss: 0.030120654930694463		[learning rate: 0.00033989]
	Learning Rate: 0.000339886
	LOSS [training: 0.041821274784503794 | validation: 0.016395739701472616]
	TIME [epoch: 8.32 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03386120086462668		[learning rate: 0.00033948]
		[batch 20/20] avg loss: 0.05047022611462319		[learning rate: 0.00033908]
	Learning Rate: 0.000339084
	LOSS [training: 0.04216571348962493 | validation: 0.0149093779434348]
	TIME [epoch: 8.3 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02222553868734345		[learning rate: 0.00033868]
		[batch 20/20] avg loss: 0.04058825000343484		[learning rate: 0.00033828]
	Learning Rate: 0.000338284
	LOSS [training: 0.031406894345389146 | validation: 0.019650384252580567]
	TIME [epoch: 8.3 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027723663676619843		[learning rate: 0.00033789]
		[batch 20/20] avg loss: 0.02793498875479859		[learning rate: 0.00033749]
	Learning Rate: 0.000337487
	LOSS [training: 0.027829326215709216 | validation: 0.01780940086455498]
	TIME [epoch: 8.29 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029939449555990816		[learning rate: 0.00033709]
		[batch 20/20] avg loss: 0.029397015662133246		[learning rate: 0.00033669]
	Learning Rate: 0.00033669
	LOSS [training: 0.02966823260906203 | validation: 0.017246946910754714]
	TIME [epoch: 8.32 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026950363416693247		[learning rate: 0.00033629]
		[batch 20/20] avg loss: 0.04427855501355911		[learning rate: 0.0003359]
	Learning Rate: 0.000335896
	LOSS [training: 0.03561445921512618 | validation: 0.060140263483192596]
	TIME [epoch: 8.29 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03678217455299042		[learning rate: 0.0003355]
		[batch 20/20] avg loss: 0.025191007311449444		[learning rate: 0.0003351]
	Learning Rate: 0.000335104
	LOSS [training: 0.030986590932219938 | validation: 0.04321156121634031]
	TIME [epoch: 8.3 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04899508009864008		[learning rate: 0.00033471]
		[batch 20/20] avg loss: 0.04198231228425558		[learning rate: 0.00033431]
	Learning Rate: 0.000334313
	LOSS [training: 0.04548869619144783 | validation: 0.01921214932204926]
	TIME [epoch: 8.29 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030927264201830822		[learning rate: 0.00033392]
		[batch 20/20] avg loss: 0.03179698569737731		[learning rate: 0.00033352]
	Learning Rate: 0.000333525
	LOSS [training: 0.03136212494960407 | validation: 0.020655711386687557]
	TIME [epoch: 8.32 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024214190048541064		[learning rate: 0.00033313]
		[batch 20/20] avg loss: 0.03550424267349947		[learning rate: 0.00033274]
	Learning Rate: 0.000332738
	LOSS [training: 0.02985921636102027 | validation: 0.04042776914521458]
	TIME [epoch: 8.3 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04833036720242995		[learning rate: 0.00033235]
		[batch 20/20] avg loss: 0.04067604623688743		[learning rate: 0.00033195]
	Learning Rate: 0.000331953
	LOSS [training: 0.04450320671965868 | validation: 0.02539589571459127]
	TIME [epoch: 8.3 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030835872400770507		[learning rate: 0.00033156]
		[batch 20/20] avg loss: 0.032351869230577604		[learning rate: 0.00033117]
	Learning Rate: 0.00033117
	LOSS [training: 0.03159387081567405 | validation: 0.017673642627423634]
	TIME [epoch: 8.3 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04839830762168633		[learning rate: 0.00033078]
		[batch 20/20] avg loss: 0.026181606667140235		[learning rate: 0.00033039]
	Learning Rate: 0.000330389
	LOSS [training: 0.03728995714441328 | validation: 0.02375280700956455]
	TIME [epoch: 8.32 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04322476837151833		[learning rate: 0.00033]
		[batch 20/20] avg loss: 0.038878613020675176		[learning rate: 0.00032961]
	Learning Rate: 0.00032961
	LOSS [training: 0.04105169069609676 | validation: 0.02498810163889254]
	TIME [epoch: 8.3 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04520587362055757		[learning rate: 0.00032922]
		[batch 20/20] avg loss: 0.022352356896001527		[learning rate: 0.00032883]
	Learning Rate: 0.000328832
	LOSS [training: 0.03377911525827955 | validation: 0.019653065890951633]
	TIME [epoch: 8.3 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03362268938276677		[learning rate: 0.00032844]
		[batch 20/20] avg loss: 0.02930976886301635		[learning rate: 0.00032806]
	Learning Rate: 0.000328056
	LOSS [training: 0.031466229122891556 | validation: 0.03627816721820668]
	TIME [epoch: 8.3 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03215354335557826		[learning rate: 0.00032767]
		[batch 20/20] avg loss: 0.03712807912500052		[learning rate: 0.00032728]
	Learning Rate: 0.000327283
	LOSS [training: 0.034640811240289386 | validation: 0.037347320105844586]
	TIME [epoch: 8.32 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03425349793045074		[learning rate: 0.0003269]
		[batch 20/20] avg loss: 0.027805222494067285		[learning rate: 0.00032651]
	Learning Rate: 0.000326511
	LOSS [training: 0.031029360212259 | validation: 0.017308329714766132]
	TIME [epoch: 8.29 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021790520891697		[learning rate: 0.00032613]
		[batch 20/20] avg loss: 0.036644085966794936		[learning rate: 0.00032574]
	Learning Rate: 0.00032574
	LOSS [training: 0.029217303429245965 | validation: 0.04659775650686624]
	TIME [epoch: 8.3 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03087566073353228		[learning rate: 0.00032536]
		[batch 20/20] avg loss: 0.02132797978197414		[learning rate: 0.00032497]
	Learning Rate: 0.000324972
	LOSS [training: 0.026101820257753205 | validation: 0.020661413245294354]
	TIME [epoch: 8.3 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033664030043342986		[learning rate: 0.00032459]
		[batch 20/20] avg loss: 0.03380577955381321		[learning rate: 0.00032421]
	Learning Rate: 0.000324206
	LOSS [training: 0.033734904798578096 | validation: 0.028581282965741913]
	TIME [epoch: 8.32 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02865476513360991		[learning rate: 0.00032382]
		[batch 20/20] avg loss: 0.03948411709191853		[learning rate: 0.00032344]
	Learning Rate: 0.000323441
	LOSS [training: 0.034069441112764214 | validation: 0.03224276003360506]
	TIME [epoch: 8.29 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034998385968924864		[learning rate: 0.00032306]
		[batch 20/20] avg loss: 0.040009495864609355		[learning rate: 0.00032268]
	Learning Rate: 0.000322678
	LOSS [training: 0.037503940916767106 | validation: 0.026786484704425406]
	TIME [epoch: 8.29 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031108730192107865		[learning rate: 0.0003223]
		[batch 20/20] avg loss: 0.03510736667723868		[learning rate: 0.00032192]
	Learning Rate: 0.000321917
	LOSS [training: 0.03310804843467328 | validation: 0.025690416336304]
	TIME [epoch: 8.3 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02726985696621422		[learning rate: 0.00032154]
		[batch 20/20] avg loss: 0.036487640387782586		[learning rate: 0.00032116]
	Learning Rate: 0.000321157
	LOSS [training: 0.03187874867699841 | validation: 0.020039380964146133]
	TIME [epoch: 8.31 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030273319008761117		[learning rate: 0.00032078]
		[batch 20/20] avg loss: 0.027940260367876994		[learning rate: 0.0003204]
	Learning Rate: 0.0003204
	LOSS [training: 0.029106789688319052 | validation: 0.037387121933467465]
	TIME [epoch: 8.3 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0640458207970456		[learning rate: 0.00032002]
		[batch 20/20] avg loss: 0.025909504577643433		[learning rate: 0.00031964]
	Learning Rate: 0.000319644
	LOSS [training: 0.044977662687344516 | validation: 0.03344240837019527]
	TIME [epoch: 8.31 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022308923835680793		[learning rate: 0.00031927]
		[batch 20/20] avg loss: 0.03187534970164037		[learning rate: 0.00031889]
	Learning Rate: 0.00031889
	LOSS [training: 0.027092136768660586 | validation: 0.024153184848438716]
	TIME [epoch: 8.3 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03605098956481552		[learning rate: 0.00031851]
		[batch 20/20] avg loss: 0.027126484672380304		[learning rate: 0.00031814]
	Learning Rate: 0.000318138
	LOSS [training: 0.03158873711859791 | validation: 0.018670794049943373]
	TIME [epoch: 8.32 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06791093793439408		[learning rate: 0.00031776]
		[batch 20/20] avg loss: 0.03659456021580235		[learning rate: 0.00031739]
	Learning Rate: 0.000317387
	LOSS [training: 0.052252749075098213 | validation: 0.015904887210419563]
	TIME [epoch: 8.3 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038277272118900404		[learning rate: 0.00031701]
		[batch 20/20] avg loss: 0.02365097378955008		[learning rate: 0.00031664]
	Learning Rate: 0.000316639
	LOSS [training: 0.030964122954225244 | validation: 0.009052949555657682]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_1512.pth
	Model improved!!!
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03154671882955514		[learning rate: 0.00031627]
		[batch 20/20] avg loss: 0.03348082080974224		[learning rate: 0.00031589]
	Learning Rate: 0.000315892
	LOSS [training: 0.032513769819648694 | validation: 0.018211096665742436]
	TIME [epoch: 8.32 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03212521918111389		[learning rate: 0.00031552]
		[batch 20/20] avg loss: 0.03692505853595025		[learning rate: 0.00031515]
	Learning Rate: 0.000315147
	LOSS [training: 0.034525138858532076 | validation: 0.02263903945503993]
	TIME [epoch: 8.32 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03208915011298919		[learning rate: 0.00031477]
		[batch 20/20] avg loss: 0.030885177333954777		[learning rate: 0.0003144]
	Learning Rate: 0.000314403
	LOSS [training: 0.03148716372347199 | validation: 0.03851944474220092]
	TIME [epoch: 8.3 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035444494502211175		[learning rate: 0.00031403]
		[batch 20/20] avg loss: 0.02956247049741379		[learning rate: 0.00031366]
	Learning Rate: 0.000313662
	LOSS [training: 0.03250348249981248 | validation: 0.029521643983827884]
	TIME [epoch: 8.31 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03841183907661293		[learning rate: 0.00031329]
		[batch 20/20] avg loss: 0.02893011826020058		[learning rate: 0.00031292]
	Learning Rate: 0.000312922
	LOSS [training: 0.03367097866840675 | validation: 0.026939570872378103]
	TIME [epoch: 8.32 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0352322506392979		[learning rate: 0.00031255]
		[batch 20/20] avg loss: 0.050259023807535365		[learning rate: 0.00031218]
	Learning Rate: 0.000312184
	LOSS [training: 0.04274563722341663 | validation: 0.03655417545795477]
	TIME [epoch: 8.32 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02729069861820682		[learning rate: 0.00031182]
		[batch 20/20] avg loss: 0.02850142316947435		[learning rate: 0.00031145]
	Learning Rate: 0.000311447
	LOSS [training: 0.027896060893840585 | validation: 0.01925308494346472]
	TIME [epoch: 8.29 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019780214096105246		[learning rate: 0.00031108]
		[batch 20/20] avg loss: 0.045584459229927675		[learning rate: 0.00031071]
	Learning Rate: 0.000310713
	LOSS [training: 0.03268233666301646 | validation: 0.016801511911840226]
	TIME [epoch: 8.3 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03135333419117779		[learning rate: 0.00031035]
		[batch 20/20] avg loss: 0.04212625731430326		[learning rate: 0.00030998]
	Learning Rate: 0.00030998
	LOSS [training: 0.03673979575274052 | validation: 0.018158441763113813]
	TIME [epoch: 8.32 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02112681199894937		[learning rate: 0.00030961]
		[batch 20/20] avg loss: 0.029792569566149375		[learning rate: 0.00030925]
	Learning Rate: 0.000309249
	LOSS [training: 0.025459690782549366 | validation: 0.015347885575984797]
	TIME [epoch: 8.32 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04085287491537461		[learning rate: 0.00030888]
		[batch 20/20] avg loss: 0.026065398727048123		[learning rate: 0.00030852]
	Learning Rate: 0.000308519
	LOSS [training: 0.03345913682121136 | validation: 0.019278415802028596]
	TIME [epoch: 8.3 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021345720817134695		[learning rate: 0.00030815]
		[batch 20/20] avg loss: 0.03002263072154019		[learning rate: 0.00030779]
	Learning Rate: 0.000307791
	LOSS [training: 0.025684175769337448 | validation: 0.025324111462329765]
	TIME [epoch: 8.3 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029955478733548292		[learning rate: 0.00030743]
		[batch 20/20] avg loss: 0.03362630872024795		[learning rate: 0.00030707]
	Learning Rate: 0.000307065
	LOSS [training: 0.03179089372689812 | validation: 0.01702185337264816]
	TIME [epoch: 8.32 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020540515106368055		[learning rate: 0.0003067]
		[batch 20/20] avg loss: 0.03588940402260549		[learning rate: 0.00030634]
	Learning Rate: 0.000306341
	LOSS [training: 0.028214959564486775 | validation: 0.021165372335666736]
	TIME [epoch: 8.31 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030338181704445355		[learning rate: 0.00030598]
		[batch 20/20] avg loss: 0.03805349718821226		[learning rate: 0.00030562]
	Learning Rate: 0.000305618
	LOSS [training: 0.034195839446328806 | validation: 0.052259467043263556]
	TIME [epoch: 8.3 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03570809693514586		[learning rate: 0.00030526]
		[batch 20/20] avg loss: 0.02678507320610522		[learning rate: 0.0003049]
	Learning Rate: 0.000304897
	LOSS [training: 0.031246585070625547 | validation: 0.016889222157892705]
	TIME [epoch: 8.3 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03045507551376635		[learning rate: 0.00030454]
		[batch 20/20] avg loss: 0.025528126029099662		[learning rate: 0.00030418]
	Learning Rate: 0.000304178
	LOSS [training: 0.027991600771433013 | validation: 0.03404741545176455]
	TIME [epoch: 8.32 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04290859342931717		[learning rate: 0.00030382]
		[batch 20/20] avg loss: 0.029855688704929682		[learning rate: 0.00030346]
	Learning Rate: 0.000303461
	LOSS [training: 0.03638214106712342 | validation: 0.02097875183375319]
	TIME [epoch: 8.31 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034157473781401326		[learning rate: 0.0003031]
		[batch 20/20] avg loss: 0.027154812880327693		[learning rate: 0.00030274]
	Learning Rate: 0.000302745
	LOSS [training: 0.03065614333086451 | validation: 0.02672831758385711]
	TIME [epoch: 8.3 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03340867771784599		[learning rate: 0.00030239]
		[batch 20/20] avg loss: 0.029173915687801956		[learning rate: 0.00030203]
	Learning Rate: 0.000302031
	LOSS [training: 0.031291296702823976 | validation: 0.016024964840684712]
	TIME [epoch: 8.3 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0265113018441094		[learning rate: 0.00030167]
		[batch 20/20] avg loss: 0.03540320009291501		[learning rate: 0.00030132]
	Learning Rate: 0.000301318
	LOSS [training: 0.030957250968512207 | validation: 0.02305462907831537]
	TIME [epoch: 8.32 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028543602519727714		[learning rate: 0.00030096]
		[batch 20/20] avg loss: 0.04323090473355061		[learning rate: 0.00030061]
	Learning Rate: 0.000300608
	LOSS [training: 0.03588725362663916 | validation: 0.03737623614580039]
	TIME [epoch: 8.31 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04978002421908486		[learning rate: 0.00030025]
		[batch 20/20] avg loss: 0.041919020453072296		[learning rate: 0.0002999]
	Learning Rate: 0.000299899
	LOSS [training: 0.04584952233607857 | validation: 0.03276377313132897]
	TIME [epoch: 8.3 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03304182523930103		[learning rate: 0.00029954]
		[batch 20/20] avg loss: 0.027995434317596057		[learning rate: 0.00029919]
	Learning Rate: 0.000299191
	LOSS [training: 0.03051862977844854 | validation: 0.005403240096426818]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r2_20240219_184950/states/model_tr_study2_1536.pth
	Model improved!!!
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032520541272017964		[learning rate: 0.00029884]
		[batch 20/20] avg loss: 0.031523669924477984		[learning rate: 0.00029849]
	Learning Rate: 0.000298485
	LOSS [training: 0.03202210559824798 | validation: 0.03196079004798451]
	TIME [epoch: 8.33 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032499856886847256		[learning rate: 0.00029813]
		[batch 20/20] avg loss: 0.022825020354889126		[learning rate: 0.00029778]
	Learning Rate: 0.000297781
	LOSS [training: 0.02766243862086819 | validation: 0.018656776544923682]
	TIME [epoch: 8.31 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02400399031679767		[learning rate: 0.00029743]
		[batch 20/20] avg loss: 0.02713539593471157		[learning rate: 0.00029708]
	Learning Rate: 0.000297079
	LOSS [training: 0.025569693125754616 | validation: 0.022590114393991695]
	TIME [epoch: 8.3 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05543883116409544		[learning rate: 0.00029673]
		[batch 20/20] avg loss: 0.037832646574729414		[learning rate: 0.00029638]
	Learning Rate: 0.000296378
	LOSS [training: 0.04663573886941243 | validation: 0.016256560704656475]
	TIME [epoch: 8.3 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03394402429178459		[learning rate: 0.00029603]
		[batch 20/20] avg loss: 0.03655890660027554		[learning rate: 0.00029568]
	Learning Rate: 0.000295679
	LOSS [training: 0.035251465446030056 | validation: 0.023916955199087125]
	TIME [epoch: 8.32 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025959257612018165		[learning rate: 0.00029533]
		[batch 20/20] avg loss: 0.04818305877664394		[learning rate: 0.00029498]
	Learning Rate: 0.000294982
	LOSS [training: 0.03707115819433106 | validation: 0.018987809052198146]
	TIME [epoch: 8.31 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029608819226680037		[learning rate: 0.00029463]
		[batch 20/20] avg loss: 0.02789934411384254		[learning rate: 0.00029429]
	Learning Rate: 0.000294286
	LOSS [training: 0.028754081670261295 | validation: 0.02287073763168128]
	TIME [epoch: 8.31 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02948380082830799		[learning rate: 0.00029394]
		[batch 20/20] avg loss: 0.03278760901596107		[learning rate: 0.00029359]
	Learning Rate: 0.000293592
	LOSS [training: 0.03113570492213453 | validation: 0.018248270756447166]
	TIME [epoch: 8.3 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030353289353181268		[learning rate: 0.00029325]
		[batch 20/20] avg loss: 0.026262924224155482		[learning rate: 0.0002929]
	Learning Rate: 0.000292899
	LOSS [training: 0.028308106788668373 | validation: 0.022047162053296543]
	TIME [epoch: 8.32 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02613813028559462		[learning rate: 0.00029255]
		[batch 20/20] avg loss: 0.03636139966866757		[learning rate: 0.00029221]
	Learning Rate: 0.000292208
	LOSS [training: 0.03124976497713109 | validation: 0.030259893798297327]
	TIME [epoch: 8.31 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027496101499971504		[learning rate: 0.00029186]
		[batch 20/20] avg loss: 0.03191549493464345		[learning rate: 0.00029152]
	Learning Rate: 0.000291519
	LOSS [training: 0.029705798217307476 | validation: 0.01686492527331923]
	TIME [epoch: 8.31 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029333866515394387		[learning rate: 0.00029117]
		[batch 20/20] avg loss: 0.030060183537594203		[learning rate: 0.00029083]
	Learning Rate: 0.000290831
	LOSS [training: 0.029697025026494295 | validation: 0.015569399696691296]
	TIME [epoch: 8.29 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02722908838156331		[learning rate: 0.00029049]
		[batch 20/20] avg loss: 0.03440442947929068		[learning rate: 0.00029015]
	Learning Rate: 0.000290145
	LOSS [training: 0.030816758930426996 | validation: 0.01696949630641485]
	TIME [epoch: 8.31 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034181570706222686		[learning rate: 0.0002898]
		[batch 20/20] avg loss: 0.025402951184527327		[learning rate: 0.00028946]
	Learning Rate: 0.000289461
	LOSS [training: 0.02979226094537501 | validation: 0.02083212740797529]
	TIME [epoch: 8.32 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026548911643131973		[learning rate: 0.00028912]
		[batch 20/20] avg loss: 0.03319508731349715		[learning rate: 0.00028878]
	Learning Rate: 0.000288778
	LOSS [training: 0.029871999478314572 | validation: 0.026982770603108833]
	TIME [epoch: 8.3 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02304499923122585		[learning rate: 0.00028844]
		[batch 20/20] avg loss: 0.03668838600517345		[learning rate: 0.0002881]
	Learning Rate: 0.000288097
	LOSS [training: 0.02986669261819964 | validation: 0.016019376838489766]
	TIME [epoch: 8.31 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021997349828931277		[learning rate: 0.00028776]
		[batch 20/20] avg loss: 0.017275276026604457		[learning rate: 0.00028742]
	Learning Rate: 0.000287417
	LOSS [training: 0.01963631292776787 | validation: 0.020692467042609426]
	TIME [epoch: 8.32 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02752325683783353		[learning rate: 0.00028708]
		[batch 20/20] avg loss: 0.023769352783369647		[learning rate: 0.00028674]
	Learning Rate: 0.000286739
	LOSS [training: 0.025646304810601595 | validation: 0.02388394469391016]
	TIME [epoch: 8.32 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056618806260964515		[learning rate: 0.0002864]
		[batch 20/20] avg loss: 0.03683172668639443		[learning rate: 0.00028606]
	Learning Rate: 0.000286063
	LOSS [training: 0.04672526647367947 | validation: 0.027255746402438368]
	TIME [epoch: 8.31 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02422906410576455		[learning rate: 0.00028573]
		[batch 20/20] avg loss: 0.027892297517626063		[learning rate: 0.00028539]
	Learning Rate: 0.000285388
	LOSS [training: 0.026060680811695304 | validation: 0.04289129885594805]
	TIME [epoch: 8.3 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029275105167330718		[learning rate: 0.00028505]
		[batch 20/20] avg loss: 0.0290714876801199		[learning rate: 0.00028471]
	Learning Rate: 0.000284715
	LOSS [training: 0.02917329642372531 | validation: 0.04386299826986424]
	TIME [epoch: 8.33 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03269267469688434		[learning rate: 0.00028438]
		[batch 20/20] avg loss: 0.030362896290032953		[learning rate: 0.00028404]
	Learning Rate: 0.000284043
	LOSS [training: 0.031527785493458645 | validation: 0.021608489586781975]
	TIME [epoch: 8.32 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028730137619151108		[learning rate: 0.00028371]
		[batch 20/20] avg loss: 0.04481061224636594		[learning rate: 0.00028337]
	Learning Rate: 0.000283373
	LOSS [training: 0.036770374932758525 | validation: 0.027647406205934207]
	TIME [epoch: 8.31 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03234601958297063		[learning rate: 0.00028304]
		[batch 20/20] avg loss: 0.04313310168990142		[learning rate: 0.0002827]
	Learning Rate: 0.000282705
	LOSS [training: 0.03773956063643602 | validation: 0.059194387698996154]
	TIME [epoch: 8.3 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04474818449698329		[learning rate: 0.00028237]
		[batch 20/20] avg loss: 0.025165786937050884		[learning rate: 0.00028204]
	Learning Rate: 0.000282038
	LOSS [training: 0.03495698571701708 | validation: 0.014597538898558438]
	TIME [epoch: 8.34 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04155121675029089		[learning rate: 0.00028171]
		[batch 20/20] avg loss: 0.030371273533002573		[learning rate: 0.00028137]
	Learning Rate: 0.000281373
	LOSS [training: 0.035961245141646725 | validation: 0.017554611381674908]
	TIME [epoch: 8.31 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02386075596747161		[learning rate: 0.00028104]
		[batch 20/20] avg loss: 0.042060015720155774		[learning rate: 0.00028071]
	Learning Rate: 0.000280709
	LOSS [training: 0.0329603858438137 | validation: 0.00694776492887641]
	TIME [epoch: 8.31 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029474411293158864		[learning rate: 0.00028038]
		[batch 20/20] avg loss: 0.02482328331718588		[learning rate: 0.00028005]
	Learning Rate: 0.000280047
	LOSS [training: 0.027148847305172374 | validation: 0.02622834049673166]
	TIME [epoch: 8.3 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027797878967188345		[learning rate: 0.00027972]
		[batch 20/20] avg loss: 0.019430808008993095		[learning rate: 0.00027939]
	Learning Rate: 0.000279386
	LOSS [training: 0.023614343488090724 | validation: 0.0352196060697418]
	TIME [epoch: 8.33 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025729196441352707		[learning rate: 0.00027906]
		[batch 20/20] avg loss: 0.03467380057861752		[learning rate: 0.00027873]
	Learning Rate: 0.000278727
	LOSS [training: 0.030201498509985113 | validation: 0.018799906747234817]
	TIME [epoch: 8.3 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021148282845795126		[learning rate: 0.0002784]
		[batch 20/20] avg loss: 0.03381623111113723		[learning rate: 0.00027807]
	Learning Rate: 0.00027807
	LOSS [training: 0.02748225697846618 | validation: 0.018667240257265797]
	TIME [epoch: 8.31 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020598308524419944		[learning rate: 0.00027774]
		[batch 20/20] avg loss: 0.02980765398917412		[learning rate: 0.00027741]
	Learning Rate: 0.000277414
	LOSS [training: 0.02520298125679703 | validation: 0.030659909233821753]
	TIME [epoch: 8.3 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03468425520287289		[learning rate: 0.00027709]
		[batch 20/20] avg loss: 0.02642453794286175		[learning rate: 0.00027676]
	Learning Rate: 0.000276759
	LOSS [training: 0.030554396572867328 | validation: 0.05373692132057186]
	TIME [epoch: 8.33 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04068751055822208		[learning rate: 0.00027643]
		[batch 20/20] avg loss: 0.024256678716392383		[learning rate: 0.00027611]
	Learning Rate: 0.000276107
	LOSS [training: 0.03247209463730723 | validation: 0.017686494652895613]
	TIME [epoch: 8.3 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021620162589307858		[learning rate: 0.00027578]
		[batch 20/20] avg loss: 0.036668807789498256		[learning rate: 0.00027546]
	Learning Rate: 0.000275455
	LOSS [training: 0.029144485189403053 | validation: 0.03209306831398398]
	TIME [epoch: 8.31 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036061715140829856		[learning rate: 0.00027513]
		[batch 20/20] avg loss: 0.04711652782809498		[learning rate: 0.00027481]
	Learning Rate: 0.000274806
	LOSS [training: 0.04158912148446241 | validation: 0.011747607687849208]
	TIME [epoch: 8.3 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025058701241857682		[learning rate: 0.00027448]
		[batch 20/20] avg loss: 0.03771802366611156		[learning rate: 0.00027416]
	Learning Rate: 0.000274157
	LOSS [training: 0.031388362453984624 | validation: 0.01709321370672095]
	TIME [epoch: 8.33 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02722029375049028		[learning rate: 0.00027383]
		[batch 20/20] avg loss: 0.021666482248745924		[learning rate: 0.00027351]
	Learning Rate: 0.000273511
	LOSS [training: 0.024443387999618104 | validation: 0.011642316491361248]
	TIME [epoch: 8.3 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025279079087657745		[learning rate: 0.00027319]
		[batch 20/20] avg loss: 0.02940964991236124		[learning rate: 0.00027287]
	Learning Rate: 0.000272866
	LOSS [training: 0.0273443645000095 | validation: 0.026787236781178028]
	TIME [epoch: 8.31 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03163922862155801		[learning rate: 0.00027254]
		[batch 20/20] avg loss: 0.03234398671327642		[learning rate: 0.00027222]
	Learning Rate: 0.000272222
	LOSS [training: 0.031991607667417224 | validation: 0.024122910194300587]
	TIME [epoch: 8.3 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032072750876839126		[learning rate: 0.0002719]
		[batch 20/20] avg loss: 0.02853398760638659		[learning rate: 0.00027158]
	Learning Rate: 0.00027158
	LOSS [training: 0.030303369241612865 | validation: 0.014732741511375193]
	TIME [epoch: 8.32 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03187885068409886		[learning rate: 0.00027126]
		[batch 20/20] avg loss: 0.03019112223098881		[learning rate: 0.00027094]
	Learning Rate: 0.000270939
	LOSS [training: 0.03103498645754383 | validation: 0.03528634361054325]
	TIME [epoch: 8.3 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026674295692088356		[learning rate: 0.00027062]
		[batch 20/20] avg loss: 0.03333359363318325		[learning rate: 0.0002703]
	Learning Rate: 0.0002703
	LOSS [training: 0.030003944662635805 | validation: 0.020647986879766798]
	TIME [epoch: 8.3 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03394077212685051		[learning rate: 0.00026998]
		[batch 20/20] avg loss: 0.0443224631646342		[learning rate: 0.00026966]
	Learning Rate: 0.000269662
	LOSS [training: 0.03913161764574235 | validation: 0.027123136634266933]
	TIME [epoch: 8.31 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03104352678521092		[learning rate: 0.00026934]
		[batch 20/20] avg loss: 0.026269660155578516		[learning rate: 0.00026903]
	Learning Rate: 0.000269026
	LOSS [training: 0.02865659347039472 | validation: 0.02717640420019985]
	TIME [epoch: 8.33 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030010999661705045		[learning rate: 0.00026871]
		[batch 20/20] avg loss: 0.026833048508236146		[learning rate: 0.00026839]
	Learning Rate: 0.000268392
	LOSS [training: 0.028422024084970597 | validation: 0.02105015014782613]
	TIME [epoch: 8.31 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03000526221476884		[learning rate: 0.00026808]
		[batch 20/20] avg loss: 0.031143381176691676		[learning rate: 0.00026776]
	Learning Rate: 0.000267759
	LOSS [training: 0.030574321695730256 | validation: 0.02795167489052385]
	TIME [epoch: 8.3 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03729016747196766		[learning rate: 0.00026744]
		[batch 20/20] avg loss: 0.0416643846647664		[learning rate: 0.00026713]
	Learning Rate: 0.000267127
	LOSS [training: 0.03947727606836703 | validation: 0.012349063983097078]
	TIME [epoch: 8.3 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03199271731227692		[learning rate: 0.00026681]
		[batch 20/20] avg loss: 0.038662358049841854		[learning rate: 0.0002665]
	Learning Rate: 0.000266497
	LOSS [training: 0.03532753768105939 | validation: 0.04281782936133903]
	TIME [epoch: 8.33 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05817274510206656		[learning rate: 0.00026618]
		[batch 20/20] avg loss: 0.02879732370171465		[learning rate: 0.00026587]
	Learning Rate: 0.000265868
	LOSS [training: 0.0434850344018906 | validation: 0.04720939143389666]
	TIME [epoch: 8.31 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03199833997631694		[learning rate: 0.00026555]
		[batch 20/20] avg loss: 0.027293714802585488		[learning rate: 0.00026524]
	Learning Rate: 0.000265241
	LOSS [training: 0.029646027389451213 | validation: 0.02039392712158635]
	TIME [epoch: 8.3 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01746988237747806		[learning rate: 0.00026493]
		[batch 20/20] avg loss: 0.031334182019041125		[learning rate: 0.00026462]
	Learning Rate: 0.000264616
	LOSS [training: 0.024402032198259586 | validation: 0.015757596513394964]
	TIME [epoch: 8.31 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03179837414868063		[learning rate: 0.0002643]
		[batch 20/20] avg loss: 0.029792241815901798		[learning rate: 0.00026399]
	Learning Rate: 0.000263991
	LOSS [training: 0.030795307982291208 | validation: 0.018804824222510192]
	TIME [epoch: 8.32 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031491998615836785		[learning rate: 0.00026368]
		[batch 20/20] avg loss: 0.02571661445590368		[learning rate: 0.00026337]
	Learning Rate: 0.000263369
	LOSS [training: 0.028604306535870233 | validation: 0.031243458691755432]
	TIME [epoch: 8.31 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021550883037227173		[learning rate: 0.00026306]
		[batch 20/20] avg loss: 0.027928645611834217		[learning rate: 0.00026275]
	Learning Rate: 0.000262747
	LOSS [training: 0.0247397643245307 | validation: 0.02314335736047711]
	TIME [epoch: 8.3 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04375549407591452		[learning rate: 0.00026244]
		[batch 20/20] avg loss: 0.0292911209372193		[learning rate: 0.00026213]
	Learning Rate: 0.000262128
	LOSS [training: 0.03652330750656691 | validation: 0.019484094240668062]
	TIME [epoch: 8.3 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01828011955415229		[learning rate: 0.00026182]
		[batch 20/20] avg loss: 0.03242457018114338		[learning rate: 0.00026151]
	Learning Rate: 0.000261509
	LOSS [training: 0.025352344867647834 | validation: 0.014786409194229851]
	TIME [epoch: 8.32 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02314084310408777		[learning rate: 0.0002612]
		[batch 20/20] avg loss: 0.03194814695808462		[learning rate: 0.00026089]
	Learning Rate: 0.000260892
	LOSS [training: 0.02754449503108619 | validation: 0.038873726290917454]
	TIME [epoch: 8.3 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03362269607766613		[learning rate: 0.00026058]
		[batch 20/20] avg loss: 0.0346881445587137		[learning rate: 0.00026028]
	Learning Rate: 0.000260277
	LOSS [training: 0.034155420318189925 | validation: 0.0154118089355726]
	TIME [epoch: 8.3 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030424428010903067		[learning rate: 0.00025997]
		[batch 20/20] avg loss: 0.03815619915037042		[learning rate: 0.00025966]
	Learning Rate: 0.000259663
	LOSS [training: 0.034290313580636736 | validation: 0.029135272432513536]
	TIME [epoch: 8.3 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02548412482022258		[learning rate: 0.00025936]
		[batch 20/20] avg loss: 0.02497703318396567		[learning rate: 0.00025905]
	Learning Rate: 0.000259051
	LOSS [training: 0.025230579002094126 | validation: 0.018884645773379113]
	TIME [epoch: 8.32 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031782998668738564		[learning rate: 0.00025874]
		[batch 20/20] avg loss: 0.03213144062308708		[learning rate: 0.00025844]
	Learning Rate: 0.00025844
	LOSS [training: 0.03195721964591282 | validation: 0.028547300081953293]
	TIME [epoch: 8.31 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028473322954487214		[learning rate: 0.00025813]
		[batch 20/20] avg loss: 0.03347646669916989		[learning rate: 0.00025783]
	Learning Rate: 0.00025783
	LOSS [training: 0.030974894826828553 | validation: 0.030197877587240837]
	TIME [epoch: 8.3 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02858825341908002		[learning rate: 0.00025753]
		[batch 20/20] avg loss: 0.028737051903195376		[learning rate: 0.00025722]
	Learning Rate: 0.000257222
	LOSS [training: 0.028662652661137704 | validation: 0.06997352411113278]
	TIME [epoch: 8.3 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04264832944591395		[learning rate: 0.00025692]
		[batch 20/20] avg loss: 0.029096761564265012		[learning rate: 0.00025662]
	Learning Rate: 0.000256615
	LOSS [training: 0.03587254550508948 | validation: 0.013561746521522226]
	TIME [epoch: 8.33 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035142460748589066		[learning rate: 0.00025631]
		[batch 20/20] avg loss: 0.030691655697461923		[learning rate: 0.00025601]
	Learning Rate: 0.00025601
	LOSS [training: 0.03291705822302549 | validation: 0.020207038636294583]
	TIME [epoch: 8.31 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03198237227564808		[learning rate: 0.00025571]
		[batch 20/20] avg loss: 0.02386273617778284		[learning rate: 0.00025541]
	Learning Rate: 0.000255406
	LOSS [training: 0.027922554226715458 | validation: 0.017278740789652844]
	TIME [epoch: 8.3 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031695753544578695		[learning rate: 0.0002551]
		[batch 20/20] avg loss: 0.026791918876102154		[learning rate: 0.0002548]
	Learning Rate: 0.000254803
	LOSS [training: 0.029243836210340424 | validation: 0.033461454309031906]
	TIME [epoch: 8.3 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039228678320669584		[learning rate: 0.0002545]
		[batch 20/20] avg loss: 0.019353440799734268		[learning rate: 0.0002542]
	Learning Rate: 0.000254202
	LOSS [training: 0.02929105956020192 | validation: 0.02643789574234846]
	TIME [epoch: 8.32 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02760921980712534		[learning rate: 0.0002539]
		[batch 20/20] avg loss: 0.02915756647539174		[learning rate: 0.0002536]
	Learning Rate: 0.000253603
	LOSS [training: 0.02838339314125854 | validation: 0.01782042790669274]
	TIME [epoch: 8.31 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023353804540223906		[learning rate: 0.0002533]
		[batch 20/20] avg loss: 0.035116236804880484		[learning rate: 0.000253]
	Learning Rate: 0.000253004
	LOSS [training: 0.0292350206725522 | validation: 0.015460640227063444]
	TIME [epoch: 8.3 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029956785576097256		[learning rate: 0.00025271]
		[batch 20/20] avg loss: 0.027898481032875254		[learning rate: 0.00025241]
	Learning Rate: 0.000252408
	LOSS [training: 0.02892763330448625 | validation: 0.011441710735500139]
	TIME [epoch: 8.3 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030102636400302453		[learning rate: 0.00025211]
		[batch 20/20] avg loss: 0.02894048161208379		[learning rate: 0.00025181]
	Learning Rate: 0.000251812
	LOSS [training: 0.02952155900619312 | validation: 0.01580442770222049]
	TIME [epoch: 8.32 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022876522471233923		[learning rate: 0.00025152]
		[batch 20/20] avg loss: 0.0488325020579428		[learning rate: 0.00025122]
	Learning Rate: 0.000251218
	LOSS [training: 0.03585451226458837 | validation: 0.0608204082837535]
	TIME [epoch: 8.31 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04234677331822516		[learning rate: 0.00025092]
		[batch 20/20] avg loss: 0.02157552038847072		[learning rate: 0.00025063]
	Learning Rate: 0.000250626
	LOSS [training: 0.03196114685334794 | validation: 0.033859771099581784]
	TIME [epoch: 8.3 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02954728477534737		[learning rate: 0.00025033]
		[batch 20/20] avg loss: 0.025001073951783515		[learning rate: 0.00025003]
	Learning Rate: 0.000250035
	LOSS [training: 0.027274179363565444 | validation: 0.02691071504757486]
	TIME [epoch: 8.3 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038288082740544335		[learning rate: 0.00024974]
		[batch 20/20] avg loss: 0.028567480571703552		[learning rate: 0.00024944]
	Learning Rate: 0.000249445
	LOSS [training: 0.03342778165612394 | validation: 0.023003685058093342]
	TIME [epoch: 8.32 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03247572535043544		[learning rate: 0.00024915]
		[batch 20/20] avg loss: 0.024880567753176965		[learning rate: 0.00024886]
	Learning Rate: 0.000248856
	LOSS [training: 0.028678146551806206 | validation: 0.016965993995996225]
	TIME [epoch: 8.3 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03190018856770576		[learning rate: 0.00024856]
		[batch 20/20] avg loss: 0.026426474909054205		[learning rate: 0.00024827]
	Learning Rate: 0.000248269
	LOSS [training: 0.029163331738379983 | validation: 0.02801426821747744]
	TIME [epoch: 8.3 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031026219140259843		[learning rate: 0.00024798]
		[batch 20/20] avg loss: 0.020603333024160543		[learning rate: 0.00024768]
	Learning Rate: 0.000247684
	LOSS [training: 0.025814776082210188 | validation: 0.018040597129589313]
	TIME [epoch: 8.3 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03328317776025335		[learning rate: 0.00024739]
		[batch 20/20] avg loss: 0.026403419164196966		[learning rate: 0.0002471]
	Learning Rate: 0.000247099
	LOSS [training: 0.029843298462225165 | validation: 0.01455878551037867]
	TIME [epoch: 8.32 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03658873880716669		[learning rate: 0.00024681]
		[batch 20/20] avg loss: 0.0341749351355653		[learning rate: 0.00024652]
	Learning Rate: 0.000246517
	LOSS [training: 0.03538183697136599 | validation: 0.01757791536389791]
	TIME [epoch: 8.3 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018230768347676736		[learning rate: 0.00024623]
		[batch 20/20] avg loss: 0.03381552749089661		[learning rate: 0.00024594]
	Learning Rate: 0.000245935
	LOSS [training: 0.02602314791928667 | validation: 0.04083199578493667]
	TIME [epoch: 8.3 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031733250557822223		[learning rate: 0.00024564]
		[batch 20/20] avg loss: 0.032295102896762753		[learning rate: 0.00024535]
	Learning Rate: 0.000245355
	LOSS [training: 0.03201417672729248 | validation: 0.019125219265108566]
	TIME [epoch: 8.3 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020333833221802933		[learning rate: 0.00024507]
		[batch 20/20] avg loss: 0.03811400941834873		[learning rate: 0.00024478]
	Learning Rate: 0.000244776
	LOSS [training: 0.029223921320075834 | validation: 0.0249598136770887]
	TIME [epoch: 8.32 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025660421369598185		[learning rate: 0.00024449]
		[batch 20/20] avg loss: 0.039349388260174825		[learning rate: 0.0002442]
	Learning Rate: 0.000244199
	LOSS [training: 0.03250490481488649 | validation: 0.023516369133996306]
	TIME [epoch: 8.31 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026022521611359074		[learning rate: 0.00024391]
		[batch 20/20] avg loss: 0.024397342090694756		[learning rate: 0.00024362]
	Learning Rate: 0.000243623
	LOSS [training: 0.025209931851026924 | validation: 0.01070962241315876]
	TIME [epoch: 8.3 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023306600577821045		[learning rate: 0.00024334]
		[batch 20/20] avg loss: 0.0295295897984016		[learning rate: 0.00024305]
	Learning Rate: 0.000243048
	LOSS [training: 0.026418095188111328 | validation: 0.02457637705446744]
	TIME [epoch: 8.3 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026417039736229976		[learning rate: 0.00024276]
		[batch 20/20] avg loss: 0.027654817952244153		[learning rate: 0.00024247]
	Learning Rate: 0.000242475
	LOSS [training: 0.027035928844237066 | validation: 0.03160719099609218]
	TIME [epoch: 8.32 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02448840491449922		[learning rate: 0.00024219]
		[batch 20/20] avg loss: 0.032094997750680046		[learning rate: 0.0002419]
	Learning Rate: 0.000241903
	LOSS [training: 0.028291701332589632 | validation: 0.045028412391247025]
	TIME [epoch: 8.3 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036922123448896685		[learning rate: 0.00024162]
		[batch 20/20] avg loss: 0.03366148106241367		[learning rate: 0.00024133]
	Learning Rate: 0.000241332
	LOSS [training: 0.03529180225565518 | validation: 0.02047605164458234]
	TIME [epoch: 8.3 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023865261555947488		[learning rate: 0.00024105]
		[batch 20/20] avg loss: 0.018661567537928685		[learning rate: 0.00024076]
	Learning Rate: 0.000240763
	LOSS [training: 0.021263414546938086 | validation: 0.017489167683156195]
	TIME [epoch: 8.3 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03148163597752978		[learning rate: 0.00024048]
		[batch 20/20] avg loss: 0.021647320341413163		[learning rate: 0.0002402]
	Learning Rate: 0.000240195
	LOSS [training: 0.026564478159471473 | validation: 0.02408596436613699]
	TIME [epoch: 8.32 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03158476667836139		[learning rate: 0.00023991]
		[batch 20/20] avg loss: 0.035072187577374674		[learning rate: 0.00023963]
	Learning Rate: 0.000239628
	LOSS [training: 0.03332847712786803 | validation: 0.0203064356196105]
	TIME [epoch: 8.3 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030896528187653538		[learning rate: 0.00023935]
		[batch 20/20] avg loss: 0.03115621963457807		[learning rate: 0.00023906]
	Learning Rate: 0.000239063
	LOSS [training: 0.031026373911115807 | validation: 0.02214373336370217]
	TIME [epoch: 8.3 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021735961507098077		[learning rate: 0.00023878]
		[batch 20/20] avg loss: 0.024303846123527966		[learning rate: 0.0002385]
	Learning Rate: 0.000238499
	LOSS [training: 0.023019903815313027 | validation: 0.03245050545091705]
	TIME [epoch: 8.3 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022721065528754435		[learning rate: 0.00023822]
		[batch 20/20] avg loss: 0.02861177704743604		[learning rate: 0.00023794]
	Learning Rate: 0.000237937
	LOSS [training: 0.025666421288095237 | validation: 0.01743943832530675]
	TIME [epoch: 8.32 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02127538148361193		[learning rate: 0.00023766]
		[batch 20/20] avg loss: 0.03091714363742925		[learning rate: 0.00023738]
	Learning Rate: 0.000237375
	LOSS [training: 0.026096262560520594 | validation: 0.02151974472074598]
	TIME [epoch: 8.3 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025860471698307873		[learning rate: 0.0002371]
		[batch 20/20] avg loss: 0.020602654327863262		[learning rate: 0.00023682]
	Learning Rate: 0.000236816
	LOSS [training: 0.023231563013085568 | validation: 0.02175671880545407]
	TIME [epoch: 8.31 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02568847428849077		[learning rate: 0.00023654]
		[batch 20/20] avg loss: 0.02507834444289589		[learning rate: 0.00023626]
	Learning Rate: 0.000236257
	LOSS [training: 0.025383409365693332 | validation: 0.033158053475820806]
	TIME [epoch: 8.3 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03190793489689066		[learning rate: 0.00023598]
		[batch 20/20] avg loss: 0.03031244239602614		[learning rate: 0.0002357]
	Learning Rate: 0.0002357
	LOSS [training: 0.031110188646458405 | validation: 0.03307790079658979]
	TIME [epoch: 8.33 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02617019806893952		[learning rate: 0.00023542]
		[batch 20/20] avg loss: 0.024287402629233738		[learning rate: 0.00023514]
	Learning Rate: 0.000235144
	LOSS [training: 0.025228800349086627 | validation: 0.02707886351561077]
	TIME [epoch: 8.3 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03971658404902821		[learning rate: 0.00023487]
		[batch 20/20] avg loss: 0.032161382833039195		[learning rate: 0.00023459]
	Learning Rate: 0.000234589
	LOSS [training: 0.0359389834410337 | validation: 0.023818209994632127]
	TIME [epoch: 8.31 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025733868146457912		[learning rate: 0.00023431]
		[batch 20/20] avg loss: 0.02892100524453492		[learning rate: 0.00023404]
	Learning Rate: 0.000234036
	LOSS [training: 0.02732743669549642 | validation: 0.016427047347501677]
	TIME [epoch: 8.3 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034965065629532915		[learning rate: 0.00023376]
		[batch 20/20] avg loss: 0.0222551386752507		[learning rate: 0.00023348]
	Learning Rate: 0.000233484
	LOSS [training: 0.028610102152391802 | validation: 0.017018760355332264]
	TIME [epoch: 8.32 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027888063880166687		[learning rate: 0.00023321]
		[batch 20/20] avg loss: 0.02725792013600062		[learning rate: 0.00023293]
	Learning Rate: 0.000232933
	LOSS [training: 0.027572992008083656 | validation: 0.030517022535380738]
	TIME [epoch: 8.31 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026423092364812106		[learning rate: 0.00023266]
		[batch 20/20] avg loss: 0.03165657735437587		[learning rate: 0.00023238]
	Learning Rate: 0.000232383
	LOSS [training: 0.029039834859594 | validation: 0.014095315898235566]
	TIME [epoch: 8.3 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02901200923487469		[learning rate: 0.00023211]
		[batch 20/20] avg loss: 0.02375199302823414		[learning rate: 0.00023184]
	Learning Rate: 0.000231835
	LOSS [training: 0.026382001131554415 | validation: 0.026886910533048012]
	TIME [epoch: 8.3 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031890963584110955		[learning rate: 0.00023156]
		[batch 20/20] avg loss: 0.04038237408401964		[learning rate: 0.00023129]
	Learning Rate: 0.000231288
	LOSS [training: 0.03613666883406529 | validation: 0.029384895918423172]
	TIME [epoch: 8.32 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02792783447872884		[learning rate: 0.00023102]
		[batch 20/20] avg loss: 0.031320824046810566		[learning rate: 0.00023074]
	Learning Rate: 0.000230743
	LOSS [training: 0.02962432926276971 | validation: 0.03009274301423482]
	TIME [epoch: 8.3 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029619479152658074		[learning rate: 0.00023047]
		[batch 20/20] avg loss: 0.024008791624188897		[learning rate: 0.0002302]
	Learning Rate: 0.000230198
	LOSS [training: 0.026814135388423484 | validation: 0.026998384168479012]
	TIME [epoch: 8.3 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03374518051797938		[learning rate: 0.00022993]
		[batch 20/20] avg loss: 0.021580947453407405		[learning rate: 0.00022966]
	Learning Rate: 0.000229656
	LOSS [training: 0.027663063985693393 | validation: 0.01962548383647226]
	TIME [epoch: 8.3 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02291358490107901		[learning rate: 0.00022938]
		[batch 20/20] avg loss: 0.04449069184975286		[learning rate: 0.00022911]
	Learning Rate: 0.000229114
	LOSS [training: 0.033702138375415934 | validation: 0.04493540269115273]
	TIME [epoch: 8.32 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02269232661520938		[learning rate: 0.00022884]
		[batch 20/20] avg loss: 0.026615228035070486		[learning rate: 0.00022857]
	Learning Rate: 0.000228573
	LOSS [training: 0.024653777325139934 | validation: 0.014380005075194154]
	TIME [epoch: 8.31 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03043119620940473		[learning rate: 0.0002283]
		[batch 20/20] avg loss: 0.042384507970903866		[learning rate: 0.00022803]
	Learning Rate: 0.000228034
	LOSS [training: 0.036407852090154305 | validation: 0.053157788316481935]
	TIME [epoch: 8.3 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03374577287109622		[learning rate: 0.00022777]
		[batch 20/20] avg loss: 0.02931687657264695		[learning rate: 0.0002275]
	Learning Rate: 0.000227496
	LOSS [training: 0.03153132472187159 | validation: 0.017649680854858792]
	TIME [epoch: 8.31 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021354024364269097		[learning rate: 0.00022723]
		[batch 20/20] avg loss: 0.03291747948447595		[learning rate: 0.00022696]
	Learning Rate: 0.00022696
	LOSS [training: 0.027135751924372526 | validation: 0.022072520337553587]
	TIME [epoch: 8.32 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026481403736369924		[learning rate: 0.00022669]
		[batch 20/20] avg loss: 0.02819476714916554		[learning rate: 0.00022642]
	Learning Rate: 0.000226424
	LOSS [training: 0.027338085442767734 | validation: 0.02308177900274703]
	TIME [epoch: 8.3 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030790361945495448		[learning rate: 0.00022616]
		[batch 20/20] avg loss: 0.023471573483711875		[learning rate: 0.00022589]
	Learning Rate: 0.00022589
	LOSS [training: 0.027130967714603667 | validation: 0.01680494283572419]
	TIME [epoch: 8.3 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031168226495430763		[learning rate: 0.00022562]
		[batch 20/20] avg loss: 0.029347032508619163		[learning rate: 0.00022536]
	Learning Rate: 0.000225357
	LOSS [training: 0.030257629502024964 | validation: 0.02126772814330493]
	TIME [epoch: 8.3 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029355329615050184		[learning rate: 0.00022509]
		[batch 20/20] avg loss: 0.026675288836409426		[learning rate: 0.00022483]
	Learning Rate: 0.000224826
	LOSS [training: 0.028015309225729805 | validation: 0.01781897313528208]
	TIME [epoch: 8.33 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024160125656445597		[learning rate: 0.00022456]
		[batch 20/20] avg loss: 0.03564613307523639		[learning rate: 0.0002243]
	Learning Rate: 0.000224295
	LOSS [training: 0.029903129365840997 | validation: 0.02301213752879889]
	TIME [epoch: 8.31 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020586030879417763		[learning rate: 0.00022403]
		[batch 20/20] avg loss: 0.02619140400181406		[learning rate: 0.00022377]
	Learning Rate: 0.000223766
	LOSS [training: 0.02338871744061591 | validation: 0.021011582372660154]
	TIME [epoch: 8.3 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02261190311760801		[learning rate: 0.0002235]
		[batch 20/20] avg loss: 0.03173054203779703		[learning rate: 0.00022324]
	Learning Rate: 0.000223239
	LOSS [training: 0.027171222577702513 | validation: 0.029475021221370926]
	TIME [epoch: 8.31 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040395880787483296		[learning rate: 0.00022298]
		[batch 20/20] avg loss: 0.02668092406375725		[learning rate: 0.00022271]
	Learning Rate: 0.000222712
	LOSS [training: 0.03353840242562027 | validation: 0.03325260349108687]
	TIME [epoch: 8.32 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024806864979710512		[learning rate: 0.00022245]
		[batch 20/20] avg loss: 0.029306272798302512		[learning rate: 0.00022219]
	Learning Rate: 0.000222187
	LOSS [training: 0.02705656888900651 | validation: 0.011947493113841388]
	TIME [epoch: 8.31 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019978555927687117		[learning rate: 0.00022192]
		[batch 20/20] avg loss: 0.03676585913922928		[learning rate: 0.00022166]
	Learning Rate: 0.000221663
	LOSS [training: 0.028372207533458198 | validation: 0.03692552686662479]
	TIME [epoch: 8.3 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04648785895409533		[learning rate: 0.0002214]
		[batch 20/20] avg loss: 0.02999079690080788		[learning rate: 0.00022114]
	Learning Rate: 0.00022114
	LOSS [training: 0.038239327927451605 | validation: 0.026217652901790903]
	TIME [epoch: 8.3 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025988670815963854		[learning rate: 0.00022088]
		[batch 20/20] avg loss: 0.029646039607615284		[learning rate: 0.00022062]
	Learning Rate: 0.000220618
	LOSS [training: 0.027817355211789567 | validation: 0.02955315415314406]
	TIME [epoch: 8.33 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041204520212353955		[learning rate: 0.00022036]
		[batch 20/20] avg loss: 0.028812837967874018		[learning rate: 0.0002201]
	Learning Rate: 0.000220098
	LOSS [training: 0.035008679090113994 | validation: 0.03780299432197726]
	TIME [epoch: 8.3 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024550938292514327		[learning rate: 0.00021984]
		[batch 20/20] avg loss: 0.03466391438087823		[learning rate: 0.00021958]
	Learning Rate: 0.000219578
	LOSS [training: 0.02960742633669628 | validation: 0.016830403767920277]
	TIME [epoch: 8.3 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029532210489697612		[learning rate: 0.00021932]
		[batch 20/20] avg loss: 0.04017185904559743		[learning rate: 0.00021906]
	Learning Rate: 0.000219061
	LOSS [training: 0.03485203476764752 | validation: 0.03165601718148405]
	TIME [epoch: 8.3 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02947332490378942		[learning rate: 0.0002188]
		[batch 20/20] avg loss: 0.028343683237316685		[learning rate: 0.00021854]
	Learning Rate: 0.000218544
	LOSS [training: 0.028908504070553058 | validation: 0.02112784980547251]
	TIME [epoch: 8.33 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025642707434857824		[learning rate: 0.00021829]
		[batch 20/20] avg loss: 0.02976726619767913		[learning rate: 0.00021803]
	Learning Rate: 0.000218028
	LOSS [training: 0.027704986816268477 | validation: 0.022662284466031583]
	TIME [epoch: 8.3 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034372118895786716		[learning rate: 0.00021777]
		[batch 20/20] avg loss: 0.024256297720485032		[learning rate: 0.00021751]
	Learning Rate: 0.000217514
	LOSS [training: 0.02931420830813588 | validation: 0.056132207735822434]
	TIME [epoch: 8.3 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03683178290552163		[learning rate: 0.00021726]
		[batch 20/20] avg loss: 0.0353432207881607		[learning rate: 0.000217]
	Learning Rate: 0.000217001
	LOSS [training: 0.03608750184684116 | validation: 0.04173552237129133]
	TIME [epoch: 8.3 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02562053432944599		[learning rate: 0.00021674]
		[batch 20/20] avg loss: 0.030360192186573355		[learning rate: 0.00021649]
	Learning Rate: 0.000216489
	LOSS [training: 0.02799036325800967 | validation: 0.019163148062359997]
	TIME [epoch: 8.33 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030441909002153274		[learning rate: 0.00021623]
		[batch 20/20] avg loss: 0.03829236958878572		[learning rate: 0.00021598]
	Learning Rate: 0.000215978
	LOSS [training: 0.0343671392954695 | validation: 0.0215541558456218]
	TIME [epoch: 8.3 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025829746324016623		[learning rate: 0.00021572]
		[batch 20/20] avg loss: 0.023460644961034338		[learning rate: 0.00021547]
	Learning Rate: 0.000215469
	LOSS [training: 0.024645195642525482 | validation: 0.012142924854416039]
	TIME [epoch: 8.31 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028480885396876036		[learning rate: 0.00021521]
		[batch 20/20] avg loss: 0.03242885509473197		[learning rate: 0.00021496]
	Learning Rate: 0.000214961
	LOSS [training: 0.03045487024580401 | validation: 0.027460273354441418]
	TIME [epoch: 8.3 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025835023986244005		[learning rate: 0.00021471]
		[batch 20/20] avg loss: 0.0229571065427898		[learning rate: 0.00021445]
	Learning Rate: 0.000214454
	LOSS [training: 0.024396065264516906 | validation: 0.019482983367690537]
	TIME [epoch: 8.32 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02802340982415393		[learning rate: 0.0002142]
		[batch 20/20] avg loss: 0.036291986268874483		[learning rate: 0.00021395]
	Learning Rate: 0.000213948
	LOSS [training: 0.03215769804651421 | validation: 0.02268519908613998]
	TIME [epoch: 8.3 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02880024972514088		[learning rate: 0.0002137]
		[batch 20/20] avg loss: 0.026124253030879552		[learning rate: 0.00021344]
	Learning Rate: 0.000213443
	LOSS [training: 0.027462251378010217 | validation: 0.02131071903139106]
	TIME [epoch: 8.3 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02248839451100728		[learning rate: 0.00021319]
		[batch 20/20] avg loss: 0.027692619569689502		[learning rate: 0.00021294]
	Learning Rate: 0.00021294
	LOSS [training: 0.025090507040348386 | validation: 0.018672707655119226]
	TIME [epoch: 8.3 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01684541556797004		[learning rate: 0.00021269]
		[batch 20/20] avg loss: 0.03019460311557856		[learning rate: 0.00021244]
	Learning Rate: 0.000212437
	LOSS [training: 0.0235200093417743 | validation: 0.038275973649937255]
	TIME [epoch: 8.32 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035092141438896114		[learning rate: 0.00021219]
		[batch 20/20] avg loss: 0.019162840635097794		[learning rate: 0.00021194]
	Learning Rate: 0.000211936
	LOSS [training: 0.027127491036996954 | validation: 0.026674531068969456]
	TIME [epoch: 8.3 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026634204184845873		[learning rate: 0.00021169]
		[batch 20/20] avg loss: 0.018907459955367023		[learning rate: 0.00021144]
	Learning Rate: 0.000211436
	LOSS [training: 0.022770832070106446 | validation: 0.012146425067843122]
	TIME [epoch: 8.3 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028380676987807486		[learning rate: 0.00021119]
		[batch 20/20] avg loss: 0.023332664472837335		[learning rate: 0.00021094]
	Learning Rate: 0.000210937
	LOSS [training: 0.025856670730322412 | validation: 0.017856253251880547]
	TIME [epoch: 8.3 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019803752259243808		[learning rate: 0.00021069]
		[batch 20/20] avg loss: 0.025334593037253834		[learning rate: 0.00021044]
	Learning Rate: 0.00021044
	LOSS [training: 0.022569172648248814 | validation: 0.021562738419535907]
	TIME [epoch: 8.32 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03257556344850699		[learning rate: 0.00021019]
		[batch 20/20] avg loss: 0.025993697262147973		[learning rate: 0.00020994]
	Learning Rate: 0.000209944
	LOSS [training: 0.029284630355327478 | validation: 0.036102991006127745]
	TIME [epoch: 8.3 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024956966421375966		[learning rate: 0.0002097]
		[batch 20/20] avg loss: 0.021873595450642117		[learning rate: 0.00020945]
	Learning Rate: 0.000209448
	LOSS [training: 0.023415280936009038 | validation: 0.012979408161836334]
	TIME [epoch: 8.3 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03440630580139852		[learning rate: 0.0002092]
		[batch 20/20] avg loss: 0.019254503109018235		[learning rate: 0.00020895]
	Learning Rate: 0.000208954
	LOSS [training: 0.02683040445520838 | validation: 0.006764814447390532]
	TIME [epoch: 8.3 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024008593034020108		[learning rate: 0.00020871]
		[batch 20/20] avg loss: 0.04022047674677006		[learning rate: 0.00020846]
	Learning Rate: 0.000208461
	LOSS [training: 0.03211453489039508 | validation: 0.01718413852622694]
	TIME [epoch: 8.33 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030995564513306824		[learning rate: 0.00020822]
		[batch 20/20] avg loss: 0.030111498728258385		[learning rate: 0.00020797]
	Learning Rate: 0.00020797
	LOSS [training: 0.030553531620782597 | validation: 0.014423503168342329]
	TIME [epoch: 8.3 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02766723605354639		[learning rate: 0.00020772]
		[batch 20/20] avg loss: 0.029162166806651298		[learning rate: 0.00020748]
	Learning Rate: 0.000207479
	LOSS [training: 0.02841470143009884 | validation: 0.028696436088502177]
	TIME [epoch: 8.3 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0295543655081562		[learning rate: 0.00020723]
		[batch 20/20] avg loss: 0.023466878899856887		[learning rate: 0.00020699]
	Learning Rate: 0.00020699
	LOSS [training: 0.02651062220400654 | validation: 0.018762982648125936]
	TIME [epoch: 8.3 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021458241991647354		[learning rate: 0.00020675]
		[batch 20/20] avg loss: 0.02304673923563376		[learning rate: 0.0002065]
	Learning Rate: 0.000206501
	LOSS [training: 0.02225249061364055 | validation: 0.02513204737769329]
	TIME [epoch: 8.33 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03225852393225986		[learning rate: 0.00020626]
		[batch 20/20] avg loss: 0.01902531835661499		[learning rate: 0.00020601]
	Learning Rate: 0.000206014
	LOSS [training: 0.025641921144437425 | validation: 0.008328554864987051]
	TIME [epoch: 8.3 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025032997771044542		[learning rate: 0.00020577]
		[batch 20/20] avg loss: 0.03027101311783853		[learning rate: 0.00020553]
	Learning Rate: 0.000205528
	LOSS [training: 0.027652005444441536 | validation: 0.031054613413891402]
	TIME [epoch: 8.3 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03926116134686185		[learning rate: 0.00020529]
		[batch 20/20] avg loss: 0.024884835814572143		[learning rate: 0.00020504]
	Learning Rate: 0.000205044
	LOSS [training: 0.032072998580717 | validation: 0.018426269903093095]
	TIME [epoch: 8.3 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02800711436837826		[learning rate: 0.0002048]
		[batch 20/20] avg loss: 0.02973001309859933		[learning rate: 0.00020456]
	Learning Rate: 0.00020456
	LOSS [training: 0.0288685637334888 | validation: 0.01804778382496871]
	TIME [epoch: 8.32 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020495459010562357		[learning rate: 0.00020432]
		[batch 20/20] avg loss: 0.02718991875751305		[learning rate: 0.00020408]
	Learning Rate: 0.000204077
	LOSS [training: 0.0238426888840377 | validation: 0.025508799475626523]
	TIME [epoch: 8.3 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03181525794112234		[learning rate: 0.00020384]
		[batch 20/20] avg loss: 0.027707381783374996		[learning rate: 0.0002036]
	Learning Rate: 0.000203596
	LOSS [training: 0.02976131986224867 | validation: 0.018157932241189563]
	TIME [epoch: 8.3 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021649933380421628		[learning rate: 0.00020336]
		[batch 20/20] avg loss: 0.0293453351599181		[learning rate: 0.00020312]
	Learning Rate: 0.000203116
	LOSS [training: 0.02549763427016987 | validation: 0.020608613070631544]
	TIME [epoch: 8.3 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021390601727593255		[learning rate: 0.00020288]
		[batch 20/20] avg loss: 0.027373852813521688		[learning rate: 0.00020264]
	Learning Rate: 0.000202637
	LOSS [training: 0.02438222727055747 | validation: 0.023146526921666334]
	TIME [epoch: 8.33 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029529143525428743		[learning rate: 0.0002024]
		[batch 20/20] avg loss: 0.021485041877131728		[learning rate: 0.00020216]
	Learning Rate: 0.000202159
	LOSS [training: 0.02550709270128023 | validation: 0.011721819661475702]
	TIME [epoch: 8.29 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036843495258175825		[learning rate: 0.00020192]
		[batch 20/20] avg loss: 0.0210967436392146		[learning rate: 0.00020168]
	Learning Rate: 0.000201682
	LOSS [training: 0.02897011944869521 | validation: 0.021037837477372923]
	TIME [epoch: 8.3 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030580513825273527		[learning rate: 0.00020144]
		[batch 20/20] avg loss: 0.019427192919001916		[learning rate: 0.00020121]
	Learning Rate: 0.000201206
	LOSS [training: 0.02500385337213772 | validation: 0.010806862770706107]
	TIME [epoch: 8.3 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02255143348397747		[learning rate: 0.00020097]
		[batch 20/20] avg loss: 0.02363422980313552		[learning rate: 0.00020073]
	Learning Rate: 0.000200731
	LOSS [training: 0.023092831643556494 | validation: 0.021667412020243776]
	TIME [epoch: 8.32 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029423129526229146		[learning rate: 0.00020049]
		[batch 20/20] avg loss: 0.017814105225109005		[learning rate: 0.00020026]
	Learning Rate: 0.000200258
	LOSS [training: 0.023618617375669077 | validation: 0.012798916988220832]
	TIME [epoch: 8.3 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025441487101557452		[learning rate: 0.00020002]
		[batch 20/20] avg loss: 0.027172224891702297		[learning rate: 0.00019979]
	Learning Rate: 0.000199786
	LOSS [training: 0.026306855996629873 | validation: 0.02527656504185945]
	TIME [epoch: 8.3 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03305979508968658		[learning rate: 0.00019955]
		[batch 20/20] avg loss: 0.0263293070783928		[learning rate: 0.00019931]
	Learning Rate: 0.000199314
	LOSS [training: 0.029694551084039688 | validation: 0.018593964822237276]
	TIME [epoch: 8.3 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027194271904601258		[learning rate: 0.00019908]
		[batch 20/20] avg loss: 0.02150807972342928		[learning rate: 0.00019884]
	Learning Rate: 0.000198844
	LOSS [training: 0.024351175814015268 | validation: 0.015681291744098044]
	TIME [epoch: 8.33 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025084824263963756		[learning rate: 0.00019861]
		[batch 20/20] avg loss: 0.03218240394343917		[learning rate: 0.00019838]
	Learning Rate: 0.000198375
	LOSS [training: 0.028633614103701473 | validation: 0.01834828461893924]
	TIME [epoch: 8.3 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021396998705827834		[learning rate: 0.00019814]
		[batch 20/20] avg loss: 0.03506801334145173		[learning rate: 0.00019791]
	Learning Rate: 0.000197907
	LOSS [training: 0.028232506023639782 | validation: 0.039831970665443556]
	TIME [epoch: 8.29 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03898982615115845		[learning rate: 0.00019767]
		[batch 20/20] avg loss: 0.02235358362980447		[learning rate: 0.00019744]
	Learning Rate: 0.00019744
	LOSS [training: 0.030671704890481453 | validation: 0.017906449132033046]
	TIME [epoch: 8.3 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02946637154549443		[learning rate: 0.00019721]
		[batch 20/20] avg loss: 0.034833278381871106		[learning rate: 0.00019697]
	Learning Rate: 0.000196975
	LOSS [training: 0.03214982496368278 | validation: 0.02191228784573331]
	TIME [epoch: 8.32 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027849560739855193		[learning rate: 0.00019674]
		[batch 20/20] avg loss: 0.029453828248416157		[learning rate: 0.00019651]
	Learning Rate: 0.00019651
	LOSS [training: 0.028651694494135682 | validation: 0.019258692453851187]
	TIME [epoch: 8.3 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03541621007163713		[learning rate: 0.00019628]
		[batch 20/20] avg loss: 0.019415846277660558		[learning rate: 0.00019605]
	Learning Rate: 0.000196046
	LOSS [training: 0.02741602817464884 | validation: 0.014819226108174061]
	TIME [epoch: 8.29 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026426819003020647		[learning rate: 0.00019582]
		[batch 20/20] avg loss: 0.022881932053268585		[learning rate: 0.00019558]
	Learning Rate: 0.000195584
	LOSS [training: 0.024654375528144614 | validation: 0.01217225496581814]
	TIME [epoch: 8.3 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027925586202063046		[learning rate: 0.00019535]
		[batch 20/20] avg loss: 0.024088648622995135		[learning rate: 0.00019512]
	Learning Rate: 0.000195123
	LOSS [training: 0.026007117412529097 | validation: 0.015217661068254968]
	TIME [epoch: 8.32 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02445077660924103		[learning rate: 0.00019489]
		[batch 20/20] avg loss: 0.02021200881627382		[learning rate: 0.00019466]
	Learning Rate: 0.000194662
	LOSS [training: 0.02233139271275742 | validation: 0.017869514777665494]
	TIME [epoch: 8.29 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021737724084747		[learning rate: 0.00019443]
		[batch 20/20] avg loss: 0.0279330348962697		[learning rate: 0.0001942]
	Learning Rate: 0.000194203
	LOSS [training: 0.02483537949050835 | validation: 0.016363375465867913]
	TIME [epoch: 8.3 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029463562214062267		[learning rate: 0.00019397]
		[batch 20/20] avg loss: 0.018820816969123733		[learning rate: 0.00019375]
	Learning Rate: 0.000193745
	LOSS [training: 0.024142189591592998 | validation: 0.01998194265322814]
	TIME [epoch: 8.3 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018557100614370808		[learning rate: 0.00019352]
		[batch 20/20] avg loss: 0.025310241894687235		[learning rate: 0.00019329]
	Learning Rate: 0.000193288
	LOSS [training: 0.021933671254529025 | validation: 0.03093639225362576]
	TIME [epoch: 8.32 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018348095577986778		[learning rate: 0.00019306]
		[batch 20/20] avg loss: 0.030734300461622076		[learning rate: 0.00019283]
	Learning Rate: 0.000192832
	LOSS [training: 0.024541198019804426 | validation: 0.016041180347569267]
	TIME [epoch: 8.29 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0319604254677885		[learning rate: 0.0001926]
		[batch 20/20] avg loss: 0.024817462590707152		[learning rate: 0.00019238]
	Learning Rate: 0.000192377
	LOSS [training: 0.02838894402924782 | validation: 0.020949368765941258]
	TIME [epoch: 8.3 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01984764269145958		[learning rate: 0.00019215]
		[batch 20/20] avg loss: 0.028265736626533984		[learning rate: 0.00019192]
	Learning Rate: 0.000191923
	LOSS [training: 0.024056689658996783 | validation: 0.026964247023435334]
	TIME [epoch: 8.3 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029826327591150937		[learning rate: 0.0001917]
		[batch 20/20] avg loss: 0.031807243422539405		[learning rate: 0.00019147]
	Learning Rate: 0.000191471
	LOSS [training: 0.03081678550684517 | validation: 0.01735635224790633]
	TIME [epoch: 8.32 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025782217627048264		[learning rate: 0.00019124]
		[batch 20/20] avg loss: 0.031101617084175602		[learning rate: 0.00019102]
	Learning Rate: 0.000191019
	LOSS [training: 0.02844191735561193 | validation: 0.026738832469909758]
	TIME [epoch: 8.3 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03034421849706757		[learning rate: 0.00019079]
		[batch 20/20] avg loss: 0.025871393783504372		[learning rate: 0.00019057]
	Learning Rate: 0.000190569
	LOSS [training: 0.02810780614028597 | validation: 0.02204700884633531]
	TIME [epoch: 8.3 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030401081761558396		[learning rate: 0.00019034]
		[batch 20/20] avg loss: 0.01736539721090622		[learning rate: 0.00019012]
	Learning Rate: 0.000190119
	LOSS [training: 0.02388323948623231 | validation: 0.017869948690266566]
	TIME [epoch: 8.3 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026780737759028915		[learning rate: 0.00018989]
		[batch 20/20] avg loss: 0.026073900637738505		[learning rate: 0.00018967]
	Learning Rate: 0.000189671
	LOSS [training: 0.02642731919838371 | validation: 0.022165948241327886]
	TIME [epoch: 8.32 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024418278569592798		[learning rate: 0.00018945]
		[batch 20/20] avg loss: 0.02610782418552256		[learning rate: 0.00018922]
	Learning Rate: 0.000189223
	LOSS [training: 0.025263051377557678 | validation: 0.016646656350078823]
	TIME [epoch: 8.3 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02187728586502251		[learning rate: 0.000189]
		[batch 20/20] avg loss: 0.030978979471278978		[learning rate: 0.00018878]
	Learning Rate: 0.000188777
	LOSS [training: 0.02642813266815074 | validation: 0.03450456586179122]
	TIME [epoch: 8.29 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020924143945519717		[learning rate: 0.00018855]
		[batch 20/20] avg loss: 0.029785054625752018		[learning rate: 0.00018833]
	Learning Rate: 0.000188332
	LOSS [training: 0.02535459928563587 | validation: 0.025383021667723316]
	TIME [epoch: 8.31 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03472035056164806		[learning rate: 0.00018811]
		[batch 20/20] avg loss: 0.034953028875061166		[learning rate: 0.00018789]
	Learning Rate: 0.000187887
	LOSS [training: 0.034836689718354616 | validation: 0.03342893072197744]
	TIME [epoch: 8.32 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03294066180725971		[learning rate: 0.00018767]
		[batch 20/20] avg loss: 0.017469179150450164		[learning rate: 0.00018744]
	Learning Rate: 0.000187444
	LOSS [training: 0.025204920478854937 | validation: 0.028103963115885845]
	TIME [epoch: 8.31 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03156816924052429		[learning rate: 0.00018722]
		[batch 20/20] avg loss: 0.028141875062165506		[learning rate: 0.000187]
	Learning Rate: 0.000187002
	LOSS [training: 0.029855022151344897 | validation: 0.030697384313629006]
	TIME [epoch: 8.3 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03208349267834541		[learning rate: 0.00018678]
		[batch 20/20] avg loss: 0.03618065158387794		[learning rate: 0.00018656]
	Learning Rate: 0.000186561
	LOSS [training: 0.034132072131111676 | validation: 0.02305410530634648]
	TIME [epoch: 8.3 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025655815587581		[learning rate: 0.00018634]
		[batch 20/20] avg loss: 0.026183752463132877		[learning rate: 0.00018612]
	Learning Rate: 0.000186121
	LOSS [training: 0.025919784025356933 | validation: 0.02809320157900925]
	TIME [epoch: 8.32 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029045092257458615		[learning rate: 0.0001859]
		[batch 20/20] avg loss: 0.03149131466387883		[learning rate: 0.00018568]
	Learning Rate: 0.000185682
	LOSS [training: 0.03026820346066873 | validation: 0.02188910941231815]
	TIME [epoch: 8.3 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023257650537618097		[learning rate: 0.00018546]
		[batch 20/20] avg loss: 0.031244555775227993		[learning rate: 0.00018524]
	Learning Rate: 0.000185244
	LOSS [training: 0.02725110315642304 | validation: 0.029094115446102676]
	TIME [epoch: 8.3 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03562503736075827		[learning rate: 0.00018503]
		[batch 20/20] avg loss: 0.027174731803930617		[learning rate: 0.00018481]
	Learning Rate: 0.000184807
	LOSS [training: 0.03139988458234444 | validation: 0.02753069889474023]
	TIME [epoch: 8.3 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028478727015990885		[learning rate: 0.00018459]
		[batch 20/20] avg loss: 0.020422218237527278		[learning rate: 0.00018437]
	Learning Rate: 0.000184371
	LOSS [training: 0.02445047262675908 | validation: 0.014686022799978755]
	TIME [epoch: 8.32 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02355348766868095		[learning rate: 0.00018415]
		[batch 20/20] avg loss: 0.01939246977715557		[learning rate: 0.00018394]
	Learning Rate: 0.000183936
	LOSS [training: 0.021472978722918257 | validation: 0.03347079621782438]
	TIME [epoch: 8.29 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030112353055532243		[learning rate: 0.00018372]
		[batch 20/20] avg loss: 0.028805326265315588		[learning rate: 0.0001835]
	Learning Rate: 0.000183502
	LOSS [training: 0.029458839660423917 | validation: 0.020029333746257934]
	TIME [epoch: 8.3 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02263037517190978		[learning rate: 0.00018329]
		[batch 20/20] avg loss: 0.026154440163928504		[learning rate: 0.00018307]
	Learning Rate: 0.000183069
	LOSS [training: 0.02439240766791915 | validation: 0.03143006898616641]
	TIME [epoch: 8.31 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019514081020543857		[learning rate: 0.00018285]
		[batch 20/20] avg loss: 0.03007743476841348		[learning rate: 0.00018264]
	Learning Rate: 0.000182637
	LOSS [training: 0.024795757894478666 | validation: 0.02877518751445451]
	TIME [epoch: 8.31 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024589911523510404		[learning rate: 0.00018242]
		[batch 20/20] avg loss: 0.029720342012349854		[learning rate: 0.00018221]
	Learning Rate: 0.000182207
	LOSS [training: 0.02715512676793013 | validation: 0.03980625852770485]
	TIME [epoch: 8.3 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029604297485025764		[learning rate: 0.00018199]
		[batch 20/20] avg loss: 0.026185639916655335		[learning rate: 0.00018178]
	Learning Rate: 0.000181777
	LOSS [training: 0.027894968700840544 | validation: 0.019489865878598786]
	TIME [epoch: 8.3 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023892842371899124		[learning rate: 0.00018156]
		[batch 20/20] avg loss: 0.04168503876857886		[learning rate: 0.00018135]
	Learning Rate: 0.000181348
	LOSS [training: 0.032788940570239 | validation: 0.014184027315755866]
	TIME [epoch: 8.31 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026064449843623792		[learning rate: 0.00018113]
		[batch 20/20] avg loss: 0.028841992130290116		[learning rate: 0.00018092]
	Learning Rate: 0.00018092
	LOSS [training: 0.027453220986956954 | validation: 0.019425873996099713]
	TIME [epoch: 8.31 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03091707309435327		[learning rate: 0.00018071]
		[batch 20/20] avg loss: 0.02100502730426087		[learning rate: 0.00018049]
	Learning Rate: 0.000180493
	LOSS [training: 0.025961050199307067 | validation: 0.017067612548082985]
	TIME [epoch: 8.3 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02472872711162629		[learning rate: 0.00018028]
		[batch 20/20] avg loss: 0.03218205813006198		[learning rate: 0.00018007]
	Learning Rate: 0.000180068
	LOSS [training: 0.02845539262084414 | validation: 0.016726278249744254]
	TIME [epoch: 8.29 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027064819896026947		[learning rate: 0.00017986]
		[batch 20/20] avg loss: 0.03431463138456009		[learning rate: 0.00017964]
	Learning Rate: 0.000179643
	LOSS [training: 0.03068972564029352 | validation: 0.0206520741138925]
	TIME [epoch: 8.31 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019390523761285496		[learning rate: 0.00017943]
		[batch 20/20] avg loss: 0.03037582267137202		[learning rate: 0.00017922]
	Learning Rate: 0.000179219
	LOSS [training: 0.024883173216328755 | validation: 0.034433166046811764]
	TIME [epoch: 8.32 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03392433837702445		[learning rate: 0.00017901]
		[batch 20/20] avg loss: 0.023325707229027378		[learning rate: 0.0001788]
	Learning Rate: 0.000178796
	LOSS [training: 0.028625022803025913 | validation: 0.026420569273010824]
	TIME [epoch: 8.3 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021497713722049366		[learning rate: 0.00017859]
		[batch 20/20] avg loss: 0.03137821338002207		[learning rate: 0.00017837]
	Learning Rate: 0.000178375
	LOSS [training: 0.026437963551035715 | validation: 0.01881404985643655]
	TIME [epoch: 8.3 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020102137572511124		[learning rate: 0.00017816]
		[batch 20/20] avg loss: 0.02542534801762425		[learning rate: 0.00017795]
	Learning Rate: 0.000177954
	LOSS [training: 0.022763742795067685 | validation: 0.016382519115194154]
	TIME [epoch: 8.3 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027612715908716057		[learning rate: 0.00017774]
		[batch 20/20] avg loss: 0.025074977116909935		[learning rate: 0.00017753]
	Learning Rate: 0.000177534
	LOSS [training: 0.026343846512812998 | validation: 0.012877268964272357]
	TIME [epoch: 8.31 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030915196003134488		[learning rate: 0.00017732]
		[batch 20/20] avg loss: 0.027943693041576644		[learning rate: 0.00017712]
	Learning Rate: 0.000177115
	LOSS [training: 0.029429444522355568 | validation: 0.017551321799012337]
	TIME [epoch: 8.3 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021653341433324962		[learning rate: 0.00017691]
		[batch 20/20] avg loss: 0.022433673035493686		[learning rate: 0.0001767]
	Learning Rate: 0.000176698
	LOSS [training: 0.022043507234409322 | validation: 0.019911805022997723]
	TIME [epoch: 8.3 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02583541242663801		[learning rate: 0.00017649]
		[batch 20/20] avg loss: 0.02252906260698088		[learning rate: 0.00017628]
	Learning Rate: 0.000176281
	LOSS [training: 0.024182237516809445 | validation: 0.02826822393089664]
	TIME [epoch: 8.31 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02435365235322965		[learning rate: 0.00017607]
		[batch 20/20] avg loss: 0.0258746584220217		[learning rate: 0.00017587]
	Learning Rate: 0.000175865
	LOSS [training: 0.025114155387625674 | validation: 0.02854729965411291]
	TIME [epoch: 8.32 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03603792740313189		[learning rate: 0.00017566]
		[batch 20/20] avg loss: 0.029323096454795798		[learning rate: 0.00017545]
	Learning Rate: 0.00017545
	LOSS [training: 0.03268051192896385 | validation: 0.021233407483807903]
	TIME [epoch: 8.29 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024246994876498645		[learning rate: 0.00017524]
		[batch 20/20] avg loss: 0.02265181361614204		[learning rate: 0.00017504]
	Learning Rate: 0.000175036
	LOSS [training: 0.023449404246320343 | validation: 0.022622406169395457]
	TIME [epoch: 8.3 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03133318058742153		[learning rate: 0.00017483]
		[batch 20/20] avg loss: 0.031447511516668226		[learning rate: 0.00017462]
	Learning Rate: 0.000174623
	LOSS [training: 0.031390346052044875 | validation: 0.02364399974135678]
	TIME [epoch: 8.3 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02319838846391993		[learning rate: 0.00017442]
		[batch 20/20] avg loss: 0.025886804543418594		[learning rate: 0.00017421]
	Learning Rate: 0.000174212
	LOSS [training: 0.024542596503669262 | validation: 0.02439889613529534]
	TIME [epoch: 8.32 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01968414333028054		[learning rate: 0.00017401]
		[batch 20/20] avg loss: 0.030697369967055403		[learning rate: 0.0001738]
	Learning Rate: 0.000173801
	LOSS [training: 0.025190756648667968 | validation: 0.018940827378367605]
	TIME [epoch: 8.3 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03058001363141217		[learning rate: 0.0001736]
		[batch 20/20] avg loss: 0.024805001132655925		[learning rate: 0.00017339]
	Learning Rate: 0.000173391
	LOSS [training: 0.02769250738203404 | validation: 0.02823597984790606]
	TIME [epoch: 8.29 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026015272335535626		[learning rate: 0.00017319]
		[batch 20/20] avg loss: 0.016445931117402654		[learning rate: 0.00017298]
	Learning Rate: 0.000172982
	LOSS [training: 0.021230601726469144 | validation: 0.017134470385532674]
	TIME [epoch: 8.31 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026479959299264926		[learning rate: 0.00017278]
		[batch 20/20] avg loss: 0.03193474385341752		[learning rate: 0.00017257]
	Learning Rate: 0.000172574
	LOSS [training: 0.029207351576341228 | validation: 0.00786838624947675]
	TIME [epoch: 8.31 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02497470919180799		[learning rate: 0.00017237]
		[batch 20/20] avg loss: 0.02637558029191786		[learning rate: 0.00017217]
	Learning Rate: 0.000172167
	LOSS [training: 0.025675144741862926 | validation: 0.025971777029946512]
	TIME [epoch: 8.29 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02435650136462671		[learning rate: 0.00017196]
		[batch 20/20] avg loss: 0.02959728097682344		[learning rate: 0.00017176]
	Learning Rate: 0.00017176
	LOSS [training: 0.02697689117072507 | validation: 0.036327382076993125]
	TIME [epoch: 8.29 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03486701689928234		[learning rate: 0.00017156]
		[batch 20/20] avg loss: 0.021274650103832154		[learning rate: 0.00017136]
	Learning Rate: 0.000171355
	LOSS [training: 0.028070833501557247 | validation: 0.008662134676661132]
	TIME [epoch: 8.31 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0192175762661521		[learning rate: 0.00017115]
		[batch 20/20] avg loss: 0.02653063903212357		[learning rate: 0.00017095]
	Learning Rate: 0.000170951
	LOSS [training: 0.02287410764913783 | validation: 0.0124902852457702]
	TIME [epoch: 8.3 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020480104238736897		[learning rate: 0.00017075]
		[batch 20/20] avg loss: 0.034734635841079074		[learning rate: 0.00017055]
	Learning Rate: 0.000170548
	LOSS [training: 0.027607370039907986 | validation: 0.023372703416554334]
	TIME [epoch: 8.29 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02935303684830624		[learning rate: 0.00017035]
		[batch 20/20] avg loss: 0.02259309746719873		[learning rate: 0.00017015]
	Learning Rate: 0.000170146
	LOSS [training: 0.025973067157752494 | validation: 0.020579436468911052]
	TIME [epoch: 8.3 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021756021730866184		[learning rate: 0.00016994]
		[batch 20/20] avg loss: 0.024552594107981607		[learning rate: 0.00016974]
	Learning Rate: 0.000169744
	LOSS [training: 0.023154307919423894 | validation: 0.01656231630532745]
	TIME [epoch: 8.31 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028543206945826925		[learning rate: 0.00016954]
		[batch 20/20] avg loss: 0.02266103541350913		[learning rate: 0.00016934]
	Learning Rate: 0.000169344
	LOSS [training: 0.025602121179668026 | validation: 0.01730635136193407]
	TIME [epoch: 8.31 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024298587469472513		[learning rate: 0.00016914]
		[batch 20/20] avg loss: 0.029150986328283123		[learning rate: 0.00016894]
	Learning Rate: 0.000168944
	LOSS [training: 0.026724786898877816 | validation: 0.043221181760749454]
	TIME [epoch: 8.29 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03511449168818981		[learning rate: 0.00016874]
		[batch 20/20] avg loss: 0.025525999964563485		[learning rate: 0.00016855]
	Learning Rate: 0.000168546
	LOSS [training: 0.030320245826376647 | validation: 0.029830724612544265]
	TIME [epoch: 8.3 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03208398869789263		[learning rate: 0.00016835]
		[batch 20/20] avg loss: 0.023126751237347996		[learning rate: 0.00016815]
	Learning Rate: 0.000168148
	LOSS [training: 0.027605369967620315 | validation: 0.026785264655499078]
	TIME [epoch: 8.3 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023968446625296565		[learning rate: 0.00016795]
		[batch 20/20] avg loss: 0.024176835064121764		[learning rate: 0.00016775]
	Learning Rate: 0.000167752
	LOSS [training: 0.024072640844709163 | validation: 0.021239785459365824]
	TIME [epoch: 8.31 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025549930783016577		[learning rate: 0.00016755]
		[batch 20/20] avg loss: 0.0190521344190959		[learning rate: 0.00016736]
	Learning Rate: 0.000167356
	LOSS [training: 0.022301032601056238 | validation: 0.020316770959975194]
	TIME [epoch: 8.3 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02858005141070814		[learning rate: 0.00016716]
		[batch 20/20] avg loss: 0.015888120222014787		[learning rate: 0.00016696]
	Learning Rate: 0.000166961
	LOSS [training: 0.02223408581636146 | validation: 0.013864908127702132]
	TIME [epoch: 8.3 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0294940463406898		[learning rate: 0.00016676]
		[batch 20/20] avg loss: 0.01982717275909639		[learning rate: 0.00016657]
	Learning Rate: 0.000166567
	LOSS [training: 0.0246606095498931 | validation: 0.017606161439483806]
	TIME [epoch: 8.31 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023196880208067004		[learning rate: 0.00016637]
		[batch 20/20] avg loss: 0.02616249349595373		[learning rate: 0.00016617]
	Learning Rate: 0.000166174
	LOSS [training: 0.024679686852010364 | validation: 0.022432467742274538]
	TIME [epoch: 8.3 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025933971838639054		[learning rate: 0.00016598]
		[batch 20/20] avg loss: 0.02094355261571559		[learning rate: 0.00016578]
	Learning Rate: 0.000165782
	LOSS [training: 0.023438762227177327 | validation: 0.01136871783674882]
	TIME [epoch: 8.29 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02006686190872868		[learning rate: 0.00016559]
		[batch 20/20] avg loss: 0.02677462436978994		[learning rate: 0.00016539]
	Learning Rate: 0.000165391
	LOSS [training: 0.023420743139259313 | validation: 0.02455325594308726]
	TIME [epoch: 8.3 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02388927544002293		[learning rate: 0.0001652]
		[batch 20/20] avg loss: 0.032306331202283066		[learning rate: 0.000165]
	Learning Rate: 0.000165001
	LOSS [training: 0.028097803321152998 | validation: 0.017300318626074458]
	TIME [epoch: 8.31 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030571551506868917		[learning rate: 0.00016481]
		[batch 20/20] avg loss: 0.026737702828001246		[learning rate: 0.00016461]
	Learning Rate: 0.000164612
	LOSS [training: 0.02865462716743508 | validation: 0.013328538957517256]
	TIME [epoch: 8.3 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029215310543219446		[learning rate: 0.00016442]
		[batch 20/20] avg loss: 0.015682082019358314		[learning rate: 0.00016422]
	Learning Rate: 0.000164224
	LOSS [training: 0.02244869628128888 | validation: 0.02124105379245903]
	TIME [epoch: 8.3 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01978316381991888		[learning rate: 0.00016403]
		[batch 20/20] avg loss: 0.032544892640104864		[learning rate: 0.00016384]
	Learning Rate: 0.000163836
	LOSS [training: 0.026164028230011876 | validation: 0.011829218898578553]
	TIME [epoch: 8.29 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01609891952865921		[learning rate: 0.00016364]
		[batch 20/20] avg loss: 0.02453329883083352		[learning rate: 0.00016345]
	Learning Rate: 0.00016345
	LOSS [training: 0.020316109179746362 | validation: 0.014866217823968485]
	TIME [epoch: 8.31 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030768395952423234		[learning rate: 0.00016326]
		[batch 20/20] avg loss: 0.024370626383482332		[learning rate: 0.00016306]
	Learning Rate: 0.000163064
	LOSS [training: 0.02756951116795278 | validation: 0.019047885100811023]
	TIME [epoch: 8.3 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021736051559831975		[learning rate: 0.00016287]
		[batch 20/20] avg loss: 0.02304533121410974		[learning rate: 0.00016268]
	Learning Rate: 0.00016268
	LOSS [training: 0.02239069138697086 | validation: 0.008326866023343335]
	TIME [epoch: 8.29 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022306377323380373		[learning rate: 0.00016249]
		[batch 20/20] avg loss: 0.023999160905552562		[learning rate: 0.0001623]
	Learning Rate: 0.000162296
	LOSS [training: 0.023152769114466468 | validation: 0.032772415492333694]
	TIME [epoch: 8.3 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028609300240928608		[learning rate: 0.0001621]
		[batch 20/20] avg loss: 0.025220899678699477		[learning rate: 0.00016191]
	Learning Rate: 0.000161913
	LOSS [training: 0.026915099959814042 | validation: 0.021762975251874195]
	TIME [epoch: 8.31 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022652986294474067		[learning rate: 0.00016172]
		[batch 20/20] avg loss: 0.025689852149314185		[learning rate: 0.00016153]
	Learning Rate: 0.000161531
	LOSS [training: 0.02417141922189412 | validation: 0.027032355512943376]
	TIME [epoch: 8.3 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024427617559562274		[learning rate: 0.00016134]
		[batch 20/20] avg loss: 0.020155752500432108		[learning rate: 0.00016115]
	Learning Rate: 0.00016115
	LOSS [training: 0.022291685029997187 | validation: 0.022308886776735544]
	TIME [epoch: 8.29 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027562286957059357		[learning rate: 0.00016096]
		[batch 20/20] avg loss: 0.020061226939391826		[learning rate: 0.00016077]
	Learning Rate: 0.00016077
	LOSS [training: 0.023811756948225585 | validation: 0.020006964968564578]
	TIME [epoch: 8.29 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02183547376288024		[learning rate: 0.00016058]
		[batch 20/20] avg loss: 0.023644118902007037		[learning rate: 0.00016039]
	Learning Rate: 0.000160391
	LOSS [training: 0.022739796332443638 | validation: 0.016009936621290723]
	TIME [epoch: 8.31 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029835464898501786		[learning rate: 0.0001602]
		[batch 20/20] avg loss: 0.018837636152632987		[learning rate: 0.00016001]
	Learning Rate: 0.000160012
	LOSS [training: 0.024336550525567387 | validation: 0.022968913897748355]
	TIME [epoch: 8.3 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027892503885068864		[learning rate: 0.00015982]
		[batch 20/20] avg loss: 0.0278097592856723		[learning rate: 0.00015964]
	Learning Rate: 0.000159635
	LOSS [training: 0.02785113158537058 | validation: 0.015362855128724012]
	TIME [epoch: 8.29 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02416434136168501		[learning rate: 0.00015945]
		[batch 20/20] avg loss: 0.0274921409545967		[learning rate: 0.00015926]
	Learning Rate: 0.000159258
	LOSS [training: 0.025828241158140856 | validation: 0.02018796299161528]
	TIME [epoch: 8.29 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030051673367615283		[learning rate: 0.00015907]
		[batch 20/20] avg loss: 0.019488453152567746		[learning rate: 0.00015888]
	Learning Rate: 0.000158883
	LOSS [training: 0.024770063260091513 | validation: 0.02335047178843514]
	TIME [epoch: 8.31 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02336644462012828		[learning rate: 0.0001587]
		[batch 20/20] avg loss: 0.024833162361830714		[learning rate: 0.00015851]
	Learning Rate: 0.000158508
	LOSS [training: 0.024099803490979503 | validation: 0.029804851460095535]
	TIME [epoch: 8.3 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025785605381616217		[learning rate: 0.00015832]
		[batch 20/20] avg loss: 0.027421221461421202		[learning rate: 0.00015813]
	Learning Rate: 0.000158134
	LOSS [training: 0.026603413421518708 | validation: 0.02491406389059037]
	TIME [epoch: 8.29 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02560113454226932		[learning rate: 0.00015795]
		[batch 20/20] avg loss: 0.021860966230971647		[learning rate: 0.00015776]
	Learning Rate: 0.000157761
	LOSS [training: 0.023731050386620482 | validation: 0.016147380372664215]
	TIME [epoch: 8.29 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031900080721760794		[learning rate: 0.00015757]
		[batch 20/20] avg loss: 0.02200086723825951		[learning rate: 0.00015739]
	Learning Rate: 0.000157389
	LOSS [training: 0.02695047398001015 | validation: 0.01685049986883284]
	TIME [epoch: 8.31 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029702906550068124		[learning rate: 0.0001572]
		[batch 20/20] avg loss: 0.03481787726037711		[learning rate: 0.00015702]
	Learning Rate: 0.000157018
	LOSS [training: 0.032260391905222625 | validation: 0.030514092096301505]
	TIME [epoch: 8.3 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03952370712283491		[learning rate: 0.00015683]
		[batch 20/20] avg loss: 0.02013541129528296		[learning rate: 0.00015665]
	Learning Rate: 0.000156647
	LOSS [training: 0.029829559209058925 | validation: 0.011517189320750175]
	TIME [epoch: 8.29 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021836670595927872		[learning rate: 0.00015646]
		[batch 20/20] avg loss: 0.02405614803024695		[learning rate: 0.00015628]
	Learning Rate: 0.000156278
	LOSS [training: 0.02294640931308741 | validation: 0.021346814083875974]
	TIME [epoch: 8.29 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023327951198062993		[learning rate: 0.00015609]
		[batch 20/20] avg loss: 0.021999670956689362		[learning rate: 0.00015591]
	Learning Rate: 0.000155909
	LOSS [training: 0.022663811077376174 | validation: 0.017301149235979994]
	TIME [epoch: 8.31 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02172023703522418		[learning rate: 0.00015573]
		[batch 20/20] avg loss: 0.015459708793593045		[learning rate: 0.00015554]
	Learning Rate: 0.000155541
	LOSS [training: 0.018589972914408608 | validation: 0.022323865473732524]
	TIME [epoch: 8.31 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023760637003940953		[learning rate: 0.00015536]
		[batch 20/20] avg loss: 0.023522534033379398		[learning rate: 0.00015517]
	Learning Rate: 0.000155175
	LOSS [training: 0.023641585518660174 | validation: 0.014563936963806992]
	TIME [epoch: 8.29 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022948199289469583		[learning rate: 0.00015499]
		[batch 20/20] avg loss: 0.02350071560435995		[learning rate: 0.00015481]
	Learning Rate: 0.000154809
	LOSS [training: 0.023224457446914766 | validation: 0.03121752296413519]
	TIME [epoch: 8.3 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02142274198423229		[learning rate: 0.00015463]
		[batch 20/20] avg loss: 0.028434064297129868		[learning rate: 0.00015444]
	Learning Rate: 0.000154443
	LOSS [training: 0.02492840314068108 | validation: 0.024691570091040515]
	TIME [epoch: 8.31 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02099300120363195		[learning rate: 0.00015426]
		[batch 20/20] avg loss: 0.028904376657914697		[learning rate: 0.00015408]
	Learning Rate: 0.000154079
	LOSS [training: 0.024948688930773323 | validation: 0.01632836978946365]
	TIME [epoch: 8.31 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027932269181855224		[learning rate: 0.0001539]
		[batch 20/20] avg loss: 0.018680494790550083		[learning rate: 0.00015372]
	Learning Rate: 0.000153716
	LOSS [training: 0.02330638198620265 | validation: 0.015798477292373957]
	TIME [epoch: 8.29 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022491155004376893		[learning rate: 0.00015353]
		[batch 20/20] avg loss: 0.020941361418557892		[learning rate: 0.00015335]
	Learning Rate: 0.000153353
	LOSS [training: 0.021716258211467394 | validation: 0.02023181087401854]
	TIME [epoch: 8.29 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025161470678322723		[learning rate: 0.00015317]
		[batch 20/20] avg loss: 0.018277472793207905		[learning rate: 0.00015299]
	Learning Rate: 0.000152991
	LOSS [training: 0.021719471735765317 | validation: 0.019678604852467002]
	TIME [epoch: 8.31 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021588130428940994		[learning rate: 0.00015281]
		[batch 20/20] avg loss: 0.027192964638783568		[learning rate: 0.00015263]
	Learning Rate: 0.00015263
	LOSS [training: 0.02439054753386228 | validation: 0.016382899365488698]
	TIME [epoch: 8.3 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02416055312068054		[learning rate: 0.00015245]
		[batch 20/20] avg loss: 0.023787260073509762		[learning rate: 0.00015227]
	Learning Rate: 0.00015227
	LOSS [training: 0.023973906597095147 | validation: 0.02770292355523189]
	TIME [epoch: 8.29 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023805550911559854		[learning rate: 0.00015209]
		[batch 20/20] avg loss: 0.02755494195226562		[learning rate: 0.00015191]
	Learning Rate: 0.000151911
	LOSS [training: 0.025680246431912735 | validation: 0.016941037818908867]
	TIME [epoch: 8.29 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02335968756862384		[learning rate: 0.00015173]
		[batch 20/20] avg loss: 0.022227212361680144		[learning rate: 0.00015155]
	Learning Rate: 0.000151553
	LOSS [training: 0.022793449965151993 | validation: 0.020575574198900933]
	TIME [epoch: 8.31 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02925325051157812		[learning rate: 0.00015137]
		[batch 20/20] avg loss: 0.025922454573263587		[learning rate: 0.0001512]
	Learning Rate: 0.000151195
	LOSS [training: 0.02758785254242086 | validation: 0.025256007935261685]
	TIME [epoch: 8.31 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03491645139134369		[learning rate: 0.00015102]
		[batch 20/20] avg loss: 0.01365987180018675		[learning rate: 0.00015084]
	Learning Rate: 0.000150839
	LOSS [training: 0.024288161595765224 | validation: 0.020964192156472456]
	TIME [epoch: 8.3 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029387251671315034		[learning rate: 0.00015066]
		[batch 20/20] avg loss: 0.01710191356325602		[learning rate: 0.00015048]
	Learning Rate: 0.000150483
	LOSS [training: 0.023244582617285525 | validation: 0.029597254773589533]
	TIME [epoch: 8.3 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022214514331343038		[learning rate: 0.00015031]
		[batch 20/20] avg loss: 0.024871625001834995		[learning rate: 0.00015013]
	Learning Rate: 0.000150128
	LOSS [training: 0.02354306966658902 | validation: 0.021284783605112087]
	TIME [epoch: 8.31 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036085435084322895		[learning rate: 0.00014995]
		[batch 20/20] avg loss: 0.021927409570851413		[learning rate: 0.00014977]
	Learning Rate: 0.000149774
	LOSS [training: 0.02900642232758715 | validation: 0.017283929274982682]
	TIME [epoch: 8.31 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023132569790132833		[learning rate: 0.0001496]
		[batch 20/20] avg loss: 0.02589467384260786		[learning rate: 0.00014942]
	Learning Rate: 0.000149421
	LOSS [training: 0.024513621816370346 | validation: 0.02164113737333923]
	TIME [epoch: 8.3 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027035376675681143		[learning rate: 0.00014924]
		[batch 20/20] avg loss: 0.02426034631936016		[learning rate: 0.00014907]
	Learning Rate: 0.000149068
	LOSS [training: 0.025647861497520647 | validation: 0.021343404056574658]
	TIME [epoch: 8.29 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02248117949097791		[learning rate: 0.00014889]
		[batch 20/20] avg loss: 0.02456767806310419		[learning rate: 0.00014872]
	Learning Rate: 0.000148716
	LOSS [training: 0.02352442877704105 | validation: 0.02161890209909974]
	TIME [epoch: 8.31 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023299552168759622		[learning rate: 0.00014854]
		[batch 20/20] avg loss: 0.021434933276468213		[learning rate: 0.00014837]
	Learning Rate: 0.000148366
	LOSS [training: 0.02236724272261392 | validation: 0.020631684596305315]
	TIME [epoch: 8.3 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021214977682723228		[learning rate: 0.00014819]
		[batch 20/20] avg loss: 0.027326607221621436		[learning rate: 0.00014802]
	Learning Rate: 0.000148016
	LOSS [training: 0.024270792452172334 | validation: 0.017719510110259247]
	TIME [epoch: 8.29 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020768565345833958		[learning rate: 0.00014784]
		[batch 20/20] avg loss: 0.02682743180775507		[learning rate: 0.00014767]
	Learning Rate: 0.000147667
	LOSS [training: 0.023797998576794512 | validation: 0.019111916629387434]
	TIME [epoch: 8.3 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029943484895280627		[learning rate: 0.00014749]
		[batch 20/20] avg loss: 0.02725396391383198		[learning rate: 0.00014732]
	Learning Rate: 0.000147318
	LOSS [training: 0.028598724404556304 | validation: 0.01787655067838537]
	TIME [epoch: 8.31 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019315915034034147		[learning rate: 0.00014714]
		[batch 20/20] avg loss: 0.029930765583511397		[learning rate: 0.00014697]
	Learning Rate: 0.000146971
	LOSS [training: 0.02462334030877277 | validation: 0.019894090330579206]
	TIME [epoch: 8.3 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02333200828042263		[learning rate: 0.0001468]
		[batch 20/20] avg loss: 0.022899089316706513		[learning rate: 0.00014662]
	Learning Rate: 0.000146624
	LOSS [training: 0.023115548798564572 | validation: 0.019342572308485514]
	TIME [epoch: 8.3 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028504775228433525		[learning rate: 0.00014645]
		[batch 20/20] avg loss: 0.022600327531463633		[learning rate: 0.00014628]
	Learning Rate: 0.000146278
	LOSS [training: 0.02555255137994858 | validation: 0.016122065631213656]
	TIME [epoch: 8.29 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019222766295510566		[learning rate: 0.00014611]
		[batch 20/20] avg loss: 0.020663839421342194		[learning rate: 0.00014593]
	Learning Rate: 0.000145933
	LOSS [training: 0.019943302858426375 | validation: 0.015800026590986185]
	TIME [epoch: 8.31 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02205561177797438		[learning rate: 0.00014576]
		[batch 20/20] avg loss: 0.019209620789053757		[learning rate: 0.00014559]
	Learning Rate: 0.000145589
	LOSS [training: 0.02063261628351407 | validation: 0.017055723562445585]
	TIME [epoch: 8.3 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030942585367678292		[learning rate: 0.00014542]
		[batch 20/20] avg loss: 0.021781102723209426		[learning rate: 0.00014525]
	Learning Rate: 0.000145245
	LOSS [training: 0.02636184404544386 | validation: 0.016485182164058265]
	TIME [epoch: 8.3 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027044259127354264		[learning rate: 0.00014507]
		[batch 20/20] avg loss: 0.0339599376975836		[learning rate: 0.0001449]
	Learning Rate: 0.000144903
	LOSS [training: 0.030502098412468932 | validation: 0.006300593124509574]
	TIME [epoch: 8.29 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02316789472454806		[learning rate: 0.00014473]
		[batch 20/20] avg loss: 0.024745886865021178		[learning rate: 0.00014456]
	Learning Rate: 0.000144561
	LOSS [training: 0.023956890794784615 | validation: 0.013768199521297163]
	TIME [epoch: 8.32 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02113293870766838		[learning rate: 0.00014439]
		[batch 20/20] avg loss: 0.024420078340511592		[learning rate: 0.00014422]
	Learning Rate: 0.00014422
	LOSS [training: 0.022776508524089988 | validation: 0.017306507687338518]
	TIME [epoch: 8.31 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01928088120215031		[learning rate: 0.00014405]
		[batch 20/20] avg loss: 0.024708778174022058		[learning rate: 0.00014388]
	Learning Rate: 0.00014388
	LOSS [training: 0.021994829688086186 | validation: 0.023212779415170748]
	TIME [epoch: 8.3 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019279258315978546		[learning rate: 0.00014371]
		[batch 20/20] avg loss: 0.03411195724528713		[learning rate: 0.00014354]
	Learning Rate: 0.00014354
	LOSS [training: 0.026695607780632836 | validation: 0.03339903943117018]
	TIME [epoch: 8.3 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020527482054647977		[learning rate: 0.00014337]
		[batch 20/20] avg loss: 0.028585518539052352		[learning rate: 0.0001432]
	Learning Rate: 0.000143202
	LOSS [training: 0.024556500296850168 | validation: 0.020152812973627282]
	TIME [epoch: 8.32 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02655868773737837		[learning rate: 0.00014303]
		[batch 20/20] avg loss: 0.019102943608535792		[learning rate: 0.00014286]
	Learning Rate: 0.000142864
	LOSS [training: 0.022830815672957083 | validation: 0.012740516966069447]
	TIME [epoch: 8.31 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021011039492954457		[learning rate: 0.0001427]
		[batch 20/20] avg loss: 0.0225651054106034		[learning rate: 0.00014253]
	Learning Rate: 0.000142527
	LOSS [training: 0.021788072451778922 | validation: 0.012195856924460603]
	TIME [epoch: 8.3 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020860144474001214		[learning rate: 0.00014236]
		[batch 20/20] avg loss: 0.01825403436537418		[learning rate: 0.00014219]
	Learning Rate: 0.000142191
	LOSS [training: 0.019557089419687694 | validation: 0.01238239733175692]
	TIME [epoch: 8.3 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028592007604184528		[learning rate: 0.00014202]
		[batch 20/20] avg loss: 0.025031046156709508		[learning rate: 0.00014186]
	Learning Rate: 0.000141855
	LOSS [training: 0.026811526880447016 | validation: 0.02474841066260396]
	TIME [epoch: 8.31 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02386281523565856		[learning rate: 0.00014169]
		[batch 20/20] avg loss: 0.01918089525555881		[learning rate: 0.00014152]
	Learning Rate: 0.000141521
	LOSS [training: 0.02152185524560869 | validation: 0.010244313975655714]
	TIME [epoch: 8.3 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02243033439754127		[learning rate: 0.00014135]
		[batch 20/20] avg loss: 0.02544824729534826		[learning rate: 0.00014119]
	Learning Rate: 0.000141187
	LOSS [training: 0.023939290846444763 | validation: 0.020379396241935667]
	TIME [epoch: 8.3 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021775320538867615		[learning rate: 0.00014102]
		[batch 20/20] avg loss: 0.023529809347488256		[learning rate: 0.00014085]
	Learning Rate: 0.000140854
	LOSS [training: 0.02265256494317793 | validation: 0.01838046604013502]
	TIME [epoch: 8.3 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013384895436096365		[learning rate: 0.00014069]
		[batch 20/20] avg loss: 0.028212746054249515		[learning rate: 0.00014052]
	Learning Rate: 0.000140522
	LOSS [training: 0.02079882074517294 | validation: 0.012586897888804984]
	TIME [epoch: 8.32 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022221567662719162		[learning rate: 0.00014036]
		[batch 20/20] avg loss: 0.02559204836843503		[learning rate: 0.00014019]
	Learning Rate: 0.00014019
	LOSS [training: 0.0239068080155771 | validation: 0.012861186731195384]
	TIME [epoch: 8.3 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023783032559306062		[learning rate: 0.00014002]
		[batch 20/20] avg loss: 0.02757398465003823		[learning rate: 0.00013986]
	Learning Rate: 0.00013986
	LOSS [training: 0.025678508604672145 | validation: 0.018493130647190152]
	TIME [epoch: 8.3 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026895193125897165		[learning rate: 0.00013969]
		[batch 20/20] avg loss: 0.023883266731366735		[learning rate: 0.00013953]
	Learning Rate: 0.00013953
	LOSS [training: 0.02538922992863194 | validation: 0.020427594325713106]
	TIME [epoch: 8.29 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022960582879173956		[learning rate: 0.00013937]
		[batch 20/20] avg loss: 0.02333119793812769		[learning rate: 0.0001392]
	Learning Rate: 0.000139201
	LOSS [training: 0.023145890408650827 | validation: 0.03082144003235414]
	TIME [epoch: 8.32 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022103285453443627		[learning rate: 0.00013904]
		[batch 20/20] avg loss: 0.023205552512763593		[learning rate: 0.00013887]
	Learning Rate: 0.000138872
	LOSS [training: 0.022654418983103608 | validation: 0.01843426689619438]
	TIME [epoch: 8.29 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037378321284939495		[learning rate: 0.00013871]
		[batch 20/20] avg loss: 0.014771864037928222		[learning rate: 0.00013854]
	Learning Rate: 0.000138545
	LOSS [training: 0.026075092661433857 | validation: 0.021455287652968977]
	TIME [epoch: 8.3 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022504229479788113		[learning rate: 0.00013838]
		[batch 20/20] avg loss: 0.019406061703714977		[learning rate: 0.00013822]
	Learning Rate: 0.000138218
	LOSS [training: 0.02095514559175154 | validation: 0.022998818870897997]
	TIME [epoch: 8.3 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024624821141409337		[learning rate: 0.00013805]
		[batch 20/20] avg loss: 0.024785128148414447		[learning rate: 0.00013789]
	Learning Rate: 0.000137892
	LOSS [training: 0.02470497464491189 | validation: 0.01406945226091822]
	TIME [epoch: 8.31 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017521978732117034		[learning rate: 0.00013773]
		[batch 20/20] avg loss: 0.024501187056761277		[learning rate: 0.00013757]
	Learning Rate: 0.000137567
	LOSS [training: 0.021011582894439156 | validation: 0.016243914695653627]
	TIME [epoch: 8.3 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02832301330932449		[learning rate: 0.0001374]
		[batch 20/20] avg loss: 0.021185659039568934		[learning rate: 0.00013724]
	Learning Rate: 0.000137242
	LOSS [training: 0.024754336174446712 | validation: 0.016162873745277972]
	TIME [epoch: 8.29 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027435870365434246		[learning rate: 0.00013708]
		[batch 20/20] avg loss: 0.022594106234477183		[learning rate: 0.00013692]
	Learning Rate: 0.000136918
	LOSS [training: 0.025014988299955715 | validation: 0.014006947711078336]
	TIME [epoch: 8.3 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018886985336484866		[learning rate: 0.00013676]
		[batch 20/20] avg loss: 0.0261353976225635		[learning rate: 0.0001366]
	Learning Rate: 0.000136595
	LOSS [training: 0.022511191479524188 | validation: 0.027694881511194775]
	TIME [epoch: 8.31 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022226249233016835		[learning rate: 0.00013643]
		[batch 20/20] avg loss: 0.03162192294751693		[learning rate: 0.00013627]
	Learning Rate: 0.000136273
	LOSS [training: 0.026924086090266874 | validation: 0.026068046079967024]
	TIME [epoch: 8.31 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027513997794435162		[learning rate: 0.00013611]
		[batch 20/20] avg loss: 0.015348560997257746		[learning rate: 0.00013595]
	Learning Rate: 0.000135952
	LOSS [training: 0.021431279395846454 | validation: 0.012507974532364305]
	TIME [epoch: 8.3 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021513025693337396		[learning rate: 0.00013579]
		[batch 20/20] avg loss: 0.019985123738903182		[learning rate: 0.00013563]
	Learning Rate: 0.000135631
	LOSS [training: 0.020749074716120287 | validation: 0.018404515451085453]
	TIME [epoch: 8.3 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019922221573950877		[learning rate: 0.00013547]
		[batch 20/20] avg loss: 0.025370644008763477		[learning rate: 0.00013531]
	Learning Rate: 0.000135311
	LOSS [training: 0.022646432791357175 | validation: 0.03228886290285448]
	TIME [epoch: 8.31 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02625872253617225		[learning rate: 0.00013515]
		[batch 20/20] avg loss: 0.02477054115474529		[learning rate: 0.00013499]
	Learning Rate: 0.000134992
	LOSS [training: 0.025514631845458775 | validation: 0.013435597348438346]
	TIME [epoch: 8.3 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023756017995604777		[learning rate: 0.00013483]
		[batch 20/20] avg loss: 0.02273508883528625		[learning rate: 0.00013467]
	Learning Rate: 0.000134673
	LOSS [training: 0.02324555341544552 | validation: 0.017789367708813328]
	TIME [epoch: 8.29 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02446581419006453		[learning rate: 0.00013451]
		[batch 20/20] avg loss: 0.024074558026875782		[learning rate: 0.00013436]
	Learning Rate: 0.000134356
	LOSS [training: 0.02427018610847016 | validation: 0.018001425485289532]
	TIME [epoch: 8.29 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022287594337359537		[learning rate: 0.0001342]
		[batch 20/20] avg loss: 0.030549985443855517		[learning rate: 0.00013404]
	Learning Rate: 0.000134039
	LOSS [training: 0.026418789890607523 | validation: 0.013354119486901708]
	TIME [epoch: 8.32 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01624499109529353		[learning rate: 0.00013388]
		[batch 20/20] avg loss: 0.028275400517961364		[learning rate: 0.00013372]
	Learning Rate: 0.000133723
	LOSS [training: 0.022260195806627442 | validation: 0.011946358218395733]
	TIME [epoch: 8.31 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02256797593472994		[learning rate: 0.00013356]
		[batch 20/20] avg loss: 0.01886525831656427		[learning rate: 0.00013341]
	Learning Rate: 0.000133407
	LOSS [training: 0.020716617125647107 | validation: 0.01918508371802555]
	TIME [epoch: 8.29 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023392877986545244		[learning rate: 0.00013325]
		[batch 20/20] avg loss: 0.027833901191211114		[learning rate: 0.00013309]
	Learning Rate: 0.000133093
	LOSS [training: 0.025613389588878176 | validation: 0.02099322178319463]
	TIME [epoch: 8.29 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03190345094972373		[learning rate: 0.00013294]
		[batch 20/20] avg loss: 0.02433277030047155		[learning rate: 0.00013278]
	Learning Rate: 0.000132779
	LOSS [training: 0.028118110625097642 | validation: 0.01595834523861967]
	TIME [epoch: 8.32 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01798202998403933		[learning rate: 0.00013262]
		[batch 20/20] avg loss: 0.019769309017167225		[learning rate: 0.00013247]
	Learning Rate: 0.000132465
	LOSS [training: 0.01887566950060328 | validation: 0.01343810369684776]
	TIME [epoch: 8.3 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02212655226070792		[learning rate: 0.00013231]
		[batch 20/20] avg loss: 0.02902840860874356		[learning rate: 0.00013215]
	Learning Rate: 0.000132153
	LOSS [training: 0.025577480434725734 | validation: 0.023758805494598464]
	TIME [epoch: 8.29 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02410128749462678		[learning rate: 0.000132]
		[batch 20/20] avg loss: 0.019581591661499264		[learning rate: 0.00013184]
	Learning Rate: 0.000131841
	LOSS [training: 0.021841439578063022 | validation: 0.018277987249602397]
	TIME [epoch: 8.3 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021567940841439058		[learning rate: 0.00013169]
		[batch 20/20] avg loss: 0.032298942058210066		[learning rate: 0.00013153]
	Learning Rate: 0.00013153
	LOSS [training: 0.026933441449824562 | validation: 0.028515810390400072]
	TIME [epoch: 8.32 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023023987410273578		[learning rate: 0.00013138]
		[batch 20/20] avg loss: 0.02614563632496523		[learning rate: 0.00013122]
	Learning Rate: 0.00013122
	LOSS [training: 0.024584811867619405 | validation: 0.03711247224581092]
	TIME [epoch: 8.31 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03844811324907803		[learning rate: 0.00013107]
		[batch 20/20] avg loss: 0.025550967604125424		[learning rate: 0.00013091]
	Learning Rate: 0.00013091
	LOSS [training: 0.03199954042660172 | validation: 0.01154188378637446]
	TIME [epoch: 8.29 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023877754998293168		[learning rate: 0.00013076]
		[batch 20/20] avg loss: 0.02104124670202639		[learning rate: 0.0001306]
	Learning Rate: 0.000130602
	LOSS [training: 0.022459500850159783 | validation: 0.027268178689022274]
	TIME [epoch: 8.3 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02251369274349619		[learning rate: 0.00013045]
		[batch 20/20] avg loss: 0.03168971235789622		[learning rate: 0.00013029]
	Learning Rate: 0.000130294
	LOSS [training: 0.02710170255069621 | validation: 0.030886003220145383]
	TIME [epoch: 8.32 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035358217214347384		[learning rate: 0.00013014]
		[batch 20/20] avg loss: 0.022632227624541743		[learning rate: 0.00012999]
	Learning Rate: 0.000129986
	LOSS [training: 0.028995222419444567 | validation: 0.020708339674614662]
	TIME [epoch: 8.3 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017053820817478825		[learning rate: 0.00012983]
		[batch 20/20] avg loss: 0.02218953101450647		[learning rate: 0.00012968]
	Learning Rate: 0.00012968
	LOSS [training: 0.01962167591599264 | validation: 0.012135688009284717]
	TIME [epoch: 8.29 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0250174666252098		[learning rate: 0.00012953]
		[batch 20/20] avg loss: 0.025961736102999185		[learning rate: 0.00012937]
	Learning Rate: 0.000129374
	LOSS [training: 0.02548960136410449 | validation: 0.019088083624580404]
	TIME [epoch: 8.3 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026628407331302745		[learning rate: 0.00012922]
		[batch 20/20] avg loss: 0.029852490820627114		[learning rate: 0.00012907]
	Learning Rate: 0.000129069
	LOSS [training: 0.02824044907596493 | validation: 0.015972047606536022]
	TIME [epoch: 8.31 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022894763342405155		[learning rate: 0.00012892]
		[batch 20/20] avg loss: 0.02312531540140017		[learning rate: 0.00012876]
	Learning Rate: 0.000128764
	LOSS [training: 0.023010039371902664 | validation: 0.014835406952100286]
	TIME [epoch: 8.29 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022607228528877046		[learning rate: 0.00012861]
		[batch 20/20] avg loss: 0.025174703923584212		[learning rate: 0.00012846]
	Learning Rate: 0.00012846
	LOSS [training: 0.023890966226230628 | validation: 0.019809789375950012]
	TIME [epoch: 8.29 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024215792955667857		[learning rate: 0.00012831]
		[batch 20/20] avg loss: 0.02238730039954507		[learning rate: 0.00012816]
	Learning Rate: 0.000128157
	LOSS [training: 0.023301546677606463 | validation: 0.017415168490289103]
	TIME [epoch: 8.29 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02411902408126659		[learning rate: 0.00012801]
		[batch 20/20] avg loss: 0.03272788403598544		[learning rate: 0.00012786]
	Learning Rate: 0.000127855
	LOSS [training: 0.028423454058626018 | validation: 0.017804256403304804]
	TIME [epoch: 8.32 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018243747754809476		[learning rate: 0.0001277]
		[batch 20/20] avg loss: 0.02996852469176802		[learning rate: 0.00012755]
	Learning Rate: 0.000127553
	LOSS [training: 0.024106136223288752 | validation: 0.018779208800768922]
	TIME [epoch: 8.3 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025409674412473666		[learning rate: 0.0001274]
		[batch 20/20] avg loss: 0.01855009697580612		[learning rate: 0.00012725]
	Learning Rate: 0.000127253
	LOSS [training: 0.021979885694139892 | validation: 0.022333130040117315]
	TIME [epoch: 8.29 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02535493625248641		[learning rate: 0.0001271]
		[batch 20/20] avg loss: 0.021748523202744285		[learning rate: 0.00012695]
	Learning Rate: 0.000126952
	LOSS [training: 0.02355172972761535 | validation: 0.016022949678657267]
	TIME [epoch: 8.3 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02176273401402338		[learning rate: 0.0001268]
		[batch 20/20] avg loss: 0.02521812499309281		[learning rate: 0.00012665]
	Learning Rate: 0.000126653
	LOSS [training: 0.023490429503558098 | validation: 0.015320839054303982]
	TIME [epoch: 8.31 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02480117116075023		[learning rate: 0.0001265]
		[batch 20/20] avg loss: 0.028248577502755702		[learning rate: 0.00012635]
	Learning Rate: 0.000126354
	LOSS [training: 0.026524874331752962 | validation: 0.014181429578109092]
	TIME [epoch: 8.3 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03093194459248036		[learning rate: 0.00012621]
		[batch 20/20] avg loss: 0.013940590460321655		[learning rate: 0.00012606]
	Learning Rate: 0.000126056
	LOSS [training: 0.02243626752640101 | validation: 0.01853749524057951]
	TIME [epoch: 8.29 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019035374571443508		[learning rate: 0.00012591]
		[batch 20/20] avg loss: 0.02223576402447864		[learning rate: 0.00012576]
	Learning Rate: 0.000125759
	LOSS [training: 0.020635569297961072 | validation: 0.016304920459131105]
	TIME [epoch: 8.3 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020013244860007506		[learning rate: 0.00012561]
		[batch 20/20] avg loss: 0.025954457552926684		[learning rate: 0.00012546]
	Learning Rate: 0.000125462
	LOSS [training: 0.0229838512064671 | validation: 0.018983350643439704]
	TIME [epoch: 8.32 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021108611729088796		[learning rate: 0.00012531]
		[batch 20/20] avg loss: 0.022506724170619007		[learning rate: 0.00012517]
	Learning Rate: 0.000125166
	LOSS [training: 0.0218076679498539 | validation: 0.019412069283501664]
	TIME [epoch: 8.3 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023437566801750558		[learning rate: 0.00012502]
		[batch 20/20] avg loss: 0.03042689955408756		[learning rate: 0.00012487]
	Learning Rate: 0.000124871
	LOSS [training: 0.02693223317791906 | validation: 0.01961260529838232]
	TIME [epoch: 8.29 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022736391727902146		[learning rate: 0.00012472]
		[batch 20/20] avg loss: 0.02841217338101898		[learning rate: 0.00012458]
	Learning Rate: 0.000124576
	LOSS [training: 0.025574282554460565 | validation: 0.015109354673074173]
	TIME [epoch: 8.29 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018423004756325242		[learning rate: 0.00012443]
		[batch 20/20] avg loss: 0.026671689588065366		[learning rate: 0.00012428]
	Learning Rate: 0.000124283
	LOSS [training: 0.022547347172195302 | validation: 0.02391492552243887]
	TIME [epoch: 8.31 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02115661950206488		[learning rate: 0.00012414]
		[batch 20/20] avg loss: 0.02541325134474585		[learning rate: 0.00012399]
	Learning Rate: 0.000123989
	LOSS [training: 0.023284935423405363 | validation: 0.043513998886738806]
	TIME [epoch: 8.3 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03678163631946169		[learning rate: 0.00012384]
		[batch 20/20] avg loss: 0.021636234380956465		[learning rate: 0.0001237]
	Learning Rate: 0.000123697
	LOSS [training: 0.02920893535020907 | validation: 0.02804848084011058]
	TIME [epoch: 8.29 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036952257087600124		[learning rate: 0.00012355]
		[batch 20/20] avg loss: 0.029220239475064295		[learning rate: 0.00012341]
	Learning Rate: 0.000123405
	LOSS [training: 0.033086248281332206 | validation: 0.01393241709488062]
	TIME [epoch: 8.3 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03145033595784412		[learning rate: 0.00012326]
		[batch 20/20] avg loss: 0.020720396783171342		[learning rate: 0.00012311]
	Learning Rate: 0.000123114
	LOSS [training: 0.026085366370507727 | validation: 0.024533526949640384]
	TIME [epoch: 8.33 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021102535920700306		[learning rate: 0.00012297]
		[batch 20/20] avg loss: 0.019238449287717528		[learning rate: 0.00012282]
	Learning Rate: 0.000122824
	LOSS [training: 0.020170492604208924 | validation: 0.02253346277261389]
	TIME [epoch: 8.3 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020863192631440514		[learning rate: 0.00012268]
		[batch 20/20] avg loss: 0.0250258060039959		[learning rate: 0.00012253]
	Learning Rate: 0.000122534
	LOSS [training: 0.022944499317718203 | validation: 0.015350794159311808]
	TIME [epoch: 8.3 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01865307218592297		[learning rate: 0.00012239]
		[batch 20/20] avg loss: 0.02847656139744213		[learning rate: 0.00012224]
	Learning Rate: 0.000122245
	LOSS [training: 0.023564816791682548 | validation: 0.021793509743476953]
	TIME [epoch: 8.3 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02279493910812789		[learning rate: 0.0001221]
		[batch 20/20] avg loss: 0.02969544548561115		[learning rate: 0.00012196]
	Learning Rate: 0.000121957
	LOSS [training: 0.02624519229686952 | validation: 0.02395433982098258]
	TIME [epoch: 8.31 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02761844824793728		[learning rate: 0.00012181]
		[batch 20/20] avg loss: 0.021592568130178895		[learning rate: 0.00012167]
	Learning Rate: 0.000121669
	LOSS [training: 0.024605508189058083 | validation: 0.02010179846663169]
	TIME [epoch: 8.3 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027377778736937513		[learning rate: 0.00012153]
		[batch 20/20] avg loss: 0.024525311276694307		[learning rate: 0.00012138]
	Learning Rate: 0.000121382
	LOSS [training: 0.02595154500681591 | validation: 0.02005266452739248]
	TIME [epoch: 8.29 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024379364768201302		[learning rate: 0.00012124]
		[batch 20/20] avg loss: 0.01899490948362393		[learning rate: 0.0001211]
	Learning Rate: 0.000121096
	LOSS [training: 0.021687137125912616 | validation: 0.02085116980359783]
	TIME [epoch: 8.3 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0197213299391099		[learning rate: 0.00012095]
		[batch 20/20] avg loss: 0.02206618559409214		[learning rate: 0.00012081]
	Learning Rate: 0.00012081
	LOSS [training: 0.020893757766601025 | validation: 0.023374724463826088]
	TIME [epoch: 8.32 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016942010720648788		[learning rate: 0.00012067]
		[batch 20/20] avg loss: 0.024887861911354926		[learning rate: 0.00012052]
	Learning Rate: 0.000120525
	LOSS [training: 0.020914936316001855 | validation: 0.030295479829601284]
	TIME [epoch: 8.3 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03331192095357181		[learning rate: 0.00012038]
		[batch 20/20] avg loss: 0.019322596596086636		[learning rate: 0.00012024]
	Learning Rate: 0.000120241
	LOSS [training: 0.02631725877482922 | validation: 0.02567764273741046]
	TIME [epoch: 8.29 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027933037662439025		[learning rate: 0.0001201]
		[batch 20/20] avg loss: 0.02511535058538218		[learning rate: 0.00011996]
	Learning Rate: 0.000119957
	LOSS [training: 0.026524194123910598 | validation: 0.013630845020104133]
	TIME [epoch: 8.29 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019229410458196206		[learning rate: 0.00011982]
		[batch 20/20] avg loss: 0.021792833101947604		[learning rate: 0.00011967]
	Learning Rate: 0.000119674
	LOSS [training: 0.020511121780071903 | validation: 0.018094956243247362]
	TIME [epoch: 8.31 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01958701218322552		[learning rate: 0.00011953]
		[batch 20/20] avg loss: 0.023689493130847793		[learning rate: 0.00011939]
	Learning Rate: 0.000119392
	LOSS [training: 0.021638252657036654 | validation: 0.024322851921636638]
	TIME [epoch: 8.3 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022434768246005557		[learning rate: 0.00011925]
		[batch 20/20] avg loss: 0.02767039043959682		[learning rate: 0.00011911]
	Learning Rate: 0.00011911
	LOSS [training: 0.025052579342801185 | validation: 0.028275120084631938]
	TIME [epoch: 8.29 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023404171245692586		[learning rate: 0.00011897]
		[batch 20/20] avg loss: 0.016904813176593307		[learning rate: 0.00011883]
	Learning Rate: 0.000118829
	LOSS [training: 0.02015449221114295 | validation: 0.013584020302422721]
	TIME [epoch: 8.3 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03030073542980511		[learning rate: 0.00011869]
		[batch 20/20] avg loss: 0.022169498742518074		[learning rate: 0.00011855]
	Learning Rate: 0.000118549
	LOSS [training: 0.026235117086161587 | validation: 0.02116614449463227]
	TIME [epoch: 8.32 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020444963976430003		[learning rate: 0.00011841]
		[batch 20/20] avg loss: 0.024901952289241268		[learning rate: 0.00011827]
	Learning Rate: 0.000118269
	LOSS [training: 0.022673458132835632 | validation: 0.012202225640612208]
	TIME [epoch: 8.3 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02445280070389742		[learning rate: 0.00011813]
		[batch 20/20] avg loss: 0.02592439577849991		[learning rate: 0.00011799]
	Learning Rate: 0.00011799
	LOSS [training: 0.02518859824119866 | validation: 0.02108156600218776]
	TIME [epoch: 8.29 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021878042328957677		[learning rate: 0.00011785]
		[batch 20/20] avg loss: 0.025580292165217188		[learning rate: 0.00011771]
	Learning Rate: 0.000117712
	LOSS [training: 0.023729167247087436 | validation: 0.014988316367488983]
	TIME [epoch: 8.29 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017559262824619233		[learning rate: 0.00011757]
		[batch 20/20] avg loss: 0.02340197340357683		[learning rate: 0.00011743]
	Learning Rate: 0.000117434
	LOSS [training: 0.02048061811409803 | validation: 0.012386398127955727]
	TIME [epoch: 8.32 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026988618452441958		[learning rate: 0.0001173]
		[batch 20/20] avg loss: 0.01671899761065467		[learning rate: 0.00011716]
	Learning Rate: 0.000117157
	LOSS [training: 0.021853808031548308 | validation: 0.019810864236033884]
	TIME [epoch: 8.3 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01975190440463254		[learning rate: 0.00011702]
		[batch 20/20] avg loss: 0.027219016788210808		[learning rate: 0.00011688]
	Learning Rate: 0.000116881
	LOSS [training: 0.02348546059642167 | validation: 0.027337533321505077]
	TIME [epoch: 8.3 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029673442387552616		[learning rate: 0.00011674]
		[batch 20/20] avg loss: 0.019717375296225657		[learning rate: 0.00011661]
	Learning Rate: 0.000116605
	LOSS [training: 0.024695408841889137 | validation: 0.014012818086012624]
	TIME [epoch: 8.3 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018959782962136552		[learning rate: 0.00011647]
		[batch 20/20] avg loss: 0.030236293559012584		[learning rate: 0.00011633]
	Learning Rate: 0.00011633
	LOSS [training: 0.024598038260574565 | validation: 0.03184313465432763]
	TIME [epoch: 8.32 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030999879211521048		[learning rate: 0.00011619]
		[batch 20/20] avg loss: 0.0255021317229903		[learning rate: 0.00011606]
	Learning Rate: 0.000116056
	LOSS [training: 0.028251005467255667 | validation: 0.026270100108542343]
	TIME [epoch: 8.3 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023769933850988578		[learning rate: 0.00011592]
		[batch 20/20] avg loss: 0.02026158353503958		[learning rate: 0.00011578]
	Learning Rate: 0.000115782
	LOSS [training: 0.02201575869301408 | validation: 0.025881276165138113]
	TIME [epoch: 8.29 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02777822315130764		[learning rate: 0.00011565]
		[batch 20/20] avg loss: 0.022311115762484863		[learning rate: 0.00011551]
	Learning Rate: 0.000115509
	LOSS [training: 0.025044669456896252 | validation: 0.013869066579572749]
	TIME [epoch: 8.3 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01955094096798652		[learning rate: 0.00011537]
		[batch 20/20] avg loss: 0.031752601213744805		[learning rate: 0.00011524]
	Learning Rate: 0.000115236
	LOSS [training: 0.025651771090865666 | validation: 0.02199130983327785]
	TIME [epoch: 8.31 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025690191696679892		[learning rate: 0.0001151]
		[batch 20/20] avg loss: 0.03078817638258568		[learning rate: 0.00011496]
	Learning Rate: 0.000114965
	LOSS [training: 0.02823918403963278 | validation: 0.027579010800586584]
	TIME [epoch: 8.29 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02206407794127509		[learning rate: 0.00011483]
		[batch 20/20] avg loss: 0.028957143226875582		[learning rate: 0.00011469]
	Learning Rate: 0.000114693
	LOSS [training: 0.02551061058407534 | validation: 0.019774688158498154]
	TIME [epoch: 8.29 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023911754241140178		[learning rate: 0.00011456]
		[batch 20/20] avg loss: 0.021153858412909957		[learning rate: 0.00011442]
	Learning Rate: 0.000114423
	LOSS [training: 0.022532806327025062 | validation: 0.01949305791698221]
	TIME [epoch: 8.3 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028757577367860547		[learning rate: 0.00011429]
		[batch 20/20] avg loss: 0.02310573252823992		[learning rate: 0.00011415]
	Learning Rate: 0.000114153
	LOSS [training: 0.025931654948050237 | validation: 0.011129350207629847]
	TIME [epoch: 8.32 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020450960654187045		[learning rate: 0.00011402]
		[batch 20/20] avg loss: 0.023271706610723687		[learning rate: 0.00011388]
	Learning Rate: 0.000113884
	LOSS [training: 0.021861333632455363 | validation: 0.01569354655316797]
	TIME [epoch: 8.3 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01828935267381006		[learning rate: 0.00011375]
		[batch 20/20] avg loss: 0.02528375554770772		[learning rate: 0.00011362]
	Learning Rate: 0.000113615
	LOSS [training: 0.02178655411075889 | validation: 0.018851054931337884]
	TIME [epoch: 8.3 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01964941901153796		[learning rate: 0.00011348]
		[batch 20/20] avg loss: 0.024263104579093118		[learning rate: 0.00011335]
	Learning Rate: 0.000113347
	LOSS [training: 0.021956261795315537 | validation: 0.019526384563617837]
	TIME [epoch: 8.29 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0268781043746802		[learning rate: 0.00011321]
		[batch 20/20] avg loss: 0.0177764683234866		[learning rate: 0.00011308]
	Learning Rate: 0.00011308
	LOSS [training: 0.0223272863490834 | validation: 0.009589789165274303]
	TIME [epoch: 8.32 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025372769088840442		[learning rate: 0.00011295]
		[batch 20/20] avg loss: 0.025753854138390674		[learning rate: 0.00011281]
	Learning Rate: 0.000112813
	LOSS [training: 0.02556331161361556 | validation: 0.02657229593606731]
	TIME [epoch: 8.29 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0208688189861716		[learning rate: 0.00011268]
		[batch 20/20] avg loss: 0.024265315732027337		[learning rate: 0.00011255]
	Learning Rate: 0.000112547
	LOSS [training: 0.022567067359099465 | validation: 0.016544227402208664]
	TIME [epoch: 8.3 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020684252704566274		[learning rate: 0.00011241]
		[batch 20/20] avg loss: 0.026243376768650573		[learning rate: 0.00011228]
	Learning Rate: 0.000112281
	LOSS [training: 0.023463814736608427 | validation: 0.025979446495074696]
	TIME [epoch: 8.3 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017200840662139734		[learning rate: 0.00011215]
		[batch 20/20] avg loss: 0.022613184195194536		[learning rate: 0.00011202]
	Learning Rate: 0.000112017
	LOSS [training: 0.019907012428667138 | validation: 0.017631842279732336]
	TIME [epoch: 8.32 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023015762641705066		[learning rate: 0.00011188]
		[batch 20/20] avg loss: 0.023621673181618917		[learning rate: 0.00011175]
	Learning Rate: 0.000111752
	LOSS [training: 0.023318717911661995 | validation: 0.02088723362168675]
	TIME [epoch: 8.3 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021942251097426306		[learning rate: 0.00011162]
		[batch 20/20] avg loss: 0.02300210703735752		[learning rate: 0.00011149]
	Learning Rate: 0.000111489
	LOSS [training: 0.022472179067391913 | validation: 0.014145706821212726]
	TIME [epoch: 8.29 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020313123533152382		[learning rate: 0.00011136]
		[batch 20/20] avg loss: 0.022156304848621033		[learning rate: 0.00011123]
	Learning Rate: 0.000111226
	LOSS [training: 0.02123471419088671 | validation: 0.017950556807937314]
	TIME [epoch: 8.29 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027444702251554386		[learning rate: 0.00011109]
		[batch 20/20] avg loss: 0.023813810959495908		[learning rate: 0.00011096]
	Learning Rate: 0.000110963
	LOSS [training: 0.025629256605525147 | validation: 0.023034166535535028]
	TIME [epoch: 8.32 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023490395173926107		[learning rate: 0.00011083]
		[batch 20/20] avg loss: 0.026914665093860073		[learning rate: 0.0001107]
	Learning Rate: 0.000110702
	LOSS [training: 0.025202530133893092 | validation: 0.016125927993771962]
	TIME [epoch: 8.29 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014173458616996013		[learning rate: 0.00011057]
		[batch 20/20] avg loss: 0.025145756673819693		[learning rate: 0.00011044]
	Learning Rate: 0.00011044
	LOSS [training: 0.019659607645407855 | validation: 0.017223732467500016]
	TIME [epoch: 8.3 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028859326779120877		[learning rate: 0.00011031]
		[batch 20/20] avg loss: 0.023017114565636355		[learning rate: 0.00011018]
	Learning Rate: 0.00011018
	LOSS [training: 0.025938220672378616 | validation: 0.015275159855430476]
	TIME [epoch: 8.3 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019779643907045848		[learning rate: 0.00011005]
		[batch 20/20] avg loss: 0.023354987827379887		[learning rate: 0.00010992]
	Learning Rate: 0.00010992
	LOSS [training: 0.021567315867212864 | validation: 0.026002513434367804]
	TIME [epoch: 8.32 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016788830272599094		[learning rate: 0.00010979]
		[batch 20/20] avg loss: 0.024934372794804927		[learning rate: 0.00010966]
	Learning Rate: 0.000109661
	LOSS [training: 0.020861601533702014 | validation: 0.021465145083013264]
	TIME [epoch: 8.3 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019635715177206685		[learning rate: 0.00010953]
		[batch 20/20] avg loss: 0.02785625411722641		[learning rate: 0.0001094]
	Learning Rate: 0.000109402
	LOSS [training: 0.02374598464721655 | validation: 0.017639605247612735]
	TIME [epoch: 8.3 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02274039957477858		[learning rate: 0.00010927]
		[batch 20/20] avg loss: 0.018505113289301045		[learning rate: 0.00010914]
	Learning Rate: 0.000109144
	LOSS [training: 0.020622756432039813 | validation: 0.019155360289111995]
	TIME [epoch: 8.3 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027525041404578933		[learning rate: 0.00010902]
		[batch 20/20] avg loss: 0.0219581904187056		[learning rate: 0.00010889]
	Learning Rate: 0.000108887
	LOSS [training: 0.02474161591164227 | validation: 0.015293959845434593]
	TIME [epoch: 8.32 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024528718305583213		[learning rate: 0.00010876]
		[batch 20/20] avg loss: 0.01700356132830233		[learning rate: 0.00010863]
	Learning Rate: 0.00010863
	LOSS [training: 0.020766139816942773 | validation: 0.014221627231279597]
	TIME [epoch: 8.3 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02520346687175732		[learning rate: 0.0001085]
		[batch 20/20] avg loss: 0.020035880859747293		[learning rate: 0.00010837]
	Learning Rate: 0.000108373
	LOSS [training: 0.022619673865752303 | validation: 0.016869110127531346]
	TIME [epoch: 8.3 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023831507241576084		[learning rate: 0.00010825]
		[batch 20/20] avg loss: 0.027572370824442517		[learning rate: 0.00010812]
	Learning Rate: 0.000108118
	LOSS [training: 0.025701939033009304 | validation: 0.015768449472243592]
	TIME [epoch: 8.3 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024371376843428744		[learning rate: 0.00010799]
		[batch 20/20] avg loss: 0.02716215371755682		[learning rate: 0.00010786]
	Learning Rate: 0.000107863
	LOSS [training: 0.025766765280492782 | validation: 0.019731648726136114]
	TIME [epoch: 8.31 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02892846426627036		[learning rate: 0.00010774]
		[batch 20/20] avg loss: 0.023018945675556915		[learning rate: 0.00010761]
	Learning Rate: 0.000107608
	LOSS [training: 0.025973704970913634 | validation: 0.024954188725712288]
	TIME [epoch: 8.28 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015593657685962149		[learning rate: 0.00010748]
		[batch 20/20] avg loss: 0.03055132932625605		[learning rate: 0.00010735]
	Learning Rate: 0.000107355
	LOSS [training: 0.0230724935061091 | validation: 0.01856424888290466]
	TIME [epoch: 8.28 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026440178485881472		[learning rate: 0.00010723]
		[batch 20/20] avg loss: 0.022362143047309206		[learning rate: 0.0001071]
	Learning Rate: 0.000107101
	LOSS [training: 0.024401160766595337 | validation: 0.014293874932405796]
	TIME [epoch: 8.29 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02188905870404006		[learning rate: 0.00010697]
		[batch 20/20] avg loss: 0.01875317908588263		[learning rate: 0.00010685]
	Learning Rate: 0.000106849
	LOSS [training: 0.02032111889496134 | validation: 0.009626992368926541]
	TIME [epoch: 8.36 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026182555639144223		[learning rate: 0.00010672]
		[batch 20/20] avg loss: 0.022185056482876653		[learning rate: 0.0001066]
	Learning Rate: 0.000106597
	LOSS [training: 0.024183806061010442 | validation: 0.014585701168692544]
	TIME [epoch: 8.34 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018411656251347754		[learning rate: 0.00010647]
		[batch 20/20] avg loss: 0.019645988328787313		[learning rate: 0.00010635]
	Learning Rate: 0.000106345
	LOSS [training: 0.01902882229006753 | validation: 0.017374601267453535]
	TIME [epoch: 8.34 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023264205412162398		[learning rate: 0.00010622]
		[batch 20/20] avg loss: 0.033729836414751094		[learning rate: 0.00010609]
	Learning Rate: 0.000106094
	LOSS [training: 0.028497020913456746 | validation: 0.016573100241544184]
	TIME [epoch: 8.34 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024263598957883766		[learning rate: 0.00010597]
		[batch 20/20] avg loss: 0.023735195934964298		[learning rate: 0.00010584]
	Learning Rate: 0.000105844
	LOSS [training: 0.023999397446424035 | validation: 0.01977950100727848]
	TIME [epoch: 8.36 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022909723261075585		[learning rate: 0.00010572]
		[batch 20/20] avg loss: 0.01959240385358552		[learning rate: 0.00010559]
	Learning Rate: 0.000105594
	LOSS [training: 0.021251063557330557 | validation: 0.021672807955472363]
	TIME [epoch: 8.34 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020733158434308018		[learning rate: 0.00010547]
		[batch 20/20] avg loss: 0.01900859229126568		[learning rate: 0.00010535]
	Learning Rate: 0.000105345
	LOSS [training: 0.019870875362786852 | validation: 0.013217129352028185]
	TIME [epoch: 8.34 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018162942845689586		[learning rate: 0.00010522]
		[batch 20/20] avg loss: 0.01872739500626961		[learning rate: 0.0001051]
	Learning Rate: 0.000105097
	LOSS [training: 0.0184451689259796 | validation: 0.01777399289585497]
	TIME [epoch: 8.32 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019346635377263006		[learning rate: 0.00010497]
		[batch 20/20] avg loss: 0.022158560409507918		[learning rate: 0.00010485]
	Learning Rate: 0.000104849
	LOSS [training: 0.020752597893385464 | validation: 0.01742067862158664]
	TIME [epoch: 8.36 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02292724879872173		[learning rate: 0.00010473]
		[batch 20/20] avg loss: 0.029292465608577196		[learning rate: 0.0001046]
	Learning Rate: 0.000104602
	LOSS [training: 0.026109857203649455 | validation: 0.022417310833235903]
	TIME [epoch: 8.34 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02077211209789371		[learning rate: 0.00010448]
		[batch 20/20] avg loss: 0.02680702427293712		[learning rate: 0.00010435]
	Learning Rate: 0.000104355
	LOSS [training: 0.02378956818541541 | validation: 0.015588795206133026]
	TIME [epoch: 8.34 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018776128007366544		[learning rate: 0.00010423]
		[batch 20/20] avg loss: 0.023450817590566476		[learning rate: 0.00010411]
	Learning Rate: 0.000104109
	LOSS [training: 0.021113472798966512 | validation: 0.020613503033106113]
	TIME [epoch: 8.35 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020828622217141338		[learning rate: 0.00010399]
		[batch 20/20] avg loss: 0.02259857611363584		[learning rate: 0.00010386]
	Learning Rate: 0.000103863
	LOSS [training: 0.02171359916538859 | validation: 0.02103222609213786]
	TIME [epoch: 8.36 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021512880059301454		[learning rate: 0.00010374]
		[batch 20/20] avg loss: 0.024342794413003292		[learning rate: 0.00010362]
	Learning Rate: 0.000103618
	LOSS [training: 0.022927837236152375 | validation: 0.028899602866174736]
	TIME [epoch: 8.34 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038715043306125886		[learning rate: 0.0001035]
		[batch 20/20] avg loss: 0.025320555202128718		[learning rate: 0.00010337]
	Learning Rate: 0.000103374
	LOSS [training: 0.0320177992541273 | validation: 0.01345534367107337]
	TIME [epoch: 8.35 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022709601334974006		[learning rate: 0.00010325]
		[batch 20/20] avg loss: 0.02131457885403961		[learning rate: 0.00010313]
	Learning Rate: 0.00010313
	LOSS [training: 0.02201209009450681 | validation: 0.012479683202477578]
	TIME [epoch: 8.34 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016842082997403383		[learning rate: 0.00010301]
		[batch 20/20] avg loss: 0.02170132079472267		[learning rate: 0.00010289]
	Learning Rate: 0.000102887
	LOSS [training: 0.019271701896063028 | validation: 0.01787210011401887]
	TIME [epoch: 8.36 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01911827382086042		[learning rate: 0.00010277]
		[batch 20/20] avg loss: 0.02697651660285864		[learning rate: 0.00010264]
	Learning Rate: 0.000102644
	LOSS [training: 0.023047395211859535 | validation: 0.010503099410138755]
	TIME [epoch: 8.34 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020975315515708776		[learning rate: 0.00010252]
		[batch 20/20] avg loss: 0.021959528689174383		[learning rate: 0.0001024]
	Learning Rate: 0.000102402
	LOSS [training: 0.021467422102441576 | validation: 0.012206923206280898]
	TIME [epoch: 8.34 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020573698669234208		[learning rate: 0.00010228]
		[batch 20/20] avg loss: 0.029265645286596788		[learning rate: 0.00010216]
	Learning Rate: 0.00010216
	LOSS [training: 0.024919671977915496 | validation: 0.013483969862512643]
	TIME [epoch: 8.35 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023072505808383697		[learning rate: 0.00010204]
		[batch 20/20] avg loss: 0.02880494132490575		[learning rate: 0.00010192]
	Learning Rate: 0.000101919
	LOSS [training: 0.025938723566644718 | validation: 0.031892380975006865]
	TIME [epoch: 8.37 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02536468593024726		[learning rate: 0.0001018]
		[batch 20/20] avg loss: 0.016225913478237413		[learning rate: 0.00010168]
	Learning Rate: 0.000101679
	LOSS [training: 0.020795299704242338 | validation: 0.018852877840449627]
	TIME [epoch: 8.34 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020143082703574018		[learning rate: 0.00010156]
		[batch 20/20] avg loss: 0.023291785469300403		[learning rate: 0.00010144]
	Learning Rate: 0.000101439
	LOSS [training: 0.02171743408643721 | validation: 0.02210017258298662]
	TIME [epoch: 8.34 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015970060496434724		[learning rate: 0.00010132]
		[batch 20/20] avg loss: 0.021640070650554066		[learning rate: 0.0001012]
	Learning Rate: 0.0001012
	LOSS [training: 0.018805065573494394 | validation: 0.01894330937386586]
	TIME [epoch: 8.34 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022446484685805252		[learning rate: 0.00010108]
		[batch 20/20] avg loss: 0.02514326387206981		[learning rate: 0.00010096]
	Learning Rate: 0.000100961
	LOSS [training: 0.023794874278937526 | validation: 0.017497327698789738]
	TIME [epoch: 8.36 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023932651377701605		[learning rate: 0.00010084]
		[batch 20/20] avg loss: 0.020436037400285167		[learning rate: 0.00010072]
	Learning Rate: 0.000100723
	LOSS [training: 0.02218434438899339 | validation: 0.022749966697464832]
	TIME [epoch: 8.34 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026387835932206883		[learning rate: 0.0001006]
		[batch 20/20] avg loss: 0.01836677753719499		[learning rate: 0.00010049]
	Learning Rate: 0.000100485
	LOSS [training: 0.022377306734700935 | validation: 0.017329973843464585]
	TIME [epoch: 8.34 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02685132615950455		[learning rate: 0.00010037]
		[batch 20/20] avg loss: 0.02563182052254497		[learning rate: 0.00010025]
	Learning Rate: 0.000100248
	LOSS [training: 0.026241573341024765 | validation: 0.010826065417339348]
	TIME [epoch: 8.35 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02661387305644251		[learning rate: 0.00010013]
		[batch 20/20] avg loss: 0.019638231271764568		[learning rate: 0.00010001]
	Learning Rate: 0.000100012
	LOSS [training: 0.023126052164103537 | validation: 0.014011141767312028]
	TIME [epoch: 8.36 sec]
Finished training in 16775.471 seconds.
