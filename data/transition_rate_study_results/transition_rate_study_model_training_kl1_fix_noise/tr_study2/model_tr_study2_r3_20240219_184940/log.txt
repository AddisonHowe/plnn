Args:
Namespace(name='model_tr_study2', outdir='out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3', training_data='data/transition_rate_studies/tr_study2/tr_study2_training/r3', validation_data='data/transition_rate_studies/tr_study2/tr_study2_validation/r3', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.1, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 981743156

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.264450573193336		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.980181844375839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.6223162087845875 | validation: 7.019781179009576]
	TIME [epoch: 79.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.886892137496847		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.106860116628219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.496876127062532 | validation: 4.710601883408352]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.299420902122113		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8327072105171567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.066064056319635 | validation: 4.947344581768064]
	TIME [epoch: 8.23 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.558393845757635		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.72164862506981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.140021235413723 | validation: 2.579305931912274]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.358721577139792		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8729774136075175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.115849495373655 | validation: 1.5989831985588552]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6842855393579377		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6296095016509882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6569475205044626 | validation: 1.3441961256275141]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4565643068181786		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6095723648034421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5330683358108104 | validation: 1.3608291278004359]
	TIME [epoch: 8.21 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8305241328643045		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.627672872487961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7290985026761327 | validation: 1.2587374147382258]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2804880607302918		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9990952284539285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.13979164459211 | validation: 0.6122654523006854]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0187331480275463		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.902550434587502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9606417913075243 | validation: 1.396620182651354]
	TIME [epoch: 8.21 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8287283439671562		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8256135229431079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8271709334551322 | validation: 1.315145860662429]
	TIME [epoch: 8.24 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1083155974096486		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.000355673849616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.054335635629632 | validation: 0.500421791567434]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.67246929279635		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7578174515318581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7151433721641042 | validation: 0.37136265640508576]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6044873029308241		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6947667130821388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6496270080064813 | validation: 0.8165420086895461]
	TIME [epoch: 8.2 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6392003538680957		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7740345178494997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7066174358587978 | validation: 0.5402406632881498]
	TIME [epoch: 8.23 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7035740015561682		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6796872193222204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6916306104391943 | validation: 0.3735346431984117]
	TIME [epoch: 8.24 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5907655956736815		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7799081511193513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6853368733965164 | validation: 0.4057259704921572]
	TIME [epoch: 8.21 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6134874149816577		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7445155049970853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6790014599893714 | validation: 0.7150301167083694]
	TIME [epoch: 8.2 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7762567027470035		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6124388780813574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6943477904141805 | validation: 0.46055401083549496]
	TIME [epoch: 8.23 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5568468086550886		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8071332652074945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6819900369312916 | validation: 0.5602923441156544]
	TIME [epoch: 8.24 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7166938215991718		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6826463714985618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6996700965488668 | validation: 0.5681550868984934]
	TIME [epoch: 8.2 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6500239177480209		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5553562384471091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6026900780975651 | validation: 0.4078252852778336]
	TIME [epoch: 8.2 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6497009158273238		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5529094511011088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6013051834642164 | validation: 0.7456865581765184]
	TIME [epoch: 8.22 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5602275921420181		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5883390724760375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5742833323090278 | validation: 0.40823176005384054]
	TIME [epoch: 8.25 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5306212515170741		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6767203937901891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6036708226536318 | validation: 0.7642064952520349]
	TIME [epoch: 8.21 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5488186445255808		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7108349793560081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6298268119407946 | validation: 0.5549444538441348]
	TIME [epoch: 8.21 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6531489279995373		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5936828652358109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6234158966176742 | validation: 0.48161846875638736]
	TIME [epoch: 8.22 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5637087517247125		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6529556881215641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6083322199231382 | validation: 0.6527311046911448]
	TIME [epoch: 8.24 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6115345166655025		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7487996728773869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6801670947714447 | validation: 0.5588770158905669]
	TIME [epoch: 8.2 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5269964005996918		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5801790147516945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5535877076756932 | validation: 0.38105445915564595]
	TIME [epoch: 8.2 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5758481191196182		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7038678339483037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6398579765339609 | validation: 0.5252916424318343]
	TIME [epoch: 8.21 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5459530400069876		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6192776818429451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5826153609249662 | validation: 0.6270617663383088]
	TIME [epoch: 8.24 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6399580908393675		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6492930570703865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.644625573954877 | validation: 0.6823448486844601]
	TIME [epoch: 8.2 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.626750804472197		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5584799592597187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5926153818659579 | validation: 0.4709975065673296]
	TIME [epoch: 8.19 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5766681049893959		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5820940428919122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5793810739406541 | validation: 0.33233073085532905]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5360112431560016		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6581438810786812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5970775621173414 | validation: 0.8345296573669262]
	TIME [epoch: 8.24 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6467517959023794		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7632809664621083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7050163811822439 | validation: 0.7739547051987504]
	TIME [epoch: 8.21 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6330960313989327		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5311340710549798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5821150512269562 | validation: 0.37161817165232613]
	TIME [epoch: 8.19 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5906510016715505		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5642768144361685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5774639080538594 | validation: 0.6300571902841832]
	TIME [epoch: 8.21 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5000246546904817		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6035962665468234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5518104606186526 | validation: 0.3260040303574497]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6264135651614715		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5316680724105071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5790408187859892 | validation: 1.1085289604731472]
	TIME [epoch: 8.21 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6881891086188807		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5074641289199232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.597826618769402 | validation: 1.2414005436954103]
	TIME [epoch: 8.19 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6624753570388806		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.563750183641561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6131127703402208 | validation: 0.8740933095164076]
	TIME [epoch: 8.21 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6829941262157313		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5712810818923053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6271376040540183 | validation: 0.4866872818806932]
	TIME [epoch: 8.23 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5312335235850414		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.570850691429116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5510421075070787 | validation: 0.5673933826388582]
	TIME [epoch: 8.21 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5760528288151437		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7129749262794863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.644513877547315 | validation: 0.42283937803319105]
	TIME [epoch: 8.2 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5302784565293732		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5065757707471295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5184271136382512 | validation: 0.486802441979287]
	TIME [epoch: 8.22 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6325979399222845		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5332384459508811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5829181929365828 | validation: 0.447311029526053]
	TIME [epoch: 8.22 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49262063949057017		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5228976934885675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5077591664895689 | validation: 0.3623748821285928]
	TIME [epoch: 8.2 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8017237937282757		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6310486869189282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7163862403236021 | validation: 0.3506868292278202]
	TIME [epoch: 8.19 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5132236656885594		[learning rate: 0.0099894]
		[batch 20/20] avg loss: 0.5020305728538234		[learning rate: 0.0099776]
	Learning Rate: 0.00997759
	LOSS [training: 0.5076271192711913 | validation: 0.5301785703450964]
	TIME [epoch: 8.2 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.525488533501436		[learning rate: 0.0099658]
		[batch 20/20] avg loss: 0.5412829601137806		[learning rate: 0.0099541]
	Learning Rate: 0.00995405
	LOSS [training: 0.5333857468076084 | validation: 0.47033915159907974]
	TIME [epoch: 8.23 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5354842982975971		[learning rate: 0.0099423]
		[batch 20/20] avg loss: 0.5095248882252548		[learning rate: 0.0099306]
	Learning Rate: 0.00993057
	LOSS [training: 0.522504593261426 | validation: 0.5563590609690774]
	TIME [epoch: 8.2 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4515974551861193		[learning rate: 0.0099189]
		[batch 20/20] avg loss: 0.5622527686906149		[learning rate: 0.0099071]
	Learning Rate: 0.00990715
	LOSS [training: 0.5069251119383673 | validation: 0.33531824271607763]
	TIME [epoch: 8.2 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5355235258989031		[learning rate: 0.0098955]
		[batch 20/20] avg loss: 0.5443261283223462		[learning rate: 0.0098838]
	Learning Rate: 0.00988378
	LOSS [training: 0.5399248271106247 | validation: 0.7250236729227124]
	TIME [epoch: 8.21 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.596936509132613		[learning rate: 0.0098721]
		[batch 20/20] avg loss: 0.4372830950682326		[learning rate: 0.0098605]
	Learning Rate: 0.00986047
	LOSS [training: 0.5171098021004228 | validation: 0.42443941358302334]
	TIME [epoch: 8.23 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6477187125345877		[learning rate: 0.0098488]
		[batch 20/20] avg loss: 0.5338921072450298		[learning rate: 0.0098372]
	Learning Rate: 0.00983721
	LOSS [training: 0.5908054098898087 | validation: 1.0204925610762527]
	TIME [epoch: 8.21 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7121676563563766		[learning rate: 0.0098256]
		[batch 20/20] avg loss: 0.4542301840769893		[learning rate: 0.009814]
	Learning Rate: 0.009814
	LOSS [training: 0.583198920216683 | validation: 0.5815975575269534]
	TIME [epoch: 8.19 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5006244497090793		[learning rate: 0.0098024]
		[batch 20/20] avg loss: 0.4321905007446373		[learning rate: 0.0097909]
	Learning Rate: 0.00979085
	LOSS [training: 0.4664074752268584 | validation: 0.49726342123374856]
	TIME [epoch: 8.22 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.590510026136629		[learning rate: 0.0097793]
		[batch 20/20] avg loss: 0.5328568984514884		[learning rate: 0.0097678]
	Learning Rate: 0.00976776
	LOSS [training: 0.5616834622940587 | validation: 0.3021709793779821]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5018717764252829		[learning rate: 0.0097562]
		[batch 20/20] avg loss: 0.47344171139653535		[learning rate: 0.0097447]
	Learning Rate: 0.00974472
	LOSS [training: 0.4876567439109093 | validation: 0.311831195482134]
	TIME [epoch: 8.21 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.525967141284394		[learning rate: 0.0097332]
		[batch 20/20] avg loss: 0.6170685817439655		[learning rate: 0.0097217]
	Learning Rate: 0.00972173
	LOSS [training: 0.5715178615141798 | validation: 0.43657035303334646]
	TIME [epoch: 8.2 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.480889801956466		[learning rate: 0.0097103]
		[batch 20/20] avg loss: 0.5305115311700314		[learning rate: 0.0096988]
	Learning Rate: 0.0096988
	LOSS [training: 0.5057006665632487 | validation: 0.3448842279247737]
	TIME [epoch: 8.21 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5309136161147432		[learning rate: 0.0096874]
		[batch 20/20] avg loss: 0.44181734635568637		[learning rate: 0.0096759]
	Learning Rate: 0.00967592
	LOSS [training: 0.48636548123521467 | validation: 0.4478001632684643]
	TIME [epoch: 8.23 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4474432051114342		[learning rate: 0.0096645]
		[batch 20/20] avg loss: 0.5364600515851927		[learning rate: 0.0096531]
	Learning Rate: 0.0096531
	LOSS [training: 0.49195162834831346 | validation: 0.4389625961252385]
	TIME [epoch: 8.21 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4826152520217518		[learning rate: 0.0096417]
		[batch 20/20] avg loss: 0.4871449686116982		[learning rate: 0.0096303]
	Learning Rate: 0.00963033
	LOSS [training: 0.48488011031672495 | validation: 0.33317267554350816]
	TIME [epoch: 8.2 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.486635187650438		[learning rate: 0.009619]
		[batch 20/20] avg loss: 0.5409507062618301		[learning rate: 0.0096076]
	Learning Rate: 0.00960761
	LOSS [training: 0.513792946956134 | validation: 0.5246524989176143]
	TIME [epoch: 8.21 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6187722267862519		[learning rate: 0.0095963]
		[batch 20/20] avg loss: 0.48095541495515864		[learning rate: 0.0095849]
	Learning Rate: 0.00958495
	LOSS [training: 0.5498638208707052 | validation: 0.6684853621394957]
	TIME [epoch: 8.24 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5037089044879343		[learning rate: 0.0095736]
		[batch 20/20] avg loss: 0.42676190262885544		[learning rate: 0.0095623]
	Learning Rate: 0.00956234
	LOSS [training: 0.4652354035583948 | validation: 0.3128980579482075]
	TIME [epoch: 8.2 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5206839709174071		[learning rate: 0.0095511]
		[batch 20/20] avg loss: 0.4217784895783529		[learning rate: 0.0095398]
	Learning Rate: 0.00953978
	LOSS [training: 0.47123123024787994 | validation: 0.4550067248601988]
	TIME [epoch: 8.2 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48992866483632386		[learning rate: 0.0095285]
		[batch 20/20] avg loss: 0.46750518101394495		[learning rate: 0.0095173]
	Learning Rate: 0.00951728
	LOSS [training: 0.47871692292513435 | validation: 0.5824368980752097]
	TIME [epoch: 8.22 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49062279637394557		[learning rate: 0.009506]
		[batch 20/20] avg loss: 0.4409481176789461		[learning rate: 0.0094948]
	Learning Rate: 0.00949483
	LOSS [training: 0.4657854570264458 | validation: 0.2595901535581082]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3911701388329395		[learning rate: 0.0094836]
		[batch 20/20] avg loss: 0.43844786420819937		[learning rate: 0.0094724]
	Learning Rate: 0.00947243
	LOSS [training: 0.41480900152056943 | validation: 0.2878952827622856]
	TIME [epoch: 8.22 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47093690787593057		[learning rate: 0.0094613]
		[batch 20/20] avg loss: 0.4441263479832728		[learning rate: 0.0094501]
	Learning Rate: 0.00945009
	LOSS [training: 0.45753162792960167 | validation: 0.30935516139586666]
	TIME [epoch: 8.19 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40263099844896805		[learning rate: 0.0094389]
		[batch 20/20] avg loss: 0.37399701454235956		[learning rate: 0.0094278]
	Learning Rate: 0.0094278
	LOSS [training: 0.3883140064956638 | validation: 0.8461154846000799]
	TIME [epoch: 8.22 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.481400757571785		[learning rate: 0.0094167]
		[batch 20/20] avg loss: 0.4497362588158841		[learning rate: 0.0094056]
	Learning Rate: 0.00940556
	LOSS [training: 0.46556850819383444 | validation: 0.27637114995663353]
	TIME [epoch: 8.2 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39983769044265843		[learning rate: 0.0093945]
		[batch 20/20] avg loss: 0.41373031930357795		[learning rate: 0.0093834]
	Learning Rate: 0.00938337
	LOSS [training: 0.4067840048731182 | validation: 0.5459255270280495]
	TIME [epoch: 8.22 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5043225674620679		[learning rate: 0.0093723]
		[batch 20/20] avg loss: 0.4060638698282892		[learning rate: 0.0093612]
	Learning Rate: 0.00936124
	LOSS [training: 0.45519321864517853 | validation: 0.36786387225171063]
	TIME [epoch: 8.19 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5700532609694859		[learning rate: 0.0093502]
		[batch 20/20] avg loss: 0.46228794461420436		[learning rate: 0.0093392]
	Learning Rate: 0.00933916
	LOSS [training: 0.5161706027918452 | validation: 0.5027930964478605]
	TIME [epoch: 8.23 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4856224181014522		[learning rate: 0.0093281]
		[batch 20/20] avg loss: 0.3971191504603853		[learning rate: 0.0093171]
	Learning Rate: 0.00931713
	LOSS [training: 0.4413707842809188 | validation: 0.5336518239006192]
	TIME [epoch: 8.21 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4062598841073357		[learning rate: 0.0093061]
		[batch 20/20] avg loss: 0.4146152346818651		[learning rate: 0.0092951]
	Learning Rate: 0.00929515
	LOSS [training: 0.41043755939460036 | validation: 0.4494771329717435]
	TIME [epoch: 8.22 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4766854332874198		[learning rate: 0.0092842]
		[batch 20/20] avg loss: 0.3857702362002287		[learning rate: 0.0092732]
	Learning Rate: 0.00927322
	LOSS [training: 0.4312278347438242 | validation: 0.6469012537304498]
	TIME [epoch: 8.2 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48415716103935613		[learning rate: 0.0092623]
		[batch 20/20] avg loss: 0.4077766589178088		[learning rate: 0.0092514]
	Learning Rate: 0.00925135
	LOSS [training: 0.4459669099785824 | validation: 0.3089942427557295]
	TIME [epoch: 8.22 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4108957527876905		[learning rate: 0.0092404]
		[batch 20/20] avg loss: 0.5541203306208311		[learning rate: 0.0092295]
	Learning Rate: 0.00922953
	LOSS [training: 0.4825080417042608 | validation: 0.2622624742855949]
	TIME [epoch: 8.2 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37730945769708396		[learning rate: 0.0092186]
		[batch 20/20] avg loss: 0.345611279030035		[learning rate: 0.0092078]
	Learning Rate: 0.00920776
	LOSS [training: 0.36146036836355955 | validation: 0.3944423794348024]
	TIME [epoch: 8.23 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6345876933819865		[learning rate: 0.0091969]
		[batch 20/20] avg loss: 0.3607342062155473		[learning rate: 0.009186]
	Learning Rate: 0.00918604
	LOSS [training: 0.49766094979876685 | validation: 0.19551729411445032]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4594198490638637		[learning rate: 0.0091752]
		[batch 20/20] avg loss: 0.3528838695330463		[learning rate: 0.0091644]
	Learning Rate: 0.00916437
	LOSS [training: 0.406151859298455 | validation: 0.4106828674654526]
	TIME [epoch: 8.22 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3971474110432291		[learning rate: 0.0091536]
		[batch 20/20] avg loss: 0.38863090650294935		[learning rate: 0.0091428]
	Learning Rate: 0.00914275
	LOSS [training: 0.39288915877308916 | validation: 0.3388934660821771]
	TIME [epoch: 8.2 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4467541373222092		[learning rate: 0.009132]
		[batch 20/20] avg loss: 0.6119742673659805		[learning rate: 0.0091212]
	Learning Rate: 0.00912119
	LOSS [training: 0.5293642023440949 | validation: 0.7653911007889308]
	TIME [epoch: 8.21 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38976696415299783		[learning rate: 0.0091104]
		[batch 20/20] avg loss: 0.3941455433772895		[learning rate: 0.0090997]
	Learning Rate: 0.00909967
	LOSS [training: 0.39195625376514365 | validation: 0.4781349127224233]
	TIME [epoch: 8.19 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3688731045312406		[learning rate: 0.0090889]
		[batch 20/20] avg loss: 0.47280100366182704		[learning rate: 0.0090782]
	Learning Rate: 0.0090782
	LOSS [training: 0.42083705409653377 | validation: 0.6642895481192616]
	TIME [epoch: 8.23 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49478581354712825		[learning rate: 0.0090675]
		[batch 20/20] avg loss: 0.38032070767330733		[learning rate: 0.0090568]
	Learning Rate: 0.00905679
	LOSS [training: 0.4375532606102177 | validation: 0.33776834185140514]
	TIME [epoch: 8.2 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4154854369727673		[learning rate: 0.0090461]
		[batch 20/20] avg loss: 0.4166454538709172		[learning rate: 0.0090354]
	Learning Rate: 0.00903543
	LOSS [training: 0.4160654454218422 | validation: 0.5054336566516965]
	TIME [epoch: 8.22 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3717561634800057		[learning rate: 0.0090248]
		[batch 20/20] avg loss: 0.39567254741753277		[learning rate: 0.0090141]
	Learning Rate: 0.00901411
	LOSS [training: 0.38371435544876925 | validation: 0.30214989865055397]
	TIME [epoch: 8.2 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38857565299682056		[learning rate: 0.0090035]
		[batch 20/20] avg loss: 0.5328617676667269		[learning rate: 0.0089929]
	Learning Rate: 0.00899285
	LOSS [training: 0.4607187103317737 | validation: 0.5839817911159266]
	TIME [epoch: 8.23 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3544719234863322		[learning rate: 0.0089822]
		[batch 20/20] avg loss: 0.4963605666391565		[learning rate: 0.0089716]
	Learning Rate: 0.00897164
	LOSS [training: 0.42541624506274445 | validation: 0.2135110140126356]
	TIME [epoch: 8.2 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4331468239243231		[learning rate: 0.0089611]
		[batch 20/20] avg loss: 0.35453677148197726		[learning rate: 0.0089505]
	Learning Rate: 0.00895048
	LOSS [training: 0.3938417977031502 | validation: 0.2608091713135474]
	TIME [epoch: 8.23 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36461179513576447		[learning rate: 0.0089399]
		[batch 20/20] avg loss: 0.3279831294919999		[learning rate: 0.0089294]
	Learning Rate: 0.00892936
	LOSS [training: 0.34629746231388214 | validation: 0.22377906013637713]
	TIME [epoch: 8.2 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.342399574294762		[learning rate: 0.0089188]
		[batch 20/20] avg loss: 0.4041877996205148		[learning rate: 0.0089083]
	Learning Rate: 0.0089083
	LOSS [training: 0.3732936869576384 | validation: 0.5110013691510913]
	TIME [epoch: 8.23 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3402862416551612		[learning rate: 0.0088978]
		[batch 20/20] avg loss: 0.4035391711350913		[learning rate: 0.0088873]
	Learning Rate: 0.00888729
	LOSS [training: 0.3719127063951263 | validation: 0.4717655108552495]
	TIME [epoch: 8.19 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4130883987293902		[learning rate: 0.0088768]
		[batch 20/20] avg loss: 0.35981249596800424		[learning rate: 0.0088663]
	Learning Rate: 0.00886632
	LOSS [training: 0.38645044734869727 | validation: 0.3364444453574466]
	TIME [epoch: 8.22 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3979133841456533		[learning rate: 0.0088559]
		[batch 20/20] avg loss: 0.3864048667415116		[learning rate: 0.0088454]
	Learning Rate: 0.00884541
	LOSS [training: 0.39215912544358245 | validation: 0.2655679979026895]
	TIME [epoch: 8.19 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3581505836424517		[learning rate: 0.008835]
		[batch 20/20] avg loss: 0.407700893242039		[learning rate: 0.0088245]
	Learning Rate: 0.00882454
	LOSS [training: 0.38292573844224537 | validation: 0.5898636400738263]
	TIME [epoch: 8.22 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39390894296622453		[learning rate: 0.0088141]
		[batch 20/20] avg loss: 0.4234348761276442		[learning rate: 0.0088037]
	Learning Rate: 0.00880373
	LOSS [training: 0.4086719095469344 | validation: 0.5487106195197603]
	TIME [epoch: 8.2 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3487785393347531		[learning rate: 0.0087933]
		[batch 20/20] avg loss: 0.33677397961803274		[learning rate: 0.008783]
	Learning Rate: 0.00878296
	LOSS [training: 0.3427762594763929 | validation: 0.17265804154440317]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32050636759083556		[learning rate: 0.0087726]
		[batch 20/20] avg loss: 0.3601240164817375		[learning rate: 0.0087622]
	Learning Rate: 0.00876225
	LOSS [training: 0.3403151920362865 | validation: 0.28673183319838613]
	TIME [epoch: 8.19 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3065149911648584		[learning rate: 0.0087519]
		[batch 20/20] avg loss: 0.3568137974318915		[learning rate: 0.0087416]
	Learning Rate: 0.00874158
	LOSS [training: 0.3316643942983749 | validation: 0.34275466323782333]
	TIME [epoch: 8.22 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3351279272727166		[learning rate: 0.0087313]
		[batch 20/20] avg loss: 0.3197698593096825		[learning rate: 0.008721]
	Learning Rate: 0.00872096
	LOSS [training: 0.3274488932911995 | validation: 0.17111679855441397]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3078486586422758		[learning rate: 0.0087107]
		[batch 20/20] avg loss: 0.36306870652042456		[learning rate: 0.0087004]
	Learning Rate: 0.00870038
	LOSS [training: 0.33545868258135014 | validation: 0.19085607638544796]
	TIME [epoch: 8.22 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31081772374873606		[learning rate: 0.0086901]
		[batch 20/20] avg loss: 0.3667376633608232		[learning rate: 0.0086799]
	Learning Rate: 0.00867986
	LOSS [training: 0.3387776935547796 | validation: 0.27115075419811946]
	TIME [epoch: 8.2 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27385494382199005		[learning rate: 0.0086696]
		[batch 20/20] avg loss: 0.30618571561340124		[learning rate: 0.0086594]
	Learning Rate: 0.00865939
	LOSS [training: 0.2900203297176956 | validation: 0.164564394585414]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39664722927131624		[learning rate: 0.0086492]
		[batch 20/20] avg loss: 0.37083567027581965		[learning rate: 0.008639]
	Learning Rate: 0.00863896
	LOSS [training: 0.38374144977356794 | validation: 0.4583934107947595]
	TIME [epoch: 8.2 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32678708793716044		[learning rate: 0.0086288]
		[batch 20/20] avg loss: 0.5452982500020016		[learning rate: 0.0086186]
	Learning Rate: 0.00861858
	LOSS [training: 0.4360426689695811 | validation: 0.3300111494840221]
	TIME [epoch: 8.22 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32182886340257355		[learning rate: 0.0086084]
		[batch 20/20] avg loss: 0.4289870836917527		[learning rate: 0.0085983]
	Learning Rate: 0.00859825
	LOSS [training: 0.37540797354716315 | validation: 0.2721552573701157]
	TIME [epoch: 8.21 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2739134933850908		[learning rate: 0.0085881]
		[batch 20/20] avg loss: 0.32280200474303744		[learning rate: 0.008578]
	Learning Rate: 0.00857797
	LOSS [training: 0.2983577490640641 | validation: 0.19417964620332878]
	TIME [epoch: 8.22 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32895234752796554		[learning rate: 0.0085678]
		[batch 20/20] avg loss: 0.5602354764608507		[learning rate: 0.0085577]
	Learning Rate: 0.00855774
	LOSS [training: 0.4445939119944081 | validation: 0.1359046451851833]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4248829592805817		[learning rate: 0.0085476]
		[batch 20/20] avg loss: 0.28001718328974495		[learning rate: 0.0085376]
	Learning Rate: 0.00853755
	LOSS [training: 0.3524500712851633 | validation: 0.9215839941171227]
	TIME [epoch: 8.23 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4268998080926691		[learning rate: 0.0085275]
		[batch 20/20] avg loss: 0.3899159037849567		[learning rate: 0.0085174]
	Learning Rate: 0.00851741
	LOSS [training: 0.40840785593881296 | validation: 0.1607779235328313]
	TIME [epoch: 8.21 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2924152331855897		[learning rate: 0.0085074]
		[batch 20/20] avg loss: 0.2564938503428234		[learning rate: 0.0084973]
	Learning Rate: 0.00849732
	LOSS [training: 0.27445454176420647 | validation: 0.3602668928161399]
	TIME [epoch: 8.21 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4025426289013049		[learning rate: 0.0084873]
		[batch 20/20] avg loss: 0.3339207043966475		[learning rate: 0.0084773]
	Learning Rate: 0.00847728
	LOSS [training: 0.36823166664897616 | validation: 0.7090272986543444]
	TIME [epoch: 8.19 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38642568522301507		[learning rate: 0.0084673]
		[batch 20/20] avg loss: 0.4384552980638345		[learning rate: 0.0084573]
	Learning Rate: 0.00845728
	LOSS [training: 0.4124404916434248 | validation: 0.2493471773550169]
	TIME [epoch: 8.22 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36979057305005025		[learning rate: 0.0084473]
		[batch 20/20] avg loss: 0.32997906011795264		[learning rate: 0.0084373]
	Learning Rate: 0.00843733
	LOSS [training: 0.34988481658400145 | validation: 0.15488219485883284]
	TIME [epoch: 8.2 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3202289788434527		[learning rate: 0.0084274]
		[batch 20/20] avg loss: 0.595449462198731		[learning rate: 0.0084174]
	Learning Rate: 0.00841743
	LOSS [training: 0.4578392205210918 | validation: 0.22602230213019342]
	TIME [epoch: 8.22 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.308791952701851		[learning rate: 0.0084075]
		[batch 20/20] avg loss: 0.4376355204386436		[learning rate: 0.0083976]
	Learning Rate: 0.00839757
	LOSS [training: 0.37321373657024726 | validation: 0.22966625865145093]
	TIME [epoch: 8.2 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30452396875877585		[learning rate: 0.0083877]
		[batch 20/20] avg loss: 0.3886671660595607		[learning rate: 0.0083778]
	Learning Rate: 0.00837777
	LOSS [training: 0.34659556740916825 | validation: 0.8322874801634367]
	TIME [epoch: 8.21 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3821121647619955		[learning rate: 0.0083679]
		[batch 20/20] avg loss: 0.3773693608315648		[learning rate: 0.008358]
	Learning Rate: 0.008358
	LOSS [training: 0.3797407627967801 | validation: 0.19783430254114553]
	TIME [epoch: 8.21 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31162663051538503		[learning rate: 0.0083481]
		[batch 20/20] avg loss: 0.33359498389643133		[learning rate: 0.0083383]
	Learning Rate: 0.00833829
	LOSS [training: 0.32261080720590823 | validation: 0.11336319485740594]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3160142695503926		[learning rate: 0.0083284]
		[batch 20/20] avg loss: 0.3188454262617544		[learning rate: 0.0083186]
	Learning Rate: 0.00831862
	LOSS [training: 0.3174298479060734 | validation: 0.17272457776218333]
	TIME [epoch: 8.19 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2725288645873386		[learning rate: 0.0083088]
		[batch 20/20] avg loss: 0.3383287325607989		[learning rate: 0.008299]
	Learning Rate: 0.008299
	LOSS [training: 0.3054287985740688 | validation: 0.2293506825598448]
	TIME [epoch: 8.21 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2941003414208281		[learning rate: 0.0082892]
		[batch 20/20] avg loss: 0.4152783291836465		[learning rate: 0.0082794]
	Learning Rate: 0.00827942
	LOSS [training: 0.35468933530223723 | validation: 0.36562679818969174]
	TIME [epoch: 8.21 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3615595364775067		[learning rate: 0.0082697]
		[batch 20/20] avg loss: 0.32858422081332844		[learning rate: 0.0082599]
	Learning Rate: 0.00825989
	LOSS [training: 0.34507187864541766 | validation: 0.6296163305762447]
	TIME [epoch: 8.21 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35437304144637893		[learning rate: 0.0082501]
		[batch 20/20] avg loss: 0.3280386617608711		[learning rate: 0.0082404]
	Learning Rate: 0.00824041
	LOSS [training: 0.34120585160362504 | validation: 0.3236239899576695]
	TIME [epoch: 8.19 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32972121828492085		[learning rate: 0.0082307]
		[batch 20/20] avg loss: 0.240463746836005		[learning rate: 0.008221]
	Learning Rate: 0.00822097
	LOSS [training: 0.28509248256046293 | validation: 0.4396830655459358]
	TIME [epoch: 8.21 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39159742465956887		[learning rate: 0.0082113]
		[batch 20/20] avg loss: 0.26515594388607416		[learning rate: 0.0082016]
	Learning Rate: 0.00820158
	LOSS [training: 0.3283766842728215 | validation: 0.21535442543911767]
	TIME [epoch: 8.19 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24765570898964953		[learning rate: 0.0081919]
		[batch 20/20] avg loss: 0.38321428904767607		[learning rate: 0.0081822]
	Learning Rate: 0.00818223
	LOSS [training: 0.3154349990186628 | validation: 0.44056290892603733]
	TIME [epoch: 8.22 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2861693763441821		[learning rate: 0.0081726]
		[batch 20/20] avg loss: 0.30247101262154497		[learning rate: 0.0081629]
	Learning Rate: 0.00816293
	LOSS [training: 0.2943201944828636 | validation: 0.5927633079964753]
	TIME [epoch: 8.19 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28250418431354973		[learning rate: 0.0081533]
		[batch 20/20] avg loss: 0.2772994695744969		[learning rate: 0.0081437]
	Learning Rate: 0.00814368
	LOSS [training: 0.2799018269440233 | validation: 0.17360253373052564]
	TIME [epoch: 8.21 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2922720439272116		[learning rate: 0.0081341]
		[batch 20/20] avg loss: 0.29206362988941664		[learning rate: 0.0081245]
	Learning Rate: 0.00812447
	LOSS [training: 0.2921678369083141 | validation: 0.175682450592842]
	TIME [epoch: 8.19 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.309081757946115		[learning rate: 0.0081149]
		[batch 20/20] avg loss: 0.25938409489790465		[learning rate: 0.0081053]
	Learning Rate: 0.0081053
	LOSS [training: 0.2842329264220099 | validation: 0.2086015088384343]
	TIME [epoch: 8.23 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24691490693148466		[learning rate: 0.0080957]
		[batch 20/20] avg loss: 0.47923481753516584		[learning rate: 0.0080862]
	Learning Rate: 0.00808618
	LOSS [training: 0.3630748622333252 | validation: 0.36945350029791846]
	TIME [epoch: 8.18 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2759242124793071		[learning rate: 0.0080766]
		[batch 20/20] avg loss: 0.32510379276053164		[learning rate: 0.0080671]
	Learning Rate: 0.00806711
	LOSS [training: 0.3005140026199193 | validation: 0.34160331845512587]
	TIME [epoch: 8.21 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27321475388250965		[learning rate: 0.0080576]
		[batch 20/20] avg loss: 0.32316654960794217		[learning rate: 0.0080481]
	Learning Rate: 0.00804808
	LOSS [training: 0.29819065174522585 | validation: 0.6286285232115861]
	TIME [epoch: 8.19 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37284237734900066		[learning rate: 0.0080386]
		[batch 20/20] avg loss: 0.2692720173487776		[learning rate: 0.0080291]
	Learning Rate: 0.0080291
	LOSS [training: 0.32105719734888905 | validation: 0.14602809192527785]
	TIME [epoch: 8.21 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3798017444779169		[learning rate: 0.0080196]
		[batch 20/20] avg loss: 0.4121142433628445		[learning rate: 0.0080102]
	Learning Rate: 0.00801016
	LOSS [training: 0.39595799392038067 | validation: 0.32666371164757474]
	TIME [epoch: 8.19 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3857695911715512		[learning rate: 0.0080007]
		[batch 20/20] avg loss: 0.24743595743089247		[learning rate: 0.0079913]
	Learning Rate: 0.00799126
	LOSS [training: 0.31660277430122175 | validation: 0.3039932590936125]
	TIME [epoch: 8.21 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2979999162043529		[learning rate: 0.0079818]
		[batch 20/20] avg loss: 0.28331454814070045		[learning rate: 0.0079724]
	Learning Rate: 0.00797241
	LOSS [training: 0.2906572321725266 | validation: 0.21129874710704094]
	TIME [epoch: 8.2 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24063342926081396		[learning rate: 0.007963]
		[batch 20/20] avg loss: 0.35663486995944516		[learning rate: 0.0079536]
	Learning Rate: 0.00795361
	LOSS [training: 0.2986341496101296 | validation: 0.22313785028496907]
	TIME [epoch: 8.22 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2492338701759825		[learning rate: 0.0079442]
		[batch 20/20] avg loss: 0.3071813009662504		[learning rate: 0.0079348]
	Learning Rate: 0.00793485
	LOSS [training: 0.27820758557111647 | validation: 0.3917566547629068]
	TIME [epoch: 8.2 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2941917383343524		[learning rate: 0.0079255]
		[batch 20/20] avg loss: 0.32097078068002355		[learning rate: 0.0079161]
	Learning Rate: 0.00791613
	LOSS [training: 0.30758125950718795 | validation: 0.24686524253353231]
	TIME [epoch: 8.21 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3578101880371786		[learning rate: 0.0079068]
		[batch 20/20] avg loss: 0.2719445690292074		[learning rate: 0.0078975]
	Learning Rate: 0.00789746
	LOSS [training: 0.314877378533193 | validation: 0.2946044753208371]
	TIME [epoch: 8.19 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27000968603779796		[learning rate: 0.0078881]
		[batch 20/20] avg loss: 0.3389459335141621		[learning rate: 0.0078788]
	Learning Rate: 0.00787883
	LOSS [training: 0.30447780977598005 | validation: 0.2488943982442107]
	TIME [epoch: 8.21 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2632550869564807		[learning rate: 0.0078695]
		[batch 20/20] avg loss: 0.3410425692249335		[learning rate: 0.0078602]
	Learning Rate: 0.00786024
	LOSS [training: 0.3021488280907071 | validation: 0.21332912641934798]
	TIME [epoch: 8.19 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26811300299739504		[learning rate: 0.007851]
		[batch 20/20] avg loss: 0.2995553337481426		[learning rate: 0.0078417]
	Learning Rate: 0.0078417
	LOSS [training: 0.2838341683727688 | validation: 0.6393764939044122]
	TIME [epoch: 8.21 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2832656032218165		[learning rate: 0.0078324]
		[batch 20/20] avg loss: 0.4513490706797268		[learning rate: 0.0078232]
	Learning Rate: 0.0078232
	LOSS [training: 0.36730733695077167 | validation: 0.1632636015523586]
	TIME [epoch: 8.2 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2876056661591999		[learning rate: 0.007814]
		[batch 20/20] avg loss: 0.35328828822518515		[learning rate: 0.0078047]
	Learning Rate: 0.00780475
	LOSS [training: 0.3204469771921925 | validation: 0.19968852717139596]
	TIME [epoch: 8.22 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26583594610284567		[learning rate: 0.0077955]
		[batch 20/20] avg loss: 0.22324440892781655		[learning rate: 0.0077863]
	Learning Rate: 0.00778634
	LOSS [training: 0.24454017751533108 | validation: 0.12425211956709978]
	TIME [epoch: 8.2 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37020416658012745		[learning rate: 0.0077772]
		[batch 20/20] avg loss: 0.28509106199552725		[learning rate: 0.007768]
	Learning Rate: 0.00776797
	LOSS [training: 0.32764761428782735 | validation: 0.23894470436820223]
	TIME [epoch: 8.21 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23938900251556833		[learning rate: 0.0077588]
		[batch 20/20] avg loss: 0.23495403929179268		[learning rate: 0.0077496]
	Learning Rate: 0.00774965
	LOSS [training: 0.23717152090368052 | validation: 0.18687933960017783]
	TIME [epoch: 8.19 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2930563513795024		[learning rate: 0.0077405]
		[batch 20/20] avg loss: 0.3510084051546957		[learning rate: 0.0077314]
	Learning Rate: 0.00773137
	LOSS [training: 0.3220323782670991 | validation: 0.19695361579002257]
	TIME [epoch: 8.21 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2601457386028546		[learning rate: 0.0077222]
		[batch 20/20] avg loss: 0.3341056113583182		[learning rate: 0.0077131]
	Learning Rate: 0.00771313
	LOSS [training: 0.29712567498058645 | validation: 0.45591718100972517]
	TIME [epoch: 8.2 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2626705271707347		[learning rate: 0.007704]
		[batch 20/20] avg loss: 0.26558095102421747		[learning rate: 0.0076949]
	Learning Rate: 0.00769494
	LOSS [training: 0.26412573909747605 | validation: 0.34277006310133373]
	TIME [epoch: 8.21 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24225712303955005		[learning rate: 0.0076859]
		[batch 20/20] avg loss: 0.2974743888287555		[learning rate: 0.0076768]
	Learning Rate: 0.00767679
	LOSS [training: 0.26986575593415274 | validation: 0.16661098535051055]
	TIME [epoch: 8.2 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2595594496067527		[learning rate: 0.0076677]
		[batch 20/20] avg loss: 0.33409101464394275		[learning rate: 0.0076587]
	Learning Rate: 0.00765868
	LOSS [training: 0.29682523212534767 | validation: 0.2708070489958055]
	TIME [epoch: 8.22 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23798524459541365		[learning rate: 0.0076496]
		[batch 20/20] avg loss: 0.23793161285881803		[learning rate: 0.0076406]
	Learning Rate: 0.00764061
	LOSS [training: 0.23795842872711584 | validation: 0.17941821970034194]
	TIME [epoch: 8.19 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2876716125731216		[learning rate: 0.0076316]
		[batch 20/20] avg loss: 0.28733294221475625		[learning rate: 0.0076226]
	Learning Rate: 0.00762259
	LOSS [training: 0.28750227739393897 | validation: 0.3337203960456122]
	TIME [epoch: 8.2 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3050584714369547		[learning rate: 0.0076136]
		[batch 20/20] avg loss: 0.22033778971367096		[learning rate: 0.0076046]
	Learning Rate: 0.00760461
	LOSS [training: 0.26269813057531277 | validation: 0.11351532804414247]
	TIME [epoch: 8.21 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24795195808715328		[learning rate: 0.0075956]
		[batch 20/20] avg loss: 0.265675015255721		[learning rate: 0.0075867]
	Learning Rate: 0.00758667
	LOSS [training: 0.2568134866714371 | validation: 0.399743000590169]
	TIME [epoch: 8.23 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22430952018555556		[learning rate: 0.0075777]
		[batch 20/20] avg loss: 0.3532123451576289		[learning rate: 0.0075688]
	Learning Rate: 0.00756878
	LOSS [training: 0.2887609326715922 | validation: 0.2706208828850059]
	TIME [epoch: 8.19 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3156776819137693		[learning rate: 0.0075598]
		[batch 20/20] avg loss: 0.22447258236647077		[learning rate: 0.0075509]
	Learning Rate: 0.00755092
	LOSS [training: 0.27007513214012013 | validation: 0.2582301494899297]
	TIME [epoch: 8.21 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21531544353516902		[learning rate: 0.007542]
		[batch 20/20] avg loss: 0.36034096457121784		[learning rate: 0.0075331]
	Learning Rate: 0.00753311
	LOSS [training: 0.2878282040531934 | validation: 0.35698234113576643]
	TIME [epoch: 8.21 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2756211194353716		[learning rate: 0.0075242]
		[batch 20/20] avg loss: 0.313739495501398		[learning rate: 0.0075153]
	Learning Rate: 0.00751534
	LOSS [training: 0.2946803074683848 | validation: 0.26153781163144135]
	TIME [epoch: 8.23 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21788516662879975		[learning rate: 0.0075065]
		[batch 20/20] avg loss: 0.27824851749879576		[learning rate: 0.0074976]
	Learning Rate: 0.00749761
	LOSS [training: 0.24806684206379778 | validation: 0.2114209960946056]
	TIME [epoch: 8.18 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36393117011838083		[learning rate: 0.0074888]
		[batch 20/20] avg loss: 0.2794894561865407		[learning rate: 0.0074799]
	Learning Rate: 0.00747993
	LOSS [training: 0.32171031315246085 | validation: 0.29624540194196325]
	TIME [epoch: 8.21 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3131878623878882		[learning rate: 0.0074711]
		[batch 20/20] avg loss: 0.24985619047028243		[learning rate: 0.0074623]
	Learning Rate: 0.00746228
	LOSS [training: 0.2815220264290853 | validation: 0.2390951782763774]
	TIME [epoch: 8.21 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23525567028703204		[learning rate: 0.0074535]
		[batch 20/20] avg loss: 0.4343165911434057		[learning rate: 0.0074447]
	Learning Rate: 0.00744468
	LOSS [training: 0.33478613071521884 | validation: 0.2794029522712258]
	TIME [epoch: 8.22 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23357328915574502		[learning rate: 0.0074359]
		[batch 20/20] avg loss: 0.25514852662800325		[learning rate: 0.0074271]
	Learning Rate: 0.00742712
	LOSS [training: 0.2443609078918742 | validation: 0.1950915036344254]
	TIME [epoch: 8.19 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24690401928992908		[learning rate: 0.0074184]
		[batch 20/20] avg loss: 0.31539184897970896		[learning rate: 0.0074096]
	Learning Rate: 0.0074096
	LOSS [training: 0.28114793413481903 | validation: 0.2001576136390123]
	TIME [epoch: 8.21 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25569885342278176		[learning rate: 0.0074009]
		[batch 20/20] avg loss: 0.24096358872014614		[learning rate: 0.0073921]
	Learning Rate: 0.00739212
	LOSS [training: 0.24833122107146396 | validation: 0.3038143894629518]
	TIME [epoch: 8.22 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32822258646879954		[learning rate: 0.0073834]
		[batch 20/20] avg loss: 0.30012431193412353		[learning rate: 0.0073747]
	Learning Rate: 0.00737469
	LOSS [training: 0.3141734492014615 | validation: 0.13794628398042244]
	TIME [epoch: 8.22 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22726332463638302		[learning rate: 0.007366]
		[batch 20/20] avg loss: 0.24411477501983433		[learning rate: 0.0073573]
	Learning Rate: 0.00735729
	LOSS [training: 0.23568904982810873 | validation: 0.13357813911322833]
	TIME [epoch: 8.19 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30921069815857616		[learning rate: 0.0073486]
		[batch 20/20] avg loss: 0.3032286592140029		[learning rate: 0.0073399]
	Learning Rate: 0.00733994
	LOSS [training: 0.3062196786862895 | validation: 0.14098367911456058]
	TIME [epoch: 8.21 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24002195564232287		[learning rate: 0.0073313]
		[batch 20/20] avg loss: 0.4326454992858532		[learning rate: 0.0073226]
	Learning Rate: 0.00732262
	LOSS [training: 0.336333727464088 | validation: 0.3927747880053498]
	TIME [epoch: 8.22 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22964943383668537		[learning rate: 0.007314]
		[batch 20/20] avg loss: 0.26097838666848344		[learning rate: 0.0073053]
	Learning Rate: 0.00730535
	LOSS [training: 0.2453139102525844 | validation: 0.10572138338012493]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2078517649349952		[learning rate: 0.0072967]
		[batch 20/20] avg loss: 0.36971460412029633		[learning rate: 0.0072881]
	Learning Rate: 0.00728812
	LOSS [training: 0.28878318452764584 | validation: 0.13753351057393734]
	TIME [epoch: 8.22 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26755628547385385		[learning rate: 0.0072795]
		[batch 20/20] avg loss: 0.2266454642516545		[learning rate: 0.0072709]
	Learning Rate: 0.00727093
	LOSS [training: 0.24710087486275417 | validation: 0.11241790612932627]
	TIME [epoch: 8.22 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24933642755675672		[learning rate: 0.0072623]
		[batch 20/20] avg loss: 0.2824344406600607		[learning rate: 0.0072538]
	Learning Rate: 0.00725377
	LOSS [training: 0.26588543410840865 | validation: 0.23671791926156316]
	TIME [epoch: 8.23 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27474810132386096		[learning rate: 0.0072452]
		[batch 20/20] avg loss: 0.21097970813509997		[learning rate: 0.0072367]
	Learning Rate: 0.00723666
	LOSS [training: 0.2428639047294804 | validation: 0.26690982972339006]
	TIME [epoch: 8.22 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25822420272696733		[learning rate: 0.0072281]
		[batch 20/20] avg loss: 0.34349600431478605		[learning rate: 0.0072196]
	Learning Rate: 0.00721959
	LOSS [training: 0.30086010352087666 | validation: 0.22733906579141486]
	TIME [epoch: 8.2 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28068151493220106		[learning rate: 0.0072111]
		[batch 20/20] avg loss: 0.36964026414854745		[learning rate: 0.0072026]
	Learning Rate: 0.00720256
	LOSS [training: 0.3251608895403742 | validation: 0.34073315266565757]
	TIME [epoch: 8.22 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22377415712302184		[learning rate: 0.0071941]
		[batch 20/20] avg loss: 0.3019907921754405		[learning rate: 0.0071856]
	Learning Rate: 0.00718558
	LOSS [training: 0.2628824746492312 | validation: 0.11665246803223989]
	TIME [epoch: 8.24 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20065559364588537		[learning rate: 0.0071771]
		[batch 20/20] avg loss: 0.21526989551526493		[learning rate: 0.0071686]
	Learning Rate: 0.00716863
	LOSS [training: 0.20796274458057512 | validation: 0.17001120638972708]
	TIME [epoch: 8.21 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29421824703074867		[learning rate: 0.0071602]
		[batch 20/20] avg loss: 0.24242630557211098		[learning rate: 0.0071517]
	Learning Rate: 0.00715172
	LOSS [training: 0.26832227630142985 | validation: 0.2678294728096157]
	TIME [epoch: 8.2 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2629003142660128		[learning rate: 0.0071433]
		[batch 20/20] avg loss: 0.18861685196123856		[learning rate: 0.0071348]
	Learning Rate: 0.00713485
	LOSS [training: 0.22575858311362573 | validation: 0.22525781531888678]
	TIME [epoch: 8.21 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3048370637971588		[learning rate: 0.0071264]
		[batch 20/20] avg loss: 0.2917224067150412		[learning rate: 0.007118]
	Learning Rate: 0.00711802
	LOSS [training: 0.2982797352561001 | validation: 0.16847846658792837]
	TIME [epoch: 8.25 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22608888855502923		[learning rate: 0.0071096]
		[batch 20/20] avg loss: 0.24101034672093863		[learning rate: 0.0071012]
	Learning Rate: 0.00710123
	LOSS [training: 0.23354961763798393 | validation: 0.22349348369047434]
	TIME [epoch: 8.2 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24407136632221937		[learning rate: 0.0070928]
		[batch 20/20] avg loss: 0.26274722745112705		[learning rate: 0.0070845]
	Learning Rate: 0.00708447
	LOSS [training: 0.2534092968866733 | validation: 0.24989057128190412]
	TIME [epoch: 8.2 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22919054982814607		[learning rate: 0.0070761]
		[batch 20/20] avg loss: 0.2227533068155239		[learning rate: 0.0070678]
	Learning Rate: 0.00706776
	LOSS [training: 0.22597192832183502 | validation: 0.12370988719801124]
	TIME [epoch: 8.21 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27209653747698215		[learning rate: 0.0070594]
		[batch 20/20] avg loss: 0.32189359593455374		[learning rate: 0.0070511]
	Learning Rate: 0.00705109
	LOSS [training: 0.296995066705768 | validation: 0.6519132808393596]
	TIME [epoch: 8.26 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3623789756803327		[learning rate: 0.0070428]
		[batch 20/20] avg loss: 0.29631960070330343		[learning rate: 0.0070345]
	Learning Rate: 0.00703446
	LOSS [training: 0.3293492881918181 | validation: 0.23978174207886088]
	TIME [epoch: 8.21 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1779291221803651		[learning rate: 0.0070262]
		[batch 20/20] avg loss: 0.19617256164463484		[learning rate: 0.0070179]
	Learning Rate: 0.00701787
	LOSS [training: 0.18705084191249993 | validation: 0.1520355801509566]
	TIME [epoch: 8.2 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41779698202160465		[learning rate: 0.0070096]
		[batch 20/20] avg loss: 0.3024015212457244		[learning rate: 0.0070013]
	Learning Rate: 0.00700131
	LOSS [training: 0.3600992516336645 | validation: 0.10404493696393763]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20506733652977158		[learning rate: 0.006993]
		[batch 20/20] avg loss: 0.2125379915732088		[learning rate: 0.0069848]
	Learning Rate: 0.0069848
	LOSS [training: 0.20880266405149017 | validation: 0.07696108210046484]
	TIME [epoch: 8.26 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25141710065666417		[learning rate: 0.0069766]
		[batch 20/20] avg loss: 0.23022929936645187		[learning rate: 0.0069683]
	Learning Rate: 0.00696832
	LOSS [training: 0.240823200011558 | validation: 0.28772735495484253]
	TIME [epoch: 8.2 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2915447835949053		[learning rate: 0.0069601]
		[batch 20/20] avg loss: 0.24669782797028636		[learning rate: 0.0069519]
	Learning Rate: 0.00695188
	LOSS [training: 0.26912130578259585 | validation: 0.2559545353510999]
	TIME [epoch: 8.2 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2283576214859085		[learning rate: 0.0069437]
		[batch 20/20] avg loss: 0.2072563307891402		[learning rate: 0.0069355]
	Learning Rate: 0.00693549
	LOSS [training: 0.21780697613752437 | validation: 0.09541596347039795]
	TIME [epoch: 8.22 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2849321261530044		[learning rate: 0.0069273]
		[batch 20/20] avg loss: 0.2621077588818218		[learning rate: 0.0069191]
	Learning Rate: 0.00691913
	LOSS [training: 0.27351994251741313 | validation: 0.37760100249106104]
	TIME [epoch: 8.24 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2903366714059463		[learning rate: 0.006911]
		[batch 20/20] avg loss: 0.2273441168172443		[learning rate: 0.0069028]
	Learning Rate: 0.00690281
	LOSS [training: 0.2588403941115953 | validation: 0.12684732206374852]
	TIME [epoch: 8.19 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24130287003758993		[learning rate: 0.0068947]
		[batch 20/20] avg loss: 0.27299849069945903		[learning rate: 0.0068865]
	Learning Rate: 0.00688652
	LOSS [training: 0.2571506803685245 | validation: 0.24761916963814865]
	TIME [epoch: 8.2 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2928398587657205		[learning rate: 0.0068784]
		[batch 20/20] avg loss: 0.3000123894090265		[learning rate: 0.0068703]
	Learning Rate: 0.00687028
	LOSS [training: 0.29642612408737345 | validation: 0.1478389924702992]
	TIME [epoch: 8.21 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.237368962115824		[learning rate: 0.0068622]
		[batch 20/20] avg loss: 0.25689502955338767		[learning rate: 0.0068541]
	Learning Rate: 0.00685407
	LOSS [training: 0.24713199583460582 | validation: 0.16782773298774645]
	TIME [epoch: 8.24 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21171824725597227		[learning rate: 0.006846]
		[batch 20/20] avg loss: 0.2762086389972791		[learning rate: 0.0068379]
	Learning Rate: 0.0068379
	LOSS [training: 0.24396344312662568 | validation: 0.11498779806503306]
	TIME [epoch: 8.2 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23919985941611127		[learning rate: 0.0068298]
		[batch 20/20] avg loss: 0.2928650405311969		[learning rate: 0.0068218]
	Learning Rate: 0.00682178
	LOSS [training: 0.26603244997365405 | validation: 0.29643945351604595]
	TIME [epoch: 8.2 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3062997657519363		[learning rate: 0.0068137]
		[batch 20/20] avg loss: 0.2437134280985182		[learning rate: 0.0068057]
	Learning Rate: 0.00680568
	LOSS [training: 0.2750065969252273 | validation: 0.1579079895095895]
	TIME [epoch: 8.22 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2843355552780516		[learning rate: 0.0067977]
		[batch 20/20] avg loss: 0.4125757865877973		[learning rate: 0.0067896]
	Learning Rate: 0.00678963
	LOSS [training: 0.34845567093292446 | validation: 0.49850140228486395]
	TIME [epoch: 8.24 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2721048892193208		[learning rate: 0.0067816]
		[batch 20/20] avg loss: 0.24744993900355722		[learning rate: 0.0067736]
	Learning Rate: 0.00677361
	LOSS [training: 0.259777414111439 | validation: 0.19929099130086023]
	TIME [epoch: 8.2 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2821210618479789		[learning rate: 0.0067656]
		[batch 20/20] avg loss: 0.21524309958703078		[learning rate: 0.0067576]
	Learning Rate: 0.00675764
	LOSS [training: 0.2486820807175049 | validation: 0.11899666721454374]
	TIME [epoch: 8.19 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3837919843362125		[learning rate: 0.0067497]
		[batch 20/20] avg loss: 0.2897650005120196		[learning rate: 0.0067417]
	Learning Rate: 0.0067417
	LOSS [training: 0.33677849242411606 | validation: 0.24584198631355209]
	TIME [epoch: 8.2 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28566704020760475		[learning rate: 0.0067337]
		[batch 20/20] avg loss: 0.28620403948808104		[learning rate: 0.0067258]
	Learning Rate: 0.00672579
	LOSS [training: 0.28593553984784287 | validation: 0.11397448696621362]
	TIME [epoch: 8.25 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19061357271224627		[learning rate: 0.0067179]
		[batch 20/20] avg loss: 0.295459482907469		[learning rate: 0.0067099]
	Learning Rate: 0.00670993
	LOSS [training: 0.24303652780985768 | validation: 0.40203655248108294]
	TIME [epoch: 8.2 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28781249931923075		[learning rate: 0.006702]
		[batch 20/20] avg loss: 0.2701790037808498		[learning rate: 0.0066941]
	Learning Rate: 0.0066941
	LOSS [training: 0.27899575155004025 | validation: 0.09850616508126536]
	TIME [epoch: 8.2 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1945333805397607		[learning rate: 0.0066862]
		[batch 20/20] avg loss: 0.2208617784552381		[learning rate: 0.0066783]
	Learning Rate: 0.00667831
	LOSS [training: 0.2076975794974994 | validation: 0.5173603085441236]
	TIME [epoch: 8.22 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30494141494867155		[learning rate: 0.0066704]
		[batch 20/20] avg loss: 0.22253474591317507		[learning rate: 0.0066626]
	Learning Rate: 0.00666256
	LOSS [training: 0.2637380804309232 | validation: 0.16626054698104933]
	TIME [epoch: 8.24 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19834364010164957		[learning rate: 0.0066547]
		[batch 20/20] avg loss: 0.24719245031525464		[learning rate: 0.0066468]
	Learning Rate: 0.00664684
	LOSS [training: 0.2227680452084521 | validation: 0.21392990488683544]
	TIME [epoch: 8.2 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18438302302732104		[learning rate: 0.006639]
		[batch 20/20] avg loss: 0.19289347046425326		[learning rate: 0.0066312]
	Learning Rate: 0.00663116
	LOSS [training: 0.18863824674578716 | validation: 0.11465093652613151]
	TIME [epoch: 8.19 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2775663363890563		[learning rate: 0.0066233]
		[batch 20/20] avg loss: 0.22675741700791976		[learning rate: 0.0066155]
	Learning Rate: 0.00661552
	LOSS [training: 0.25216187669848805 | validation: 0.15515766717311863]
	TIME [epoch: 8.2 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2118833605642561		[learning rate: 0.0066077]
		[batch 20/20] avg loss: 0.2647760494033166		[learning rate: 0.0065999]
	Learning Rate: 0.00659992
	LOSS [training: 0.23832970498378642 | validation: 0.20353802925059905]
	TIME [epoch: 8.26 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27406766924658676		[learning rate: 0.0065921]
		[batch 20/20] avg loss: 0.18521411623318298		[learning rate: 0.0065843]
	Learning Rate: 0.00658435
	LOSS [training: 0.2296408927398849 | validation: 0.11296652775401711]
	TIME [epoch: 8.2 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1747422319811738		[learning rate: 0.0065766]
		[batch 20/20] avg loss: 0.20165003792349076		[learning rate: 0.0065688]
	Learning Rate: 0.00656882
	LOSS [training: 0.18819613495233226 | validation: 0.15741571317251335]
	TIME [epoch: 8.2 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18778812863593516		[learning rate: 0.0065611]
		[batch 20/20] avg loss: 0.26596938983763346		[learning rate: 0.0065533]
	Learning Rate: 0.00655332
	LOSS [training: 0.22687875923678433 | validation: 0.18265890277714614]
	TIME [epoch: 8.21 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.289061928524879		[learning rate: 0.0065456]
		[batch 20/20] avg loss: 0.1847481685806307		[learning rate: 0.0065379]
	Learning Rate: 0.00653786
	LOSS [training: 0.23690504855275485 | validation: 0.1688216910997457]
	TIME [epoch: 8.25 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28684533150515035		[learning rate: 0.0065301]
		[batch 20/20] avg loss: 0.21371173767486296		[learning rate: 0.0065224]
	Learning Rate: 0.00652244
	LOSS [training: 0.2502785345900066 | validation: 0.21429516142663702]
	TIME [epoch: 8.2 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2452608091678447		[learning rate: 0.0065147]
		[batch 20/20] avg loss: 0.22397314097422888		[learning rate: 0.0065071]
	Learning Rate: 0.00650706
	LOSS [training: 0.23461697507103682 | validation: 0.16546305764231645]
	TIME [epoch: 8.19 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25043727151587103		[learning rate: 0.0064994]
		[batch 20/20] avg loss: 0.2047522915485473		[learning rate: 0.0064917]
	Learning Rate: 0.00649171
	LOSS [training: 0.22759478153220924 | validation: 0.43750287017226563]
	TIME [epoch: 8.2 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28453488876146105		[learning rate: 0.006484]
		[batch 20/20] avg loss: 0.3119700572564292		[learning rate: 0.0064764]
	Learning Rate: 0.00647639
	LOSS [training: 0.2982524730089451 | validation: 0.18223191006145292]
	TIME [epoch: 8.25 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33617774354244306		[learning rate: 0.0064688]
		[batch 20/20] avg loss: 0.2435219015447828		[learning rate: 0.0064611]
	Learning Rate: 0.00646112
	LOSS [training: 0.2898498225436129 | validation: 0.14472598428409283]
	TIME [epoch: 8.2 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3207807933628736		[learning rate: 0.0064535]
		[batch 20/20] avg loss: 0.2121735498244953		[learning rate: 0.0064459]
	Learning Rate: 0.00644588
	LOSS [training: 0.26647717159368445 | validation: 0.15803045101272836]
	TIME [epoch: 8.19 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23284506587504436		[learning rate: 0.0064383]
		[batch 20/20] avg loss: 0.2653576931919809		[learning rate: 0.0064307]
	Learning Rate: 0.00643067
	LOSS [training: 0.24910137953351263 | validation: 0.14476740200287913]
	TIME [epoch: 8.21 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2619585312935597		[learning rate: 0.0064231]
		[batch 20/20] avg loss: 0.2100604841750632		[learning rate: 0.0064155]
	Learning Rate: 0.0064155
	LOSS [training: 0.23600950773431145 | validation: 0.16705899760579496]
	TIME [epoch: 8.25 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2418715037483278		[learning rate: 0.0064079]
		[batch 20/20] avg loss: 0.2074685246709717		[learning rate: 0.0064004]
	Learning Rate: 0.00640037
	LOSS [training: 0.22467001420964977 | validation: 0.313110242424893]
	TIME [epoch: 8.19 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22479193014957422		[learning rate: 0.0063928]
		[batch 20/20] avg loss: 0.1777630966787241		[learning rate: 0.0063853]
	Learning Rate: 0.00638527
	LOSS [training: 0.20127751341414918 | validation: 0.2513326328956189]
	TIME [epoch: 8.19 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22662908880883217		[learning rate: 0.0063777]
		[batch 20/20] avg loss: 0.2882687826496388		[learning rate: 0.0063702]
	Learning Rate: 0.00637021
	LOSS [training: 0.25744893572923544 | validation: 0.12368752964559096]
	TIME [epoch: 8.21 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17997365155435216		[learning rate: 0.0063627]
		[batch 20/20] avg loss: 0.2567847678128109		[learning rate: 0.0063552]
	Learning Rate: 0.00635518
	LOSS [training: 0.21837920968358154 | validation: 0.10909236803924569]
	TIME [epoch: 8.25 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19140966224161213		[learning rate: 0.0063477]
		[batch 20/20] avg loss: 0.1960466567019487		[learning rate: 0.0063402]
	Learning Rate: 0.00634019
	LOSS [training: 0.1937281594717804 | validation: 0.1717458354895504]
	TIME [epoch: 8.2 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24634076197771088		[learning rate: 0.0063327]
		[batch 20/20] avg loss: 0.20723537663983865		[learning rate: 0.0063252]
	Learning Rate: 0.00632524
	LOSS [training: 0.22678806930877476 | validation: 0.11588110138032637]
	TIME [epoch: 8.19 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2216679138224457		[learning rate: 0.0063178]
		[batch 20/20] avg loss: 0.22287792651722022		[learning rate: 0.0063103]
	Learning Rate: 0.00631032
	LOSS [training: 0.22227292016983294 | validation: 0.131629668538536]
	TIME [epoch: 8.21 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2055563890224435		[learning rate: 0.0063029]
		[batch 20/20] avg loss: 0.2265961599643372		[learning rate: 0.0062954]
	Learning Rate: 0.00629543
	LOSS [training: 0.21607627449339034 | validation: 0.2750394207143944]
	TIME [epoch: 8.24 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1608263385906989		[learning rate: 0.006288]
		[batch 20/20] avg loss: 0.19338158483365264		[learning rate: 0.0062806]
	Learning Rate: 0.00628058
	LOSS [training: 0.17710396171217577 | validation: 0.7589420726470162]
	TIME [epoch: 8.2 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3130072889842174		[learning rate: 0.0062732]
		[batch 20/20] avg loss: 0.1916425683845429		[learning rate: 0.0062658]
	Learning Rate: 0.00626577
	LOSS [training: 0.25232492868438017 | validation: 0.11702616570306265]
	TIME [epoch: 8.19 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2782740250675392		[learning rate: 0.0062584]
		[batch 20/20] avg loss: 0.1914748653518422		[learning rate: 0.006251]
	Learning Rate: 0.00625099
	LOSS [training: 0.23487444520969064 | validation: 0.4645910401072606]
	TIME [epoch: 8.21 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1949133170345078		[learning rate: 0.0062436]
		[batch 20/20] avg loss: 0.2152705868032244		[learning rate: 0.0062362]
	Learning Rate: 0.00623624
	LOSS [training: 0.20509195191886614 | validation: 0.12951867353114563]
	TIME [epoch: 8.25 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24503447200583378		[learning rate: 0.0062289]
		[batch 20/20] avg loss: 0.2025800662854691		[learning rate: 0.0062215]
	Learning Rate: 0.00622153
	LOSS [training: 0.22380726914565147 | validation: 0.12723738952436064]
	TIME [epoch: 8.2 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24315864138644364		[learning rate: 0.0062142]
		[batch 20/20] avg loss: 0.25737143979577526		[learning rate: 0.0062069]
	Learning Rate: 0.00620686
	LOSS [training: 0.2502650405911094 | validation: 0.18099692021506797]
	TIME [epoch: 8.2 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22403453256490263		[learning rate: 0.0061995]
		[batch 20/20] avg loss: 0.18653024880206254		[learning rate: 0.0061922]
	Learning Rate: 0.00619222
	LOSS [training: 0.20528239068348259 | validation: 0.17557934327475025]
	TIME [epoch: 8.21 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2563054259615512		[learning rate: 0.0061849]
		[batch 20/20] avg loss: 0.22165105755342657		[learning rate: 0.0061776]
	Learning Rate: 0.00617761
	LOSS [training: 0.23897824175748889 | validation: 0.20859646697086265]
	TIME [epoch: 8.24 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3125799951670521		[learning rate: 0.0061703]
		[batch 20/20] avg loss: 0.18897472892202816		[learning rate: 0.006163]
	Learning Rate: 0.00616304
	LOSS [training: 0.25077736204454004 | validation: 0.31577754276881714]
	TIME [epoch: 8.21 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21271556005767897		[learning rate: 0.0061558]
		[batch 20/20] avg loss: 0.22946768618905153		[learning rate: 0.0061485]
	Learning Rate: 0.0061485
	LOSS [training: 0.2210916231233652 | validation: 0.07144646051721547]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22092329967486704		[learning rate: 0.0061412]
		[batch 20/20] avg loss: 0.24364246487101454		[learning rate: 0.006134]
	Learning Rate: 0.006134
	LOSS [training: 0.2322828822729408 | validation: 0.15357286603190523]
	TIME [epoch: 8.21 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22903820182733953		[learning rate: 0.0061268]
		[batch 20/20] avg loss: 0.2054480554064615		[learning rate: 0.0061195]
	Learning Rate: 0.00611953
	LOSS [training: 0.2172431286169006 | validation: 0.09933105473645809]
	TIME [epoch: 8.23 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29977301138361534		[learning rate: 0.0061123]
		[batch 20/20] avg loss: 0.2845129861858615		[learning rate: 0.0061051]
	Learning Rate: 0.00610509
	LOSS [training: 0.2921429987847384 | validation: 0.12167574678399218]
	TIME [epoch: 8.2 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26006530638374237		[learning rate: 0.0060979]
		[batch 20/20] avg loss: 0.23661368370801145		[learning rate: 0.0060907]
	Learning Rate: 0.00609069
	LOSS [training: 0.2483394950458769 | validation: 0.584622234627573]
	TIME [epoch: 8.19 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27914542951506965		[learning rate: 0.0060835]
		[batch 20/20] avg loss: 0.25783872532167773		[learning rate: 0.0060763]
	Learning Rate: 0.00607633
	LOSS [training: 0.2684920774183737 | validation: 0.22201725098289898]
	TIME [epoch: 8.21 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3344753279043743		[learning rate: 0.0060692]
		[batch 20/20] avg loss: 0.3561119253574706		[learning rate: 0.006062]
	Learning Rate: 0.00606199
	LOSS [training: 0.34529362663092245 | validation: 0.31178708526472876]
	TIME [epoch: 8.24 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23800002790212132		[learning rate: 0.0060548]
		[batch 20/20] avg loss: 0.2763407804550752		[learning rate: 0.0060477]
	Learning Rate: 0.00604769
	LOSS [training: 0.2571704041785983 | validation: 0.18918567170579376]
	TIME [epoch: 8.2 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2758948116941152		[learning rate: 0.0060406]
		[batch 20/20] avg loss: 0.17205314182363038		[learning rate: 0.0060334]
	Learning Rate: 0.00603343
	LOSS [training: 0.22397397675887282 | validation: 0.1719670197493836]
	TIME [epoch: 8.2 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23705286727052627		[learning rate: 0.0060263]
		[batch 20/20] avg loss: 0.1880799821611359		[learning rate: 0.0060192]
	Learning Rate: 0.0060192
	LOSS [training: 0.21256642471583106 | validation: 0.4864223898721073]
	TIME [epoch: 8.21 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3166255594510966		[learning rate: 0.0060121]
		[batch 20/20] avg loss: 0.20784399490164693		[learning rate: 0.006005]
	Learning Rate: 0.006005
	LOSS [training: 0.2622347771763718 | validation: 0.4000883858047136]
	TIME [epoch: 8.24 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2217372669972711		[learning rate: 0.0059979]
		[batch 20/20] avg loss: 0.2707998032050097		[learning rate: 0.0059908]
	Learning Rate: 0.00599083
	LOSS [training: 0.24626853510114036 | validation: 0.09236143902850544]
	TIME [epoch: 8.19 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22785182282202107		[learning rate: 0.0059838]
		[batch 20/20] avg loss: 0.35203164714104274		[learning rate: 0.0059767]
	Learning Rate: 0.0059767
	LOSS [training: 0.289941734981532 | validation: 0.27488914644674595]
	TIME [epoch: 8.18 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24776499433125537		[learning rate: 0.0059696]
		[batch 20/20] avg loss: 0.28152585653309015		[learning rate: 0.0059626]
	Learning Rate: 0.0059626
	LOSS [training: 0.26464542543217273 | validation: 0.20079996057792537]
	TIME [epoch: 8.21 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33626871722475665		[learning rate: 0.0059556]
		[batch 20/20] avg loss: 0.24126077602497836		[learning rate: 0.0059485]
	Learning Rate: 0.00594854
	LOSS [training: 0.2887647466248675 | validation: 0.10014215826375275]
	TIME [epoch: 8.23 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21988948503157985		[learning rate: 0.0059415]
		[batch 20/20] avg loss: 0.4645139954557712		[learning rate: 0.0059345]
	Learning Rate: 0.00593451
	LOSS [training: 0.34220174024367556 | validation: 0.07663598843105907]
	TIME [epoch: 8.2 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2578333170805698		[learning rate: 0.0059275]
		[batch 20/20] avg loss: 0.24179864716109317		[learning rate: 0.0059205]
	Learning Rate: 0.00592051
	LOSS [training: 0.2498159821208314 | validation: 0.17734572214187974]
	TIME [epoch: 8.19 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2473175620230374		[learning rate: 0.0059135]
		[batch 20/20] avg loss: 0.2257291756864462		[learning rate: 0.0059065]
	Learning Rate: 0.00590654
	LOSS [training: 0.23652336885474182 | validation: 0.2371963559742851]
	TIME [epoch: 8.21 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2666449332058716		[learning rate: 0.0058996]
		[batch 20/20] avg loss: 0.2940330881709148		[learning rate: 0.0058926]
	Learning Rate: 0.00589261
	LOSS [training: 0.2803390106883932 | validation: 0.20775524709189774]
	TIME [epoch: 8.23 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2528366017711709		[learning rate: 0.0058857]
		[batch 20/20] avg loss: 0.2938278796588397		[learning rate: 0.0058787]
	Learning Rate: 0.00587871
	LOSS [training: 0.2733322407150054 | validation: 0.9543663310224626]
	TIME [epoch: 8.19 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3891780044700077		[learning rate: 0.0058718]
		[batch 20/20] avg loss: 0.24030405262733204		[learning rate: 0.0058648]
	Learning Rate: 0.00586484
	LOSS [training: 0.3147410285486698 | validation: 0.6762297743610113]
	TIME [epoch: 8.19 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28319501686692516		[learning rate: 0.0058579]
		[batch 20/20] avg loss: 0.19976394145876736		[learning rate: 0.005851]
	Learning Rate: 0.00585101
	LOSS [training: 0.24147947916284634 | validation: 0.2025203407299318]
	TIME [epoch: 8.21 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29227385025488417		[learning rate: 0.0058441]
		[batch 20/20] avg loss: 0.30432803284982723		[learning rate: 0.0058372]
	Learning Rate: 0.00583721
	LOSS [training: 0.29830094155235576 | validation: 0.6059233004917853]
	TIME [epoch: 8.23 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4913066058526162		[learning rate: 0.0058303]
		[batch 20/20] avg loss: 0.2821333246618171		[learning rate: 0.0058234]
	Learning Rate: 0.00582344
	LOSS [training: 0.38671996525721664 | validation: 0.11067626905185078]
	TIME [epoch: 8.21 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25380752700818926		[learning rate: 0.0058166]
		[batch 20/20] avg loss: 0.29621090305368547		[learning rate: 0.0058097]
	Learning Rate: 0.0058097
	LOSS [training: 0.2750092150309374 | validation: 0.524036805647501]
	TIME [epoch: 8.19 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33982513871790804		[learning rate: 0.0058028]
		[batch 20/20] avg loss: 0.21097696866898757		[learning rate: 0.005796]
	Learning Rate: 0.005796
	LOSS [training: 0.27540105369344775 | validation: 0.19256706316774605]
	TIME [epoch: 8.22 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28829787425631187		[learning rate: 0.0057892]
		[batch 20/20] avg loss: 0.298333895517034		[learning rate: 0.0057823]
	Learning Rate: 0.00578233
	LOSS [training: 0.29331588488667293 | validation: 0.3636894780208665]
	TIME [epoch: 8.22 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2386523010288959		[learning rate: 0.0057755]
		[batch 20/20] avg loss: 0.21177198816086545		[learning rate: 0.0057687]
	Learning Rate: 0.00576869
	LOSS [training: 0.22521214459488065 | validation: 0.32710677058300053]
	TIME [epoch: 8.21 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4041424074922773		[learning rate: 0.0057619]
		[batch 20/20] avg loss: 0.2035431106517731		[learning rate: 0.0057551]
	Learning Rate: 0.00575508
	LOSS [training: 0.30384275907202524 | validation: 0.19785128688256545]
	TIME [epoch: 8.19 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28196941340444887		[learning rate: 0.0057483]
		[batch 20/20] avg loss: 0.24201275631422803		[learning rate: 0.0057415]
	Learning Rate: 0.0057415
	LOSS [training: 0.2619910848593384 | validation: 0.35608318057354793]
	TIME [epoch: 8.23 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19007834127152684		[learning rate: 0.0057347]
		[batch 20/20] avg loss: 0.23023480002128563		[learning rate: 0.005728]
	Learning Rate: 0.00572796
	LOSS [training: 0.21015657064640622 | validation: 0.2483581352547944]
	TIME [epoch: 8.22 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26420587098422194		[learning rate: 0.0057212]
		[batch 20/20] avg loss: 0.29067100039215227		[learning rate: 0.0057144]
	Learning Rate: 0.00571445
	LOSS [training: 0.27743843568818705 | validation: 0.14641872481710683]
	TIME [epoch: 8.22 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21829224984705003		[learning rate: 0.0057077]
		[batch 20/20] avg loss: 0.38756941311038845		[learning rate: 0.005701]
	Learning Rate: 0.00570097
	LOSS [training: 0.30293083147871924 | validation: 0.21930411889366216]
	TIME [epoch: 8.22 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22238628198720223		[learning rate: 0.0056942]
		[batch 20/20] avg loss: 0.33965195652753677		[learning rate: 0.0056875]
	Learning Rate: 0.00568752
	LOSS [training: 0.2810191192573695 | validation: 0.19090940119509975]
	TIME [epoch: 8.24 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2934369465006924		[learning rate: 0.0056808]
		[batch 20/20] avg loss: 0.23895194436085682		[learning rate: 0.0056741]
	Learning Rate: 0.00567411
	LOSS [training: 0.26619444543077453 | validation: 0.09649755920248014]
	TIME [epoch: 8.24 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29414621997968365		[learning rate: 0.0056674]
		[batch 20/20] avg loss: 0.28702791732211735		[learning rate: 0.0056607]
	Learning Rate: 0.00566072
	LOSS [training: 0.2905870686509004 | validation: 0.58386693195326]
	TIME [epoch: 8.23 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31440438969658313		[learning rate: 0.005654]
		[batch 20/20] avg loss: 0.20626953144135934		[learning rate: 0.0056474]
	Learning Rate: 0.00564737
	LOSS [training: 0.26033696056897115 | validation: 0.2740654039302855]
	TIME [epoch: 8.21 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3114351697433704		[learning rate: 0.0056407]
		[batch 20/20] avg loss: 0.27041572104436346		[learning rate: 0.005634]
	Learning Rate: 0.00563405
	LOSS [training: 0.29092544539386694 | validation: 0.20894135251606566]
	TIME [epoch: 8.24 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21331249499931068		[learning rate: 0.0056274]
		[batch 20/20] avg loss: 0.21982795498255756		[learning rate: 0.0056208]
	Learning Rate: 0.00562076
	LOSS [training: 0.21657022499093412 | validation: 0.19705486076354906]
	TIME [epoch: 8.23 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19192318657322024		[learning rate: 0.0056141]
		[batch 20/20] avg loss: 0.4629557261758306		[learning rate: 0.0056075]
	Learning Rate: 0.0056075
	LOSS [training: 0.3274394563745253 | validation: 0.30762055562966123]
	TIME [epoch: 8.23 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19826948336199698		[learning rate: 0.0056009]
		[batch 20/20] avg loss: 0.22201301523880207		[learning rate: 0.0055943]
	Learning Rate: 0.00559427
	LOSS [training: 0.2101412493003995 | validation: 0.22535077550652782]
	TIME [epoch: 8.21 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2398234023433028		[learning rate: 0.0055877]
		[batch 20/20] avg loss: 0.2299989725029214		[learning rate: 0.0055811]
	Learning Rate: 0.00558108
	LOSS [training: 0.23491118742311207 | validation: 0.11238803670731953]
	TIME [epoch: 8.24 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2305443221669239		[learning rate: 0.0055745]
		[batch 20/20] avg loss: 0.2067154597328095		[learning rate: 0.0055679]
	Learning Rate: 0.00556791
	LOSS [training: 0.21862989094986665 | validation: 0.17781147249211315]
	TIME [epoch: 8.24 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2758833042647749		[learning rate: 0.0055613]
		[batch 20/20] avg loss: 0.2506291419526962		[learning rate: 0.0055548]
	Learning Rate: 0.00555478
	LOSS [training: 0.26325622310873553 | validation: 0.1341959324926466]
	TIME [epoch: 8.24 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2112886956613024		[learning rate: 0.0055482]
		[batch 20/20] avg loss: 0.24111299982354623		[learning rate: 0.0055417]
	Learning Rate: 0.00554167
	LOSS [training: 0.2262008477424243 | validation: 0.14390569501381834]
	TIME [epoch: 8.19 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2385571326877348		[learning rate: 0.0055351]
		[batch 20/20] avg loss: 0.22629338301569488		[learning rate: 0.0055286]
	Learning Rate: 0.0055286
	LOSS [training: 0.23242525785171483 | validation: 0.08319941678374959]
	TIME [epoch: 8.25 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19380430615388797		[learning rate: 0.0055221]
		[batch 20/20] avg loss: 0.22904885910464526		[learning rate: 0.0055156]
	Learning Rate: 0.00551556
	LOSS [training: 0.21142658262926667 | validation: 0.1359210438565419]
	TIME [epoch: 8.24 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24058681121491493		[learning rate: 0.0055091]
		[batch 20/20] avg loss: 0.39201553193163374		[learning rate: 0.0055026]
	Learning Rate: 0.00550255
	LOSS [training: 0.3163011715732743 | validation: 0.16611410074547428]
	TIME [epoch: 8.23 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24818281177513857		[learning rate: 0.0054961]
		[batch 20/20] avg loss: 0.2509421259433601		[learning rate: 0.0054896]
	Learning Rate: 0.00548957
	LOSS [training: 0.24956246885924935 | validation: 0.22154780696053766]
	TIME [epoch: 8.21 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21941328514731007		[learning rate: 0.0054831]
		[batch 20/20] avg loss: 0.2355070216958517		[learning rate: 0.0054766]
	Learning Rate: 0.00547662
	LOSS [training: 0.22746015342158082 | validation: 0.17274186257377716]
	TIME [epoch: 8.24 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20072836252083087		[learning rate: 0.0054702]
		[batch 20/20] avg loss: 0.21376231337762291		[learning rate: 0.0054637]
	Learning Rate: 0.0054637
	LOSS [training: 0.2072453379492269 | validation: 0.308113879098607]
	TIME [epoch: 8.21 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2495691715322975		[learning rate: 0.0054573]
		[batch 20/20] avg loss: 0.2750016453580447		[learning rate: 0.0054508]
	Learning Rate: 0.00545082
	LOSS [training: 0.2622854084451711 | validation: 0.19857910637228082]
	TIME [epoch: 8.24 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22357926124550304		[learning rate: 0.0054444]
		[batch 20/20] avg loss: 0.13578653339297153		[learning rate: 0.005438]
	Learning Rate: 0.00543796
	LOSS [training: 0.17968289731923726 | validation: 0.5206992687904115]
	TIME [epoch: 8.22 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30054828213774165		[learning rate: 0.0054315]
		[batch 20/20] avg loss: 0.20206876901486331		[learning rate: 0.0054251]
	Learning Rate: 0.00542513
	LOSS [training: 0.25130852557630246 | validation: 0.1693942740850334]
	TIME [epoch: 8.24 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2563414136143608		[learning rate: 0.0054187]
		[batch 20/20] avg loss: 0.19642877644788828		[learning rate: 0.0054123]
	Learning Rate: 0.00541233
	LOSS [training: 0.22638509503112458 | validation: 0.09606607890818011]
	TIME [epoch: 8.2 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2007802325623173		[learning rate: 0.0054059]
		[batch 20/20] avg loss: 0.22085072931363653		[learning rate: 0.0053996]
	Learning Rate: 0.00539957
	LOSS [training: 0.21081548093797697 | validation: 0.08808321014727953]
	TIME [epoch: 8.22 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14629680091631542		[learning rate: 0.0053932]
		[batch 20/20] avg loss: 0.23779321748665067		[learning rate: 0.0053868]
	Learning Rate: 0.00538683
	LOSS [training: 0.1920450092014831 | validation: 0.08049677188487425]
	TIME [epoch: 8.2 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1725436375473904		[learning rate: 0.0053805]
		[batch 20/20] avg loss: 0.17950491660333337		[learning rate: 0.0053741]
	Learning Rate: 0.00537412
	LOSS [training: 0.1760242770753619 | validation: 0.18110537934457568]
	TIME [epoch: 8.22 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22192852126233076		[learning rate: 0.0053678]
		[batch 20/20] avg loss: 0.21486123399335116		[learning rate: 0.0053614]
	Learning Rate: 0.00536145
	LOSS [training: 0.21839487762784096 | validation: 0.1242738732050586]
	TIME [epoch: 8.2 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17309225586110089		[learning rate: 0.0053551]
		[batch 20/20] avg loss: 0.20216273240722468		[learning rate: 0.0053488]
	Learning Rate: 0.0053488
	LOSS [training: 0.18762749413416277 | validation: 0.10625674314460433]
	TIME [epoch: 8.22 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1609148790947113		[learning rate: 0.0053425]
		[batch 20/20] avg loss: 0.21293903387655227		[learning rate: 0.0053362]
	Learning Rate: 0.00533618
	LOSS [training: 0.1869269564856318 | validation: 0.12518115713578837]
	TIME [epoch: 8.21 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20819105230247245		[learning rate: 0.0053299]
		[batch 20/20] avg loss: 0.17720530553455527		[learning rate: 0.0053236]
	Learning Rate: 0.0053236
	LOSS [training: 0.19269817891851387 | validation: 0.11010367548442349]
	TIME [epoch: 8.22 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22126085952419516		[learning rate: 0.0053173]
		[batch 20/20] avg loss: 0.2219910571698714		[learning rate: 0.005311]
	Learning Rate: 0.00531104
	LOSS [training: 0.2216259583470333 | validation: 0.34951868290941074]
	TIME [epoch: 8.19 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17024802244183246		[learning rate: 0.0053048]
		[batch 20/20] avg loss: 0.19617979370203828		[learning rate: 0.0052985]
	Learning Rate: 0.00529851
	LOSS [training: 0.18321390807193533 | validation: 0.2513738089069495]
	TIME [epoch: 8.21 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25080573109534465		[learning rate: 0.0052923]
		[batch 20/20] avg loss: 0.20899019225727464		[learning rate: 0.005286]
	Learning Rate: 0.00528601
	LOSS [training: 0.22989796167630958 | validation: 0.1800863244000051]
	TIME [epoch: 8.21 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16796197221961387		[learning rate: 0.0052798]
		[batch 20/20] avg loss: 0.15529881091362335		[learning rate: 0.0052735]
	Learning Rate: 0.00527354
	LOSS [training: 0.1616303915666186 | validation: 0.22198859667634077]
	TIME [epoch: 8.21 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15782668992594123		[learning rate: 0.0052673]
		[batch 20/20] avg loss: 0.1861313563726607		[learning rate: 0.0052611]
	Learning Rate: 0.0052611
	LOSS [training: 0.17197902314930097 | validation: 0.17016672009312986]
	TIME [epoch: 8.19 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2456744190471254		[learning rate: 0.0052549]
		[batch 20/20] avg loss: 0.15057930418240292		[learning rate: 0.0052487]
	Learning Rate: 0.00524869
	LOSS [training: 0.19812686161476414 | validation: 0.1706272635293575]
	TIME [epoch: 8.22 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18802190706079197		[learning rate: 0.0052425]
		[batch 20/20] avg loss: 0.27139541602040534		[learning rate: 0.0052363]
	Learning Rate: 0.00523631
	LOSS [training: 0.2297086615405986 | validation: 0.12265216300618512]
	TIME [epoch: 8.21 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21449527040426558		[learning rate: 0.0052301]
		[batch 20/20] avg loss: 0.2392430172857806		[learning rate: 0.005224]
	Learning Rate: 0.00522396
	LOSS [training: 0.22686914384502305 | validation: 0.1979874585491191]
	TIME [epoch: 8.21 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18569550738814772		[learning rate: 0.0052178]
		[batch 20/20] avg loss: 0.22071425760927527		[learning rate: 0.0052116]
	Learning Rate: 0.00521164
	LOSS [training: 0.20320488249871152 | validation: 0.19760641088869849]
	TIME [epoch: 8.19 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21363175137833315		[learning rate: 0.0052055]
		[batch 20/20] avg loss: 0.1522296166950278		[learning rate: 0.0051993]
	Learning Rate: 0.00519935
	LOSS [training: 0.18293068403668047 | validation: 0.08017488539475182]
	TIME [epoch: 8.23 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15388769498209667		[learning rate: 0.0051932]
		[batch 20/20] avg loss: 0.17458435924503427		[learning rate: 0.0051871]
	Learning Rate: 0.00518708
	LOSS [training: 0.16423602711356547 | validation: 0.365193245070513]
	TIME [epoch: 8.23 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2536062006320775		[learning rate: 0.005181]
		[batch 20/20] avg loss: 0.1739581257094725		[learning rate: 0.0051748]
	Learning Rate: 0.00517485
	LOSS [training: 0.21378216317077503 | validation: 0.18876872778138887]
	TIME [epoch: 8.21 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19256969465398763		[learning rate: 0.0051687]
		[batch 20/20] avg loss: 0.18075693239227053		[learning rate: 0.0051626]
	Learning Rate: 0.00516264
	LOSS [training: 0.18666331352312907 | validation: 0.15305253128717083]
	TIME [epoch: 8.2 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15639282793834988		[learning rate: 0.0051565]
		[batch 20/20] avg loss: 0.1640710768229192		[learning rate: 0.0051505]
	Learning Rate: 0.00515046
	LOSS [training: 0.16023195238063453 | validation: 0.12678533444323142]
	TIME [epoch: 8.22 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21897330967111847		[learning rate: 0.0051444]
		[batch 20/20] avg loss: 0.18031486492425325		[learning rate: 0.0051383]
	Learning Rate: 0.00513831
	LOSS [training: 0.19964408729768585 | validation: 0.12757298595815364]
	TIME [epoch: 8.23 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20059728043863423		[learning rate: 0.0051322]
		[batch 20/20] avg loss: 0.2328116105745741		[learning rate: 0.0051262]
	Learning Rate: 0.00512619
	LOSS [training: 0.21670444550660412 | validation: 0.07469001715059033]
	TIME [epoch: 8.2 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18976628516436536		[learning rate: 0.0051201]
		[batch 20/20] avg loss: 0.19257733082444825		[learning rate: 0.0051141]
	Learning Rate: 0.0051141
	LOSS [training: 0.19117180799440683 | validation: 0.21202567811681866]
	TIME [epoch: 8.19 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18969994212328795		[learning rate: 0.0051081]
		[batch 20/20] avg loss: 0.2290801786384748		[learning rate: 0.005102]
	Learning Rate: 0.00510204
	LOSS [training: 0.20939006038088137 | validation: 0.19356873606880393]
	TIME [epoch: 8.22 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16207159380961011		[learning rate: 0.005096]
		[batch 20/20] avg loss: 0.2000388401429501		[learning rate: 0.00509]
	Learning Rate: 0.00509
	LOSS [training: 0.18105521697628008 | validation: 0.2175354632396394]
	TIME [epoch: 8.22 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17860637799773507		[learning rate: 0.005084]
		[batch 20/20] avg loss: 0.1955255919404324		[learning rate: 0.005078]
	Learning Rate: 0.00507799
	LOSS [training: 0.18706598496908372 | validation: 0.16029050064476175]
	TIME [epoch: 8.2 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15555581987988626		[learning rate: 0.005072]
		[batch 20/20] avg loss: 0.2434467652641336		[learning rate: 0.005066]
	Learning Rate: 0.00506602
	LOSS [training: 0.19950129257200994 | validation: 0.10416009412279346]
	TIME [epoch: 8.19 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26562934394853616		[learning rate: 0.00506]
		[batch 20/20] avg loss: 0.2347271367947863		[learning rate: 0.0050541]
	Learning Rate: 0.00505407
	LOSS [training: 0.2501782403716613 | validation: 0.13448007656941927]
	TIME [epoch: 8.22 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21466303449490315		[learning rate: 0.0050481]
		[batch 20/20] avg loss: 0.16511034167994806		[learning rate: 0.0050421]
	Learning Rate: 0.00504215
	LOSS [training: 0.18988668808742562 | validation: 0.1917060633314918]
	TIME [epoch: 8.22 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2138947886767799		[learning rate: 0.0050362]
		[batch 20/20] avg loss: 0.1523150317790512		[learning rate: 0.0050303]
	Learning Rate: 0.00503025
	LOSS [training: 0.18310491022791558 | validation: 0.18259784321190642]
	TIME [epoch: 8.21 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17870090180643075		[learning rate: 0.0050243]
		[batch 20/20] avg loss: 0.21711317021976817		[learning rate: 0.0050184]
	Learning Rate: 0.00501839
	LOSS [training: 0.19790703601309945 | validation: 0.22215688136071715]
	TIME [epoch: 8.2 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17756520861968977		[learning rate: 0.0050125]
		[batch 20/20] avg loss: 0.16873679580780926		[learning rate: 0.0050065]
	Learning Rate: 0.00500655
	LOSS [training: 0.17315100221374952 | validation: 0.08655224751773193]
	TIME [epoch: 8.22 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20518331219108515		[learning rate: 0.0050006]
		[batch 20/20] avg loss: 0.16398555460753247		[learning rate: 0.0049947]
	Learning Rate: 0.00499474
	LOSS [training: 0.1845844333993088 | validation: 0.054747230410095556]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_344.pth
	Model improved!!!
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20178475717385527		[learning rate: 0.0049888]
		[batch 20/20] avg loss: 0.19295520286876683		[learning rate: 0.004983]
	Learning Rate: 0.00498296
	LOSS [training: 0.19736998002131106 | validation: 0.08972357837342071]
	TIME [epoch: 8.2 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15894415915982701		[learning rate: 0.0049771]
		[batch 20/20] avg loss: 0.2267601384213124		[learning rate: 0.0049712]
	Learning Rate: 0.0049712
	LOSS [training: 0.19285214879056975 | validation: 0.0801158048440441]
	TIME [epoch: 8.19 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24006471208348654		[learning rate: 0.0049653]
		[batch 20/20] avg loss: 0.1381223435721495		[learning rate: 0.0049595]
	Learning Rate: 0.00495948
	LOSS [training: 0.18909352782781802 | validation: 0.11025797642108214]
	TIME [epoch: 8.22 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14932115506562332		[learning rate: 0.0049536]
		[batch 20/20] avg loss: 0.2319258770945855		[learning rate: 0.0049478]
	Learning Rate: 0.00494778
	LOSS [training: 0.1906235160801044 | validation: 0.22894106990250407]
	TIME [epoch: 8.23 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21260356761641813		[learning rate: 0.0049419]
		[batch 20/20] avg loss: 0.2801191680926062		[learning rate: 0.0049361]
	Learning Rate: 0.00493611
	LOSS [training: 0.24636136785451215 | validation: 0.3963959063713156]
	TIME [epoch: 8.2 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21282649962590222		[learning rate: 0.0049303]
		[batch 20/20] avg loss: 0.17979895125460138		[learning rate: 0.0049245]
	Learning Rate: 0.00492446
	LOSS [training: 0.19631272544025177 | validation: 0.2114976138073677]
	TIME [epoch: 8.19 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22343406983821734		[learning rate: 0.0049187]
		[batch 20/20] avg loss: 0.14609067622129981		[learning rate: 0.0049128]
	Learning Rate: 0.00491285
	LOSS [training: 0.18476237302975856 | validation: 0.26266308247338443]
	TIME [epoch: 8.23 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21631488066543705		[learning rate: 0.0049071]
		[batch 20/20] avg loss: 0.17799471534232753		[learning rate: 0.0049013]
	Learning Rate: 0.00490126
	LOSS [training: 0.19715479800388228 | validation: 0.16199936618807817]
	TIME [epoch: 8.24 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18990185519805197		[learning rate: 0.0048955]
		[batch 20/20] avg loss: 0.3373531285515673		[learning rate: 0.0048897]
	Learning Rate: 0.0048897
	LOSS [training: 0.2636274918748097 | validation: 0.08517120778249394]
	TIME [epoch: 8.21 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.464826201249717		[learning rate: 0.0048839]
		[batch 20/20] avg loss: 0.1617980093431538		[learning rate: 0.0048782]
	Learning Rate: 0.00487816
	LOSS [training: 0.3133121052964354 | validation: 0.18439179678823853]
	TIME [epoch: 8.2 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18182346015782158		[learning rate: 0.0048724]
		[batch 20/20] avg loss: 0.12872616835217698		[learning rate: 0.0048667]
	Learning Rate: 0.00486666
	LOSS [training: 0.15527481425499925 | validation: 0.3738251140659243]
	TIME [epoch: 8.22 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21766695641190306		[learning rate: 0.0048609]
		[batch 20/20] avg loss: 0.1642399280312719		[learning rate: 0.0048552]
	Learning Rate: 0.00485518
	LOSS [training: 0.19095344222158747 | validation: 0.09517572343100891]
	TIME [epoch: 8.24 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24039036080738554		[learning rate: 0.0048494]
		[batch 20/20] avg loss: 0.23709558228225075		[learning rate: 0.0048437]
	Learning Rate: 0.00484372
	LOSS [training: 0.23874297154481816 | validation: 0.45933021087748926]
	TIME [epoch: 8.2 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1914242419271895		[learning rate: 0.004838]
		[batch 20/20] avg loss: 0.14301427951737017		[learning rate: 0.0048323]
	Learning Rate: 0.0048323
	LOSS [training: 0.16721926072227977 | validation: 0.1994110233273541]
	TIME [epoch: 8.2 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15392811030641293		[learning rate: 0.0048266]
		[batch 20/20] avg loss: 0.22635856423720857		[learning rate: 0.0048209]
	Learning Rate: 0.0048209
	LOSS [training: 0.19014333727181074 | validation: 0.5456761012991473]
	TIME [epoch: 8.22 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19757122349167847		[learning rate: 0.0048152]
		[batch 20/20] avg loss: 0.17920836174364643		[learning rate: 0.0048095]
	Learning Rate: 0.00480953
	LOSS [training: 0.18838979261766245 | validation: 0.149442232999162]
	TIME [epoch: 8.24 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16004067629865085		[learning rate: 0.0048039]
		[batch 20/20] avg loss: 0.1473818982274189		[learning rate: 0.0047982]
	Learning Rate: 0.00479818
	LOSS [training: 0.15371128726303487 | validation: 0.21134000382997176]
	TIME [epoch: 8.19 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20538379173249557		[learning rate: 0.0047925]
		[batch 20/20] avg loss: 0.24265439616305856		[learning rate: 0.0047869]
	Learning Rate: 0.00478687
	LOSS [training: 0.22401909394777703 | validation: 0.2052935001390948]
	TIME [epoch: 8.2 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19298858282076886		[learning rate: 0.0047812]
		[batch 20/20] avg loss: 0.15250801132304134		[learning rate: 0.0047756]
	Learning Rate: 0.00477557
	LOSS [training: 0.17274829707190512 | validation: 0.39397999197234235]
	TIME [epoch: 8.22 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16805747034339652		[learning rate: 0.0047699]
		[batch 20/20] avg loss: 0.1535954795845801		[learning rate: 0.0047643]
	Learning Rate: 0.00476431
	LOSS [training: 0.16082647496398833 | validation: 0.18523473285302758]
	TIME [epoch: 8.23 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15865376052147187		[learning rate: 0.0047587]
		[batch 20/20] avg loss: 0.14790363891119723		[learning rate: 0.0047531]
	Learning Rate: 0.00475307
	LOSS [training: 0.1532786997163345 | validation: 0.17364812684993075]
	TIME [epoch: 8.2 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20848338969376806		[learning rate: 0.0047475]
		[batch 20/20] avg loss: 0.19841426223504519		[learning rate: 0.0047419]
	Learning Rate: 0.00474186
	LOSS [training: 0.20344882596440664 | validation: 0.12857868703393016]
	TIME [epoch: 8.19 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1688045911738306		[learning rate: 0.0047363]
		[batch 20/20] avg loss: 0.14961130112278317		[learning rate: 0.0047307]
	Learning Rate: 0.00473067
	LOSS [training: 0.15920794614830688 | validation: 0.1399180160143051]
	TIME [epoch: 8.24 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21893567624664537		[learning rate: 0.0047251]
		[batch 20/20] avg loss: 0.17228830241606619		[learning rate: 0.0047195]
	Learning Rate: 0.00471952
	LOSS [training: 0.19561198933135576 | validation: 0.08747710992232395]
	TIME [epoch: 8.23 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11819133158086059		[learning rate: 0.0047139]
		[batch 20/20] avg loss: 0.24707729237642967		[learning rate: 0.0047084]
	Learning Rate: 0.00470838
	LOSS [training: 0.18263431197864516 | validation: 0.13460488328261805]
	TIME [epoch: 8.19 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1324852099583154		[learning rate: 0.0047028]
		[batch 20/20] avg loss: 0.27688536501475886		[learning rate: 0.0046973]
	Learning Rate: 0.00469728
	LOSS [training: 0.2046852874865371 | validation: 0.10465587030863893]
	TIME [epoch: 8.19 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1447018179156537		[learning rate: 0.0046917]
		[batch 20/20] avg loss: 0.24301920643119654		[learning rate: 0.0046862]
	Learning Rate: 0.0046862
	LOSS [training: 0.19386051217342512 | validation: 0.1894043905290915]
	TIME [epoch: 8.23 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15122531308875825		[learning rate: 0.0046807]
		[batch 20/20] avg loss: 0.12841303987819014		[learning rate: 0.0046751]
	Learning Rate: 0.00467514
	LOSS [training: 0.1398191764834742 | validation: 0.10624988497434322]
	TIME [epoch: 8.22 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15784851287441395		[learning rate: 0.0046696]
		[batch 20/20] avg loss: 0.19343847823828803		[learning rate: 0.0046641]
	Learning Rate: 0.00466411
	LOSS [training: 0.17564349555635098 | validation: 0.1560591563691463]
	TIME [epoch: 8.2 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13958345649092824		[learning rate: 0.0046586]
		[batch 20/20] avg loss: 0.2662778729981261		[learning rate: 0.0046531]
	Learning Rate: 0.00465311
	LOSS [training: 0.20293066474452717 | validation: 0.1943210022466679]
	TIME [epoch: 8.19 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1899880383862002		[learning rate: 0.0046476]
		[batch 20/20] avg loss: 0.13305099809273693		[learning rate: 0.0046421]
	Learning Rate: 0.00464214
	LOSS [training: 0.16151951823946858 | validation: 0.0983812039266637]
	TIME [epoch: 8.21 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22785840819680261		[learning rate: 0.0046367]
		[batch 20/20] avg loss: 0.14047452502208657		[learning rate: 0.0046312]
	Learning Rate: 0.00463119
	LOSS [training: 0.18416646660944458 | validation: 0.12017162425857023]
	TIME [epoch: 8.24 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1721176619663644		[learning rate: 0.0046257]
		[batch 20/20] avg loss: 0.22725754720979313		[learning rate: 0.0046203]
	Learning Rate: 0.00462026
	LOSS [training: 0.19968760458807877 | validation: 0.0936235045799648]
	TIME [epoch: 8.19 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1753600198783858		[learning rate: 0.0046148]
		[batch 20/20] avg loss: 0.17945997678564915		[learning rate: 0.0046094]
	Learning Rate: 0.00460936
	LOSS [training: 0.17740999833201748 | validation: 0.160091596526945]
	TIME [epoch: 8.19 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17999722807430546		[learning rate: 0.0046039]
		[batch 20/20] avg loss: 0.21401455948913553		[learning rate: 0.0045985]
	Learning Rate: 0.00459849
	LOSS [training: 0.19700589378172054 | validation: 0.10578866897530532]
	TIME [epoch: 8.22 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17024363315593424		[learning rate: 0.0045931]
		[batch 20/20] avg loss: 0.1684368205494431		[learning rate: 0.0045876]
	Learning Rate: 0.00458764
	LOSS [training: 0.1693402268526887 | validation: 0.20403169464634632]
	TIME [epoch: 8.24 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20102707653532031		[learning rate: 0.0045822]
		[batch 20/20] avg loss: 0.20470356572135523		[learning rate: 0.0045768]
	Learning Rate: 0.00457682
	LOSS [training: 0.20286532112833783 | validation: 0.06413807247777537]
	TIME [epoch: 8.19 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17422101331509138		[learning rate: 0.0045714]
		[batch 20/20] avg loss: 0.17924448455547606		[learning rate: 0.004566]
	Learning Rate: 0.00456603
	LOSS [training: 0.17673274893528373 | validation: 0.12179208736604308]
	TIME [epoch: 8.2 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14946039507702227		[learning rate: 0.0045606]
		[batch 20/20] avg loss: 0.18281805311575575		[learning rate: 0.0045553]
	Learning Rate: 0.00455526
	LOSS [training: 0.166139224096389 | validation: 0.06310420040847302]
	TIME [epoch: 8.23 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2339904933034655		[learning rate: 0.0045499]
		[batch 20/20] avg loss: 0.16593352171335934		[learning rate: 0.0045445]
	Learning Rate: 0.00454451
	LOSS [training: 0.19996200750841245 | validation: 0.2382890711367794]
	TIME [epoch: 8.21 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1646259406926009		[learning rate: 0.0045391]
		[batch 20/20] avg loss: 0.1621932212197474		[learning rate: 0.0045338]
	Learning Rate: 0.00453379
	LOSS [training: 0.16340958095617414 | validation: 0.18526046856591638]
	TIME [epoch: 8.19 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31363468340892675		[learning rate: 0.0045284]
		[batch 20/20] avg loss: 0.2281796083247792		[learning rate: 0.0045231]
	Learning Rate: 0.0045231
	LOSS [training: 0.270907145866853 | validation: 0.13442920562153837]
	TIME [epoch: 8.19 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13098950458069858		[learning rate: 0.0045178]
		[batch 20/20] avg loss: 0.16534207205207957		[learning rate: 0.0045124]
	Learning Rate: 0.00451243
	LOSS [training: 0.14816578831638905 | validation: 0.08891015970087501]
	TIME [epoch: 8.24 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19167424337838815		[learning rate: 0.0045071]
		[batch 20/20] avg loss: 0.22418968747269022		[learning rate: 0.0045018]
	Learning Rate: 0.00450178
	LOSS [training: 0.2079319654255392 | validation: 0.1047107653304116]
	TIME [epoch: 8.22 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16380849486599197		[learning rate: 0.0044965]
		[batch 20/20] avg loss: 0.18844192533516702		[learning rate: 0.0044912]
	Learning Rate: 0.00449116
	LOSS [training: 0.17612521010057952 | validation: 0.11855641546387974]
	TIME [epoch: 8.19 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1747062548620161		[learning rate: 0.0044859]
		[batch 20/20] avg loss: 0.15562282118463838		[learning rate: 0.0044806]
	Learning Rate: 0.00448057
	LOSS [training: 0.16516453802332723 | validation: 0.2632192269271975]
	TIME [epoch: 8.19 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24532216135028131		[learning rate: 0.0044753]
		[batch 20/20] avg loss: 0.17611096244150837		[learning rate: 0.00447]
	Learning Rate: 0.00447
	LOSS [training: 0.21071656189589488 | validation: 0.10167645882165374]
	TIME [epoch: 8.22 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18234472948989816		[learning rate: 0.0044647]
		[batch 20/20] avg loss: 0.17735327583583388		[learning rate: 0.0044595]
	Learning Rate: 0.00445946
	LOSS [training: 0.17984900266286602 | validation: 0.06868324971063866]
	TIME [epoch: 8.22 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1446928725629854		[learning rate: 0.0044542]
		[batch 20/20] avg loss: 0.18071758427778614		[learning rate: 0.0044489]
	Learning Rate: 0.00444894
	LOSS [training: 0.16270522842038576 | validation: 0.15009845136415584]
	TIME [epoch: 8.19 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.189795446217499		[learning rate: 0.0044437]
		[batch 20/20] avg loss: 0.21353651686895145		[learning rate: 0.0044384]
	Learning Rate: 0.00443844
	LOSS [training: 0.2016659815432252 | validation: 0.12020001130433008]
	TIME [epoch: 8.18 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17967232401946626		[learning rate: 0.0044332]
		[batch 20/20] avg loss: 0.18941512040810365		[learning rate: 0.004428]
	Learning Rate: 0.00442797
	LOSS [training: 0.18454372221378496 | validation: 0.11175049280292848]
	TIME [epoch: 8.21 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16109405114967115		[learning rate: 0.0044227]
		[batch 20/20] avg loss: 0.18403282071656077		[learning rate: 0.0044175]
	Learning Rate: 0.00441753
	LOSS [training: 0.17256343593311596 | validation: 0.19011525492904058]
	TIME [epoch: 8.24 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1393555649120927		[learning rate: 0.0044123]
		[batch 20/20] avg loss: 0.1397257548957615		[learning rate: 0.0044071]
	Learning Rate: 0.00440711
	LOSS [training: 0.13954065990392708 | validation: 0.08747059811988166]
	TIME [epoch: 8.2 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1295440998766346		[learning rate: 0.0044019]
		[batch 20/20] avg loss: 0.3738473009104829		[learning rate: 0.0043967]
	Learning Rate: 0.00439671
	LOSS [training: 0.25169570039355876 | validation: 0.3531651881819279]
	TIME [epoch: 8.2 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15410269551248237		[learning rate: 0.0043915]
		[batch 20/20] avg loss: 0.17154908931585236		[learning rate: 0.0043863]
	Learning Rate: 0.00438634
	LOSS [training: 0.1628258924141674 | validation: 0.09071601375033811]
	TIME [epoch: 8.22 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22581270200394518		[learning rate: 0.0043812]
		[batch 20/20] avg loss: 0.14316290510137608		[learning rate: 0.004376]
	Learning Rate: 0.004376
	LOSS [training: 0.18448780355266062 | validation: 0.22933418359284014]
	TIME [epoch: 8.23 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16426257732728133		[learning rate: 0.0043708]
		[batch 20/20] avg loss: 0.18390390179103996		[learning rate: 0.0043657]
	Learning Rate: 0.00436567
	LOSS [training: 0.1740832395591606 | validation: 0.21340263861955258]
	TIME [epoch: 8.2 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15090693125418028		[learning rate: 0.0043605]
		[batch 20/20] avg loss: 0.18740872157886534		[learning rate: 0.0043554]
	Learning Rate: 0.00435538
	LOSS [training: 0.16915782641652277 | validation: 0.16205546981122762]
	TIME [epoch: 8.2 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15967556189771662		[learning rate: 0.0043502]
		[batch 20/20] avg loss: 0.17659935029609325		[learning rate: 0.0043451]
	Learning Rate: 0.0043451
	LOSS [training: 0.16813745609690492 | validation: 0.10394879619184313]
	TIME [epoch: 8.22 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14096645063546936		[learning rate: 0.00434]
		[batch 20/20] avg loss: 0.1694595296063062		[learning rate: 0.0043349]
	Learning Rate: 0.00433485
	LOSS [training: 0.1552129901208878 | validation: 0.0717833376104558]
	TIME [epoch: 8.23 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10192886692468042		[learning rate: 0.0043297]
		[batch 20/20] avg loss: 0.16488030026333775		[learning rate: 0.0043246]
	Learning Rate: 0.00432463
	LOSS [training: 0.13340458359400909 | validation: 0.161766127138908]
	TIME [epoch: 8.2 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17231529390002806		[learning rate: 0.0043195]
		[batch 20/20] avg loss: 0.19158981688854287		[learning rate: 0.0043144]
	Learning Rate: 0.00431443
	LOSS [training: 0.18195255539428545 | validation: 0.1874375077825193]
	TIME [epoch: 8.2 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21127432241073904		[learning rate: 0.0043093]
		[batch 20/20] avg loss: 0.14092712840249913		[learning rate: 0.0043042]
	Learning Rate: 0.00430425
	LOSS [training: 0.17610072540661909 | validation: 0.13544551786266945]
	TIME [epoch: 8.23 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21255972189062913		[learning rate: 0.0042992]
		[batch 20/20] avg loss: 0.1436227362001216		[learning rate: 0.0042941]
	Learning Rate: 0.0042941
	LOSS [training: 0.1780912290453754 | validation: 0.19589546070469716]
	TIME [epoch: 8.24 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1656343070295149		[learning rate: 0.004289]
		[batch 20/20] avg loss: 0.18485125496259264		[learning rate: 0.004284]
	Learning Rate: 0.00428397
	LOSS [training: 0.17524278099605375 | validation: 0.0730617622879447]
	TIME [epoch: 8.2 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22485377428335057		[learning rate: 0.0042789]
		[batch 20/20] avg loss: 0.14522303706175346		[learning rate: 0.0042739]
	Learning Rate: 0.00427386
	LOSS [training: 0.18503840567255203 | validation: 0.1066726634833885]
	TIME [epoch: 8.2 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19248797465090378		[learning rate: 0.0042688]
		[batch 20/20] avg loss: 0.12048392951642065		[learning rate: 0.0042638]
	Learning Rate: 0.00426378
	LOSS [training: 0.15648595208366226 | validation: 0.05197104673350111]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18482421240555888		[learning rate: 0.0042587]
		[batch 20/20] avg loss: 0.16696840876124197		[learning rate: 0.0042537]
	Learning Rate: 0.00425372
	LOSS [training: 0.17589631058340044 | validation: 0.14481335215117164]
	TIME [epoch: 8.24 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10945511356461837		[learning rate: 0.0042487]
		[batch 20/20] avg loss: 0.15650647798008116		[learning rate: 0.0042437]
	Learning Rate: 0.00424369
	LOSS [training: 0.13298079577234975 | validation: 0.07039999207172809]
	TIME [epoch: 8.2 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20939688719869073		[learning rate: 0.0042387]
		[batch 20/20] avg loss: 0.19087184919608172		[learning rate: 0.0042337]
	Learning Rate: 0.00423368
	LOSS [training: 0.20013436819738623 | validation: 0.08765228596652777]
	TIME [epoch: 8.19 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15046669384260408		[learning rate: 0.0042287]
		[batch 20/20] avg loss: 0.1288852070284065		[learning rate: 0.0042237]
	Learning Rate: 0.00422369
	LOSS [training: 0.1396759504355053 | validation: 0.05267773930594721]
	TIME [epoch: 8.22 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1455786536683446		[learning rate: 0.0042187]
		[batch 20/20] avg loss: 0.19112144903772693		[learning rate: 0.0042137]
	Learning Rate: 0.00421373
	LOSS [training: 0.16835005135303577 | validation: 0.08086205857470094]
	TIME [epoch: 8.25 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14154621585955215		[learning rate: 0.0042088]
		[batch 20/20] avg loss: 0.1442861504212405		[learning rate: 0.0042038]
	Learning Rate: 0.00420379
	LOSS [training: 0.14291618314039634 | validation: 0.1917047235844587]
	TIME [epoch: 8.2 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18676419600840583		[learning rate: 0.0041988]
		[batch 20/20] avg loss: 0.14089548912637023		[learning rate: 0.0041939]
	Learning Rate: 0.00419387
	LOSS [training: 0.16382984256738797 | validation: 0.17836207011652463]
	TIME [epoch: 8.2 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15860603437148285		[learning rate: 0.0041889]
		[batch 20/20] avg loss: 0.23691873297186822		[learning rate: 0.004184]
	Learning Rate: 0.00418398
	LOSS [training: 0.19776238367167556 | validation: 0.13749043325955032]
	TIME [epoch: 8.23 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15272061967569298		[learning rate: 0.004179]
		[batch 20/20] avg loss: 0.14551303943908728		[learning rate: 0.0041741]
	Learning Rate: 0.00417411
	LOSS [training: 0.1491168295573901 | validation: 0.21568191168308262]
	TIME [epoch: 8.25 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18372348410578548		[learning rate: 0.0041692]
		[batch 20/20] avg loss: 0.1707934036264167		[learning rate: 0.0041643]
	Learning Rate: 0.00416427
	LOSS [training: 0.1772584438661011 | validation: 0.0669172463488199]
	TIME [epoch: 8.2 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14342657906619918		[learning rate: 0.0041594]
		[batch 20/20] avg loss: 0.20589665822744857		[learning rate: 0.0041544]
	Learning Rate: 0.00415444
	LOSS [training: 0.1746616186468239 | validation: 0.0706369437154064]
	TIME [epoch: 8.2 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15242570243578174		[learning rate: 0.0041495]
		[batch 20/20] avg loss: 0.161986888814006		[learning rate: 0.0041446]
	Learning Rate: 0.00414464
	LOSS [training: 0.15720629562489388 | validation: 0.18671051668246763]
	TIME [epoch: 8.23 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24304877019230292		[learning rate: 0.0041398]
		[batch 20/20] avg loss: 0.144951467280618		[learning rate: 0.0041349]
	Learning Rate: 0.00413487
	LOSS [training: 0.19400011873646045 | validation: 0.09483615221589761]
	TIME [epoch: 8.24 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1539551512950972		[learning rate: 0.00413]
		[batch 20/20] avg loss: 0.14099279894233033		[learning rate: 0.0041251]
	Learning Rate: 0.00412511
	LOSS [training: 0.1474739751187138 | validation: 0.1162176439593597]
	TIME [epoch: 8.19 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1966446109555074		[learning rate: 0.0041202]
		[batch 20/20] avg loss: 0.11837104657276054		[learning rate: 0.0041154]
	Learning Rate: 0.00411538
	LOSS [training: 0.15750782876413397 | validation: 0.1032460908701276]
	TIME [epoch: 8.2 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15145062831324213		[learning rate: 0.0041105]
		[batch 20/20] avg loss: 0.15760398193396083		[learning rate: 0.0041057]
	Learning Rate: 0.00410568
	LOSS [training: 0.1545273051236015 | validation: 0.217849812733374]
	TIME [epoch: 8.24 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12350358914404891		[learning rate: 0.0041008]
		[batch 20/20] avg loss: 0.21243956426961033		[learning rate: 0.004096]
	Learning Rate: 0.00409599
	LOSS [training: 0.1679715767068296 | validation: 0.3935176873859166]
	TIME [epoch: 8.21 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15751361027354321		[learning rate: 0.0040912]
		[batch 20/20] avg loss: 0.15067833899310826		[learning rate: 0.0040863]
	Learning Rate: 0.00408633
	LOSS [training: 0.15409597463332572 | validation: 0.16863840564554736]
	TIME [epoch: 8.2 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2527208878244663		[learning rate: 0.0040815]
		[batch 20/20] avg loss: 0.21132768741813637		[learning rate: 0.0040767]
	Learning Rate: 0.00407669
	LOSS [training: 0.2320242876213013 | validation: 0.24575798219207445]
	TIME [epoch: 8.19 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16414907087506364		[learning rate: 0.0040719]
		[batch 20/20] avg loss: 0.11962344499783106		[learning rate: 0.0040671]
	Learning Rate: 0.00406707
	LOSS [training: 0.14188625793644738 | validation: 0.055356321840138306]
	TIME [epoch: 8.23 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2041991972167812		[learning rate: 0.0040623]
		[batch 20/20] avg loss: 0.12766563094220537		[learning rate: 0.0040575]
	Learning Rate: 0.00405748
	LOSS [training: 0.16593241407949327 | validation: 0.06298520938259135]
	TIME [epoch: 8.23 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20156867393679762		[learning rate: 0.0040527]
		[batch 20/20] avg loss: 0.2164857753985519		[learning rate: 0.0040479]
	Learning Rate: 0.00404791
	LOSS [training: 0.2090272246676747 | validation: 0.0909299273451433]
	TIME [epoch: 8.19 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17110802383863452		[learning rate: 0.0040431]
		[batch 20/20] avg loss: 0.15401361999918034		[learning rate: 0.0040384]
	Learning Rate: 0.00403836
	LOSS [training: 0.16256082191890747 | validation: 0.09782206621999866]
	TIME [epoch: 8.19 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15277405748765185		[learning rate: 0.0040336]
		[batch 20/20] avg loss: 0.1364938634039341		[learning rate: 0.0040288]
	Learning Rate: 0.00402883
	LOSS [training: 0.14463396044579296 | validation: 0.15860463444338777]
	TIME [epoch: 8.22 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12517684824986075		[learning rate: 0.0040241]
		[batch 20/20] avg loss: 0.15184472080030081		[learning rate: 0.0040193]
	Learning Rate: 0.00401933
	LOSS [training: 0.13851078452508078 | validation: 0.11558325164108046]
	TIME [epoch: 8.22 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13673073652587098		[learning rate: 0.0040146]
		[batch 20/20] avg loss: 0.14523369864701488		[learning rate: 0.0040099]
	Learning Rate: 0.00400985
	LOSS [training: 0.14098221758644297 | validation: 0.11356528462725839]
	TIME [epoch: 8.2 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15345480610860862		[learning rate: 0.0040051]
		[batch 20/20] avg loss: 0.13090563480650313		[learning rate: 0.0040004]
	Learning Rate: 0.00400039
	LOSS [training: 0.14218022045755585 | validation: 0.11558481399689352]
	TIME [epoch: 8.2 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15081763241502416		[learning rate: 0.0039957]
		[batch 20/20] avg loss: 0.23420213222279243		[learning rate: 0.003991]
	Learning Rate: 0.00399096
	LOSS [training: 0.19250988231890825 | validation: 0.11600268109365731]
	TIME [epoch: 8.24 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13461189502227974		[learning rate: 0.0039862]
		[batch 20/20] avg loss: 0.1699393290382282		[learning rate: 0.0039815]
	Learning Rate: 0.00398154
	LOSS [training: 0.15227561203025394 | validation: 0.1938858639433511]
	TIME [epoch: 8.22 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13319347701247408		[learning rate: 0.0039768]
		[batch 20/20] avg loss: 0.1737147314702801		[learning rate: 0.0039721]
	Learning Rate: 0.00397215
	LOSS [training: 0.15345410424137712 | validation: 0.08820289863174229]
	TIME [epoch: 8.2 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12508827262524555		[learning rate: 0.0039675]
		[batch 20/20] avg loss: 0.15755435894602646		[learning rate: 0.0039628]
	Learning Rate: 0.00396278
	LOSS [training: 0.14132131578563606 | validation: 0.09089662428863579]
	TIME [epoch: 8.2 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1193020038897666		[learning rate: 0.0039581]
		[batch 20/20] avg loss: 0.231877746248268		[learning rate: 0.0039534]
	Learning Rate: 0.00395343
	LOSS [training: 0.17558987506901727 | validation: 0.14216761419926788]
	TIME [epoch: 8.23 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1742384017275966		[learning rate: 0.0039488]
		[batch 20/20] avg loss: 0.11515769019175208		[learning rate: 0.0039441]
	Learning Rate: 0.00394411
	LOSS [training: 0.14469804595967437 | validation: 0.06632312989604315]
	TIME [epoch: 8.23 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18434228638538655		[learning rate: 0.0039395]
		[batch 20/20] avg loss: 0.16844430297653462		[learning rate: 0.0039348]
	Learning Rate: 0.0039348
	LOSS [training: 0.17639329468096057 | validation: 0.21370644007951123]
	TIME [epoch: 8.2 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13327626096490014		[learning rate: 0.0039302]
		[batch 20/20] avg loss: 0.19142652955139988		[learning rate: 0.0039255]
	Learning Rate: 0.00392552
	LOSS [training: 0.16235139525815004 | validation: 0.05050067202937476]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12598039884384651		[learning rate: 0.0039209]
		[batch 20/20] avg loss: 0.14932638035246418		[learning rate: 0.0039163]
	Learning Rate: 0.00391626
	LOSS [training: 0.13765338959815535 | validation: 0.06586624729640385]
	TIME [epoch: 8.22 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11959543113118161		[learning rate: 0.0039116]
		[batch 20/20] avg loss: 0.12509290798948952		[learning rate: 0.003907]
	Learning Rate: 0.00390702
	LOSS [training: 0.12234416956033559 | validation: 0.0657543585591954]
	TIME [epoch: 8.21 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17696998480881282		[learning rate: 0.0039024]
		[batch 20/20] avg loss: 0.16068318066958387		[learning rate: 0.0038978]
	Learning Rate: 0.00389781
	LOSS [training: 0.16882658273919834 | validation: 0.196895184181116]
	TIME [epoch: 8.2 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1618391040209057		[learning rate: 0.0038932]
		[batch 20/20] avg loss: 0.14438304688446935		[learning rate: 0.0038886]
	Learning Rate: 0.00388861
	LOSS [training: 0.15311107545268754 | validation: 0.10200384517541347]
	TIME [epoch: 8.19 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14012786766283836		[learning rate: 0.003884]
		[batch 20/20] avg loss: 0.16888337472681936		[learning rate: 0.0038794]
	Learning Rate: 0.00387944
	LOSS [training: 0.15450562119482886 | validation: 0.041037677339277295]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15961204637243148		[learning rate: 0.0038749]
		[batch 20/20] avg loss: 0.20704754024616484		[learning rate: 0.0038703]
	Learning Rate: 0.00387029
	LOSS [training: 0.18332979330929816 | validation: 0.0634149983161072]
	TIME [epoch: 8.21 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1399287711492871		[learning rate: 0.0038657]
		[batch 20/20] avg loss: 0.13649901946148285		[learning rate: 0.0038612]
	Learning Rate: 0.00386116
	LOSS [training: 0.13821389530538497 | validation: 0.08646659851946448]
	TIME [epoch: 8.2 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14276358390403837		[learning rate: 0.0038566]
		[batch 20/20] avg loss: 0.12915879944999825		[learning rate: 0.0038521]
	Learning Rate: 0.00385205
	LOSS [training: 0.1359611916770183 | validation: 0.17702672778445888]
	TIME [epoch: 8.2 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15619819907859842		[learning rate: 0.0038475]
		[batch 20/20] avg loss: 0.17092015591946344		[learning rate: 0.003843]
	Learning Rate: 0.00384297
	LOSS [training: 0.1635591774990309 | validation: 0.08978179508295411]
	TIME [epoch: 8.2 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17598537047301377		[learning rate: 0.0038384]
		[batch 20/20] avg loss: 0.12094373790849747		[learning rate: 0.0038339]
	Learning Rate: 0.0038339
	LOSS [training: 0.14846455419075566 | validation: 0.13883707471505172]
	TIME [epoch: 8.22 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23740275439095476		[learning rate: 0.0038294]
		[batch 20/20] avg loss: 0.16526313008270838		[learning rate: 0.0038249]
	Learning Rate: 0.00382486
	LOSS [training: 0.20133294223683157 | validation: 0.06276149882818195]
	TIME [epoch: 8.19 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18943034651226776		[learning rate: 0.0038203]
		[batch 20/20] avg loss: 0.197092206241308		[learning rate: 0.0038158]
	Learning Rate: 0.00381584
	LOSS [training: 0.1932612763767879 | validation: 0.15865263009280517]
	TIME [epoch: 8.21 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13743938547941248		[learning rate: 0.0038113]
		[batch 20/20] avg loss: 0.1612313800699293		[learning rate: 0.0038068]
	Learning Rate: 0.00380684
	LOSS [training: 0.14933538277467087 | validation: 0.1468997723043766]
	TIME [epoch: 8.21 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1644519538879164		[learning rate: 0.0038023]
		[batch 20/20] avg loss: 0.1633126728646301		[learning rate: 0.0037979]
	Learning Rate: 0.00379786
	LOSS [training: 0.16388231337627326 | validation: 0.2892251917283188]
	TIME [epoch: 8.22 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18148592211905262		[learning rate: 0.0037934]
		[batch 20/20] avg loss: 0.2025686793664001		[learning rate: 0.0037889]
	Learning Rate: 0.0037889
	LOSS [training: 0.19202730074272636 | validation: 0.13903519680573703]
	TIME [epoch: 8.2 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14981859189238242		[learning rate: 0.0037844]
		[batch 20/20] avg loss: 0.1585126225390395		[learning rate: 0.00378]
	Learning Rate: 0.00377996
	LOSS [training: 0.15416560721571096 | validation: 0.209276192277212]
	TIME [epoch: 8.22 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16398696694219272		[learning rate: 0.0037755]
		[batch 20/20] avg loss: 0.15703425257998493		[learning rate: 0.003771]
	Learning Rate: 0.00377104
	LOSS [training: 0.16051060976108886 | validation: 0.17079428684442363]
	TIME [epoch: 8.21 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14958731859518562		[learning rate: 0.0037666]
		[batch 20/20] avg loss: 0.14779709712477512		[learning rate: 0.0037621]
	Learning Rate: 0.00376215
	LOSS [training: 0.1486922078599804 | validation: 0.18049417453327368]
	TIME [epoch: 8.22 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17793926161162954		[learning rate: 0.0037577]
		[batch 20/20] avg loss: 0.12306794447624918		[learning rate: 0.0037533]
	Learning Rate: 0.00375327
	LOSS [training: 0.15050360304393934 | validation: 0.46241717346402916]
	TIME [epoch: 8.19 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17705922322892817		[learning rate: 0.0037488]
		[batch 20/20] avg loss: 0.18751879501197494		[learning rate: 0.0037444]
	Learning Rate: 0.00374442
	LOSS [training: 0.18228900912045157 | validation: 0.14176742297966025]
	TIME [epoch: 8.22 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26611882081982363		[learning rate: 0.00374]
		[batch 20/20] avg loss: 0.13955412844644427		[learning rate: 0.0037356]
	Learning Rate: 0.00373559
	LOSS [training: 0.20283647463313387 | validation: 0.12435173304542646]
	TIME [epoch: 8.21 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14572021516569572		[learning rate: 0.0037312]
		[batch 20/20] avg loss: 0.2678352546239985		[learning rate: 0.0037268]
	Learning Rate: 0.00372678
	LOSS [training: 0.20677773489484713 | validation: 0.16586093754096273]
	TIME [epoch: 8.22 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15240697505593237		[learning rate: 0.0037224]
		[batch 20/20] avg loss: 0.1443856308876159		[learning rate: 0.003718]
	Learning Rate: 0.00371799
	LOSS [training: 0.14839630297177414 | validation: 0.18499205009193093]
	TIME [epoch: 8.2 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13165837670840222		[learning rate: 0.0037136]
		[batch 20/20] avg loss: 0.3104107959195626		[learning rate: 0.0037092]
	Learning Rate: 0.00370922
	LOSS [training: 0.22103458631398243 | validation: 0.273914852883097]
	TIME [epoch: 8.22 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17965198072735003		[learning rate: 0.0037048]
		[batch 20/20] avg loss: 0.1717661271235958		[learning rate: 0.0037005]
	Learning Rate: 0.00370047
	LOSS [training: 0.1757090539254729 | validation: 0.11748582745718164]
	TIME [epoch: 8.2 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1506315445576719		[learning rate: 0.0036961]
		[batch 20/20] avg loss: 0.19215186793880373		[learning rate: 0.0036917]
	Learning Rate: 0.00369174
	LOSS [training: 0.1713917062482378 | validation: 0.0855683545966075]
	TIME [epoch: 8.21 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14406641964208977		[learning rate: 0.0036874]
		[batch 20/20] avg loss: 0.1499112078747668		[learning rate: 0.003683]
	Learning Rate: 0.00368303
	LOSS [training: 0.14698881375842832 | validation: 0.09313977402764476]
	TIME [epoch: 8.2 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1819634687445562		[learning rate: 0.0036787]
		[batch 20/20] avg loss: 0.14276266679815305		[learning rate: 0.0036743]
	Learning Rate: 0.00367434
	LOSS [training: 0.16236306777135462 | validation: 0.15676052801849072]
	TIME [epoch: 8.22 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15466543077570502		[learning rate: 0.00367]
		[batch 20/20] avg loss: 0.16753641241429584		[learning rate: 0.0036657]
	Learning Rate: 0.00366567
	LOSS [training: 0.16110092159500045 | validation: 0.1464986491293585]
	TIME [epoch: 8.2 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17933044909230347		[learning rate: 0.0036613]
		[batch 20/20] avg loss: 0.18020373866802025		[learning rate: 0.003657]
	Learning Rate: 0.00365703
	LOSS [training: 0.17976709388016185 | validation: 0.12381004400500645]
	TIME [epoch: 8.21 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15532322487482805		[learning rate: 0.0036527]
		[batch 20/20] avg loss: 0.13291876930046947		[learning rate: 0.0036484]
	Learning Rate: 0.0036484
	LOSS [training: 0.14412099708764875 | validation: 0.0930523239509643]
	TIME [epoch: 8.19 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19301770384432917		[learning rate: 0.0036441]
		[batch 20/20] avg loss: 0.11420988024166609		[learning rate: 0.0036398]
	Learning Rate: 0.00363979
	LOSS [training: 0.15361379204299766 | validation: 0.06006986428049989]
	TIME [epoch: 8.23 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13632188951124907		[learning rate: 0.0036355]
		[batch 20/20] avg loss: 0.19975283464733778		[learning rate: 0.0036312]
	Learning Rate: 0.00363121
	LOSS [training: 0.16803736207929343 | validation: 0.2226716175501008]
	TIME [epoch: 8.19 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15153892866942772		[learning rate: 0.0036269]
		[batch 20/20] avg loss: 0.15252593004891654		[learning rate: 0.0036226]
	Learning Rate: 0.00362264
	LOSS [training: 0.15203242935917216 | validation: 0.09323509436305003]
	TIME [epoch: 8.21 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14516403559352903		[learning rate: 0.0036184]
		[batch 20/20] avg loss: 0.1403717656629448		[learning rate: 0.0036141]
	Learning Rate: 0.0036141
	LOSS [training: 0.1427679006282369 | validation: 0.16526868287059815]
	TIME [epoch: 8.2 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1362746714043161		[learning rate: 0.0036098]
		[batch 20/20] avg loss: 0.17364492440898002		[learning rate: 0.0036056]
	Learning Rate: 0.00360557
	LOSS [training: 0.15495979790664804 | validation: 0.08338138562786049]
	TIME [epoch: 8.23 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17456326184604326		[learning rate: 0.0036013]
		[batch 20/20] avg loss: 0.12391521266016106		[learning rate: 0.0035971]
	Learning Rate: 0.00359707
	LOSS [training: 0.14923923725310215 | validation: 0.12443604101988745]
	TIME [epoch: 8.21 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2055513900022083		[learning rate: 0.0035928]
		[batch 20/20] avg loss: 0.1800535538067556		[learning rate: 0.0035886]
	Learning Rate: 0.00358858
	LOSS [training: 0.1928024719044819 | validation: 0.0628451689662723]
	TIME [epoch: 8.21 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17891055900217362		[learning rate: 0.0035843]
		[batch 20/20] avg loss: 0.18801322075440752		[learning rate: 0.0035801]
	Learning Rate: 0.00358012
	LOSS [training: 0.18346188987829057 | validation: 0.35763372536563487]
	TIME [epoch: 8.2 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15910809083640381		[learning rate: 0.0035759]
		[batch 20/20] avg loss: 0.13816448947506912		[learning rate: 0.0035717]
	Learning Rate: 0.00357167
	LOSS [training: 0.14863629015573643 | validation: 0.10137407585815963]
	TIME [epoch: 8.23 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19438385669446429		[learning rate: 0.0035675]
		[batch 20/20] avg loss: 0.1897678284075581		[learning rate: 0.0035632]
	Learning Rate: 0.00356325
	LOSS [training: 0.19207584255101123 | validation: 0.08586815075823229]
	TIME [epoch: 8.2 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1658681482168433		[learning rate: 0.003559]
		[batch 20/20] avg loss: 0.1508188861815194		[learning rate: 0.0035548]
	Learning Rate: 0.00355484
	LOSS [training: 0.15834351719918136 | validation: 0.05054619242645603]
	TIME [epoch: 8.2 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14553611141080272		[learning rate: 0.0035506]
		[batch 20/20] avg loss: 0.13827948703726634		[learning rate: 0.0035465]
	Learning Rate: 0.00354646
	LOSS [training: 0.14190779922403446 | validation: 0.11547369393505183]
	TIME [epoch: 8.19 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13335251936115383		[learning rate: 0.0035423]
		[batch 20/20] avg loss: 0.35963793787960374		[learning rate: 0.0035381]
	Learning Rate: 0.00353809
	LOSS [training: 0.24649522862037876 | validation: 0.049916141126946034]
	TIME [epoch: 8.22 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1337172078666425		[learning rate: 0.0035339]
		[batch 20/20] avg loss: 0.1603188977396046		[learning rate: 0.0035297]
	Learning Rate: 0.00352975
	LOSS [training: 0.14701805280312358 | validation: 0.6165670005766323]
	TIME [epoch: 8.19 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24540911068938892		[learning rate: 0.0035256]
		[batch 20/20] avg loss: 0.1725692639305611		[learning rate: 0.0035214]
	Learning Rate: 0.00352142
	LOSS [training: 0.208989187309975 | validation: 0.2240582714746099]
	TIME [epoch: 8.21 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17325181041197246		[learning rate: 0.0035173]
		[batch 20/20] avg loss: 0.1323681817745957		[learning rate: 0.0035131]
	Learning Rate: 0.00351311
	LOSS [training: 0.1528099960932841 | validation: 0.1027782031932587]
	TIME [epoch: 8.21 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18826159320706917		[learning rate: 0.003509]
		[batch 20/20] avg loss: 0.12314381450614464		[learning rate: 0.0035048]
	Learning Rate: 0.00350483
	LOSS [training: 0.15570270385660687 | validation: 0.08755662348344552]
	TIME [epoch: 8.22 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14395216090880109		[learning rate: 0.0035007]
		[batch 20/20] avg loss: 0.13411147694149528		[learning rate: 0.0034966]
	Learning Rate: 0.00349656
	LOSS [training: 0.1390318189251482 | validation: 0.06843536821090217]
	TIME [epoch: 8.19 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14215418532485663		[learning rate: 0.0034924]
		[batch 20/20] avg loss: 0.10294703616593162		[learning rate: 0.0034883]
	Learning Rate: 0.00348831
	LOSS [training: 0.12255061074539413 | validation: 0.39550065310032023]
	TIME [epoch: 8.21 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20412283117462593		[learning rate: 0.0034842]
		[batch 20/20] avg loss: 0.12612624469066375		[learning rate: 0.0034801]
	Learning Rate: 0.00348008
	LOSS [training: 0.16512453793264487 | validation: 0.20875424362090444]
	TIME [epoch: 8.21 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15476334125367552		[learning rate: 0.003476]
		[batch 20/20] avg loss: 0.18425008102071253		[learning rate: 0.0034719]
	Learning Rate: 0.00347187
	LOSS [training: 0.169506711137194 | validation: 0.09893394503238187]
	TIME [epoch: 8.22 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19937086117280567		[learning rate: 0.0034678]
		[batch 20/20] avg loss: 0.12830852698381612		[learning rate: 0.0034637]
	Learning Rate: 0.00346369
	LOSS [training: 0.1638396940783109 | validation: 0.11134454789222095]
	TIME [epoch: 8.18 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14604218544881348		[learning rate: 0.0034596]
		[batch 20/20] avg loss: 0.18102972273486967		[learning rate: 0.0034555]
	Learning Rate: 0.00345552
	LOSS [training: 0.16353595409184157 | validation: 0.07065544750161773]
	TIME [epoch: 8.21 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18637740773920727		[learning rate: 0.0034514]
		[batch 20/20] avg loss: 0.19072473654801386		[learning rate: 0.0034474]
	Learning Rate: 0.00344736
	LOSS [training: 0.18855107214361055 | validation: 0.22405136732414982]
	TIME [epoch: 8.21 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13590443350436293		[learning rate: 0.0034433]
		[batch 20/20] avg loss: 0.1638987644603418		[learning rate: 0.0034392]
	Learning Rate: 0.00343923
	LOSS [training: 0.14990159898235236 | validation: 0.06244102371487242]
	TIME [epoch: 8.22 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16345535525038285		[learning rate: 0.0034352]
		[batch 20/20] avg loss: 0.12098063061669702		[learning rate: 0.0034311]
	Learning Rate: 0.00343112
	LOSS [training: 0.14221799293353993 | validation: 0.17314060926588898]
	TIME [epoch: 8.19 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18528183591825345		[learning rate: 0.0034271]
		[batch 20/20] avg loss: 0.1688171678820365		[learning rate: 0.003423]
	Learning Rate: 0.00342303
	LOSS [training: 0.17704950190014496 | validation: 0.16997675938317444]
	TIME [epoch: 8.2 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1279145942171702		[learning rate: 0.003419]
		[batch 20/20] avg loss: 0.13353717982003158		[learning rate: 0.003415]
	Learning Rate: 0.00341495
	LOSS [training: 0.13072588701860088 | validation: 0.10094322462735222]
	TIME [epoch: 8.21 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17757262518140268		[learning rate: 0.0034109]
		[batch 20/20] avg loss: 0.15215601066450962		[learning rate: 0.0034069]
	Learning Rate: 0.0034069
	LOSS [training: 0.16486431792295617 | validation: 0.19874444051282053]
	TIME [epoch: 8.21 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13072065266318125		[learning rate: 0.0034029]
		[batch 20/20] avg loss: 0.21319553390702076		[learning rate: 0.0033989]
	Learning Rate: 0.00339886
	LOSS [training: 0.171958093285101 | validation: 0.23766369564680478]
	TIME [epoch: 8.2 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19844893370849598		[learning rate: 0.0033948]
		[batch 20/20] avg loss: 0.17494298727420685		[learning rate: 0.0033908]
	Learning Rate: 0.00339084
	LOSS [training: 0.18669596049135143 | validation: 0.1762451551716461]
	TIME [epoch: 8.2 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1395078914748555		[learning rate: 0.0033868]
		[batch 20/20] avg loss: 0.23539012508643528		[learning rate: 0.0033828]
	Learning Rate: 0.00338284
	LOSS [training: 0.18744900828064542 | validation: 0.41272578499737506]
	TIME [epoch: 8.24 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16884235822789145		[learning rate: 0.0033789]
		[batch 20/20] avg loss: 0.11976977065153563		[learning rate: 0.0033749]
	Learning Rate: 0.00337487
	LOSS [training: 0.14430606443971353 | validation: 0.1367802497371027]
	TIME [epoch: 8.21 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1084733831047062		[learning rate: 0.0033709]
		[batch 20/20] avg loss: 0.1485828954140957		[learning rate: 0.0033669]
	Learning Rate: 0.0033669
	LOSS [training: 0.12852813925940093 | validation: 0.1348084138007711]
	TIME [epoch: 8.2 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1329672770766555		[learning rate: 0.0033629]
		[batch 20/20] avg loss: 0.1324912201966095		[learning rate: 0.003359]
	Learning Rate: 0.00335896
	LOSS [training: 0.13272924863663246 | validation: 0.09667269193380654]
	TIME [epoch: 8.2 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15134423642197642		[learning rate: 0.003355]
		[batch 20/20] avg loss: 0.1029992352185656		[learning rate: 0.003351]
	Learning Rate: 0.00335104
	LOSS [training: 0.12717173582027103 | validation: 0.08502554946884394]
	TIME [epoch: 8.23 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11938603524304778		[learning rate: 0.0033471]
		[batch 20/20] avg loss: 0.09059093497833466		[learning rate: 0.0033431]
	Learning Rate: 0.00334313
	LOSS [training: 0.10498848511069123 | validation: 0.035561412841756165]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_514.pth
	Model improved!!!
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1362704205780883		[learning rate: 0.0033392]
		[batch 20/20] avg loss: 0.16704096098944404		[learning rate: 0.0033352]
	Learning Rate: 0.00333525
	LOSS [training: 0.15165569078376617 | validation: 0.10341277872049368]
	TIME [epoch: 8.2 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18762845467764025		[learning rate: 0.0033313]
		[batch 20/20] avg loss: 0.18061128622020356		[learning rate: 0.0033274]
	Learning Rate: 0.00332738
	LOSS [training: 0.18411987044892195 | validation: 0.10725452815840342]
	TIME [epoch: 8.2 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1190870735967133		[learning rate: 0.0033235]
		[batch 20/20] avg loss: 0.14205389236786856		[learning rate: 0.0033195]
	Learning Rate: 0.00331953
	LOSS [training: 0.1305704829822909 | validation: 0.07484371742341696]
	TIME [epoch: 8.24 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16746694683798788		[learning rate: 0.0033156]
		[batch 20/20] avg loss: 0.12432664695063844		[learning rate: 0.0033117]
	Learning Rate: 0.0033117
	LOSS [training: 0.14589679689431315 | validation: 0.059785952400321626]
	TIME [epoch: 8.2 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18758824394483686		[learning rate: 0.0033078]
		[batch 20/20] avg loss: 0.1124458265560175		[learning rate: 0.0033039]
	Learning Rate: 0.00330389
	LOSS [training: 0.1500170352504272 | validation: 0.04895815685058758]
	TIME [epoch: 8.19 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10301599036094584		[learning rate: 0.0033]
		[batch 20/20] avg loss: 0.1443190512944862		[learning rate: 0.0032961]
	Learning Rate: 0.0032961
	LOSS [training: 0.12366752082771597 | validation: 0.20775596292016102]
	TIME [epoch: 8.2 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17262303498926276		[learning rate: 0.0032922]
		[batch 20/20] avg loss: 0.10170227415686717		[learning rate: 0.0032883]
	Learning Rate: 0.00328832
	LOSS [training: 0.13716265457306495 | validation: 0.17695349604001184]
	TIME [epoch: 8.24 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12291348749702202		[learning rate: 0.0032844]
		[batch 20/20] avg loss: 0.11899368646880981		[learning rate: 0.0032806]
	Learning Rate: 0.00328057
	LOSS [training: 0.12095358698291594 | validation: 0.18091433172825128]
	TIME [epoch: 8.19 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13512069321518688		[learning rate: 0.0032767]
		[batch 20/20] avg loss: 0.16468386928086962		[learning rate: 0.0032728]
	Learning Rate: 0.00327283
	LOSS [training: 0.14990228124802824 | validation: 0.08740721769598966]
	TIME [epoch: 8.19 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12877697862140322		[learning rate: 0.003269]
		[batch 20/20] avg loss: 0.13022653564975922		[learning rate: 0.0032651]
	Learning Rate: 0.00326511
	LOSS [training: 0.12950175713558124 | validation: 0.23920317591782161]
	TIME [epoch: 8.19 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1529683410000445		[learning rate: 0.0032613]
		[batch 20/20] avg loss: 0.13424054881215886		[learning rate: 0.0032574]
	Learning Rate: 0.00325741
	LOSS [training: 0.1436044449061017 | validation: 0.06411959175712062]
	TIME [epoch: 8.25 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18620286352425952		[learning rate: 0.0032536]
		[batch 20/20] avg loss: 0.13001726757945103		[learning rate: 0.0032497]
	Learning Rate: 0.00324972
	LOSS [training: 0.1581100655518553 | validation: 0.15564806730062286]
	TIME [epoch: 8.18 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12566375006672742		[learning rate: 0.0032459]
		[batch 20/20] avg loss: 0.17261678423727728		[learning rate: 0.0032421]
	Learning Rate: 0.00324206
	LOSS [training: 0.14914026715200235 | validation: 0.11094222850174562]
	TIME [epoch: 8.2 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12320927452967798		[learning rate: 0.0032382]
		[batch 20/20] avg loss: 0.0981631756429067		[learning rate: 0.0032344]
	Learning Rate: 0.00323441
	LOSS [training: 0.11068622508629233 | validation: 0.0834731043922809]
	TIME [epoch: 8.18 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19156548090062028		[learning rate: 0.0032306]
		[batch 20/20] avg loss: 0.14160300678748547		[learning rate: 0.0032268]
	Learning Rate: 0.00322678
	LOSS [training: 0.1665842438440529 | validation: 0.3835258894385327]
	TIME [epoch: 8.25 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15743018537299394		[learning rate: 0.003223]
		[batch 20/20] avg loss: 0.1697865658757124		[learning rate: 0.0032192]
	Learning Rate: 0.00321917
	LOSS [training: 0.1636083756243531 | validation: 0.20191261375491665]
	TIME [epoch: 8.19 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1197495240545224		[learning rate: 0.0032154]
		[batch 20/20] avg loss: 0.1275957667863981		[learning rate: 0.0032116]
	Learning Rate: 0.00321157
	LOSS [training: 0.1236726454204603 | validation: 0.25022975800870007]
	TIME [epoch: 8.19 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.168540837160422		[learning rate: 0.0032078]
		[batch 20/20] avg loss: 0.12795743502136156		[learning rate: 0.003204]
	Learning Rate: 0.003204
	LOSS [training: 0.1482491360908918 | validation: 0.1338859971401991]
	TIME [epoch: 8.2 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1450541542484755		[learning rate: 0.0032002]
		[batch 20/20] avg loss: 0.11441224200407718		[learning rate: 0.0031964]
	Learning Rate: 0.00319644
	LOSS [training: 0.12973319812627634 | validation: 0.029941371604470607]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_533.pth
	Model improved!!!
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13098179625669107		[learning rate: 0.0031927]
		[batch 20/20] avg loss: 0.13088586951795267		[learning rate: 0.0031889]
	Learning Rate: 0.0031889
	LOSS [training: 0.1309338328873219 | validation: 0.08962457063737485]
	TIME [epoch: 8.19 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17360374348009405		[learning rate: 0.0031851]
		[batch 20/20] avg loss: 0.14907983949220047		[learning rate: 0.0031814]
	Learning Rate: 0.00318138
	LOSS [training: 0.16134179148614722 | validation: 0.07620214208664158]
	TIME [epoch: 8.19 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1432505738026119		[learning rate: 0.0031776]
		[batch 20/20] avg loss: 0.14968576106785286		[learning rate: 0.0031739]
	Learning Rate: 0.00317387
	LOSS [training: 0.1464681674352324 | validation: 0.10583384393887241]
	TIME [epoch: 8.21 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14340538389423305		[learning rate: 0.0031701]
		[batch 20/20] avg loss: 0.19139815308120528		[learning rate: 0.0031664]
	Learning Rate: 0.00316639
	LOSS [training: 0.16740176848771915 | validation: 0.0678289083981243]
	TIME [epoch: 8.24 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11540320547982943		[learning rate: 0.0031627]
		[batch 20/20] avg loss: 0.13252517071334485		[learning rate: 0.0031589]
	Learning Rate: 0.00315892
	LOSS [training: 0.12396418809658716 | validation: 0.04178949806463227]
	TIME [epoch: 8.19 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08272700647095367		[learning rate: 0.0031552]
		[batch 20/20] avg loss: 0.23418570852992504		[learning rate: 0.0031515]
	Learning Rate: 0.00315147
	LOSS [training: 0.15845635750043935 | validation: 0.14280760969118808]
	TIME [epoch: 8.18 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18042857291792794		[learning rate: 0.0031477]
		[batch 20/20] avg loss: 0.10914075998406192		[learning rate: 0.003144]
	Learning Rate: 0.00314403
	LOSS [training: 0.1447846664509949 | validation: 0.07300013907757537]
	TIME [epoch: 8.21 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15818677411777118		[learning rate: 0.0031403]
		[batch 20/20] avg loss: 0.10165432160613017		[learning rate: 0.0031366]
	Learning Rate: 0.00313662
	LOSS [training: 0.12992054786195067 | validation: 0.25461631698035225]
	TIME [epoch: 8.24 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19580883324518183		[learning rate: 0.0031329]
		[batch 20/20] avg loss: 0.15910334997775216		[learning rate: 0.0031292]
	Learning Rate: 0.00312922
	LOSS [training: 0.17745609161146697 | validation: 0.05484259680689708]
	TIME [epoch: 8.19 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13578123023816374		[learning rate: 0.0031255]
		[batch 20/20] avg loss: 0.1597789113131151		[learning rate: 0.0031218]
	Learning Rate: 0.00312184
	LOSS [training: 0.1477800707756394 | validation: 0.13052814302042604]
	TIME [epoch: 8.19 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.194667243973707		[learning rate: 0.0031182]
		[batch 20/20] avg loss: 0.13443744115359094		[learning rate: 0.0031145]
	Learning Rate: 0.00311447
	LOSS [training: 0.16455234256364892 | validation: 0.06520293845563487]
	TIME [epoch: 8.21 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13744055949839917		[learning rate: 0.0031108]
		[batch 20/20] avg loss: 0.1686159758482223		[learning rate: 0.0031071]
	Learning Rate: 0.00310713
	LOSS [training: 0.15302826767331074 | validation: 0.16153042691341463]
	TIME [epoch: 8.23 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19324515858068847		[learning rate: 0.0031035]
		[batch 20/20] avg loss: 0.1513314301741213		[learning rate: 0.0030998]
	Learning Rate: 0.0030998
	LOSS [training: 0.1722882943774049 | validation: 0.09135514857870074]
	TIME [epoch: 8.18 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11062218046213608		[learning rate: 0.0030961]
		[batch 20/20] avg loss: 0.20384712164792668		[learning rate: 0.0030925]
	Learning Rate: 0.00309249
	LOSS [training: 0.15723465105503137 | validation: 0.14420363766634783]
	TIME [epoch: 8.19 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1252666470626546		[learning rate: 0.0030888]
		[batch 20/20] avg loss: 0.15070390003214781		[learning rate: 0.0030852]
	Learning Rate: 0.00308519
	LOSS [training: 0.1379852735474012 | validation: 0.09902479489079109]
	TIME [epoch: 8.2 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11888784789428568		[learning rate: 0.0030815]
		[batch 20/20] avg loss: 0.22488816071096415		[learning rate: 0.0030779]
	Learning Rate: 0.00307791
	LOSS [training: 0.17188800430262494 | validation: 0.24527763112679657]
	TIME [epoch: 8.22 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14711140786807628		[learning rate: 0.0030743]
		[batch 20/20] avg loss: 0.18370120339994078		[learning rate: 0.0030707]
	Learning Rate: 0.00307065
	LOSS [training: 0.16540630563400854 | validation: 0.07711636802733828]
	TIME [epoch: 8.19 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13512747023657296		[learning rate: 0.003067]
		[batch 20/20] avg loss: 0.13014352653432945		[learning rate: 0.0030634]
	Learning Rate: 0.00306341
	LOSS [training: 0.13263549838545122 | validation: 0.1299454451227061]
	TIME [epoch: 8.22 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16654466943827023		[learning rate: 0.0030598]
		[batch 20/20] avg loss: 0.13242282926899165		[learning rate: 0.0030562]
	Learning Rate: 0.00305618
	LOSS [training: 0.14948374935363096 | validation: 0.1333111970622168]
	TIME [epoch: 8.19 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2251871041428501		[learning rate: 0.0030526]
		[batch 20/20] avg loss: 0.14405997090248657		[learning rate: 0.003049]
	Learning Rate: 0.00304897
	LOSS [training: 0.18462353752266836 | validation: 0.06233512595431463]
	TIME [epoch: 8.22 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13702300723998145		[learning rate: 0.0030454]
		[batch 20/20] avg loss: 0.1280189656947625		[learning rate: 0.0030418]
	Learning Rate: 0.00304178
	LOSS [training: 0.132520986467372 | validation: 0.19445947418363294]
	TIME [epoch: 8.21 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17374956299346783		[learning rate: 0.0030382]
		[batch 20/20] avg loss: 0.15372970787039078		[learning rate: 0.0030346]
	Learning Rate: 0.00303461
	LOSS [training: 0.16373963543192932 | validation: 0.07737444538880962]
	TIME [epoch: 8.22 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11868939299672128		[learning rate: 0.003031]
		[batch 20/20] avg loss: 0.14083516077120783		[learning rate: 0.0030274]
	Learning Rate: 0.00302745
	LOSS [training: 0.12976227688396458 | validation: 0.19725282398633132]
	TIME [epoch: 8.2 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1451622339978071		[learning rate: 0.0030239]
		[batch 20/20] avg loss: 0.14018050817545408		[learning rate: 0.0030203]
	Learning Rate: 0.00302031
	LOSS [training: 0.14267137108663056 | validation: 0.08997189906971188]
	TIME [epoch: 8.22 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15013000812270083		[learning rate: 0.0030167]
		[batch 20/20] avg loss: 0.1399905797708863		[learning rate: 0.0030132]
	Learning Rate: 0.00301318
	LOSS [training: 0.1450602939467936 | validation: 0.1430462967477899]
	TIME [epoch: 8.22 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18184371845801783		[learning rate: 0.0030096]
		[batch 20/20] avg loss: 0.12986079148219948		[learning rate: 0.0030061]
	Learning Rate: 0.00300608
	LOSS [training: 0.15585225497010866 | validation: 0.0658468659269698]
	TIME [epoch: 8.21 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1626743732754347		[learning rate: 0.0030025]
		[batch 20/20] avg loss: 0.15841897303732333		[learning rate: 0.002999]
	Learning Rate: 0.00299899
	LOSS [training: 0.16054667315637902 | validation: 0.06679662208899104]
	TIME [epoch: 8.19 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1129597038563644		[learning rate: 0.0029954]
		[batch 20/20] avg loss: 0.11883387883281023		[learning rate: 0.0029919]
	Learning Rate: 0.00299191
	LOSS [training: 0.11589679134458727 | validation: 0.26337437419626875]
	TIME [epoch: 8.22 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16089756154955032		[learning rate: 0.0029884]
		[batch 20/20] avg loss: 0.11476714035860489		[learning rate: 0.0029849]
	Learning Rate: 0.00298485
	LOSS [training: 0.13783235095407761 | validation: 0.3135369648873264]
	TIME [epoch: 8.22 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16885462486157327		[learning rate: 0.0029813]
		[batch 20/20] avg loss: 0.11890542986597774		[learning rate: 0.0029778]
	Learning Rate: 0.00297781
	LOSS [training: 0.14388002736377553 | validation: 0.11929638958560736]
	TIME [epoch: 8.2 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17281231611830805		[learning rate: 0.0029743]
		[batch 20/20] avg loss: 0.11985079600727647		[learning rate: 0.0029708]
	Learning Rate: 0.00297079
	LOSS [training: 0.14633155606279227 | validation: 0.122137208294978]
	TIME [epoch: 8.19 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16432728433345772		[learning rate: 0.0029673]
		[batch 20/20] avg loss: 0.16609317258016984		[learning rate: 0.0029638]
	Learning Rate: 0.00296378
	LOSS [training: 0.16521022845681377 | validation: 0.2158472539690774]
	TIME [epoch: 8.22 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1211591839468041		[learning rate: 0.0029603]
		[batch 20/20] avg loss: 0.16284150226681987		[learning rate: 0.0029568]
	Learning Rate: 0.00295679
	LOSS [training: 0.14200034310681203 | validation: 0.14311013333698813]
	TIME [epoch: 8.22 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14589863889629293		[learning rate: 0.0029533]
		[batch 20/20] avg loss: 0.1903552327981985		[learning rate: 0.0029498]
	Learning Rate: 0.00294982
	LOSS [training: 0.16812693584724572 | validation: 0.07625699821606287]
	TIME [epoch: 8.21 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14382322949101714		[learning rate: 0.0029463]
		[batch 20/20] avg loss: 0.1716110175142116		[learning rate: 0.0029429]
	Learning Rate: 0.00294286
	LOSS [training: 0.15771712350261435 | validation: 0.15992626944085975]
	TIME [epoch: 8.19 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19583909948722716		[learning rate: 0.0029394]
		[batch 20/20] avg loss: 0.13615376637816326		[learning rate: 0.0029359]
	Learning Rate: 0.00293592
	LOSS [training: 0.16599643293269523 | validation: 0.05565794123174588]
	TIME [epoch: 8.21 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1438510015806176		[learning rate: 0.0029325]
		[batch 20/20] avg loss: 0.12976810827604174		[learning rate: 0.002929]
	Learning Rate: 0.00292899
	LOSS [training: 0.13680955492832964 | validation: 0.09163556433792393]
	TIME [epoch: 8.21 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16081323895714367		[learning rate: 0.0029255]
		[batch 20/20] avg loss: 0.1866810715575469		[learning rate: 0.0029221]
	Learning Rate: 0.00292208
	LOSS [training: 0.17374715525734533 | validation: 0.0653579018915496]
	TIME [epoch: 8.2 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13144524329758825		[learning rate: 0.0029186]
		[batch 20/20] avg loss: 0.11101160225604992		[learning rate: 0.0029152]
	Learning Rate: 0.00291519
	LOSS [training: 0.12122842277681908 | validation: 0.045072490534681604]
	TIME [epoch: 8.19 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10527858704983287		[learning rate: 0.0029117]
		[batch 20/20] avg loss: 0.16419777470258315		[learning rate: 0.0029083]
	Learning Rate: 0.00290831
	LOSS [training: 0.13473818087620798 | validation: 0.28063758225365193]
	TIME [epoch: 8.22 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15462843851695218		[learning rate: 0.0029049]
		[batch 20/20] avg loss: 0.13359745687323135		[learning rate: 0.0029015]
	Learning Rate: 0.00290145
	LOSS [training: 0.14411294769509173 | validation: 0.07629179838105032]
	TIME [epoch: 8.22 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18604355447545318		[learning rate: 0.002898]
		[batch 20/20] avg loss: 0.16510162446040857		[learning rate: 0.0028946]
	Learning Rate: 0.00289461
	LOSS [training: 0.17557258946793092 | validation: 0.1500121069470865]
	TIME [epoch: 8.19 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13995295388345114		[learning rate: 0.0028912]
		[batch 20/20] avg loss: 0.14985430662364022		[learning rate: 0.0028878]
	Learning Rate: 0.00288778
	LOSS [training: 0.14490363025354563 | validation: 0.08093868261976851]
	TIME [epoch: 8.2 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1628097565884899		[learning rate: 0.0028844]
		[batch 20/20] avg loss: 0.13558181058406832		[learning rate: 0.002881]
	Learning Rate: 0.00288097
	LOSS [training: 0.1491957835862791 | validation: 0.07099970420221477]
	TIME [epoch: 8.22 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12572337618497387		[learning rate: 0.0028776]
		[batch 20/20] avg loss: 0.21557270972150072		[learning rate: 0.0028742]
	Learning Rate: 0.00287417
	LOSS [training: 0.1706480429532373 | validation: 0.15787985531709264]
	TIME [epoch: 8.23 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13442900372277777		[learning rate: 0.0028708]
		[batch 20/20] avg loss: 0.10997849137723978		[learning rate: 0.0028674]
	Learning Rate: 0.00286739
	LOSS [training: 0.12220374755000878 | validation: 0.06758918734283556]
	TIME [epoch: 8.19 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12578622744740903		[learning rate: 0.002864]
		[batch 20/20] avg loss: 0.18273513132335512		[learning rate: 0.0028606]
	Learning Rate: 0.00286063
	LOSS [training: 0.15426067938538207 | validation: 0.0743440098393286]
	TIME [epoch: 8.19 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11956227580312626		[learning rate: 0.0028573]
		[batch 20/20] avg loss: 0.17787197931584436		[learning rate: 0.0028539]
	Learning Rate: 0.00285388
	LOSS [training: 0.14871712755948532 | validation: 0.06101675533228717]
	TIME [epoch: 8.22 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19330412794345722		[learning rate: 0.0028505]
		[batch 20/20] avg loss: 0.13286918610041656		[learning rate: 0.0028471]
	Learning Rate: 0.00284715
	LOSS [training: 0.16308665702193686 | validation: 0.16337805735206995]
	TIME [epoch: 8.23 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1525176873457452		[learning rate: 0.0028438]
		[batch 20/20] avg loss: 0.1401722785195953		[learning rate: 0.0028404]
	Learning Rate: 0.00284043
	LOSS [training: 0.14634498293267023 | validation: 0.1117831388906172]
	TIME [epoch: 8.2 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1742535887846474		[learning rate: 0.0028371]
		[batch 20/20] avg loss: 0.1447131508625415		[learning rate: 0.0028337]
	Learning Rate: 0.00283373
	LOSS [training: 0.1594833698235944 | validation: 0.27203203757589395]
	TIME [epoch: 8.19 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22054406909932672		[learning rate: 0.0028304]
		[batch 20/20] avg loss: 0.12098531202476652		[learning rate: 0.002827]
	Learning Rate: 0.00282705
	LOSS [training: 0.17076469056204663 | validation: 0.07036148651098544]
	TIME [epoch: 8.2 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1304652428789997		[learning rate: 0.0028237]
		[batch 20/20] avg loss: 0.1342179547169986		[learning rate: 0.0028204]
	Learning Rate: 0.00282038
	LOSS [training: 0.13234159879799917 | validation: 0.06351898607348996]
	TIME [epoch: 8.24 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1200057821471745		[learning rate: 0.0028171]
		[batch 20/20] avg loss: 0.11902872496867647		[learning rate: 0.0028137]
	Learning Rate: 0.00281373
	LOSS [training: 0.11951725355792549 | validation: 0.12481802009696491]
	TIME [epoch: 8.19 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1295015749999426		[learning rate: 0.0028104]
		[batch 20/20] avg loss: 0.16197221298644904		[learning rate: 0.0028071]
	Learning Rate: 0.00280709
	LOSS [training: 0.14573689399319584 | validation: 0.08459704456932257]
	TIME [epoch: 8.18 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19895823158243728		[learning rate: 0.0028038]
		[batch 20/20] avg loss: 0.1680451667291923		[learning rate: 0.0028005]
	Learning Rate: 0.00280047
	LOSS [training: 0.18350169915581477 | validation: 0.11677053385381231]
	TIME [epoch: 8.21 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1252920193368367		[learning rate: 0.0027972]
		[batch 20/20] avg loss: 0.10586454965308312		[learning rate: 0.0027939]
	Learning Rate: 0.00279386
	LOSS [training: 0.11557828449495991 | validation: 0.11697488528276953]
	TIME [epoch: 8.24 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14693081084112847		[learning rate: 0.0027906]
		[batch 20/20] avg loss: 0.16768822266515332		[learning rate: 0.0027873]
	Learning Rate: 0.00278727
	LOSS [training: 0.15730951675314092 | validation: 0.14406810481655855]
	TIME [epoch: 8.2 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1365734584242878		[learning rate: 0.002784]
		[batch 20/20] avg loss: 0.13305986679971143		[learning rate: 0.0027807]
	Learning Rate: 0.0027807
	LOSS [training: 0.1348166626119996 | validation: 0.13353093903158306]
	TIME [epoch: 8.19 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15274023473627601		[learning rate: 0.0027774]
		[batch 20/20] avg loss: 0.13749917256629998		[learning rate: 0.0027741]
	Learning Rate: 0.00277414
	LOSS [training: 0.14511970365128798 | validation: 0.09388488996174063]
	TIME [epoch: 8.21 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1192545143723857		[learning rate: 0.0027709]
		[batch 20/20] avg loss: 0.1563162449652316		[learning rate: 0.0027676]
	Learning Rate: 0.00276759
	LOSS [training: 0.13778537966880866 | validation: 0.07135126702199268]
	TIME [epoch: 8.24 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14700155047939145		[learning rate: 0.0027643]
		[batch 20/20] avg loss: 0.18787207638104825		[learning rate: 0.0027611]
	Learning Rate: 0.00276107
	LOSS [training: 0.16743681343021982 | validation: 0.2533362480680894]
	TIME [epoch: 8.18 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1668592247763442		[learning rate: 0.0027578]
		[batch 20/20] avg loss: 0.17190834380837888		[learning rate: 0.0027546]
	Learning Rate: 0.00275455
	LOSS [training: 0.16938378429236153 | validation: 0.3729180250305252]
	TIME [epoch: 8.19 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20571086530981555		[learning rate: 0.0027513]
		[batch 20/20] avg loss: 0.1783114189946505		[learning rate: 0.0027481]
	Learning Rate: 0.00274806
	LOSS [training: 0.192011142152233 | validation: 0.1498871493954586]
	TIME [epoch: 8.21 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16300238947286239		[learning rate: 0.0027448]
		[batch 20/20] avg loss: 0.19768828736671745		[learning rate: 0.0027416]
	Learning Rate: 0.00274157
	LOSS [training: 0.1803453384197899 | validation: 0.06658619684199743]
	TIME [epoch: 8.24 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1387987241224565		[learning rate: 0.0027383]
		[batch 20/20] avg loss: 0.1461276029494789		[learning rate: 0.0027351]
	Learning Rate: 0.00273511
	LOSS [training: 0.1424631635359677 | validation: 0.41542142708174123]
	TIME [epoch: 8.18 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19896951143627367		[learning rate: 0.0027319]
		[batch 20/20] avg loss: 0.10782293881744617		[learning rate: 0.0027287]
	Learning Rate: 0.00272866
	LOSS [training: 0.1533962251268599 | validation: 0.07877012599573355]
	TIME [epoch: 8.19 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12478798155914692		[learning rate: 0.0027254]
		[batch 20/20] avg loss: 0.2003352729972398		[learning rate: 0.0027222]
	Learning Rate: 0.00272222
	LOSS [training: 0.16256162727819343 | validation: 0.062232212532394714]
	TIME [epoch: 8.22 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11323873720355568		[learning rate: 0.002719]
		[batch 20/20] avg loss: 0.18894672521905448		[learning rate: 0.0027158]
	Learning Rate: 0.0027158
	LOSS [training: 0.1510927312113051 | validation: 0.08002584944543027]
	TIME [epoch: 8.23 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19509504359234317		[learning rate: 0.0027126]
		[batch 20/20] avg loss: 0.10940731212858965		[learning rate: 0.0027094]
	Learning Rate: 0.00270939
	LOSS [training: 0.15225117786046644 | validation: 0.04289314649625222]
	TIME [epoch: 8.19 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13575298870451252		[learning rate: 0.0027062]
		[batch 20/20] avg loss: 0.13755411950106555		[learning rate: 0.002703]
	Learning Rate: 0.002703
	LOSS [training: 0.13665355410278904 | validation: 0.12085909218902965]
	TIME [epoch: 8.19 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10818813543944031		[learning rate: 0.0026998]
		[batch 20/20] avg loss: 0.1126744636347963		[learning rate: 0.0026966]
	Learning Rate: 0.00269662
	LOSS [training: 0.1104312995371183 | validation: 0.2859234323385664]
	TIME [epoch: 8.23 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1389877050770993		[learning rate: 0.0026934]
		[batch 20/20] avg loss: 0.159108242574228		[learning rate: 0.0026903]
	Learning Rate: 0.00269026
	LOSS [training: 0.14904797382566365 | validation: 0.07692447905111406]
	TIME [epoch: 8.23 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12812953394844678		[learning rate: 0.0026871]
		[batch 20/20] avg loss: 0.1376890783721321		[learning rate: 0.0026839]
	Learning Rate: 0.00268392
	LOSS [training: 0.13290930616028943 | validation: 0.36685886385409244]
	TIME [epoch: 8.2 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1780156737898913		[learning rate: 0.0026808]
		[batch 20/20] avg loss: 0.14092782479349894		[learning rate: 0.0026776]
	Learning Rate: 0.00267759
	LOSS [training: 0.15947174929169508 | validation: 0.1833105561438528]
	TIME [epoch: 8.19 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17418509172496793		[learning rate: 0.0026744]
		[batch 20/20] avg loss: 0.16271711938179223		[learning rate: 0.0026713]
	Learning Rate: 0.00267127
	LOSS [training: 0.16845110555338005 | validation: 0.05141859144152951]
	TIME [epoch: 8.21 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.138499145390016		[learning rate: 0.0026681]
		[batch 20/20] avg loss: 0.13143292828356773		[learning rate: 0.002665]
	Learning Rate: 0.00266497
	LOSS [training: 0.1349660368367919 | validation: 0.05826098086636261]
	TIME [epoch: 8.22 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16396953520419846		[learning rate: 0.0026618]
		[batch 20/20] avg loss: 0.14550668395021943		[learning rate: 0.0026587]
	Learning Rate: 0.00265868
	LOSS [training: 0.15473810957720893 | validation: 0.1693724707656227]
	TIME [epoch: 8.19 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1907538228350241		[learning rate: 0.0026555]
		[batch 20/20] avg loss: 0.12227216996994539		[learning rate: 0.0026524]
	Learning Rate: 0.00265241
	LOSS [training: 0.15651299640248476 | validation: 0.10039780199972935]
	TIME [epoch: 8.19 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1457740525528642		[learning rate: 0.0026493]
		[batch 20/20] avg loss: 0.15439515565938824		[learning rate: 0.0026462]
	Learning Rate: 0.00264616
	LOSS [training: 0.15008460410612623 | validation: 0.07623001392657308]
	TIME [epoch: 8.22 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14418876006960857		[learning rate: 0.002643]
		[batch 20/20] avg loss: 0.10808500387722411		[learning rate: 0.0026399]
	Learning Rate: 0.00263991
	LOSS [training: 0.12613688197341633 | validation: 0.17998674014824886]
	TIME [epoch: 8.23 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1631359749537468		[learning rate: 0.0026368]
		[batch 20/20] avg loss: 0.14672476083521596		[learning rate: 0.0026337]
	Learning Rate: 0.00263369
	LOSS [training: 0.1549303678944814 | validation: 0.13865446099641482]
	TIME [epoch: 8.19 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15876537148249475		[learning rate: 0.0026306]
		[batch 20/20] avg loss: 0.1206399234116027		[learning rate: 0.0026275]
	Learning Rate: 0.00262747
	LOSS [training: 0.1397026474470487 | validation: 0.04900419379752038]
	TIME [epoch: 8.19 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1696394478971258		[learning rate: 0.0026244]
		[batch 20/20] avg loss: 0.09227665928264032		[learning rate: 0.0026213]
	Learning Rate: 0.00262128
	LOSS [training: 0.13095805358988305 | validation: 0.1275289395518236]
	TIME [epoch: 8.22 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12317902292580711		[learning rate: 0.0026182]
		[batch 20/20] avg loss: 0.15318307840221096		[learning rate: 0.0026151]
	Learning Rate: 0.00261509
	LOSS [training: 0.13818105066400904 | validation: 0.0735762583834452]
	TIME [epoch: 8.24 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15145258586306648		[learning rate: 0.002612]
		[batch 20/20] avg loss: 0.14210916417781952		[learning rate: 0.0026089]
	Learning Rate: 0.00260892
	LOSS [training: 0.14678087502044299 | validation: 0.15159271875776895]
	TIME [epoch: 8.2 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1486648193736727		[learning rate: 0.0026058]
		[batch 20/20] avg loss: 0.18002187107539896		[learning rate: 0.0026028]
	Learning Rate: 0.00260277
	LOSS [training: 0.16434334522453584 | validation: 0.13658595372943674]
	TIME [epoch: 8.2 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1669799505759086		[learning rate: 0.0025997]
		[batch 20/20] avg loss: 0.15517847804517662		[learning rate: 0.0025966]
	Learning Rate: 0.00259663
	LOSS [training: 0.16107921431054265 | validation: 0.10710945286689555]
	TIME [epoch: 8.21 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14623419166327953		[learning rate: 0.0025936]
		[batch 20/20] avg loss: 0.14705448355125045		[learning rate: 0.0025905]
	Learning Rate: 0.00259051
	LOSS [training: 0.14664433760726497 | validation: 0.19885936952914685]
	TIME [epoch: 8.23 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21221245199574484		[learning rate: 0.0025874]
		[batch 20/20] avg loss: 0.12891681333482888		[learning rate: 0.0025844]
	Learning Rate: 0.0025844
	LOSS [training: 0.17056463266528687 | validation: 0.13131822564342524]
	TIME [epoch: 8.2 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17570346198319153		[learning rate: 0.0025813]
		[batch 20/20] avg loss: 0.12418759679086525		[learning rate: 0.0025783]
	Learning Rate: 0.0025783
	LOSS [training: 0.14994552938702835 | validation: 0.11170558099517831]
	TIME [epoch: 8.19 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15681223303920777		[learning rate: 0.0025753]
		[batch 20/20] avg loss: 0.16564682894374824		[learning rate: 0.0025722]
	Learning Rate: 0.00257222
	LOSS [training: 0.16122953099147802 | validation: 0.5091032353221216]
	TIME [epoch: 8.23 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23665871707796607		[learning rate: 0.0025692]
		[batch 20/20] avg loss: 0.12706998324384974		[learning rate: 0.0025662]
	Learning Rate: 0.00256615
	LOSS [training: 0.1818643501609079 | validation: 0.061169306019712724]
	TIME [epoch: 8.23 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1803990647069681		[learning rate: 0.0025631]
		[batch 20/20] avg loss: 0.1768301226600345		[learning rate: 0.0025601]
	Learning Rate: 0.0025601
	LOSS [training: 0.17861459368350127 | validation: 0.0703795955895787]
	TIME [epoch: 8.2 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29982192888084225		[learning rate: 0.0025571]
		[batch 20/20] avg loss: 0.16386950027825933		[learning rate: 0.0025541]
	Learning Rate: 0.00255406
	LOSS [training: 0.23184571457955086 | validation: 0.07099532663536146]
	TIME [epoch: 8.2 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1420770605759869		[learning rate: 0.002551]
		[batch 20/20] avg loss: 0.21183304303943915		[learning rate: 0.002548]
	Learning Rate: 0.00254803
	LOSS [training: 0.17695505180771298 | validation: 0.12602885089959723]
	TIME [epoch: 8.24 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1640022776765535		[learning rate: 0.002545]
		[batch 20/20] avg loss: 0.1120335003524305		[learning rate: 0.002542]
	Learning Rate: 0.00254202
	LOSS [training: 0.13801788901449202 | validation: 0.15035555508279946]
	TIME [epoch: 8.23 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27578829493240276		[learning rate: 0.002539]
		[batch 20/20] avg loss: 0.1474831148695413		[learning rate: 0.002536]
	Learning Rate: 0.00253603
	LOSS [training: 0.211635704900972 | validation: 0.07349539951045415]
	TIME [epoch: 8.2 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18391539425264328		[learning rate: 0.002533]
		[batch 20/20] avg loss: 0.09273242406809967		[learning rate: 0.00253]
	Learning Rate: 0.00253004
	LOSS [training: 0.13832390916037146 | validation: 0.11249675927456598]
	TIME [epoch: 8.2 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1687519991786462		[learning rate: 0.0025271]
		[batch 20/20] avg loss: 0.15914111470806253		[learning rate: 0.0025241]
	Learning Rate: 0.00252408
	LOSS [training: 0.1639465569433544 | validation: 0.06584248244377555]
	TIME [epoch: 8.24 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11917051452942977		[learning rate: 0.0025211]
		[batch 20/20] avg loss: 0.13160043000687863		[learning rate: 0.0025181]
	Learning Rate: 0.00251812
	LOSS [training: 0.1253854722681542 | validation: 0.09861084310402038]
	TIME [epoch: 8.23 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12448098683975724		[learning rate: 0.0025152]
		[batch 20/20] avg loss: 0.1974819374956232		[learning rate: 0.0025122]
	Learning Rate: 0.00251218
	LOSS [training: 0.16098146216769024 | validation: 0.052328920354545054]
	TIME [epoch: 8.2 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1552605206755173		[learning rate: 0.0025092]
		[batch 20/20] avg loss: 0.17824710747110492		[learning rate: 0.0025063]
	Learning Rate: 0.00250626
	LOSS [training: 0.16675381407331114 | validation: 0.15397042720643261]
	TIME [epoch: 8.19 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23880530078930265		[learning rate: 0.0025033]
		[batch 20/20] avg loss: 0.12525453273118814		[learning rate: 0.0025003]
	Learning Rate: 0.00250035
	LOSS [training: 0.1820299167602454 | validation: 0.1386364293586908]
	TIME [epoch: 8.23 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14174669145306812		[learning rate: 0.0024974]
		[batch 20/20] avg loss: 0.2253858163792676		[learning rate: 0.0024944]
	Learning Rate: 0.00249445
	LOSS [training: 0.18356625391616788 | validation: 0.09268912582266173]
	TIME [epoch: 8.22 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13589719865271038		[learning rate: 0.0024915]
		[batch 20/20] avg loss: 0.22987348008315256		[learning rate: 0.0024886]
	Learning Rate: 0.00248856
	LOSS [training: 0.18288533936793147 | validation: 0.16561946022324425]
	TIME [epoch: 8.19 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19035041283915174		[learning rate: 0.0024856]
		[batch 20/20] avg loss: 0.1536573062486038		[learning rate: 0.0024827]
	Learning Rate: 0.00248269
	LOSS [training: 0.17200385954387779 | validation: 0.05276940233601625]
	TIME [epoch: 8.19 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18173602534460476		[learning rate: 0.0024798]
		[batch 20/20] avg loss: 0.2288590451857811		[learning rate: 0.0024768]
	Learning Rate: 0.00247684
	LOSS [training: 0.2052975352651929 | validation: 0.2185788472024439]
	TIME [epoch: 8.22 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18969740326000778		[learning rate: 0.0024739]
		[batch 20/20] avg loss: 0.22153886921491592		[learning rate: 0.002471]
	Learning Rate: 0.00247099
	LOSS [training: 0.20561813623746183 | validation: 0.11717843248741301]
	TIME [epoch: 8.23 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21425235366971637		[learning rate: 0.0024681]
		[batch 20/20] avg loss: 0.27042260477388125		[learning rate: 0.0024652]
	Learning Rate: 0.00246517
	LOSS [training: 0.2423374792217988 | validation: 0.1669923581199262]
	TIME [epoch: 8.19 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17429682503868152		[learning rate: 0.0024623]
		[batch 20/20] avg loss: 0.2447046143479855		[learning rate: 0.0024594]
	Learning Rate: 0.00245935
	LOSS [training: 0.20950071969333353 | validation: 0.17175750561992637]
	TIME [epoch: 8.2 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19411547897400272		[learning rate: 0.0024564]
		[batch 20/20] avg loss: 0.16328805751054687		[learning rate: 0.0024535]
	Learning Rate: 0.00245355
	LOSS [training: 0.17870176824227482 | validation: 0.16829302972630286]
	TIME [epoch: 8.23 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.199741435072267		[learning rate: 0.0024507]
		[batch 20/20] avg loss: 0.17691782746525822		[learning rate: 0.0024478]
	Learning Rate: 0.00244776
	LOSS [training: 0.18832963126876257 | validation: 0.10598837831366785]
	TIME [epoch: 8.22 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24257805014423556		[learning rate: 0.0024449]
		[batch 20/20] avg loss: 0.15516804243822419		[learning rate: 0.002442]
	Learning Rate: 0.00244199
	LOSS [training: 0.19887304629122987 | validation: 0.049279559653566965]
	TIME [epoch: 8.2 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17170786558551732		[learning rate: 0.0024391]
		[batch 20/20] avg loss: 0.21017742344879636		[learning rate: 0.0024362]
	Learning Rate: 0.00243623
	LOSS [training: 0.1909426445171568 | validation: 0.4609715346275516]
	TIME [epoch: 8.21 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35116264713431333		[learning rate: 0.0024334]
		[batch 20/20] avg loss: 0.16155120396152306		[learning rate: 0.0024305]
	Learning Rate: 0.00243048
	LOSS [training: 0.2563569255479182 | validation: 0.08607847848991554]
	TIME [epoch: 8.22 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1645704026793688		[learning rate: 0.0024276]
		[batch 20/20] avg loss: 0.24435173141388383		[learning rate: 0.0024247]
	Learning Rate: 0.00242475
	LOSS [training: 0.20446106704662634 | validation: 0.13518057064137826]
	TIME [epoch: 8.23 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14143123156285756		[learning rate: 0.0024219]
		[batch 20/20] avg loss: 0.13295943867729645		[learning rate: 0.002419]
	Learning Rate: 0.00241903
	LOSS [training: 0.137195335120077 | validation: 0.10414469921812536]
	TIME [epoch: 8.2 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17739027816192504		[learning rate: 0.0024162]
		[batch 20/20] avg loss: 0.1867707701790217		[learning rate: 0.0024133]
	Learning Rate: 0.00241332
	LOSS [training: 0.18208052417047338 | validation: 0.11714882653039034]
	TIME [epoch: 8.21 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23496495560854408		[learning rate: 0.0024105]
		[batch 20/20] avg loss: 0.16296603501354495		[learning rate: 0.0024076]
	Learning Rate: 0.00240763
	LOSS [training: 0.1989654953110445 | validation: 0.28317978565442503]
	TIME [epoch: 8.2 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2423791456387257		[learning rate: 0.0024048]
		[batch 20/20] avg loss: 0.1781880492923774		[learning rate: 0.002402]
	Learning Rate: 0.00240195
	LOSS [training: 0.21028359746555153 | validation: 0.050244950103198203]
	TIME [epoch: 8.22 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16534245326229288		[learning rate: 0.0023991]
		[batch 20/20] avg loss: 0.1462978093802142		[learning rate: 0.0023963]
	Learning Rate: 0.00239628
	LOSS [training: 0.15582013132125355 | validation: 0.14125124620418936]
	TIME [epoch: 8.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18893944393689965		[learning rate: 0.0023935]
		[batch 20/20] avg loss: 0.26534982784910977		[learning rate: 0.0023906]
	Learning Rate: 0.00239063
	LOSS [training: 0.2271446358930047 | validation: 0.10062389821351443]
	TIME [epoch: 8.21 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17382011682215603		[learning rate: 0.0023878]
		[batch 20/20] avg loss: 0.22167689294175835		[learning rate: 0.002385]
	Learning Rate: 0.00238499
	LOSS [training: 0.19774850488195722 | validation: 0.12137212136759569]
	TIME [epoch: 8.2 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1691437151679705		[learning rate: 0.0023822]
		[batch 20/20] avg loss: 0.28484437637776977		[learning rate: 0.0023794]
	Learning Rate: 0.00237937
	LOSS [training: 0.22699404577287013 | validation: 0.13188474264806216]
	TIME [epoch: 8.22 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1211497864746989		[learning rate: 0.0023766]
		[batch 20/20] avg loss: 0.18388397454660618		[learning rate: 0.0023738]
	Learning Rate: 0.00237375
	LOSS [training: 0.15251688051065257 | validation: 0.12162365476587335]
	TIME [epoch: 8.2 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17296545066799224		[learning rate: 0.002371]
		[batch 20/20] avg loss: 0.16766363487033473		[learning rate: 0.0023682]
	Learning Rate: 0.00236816
	LOSS [training: 0.17031454276916344 | validation: 0.15188649760758074]
	TIME [epoch: 8.22 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17423153365702038		[learning rate: 0.0023654]
		[batch 20/20] avg loss: 0.15123234870553018		[learning rate: 0.0023626]
	Learning Rate: 0.00236257
	LOSS [training: 0.16273194118127526 | validation: 0.17614432701328692]
	TIME [epoch: 8.21 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25791835241034006		[learning rate: 0.0023598]
		[batch 20/20] avg loss: 0.2659521420428354		[learning rate: 0.002357]
	Learning Rate: 0.002357
	LOSS [training: 0.2619352472265878 | validation: 0.07065313147202765]
	TIME [epoch: 8.22 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17164154419875394		[learning rate: 0.0023542]
		[batch 20/20] avg loss: 0.2000451310838835		[learning rate: 0.0023514]
	Learning Rate: 0.00235144
	LOSS [training: 0.18584333764131872 | validation: 0.08166167845532565]
	TIME [epoch: 8.2 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1544634534901716		[learning rate: 0.0023487]
		[batch 20/20] avg loss: 0.1769291426542154		[learning rate: 0.0023459]
	Learning Rate: 0.00234589
	LOSS [training: 0.1656962980721935 | validation: 0.10404111559881582]
	TIME [epoch: 8.23 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14156110332424685		[learning rate: 0.0023431]
		[batch 20/20] avg loss: 0.21424868154537607		[learning rate: 0.0023404]
	Learning Rate: 0.00234036
	LOSS [training: 0.17790489243481147 | validation: 0.19864596672285117]
	TIME [epoch: 8.21 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22049948287877214		[learning rate: 0.0023376]
		[batch 20/20] avg loss: 0.2313916630562065		[learning rate: 0.0023348]
	Learning Rate: 0.00233484
	LOSS [training: 0.22594557296748935 | validation: 0.1330674577744148]
	TIME [epoch: 8.22 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18213189417953854		[learning rate: 0.0023321]
		[batch 20/20] avg loss: 0.1516535939549814		[learning rate: 0.0023293]
	Learning Rate: 0.00232933
	LOSS [training: 0.16689274406725998 | validation: 0.07522955470882603]
	TIME [epoch: 8.19 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20387768911782364		[learning rate: 0.0023266]
		[batch 20/20] avg loss: 0.2258205341264583		[learning rate: 0.0023238]
	Learning Rate: 0.00232383
	LOSS [training: 0.21484911162214093 | validation: 0.07844112118430768]
	TIME [epoch: 8.23 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27637780426337333		[learning rate: 0.0023211]
		[batch 20/20] avg loss: 0.25055365520675466		[learning rate: 0.0023184]
	Learning Rate: 0.00231835
	LOSS [training: 0.26346572973506394 | validation: 0.13400038287941382]
	TIME [epoch: 8.19 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16978057179288664		[learning rate: 0.0023156]
		[batch 20/20] avg loss: 0.29086792225569186		[learning rate: 0.0023129]
	Learning Rate: 0.00231288
	LOSS [training: 0.23032424702428922 | validation: 0.14316680248386787]
	TIME [epoch: 8.22 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18765528906776058		[learning rate: 0.0023102]
		[batch 20/20] avg loss: 0.18064420159472544		[learning rate: 0.0023074]
	Learning Rate: 0.00230743
	LOSS [training: 0.184149745331243 | validation: 0.13078993010007942]
	TIME [epoch: 8.2 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3224613497631968		[learning rate: 0.0023047]
		[batch 20/20] avg loss: 0.302264760758362		[learning rate: 0.002302]
	Learning Rate: 0.00230199
	LOSS [training: 0.3123630552607794 | validation: 0.20763333694516692]
	TIME [epoch: 8.22 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2484215368884784		[learning rate: 0.0022993]
		[batch 20/20] avg loss: 0.15291175040427943		[learning rate: 0.0022966]
	Learning Rate: 0.00229656
	LOSS [training: 0.20066664364637893 | validation: 0.0729091450423573]
	TIME [epoch: 8.2 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.261948186004975		[learning rate: 0.0022938]
		[batch 20/20] avg loss: 0.2228521048087019		[learning rate: 0.0022911]
	Learning Rate: 0.00229114
	LOSS [training: 0.24240014540683844 | validation: 0.2541938955723204]
	TIME [epoch: 8.23 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3063684944313029		[learning rate: 0.0022884]
		[batch 20/20] avg loss: 0.17378375499123946		[learning rate: 0.0022857]
	Learning Rate: 0.00228573
	LOSS [training: 0.24007612471127118 | validation: 0.17278712721639478]
	TIME [epoch: 8.21 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36101207243970046		[learning rate: 0.002283]
		[batch 20/20] avg loss: 0.22037470320582844		[learning rate: 0.0022803]
	Learning Rate: 0.00228034
	LOSS [training: 0.29069338782276444 | validation: 0.1224877841125839]
	TIME [epoch: 8.22 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18067152521901308		[learning rate: 0.0022777]
		[batch 20/20] avg loss: 0.3815579101121167		[learning rate: 0.002275]
	Learning Rate: 0.00227496
	LOSS [training: 0.2811147176655649 | validation: 0.1431409569446668]
	TIME [epoch: 8.21 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26876345385687894		[learning rate: 0.0022723]
		[batch 20/20] avg loss: 0.24166236206185224		[learning rate: 0.0022696]
	Learning Rate: 0.0022696
	LOSS [training: 0.2552129079593656 | validation: 0.1336054139622045]
	TIME [epoch: 8.22 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18180037117301734		[learning rate: 0.0022669]
		[batch 20/20] avg loss: 0.2632848001650044		[learning rate: 0.0022642]
	Learning Rate: 0.00226424
	LOSS [training: 0.22254258566901086 | validation: 0.2529279183053298]
	TIME [epoch: 8.2 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21215023063813362		[learning rate: 0.0022616]
		[batch 20/20] avg loss: 0.22178814658363016		[learning rate: 0.0022589]
	Learning Rate: 0.0022589
	LOSS [training: 0.21696918861088194 | validation: 0.16297635397091276]
	TIME [epoch: 8.23 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.235728490124548		[learning rate: 0.0022562]
		[batch 20/20] avg loss: 0.16154518251573044		[learning rate: 0.0022536]
	Learning Rate: 0.00225357
	LOSS [training: 0.19863683632013923 | validation: 0.5252110816185365]
	TIME [epoch: 8.2 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2598946584202057		[learning rate: 0.0022509]
		[batch 20/20] avg loss: 0.26503324512132703		[learning rate: 0.0022483]
	Learning Rate: 0.00224826
	LOSS [training: 0.2624639517707664 | validation: 0.11483135965362112]
	TIME [epoch: 8.21 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2343794915812479		[learning rate: 0.0022456]
		[batch 20/20] avg loss: 0.160730752269507		[learning rate: 0.002243]
	Learning Rate: 0.00224295
	LOSS [training: 0.19755512192537744 | validation: 0.3861655773313636]
	TIME [epoch: 8.2 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3154445642334119		[learning rate: 0.0022403]
		[batch 20/20] avg loss: 0.18996531844860057		[learning rate: 0.0022377]
	Learning Rate: 0.00223766
	LOSS [training: 0.2527049413410062 | validation: 0.11329477090484938]
	TIME [epoch: 8.22 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2581066069432505		[learning rate: 0.002235]
		[batch 20/20] avg loss: 0.20351763758694924		[learning rate: 0.0022324]
	Learning Rate: 0.00223239
	LOSS [training: 0.2308121222650999 | validation: 0.07835707037884025]
	TIME [epoch: 8.19 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26472463078733366		[learning rate: 0.0022298]
		[batch 20/20] avg loss: 0.3045479340519512		[learning rate: 0.0022271]
	Learning Rate: 0.00222712
	LOSS [training: 0.2846362824196425 | validation: 0.23501006274252995]
	TIME [epoch: 8.21 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25421205709559846		[learning rate: 0.0022245]
		[batch 20/20] avg loss: 0.2687873098621404		[learning rate: 0.0022219]
	Learning Rate: 0.00222187
	LOSS [training: 0.26149968347886954 | validation: 0.08260241677348754]
	TIME [epoch: 8.21 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20706198920659072		[learning rate: 0.0022192]
		[batch 20/20] avg loss: 0.2115322766472353		[learning rate: 0.0022166]
	Learning Rate: 0.00221663
	LOSS [training: 0.20929713292691302 | validation: 0.12108121048863296]
	TIME [epoch: 8.23 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22588505353826122		[learning rate: 0.002214]
		[batch 20/20] avg loss: 0.4543775688910279		[learning rate: 0.0022114]
	Learning Rate: 0.0022114
	LOSS [training: 0.34013131121464457 | validation: 0.35007250678707275]
	TIME [epoch: 8.19 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3506017341664287		[learning rate: 0.0022088]
		[batch 20/20] avg loss: 0.27383552255928795		[learning rate: 0.0022062]
	Learning Rate: 0.00220618
	LOSS [training: 0.3122186283628583 | validation: 0.6409544873819077]
	TIME [epoch: 8.2 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38332998902864723		[learning rate: 0.0022036]
		[batch 20/20] avg loss: 0.21165644431177677		[learning rate: 0.002201]
	Learning Rate: 0.00220098
	LOSS [training: 0.297493216670212 | validation: 0.1720914938550231]
	TIME [epoch: 8.21 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2618300558708677		[learning rate: 0.0021984]
		[batch 20/20] avg loss: 0.21133409105287101		[learning rate: 0.0021958]
	Learning Rate: 0.00219578
	LOSS [training: 0.23658207346186938 | validation: 0.2940417551381228]
	TIME [epoch: 8.24 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30248191192743124		[learning rate: 0.0021932]
		[batch 20/20] avg loss: 0.27313335800694283		[learning rate: 0.0021906]
	Learning Rate: 0.0021906
	LOSS [training: 0.2878076349671871 | validation: 0.21364216843412048]
	TIME [epoch: 8.19 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18658047029723804		[learning rate: 0.002188]
		[batch 20/20] avg loss: 0.35383298958765835		[learning rate: 0.0021854]
	Learning Rate: 0.00218544
	LOSS [training: 0.2702067299424482 | validation: 0.1967267316428593]
	TIME [epoch: 8.21 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2520619821811189		[learning rate: 0.0021829]
		[batch 20/20] avg loss: 0.23719458727032716		[learning rate: 0.0021803]
	Learning Rate: 0.00218028
	LOSS [training: 0.24462828472572307 | validation: 0.09017765745471372]
	TIME [epoch: 8.22 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1918442157430197		[learning rate: 0.0021777]
		[batch 20/20] avg loss: 0.2628716004269024		[learning rate: 0.0021751]
	Learning Rate: 0.00217514
	LOSS [training: 0.22735790808496104 | validation: 0.40593161054735893]
	TIME [epoch: 8.22 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2445935245857566		[learning rate: 0.0021726]
		[batch 20/20] avg loss: 0.3416438627070916		[learning rate: 0.00217]
	Learning Rate: 0.00217001
	LOSS [training: 0.2931186936464241 | validation: 0.16976415094852815]
	TIME [epoch: 8.18 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4796908603132743		[learning rate: 0.0021674]
		[batch 20/20] avg loss: 0.30401982112110837		[learning rate: 0.0021649]
	Learning Rate: 0.00216489
	LOSS [training: 0.3918553407171913 | validation: 0.2839362987943568]
	TIME [epoch: 8.21 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23362603503125468		[learning rate: 0.0021623]
		[batch 20/20] avg loss: 0.2876615962202955		[learning rate: 0.0021598]
	Learning Rate: 0.00215978
	LOSS [training: 0.2606438156257751 | validation: 0.1427152478346165]
	TIME [epoch: 8.21 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25271180057897763		[learning rate: 0.0021572]
		[batch 20/20] avg loss: 0.21741011877765595		[learning rate: 0.0021547]
	Learning Rate: 0.00215469
	LOSS [training: 0.2350609596783168 | validation: 0.572431892640471]
	TIME [epoch: 8.21 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3673232917967578		[learning rate: 0.0021521]
		[batch 20/20] avg loss: 0.3033793546192046		[learning rate: 0.0021496]
	Learning Rate: 0.00214961
	LOSS [training: 0.3353513232079812 | validation: 0.39390061629155004]
	TIME [epoch: 8.19 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4216404256060916		[learning rate: 0.0021471]
		[batch 20/20] avg loss: 0.2333535076075551		[learning rate: 0.0021445]
	Learning Rate: 0.00214454
	LOSS [training: 0.32749696660682337 | validation: 0.19587668886116388]
	TIME [epoch: 8.2 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3364196951449282		[learning rate: 0.002142]
		[batch 20/20] avg loss: 0.38484425907362857		[learning rate: 0.0021395]
	Learning Rate: 0.00213948
	LOSS [training: 0.3606319771092784 | validation: 0.2552499975807661]
	TIME [epoch: 8.22 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2902947366643626		[learning rate: 0.002137]
		[batch 20/20] avg loss: 0.388366561196425		[learning rate: 0.0021344]
	Learning Rate: 0.00213443
	LOSS [training: 0.3393306489303939 | validation: 0.14196188973250734]
	TIME [epoch: 8.21 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21489631282037358		[learning rate: 0.0021319]
		[batch 20/20] avg loss: 0.28797003637998214		[learning rate: 0.0021294]
	Learning Rate: 0.0021294
	LOSS [training: 0.2514331746001778 | validation: 0.1446090288217283]
	TIME [epoch: 8.19 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3073581407419209		[learning rate: 0.0021269]
		[batch 20/20] avg loss: 0.36127894732219223		[learning rate: 0.0021244]
	Learning Rate: 0.00212437
	LOSS [training: 0.3343185440320566 | validation: 0.17680790620413311]
	TIME [epoch: 8.2 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2694574439975566		[learning rate: 0.0021219]
		[batch 20/20] avg loss: 0.22328605114530015		[learning rate: 0.0021194]
	Learning Rate: 0.00211936
	LOSS [training: 0.24637174757142835 | validation: 0.1775345182118842]
	TIME [epoch: 8.24 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3475050626116419		[learning rate: 0.0021169]
		[batch 20/20] avg loss: 0.32237934724005135		[learning rate: 0.0021144]
	Learning Rate: 0.00211436
	LOSS [training: 0.33494220492584664 | validation: 0.13704175927990458]
	TIME [epoch: 8.22 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3256377314621387		[learning rate: 0.0021119]
		[batch 20/20] avg loss: 0.4157654595234237		[learning rate: 0.0021094]
	Learning Rate: 0.00210938
	LOSS [training: 0.3707015954927812 | validation: 0.20996605569555277]
	TIME [epoch: 8.2 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36232468082896335		[learning rate: 0.0021069]
		[batch 20/20] avg loss: 0.46995416742007323		[learning rate: 0.0021044]
	Learning Rate: 0.0021044
	LOSS [training: 0.41613942412451843 | validation: 0.31670811093178425]
	TIME [epoch: 8.2 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4009090923368511		[learning rate: 0.0021019]
		[batch 20/20] avg loss: 0.41109669045296826		[learning rate: 0.0020994]
	Learning Rate: 0.00209944
	LOSS [training: 0.4060028913949097 | validation: 0.47728605098918603]
	TIME [epoch: 8.23 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3365838390252231		[learning rate: 0.002097]
		[batch 20/20] avg loss: 0.29902837426253764		[learning rate: 0.0020945]
	Learning Rate: 0.00209448
	LOSS [training: 0.3178061066438803 | validation: 0.19395090415868851]
	TIME [epoch: 8.21 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.386696080991028		[learning rate: 0.002092]
		[batch 20/20] avg loss: 0.3028314937151322		[learning rate: 0.0020895]
	Learning Rate: 0.00208954
	LOSS [training: 0.34476378735308005 | validation: 0.2882721589547508]
	TIME [epoch: 8.19 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6550942364572118		[learning rate: 0.0020871]
		[batch 20/20] avg loss: 0.33802200583493036		[learning rate: 0.0020846]
	Learning Rate: 0.00208461
	LOSS [training: 0.49655812114607095 | validation: 0.23695691200507077]
	TIME [epoch: 8.2 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.315409915479083		[learning rate: 0.0020822]
		[batch 20/20] avg loss: 0.39611939961831044		[learning rate: 0.0020797]
	Learning Rate: 0.0020797
	LOSS [training: 0.3557646575486967 | validation: 0.16597275628667799]
	TIME [epoch: 8.24 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37422477766450596		[learning rate: 0.0020772]
		[batch 20/20] avg loss: 0.2798988820092743		[learning rate: 0.0020748]
	Learning Rate: 0.00207479
	LOSS [training: 0.3270618298368901 | validation: 0.19426150344161214]
	TIME [epoch: 8.21 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20784846222338055		[learning rate: 0.0020723]
		[batch 20/20] avg loss: 0.3365690049233644		[learning rate: 0.0020699]
	Learning Rate: 0.0020699
	LOSS [training: 0.27220873357337244 | validation: 0.3184051383773511]
	TIME [epoch: 8.19 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2883040316529252		[learning rate: 0.0020675]
		[batch 20/20] avg loss: 0.2844988995972635		[learning rate: 0.002065]
	Learning Rate: 0.00206501
	LOSS [training: 0.2864014656250943 | validation: 0.21054537161298553]
	TIME [epoch: 8.19 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25784585203679444		[learning rate: 0.0020626]
		[batch 20/20] avg loss: 0.23345040316389007		[learning rate: 0.0020601]
	Learning Rate: 0.00206014
	LOSS [training: 0.24564812760034224 | validation: 0.17565915432249296]
	TIME [epoch: 8.23 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20341525635336755		[learning rate: 0.0020577]
		[batch 20/20] avg loss: 0.33312847567037784		[learning rate: 0.0020553]
	Learning Rate: 0.00205528
	LOSS [training: 0.2682718660118727 | validation: 0.15244120393512328]
	TIME [epoch: 8.2 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2508625728705271		[learning rate: 0.0020529]
		[batch 20/20] avg loss: 0.327095104629071		[learning rate: 0.0020504]
	Learning Rate: 0.00205044
	LOSS [training: 0.288978838749799 | validation: 0.2959087708905417]
	TIME [epoch: 8.17 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24705460257467782		[learning rate: 0.002048]
		[batch 20/20] avg loss: 0.3682450313347776		[learning rate: 0.0020456]
	Learning Rate: 0.0020456
	LOSS [training: 0.3076498169547277 | validation: 0.18366336080183499]
	TIME [epoch: 8.19 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30382603126699453		[learning rate: 0.0020432]
		[batch 20/20] avg loss: 0.36203264590478035		[learning rate: 0.0020408]
	Learning Rate: 0.00204077
	LOSS [training: 0.33292933858588747 | validation: 0.19320110720540884]
	TIME [epoch: 8.24 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30452867180935994		[learning rate: 0.0020384]
		[batch 20/20] avg loss: 0.2813941020836561		[learning rate: 0.002036]
	Learning Rate: 0.00203596
	LOSS [training: 0.2929613869465081 | validation: 0.25310336063895095]
	TIME [epoch: 8.2 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3764018745946194		[learning rate: 0.0020336]
		[batch 20/20] avg loss: 0.4034953877562		[learning rate: 0.0020312]
	Learning Rate: 0.00203116
	LOSS [training: 0.3899486311754096 | validation: 0.3818581711789544]
	TIME [epoch: 8.18 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33058640799431027		[learning rate: 0.0020288]
		[batch 20/20] avg loss: 0.30996630188645014		[learning rate: 0.0020264]
	Learning Rate: 0.00202637
	LOSS [training: 0.3202763549403802 | validation: 0.11020394768899232]
	TIME [epoch: 8.19 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3164797175283304		[learning rate: 0.002024]
		[batch 20/20] avg loss: 0.28934977298044623		[learning rate: 0.0020216]
	Learning Rate: 0.00202159
	LOSS [training: 0.30291474525438833 | validation: 0.16525251778247724]
	TIME [epoch: 8.25 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34061429364533413		[learning rate: 0.0020192]
		[batch 20/20] avg loss: 0.2861336076806943		[learning rate: 0.0020168]
	Learning Rate: 0.00201682
	LOSS [training: 0.31337395066301416 | validation: 0.25751031652604284]
	TIME [epoch: 8.19 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4125625924155272		[learning rate: 0.0020144]
		[batch 20/20] avg loss: 0.3640692329376304		[learning rate: 0.0020121]
	Learning Rate: 0.00201206
	LOSS [training: 0.38831591267657883 | validation: 0.3528336541387959]
	TIME [epoch: 8.19 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3825352670364496		[learning rate: 0.0020097]
		[batch 20/20] avg loss: 0.34890668587264206		[learning rate: 0.0020073]
	Learning Rate: 0.00200731
	LOSS [training: 0.3657209764545458 | validation: 0.5221918431477485]
	TIME [epoch: 8.19 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4514877827752434		[learning rate: 0.0020049]
		[batch 20/20] avg loss: 0.33561606396377863		[learning rate: 0.0020026]
	Learning Rate: 0.00200258
	LOSS [training: 0.393551923369511 | validation: 0.29655751121755314]
	TIME [epoch: 8.24 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34519009584047355		[learning rate: 0.0020002]
		[batch 20/20] avg loss: 0.3891314017668157		[learning rate: 0.0019979]
	Learning Rate: 0.00199786
	LOSS [training: 0.36716074880364463 | validation: 0.19674761859881967]
	TIME [epoch: 8.19 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3293524212124786		[learning rate: 0.0019955]
		[batch 20/20] avg loss: 0.29267203637711275		[learning rate: 0.0019931]
	Learning Rate: 0.00199314
	LOSS [training: 0.3110122287947957 | validation: 0.1477149025977445]
	TIME [epoch: 8.2 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28824629060250606		[learning rate: 0.0019908]
		[batch 20/20] avg loss: 0.21185476118254076		[learning rate: 0.0019884]
	Learning Rate: 0.00198844
	LOSS [training: 0.2500505258925234 | validation: 0.2040915657923854]
	TIME [epoch: 8.21 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23884369729248398		[learning rate: 0.0019861]
		[batch 20/20] avg loss: 0.3601411657375921		[learning rate: 0.0019838]
	Learning Rate: 0.00198375
	LOSS [training: 0.29949243151503807 | validation: 0.15477820367925882]
	TIME [epoch: 8.27 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2805416032188924		[learning rate: 0.0019814]
		[batch 20/20] avg loss: 0.28607253392747223		[learning rate: 0.0019791]
	Learning Rate: 0.00197907
	LOSS [training: 0.28330706857318233 | validation: 0.203740118957046]
	TIME [epoch: 8.22 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2354702416979127		[learning rate: 0.0019767]
		[batch 20/20] avg loss: 0.1605333412538152		[learning rate: 0.0019744]
	Learning Rate: 0.0019744
	LOSS [training: 0.1980017914758639 | validation: 0.1551363849300248]
	TIME [epoch: 8.21 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23332005179809068		[learning rate: 0.0019721]
		[batch 20/20] avg loss: 0.3448072987998922		[learning rate: 0.0019697]
	Learning Rate: 0.00196975
	LOSS [training: 0.28906367529899135 | validation: 0.1033122548083855]
	TIME [epoch: 8.24 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24116219449647272		[learning rate: 0.0019674]
		[batch 20/20] avg loss: 0.37536841725472436		[learning rate: 0.0019651]
	Learning Rate: 0.0019651
	LOSS [training: 0.3082653058755985 | validation: 0.27237284419763746]
	TIME [epoch: 8.26 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2198039600193205		[learning rate: 0.0019628]
		[batch 20/20] avg loss: 0.31403518646796774		[learning rate: 0.0019605]
	Learning Rate: 0.00196046
	LOSS [training: 0.2669195732436441 | validation: 0.13926813702645474]
	TIME [epoch: 8.21 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17567038051966172		[learning rate: 0.0019582]
		[batch 20/20] avg loss: 0.23745151193362807		[learning rate: 0.0019558]
	Learning Rate: 0.00195584
	LOSS [training: 0.2065609462266449 | validation: 0.1607831434176732]
	TIME [epoch: 8.22 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23271387897194273		[learning rate: 0.0019535]
		[batch 20/20] avg loss: 0.28516580365786615		[learning rate: 0.0019512]
	Learning Rate: 0.00195123
	LOSS [training: 0.25893984131490444 | validation: 0.29132522791541404]
	TIME [epoch: 8.22 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3154250492067962		[learning rate: 0.0019489]
		[batch 20/20] avg loss: 0.30398758131318687		[learning rate: 0.0019466]
	Learning Rate: 0.00194662
	LOSS [training: 0.3097063152599916 | validation: 0.14693815614884767]
	TIME [epoch: 8.24 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31922338981113063		[learning rate: 0.0019443]
		[batch 20/20] avg loss: 0.22206316313830357		[learning rate: 0.001942]
	Learning Rate: 0.00194203
	LOSS [training: 0.27064327647471714 | validation: 0.22846510524915342]
	TIME [epoch: 8.22 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2968228514406445		[learning rate: 0.0019397]
		[batch 20/20] avg loss: 0.38043645294697925		[learning rate: 0.0019375]
	Learning Rate: 0.00193745
	LOSS [training: 0.3386296521938119 | validation: 0.3456866172253728]
	TIME [epoch: 8.21 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32743321506769596		[learning rate: 0.0019352]
		[batch 20/20] avg loss: 0.2438117779869568		[learning rate: 0.0019329]
	Learning Rate: 0.00193288
	LOSS [training: 0.2856224965273264 | validation: 0.20561141849876124]
	TIME [epoch: 8.21 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32710234498105695		[learning rate: 0.0019306]
		[batch 20/20] avg loss: 0.2658510293084432		[learning rate: 0.0019283]
	Learning Rate: 0.00192832
	LOSS [training: 0.29647668714475006 | validation: 0.31052642471707126]
	TIME [epoch: 8.21 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3051893751581126		[learning rate: 0.001926]
		[batch 20/20] avg loss: 0.29218911988545215		[learning rate: 0.0019238]
	Learning Rate: 0.00192377
	LOSS [training: 0.2986892475217824 | validation: 0.20287099242131099]
	TIME [epoch: 8.16 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2732261371591527		[learning rate: 0.0019215]
		[batch 20/20] avg loss: 0.2571091596909417		[learning rate: 0.0019192]
	Learning Rate: 0.00191924
	LOSS [training: 0.2651676484250472 | validation: 0.17059917432050092]
	TIME [epoch: 8.17 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23243996026839703		[learning rate: 0.001917]
		[batch 20/20] avg loss: 0.246072567550091		[learning rate: 0.0019147]
	Learning Rate: 0.00191471
	LOSS [training: 0.239256263909244 | validation: 0.12231158116261465]
	TIME [epoch: 8.22 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35017556859600263		[learning rate: 0.0019124]
		[batch 20/20] avg loss: 0.27275664669704386		[learning rate: 0.0019102]
	Learning Rate: 0.00191019
	LOSS [training: 0.3114661076465232 | validation: 0.21808273303806047]
	TIME [epoch: 8.27 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23576214984607707		[learning rate: 0.0019079]
		[batch 20/20] avg loss: 0.27114820200886813		[learning rate: 0.0019057]
	Learning Rate: 0.00190569
	LOSS [training: 0.2534551759274726 | validation: 0.19819713630506705]
	TIME [epoch: 8.22 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22131462541261696		[learning rate: 0.0019034]
		[batch 20/20] avg loss: 0.33142434070834853		[learning rate: 0.0019012]
	Learning Rate: 0.00190119
	LOSS [training: 0.2763694830604827 | validation: 0.08956493833254311]
	TIME [epoch: 8.22 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29570322639016267		[learning rate: 0.0018989]
		[batch 20/20] avg loss: 0.3063748357404502		[learning rate: 0.0018967]
	Learning Rate: 0.00189671
	LOSS [training: 0.30103903106530644 | validation: 0.17328211071636065]
	TIME [epoch: 8.23 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30376014165695503		[learning rate: 0.0018945]
		[batch 20/20] avg loss: 0.2823346146066659		[learning rate: 0.0018922]
	Learning Rate: 0.00189223
	LOSS [training: 0.2930473781318105 | validation: 0.44619227405829753]
	TIME [epoch: 8.24 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.359903106867561		[learning rate: 0.00189]
		[batch 20/20] avg loss: 0.2766570617872146		[learning rate: 0.0018878]
	Learning Rate: 0.00188777
	LOSS [training: 0.3182800843273878 | validation: 0.20984365481527623]
	TIME [epoch: 8.2 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2667525421721666		[learning rate: 0.0018855]
		[batch 20/20] avg loss: 0.2750854009596096		[learning rate: 0.0018833]
	Learning Rate: 0.00188332
	LOSS [training: 0.27091897156588807 | validation: 0.1465265058332854]
	TIME [epoch: 8.21 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35989812957966494		[learning rate: 0.0018811]
		[batch 20/20] avg loss: 0.31538299605931513		[learning rate: 0.0018789]
	Learning Rate: 0.00187887
	LOSS [training: 0.33764056281949 | validation: 0.3420669780217423]
	TIME [epoch: 8.21 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26482188557647524		[learning rate: 0.0018767]
		[batch 20/20] avg loss: 0.28124332717700595		[learning rate: 0.0018744]
	Learning Rate: 0.00187444
	LOSS [training: 0.2730326063767406 | validation: 0.418171583790368]
	TIME [epoch: 8.24 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3148264921094518		[learning rate: 0.0018722]
		[batch 20/20] avg loss: 0.30351370162769686		[learning rate: 0.00187]
	Learning Rate: 0.00187002
	LOSS [training: 0.30917009686857444 | validation: 0.21061792401200033]
	TIME [epoch: 8.18 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19783034625241575		[learning rate: 0.0018678]
		[batch 20/20] avg loss: 0.17441487157472474		[learning rate: 0.0018656]
	Learning Rate: 0.00186561
	LOSS [training: 0.18612260891357024 | validation: 0.12744207084202241]
	TIME [epoch: 8.18 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1758859444260994		[learning rate: 0.0018634]
		[batch 20/20] avg loss: 0.20111295590282802		[learning rate: 0.0018612]
	Learning Rate: 0.00186121
	LOSS [training: 0.1884994501644637 | validation: 0.126286649718265]
	TIME [epoch: 8.2 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27098384672548903		[learning rate: 0.001859]
		[batch 20/20] avg loss: 0.2066571741846867		[learning rate: 0.0018568]
	Learning Rate: 0.00185682
	LOSS [training: 0.23882051045508784 | validation: 0.09360900706418467]
	TIME [epoch: 8.23 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19887864068125097		[learning rate: 0.0018546]
		[batch 20/20] avg loss: 0.22657334647677202		[learning rate: 0.0018524]
	Learning Rate: 0.00185244
	LOSS [training: 0.2127259935790115 | validation: 0.10818172118083745]
	TIME [epoch: 8.18 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17364681022202985		[learning rate: 0.0018503]
		[batch 20/20] avg loss: 0.15814825111935432		[learning rate: 0.0018481]
	Learning Rate: 0.00184807
	LOSS [training: 0.16589753067069207 | validation: 0.3736947364985175]
	TIME [epoch: 8.19 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19072716948145743		[learning rate: 0.0018459]
		[batch 20/20] avg loss: 0.18437208906349756		[learning rate: 0.0018437]
	Learning Rate: 0.00184371
	LOSS [training: 0.18754962927247748 | validation: 0.10046701059306475]
	TIME [epoch: 8.2 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18868512712557398		[learning rate: 0.0018415]
		[batch 20/20] avg loss: 0.2500419019383812		[learning rate: 0.0018394]
	Learning Rate: 0.00183936
	LOSS [training: 0.21936351453197758 | validation: 0.13006669599059026]
	TIME [epoch: 8.23 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23123561499899906		[learning rate: 0.0018372]
		[batch 20/20] avg loss: 0.1870160004683898		[learning rate: 0.001835]
	Learning Rate: 0.00183502
	LOSS [training: 0.20912580773369438 | validation: 0.08064199002927785]
	TIME [epoch: 8.19 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24530873117518626		[learning rate: 0.0018329]
		[batch 20/20] avg loss: 0.21382858831450452		[learning rate: 0.0018307]
	Learning Rate: 0.00183069
	LOSS [training: 0.22956865974484542 | validation: 0.23549736418374675]
	TIME [epoch: 8.19 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1895026873929929		[learning rate: 0.0018285]
		[batch 20/20] avg loss: 0.23402895920249267		[learning rate: 0.0018264]
	Learning Rate: 0.00182637
	LOSS [training: 0.2117658232977428 | validation: 0.12699247606994643]
	TIME [epoch: 8.21 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17154972489872972		[learning rate: 0.0018242]
		[batch 20/20] avg loss: 0.1891445956001959		[learning rate: 0.0018221]
	Learning Rate: 0.00182207
	LOSS [training: 0.1803471602494628 | validation: 0.1053222813252758]
	TIME [epoch: 8.23 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18717987830363542		[learning rate: 0.0018199]
		[batch 20/20] avg loss: 0.25043783983906376		[learning rate: 0.0018178]
	Learning Rate: 0.00181777
	LOSS [training: 0.2188088590713496 | validation: 0.06252253245962971]
	TIME [epoch: 8.19 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15037606551125116		[learning rate: 0.0018156]
		[batch 20/20] avg loss: 0.2519169542196945		[learning rate: 0.0018135]
	Learning Rate: 0.00181348
	LOSS [training: 0.20114650986547283 | validation: 0.19963998000999955]
	TIME [epoch: 8.19 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22192532272640567		[learning rate: 0.0018113]
		[batch 20/20] avg loss: 0.15808865152829332		[learning rate: 0.0018092]
	Learning Rate: 0.0018092
	LOSS [training: 0.19000698712734948 | validation: 0.08819575730647741]
	TIME [epoch: 8.22 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2007422398810914		[learning rate: 0.0018071]
		[batch 20/20] avg loss: 0.1783853456548214		[learning rate: 0.0018049]
	Learning Rate: 0.00180493
	LOSS [training: 0.18956379276795643 | validation: 0.21336085763665724]
	TIME [epoch: 8.21 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22442241795214746		[learning rate: 0.0018028]
		[batch 20/20] avg loss: 0.30075960605385776		[learning rate: 0.0018007]
	Learning Rate: 0.00180068
	LOSS [training: 0.2625910120030026 | validation: 0.106787635983359]
	TIME [epoch: 8.19 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24804065193814276		[learning rate: 0.0017986]
		[batch 20/20] avg loss: 0.25319323240855957		[learning rate: 0.0017964]
	Learning Rate: 0.00179643
	LOSS [training: 0.2506169421733512 | validation: 0.0780605955236842]
	TIME [epoch: 8.17 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25874509850296923		[learning rate: 0.0017943]
		[batch 20/20] avg loss: 0.16851455462567316		[learning rate: 0.0017922]
	Learning Rate: 0.00179219
	LOSS [training: 0.21362982656432122 | validation: 0.10489134165910143]
	TIME [epoch: 8.21 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1916349695727775		[learning rate: 0.0017901]
		[batch 20/20] avg loss: 0.15228265582803532		[learning rate: 0.001788]
	Learning Rate: 0.00178796
	LOSS [training: 0.17195881270040636 | validation: 0.3311325155884323]
	TIME [epoch: 8.2 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1820845083164669		[learning rate: 0.0017859]
		[batch 20/20] avg loss: 0.16183439825970994		[learning rate: 0.0017837]
	Learning Rate: 0.00178375
	LOSS [training: 0.1719594532880884 | validation: 0.07957015645021102]
	TIME [epoch: 8.19 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15336109871933953		[learning rate: 0.0017816]
		[batch 20/20] avg loss: 0.22409336068092583		[learning rate: 0.0017795]
	Learning Rate: 0.00177954
	LOSS [training: 0.1887272297001327 | validation: 0.06746153330603406]
	TIME [epoch: 8.18 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15749620951080406		[learning rate: 0.0017774]
		[batch 20/20] avg loss: 0.203151673043447		[learning rate: 0.0017753]
	Learning Rate: 0.00177534
	LOSS [training: 0.18032394127712556 | validation: 0.2120760280546612]
	TIME [epoch: 8.22 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1689804270002856		[learning rate: 0.0017732]
		[batch 20/20] avg loss: 0.20124261142036465		[learning rate: 0.0017712]
	Learning Rate: 0.00177115
	LOSS [training: 0.18511151921032515 | validation: 0.09404477189280391]
	TIME [epoch: 8.2 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1497300913288697		[learning rate: 0.0017691]
		[batch 20/20] avg loss: 0.1862467352298447		[learning rate: 0.001767]
	Learning Rate: 0.00176698
	LOSS [training: 0.1679884132793572 | validation: 0.08508046553075405]
	TIME [epoch: 8.2 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14770957544648225		[learning rate: 0.0017649]
		[batch 20/20] avg loss: 0.2133730128637244		[learning rate: 0.0017628]
	Learning Rate: 0.00176281
	LOSS [training: 0.1805412941551033 | validation: 0.10981938408308113]
	TIME [epoch: 8.19 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3814264084857758		[learning rate: 0.0017607]
		[batch 20/20] avg loss: 0.14490955094157176		[learning rate: 0.0017587]
	Learning Rate: 0.00175865
	LOSS [training: 0.2631679797136738 | validation: 0.0789541246844555]
	TIME [epoch: 8.23 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2302738063665682		[learning rate: 0.0017566]
		[batch 20/20] avg loss: 0.18506597215206017		[learning rate: 0.0017545]
	Learning Rate: 0.0017545
	LOSS [training: 0.20766988925931415 | validation: 0.11060055059626037]
	TIME [epoch: 8.2 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1818825202721099		[learning rate: 0.0017524]
		[batch 20/20] avg loss: 0.19145935937298034		[learning rate: 0.0017504]
	Learning Rate: 0.00175036
	LOSS [training: 0.1866709398225451 | validation: 0.3222067613860576]
	TIME [epoch: 8.19 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23032862935290774		[learning rate: 0.0017483]
		[batch 20/20] avg loss: 0.14698443331407496		[learning rate: 0.0017462]
	Learning Rate: 0.00174623
	LOSS [training: 0.18865653133349133 | validation: 0.0920620576096302]
	TIME [epoch: 8.18 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12790537910465674		[learning rate: 0.0017442]
		[batch 20/20] avg loss: 0.23913803661002392		[learning rate: 0.0017421]
	Learning Rate: 0.00174212
	LOSS [training: 0.18352170785734032 | validation: 0.11899390561168967]
	TIME [epoch: 8.22 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15137058991967187		[learning rate: 0.0017401]
		[batch 20/20] avg loss: 0.15062520390319384		[learning rate: 0.001738]
	Learning Rate: 0.00173801
	LOSS [training: 0.15099789691143278 | validation: 0.07077177583113162]
	TIME [epoch: 8.2 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15003341847781906		[learning rate: 0.001736]
		[batch 20/20] avg loss: 0.1426059837492253		[learning rate: 0.0017339]
	Learning Rate: 0.00173391
	LOSS [training: 0.14631970111352216 | validation: 0.39704380605787243]
	TIME [epoch: 8.19 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1867651317330115		[learning rate: 0.0017319]
		[batch 20/20] avg loss: 0.2182772120289489		[learning rate: 0.0017298]
	Learning Rate: 0.00172982
	LOSS [training: 0.2025211718809802 | validation: 0.05221699955426526]
	TIME [epoch: 8.19 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3212972227553062		[learning rate: 0.0017278]
		[batch 20/20] avg loss: 0.1893252551242687		[learning rate: 0.0017257]
	Learning Rate: 0.00172574
	LOSS [training: 0.2553112389397874 | validation: 0.1750983766121116]
	TIME [epoch: 8.21 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15689174002022818		[learning rate: 0.0017237]
		[batch 20/20] avg loss: 0.14547262137567474		[learning rate: 0.0017217]
	Learning Rate: 0.00172167
	LOSS [training: 0.15118218069795145 | validation: 0.09926846857831048]
	TIME [epoch: 8.19 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2217761621791406		[learning rate: 0.0017196]
		[batch 20/20] avg loss: 0.15784311848613702		[learning rate: 0.0017176]
	Learning Rate: 0.0017176
	LOSS [training: 0.18980964033263883 | validation: 0.1276033625347514]
	TIME [epoch: 8.2 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1113226579342399		[learning rate: 0.0017156]
		[batch 20/20] avg loss: 0.2703929976215528		[learning rate: 0.0017136]
	Learning Rate: 0.00171355
	LOSS [training: 0.1908578277778963 | validation: 0.08629543841947276]
	TIME [epoch: 8.2 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17477586239576196		[learning rate: 0.0017115]
		[batch 20/20] avg loss: 0.11483956908526796		[learning rate: 0.0017095]
	Learning Rate: 0.00170951
	LOSS [training: 0.14480771574051493 | validation: 0.0636109322491608]
	TIME [epoch: 8.2 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17530617619773428		[learning rate: 0.0017075]
		[batch 20/20] avg loss: 0.12528874674364449		[learning rate: 0.0017055]
	Learning Rate: 0.00170548
	LOSS [training: 0.15029746147068934 | validation: 0.0940645338295303]
	TIME [epoch: 8.19 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13194455705599956		[learning rate: 0.0017035]
		[batch 20/20] avg loss: 0.17709174243791487		[learning rate: 0.0017015]
	Learning Rate: 0.00170146
	LOSS [training: 0.1545181497469572 | validation: 0.04854702197466246]
	TIME [epoch: 8.21 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15282023768415454		[learning rate: 0.0016994]
		[batch 20/20] avg loss: 0.17201053583198533		[learning rate: 0.0016974]
	Learning Rate: 0.00169744
	LOSS [training: 0.16241538675806994 | validation: 0.079935952566317]
	TIME [epoch: 8.19 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.130556884993018		[learning rate: 0.0016954]
		[batch 20/20] avg loss: 0.188092326984958		[learning rate: 0.0016934]
	Learning Rate: 0.00169344
	LOSS [training: 0.15932460598898804 | validation: 0.10718152727831264]
	TIME [epoch: 8.21 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16727789170220478		[learning rate: 0.0016914]
		[batch 20/20] avg loss: 0.1673372753302772		[learning rate: 0.0016894]
	Learning Rate: 0.00168944
	LOSS [training: 0.16730758351624098 | validation: 0.1791520354583909]
	TIME [epoch: 8.18 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14407273913976895		[learning rate: 0.0016874]
		[batch 20/20] avg loss: 0.14915800118175912		[learning rate: 0.0016855]
	Learning Rate: 0.00168546
	LOSS [training: 0.14661537016076404 | validation: 0.08350328364833427]
	TIME [epoch: 8.2 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17957649483290528		[learning rate: 0.0016835]
		[batch 20/20] avg loss: 0.16902741275791291		[learning rate: 0.0016815]
	Learning Rate: 0.00168148
	LOSS [training: 0.17430195379540908 | validation: 0.06525882805268993]
	TIME [epoch: 8.21 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15181835396203433		[learning rate: 0.0016795]
		[batch 20/20] avg loss: 0.20587705697715236		[learning rate: 0.0016775]
	Learning Rate: 0.00167752
	LOSS [training: 0.17884770546959336 | validation: 0.07629842280001338]
	TIME [epoch: 8.21 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15896193190112534		[learning rate: 0.0016755]
		[batch 20/20] avg loss: 0.1803442961232971		[learning rate: 0.0016736]
	Learning Rate: 0.00167356
	LOSS [training: 0.16965311401221123 | validation: 0.16106346139437674]
	TIME [epoch: 8.18 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13675313018599478		[learning rate: 0.0016716]
		[batch 20/20] avg loss: 0.12091022940956253		[learning rate: 0.0016696]
	Learning Rate: 0.00166961
	LOSS [training: 0.12883167979777865 | validation: 0.14038845896407612]
	TIME [epoch: 8.2 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16834816790428458		[learning rate: 0.0016676]
		[batch 20/20] avg loss: 0.17755030349476034		[learning rate: 0.0016657]
	Learning Rate: 0.00166567
	LOSS [training: 0.17294923569952242 | validation: 0.07260885808483858]
	TIME [epoch: 8.2 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16337184268891997		[learning rate: 0.0016637]
		[batch 20/20] avg loss: 0.1752981517450301		[learning rate: 0.0016617]
	Learning Rate: 0.00166174
	LOSS [training: 0.16933499721697504 | validation: 0.23533820679373033]
	TIME [epoch: 8.21 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1631725145424817		[learning rate: 0.0016598]
		[batch 20/20] avg loss: 0.11935985281428267		[learning rate: 0.0016578]
	Learning Rate: 0.00165782
	LOSS [training: 0.14126618367838217 | validation: 0.07645363178792802]
	TIME [epoch: 8.19 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12490466251605667		[learning rate: 0.0016559]
		[batch 20/20] avg loss: 0.226574535391645		[learning rate: 0.0016539]
	Learning Rate: 0.00165391
	LOSS [training: 0.1757395989538508 | validation: 0.06629836823180917]
	TIME [epoch: 8.21 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1557416553133436		[learning rate: 0.001652]
		[batch 20/20] avg loss: 0.1364508426042866		[learning rate: 0.00165]
	Learning Rate: 0.00165001
	LOSS [training: 0.1460962489588151 | validation: 0.07129481955515878]
	TIME [epoch: 8.21 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17107034635809942		[learning rate: 0.0016481]
		[batch 20/20] avg loss: 0.15089743159235486		[learning rate: 0.0016461]
	Learning Rate: 0.00164612
	LOSS [training: 0.1609838889752271 | validation: 0.21249708439744563]
	TIME [epoch: 8.21 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14009149950533836		[learning rate: 0.0016442]
		[batch 20/20] avg loss: 0.27567780053754526		[learning rate: 0.0016422]
	Learning Rate: 0.00164224
	LOSS [training: 0.20788465002144182 | validation: 0.07784932882570268]
	TIME [epoch: 8.19 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12512802948585994		[learning rate: 0.0016403]
		[batch 20/20] avg loss: 0.15908789263769657		[learning rate: 0.0016384]
	Learning Rate: 0.00163836
	LOSS [training: 0.14210796106177828 | validation: 0.07109981209989964]
	TIME [epoch: 8.21 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15036520205361192		[learning rate: 0.0016364]
		[batch 20/20] avg loss: 0.1265304915214066		[learning rate: 0.0016345]
	Learning Rate: 0.0016345
	LOSS [training: 0.13844784678750924 | validation: 0.07804709428060128]
	TIME [epoch: 8.2 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14965554723602462		[learning rate: 0.0016326]
		[batch 20/20] avg loss: 0.13613444850471862		[learning rate: 0.0016306]
	Learning Rate: 0.00163064
	LOSS [training: 0.1428949978703716 | validation: 0.4131667183021034]
	TIME [epoch: 8.21 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17495311970066846		[learning rate: 0.0016287]
		[batch 20/20] avg loss: 0.14500390882447084		[learning rate: 0.0016268]
	Learning Rate: 0.0016268
	LOSS [training: 0.15997851426256965 | validation: 0.1554923847400197]
	TIME [epoch: 8.18 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1443607175795044		[learning rate: 0.0016249]
		[batch 20/20] avg loss: 0.14231701350612885		[learning rate: 0.001623]
	Learning Rate: 0.00162296
	LOSS [training: 0.14333886554281663 | validation: 0.08803119939543753]
	TIME [epoch: 8.22 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1277581519142277		[learning rate: 0.001621]
		[batch 20/20] avg loss: 0.16976973627849135		[learning rate: 0.0016191]
	Learning Rate: 0.00161913
	LOSS [training: 0.14876394409635954 | validation: 0.08812719689102333]
	TIME [epoch: 8.19 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12097598655292582		[learning rate: 0.0016172]
		[batch 20/20] avg loss: 0.15468944638979296		[learning rate: 0.0016153]
	Learning Rate: 0.00161531
	LOSS [training: 0.1378327164713594 | validation: 0.5964033868461025]
	TIME [epoch: 8.21 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2878513589455328		[learning rate: 0.0016134]
		[batch 20/20] avg loss: 0.12264042238310688		[learning rate: 0.0016115]
	Learning Rate: 0.0016115
	LOSS [training: 0.2052458906643198 | validation: 0.1158063418350342]
	TIME [epoch: 8.19 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14654595653763602		[learning rate: 0.0016096]
		[batch 20/20] avg loss: 0.2230911955029911		[learning rate: 0.0016077]
	Learning Rate: 0.0016077
	LOSS [training: 0.18481857602031357 | validation: 0.11524185878332563]
	TIME [epoch: 8.21 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15758759565660793		[learning rate: 0.0016058]
		[batch 20/20] avg loss: 0.17315926385562835		[learning rate: 0.0016039]
	Learning Rate: 0.00160391
	LOSS [training: 0.16537342975611816 | validation: 0.21830738517892714]
	TIME [epoch: 8.21 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1366019366475601		[learning rate: 0.001602]
		[batch 20/20] avg loss: 0.1623309284404241		[learning rate: 0.0016001]
	Learning Rate: 0.00160012
	LOSS [training: 0.14946643254399214 | validation: 0.09649870611303554]
	TIME [epoch: 8.21 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12961768306683336		[learning rate: 0.0015982]
		[batch 20/20] avg loss: 0.15119640523073047		[learning rate: 0.0015964]
	Learning Rate: 0.00159635
	LOSS [training: 0.14040704414878188 | validation: 0.09548343203293135]
	TIME [epoch: 8.18 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17407233631554417		[learning rate: 0.0015945]
		[batch 20/20] avg loss: 0.14291661865932553		[learning rate: 0.0015926]
	Learning Rate: 0.00159258
	LOSS [training: 0.15849447748743486 | validation: 0.05317939968058095]
	TIME [epoch: 8.2 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.202104338460116		[learning rate: 0.0015907]
		[batch 20/20] avg loss: 0.14835861609236595		[learning rate: 0.0015888]
	Learning Rate: 0.00158883
	LOSS [training: 0.17523147727624094 | validation: 0.10230125456533928]
	TIME [epoch: 8.21 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1279319064327776		[learning rate: 0.001587]
		[batch 20/20] avg loss: 0.12251817812535455		[learning rate: 0.0015851]
	Learning Rate: 0.00158508
	LOSS [training: 0.12522504227906608 | validation: 0.20204362130279735]
	TIME [epoch: 8.21 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22638096690910384		[learning rate: 0.0015832]
		[batch 20/20] avg loss: 0.1729257289885669		[learning rate: 0.0015813]
	Learning Rate: 0.00158134
	LOSS [training: 0.19965334794883538 | validation: 0.09951168793791992]
	TIME [epoch: 8.18 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13435499414818233		[learning rate: 0.0015795]
		[batch 20/20] avg loss: 0.1730433440633931		[learning rate: 0.0015776]
	Learning Rate: 0.00157761
	LOSS [training: 0.15369916910578768 | validation: 0.17303057217559528]
	TIME [epoch: 8.21 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18869259987838846		[learning rate: 0.0015757]
		[batch 20/20] avg loss: 0.1650031543512777		[learning rate: 0.0015739]
	Learning Rate: 0.00157389
	LOSS [training: 0.1768478771148331 | validation: 0.10253779442156548]
	TIME [epoch: 8.2 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18377724845900328		[learning rate: 0.001572]
		[batch 20/20] avg loss: 0.18140364414686472		[learning rate: 0.0015702]
	Learning Rate: 0.00157018
	LOSS [training: 0.182590446302934 | validation: 0.0859848644016548]
	TIME [epoch: 8.2 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18594339013239453		[learning rate: 0.0015683]
		[batch 20/20] avg loss: 0.21938344900355386		[learning rate: 0.0015665]
	Learning Rate: 0.00156647
	LOSS [training: 0.20266341956797423 | validation: 0.1671269261470229]
	TIME [epoch: 8.17 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22658437302214657		[learning rate: 0.0015646]
		[batch 20/20] avg loss: 0.18886859808245443		[learning rate: 0.0015628]
	Learning Rate: 0.00156278
	LOSS [training: 0.2077264855523005 | validation: 0.1331298577840408]
	TIME [epoch: 8.2 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.187340160535722		[learning rate: 0.0015609]
		[batch 20/20] avg loss: 0.20749652307025385		[learning rate: 0.0015591]
	Learning Rate: 0.00155909
	LOSS [training: 0.19741834180298795 | validation: 0.10403903753986753]
	TIME [epoch: 8.21 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13009036519484105		[learning rate: 0.0015573]
		[batch 20/20] avg loss: 0.13375045294887278		[learning rate: 0.0015554]
	Learning Rate: 0.00155541
	LOSS [training: 0.1319204090718569 | validation: 0.06110291431516317]
	TIME [epoch: 8.19 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1260466899766591		[learning rate: 0.0015536]
		[batch 20/20] avg loss: 0.15727952837998183		[learning rate: 0.0015517]
	Learning Rate: 0.00155175
	LOSS [training: 0.14166310917832045 | validation: 0.14167893818243443]
	TIME [epoch: 8.18 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.122936104308639		[learning rate: 0.0015499]
		[batch 20/20] avg loss: 0.2306442801248645		[learning rate: 0.0015481]
	Learning Rate: 0.00154809
	LOSS [training: 0.17679019221675177 | validation: 0.06259750414126455]
	TIME [epoch: 8.22 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1457782637042109		[learning rate: 0.0015463]
		[batch 20/20] avg loss: 0.17334467019764505		[learning rate: 0.0015444]
	Learning Rate: 0.00154443
	LOSS [training: 0.15956146695092796 | validation: 0.28359214872882504]
	TIME [epoch: 8.21 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24769007668673787		[learning rate: 0.0015426]
		[batch 20/20] avg loss: 0.22146553440885		[learning rate: 0.0015408]
	Learning Rate: 0.00154079
	LOSS [training: 0.23457780554779392 | validation: 0.23436789184995468]
	TIME [epoch: 8.19 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37118757149985443		[learning rate: 0.001539]
		[batch 20/20] avg loss: 0.142958700401795		[learning rate: 0.0015372]
	Learning Rate: 0.00153716
	LOSS [training: 0.2570731359508247 | validation: 0.07992499518037904]
	TIME [epoch: 8.17 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20699930498715732		[learning rate: 0.0015353]
		[batch 20/20] avg loss: 0.12921747960840851		[learning rate: 0.0015335]
	Learning Rate: 0.00153353
	LOSS [training: 0.1681083922977829 | validation: 0.10131824189694305]
	TIME [epoch: 8.21 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12979343344975966		[learning rate: 0.0015317]
		[batch 20/20] avg loss: 0.1328990732926359		[learning rate: 0.0015299]
	Learning Rate: 0.00152991
	LOSS [training: 0.1313462533711978 | validation: 0.19120641474999342]
	TIME [epoch: 8.22 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19434455312816912		[learning rate: 0.0015281]
		[batch 20/20] avg loss: 0.17387621620646304		[learning rate: 0.0015263]
	Learning Rate: 0.0015263
	LOSS [training: 0.18411038466731605 | validation: 0.23602547309379146]
	TIME [epoch: 8.19 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1674232817074505		[learning rate: 0.0015245]
		[batch 20/20] avg loss: 0.13719288276096903		[learning rate: 0.0015227]
	Learning Rate: 0.0015227
	LOSS [training: 0.15230808223420975 | validation: 0.058835124652300494]
	TIME [epoch: 8.17 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14407998313159506		[learning rate: 0.0015209]
		[batch 20/20] avg loss: 0.15375177323981007		[learning rate: 0.0015191]
	Learning Rate: 0.00151911
	LOSS [training: 0.14891587818570257 | validation: 0.11456455272479477]
	TIME [epoch: 8.2 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13962897741174043		[learning rate: 0.0015173]
		[batch 20/20] avg loss: 0.10549183600840273		[learning rate: 0.0015155]
	Learning Rate: 0.00151553
	LOSS [training: 0.1225604067100716 | validation: 0.15813464159050772]
	TIME [epoch: 8.22 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1432385341054813		[learning rate: 0.0015137]
		[batch 20/20] avg loss: 0.14742795314833287		[learning rate: 0.001512]
	Learning Rate: 0.00151195
	LOSS [training: 0.14533324362690708 | validation: 0.13175293599480575]
	TIME [epoch: 8.17 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1648285340147258		[learning rate: 0.0015102]
		[batch 20/20] avg loss: 0.1757236311274649		[learning rate: 0.0015084]
	Learning Rate: 0.00150839
	LOSS [training: 0.17027608257109533 | validation: 0.0557777168539169]
	TIME [epoch: 8.17 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17027922297080716		[learning rate: 0.0015066]
		[batch 20/20] avg loss: 0.17541420628110627		[learning rate: 0.0015048]
	Learning Rate: 0.00150483
	LOSS [training: 0.17284671462595672 | validation: 0.08617663353331571]
	TIME [epoch: 8.2 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13296211432342114		[learning rate: 0.0015031]
		[batch 20/20] avg loss: 0.14094231575238939		[learning rate: 0.0015013]
	Learning Rate: 0.00150128
	LOSS [training: 0.13695221503790528 | validation: 0.12983792649494863]
	TIME [epoch: 8.22 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11728238943848972		[learning rate: 0.0014995]
		[batch 20/20] avg loss: 0.16668669641497358		[learning rate: 0.0014977]
	Learning Rate: 0.00149774
	LOSS [training: 0.14198454292673163 | validation: 0.15308799025333844]
	TIME [epoch: 8.19 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15863236301607367		[learning rate: 0.001496]
		[batch 20/20] avg loss: 0.12028905909063817		[learning rate: 0.0014942]
	Learning Rate: 0.0014942
	LOSS [training: 0.13946071105335592 | validation: 0.10910410994276826]
	TIME [epoch: 8.18 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15201000462807374		[learning rate: 0.0014924]
		[batch 20/20] avg loss: 0.14343113238710328		[learning rate: 0.0014907]
	Learning Rate: 0.00149068
	LOSS [training: 0.1477205685075885 | validation: 0.051372495898116294]
	TIME [epoch: 8.2 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1485558206215017		[learning rate: 0.0014889]
		[batch 20/20] avg loss: 0.15787322726559289		[learning rate: 0.0014872]
	Learning Rate: 0.00148716
	LOSS [training: 0.1532145239435473 | validation: 0.11496387722738634]
	TIME [epoch: 8.23 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15160946770357253		[learning rate: 0.0014854]
		[batch 20/20] avg loss: 0.1279029285087066		[learning rate: 0.0014837]
	Learning Rate: 0.00148366
	LOSS [training: 0.13975619810613954 | validation: 0.04922542493986993]
	TIME [epoch: 8.17 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1338202163736844		[learning rate: 0.0014819]
		[batch 20/20] avg loss: 0.13683900080558797		[learning rate: 0.0014802]
	Learning Rate: 0.00148016
	LOSS [training: 0.13532960858963616 | validation: 0.08151726554212804]
	TIME [epoch: 8.17 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17646899943023733		[learning rate: 0.0014784]
		[batch 20/20] avg loss: 0.10912241777210303		[learning rate: 0.0014767]
	Learning Rate: 0.00147667
	LOSS [training: 0.14279570860117022 | validation: 0.039785007079316835]
	TIME [epoch: 8.18 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11958015271871918		[learning rate: 0.0014749]
		[batch 20/20] avg loss: 0.12679962846290946		[learning rate: 0.0014732]
	Learning Rate: 0.00147318
	LOSS [training: 0.12318989059081431 | validation: 0.08573943732711606]
	TIME [epoch: 8.24 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13140879150801044		[learning rate: 0.0014714]
		[batch 20/20] avg loss: 0.10170870636629678		[learning rate: 0.0014697]
	Learning Rate: 0.00146971
	LOSS [training: 0.11655874893715361 | validation: 0.07266552644272747]
	TIME [epoch: 8.18 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19848096767490994		[learning rate: 0.001468]
		[batch 20/20] avg loss: 0.1475909016749685		[learning rate: 0.0014662]
	Learning Rate: 0.00146624
	LOSS [training: 0.17303593467493922 | validation: 0.09289345680046449]
	TIME [epoch: 8.17 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15015691922086832		[learning rate: 0.0014645]
		[batch 20/20] avg loss: 0.13193930729029085		[learning rate: 0.0014628]
	Learning Rate: 0.00146278
	LOSS [training: 0.14104811325557956 | validation: 0.13106969551397152]
	TIME [epoch: 8.21 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17892410759198474		[learning rate: 0.0014611]
		[batch 20/20] avg loss: 0.17696405500460258		[learning rate: 0.0014593]
	Learning Rate: 0.00145933
	LOSS [training: 0.17794408129829364 | validation: 0.18523111565536804]
	TIME [epoch: 8.21 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13392593431503078		[learning rate: 0.0014576]
		[batch 20/20] avg loss: 0.15919242529431626		[learning rate: 0.0014559]
	Learning Rate: 0.00145589
	LOSS [training: 0.14655917980467353 | validation: 0.0854570739893207]
	TIME [epoch: 8.18 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13601063875048064		[learning rate: 0.0014542]
		[batch 20/20] avg loss: 0.22659805821202025		[learning rate: 0.0014525]
	Learning Rate: 0.00145245
	LOSS [training: 0.18130434848125046 | validation: 0.15597785035160378]
	TIME [epoch: 8.18 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1190391023956161		[learning rate: 0.0014507]
		[batch 20/20] avg loss: 0.3402797146138971		[learning rate: 0.001449]
	Learning Rate: 0.00144903
	LOSS [training: 0.2296594085047566 | validation: 0.17952249815301657]
	TIME [epoch: 8.22 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24864912592635738		[learning rate: 0.0014473]
		[batch 20/20] avg loss: 0.14373415881075194		[learning rate: 0.0014456]
	Learning Rate: 0.00144561
	LOSS [training: 0.19619164236855463 | validation: 0.05095629558298712]
	TIME [epoch: 8.23 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16152876296235283		[learning rate: 0.0014439]
		[batch 20/20] avg loss: 0.12378494959883743		[learning rate: 0.0014422]
	Learning Rate: 0.0014422
	LOSS [training: 0.14265685628059513 | validation: 0.1337020476496706]
	TIME [epoch: 8.18 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13596960579985473		[learning rate: 0.0014405]
		[batch 20/20] avg loss: 0.15221774362116372		[learning rate: 0.0014388]
	Learning Rate: 0.0014388
	LOSS [training: 0.14409367471050924 | validation: 0.1444904825784042]
	TIME [epoch: 8.19 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1662716105339753		[learning rate: 0.0014371]
		[batch 20/20] avg loss: 0.10508007045499594		[learning rate: 0.0014354]
	Learning Rate: 0.0014354
	LOSS [training: 0.1356758404944856 | validation: 0.14547388807593944]
	TIME [epoch: 8.21 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12727311072925238		[learning rate: 0.0014337]
		[batch 20/20] avg loss: 0.12722629188352275		[learning rate: 0.001432]
	Learning Rate: 0.00143202
	LOSS [training: 0.12724970130638757 | validation: 0.08851041617769151]
	TIME [epoch: 8.23 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12753930469725988		[learning rate: 0.0014303]
		[batch 20/20] avg loss: 0.1600034500395681		[learning rate: 0.0014286]
	Learning Rate: 0.00142864
	LOSS [training: 0.14377137736841397 | validation: 0.08933262955452836]
	TIME [epoch: 8.18 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1408910113985006		[learning rate: 0.001427]
		[batch 20/20] avg loss: 0.13629220401287184		[learning rate: 0.0014253]
	Learning Rate: 0.00142527
	LOSS [training: 0.1385916077056862 | validation: 0.06458884506822105]
	TIME [epoch: 8.18 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11818647725956921		[learning rate: 0.0014236]
		[batch 20/20] avg loss: 0.12543120498689728		[learning rate: 0.0014219]
	Learning Rate: 0.00142191
	LOSS [training: 0.12180884112323324 | validation: 0.10673216558064105]
	TIME [epoch: 8.22 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11437727045273897		[learning rate: 0.0014202]
		[batch 20/20] avg loss: 0.11197369585838561		[learning rate: 0.0014186]
	Learning Rate: 0.00141855
	LOSS [training: 0.11317548315556228 | validation: 0.052659892042163106]
	TIME [epoch: 8.23 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12229022327348613		[learning rate: 0.0014169]
		[batch 20/20] avg loss: 0.1340128922173669		[learning rate: 0.0014152]
	Learning Rate: 0.00141521
	LOSS [training: 0.12815155774542647 | validation: 0.0678533972795302]
	TIME [epoch: 8.18 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12945850300043188		[learning rate: 0.0014135]
		[batch 20/20] avg loss: 0.14641178206848596		[learning rate: 0.0014119]
	Learning Rate: 0.00141187
	LOSS [training: 0.13793514253445893 | validation: 0.09791903149637698]
	TIME [epoch: 8.18 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12876226855579948		[learning rate: 0.0014102]
		[batch 20/20] avg loss: 0.20024083339711107		[learning rate: 0.0014085]
	Learning Rate: 0.00140854
	LOSS [training: 0.16450155097645527 | validation: 0.10463608248695612]
	TIME [epoch: 8.22 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10886705541872488		[learning rate: 0.0014069]
		[batch 20/20] avg loss: 0.29152503167653293		[learning rate: 0.0014052]
	Learning Rate: 0.00140522
	LOSS [training: 0.20019604354762893 | validation: 0.13588522375382528]
	TIME [epoch: 8.22 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11265354748970155		[learning rate: 0.0014036]
		[batch 20/20] avg loss: 0.14588241059250057		[learning rate: 0.0014019]
	Learning Rate: 0.0014019
	LOSS [training: 0.12926797904110104 | validation: 0.05052871160645476]
	TIME [epoch: 8.19 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14929940964719557		[learning rate: 0.0014002]
		[batch 20/20] avg loss: 0.11832008222336625		[learning rate: 0.0013986]
	Learning Rate: 0.0013986
	LOSS [training: 0.1338097459352809 | validation: 0.04319028760406428]
	TIME [epoch: 8.19 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12162391935192067		[learning rate: 0.0013969]
		[batch 20/20] avg loss: 0.11055084139709867		[learning rate: 0.0013953]
	Learning Rate: 0.0013953
	LOSS [training: 0.11608738037450968 | validation: 0.14447602665101328]
	TIME [epoch: 8.21 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1529288484982091		[learning rate: 0.0013937]
		[batch 20/20] avg loss: 0.1623219395604949		[learning rate: 0.001392]
	Learning Rate: 0.00139201
	LOSS [training: 0.157625394029352 | validation: 0.18639129697361617]
	TIME [epoch: 8.22 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14860492368641456		[learning rate: 0.0013904]
		[batch 20/20] avg loss: 0.15373067164585072		[learning rate: 0.0013887]
	Learning Rate: 0.00138872
	LOSS [training: 0.15116779766613267 | validation: 0.16077435791371447]
	TIME [epoch: 8.19 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1510061696087634		[learning rate: 0.0013871]
		[batch 20/20] avg loss: 0.11547044701128276		[learning rate: 0.0013854]
	Learning Rate: 0.00138545
	LOSS [training: 0.1332383083100231 | validation: 0.0977182418965594]
	TIME [epoch: 8.18 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13933077893116558		[learning rate: 0.0013838]
		[batch 20/20] avg loss: 0.11525249253295367		[learning rate: 0.0013822]
	Learning Rate: 0.00138218
	LOSS [training: 0.12729163573205965 | validation: 0.06590487833885114]
	TIME [epoch: 8.23 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1664264428323845		[learning rate: 0.0013805]
		[batch 20/20] avg loss: 0.11638856450333632		[learning rate: 0.0013789]
	Learning Rate: 0.00137892
	LOSS [training: 0.14140750366786042 | validation: 0.05094672252605756]
	TIME [epoch: 8.24 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1116222396646295		[learning rate: 0.0013773]
		[batch 20/20] avg loss: 0.13661906279470373		[learning rate: 0.0013757]
	Learning Rate: 0.00137567
	LOSS [training: 0.12412065122966662 | validation: 0.08259148122894203]
	TIME [epoch: 8.18 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12389774088695611		[learning rate: 0.001374]
		[batch 20/20] avg loss: 0.1307129175069344		[learning rate: 0.0013724]
	Learning Rate: 0.00137242
	LOSS [training: 0.12730532919694523 | validation: 0.053322560913978814]
	TIME [epoch: 8.19 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13527457086267097		[learning rate: 0.0013708]
		[batch 20/20] avg loss: 0.18403314806779003		[learning rate: 0.0013692]
	Learning Rate: 0.00136918
	LOSS [training: 0.1596538594652305 | validation: 0.18508837724632748]
	TIME [epoch: 8.21 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12885946019375136		[learning rate: 0.0013676]
		[batch 20/20] avg loss: 0.11837937087566397		[learning rate: 0.001366]
	Learning Rate: 0.00136595
	LOSS [training: 0.12361941553470768 | validation: 0.07674665820655874]
	TIME [epoch: 8.23 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13018824793482897		[learning rate: 0.0013643]
		[batch 20/20] avg loss: 0.20142497145141092		[learning rate: 0.0013627]
	Learning Rate: 0.00136273
	LOSS [training: 0.16580660969311994 | validation: 0.11437790858069617]
	TIME [epoch: 8.19 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11720971642777625		[learning rate: 0.0013611]
		[batch 20/20] avg loss: 0.15090971815015586		[learning rate: 0.0013595]
	Learning Rate: 0.00135952
	LOSS [training: 0.13405971728896607 | validation: 0.08761513817301413]
	TIME [epoch: 8.19 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15668992597477013		[learning rate: 0.0013579]
		[batch 20/20] avg loss: 0.15679341556662552		[learning rate: 0.0013563]
	Learning Rate: 0.00135631
	LOSS [training: 0.15674167077069784 | validation: 0.058347061959691575]
	TIME [epoch: 8.2 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11425413433419976		[learning rate: 0.0013547]
		[batch 20/20] avg loss: 0.15922968515653851		[learning rate: 0.0013531]
	Learning Rate: 0.00135311
	LOSS [training: 0.1367419097453691 | validation: 0.385847909639497]
	TIME [epoch: 8.24 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17497486441163396		[learning rate: 0.0013515]
		[batch 20/20] avg loss: 0.1326487461611759		[learning rate: 0.0013499]
	Learning Rate: 0.00134992
	LOSS [training: 0.15381180528640495 | validation: 0.12576830187321497]
	TIME [epoch: 8.19 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12445857767192911		[learning rate: 0.0013483]
		[batch 20/20] avg loss: 0.16275765663282818		[learning rate: 0.0013467]
	Learning Rate: 0.00134673
	LOSS [training: 0.14360811715237867 | validation: 0.14573957951743394]
	TIME [epoch: 8.19 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1299142217721176		[learning rate: 0.0013451]
		[batch 20/20] avg loss: 0.14441443675679272		[learning rate: 0.0013436]
	Learning Rate: 0.00134356
	LOSS [training: 0.13716432926445518 | validation: 0.06487021367577864]
	TIME [epoch: 8.22 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12420229605447966		[learning rate: 0.001342]
		[batch 20/20] avg loss: 0.1321288156094428		[learning rate: 0.0013404]
	Learning Rate: 0.00134039
	LOSS [training: 0.12816555583196124 | validation: 0.09431559885941208]
	TIME [epoch: 8.23 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12662201722990962		[learning rate: 0.0013388]
		[batch 20/20] avg loss: 0.12489869028387084		[learning rate: 0.0013372]
	Learning Rate: 0.00133723
	LOSS [training: 0.12576035375689026 | validation: 0.062304198214997344]
	TIME [epoch: 8.19 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12102603387387786		[learning rate: 0.0013356]
		[batch 20/20] avg loss: 0.12799054712132074		[learning rate: 0.0013341]
	Learning Rate: 0.00133407
	LOSS [training: 0.12450829049759929 | validation: 0.05988296089871889]
	TIME [epoch: 8.18 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10412315101403594		[learning rate: 0.0013325]
		[batch 20/20] avg loss: 0.11275129810660152		[learning rate: 0.0013309]
	Learning Rate: 0.00133093
	LOSS [training: 0.10843722456031872 | validation: 0.08905910016599926]
	TIME [epoch: 8.21 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1322124681671191		[learning rate: 0.0013294]
		[batch 20/20] avg loss: 0.1667477787258777		[learning rate: 0.0013278]
	Learning Rate: 0.00132779
	LOSS [training: 0.14948012344649841 | validation: 0.0799784086093918]
	TIME [epoch: 8.23 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11782203341549691		[learning rate: 0.0013262]
		[batch 20/20] avg loss: 0.143976660090296		[learning rate: 0.0013247]
	Learning Rate: 0.00132465
	LOSS [training: 0.1308993467528965 | validation: 0.10055145454283353]
	TIME [epoch: 8.19 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13639906346241742		[learning rate: 0.0013231]
		[batch 20/20] avg loss: 0.15493202206944828		[learning rate: 0.0013215]
	Learning Rate: 0.00132153
	LOSS [training: 0.14566554276593285 | validation: 0.0834360577584081]
	TIME [epoch: 8.18 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16310499080734908		[learning rate: 0.00132]
		[batch 20/20] avg loss: 0.1281255854487819		[learning rate: 0.0013184]
	Learning Rate: 0.00131841
	LOSS [training: 0.1456152881280655 | validation: 0.16469536918291702]
	TIME [epoch: 8.22 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19937985358698568		[learning rate: 0.0013169]
		[batch 20/20] avg loss: 0.13138212071248118		[learning rate: 0.0013153]
	Learning Rate: 0.0013153
	LOSS [training: 0.16538098714973343 | validation: 0.06578892138000736]
	TIME [epoch: 8.23 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09990677196732704		[learning rate: 0.0013138]
		[batch 20/20] avg loss: 0.1606497666335991		[learning rate: 0.0013122]
	Learning Rate: 0.0013122
	LOSS [training: 0.13027826930046305 | validation: 0.06487137836053361]
	TIME [epoch: 8.19 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09918347898613371		[learning rate: 0.0013107]
		[batch 20/20] avg loss: 0.09550023525670119		[learning rate: 0.0013091]
	Learning Rate: 0.0013091
	LOSS [training: 0.09734185712141746 | validation: 0.08974893532532754]
	TIME [epoch: 8.2 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12836050609638897		[learning rate: 0.0013076]
		[batch 20/20] avg loss: 0.1204671779323293		[learning rate: 0.001306]
	Learning Rate: 0.00130602
	LOSS [training: 0.12441384201435912 | validation: 0.07936311037998646]
	TIME [epoch: 8.22 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10410145462336287		[learning rate: 0.0013045]
		[batch 20/20] avg loss: 0.13001976776706214		[learning rate: 0.0013029]
	Learning Rate: 0.00130294
	LOSS [training: 0.11706061119521252 | validation: 0.05673133225748206]
	TIME [epoch: 8.23 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12106043779671086		[learning rate: 0.0013014]
		[batch 20/20] avg loss: 0.10783888910781703		[learning rate: 0.0012999]
	Learning Rate: 0.00129986
	LOSS [training: 0.11444966345226396 | validation: 0.1413720161363791]
	TIME [epoch: 8.18 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1252260327733778		[learning rate: 0.0012983]
		[batch 20/20] avg loss: 0.1361265771628439		[learning rate: 0.0012968]
	Learning Rate: 0.0012968
	LOSS [training: 0.13067630496811083 | validation: 0.24161242288123777]
	TIME [epoch: 8.18 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12996280042380765		[learning rate: 0.0012953]
		[batch 20/20] avg loss: 0.1264418114528829		[learning rate: 0.0012937]
	Learning Rate: 0.00129374
	LOSS [training: 0.1282023059383453 | validation: 0.11217514559277621]
	TIME [epoch: 8.21 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14921619066490147		[learning rate: 0.0012922]
		[batch 20/20] avg loss: 0.09536935680408899		[learning rate: 0.0012907]
	Learning Rate: 0.00129069
	LOSS [training: 0.12229277373449524 | validation: 0.06593226439215608]
	TIME [epoch: 8.22 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16820287896079952		[learning rate: 0.0012892]
		[batch 20/20] avg loss: 0.14008907729322045		[learning rate: 0.0012876]
	Learning Rate: 0.00128764
	LOSS [training: 0.15414597812700997 | validation: 0.0544492719804692]
	TIME [epoch: 8.19 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17174411611801665		[learning rate: 0.0012861]
		[batch 20/20] avg loss: 0.1290610664886714		[learning rate: 0.0012846]
	Learning Rate: 0.0012846
	LOSS [training: 0.15040259130334405 | validation: 0.044844407711946706]
	TIME [epoch: 8.19 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09995552005336647		[learning rate: 0.0012831]
		[batch 20/20] avg loss: 0.1575362571903699		[learning rate: 0.0012816]
	Learning Rate: 0.00128157
	LOSS [training: 0.12874588862186817 | validation: 0.045145860763986696]
	TIME [epoch: 8.22 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09097660729796832		[learning rate: 0.0012801]
		[batch 20/20] avg loss: 0.1292489522361133		[learning rate: 0.0012786]
	Learning Rate: 0.00127855
	LOSS [training: 0.11011277976704077 | validation: 0.07870418788865763]
	TIME [epoch: 8.23 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14845920441337998		[learning rate: 0.001277]
		[batch 20/20] avg loss: 0.1484011029277675		[learning rate: 0.0012755]
	Learning Rate: 0.00127553
	LOSS [training: 0.14843015367057374 | validation: 0.06499136536793926]
	TIME [epoch: 8.2 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11596031106974988		[learning rate: 0.001274]
		[batch 20/20] avg loss: 0.14225787025168035		[learning rate: 0.0012725]
	Learning Rate: 0.00127253
	LOSS [training: 0.12910909066071513 | validation: 0.15654461577976242]
	TIME [epoch: 8.19 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11721076460115143		[learning rate: 0.001271]
		[batch 20/20] avg loss: 0.11687612466626252		[learning rate: 0.0012695]
	Learning Rate: 0.00126952
	LOSS [training: 0.11704344463370697 | validation: 0.08194906216394242]
	TIME [epoch: 8.21 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13476467004925458		[learning rate: 0.001268]
		[batch 20/20] avg loss: 0.13307985869371033		[learning rate: 0.0012665]
	Learning Rate: 0.00126653
	LOSS [training: 0.13392226437148244 | validation: 0.11442984476069949]
	TIME [epoch: 8.23 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13538028761582233		[learning rate: 0.001265]
		[batch 20/20] avg loss: 0.14089224156292454		[learning rate: 0.0012635]
	Learning Rate: 0.00126354
	LOSS [training: 0.13813626458937342 | validation: 0.1034665080389258]
	TIME [epoch: 8.2 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11603735742239585		[learning rate: 0.0012621]
		[batch 20/20] avg loss: 0.1811790699840101		[learning rate: 0.0012606]
	Learning Rate: 0.00126056
	LOSS [training: 0.14860821370320298 | validation: 0.11918466600857805]
	TIME [epoch: 8.18 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.142791180957847		[learning rate: 0.0012591]
		[batch 20/20] avg loss: 0.1553058123051779		[learning rate: 0.0012576]
	Learning Rate: 0.00125759
	LOSS [training: 0.14904849663151246 | validation: 0.04983181248713989]
	TIME [epoch: 8.22 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10899308335167297		[learning rate: 0.0012561]
		[batch 20/20] avg loss: 0.15549837606074707		[learning rate: 0.0012546]
	Learning Rate: 0.00125462
	LOSS [training: 0.13224572970620999 | validation: 0.08507172034749393]
	TIME [epoch: 8.21 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11279463264073394		[learning rate: 0.0012531]
		[batch 20/20] avg loss: 0.10335261925469433		[learning rate: 0.0012517]
	Learning Rate: 0.00125166
	LOSS [training: 0.10807362594771412 | validation: 0.13728252468921265]
	TIME [epoch: 8.2 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15813095353240833		[learning rate: 0.0012502]
		[batch 20/20] avg loss: 0.13356661315066348		[learning rate: 0.0012487]
	Learning Rate: 0.00124871
	LOSS [training: 0.14584878334153592 | validation: 0.07250284073430224]
	TIME [epoch: 8.18 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14529445221864937		[learning rate: 0.0012472]
		[batch 20/20] avg loss: 0.1562752025956779		[learning rate: 0.0012458]
	Learning Rate: 0.00124576
	LOSS [training: 0.15078482740716365 | validation: 0.09751113275509302]
	TIME [epoch: 8.2 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09093516585704385		[learning rate: 0.0012443]
		[batch 20/20] avg loss: 0.10759977395349556		[learning rate: 0.0012428]
	Learning Rate: 0.00124283
	LOSS [training: 0.0992674699052697 | validation: 0.056621427158311685]
	TIME [epoch: 8.23 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11855380801377982		[learning rate: 0.0012414]
		[batch 20/20] avg loss: 0.10983005209481626		[learning rate: 0.0012399]
	Learning Rate: 0.00123989
	LOSS [training: 0.11419193005429804 | validation: 0.12833228657703805]
	TIME [epoch: 8.19 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10554901144559507		[learning rate: 0.0012384]
		[batch 20/20] avg loss: 0.11890473517273784		[learning rate: 0.001237]
	Learning Rate: 0.00123697
	LOSS [training: 0.11222687330916647 | validation: 0.07200210742801455]
	TIME [epoch: 8.19 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13871073685005578		[learning rate: 0.0012355]
		[batch 20/20] avg loss: 0.10537862032286545		[learning rate: 0.0012341]
	Learning Rate: 0.00123405
	LOSS [training: 0.12204467858646062 | validation: 0.08733089675612947]
	TIME [epoch: 8.2 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12429090990918543		[learning rate: 0.0012326]
		[batch 20/20] avg loss: 0.13780144974857841		[learning rate: 0.0012311]
	Learning Rate: 0.00123114
	LOSS [training: 0.13104617982888192 | validation: 0.09916587402705299]
	TIME [epoch: 8.23 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12852655979158176		[learning rate: 0.0012297]
		[batch 20/20] avg loss: 0.10712567153563632		[learning rate: 0.0012282]
	Learning Rate: 0.00122824
	LOSS [training: 0.11782611566360904 | validation: 0.1644783626289394]
	TIME [epoch: 8.19 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1571986284891031		[learning rate: 0.0012268]
		[batch 20/20] avg loss: 0.12418291375546249		[learning rate: 0.0012253]
	Learning Rate: 0.00122534
	LOSS [training: 0.1406907711222828 | validation: 0.07549666157313138]
	TIME [epoch: 8.19 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11664627092764009		[learning rate: 0.0012239]
		[batch 20/20] avg loss: 0.12496670302669892		[learning rate: 0.0012224]
	Learning Rate: 0.00122245
	LOSS [training: 0.12080648697716949 | validation: 0.077984478951181]
	TIME [epoch: 8.2 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08641453930975979		[learning rate: 0.001221]
		[batch 20/20] avg loss: 0.12987038081309982		[learning rate: 0.0012196]
	Learning Rate: 0.00121957
	LOSS [training: 0.1081424600614298 | validation: 0.08286399027562147]
	TIME [epoch: 8.22 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08553978061432818		[learning rate: 0.0012181]
		[batch 20/20] avg loss: 0.10427805252217766		[learning rate: 0.0012167]
	Learning Rate: 0.00121669
	LOSS [training: 0.09490891656825293 | validation: 0.05769508400003049]
	TIME [epoch: 8.18 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11499382371141446		[learning rate: 0.0012153]
		[batch 20/20] avg loss: 0.11533281664934167		[learning rate: 0.0012138]
	Learning Rate: 0.00121382
	LOSS [training: 0.11516332018037805 | validation: 0.1077545480752902]
	TIME [epoch: 8.19 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09763841093318071		[learning rate: 0.0012124]
		[batch 20/20] avg loss: 0.10888407850118442		[learning rate: 0.001211]
	Learning Rate: 0.00121096
	LOSS [training: 0.10326124471718257 | validation: 0.17741684754566864]
	TIME [epoch: 8.21 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1202450805399466		[learning rate: 0.0012095]
		[batch 20/20] avg loss: 0.09922606710532074		[learning rate: 0.0012081]
	Learning Rate: 0.0012081
	LOSS [training: 0.10973557382263366 | validation: 0.06358358380718837]
	TIME [epoch: 8.25 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09700755095682187		[learning rate: 0.0012067]
		[batch 20/20] avg loss: 0.12916945327373658		[learning rate: 0.0012052]
	Learning Rate: 0.00120525
	LOSS [training: 0.11308850211527918 | validation: 0.10215822723775586]
	TIME [epoch: 8.19 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12413804517985064		[learning rate: 0.0012038]
		[batch 20/20] avg loss: 0.1143174263566139		[learning rate: 0.0012024]
	Learning Rate: 0.00120241
	LOSS [training: 0.11922773576823227 | validation: 0.05856617610598559]
	TIME [epoch: 8.18 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09839056948207989		[learning rate: 0.001201]
		[batch 20/20] avg loss: 0.10115038502863698		[learning rate: 0.0011996]
	Learning Rate: 0.00119957
	LOSS [training: 0.09977047725535845 | validation: 0.08699672969555204]
	TIME [epoch: 8.2 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11250743414868844		[learning rate: 0.0011982]
		[batch 20/20] avg loss: 0.10464544349369886		[learning rate: 0.0011967]
	Learning Rate: 0.00119674
	LOSS [training: 0.10857643882119364 | validation: 0.05738834734313532]
	TIME [epoch: 8.23 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12115451947217445		[learning rate: 0.0011953]
		[batch 20/20] avg loss: 0.09462386994109859		[learning rate: 0.0011939]
	Learning Rate: 0.00119392
	LOSS [training: 0.10788919470663652 | validation: 0.13707823429758426]
	TIME [epoch: 8.19 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13517313376031106		[learning rate: 0.0011925]
		[batch 20/20] avg loss: 0.13411720917929854		[learning rate: 0.0011911]
	Learning Rate: 0.0011911
	LOSS [training: 0.1346451714698048 | validation: 0.061021612432172995]
	TIME [epoch: 8.19 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08666801397931716		[learning rate: 0.0011897]
		[batch 20/20] avg loss: 0.08362069523594531		[learning rate: 0.0011883]
	Learning Rate: 0.00118829
	LOSS [training: 0.08514435460763123 | validation: 0.09016281519649685]
	TIME [epoch: 8.2 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1373927831878491		[learning rate: 0.0011869]
		[batch 20/20] avg loss: 0.09930222879398595		[learning rate: 0.0011855]
	Learning Rate: 0.00118549
	LOSS [training: 0.1183475059909175 | validation: 0.09495912532585644]
	TIME [epoch: 8.22 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11079418364346391		[learning rate: 0.0011841]
		[batch 20/20] avg loss: 0.1274403301905472		[learning rate: 0.0011827]
	Learning Rate: 0.00118269
	LOSS [training: 0.11911725691700556 | validation: 0.09478662505462226]
	TIME [epoch: 8.19 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11255906896140666		[learning rate: 0.0011813]
		[batch 20/20] avg loss: 0.10410596901902287		[learning rate: 0.0011799]
	Learning Rate: 0.0011799
	LOSS [training: 0.10833251899021476 | validation: 0.0356678888058837]
	TIME [epoch: 8.18 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08986465056948445		[learning rate: 0.0011785]
		[batch 20/20] avg loss: 0.08906880146673206		[learning rate: 0.0011771]
	Learning Rate: 0.00117712
	LOSS [training: 0.08946672601810826 | validation: 0.05200708922691117]
	TIME [epoch: 8.21 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10776113711051327		[learning rate: 0.0011757]
		[batch 20/20] avg loss: 0.09455096161911128		[learning rate: 0.0011743]
	Learning Rate: 0.00117434
	LOSS [training: 0.1011560493648123 | validation: 0.08314264753529091]
	TIME [epoch: 8.23 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09281978854902553		[learning rate: 0.001173]
		[batch 20/20] avg loss: 0.11816552874532889		[learning rate: 0.0011716]
	Learning Rate: 0.00117157
	LOSS [training: 0.10549265864717723 | validation: 0.13663551689077044]
	TIME [epoch: 8.19 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11324053081623793		[learning rate: 0.0011702]
		[batch 20/20] avg loss: 0.12416124168766766		[learning rate: 0.0011688]
	Learning Rate: 0.00116881
	LOSS [training: 0.11870088625195278 | validation: 0.12475243824937372]
	TIME [epoch: 8.19 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1070438249053944		[learning rate: 0.0011674]
		[batch 20/20] avg loss: 0.108123080617288		[learning rate: 0.0011661]
	Learning Rate: 0.00116605
	LOSS [training: 0.10758345276134118 | validation: 0.09034134916069778]
	TIME [epoch: 8.2 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12263806255705254		[learning rate: 0.0011647]
		[batch 20/20] avg loss: 0.10760960679556511		[learning rate: 0.0011633]
	Learning Rate: 0.0011633
	LOSS [training: 0.11512383467630885 | validation: 0.048545879606483495]
	TIME [epoch: 8.23 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11911689184281316		[learning rate: 0.0011619]
		[batch 20/20] avg loss: 0.1090781447928739		[learning rate: 0.0011606]
	Learning Rate: 0.00116056
	LOSS [training: 0.11409751831784351 | validation: 0.06153691348445446]
	TIME [epoch: 8.19 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08056221895059076		[learning rate: 0.0011592]
		[batch 20/20] avg loss: 0.12302773491604668		[learning rate: 0.0011578]
	Learning Rate: 0.00115782
	LOSS [training: 0.10179497693331871 | validation: 0.07020629752862455]
	TIME [epoch: 8.18 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09727082589335137		[learning rate: 0.0011565]
		[batch 20/20] avg loss: 0.10204589494550768		[learning rate: 0.0011551]
	Learning Rate: 0.00115509
	LOSS [training: 0.09965836041942953 | validation: 0.045793446095494385]
	TIME [epoch: 8.2 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08757358877188685		[learning rate: 0.0011537]
		[batch 20/20] avg loss: 0.11326542163334904		[learning rate: 0.0011524]
	Learning Rate: 0.00115236
	LOSS [training: 0.10041950520261796 | validation: 0.05030181066671907]
	TIME [epoch: 8.21 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10508429860589674		[learning rate: 0.001151]
		[batch 20/20] avg loss: 0.10992404373660067		[learning rate: 0.0011496]
	Learning Rate: 0.00114965
	LOSS [training: 0.10750417117124873 | validation: 0.07697313982804457]
	TIME [epoch: 8.19 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.070040714809224		[learning rate: 0.0011483]
		[batch 20/20] avg loss: 0.08752652301184749		[learning rate: 0.0011469]
	Learning Rate: 0.00114693
	LOSS [training: 0.07878361891053574 | validation: 0.03781241194687605]
	TIME [epoch: 8.19 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07975180369548833		[learning rate: 0.0011456]
		[batch 20/20] avg loss: 0.10143779232275645		[learning rate: 0.0011442]
	Learning Rate: 0.00114423
	LOSS [training: 0.0905947980091224 | validation: 0.10676225097581203]
	TIME [epoch: 8.21 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08725618767873125		[learning rate: 0.0011429]
		[batch 20/20] avg loss: 0.09859237855002394		[learning rate: 0.0011415]
	Learning Rate: 0.00114153
	LOSS [training: 0.09292428311437761 | validation: 0.07323899944569512]
	TIME [epoch: 8.22 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08180706290047876		[learning rate: 0.0011402]
		[batch 20/20] avg loss: 0.08832296852336115		[learning rate: 0.0011388]
	Learning Rate: 0.00113884
	LOSS [training: 0.08506501571191996 | validation: 0.07141840524396684]
	TIME [epoch: 8.2 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0848718956934552		[learning rate: 0.0011375]
		[batch 20/20] avg loss: 0.08642017450384198		[learning rate: 0.0011362]
	Learning Rate: 0.00113615
	LOSS [training: 0.08564603509864857 | validation: 0.048521761489950105]
	TIME [epoch: 8.18 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11124902944938948		[learning rate: 0.0011348]
		[batch 20/20] avg loss: 0.09397374226842256		[learning rate: 0.0011335]
	Learning Rate: 0.00113347
	LOSS [training: 0.10261138585890603 | validation: 0.060196975168274686]
	TIME [epoch: 8.2 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13460929878667294		[learning rate: 0.0011321]
		[batch 20/20] avg loss: 0.0920893361107579		[learning rate: 0.0011308]
	Learning Rate: 0.0011308
	LOSS [training: 0.11334931744871539 | validation: 0.14548997076548845]
	TIME [epoch: 8.19 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11032364865176203		[learning rate: 0.0011295]
		[batch 20/20] avg loss: 0.10463198580190976		[learning rate: 0.0011281]
	Learning Rate: 0.00112813
	LOSS [training: 0.10747781722683589 | validation: 0.03377362868218326]
	TIME [epoch: 8.22 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0743712558225399		[learning rate: 0.0011268]
		[batch 20/20] avg loss: 0.11256421012393139		[learning rate: 0.0011255]
	Learning Rate: 0.00112547
	LOSS [training: 0.09346773297323566 | validation: 0.08099468424807386]
	TIME [epoch: 8.19 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1276468113475947		[learning rate: 0.0011241]
		[batch 20/20] avg loss: 0.08607696883633198		[learning rate: 0.0011228]
	Learning Rate: 0.00112281
	LOSS [training: 0.10686189009196334 | validation: 0.06270276261489977]
	TIME [epoch: 8.19 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08872393553055456		[learning rate: 0.0011215]
		[batch 20/20] avg loss: 0.10052422423357879		[learning rate: 0.0011202]
	Learning Rate: 0.00112017
	LOSS [training: 0.0946240798820667 | validation: 0.03746291338952368]
	TIME [epoch: 8.18 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08692489879676965		[learning rate: 0.0011188]
		[batch 20/20] avg loss: 0.09642447666127076		[learning rate: 0.0011175]
	Learning Rate: 0.00111752
	LOSS [training: 0.09167468772902021 | validation: 0.06915233230894692]
	TIME [epoch: 8.22 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1003184838509386		[learning rate: 0.0011162]
		[batch 20/20] avg loss: 0.08272008796592421		[learning rate: 0.0011149]
	Learning Rate: 0.00111489
	LOSS [training: 0.09151928590843139 | validation: 0.0491736884918688]
	TIME [epoch: 8.19 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10127817818650937		[learning rate: 0.0011136]
		[batch 20/20] avg loss: 0.09339889220800193		[learning rate: 0.0011123]
	Learning Rate: 0.00111226
	LOSS [training: 0.09733853519725563 | validation: 0.10332614091708171]
	TIME [epoch: 8.21 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10674019589647357		[learning rate: 0.0011109]
		[batch 20/20] avg loss: 0.08924765563513787		[learning rate: 0.0011096]
	Learning Rate: 0.00110963
	LOSS [training: 0.09799392576580572 | validation: 0.04432355727378179]
	TIME [epoch: 8.19 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08342504580597629		[learning rate: 0.0011083]
		[batch 20/20] avg loss: 0.08240803003779809		[learning rate: 0.001107]
	Learning Rate: 0.00110702
	LOSS [training: 0.08291653792188718 | validation: 0.028225315170748087]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_982.pth
	Model improved!!!
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08449586677185295		[learning rate: 0.0011057]
		[batch 20/20] avg loss: 0.09270471929893062		[learning rate: 0.0011044]
	Learning Rate: 0.0011044
	LOSS [training: 0.08860029303539177 | validation: 0.03901959099299483]
	TIME [epoch: 8.19 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12393705237859028		[learning rate: 0.0011031]
		[batch 20/20] avg loss: 0.12165044680186236		[learning rate: 0.0011018]
	Learning Rate: 0.0011018
	LOSS [training: 0.12279374959022633 | validation: 0.08584892138703083]
	TIME [epoch: 8.22 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09760786462169938		[learning rate: 0.0011005]
		[batch 20/20] avg loss: 0.08459283445450363		[learning rate: 0.0010992]
	Learning Rate: 0.0010992
	LOSS [training: 0.0911003495381015 | validation: 0.07923504890665888]
	TIME [epoch: 8.22 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0770888141411932		[learning rate: 0.0010979]
		[batch 20/20] avg loss: 0.09906526871870229		[learning rate: 0.0010966]
	Learning Rate: 0.00109661
	LOSS [training: 0.08807704142994774 | validation: 0.07166707392750708]
	TIME [epoch: 8.2 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09908471846290685		[learning rate: 0.0010953]
		[batch 20/20] avg loss: 0.09025316997609766		[learning rate: 0.001094]
	Learning Rate: 0.00109402
	LOSS [training: 0.09466894421950225 | validation: 0.058050531475363515]
	TIME [epoch: 8.17 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06805061076982502		[learning rate: 0.0010927]
		[batch 20/20] avg loss: 0.0914719320357126		[learning rate: 0.0010914]
	Learning Rate: 0.00109144
	LOSS [training: 0.0797612714027688 | validation: 0.06204723385848879]
	TIME [epoch: 8.2 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12479478622458537		[learning rate: 0.0010902]
		[batch 20/20] avg loss: 0.07868673739042792		[learning rate: 0.0010889]
	Learning Rate: 0.00108887
	LOSS [training: 0.10174076180750664 | validation: 0.13471382432527465]
	TIME [epoch: 8.2 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12657376642217733		[learning rate: 0.0010876]
		[batch 20/20] avg loss: 0.08604738567161963		[learning rate: 0.0010863]
	Learning Rate: 0.0010863
	LOSS [training: 0.10631057604689846 | validation: 0.052816118618317406]
	TIME [epoch: 8.2 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10119437225130015		[learning rate: 0.001085]
		[batch 20/20] avg loss: 0.09839232168032114		[learning rate: 0.0010837]
	Learning Rate: 0.00108373
	LOSS [training: 0.09979334696581064 | validation: 0.12280443473818714]
	TIME [epoch: 8.17 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09086917599243564		[learning rate: 0.0010825]
		[batch 20/20] avg loss: 0.13609745208602098		[learning rate: 0.0010812]
	Learning Rate: 0.00108118
	LOSS [training: 0.11348331403922832 | validation: 0.13903385279134226]
	TIME [epoch: 8.2 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08174457718999313		[learning rate: 0.0010799]
		[batch 20/20] avg loss: 0.13934325279358697		[learning rate: 0.0010786]
	Learning Rate: 0.00107863
	LOSS [training: 0.11054391499179006 | validation: 0.06314515375928148]
	TIME [epoch: 8.21 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1214389865472437		[learning rate: 0.0010774]
		[batch 20/20] avg loss: 0.11864410097774418		[learning rate: 0.0010761]
	Learning Rate: 0.00107608
	LOSS [training: 0.12004154376249394 | validation: 0.17767876712041308]
	TIME [epoch: 8.2 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.113160674598767		[learning rate: 0.0010748]
		[batch 20/20] avg loss: 0.12679951423748745		[learning rate: 0.0010735]
	Learning Rate: 0.00107355
	LOSS [training: 0.11998009441812722 | validation: 0.043701839105017695]
	TIME [epoch: 8.18 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08316616022892248		[learning rate: 0.0010723]
		[batch 20/20] avg loss: 0.09052933932618983		[learning rate: 0.001071]
	Learning Rate: 0.00107101
	LOSS [training: 0.08684774977755616 | validation: 0.06548666167540855]
	TIME [epoch: 8.2 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10100860112767943		[learning rate: 0.0010697]
		[batch 20/20] avg loss: 0.08214004031910786		[learning rate: 0.0010685]
	Learning Rate: 0.00106849
	LOSS [training: 0.09157432072339364 | validation: 0.05614846693355117]
	TIME [epoch: 8.2 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09662035924871884		[learning rate: 0.0010672]
		[batch 20/20] avg loss: 0.12526369026965295		[learning rate: 0.001066]
	Learning Rate: 0.00106597
	LOSS [training: 0.11094202475918587 | validation: 0.06814102112048442]
	TIME [epoch: 8.21 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0855623010274553		[learning rate: 0.0010647]
		[batch 20/20] avg loss: 0.07216439640536555		[learning rate: 0.0010635]
	Learning Rate: 0.00106345
	LOSS [training: 0.0788633487164104 | validation: 0.04679322431434514]
	TIME [epoch: 8.19 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0885545086079753		[learning rate: 0.0010622]
		[batch 20/20] avg loss: 0.06333243386073525		[learning rate: 0.0010609]
	Learning Rate: 0.00106094
	LOSS [training: 0.07594347123435527 | validation: 0.04124944838297321]
	TIME [epoch: 8.22 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0777685366297691		[learning rate: 0.0010597]
		[batch 20/20] avg loss: 0.09114940946001839		[learning rate: 0.0010584]
	Learning Rate: 0.00105844
	LOSS [training: 0.08445897304489375 | validation: 0.02067634469731924]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_1001.pth
	Model improved!!!
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08965411567363415		[learning rate: 0.0010572]
		[batch 20/20] avg loss: 0.08031519963828705		[learning rate: 0.0010559]
	Learning Rate: 0.00105594
	LOSS [training: 0.0849846576559606 | validation: 0.04980019871086979]
	TIME [epoch: 8.21 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09480970293758406		[learning rate: 0.0010547]
		[batch 20/20] avg loss: 0.11207027980675814		[learning rate: 0.0010535]
	Learning Rate: 0.00105345
	LOSS [training: 0.10343999137217111 | validation: 0.028365981509921867]
	TIME [epoch: 8.2 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11369742391590272		[learning rate: 0.0010522]
		[batch 20/20] avg loss: 0.13220125453537152		[learning rate: 0.001051]
	Learning Rate: 0.00105097
	LOSS [training: 0.12294933922563711 | validation: 0.09553393996247844]
	TIME [epoch: 8.22 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09307462807336053		[learning rate: 0.0010497]
		[batch 20/20] avg loss: 0.0865802687410092		[learning rate: 0.0010485]
	Learning Rate: 0.00104849
	LOSS [training: 0.08982744840718487 | validation: 0.14285507907857217]
	TIME [epoch: 8.21 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12283085934751647		[learning rate: 0.0010473]
		[batch 20/20] avg loss: 0.08257097073861862		[learning rate: 0.001046]
	Learning Rate: 0.00104602
	LOSS [training: 0.10270091504306753 | validation: 0.10087762115372598]
	TIME [epoch: 8.22 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08954178177001572		[learning rate: 0.0010448]
		[batch 20/20] avg loss: 0.11087029853338852		[learning rate: 0.0010435]
	Learning Rate: 0.00104355
	LOSS [training: 0.10020604015170209 | validation: 0.052709394191748055]
	TIME [epoch: 8.2 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0919679189803474		[learning rate: 0.0010423]
		[batch 20/20] avg loss: 0.08453744502505131		[learning rate: 0.0010411]
	Learning Rate: 0.00104109
	LOSS [training: 0.08825268200269935 | validation: 0.049182880942901935]
	TIME [epoch: 8.24 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07848401226941383		[learning rate: 0.0010399]
		[batch 20/20] avg loss: 0.08798243835518396		[learning rate: 0.0010386]
	Learning Rate: 0.00103863
	LOSS [training: 0.0832332253122989 | validation: 0.18673761449782272]
	TIME [epoch: 8.21 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1065363555506865		[learning rate: 0.0010374]
		[batch 20/20] avg loss: 0.08731412890766334		[learning rate: 0.0010362]
	Learning Rate: 0.00103618
	LOSS [training: 0.09692524222917491 | validation: 0.02372915562335596]
	TIME [epoch: 8.22 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08410832901469265		[learning rate: 0.001035]
		[batch 20/20] avg loss: 0.07156186704214282		[learning rate: 0.0010337]
	Learning Rate: 0.00103374
	LOSS [training: 0.07783509802841772 | validation: 0.0452042992945488]
	TIME [epoch: 8.19 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06389535408766092		[learning rate: 0.0010325]
		[batch 20/20] avg loss: 0.08656249246446139		[learning rate: 0.0010313]
	Learning Rate: 0.0010313
	LOSS [training: 0.07522892327606114 | validation: 0.06101843157949369]
	TIME [epoch: 8.23 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1165213045265939		[learning rate: 0.0010301]
		[batch 20/20] avg loss: 0.09879950010746939		[learning rate: 0.0010289]
	Learning Rate: 0.00102887
	LOSS [training: 0.10766040231703164 | validation: 0.09387523940580167]
	TIME [epoch: 8.22 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09981663443575382		[learning rate: 0.0010277]
		[batch 20/20] avg loss: 0.09480870070760175		[learning rate: 0.0010264]
	Learning Rate: 0.00102644
	LOSS [training: 0.0973126675716778 | validation: 0.07441029327554799]
	TIME [epoch: 8.22 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08355781235281148		[learning rate: 0.0010252]
		[batch 20/20] avg loss: 0.08387358721557994		[learning rate: 0.001024]
	Learning Rate: 0.00102402
	LOSS [training: 0.08371569978419571 | validation: 0.06911772859314153]
	TIME [epoch: 8.2 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10980913270267227		[learning rate: 0.0010228]
		[batch 20/20] avg loss: 0.144052954503448		[learning rate: 0.0010216]
	Learning Rate: 0.0010216
	LOSS [training: 0.12693104360306012 | validation: 0.10455133116967842]
	TIME [epoch: 8.2 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12169382761639788		[learning rate: 0.0010204]
		[batch 20/20] avg loss: 0.08970328766987787		[learning rate: 0.0010192]
	Learning Rate: 0.00101919
	LOSS [training: 0.10569855764313789 | validation: 0.04741211876059155]
	TIME [epoch: 8.19 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10637641904344694		[learning rate: 0.001018]
		[batch 20/20] avg loss: 0.11764575896726084		[learning rate: 0.0010168]
	Learning Rate: 0.00101679
	LOSS [training: 0.11201108900535392 | validation: 0.036736879802879055]
	TIME [epoch: 8.24 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07594743926252082		[learning rate: 0.0010156]
		[batch 20/20] avg loss: 0.07816011618137629		[learning rate: 0.0010144]
	Learning Rate: 0.00101439
	LOSS [training: 0.07705377772194856 | validation: 0.06094239441811665]
	TIME [epoch: 8.19 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06745960157012211		[learning rate: 0.0010132]
		[batch 20/20] avg loss: 0.13852553887702923		[learning rate: 0.001012]
	Learning Rate: 0.001012
	LOSS [training: 0.10299257022357566 | validation: 0.06109598917911972]
	TIME [epoch: 8.18 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07844356211735456		[learning rate: 0.0010108]
		[batch 20/20] avg loss: 0.11727362897970195		[learning rate: 0.0010096]
	Learning Rate: 0.00100961
	LOSS [training: 0.09785859554852828 | validation: 0.034026140897213585]
	TIME [epoch: 8.21 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11002862153917345		[learning rate: 0.0010084]
		[batch 20/20] avg loss: 0.09371796313070549		[learning rate: 0.0010072]
	Learning Rate: 0.00100723
	LOSS [training: 0.1018732923349395 | validation: 0.027131480168779948]
	TIME [epoch: 8.22 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06697817135253517		[learning rate: 0.001006]
		[batch 20/20] avg loss: 0.09060994954960067		[learning rate: 0.0010049]
	Learning Rate: 0.00100485
	LOSS [training: 0.07879406045106793 | validation: 0.06233862821448908]
	TIME [epoch: 8.22 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10330824110206907		[learning rate: 0.0010037]
		[batch 20/20] avg loss: 0.06700236429909937		[learning rate: 0.0010025]
	Learning Rate: 0.00100248
	LOSS [training: 0.08515530270058422 | validation: 0.11224137580679269]
	TIME [epoch: 8.19 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10989713677585922		[learning rate: 0.0010013]
		[batch 20/20] avg loss: 0.06545285523593011		[learning rate: 0.0010001]
	Learning Rate: 0.00100012
	LOSS [training: 0.08767499600589466 | validation: 0.04620618706386006]
	TIME [epoch: 8.19 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08455264774493892		[learning rate: 0.00099894]
		[batch 20/20] avg loss: 0.07718990837222357		[learning rate: 0.00099776]
	Learning Rate: 0.000997759
	LOSS [training: 0.08087127805858126 | validation: 0.09877948607608109]
	TIME [epoch: 8.24 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10242417155515607		[learning rate: 0.00099658]
		[batch 20/20] avg loss: 0.11472323242339436		[learning rate: 0.00099541]
	Learning Rate: 0.000995405
	LOSS [training: 0.10857370198927518 | validation: 0.1512102299080785]
	TIME [epoch: 8.22 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11679835903659819		[learning rate: 0.00099423]
		[batch 20/20] avg loss: 0.09870487815440618		[learning rate: 0.00099306]
	Learning Rate: 0.000993057
	LOSS [training: 0.10775161859550215 | validation: 0.03783700579133723]
	TIME [epoch: 8.19 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07986414581257475		[learning rate: 0.00099189]
		[batch 20/20] avg loss: 0.10411288423918197		[learning rate: 0.00099071]
	Learning Rate: 0.000990715
	LOSS [training: 0.09198851502587835 | validation: 0.060051990692826324]
	TIME [epoch: 8.2 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08706000779900927		[learning rate: 0.00098955]
		[batch 20/20] avg loss: 0.10520036579555399		[learning rate: 0.00098838]
	Learning Rate: 0.000988378
	LOSS [training: 0.09613018679728164 | validation: 0.06652750959634099]
	TIME [epoch: 8.24 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0714177269661803		[learning rate: 0.00098721]
		[batch 20/20] avg loss: 0.0778110049908777		[learning rate: 0.00098605]
	Learning Rate: 0.000986047
	LOSS [training: 0.07461436597852901 | validation: 0.04910599747841988]
	TIME [epoch: 8.21 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1089556980974276		[learning rate: 0.00098488]
		[batch 20/20] avg loss: 0.13618279652037338		[learning rate: 0.00098372]
	Learning Rate: 0.000983721
	LOSS [training: 0.12256924730890051 | validation: 0.11697705666529176]
	TIME [epoch: 8.19 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11057680238234376		[learning rate: 0.00098256]
		[batch 20/20] avg loss: 0.0989257687100761		[learning rate: 0.0009814]
	Learning Rate: 0.0009814
	LOSS [training: 0.10475128554620991 | validation: 0.05077983416708756]
	TIME [epoch: 8.19 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10067600626370901		[learning rate: 0.00098024]
		[batch 20/20] avg loss: 0.09664000301330392		[learning rate: 0.00097909]
	Learning Rate: 0.000979085
	LOSS [training: 0.09865800463850646 | validation: 0.05029218865593457]
	TIME [epoch: 8.25 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06909516279357318		[learning rate: 0.00097793]
		[batch 20/20] avg loss: 0.08946333719105451		[learning rate: 0.00097678]
	Learning Rate: 0.000976776
	LOSS [training: 0.07927924999231385 | validation: 0.140789601624894]
	TIME [epoch: 8.21 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10059827190770206		[learning rate: 0.00097562]
		[batch 20/20] avg loss: 0.06753080384427036		[learning rate: 0.00097447]
	Learning Rate: 0.000974472
	LOSS [training: 0.08406453787598621 | validation: 0.06973744664336505]
	TIME [epoch: 8.19 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09723655975919998		[learning rate: 0.00097332]
		[batch 20/20] avg loss: 0.12370913417720611		[learning rate: 0.00097217]
	Learning Rate: 0.000972173
	LOSS [training: 0.11047284696820303 | validation: 0.04540491612087733]
	TIME [epoch: 8.2 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09859822320181574		[learning rate: 0.00097103]
		[batch 20/20] avg loss: 0.0888767735938702		[learning rate: 0.00096988]
	Learning Rate: 0.00096988
	LOSS [training: 0.09373749839784297 | validation: 0.08045431143226472]
	TIME [epoch: 8.26 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08537071144153134		[learning rate: 0.00096874]
		[batch 20/20] avg loss: 0.07249640086755851		[learning rate: 0.00096759]
	Learning Rate: 0.000967592
	LOSS [training: 0.07893355615454493 | validation: 0.051296798449497735]
	TIME [epoch: 8.21 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07623319633385042		[learning rate: 0.00096645]
		[batch 20/20] avg loss: 0.10854347676512448		[learning rate: 0.00096531]
	Learning Rate: 0.00096531
	LOSS [training: 0.09238833654948744 | validation: 0.06597400676027833]
	TIME [epoch: 8.19 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10356078762643002		[learning rate: 0.00096417]
		[batch 20/20] avg loss: 0.058644595693309785		[learning rate: 0.00096303]
	Learning Rate: 0.000963033
	LOSS [training: 0.0811026916598699 | validation: 0.0676411764209249]
	TIME [epoch: 8.2 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08387553708727494		[learning rate: 0.0009619]
		[batch 20/20] avg loss: 0.08932241242906384		[learning rate: 0.00096076]
	Learning Rate: 0.000960761
	LOSS [training: 0.0865989747581694 | validation: 0.04917253164260871]
	TIME [epoch: 8.26 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0806612293737403		[learning rate: 0.00095963]
		[batch 20/20] avg loss: 0.09856655951412145		[learning rate: 0.00095849]
	Learning Rate: 0.000958495
	LOSS [training: 0.08961389444393089 | validation: 0.07946954984086571]
	TIME [epoch: 8.21 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0803679943625151		[learning rate: 0.00095736]
		[batch 20/20] avg loss: 0.08298969484627716		[learning rate: 0.00095623]
	Learning Rate: 0.000956234
	LOSS [training: 0.08167884460439613 | validation: 0.049343115042023857]
	TIME [epoch: 8.2 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09498552123106752		[learning rate: 0.00095511]
		[batch 20/20] avg loss: 0.08597988026449628		[learning rate: 0.00095398]
	Learning Rate: 0.000953978
	LOSS [training: 0.09048270074778189 | validation: 0.054275554869014545]
	TIME [epoch: 8.2 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13858881090262437		[learning rate: 0.00095285]
		[batch 20/20] avg loss: 0.10211149373443269		[learning rate: 0.00095173]
	Learning Rate: 0.000951728
	LOSS [training: 0.12035015231852852 | validation: 0.09763873237410609]
	TIME [epoch: 8.23 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08090426853420991		[learning rate: 0.0009506]
		[batch 20/20] avg loss: 0.09567375077113852		[learning rate: 0.00094948]
	Learning Rate: 0.000949483
	LOSS [training: 0.0882890096526742 | validation: 0.04128186460254969]
	TIME [epoch: 8.2 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09398986042972098		[learning rate: 0.00094836]
		[batch 20/20] avg loss: 0.07805739042190654		[learning rate: 0.00094724]
	Learning Rate: 0.000947243
	LOSS [training: 0.08602362542581377 | validation: 0.06545799905353952]
	TIME [epoch: 8.23 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08806926020801095		[learning rate: 0.00094613]
		[batch 20/20] avg loss: 0.0803618987752647		[learning rate: 0.00094501]
	Learning Rate: 0.000945009
	LOSS [training: 0.08421557949163783 | validation: 0.03262798166826014]
	TIME [epoch: 8.2 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06878914810392098		[learning rate: 0.00094389]
		[batch 20/20] avg loss: 0.07734909595210476		[learning rate: 0.00094278]
	Learning Rate: 0.00094278
	LOSS [training: 0.07306912202801287 | validation: 0.04356932044089312]
	TIME [epoch: 8.23 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08004316310688372		[learning rate: 0.00094167]
		[batch 20/20] avg loss: 0.06874139060010713		[learning rate: 0.00094056]
	Learning Rate: 0.000940556
	LOSS [training: 0.07439227685349543 | validation: 0.03133729360210861]
	TIME [epoch: 8.23 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07953331676898368		[learning rate: 0.00093945]
		[batch 20/20] avg loss: 0.062375568316433615		[learning rate: 0.00093834]
	Learning Rate: 0.000938337
	LOSS [training: 0.07095444254270863 | validation: 0.04651827583934186]
	TIME [epoch: 8.22 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06759173699800176		[learning rate: 0.00093723]
		[batch 20/20] avg loss: 0.08808799351952971		[learning rate: 0.00093612]
	Learning Rate: 0.000936124
	LOSS [training: 0.07783986525876575 | validation: 0.10707719733253052]
	TIME [epoch: 8.2 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12812047967388024		[learning rate: 0.00093502]
		[batch 20/20] avg loss: 0.08840546369136423		[learning rate: 0.00093392]
	Learning Rate: 0.000933916
	LOSS [training: 0.10826297168262225 | validation: 0.04834230491633193]
	TIME [epoch: 8.22 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05798885029173475		[learning rate: 0.00093281]
		[batch 20/20] avg loss: 0.1003376097092652		[learning rate: 0.00093171]
	Learning Rate: 0.000931713
	LOSS [training: 0.07916323000049996 | validation: 0.04522557746495272]
	TIME [epoch: 8.22 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07092919860380682		[learning rate: 0.00093061]
		[batch 20/20] avg loss: 0.11442890779664308		[learning rate: 0.00092951]
	Learning Rate: 0.000929515
	LOSS [training: 0.09267905320022493 | validation: 0.05402033924321874]
	TIME [epoch: 8.21 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07432299108662625		[learning rate: 0.00092842]
		[batch 20/20] avg loss: 0.09546176243941382		[learning rate: 0.00092732]
	Learning Rate: 0.000927322
	LOSS [training: 0.08489237676302003 | validation: 0.0486149933434246]
	TIME [epoch: 8.19 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08325453046114502		[learning rate: 0.00092623]
		[batch 20/20] avg loss: 0.08214641851094365		[learning rate: 0.00092513]
	Learning Rate: 0.000925135
	LOSS [training: 0.08270047448604431 | validation: 0.050041955210990065]
	TIME [epoch: 8.23 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09160556048119703		[learning rate: 0.00092404]
		[batch 20/20] avg loss: 0.09370947041802923		[learning rate: 0.00092295]
	Learning Rate: 0.000922953
	LOSS [training: 0.09265751544961312 | validation: 0.06793288661585836]
	TIME [epoch: 8.22 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09790211176114805		[learning rate: 0.00092186]
		[batch 20/20] avg loss: 0.1455532177578382		[learning rate: 0.00092078]
	Learning Rate: 0.000920776
	LOSS [training: 0.12172766475949312 | validation: 0.053076075442499766]
	TIME [epoch: 8.22 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07307618149714684		[learning rate: 0.00091969]
		[batch 20/20] avg loss: 0.10274332668848127		[learning rate: 0.0009186]
	Learning Rate: 0.000918604
	LOSS [training: 0.08790975409281405 | validation: 0.0735193333119001]
	TIME [epoch: 8.19 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12570241759850065		[learning rate: 0.00091752]
		[batch 20/20] avg loss: 0.09072735813451907		[learning rate: 0.00091644]
	Learning Rate: 0.000916437
	LOSS [training: 0.10821488786650986 | validation: 0.06877103487647068]
	TIME [epoch: 8.22 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0956491141815217		[learning rate: 0.00091536]
		[batch 20/20] avg loss: 0.07894580593737534		[learning rate: 0.00091428]
	Learning Rate: 0.000914275
	LOSS [training: 0.0872974600594485 | validation: 0.06062154894075352]
	TIME [epoch: 8.23 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08361100440154397		[learning rate: 0.0009132]
		[batch 20/20] avg loss: 0.07444156971571683		[learning rate: 0.00091212]
	Learning Rate: 0.000912119
	LOSS [training: 0.0790262870586304 | validation: 0.031740551315032346]
	TIME [epoch: 8.21 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07112494782947285		[learning rate: 0.00091104]
		[batch 20/20] avg loss: 0.10511099407912991		[learning rate: 0.00090997]
	Learning Rate: 0.000909967
	LOSS [training: 0.08811797095430138 | validation: 0.0362368683200284]
	TIME [epoch: 8.2 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061890586922030144		[learning rate: 0.00090889]
		[batch 20/20] avg loss: 0.07407234530961168		[learning rate: 0.00090782]
	Learning Rate: 0.00090782
	LOSS [training: 0.06798146611582091 | validation: 0.090021790402355]
	TIME [epoch: 8.21 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09738274275606285		[learning rate: 0.00090675]
		[batch 20/20] avg loss: 0.080835672172934		[learning rate: 0.00090568]
	Learning Rate: 0.000905679
	LOSS [training: 0.08910920746449844 | validation: 0.05203786901396]
	TIME [epoch: 8.24 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08157729284578565		[learning rate: 0.00090461]
		[batch 20/20] avg loss: 0.08529115920323961		[learning rate: 0.00090354]
	Learning Rate: 0.000903543
	LOSS [training: 0.08343422602451264 | validation: 0.08194308311722076]
	TIME [epoch: 8.2 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07004398238385147		[learning rate: 0.00090248]
		[batch 20/20] avg loss: 0.10821996756565425		[learning rate: 0.00090141]
	Learning Rate: 0.000901411
	LOSS [training: 0.08913197497475285 | validation: 0.07090699864884155]
	TIME [epoch: 8.19 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08080007034088768		[learning rate: 0.00090035]
		[batch 20/20] avg loss: 0.07436881987158285		[learning rate: 0.00089929]
	Learning Rate: 0.000899285
	LOSS [training: 0.07758444510623523 | validation: 0.038977367222421774]
	TIME [epoch: 8.21 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07167779884098777		[learning rate: 0.00089822]
		[batch 20/20] avg loss: 0.082877818798282		[learning rate: 0.00089716]
	Learning Rate: 0.000897164
	LOSS [training: 0.07727780881963486 | validation: 0.06545663065265683]
	TIME [epoch: 8.25 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10141000623199174		[learning rate: 0.00089611]
		[batch 20/20] avg loss: 0.09128378763782949		[learning rate: 0.00089505]
	Learning Rate: 0.000895048
	LOSS [training: 0.09634689693491061 | validation: 0.05391682219321842]
	TIME [epoch: 8.21 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08275710577841157		[learning rate: 0.00089399]
		[batch 20/20] avg loss: 0.10827664502064091		[learning rate: 0.00089294]
	Learning Rate: 0.000892936
	LOSS [training: 0.09551687539952623 | validation: 0.03733782118822899]
	TIME [epoch: 8.19 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09291044379939221		[learning rate: 0.00089188]
		[batch 20/20] avg loss: 0.0705146784377714		[learning rate: 0.00089083]
	Learning Rate: 0.00089083
	LOSS [training: 0.08171256111858181 | validation: 0.07989194397465146]
	TIME [epoch: 8.21 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07823903181895843		[learning rate: 0.00088978]
		[batch 20/20] avg loss: 0.07985014798734033		[learning rate: 0.00088873]
	Learning Rate: 0.000888729
	LOSS [training: 0.0790445899031494 | validation: 0.14220263166452068]
	TIME [epoch: 8.25 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07885486508822775		[learning rate: 0.00088768]
		[batch 20/20] avg loss: 0.09385807737626051		[learning rate: 0.00088663]
	Learning Rate: 0.000886632
	LOSS [training: 0.08635647123224413 | validation: 0.06979353627767405]
	TIME [epoch: 8.19 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10715512670358902		[learning rate: 0.00088559]
		[batch 20/20] avg loss: 0.055150459280809694		[learning rate: 0.00088454]
	Learning Rate: 0.000884541
	LOSS [training: 0.08115279299219938 | validation: 0.09332350077559007]
	TIME [epoch: 8.19 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08395654133546043		[learning rate: 0.0008835]
		[batch 20/20] avg loss: 0.10034203761615892		[learning rate: 0.00088245]
	Learning Rate: 0.000882454
	LOSS [training: 0.09214928947580968 | validation: 0.03262679773368053]
	TIME [epoch: 8.22 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08433704112347516		[learning rate: 0.00088141]
		[batch 20/20] avg loss: 0.07359277030773329		[learning rate: 0.00088037]
	Learning Rate: 0.000880373
	LOSS [training: 0.07896490571560423 | validation: 0.04535261340056177]
	TIME [epoch: 8.24 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07923193585499104		[learning rate: 0.00087933]
		[batch 20/20] avg loss: 0.08801379370678927		[learning rate: 0.0008783]
	Learning Rate: 0.000878296
	LOSS [training: 0.08362286478089016 | validation: 0.03780613801985955]
	TIME [epoch: 8.19 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07097939890852907		[learning rate: 0.00087726]
		[batch 20/20] avg loss: 0.08646710890826483		[learning rate: 0.00087622]
	Learning Rate: 0.000876224
	LOSS [training: 0.07872325390839695 | validation: 0.05887371054407939]
	TIME [epoch: 8.2 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0945765875321993		[learning rate: 0.00087519]
		[batch 20/20] avg loss: 0.08713700024587505		[learning rate: 0.00087416]
	Learning Rate: 0.000874157
	LOSS [training: 0.09085679388903717 | validation: 0.04526424478805073]
	TIME [epoch: 8.22 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06017237364719195		[learning rate: 0.00087313]
		[batch 20/20] avg loss: 0.07616646749473661		[learning rate: 0.0008721]
	Learning Rate: 0.000872096
	LOSS [training: 0.06816942057096428 | validation: 0.029931428827139735]
	TIME [epoch: 8.2 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09448610347544344		[learning rate: 0.00087107]
		[batch 20/20] avg loss: 0.07096749608992567		[learning rate: 0.00087004]
	Learning Rate: 0.000870038
	LOSS [training: 0.08272679978268456 | validation: 0.06339806882680783]
	TIME [epoch: 8.19 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07759194462612765		[learning rate: 0.00086901]
		[batch 20/20] avg loss: 0.0711997492610641		[learning rate: 0.00086799]
	Learning Rate: 0.000867986
	LOSS [training: 0.07439584694359588 | validation: 0.019353686842487478]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_1085.pth
	Model improved!!!
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0957144029515857		[learning rate: 0.00086696]
		[batch 20/20] avg loss: 0.09920418031869281		[learning rate: 0.00086594]
	Learning Rate: 0.000865939
	LOSS [training: 0.09745929163513924 | validation: 0.025184793127179]
	TIME [epoch: 8.23 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07633430589650791		[learning rate: 0.00086492]
		[batch 20/20] avg loss: 0.07884261581000766		[learning rate: 0.0008639]
	Learning Rate: 0.000863896
	LOSS [training: 0.0775884608532578 | validation: 0.055886281767654394]
	TIME [epoch: 8.2 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10258167104053288		[learning rate: 0.00086288]
		[batch 20/20] avg loss: 0.08175953602159272		[learning rate: 0.00086186]
	Learning Rate: 0.000861858
	LOSS [training: 0.09217060353106281 | validation: 0.03670378283807508]
	TIME [epoch: 8.19 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06688682682572031		[learning rate: 0.00086084]
		[batch 20/20] avg loss: 0.09135345629293254		[learning rate: 0.00085983]
	Learning Rate: 0.000859825
	LOSS [training: 0.07912014155932642 | validation: 0.029291578616897927]
	TIME [epoch: 8.2 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06295678480447958		[learning rate: 0.00085881]
		[batch 20/20] avg loss: 0.11902666319443049		[learning rate: 0.0008578]
	Learning Rate: 0.000857797
	LOSS [training: 0.09099172399945506 | validation: 0.08674074758047941]
	TIME [epoch: 8.24 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07030546240145422		[learning rate: 0.00085678]
		[batch 20/20] avg loss: 0.07549035805262491		[learning rate: 0.00085577]
	Learning Rate: 0.000855774
	LOSS [training: 0.07289791022703958 | validation: 0.07509474335362165]
	TIME [epoch: 8.2 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09356930391655882		[learning rate: 0.00085476]
		[batch 20/20] avg loss: 0.08607918648211446		[learning rate: 0.00085376]
	Learning Rate: 0.000853755
	LOSS [training: 0.08982424519933666 | validation: 0.05251813604608105]
	TIME [epoch: 8.19 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10975570525537497		[learning rate: 0.00085275]
		[batch 20/20] avg loss: 0.09724442517372436		[learning rate: 0.00085174]
	Learning Rate: 0.000851741
	LOSS [training: 0.10350006521454966 | validation: 0.03227824159120782]
	TIME [epoch: 8.21 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07084374632315302		[learning rate: 0.00085074]
		[batch 20/20] avg loss: 0.12385265579692148		[learning rate: 0.00084973]
	Learning Rate: 0.000849732
	LOSS [training: 0.09734820106003725 | validation: 0.09201006120123585]
	TIME [epoch: 8.23 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10954495352321966		[learning rate: 0.00084873]
		[batch 20/20] avg loss: 0.079721192605508		[learning rate: 0.00084773]
	Learning Rate: 0.000847728
	LOSS [training: 0.09463307306436385 | validation: 0.03314177746393537]
	TIME [epoch: 8.21 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09722862502280687		[learning rate: 0.00084673]
		[batch 20/20] avg loss: 0.06967211473683303		[learning rate: 0.00084573]
	Learning Rate: 0.000845728
	LOSS [training: 0.08345036987981995 | validation: 0.0845090901531963]
	TIME [epoch: 8.19 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08284482897057094		[learning rate: 0.00084473]
		[batch 20/20] avg loss: 0.056158204036885394		[learning rate: 0.00084373]
	Learning Rate: 0.000843733
	LOSS [training: 0.06950151650372817 | validation: 0.0519855524949023]
	TIME [epoch: 8.22 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06530790000159126		[learning rate: 0.00084274]
		[batch 20/20] avg loss: 0.0883508907611977		[learning rate: 0.00084174]
	Learning Rate: 0.000841743
	LOSS [training: 0.0768293953813945 | validation: 0.1710966903783492]
	TIME [epoch: 8.24 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10327928018008552		[learning rate: 0.00084075]
		[batch 20/20] avg loss: 0.08322202487905579		[learning rate: 0.00083976]
	Learning Rate: 0.000839757
	LOSS [training: 0.09325065252957065 | validation: 0.18505046469538744]
	TIME [epoch: 8.21 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11773537738295745		[learning rate: 0.00083877]
		[batch 20/20] avg loss: 0.07402590353908488		[learning rate: 0.00083778]
	Learning Rate: 0.000837777
	LOSS [training: 0.09588064046102117 | validation: 0.028120273073181032]
	TIME [epoch: 8.19 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06810699740289068		[learning rate: 0.00083679]
		[batch 20/20] avg loss: 0.07884751402466564		[learning rate: 0.0008358]
	Learning Rate: 0.0008358
	LOSS [training: 0.07347725571377817 | validation: 0.04912721124604719]
	TIME [epoch: 8.21 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12199582638552339		[learning rate: 0.00083481]
		[batch 20/20] avg loss: 0.07675477679667587		[learning rate: 0.00083383]
	Learning Rate: 0.000833829
	LOSS [training: 0.09937530159109964 | validation: 0.06540949423406224]
	TIME [epoch: 8.22 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09961432856994097		[learning rate: 0.00083284]
		[batch 20/20] avg loss: 0.09896124431316919		[learning rate: 0.00083186]
	Learning Rate: 0.000831862
	LOSS [training: 0.09928778644155509 | validation: 0.0967463571678597]
	TIME [epoch: 8.22 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08588787639812795		[learning rate: 0.00083088]
		[batch 20/20] avg loss: 0.10085255195758192		[learning rate: 0.0008299]
	Learning Rate: 0.0008299
	LOSS [training: 0.09337021417785493 | validation: 0.05328292986724213]
	TIME [epoch: 8.19 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09479422581723054		[learning rate: 0.00082892]
		[batch 20/20] avg loss: 0.11694260498170128		[learning rate: 0.00082794]
	Learning Rate: 0.000827942
	LOSS [training: 0.10586841539946593 | validation: 0.033240743435710804]
	TIME [epoch: 8.21 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05883389023389105		[learning rate: 0.00082697]
		[batch 20/20] avg loss: 0.07373369510045394		[learning rate: 0.00082599]
	Learning Rate: 0.000825989
	LOSS [training: 0.06628379266717249 | validation: 0.053821707248917765]
	TIME [epoch: 8.21 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07639416777699808		[learning rate: 0.00082501]
		[batch 20/20] avg loss: 0.09608352792581913		[learning rate: 0.00082404]
	Learning Rate: 0.000824041
	LOSS [training: 0.08623884785140859 | validation: 0.07296001751785405]
	TIME [epoch: 8.21 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08939962600605471		[learning rate: 0.00082307]
		[batch 20/20] avg loss: 0.10091549314222692		[learning rate: 0.0008221]
	Learning Rate: 0.000822097
	LOSS [training: 0.0951575595741408 | validation: 0.09654328637096064]
	TIME [epoch: 8.2 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06427344136531356		[learning rate: 0.00082113]
		[batch 20/20] avg loss: 0.09438099752191073		[learning rate: 0.00082016]
	Learning Rate: 0.000820158
	LOSS [training: 0.07932721944361214 | validation: 0.03741394339409806]
	TIME [epoch: 8.23 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09249982267263163		[learning rate: 0.00081919]
		[batch 20/20] avg loss: 0.05778772510115036		[learning rate: 0.00081822]
	Learning Rate: 0.000818223
	LOSS [training: 0.07514377388689099 | validation: 0.08489476000725327]
	TIME [epoch: 8.21 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09473026116896655		[learning rate: 0.00081726]
		[batch 20/20] avg loss: 0.0831316489241658		[learning rate: 0.00081629]
	Learning Rate: 0.000816293
	LOSS [training: 0.08893095504656617 | validation: 0.10924448822090407]
	TIME [epoch: 8.22 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09383691941860246		[learning rate: 0.00081533]
		[batch 20/20] avg loss: 0.07055505743383968		[learning rate: 0.00081437]
	Learning Rate: 0.000814368
	LOSS [training: 0.08219598842622106 | validation: 0.07159741299690993]
	TIME [epoch: 8.19 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0996681527824778		[learning rate: 0.00081341]
		[batch 20/20] avg loss: 0.059923829387706906		[learning rate: 0.00081245]
	Learning Rate: 0.000812447
	LOSS [training: 0.07979599108509236 | validation: 0.034354258130549126]
	TIME [epoch: 8.22 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06630662616535532		[learning rate: 0.00081149]
		[batch 20/20] avg loss: 0.08612676893172326		[learning rate: 0.00081053]
	Learning Rate: 0.00081053
	LOSS [training: 0.0762166975485393 | validation: 0.09822945565134558]
	TIME [epoch: 8.2 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07700360161141433		[learning rate: 0.00080957]
		[batch 20/20] avg loss: 0.10469592112811384		[learning rate: 0.00080862]
	Learning Rate: 0.000808618
	LOSS [training: 0.09084976136976405 | validation: 0.03158282856895382]
	TIME [epoch: 8.22 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06564156794529497		[learning rate: 0.00080766]
		[batch 20/20] avg loss: 0.08618582833513758		[learning rate: 0.00080671]
	Learning Rate: 0.000806711
	LOSS [training: 0.07591369814021628 | validation: 0.03974041251103367]
	TIME [epoch: 8.2 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06862259162740586		[learning rate: 0.00080576]
		[batch 20/20] avg loss: 0.07143153403874439		[learning rate: 0.00080481]
	Learning Rate: 0.000804808
	LOSS [training: 0.07002706283307512 | validation: 0.028756449169288205]
	TIME [epoch: 8.24 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06698260416635193		[learning rate: 0.00080386]
		[batch 20/20] avg loss: 0.06283263955344968		[learning rate: 0.00080291]
	Learning Rate: 0.00080291
	LOSS [training: 0.06490762185990082 | validation: 0.04063385930332607]
	TIME [epoch: 8.21 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06102656518572068		[learning rate: 0.00080196]
		[batch 20/20] avg loss: 0.0922389310737772		[learning rate: 0.00080102]
	Learning Rate: 0.000801016
	LOSS [training: 0.07663274812974893 | validation: 0.032429448332362165]
	TIME [epoch: 8.23 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0819290995376729		[learning rate: 0.00080007]
		[batch 20/20] avg loss: 0.09719081130603482		[learning rate: 0.00079913]
	Learning Rate: 0.000799126
	LOSS [training: 0.08955995542185388 | validation: 0.03544399226907077]
	TIME [epoch: 8.19 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06338942010314591		[learning rate: 0.00079818]
		[batch 20/20] avg loss: 0.07552304371275179		[learning rate: 0.00079724]
	Learning Rate: 0.000797241
	LOSS [training: 0.06945623190794883 | validation: 0.037451228116531364]
	TIME [epoch: 8.24 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07069977326082239		[learning rate: 0.0007963]
		[batch 20/20] avg loss: 0.09136511206792572		[learning rate: 0.00079536]
	Learning Rate: 0.000795361
	LOSS [training: 0.08103244266437404 | validation: 0.0471235729396815]
	TIME [epoch: 8.19 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05506247508437678		[learning rate: 0.00079442]
		[batch 20/20] avg loss: 0.10224053021212061		[learning rate: 0.00079348]
	Learning Rate: 0.000793484
	LOSS [training: 0.07865150264824872 | validation: 0.07085135313995097]
	TIME [epoch: 8.23 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07575146632741239		[learning rate: 0.00079255]
		[batch 20/20] avg loss: 0.06516145181791749		[learning rate: 0.00079161]
	Learning Rate: 0.000791613
	LOSS [training: 0.07045645907266493 | validation: 0.04035404928498176]
	TIME [epoch: 8.2 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07726464021439645		[learning rate: 0.00079068]
		[batch 20/20] avg loss: 0.07496052766121578		[learning rate: 0.00078975]
	Learning Rate: 0.000789745
	LOSS [training: 0.07611258393780612 | validation: 0.06629991241235446]
	TIME [epoch: 8.23 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0747834257696399		[learning rate: 0.00078881]
		[batch 20/20] avg loss: 0.07043866150514078		[learning rate: 0.00078788]
	Learning Rate: 0.000787882
	LOSS [training: 0.07261104363739032 | validation: 0.05911423649700464]
	TIME [epoch: 8.19 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0543672065457643		[learning rate: 0.00078695]
		[batch 20/20] avg loss: 0.08779776933760858		[learning rate: 0.00078602]
	Learning Rate: 0.000786024
	LOSS [training: 0.07108248794168641 | validation: 0.03842455829707885]
	TIME [epoch: 8.23 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0802748332200367		[learning rate: 0.0007851]
		[batch 20/20] avg loss: 0.055859901737853344		[learning rate: 0.00078417]
	Learning Rate: 0.00078417
	LOSS [training: 0.06806736747894504 | validation: 0.06486296341644669]
	TIME [epoch: 8.21 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07597501396908095		[learning rate: 0.00078324]
		[batch 20/20] avg loss: 0.09206518940254724		[learning rate: 0.00078232]
	Learning Rate: 0.00078232
	LOSS [training: 0.0840201016858141 | validation: 0.024321052514204335]
	TIME [epoch: 8.22 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06903692078969252		[learning rate: 0.0007814]
		[batch 20/20] avg loss: 0.07870372717840142		[learning rate: 0.00078047]
	Learning Rate: 0.000780475
	LOSS [training: 0.07387032398404697 | validation: 0.058023305990525825]
	TIME [epoch: 8.2 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0892436758821956		[learning rate: 0.00077955]
		[batch 20/20] avg loss: 0.05791456354048058		[learning rate: 0.00077863]
	Learning Rate: 0.000778634
	LOSS [training: 0.07357911971133807 | validation: 0.11818996326023223]
	TIME [epoch: 8.22 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08817691648209383		[learning rate: 0.00077772]
		[batch 20/20] avg loss: 0.07488349334706519		[learning rate: 0.0007768]
	Learning Rate: 0.000776797
	LOSS [training: 0.0815302049145795 | validation: 0.12554307246730137]
	TIME [epoch: 8.22 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07755103343916386		[learning rate: 0.00077588]
		[batch 20/20] avg loss: 0.09435466822661835		[learning rate: 0.00077496]
	Learning Rate: 0.000774965
	LOSS [training: 0.0859528508328911 | validation: 0.07282984956808948]
	TIME [epoch: 8.21 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07586484203032216		[learning rate: 0.00077405]
		[batch 20/20] avg loss: 0.07592620494674344		[learning rate: 0.00077314]
	Learning Rate: 0.000773137
	LOSS [training: 0.0758955234885328 | validation: 0.06487599983287726]
	TIME [epoch: 8.19 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06523611942780858		[learning rate: 0.00077222]
		[batch 20/20] avg loss: 0.08281400454595214		[learning rate: 0.00077131]
	Learning Rate: 0.000771313
	LOSS [training: 0.07402506198688037 | validation: 0.034981170489130284]
	TIME [epoch: 8.22 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08779919029370836		[learning rate: 0.0007704]
		[batch 20/20] avg loss: 0.0843533268843838		[learning rate: 0.00076949]
	Learning Rate: 0.000769494
	LOSS [training: 0.08607625858904606 | validation: 0.05994794040427569]
	TIME [epoch: 8.22 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0655978623118366		[learning rate: 0.00076859]
		[batch 20/20] avg loss: 0.08007777858069105		[learning rate: 0.00076768]
	Learning Rate: 0.000767679
	LOSS [training: 0.07283782044626382 | validation: 0.05826125997858059]
	TIME [epoch: 8.22 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07121568117048296		[learning rate: 0.00076677]
		[batch 20/20] avg loss: 0.0796251209975346		[learning rate: 0.00076587]
	Learning Rate: 0.000765868
	LOSS [training: 0.07542040108400877 | validation: 0.03338891304553316]
	TIME [epoch: 8.19 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07594352654330438		[learning rate: 0.00076496]
		[batch 20/20] avg loss: 0.06470806390986852		[learning rate: 0.00076406]
	Learning Rate: 0.000764061
	LOSS [training: 0.07032579522658644 | validation: 0.08042361784431136]
	TIME [epoch: 8.22 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09033181580449413		[learning rate: 0.00076316]
		[batch 20/20] avg loss: 0.06635337444298671		[learning rate: 0.00076226]
	Learning Rate: 0.000762259
	LOSS [training: 0.07834259512374041 | validation: 0.05105273837185147]
	TIME [epoch: 8.22 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07551831332036424		[learning rate: 0.00076136]
		[batch 20/20] avg loss: 0.09921686982967691		[learning rate: 0.00076046]
	Learning Rate: 0.000760461
	LOSS [training: 0.08736759157502058 | validation: 0.074007655317803]
	TIME [epoch: 8.22 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06316305924538332		[learning rate: 0.00075956]
		[batch 20/20] avg loss: 0.0647355360390265		[learning rate: 0.00075867]
	Learning Rate: 0.000758667
	LOSS [training: 0.06394929764220492 | validation: 0.039154677988249585]
	TIME [epoch: 8.19 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0828004584870527		[learning rate: 0.00075777]
		[batch 20/20] avg loss: 0.04074141922845668		[learning rate: 0.00075688]
	Learning Rate: 0.000756877
	LOSS [training: 0.0617709388577547 | validation: 0.07682160773723087]
	TIME [epoch: 8.22 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07080534480710672		[learning rate: 0.00075598]
		[batch 20/20] avg loss: 0.076603781346396		[learning rate: 0.00075509]
	Learning Rate: 0.000755092
	LOSS [training: 0.07370456307675136 | validation: 0.06011297700832767]
	TIME [epoch: 8.24 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06408939486994127		[learning rate: 0.0007542]
		[batch 20/20] avg loss: 0.0588584254754219		[learning rate: 0.00075331]
	Learning Rate: 0.000753311
	LOSS [training: 0.06147391017268158 | validation: 0.07046323731806982]
	TIME [epoch: 8.21 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06091783040131471		[learning rate: 0.00075242]
		[batch 20/20] avg loss: 0.07641020224357822		[learning rate: 0.00075153]
	Learning Rate: 0.000751534
	LOSS [training: 0.06866401632244648 | validation: 0.03320922780619482]
	TIME [epoch: 8.2 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05361881974356568		[learning rate: 0.00075065]
		[batch 20/20] avg loss: 0.05655646587073991		[learning rate: 0.00074976]
	Learning Rate: 0.000749761
	LOSS [training: 0.0550876428071528 | validation: 0.052828604286216475]
	TIME [epoch: 8.22 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08918256754144814		[learning rate: 0.00074888]
		[batch 20/20] avg loss: 0.06515881031893038		[learning rate: 0.00074799]
	Learning Rate: 0.000747993
	LOSS [training: 0.07717068893018927 | validation: 0.05980859456332313]
	TIME [epoch: 8.23 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09881332705244333		[learning rate: 0.00074711]
		[batch 20/20] avg loss: 0.07878376421608557		[learning rate: 0.00074623]
	Learning Rate: 0.000746228
	LOSS [training: 0.08879854563426445 | validation: 0.050739548546366364]
	TIME [epoch: 8.2 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07669549301751158		[learning rate: 0.00074535]
		[batch 20/20] avg loss: 0.08682704142882097		[learning rate: 0.00074447]
	Learning Rate: 0.000744468
	LOSS [training: 0.08176126722316628 | validation: 0.0400221830774357]
	TIME [epoch: 8.19 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06199765555810573		[learning rate: 0.00074359]
		[batch 20/20] avg loss: 0.07650749990426355		[learning rate: 0.00074271]
	Learning Rate: 0.000742712
	LOSS [training: 0.06925257773118462 | validation: 0.022736163551430964]
	TIME [epoch: 8.22 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07389784626673404		[learning rate: 0.00074184]
		[batch 20/20] avg loss: 0.10922288664253228		[learning rate: 0.00074096]
	Learning Rate: 0.00074096
	LOSS [training: 0.09156036645463314 | validation: 0.01655968433363346]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_1152.pth
	Model improved!!!
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059336526138384325		[learning rate: 0.00074009]
		[batch 20/20] avg loss: 0.07568644374537875		[learning rate: 0.00073921]
	Learning Rate: 0.000739212
	LOSS [training: 0.06751148494188156 | validation: 0.053180063169091435]
	TIME [epoch: 8.19 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08550949578727542		[learning rate: 0.00073834]
		[batch 20/20] avg loss: 0.06091688933882078		[learning rate: 0.00073747]
	Learning Rate: 0.000737469
	LOSS [training: 0.0732131925630481 | validation: 0.06538532146742909]
	TIME [epoch: 8.19 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09911980223197356		[learning rate: 0.0007366]
		[batch 20/20] avg loss: 0.08263423274931048		[learning rate: 0.00073573]
	Learning Rate: 0.000735729
	LOSS [training: 0.09087701749064202 | validation: 0.051853971447098975]
	TIME [epoch: 8.21 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08540647097932196		[learning rate: 0.00073486]
		[batch 20/20] avg loss: 0.05694627974115157		[learning rate: 0.00073399]
	Learning Rate: 0.000733994
	LOSS [training: 0.07117637536023677 | validation: 0.03058605169125008]
	TIME [epoch: 8.22 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07358147116579464		[learning rate: 0.00073313]
		[batch 20/20] avg loss: 0.08837626232652071		[learning rate: 0.00073226]
	Learning Rate: 0.000732262
	LOSS [training: 0.08097886674615769 | validation: 0.042616800971766955]
	TIME [epoch: 8.19 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08139355800098853		[learning rate: 0.0007314]
		[batch 20/20] avg loss: 0.06479488946131629		[learning rate: 0.00073054]
	Learning Rate: 0.000730535
	LOSS [training: 0.0730942237311524 | validation: 0.05061926983947343]
	TIME [epoch: 8.18 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08758708742692553		[learning rate: 0.00072967]
		[batch 20/20] avg loss: 0.10143248112812045		[learning rate: 0.00072881]
	Learning Rate: 0.000728812
	LOSS [training: 0.09450978427752299 | validation: 0.05495364290124987]
	TIME [epoch: 8.22 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07846816527682192		[learning rate: 0.00072795]
		[batch 20/20] avg loss: 0.07761656143918141		[learning rate: 0.00072709]
	Learning Rate: 0.000727093
	LOSS [training: 0.07804236335800167 | validation: 0.03653155227383991]
	TIME [epoch: 8.23 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06122850709338307		[learning rate: 0.00072623]
		[batch 20/20] avg loss: 0.06843521803113375		[learning rate: 0.00072538]
	Learning Rate: 0.000725378
	LOSS [training: 0.0648318625622584 | validation: 0.025348299062537463]
	TIME [epoch: 8.19 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07137680075490022		[learning rate: 0.00072452]
		[batch 20/20] avg loss: 0.06750774532880252		[learning rate: 0.00072367]
	Learning Rate: 0.000723666
	LOSS [training: 0.06944227304185135 | validation: 0.07979826945165697]
	TIME [epoch: 8.18 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06789562305184793		[learning rate: 0.00072281]
		[batch 20/20] avg loss: 0.08010753809560953		[learning rate: 0.00072196]
	Learning Rate: 0.000721959
	LOSS [training: 0.07400158057372873 | validation: 0.06670633464222878]
	TIME [epoch: 8.23 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06547489526417177		[learning rate: 0.00072111]
		[batch 20/20] avg loss: 0.078376096099953		[learning rate: 0.00072026]
	Learning Rate: 0.000720257
	LOSS [training: 0.07192549568206238 | validation: 0.02386448580444419]
	TIME [epoch: 8.23 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06248133104715692		[learning rate: 0.00071941]
		[batch 20/20] avg loss: 0.06764669624945713		[learning rate: 0.00071856]
	Learning Rate: 0.000718557
	LOSS [training: 0.06506401364830702 | validation: 0.06942922806659912]
	TIME [epoch: 8.18 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07776164073140618		[learning rate: 0.00071771]
		[batch 20/20] avg loss: 0.0769289471120064		[learning rate: 0.00071686]
	Learning Rate: 0.000716863
	LOSS [training: 0.07734529392170628 | validation: 0.028573767350491098]
	TIME [epoch: 8.18 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06656856445007676		[learning rate: 0.00071602]
		[batch 20/20] avg loss: 0.07264357615883081		[learning rate: 0.00071517]
	Learning Rate: 0.000715172
	LOSS [training: 0.06960607030445379 | validation: 0.0652241535447833]
	TIME [epoch: 8.22 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08338158377878227		[learning rate: 0.00071433]
		[batch 20/20] avg loss: 0.10098295747974491		[learning rate: 0.00071348]
	Learning Rate: 0.000713485
	LOSS [training: 0.09218227062926358 | validation: 0.042717813047623454]
	TIME [epoch: 8.22 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05938609101330781		[learning rate: 0.00071264]
		[batch 20/20] avg loss: 0.07723647702858702		[learning rate: 0.0007118]
	Learning Rate: 0.000711802
	LOSS [training: 0.06831128402094741 | validation: 0.037571765176286694]
	TIME [epoch: 8.19 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07157636133104081		[learning rate: 0.00071096]
		[batch 20/20] avg loss: 0.0609859259878869		[learning rate: 0.00071012]
	Learning Rate: 0.000710123
	LOSS [training: 0.06628114365946385 | validation: 0.021652576696359463]
	TIME [epoch: 8.18 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06690860087443287		[learning rate: 0.00070928]
		[batch 20/20] avg loss: 0.07344643249865107		[learning rate: 0.00070845]
	Learning Rate: 0.000708448
	LOSS [training: 0.07017751668654196 | validation: 0.027465745361288374]
	TIME [epoch: 8.23 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07195424890470405		[learning rate: 0.00070761]
		[batch 20/20] avg loss: 0.058197987959193655		[learning rate: 0.00070678]
	Learning Rate: 0.000706776
	LOSS [training: 0.06507611843194885 | validation: 0.0795333420194799]
	TIME [epoch: 8.21 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07435848749450408		[learning rate: 0.00070594]
		[batch 20/20] avg loss: 0.0495205380503003		[learning rate: 0.00070511]
	Learning Rate: 0.000705109
	LOSS [training: 0.061939512772402186 | validation: 0.08326873665048157]
	TIME [epoch: 8.19 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06638507145638356		[learning rate: 0.00070428]
		[batch 20/20] avg loss: 0.05836490013698879		[learning rate: 0.00070345]
	Learning Rate: 0.000703446
	LOSS [training: 0.06237498579668617 | validation: 0.0746439580090775]
	TIME [epoch: 8.19 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0713791905061886		[learning rate: 0.00070262]
		[batch 20/20] avg loss: 0.07103565771802571		[learning rate: 0.00070179]
	Learning Rate: 0.000701787
	LOSS [training: 0.07120742411210716 | validation: 0.03833958280087328]
	TIME [epoch: 8.22 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07351530614200064		[learning rate: 0.00070096]
		[batch 20/20] avg loss: 0.06554831156090897		[learning rate: 0.00070013]
	Learning Rate: 0.000700131
	LOSS [training: 0.0695318088514548 | validation: 0.049450174305283714]
	TIME [epoch: 8.22 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07729497140693578		[learning rate: 0.00069931]
		[batch 20/20] avg loss: 0.07208947666871217		[learning rate: 0.00069848]
	Learning Rate: 0.00069848
	LOSS [training: 0.07469222403782397 | validation: 0.09866585600927096]
	TIME [epoch: 8.19 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10099974971669448		[learning rate: 0.00069766]
		[batch 20/20] avg loss: 0.08372224518620679		[learning rate: 0.00069683]
	Learning Rate: 0.000696832
	LOSS [training: 0.09236099745145064 | validation: 0.04895103599825582]
	TIME [epoch: 8.19 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08735144548574424		[learning rate: 0.00069601]
		[batch 20/20] avg loss: 0.08569970690025637		[learning rate: 0.00069519]
	Learning Rate: 0.000695188
	LOSS [training: 0.0865255761930003 | validation: 0.045679962867995166]
	TIME [epoch: 8.24 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057849359516601015		[learning rate: 0.00069437]
		[batch 20/20] avg loss: 0.09188446802214388		[learning rate: 0.00069355]
	Learning Rate: 0.000693549
	LOSS [training: 0.07486691376937245 | validation: 0.036776669696003336]
	TIME [epoch: 8.2 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07437055126746298		[learning rate: 0.00069273]
		[batch 20/20] avg loss: 0.05241515061245004		[learning rate: 0.00069191]
	Learning Rate: 0.000691913
	LOSS [training: 0.0633928509399565 | validation: 0.044518494925931545]
	TIME [epoch: 8.19 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06850308681180797		[learning rate: 0.0006911]
		[batch 20/20] avg loss: 0.06035452595779604		[learning rate: 0.00069028]
	Learning Rate: 0.00069028
	LOSS [training: 0.064428806384802 | validation: 0.04930494590709082]
	TIME [epoch: 8.2 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05893104398079194		[learning rate: 0.00068947]
		[batch 20/20] avg loss: 0.07014640294734661		[learning rate: 0.00068865]
	Learning Rate: 0.000688652
	LOSS [training: 0.06453872346406928 | validation: 0.0336952558313201]
	TIME [epoch: 8.21 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060980818624142084		[learning rate: 0.00068784]
		[batch 20/20] avg loss: 0.06479102291126798		[learning rate: 0.00068703]
	Learning Rate: 0.000687028
	LOSS [training: 0.06288592076770502 | validation: 0.02839168573912536]
	TIME [epoch: 8.2 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0677970630946952		[learning rate: 0.00068622]
		[batch 20/20] avg loss: 0.08572216142449207		[learning rate: 0.00068541]
	Learning Rate: 0.000685407
	LOSS [training: 0.07675961225959364 | validation: 0.06517151143771774]
	TIME [epoch: 8.21 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0690229489817068		[learning rate: 0.0006846]
		[batch 20/20] avg loss: 0.07063969039827211		[learning rate: 0.00068379]
	Learning Rate: 0.00068379
	LOSS [training: 0.06983131968998946 | validation: 0.03543194165928468]
	TIME [epoch: 8.2 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061351890094629116		[learning rate: 0.00068298]
		[batch 20/20] avg loss: 0.059517224318451835		[learning rate: 0.00068218]
	Learning Rate: 0.000682178
	LOSS [training: 0.06043455720654047 | validation: 0.068469099843759]
	TIME [epoch: 8.2 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07889924596456249		[learning rate: 0.00068137]
		[batch 20/20] avg loss: 0.05224381606356558		[learning rate: 0.00068057]
	Learning Rate: 0.000680568
	LOSS [training: 0.06557153101406403 | validation: 0.052560722066983576]
	TIME [epoch: 8.21 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061909878035299695		[learning rate: 0.00067977]
		[batch 20/20] avg loss: 0.09800195915325463		[learning rate: 0.00067896]
	Learning Rate: 0.000678963
	LOSS [training: 0.07995591859427717 | validation: 0.02490004073390665]
	TIME [epoch: 8.22 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06126516229804513		[learning rate: 0.00067816]
		[batch 20/20] avg loss: 0.058076205097219204		[learning rate: 0.00067736]
	Learning Rate: 0.000677362
	LOSS [training: 0.05967068369763216 | validation: 0.03514034031705697]
	TIME [epoch: 8.2 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07016742107416919		[learning rate: 0.00067656]
		[batch 20/20] avg loss: 0.05961839556700199		[learning rate: 0.00067576]
	Learning Rate: 0.000675764
	LOSS [training: 0.0648929083205856 | validation: 0.037283520242118416]
	TIME [epoch: 8.21 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08982165324731109		[learning rate: 0.00067497]
		[batch 20/20] avg loss: 0.05703498376306957		[learning rate: 0.00067417]
	Learning Rate: 0.00067417
	LOSS [training: 0.07342831850519035 | validation: 0.026407209183392656]
	TIME [epoch: 8.19 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04405264581925452		[learning rate: 0.00067337]
		[batch 20/20] avg loss: 0.06827903258178776		[learning rate: 0.00067258]
	Learning Rate: 0.000672579
	LOSS [training: 0.05616583920052114 | validation: 0.05820130460742248]
	TIME [epoch: 8.23 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06907532817002796		[learning rate: 0.00067179]
		[batch 20/20] avg loss: 0.07771255548160508		[learning rate: 0.00067099]
	Learning Rate: 0.000670993
	LOSS [training: 0.07339394182581652 | validation: 0.06880133136210698]
	TIME [epoch: 8.19 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07861489910987805		[learning rate: 0.0006702]
		[batch 20/20] avg loss: 0.09558085217669668		[learning rate: 0.00066941]
	Learning Rate: 0.00066941
	LOSS [training: 0.08709787564328737 | validation: 0.03951714375462471]
	TIME [epoch: 8.2 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06404116869305568		[learning rate: 0.00066862]
		[batch 20/20] avg loss: 0.05833648939655474		[learning rate: 0.00066783]
	Learning Rate: 0.000667831
	LOSS [training: 0.0611888290448052 | validation: 0.04229914743483164]
	TIME [epoch: 8.19 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08360310051685374		[learning rate: 0.00066704]
		[batch 20/20] avg loss: 0.05036728653883493		[learning rate: 0.00066626]
	Learning Rate: 0.000666256
	LOSS [training: 0.06698519352784434 | validation: 0.036590611086222885]
	TIME [epoch: 8.23 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06392367819695985		[learning rate: 0.00066547]
		[batch 20/20] avg loss: 0.07840609815919682		[learning rate: 0.00066468]
	Learning Rate: 0.000664684
	LOSS [training: 0.07116488817807834 | validation: 0.04196155688715962]
	TIME [epoch: 8.18 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0735034550528329		[learning rate: 0.0006639]
		[batch 20/20] avg loss: 0.049799413630342085		[learning rate: 0.00066312]
	Learning Rate: 0.000663116
	LOSS [training: 0.0616514343415875 | validation: 0.03263789800002914]
	TIME [epoch: 8.21 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06807957164769074		[learning rate: 0.00066233]
		[batch 20/20] avg loss: 0.05225514736851875		[learning rate: 0.00066155]
	Learning Rate: 0.000661552
	LOSS [training: 0.06016735950810474 | validation: 0.04423760671250972]
	TIME [epoch: 8.2 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05749621591789977		[learning rate: 0.00066077]
		[batch 20/20] avg loss: 0.05441805775711979		[learning rate: 0.00065999]
	Learning Rate: 0.000659992
	LOSS [training: 0.05595713683750979 | validation: 0.03330641372333492]
	TIME [epoch: 8.23 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0691092933339446		[learning rate: 0.00065921]
		[batch 20/20] avg loss: 0.0715575504885945		[learning rate: 0.00065843]
	Learning Rate: 0.000658435
	LOSS [training: 0.07033342191126955 | validation: 0.04861760209843112]
	TIME [epoch: 8.19 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07853960511897642		[learning rate: 0.00065766]
		[batch 20/20] avg loss: 0.058578998782509106		[learning rate: 0.00065688]
	Learning Rate: 0.000656882
	LOSS [training: 0.06855930195074275 | validation: 0.051867762148583456]
	TIME [epoch: 8.2 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08790954635633655		[learning rate: 0.00065611]
		[batch 20/20] avg loss: 0.06166549791312995		[learning rate: 0.00065533]
	Learning Rate: 0.000655332
	LOSS [training: 0.07478752213473325 | validation: 0.06664499418371758]
	TIME [epoch: 8.22 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0654202317973157		[learning rate: 0.00065456]
		[batch 20/20] avg loss: 0.07728788645220018		[learning rate: 0.00065379]
	Learning Rate: 0.000653786
	LOSS [training: 0.07135405912475794 | validation: 0.030203211280810306]
	TIME [epoch: 8.18 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08973534982807936		[learning rate: 0.00065301]
		[batch 20/20] avg loss: 0.05810198232660122		[learning rate: 0.00065224]
	Learning Rate: 0.000652244
	LOSS [training: 0.0739186660773403 | validation: 0.07910624351908653]
	TIME [epoch: 8.19 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10480132488077845		[learning rate: 0.00065147]
		[batch 20/20] avg loss: 0.07524232727654472		[learning rate: 0.00065071]
	Learning Rate: 0.000650706
	LOSS [training: 0.09002182607866158 | validation: 0.012063911547103675]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_1207.pth
	Model improved!!!
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05386510666524509		[learning rate: 0.00064994]
		[batch 20/20] avg loss: 0.049350812097588084		[learning rate: 0.00064917]
	Learning Rate: 0.000649171
	LOSS [training: 0.05160795938141658 | validation: 0.03837718826023054]
	TIME [epoch: 8.2 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0642790039672447		[learning rate: 0.0006484]
		[batch 20/20] avg loss: 0.06039328776556328		[learning rate: 0.00064764]
	Learning Rate: 0.000647639
	LOSS [training: 0.062336145866404 | validation: 0.07690184164178573]
	TIME [epoch: 8.18 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07807995684913818		[learning rate: 0.00064688]
		[batch 20/20] avg loss: 0.07461300218648623		[learning rate: 0.00064611]
	Learning Rate: 0.000646112
	LOSS [training: 0.07634647951781219 | validation: 0.017415933653962264]
	TIME [epoch: 8.21 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059225195901762374		[learning rate: 0.00064535]
		[batch 20/20] avg loss: 0.09578884908138399		[learning rate: 0.00064459]
	Learning Rate: 0.000644588
	LOSS [training: 0.07750702249157317 | validation: 0.053191050005012494]
	TIME [epoch: 8.21 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057778041041940745		[learning rate: 0.00064383]
		[batch 20/20] avg loss: 0.07576237795621704		[learning rate: 0.00064307]
	Learning Rate: 0.000643067
	LOSS [training: 0.0667702094990789 | validation: 0.023431026514080265]
	TIME [epoch: 8.21 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059348722580883015		[learning rate: 0.00064231]
		[batch 20/20] avg loss: 0.04646631513969489		[learning rate: 0.00064155]
	Learning Rate: 0.00064155
	LOSS [training: 0.05290751886028897 | validation: 0.036233064482182456]
	TIME [epoch: 8.18 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060474484486419386		[learning rate: 0.00064079]
		[batch 20/20] avg loss: 0.061175894802179095		[learning rate: 0.00064004]
	Learning Rate: 0.000640037
	LOSS [training: 0.06082518964429925 | validation: 0.03498623559890768]
	TIME [epoch: 8.22 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0713250296285288		[learning rate: 0.00063928]
		[batch 20/20] avg loss: 0.06517241911945268		[learning rate: 0.00063853]
	Learning Rate: 0.000638527
	LOSS [training: 0.06824872437399072 | validation: 0.0362416654894302]
	TIME [epoch: 8.21 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058095241394934584		[learning rate: 0.00063777]
		[batch 20/20] avg loss: 0.07766860353633606		[learning rate: 0.00063702]
	Learning Rate: 0.000637021
	LOSS [training: 0.06788192246563532 | validation: 0.05282714359772043]
	TIME [epoch: 8.21 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07960374852366296		[learning rate: 0.00063627]
		[batch 20/20] avg loss: 0.11236739821610145		[learning rate: 0.00063552]
	Learning Rate: 0.000635519
	LOSS [training: 0.0959855733698822 | validation: 0.05138296336681593]
	TIME [epoch: 8.19 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05784874493786264		[learning rate: 0.00063477]
		[batch 20/20] avg loss: 0.0659855826989104		[learning rate: 0.00063402]
	Learning Rate: 0.000634019
	LOSS [training: 0.06191716381838652 | validation: 0.03185414120760095]
	TIME [epoch: 8.22 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06153388261278277		[learning rate: 0.00063327]
		[batch 20/20] avg loss: 0.06148492737449022		[learning rate: 0.00063252]
	Learning Rate: 0.000632524
	LOSS [training: 0.06150940499363651 | validation: 0.02817427350836637]
	TIME [epoch: 8.2 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05847735724230781		[learning rate: 0.00063178]
		[batch 20/20] avg loss: 0.06231175125891952		[learning rate: 0.00063103]
	Learning Rate: 0.000631032
	LOSS [training: 0.060394554250613686 | validation: 0.0692141582372577]
	TIME [epoch: 8.2 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06188315326479674		[learning rate: 0.00063029]
		[batch 20/20] avg loss: 0.08268998484502559		[learning rate: 0.00062954]
	Learning Rate: 0.000629543
	LOSS [training: 0.07228656905491118 | validation: 0.051592293201748005]
	TIME [epoch: 8.19 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07486839197129151		[learning rate: 0.0006288]
		[batch 20/20] avg loss: 0.0675637054592817		[learning rate: 0.00062806]
	Learning Rate: 0.000628058
	LOSS [training: 0.0712160487152866 | validation: 0.03537410657515026]
	TIME [epoch: 8.22 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05819654659311822		[learning rate: 0.00062732]
		[batch 20/20] avg loss: 0.06545606620296177		[learning rate: 0.00062658]
	Learning Rate: 0.000626577
	LOSS [training: 0.06182630639803999 | validation: 0.06896869623462264]
	TIME [epoch: 8.2 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07425843993181595		[learning rate: 0.00062584]
		[batch 20/20] avg loss: 0.0749367685992119		[learning rate: 0.0006251]
	Learning Rate: 0.000625099
	LOSS [training: 0.0745976042655139 | validation: 0.10124684616008983]
	TIME [epoch: 8.21 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08344775049022521		[learning rate: 0.00062436]
		[batch 20/20] avg loss: 0.05401976699256063		[learning rate: 0.00062362]
	Learning Rate: 0.000623624
	LOSS [training: 0.0687337587413929 | validation: 0.06531451520157373]
	TIME [epoch: 8.19 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05507048930081722		[learning rate: 0.00062289]
		[batch 20/20] avg loss: 0.08123919681030262		[learning rate: 0.00062215]
	Learning Rate: 0.000622153
	LOSS [training: 0.06815484305555991 | validation: 0.029822482557624597]
	TIME [epoch: 8.23 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0552021928034396		[learning rate: 0.00062142]
		[batch 20/20] avg loss: 0.06409571512330323		[learning rate: 0.00062069]
	Learning Rate: 0.000620686
	LOSS [training: 0.0596489539633714 | validation: 0.014393128338415405]
	TIME [epoch: 8.19 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058124083209713365		[learning rate: 0.00061995]
		[batch 20/20] avg loss: 0.06550835489604433		[learning rate: 0.00061922]
	Learning Rate: 0.000619222
	LOSS [training: 0.06181621905287886 | validation: 0.0618189851032129]
	TIME [epoch: 8.21 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07661945111185833		[learning rate: 0.00061849]
		[batch 20/20] avg loss: 0.049262820815024734		[learning rate: 0.00061776]
	Learning Rate: 0.000617761
	LOSS [training: 0.06294113596344152 | validation: 0.0331815927837866]
	TIME [epoch: 8.18 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056120283232101345		[learning rate: 0.00061703]
		[batch 20/20] avg loss: 0.059816997672178		[learning rate: 0.0006163]
	Learning Rate: 0.000616304
	LOSS [training: 0.05796864045213966 | validation: 0.0392002635082382]
	TIME [epoch: 8.23 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04826016723902361		[learning rate: 0.00061558]
		[batch 20/20] avg loss: 0.0850998530925419		[learning rate: 0.00061485]
	Learning Rate: 0.00061485
	LOSS [training: 0.06668001016578276 | validation: 0.021445017064350372]
	TIME [epoch: 8.19 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06567481281162227		[learning rate: 0.00061412]
		[batch 20/20] avg loss: 0.05977924919291981		[learning rate: 0.0006134]
	Learning Rate: 0.0006134
	LOSS [training: 0.06272703100227103 | validation: 0.030555034577616477]
	TIME [epoch: 8.22 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05326270625240764		[learning rate: 0.00061268]
		[batch 20/20] avg loss: 0.060645767951523766		[learning rate: 0.00061195]
	Learning Rate: 0.000611953
	LOSS [training: 0.0569542371019657 | validation: 0.03441113113735977]
	TIME [epoch: 8.21 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05985584031933876		[learning rate: 0.00061123]
		[batch 20/20] avg loss: 0.06817597321128924		[learning rate: 0.00061051]
	Learning Rate: 0.000610509
	LOSS [training: 0.06401590676531399 | validation: 0.036947210167624936]
	TIME [epoch: 8.22 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055006155385113595		[learning rate: 0.00060979]
		[batch 20/20] avg loss: 0.07171954908149283		[learning rate: 0.00060907]
	Learning Rate: 0.000609069
	LOSS [training: 0.06336285223330322 | validation: 0.02491794600887104]
	TIME [epoch: 8.19 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0654744700107533		[learning rate: 0.00060835]
		[batch 20/20] avg loss: 0.060612945842983115		[learning rate: 0.00060763]
	Learning Rate: 0.000607632
	LOSS [training: 0.06304370792686821 | validation: 0.04506650505245543]
	TIME [epoch: 8.22 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05238546651633573		[learning rate: 0.00060692]
		[batch 20/20] avg loss: 0.09116995149302967		[learning rate: 0.0006062]
	Learning Rate: 0.000606199
	LOSS [training: 0.0717777090046827 | validation: 0.06617940212043731]
	TIME [epoch: 8.21 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.082662248933913		[learning rate: 0.00060548]
		[batch 20/20] avg loss: 0.06742557821593306		[learning rate: 0.00060477]
	Learning Rate: 0.000604769
	LOSS [training: 0.07504391357492302 | validation: 0.015000112327940742]
	TIME [epoch: 8.21 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06517391462579102		[learning rate: 0.00060406]
		[batch 20/20] avg loss: 0.06034404540658379		[learning rate: 0.00060334]
	Learning Rate: 0.000603343
	LOSS [training: 0.06275898001618742 | validation: 0.01622007759962634]
	TIME [epoch: 8.2 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061430231871825315		[learning rate: 0.00060263]
		[batch 20/20] avg loss: 0.05938119925452552		[learning rate: 0.00060192]
	Learning Rate: 0.00060192
	LOSS [training: 0.06040571556317542 | validation: 0.022811126387816856]
	TIME [epoch: 8.22 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058942747511283755		[learning rate: 0.00060121]
		[batch 20/20] avg loss: 0.065146942637388		[learning rate: 0.0006005]
	Learning Rate: 0.0006005
	LOSS [training: 0.06204484507433587 | validation: 0.0296027348559324]
	TIME [epoch: 8.21 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06724422913007383		[learning rate: 0.00059979]
		[batch 20/20] avg loss: 0.044946334424564624		[learning rate: 0.00059908]
	Learning Rate: 0.000599083
	LOSS [training: 0.05609528177731922 | validation: 0.023606053138530453]
	TIME [epoch: 8.17 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05577303331475253		[learning rate: 0.00059838]
		[batch 20/20] avg loss: 0.05348845986102726		[learning rate: 0.00059767]
	Learning Rate: 0.00059767
	LOSS [training: 0.05463074658788989 | validation: 0.0294312475161226]
	TIME [epoch: 8.21 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051669736882068294		[learning rate: 0.00059696]
		[batch 20/20] avg loss: 0.07960943297558812		[learning rate: 0.00059626]
	Learning Rate: 0.00059626
	LOSS [training: 0.06563958492882822 | validation: 0.06017833253774299]
	TIME [epoch: 8.22 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08036857689084563		[learning rate: 0.00059556]
		[batch 20/20] avg loss: 0.06445794727577966		[learning rate: 0.00059485]
	Learning Rate: 0.000594854
	LOSS [training: 0.07241326208331264 | validation: 0.03569077795887697]
	TIME [epoch: 8.19 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10461117539421896		[learning rate: 0.00059415]
		[batch 20/20] avg loss: 0.06164173040453334		[learning rate: 0.00059345]
	Learning Rate: 0.000593451
	LOSS [training: 0.08312645289937615 | validation: 0.02144623023547617]
	TIME [epoch: 8.19 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1037271865924787		[learning rate: 0.00059275]
		[batch 20/20] avg loss: 0.06375248187803408		[learning rate: 0.00059205]
	Learning Rate: 0.000592051
	LOSS [training: 0.08373983423525638 | validation: 0.053377303449312165]
	TIME [epoch: 8.24 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061600854584992704		[learning rate: 0.00059135]
		[batch 20/20] avg loss: 0.06837951429204626		[learning rate: 0.00059065]
	Learning Rate: 0.000590654
	LOSS [training: 0.06499018443851948 | validation: 0.06817423505000908]
	TIME [epoch: 8.22 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05957244540246198		[learning rate: 0.00058996]
		[batch 20/20] avg loss: 0.06791478534128255		[learning rate: 0.00058926]
	Learning Rate: 0.000589261
	LOSS [training: 0.06374361537187226 | validation: 0.03371084017425955]
	TIME [epoch: 8.2 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08162516110480084		[learning rate: 0.00058857]
		[batch 20/20] avg loss: 0.07035656187019405		[learning rate: 0.00058787]
	Learning Rate: 0.000587871
	LOSS [training: 0.07599086148749745 | validation: 0.017381555231749363]
	TIME [epoch: 8.2 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06841420635861493		[learning rate: 0.00058718]
		[batch 20/20] avg loss: 0.07579413517587467		[learning rate: 0.00058648]
	Learning Rate: 0.000586484
	LOSS [training: 0.07210417076724483 | validation: 0.07275464623980704]
	TIME [epoch: 8.23 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06475288591693493		[learning rate: 0.00058579]
		[batch 20/20] avg loss: 0.0704078421854228		[learning rate: 0.0005851]
	Learning Rate: 0.000585101
	LOSS [training: 0.06758036405117884 | validation: 0.024149358792407566]
	TIME [epoch: 8.21 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05437702556098813		[learning rate: 0.00058441]
		[batch 20/20] avg loss: 0.05855890592002895		[learning rate: 0.00058372]
	Learning Rate: 0.000583721
	LOSS [training: 0.05646796574050854 | validation: 0.0440395879195441]
	TIME [epoch: 8.19 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06227381625474191		[learning rate: 0.00058303]
		[batch 20/20] avg loss: 0.07208614680504674		[learning rate: 0.00058234]
	Learning Rate: 0.000582344
	LOSS [training: 0.06717998152989432 | validation: 0.016653186500711917]
	TIME [epoch: 8.21 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07156915456369495		[learning rate: 0.00058166]
		[batch 20/20] avg loss: 0.07069316656135712		[learning rate: 0.00058097]
	Learning Rate: 0.00058097
	LOSS [training: 0.07113116056252602 | validation: 0.022122857874639128]
	TIME [epoch: 8.21 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04164805565098689		[learning rate: 0.00058028]
		[batch 20/20] avg loss: 0.04967431671178769		[learning rate: 0.0005796]
	Learning Rate: 0.0005796
	LOSS [training: 0.0456611861813873 | validation: 0.026743767343695372]
	TIME [epoch: 8.21 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07631581459285378		[learning rate: 0.00057892]
		[batch 20/20] avg loss: 0.06363759490790402		[learning rate: 0.00057823]
	Learning Rate: 0.000578233
	LOSS [training: 0.06997670475037888 | validation: 0.05397291814702175]
	TIME [epoch: 8.19 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06557681233919375		[learning rate: 0.00057755]
		[batch 20/20] avg loss: 0.06592316394151515		[learning rate: 0.00057687]
	Learning Rate: 0.000576869
	LOSS [training: 0.06574998814035446 | validation: 0.034787186175284125]
	TIME [epoch: 8.21 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07514507946452069		[learning rate: 0.00057619]
		[batch 20/20] avg loss: 0.06852431311462279		[learning rate: 0.00057551]
	Learning Rate: 0.000575508
	LOSS [training: 0.07183469628957173 | validation: 0.035033583665609436]
	TIME [epoch: 8.2 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05961221473384491		[learning rate: 0.00057483]
		[batch 20/20] avg loss: 0.04699312957200625		[learning rate: 0.00057415]
	Learning Rate: 0.00057415
	LOSS [training: 0.05330267215292559 | validation: 0.021860066063741686]
	TIME [epoch: 8.21 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05106265403580103		[learning rate: 0.00057347]
		[batch 20/20] avg loss: 0.07488255129633714		[learning rate: 0.0005728]
	Learning Rate: 0.000572796
	LOSS [training: 0.06297260266606909 | validation: 0.02451308828275245]
	TIME [epoch: 8.19 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06125719895882953		[learning rate: 0.00057212]
		[batch 20/20] avg loss: 0.056440615142187865		[learning rate: 0.00057144]
	Learning Rate: 0.000571445
	LOSS [training: 0.05884890705050869 | validation: 0.05818111207480688]
	TIME [epoch: 8.21 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059816118685311906		[learning rate: 0.00057077]
		[batch 20/20] avg loss: 0.07066059701844428		[learning rate: 0.0005701]
	Learning Rate: 0.000570097
	LOSS [training: 0.06523835785187809 | validation: 0.03717703809519494]
	TIME [epoch: 8.21 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05909704974559182		[learning rate: 0.00056942]
		[batch 20/20] avg loss: 0.04822300046925791		[learning rate: 0.00056875]
	Learning Rate: 0.000568752
	LOSS [training: 0.05366002510742487 | validation: 0.024014530832633663]
	TIME [epoch: 8.21 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07037221777209643		[learning rate: 0.00056808]
		[batch 20/20] avg loss: 0.058436820247540114		[learning rate: 0.00056741]
	Learning Rate: 0.000567411
	LOSS [training: 0.06440451900981828 | validation: 0.050300556368991764]
	TIME [epoch: 8.18 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08927568863966925		[learning rate: 0.00056674]
		[batch 20/20] avg loss: 0.0802354897196365		[learning rate: 0.00056607]
	Learning Rate: 0.000566072
	LOSS [training: 0.08475558917965288 | validation: 0.05822254741779845]
	TIME [epoch: 8.21 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057785862803871345		[learning rate: 0.0005654]
		[batch 20/20] avg loss: 0.06347614057659112		[learning rate: 0.00056474]
	Learning Rate: 0.000564737
	LOSS [training: 0.060631001690231244 | validation: 0.05230008372851592]
	TIME [epoch: 8.21 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09988959768673476		[learning rate: 0.00056407]
		[batch 20/20] avg loss: 0.07337339881202598		[learning rate: 0.0005634]
	Learning Rate: 0.000563405
	LOSS [training: 0.08663149824938038 | validation: 0.03493258622190227]
	TIME [epoch: 8.21 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06682457930467409		[learning rate: 0.00056274]
		[batch 20/20] avg loss: 0.06373256641001165		[learning rate: 0.00056208]
	Learning Rate: 0.000562076
	LOSS [training: 0.06527857285734287 | validation: 0.04133418583344308]
	TIME [epoch: 8.19 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047384993388070965		[learning rate: 0.00056141]
		[batch 20/20] avg loss: 0.07796197423495892		[learning rate: 0.00056075]
	Learning Rate: 0.00056075
	LOSS [training: 0.06267348381151493 | validation: 0.03178097419257369]
	TIME [epoch: 8.22 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048209400845035284		[learning rate: 0.00056009]
		[batch 20/20] avg loss: 0.08979998760028596		[learning rate: 0.00055943]
	Learning Rate: 0.000559427
	LOSS [training: 0.06900469422266062 | validation: 0.02106621047468937]
	TIME [epoch: 8.2 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06223713892945073		[learning rate: 0.00055877]
		[batch 20/20] avg loss: 0.07299740099201871		[learning rate: 0.00055811]
	Learning Rate: 0.000558108
	LOSS [training: 0.06761726996073472 | validation: 0.01877640774005103]
	TIME [epoch: 8.21 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06615469434067119		[learning rate: 0.00055745]
		[batch 20/20] avg loss: 0.07052756471409485		[learning rate: 0.00055679]
	Learning Rate: 0.000556791
	LOSS [training: 0.06834112952738301 | validation: 0.02765554208047656]
	TIME [epoch: 8.18 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053908247322507076		[learning rate: 0.00055613]
		[batch 20/20] avg loss: 0.04935633746414211		[learning rate: 0.00055548]
	Learning Rate: 0.000555478
	LOSS [training: 0.051632292393324584 | validation: 0.031702775740768574]
	TIME [epoch: 8.21 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0633824179698435		[learning rate: 0.00055482]
		[batch 20/20] avg loss: 0.06714200953220725		[learning rate: 0.00055417]
	Learning Rate: 0.000554167
	LOSS [training: 0.06526221375102537 | validation: 0.034458738120766635]
	TIME [epoch: 8.2 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055985663935341004		[learning rate: 0.00055351]
		[batch 20/20] avg loss: 0.06874974402268162		[learning rate: 0.00055286]
	Learning Rate: 0.00055286
	LOSS [training: 0.06236770397901131 | validation: 0.07576752920397328]
	TIME [epoch: 8.2 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06888016455913173		[learning rate: 0.00055221]
		[batch 20/20] avg loss: 0.054796422356039355		[learning rate: 0.00055156]
	Learning Rate: 0.000551556
	LOSS [training: 0.061838293457585534 | validation: 0.021271676949969356]
	TIME [epoch: 8.18 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06013411078465207		[learning rate: 0.00055091]
		[batch 20/20] avg loss: 0.05793692810384917		[learning rate: 0.00055026]
	Learning Rate: 0.000550255
	LOSS [training: 0.05903551944425064 | validation: 0.02914909853381821]
	TIME [epoch: 8.22 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04856738535711564		[learning rate: 0.00054961]
		[batch 20/20] avg loss: 0.060798350443408054		[learning rate: 0.00054896]
	Learning Rate: 0.000548957
	LOSS [training: 0.05468286790026185 | validation: 0.011501101662670526]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_1279.pth
	Model improved!!!
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043276351628785334		[learning rate: 0.00054831]
		[batch 20/20] avg loss: 0.05025026154499209		[learning rate: 0.00054766]
	Learning Rate: 0.000547662
	LOSS [training: 0.046763306586888716 | validation: 0.05528137554386808]
	TIME [epoch: 8.22 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07231932393425337		[learning rate: 0.00054702]
		[batch 20/20] avg loss: 0.05631097329917838		[learning rate: 0.00054637]
	Learning Rate: 0.00054637
	LOSS [training: 0.06431514861671586 | validation: 0.036694079808057]
	TIME [epoch: 8.19 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05463311852152449		[learning rate: 0.00054573]
		[batch 20/20] avg loss: 0.05134200331169626		[learning rate: 0.00054508]
	Learning Rate: 0.000545082
	LOSS [training: 0.0529875609166104 | validation: 0.04133925416903744]
	TIME [epoch: 8.23 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060548359422269514		[learning rate: 0.00054444]
		[batch 20/20] avg loss: 0.0652187196663282		[learning rate: 0.0005438]
	Learning Rate: 0.000543796
	LOSS [training: 0.06288353954429884 | validation: 0.026004087532307117]
	TIME [epoch: 8.19 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056854297149425505		[learning rate: 0.00054315]
		[batch 20/20] avg loss: 0.05668266678031488		[learning rate: 0.00054251]
	Learning Rate: 0.000542513
	LOSS [training: 0.056768481964870186 | validation: 0.0473374796350287]
	TIME [epoch: 8.21 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06792337042420453		[learning rate: 0.00054187]
		[batch 20/20] avg loss: 0.06012558143040871		[learning rate: 0.00054123]
	Learning Rate: 0.000541233
	LOSS [training: 0.06402447592730662 | validation: 0.047472919774099336]
	TIME [epoch: 8.19 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07254841535863137		[learning rate: 0.00054059]
		[batch 20/20] avg loss: 0.053446010622313155		[learning rate: 0.00053996]
	Learning Rate: 0.000539957
	LOSS [training: 0.06299721299047226 | validation: 0.041315027269712055]
	TIME [epoch: 8.23 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06968724881696226		[learning rate: 0.00053932]
		[batch 20/20] avg loss: 0.055150900780164755		[learning rate: 0.00053868]
	Learning Rate: 0.000538683
	LOSS [training: 0.06241907479856351 | validation: 0.04031894998207561]
	TIME [epoch: 8.19 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06028424237452947		[learning rate: 0.00053805]
		[batch 20/20] avg loss: 0.059375955559164806		[learning rate: 0.00053741]
	Learning Rate: 0.000537412
	LOSS [training: 0.05983009896684714 | validation: 0.06922000618622046]
	TIME [epoch: 8.21 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06700336153748951		[learning rate: 0.00053678]
		[batch 20/20] avg loss: 0.046825560104870614		[learning rate: 0.00053614]
	Learning Rate: 0.000536145
	LOSS [training: 0.056914460821180055 | validation: 0.02802004020666774]
	TIME [epoch: 8.19 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0547544908259605		[learning rate: 0.00053551]
		[batch 20/20] avg loss: 0.06457417205621147		[learning rate: 0.00053488]
	Learning Rate: 0.00053488
	LOSS [training: 0.059664331441085974 | validation: 0.03634128439931078]
	TIME [epoch: 8.24 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0713606714281833		[learning rate: 0.00053425]
		[batch 20/20] avg loss: 0.06835365874159521		[learning rate: 0.00053362]
	Learning Rate: 0.000533618
	LOSS [training: 0.06985716508488925 | validation: 0.03724555157243359]
	TIME [epoch: 8.18 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07355772110507562		[learning rate: 0.00053299]
		[batch 20/20] avg loss: 0.051566586556569746		[learning rate: 0.00053236]
	Learning Rate: 0.00053236
	LOSS [training: 0.06256215383082267 | validation: 0.03165838953485645]
	TIME [epoch: 8.21 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06214210861912706		[learning rate: 0.00053173]
		[batch 20/20] avg loss: 0.06600239275857214		[learning rate: 0.0005311]
	Learning Rate: 0.000531104
	LOSS [training: 0.06407225068884961 | validation: 0.03046945297257279]
	TIME [epoch: 8.18 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05744800175441853		[learning rate: 0.00053048]
		[batch 20/20] avg loss: 0.04729096514349635		[learning rate: 0.00052985]
	Learning Rate: 0.000529851
	LOSS [training: 0.052369483448957435 | validation: 0.04343501350839604]
	TIME [epoch: 8.22 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07173038262307296		[learning rate: 0.00052923]
		[batch 20/20] avg loss: 0.055340713881020555		[learning rate: 0.0005286]
	Learning Rate: 0.000528601
	LOSS [training: 0.06353554825204674 | validation: 0.021629647577751154]
	TIME [epoch: 8.19 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06905306210283232		[learning rate: 0.00052798]
		[batch 20/20] avg loss: 0.05300153008741881		[learning rate: 0.00052735]
	Learning Rate: 0.000527354
	LOSS [training: 0.06102729609512557 | validation: 0.015604842443551658]
	TIME [epoch: 8.2 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06497159730211285		[learning rate: 0.00052673]
		[batch 20/20] avg loss: 0.042593312658646645		[learning rate: 0.00052611]
	Learning Rate: 0.00052611
	LOSS [training: 0.05378245498037974 | validation: 0.02723290076706745]
	TIME [epoch: 8.21 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0639430977957524		[learning rate: 0.00052549]
		[batch 20/20] avg loss: 0.06373211021320714		[learning rate: 0.00052487]
	Learning Rate: 0.000524869
	LOSS [training: 0.06383760400447977 | validation: 0.04063377324269087]
	TIME [epoch: 8.21 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07011303643008351		[learning rate: 0.00052425]
		[batch 20/20] avg loss: 0.048374029913480984		[learning rate: 0.00052363]
	Learning Rate: 0.000523631
	LOSS [training: 0.05924353317178225 | validation: 0.01895128976765936]
	TIME [epoch: 8.18 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0469624008795064		[learning rate: 0.00052301]
		[batch 20/20] avg loss: 0.06486641720735757		[learning rate: 0.0005224]
	Learning Rate: 0.000522396
	LOSS [training: 0.055914409043432 | validation: 0.019354074879059345]
	TIME [epoch: 8.2 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0575982673143855		[learning rate: 0.00052178]
		[batch 20/20] avg loss: 0.056713814455879065		[learning rate: 0.00052116]
	Learning Rate: 0.000521164
	LOSS [training: 0.05715604088513228 | validation: 0.03550233776185052]
	TIME [epoch: 8.21 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05240723072160121		[learning rate: 0.00052055]
		[batch 20/20] avg loss: 0.06577337417452626		[learning rate: 0.00051993]
	Learning Rate: 0.000519935
	LOSS [training: 0.059090302448063736 | validation: 0.01033849547782795]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_1302.pth
	Model improved!!!
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0627846893388931		[learning rate: 0.00051932]
		[batch 20/20] avg loss: 0.0653420996803603		[learning rate: 0.00051871]
	Learning Rate: 0.000518708
	LOSS [training: 0.0640633945096267 | validation: 0.02102394686602604]
	TIME [epoch: 8.22 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04667884994046838		[learning rate: 0.0005181]
		[batch 20/20] avg loss: 0.060377150868594465		[learning rate: 0.00051748]
	Learning Rate: 0.000517485
	LOSS [training: 0.05352800040453143 | validation: 0.05461505123837912]
	TIME [epoch: 8.19 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07312592110179647		[learning rate: 0.00051687]
		[batch 20/20] avg loss: 0.057088383163008904		[learning rate: 0.00051626]
	Learning Rate: 0.000516264
	LOSS [training: 0.0651071521324027 | validation: 0.019205935231083774]
	TIME [epoch: 8.19 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08003468838842566		[learning rate: 0.00051565]
		[batch 20/20] avg loss: 0.06247249231895489		[learning rate: 0.00051505]
	Learning Rate: 0.000515046
	LOSS [training: 0.07125359035369028 | validation: 0.03842969776624451]
	TIME [epoch: 8.19 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04547173433739846		[learning rate: 0.00051444]
		[batch 20/20] avg loss: 0.0801338113177218		[learning rate: 0.00051383]
	Learning Rate: 0.000513831
	LOSS [training: 0.06280277282756011 | validation: 0.029426796758508365]
	TIME [epoch: 8.18 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06157992121671639		[learning rate: 0.00051322]
		[batch 20/20] avg loss: 0.07132038043433596		[learning rate: 0.00051262]
	Learning Rate: 0.000512619
	LOSS [training: 0.06645015082552617 | validation: 0.017853963648580808]
	TIME [epoch: 8.22 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0685321185817544		[learning rate: 0.00051201]
		[batch 20/20] avg loss: 0.06811653958226545		[learning rate: 0.00051141]
	Learning Rate: 0.00051141
	LOSS [training: 0.06832432908200992 | validation: 0.036860360909269516]
	TIME [epoch: 8.18 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06673982491552251		[learning rate: 0.00051081]
		[batch 20/20] avg loss: 0.06563687900620882		[learning rate: 0.0005102]
	Learning Rate: 0.000510204
	LOSS [training: 0.06618835196086566 | validation: 0.03056330414285108]
	TIME [epoch: 8.18 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054411856219396385		[learning rate: 0.0005096]
		[batch 20/20] avg loss: 0.06589740019204378		[learning rate: 0.000509]
	Learning Rate: 0.000509
	LOSS [training: 0.060154628205720075 | validation: 0.09186026688867303]
	TIME [epoch: 8.2 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07075615660204937		[learning rate: 0.0005084]
		[batch 20/20] avg loss: 0.046967208039709385		[learning rate: 0.0005078]
	Learning Rate: 0.0005078
	LOSS [training: 0.058861682320879384 | validation: 0.022108770807163385]
	TIME [epoch: 8.23 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05108067514393455		[learning rate: 0.0005072]
		[batch 20/20] avg loss: 0.05699575890466876		[learning rate: 0.0005066]
	Learning Rate: 0.000506602
	LOSS [training: 0.05403821702430167 | validation: 0.043431139128088186]
	TIME [epoch: 8.19 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07175629088981779		[learning rate: 0.000506]
		[batch 20/20] avg loss: 0.056174473102328384		[learning rate: 0.00050541]
	Learning Rate: 0.000505407
	LOSS [training: 0.06396538199607309 | validation: 0.024381382341538214]
	TIME [epoch: 8.17 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06604700506324333		[learning rate: 0.00050481]
		[batch 20/20] avg loss: 0.08447982629388143		[learning rate: 0.00050421]
	Learning Rate: 0.000504215
	LOSS [training: 0.07526341567856239 | validation: 0.031004403802865337]
	TIME [epoch: 8.19 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07586499568353462		[learning rate: 0.00050362]
		[batch 20/20] avg loss: 0.06337176963714326		[learning rate: 0.00050303]
	Learning Rate: 0.000503025
	LOSS [training: 0.06961838266033894 | validation: 0.03970965166977117]
	TIME [epoch: 8.23 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07178663088609022		[learning rate: 0.00050243]
		[batch 20/20] avg loss: 0.05186417512770699		[learning rate: 0.00050184]
	Learning Rate: 0.000501839
	LOSS [training: 0.061825403006898615 | validation: 0.035175672435474314]
	TIME [epoch: 8.19 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06269793646521153		[learning rate: 0.00050125]
		[batch 20/20] avg loss: 0.05912739365292693		[learning rate: 0.00050065]
	Learning Rate: 0.000500655
	LOSS [training: 0.06091266505906925 | validation: 0.03344218144147595]
	TIME [epoch: 8.18 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0775027236176501		[learning rate: 0.00050006]
		[batch 20/20] avg loss: 0.049828256567125		[learning rate: 0.00049947]
	Learning Rate: 0.000499474
	LOSS [training: 0.06366549009238755 | validation: 0.027004965548728152]
	TIME [epoch: 8.19 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053938773711736644		[learning rate: 0.00049888]
		[batch 20/20] avg loss: 0.07377228225707828		[learning rate: 0.0004983]
	Learning Rate: 0.000498296
	LOSS [training: 0.06385552798440745 | validation: 0.04677809879036365]
	TIME [epoch: 8.21 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06383666833450907		[learning rate: 0.00049771]
		[batch 20/20] avg loss: 0.05414998292314623		[learning rate: 0.00049712]
	Learning Rate: 0.00049712
	LOSS [training: 0.058993325628827656 | validation: 0.05318437473398292]
	TIME [epoch: 8.19 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06004468267024983		[learning rate: 0.00049653]
		[batch 20/20] avg loss: 0.05608296135420103		[learning rate: 0.00049595]
	Learning Rate: 0.000495948
	LOSS [training: 0.05806382201222543 | validation: 0.026915844095328847]
	TIME [epoch: 8.17 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05570629641014966		[learning rate: 0.00049536]
		[batch 20/20] avg loss: 0.047614901839020396		[learning rate: 0.00049478]
	Learning Rate: 0.000494778
	LOSS [training: 0.05166059912458504 | validation: 0.03356805992414552]
	TIME [epoch: 8.19 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05850335782062771		[learning rate: 0.00049419]
		[batch 20/20] avg loss: 0.0527084970111793		[learning rate: 0.00049361]
	Learning Rate: 0.000493611
	LOSS [training: 0.0556059274159035 | validation: 0.06214985370534103]
	TIME [epoch: 8.21 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060849070195166735		[learning rate: 0.00049303]
		[batch 20/20] avg loss: 0.06206741945884292		[learning rate: 0.00049245]
	Learning Rate: 0.000492446
	LOSS [training: 0.06145824482700483 | validation: 0.0337360455596021]
	TIME [epoch: 8.18 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06860522536870672		[learning rate: 0.00049187]
		[batch 20/20] avg loss: 0.0654472724725827		[learning rate: 0.00049128]
	Learning Rate: 0.000491285
	LOSS [training: 0.06702624892064471 | validation: 0.034702294303586824]
	TIME [epoch: 8.17 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06307061199527167		[learning rate: 0.0004907]
		[batch 20/20] avg loss: 0.05929429990204792		[learning rate: 0.00049013]
	Learning Rate: 0.000490126
	LOSS [training: 0.061182455948659786 | validation: 0.06894043655200133]
	TIME [epoch: 8.19 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060095548946734224		[learning rate: 0.00048955]
		[batch 20/20] avg loss: 0.04909116641872364		[learning rate: 0.00048897]
	Learning Rate: 0.00048897
	LOSS [training: 0.05459335768272892 | validation: 0.021725464627412004]
	TIME [epoch: 8.21 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05777776770356258		[learning rate: 0.00048839]
		[batch 20/20] avg loss: 0.051191072437270765		[learning rate: 0.00048782]
	Learning Rate: 0.000487816
	LOSS [training: 0.05448442007041667 | validation: 0.031967255430517684]
	TIME [epoch: 8.18 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0647736802108116		[learning rate: 0.00048724]
		[batch 20/20] avg loss: 0.05318952439337924		[learning rate: 0.00048667]
	Learning Rate: 0.000486666
	LOSS [training: 0.05898160230209541 | validation: 0.04975520839555051]
	TIME [epoch: 8.17 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04695743475384419		[learning rate: 0.00048609]
		[batch 20/20] avg loss: 0.0739383728372209		[learning rate: 0.00048552]
	Learning Rate: 0.000485518
	LOSS [training: 0.060447903795532554 | validation: 0.06656186051143068]
	TIME [epoch: 8.2 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05940255854049983		[learning rate: 0.00048494]
		[batch 20/20] avg loss: 0.07843855321961693		[learning rate: 0.00048437]
	Learning Rate: 0.000484372
	LOSS [training: 0.06892055588005838 | validation: 0.057096519509307816]
	TIME [epoch: 8.19 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059707147618022906		[learning rate: 0.0004838]
		[batch 20/20] avg loss: 0.053867394955257594		[learning rate: 0.00048323]
	Learning Rate: 0.00048323
	LOSS [training: 0.05678727128664024 | validation: 0.058646570243366045]
	TIME [epoch: 8.2 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06418003657833767		[learning rate: 0.00048266]
		[batch 20/20] avg loss: 0.08165471374524891		[learning rate: 0.00048209]
	Learning Rate: 0.00048209
	LOSS [training: 0.07291737516179328 | validation: 0.04496474953530004]
	TIME [epoch: 8.19 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07100322525271367		[learning rate: 0.00048152]
		[batch 20/20] avg loss: 0.056816157148473365		[learning rate: 0.00048095]
	Learning Rate: 0.000480953
	LOSS [training: 0.06390969120059352 | validation: 0.02894567558130625]
	TIME [epoch: 8.22 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05540774371020962		[learning rate: 0.00048039]
		[batch 20/20] avg loss: 0.05517186725680725		[learning rate: 0.00047982]
	Learning Rate: 0.000479818
	LOSS [training: 0.055289805483508425 | validation: 0.028081341482579982]
	TIME [epoch: 8.19 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047179395122636886		[learning rate: 0.00047925]
		[batch 20/20] avg loss: 0.043910418572077385		[learning rate: 0.00047869]
	Learning Rate: 0.000478687
	LOSS [training: 0.04554490684735714 | validation: 0.020180122439966225]
	TIME [epoch: 8.2 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045194262525915255		[learning rate: 0.00047812]
		[batch 20/20] avg loss: 0.05863591420215395		[learning rate: 0.00047756]
	Learning Rate: 0.000477557
	LOSS [training: 0.05191508836403461 | validation: 0.07460860579388215]
	TIME [epoch: 8.18 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06032176253502819		[learning rate: 0.00047699]
		[batch 20/20] avg loss: 0.05412283544738252		[learning rate: 0.00047643]
	Learning Rate: 0.000476431
	LOSS [training: 0.05722229899120536 | validation: 0.012980060057045413]
	TIME [epoch: 8.22 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048878711388134835		[learning rate: 0.00047587]
		[batch 20/20] avg loss: 0.042844950112629496		[learning rate: 0.00047531]
	Learning Rate: 0.000475307
	LOSS [training: 0.045861830750382165 | validation: 0.03363133561003955]
	TIME [epoch: 8.18 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05067696087388514		[learning rate: 0.00047475]
		[batch 20/20] avg loss: 0.06094192742338679		[learning rate: 0.00047419]
	Learning Rate: 0.000474186
	LOSS [training: 0.055809444148635964 | validation: 0.04484657860010243]
	TIME [epoch: 8.2 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05500863040680053		[learning rate: 0.00047363]
		[batch 20/20] avg loss: 0.05612459519513511		[learning rate: 0.00047307]
	Learning Rate: 0.000473067
	LOSS [training: 0.05556661280096781 | validation: 0.03124392723513354]
	TIME [epoch: 8.17 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07830603184226445		[learning rate: 0.00047251]
		[batch 20/20] avg loss: 0.06880108884757749		[learning rate: 0.00047195]
	Learning Rate: 0.000471951
	LOSS [training: 0.07355356034492097 | validation: 0.034913173443073924]
	TIME [epoch: 8.22 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06410122667240045		[learning rate: 0.00047139]
		[batch 20/20] avg loss: 0.048279907839038155		[learning rate: 0.00047084]
	Learning Rate: 0.000470838
	LOSS [training: 0.05619056725571932 | validation: 0.02704549742582206]
	TIME [epoch: 8.18 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04783045217285574		[learning rate: 0.00047028]
		[batch 20/20] avg loss: 0.05702567759799877		[learning rate: 0.00046973]
	Learning Rate: 0.000469728
	LOSS [training: 0.05242806488542725 | validation: 0.025782926287493256]
	TIME [epoch: 8.2 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04631515008070748		[learning rate: 0.00046917]
		[batch 20/20] avg loss: 0.06311278933042168		[learning rate: 0.00046862]
	Learning Rate: 0.00046862
	LOSS [training: 0.054713969705564566 | validation: 0.01541126773572464]
	TIME [epoch: 8.18 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04325187392013414		[learning rate: 0.00046807]
		[batch 20/20] avg loss: 0.0549201836445042		[learning rate: 0.00046751]
	Learning Rate: 0.000467514
	LOSS [training: 0.04908602878231917 | validation: 0.050790848954499535]
	TIME [epoch: 8.21 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04767241334937662		[learning rate: 0.00046696]
		[batch 20/20] avg loss: 0.06904536673072097		[learning rate: 0.00046641]
	Learning Rate: 0.000466411
	LOSS [training: 0.058358890040048794 | validation: 0.018675233183542334]
	TIME [epoch: 8.17 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0574624821912376		[learning rate: 0.00046586]
		[batch 20/20] avg loss: 0.056556242600696395		[learning rate: 0.00046531]
	Learning Rate: 0.000465311
	LOSS [training: 0.05700936239596699 | validation: 0.015855369898296428]
	TIME [epoch: 8.2 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06157717456689728		[learning rate: 0.00046476]
		[batch 20/20] avg loss: 0.058338910613918125		[learning rate: 0.00046421]
	Learning Rate: 0.000464214
	LOSS [training: 0.05995804259040771 | validation: 0.011500687494524653]
	TIME [epoch: 8.21 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06203390701567359		[learning rate: 0.00046367]
		[batch 20/20] avg loss: 0.04567893541750377		[learning rate: 0.00046312]
	Learning Rate: 0.000463119
	LOSS [training: 0.053856421216588665 | validation: 0.017180852791154198]
	TIME [epoch: 8.2 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04475437286286809		[learning rate: 0.00046257]
		[batch 20/20] avg loss: 0.04412604755854043		[learning rate: 0.00046203]
	Learning Rate: 0.000462026
	LOSS [training: 0.04444021021070426 | validation: 0.05361325765605002]
	TIME [epoch: 8.18 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06498994104522432		[learning rate: 0.00046148]
		[batch 20/20] avg loss: 0.046558849253636084		[learning rate: 0.00046094]
	Learning Rate: 0.000460936
	LOSS [training: 0.05577439514943021 | validation: 0.025424811049393016]
	TIME [epoch: 8.2 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05135912201648903		[learning rate: 0.00046039]
		[batch 20/20] avg loss: 0.06458460371624083		[learning rate: 0.00045985]
	Learning Rate: 0.000459849
	LOSS [training: 0.05797186286636493 | validation: 0.0435605665758342]
	TIME [epoch: 8.2 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06567007069225886		[learning rate: 0.00045931]
		[batch 20/20] avg loss: 0.07122660921472437		[learning rate: 0.00045876]
	Learning Rate: 0.000458764
	LOSS [training: 0.06844833995349162 | validation: 0.019991239181433754]
	TIME [epoch: 8.21 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048083614599093445		[learning rate: 0.00045822]
		[batch 20/20] avg loss: 0.039773679955468884		[learning rate: 0.00045768]
	Learning Rate: 0.000457682
	LOSS [training: 0.043928647277281165 | validation: 0.019552716729490074]
	TIME [epoch: 8.19 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05202848995777761		[learning rate: 0.00045714]
		[batch 20/20] avg loss: 0.03905653423536427		[learning rate: 0.0004566]
	Learning Rate: 0.000456603
	LOSS [training: 0.04554251209657094 | validation: 0.019661021988505972]
	TIME [epoch: 8.21 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06806123349866798		[learning rate: 0.00045606]
		[batch 20/20] avg loss: 0.04371446195884525		[learning rate: 0.00045553]
	Learning Rate: 0.000455526
	LOSS [training: 0.055887847728756615 | validation: 0.036709977239845315]
	TIME [epoch: 8.19 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04531943229369288		[learning rate: 0.00045499]
		[batch 20/20] avg loss: 0.04704396717797392		[learning rate: 0.00045445]
	Learning Rate: 0.000454451
	LOSS [training: 0.04618169973583339 | validation: 0.03445184935840633]
	TIME [epoch: 8.22 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05380658630455907		[learning rate: 0.00045391]
		[batch 20/20] avg loss: 0.06181504606027124		[learning rate: 0.00045338]
	Learning Rate: 0.000453379
	LOSS [training: 0.057810816182415146 | validation: 0.04036923627255472]
	TIME [epoch: 8.18 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06313932119176544		[learning rate: 0.00045284]
		[batch 20/20] avg loss: 0.055938840773884814		[learning rate: 0.00045231]
	Learning Rate: 0.00045231
	LOSS [training: 0.05953908098282512 | validation: 0.03574743189848019]
	TIME [epoch: 8.21 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04617806460903472		[learning rate: 0.00045178]
		[batch 20/20] avg loss: 0.04489653593959848		[learning rate: 0.00045124]
	Learning Rate: 0.000451243
	LOSS [training: 0.0455373002743166 | validation: 0.03098878806296975]
	TIME [epoch: 8.2 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0667369764546448		[learning rate: 0.00045071]
		[batch 20/20] avg loss: 0.05084246207800448		[learning rate: 0.00045018]
	Learning Rate: 0.000450178
	LOSS [training: 0.05878971926632465 | validation: 0.015905209208779428]
	TIME [epoch: 8.21 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03346342757111813		[learning rate: 0.00044965]
		[batch 20/20] avg loss: 0.06629969372101663		[learning rate: 0.00044912]
	Learning Rate: 0.000449116
	LOSS [training: 0.04988156064606737 | validation: 0.036173318281782996]
	TIME [epoch: 8.18 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05810686873950481		[learning rate: 0.00044859]
		[batch 20/20] avg loss: 0.043611939141248876		[learning rate: 0.00044806]
	Learning Rate: 0.000448057
	LOSS [training: 0.05085940394037684 | validation: 0.020094420213833605]
	TIME [epoch: 8.2 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047610974566364025		[learning rate: 0.00044753]
		[batch 20/20] avg loss: 0.05670029744670162		[learning rate: 0.000447]
	Learning Rate: 0.000447
	LOSS [training: 0.05215563600653282 | validation: 0.016805018496552204]
	TIME [epoch: 8.2 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055970230640285834		[learning rate: 0.00044647]
		[batch 20/20] avg loss: 0.04936791267016987		[learning rate: 0.00044595]
	Learning Rate: 0.000445946
	LOSS [training: 0.05266907165522785 | validation: 0.02804567146606447]
	TIME [epoch: 8.21 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0458133842060168		[learning rate: 0.00044542]
		[batch 20/20] avg loss: 0.05603166820566343		[learning rate: 0.00044489]
	Learning Rate: 0.000444894
	LOSS [training: 0.050922526205840125 | validation: 0.029140402125188175]
	TIME [epoch: 8.18 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0565198879388213		[learning rate: 0.00044437]
		[batch 20/20] avg loss: 0.053044882527323076		[learning rate: 0.00044384]
	Learning Rate: 0.000443844
	LOSS [training: 0.054782385233072196 | validation: 0.026451233173853325]
	TIME [epoch: 8.21 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045483735628109166		[learning rate: 0.00044332]
		[batch 20/20] avg loss: 0.05707663713842388		[learning rate: 0.0004428]
	Learning Rate: 0.000442797
	LOSS [training: 0.05128018638326652 | validation: 0.03086404014088677]
	TIME [epoch: 8.2 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054474012951832615		[learning rate: 0.00044227]
		[batch 20/20] avg loss: 0.06853682514584555		[learning rate: 0.00044175]
	Learning Rate: 0.000441753
	LOSS [training: 0.06150541904883909 | validation: 0.02129982524427241]
	TIME [epoch: 8.2 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04825361850403407		[learning rate: 0.00044123]
		[batch 20/20] avg loss: 0.04684648513759658		[learning rate: 0.00044071]
	Learning Rate: 0.000440711
	LOSS [training: 0.04755005182081532 | validation: 0.032423182705100806]
	TIME [epoch: 8.2 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05948994512252681		[learning rate: 0.00044019]
		[batch 20/20] avg loss: 0.06585897660608306		[learning rate: 0.00043967]
	Learning Rate: 0.000439671
	LOSS [training: 0.06267446086430495 | validation: 0.0190843614882332]
	TIME [epoch: 8.2 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061379465724259855		[learning rate: 0.00043915]
		[batch 20/20] avg loss: 0.06071702539672804		[learning rate: 0.00043863]
	Learning Rate: 0.000438634
	LOSS [training: 0.061048245560493944 | validation: 0.028706972462507828]
	TIME [epoch: 8.21 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05707444749577686		[learning rate: 0.00043812]
		[batch 20/20] avg loss: 0.05830287125664575		[learning rate: 0.0004376]
	Learning Rate: 0.0004376
	LOSS [training: 0.057688659376211295 | validation: 0.027481226172096966]
	TIME [epoch: 8.2 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048989364557931306		[learning rate: 0.00043708]
		[batch 20/20] avg loss: 0.06688485107571987		[learning rate: 0.00043657]
	Learning Rate: 0.000436567
	LOSS [training: 0.057937107816825574 | validation: 0.02184834496546281]
	TIME [epoch: 8.18 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07060280963634066		[learning rate: 0.00043605]
		[batch 20/20] avg loss: 0.06343913249230829		[learning rate: 0.00043554]
	Learning Rate: 0.000435538
	LOSS [training: 0.06702097106432447 | validation: 0.03638765383959201]
	TIME [epoch: 8.2 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047828098194050754		[learning rate: 0.00043502]
		[batch 20/20] avg loss: 0.049816256808650386		[learning rate: 0.00043451]
	Learning Rate: 0.00043451
	LOSS [training: 0.048822177501350573 | validation: 0.06185313545237555]
	TIME [epoch: 8.22 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040989892387031726		[learning rate: 0.000434]
		[batch 20/20] avg loss: 0.052401691023969486		[learning rate: 0.00043349]
	Learning Rate: 0.000433485
	LOSS [training: 0.04669579170550062 | validation: 0.04917096558907006]
	TIME [epoch: 8.19 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057212471170022		[learning rate: 0.00043297]
		[batch 20/20] avg loss: 0.05206398907506297		[learning rate: 0.00043246]
	Learning Rate: 0.000432463
	LOSS [training: 0.05463823012254247 | validation: 0.03455178860707166]
	TIME [epoch: 8.19 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052248173244569984		[learning rate: 0.00043195]
		[batch 20/20] avg loss: 0.06177926625078524		[learning rate: 0.00043144]
	Learning Rate: 0.000431443
	LOSS [training: 0.057013719747677605 | validation: 0.018481720052355325]
	TIME [epoch: 8.21 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05675961770808534		[learning rate: 0.00043093]
		[batch 20/20] avg loss: 0.04644759231287515		[learning rate: 0.00043042]
	Learning Rate: 0.000430425
	LOSS [training: 0.05160360501048024 | validation: 0.024985372921917517]
	TIME [epoch: 8.22 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05439765668363421		[learning rate: 0.00042992]
		[batch 20/20] avg loss: 0.060999548693967544		[learning rate: 0.00042941]
	Learning Rate: 0.00042941
	LOSS [training: 0.05769860268880088 | validation: 0.04811740634785133]
	TIME [epoch: 8.19 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06144515053335693		[learning rate: 0.0004289]
		[batch 20/20] avg loss: 0.05208595967579396		[learning rate: 0.0004284]
	Learning Rate: 0.000428397
	LOSS [training: 0.05676555510457543 | validation: 0.015696219844436955]
	TIME [epoch: 8.18 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0677582033841937		[learning rate: 0.00042789]
		[batch 20/20] avg loss: 0.05853093329962442		[learning rate: 0.00042739]
	Learning Rate: 0.000427386
	LOSS [training: 0.06314456834190907 | validation: 0.022526610264839823]
	TIME [epoch: 8.21 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06309293273900375		[learning rate: 0.00042688]
		[batch 20/20] avg loss: 0.056005037466176445		[learning rate: 0.00042638]
	Learning Rate: 0.000426378
	LOSS [training: 0.0595489851025901 | validation: 0.01570913199688564]
	TIME [epoch: 8.23 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06339755547305446		[learning rate: 0.00042587]
		[batch 20/20] avg loss: 0.05838811277496107		[learning rate: 0.00042537]
	Learning Rate: 0.000425372
	LOSS [training: 0.06089283412400777 | validation: 0.02199564452633157]
	TIME [epoch: 8.18 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05237956266344609		[learning rate: 0.00042487]
		[batch 20/20] avg loss: 0.06007548130804393		[learning rate: 0.00042437]
	Learning Rate: 0.000424369
	LOSS [training: 0.05622752198574501 | validation: 0.024461130852818782]
	TIME [epoch: 8.19 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06458017872288373		[learning rate: 0.00042387]
		[batch 20/20] avg loss: 0.056107249827247595		[learning rate: 0.00042337]
	Learning Rate: 0.000423368
	LOSS [training: 0.06034371427506567 | validation: 0.018667386710150946]
	TIME [epoch: 8.19 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06124100937190741		[learning rate: 0.00042287]
		[batch 20/20] avg loss: 0.05800156603854846		[learning rate: 0.00042237]
	Learning Rate: 0.000422369
	LOSS [training: 0.059621287705227934 | validation: 0.01475961057205972]
	TIME [epoch: 8.23 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07784761941294441		[learning rate: 0.00042187]
		[batch 20/20] avg loss: 0.04644066662189054		[learning rate: 0.00042137]
	Learning Rate: 0.000421373
	LOSS [training: 0.06214414301741746 | validation: 0.02195268790689167]
	TIME [epoch: 8.17 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05111326779769386		[learning rate: 0.00042088]
		[batch 20/20] avg loss: 0.04639522831968732		[learning rate: 0.00042038]
	Learning Rate: 0.000420379
	LOSS [training: 0.048754248058690594 | validation: 0.03401016544080654]
	TIME [epoch: 8.18 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07407359186764573		[learning rate: 0.00041988]
		[batch 20/20] avg loss: 0.05223077808671019		[learning rate: 0.00041939]
	Learning Rate: 0.000419387
	LOSS [training: 0.06315218497717796 | validation: 0.041068227156260656]
	TIME [epoch: 8.2 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053537688068001056		[learning rate: 0.00041889]
		[batch 20/20] avg loss: 0.05864718182639089		[learning rate: 0.0004184]
	Learning Rate: 0.000418398
	LOSS [training: 0.05609243494719597 | validation: 0.018797593151573228]
	TIME [epoch: 8.21 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07248812098145478		[learning rate: 0.0004179]
		[batch 20/20] avg loss: 0.0633150156879374		[learning rate: 0.00041741]
	Learning Rate: 0.000417411
	LOSS [training: 0.06790156833469609 | validation: 0.00587733407786897]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_1395.pth
	Model improved!!!
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04651307319815355		[learning rate: 0.00041692]
		[batch 20/20] avg loss: 0.04505133347951058		[learning rate: 0.00041643]
	Learning Rate: 0.000416427
	LOSS [training: 0.04578220333883206 | validation: 0.017211852202483863]
	TIME [epoch: 8.19 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04261276031636026		[learning rate: 0.00041594]
		[batch 20/20] avg loss: 0.05674970275482991		[learning rate: 0.00041544]
	Learning Rate: 0.000415444
	LOSS [training: 0.04968123153559508 | validation: 0.021918454301995984]
	TIME [epoch: 8.22 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05457520152574395		[learning rate: 0.00041495]
		[batch 20/20] avg loss: 0.05709939991649554		[learning rate: 0.00041446]
	Learning Rate: 0.000414464
	LOSS [training: 0.05583730072111973 | validation: 0.013581577036062293]
	TIME [epoch: 8.19 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052187436684521374		[learning rate: 0.00041398]
		[batch 20/20] avg loss: 0.06699129651043724		[learning rate: 0.00041349]
	Learning Rate: 0.000413487
	LOSS [training: 0.05958936659747931 | validation: 0.04255488456505535]
	TIME [epoch: 8.17 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06684454412704217		[learning rate: 0.000413]
		[batch 20/20] avg loss: 0.050357981042526014		[learning rate: 0.00041251]
	Learning Rate: 0.000412511
	LOSS [training: 0.05860126258478408 | validation: 0.03557707088177342]
	TIME [epoch: 8.18 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051002452986018486		[learning rate: 0.00041202]
		[batch 20/20] avg loss: 0.042634129331376384		[learning rate: 0.00041154]
	Learning Rate: 0.000411538
	LOSS [training: 0.04681829115869744 | validation: 0.02439377004635194]
	TIME [epoch: 8.21 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04367245660520801		[learning rate: 0.00041105]
		[batch 20/20] avg loss: 0.06511593192700438		[learning rate: 0.00041057]
	Learning Rate: 0.000410568
	LOSS [training: 0.05439419426610618 | validation: 0.025972858203402505]
	TIME [epoch: 8.2 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05230544905908078		[learning rate: 0.00041008]
		[batch 20/20] avg loss: 0.05108278193797129		[learning rate: 0.0004096]
	Learning Rate: 0.000409599
	LOSS [training: 0.05169411549852604 | validation: 0.03922092371114632]
	TIME [epoch: 8.18 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048523319624546056		[learning rate: 0.00040912]
		[batch 20/20] avg loss: 0.043038647353264806		[learning rate: 0.00040863]
	Learning Rate: 0.000408633
	LOSS [training: 0.04578098348890542 | validation: 0.015378842375526879]
	TIME [epoch: 8.18 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05707718259812613		[learning rate: 0.00040815]
		[batch 20/20] avg loss: 0.05684989182902646		[learning rate: 0.00040767]
	Learning Rate: 0.000407669
	LOSS [training: 0.05696353721357631 | validation: 0.038975957698230174]
	TIME [epoch: 8.21 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06466747630439419		[learning rate: 0.00040719]
		[batch 20/20] avg loss: 0.05256145492114165		[learning rate: 0.00040671]
	Learning Rate: 0.000406707
	LOSS [training: 0.05861446561276792 | validation: 0.01537503367122069]
	TIME [epoch: 8.2 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06543995008622958		[learning rate: 0.00040623]
		[batch 20/20] avg loss: 0.04507280438016297		[learning rate: 0.00040575]
	Learning Rate: 0.000405748
	LOSS [training: 0.05525637723319628 | validation: 0.02527223537003504]
	TIME [epoch: 8.19 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05029160613433244		[learning rate: 0.00040527]
		[batch 20/20] avg loss: 0.05345570218892402		[learning rate: 0.00040479]
	Learning Rate: 0.000404791
	LOSS [training: 0.05187365416162822 | validation: 0.014025667252320385]
	TIME [epoch: 8.18 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04804825346384182		[learning rate: 0.00040431]
		[batch 20/20] avg loss: 0.045747999411822046		[learning rate: 0.00040384]
	Learning Rate: 0.000403836
	LOSS [training: 0.04689812643783194 | validation: 0.01136672275379546]
	TIME [epoch: 8.23 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039777981539601576		[learning rate: 0.00040336]
		[batch 20/20] avg loss: 0.048576023978886475		[learning rate: 0.00040288]
	Learning Rate: 0.000402883
	LOSS [training: 0.04417700275924402 | validation: 0.018905463222895262]
	TIME [epoch: 8.2 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05674194577523928		[learning rate: 0.00040241]
		[batch 20/20] avg loss: 0.04788431618355733		[learning rate: 0.00040193]
	Learning Rate: 0.000401933
	LOSS [training: 0.05231313097939829 | validation: 0.022962274940829287]
	TIME [epoch: 8.17 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05280899098583527		[learning rate: 0.00040146]
		[batch 20/20] avg loss: 0.04601220301866959		[learning rate: 0.00040099]
	Learning Rate: 0.000400985
	LOSS [training: 0.04941059700225243 | validation: 0.04173116219526415]
	TIME [epoch: 8.17 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053739694452044365		[learning rate: 0.00040051]
		[batch 20/20] avg loss: 0.05422972095988641		[learning rate: 0.00040004]
	Learning Rate: 0.000400039
	LOSS [training: 0.0539847077059654 | validation: 0.06057639305042986]
	TIME [epoch: 8.23 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06935571083069236		[learning rate: 0.00039957]
		[batch 20/20] avg loss: 0.0368826433066584		[learning rate: 0.0003991]
	Learning Rate: 0.000399096
	LOSS [training: 0.05311917706867538 | validation: 0.024803255927544333]
	TIME [epoch: 8.19 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0560802365105319		[learning rate: 0.00039862]
		[batch 20/20] avg loss: 0.037818265448213435		[learning rate: 0.00039815]
	Learning Rate: 0.000398154
	LOSS [training: 0.04694925097937267 | validation: 0.02530001160709584]
	TIME [epoch: 8.18 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04036145651182387		[learning rate: 0.00039768]
		[batch 20/20] avg loss: 0.047797943552798		[learning rate: 0.00039721]
	Learning Rate: 0.000397215
	LOSS [training: 0.04407970003231094 | validation: 0.027227046146185672]
	TIME [epoch: 8.18 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0730288845746014		[learning rate: 0.00039675]
		[batch 20/20] avg loss: 0.06978000228597951		[learning rate: 0.00039628]
	Learning Rate: 0.000396278
	LOSS [training: 0.07140444343029043 | validation: 0.032578716924651425]
	TIME [epoch: 8.24 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05085283508242281		[learning rate: 0.00039581]
		[batch 20/20] avg loss: 0.05776574917852588		[learning rate: 0.00039534]
	Learning Rate: 0.000395343
	LOSS [training: 0.05430929213047434 | validation: 0.019144340511115306]
	TIME [epoch: 8.19 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03979638516611904		[learning rate: 0.00039488]
		[batch 20/20] avg loss: 0.04827576610550129		[learning rate: 0.00039441]
	Learning Rate: 0.000394411
	LOSS [training: 0.04403607563581017 | validation: 0.027529839192751947]
	TIME [epoch: 8.19 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060423435233554056		[learning rate: 0.00039395]
		[batch 20/20] avg loss: 0.05134543434118059		[learning rate: 0.00039348]
	Learning Rate: 0.00039348
	LOSS [training: 0.055884434787367335 | validation: 0.03803929432124953]
	TIME [epoch: 8.19 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0639750102093749		[learning rate: 0.00039302]
		[batch 20/20] avg loss: 0.05017545828111434		[learning rate: 0.00039255]
	Learning Rate: 0.000392552
	LOSS [training: 0.05707523424524462 | validation: 0.01915183057919506]
	TIME [epoch: 8.23 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04683893963255197		[learning rate: 0.00039209]
		[batch 20/20] avg loss: 0.05987589900913139		[learning rate: 0.00039163]
	Learning Rate: 0.000391626
	LOSS [training: 0.05335741932084168 | validation: 0.019567570374663938]
	TIME [epoch: 8.2 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043596930852910866		[learning rate: 0.00039116]
		[batch 20/20] avg loss: 0.04629193409065917		[learning rate: 0.0003907]
	Learning Rate: 0.000390702
	LOSS [training: 0.04494443247178502 | validation: 0.026767920978762263]
	TIME [epoch: 8.18 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057582650287561196		[learning rate: 0.00039024]
		[batch 20/20] avg loss: 0.051529143936651575		[learning rate: 0.00038978]
	Learning Rate: 0.000389781
	LOSS [training: 0.054555897112106386 | validation: 0.03888222574825224]
	TIME [epoch: 8.2 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05808266169659517		[learning rate: 0.00038932]
		[batch 20/20] avg loss: 0.058497202557173875		[learning rate: 0.00038886]
	Learning Rate: 0.000388861
	LOSS [training: 0.05828993212688451 | validation: 0.01175483063358294]
	TIME [epoch: 8.23 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04325244716220983		[learning rate: 0.0003884]
		[batch 20/20] avg loss: 0.04663534654792102		[learning rate: 0.00038794]
	Learning Rate: 0.000387944
	LOSS [training: 0.044943896855065425 | validation: 0.023867120296831877]
	TIME [epoch: 8.19 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05232301332320237		[learning rate: 0.00038749]
		[batch 20/20] avg loss: 0.04041974137981379		[learning rate: 0.00038703]
	Learning Rate: 0.000387029
	LOSS [training: 0.046371377351508085 | validation: 0.031236950507169764]
	TIME [epoch: 8.18 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054316158455218436		[learning rate: 0.00038657]
		[batch 20/20] avg loss: 0.06437653635765314		[learning rate: 0.00038612]
	Learning Rate: 0.000386116
	LOSS [training: 0.059346347406435795 | validation: 0.043280211102256674]
	TIME [epoch: 8.2 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05455979683806071		[learning rate: 0.00038566]
		[batch 20/20] avg loss: 0.06693358655898296		[learning rate: 0.00038521]
	Learning Rate: 0.000385205
	LOSS [training: 0.06074669169852183 | validation: 0.022847999728631198]
	TIME [epoch: 8.22 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0489876972457169		[learning rate: 0.00038475]
		[batch 20/20] avg loss: 0.05116945747890063		[learning rate: 0.0003843]
	Learning Rate: 0.000384297
	LOSS [training: 0.050078577362308765 | validation: 0.025963863936681597]
	TIME [epoch: 8.19 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049366198675006155		[learning rate: 0.00038384]
		[batch 20/20] avg loss: 0.05320128712384018		[learning rate: 0.00038339]
	Learning Rate: 0.00038339
	LOSS [training: 0.05128374289942317 | validation: 0.08045686009048886]
	TIME [epoch: 8.18 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05550089279322036		[learning rate: 0.00038294]
		[batch 20/20] avg loss: 0.0598664431536283		[learning rate: 0.00038249]
	Learning Rate: 0.000382486
	LOSS [training: 0.05768366797342432 | validation: 0.023717290108735342]
	TIME [epoch: 8.2 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052563119605592255		[learning rate: 0.00038203]
		[batch 20/20] avg loss: 0.04361451784304658		[learning rate: 0.00038158]
	Learning Rate: 0.000381584
	LOSS [training: 0.048088818724319415 | validation: 0.01860753955961382]
	TIME [epoch: 8.21 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03464618472956623		[learning rate: 0.00038113]
		[batch 20/20] avg loss: 0.05471938549396087		[learning rate: 0.00038068]
	Learning Rate: 0.000380684
	LOSS [training: 0.044682785111763546 | validation: 0.031882292384325106]
	TIME [epoch: 8.19 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05965975296186214		[learning rate: 0.00038023]
		[batch 20/20] avg loss: 0.04518865252138345		[learning rate: 0.00037979]
	Learning Rate: 0.000379786
	LOSS [training: 0.05242420274162281 | validation: 0.019717782182175]
	TIME [epoch: 8.19 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05158687289626217		[learning rate: 0.00037934]
		[batch 20/20] avg loss: 0.04335858819631825		[learning rate: 0.00037889]
	Learning Rate: 0.00037889
	LOSS [training: 0.047472730546290205 | validation: 0.01862313392104315]
	TIME [epoch: 8.22 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05877499348572274		[learning rate: 0.00037844]
		[batch 20/20] avg loss: 0.054888640942845734		[learning rate: 0.000378]
	Learning Rate: 0.000377996
	LOSS [training: 0.05683181721428423 | validation: 0.020121271287241276]
	TIME [epoch: 8.2 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044582216732005076		[learning rate: 0.00037755]
		[batch 20/20] avg loss: 0.04059008605022249		[learning rate: 0.0003771]
	Learning Rate: 0.000377104
	LOSS [training: 0.042586151391113776 | validation: 0.028643473729758645]
	TIME [epoch: 8.21 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05251016892509456		[learning rate: 0.00037666]
		[batch 20/20] avg loss: 0.07221873767118597		[learning rate: 0.00037621]
	Learning Rate: 0.000376215
	LOSS [training: 0.06236445329814028 | validation: 0.028202746696881754]
	TIME [epoch: 8.2 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04210655547100822		[learning rate: 0.00037577]
		[batch 20/20] avg loss: 0.042941500121884094		[learning rate: 0.00037533]
	Learning Rate: 0.000375327
	LOSS [training: 0.042524027796446165 | validation: 0.015056586984041349]
	TIME [epoch: 8.24 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04056046112635423		[learning rate: 0.00037488]
		[batch 20/20] avg loss: 0.05797647994787677		[learning rate: 0.00037444]
	Learning Rate: 0.000374442
	LOSS [training: 0.04926847053711551 | validation: 0.03277883137800685]
	TIME [epoch: 8.19 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04648468595659906		[learning rate: 0.000374]
		[batch 20/20] avg loss: 0.04825123976269886		[learning rate: 0.00037356]
	Learning Rate: 0.000373559
	LOSS [training: 0.047367962859648956 | validation: 0.01737900740033837]
	TIME [epoch: 8.22 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05050904705297878		[learning rate: 0.00037312]
		[batch 20/20] avg loss: 0.05541647980588223		[learning rate: 0.00037268]
	Learning Rate: 0.000372678
	LOSS [training: 0.052962763429430515 | validation: 0.028314028624851224]
	TIME [epoch: 8.19 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04606582984638498		[learning rate: 0.00037224]
		[batch 20/20] avg loss: 0.09173352276547993		[learning rate: 0.0003718]
	Learning Rate: 0.000371799
	LOSS [training: 0.06889967630593247 | validation: 0.017389902699624475]
	TIME [epoch: 8.24 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04604285216582454		[learning rate: 0.00037136]
		[batch 20/20] avg loss: 0.04673415066447239		[learning rate: 0.00037092]
	Learning Rate: 0.000370922
	LOSS [training: 0.04638850141514846 | validation: 0.016927681630637684]
	TIME [epoch: 8.18 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04612214116956852		[learning rate: 0.00037048]
		[batch 20/20] avg loss: 0.05683975778202784		[learning rate: 0.00037005]
	Learning Rate: 0.000370047
	LOSS [training: 0.05148094947579818 | validation: 0.03169627018746132]
	TIME [epoch: 8.21 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05378255152428542		[learning rate: 0.00036961]
		[batch 20/20] avg loss: 0.045745977208611836		[learning rate: 0.00036917]
	Learning Rate: 0.000369174
	LOSS [training: 0.049764264366448624 | validation: 0.017480809025014827]
	TIME [epoch: 8.2 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054777276164920216		[learning rate: 0.00036874]
		[batch 20/20] avg loss: 0.055592580798388046		[learning rate: 0.0003683]
	Learning Rate: 0.000368303
	LOSS [training: 0.05518492848165414 | validation: 0.024654236304744545]
	TIME [epoch: 8.22 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05605829806855148		[learning rate: 0.00036787]
		[batch 20/20] avg loss: 0.04917824984672104		[learning rate: 0.00036743]
	Learning Rate: 0.000367434
	LOSS [training: 0.05261827395763626 | validation: 0.020827120271058083]
	TIME [epoch: 8.18 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046424809689489685		[learning rate: 0.000367]
		[batch 20/20] avg loss: 0.0473629178479177		[learning rate: 0.00036657]
	Learning Rate: 0.000366567
	LOSS [training: 0.04689386376870369 | validation: 0.04616489128269495]
	TIME [epoch: 8.2 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05359367952316683		[learning rate: 0.00036613]
		[batch 20/20] avg loss: 0.04636794235113363		[learning rate: 0.0003657]
	Learning Rate: 0.000365703
	LOSS [training: 0.04998081093715023 | validation: 0.03017953951310437]
	TIME [epoch: 8.2 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03867843791292249		[learning rate: 0.00036527]
		[batch 20/20] avg loss: 0.0685399434771898		[learning rate: 0.00036484]
	Learning Rate: 0.00036484
	LOSS [training: 0.05360919069505614 | validation: 0.09119934011871932]
	TIME [epoch: 8.2 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0634242110700122		[learning rate: 0.00036441]
		[batch 20/20] avg loss: 0.051692015038478666		[learning rate: 0.00036398]
	Learning Rate: 0.000363979
	LOSS [training: 0.05755811305424543 | validation: 0.023532423102621342]
	TIME [epoch: 8.19 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06399325455700153		[learning rate: 0.00036355]
		[batch 20/20] avg loss: 0.051278663064279674		[learning rate: 0.00036312]
	Learning Rate: 0.000363121
	LOSS [training: 0.0576359588106406 | validation: 0.02908016146088692]
	TIME [epoch: 8.21 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06121688843857633		[learning rate: 0.00036269]
		[batch 20/20] avg loss: 0.046597232141371826		[learning rate: 0.00036226]
	Learning Rate: 0.000362264
	LOSS [training: 0.05390706028997407 | validation: 0.011488287520605954]
	TIME [epoch: 8.2 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048226979188948915		[learning rate: 0.00036184]
		[batch 20/20] avg loss: 0.03914631457661523		[learning rate: 0.00036141]
	Learning Rate: 0.00036141
	LOSS [training: 0.04368664688278207 | validation: 0.034154349421428386]
	TIME [epoch: 8.21 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0562066572073468		[learning rate: 0.00036098]
		[batch 20/20] avg loss: 0.04945796369489104		[learning rate: 0.00036056]
	Learning Rate: 0.000360557
	LOSS [training: 0.05283231045111893 | validation: 0.013774424341930328]
	TIME [epoch: 8.19 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05052716591882258		[learning rate: 0.00036013]
		[batch 20/20] avg loss: 0.06530663009127628		[learning rate: 0.00035971]
	Learning Rate: 0.000359707
	LOSS [training: 0.05791689800504944 | validation: 0.061436682640420806]
	TIME [epoch: 8.21 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060306195947322075		[learning rate: 0.00035928]
		[batch 20/20] avg loss: 0.046083710264056284		[learning rate: 0.00035886]
	Learning Rate: 0.000358858
	LOSS [training: 0.05319495310568917 | validation: 0.016782035347882165]
	TIME [epoch: 8.2 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0366398850531921		[learning rate: 0.00035843]
		[batch 20/20] avg loss: 0.04955673295765996		[learning rate: 0.00035801]
	Learning Rate: 0.000358012
	LOSS [training: 0.04309830900542603 | validation: 0.006767023775969358]
	TIME [epoch: 8.2 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05614303388292156		[learning rate: 0.00035759]
		[batch 20/20] avg loss: 0.0449345451135781		[learning rate: 0.00035717]
	Learning Rate: 0.000357167
	LOSS [training: 0.050538789498249824 | validation: 0.026083992186778357]
	TIME [epoch: 8.19 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059094784365302674		[learning rate: 0.00035675]
		[batch 20/20] avg loss: 0.04603084618688189		[learning rate: 0.00035632]
	Learning Rate: 0.000356325
	LOSS [training: 0.05256281527609228 | validation: 0.02503461743415184]
	TIME [epoch: 8.21 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04825721576968738		[learning rate: 0.0003559]
		[batch 20/20] avg loss: 0.046738206816030956		[learning rate: 0.00035548]
	Learning Rate: 0.000355484
	LOSS [training: 0.047497711292859165 | validation: 0.030817065796055858]
	TIME [epoch: 8.21 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0552902526562709		[learning rate: 0.00035506]
		[batch 20/20] avg loss: 0.0683664134122992		[learning rate: 0.00035465]
	Learning Rate: 0.000354646
	LOSS [training: 0.061828333034285074 | validation: 0.02181508214135913]
	TIME [epoch: 8.2 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046204590233259964		[learning rate: 0.00035423]
		[batch 20/20] avg loss: 0.05145406574503575		[learning rate: 0.00035381]
	Learning Rate: 0.000353809
	LOSS [training: 0.04882932798914786 | validation: 0.03136410947616145]
	TIME [epoch: 8.18 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04584547508773266		[learning rate: 0.00035339]
		[batch 20/20] avg loss: 0.05106261912471914		[learning rate: 0.00035297]
	Learning Rate: 0.000352975
	LOSS [training: 0.0484540471062259 | validation: 0.023585542069665816]
	TIME [epoch: 8.2 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053527481566430515		[learning rate: 0.00035256]
		[batch 20/20] avg loss: 0.045718389552259105		[learning rate: 0.00035214]
	Learning Rate: 0.000352142
	LOSS [training: 0.049622935559344813 | validation: 0.018949421785777645]
	TIME [epoch: 8.22 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05901523937251264		[learning rate: 0.00035173]
		[batch 20/20] avg loss: 0.04460937301558335		[learning rate: 0.00035131]
	Learning Rate: 0.000351311
	LOSS [training: 0.051812306194047995 | validation: 0.015895558880386]
	TIME [epoch: 8.2 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034622603717479564		[learning rate: 0.0003509]
		[batch 20/20] avg loss: 0.061954153553200816		[learning rate: 0.00035048]
	Learning Rate: 0.000350483
	LOSS [training: 0.04828837863534019 | validation: 0.009596472809777315]
	TIME [epoch: 8.18 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03867843369725375		[learning rate: 0.00035007]
		[batch 20/20] avg loss: 0.05371185940239452		[learning rate: 0.00034966]
	Learning Rate: 0.000349656
	LOSS [training: 0.04619514654982414 | validation: 0.0313534791027298]
	TIME [epoch: 8.2 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05683537638753079		[learning rate: 0.00034924]
		[batch 20/20] avg loss: 0.048115837414817335		[learning rate: 0.00034883]
	Learning Rate: 0.000348831
	LOSS [training: 0.05247560690117405 | validation: 0.030001133642060897]
	TIME [epoch: 8.23 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05183779728431005		[learning rate: 0.00034842]
		[batch 20/20] avg loss: 0.0474995495627807		[learning rate: 0.00034801]
	Learning Rate: 0.000348008
	LOSS [training: 0.049668673423545376 | validation: 0.01926830780382699]
	TIME [epoch: 8.19 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06228397656768312		[learning rate: 0.0003476]
		[batch 20/20] avg loss: 0.052196512010612636		[learning rate: 0.00034719]
	Learning Rate: 0.000347187
	LOSS [training: 0.05724024428914788 | validation: 0.012354159576107526]
	TIME [epoch: 8.17 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038209048113385886		[learning rate: 0.00034678]
		[batch 20/20] avg loss: 0.058145447353470534		[learning rate: 0.00034637]
	Learning Rate: 0.000346369
	LOSS [training: 0.0481772477334282 | validation: 0.009124159002402188]
	TIME [epoch: 8.2 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047841973220197384		[learning rate: 0.00034596]
		[batch 20/20] avg loss: 0.05255204876117838		[learning rate: 0.00034555]
	Learning Rate: 0.000345552
	LOSS [training: 0.050197010990687886 | validation: 0.017474536278089526]
	TIME [epoch: 8.23 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0573269387432767		[learning rate: 0.00034514]
		[batch 20/20] avg loss: 0.03518783963067519		[learning rate: 0.00034474]
	Learning Rate: 0.000344736
	LOSS [training: 0.04625738918697595 | validation: 0.013677984322708014]
	TIME [epoch: 8.19 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052015859458758584		[learning rate: 0.00034433]
		[batch 20/20] avg loss: 0.05111256001562625		[learning rate: 0.00034392]
	Learning Rate: 0.000343923
	LOSS [training: 0.05156420973719241 | validation: 0.022407020241337224]
	TIME [epoch: 8.17 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04018569465722797		[learning rate: 0.00034352]
		[batch 20/20] avg loss: 0.04518892808217213		[learning rate: 0.00034311]
	Learning Rate: 0.000343112
	LOSS [training: 0.042687311369700054 | validation: 0.01836086580638625]
	TIME [epoch: 8.2 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048361781538276416		[learning rate: 0.00034271]
		[batch 20/20] avg loss: 0.05320015657765561		[learning rate: 0.0003423]
	Learning Rate: 0.000342303
	LOSS [training: 0.05078096905796603 | validation: 0.02156363749057043]
	TIME [epoch: 8.23 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0455137737239248		[learning rate: 0.0003419]
		[batch 20/20] avg loss: 0.05210383847657836		[learning rate: 0.0003415]
	Learning Rate: 0.000341495
	LOSS [training: 0.04880880610025157 | validation: 0.041170675230380434]
	TIME [epoch: 8.18 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04455607976242135		[learning rate: 0.00034109]
		[batch 20/20] avg loss: 0.04763680215814644		[learning rate: 0.00034069]
	Learning Rate: 0.00034069
	LOSS [training: 0.04609644096028389 | validation: 0.030837602417185354]
	TIME [epoch: 8.19 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05196937297984622		[learning rate: 0.00034029]
		[batch 20/20] avg loss: 0.03845433723650226		[learning rate: 0.00033989]
	Learning Rate: 0.000339886
	LOSS [training: 0.04521185510817424 | validation: 0.011715722030626433]
	TIME [epoch: 8.22 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04067396900066802		[learning rate: 0.00033948]
		[batch 20/20] avg loss: 0.06654994691031962		[learning rate: 0.00033908]
	Learning Rate: 0.000339084
	LOSS [training: 0.053611957955493825 | validation: 0.05709785190354866]
	TIME [epoch: 8.22 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05186322396541918		[learning rate: 0.00033868]
		[batch 20/20] avg loss: 0.053754022054516334		[learning rate: 0.00033828]
	Learning Rate: 0.000338284
	LOSS [training: 0.05280862300996776 | validation: 0.03742625539698131]
	TIME [epoch: 8.18 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04672863528859356		[learning rate: 0.00033789]
		[batch 20/20] avg loss: 0.05118662177920036		[learning rate: 0.00033749]
	Learning Rate: 0.000337487
	LOSS [training: 0.048957628533896964 | validation: 0.012452758890008348]
	TIME [epoch: 8.18 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04003543368549814		[learning rate: 0.00033709]
		[batch 20/20] avg loss: 0.051420667202502365		[learning rate: 0.00033669]
	Learning Rate: 0.00033669
	LOSS [training: 0.04572805044400025 | validation: 0.030526107640106583]
	TIME [epoch: 8.22 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056301753290022384		[learning rate: 0.00033629]
		[batch 20/20] avg loss: 0.04602406006299635		[learning rate: 0.0003359]
	Learning Rate: 0.000335896
	LOSS [training: 0.05116290667650937 | validation: 0.04169632800317449]
	TIME [epoch: 8.21 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04165807280301558		[learning rate: 0.0003355]
		[batch 20/20] avg loss: 0.04766421321122445		[learning rate: 0.0003351]
	Learning Rate: 0.000335104
	LOSS [training: 0.04466114300712003 | validation: 0.015895779803312606]
	TIME [epoch: 8.18 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051379724746092516		[learning rate: 0.00033471]
		[batch 20/20] avg loss: 0.07297838173418049		[learning rate: 0.00033431]
	Learning Rate: 0.000334313
	LOSS [training: 0.06217905324013649 | validation: 0.05346814796232456]
	TIME [epoch: 8.18 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06206543020893809		[learning rate: 0.00033392]
		[batch 20/20] avg loss: 0.04414022585446834		[learning rate: 0.00033352]
	Learning Rate: 0.000333525
	LOSS [training: 0.05310282803170322 | validation: 0.023287233287973136]
	TIME [epoch: 8.22 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04953986253026014		[learning rate: 0.00033313]
		[batch 20/20] avg loss: 0.04256140474774538		[learning rate: 0.00033274]
	Learning Rate: 0.000332738
	LOSS [training: 0.04605063363900275 | validation: 0.023991365397544012]
	TIME [epoch: 8.21 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05274869308928232		[learning rate: 0.00033235]
		[batch 20/20] avg loss: 0.04422135817244051		[learning rate: 0.00033195]
	Learning Rate: 0.000331953
	LOSS [training: 0.04848502563086142 | validation: 0.019966489859023627]
	TIME [epoch: 8.18 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04011070399909129		[learning rate: 0.00033156]
		[batch 20/20] avg loss: 0.03943556949334511		[learning rate: 0.00033117]
	Learning Rate: 0.00033117
	LOSS [training: 0.039773136746218206 | validation: 0.028658652576125133]
	TIME [epoch: 8.18 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04086293334440281		[learning rate: 0.00033078]
		[batch 20/20] avg loss: 0.05271668809732445		[learning rate: 0.00033039]
	Learning Rate: 0.000330389
	LOSS [training: 0.046789810720863634 | validation: 0.042237810093240305]
	TIME [epoch: 8.23 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056879288214849964		[learning rate: 0.00033]
		[batch 20/20] avg loss: 0.041863712470862804		[learning rate: 0.00032961]
	Learning Rate: 0.00032961
	LOSS [training: 0.04937150034285638 | validation: 0.018044454972260662]
	TIME [epoch: 8.21 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04163321157619975		[learning rate: 0.00032922]
		[batch 20/20] avg loss: 0.05114110650569218		[learning rate: 0.00032883]
	Learning Rate: 0.000328832
	LOSS [training: 0.046387159040945966 | validation: 0.02446653557673355]
	TIME [epoch: 8.19 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04708286377087568		[learning rate: 0.00032844]
		[batch 20/20] avg loss: 0.038375114390628566		[learning rate: 0.00032806]
	Learning Rate: 0.000328056
	LOSS [training: 0.04272898908075211 | validation: 0.024286199225078617]
	TIME [epoch: 8.18 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04913958323156982		[learning rate: 0.00032767]
		[batch 20/20] avg loss: 0.05979946088415744		[learning rate: 0.00032728]
	Learning Rate: 0.000327283
	LOSS [training: 0.05446952205786363 | validation: 0.022119390649289347]
	TIME [epoch: 8.24 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05996936493678947		[learning rate: 0.0003269]
		[batch 20/20] avg loss: 0.04812160490876343		[learning rate: 0.00032651]
	Learning Rate: 0.000326511
	LOSS [training: 0.05404548492277646 | validation: 0.035525975299771875]
	TIME [epoch: 8.2 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04177638402293151		[learning rate: 0.00032613]
		[batch 20/20] avg loss: 0.052500845234086026		[learning rate: 0.00032574]
	Learning Rate: 0.00032574
	LOSS [training: 0.047138614628508775 | validation: 0.029700387409586894]
	TIME [epoch: 8.19 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05047422557386788		[learning rate: 0.00032536]
		[batch 20/20] avg loss: 0.05175046294633332		[learning rate: 0.00032497]
	Learning Rate: 0.000324972
	LOSS [training: 0.0511123442601006 | validation: 0.05578255482187819]
	TIME [epoch: 8.19 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0465866933228823		[learning rate: 0.00032459]
		[batch 20/20] avg loss: 0.06264670164991618		[learning rate: 0.00032421]
	Learning Rate: 0.000324206
	LOSS [training: 0.054616697486399246 | validation: 0.04769165048617605]
	TIME [epoch: 8.24 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0405027315908284		[learning rate: 0.00032382]
		[batch 20/20] avg loss: 0.061455967033937894		[learning rate: 0.00032344]
	Learning Rate: 0.000323441
	LOSS [training: 0.05097934931238314 | validation: 0.04720361075729239]
	TIME [epoch: 8.19 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057523841105546		[learning rate: 0.00032306]
		[batch 20/20] avg loss: 0.03769739975046029		[learning rate: 0.00032268]
	Learning Rate: 0.000322678
	LOSS [training: 0.04761062042800314 | validation: 0.051782406174890896]
	TIME [epoch: 8.17 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054578235503580784		[learning rate: 0.0003223]
		[batch 20/20] avg loss: 0.055026821093738285		[learning rate: 0.00032192]
	Learning Rate: 0.000321917
	LOSS [training: 0.05480252829865954 | validation: 0.011948714441530723]
	TIME [epoch: 8.18 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047625224298302196		[learning rate: 0.00032154]
		[batch 20/20] avg loss: 0.044075321191716976		[learning rate: 0.00032116]
	Learning Rate: 0.000321157
	LOSS [training: 0.045850272745009586 | validation: 0.01797147790151366]
	TIME [epoch: 8.23 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04627337109348856		[learning rate: 0.00032078]
		[batch 20/20] avg loss: 0.06060819636768834		[learning rate: 0.0003204]
	Learning Rate: 0.0003204
	LOSS [training: 0.05344078373058845 | validation: 0.053343848367847356]
	TIME [epoch: 8.19 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0669953643971484		[learning rate: 0.00032002]
		[batch 20/20] avg loss: 0.046087417478828746		[learning rate: 0.00031964]
	Learning Rate: 0.000319644
	LOSS [training: 0.056541390937988566 | validation: 0.02018564820047431]
	TIME [epoch: 8.17 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05069787683389806		[learning rate: 0.00031927]
		[batch 20/20] avg loss: 0.03910303744910089		[learning rate: 0.00031889]
	Learning Rate: 0.00031889
	LOSS [training: 0.04490045714149948 | validation: 0.031307134836856536]
	TIME [epoch: 8.2 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06544829138001709		[learning rate: 0.00031851]
		[batch 20/20] avg loss: 0.05150552438097816		[learning rate: 0.00031814]
	Learning Rate: 0.000318138
	LOSS [training: 0.05847690788049763 | validation: 0.02521240266499318]
	TIME [epoch: 8.22 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038270744042907257		[learning rate: 0.00031776]
		[batch 20/20] avg loss: 0.04883001652001362		[learning rate: 0.00031739]
	Learning Rate: 0.000317387
	LOSS [training: 0.04355038028146043 | validation: 0.024786033195389027]
	TIME [epoch: 8.19 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06289853199491433		[learning rate: 0.00031701]
		[batch 20/20] avg loss: 0.0323977811552758		[learning rate: 0.00031664]
	Learning Rate: 0.000316639
	LOSS [training: 0.04764815657509506 | validation: 0.023177099221710986]
	TIME [epoch: 8.18 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048969528583792984		[learning rate: 0.00031627]
		[batch 20/20] avg loss: 0.04583011874233658		[learning rate: 0.00031589]
	Learning Rate: 0.000315892
	LOSS [training: 0.04739982366306478 | validation: 0.027150245605704948]
	TIME [epoch: 8.22 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051415829016003764		[learning rate: 0.00031552]
		[batch 20/20] avg loss: 0.05439149498676452		[learning rate: 0.00031515]
	Learning Rate: 0.000315147
	LOSS [training: 0.05290366200138415 | validation: 0.01391294539289328]
	TIME [epoch: 8.21 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039286015443704164		[learning rate: 0.00031477]
		[batch 20/20] avg loss: 0.043505958006780046		[learning rate: 0.0003144]
	Learning Rate: 0.000314403
	LOSS [training: 0.04139598672524211 | validation: 0.014407858798948773]
	TIME [epoch: 8.19 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047707085075826175		[learning rate: 0.00031403]
		[batch 20/20] avg loss: 0.04648123797975053		[learning rate: 0.00031366]
	Learning Rate: 0.000313662
	LOSS [training: 0.04709416152778836 | validation: 0.050602000909553066]
	TIME [epoch: 8.18 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04702567930820184		[learning rate: 0.00031329]
		[batch 20/20] avg loss: 0.05102908651872341		[learning rate: 0.00031292]
	Learning Rate: 0.000312922
	LOSS [training: 0.04902738291346262 | validation: 0.03239280736185978]
	TIME [epoch: 8.19 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04301200103778132		[learning rate: 0.00031255]
		[batch 20/20] avg loss: 0.05275264963393554		[learning rate: 0.00031218]
	Learning Rate: 0.000312184
	LOSS [training: 0.04788232533585842 | validation: 0.02030575857349167]
	TIME [epoch: 8.23 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0471008301701112		[learning rate: 0.00031182]
		[batch 20/20] avg loss: 0.04535160815939614		[learning rate: 0.00031145]
	Learning Rate: 0.000311447
	LOSS [training: 0.04622621916475367 | validation: 0.014459440162132143]
	TIME [epoch: 8.19 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04996348802610413		[learning rate: 0.00031108]
		[batch 20/20] avg loss: 0.03882900803605775		[learning rate: 0.00031071]
	Learning Rate: 0.000310713
	LOSS [training: 0.04439624803108095 | validation: 0.02094788719601452]
	TIME [epoch: 8.19 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04714661685480458		[learning rate: 0.00031035]
		[batch 20/20] avg loss: 0.05074899738722656		[learning rate: 0.00030998]
	Learning Rate: 0.00030998
	LOSS [training: 0.048947807121015575 | validation: 0.02480098107486598]
	TIME [epoch: 8.19 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04562463764533779		[learning rate: 0.00030961]
		[batch 20/20] avg loss: 0.045455991709971874		[learning rate: 0.00030925]
	Learning Rate: 0.000309249
	LOSS [training: 0.04554031467765483 | validation: 0.048354693389098966]
	TIME [epoch: 8.2 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05355661137857724		[learning rate: 0.00030888]
		[batch 20/20] avg loss: 0.044403505937555196		[learning rate: 0.00030852]
	Learning Rate: 0.000308519
	LOSS [training: 0.04898005865806622 | validation: 0.06021697502572053]
	TIME [epoch: 8.19 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05357673826200121		[learning rate: 0.00030815]
		[batch 20/20] avg loss: 0.037044710817866096		[learning rate: 0.00030779]
	Learning Rate: 0.000307791
	LOSS [training: 0.04531072453993366 | validation: 0.02665771232251787]
	TIME [epoch: 8.17 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059735440358709056		[learning rate: 0.00030743]
		[batch 20/20] avg loss: 0.05482366285255604		[learning rate: 0.00030707]
	Learning Rate: 0.000307065
	LOSS [training: 0.05727955160563253 | validation: 0.02142423245152715]
	TIME [epoch: 8.2 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04520710354837655		[learning rate: 0.0003067]
		[batch 20/20] avg loss: 0.058001424788251474		[learning rate: 0.00030634]
	Learning Rate: 0.000306341
	LOSS [training: 0.051604264168314005 | validation: 0.056906369011393704]
	TIME [epoch: 8.21 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05799545938583549		[learning rate: 0.00030598]
		[batch 20/20] avg loss: 0.047774737507040685		[learning rate: 0.00030562]
	Learning Rate: 0.000305618
	LOSS [training: 0.052885098446438086 | validation: 0.01903599824182068]
	TIME [epoch: 8.19 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034486087494456974		[learning rate: 0.00030526]
		[batch 20/20] avg loss: 0.04203928649211629		[learning rate: 0.0003049]
	Learning Rate: 0.000304897
	LOSS [training: 0.03826268699328663 | validation: 0.02771708570267345]
	TIME [epoch: 8.17 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04777852255568896		[learning rate: 0.00030454]
		[batch 20/20] avg loss: 0.04223645172040301		[learning rate: 0.00030418]
	Learning Rate: 0.000304178
	LOSS [training: 0.04500748713804598 | validation: 0.022394250202534556]
	TIME [epoch: 8.21 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039597144915239114		[learning rate: 0.00030382]
		[batch 20/20] avg loss: 0.04871038620849129		[learning rate: 0.00030346]
	Learning Rate: 0.000303461
	LOSS [training: 0.044153765561865205 | validation: 0.0258461472605912]
	TIME [epoch: 8.2 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03629582619689457		[learning rate: 0.0003031]
		[batch 20/20] avg loss: 0.058697090734293966		[learning rate: 0.00030274]
	Learning Rate: 0.000302745
	LOSS [training: 0.04749645846559428 | validation: 0.01383079371718949]
	TIME [epoch: 8.19 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04687881780479998		[learning rate: 0.00030239]
		[batch 20/20] avg loss: 0.03310416871318177		[learning rate: 0.00030203]
	Learning Rate: 0.000302031
	LOSS [training: 0.03999149325899088 | validation: 0.01303761455936368]
	TIME [epoch: 8.18 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042781252330349594		[learning rate: 0.00030167]
		[batch 20/20] avg loss: 0.03751216258267887		[learning rate: 0.00030132]
	Learning Rate: 0.000301318
	LOSS [training: 0.04014670745651423 | validation: 0.02136490234190488]
	TIME [epoch: 8.21 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03997926077212459		[learning rate: 0.00030096]
		[batch 20/20] avg loss: 0.039664364657657825		[learning rate: 0.00030061]
	Learning Rate: 0.000300608
	LOSS [training: 0.039821812714891214 | validation: 0.006979646124321011]
	TIME [epoch: 8.19 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057497643325110905		[learning rate: 0.00030025]
		[batch 20/20] avg loss: 0.04385245774161546		[learning rate: 0.0002999]
	Learning Rate: 0.000299899
	LOSS [training: 0.050675050533363186 | validation: 0.023033946495795855]
	TIME [epoch: 8.19 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05838960011286144		[learning rate: 0.00029954]
		[batch 20/20] avg loss: 0.04613467872238327		[learning rate: 0.00029919]
	Learning Rate: 0.000299191
	LOSS [training: 0.05226213941762235 | validation: 0.024617266275517115]
	TIME [epoch: 8.17 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04953026927742852		[learning rate: 0.00029884]
		[batch 20/20] avg loss: 0.047260849805071406		[learning rate: 0.00029849]
	Learning Rate: 0.000298485
	LOSS [training: 0.04839555954124998 | validation: 0.017967649045348116]
	TIME [epoch: 8.22 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050776195065795435		[learning rate: 0.00029813]
		[batch 20/20] avg loss: 0.043364952792240466		[learning rate: 0.00029778]
	Learning Rate: 0.000297781
	LOSS [training: 0.04707057392901794 | validation: 0.0138693795542479]
	TIME [epoch: 8.18 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047138783364880164		[learning rate: 0.00029743]
		[batch 20/20] avg loss: 0.04341515747245241		[learning rate: 0.00029708]
	Learning Rate: 0.000297079
	LOSS [training: 0.04527697041866629 | validation: 0.012883763796852108]
	TIME [epoch: 8.19 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044996257787038016		[learning rate: 0.00029673]
		[batch 20/20] avg loss: 0.04620143625496302		[learning rate: 0.00029638]
	Learning Rate: 0.000296378
	LOSS [training: 0.04559884702100052 | validation: 0.024792566305016753]
	TIME [epoch: 8.18 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04810525521548887		[learning rate: 0.00029603]
		[batch 20/20] avg loss: 0.04721492532189682		[learning rate: 0.00029568]
	Learning Rate: 0.000295679
	LOSS [training: 0.04766009026869285 | validation: 0.012222138007272947]
	TIME [epoch: 8.21 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050573917117930786		[learning rate: 0.00029533]
		[batch 20/20] avg loss: 0.03826177425163462		[learning rate: 0.00029498]
	Learning Rate: 0.000294982
	LOSS [training: 0.0444178456847827 | validation: 0.03548700064281374]
	TIME [epoch: 8.17 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0474063641665874		[learning rate: 0.00029463]
		[batch 20/20] avg loss: 0.045729095051272914		[learning rate: 0.00029429]
	Learning Rate: 0.000294286
	LOSS [training: 0.04656772960893016 | validation: 0.01631843477797957]
	TIME [epoch: 8.21 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04348236972759624		[learning rate: 0.00029394]
		[batch 20/20] avg loss: 0.04539699109640203		[learning rate: 0.00029359]
	Learning Rate: 0.000293592
	LOSS [training: 0.04443968041199914 | validation: 0.016236077673259672]
	TIME [epoch: 8.19 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06324346162033437		[learning rate: 0.00029325]
		[batch 20/20] avg loss: 0.048153529999128954		[learning rate: 0.0002929]
	Learning Rate: 0.000292899
	LOSS [training: 0.055698495809731664 | validation: 0.037262556606933624]
	TIME [epoch: 8.2 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052678779220056794		[learning rate: 0.00029255]
		[batch 20/20] avg loss: 0.0652989342748338		[learning rate: 0.00029221]
	Learning Rate: 0.000292208
	LOSS [training: 0.05898885674744529 | validation: 0.0206983428195447]
	TIME [epoch: 8.17 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04273571490981509		[learning rate: 0.00029186]
		[batch 20/20] avg loss: 0.04452222666735007		[learning rate: 0.00029152]
	Learning Rate: 0.000291519
	LOSS [training: 0.043628970788582584 | validation: 0.014661444387517757]
	TIME [epoch: 8.21 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04413191440492993		[learning rate: 0.00029117]
		[batch 20/20] avg loss: 0.05342846101340848		[learning rate: 0.00029083]
	Learning Rate: 0.000290831
	LOSS [training: 0.04878018770916921 | validation: 0.017091313810671777]
	TIME [epoch: 8.2 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032736351123391565		[learning rate: 0.00029049]
		[batch 20/20] avg loss: 0.04880175080229131		[learning rate: 0.00029015]
	Learning Rate: 0.000290145
	LOSS [training: 0.04076905096284145 | validation: 0.008079808169859162]
	TIME [epoch: 8.2 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05244307971014948		[learning rate: 0.0002898]
		[batch 20/20] avg loss: 0.042032582622854794		[learning rate: 0.00028946]
	Learning Rate: 0.000289461
	LOSS [training: 0.047237831166502126 | validation: 0.014257084008859566]
	TIME [epoch: 8.18 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04504707092673646		[learning rate: 0.00028912]
		[batch 20/20] avg loss: 0.055682121290976314		[learning rate: 0.00028878]
	Learning Rate: 0.000288778
	LOSS [training: 0.050364596108856394 | validation: 0.02104447990727471]
	TIME [epoch: 8.2 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04040390559313583		[learning rate: 0.00028844]
		[batch 20/20] avg loss: 0.04630830156212846		[learning rate: 0.0002881]
	Learning Rate: 0.000288097
	LOSS [training: 0.04335610357763213 | validation: 0.019693175053646244]
	TIME [epoch: 8.2 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044650800533771504		[learning rate: 0.00028776]
		[batch 20/20] avg loss: 0.045712563316703425		[learning rate: 0.00028742]
	Learning Rate: 0.000287417
	LOSS [training: 0.045181681925237464 | validation: 0.009635265742462163]
	TIME [epoch: 8.2 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029602292975080612		[learning rate: 0.00028708]
		[batch 20/20] avg loss: 0.046908643788759224		[learning rate: 0.00028674]
	Learning Rate: 0.000286739
	LOSS [training: 0.03825546838191993 | validation: 0.010548843836470617]
	TIME [epoch: 8.18 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04584381156889659		[learning rate: 0.0002864]
		[batch 20/20] avg loss: 0.04475120902369504		[learning rate: 0.00028606]
	Learning Rate: 0.000286063
	LOSS [training: 0.04529751029629582 | validation: 0.03274998943643258]
	TIME [epoch: 8.19 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04846658362328196		[learning rate: 0.00028573]
		[batch 20/20] avg loss: 0.04757775220136536		[learning rate: 0.00028539]
	Learning Rate: 0.000285388
	LOSS [training: 0.048022167912323666 | validation: 0.015207623241728967]
	TIME [epoch: 8.21 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04086337526805525		[learning rate: 0.00028505]
		[batch 20/20] avg loss: 0.04734452861334385		[learning rate: 0.00028471]
	Learning Rate: 0.000284715
	LOSS [training: 0.04410395194069956 | validation: 0.015296783062999282]
	TIME [epoch: 8.17 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0421543196680905		[learning rate: 0.00028438]
		[batch 20/20] avg loss: 0.04504794973798209		[learning rate: 0.00028404]
	Learning Rate: 0.000284043
	LOSS [training: 0.043601134703036296 | validation: 0.017885429080178636]
	TIME [epoch: 8.18 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0494667072142872		[learning rate: 0.00028371]
		[batch 20/20] avg loss: 0.04291586849868913		[learning rate: 0.00028337]
	Learning Rate: 0.000283373
	LOSS [training: 0.04619128785648816 | validation: 0.018411429983938965]
	TIME [epoch: 8.21 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05117870761822041		[learning rate: 0.00028304]
		[batch 20/20] avg loss: 0.05019881434843802		[learning rate: 0.0002827]
	Learning Rate: 0.000282705
	LOSS [training: 0.05068876098332923 | validation: 0.009628885963092932]
	TIME [epoch: 8.22 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037096595153728304		[learning rate: 0.00028237]
		[batch 20/20] avg loss: 0.043959829217770405		[learning rate: 0.00028204]
	Learning Rate: 0.000282038
	LOSS [training: 0.04052821218574935 | validation: 0.02812859773262032]
	TIME [epoch: 8.18 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059688226478856965		[learning rate: 0.00028171]
		[batch 20/20] avg loss: 0.04720634427234691		[learning rate: 0.00028137]
	Learning Rate: 0.000281373
	LOSS [training: 0.053447285375601936 | validation: 0.04561428976876117]
	TIME [epoch: 8.18 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05413962385769564		[learning rate: 0.00028104]
		[batch 20/20] avg loss: 0.047640531314890336		[learning rate: 0.00028071]
	Learning Rate: 0.000280709
	LOSS [training: 0.05089007758629299 | validation: 0.03872165199698226]
	TIME [epoch: 8.2 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047829995220407436		[learning rate: 0.00028038]
		[batch 20/20] avg loss: 0.04026702558839994		[learning rate: 0.00028005]
	Learning Rate: 0.000280047
	LOSS [training: 0.04404851040440368 | validation: 0.017424647300967613]
	TIME [epoch: 8.22 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057374018131995365		[learning rate: 0.00027972]
		[batch 20/20] avg loss: 0.04545302147175871		[learning rate: 0.00027939]
	Learning Rate: 0.000279386
	LOSS [training: 0.05141351980187703 | validation: 0.0281763832497578]
	TIME [epoch: 8.17 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044728160707055674		[learning rate: 0.00027906]
		[batch 20/20] avg loss: 0.05624834979300336		[learning rate: 0.00027873]
	Learning Rate: 0.000278727
	LOSS [training: 0.050488255250029514 | validation: 0.00774465839910401]
	TIME [epoch: 8.18 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03737040511819008		[learning rate: 0.0002784]
		[batch 20/20] avg loss: 0.04672084479239486		[learning rate: 0.00027807]
	Learning Rate: 0.00027807
	LOSS [training: 0.042045624955292474 | validation: 0.02852566761328037]
	TIME [epoch: 8.21 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04159491403152249		[learning rate: 0.00027774]
		[batch 20/20] avg loss: 0.0551972608464121		[learning rate: 0.00027741]
	Learning Rate: 0.000277414
	LOSS [training: 0.0483960874389673 | validation: 0.01599140104482198]
	TIME [epoch: 8.22 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039529818100118394		[learning rate: 0.00027709]
		[batch 20/20] avg loss: 0.04303908006791806		[learning rate: 0.00027676]
	Learning Rate: 0.000276759
	LOSS [training: 0.041284449084018224 | validation: 0.03869811345729963]
	TIME [epoch: 8.17 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04912237428060999		[learning rate: 0.00027643]
		[batch 20/20] avg loss: 0.044287648704384674		[learning rate: 0.00027611]
	Learning Rate: 0.000276107
	LOSS [training: 0.046705011492497325 | validation: 0.01684998947217413]
	TIME [epoch: 8.17 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04276398464926344		[learning rate: 0.00027578]
		[batch 20/20] avg loss: 0.04659281048757412		[learning rate: 0.00027546]
	Learning Rate: 0.000275455
	LOSS [training: 0.04467839756841877 | validation: 0.04966069716393028]
	TIME [epoch: 8.2 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04611651991759343		[learning rate: 0.00027513]
		[batch 20/20] avg loss: 0.040670756486885624		[learning rate: 0.00027481]
	Learning Rate: 0.000274806
	LOSS [training: 0.04339363820223953 | validation: 0.026711447554776557]
	TIME [epoch: 8.21 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058431010835743966		[learning rate: 0.00027448]
		[batch 20/20] avg loss: 0.0494835186377864		[learning rate: 0.00027416]
	Learning Rate: 0.000274157
	LOSS [training: 0.053957264736765174 | validation: 0.01766945610214679]
	TIME [epoch: 8.17 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04841834664077077		[learning rate: 0.00027383]
		[batch 20/20] avg loss: 0.03452390204084827		[learning rate: 0.00027351]
	Learning Rate: 0.000273511
	LOSS [training: 0.04147112434080952 | validation: 0.029839300834793667]
	TIME [epoch: 8.18 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041197924878573836		[learning rate: 0.00027319]
		[batch 20/20] avg loss: 0.0333936377538443		[learning rate: 0.00027287]
	Learning Rate: 0.000272866
	LOSS [training: 0.03729578131620907 | validation: 0.022470589263846548]
	TIME [epoch: 8.22 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04102948863822335		[learning rate: 0.00027254]
		[batch 20/20] avg loss: 0.05244034801777571		[learning rate: 0.00027222]
	Learning Rate: 0.000272222
	LOSS [training: 0.04673491832799953 | validation: 0.016225221438164614]
	TIME [epoch: 8.2 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037180146090801704		[learning rate: 0.0002719]
		[batch 20/20] avg loss: 0.04956026466679348		[learning rate: 0.00027158]
	Learning Rate: 0.00027158
	LOSS [training: 0.043370205378797586 | validation: 0.020383159370141703]
	TIME [epoch: 8.16 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046305289553307274		[learning rate: 0.00027126]
		[batch 20/20] avg loss: 0.04180809558381768		[learning rate: 0.00027094]
	Learning Rate: 0.000270939
	LOSS [training: 0.04405669256856247 | validation: 0.01136073136388027]
	TIME [epoch: 8.17 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03562724343221405		[learning rate: 0.00027062]
		[batch 20/20] avg loss: 0.04955202079520166		[learning rate: 0.0002703]
	Learning Rate: 0.0002703
	LOSS [training: 0.04258963211370785 | validation: 0.01898545279684514]
	TIME [epoch: 8.22 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05513471284229295		[learning rate: 0.00026998]
		[batch 20/20] avg loss: 0.04592737389984045		[learning rate: 0.00026966]
	Learning Rate: 0.000269662
	LOSS [training: 0.050531043371066696 | validation: 0.012995023827713007]
	TIME [epoch: 8.2 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041471867314496		[learning rate: 0.00026934]
		[batch 20/20] avg loss: 0.04696713182313457		[learning rate: 0.00026903]
	Learning Rate: 0.000269026
	LOSS [training: 0.04421949956881528 | validation: 0.013176359556603914]
	TIME [epoch: 8.18 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038866731612683454		[learning rate: 0.00026871]
		[batch 20/20] avg loss: 0.053711459251846325		[learning rate: 0.00026839]
	Learning Rate: 0.000268392
	LOSS [training: 0.04628909543226489 | validation: 0.013691133561032822]
	TIME [epoch: 8.17 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04406468026462316		[learning rate: 0.00026808]
		[batch 20/20] avg loss: 0.05981555146091423		[learning rate: 0.00026776]
	Learning Rate: 0.000267759
	LOSS [training: 0.0519401158627687 | validation: 0.0260575567589075]
	TIME [epoch: 8.24 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026112990448401423		[learning rate: 0.00026744]
		[batch 20/20] avg loss: 0.053174205414666866		[learning rate: 0.00026713]
	Learning Rate: 0.000267127
	LOSS [training: 0.039643597931534144 | validation: 0.01530684134339489]
	TIME [epoch: 8.18 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042608467516576896		[learning rate: 0.00026681]
		[batch 20/20] avg loss: 0.06841193423489315		[learning rate: 0.0002665]
	Learning Rate: 0.000266497
	LOSS [training: 0.05551020087573503 | validation: 0.022594036235491787]
	TIME [epoch: 8.17 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036757571728644786		[learning rate: 0.00026618]
		[batch 20/20] avg loss: 0.04209919158643331		[learning rate: 0.00026587]
	Learning Rate: 0.000265868
	LOSS [training: 0.039428381657539056 | validation: 0.02184471742310465]
	TIME [epoch: 8.17 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04360310977936749		[learning rate: 0.00026555]
		[batch 20/20] avg loss: 0.046393037719016775		[learning rate: 0.00026524]
	Learning Rate: 0.000265241
	LOSS [training: 0.04499807374919213 | validation: 0.04201030545348419]
	TIME [epoch: 8.23 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04513331049150515		[learning rate: 0.00026493]
		[batch 20/20] avg loss: 0.05441092924044851		[learning rate: 0.00026462]
	Learning Rate: 0.000264616
	LOSS [training: 0.049772119865976835 | validation: 0.010326118126213663]
	TIME [epoch: 8.18 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05118231861372226		[learning rate: 0.0002643]
		[batch 20/20] avg loss: 0.04066764233608569		[learning rate: 0.00026399]
	Learning Rate: 0.000263991
	LOSS [training: 0.045924980474903974 | validation: 0.021277644443262574]
	TIME [epoch: 8.18 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04966593254705444		[learning rate: 0.00026368]
		[batch 20/20] avg loss: 0.040362378764511694		[learning rate: 0.00026337]
	Learning Rate: 0.000263369
	LOSS [training: 0.04501415565578307 | validation: 0.010833735849202157]
	TIME [epoch: 8.18 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03999322695410851		[learning rate: 0.00026306]
		[batch 20/20] avg loss: 0.03955698521273866		[learning rate: 0.00026275]
	Learning Rate: 0.000262747
	LOSS [training: 0.03977510608342359 | validation: 0.02204100950844616]
	TIME [epoch: 8.25 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04253334018494487		[learning rate: 0.00026244]
		[batch 20/20] avg loss: 0.04718453242347143		[learning rate: 0.00026213]
	Learning Rate: 0.000262128
	LOSS [training: 0.04485893630420816 | validation: 0.025234063445329927]
	TIME [epoch: 8.18 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06078442587613682		[learning rate: 0.00026182]
		[batch 20/20] avg loss: 0.04016303112972973		[learning rate: 0.00026151]
	Learning Rate: 0.000261509
	LOSS [training: 0.05047372850293327 | validation: 0.016384735030189782]
	TIME [epoch: 8.18 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040283292850399666		[learning rate: 0.0002612]
		[batch 20/20] avg loss: 0.04403362026659126		[learning rate: 0.00026089]
	Learning Rate: 0.000260892
	LOSS [training: 0.04215845655849546 | validation: 0.03935149059329204]
	TIME [epoch: 8.18 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05107222222743735		[learning rate: 0.00026058]
		[batch 20/20] avg loss: 0.03646691202171919		[learning rate: 0.00026028]
	Learning Rate: 0.000260277
	LOSS [training: 0.043769567124578274 | validation: 0.012980921503621574]
	TIME [epoch: 8.24 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04147355850004912		[learning rate: 0.00025997]
		[batch 20/20] avg loss: 0.04241072322167218		[learning rate: 0.00025966]
	Learning Rate: 0.000259663
	LOSS [training: 0.04194214086086066 | validation: 0.017720966901934154]
	TIME [epoch: 8.19 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05052830112163141		[learning rate: 0.00025936]
		[batch 20/20] avg loss: 0.05028612015533758		[learning rate: 0.00025905]
	Learning Rate: 0.000259051
	LOSS [training: 0.0504072106384845 | validation: 0.02888941905462759]
	TIME [epoch: 8.18 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05253578930782203		[learning rate: 0.00025874]
		[batch 20/20] avg loss: 0.0418356135459881		[learning rate: 0.00025844]
	Learning Rate: 0.00025844
	LOSS [training: 0.04718570142690508 | validation: 0.014216360327850213]
	TIME [epoch: 8.17 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03883791642260309		[learning rate: 0.00025813]
		[batch 20/20] avg loss: 0.047195480766040146		[learning rate: 0.00025783]
	Learning Rate: 0.00025783
	LOSS [training: 0.043016698594321626 | validation: 0.019249512504619964]
	TIME [epoch: 8.26 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03783540625379643		[learning rate: 0.00025753]
		[batch 20/20] avg loss: 0.0387379014338866		[learning rate: 0.00025722]
	Learning Rate: 0.000257222
	LOSS [training: 0.038286653843841514 | validation: 0.028055150695485764]
	TIME [epoch: 8.18 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038751703098433364		[learning rate: 0.00025692]
		[batch 20/20] avg loss: 0.043842444824631545		[learning rate: 0.00025662]
	Learning Rate: 0.000256615
	LOSS [training: 0.041297073961532454 | validation: 0.01540406744562843]
	TIME [epoch: 8.18 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03691893027668217		[learning rate: 0.00025631]
		[batch 20/20] avg loss: 0.03904236770603561		[learning rate: 0.00025601]
	Learning Rate: 0.00025601
	LOSS [training: 0.0379806489913589 | validation: 0.013778296019361664]
	TIME [epoch: 8.19 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044569032652365434		[learning rate: 0.00025571]
		[batch 20/20] avg loss: 0.047732686978973135		[learning rate: 0.00025541]
	Learning Rate: 0.000255406
	LOSS [training: 0.04615085981566928 | validation: 0.019792965030005792]
	TIME [epoch: 8.23 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04530534857449964		[learning rate: 0.0002551]
		[batch 20/20] avg loss: 0.04730594447381792		[learning rate: 0.0002548]
	Learning Rate: 0.000254803
	LOSS [training: 0.04630564652415878 | validation: 0.022408242112167444]
	TIME [epoch: 8.19 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0410251437726646		[learning rate: 0.0002545]
		[batch 20/20] avg loss: 0.04427990500768129		[learning rate: 0.0002542]
	Learning Rate: 0.000254202
	LOSS [training: 0.04265252439017295 | validation: 0.01192883549149693]
	TIME [epoch: 8.18 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037934057743552255		[learning rate: 0.0002539]
		[batch 20/20] avg loss: 0.04428253579256701		[learning rate: 0.0002536]
	Learning Rate: 0.000253603
	LOSS [training: 0.041108296768059625 | validation: 0.020341570218085726]
	TIME [epoch: 8.2 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036057935943941376		[learning rate: 0.0002533]
		[batch 20/20] avg loss: 0.036568564424587155		[learning rate: 0.000253]
	Learning Rate: 0.000253004
	LOSS [training: 0.03631325018426426 | validation: 0.031089611877500638]
	TIME [epoch: 8.22 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045928743066685616		[learning rate: 0.00025271]
		[batch 20/20] avg loss: 0.039503023956099156		[learning rate: 0.00025241]
	Learning Rate: 0.000252408
	LOSS [training: 0.04271588351139239 | validation: 0.006890847954110592]
	TIME [epoch: 8.18 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03743572987067654		[learning rate: 0.00025211]
		[batch 20/20] avg loss: 0.04054456047041133		[learning rate: 0.00025181]
	Learning Rate: 0.000251812
	LOSS [training: 0.03899014517054393 | validation: 0.02007068819828176]
	TIME [epoch: 8.17 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038807464419175715		[learning rate: 0.00025152]
		[batch 20/20] avg loss: 0.03872857888314949		[learning rate: 0.00025122]
	Learning Rate: 0.000251218
	LOSS [training: 0.0387680216511626 | validation: 0.01993796427827991]
	TIME [epoch: 8.2 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04349717542964378		[learning rate: 0.00025092]
		[batch 20/20] avg loss: 0.039009001596981785		[learning rate: 0.00025063]
	Learning Rate: 0.000250626
	LOSS [training: 0.04125308851331279 | validation: 0.018663096811040253]
	TIME [epoch: 8.22 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053359798535532056		[learning rate: 0.00025033]
		[batch 20/20] avg loss: 0.04969689838208282		[learning rate: 0.00025003]
	Learning Rate: 0.000250035
	LOSS [training: 0.05152834845880745 | validation: 0.012493493378516985]
	TIME [epoch: 8.19 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04552530821035688		[learning rate: 0.00024974]
		[batch 20/20] avg loss: 0.041303951363598115		[learning rate: 0.00024944]
	Learning Rate: 0.000249445
	LOSS [training: 0.04341462978697749 | validation: 0.008080101642351682]
	TIME [epoch: 8.17 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031502451478542		[learning rate: 0.00024915]
		[batch 20/20] avg loss: 0.04207687099715907		[learning rate: 0.00024886]
	Learning Rate: 0.000248856
	LOSS [training: 0.03678966123785053 | validation: 0.03631934186075167]
	TIME [epoch: 8.21 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03490167055090039		[learning rate: 0.00024856]
		[batch 20/20] avg loss: 0.040565796807854396		[learning rate: 0.00024827]
	Learning Rate: 0.000248269
	LOSS [training: 0.03773373367937739 | validation: 0.01660113671316773]
	TIME [epoch: 8.2 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052507041333873336		[learning rate: 0.00024798]
		[batch 20/20] avg loss: 0.038476264892762777		[learning rate: 0.00024768]
	Learning Rate: 0.000247684
	LOSS [training: 0.045491653113318056 | validation: 0.011574272236389151]
	TIME [epoch: 8.19 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03602000873921456		[learning rate: 0.00024739]
		[batch 20/20] avg loss: 0.04135635549972618		[learning rate: 0.0002471]
	Learning Rate: 0.000247099
	LOSS [training: 0.038688182119470366 | validation: 0.027262078016145667]
	TIME [epoch: 8.18 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04392196042268505		[learning rate: 0.00024681]
		[batch 20/20] avg loss: 0.05062296328337808		[learning rate: 0.00024652]
	Learning Rate: 0.000246517
	LOSS [training: 0.047272461853031564 | validation: 0.059654140239945155]
	TIME [epoch: 8.22 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06336445948546136		[learning rate: 0.00024623]
		[batch 20/20] avg loss: 0.03404937389974433		[learning rate: 0.00024594]
	Learning Rate: 0.000245935
	LOSS [training: 0.048706916692602846 | validation: 0.008127865580136043]
	TIME [epoch: 8.2 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045107539163563595		[learning rate: 0.00024564]
		[batch 20/20] avg loss: 0.040345706445104594		[learning rate: 0.00024535]
	Learning Rate: 0.000245355
	LOSS [training: 0.0427266228043341 | validation: 0.01687692987889934]
	TIME [epoch: 8.2 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04211039480603544		[learning rate: 0.00024507]
		[batch 20/20] avg loss: 0.03080552001723564		[learning rate: 0.00024478]
	Learning Rate: 0.000244776
	LOSS [training: 0.03645795741163554 | validation: 0.014732347292445606]
	TIME [epoch: 8.18 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03209031264955779		[learning rate: 0.00024449]
		[batch 20/20] avg loss: 0.04711676679489976		[learning rate: 0.0002442]
	Learning Rate: 0.000244199
	LOSS [training: 0.03960353972222878 | validation: 0.016870910278649003]
	TIME [epoch: 8.21 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04126254986652824		[learning rate: 0.00024391]
		[batch 20/20] avg loss: 0.03859034114252359		[learning rate: 0.00024362]
	Learning Rate: 0.000243623
	LOSS [training: 0.03992644550452591 | validation: 0.011822821233885492]
	TIME [epoch: 8.19 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037795820284215464		[learning rate: 0.00024334]
		[batch 20/20] avg loss: 0.03978125355996604		[learning rate: 0.00024305]
	Learning Rate: 0.000243048
	LOSS [training: 0.03878853692209075 | validation: 0.020055788657440003]
	TIME [epoch: 8.19 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05063389802249767		[learning rate: 0.00024276]
		[batch 20/20] avg loss: 0.04603419537993442		[learning rate: 0.00024247]
	Learning Rate: 0.000242475
	LOSS [training: 0.04833404670121604 | validation: 0.00806245233277127]
	TIME [epoch: 8.18 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05820945562597975		[learning rate: 0.00024219]
		[batch 20/20] avg loss: 0.044292824172557296		[learning rate: 0.0002419]
	Learning Rate: 0.000241903
	LOSS [training: 0.05125113989926853 | validation: 0.026534440289857526]
	TIME [epoch: 8.22 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04597360959367727		[learning rate: 0.00024162]
		[batch 20/20] avg loss: 0.046852679537518385		[learning rate: 0.00024133]
	Learning Rate: 0.000241332
	LOSS [training: 0.04641314456559783 | validation: 0.020926791497375065]
	TIME [epoch: 8.2 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033334601549618356		[learning rate: 0.00024105]
		[batch 20/20] avg loss: 0.045093674431272324		[learning rate: 0.00024076]
	Learning Rate: 0.000240763
	LOSS [training: 0.03921413799044534 | validation: 0.010618896112810266]
	TIME [epoch: 8.19 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04397116255598975		[learning rate: 0.00024048]
		[batch 20/20] avg loss: 0.04290862051854421		[learning rate: 0.0002402]
	Learning Rate: 0.000240195
	LOSS [training: 0.04343989153726699 | validation: 0.018590784485522392]
	TIME [epoch: 8.18 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04553989903286317		[learning rate: 0.00023991]
		[batch 20/20] avg loss: 0.03643660549300839		[learning rate: 0.00023963]
	Learning Rate: 0.000239628
	LOSS [training: 0.04098825226293578 | validation: 0.010134228393694537]
	TIME [epoch: 8.22 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04831807983954476		[learning rate: 0.00023935]
		[batch 20/20] avg loss: 0.04278317355849256		[learning rate: 0.00023906]
	Learning Rate: 0.000239063
	LOSS [training: 0.045550626699018666 | validation: 0.012955541124671954]
	TIME [epoch: 8.18 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040916002938930204		[learning rate: 0.00023878]
		[batch 20/20] avg loss: 0.041994407982733614		[learning rate: 0.0002385]
	Learning Rate: 0.000238499
	LOSS [training: 0.04145520546083191 | validation: 0.016880729077856888]
	TIME [epoch: 8.19 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055767354409084015		[learning rate: 0.00023822]
		[batch 20/20] avg loss: 0.04732515676134533		[learning rate: 0.00023794]
	Learning Rate: 0.000237937
	LOSS [training: 0.051546255585214674 | validation: 0.02380004630330625]
	TIME [epoch: 8.17 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04593951553475973		[learning rate: 0.00023766]
		[batch 20/20] avg loss: 0.04360453580289751		[learning rate: 0.00023738]
	Learning Rate: 0.000237375
	LOSS [training: 0.044772025668828624 | validation: 0.021907174645150837]
	TIME [epoch: 8.22 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04178976773583173		[learning rate: 0.0002371]
		[batch 20/20] avg loss: 0.04403183085922528		[learning rate: 0.00023682]
	Learning Rate: 0.000236816
	LOSS [training: 0.042910799297528494 | validation: 0.019783879421161665]
	TIME [epoch: 8.17 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044423650613928875		[learning rate: 0.00023654]
		[batch 20/20] avg loss: 0.0432324456224577		[learning rate: 0.00023626]
	Learning Rate: 0.000236257
	LOSS [training: 0.04382804811819328 | validation: 0.007394253366017994]
	TIME [epoch: 8.2 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04561703458023968		[learning rate: 0.00023598]
		[batch 20/20] avg loss: 0.03597094863634337		[learning rate: 0.0002357]
	Learning Rate: 0.0002357
	LOSS [training: 0.04079399160829153 | validation: 0.011564169005761701]
	TIME [epoch: 8.19 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04489381432510194		[learning rate: 0.00023542]
		[batch 20/20] avg loss: 0.03836409826568819		[learning rate: 0.00023514]
	Learning Rate: 0.000235144
	LOSS [training: 0.04162895629539507 | validation: 0.017324031742747957]
	TIME [epoch: 8.21 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045043127547607906		[learning rate: 0.00023487]
		[batch 20/20] avg loss: 0.03595321418851385		[learning rate: 0.00023459]
	Learning Rate: 0.000234589
	LOSS [training: 0.04049817086806087 | validation: 0.01736760422474993]
	TIME [epoch: 8.17 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046920025153800005		[learning rate: 0.00023431]
		[batch 20/20] avg loss: 0.041928091325188946		[learning rate: 0.00023404]
	Learning Rate: 0.000234036
	LOSS [training: 0.044424058239494475 | validation: 0.01730436557573146]
	TIME [epoch: 8.19 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03276084743732051		[learning rate: 0.00023376]
		[batch 20/20] avg loss: 0.04389269954217918		[learning rate: 0.00023348]
	Learning Rate: 0.000233484
	LOSS [training: 0.038326773489749846 | validation: 0.03898739232050624]
	TIME [epoch: 8.19 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051718631137687855		[learning rate: 0.00023321]
		[batch 20/20] avg loss: 0.04063187438007735		[learning rate: 0.00023293]
	Learning Rate: 0.000232933
	LOSS [training: 0.04617525275888261 | validation: 0.026163812539330287]
	TIME [epoch: 8.2 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04053782392278828		[learning rate: 0.00023266]
		[batch 20/20] avg loss: 0.039240126335481726		[learning rate: 0.00023238]
	Learning Rate: 0.000232383
	LOSS [training: 0.039888975129135 | validation: 0.022075005317590393]
	TIME [epoch: 8.17 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04143741984060944		[learning rate: 0.00023211]
		[batch 20/20] avg loss: 0.0346097524451479		[learning rate: 0.00023184]
	Learning Rate: 0.000231835
	LOSS [training: 0.03802358614287867 | validation: 0.008295464121723155]
	TIME [epoch: 8.2 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03598676049861132		[learning rate: 0.00023156]
		[batch 20/20] avg loss: 0.03537966801059604		[learning rate: 0.00023129]
	Learning Rate: 0.000231288
	LOSS [training: 0.035683214254603676 | validation: 0.0151484878189116]
	TIME [epoch: 8.2 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04405816944884801		[learning rate: 0.00023102]
		[batch 20/20] avg loss: 0.041045136358478077		[learning rate: 0.00023074]
	Learning Rate: 0.000230743
	LOSS [training: 0.04255165290366304 | validation: 0.018252193294473012]
	TIME [epoch: 8.2 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041942337055766034		[learning rate: 0.00023047]
		[batch 20/20] avg loss: 0.03886391364577895		[learning rate: 0.0002302]
	Learning Rate: 0.000230198
	LOSS [training: 0.04040312535077249 | validation: 0.02430000130359139]
	TIME [epoch: 8.17 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045097152571405605		[learning rate: 0.00022993]
		[batch 20/20] avg loss: 0.040279474905963286		[learning rate: 0.00022966]
	Learning Rate: 0.000229656
	LOSS [training: 0.04268831373868444 | validation: 0.02840487019762363]
	TIME [epoch: 8.19 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04209341540106175		[learning rate: 0.00022938]
		[batch 20/20] avg loss: 0.04580537691982052		[learning rate: 0.00022911]
	Learning Rate: 0.000229114
	LOSS [training: 0.04394939616044113 | validation: 0.015866528343849405]
	TIME [epoch: 8.2 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035076264897451234		[learning rate: 0.00022884]
		[batch 20/20] avg loss: 0.04167428139127784		[learning rate: 0.00022857]
	Learning Rate: 0.000228573
	LOSS [training: 0.03837527314436454 | validation: 0.018004175214855672]
	TIME [epoch: 8.21 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03344305504102195		[learning rate: 0.0002283]
		[batch 20/20] avg loss: 0.04476562074506141		[learning rate: 0.00022803]
	Learning Rate: 0.000228034
	LOSS [training: 0.039104337893041685 | validation: 0.012860958545151378]
	TIME [epoch: 8.18 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03608688102978837		[learning rate: 0.00022777]
		[batch 20/20] avg loss: 0.040568698387398565		[learning rate: 0.0002275]
	Learning Rate: 0.000227496
	LOSS [training: 0.03832778970859347 | validation: 0.014235967538424652]
	TIME [epoch: 8.2 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037661912146707756		[learning rate: 0.00022723]
		[batch 20/20] avg loss: 0.03903803288059883		[learning rate: 0.00022696]
	Learning Rate: 0.00022696
	LOSS [training: 0.03834997251365329 | validation: 0.01323283873140417]
	TIME [epoch: 8.2 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038137015940856014		[learning rate: 0.00022669]
		[batch 20/20] avg loss: 0.03424054550412123		[learning rate: 0.00022642]
	Learning Rate: 0.000226424
	LOSS [training: 0.03618878072248862 | validation: 0.008521980785542005]
	TIME [epoch: 8.19 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04422523067252958		[learning rate: 0.00022616]
		[batch 20/20] avg loss: 0.03650079504765264		[learning rate: 0.00022589]
	Learning Rate: 0.00022589
	LOSS [training: 0.040363012860091106 | validation: 0.012695620336103611]
	TIME [epoch: 8.18 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041881458487690336		[learning rate: 0.00022562]
		[batch 20/20] avg loss: 0.042847800506189845		[learning rate: 0.00022536]
	Learning Rate: 0.000225357
	LOSS [training: 0.042364629496940094 | validation: 0.01370392082228672]
	TIME [epoch: 8.2 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043558050302446846		[learning rate: 0.00022509]
		[batch 20/20] avg loss: 0.035967282331320224		[learning rate: 0.00022483]
	Learning Rate: 0.000224826
	LOSS [training: 0.03976266631688353 | validation: 0.01679221212985005]
	TIME [epoch: 8.21 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03449284484206161		[learning rate: 0.00022456]
		[batch 20/20] avg loss: 0.040273428087608566		[learning rate: 0.0002243]
	Learning Rate: 0.000224295
	LOSS [training: 0.037383136464835084 | validation: 0.01730355104850622]
	TIME [epoch: 8.19 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03827050773397116		[learning rate: 0.00022403]
		[batch 20/20] avg loss: 0.05943410824558425		[learning rate: 0.00022377]
	Learning Rate: 0.000223766
	LOSS [training: 0.0488523079897777 | validation: 0.02136900910679208]
	TIME [epoch: 8.17 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05064923197461958		[learning rate: 0.0002235]
		[batch 20/20] avg loss: 0.04672800755659835		[learning rate: 0.00022324]
	Learning Rate: 0.000223239
	LOSS [training: 0.04868861976560897 | validation: 0.027480496349528594]
	TIME [epoch: 8.21 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04419725753055994		[learning rate: 0.00022298]
		[batch 20/20] avg loss: 0.047970902675447805		[learning rate: 0.00022271]
	Learning Rate: 0.000222712
	LOSS [training: 0.04608408010300388 | validation: 0.012332171191100652]
	TIME [epoch: 8.22 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03495251911337637		[learning rate: 0.00022245]
		[batch 20/20] avg loss: 0.0351088608816303		[learning rate: 0.00022219]
	Learning Rate: 0.000222187
	LOSS [training: 0.035030689997503334 | validation: 0.016911333641151903]
	TIME [epoch: 8.18 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03336277550509685		[learning rate: 0.00022192]
		[batch 20/20] avg loss: 0.041762527417936256		[learning rate: 0.00022166]
	Learning Rate: 0.000221663
	LOSS [training: 0.037562651461516564 | validation: 0.004220788394014694]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_1663.pth
	Model improved!!!
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034469796489631485		[learning rate: 0.0002214]
		[batch 20/20] avg loss: 0.040266056129581326		[learning rate: 0.00022114]
	Learning Rate: 0.00022114
	LOSS [training: 0.037367926309606406 | validation: 0.013942619250946472]
	TIME [epoch: 8.2 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044767709089985536		[learning rate: 0.00022088]
		[batch 20/20] avg loss: 0.0358289358954088		[learning rate: 0.00022062]
	Learning Rate: 0.000220618
	LOSS [training: 0.040298322492697165 | validation: 0.018097271573543988]
	TIME [epoch: 8.22 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03618153368098551		[learning rate: 0.00022036]
		[batch 20/20] avg loss: 0.045955139252591176		[learning rate: 0.0002201]
	Learning Rate: 0.000220098
	LOSS [training: 0.04106833646678834 | validation: 0.017749480144059967]
	TIME [epoch: 8.17 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037448213791512745		[learning rate: 0.00021984]
		[batch 20/20] avg loss: 0.04173165819007203		[learning rate: 0.00021958]
	Learning Rate: 0.000219578
	LOSS [training: 0.03958993599079239 | validation: 0.01577226312376733]
	TIME [epoch: 8.18 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03334209483455457		[learning rate: 0.00021932]
		[batch 20/20] avg loss: 0.0440496685076162		[learning rate: 0.00021906]
	Learning Rate: 0.000219061
	LOSS [training: 0.03869588167108538 | validation: 0.02110143085621229]
	TIME [epoch: 8.21 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04924644516565545		[learning rate: 0.0002188]
		[batch 20/20] avg loss: 0.04915968926811456		[learning rate: 0.00021854]
	Learning Rate: 0.000218544
	LOSS [training: 0.04920306721688499 | validation: 0.029462314132142443]
	TIME [epoch: 8.21 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039757946692458176		[learning rate: 0.00021829]
		[batch 20/20] avg loss: 0.04650818725420639		[learning rate: 0.00021803]
	Learning Rate: 0.000218028
	LOSS [training: 0.04313306697333229 | validation: 0.038621018770806224]
	TIME [epoch: 8.17 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043544690000666894		[learning rate: 0.00021777]
		[batch 20/20] avg loss: 0.04790909873998593		[learning rate: 0.00021751]
	Learning Rate: 0.000217514
	LOSS [training: 0.04572689437032641 | validation: 0.016322517900631495]
	TIME [epoch: 8.17 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037945131405103624		[learning rate: 0.00021726]
		[batch 20/20] avg loss: 0.045955391462719736		[learning rate: 0.000217]
	Learning Rate: 0.000217001
	LOSS [training: 0.04195026143391167 | validation: 0.02401486433209423]
	TIME [epoch: 8.21 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05035130756432273		[learning rate: 0.00021674]
		[batch 20/20] avg loss: 0.03235160711890974		[learning rate: 0.00021649]
	Learning Rate: 0.000216489
	LOSS [training: 0.041351457341616236 | validation: 0.011038317443409485]
	TIME [epoch: 8.21 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035562579153396376		[learning rate: 0.00021623]
		[batch 20/20] avg loss: 0.038299816128799416		[learning rate: 0.00021598]
	Learning Rate: 0.000215978
	LOSS [training: 0.036931197641097896 | validation: 0.023412105666751323]
	TIME [epoch: 8.18 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038583017029462915		[learning rate: 0.00021572]
		[batch 20/20] avg loss: 0.03199158806537588		[learning rate: 0.00021547]
	Learning Rate: 0.000215469
	LOSS [training: 0.035287302547419394 | validation: 0.025280068785380208]
	TIME [epoch: 8.17 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0355568976319419		[learning rate: 0.00021521]
		[batch 20/20] avg loss: 0.048138759235970253		[learning rate: 0.00021496]
	Learning Rate: 0.000214961
	LOSS [training: 0.041847828433956076 | validation: 0.024232465223478793]
	TIME [epoch: 8.22 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03624964313892678		[learning rate: 0.00021471]
		[batch 20/20] avg loss: 0.04953674068383568		[learning rate: 0.00021445]
	Learning Rate: 0.000214454
	LOSS [training: 0.04289319191138122 | validation: 0.023412387872417863]
	TIME [epoch: 8.21 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048271567135979555		[learning rate: 0.0002142]
		[batch 20/20] avg loss: 0.03192662349038374		[learning rate: 0.00021395]
	Learning Rate: 0.000213948
	LOSS [training: 0.040099095313181655 | validation: 0.009065765744959825]
	TIME [epoch: 8.18 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038127491655778875		[learning rate: 0.0002137]
		[batch 20/20] avg loss: 0.03695905167393678		[learning rate: 0.00021344]
	Learning Rate: 0.000213443
	LOSS [training: 0.037543271664857825 | validation: 0.0218645184811741]
	TIME [epoch: 8.17 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052848506825122044		[learning rate: 0.00021319]
		[batch 20/20] avg loss: 0.04046940045016036		[learning rate: 0.00021294]
	Learning Rate: 0.00021294
	LOSS [training: 0.04665895363764121 | validation: 0.01896754643940652]
	TIME [epoch: 8.22 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04450372314122724		[learning rate: 0.00021269]
		[batch 20/20] avg loss: 0.034018467711236		[learning rate: 0.00021244]
	Learning Rate: 0.000212437
	LOSS [training: 0.03926109542623161 | validation: 0.01002097228680578]
	TIME [epoch: 8.2 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03275154633508559		[learning rate: 0.00021219]
		[batch 20/20] avg loss: 0.046158624369382464		[learning rate: 0.00021194]
	Learning Rate: 0.000211936
	LOSS [training: 0.039455085352234034 | validation: 0.00982877951591148]
	TIME [epoch: 8.17 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036676298053787956		[learning rate: 0.00021169]
		[batch 20/20] avg loss: 0.03865291184151916		[learning rate: 0.00021144]
	Learning Rate: 0.000211436
	LOSS [training: 0.03766460494765356 | validation: 0.012266466798958833]
	TIME [epoch: 8.17 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04563335579878526		[learning rate: 0.00021119]
		[batch 20/20] avg loss: 0.03406506315368754		[learning rate: 0.00021094]
	Learning Rate: 0.000210937
	LOSS [training: 0.03984920947623641 | validation: 0.013981505294596047]
	TIME [epoch: 8.23 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051047688052719906		[learning rate: 0.00021069]
		[batch 20/20] avg loss: 0.03347822672612112		[learning rate: 0.00021044]
	Learning Rate: 0.00021044
	LOSS [training: 0.042262957389420513 | validation: 0.022340475020795947]
	TIME [epoch: 8.2 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02926241614852556		[learning rate: 0.00021019]
		[batch 20/20] avg loss: 0.06078678346186058		[learning rate: 0.00020994]
	Learning Rate: 0.000209944
	LOSS [training: 0.04502459980519307 | validation: 0.013466079983061886]
	TIME [epoch: 8.17 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041557757541849455		[learning rate: 0.0002097]
		[batch 20/20] avg loss: 0.03718729049178693		[learning rate: 0.00020945]
	Learning Rate: 0.000209448
	LOSS [training: 0.03937252401681819 | validation: 0.030609116135215005]
	TIME [epoch: 8.17 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040288429180182314		[learning rate: 0.0002092]
		[batch 20/20] avg loss: 0.03711292783116023		[learning rate: 0.00020895]
	Learning Rate: 0.000208954
	LOSS [training: 0.03870067850567127 | validation: 0.01115899702406407]
	TIME [epoch: 8.22 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04041157129465511		[learning rate: 0.00020871]
		[batch 20/20] avg loss: 0.037316335542819226		[learning rate: 0.00020846]
	Learning Rate: 0.000208461
	LOSS [training: 0.03886395341873718 | validation: 0.04694336839868203]
	TIME [epoch: 8.2 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040022724810373514		[learning rate: 0.00020822]
		[batch 20/20] avg loss: 0.049811657526227786		[learning rate: 0.00020797]
	Learning Rate: 0.00020797
	LOSS [training: 0.04491719116830065 | validation: 0.015567068613887238]
	TIME [epoch: 8.17 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04341312018826555		[learning rate: 0.00020772]
		[batch 20/20] avg loss: 0.034670541891013144		[learning rate: 0.00020748]
	Learning Rate: 0.000207479
	LOSS [training: 0.03904183103963934 | validation: 0.016135343225727468]
	TIME [epoch: 8.18 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03739175668139973		[learning rate: 0.00020723]
		[batch 20/20] avg loss: 0.04793364576249672		[learning rate: 0.00020699]
	Learning Rate: 0.00020699
	LOSS [training: 0.04266270122194823 | validation: 0.020880275566911913]
	TIME [epoch: 8.24 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05203991377284449		[learning rate: 0.00020675]
		[batch 20/20] avg loss: 0.040740988518824316		[learning rate: 0.0002065]
	Learning Rate: 0.000206501
	LOSS [training: 0.046390451145834406 | validation: 0.01686755713068013]
	TIME [epoch: 8.19 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04367960068975025		[learning rate: 0.00020626]
		[batch 20/20] avg loss: 0.035799021315466625		[learning rate: 0.00020601]
	Learning Rate: 0.000206014
	LOSS [training: 0.039739311002608434 | validation: 0.019004078107033372]
	TIME [epoch: 8.18 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042101243258795884		[learning rate: 0.00020577]
		[batch 20/20] avg loss: 0.03292794474938174		[learning rate: 0.00020553]
	Learning Rate: 0.000205528
	LOSS [training: 0.03751459400408881 | validation: 0.014878712785135122]
	TIME [epoch: 8.17 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04590525788644239		[learning rate: 0.00020529]
		[batch 20/20] avg loss: 0.03512702844384623		[learning rate: 0.00020504]
	Learning Rate: 0.000205044
	LOSS [training: 0.04051614316514432 | validation: 0.02638225057339881]
	TIME [epoch: 8.24 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045510269861520944		[learning rate: 0.0002048]
		[batch 20/20] avg loss: 0.041282486272766566		[learning rate: 0.00020456]
	Learning Rate: 0.00020456
	LOSS [training: 0.04339637806714376 | validation: 0.017548962202210576]
	TIME [epoch: 8.17 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03572864208060869		[learning rate: 0.00020432]
		[batch 20/20] avg loss: 0.040875313374285974		[learning rate: 0.00020408]
	Learning Rate: 0.000204077
	LOSS [training: 0.03830197772744734 | validation: 0.02119100177263001]
	TIME [epoch: 8.18 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03778545530357897		[learning rate: 0.00020384]
		[batch 20/20] avg loss: 0.03358370069854961		[learning rate: 0.0002036]
	Learning Rate: 0.000203596
	LOSS [training: 0.03568457800106428 | validation: 0.019072253886697234]
	TIME [epoch: 8.19 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03654199037531697		[learning rate: 0.00020336]
		[batch 20/20] avg loss: 0.036358884594120636		[learning rate: 0.00020312]
	Learning Rate: 0.000203116
	LOSS [training: 0.036450437484718795 | validation: 0.023411482830312212]
	TIME [epoch: 8.25 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05071239917980451		[learning rate: 0.00020288]
		[batch 20/20] avg loss: 0.03786341817541069		[learning rate: 0.00020264]
	Learning Rate: 0.000202637
	LOSS [training: 0.0442879086776076 | validation: 0.008056788293448068]
	TIME [epoch: 8.18 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04498137064395403		[learning rate: 0.0002024]
		[batch 20/20] avg loss: 0.047450161216113625		[learning rate: 0.00020216]
	Learning Rate: 0.000202159
	LOSS [training: 0.04621576593003383 | validation: 0.0099503955539819]
	TIME [epoch: 8.18 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029253308660661455		[learning rate: 0.00020192]
		[batch 20/20] avg loss: 0.041157180097548526		[learning rate: 0.00020168]
	Learning Rate: 0.000201682
	LOSS [training: 0.03520524437910498 | validation: 0.013122588294720364]
	TIME [epoch: 8.18 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034989142474233674		[learning rate: 0.00020144]
		[batch 20/20] avg loss: 0.04999685836532304		[learning rate: 0.00020121]
	Learning Rate: 0.000201206
	LOSS [training: 0.04249300041977836 | validation: 0.013071637011584897]
	TIME [epoch: 8.24 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04292768685535556		[learning rate: 0.00020097]
		[batch 20/20] avg loss: 0.032236152193154885		[learning rate: 0.00020073]
	Learning Rate: 0.000200731
	LOSS [training: 0.037581919524255214 | validation: 0.007929765970195776]
	TIME [epoch: 8.18 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03555092912964595		[learning rate: 0.00020049]
		[batch 20/20] avg loss: 0.04587252231896979		[learning rate: 0.00020026]
	Learning Rate: 0.000200258
	LOSS [training: 0.04071172572430787 | validation: 0.01986728749989341]
	TIME [epoch: 8.18 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041679284878318054		[learning rate: 0.00020002]
		[batch 20/20] avg loss: 0.03820684585215396		[learning rate: 0.00019979]
	Learning Rate: 0.000199786
	LOSS [training: 0.039943065365236 | validation: 0.013918489816731081]
	TIME [epoch: 8.19 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03724269365374878		[learning rate: 0.00019955]
		[batch 20/20] avg loss: 0.03591068574761565		[learning rate: 0.00019931]
	Learning Rate: 0.000199314
	LOSS [training: 0.036576689700682216 | validation: 0.033702225168831715]
	TIME [epoch: 8.25 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04317256516602489		[learning rate: 0.00019908]
		[batch 20/20] avg loss: 0.03840109605346766		[learning rate: 0.00019884]
	Learning Rate: 0.000198844
	LOSS [training: 0.04078683060974627 | validation: 0.021423010882749827]
	TIME [epoch: 8.19 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051322403630947755		[learning rate: 0.00019861]
		[batch 20/20] avg loss: 0.03157401878389667		[learning rate: 0.00019838]
	Learning Rate: 0.000198375
	LOSS [training: 0.04144821120742222 | validation: 0.011151429734587655]
	TIME [epoch: 8.19 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038853965551003025		[learning rate: 0.00019814]
		[batch 20/20] avg loss: 0.04507538444894009		[learning rate: 0.00019791]
	Learning Rate: 0.000197907
	LOSS [training: 0.04196467499997156 | validation: 0.02741117806299047]
	TIME [epoch: 8.2 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04300645956884763		[learning rate: 0.00019767]
		[batch 20/20] avg loss: 0.04694408227245597		[learning rate: 0.00019744]
	Learning Rate: 0.00019744
	LOSS [training: 0.0449752709206518 | validation: 0.01661269525867628]
	TIME [epoch: 8.23 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036518881249866554		[learning rate: 0.00019721]
		[batch 20/20] avg loss: 0.04474230742234719		[learning rate: 0.00019697]
	Learning Rate: 0.000196975
	LOSS [training: 0.04063059433610687 | validation: 0.011416296709780852]
	TIME [epoch: 8.19 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02938332628131122		[learning rate: 0.00019674]
		[batch 20/20] avg loss: 0.04080251723407874		[learning rate: 0.00019651]
	Learning Rate: 0.00019651
	LOSS [training: 0.03509292175769498 | validation: 0.018325312948903258]
	TIME [epoch: 8.18 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033986083615717186		[learning rate: 0.00019628]
		[batch 20/20] avg loss: 0.03730290486469696		[learning rate: 0.00019605]
	Learning Rate: 0.000196046
	LOSS [training: 0.035644494240207075 | validation: 0.0251069755723544]
	TIME [epoch: 8.21 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04766227446635809		[learning rate: 0.00019582]
		[batch 20/20] avg loss: 0.039252694457250506		[learning rate: 0.00019558]
	Learning Rate: 0.000195584
	LOSS [training: 0.04345748446180429 | validation: 0.016473605950381373]
	TIME [epoch: 8.23 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03139246890384463		[learning rate: 0.00019535]
		[batch 20/20] avg loss: 0.031761815268874075		[learning rate: 0.00019512]
	Learning Rate: 0.000195123
	LOSS [training: 0.03157714208635935 | validation: 0.013528787555968267]
	TIME [epoch: 8.2 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03371458793379052		[learning rate: 0.00019489]
		[batch 20/20] avg loss: 0.03908086060863031		[learning rate: 0.00019466]
	Learning Rate: 0.000194662
	LOSS [training: 0.03639772427121042 | validation: 0.018307812119778912]
	TIME [epoch: 8.18 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03632494647095722		[learning rate: 0.00019443]
		[batch 20/20] avg loss: 0.03607354902447048		[learning rate: 0.0001942]
	Learning Rate: 0.000194203
	LOSS [training: 0.03619924774771385 | validation: 0.015709098194294988]
	TIME [epoch: 8.2 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037878631102313376		[learning rate: 0.00019397]
		[batch 20/20] avg loss: 0.04402823287323652		[learning rate: 0.00019375]
	Learning Rate: 0.000193745
	LOSS [training: 0.040953431987774944 | validation: 0.050000280992419904]
	TIME [epoch: 8.21 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05332163117226034		[learning rate: 0.00019352]
		[batch 20/20] avg loss: 0.03124607374791723		[learning rate: 0.00019329]
	Learning Rate: 0.000193288
	LOSS [training: 0.0422838524600888 | validation: 0.008382920994780578]
	TIME [epoch: 8.2 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03941168836762022		[learning rate: 0.00019306]
		[batch 20/20] avg loss: 0.033947389534068936		[learning rate: 0.00019283]
	Learning Rate: 0.000192832
	LOSS [training: 0.036679538950844574 | validation: 0.01570198651781939]
	TIME [epoch: 8.18 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04104882712772699		[learning rate: 0.0001926]
		[batch 20/20] avg loss: 0.03226219513362225		[learning rate: 0.00019238]
	Learning Rate: 0.000192377
	LOSS [training: 0.036655511130674626 | validation: 0.009609086566445942]
	TIME [epoch: 8.21 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030615193243821236		[learning rate: 0.00019215]
		[batch 20/20] avg loss: 0.04435665456891597		[learning rate: 0.00019192]
	Learning Rate: 0.000191923
	LOSS [training: 0.03748592390636861 | validation: 0.014354906510705766]
	TIME [epoch: 8.21 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041921141136768314		[learning rate: 0.0001917]
		[batch 20/20] avg loss: 0.03184921865237468		[learning rate: 0.00019147]
	Learning Rate: 0.000191471
	LOSS [training: 0.036885179894571504 | validation: 0.012290015172631357]
	TIME [epoch: 8.2 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03815462826069295		[learning rate: 0.00019124]
		[batch 20/20] avg loss: 0.03836679505476937		[learning rate: 0.00019102]
	Learning Rate: 0.000191019
	LOSS [training: 0.03826071165773116 | validation: 0.003658011888243768]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_1726.pth
	Model improved!!!
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04284775192916064		[learning rate: 0.00019079]
		[batch 20/20] avg loss: 0.029731929543407965		[learning rate: 0.00019057]
	Learning Rate: 0.000190569
	LOSS [training: 0.0362898407362843 | validation: 0.00959058438525449]
	TIME [epoch: 8.23 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034048276733090584		[learning rate: 0.00019034]
		[batch 20/20] avg loss: 0.03901377889799202		[learning rate: 0.00019012]
	Learning Rate: 0.000190119
	LOSS [training: 0.036531027815541295 | validation: 0.007569063382008071]
	TIME [epoch: 8.2 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04017713904703605		[learning rate: 0.00018989]
		[batch 20/20] avg loss: 0.03769675811983897		[learning rate: 0.00018967]
	Learning Rate: 0.000189671
	LOSS [training: 0.038936948583437506 | validation: 0.028889218630950373]
	TIME [epoch: 8.19 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04562214163286748		[learning rate: 0.00018945]
		[batch 20/20] avg loss: 0.044047397334951735		[learning rate: 0.00018922]
	Learning Rate: 0.000189223
	LOSS [training: 0.0448347694839096 | validation: 0.014101062818949104]
	TIME [epoch: 8.17 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03320388889614882		[learning rate: 0.000189]
		[batch 20/20] avg loss: 0.03685351669785807		[learning rate: 0.00018878]
	Learning Rate: 0.000188777
	LOSS [training: 0.035028702797003444 | validation: 0.0075690359409883844]
	TIME [epoch: 8.21 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03465037736401507		[learning rate: 0.00018855]
		[batch 20/20] avg loss: 0.04105565086044001		[learning rate: 0.00018833]
	Learning Rate: 0.000188332
	LOSS [training: 0.03785301411222754 | validation: 0.014917300847901462]
	TIME [epoch: 8.19 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03896110129533461		[learning rate: 0.00018811]
		[batch 20/20] avg loss: 0.04386963387904868		[learning rate: 0.00018789]
	Learning Rate: 0.000187887
	LOSS [training: 0.04141536758719165 | validation: 0.023461585745523535]
	TIME [epoch: 8.19 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0395767900943464		[learning rate: 0.00018767]
		[batch 20/20] avg loss: 0.035210121345646064		[learning rate: 0.00018744]
	Learning Rate: 0.000187444
	LOSS [training: 0.037393455719996235 | validation: 0.01837505315544611]
	TIME [epoch: 8.18 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0347708151103062		[learning rate: 0.00018722]
		[batch 20/20] avg loss: 0.03699488011615594		[learning rate: 0.000187]
	Learning Rate: 0.000187002
	LOSS [training: 0.035882847613231066 | validation: 0.013803227152467056]
	TIME [epoch: 8.22 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03645353276975653		[learning rate: 0.00018678]
		[batch 20/20] avg loss: 0.03742369218766017		[learning rate: 0.00018656]
	Learning Rate: 0.000186561
	LOSS [training: 0.036938612478708346 | validation: 0.01616701043591029]
	TIME [epoch: 8.21 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03578398331738311		[learning rate: 0.00018634]
		[batch 20/20] avg loss: 0.03608416294912696		[learning rate: 0.00018612]
	Learning Rate: 0.000186121
	LOSS [training: 0.03593407313325504 | validation: 0.006216376330076259]
	TIME [epoch: 8.19 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03141181313425511		[learning rate: 0.0001859]
		[batch 20/20] avg loss: 0.039552235864707		[learning rate: 0.00018568]
	Learning Rate: 0.000185682
	LOSS [training: 0.035482024499481055 | validation: 0.013642468352639464]
	TIME [epoch: 8.19 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04039303801098595		[learning rate: 0.00018546]
		[batch 20/20] avg loss: 0.03512596818539074		[learning rate: 0.00018524]
	Learning Rate: 0.000185244
	LOSS [training: 0.03775950309818835 | validation: 0.010982209440077613]
	TIME [epoch: 8.2 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03530325906573415		[learning rate: 0.00018503]
		[batch 20/20] avg loss: 0.03811456976745303		[learning rate: 0.00018481]
	Learning Rate: 0.000184807
	LOSS [training: 0.03670891441659359 | validation: 0.01668690230865272]
	TIME [epoch: 8.19 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031655391807179814		[learning rate: 0.00018459]
		[batch 20/20] avg loss: 0.04436006094850578		[learning rate: 0.00018437]
	Learning Rate: 0.000184371
	LOSS [training: 0.0380077263778428 | validation: 0.02044386401687185]
	TIME [epoch: 8.2 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03667970891251342		[learning rate: 0.00018415]
		[batch 20/20] avg loss: 0.03905170411263644		[learning rate: 0.00018394]
	Learning Rate: 0.000183936
	LOSS [training: 0.03786570651257493 | validation: 0.006483916799212414]
	TIME [epoch: 8.2 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032245567021553026		[learning rate: 0.00018372]
		[batch 20/20] avg loss: 0.03614388622595465		[learning rate: 0.0001835]
	Learning Rate: 0.000183502
	LOSS [training: 0.03419472662375383 | validation: 0.011787750459710207]
	TIME [epoch: 8.22 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03960412876358083		[learning rate: 0.00018329]
		[batch 20/20] avg loss: 0.05735771159037045		[learning rate: 0.00018307]
	Learning Rate: 0.000183069
	LOSS [training: 0.04848092017697564 | validation: 0.02330452347634908]
	TIME [epoch: 8.2 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03917617086762899		[learning rate: 0.00018285]
		[batch 20/20] avg loss: 0.05349755206226149		[learning rate: 0.00018264]
	Learning Rate: 0.000182637
	LOSS [training: 0.04633686146494524 | validation: 0.012515028643159117]
	TIME [epoch: 8.19 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04072846890927974		[learning rate: 0.00018242]
		[batch 20/20] avg loss: 0.043043137133537336		[learning rate: 0.00018221]
	Learning Rate: 0.000182207
	LOSS [training: 0.04188580302140854 | validation: 0.00625094232118048]
	TIME [epoch: 8.2 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03783038457979426		[learning rate: 0.00018199]
		[batch 20/20] avg loss: 0.03509215425620958		[learning rate: 0.00018178]
	Learning Rate: 0.000181777
	LOSS [training: 0.03646126941800192 | validation: 0.016324920929116422]
	TIME [epoch: 8.21 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03822855143399401		[learning rate: 0.00018156]
		[batch 20/20] avg loss: 0.04181260800599047		[learning rate: 0.00018135]
	Learning Rate: 0.000181348
	LOSS [training: 0.04002057971999224 | validation: 0.012432949098373933]
	TIME [epoch: 8.19 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04413982332337367		[learning rate: 0.00018113]
		[batch 20/20] avg loss: 0.04415990128731404		[learning rate: 0.00018092]
	Learning Rate: 0.00018092
	LOSS [training: 0.044149862305343845 | validation: 0.021472735819716864]
	TIME [epoch: 8.21 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032790952579969265		[learning rate: 0.00018071]
		[batch 20/20] avg loss: 0.049772065665652614		[learning rate: 0.00018049]
	Learning Rate: 0.000180493
	LOSS [training: 0.04128150912281094 | validation: 0.030809501687621713]
	TIME [epoch: 8.21 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04954582596671732		[learning rate: 0.00018028]
		[batch 20/20] avg loss: 0.03073523487958395		[learning rate: 0.00018007]
	Learning Rate: 0.000180068
	LOSS [training: 0.04014053042315064 | validation: 0.01749247069805251]
	TIME [epoch: 8.21 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04369294194741731		[learning rate: 0.00017986]
		[batch 20/20] avg loss: 0.027996283170690455		[learning rate: 0.00017964]
	Learning Rate: 0.000179643
	LOSS [training: 0.03584461255905388 | validation: 0.010887967622321406]
	TIME [epoch: 8.19 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038249039950642036		[learning rate: 0.00017943]
		[batch 20/20] avg loss: 0.029293820829554777		[learning rate: 0.00017922]
	Learning Rate: 0.000179219
	LOSS [training: 0.03377143039009841 | validation: 0.010797808945138383]
	TIME [epoch: 8.2 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0390045037771385		[learning rate: 0.00017901]
		[batch 20/20] avg loss: 0.04244658135608329		[learning rate: 0.0001788]
	Learning Rate: 0.000178796
	LOSS [training: 0.0407255425666109 | validation: 0.018079366700688503]
	TIME [epoch: 8.21 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04454734530513224		[learning rate: 0.00017859]
		[batch 20/20] avg loss: 0.038137259482166286		[learning rate: 0.00017837]
	Learning Rate: 0.000178375
	LOSS [training: 0.04134230239364927 | validation: 0.018809370050630523]
	TIME [epoch: 8.2 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034241219686663237		[learning rate: 0.00017816]
		[batch 20/20] avg loss: 0.03676734985073998		[learning rate: 0.00017795]
	Learning Rate: 0.000177954
	LOSS [training: 0.03550428476870161 | validation: 0.014435420924620728]
	TIME [epoch: 8.19 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0296311144989648		[learning rate: 0.00017774]
		[batch 20/20] avg loss: 0.050837148179236326		[learning rate: 0.00017753]
	Learning Rate: 0.000177534
	LOSS [training: 0.04023413133910055 | validation: 0.020410574424050417]
	TIME [epoch: 8.21 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03473476219635321		[learning rate: 0.00017732]
		[batch 20/20] avg loss: 0.033152087722837814		[learning rate: 0.00017712]
	Learning Rate: 0.000177115
	LOSS [training: 0.033943424959595514 | validation: 0.010901010226624284]
	TIME [epoch: 8.21 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03234000492391733		[learning rate: 0.00017691]
		[batch 20/20] avg loss: 0.03828775543820825		[learning rate: 0.0001767]
	Learning Rate: 0.000176698
	LOSS [training: 0.035313880181062796 | validation: 0.013244771098329909]
	TIME [epoch: 8.19 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048601629039888475		[learning rate: 0.00017649]
		[batch 20/20] avg loss: 0.04503596690889435		[learning rate: 0.00017628]
	Learning Rate: 0.000176281
	LOSS [training: 0.04681879797439141 | validation: 0.0106989771075124]
	TIME [epoch: 8.18 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03890859787896324		[learning rate: 0.00017607]
		[batch 20/20] avg loss: 0.04581443794418148		[learning rate: 0.00017587]
	Learning Rate: 0.000175865
	LOSS [training: 0.042361517911572374 | validation: 0.024274745645484198]
	TIME [epoch: 8.21 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04121112559569651		[learning rate: 0.00017566]
		[batch 20/20] avg loss: 0.034787830363867636		[learning rate: 0.00017545]
	Learning Rate: 0.00017545
	LOSS [training: 0.03799947797978208 | validation: 0.015869693840836013]
	TIME [epoch: 8.23 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0378775247970491		[learning rate: 0.00017524]
		[batch 20/20] avg loss: 0.03726848259327876		[learning rate: 0.00017504]
	Learning Rate: 0.000175036
	LOSS [training: 0.03757300369516393 | validation: 0.010288386244640409]
	TIME [epoch: 8.2 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0386813297564451		[learning rate: 0.00017483]
		[batch 20/20] avg loss: 0.03903883692078543		[learning rate: 0.00017462]
	Learning Rate: 0.000174623
	LOSS [training: 0.038860083338615264 | validation: 0.013429839515910437]
	TIME [epoch: 8.19 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035240668426796365		[learning rate: 0.00017442]
		[batch 20/20] avg loss: 0.033512745180331004		[learning rate: 0.00017421]
	Learning Rate: 0.000174212
	LOSS [training: 0.03437670680356368 | validation: 0.021621247752712562]
	TIME [epoch: 8.22 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04279826372445354		[learning rate: 0.00017401]
		[batch 20/20] avg loss: 0.04822549997742799		[learning rate: 0.0001738]
	Learning Rate: 0.000173801
	LOSS [training: 0.04551188185094077 | validation: 0.018390259382978363]
	TIME [epoch: 8.23 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04390468009268479		[learning rate: 0.0001736]
		[batch 20/20] avg loss: 0.03987190369729575		[learning rate: 0.00017339]
	Learning Rate: 0.000173391
	LOSS [training: 0.04188829189499027 | validation: 0.012553628304081691]
	TIME [epoch: 8.19 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04297606947582367		[learning rate: 0.00017319]
		[batch 20/20] avg loss: 0.03153936377064915		[learning rate: 0.00017298]
	Learning Rate: 0.000172982
	LOSS [training: 0.0372577166232364 | validation: 0.01123224055955452]
	TIME [epoch: 8.18 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04948760863759163		[learning rate: 0.00017278]
		[batch 20/20] avg loss: 0.03723893276335254		[learning rate: 0.00017257]
	Learning Rate: 0.000172574
	LOSS [training: 0.04336327070047209 | validation: 0.029065497133510407]
	TIME [epoch: 8.21 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05519601299132323		[learning rate: 0.00017237]
		[batch 20/20] avg loss: 0.03796206879906606		[learning rate: 0.00017217]
	Learning Rate: 0.000172167
	LOSS [training: 0.04657904089519464 | validation: 0.016488223726792407]
	TIME [epoch: 8.22 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03315266339026384		[learning rate: 0.00017196]
		[batch 20/20] avg loss: 0.0392205367159072		[learning rate: 0.00017176]
	Learning Rate: 0.00017176
	LOSS [training: 0.03618660005308553 | validation: 0.01688909945919838]
	TIME [epoch: 8.19 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0395591911914943		[learning rate: 0.00017156]
		[batch 20/20] avg loss: 0.036669751512876166		[learning rate: 0.00017136]
	Learning Rate: 0.000171355
	LOSS [training: 0.038114471352185236 | validation: 0.01239627760164729]
	TIME [epoch: 8.18 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03732346907878553		[learning rate: 0.00017115]
		[batch 20/20] avg loss: 0.04369738924985003		[learning rate: 0.00017095]
	Learning Rate: 0.000170951
	LOSS [training: 0.04051042916431778 | validation: 0.006127674392154893]
	TIME [epoch: 8.22 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03839441856044544		[learning rate: 0.00017075]
		[batch 20/20] avg loss: 0.0341744923353706		[learning rate: 0.00017055]
	Learning Rate: 0.000170548
	LOSS [training: 0.036284455447908016 | validation: 0.020631035118788162]
	TIME [epoch: 8.23 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04309192609908916		[learning rate: 0.00017035]
		[batch 20/20] avg loss: 0.038325164415255505		[learning rate: 0.00017015]
	Learning Rate: 0.000170146
	LOSS [training: 0.04070854525717234 | validation: 0.012049772476071047]
	TIME [epoch: 8.19 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0360883241893133		[learning rate: 0.00016994]
		[batch 20/20] avg loss: 0.03392057331991595		[learning rate: 0.00016974]
	Learning Rate: 0.000169744
	LOSS [training: 0.03500444875461462 | validation: 0.01608418577625781]
	TIME [epoch: 8.2 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03277145497548011		[learning rate: 0.00016954]
		[batch 20/20] avg loss: 0.03294957273465609		[learning rate: 0.00016934]
	Learning Rate: 0.000169344
	LOSS [training: 0.03286051385506811 | validation: 0.01239000079189173]
	TIME [epoch: 8.23 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03477092381118764		[learning rate: 0.00016914]
		[batch 20/20] avg loss: 0.03858748354951651		[learning rate: 0.00016894]
	Learning Rate: 0.000168944
	LOSS [training: 0.036679203680352074 | validation: 0.01307604871090841]
	TIME [epoch: 8.21 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040237623422032714		[learning rate: 0.00016874]
		[batch 20/20] avg loss: 0.04275619674090077		[learning rate: 0.00016855]
	Learning Rate: 0.000168546
	LOSS [training: 0.04149691008146674 | validation: 0.01435529476691911]
	TIME [epoch: 8.18 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033281041230570976		[learning rate: 0.00016835]
		[batch 20/20] avg loss: 0.04335416318896608		[learning rate: 0.00016815]
	Learning Rate: 0.000168148
	LOSS [training: 0.038317602209768514 | validation: 0.014268705522407148]
	TIME [epoch: 8.18 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041278964987890646		[learning rate: 0.00016795]
		[batch 20/20] avg loss: 0.042541493981307445		[learning rate: 0.00016775]
	Learning Rate: 0.000167752
	LOSS [training: 0.04191022948459904 | validation: 0.025412779725052737]
	TIME [epoch: 8.23 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03091946132266802		[learning rate: 0.00016755]
		[batch 20/20] avg loss: 0.04582713652729693		[learning rate: 0.00016736]
	Learning Rate: 0.000167356
	LOSS [training: 0.03837329892498247 | validation: 0.023709434284965712]
	TIME [epoch: 8.2 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04892028420056382		[learning rate: 0.00016716]
		[batch 20/20] avg loss: 0.03903032133617544		[learning rate: 0.00016696]
	Learning Rate: 0.000166961
	LOSS [training: 0.043975302768369626 | validation: 0.020977861736854408]
	TIME [epoch: 8.17 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03903955875867195		[learning rate: 0.00016676]
		[batch 20/20] avg loss: 0.032645856156145334		[learning rate: 0.00016657]
	Learning Rate: 0.000166567
	LOSS [training: 0.03584270745740863 | validation: 0.010552058927925483]
	TIME [epoch: 8.18 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03962788714472036		[learning rate: 0.00016637]
		[batch 20/20] avg loss: 0.04320035279532106		[learning rate: 0.00016617]
	Learning Rate: 0.000166174
	LOSS [training: 0.041414119970020716 | validation: 0.0159602718018799]
	TIME [epoch: 8.22 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03769316656206648		[learning rate: 0.00016598]
		[batch 20/20] avg loss: 0.04530069924136017		[learning rate: 0.00016578]
	Learning Rate: 0.000165782
	LOSS [training: 0.04149693290171332 | validation: 0.019106438551361248]
	TIME [epoch: 8.22 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03981716558307059		[learning rate: 0.00016559]
		[batch 20/20] avg loss: 0.03119393359867372		[learning rate: 0.00016539]
	Learning Rate: 0.000165391
	LOSS [training: 0.035505549590872154 | validation: 0.011473012773241049]
	TIME [epoch: 8.19 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03958512093922034		[learning rate: 0.0001652]
		[batch 20/20] avg loss: 0.04134018774229599		[learning rate: 0.000165]
	Learning Rate: 0.000165001
	LOSS [training: 0.040462654340758165 | validation: 0.022082766258093293]
	TIME [epoch: 8.19 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03940274222139291		[learning rate: 0.00016481]
		[batch 20/20] avg loss: 0.03713288947465994		[learning rate: 0.00016461]
	Learning Rate: 0.000164612
	LOSS [training: 0.038267815848026424 | validation: 0.009321084408582434]
	TIME [epoch: 8.23 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03834101363652495		[learning rate: 0.00016442]
		[batch 20/20] avg loss: 0.028811700118139848		[learning rate: 0.00016422]
	Learning Rate: 0.000164224
	LOSS [training: 0.0335763568773324 | validation: 0.012328245972647213]
	TIME [epoch: 8.21 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026586387675853734		[learning rate: 0.00016403]
		[batch 20/20] avg loss: 0.0443470807092599		[learning rate: 0.00016384]
	Learning Rate: 0.000163836
	LOSS [training: 0.03546673419255681 | validation: 0.01143897714711141]
	TIME [epoch: 8.19 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03430022345823082		[learning rate: 0.00016364]
		[batch 20/20] avg loss: 0.055928831465198746		[learning rate: 0.00016345]
	Learning Rate: 0.00016345
	LOSS [training: 0.04511452746171478 | validation: 0.016300865200930596]
	TIME [epoch: 8.18 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035597963772500935		[learning rate: 0.00016326]
		[batch 20/20] avg loss: 0.04422078077890494		[learning rate: 0.00016306]
	Learning Rate: 0.000163064
	LOSS [training: 0.039909372275702945 | validation: 0.016336492597404467]
	TIME [epoch: 8.24 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031747927353062436		[learning rate: 0.00016287]
		[batch 20/20] avg loss: 0.03603552971917202		[learning rate: 0.00016268]
	Learning Rate: 0.00016268
	LOSS [training: 0.03389172853611723 | validation: 0.007129427373063433]
	TIME [epoch: 8.19 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045463097920135406		[learning rate: 0.00016249]
		[batch 20/20] avg loss: 0.03588046936387897		[learning rate: 0.0001623]
	Learning Rate: 0.000162296
	LOSS [training: 0.04067178364200719 | validation: 0.02468498202182585]
	TIME [epoch: 8.19 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03405784620358098		[learning rate: 0.0001621]
		[batch 20/20] avg loss: 0.03646531001557886		[learning rate: 0.00016191]
	Learning Rate: 0.000161913
	LOSS [training: 0.03526157810957991 | validation: 0.015993054097541697]
	TIME [epoch: 8.18 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03791735235600623		[learning rate: 0.00016172]
		[batch 20/20] avg loss: 0.03413116030836463		[learning rate: 0.00016153]
	Learning Rate: 0.000161531
	LOSS [training: 0.03602425633218542 | validation: 0.003990731729580579]
	TIME [epoch: 8.25 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031503446614027736		[learning rate: 0.00016134]
		[batch 20/20] avg loss: 0.04681476929438786		[learning rate: 0.00016115]
	Learning Rate: 0.00016115
	LOSS [training: 0.039159107954207795 | validation: 0.023330133474329093]
	TIME [epoch: 8.2 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040405988033614726		[learning rate: 0.00016096]
		[batch 20/20] avg loss: 0.043611417401221965		[learning rate: 0.00016077]
	Learning Rate: 0.00016077
	LOSS [training: 0.04200870271741835 | validation: 0.02430890677147575]
	TIME [epoch: 8.18 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03631545478582611		[learning rate: 0.00016058]
		[batch 20/20] avg loss: 0.04894586474281892		[learning rate: 0.00016039]
	Learning Rate: 0.000160391
	LOSS [training: 0.04263065976432252 | validation: 0.017653568646626344]
	TIME [epoch: 8.19 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03478156568866192		[learning rate: 0.0001602]
		[batch 20/20] avg loss: 0.04841952975902199		[learning rate: 0.00016001]
	Learning Rate: 0.000160012
	LOSS [training: 0.04160054772384196 | validation: 0.019549200360538352]
	TIME [epoch: 8.24 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039999269112770165		[learning rate: 0.00015982]
		[batch 20/20] avg loss: 0.04244978158809089		[learning rate: 0.00015964]
	Learning Rate: 0.000159635
	LOSS [training: 0.04122452535043052 | validation: 0.01698377207705386]
	TIME [epoch: 8.19 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029480843188062305		[learning rate: 0.00015945]
		[batch 20/20] avg loss: 0.03310794320843518		[learning rate: 0.00015926]
	Learning Rate: 0.000159258
	LOSS [training: 0.03129439319824875 | validation: 0.013873856753535835]
	TIME [epoch: 8.19 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03951715536354515		[learning rate: 0.00015907]
		[batch 20/20] avg loss: 0.03938972756999808		[learning rate: 0.00015888]
	Learning Rate: 0.000158883
	LOSS [training: 0.03945344146677163 | validation: 0.006704633843661387]
	TIME [epoch: 8.19 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04747677723926198		[learning rate: 0.0001587]
		[batch 20/20] avg loss: 0.04554718317714247		[learning rate: 0.00015851]
	Learning Rate: 0.000158508
	LOSS [training: 0.04651198020820223 | validation: 0.012017190338119455]
	TIME [epoch: 8.25 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0371224829785952		[learning rate: 0.00015832]
		[batch 20/20] avg loss: 0.03760744283201504		[learning rate: 0.00015813]
	Learning Rate: 0.000158134
	LOSS [training: 0.03736496290530512 | validation: 0.02399085471288671]
	TIME [epoch: 8.19 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041860760798288016		[learning rate: 0.00015795]
		[batch 20/20] avg loss: 0.0355239209374293		[learning rate: 0.00015776]
	Learning Rate: 0.000157761
	LOSS [training: 0.038692340867858666 | validation: 0.018089579546978728]
	TIME [epoch: 8.18 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03431169930975968		[learning rate: 0.00015757]
		[batch 20/20] avg loss: 0.039761886850237106		[learning rate: 0.00015739]
	Learning Rate: 0.000157389
	LOSS [training: 0.0370367930799984 | validation: 0.010412141177789875]
	TIME [epoch: 8.19 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04288264935550311		[learning rate: 0.0001572]
		[batch 20/20] avg loss: 0.034762456257631844		[learning rate: 0.00015702]
	Learning Rate: 0.000157018
	LOSS [training: 0.03882255280656748 | validation: 0.013579769089736486]
	TIME [epoch: 8.25 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035630448672484806		[learning rate: 0.00015683]
		[batch 20/20] avg loss: 0.044476552254628744		[learning rate: 0.00015665]
	Learning Rate: 0.000156647
	LOSS [training: 0.040053500463556775 | validation: 0.002199013872565546]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r3_20240219_184940/states/model_tr_study2_1810.pth
	Model improved!!!
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03424483327466		[learning rate: 0.00015646]
		[batch 20/20] avg loss: 0.0338128747723649		[learning rate: 0.00015628]
	Learning Rate: 0.000156278
	LOSS [training: 0.034028854023512446 | validation: 0.0032707380352255087]
	TIME [epoch: 8.18 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03733499723160572		[learning rate: 0.00015609]
		[batch 20/20] avg loss: 0.029567464668109455		[learning rate: 0.00015591]
	Learning Rate: 0.000155909
	LOSS [training: 0.03345123094985759 | validation: 0.009349436156498137]
	TIME [epoch: 8.19 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03273245259717677		[learning rate: 0.00015573]
		[batch 20/20] avg loss: 0.03996706940369709		[learning rate: 0.00015554]
	Learning Rate: 0.000155541
	LOSS [training: 0.03634976100043693 | validation: 0.023059777780388487]
	TIME [epoch: 8.23 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03832815205088427		[learning rate: 0.00015536]
		[batch 20/20] avg loss: 0.0357432021897512		[learning rate: 0.00015517]
	Learning Rate: 0.000155175
	LOSS [training: 0.03703567712031774 | validation: 0.01431448838865262]
	TIME [epoch: 8.17 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03576291586500312		[learning rate: 0.00015499]
		[batch 20/20] avg loss: 0.03909285574955147		[learning rate: 0.00015481]
	Learning Rate: 0.000154809
	LOSS [training: 0.03742788580727729 | validation: 0.013839291735635431]
	TIME [epoch: 8.18 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03341957493377562		[learning rate: 0.00015463]
		[batch 20/20] avg loss: 0.04927138445966173		[learning rate: 0.00015444]
	Learning Rate: 0.000154443
	LOSS [training: 0.041345479696718666 | validation: 0.0115469574327862]
	TIME [epoch: 8.2 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037184206716362814		[learning rate: 0.00015426]
		[batch 20/20] avg loss: 0.04189191338254337		[learning rate: 0.00015408]
	Learning Rate: 0.000154079
	LOSS [training: 0.03953806004945309 | validation: 0.011209318991563402]
	TIME [epoch: 8.22 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03168743168270149		[learning rate: 0.0001539]
		[batch 20/20] avg loss: 0.03443650372428459		[learning rate: 0.00015372]
	Learning Rate: 0.000153716
	LOSS [training: 0.03306196770349305 | validation: 0.010592763593441173]
	TIME [epoch: 8.18 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03910698142507092		[learning rate: 0.00015353]
		[batch 20/20] avg loss: 0.029248247242333075		[learning rate: 0.00015335]
	Learning Rate: 0.000153353
	LOSS [training: 0.03417761433370199 | validation: 0.025629506706571326]
	TIME [epoch: 8.18 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03816661896724992		[learning rate: 0.00015317]
		[batch 20/20] avg loss: 0.03495566348063876		[learning rate: 0.00015299]
	Learning Rate: 0.000152991
	LOSS [training: 0.03656114122394434 | validation: 0.006676860238383965]
	TIME [epoch: 8.19 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03337654972038428		[learning rate: 0.00015281]
		[batch 20/20] avg loss: 0.03032700491774703		[learning rate: 0.00015263]
	Learning Rate: 0.00015263
	LOSS [training: 0.031851777319065654 | validation: 0.010737430905778092]
	TIME [epoch: 8.22 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034277108251686755		[learning rate: 0.00015245]
		[batch 20/20] avg loss: 0.04709255502609547		[learning rate: 0.00015227]
	Learning Rate: 0.00015227
	LOSS [training: 0.040684831638891104 | validation: 0.012088391752756451]
	TIME [epoch: 8.18 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04004780897484173		[learning rate: 0.00015209]
		[batch 20/20] avg loss: 0.033192816300936165		[learning rate: 0.00015191]
	Learning Rate: 0.000151911
	LOSS [training: 0.03662031263788895 | validation: 0.010694864183039565]
	TIME [epoch: 8.17 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037280234716565906		[learning rate: 0.00015173]
		[batch 20/20] avg loss: 0.03695630111973362		[learning rate: 0.00015155]
	Learning Rate: 0.000151553
	LOSS [training: 0.03711826791814977 | validation: 0.014997734055423862]
	TIME [epoch: 8.21 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0452990631185065		[learning rate: 0.00015137]
		[batch 20/20] avg loss: 0.032518771909810874		[learning rate: 0.0001512]
	Learning Rate: 0.000151195
	LOSS [training: 0.03890891751415869 | validation: 0.017829793496769736]
	TIME [epoch: 8.21 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035615219906163315		[learning rate: 0.00015102]
		[batch 20/20] avg loss: 0.0361742483492727		[learning rate: 0.00015084]
	Learning Rate: 0.000150839
	LOSS [training: 0.03589473412771801 | validation: 0.013008223991589159]
	TIME [epoch: 8.18 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03782070428106394		[learning rate: 0.00015066]
		[batch 20/20] avg loss: 0.030853716326275142		[learning rate: 0.00015048]
	Learning Rate: 0.000150483
	LOSS [training: 0.034337210303669534 | validation: 0.01697050633211196]
	TIME [epoch: 8.18 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04007620660971287		[learning rate: 0.00015031]
		[batch 20/20] avg loss: 0.03684963001713691		[learning rate: 0.00015013]
	Learning Rate: 0.000150128
	LOSS [training: 0.03846291831342489 | validation: 0.012441172975801848]
	TIME [epoch: 8.22 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040311784166723666		[learning rate: 0.00014995]
		[batch 20/20] avg loss: 0.036163763682997604		[learning rate: 0.00014977]
	Learning Rate: 0.000149774
	LOSS [training: 0.03823777392486062 | validation: 0.0172307103519163]
	TIME [epoch: 8.21 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03763015083835208		[learning rate: 0.0001496]
		[batch 20/20] avg loss: 0.03578139225150417		[learning rate: 0.00014942]
	Learning Rate: 0.000149421
	LOSS [training: 0.03670577154492813 | validation: 0.017823238084360003]
	TIME [epoch: 8.18 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03641873110225967		[learning rate: 0.00014924]
		[batch 20/20] avg loss: 0.034583385044641306		[learning rate: 0.00014907]
	Learning Rate: 0.000149068
	LOSS [training: 0.0355010580734505 | validation: 0.010979127549038497]
	TIME [epoch: 8.18 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030986177229036183		[learning rate: 0.00014889]
		[batch 20/20] avg loss: 0.04281632003820364		[learning rate: 0.00014872]
	Learning Rate: 0.000148716
	LOSS [training: 0.03690124863361991 | validation: 0.020328020367649514]
	TIME [epoch: 8.23 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03322230850879396		[learning rate: 0.00014854]
		[batch 20/20] avg loss: 0.03113787889001942		[learning rate: 0.00014837]
	Learning Rate: 0.000148366
	LOSS [training: 0.0321800936994067 | validation: 0.010885026612146026]
	TIME [epoch: 8.2 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04239382338928897		[learning rate: 0.00014819]
		[batch 20/20] avg loss: 0.03539568562273472		[learning rate: 0.00014802]
	Learning Rate: 0.000148016
	LOSS [training: 0.038894754506011854 | validation: 0.00854246058433288]
	TIME [epoch: 8.18 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034973665505946314		[learning rate: 0.00014784]
		[batch 20/20] avg loss: 0.05046112723746796		[learning rate: 0.00014767]
	Learning Rate: 0.000147667
	LOSS [training: 0.04271739637170715 | validation: 0.017179040255925777]
	TIME [epoch: 8.17 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042239378967179705		[learning rate: 0.00014749]
		[batch 20/20] avg loss: 0.031405939463857335		[learning rate: 0.00014732]
	Learning Rate: 0.000147318
	LOSS [training: 0.03682265921551853 | validation: 0.00979553313067633]
	TIME [epoch: 8.22 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030939216143022058		[learning rate: 0.00014714]
		[batch 20/20] avg loss: 0.029636997315827335		[learning rate: 0.00014697]
	Learning Rate: 0.000146971
	LOSS [training: 0.030288106729424696 | validation: 0.014369835596681431]
	TIME [epoch: 8.2 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03550441437461686		[learning rate: 0.0001468]
		[batch 20/20] avg loss: 0.03810104918026393		[learning rate: 0.00014662]
	Learning Rate: 0.000146624
	LOSS [training: 0.036802731777440395 | validation: 0.013705760741621054]
	TIME [epoch: 8.18 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03585503745053085		[learning rate: 0.00014645]
		[batch 20/20] avg loss: 0.03460567728704213		[learning rate: 0.00014628]
	Learning Rate: 0.000146278
	LOSS [training: 0.035230357368786484 | validation: 0.01613702234273367]
	TIME [epoch: 8.19 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034365759245052016		[learning rate: 0.00014611]
		[batch 20/20] avg loss: 0.04479183359224848		[learning rate: 0.00014593]
	Learning Rate: 0.000145933
	LOSS [training: 0.039578796418650246 | validation: 0.027369290350753937]
	TIME [epoch: 8.21 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037778295264492424		[learning rate: 0.00014576]
		[batch 20/20] avg loss: 0.043602205749331525		[learning rate: 0.00014559]
	Learning Rate: 0.000145589
	LOSS [training: 0.04069025050691196 | validation: 0.011006541096242412]
	TIME [epoch: 8.2 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03441516618239913		[learning rate: 0.00014542]
		[batch 20/20] avg loss: 0.03448246848620272		[learning rate: 0.00014525]
	Learning Rate: 0.000145245
	LOSS [training: 0.03444881733430092 | validation: 0.017442412519617343]
	TIME [epoch: 8.19 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0340905628757281		[learning rate: 0.00014507]
		[batch 20/20] avg loss: 0.038467083094300074		[learning rate: 0.0001449]
	Learning Rate: 0.000144903
	LOSS [training: 0.03627882298501409 | validation: 0.00478543972738203]
	TIME [epoch: 8.2 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03767994905964405		[learning rate: 0.00014473]
		[batch 20/20] avg loss: 0.03847239242229667		[learning rate: 0.00014456]
	Learning Rate: 0.000144561
	LOSS [training: 0.03807617074097036 | validation: 0.022202783410728736]
	TIME [epoch: 8.21 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037584586256449296		[learning rate: 0.00014439]
		[batch 20/20] avg loss: 0.03669411820652666		[learning rate: 0.00014422]
	Learning Rate: 0.00014422
	LOSS [training: 0.03713935223148799 | validation: 0.01661467394329654]
	TIME [epoch: 8.19 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035047116277082464		[learning rate: 0.00014405]
		[batch 20/20] avg loss: 0.03858506931201413		[learning rate: 0.00014388]
	Learning Rate: 0.00014388
	LOSS [training: 0.0368160927945483 | validation: 0.011754633064027796]
	TIME [epoch: 8.2 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02611671732127104		[learning rate: 0.00014371]
		[batch 20/20] avg loss: 0.03474682869057345		[learning rate: 0.00014354]
	Learning Rate: 0.00014354
	LOSS [training: 0.030431773005922248 | validation: 0.00605396144045561]
	TIME [epoch: 8.2 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03689376169973692		[learning rate: 0.00014337]
		[batch 20/20] avg loss: 0.03162834627215525		[learning rate: 0.0001432]
	Learning Rate: 0.000143202
	LOSS [training: 0.03426105398594609 | validation: 0.020781523296375944]
	TIME [epoch: 8.2 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04120985970889216		[learning rate: 0.00014303]
		[batch 20/20] avg loss: 0.03502959162346949		[learning rate: 0.00014286]
	Learning Rate: 0.000142864
	LOSS [training: 0.038119725666180826 | validation: 0.007929452074390516]
	TIME [epoch: 8.19 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03045787292980807		[learning rate: 0.0001427]
		[batch 20/20] avg loss: 0.03829091680937857		[learning rate: 0.00014253]
	Learning Rate: 0.000142527
	LOSS [training: 0.034374394869593324 | validation: 0.013524527466396347]
	TIME [epoch: 8.19 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03495785177173783		[learning rate: 0.00014236]
		[batch 20/20] avg loss: 0.036065573570731625		[learning rate: 0.00014219]
	Learning Rate: 0.000142191
	LOSS [training: 0.03551171267123473 | validation: 0.012022341774435919]
	TIME [epoch: 8.21 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03209168130742834		[learning rate: 0.00014202]
		[batch 20/20] avg loss: 0.03834820709286539		[learning rate: 0.00014186]
	Learning Rate: 0.000141855
	LOSS [training: 0.03521994420014687 | validation: 0.012404553723948384]
	TIME [epoch: 8.2 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03348317758333283		[learning rate: 0.00014169]
		[batch 20/20] avg loss: 0.05227401306813576		[learning rate: 0.00014152]
	Learning Rate: 0.000141521
	LOSS [training: 0.04287859532573429 | validation: 0.038961944964195985]
	TIME [epoch: 8.2 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04602439036851723		[learning rate: 0.00014135]
		[batch 20/20] avg loss: 0.03882881964555597		[learning rate: 0.00014119]
	Learning Rate: 0.000141187
	LOSS [training: 0.042426605007036586 | validation: 0.017500153890332847]
	TIME [epoch: 8.2 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02606923607116256		[learning rate: 0.00014102]
		[batch 20/20] avg loss: 0.04059664879543728		[learning rate: 0.00014085]
	Learning Rate: 0.000140854
	LOSS [training: 0.03333294243329993 | validation: 0.00554927046565083]
	TIME [epoch: 8.21 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03376552674800584		[learning rate: 0.00014069]
		[batch 20/20] avg loss: 0.02990443056539769		[learning rate: 0.00014052]
	Learning Rate: 0.000140522
	LOSS [training: 0.03183497865670176 | validation: 0.0032356556345589553]
	TIME [epoch: 8.19 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03711281161571066		[learning rate: 0.00014036]
		[batch 20/20] avg loss: 0.0393320683428217		[learning rate: 0.00014019]
	Learning Rate: 0.00014019
	LOSS [training: 0.03822243997926618 | validation: 0.014307246631385837]
	TIME [epoch: 8.2 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0396407204714983		[learning rate: 0.00014002]
		[batch 20/20] avg loss: 0.03182624041011827		[learning rate: 0.00013986]
	Learning Rate: 0.00013986
	LOSS [training: 0.035733480440808284 | validation: 0.01524627282082698]
	TIME [epoch: 8.2 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034242975875919686		[learning rate: 0.00013969]
		[batch 20/20] avg loss: 0.036468429438021324		[learning rate: 0.00013953]
	Learning Rate: 0.00013953
	LOSS [training: 0.0353557026569705 | validation: 0.015588435623734152]
	TIME [epoch: 8.23 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035703400847897936		[learning rate: 0.00013937]
		[batch 20/20] avg loss: 0.03759357186723179		[learning rate: 0.0001392]
	Learning Rate: 0.000139201
	LOSS [training: 0.03664848635756486 | validation: 0.02109521981080115]
	TIME [epoch: 8.19 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031396351402607096		[learning rate: 0.00013904]
		[batch 20/20] avg loss: 0.04373022219050006		[learning rate: 0.00013887]
	Learning Rate: 0.000138872
	LOSS [training: 0.03756328679655358 | validation: 0.007204726603566296]
	TIME [epoch: 8.2 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027878968595854786		[learning rate: 0.00013871]
		[batch 20/20] avg loss: 0.042401895270164705		[learning rate: 0.00013854]
	Learning Rate: 0.000138545
	LOSS [training: 0.03514043193300975 | validation: 0.008623213420094101]
	TIME [epoch: 8.2 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03134056882646241		[learning rate: 0.00013838]
		[batch 20/20] avg loss: 0.037127894505915346		[learning rate: 0.00013822]
	Learning Rate: 0.000138218
	LOSS [training: 0.03423423166618887 | validation: 0.010630348580069542]
	TIME [epoch: 8.22 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038982828939804884		[learning rate: 0.00013805]
		[batch 20/20] avg loss: 0.0293528001614101		[learning rate: 0.00013789]
	Learning Rate: 0.000137892
	LOSS [training: 0.034167814550607495 | validation: 0.014586426226135093]
	TIME [epoch: 8.18 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03792640468862015		[learning rate: 0.00013773]
		[batch 20/20] avg loss: 0.03177575992860267		[learning rate: 0.00013757]
	Learning Rate: 0.000137567
	LOSS [training: 0.0348510823086114 | validation: 0.009446662908676958]
	TIME [epoch: 8.19 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03184426896368508		[learning rate: 0.0001374]
		[batch 20/20] avg loss: 0.03753272023319349		[learning rate: 0.00013724]
	Learning Rate: 0.000137242
	LOSS [training: 0.034688494598439285 | validation: 0.008957703219886292]
	TIME [epoch: 8.21 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030536158682257432		[learning rate: 0.00013708]
		[batch 20/20] avg loss: 0.03479677081877067		[learning rate: 0.00013692]
	Learning Rate: 0.000136918
	LOSS [training: 0.03266646475051405 | validation: 0.01558052321770427]
	TIME [epoch: 8.23 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034273287444227335		[learning rate: 0.00013676]
		[batch 20/20] avg loss: 0.04467416022138625		[learning rate: 0.0001366]
	Learning Rate: 0.000136595
	LOSS [training: 0.0394737238328068 | validation: 0.025297663588091417]
	TIME [epoch: 8.18 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046862118618458556		[learning rate: 0.00013643]
		[batch 20/20] avg loss: 0.036298069776834405		[learning rate: 0.00013627]
	Learning Rate: 0.000136273
	LOSS [training: 0.04158009419764649 | validation: 0.008427120128009127]
	TIME [epoch: 8.19 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03469325324344707		[learning rate: 0.00013611]
		[batch 20/20] avg loss: 0.055315088106972		[learning rate: 0.00013595]
	Learning Rate: 0.000135952
	LOSS [training: 0.04500417067520953 | validation: 0.017135595499121064]
	TIME [epoch: 8.19 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034321194771222543		[learning rate: 0.00013579]
		[batch 20/20] avg loss: 0.04585239648528042		[learning rate: 0.00013563]
	Learning Rate: 0.000135631
	LOSS [training: 0.04008679562825148 | validation: 0.013403187386097703]
	TIME [epoch: 8.22 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04522817094993249		[learning rate: 0.00013547]
		[batch 20/20] avg loss: 0.027193428296405364		[learning rate: 0.00013531]
	Learning Rate: 0.000135311
	LOSS [training: 0.03621079962316893 | validation: 0.01439282024556595]
	TIME [epoch: 8.18 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023362346178845955		[learning rate: 0.00013515]
		[batch 20/20] avg loss: 0.047064032818847856		[learning rate: 0.00013499]
	Learning Rate: 0.000134992
	LOSS [training: 0.03521318949884691 | validation: 0.03244211995590991]
	TIME [epoch: 8.19 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036263463639190065		[learning rate: 0.00013483]
		[batch 20/20] avg loss: 0.04311666259858791		[learning rate: 0.00013467]
	Learning Rate: 0.000134673
	LOSS [training: 0.03969006311888897 | validation: 0.018729888853831898]
	TIME [epoch: 8.21 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032198930002522594		[learning rate: 0.00013451]
		[batch 20/20] avg loss: 0.03301462866454944		[learning rate: 0.00013436]
	Learning Rate: 0.000134356
	LOSS [training: 0.03260677933353602 | validation: 0.01866553870810741]
	TIME [epoch: 8.23 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03323345031512344		[learning rate: 0.0001342]
		[batch 20/20] avg loss: 0.04069063914760511		[learning rate: 0.00013404]
	Learning Rate: 0.000134039
	LOSS [training: 0.03696204473136427 | validation: 0.020754912577747718]
	TIME [epoch: 8.19 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04172908169519324		[learning rate: 0.00013388]
		[batch 20/20] avg loss: 0.039601112880894164		[learning rate: 0.00013372]
	Learning Rate: 0.000133723
	LOSS [training: 0.0406650972880437 | validation: 0.013698304696670648]
	TIME [epoch: 8.19 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03435499625826427		[learning rate: 0.00013356]
		[batch 20/20] avg loss: 0.0410159645185852		[learning rate: 0.00013341]
	Learning Rate: 0.000133407
	LOSS [training: 0.03768548038842473 | validation: 0.008224024895025956]
	TIME [epoch: 8.23 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03542109723072721		[learning rate: 0.00013325]
		[batch 20/20] avg loss: 0.03572571225478652		[learning rate: 0.00013309]
	Learning Rate: 0.000133093
	LOSS [training: 0.035573404742756866 | validation: 0.01867366386339428]
	TIME [epoch: 8.22 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03269614487885168		[learning rate: 0.00013294]
		[batch 20/20] avg loss: 0.038453506080453385		[learning rate: 0.00013278]
	Learning Rate: 0.000132779
	LOSS [training: 0.03557482547965253 | validation: 0.019195297761895457]
	TIME [epoch: 8.18 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03539635045005489		[learning rate: 0.00013262]
		[batch 20/20] avg loss: 0.03952696543977712		[learning rate: 0.00013247]
	Learning Rate: 0.000132465
	LOSS [training: 0.037461657944916 | validation: 0.010680313603609198]
	TIME [epoch: 8.19 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03737060074753289		[learning rate: 0.00013231]
		[batch 20/20] avg loss: 0.02845673856329699		[learning rate: 0.00013215]
	Learning Rate: 0.000132153
	LOSS [training: 0.03291366965541494 | validation: 0.008999928667251189]
	TIME [epoch: 8.23 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03776684700075355		[learning rate: 0.000132]
		[batch 20/20] avg loss: 0.034410092728199324		[learning rate: 0.00013184]
	Learning Rate: 0.000131841
	LOSS [training: 0.03608846986447643 | validation: 0.021740678886921133]
	TIME [epoch: 8.21 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035108504700753544		[learning rate: 0.00013169]
		[batch 20/20] avg loss: 0.03367118490890312		[learning rate: 0.00013153]
	Learning Rate: 0.00013153
	LOSS [training: 0.03438984480482833 | validation: 0.012455668743252636]
	TIME [epoch: 8.19 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03434820619613742		[learning rate: 0.00013138]
		[batch 20/20] avg loss: 0.03473865450455203		[learning rate: 0.00013122]
	Learning Rate: 0.00013122
	LOSS [training: 0.03454343035034473 | validation: 0.019273610479948987]
	TIME [epoch: 8.19 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04187891809835627		[learning rate: 0.00013107]
		[batch 20/20] avg loss: 0.02913311448978363		[learning rate: 0.00013091]
	Learning Rate: 0.00013091
	LOSS [training: 0.03550601629406994 | validation: 0.016452200423072175]
	TIME [epoch: 8.23 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039756417121480474		[learning rate: 0.00013076]
		[batch 20/20] avg loss: 0.03341409162773775		[learning rate: 0.0001306]
	Learning Rate: 0.000130602
	LOSS [training: 0.03658525437460912 | validation: 0.016690621102021096]
	TIME [epoch: 8.21 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031959222000414615		[learning rate: 0.00013045]
		[batch 20/20] avg loss: 0.042723537779088334		[learning rate: 0.00013029]
	Learning Rate: 0.000130294
	LOSS [training: 0.03734137988975148 | validation: 0.017632144976081458]
	TIME [epoch: 8.18 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04223947958181608		[learning rate: 0.00013014]
		[batch 20/20] avg loss: 0.046901999246432124		[learning rate: 0.00012999]
	Learning Rate: 0.000129986
	LOSS [training: 0.044570739414124094 | validation: 0.02526452383590778]
	TIME [epoch: 8.19 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032219604759563744		[learning rate: 0.00012983]
		[batch 20/20] avg loss: 0.0389833599064524		[learning rate: 0.00012968]
	Learning Rate: 0.00012968
	LOSS [training: 0.03560148233300807 | validation: 0.016732824778616463]
	TIME [epoch: 8.24 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03306296948527713		[learning rate: 0.00012953]
		[batch 20/20] avg loss: 0.04115220883953736		[learning rate: 0.00012937]
	Learning Rate: 0.000129374
	LOSS [training: 0.03710758916240725 | validation: 0.01952672750360854]
	TIME [epoch: 8.2 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050616622803380175		[learning rate: 0.00012922]
		[batch 20/20] avg loss: 0.02964140510306466		[learning rate: 0.00012907]
	Learning Rate: 0.000129069
	LOSS [training: 0.04012901395322242 | validation: 0.014551580526602754]
	TIME [epoch: 8.17 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02974692052145201		[learning rate: 0.00012892]
		[batch 20/20] avg loss: 0.040476956512905574		[learning rate: 0.00012876]
	Learning Rate: 0.000128764
	LOSS [training: 0.035111938517178795 | validation: 0.00917628056936412]
	TIME [epoch: 8.19 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03823920019030479		[learning rate: 0.00012861]
		[batch 20/20] avg loss: 0.033706964352271836		[learning rate: 0.00012846]
	Learning Rate: 0.00012846
	LOSS [training: 0.035973082271288315 | validation: 0.009412050271784198]
	TIME [epoch: 8.25 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0372266570732033		[learning rate: 0.00012831]
		[batch 20/20] avg loss: 0.03663943061824842		[learning rate: 0.00012816]
	Learning Rate: 0.000128157
	LOSS [training: 0.03693304384572586 | validation: 0.014143414880243418]
	TIME [epoch: 8.19 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037663728102512936		[learning rate: 0.00012801]
		[batch 20/20] avg loss: 0.03196972523840319		[learning rate: 0.00012786]
	Learning Rate: 0.000127855
	LOSS [training: 0.034816726670458056 | validation: 0.01553301794368334]
	TIME [epoch: 8.19 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04107998634081651		[learning rate: 0.0001277]
		[batch 20/20] avg loss: 0.033552769140179566		[learning rate: 0.00012755]
	Learning Rate: 0.000127553
	LOSS [training: 0.03731637774049804 | validation: 0.018654937487974057]
	TIME [epoch: 8.18 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03119360163792604		[learning rate: 0.0001274]
		[batch 20/20] avg loss: 0.036903384855944465		[learning rate: 0.00012725]
	Learning Rate: 0.000127253
	LOSS [training: 0.034048493246935256 | validation: 0.014533363220996815]
	TIME [epoch: 8.25 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0350315106371097		[learning rate: 0.0001271]
		[batch 20/20] avg loss: 0.03526786220333643		[learning rate: 0.00012695]
	Learning Rate: 0.000126952
	LOSS [training: 0.03514968642022306 | validation: 0.01136591874414015]
	TIME [epoch: 8.19 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031325678592117136		[learning rate: 0.0001268]
		[batch 20/20] avg loss: 0.04858706773782053		[learning rate: 0.00012665]
	Learning Rate: 0.000126653
	LOSS [training: 0.03995637316496883 | validation: 0.016498062233779597]
	TIME [epoch: 8.19 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03346945486028097		[learning rate: 0.0001265]
		[batch 20/20] avg loss: 0.03388113191343539		[learning rate: 0.00012635]
	Learning Rate: 0.000126354
	LOSS [training: 0.03367529338685818 | validation: 0.019348792487099822]
	TIME [epoch: 8.19 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03781587750068503		[learning rate: 0.00012621]
		[batch 20/20] avg loss: 0.03443129429361243		[learning rate: 0.00012606]
	Learning Rate: 0.000126056
	LOSS [training: 0.03612358589714873 | validation: 0.008460664955407222]
	TIME [epoch: 8.25 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03133215591396213		[learning rate: 0.00012591]
		[batch 20/20] avg loss: 0.041953297967230865		[learning rate: 0.00012576]
	Learning Rate: 0.000125759
	LOSS [training: 0.03664272694059649 | validation: 0.01776065626565087]
	TIME [epoch: 8.19 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039108927647647056		[learning rate: 0.00012561]
		[batch 20/20] avg loss: 0.030233498824282824		[learning rate: 0.00012546]
	Learning Rate: 0.000125462
	LOSS [training: 0.034671213235964934 | validation: 0.015941310568443383]
	TIME [epoch: 8.18 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03568410965583621		[learning rate: 0.00012531]
		[batch 20/20] avg loss: 0.03960025201220738		[learning rate: 0.00012517]
	Learning Rate: 0.000125166
	LOSS [training: 0.0376421808340218 | validation: 0.01710569932508819]
	TIME [epoch: 8.18 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03984665852626631		[learning rate: 0.00012502]
		[batch 20/20] avg loss: 0.04687526559171369		[learning rate: 0.00012487]
	Learning Rate: 0.000124871
	LOSS [training: 0.04336096205899 | validation: 0.025842828616701438]
	TIME [epoch: 8.25 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032479708628268086		[learning rate: 0.00012472]
		[batch 20/20] avg loss: 0.035166180868365544		[learning rate: 0.00012458]
	Learning Rate: 0.000124576
	LOSS [training: 0.03382294474831681 | validation: 0.010546674329803912]
	TIME [epoch: 8.19 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04248375741298667		[learning rate: 0.00012443]
		[batch 20/20] avg loss: 0.03208506850813919		[learning rate: 0.00012428]
	Learning Rate: 0.000124283
	LOSS [training: 0.037284412960562924 | validation: 0.008975044650277578]
	TIME [epoch: 8.18 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034569281460582746		[learning rate: 0.00012414]
		[batch 20/20] avg loss: 0.03603884005499857		[learning rate: 0.00012399]
	Learning Rate: 0.000123989
	LOSS [training: 0.03530406075779066 | validation: 0.016848754257562645]
	TIME [epoch: 8.2 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029111923698842607		[learning rate: 0.00012384]
		[batch 20/20] avg loss: 0.038198691901039514		[learning rate: 0.0001237]
	Learning Rate: 0.000123697
	LOSS [training: 0.03365530779994105 | validation: 0.008395696459721366]
	TIME [epoch: 8.24 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0353345305789224		[learning rate: 0.00012355]
		[batch 20/20] avg loss: 0.0345042557801283		[learning rate: 0.00012341]
	Learning Rate: 0.000123405
	LOSS [training: 0.034919393179525346 | validation: 0.02213965144436246]
	TIME [epoch: 8.19 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03222240787959165		[learning rate: 0.00012326]
		[batch 20/20] avg loss: 0.044484533236457714		[learning rate: 0.00012311]
	Learning Rate: 0.000123114
	LOSS [training: 0.03835347055802469 | validation: 0.029272324847302484]
	TIME [epoch: 8.18 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03759122206912004		[learning rate: 0.00012297]
		[batch 20/20] avg loss: 0.03698976570046988		[learning rate: 0.00012282]
	Learning Rate: 0.000122824
	LOSS [training: 0.037290493884794954 | validation: 0.007825008789037882]
	TIME [epoch: 8.21 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029666925274187073		[learning rate: 0.00012268]
		[batch 20/20] avg loss: 0.03940906807656073		[learning rate: 0.00012253]
	Learning Rate: 0.000122534
	LOSS [training: 0.0345379966753739 | validation: 0.015930380554854965]
	TIME [epoch: 8.23 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03208852452185172		[learning rate: 0.00012239]
		[batch 20/20] avg loss: 0.04638046859140767		[learning rate: 0.00012224]
	Learning Rate: 0.000122245
	LOSS [training: 0.039234496556629694 | validation: 0.013549391255064379]
	TIME [epoch: 8.19 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03843956681624945		[learning rate: 0.0001221]
		[batch 20/20] avg loss: 0.03928069435452123		[learning rate: 0.00012196]
	Learning Rate: 0.000121957
	LOSS [training: 0.03886013058538534 | validation: 0.013161825974202408]
	TIME [epoch: 8.19 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034423508767462585		[learning rate: 0.00012181]
		[batch 20/20] avg loss: 0.0297210098045445		[learning rate: 0.00012167]
	Learning Rate: 0.000121669
	LOSS [training: 0.032072259286003546 | validation: 0.01258211695328753]
	TIME [epoch: 8.23 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04425882437247118		[learning rate: 0.00012153]
		[batch 20/20] avg loss: 0.03161017166494538		[learning rate: 0.00012138]
	Learning Rate: 0.000121382
	LOSS [training: 0.03793449801870828 | validation: 0.012513761505137527]
	TIME [epoch: 8.23 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03913456149476034		[learning rate: 0.00012124]
		[batch 20/20] avg loss: 0.033827459849086854		[learning rate: 0.0001211]
	Learning Rate: 0.000121096
	LOSS [training: 0.0364810106719236 | validation: 0.013980767046243238]
	TIME [epoch: 8.18 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037438987354246264		[learning rate: 0.00012095]
		[batch 20/20] avg loss: 0.03200563649131742		[learning rate: 0.00012081]
	Learning Rate: 0.00012081
	LOSS [training: 0.03472231192278184 | validation: 0.006985208921481323]
	TIME [epoch: 8.18 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03445701840300761		[learning rate: 0.00012067]
		[batch 20/20] avg loss: 0.036248116026042124		[learning rate: 0.00012052]
	Learning Rate: 0.000120525
	LOSS [training: 0.035352567214524865 | validation: 0.019729596949444192]
	TIME [epoch: 8.22 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04149827883032923		[learning rate: 0.00012038]
		[batch 20/20] avg loss: 0.029232981194354846		[learning rate: 0.00012024]
	Learning Rate: 0.000120241
	LOSS [training: 0.03536563001234204 | validation: 0.012606465523247115]
	TIME [epoch: 8.21 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045426080713547576		[learning rate: 0.0001201]
		[batch 20/20] avg loss: 0.026968281771288943		[learning rate: 0.00011996]
	Learning Rate: 0.000119957
	LOSS [training: 0.036197181242418254 | validation: 0.01445850058829408]
	TIME [epoch: 8.18 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032445662126775264		[learning rate: 0.00011982]
		[batch 20/20] avg loss: 0.03701764557264318		[learning rate: 0.00011967]
	Learning Rate: 0.000119674
	LOSS [training: 0.034731653849709214 | validation: 0.01716294177644467]
	TIME [epoch: 8.17 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03234488837002533		[learning rate: 0.00011953]
		[batch 20/20] avg loss: 0.040949354888607445		[learning rate: 0.00011939]
	Learning Rate: 0.000119392
	LOSS [training: 0.036647121629316375 | validation: 0.008181652499745666]
	TIME [epoch: 8.21 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04124096718094312		[learning rate: 0.00011925]
		[batch 20/20] avg loss: 0.03735915891905995		[learning rate: 0.00011911]
	Learning Rate: 0.00011911
	LOSS [training: 0.039300063050001534 | validation: 0.030721645510393136]
	TIME [epoch: 8.2 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05332734428981841		[learning rate: 0.00011897]
		[batch 20/20] avg loss: 0.03560015582354734		[learning rate: 0.00011883]
	Learning Rate: 0.000118829
	LOSS [training: 0.04446375005668288 | validation: 0.009496915005905068]
	TIME [epoch: 8.17 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03714573583450308		[learning rate: 0.00011869]
		[batch 20/20] avg loss: 0.030095756654963522		[learning rate: 0.00011855]
	Learning Rate: 0.000118549
	LOSS [training: 0.03362074624473329 | validation: 0.008300870764774964]
	TIME [epoch: 8.18 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04026710527938685		[learning rate: 0.00011841]
		[batch 20/20] avg loss: 0.02457670015164689		[learning rate: 0.00011827]
	Learning Rate: 0.000118269
	LOSS [training: 0.03242190271551687 | validation: 0.01047697131408749]
	TIME [epoch: 8.26 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02978492840521025		[learning rate: 0.00011813]
		[batch 20/20] avg loss: 0.03207655606958375		[learning rate: 0.00011799]
	Learning Rate: 0.00011799
	LOSS [training: 0.03093074223739699 | validation: 0.010029500906498179]
	TIME [epoch: 8.21 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034177239222433264		[learning rate: 0.00011785]
		[batch 20/20] avg loss: 0.03071570214481683		[learning rate: 0.00011771]
	Learning Rate: 0.000117712
	LOSS [training: 0.03244647068362505 | validation: 0.009178393266138672]
	TIME [epoch: 8.19 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036221728369112416		[learning rate: 0.00011757]
		[batch 20/20] avg loss: 0.03337637596873501		[learning rate: 0.00011743]
	Learning Rate: 0.000117434
	LOSS [training: 0.03479905216892372 | validation: 0.014380393813526345]
	TIME [epoch: 8.2 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037605207737134105		[learning rate: 0.0001173]
		[batch 20/20] avg loss: 0.03501086356138801		[learning rate: 0.00011716]
	Learning Rate: 0.000117157
	LOSS [training: 0.03630803564926106 | validation: 0.0098370003186955]
	TIME [epoch: 8.21 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03308252476786285		[learning rate: 0.00011702]
		[batch 20/20] avg loss: 0.03143402424596724		[learning rate: 0.00011688]
	Learning Rate: 0.000116881
	LOSS [training: 0.03225827450691504 | validation: 0.006690792064472855]
	TIME [epoch: 8.19 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03268885391335439		[learning rate: 0.00011674]
		[batch 20/20] avg loss: 0.032585231618225616		[learning rate: 0.00011661]
	Learning Rate: 0.000116605
	LOSS [training: 0.03263704276579 | validation: 0.014534364235067787]
	TIME [epoch: 8.19 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03598406014114166		[learning rate: 0.00011647]
		[batch 20/20] avg loss: 0.03539648685948463		[learning rate: 0.00011633]
	Learning Rate: 0.00011633
	LOSS [training: 0.03569027350031314 | validation: 0.01844789182643893]
	TIME [epoch: 8.2 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037747982514559034		[learning rate: 0.00011619]
		[batch 20/20] avg loss: 0.03446724721061202		[learning rate: 0.00011606]
	Learning Rate: 0.000116056
	LOSS [training: 0.03610761486258553 | validation: 0.008839136303967743]
	TIME [epoch: 8.21 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02275659095866599		[learning rate: 0.00011592]
		[batch 20/20] avg loss: 0.04008869839462916		[learning rate: 0.00011578]
	Learning Rate: 0.000115782
	LOSS [training: 0.03142264467664757 | validation: 0.008180701554175515]
	TIME [epoch: 8.21 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03456780178674624		[learning rate: 0.00011565]
		[batch 20/20] avg loss: 0.03501145896805118		[learning rate: 0.00011551]
	Learning Rate: 0.000115509
	LOSS [training: 0.034789630377398705 | validation: 0.010978598358883731]
	TIME [epoch: 8.2 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03898676898059615		[learning rate: 0.00011537]
		[batch 20/20] avg loss: 0.053461927941701196		[learning rate: 0.00011524]
	Learning Rate: 0.000115236
	LOSS [training: 0.04622434846114868 | validation: 0.019932545579133536]
	TIME [epoch: 8.22 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03846525241722165		[learning rate: 0.0001151]
		[batch 20/20] avg loss: 0.03436847086399429		[learning rate: 0.00011496]
	Learning Rate: 0.000114965
	LOSS [training: 0.03641686164060798 | validation: 0.002461123108350383]
	TIME [epoch: 8.2 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03341045471443078		[learning rate: 0.00011483]
		[batch 20/20] avg loss: 0.03365737580518045		[learning rate: 0.00011469]
	Learning Rate: 0.000114693
	LOSS [training: 0.03353391525980561 | validation: 0.005621293318439146]
	TIME [epoch: 8.2 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03367027238677671		[learning rate: 0.00011456]
		[batch 20/20] avg loss: 0.036387730394955		[learning rate: 0.00011442]
	Learning Rate: 0.000114423
	LOSS [training: 0.03502900139086586 | validation: 0.01268927550956461]
	TIME [epoch: 8.18 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029829967280451848		[learning rate: 0.00011429]
		[batch 20/20] avg loss: 0.03170384610676703		[learning rate: 0.00011415]
	Learning Rate: 0.000114153
	LOSS [training: 0.03076690669360944 | validation: 0.011314046446778324]
	TIME [epoch: 8.2 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03304178808951397		[learning rate: 0.00011402]
		[batch 20/20] avg loss: 0.0431182610467904		[learning rate: 0.00011388]
	Learning Rate: 0.000113884
	LOSS [training: 0.03808002456815219 | validation: 0.011526977147966418]
	TIME [epoch: 8.19 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045514953176965835		[learning rate: 0.00011375]
		[batch 20/20] avg loss: 0.03228035677794973		[learning rate: 0.00011362]
	Learning Rate: 0.000113615
	LOSS [training: 0.038897654977457785 | validation: 0.011706002336135083]
	TIME [epoch: 8.2 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030427334617854834		[learning rate: 0.00011348]
		[batch 20/20] avg loss: 0.038757141948873516		[learning rate: 0.00011335]
	Learning Rate: 0.000113347
	LOSS [training: 0.03459223828336418 | validation: 0.010910668524742068]
	TIME [epoch: 8.18 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04617400996676458		[learning rate: 0.00011321]
		[batch 20/20] avg loss: 0.031409735218626764		[learning rate: 0.00011308]
	Learning Rate: 0.00011308
	LOSS [training: 0.03879187259269568 | validation: 0.00919277732266214]
	TIME [epoch: 8.22 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033181404651943004		[learning rate: 0.00011295]
		[batch 20/20] avg loss: 0.04014162056532062		[learning rate: 0.00011281]
	Learning Rate: 0.000112813
	LOSS [training: 0.03666151260863181 | validation: 0.0037832771876589945]
	TIME [epoch: 8.2 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03589567823758369		[learning rate: 0.00011268]
		[batch 20/20] avg loss: 0.032941960618719734		[learning rate: 0.00011255]
	Learning Rate: 0.000112547
	LOSS [training: 0.03441881942815171 | validation: 0.0036715168041688653]
	TIME [epoch: 8.19 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03703685327998366		[learning rate: 0.00011241]
		[batch 20/20] avg loss: 0.0392218575409309		[learning rate: 0.00011228]
	Learning Rate: 0.000112281
	LOSS [training: 0.03812935541045728 | validation: 0.018349205295675356]
	TIME [epoch: 8.2 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033880302929311226		[learning rate: 0.00011215]
		[batch 20/20] avg loss: 0.031483200119597064		[learning rate: 0.00011202]
	Learning Rate: 0.000112017
	LOSS [training: 0.03268175152445415 | validation: 0.008169424430305692]
	TIME [epoch: 8.22 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03692311461369637		[learning rate: 0.00011188]
		[batch 20/20] avg loss: 0.026089163998459213		[learning rate: 0.00011175]
	Learning Rate: 0.000111752
	LOSS [training: 0.03150613930607779 | validation: 0.009384952033649408]
	TIME [epoch: 8.19 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0382967634198062		[learning rate: 0.00011162]
		[batch 20/20] avg loss: 0.03323300882467166		[learning rate: 0.00011149]
	Learning Rate: 0.000111489
	LOSS [training: 0.03576488612223893 | validation: 0.015803916000815127]
	TIME [epoch: 8.2 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029343297687654125		[learning rate: 0.00011136]
		[batch 20/20] avg loss: 0.0347287823335253		[learning rate: 0.00011123]
	Learning Rate: 0.000111226
	LOSS [training: 0.032036040010589704 | validation: 0.02082809198036346]
	TIME [epoch: 8.2 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035473929853339074		[learning rate: 0.00011109]
		[batch 20/20] avg loss: 0.03809943128569897		[learning rate: 0.00011096]
	Learning Rate: 0.000110963
	LOSS [training: 0.03678668056951903 | validation: 0.019302227269912986]
	TIME [epoch: 8.23 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03913599039970238		[learning rate: 0.00011083]
		[batch 20/20] avg loss: 0.03316859516480099		[learning rate: 0.0001107]
	Learning Rate: 0.000110702
	LOSS [training: 0.036152292782251676 | validation: 0.0037456546848755844]
	TIME [epoch: 8.19 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0343033265863306		[learning rate: 0.00011057]
		[batch 20/20] avg loss: 0.03557336411828951		[learning rate: 0.00011044]
	Learning Rate: 0.00011044
	LOSS [training: 0.03493834535231005 | validation: 0.00943711816162848]
	TIME [epoch: 8.19 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04031340017388603		[learning rate: 0.00011031]
		[batch 20/20] avg loss: 0.025006456573667214		[learning rate: 0.00011018]
	Learning Rate: 0.00011018
	LOSS [training: 0.03265992837377661 | validation: 0.007205432639111358]
	TIME [epoch: 8.2 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03647012782125731		[learning rate: 0.00011005]
		[batch 20/20] avg loss: 0.030797294949246767		[learning rate: 0.00010992]
	Learning Rate: 0.00010992
	LOSS [training: 0.03363371138525204 | validation: 0.012325827115010983]
	TIME [epoch: 8.23 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030156434307324294		[learning rate: 0.00010979]
		[batch 20/20] avg loss: 0.03644678969808472		[learning rate: 0.00010966]
	Learning Rate: 0.000109661
	LOSS [training: 0.0333016120027045 | validation: 0.01623808666462066]
	TIME [epoch: 8.19 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0372862692340792		[learning rate: 0.00010953]
		[batch 20/20] avg loss: 0.026734855136227892		[learning rate: 0.0001094]
	Learning Rate: 0.000109402
	LOSS [training: 0.032010562185153545 | validation: 0.0166432618411201]
	TIME [epoch: 8.19 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034757789662762315		[learning rate: 0.00010927]
		[batch 20/20] avg loss: 0.036325490596951816		[learning rate: 0.00010914]
	Learning Rate: 0.000109144
	LOSS [training: 0.035541640129857055 | validation: 0.009493178641912967]
	TIME [epoch: 8.2 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03398628805951222		[learning rate: 0.00010902]
		[batch 20/20] avg loss: 0.04222197424102643		[learning rate: 0.00010889]
	Learning Rate: 0.000108887
	LOSS [training: 0.038104131150269324 | validation: 0.011107606038784352]
	TIME [epoch: 8.21 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0359951803320825		[learning rate: 0.00010876]
		[batch 20/20] avg loss: 0.033072296903260585		[learning rate: 0.00010863]
	Learning Rate: 0.00010863
	LOSS [training: 0.03453373861767154 | validation: 0.010009994652794523]
	TIME [epoch: 8.18 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03151384772691464		[learning rate: 0.0001085]
		[batch 20/20] avg loss: 0.03959211042757506		[learning rate: 0.00010837]
	Learning Rate: 0.000108373
	LOSS [training: 0.03555297907724485 | validation: 0.014751719431433871]
	TIME [epoch: 8.2 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04218522569088764		[learning rate: 0.00010825]
		[batch 20/20] avg loss: 0.03701263084158016		[learning rate: 0.00010812]
	Learning Rate: 0.000108118
	LOSS [training: 0.0395989282662339 | validation: 0.015254361350423554]
	TIME [epoch: 8.21 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029573120657028572		[learning rate: 0.00010799]
		[batch 20/20] avg loss: 0.034948626838110064		[learning rate: 0.00010786]
	Learning Rate: 0.000107863
	LOSS [training: 0.03226087374756932 | validation: 0.010674842036398388]
	TIME [epoch: 8.2 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03918278113746532		[learning rate: 0.00010774]
		[batch 20/20] avg loss: 0.02828284460381092		[learning rate: 0.00010761]
	Learning Rate: 0.000107608
	LOSS [training: 0.03373281287063812 | validation: 0.010224333733570238]
	TIME [epoch: 8.19 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03663407338942108		[learning rate: 0.00010748]
		[batch 20/20] avg loss: 0.03201205455802817		[learning rate: 0.00010735]
	Learning Rate: 0.000107355
	LOSS [training: 0.03432306397372462 | validation: 0.008147966332483254]
	TIME [epoch: 8.2 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030089260142638713		[learning rate: 0.00010723]
		[batch 20/20] avg loss: 0.03487810433678225		[learning rate: 0.0001071]
	Learning Rate: 0.000107101
	LOSS [training: 0.03248368223971048 | validation: 0.014926749320902492]
	TIME [epoch: 8.21 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037372464237399136		[learning rate: 0.00010697]
		[batch 20/20] avg loss: 0.02788922317939311		[learning rate: 0.00010685]
	Learning Rate: 0.000106849
	LOSS [training: 0.032630843708396126 | validation: 0.013583778594906733]
	TIME [epoch: 8.21 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029855695573398195		[learning rate: 0.00010672]
		[batch 20/20] avg loss: 0.03840191764828327		[learning rate: 0.0001066]
	Learning Rate: 0.000106597
	LOSS [training: 0.03412880661084074 | validation: 0.010534591612709782]
	TIME [epoch: 8.18 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031172134463766227		[learning rate: 0.00010647]
		[batch 20/20] avg loss: 0.035835164408083084		[learning rate: 0.00010635]
	Learning Rate: 0.000106345
	LOSS [training: 0.033503649435924654 | validation: 0.019123213676695262]
	TIME [epoch: 8.2 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03526386650382362		[learning rate: 0.00010622]
		[batch 20/20] avg loss: 0.0339386977621085		[learning rate: 0.00010609]
	Learning Rate: 0.000106094
	LOSS [training: 0.03460128213296605 | validation: 0.01338833789967149]
	TIME [epoch: 8.22 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032138474468340304		[learning rate: 0.00010597]
		[batch 20/20] avg loss: 0.038314126251356204		[learning rate: 0.00010584]
	Learning Rate: 0.000105844
	LOSS [training: 0.03522630035984825 | validation: 0.010988511288208757]
	TIME [epoch: 8.21 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03510906721443753		[learning rate: 0.00010572]
		[batch 20/20] avg loss: 0.04137383787635968		[learning rate: 0.00010559]
	Learning Rate: 0.000105594
	LOSS [training: 0.03824145254539861 | validation: 0.008434959929964321]
	TIME [epoch: 8.18 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04922288306202127		[learning rate: 0.00010547]
		[batch 20/20] avg loss: 0.03742579661624758		[learning rate: 0.00010535]
	Learning Rate: 0.000105345
	LOSS [training: 0.043324339839134425 | validation: 0.013790874155483699]
	TIME [epoch: 8.19 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03323615130704746		[learning rate: 0.00010522]
		[batch 20/20] avg loss: 0.03775567866615677		[learning rate: 0.0001051]
	Learning Rate: 0.000105097
	LOSS [training: 0.03549591498660211 | validation: 0.0077896052946037735]
	TIME [epoch: 8.23 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03420319100045587		[learning rate: 0.00010497]
		[batch 20/20] avg loss: 0.03325733631607547		[learning rate: 0.00010485]
	Learning Rate: 0.000104849
	LOSS [training: 0.033730263658265675 | validation: 0.0033588909374245874]
	TIME [epoch: 8.2 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03280652660048537		[learning rate: 0.00010473]
		[batch 20/20] avg loss: 0.02874693096393955		[learning rate: 0.0001046]
	Learning Rate: 0.000104602
	LOSS [training: 0.030776728782212458 | validation: 0.011948886052940137]
	TIME [epoch: 8.18 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02754933180515896		[learning rate: 0.00010448]
		[batch 20/20] avg loss: 0.03861406552866759		[learning rate: 0.00010435]
	Learning Rate: 0.000104355
	LOSS [training: 0.03308169866691328 | validation: 0.01061900319071901]
	TIME [epoch: 8.19 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03708215654653511		[learning rate: 0.00010423]
		[batch 20/20] avg loss: 0.037712289565724595		[learning rate: 0.00010411]
	Learning Rate: 0.000104109
	LOSS [training: 0.037397223056129844 | validation: 0.017962407544062097]
	TIME [epoch: 8.24 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043505882282174946		[learning rate: 0.00010399]
		[batch 20/20] avg loss: 0.032224056626255174		[learning rate: 0.00010386]
	Learning Rate: 0.000103863
	LOSS [training: 0.03786496945421507 | validation: 0.011965174377814896]
	TIME [epoch: 8.2 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031038909329330743		[learning rate: 0.00010374]
		[batch 20/20] avg loss: 0.040387047614247204		[learning rate: 0.00010362]
	Learning Rate: 0.000103618
	LOSS [training: 0.03571297847178897 | validation: 0.009713316159178344]
	TIME [epoch: 8.19 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034725401191838076		[learning rate: 0.0001035]
		[batch 20/20] avg loss: 0.029353086174613824		[learning rate: 0.00010337]
	Learning Rate: 0.000103374
	LOSS [training: 0.03203924368322595 | validation: 0.014834704763930041]
	TIME [epoch: 8.2 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03368120276141688		[learning rate: 0.00010325]
		[batch 20/20] avg loss: 0.026991676466362406		[learning rate: 0.00010313]
	Learning Rate: 0.00010313
	LOSS [training: 0.030336439613889644 | validation: 0.007074184310480378]
	TIME [epoch: 8.24 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03755135457541525		[learning rate: 0.00010301]
		[batch 20/20] avg loss: 0.03292876797998663		[learning rate: 0.00010289]
	Learning Rate: 0.000102887
	LOSS [training: 0.03524006127770095 | validation: 0.017134587233182185]
	TIME [epoch: 8.19 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027201531596274038		[learning rate: 0.00010277]
		[batch 20/20] avg loss: 0.038729865475212996		[learning rate: 0.00010264]
	Learning Rate: 0.000102644
	LOSS [training: 0.032965698535743514 | validation: 0.005978796846204575]
	TIME [epoch: 8.19 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028860053994880445		[learning rate: 0.00010252]
		[batch 20/20] avg loss: 0.03250875122976607		[learning rate: 0.0001024]
	Learning Rate: 0.000102402
	LOSS [training: 0.030684402612323258 | validation: 0.013225176377310802]
	TIME [epoch: 8.19 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025370664947660198		[learning rate: 0.00010228]
		[batch 20/20] avg loss: 0.03653966426453652		[learning rate: 0.00010216]
	Learning Rate: 0.00010216
	LOSS [training: 0.030955164606098363 | validation: 0.013766779558718103]
	TIME [epoch: 8.26 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03524300927082517		[learning rate: 0.00010204]
		[batch 20/20] avg loss: 0.03469782258548469		[learning rate: 0.00010192]
	Learning Rate: 0.000101919
	LOSS [training: 0.03497041592815493 | validation: 0.0173000016725578]
	TIME [epoch: 8.19 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03963379952678191		[learning rate: 0.0001018]
		[batch 20/20] avg loss: 0.03701020518254207		[learning rate: 0.00010168]
	Learning Rate: 0.000101679
	LOSS [training: 0.038322002354661995 | validation: 0.027003025277913192]
	TIME [epoch: 8.18 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05334222285813396		[learning rate: 0.00010156]
		[batch 20/20] avg loss: 0.031320532514433216		[learning rate: 0.00010144]
	Learning Rate: 0.000101439
	LOSS [training: 0.042331377686283575 | validation: 0.0037078294554029963]
	TIME [epoch: 8.18 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03667295664910975		[learning rate: 0.00010132]
		[batch 20/20] avg loss: 0.033166453540982195		[learning rate: 0.0001012]
	Learning Rate: 0.0001012
	LOSS [training: 0.03491970509504597 | validation: 0.008914077538930924]
	TIME [epoch: 8.25 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03549271989827677		[learning rate: 0.00010108]
		[batch 20/20] avg loss: 0.030409924114481596		[learning rate: 0.00010096]
	Learning Rate: 0.000100961
	LOSS [training: 0.03295132200637918 | validation: 0.009302201647925858]
	TIME [epoch: 8.19 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03430579843028541		[learning rate: 0.00010084]
		[batch 20/20] avg loss: 0.027952565102484346		[learning rate: 0.00010072]
	Learning Rate: 0.000100723
	LOSS [training: 0.03112918176638487 | validation: 0.005249610332915931]
	TIME [epoch: 8.19 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03053062640626929		[learning rate: 0.0001006]
		[batch 20/20] avg loss: 0.04561839492274534		[learning rate: 0.00010049]
	Learning Rate: 0.000100485
	LOSS [training: 0.03807451066450731 | validation: 0.018083087126481226]
	TIME [epoch: 8.2 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03362323954300629		[learning rate: 0.00010037]
		[batch 20/20] avg loss: 0.03574031614824399		[learning rate: 0.00010025]
	Learning Rate: 0.000100248
	LOSS [training: 0.03468177784562514 | validation: 0.01818950557215736]
	TIME [epoch: 8.24 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050292667599219944		[learning rate: 0.00010013]
		[batch 20/20] avg loss: 0.02248686536989883		[learning rate: 0.00010001]
	Learning Rate: 0.000100012
	LOSS [training: 0.03638976648455938 | validation: 0.011845600006596006]
	TIME [epoch: 8.19 sec]
Finished training in 16554.987 seconds.
