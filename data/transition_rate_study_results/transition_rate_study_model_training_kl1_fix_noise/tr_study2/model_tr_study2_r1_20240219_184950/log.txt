Args:
Namespace(name='model_tr_study2', outdir='out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1', training_data='data/transition_rate_studies/tr_study2/tr_study2_training/r1', validation_data='data/transition_rate_studies/tr_study2/tr_study2_validation/r1', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.1, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 572679360

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.132619605372364		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.217274073599707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.674946839486033 | validation: 7.423238801435748]
	TIME [epoch: 79.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.548846582466216		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.578901533351306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.06387405790876 | validation: 5.4659400638874365]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.053987523661031		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.288675989396916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.671331756528973 | validation: 4.53352585659757]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.0062602313705735		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5767457928710824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7915030121208275 | validation: 3.443083242008722]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.388287089089123		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.408482039072541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.398384564080831 | validation: 3.211925751739721]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2407434282371232		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.192060406804985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.216401917521054 | validation: 2.4718584741350087]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.108114425386364		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7335329194232156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9208236724047896 | validation: 1.7187543517312922]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4844800197560732		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.386061084830423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4352705522932478 | validation: 1.5317339762851914]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3339131376763538		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4177978438910346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.375855490783694 | validation: 1.1364866094621013]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3692391205732064		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3603470086675222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3647930646203643 | validation: 1.0774056439584758]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3992265585286014		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1770493979581436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2881379782433724 | validation: 1.4680476170604317]
	TIME [epoch: 8.37 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2299233611618217		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2982077776009042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.264065569381363 | validation: 3.296941063753329]
	TIME [epoch: 8.38 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0179883773376246		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2608660160067362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6394271966721803 | validation: 0.9550851484982417]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1197951159421806		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0682641548913847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0940296354167827 | validation: 0.8444864920993336]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9559371256861233		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7732764158413914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8646067707637574 | validation: 1.8186182072822106]
	TIME [epoch: 8.36 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0607916976845655		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1650029507287623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1128973242066642 | validation: 0.5609213119846954]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7620269966280702		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7794590705261567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7707430335771136 | validation: 0.37684073762695636]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8538505436591268		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8824775051722247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8681640244156759 | validation: 0.5019711267081322]
	TIME [epoch: 8.36 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7304092135663764		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.609660936076635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6700350748215056 | validation: 0.5253061121956987]
	TIME [epoch: 8.36 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6893063381130482		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.586563720650785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6379350293819166 | validation: 1.5429296688758678]
	TIME [epoch: 8.38 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7191977156179019		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.621215768411556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6702067420147289 | validation: 0.44271404870969133]
	TIME [epoch: 8.35 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6660078314812262		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6015704597650051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6337891456231156 | validation: 0.8054442348463493]
	TIME [epoch: 8.35 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6761041451556384		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7990696572607761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7375869012082072 | validation: 0.5725405060034797]
	TIME [epoch: 8.35 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6831763124774375		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7321911698662122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7076837411718249 | validation: 1.016684046522896]
	TIME [epoch: 8.38 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6563524164184392		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.560604049577934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6084782329981864 | validation: 0.45400460437000834]
	TIME [epoch: 8.35 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5804421649456959		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5399851465894985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5602136557675972 | validation: 0.6040761299473918]
	TIME [epoch: 8.35 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7160755359986327		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5312706636085369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6236730998035849 | validation: 0.31306835875588973]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4658639974195903		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.622612217402793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5442381074111916 | validation: 0.29219918214477814]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6236577923406016		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5230663026375514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5733620474890764 | validation: 0.49163512585232205]
	TIME [epoch: 8.35 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.503097284034453		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4621718024546147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.482634543244534 | validation: 0.36744837350773096]
	TIME [epoch: 8.35 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5463764535445654		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.605032726342199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5757045899433823 | validation: 0.36176708155818316]
	TIME [epoch: 8.35 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6725208469904269		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5849513485297083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6287360977600676 | validation: 0.2910442589844848]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5236481538082425		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5792173924064936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5514327731073682 | validation: 0.436935211636697]
	TIME [epoch: 8.35 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5961934557844994		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4955086454429377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5458510506137186 | validation: 0.6805359604676653]
	TIME [epoch: 8.35 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5539269689931239		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5091642294754402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.531545599234282 | validation: 0.5881188342277277]
	TIME [epoch: 8.34 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4488397090807016		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.44935171842308197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4490957137518918 | validation: 0.578953649738401]
	TIME [epoch: 8.37 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5398933495556262		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4986655596823232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5192794546189746 | validation: 0.6263312173419358]
	TIME [epoch: 8.34 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5368206915971957		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4550389494376185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4959298205174072 | validation: 0.4170057075092386]
	TIME [epoch: 8.33 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.582992981276547		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4609914559129056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5219922185947263 | validation: 0.3002427309617499]
	TIME [epoch: 8.34 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5070915729041752		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5827668658123484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5449292193582618 | validation: 0.332140539804997]
	TIME [epoch: 8.37 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.594882814997186		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5299946827272197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5624387488622029 | validation: 0.2957843135893477]
	TIME [epoch: 8.34 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5261762494802344		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5883024214458548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5572393354630445 | validation: 0.48120294097617544]
	TIME [epoch: 8.33 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5060211783782178		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4656979862101142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.485859582294166 | validation: 0.3688889778157133]
	TIME [epoch: 8.34 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5622581615102349		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4821239957314723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5221910786208536 | validation: 0.5913246986814964]
	TIME [epoch: 8.37 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45848232898176117		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5639328904712927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5112076097265269 | validation: 0.31705446069838095]
	TIME [epoch: 8.34 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49242268108323123		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4779596654680847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48519117327565786 | validation: 0.39702946538083633]
	TIME [epoch: 8.33 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5730631131846581		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5106608566144978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5418619848995779 | validation: 0.3800537434940198]
	TIME [epoch: 8.33 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46827672914721113		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6019974868800249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.535137108013618 | validation: 0.253243713388542]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43041170028144576		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5885948993794069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5095032998304264 | validation: 0.6707233003039622]
	TIME [epoch: 8.34 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8130337411278582		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5808965617799406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6969651514538996 | validation: 0.2939201361929839]
	TIME [epoch: 8.34 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4181739866991724		[learning rate: 0.0099894]
		[batch 20/20] avg loss: 0.5863555512951599		[learning rate: 0.0099776]
	Learning Rate: 0.00997759
	LOSS [training: 0.5022647689971662 | validation: 0.2863208219387998]
	TIME [epoch: 8.34 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4631392181302485		[learning rate: 0.0099658]
		[batch 20/20] avg loss: 0.4404588928290166		[learning rate: 0.0099541]
	Learning Rate: 0.00995405
	LOSS [training: 0.4517990554796326 | validation: 0.6093622980139601]
	TIME [epoch: 8.36 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47742368962659815		[learning rate: 0.0099423]
		[batch 20/20] avg loss: 0.4868206038945428		[learning rate: 0.0099306]
	Learning Rate: 0.00993057
	LOSS [training: 0.4821221467605706 | validation: 0.3054261577411186]
	TIME [epoch: 8.34 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5430774725912149		[learning rate: 0.0099189]
		[batch 20/20] avg loss: 0.4819444016207495		[learning rate: 0.0099071]
	Learning Rate: 0.00990715
	LOSS [training: 0.5125109371059822 | validation: 0.5485768556511266]
	TIME [epoch: 8.34 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4566172847273181		[learning rate: 0.0098955]
		[batch 20/20] avg loss: 0.5138189462523024		[learning rate: 0.0098838]
	Learning Rate: 0.00988378
	LOSS [training: 0.48521811548981014 | validation: 0.3211867406687885]
	TIME [epoch: 8.34 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45050585012797856		[learning rate: 0.0098721]
		[batch 20/20] avg loss: 0.4688629026891271		[learning rate: 0.0098605]
	Learning Rate: 0.00986047
	LOSS [training: 0.4596843764085528 | validation: 0.441793275899488]
	TIME [epoch: 8.36 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5628985974563413		[learning rate: 0.0098488]
		[batch 20/20] avg loss: 0.40063122315333743		[learning rate: 0.0098372]
	Learning Rate: 0.00983721
	LOSS [training: 0.4817649103048394 | validation: 0.3852829936672219]
	TIME [epoch: 8.34 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4878200935542291		[learning rate: 0.0098256]
		[batch 20/20] avg loss: 0.5118701121230849		[learning rate: 0.009814]
	Learning Rate: 0.009814
	LOSS [training: 0.49984510283865696 | validation: 0.41936816081971706]
	TIME [epoch: 8.34 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46170712088300486		[learning rate: 0.0098024]
		[batch 20/20] avg loss: 0.5428888112139502		[learning rate: 0.0097909]
	Learning Rate: 0.00979085
	LOSS [training: 0.5022979660484774 | validation: 0.5283008726871369]
	TIME [epoch: 8.34 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4461516933606708		[learning rate: 0.0097793]
		[batch 20/20] avg loss: 0.437753654106025		[learning rate: 0.0097678]
	Learning Rate: 0.00976776
	LOSS [training: 0.4419526737333479 | validation: 0.3906006547670322]
	TIME [epoch: 8.36 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5134210499631173		[learning rate: 0.0097562]
		[batch 20/20] avg loss: 0.5810213151294514		[learning rate: 0.0097447]
	Learning Rate: 0.00974472
	LOSS [training: 0.5472211825462844 | validation: 0.4632372577175211]
	TIME [epoch: 8.34 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5820030026919027		[learning rate: 0.0097332]
		[batch 20/20] avg loss: 0.5023066039740514		[learning rate: 0.0097217]
	Learning Rate: 0.00972173
	LOSS [training: 0.542154803332977 | validation: 0.3169597222870432]
	TIME [epoch: 8.33 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46373946798699117		[learning rate: 0.0097103]
		[batch 20/20] avg loss: 0.5313924935681985		[learning rate: 0.0096988]
	Learning Rate: 0.0096988
	LOSS [training: 0.49756598077759484 | validation: 0.8335823350497514]
	TIME [epoch: 8.34 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5691455776832912		[learning rate: 0.0096874]
		[batch 20/20] avg loss: 0.44576757787081167		[learning rate: 0.0096759]
	Learning Rate: 0.00967592
	LOSS [training: 0.5074565777770513 | validation: 0.35779621360741815]
	TIME [epoch: 8.37 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5143300796532005		[learning rate: 0.0096645]
		[batch 20/20] avg loss: 0.49825631289296907		[learning rate: 0.0096531]
	Learning Rate: 0.0096531
	LOSS [training: 0.5062931962730848 | validation: 0.3927940580174717]
	TIME [epoch: 8.35 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.442897981976578		[learning rate: 0.0096417]
		[batch 20/20] avg loss: 0.4596183802735168		[learning rate: 0.0096303]
	Learning Rate: 0.00963033
	LOSS [training: 0.45125818112504745 | validation: 0.42860479800113804]
	TIME [epoch: 8.34 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4508683616622406		[learning rate: 0.009619]
		[batch 20/20] avg loss: 0.5069694439888558		[learning rate: 0.0096076]
	Learning Rate: 0.00960761
	LOSS [training: 0.47891890282554817 | validation: 0.4477585117359784]
	TIME [epoch: 8.35 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.624292615964907		[learning rate: 0.0095963]
		[batch 20/20] avg loss: 0.4417353766251971		[learning rate: 0.0095849]
	Learning Rate: 0.00958495
	LOSS [training: 0.533013996295052 | validation: 0.296136816118405]
	TIME [epoch: 8.37 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5394646373519412		[learning rate: 0.0095736]
		[batch 20/20] avg loss: 0.4785828069496077		[learning rate: 0.0095623]
	Learning Rate: 0.00956234
	LOSS [training: 0.5090237221507745 | validation: 0.27903241193042255]
	TIME [epoch: 8.35 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4471932527019125		[learning rate: 0.0095511]
		[batch 20/20] avg loss: 0.4870429219807641		[learning rate: 0.0095398]
	Learning Rate: 0.00953978
	LOSS [training: 0.4671180873413383 | validation: 0.8721884606328536]
	TIME [epoch: 8.35 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43099518701695005		[learning rate: 0.0095285]
		[batch 20/20] avg loss: 0.47839406135574575		[learning rate: 0.0095173]
	Learning Rate: 0.00951728
	LOSS [training: 0.4546946241863479 | validation: 0.4989367732185297]
	TIME [epoch: 8.36 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48832397424944896		[learning rate: 0.009506]
		[batch 20/20] avg loss: 0.547470748645907		[learning rate: 0.0094948]
	Learning Rate: 0.00949483
	LOSS [training: 0.517897361447678 | validation: 0.2559231192402274]
	TIME [epoch: 8.37 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39908681493782633		[learning rate: 0.0094836]
		[batch 20/20] avg loss: 0.4179047182253294		[learning rate: 0.0094724]
	Learning Rate: 0.00947243
	LOSS [training: 0.4084957665815779 | validation: 0.82876186430241]
	TIME [epoch: 8.35 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4808130367880478		[learning rate: 0.0094613]
		[batch 20/20] avg loss: 0.5196290615860206		[learning rate: 0.0094501]
	Learning Rate: 0.00945009
	LOSS [training: 0.5002210491870343 | validation: 0.2884215157015558]
	TIME [epoch: 8.35 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40649545305083523		[learning rate: 0.0094389]
		[batch 20/20] avg loss: 0.47522619953458056		[learning rate: 0.0094278]
	Learning Rate: 0.0094278
	LOSS [training: 0.44086082629270795 | validation: 0.38321935615531655]
	TIME [epoch: 8.35 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36878630371329174		[learning rate: 0.0094167]
		[batch 20/20] avg loss: 0.5627448432915705		[learning rate: 0.0094056]
	Learning Rate: 0.00940556
	LOSS [training: 0.4657655735024312 | validation: 0.3339855419356687]
	TIME [epoch: 8.36 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4577521486060359		[learning rate: 0.0093945]
		[batch 20/20] avg loss: 0.43779495092254395		[learning rate: 0.0093834]
	Learning Rate: 0.00938337
	LOSS [training: 0.44777354976428996 | validation: 0.3565149302642545]
	TIME [epoch: 8.35 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5211462468604815		[learning rate: 0.0093723]
		[batch 20/20] avg loss: 0.4107265248827544		[learning rate: 0.0093612]
	Learning Rate: 0.00936124
	LOSS [training: 0.465936385871618 | validation: 1.2743290622839922]
	TIME [epoch: 8.35 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6439457377498158		[learning rate: 0.0093502]
		[batch 20/20] avg loss: 0.4540235727741636		[learning rate: 0.0093392]
	Learning Rate: 0.00933916
	LOSS [training: 0.5489846552619898 | validation: 0.3344258751349626]
	TIME [epoch: 8.35 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42051900275300935		[learning rate: 0.0093281]
		[batch 20/20] avg loss: 0.3709636635016341		[learning rate: 0.0093171]
	Learning Rate: 0.00931713
	LOSS [training: 0.39574133312732174 | validation: 0.43226234976596306]
	TIME [epoch: 8.36 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4911428553536831		[learning rate: 0.0093061]
		[batch 20/20] avg loss: 0.7962202471149602		[learning rate: 0.0092951]
	Learning Rate: 0.00929515
	LOSS [training: 0.6436815512343216 | validation: 0.3762897391587714]
	TIME [epoch: 8.35 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4018425789722694		[learning rate: 0.0092842]
		[batch 20/20] avg loss: 0.3901954541899359		[learning rate: 0.0092732]
	Learning Rate: 0.00927322
	LOSS [training: 0.39601901658110267 | validation: 0.3707467929192776]
	TIME [epoch: 8.34 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4266952583813385		[learning rate: 0.0092623]
		[batch 20/20] avg loss: 0.5180364650008016		[learning rate: 0.0092514]
	Learning Rate: 0.00925135
	LOSS [training: 0.47236586169107 | validation: 0.630672162399143]
	TIME [epoch: 8.35 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5398068822850723		[learning rate: 0.0092404]
		[batch 20/20] avg loss: 0.35292401805289575		[learning rate: 0.0092295]
	Learning Rate: 0.00922953
	LOSS [training: 0.4463654501689841 | validation: 0.4149306447502111]
	TIME [epoch: 8.36 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5542367879134498		[learning rate: 0.0092186]
		[batch 20/20] avg loss: 0.6160911982765654		[learning rate: 0.0092078]
	Learning Rate: 0.00920776
	LOSS [training: 0.5851639930950077 | validation: 0.4715233424326615]
	TIME [epoch: 8.35 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46758865104071673		[learning rate: 0.0091969]
		[batch 20/20] avg loss: 0.4443078276854253		[learning rate: 0.009186]
	Learning Rate: 0.00918604
	LOSS [training: 0.45594823936307105 | validation: 0.4847288190130221]
	TIME [epoch: 8.34 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49045131543285975		[learning rate: 0.0091752]
		[batch 20/20] avg loss: 0.3918157762135236		[learning rate: 0.0091644]
	Learning Rate: 0.00916437
	LOSS [training: 0.44113354582319164 | validation: 0.3462691132572462]
	TIME [epoch: 8.36 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39412432879092624		[learning rate: 0.0091536]
		[batch 20/20] avg loss: 0.445761811750946		[learning rate: 0.0091428]
	Learning Rate: 0.00914275
	LOSS [training: 0.41994307027093614 | validation: 0.23786662725036456]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4372922274156439		[learning rate: 0.009132]
		[batch 20/20] avg loss: 0.44213508007444113		[learning rate: 0.0091212]
	Learning Rate: 0.00912119
	LOSS [training: 0.4397136537450426 | validation: 0.27391478041831296]
	TIME [epoch: 8.66 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38555081920646356		[learning rate: 0.0091104]
		[batch 20/20] avg loss: 0.4817832895616405		[learning rate: 0.0090997]
	Learning Rate: 0.00909967
	LOSS [training: 0.43366705438405206 | validation: 0.3356465666690184]
	TIME [epoch: 8.36 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3464431306941369		[learning rate: 0.0090889]
		[batch 20/20] avg loss: 0.44620405804369295		[learning rate: 0.0090782]
	Learning Rate: 0.0090782
	LOSS [training: 0.39632359436891496 | validation: 0.32574148376884815]
	TIME [epoch: 8.38 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38736021198876625		[learning rate: 0.0090675]
		[batch 20/20] avg loss: 0.43548890573960436		[learning rate: 0.0090568]
	Learning Rate: 0.00905679
	LOSS [training: 0.4114245588641852 | validation: 0.29025976859068775]
	TIME [epoch: 8.37 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40127584318090603		[learning rate: 0.0090461]
		[batch 20/20] avg loss: 0.4779839982125953		[learning rate: 0.0090354]
	Learning Rate: 0.00903543
	LOSS [training: 0.4396299206967506 | validation: 0.23691118120884025]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44866347387382016		[learning rate: 0.0090248]
		[batch 20/20] avg loss: 0.5034970850461362		[learning rate: 0.0090141]
	Learning Rate: 0.00901411
	LOSS [training: 0.47608027945997833 | validation: 0.3081395175244395]
	TIME [epoch: 8.36 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4599231913655502		[learning rate: 0.0090035]
		[batch 20/20] avg loss: 0.48043030442771684		[learning rate: 0.0089929]
	Learning Rate: 0.00899285
	LOSS [training: 0.47017674789663344 | validation: 0.32910054913111686]
	TIME [epoch: 8.38 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4193085154301873		[learning rate: 0.0089822]
		[batch 20/20] avg loss: 0.410865388718482		[learning rate: 0.0089716]
	Learning Rate: 0.00897164
	LOSS [training: 0.41508695207433466 | validation: 0.29172546472618416]
	TIME [epoch: 8.37 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46070991285005974		[learning rate: 0.0089611]
		[batch 20/20] avg loss: 0.42735669461836157		[learning rate: 0.0089505]
	Learning Rate: 0.00895048
	LOSS [training: 0.4440333037342106 | validation: 1.029043431507911]
	TIME [epoch: 8.36 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46792190940697564		[learning rate: 0.0089399]
		[batch 20/20] avg loss: 0.42591916054723156		[learning rate: 0.0089294]
	Learning Rate: 0.00892936
	LOSS [training: 0.4469205349771036 | validation: 0.36796736265612523]
	TIME [epoch: 8.36 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39686491064880663		[learning rate: 0.0089188]
		[batch 20/20] avg loss: 0.4395893774001592		[learning rate: 0.0089083]
	Learning Rate: 0.0089083
	LOSS [training: 0.4182271440244828 | validation: 0.2931378259309063]
	TIME [epoch: 8.37 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36135983703391467		[learning rate: 0.0088978]
		[batch 20/20] avg loss: 0.3419158446428654		[learning rate: 0.0088873]
	Learning Rate: 0.00888729
	LOSS [training: 0.35163784083839 | validation: 0.33888498430637826]
	TIME [epoch: 8.37 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4250181842390428		[learning rate: 0.0088768]
		[batch 20/20] avg loss: 0.5086073988355176		[learning rate: 0.0088663]
	Learning Rate: 0.00886632
	LOSS [training: 0.4668127915372803 | validation: 0.2647608121498809]
	TIME [epoch: 8.36 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5810061092557985		[learning rate: 0.0088559]
		[batch 20/20] avg loss: 0.46646030514236336		[learning rate: 0.0088454]
	Learning Rate: 0.00884541
	LOSS [training: 0.5237332071990809 | validation: 0.24188033910738527]
	TIME [epoch: 8.36 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37848825137587566		[learning rate: 0.008835]
		[batch 20/20] avg loss: 0.4082678598323576		[learning rate: 0.0088245]
	Learning Rate: 0.00882454
	LOSS [training: 0.3933780556041166 | validation: 0.2910583024697115]
	TIME [epoch: 8.38 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46914895225243897		[learning rate: 0.0088141]
		[batch 20/20] avg loss: 0.4189407272985785		[learning rate: 0.0088037]
	Learning Rate: 0.00880373
	LOSS [training: 0.44404483977550874 | validation: 0.36936637783379933]
	TIME [epoch: 8.36 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45160132239272305		[learning rate: 0.0087933]
		[batch 20/20] avg loss: 0.5320585017515957		[learning rate: 0.008783]
	Learning Rate: 0.00878296
	LOSS [training: 0.49182991207215954 | validation: 0.4051079806446539]
	TIME [epoch: 8.36 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47196011485550693		[learning rate: 0.0087726]
		[batch 20/20] avg loss: 0.3652187235601944		[learning rate: 0.0087622]
	Learning Rate: 0.00876225
	LOSS [training: 0.4185894192078507 | validation: 0.3797338732048759]
	TIME [epoch: 8.35 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4082102874095982		[learning rate: 0.0087519]
		[batch 20/20] avg loss: 0.3557998136710043		[learning rate: 0.0087416]
	Learning Rate: 0.00874158
	LOSS [training: 0.3820050505403013 | validation: 0.28213483659755956]
	TIME [epoch: 8.38 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6835803653185882		[learning rate: 0.0087313]
		[batch 20/20] avg loss: 0.387146559452474		[learning rate: 0.008721]
	Learning Rate: 0.00872096
	LOSS [training: 0.5353634623855311 | validation: 0.7507019895662927]
	TIME [epoch: 8.36 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47871504434529977		[learning rate: 0.0087107]
		[batch 20/20] avg loss: 0.41995224385478147		[learning rate: 0.0087004]
	Learning Rate: 0.00870038
	LOSS [training: 0.4493336441000405 | validation: 0.3476479406989176]
	TIME [epoch: 8.36 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4315752630721068		[learning rate: 0.0086901]
		[batch 20/20] avg loss: 0.35249886137566405		[learning rate: 0.0086799]
	Learning Rate: 0.00867986
	LOSS [training: 0.3920370622238854 | validation: 0.17161763219580742]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3836574943378796		[learning rate: 0.0086696]
		[batch 20/20] avg loss: 0.41904007864123055		[learning rate: 0.0086594]
	Learning Rate: 0.00865939
	LOSS [training: 0.40134878648955513 | validation: 0.18912019198794505]
	TIME [epoch: 8.38 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32313102000172833		[learning rate: 0.0086492]
		[batch 20/20] avg loss: 0.3797378113263395		[learning rate: 0.008639]
	Learning Rate: 0.00863896
	LOSS [training: 0.3514344156640339 | validation: 0.36393329840971383]
	TIME [epoch: 8.36 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37960274924232223		[learning rate: 0.0086288]
		[batch 20/20] avg loss: 0.40476839107599005		[learning rate: 0.0086186]
	Learning Rate: 0.00861858
	LOSS [training: 0.39218557015915606 | validation: 0.28660004655441396]
	TIME [epoch: 8.35 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3899629821176144		[learning rate: 0.0086084]
		[batch 20/20] avg loss: 0.5382981657194023		[learning rate: 0.0085983]
	Learning Rate: 0.00859825
	LOSS [training: 0.4641305739185083 | validation: 0.339124637676881]
	TIME [epoch: 8.36 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3718563558347875		[learning rate: 0.0085881]
		[batch 20/20] avg loss: 0.39096528164969385		[learning rate: 0.008578]
	Learning Rate: 0.00857797
	LOSS [training: 0.3814108187422406 | validation: 0.2849405598992504]
	TIME [epoch: 8.37 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3614278810478604		[learning rate: 0.0085678]
		[batch 20/20] avg loss: 0.49401565601968234		[learning rate: 0.0085577]
	Learning Rate: 0.00855774
	LOSS [training: 0.4277217685337714 | validation: 0.8139741470892832]
	TIME [epoch: 8.36 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4164832756978707		[learning rate: 0.0085476]
		[batch 20/20] avg loss: 0.36379393684724876		[learning rate: 0.0085376]
	Learning Rate: 0.00853755
	LOSS [training: 0.3901386062725597 | validation: 0.3231133295329264]
	TIME [epoch: 8.35 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32712553175340275		[learning rate: 0.0085275]
		[batch 20/20] avg loss: 0.37029806916615715		[learning rate: 0.0085174]
	Learning Rate: 0.00851741
	LOSS [training: 0.34871180045977995 | validation: 0.23007101187526308]
	TIME [epoch: 8.35 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33264757208079093		[learning rate: 0.0085074]
		[batch 20/20] avg loss: 0.5109623578378112		[learning rate: 0.0084973]
	Learning Rate: 0.00849732
	LOSS [training: 0.42180496495930103 | validation: 0.40361365202611826]
	TIME [epoch: 8.37 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34187693801943586		[learning rate: 0.0084873]
		[batch 20/20] avg loss: 0.38560722009663034		[learning rate: 0.0084773]
	Learning Rate: 0.00847728
	LOSS [training: 0.36374207905803313 | validation: 0.22743186181453318]
	TIME [epoch: 8.35 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.516401737639123		[learning rate: 0.0084673]
		[batch 20/20] avg loss: 0.4089076161374422		[learning rate: 0.0084573]
	Learning Rate: 0.00845728
	LOSS [training: 0.4626546768882826 | validation: 0.17976940011047288]
	TIME [epoch: 8.35 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36395489153882793		[learning rate: 0.0084473]
		[batch 20/20] avg loss: 0.3238192655057819		[learning rate: 0.0084373]
	Learning Rate: 0.00843733
	LOSS [training: 0.34388707852230493 | validation: 0.30606862623007824]
	TIME [epoch: 8.34 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3295331240114471		[learning rate: 0.0084274]
		[batch 20/20] avg loss: 0.4290934918281167		[learning rate: 0.0084174]
	Learning Rate: 0.00841743
	LOSS [training: 0.37931330791978185 | validation: 0.9533092079793801]
	TIME [epoch: 8.37 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5789928906563153		[learning rate: 0.0084075]
		[batch 20/20] avg loss: 0.3259687554526392		[learning rate: 0.0083976]
	Learning Rate: 0.00839757
	LOSS [training: 0.4524808230544773 | validation: 0.17978383635920142]
	TIME [epoch: 8.36 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2738362569123821		[learning rate: 0.0083877]
		[batch 20/20] avg loss: 0.3715337024856519		[learning rate: 0.0083778]
	Learning Rate: 0.00837777
	LOSS [training: 0.32268497969901694 | validation: 0.3980814064769366]
	TIME [epoch: 8.35 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3201099504655916		[learning rate: 0.0083679]
		[batch 20/20] avg loss: 0.3527198215458349		[learning rate: 0.008358]
	Learning Rate: 0.008358
	LOSS [training: 0.33641488600571334 | validation: 0.3572170540338675]
	TIME [epoch: 8.35 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3025375450081292		[learning rate: 0.0083481]
		[batch 20/20] avg loss: 0.38483123203798575		[learning rate: 0.0083383]
	Learning Rate: 0.00833829
	LOSS [training: 0.3436843885230575 | validation: 0.2596935107605215]
	TIME [epoch: 8.38 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43234867332488386		[learning rate: 0.0083284]
		[batch 20/20] avg loss: 0.37060414708026557		[learning rate: 0.0083186]
	Learning Rate: 0.00831862
	LOSS [training: 0.40147641020257474 | validation: 0.17245179156296178]
	TIME [epoch: 8.36 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3791144330284552		[learning rate: 0.0083088]
		[batch 20/20] avg loss: 0.3849802177659419		[learning rate: 0.008299]
	Learning Rate: 0.008299
	LOSS [training: 0.38204732539719843 | validation: 0.30774083033268296]
	TIME [epoch: 8.35 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3285097461335008		[learning rate: 0.0082892]
		[batch 20/20] avg loss: 0.32822738354764847		[learning rate: 0.0082794]
	Learning Rate: 0.00827942
	LOSS [training: 0.32836856484057464 | validation: 0.24074809256345184]
	TIME [epoch: 8.34 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37045001365123503		[learning rate: 0.0082697]
		[batch 20/20] avg loss: 0.2902663114332279		[learning rate: 0.0082599]
	Learning Rate: 0.00825989
	LOSS [training: 0.3303581625422315 | validation: 0.5857749759157397]
	TIME [epoch: 8.37 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3454386845628413		[learning rate: 0.0082501]
		[batch 20/20] avg loss: 0.33147848600677843		[learning rate: 0.0082404]
	Learning Rate: 0.00824041
	LOSS [training: 0.3384585852848098 | validation: 0.1327565366614418]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43551554812035354		[learning rate: 0.0082307]
		[batch 20/20] avg loss: 0.387142005350806		[learning rate: 0.008221]
	Learning Rate: 0.00822097
	LOSS [training: 0.4113287767355797 | validation: 0.23695432111605552]
	TIME [epoch: 8.35 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45017206496944545		[learning rate: 0.0082113]
		[batch 20/20] avg loss: 0.3978483838199116		[learning rate: 0.0082016]
	Learning Rate: 0.00820158
	LOSS [training: 0.4240102243946785 | validation: 0.29837597570500085]
	TIME [epoch: 8.35 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40239073539642717		[learning rate: 0.0081919]
		[batch 20/20] avg loss: 0.33344088400931926		[learning rate: 0.0081822]
	Learning Rate: 0.00818223
	LOSS [training: 0.3679158097028733 | validation: 0.2956822949213191]
	TIME [epoch: 8.37 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3608522141261991		[learning rate: 0.0081726]
		[batch 20/20] avg loss: 0.305615958190712		[learning rate: 0.0081629]
	Learning Rate: 0.00816293
	LOSS [training: 0.3332340861584556 | validation: 0.32415562065989284]
	TIME [epoch: 8.35 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2828945544888667		[learning rate: 0.0081533]
		[batch 20/20] avg loss: 0.38728750029642883		[learning rate: 0.0081437]
	Learning Rate: 0.00814368
	LOSS [training: 0.3350910273926477 | validation: 0.21692807615592807]
	TIME [epoch: 8.34 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30509047705338577		[learning rate: 0.0081341]
		[batch 20/20] avg loss: 0.3659794951774068		[learning rate: 0.0081245]
	Learning Rate: 0.00812447
	LOSS [training: 0.33553498611539634 | validation: 0.14640552140882102]
	TIME [epoch: 8.35 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3517177508914934		[learning rate: 0.0081149]
		[batch 20/20] avg loss: 0.3334920808497105		[learning rate: 0.0081053]
	Learning Rate: 0.0081053
	LOSS [training: 0.34260491587060193 | validation: 0.20238976732672936]
	TIME [epoch: 8.37 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3392817202949011		[learning rate: 0.0080957]
		[batch 20/20] avg loss: 0.3423835447207691		[learning rate: 0.0080862]
	Learning Rate: 0.00808618
	LOSS [training: 0.34083263250783513 | validation: 0.32911495814592373]
	TIME [epoch: 8.35 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33047290446802696		[learning rate: 0.0080766]
		[batch 20/20] avg loss: 0.2633437698495871		[learning rate: 0.0080671]
	Learning Rate: 0.00806711
	LOSS [training: 0.29690833715880705 | validation: 0.16108138844912046]
	TIME [epoch: 8.35 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23051824234854004		[learning rate: 0.0080576]
		[batch 20/20] avg loss: 0.406687189716215		[learning rate: 0.0080481]
	Learning Rate: 0.00804808
	LOSS [training: 0.31860271603237755 | validation: 0.24359555016810475]
	TIME [epoch: 8.35 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.341172842034042		[learning rate: 0.0080386]
		[batch 20/20] avg loss: 0.3667755955120565		[learning rate: 0.0080291]
	Learning Rate: 0.0080291
	LOSS [training: 0.3539742187730493 | validation: 0.4000517737320662]
	TIME [epoch: 8.38 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2908722925899462		[learning rate: 0.0080196]
		[batch 20/20] avg loss: 0.4570849695833587		[learning rate: 0.0080102]
	Learning Rate: 0.00801016
	LOSS [training: 0.3739786310866524 | validation: 0.6663468946458693]
	TIME [epoch: 8.35 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32341992861650687		[learning rate: 0.0080007]
		[batch 20/20] avg loss: 0.2860255101202382		[learning rate: 0.0079913]
	Learning Rate: 0.00799126
	LOSS [training: 0.30472271936837253 | validation: 0.2617001372052276]
	TIME [epoch: 8.35 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31234477530563776		[learning rate: 0.0079818]
		[batch 20/20] avg loss: 0.3042135719269512		[learning rate: 0.0079724]
	Learning Rate: 0.00797241
	LOSS [training: 0.3082791736162944 | validation: 0.1652289032533422]
	TIME [epoch: 8.35 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34900870989073834		[learning rate: 0.007963]
		[batch 20/20] avg loss: 0.3022700940877661		[learning rate: 0.0079536]
	Learning Rate: 0.00795361
	LOSS [training: 0.3256394019892522 | validation: 0.456125346850809]
	TIME [epoch: 8.38 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38926668031468203		[learning rate: 0.0079442]
		[batch 20/20] avg loss: 0.3706954854609951		[learning rate: 0.0079348]
	Learning Rate: 0.00793485
	LOSS [training: 0.3799810828878386 | validation: 0.2572044565745175]
	TIME [epoch: 8.35 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39232966063730274		[learning rate: 0.0079255]
		[batch 20/20] avg loss: 0.3465697290041734		[learning rate: 0.0079161]
	Learning Rate: 0.00791613
	LOSS [training: 0.3694496948207381 | validation: 0.19442695822374687]
	TIME [epoch: 8.35 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3323970365989597		[learning rate: 0.0079068]
		[batch 20/20] avg loss: 0.34607080759789977		[learning rate: 0.0078975]
	Learning Rate: 0.00789746
	LOSS [training: 0.33923392209842984 | validation: 0.24000002137940274]
	TIME [epoch: 8.34 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30839345465350887		[learning rate: 0.0078881]
		[batch 20/20] avg loss: 0.36454769523296726		[learning rate: 0.0078788]
	Learning Rate: 0.00787883
	LOSS [training: 0.33647057494323807 | validation: 0.28697134460921037]
	TIME [epoch: 8.37 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3548020669431492		[learning rate: 0.0078695]
		[batch 20/20] avg loss: 0.28162644360375766		[learning rate: 0.0078602]
	Learning Rate: 0.00786024
	LOSS [training: 0.3182142552734534 | validation: 0.2823031398306861]
	TIME [epoch: 8.35 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29842354820938116		[learning rate: 0.007851]
		[batch 20/20] avg loss: 0.25273617862827474		[learning rate: 0.0078417]
	Learning Rate: 0.0078417
	LOSS [training: 0.2755798634188279 | validation: 0.2967856163910983]
	TIME [epoch: 8.35 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24694132083194448		[learning rate: 0.0078324]
		[batch 20/20] avg loss: 0.28164039647311256		[learning rate: 0.0078232]
	Learning Rate: 0.0078232
	LOSS [training: 0.26429085865252844 | validation: 0.156785902253803]
	TIME [epoch: 8.35 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3006992445134096		[learning rate: 0.007814]
		[batch 20/20] avg loss: 0.34779286738759485		[learning rate: 0.0078047]
	Learning Rate: 0.00780475
	LOSS [training: 0.32424605595050227 | validation: 0.4252992611543648]
	TIME [epoch: 8.38 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32264994712679085		[learning rate: 0.0077955]
		[batch 20/20] avg loss: 0.2552864249828303		[learning rate: 0.0077863]
	Learning Rate: 0.00778634
	LOSS [training: 0.2889681860548105 | validation: 0.2938232867316545]
	TIME [epoch: 8.35 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25716800228012915		[learning rate: 0.0077772]
		[batch 20/20] avg loss: 0.3042473609626444		[learning rate: 0.007768]
	Learning Rate: 0.00776797
	LOSS [training: 0.28070768162138676 | validation: 0.25504640229766984]
	TIME [epoch: 8.35 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29736708422123287		[learning rate: 0.0077588]
		[batch 20/20] avg loss: 0.3496765329227728		[learning rate: 0.0077496]
	Learning Rate: 0.00774965
	LOSS [training: 0.32352180857200286 | validation: 0.1677848072719225]
	TIME [epoch: 8.35 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29757969685213265		[learning rate: 0.0077405]
		[batch 20/20] avg loss: 0.39150992313791483		[learning rate: 0.0077314]
	Learning Rate: 0.00773137
	LOSS [training: 0.3445448099950238 | validation: 0.3570228240509909]
	TIME [epoch: 8.38 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32160378302997233		[learning rate: 0.0077222]
		[batch 20/20] avg loss: 0.25593740714696883		[learning rate: 0.0077131]
	Learning Rate: 0.00771313
	LOSS [training: 0.2887705950884706 | validation: 0.12793910274577455]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3602335524981207		[learning rate: 0.007704]
		[batch 20/20] avg loss: 0.3208130659418555		[learning rate: 0.0076949]
	Learning Rate: 0.00769494
	LOSS [training: 0.3405233092199881 | validation: 0.13407980411546574]
	TIME [epoch: 8.34 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3082914341208093		[learning rate: 0.0076859]
		[batch 20/20] avg loss: 0.3041104697017608		[learning rate: 0.0076768]
	Learning Rate: 0.00767679
	LOSS [training: 0.30620095191128505 | validation: 0.2038438016699638]
	TIME [epoch: 8.34 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28762840492867564		[learning rate: 0.0076677]
		[batch 20/20] avg loss: 0.23965072435884255		[learning rate: 0.0076587]
	Learning Rate: 0.00765868
	LOSS [training: 0.2636395646437591 | validation: 0.7293155110748557]
	TIME [epoch: 8.37 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33658592351280875		[learning rate: 0.0076496]
		[batch 20/20] avg loss: 0.31865845236561213		[learning rate: 0.0076406]
	Learning Rate: 0.00764061
	LOSS [training: 0.3276221879392104 | validation: 0.3177264866644887]
	TIME [epoch: 8.35 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24963862934999898		[learning rate: 0.0076316]
		[batch 20/20] avg loss: 0.2511042466500853		[learning rate: 0.0076226]
	Learning Rate: 0.00762259
	LOSS [training: 0.25037143800004213 | validation: 0.1469703182700967]
	TIME [epoch: 8.35 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2553741041145899		[learning rate: 0.0076136]
		[batch 20/20] avg loss: 0.33602361676298853		[learning rate: 0.0076046]
	Learning Rate: 0.00760461
	LOSS [training: 0.29569886043878923 | validation: 0.1340273489332967]
	TIME [epoch: 8.34 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21297831319962443		[learning rate: 0.0075956]
		[batch 20/20] avg loss: 0.23325493377254727		[learning rate: 0.0075867]
	Learning Rate: 0.00758667
	LOSS [training: 0.2231166234860858 | validation: 0.14997264285690498]
	TIME [epoch: 8.37 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3002194262239001		[learning rate: 0.0075777]
		[batch 20/20] avg loss: 0.33834519396390383		[learning rate: 0.0075688]
	Learning Rate: 0.00756878
	LOSS [training: 0.31928231009390196 | validation: 0.13619338113477944]
	TIME [epoch: 8.34 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23995851536524077		[learning rate: 0.0075598]
		[batch 20/20] avg loss: 0.30975752982823873		[learning rate: 0.0075509]
	Learning Rate: 0.00755092
	LOSS [training: 0.27485802259673975 | validation: 0.2789521225284499]
	TIME [epoch: 8.34 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23116084976567794		[learning rate: 0.007542]
		[batch 20/20] avg loss: 0.25741587711189945		[learning rate: 0.0075331]
	Learning Rate: 0.00753311
	LOSS [training: 0.24428836343878874 | validation: 0.3428431566620766]
	TIME [epoch: 8.35 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23951240249401556		[learning rate: 0.0075242]
		[batch 20/20] avg loss: 0.2617737113246428		[learning rate: 0.0075153]
	Learning Rate: 0.00751534
	LOSS [training: 0.2506430569093292 | validation: 0.24025804533044026]
	TIME [epoch: 8.37 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29359951466669376		[learning rate: 0.0075065]
		[batch 20/20] avg loss: 0.31946229677983984		[learning rate: 0.0074976]
	Learning Rate: 0.00749761
	LOSS [training: 0.3065309057232668 | validation: 0.31856030516844236]
	TIME [epoch: 8.34 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24402436624612972		[learning rate: 0.0074888]
		[batch 20/20] avg loss: 0.22704304191076488		[learning rate: 0.0074799]
	Learning Rate: 0.00747993
	LOSS [training: 0.23553370407844731 | validation: 0.20082505445747278]
	TIME [epoch: 8.34 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2674521668177717		[learning rate: 0.0074711]
		[batch 20/20] avg loss: 0.2951932719515308		[learning rate: 0.0074623]
	Learning Rate: 0.00746228
	LOSS [training: 0.2813227193846513 | validation: 0.2651007288255497]
	TIME [epoch: 8.34 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20475390753644024		[learning rate: 0.0074535]
		[batch 20/20] avg loss: 0.21866179207609387		[learning rate: 0.0074447]
	Learning Rate: 0.00744468
	LOSS [training: 0.21170784980626706 | validation: 0.11436486856204098]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24490803086076288		[learning rate: 0.0074359]
		[batch 20/20] avg loss: 0.23942781405846078		[learning rate: 0.0074271]
	Learning Rate: 0.00742712
	LOSS [training: 0.24216792245961188 | validation: 0.29060303901621964]
	TIME [epoch: 8.34 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2835453639814272		[learning rate: 0.0074184]
		[batch 20/20] avg loss: 0.27640949424146216		[learning rate: 0.0074096]
	Learning Rate: 0.0074096
	LOSS [training: 0.27997742911144463 | validation: 0.3441133652606939]
	TIME [epoch: 8.35 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23874380516870036		[learning rate: 0.0074009]
		[batch 20/20] avg loss: 0.4196652973819715		[learning rate: 0.0073921]
	Learning Rate: 0.00739212
	LOSS [training: 0.32920455127533593 | validation: 0.5321894135883026]
	TIME [epoch: 8.35 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33233728992351236		[learning rate: 0.0073834]
		[batch 20/20] avg loss: 0.23090331413282236		[learning rate: 0.0073747]
	Learning Rate: 0.00737469
	LOSS [training: 0.2816203020281674 | validation: 0.27304037628039496]
	TIME [epoch: 8.37 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3562061386666765		[learning rate: 0.007366]
		[batch 20/20] avg loss: 0.32658405662341894		[learning rate: 0.0073573]
	Learning Rate: 0.00735729
	LOSS [training: 0.3413950976450478 | validation: 0.444344726531646]
	TIME [epoch: 8.34 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3577267702989792		[learning rate: 0.0073486]
		[batch 20/20] avg loss: 0.332852706036869		[learning rate: 0.0073399]
	Learning Rate: 0.00733994
	LOSS [training: 0.3452897381679241 | validation: 0.2685655236346552]
	TIME [epoch: 8.35 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21844615100974232		[learning rate: 0.0073313]
		[batch 20/20] avg loss: 0.36828800001026357		[learning rate: 0.0073226]
	Learning Rate: 0.00732262
	LOSS [training: 0.29336707551000296 | validation: 0.20376585697565158]
	TIME [epoch: 8.35 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1558205393577383		[learning rate: 0.007314]
		[batch 20/20] avg loss: 0.2052564324815041		[learning rate: 0.0073053]
	Learning Rate: 0.00730535
	LOSS [training: 0.18053848591962116 | validation: 0.25411118712335934]
	TIME [epoch: 8.37 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22912100913185843		[learning rate: 0.0072967]
		[batch 20/20] avg loss: 0.28631526586947753		[learning rate: 0.0072881]
	Learning Rate: 0.00728812
	LOSS [training: 0.257718137500668 | validation: 0.14433486313106475]
	TIME [epoch: 8.35 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1838696740051951		[learning rate: 0.0072795]
		[batch 20/20] avg loss: 0.2495983561584179		[learning rate: 0.0072709]
	Learning Rate: 0.00727093
	LOSS [training: 0.2167340150818065 | validation: 0.12940349231815804]
	TIME [epoch: 8.34 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3385601063371355		[learning rate: 0.0072623]
		[batch 20/20] avg loss: 0.2207740990379224		[learning rate: 0.0072538]
	Learning Rate: 0.00725377
	LOSS [training: 0.2796671026875289 | validation: 0.12636405197392103]
	TIME [epoch: 8.36 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23281218178490656		[learning rate: 0.0072452]
		[batch 20/20] avg loss: 0.2383343729068655		[learning rate: 0.0072367]
	Learning Rate: 0.00723666
	LOSS [training: 0.23557327734588607 | validation: 0.2519503607820318]
	TIME [epoch: 8.37 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2107065064638785		[learning rate: 0.0072281]
		[batch 20/20] avg loss: 0.2128921416002941		[learning rate: 0.0072196]
	Learning Rate: 0.00721959
	LOSS [training: 0.2117993240320863 | validation: 0.26790773951169916]
	TIME [epoch: 8.35 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2324792718720219		[learning rate: 0.0072111]
		[batch 20/20] avg loss: 0.2548085597412275		[learning rate: 0.0072026]
	Learning Rate: 0.00720256
	LOSS [training: 0.24364391580662476 | validation: 0.15543934357652833]
	TIME [epoch: 8.35 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22734322898463658		[learning rate: 0.0071941]
		[batch 20/20] avg loss: 0.35788996001603496		[learning rate: 0.0071856]
	Learning Rate: 0.00718558
	LOSS [training: 0.2926165945003357 | validation: 0.1581562309868083]
	TIME [epoch: 8.36 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.265052239347389		[learning rate: 0.0071771]
		[batch 20/20] avg loss: 0.18465712560268271		[learning rate: 0.0071686]
	Learning Rate: 0.00716863
	LOSS [training: 0.22485468247503587 | validation: 0.15198468613614874]
	TIME [epoch: 8.36 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24129761679066003		[learning rate: 0.0071602]
		[batch 20/20] avg loss: 0.29069272020233583		[learning rate: 0.0071517]
	Learning Rate: 0.00715172
	LOSS [training: 0.26599516849649796 | validation: 0.1736574238402609]
	TIME [epoch: 8.35 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25951920433780223		[learning rate: 0.0071433]
		[batch 20/20] avg loss: 0.2547694773263153		[learning rate: 0.0071348]
	Learning Rate: 0.00713485
	LOSS [training: 0.2571443408320587 | validation: 0.1300318888463692]
	TIME [epoch: 8.34 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19567891175053626		[learning rate: 0.0071264]
		[batch 20/20] avg loss: 0.21198614252216244		[learning rate: 0.007118]
	Learning Rate: 0.00711802
	LOSS [training: 0.20383252713634933 | validation: 0.2642119777770024]
	TIME [epoch: 8.37 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21821834856616143		[learning rate: 0.0071096]
		[batch 20/20] avg loss: 0.15940404542418748		[learning rate: 0.0071012]
	Learning Rate: 0.00710123
	LOSS [training: 0.18881119699517446 | validation: 0.2780995806181738]
	TIME [epoch: 8.36 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2119207449942322		[learning rate: 0.0070928]
		[batch 20/20] avg loss: 0.35552213567945434		[learning rate: 0.0070845]
	Learning Rate: 0.00708447
	LOSS [training: 0.28372144033684327 | validation: 0.3313709605473818]
	TIME [epoch: 8.35 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2335827169918611		[learning rate: 0.0070761]
		[batch 20/20] avg loss: 0.3213135100900806		[learning rate: 0.0070678]
	Learning Rate: 0.00706776
	LOSS [training: 0.27744811354097076 | validation: 0.15597467729124243]
	TIME [epoch: 8.34 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19257853147754103		[learning rate: 0.0070594]
		[batch 20/20] avg loss: 0.17300146389276078		[learning rate: 0.0070511]
	Learning Rate: 0.00705109
	LOSS [training: 0.18278999768515095 | validation: 0.12118179647059121]
	TIME [epoch: 8.36 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19366679988341973		[learning rate: 0.0070428]
		[batch 20/20] avg loss: 0.18858862339072285		[learning rate: 0.0070345]
	Learning Rate: 0.00703446
	LOSS [training: 0.1911277116370713 | validation: 0.11678602830247065]
	TIME [epoch: 8.35 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1790920053025466		[learning rate: 0.0070262]
		[batch 20/20] avg loss: 0.30668730820187584		[learning rate: 0.0070179]
	Learning Rate: 0.00701787
	LOSS [training: 0.24288965675221125 | validation: 0.31172739936945654]
	TIME [epoch: 8.35 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.216248320208847		[learning rate: 0.0070096]
		[batch 20/20] avg loss: 0.23873300139124662		[learning rate: 0.0070013]
	Learning Rate: 0.00700131
	LOSS [training: 0.22749066080004682 | validation: 0.14893335045605915]
	TIME [epoch: 8.34 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20549062087505146		[learning rate: 0.006993]
		[batch 20/20] avg loss: 0.24514354051785164		[learning rate: 0.0069848]
	Learning Rate: 0.0069848
	LOSS [training: 0.2253170806964516 | validation: 0.10965328814164234]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2339591260513322		[learning rate: 0.0069766]
		[batch 20/20] avg loss: 0.18777431800283573		[learning rate: 0.0069683]
	Learning Rate: 0.00696832
	LOSS [training: 0.2108667220270839 | validation: 0.2132321430537108]
	TIME [epoch: 8.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20788132829506006		[learning rate: 0.0069601]
		[batch 20/20] avg loss: 0.3789323772037172		[learning rate: 0.0069519]
	Learning Rate: 0.00695188
	LOSS [training: 0.29340685274938866 | validation: 0.15190776161726371]
	TIME [epoch: 8.34 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35733827237923543		[learning rate: 0.0069437]
		[batch 20/20] avg loss: 0.3095214713985436		[learning rate: 0.0069355]
	Learning Rate: 0.00693549
	LOSS [training: 0.33342987188888956 | validation: 0.30124760952255536]
	TIME [epoch: 8.34 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20076408152966244		[learning rate: 0.0069273]
		[batch 20/20] avg loss: 0.26816768078912456		[learning rate: 0.0069191]
	Learning Rate: 0.00691913
	LOSS [training: 0.2344658811593935 | validation: 0.21711789731726203]
	TIME [epoch: 8.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2971666565934084		[learning rate: 0.006911]
		[batch 20/20] avg loss: 0.3066642014583437		[learning rate: 0.0069028]
	Learning Rate: 0.00690281
	LOSS [training: 0.3019154290258761 | validation: 0.09347295604646444]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19354584260593044		[learning rate: 0.0068947]
		[batch 20/20] avg loss: 0.3020895915207944		[learning rate: 0.0068865]
	Learning Rate: 0.00688652
	LOSS [training: 0.24781771706336236 | validation: 0.6486105346995028]
	TIME [epoch: 8.35 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29074218487677383		[learning rate: 0.0068784]
		[batch 20/20] avg loss: 0.2656680435120736		[learning rate: 0.0068703]
	Learning Rate: 0.00687028
	LOSS [training: 0.2782051141944237 | validation: 0.0963360708256]
	TIME [epoch: 8.34 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23891015451089342		[learning rate: 0.0068622]
		[batch 20/20] avg loss: 0.1652189260592352		[learning rate: 0.0068541]
	Learning Rate: 0.00685407
	LOSS [training: 0.20206454028506432 | validation: 0.19266280450801243]
	TIME [epoch: 8.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24582927833679		[learning rate: 0.006846]
		[batch 20/20] avg loss: 0.27040469133665795		[learning rate: 0.0068379]
	Learning Rate: 0.0068379
	LOSS [training: 0.258116984836724 | validation: 0.22767373329282123]
	TIME [epoch: 8.36 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20371653465914702		[learning rate: 0.0068298]
		[batch 20/20] avg loss: 0.2041419873565841		[learning rate: 0.0068218]
	Learning Rate: 0.00682178
	LOSS [training: 0.20392926100786557 | validation: 0.21612589955002715]
	TIME [epoch: 8.34 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2636786108332042		[learning rate: 0.0068137]
		[batch 20/20] avg loss: 0.2798659053938894		[learning rate: 0.0068057]
	Learning Rate: 0.00680568
	LOSS [training: 0.2717722581135468 | validation: 0.1755068826769628]
	TIME [epoch: 8.34 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16303199976721142		[learning rate: 0.0067977]
		[batch 20/20] avg loss: 0.21577461051407285		[learning rate: 0.0067896]
	Learning Rate: 0.00678963
	LOSS [training: 0.1894033051406421 | validation: 0.10325975757785548]
	TIME [epoch: 8.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20539986431874313		[learning rate: 0.0067816]
		[batch 20/20] avg loss: 0.2113767246947232		[learning rate: 0.0067736]
	Learning Rate: 0.00677361
	LOSS [training: 0.20838829450673318 | validation: 0.08090264717322981]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13252032537073194		[learning rate: 0.0067656]
		[batch 20/20] avg loss: 0.27378858763868213		[learning rate: 0.0067576]
	Learning Rate: 0.00675764
	LOSS [training: 0.20315445650470704 | validation: 0.15765866189831637]
	TIME [epoch: 8.34 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24192437991414223		[learning rate: 0.0067497]
		[batch 20/20] avg loss: 0.1844737672699718		[learning rate: 0.0067417]
	Learning Rate: 0.0067417
	LOSS [training: 0.21319907359205698 | validation: 0.09956883595099719]
	TIME [epoch: 8.34 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21507284816619182		[learning rate: 0.0067337]
		[batch 20/20] avg loss: 0.26975928947173033		[learning rate: 0.0067258]
	Learning Rate: 0.00672579
	LOSS [training: 0.24241606881896108 | validation: 0.19667333927052297]
	TIME [epoch: 8.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18789604107318494		[learning rate: 0.0067179]
		[batch 20/20] avg loss: 0.18655172161865527		[learning rate: 0.0067099]
	Learning Rate: 0.00670993
	LOSS [training: 0.1872238813459201 | validation: 0.12971278783970394]
	TIME [epoch: 8.34 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23115890334258915		[learning rate: 0.006702]
		[batch 20/20] avg loss: 0.19030146164814268		[learning rate: 0.0066941]
	Learning Rate: 0.0066941
	LOSS [training: 0.21073018249536593 | validation: 0.21186302990562028]
	TIME [epoch: 8.33 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3001090308647345		[learning rate: 0.0066862]
		[batch 20/20] avg loss: 0.3486120201299221		[learning rate: 0.0066783]
	Learning Rate: 0.00667831
	LOSS [training: 0.3243605254973283 | validation: 0.13060906284816307]
	TIME [epoch: 8.34 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11916272902998948		[learning rate: 0.0066704]
		[batch 20/20] avg loss: 0.2048296702661334		[learning rate: 0.0066626]
	Learning Rate: 0.00666256
	LOSS [training: 0.16199619964806147 | validation: 0.1408576678122177]
	TIME [epoch: 8.37 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19133101051814966		[learning rate: 0.0066547]
		[batch 20/20] avg loss: 0.1712687341617414		[learning rate: 0.0066468]
	Learning Rate: 0.00664684
	LOSS [training: 0.18129987233994554 | validation: 0.1532556426636601]
	TIME [epoch: 8.34 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15243300187892755		[learning rate: 0.006639]
		[batch 20/20] avg loss: 0.20307159256611662		[learning rate: 0.0066312]
	Learning Rate: 0.00663116
	LOSS [training: 0.17775229722252212 | validation: 0.1994048603059304]
	TIME [epoch: 8.33 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25625231343687		[learning rate: 0.0066233]
		[batch 20/20] avg loss: 0.151702052064706		[learning rate: 0.0066155]
	Learning Rate: 0.00661552
	LOSS [training: 0.203977182750788 | validation: 0.5193249760130758]
	TIME [epoch: 8.34 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2298825433673531		[learning rate: 0.0066077]
		[batch 20/20] avg loss: 0.20619275875213186		[learning rate: 0.0065999]
	Learning Rate: 0.00659992
	LOSS [training: 0.21803765105974243 | validation: 0.509769348882169]
	TIME [epoch: 8.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2578981557875831		[learning rate: 0.0065921]
		[batch 20/20] avg loss: 0.2596986524493734		[learning rate: 0.0065843]
	Learning Rate: 0.00658435
	LOSS [training: 0.2587984041184782 | validation: 0.15732211643419358]
	TIME [epoch: 8.34 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18543898532851538		[learning rate: 0.0065766]
		[batch 20/20] avg loss: 0.23249199970548268		[learning rate: 0.0065688]
	Learning Rate: 0.00656882
	LOSS [training: 0.20896549251699903 | validation: 0.19825992933395287]
	TIME [epoch: 8.34 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13499799203892987		[learning rate: 0.0065611]
		[batch 20/20] avg loss: 0.20395216156137347		[learning rate: 0.0065533]
	Learning Rate: 0.00655332
	LOSS [training: 0.16947507680015167 | validation: 0.20448792232375362]
	TIME [epoch: 8.34 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18548899415733439		[learning rate: 0.0065456]
		[batch 20/20] avg loss: 0.1869640021802821		[learning rate: 0.0065379]
	Learning Rate: 0.00653786
	LOSS [training: 0.18622649816880826 | validation: 0.07981163928523483]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19430529006797131		[learning rate: 0.0065301]
		[batch 20/20] avg loss: 0.19774568259909628		[learning rate: 0.0065224]
	Learning Rate: 0.00652244
	LOSS [training: 0.19602548633353378 | validation: 0.0858550528534247]
	TIME [epoch: 8.34 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19680724684296638		[learning rate: 0.0065147]
		[batch 20/20] avg loss: 0.19284106449626626		[learning rate: 0.0065071]
	Learning Rate: 0.00650706
	LOSS [training: 0.1948241556696163 | validation: 0.3596220522306812]
	TIME [epoch: 8.34 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23188190734319178		[learning rate: 0.0064994]
		[batch 20/20] avg loss: 0.23575074857517567		[learning rate: 0.0064917]
	Learning Rate: 0.00649171
	LOSS [training: 0.2338163279591837 | validation: 0.17944811808991973]
	TIME [epoch: 8.33 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16005227031449165		[learning rate: 0.006484]
		[batch 20/20] avg loss: 0.22512284868880025		[learning rate: 0.0064764]
	Learning Rate: 0.00647639
	LOSS [training: 0.1925875595016459 | validation: 0.9756961230053508]
	TIME [epoch: 8.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2740947409098916		[learning rate: 0.0064688]
		[batch 20/20] avg loss: 0.2326793469742645		[learning rate: 0.0064611]
	Learning Rate: 0.00646112
	LOSS [training: 0.253387043942078 | validation: 0.23596835325899734]
	TIME [epoch: 8.35 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23145295383668146		[learning rate: 0.0064535]
		[batch 20/20] avg loss: 0.1691711239890944		[learning rate: 0.0064459]
	Learning Rate: 0.00644588
	LOSS [training: 0.20031203891288793 | validation: 0.060961998929108446]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24857521530922222		[learning rate: 0.0064383]
		[batch 20/20] avg loss: 0.2562366020980522		[learning rate: 0.0064307]
	Learning Rate: 0.00643067
	LOSS [training: 0.2524059087036372 | validation: 0.20659060048156908]
	TIME [epoch: 8.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35914532904524654		[learning rate: 0.0064231]
		[batch 20/20] avg loss: 0.21911071860794912		[learning rate: 0.0064155]
	Learning Rate: 0.0064155
	LOSS [training: 0.28912802382659797 | validation: 0.09620913356578716]
	TIME [epoch: 8.37 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13688964413383892		[learning rate: 0.0064079]
		[batch 20/20] avg loss: 0.1739572125169707		[learning rate: 0.0064004]
	Learning Rate: 0.00640037
	LOSS [training: 0.1554234283254048 | validation: 0.3016835828628893]
	TIME [epoch: 8.36 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2155776821360555		[learning rate: 0.0063928]
		[batch 20/20] avg loss: 0.180305539020004		[learning rate: 0.0063853]
	Learning Rate: 0.00638527
	LOSS [training: 0.19794161057802975 | validation: 0.15416086479412638]
	TIME [epoch: 8.34 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15033995806899073		[learning rate: 0.0063777]
		[batch 20/20] avg loss: 0.2299676046658295		[learning rate: 0.0063702]
	Learning Rate: 0.00637021
	LOSS [training: 0.19015378136741012 | validation: 0.30463429374478074]
	TIME [epoch: 8.34 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1994948103971764		[learning rate: 0.0063627]
		[batch 20/20] avg loss: 0.2436394893559008		[learning rate: 0.0063552]
	Learning Rate: 0.00635518
	LOSS [training: 0.22156714987653858 | validation: 0.18648713072212]
	TIME [epoch: 8.38 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24753276422159182		[learning rate: 0.0063477]
		[batch 20/20] avg loss: 0.29236164215095417		[learning rate: 0.0063402]
	Learning Rate: 0.00634019
	LOSS [training: 0.269947203186273 | validation: 0.36281345124962294]
	TIME [epoch: 8.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25300364657822216		[learning rate: 0.0063327]
		[batch 20/20] avg loss: 0.2877373935198713		[learning rate: 0.0063252]
	Learning Rate: 0.00632524
	LOSS [training: 0.2703705200490467 | validation: 0.11567251310318913]
	TIME [epoch: 8.34 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20289352495377858		[learning rate: 0.0063178]
		[batch 20/20] avg loss: 0.17170869880768808		[learning rate: 0.0063103]
	Learning Rate: 0.00631032
	LOSS [training: 0.1873011118807333 | validation: 0.21278780063406103]
	TIME [epoch: 8.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37370696768721984		[learning rate: 0.0063029]
		[batch 20/20] avg loss: 0.18038447584555867		[learning rate: 0.0062954]
	Learning Rate: 0.00629543
	LOSS [training: 0.2770457217663892 | validation: 0.11522879204575573]
	TIME [epoch: 8.38 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20471128741577843		[learning rate: 0.006288]
		[batch 20/20] avg loss: 0.19294374952512378		[learning rate: 0.0062806]
	Learning Rate: 0.00628058
	LOSS [training: 0.19882751847045108 | validation: 0.23424745902516858]
	TIME [epoch: 8.36 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1699163197067982		[learning rate: 0.0062732]
		[batch 20/20] avg loss: 0.16124793323837683		[learning rate: 0.0062658]
	Learning Rate: 0.00626577
	LOSS [training: 0.16558212647258752 | validation: 0.07218006912437797]
	TIME [epoch: 8.32 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2064810230470986		[learning rate: 0.0062584]
		[batch 20/20] avg loss: 0.16075455671628153		[learning rate: 0.006251]
	Learning Rate: 0.00625099
	LOSS [training: 0.18361778988169003 | validation: 0.34447839521423673]
	TIME [epoch: 8.32 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19122432634822178		[learning rate: 0.0062436]
		[batch 20/20] avg loss: 0.17591382530582644		[learning rate: 0.0062362]
	Learning Rate: 0.00623624
	LOSS [training: 0.18356907582702406 | validation: 0.30301673122039297]
	TIME [epoch: 8.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22640678415691534		[learning rate: 0.0062289]
		[batch 20/20] avg loss: 0.14037634996488335		[learning rate: 0.0062215]
	Learning Rate: 0.00622153
	LOSS [training: 0.18339156706089932 | validation: 0.26771924632260546]
	TIME [epoch: 8.38 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3035552445308623		[learning rate: 0.0062142]
		[batch 20/20] avg loss: 0.2536884571246548		[learning rate: 0.0062069]
	Learning Rate: 0.00620686
	LOSS [training: 0.27862185082775853 | validation: 0.12013894359077698]
	TIME [epoch: 8.37 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1958220458558407		[learning rate: 0.0061995]
		[batch 20/20] avg loss: 0.272768287451157		[learning rate: 0.0061922]
	Learning Rate: 0.00619222
	LOSS [training: 0.23429516665349887 | validation: 0.08673535346979978]
	TIME [epoch: 8.35 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15027800285568357		[learning rate: 0.0061849]
		[batch 20/20] avg loss: 0.20136671659703193		[learning rate: 0.0061776]
	Learning Rate: 0.00617761
	LOSS [training: 0.17582235972635776 | validation: 0.05659012438580488]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22108882312042302		[learning rate: 0.0061703]
		[batch 20/20] avg loss: 0.28353685815340846		[learning rate: 0.006163]
	Learning Rate: 0.00616304
	LOSS [training: 0.2523128406369158 | validation: 0.13496725365525997]
	TIME [epoch: 8.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2164370373291437		[learning rate: 0.0061558]
		[batch 20/20] avg loss: 0.24228212424633933		[learning rate: 0.0061485]
	Learning Rate: 0.0061485
	LOSS [training: 0.22935958078774155 | validation: 0.16361274829763567]
	TIME [epoch: 8.33 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34563174289299436		[learning rate: 0.0061412]
		[batch 20/20] avg loss: 0.20592428766684465		[learning rate: 0.006134]
	Learning Rate: 0.006134
	LOSS [training: 0.2757780152799195 | validation: 0.08968153809766022]
	TIME [epoch: 8.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1732567085433119		[learning rate: 0.0061268]
		[batch 20/20] avg loss: 0.17195526174559586		[learning rate: 0.0061195]
	Learning Rate: 0.00611953
	LOSS [training: 0.1726059851444539 | validation: 0.901961812567234]
	TIME [epoch: 8.37 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3381521634110273		[learning rate: 0.0061123]
		[batch 20/20] avg loss: 0.20211950130073558		[learning rate: 0.0061051]
	Learning Rate: 0.00610509
	LOSS [training: 0.27013583235588146 | validation: 0.20603015020060783]
	TIME [epoch: 8.33 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15673591835840203		[learning rate: 0.0060979]
		[batch 20/20] avg loss: 0.22768763140542542		[learning rate: 0.0060907]
	Learning Rate: 0.00609069
	LOSS [training: 0.19221177488191374 | validation: 0.24391522491286255]
	TIME [epoch: 8.33 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21628812319708643		[learning rate: 0.0060835]
		[batch 20/20] avg loss: 0.14679463217962577		[learning rate: 0.0060763]
	Learning Rate: 0.00607633
	LOSS [training: 0.18154137768835607 | validation: 0.41951717795618243]
	TIME [epoch: 8.34 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22407971158255652		[learning rate: 0.0060692]
		[batch 20/20] avg loss: 0.18152304406796566		[learning rate: 0.006062]
	Learning Rate: 0.00606199
	LOSS [training: 0.2028013778252611 | validation: 0.30960698366512346]
	TIME [epoch: 8.36 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1570262256394271		[learning rate: 0.0060548]
		[batch 20/20] avg loss: 0.2684859191960998		[learning rate: 0.0060477]
	Learning Rate: 0.00604769
	LOSS [training: 0.21275607241776345 | validation: 0.2014140611485788]
	TIME [epoch: 8.34 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.189596124720048		[learning rate: 0.0060406]
		[batch 20/20] avg loss: 0.6800828444262562		[learning rate: 0.0060334]
	Learning Rate: 0.00603343
	LOSS [training: 0.4348394845731522 | validation: 0.2625082772763618]
	TIME [epoch: 8.33 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1833857184678988		[learning rate: 0.0060263]
		[batch 20/20] avg loss: 0.20818563289392675		[learning rate: 0.0060192]
	Learning Rate: 0.0060192
	LOSS [training: 0.19578567568091282 | validation: 0.07204448995441842]
	TIME [epoch: 8.34 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1740250793600369		[learning rate: 0.0060121]
		[batch 20/20] avg loss: 0.19827055961818593		[learning rate: 0.006005]
	Learning Rate: 0.006005
	LOSS [training: 0.18614781948911138 | validation: 0.1776024760400286]
	TIME [epoch: 8.36 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18138086293984754		[learning rate: 0.0059979]
		[batch 20/20] avg loss: 0.16548973907528902		[learning rate: 0.0059908]
	Learning Rate: 0.00599083
	LOSS [training: 0.17343530100756827 | validation: 0.0684397839996426]
	TIME [epoch: 8.34 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33269777696940006		[learning rate: 0.0059838]
		[batch 20/20] avg loss: 0.19068154037535995		[learning rate: 0.0059767]
	Learning Rate: 0.0059767
	LOSS [training: 0.26168965867238 | validation: 0.07450983424457153]
	TIME [epoch: 8.34 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2150510365263989		[learning rate: 0.0059696]
		[batch 20/20] avg loss: 0.201871850838046		[learning rate: 0.0059626]
	Learning Rate: 0.0059626
	LOSS [training: 0.20846144368222244 | validation: 0.24980354705475769]
	TIME [epoch: 8.34 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22111302850679646		[learning rate: 0.0059556]
		[batch 20/20] avg loss: 0.1874958403495222		[learning rate: 0.0059485]
	Learning Rate: 0.00594854
	LOSS [training: 0.20430443442815935 | validation: 0.7271667356200571]
	TIME [epoch: 8.37 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23829035558493375		[learning rate: 0.0059415]
		[batch 20/20] avg loss: 0.24143408123565857		[learning rate: 0.0059345]
	Learning Rate: 0.00593451
	LOSS [training: 0.23986221841029615 | validation: 0.3662922853638049]
	TIME [epoch: 8.34 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21466075007173763		[learning rate: 0.0059275]
		[batch 20/20] avg loss: 0.1442850682097339		[learning rate: 0.0059205]
	Learning Rate: 0.00592051
	LOSS [training: 0.17947290914073571 | validation: 0.19874434855009268]
	TIME [epoch: 8.34 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18125135943596782		[learning rate: 0.0059135]
		[batch 20/20] avg loss: 0.21794319775273277		[learning rate: 0.0059065]
	Learning Rate: 0.00590654
	LOSS [training: 0.19959727859435028 | validation: 0.10999845033415231]
	TIME [epoch: 8.34 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2635189026783385		[learning rate: 0.0058996]
		[batch 20/20] avg loss: 0.2260487335137426		[learning rate: 0.0058926]
	Learning Rate: 0.00589261
	LOSS [training: 0.24478381809604052 | validation: 0.10227863747037785]
	TIME [epoch: 8.37 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2672995156033324		[learning rate: 0.0058857]
		[batch 20/20] avg loss: 0.13749413821701964		[learning rate: 0.0058787]
	Learning Rate: 0.00587871
	LOSS [training: 0.20239682691017605 | validation: 0.37727386593621565]
	TIME [epoch: 8.35 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2518415980784191		[learning rate: 0.0058718]
		[batch 20/20] avg loss: 0.38384872493494565		[learning rate: 0.0058648]
	Learning Rate: 0.00586484
	LOSS [training: 0.3178451615066823 | validation: 0.6542400627331462]
	TIME [epoch: 8.34 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2722520804408092		[learning rate: 0.0058579]
		[batch 20/20] avg loss: 0.24965513656226584		[learning rate: 0.005851]
	Learning Rate: 0.00585101
	LOSS [training: 0.2609536085015375 | validation: 0.1111342332741577]
	TIME [epoch: 8.34 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3766047588486304		[learning rate: 0.0058441]
		[batch 20/20] avg loss: 0.47855455674526653		[learning rate: 0.0058372]
	Learning Rate: 0.00583721
	LOSS [training: 0.42757965779694845 | validation: 0.14793609406297623]
	TIME [epoch: 8.37 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28097000901347163		[learning rate: 0.0058303]
		[batch 20/20] avg loss: 0.3408717192121124		[learning rate: 0.0058234]
	Learning Rate: 0.00582344
	LOSS [training: 0.31092086411279196 | validation: 0.24933058643484488]
	TIME [epoch: 8.34 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30177096796341163		[learning rate: 0.0058166]
		[batch 20/20] avg loss: 0.24409150783089903		[learning rate: 0.0058097]
	Learning Rate: 0.0058097
	LOSS [training: 0.27293123789715534 | validation: 0.07236406361259584]
	TIME [epoch: 8.34 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8035283660301584		[learning rate: 0.0058028]
		[batch 20/20] avg loss: 0.7811846924909706		[learning rate: 0.005796]
	Learning Rate: 0.005796
	LOSS [training: 0.7923565292605644 | validation: 0.5832247189547591]
	TIME [epoch: 8.34 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27317484290502786		[learning rate: 0.0057892]
		[batch 20/20] avg loss: 0.34973765916523364		[learning rate: 0.0057823]
	Learning Rate: 0.00578233
	LOSS [training: 0.31145625103513075 | validation: 0.19358673088661132]
	TIME [epoch: 8.37 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24326972019291376		[learning rate: 0.0057755]
		[batch 20/20] avg loss: 0.3071442610852891		[learning rate: 0.0057687]
	Learning Rate: 0.00576869
	LOSS [training: 0.27520699063910137 | validation: 0.12675863446993524]
	TIME [epoch: 8.34 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3423223862709858		[learning rate: 0.0057619]
		[batch 20/20] avg loss: 0.16112212915001753		[learning rate: 0.0057551]
	Learning Rate: 0.00575508
	LOSS [training: 0.25172225771050166 | validation: 0.09015346378059527]
	TIME [epoch: 8.34 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2125257297432152		[learning rate: 0.0057483]
		[batch 20/20] avg loss: 0.33449851256980456		[learning rate: 0.0057415]
	Learning Rate: 0.0057415
	LOSS [training: 0.2735121211565098 | validation: 0.1979569983866017]
	TIME [epoch: 8.35 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20485348428752648		[learning rate: 0.0057347]
		[batch 20/20] avg loss: 0.3054084093868903		[learning rate: 0.005728]
	Learning Rate: 0.00572796
	LOSS [training: 0.2551309468372084 | validation: 0.17348756827860667]
	TIME [epoch: 8.37 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15208164902050794		[learning rate: 0.0057212]
		[batch 20/20] avg loss: 0.23390875834773867		[learning rate: 0.0057144]
	Learning Rate: 0.00571445
	LOSS [training: 0.19299520368412337 | validation: 0.0765515268729157]
	TIME [epoch: 8.34 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18466153002153843		[learning rate: 0.0057077]
		[batch 20/20] avg loss: 0.1853187689880374		[learning rate: 0.005701]
	Learning Rate: 0.00570097
	LOSS [training: 0.18499014950478787 | validation: 0.15480521148022713]
	TIME [epoch: 8.35 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15887869261436519		[learning rate: 0.0056942]
		[batch 20/20] avg loss: 0.37839762393178356		[learning rate: 0.0056875]
	Learning Rate: 0.00568752
	LOSS [training: 0.2686381582730744 | validation: 0.13748734675941338]
	TIME [epoch: 8.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.212903122194915		[learning rate: 0.0056808]
		[batch 20/20] avg loss: 0.1500658764402643		[learning rate: 0.0056741]
	Learning Rate: 0.00567411
	LOSS [training: 0.18148449931758964 | validation: 0.13372742367482407]
	TIME [epoch: 8.37 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16577265552211778		[learning rate: 0.0056674]
		[batch 20/20] avg loss: 0.30595168886222124		[learning rate: 0.0056607]
	Learning Rate: 0.00566072
	LOSS [training: 0.2358621721921695 | validation: 0.06338798722878487]
	TIME [epoch: 8.34 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14268990502158638		[learning rate: 0.005654]
		[batch 20/20] avg loss: 0.2034460529960561		[learning rate: 0.0056474]
	Learning Rate: 0.00564737
	LOSS [training: 0.17306797900882126 | validation: 0.0662485307203311]
	TIME [epoch: 8.34 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21478256499082532		[learning rate: 0.0056407]
		[batch 20/20] avg loss: 0.25258818998226995		[learning rate: 0.005634]
	Learning Rate: 0.00563405
	LOSS [training: 0.23368537748654764 | validation: 0.24051742022797756]
	TIME [epoch: 8.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15495258519190241		[learning rate: 0.0056274]
		[batch 20/20] avg loss: 0.2174219051189632		[learning rate: 0.0056208]
	Learning Rate: 0.00562076
	LOSS [training: 0.1861872451554328 | validation: 0.2393898434828287]
	TIME [epoch: 8.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22114970179571153		[learning rate: 0.0056141]
		[batch 20/20] avg loss: 0.2083402425885005		[learning rate: 0.0056075]
	Learning Rate: 0.0056075
	LOSS [training: 0.21474497219210598 | validation: 0.28726054947371643]
	TIME [epoch: 8.34 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22053601250281854		[learning rate: 0.0056009]
		[batch 20/20] avg loss: 0.1608523532719069		[learning rate: 0.0055943]
	Learning Rate: 0.00559427
	LOSS [training: 0.1906941828873627 | validation: 0.9032850445660073]
	TIME [epoch: 8.34 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2670034181004812		[learning rate: 0.0055877]
		[batch 20/20] avg loss: 0.1660285683726571		[learning rate: 0.0055811]
	Learning Rate: 0.00558108
	LOSS [training: 0.21651599323656917 | validation: 0.18388002600702752]
	TIME [epoch: 8.35 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20428325675647924		[learning rate: 0.0055745]
		[batch 20/20] avg loss: 0.21326939555712549		[learning rate: 0.0055679]
	Learning Rate: 0.00556791
	LOSS [training: 0.20877632615680236 | validation: 0.3788944144622134]
	TIME [epoch: 8.37 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24292301611338166		[learning rate: 0.0055613]
		[batch 20/20] avg loss: 0.15056281390289203		[learning rate: 0.0055548]
	Learning Rate: 0.00555478
	LOSS [training: 0.19674291500813687 | validation: 0.1932032185955294]
	TIME [epoch: 8.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20615778398353285		[learning rate: 0.0055482]
		[batch 20/20] avg loss: 0.236233816953123		[learning rate: 0.0055417]
	Learning Rate: 0.00554167
	LOSS [training: 0.22119580046832796 | validation: 1.0069672393226008]
	TIME [epoch: 8.35 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.527916842192462		[learning rate: 0.0055351]
		[batch 20/20] avg loss: 0.4099454260540988		[learning rate: 0.0055286]
	Learning Rate: 0.0055286
	LOSS [training: 0.46893113412328047 | validation: 0.30846677108837767]
	TIME [epoch: 8.36 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5132257714557805		[learning rate: 0.0055221]
		[batch 20/20] avg loss: 0.5381009475319255		[learning rate: 0.0055156]
	Learning Rate: 0.00551556
	LOSS [training: 0.5256633594938529 | validation: 0.27753227288765764]
	TIME [epoch: 8.37 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29347830588793833		[learning rate: 0.0055091]
		[batch 20/20] avg loss: 0.19967309513597603		[learning rate: 0.0055026]
	Learning Rate: 0.00550255
	LOSS [training: 0.24657570051195718 | validation: 0.07071950722707387]
	TIME [epoch: 8.35 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20309300018479828		[learning rate: 0.0054961]
		[batch 20/20] avg loss: 0.20633725227688032		[learning rate: 0.0054896]
	Learning Rate: 0.00548957
	LOSS [training: 0.20471512623083932 | validation: 0.17852107476164814]
	TIME [epoch: 8.35 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.270671584646136		[learning rate: 0.0054831]
		[batch 20/20] avg loss: 0.29647915255200585		[learning rate: 0.0054766]
	Learning Rate: 0.00547662
	LOSS [training: 0.28357536859907095 | validation: 0.09753993699673154]
	TIME [epoch: 8.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20938659850471014		[learning rate: 0.0054702]
		[batch 20/20] avg loss: 0.20215153679932976		[learning rate: 0.0054637]
	Learning Rate: 0.0054637
	LOSS [training: 0.20576906765201994 | validation: 0.17108176537193354]
	TIME [epoch: 8.37 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15776475602857873		[learning rate: 0.0054573]
		[batch 20/20] avg loss: 0.3285603632833426		[learning rate: 0.0054508]
	Learning Rate: 0.00545082
	LOSS [training: 0.2431625596559607 | validation: 0.15304023170690664]
	TIME [epoch: 8.35 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20022328732233566		[learning rate: 0.0054444]
		[batch 20/20] avg loss: 0.21691034893125255		[learning rate: 0.005438]
	Learning Rate: 0.00543796
	LOSS [training: 0.20856681812679412 | validation: 0.054287030227073284]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r1_20240219_184950/states/model_tr_study2_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21608269508929295		[learning rate: 0.0054315]
		[batch 20/20] avg loss: 0.24546722001834537		[learning rate: 0.0054251]
	Learning Rate: 0.00542513
	LOSS [training: 0.23077495755381916 | validation: 0.22900940751145388]
	TIME [epoch: 8.38 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14651873126020046		[learning rate: 0.0054187]
		[batch 20/20] avg loss: 0.24318252592101558		[learning rate: 0.0054123]
	Learning Rate: 0.00541233
	LOSS [training: 0.19485062859060803 | validation: 0.1480341448547901]
	TIME [epoch: 8.37 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26337866265905013		[learning rate: 0.0054059]
		[batch 20/20] avg loss: 0.19013837117277105		[learning rate: 0.0053996]
	Learning Rate: 0.00539957
	LOSS [training: 0.2267585169159106 | validation: 0.10667374077055058]
	TIME [epoch: 8.37 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18937637408714703		[learning rate: 0.0053932]
		[batch 20/20] avg loss: 0.19522630335783103		[learning rate: 0.0053868]
	Learning Rate: 0.00538683
	LOSS [training: 0.19230133872248903 | validation: 0.233283385463011]
	TIME [epoch: 8.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19597697091539978		[learning rate: 0.0053805]
		[batch 20/20] avg loss: 0.1672691831717366		[learning rate: 0.0053741]
	Learning Rate: 0.00537412
	LOSS [training: 0.18162307704356823 | validation: 0.13985672046752215]
	TIME [epoch: 8.38 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1445584485219776		[learning rate: 0.0053678]
		[batch 20/20] avg loss: 0.21101513526768328		[learning rate: 0.0053614]
	Learning Rate: 0.00536145
	LOSS [training: 0.17778679189483046 | validation: 0.12784632218264588]
	TIME [epoch: 8.37 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17796872639424946		[learning rate: 0.0053551]
		[batch 20/20] avg loss: 0.20580948267962232		[learning rate: 0.0053488]
	Learning Rate: 0.0053488
	LOSS [training: 0.1918891045369359 | validation: 0.07070921109541317]
	TIME [epoch: 8.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1827928868501549		[learning rate: 0.0053425]
		[batch 20/20] avg loss: 0.223507829102205		[learning rate: 0.0053362]
	Learning Rate: 0.00533618
	LOSS [training: 0.20315035797617992 | validation: 0.3168456876355107]
	TIME [epoch: 8.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16742995953988143		[learning rate: 0.0053299]
		[batch 20/20] avg loss: 0.1637729080169613		[learning rate: 0.0053236]
	Learning Rate: 0.0053236
	LOSS [training: 0.16560143377842135 | validation: 0.1396094802418288]
	TIME [epoch: 8.38 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19682450776362928		[learning rate: 0.0053173]
		[batch 20/20] avg loss: 0.1814208138794908		[learning rate: 0.005311]
	Learning Rate: 0.00531104
	LOSS [training: 0.18912266082156004 | validation: 2.822231773833516]
	TIME [epoch: 8.37 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9011795021389828		[learning rate: 0.0053048]
		[batch 20/20] avg loss: 0.2446966234838885		[learning rate: 0.0052985]
	Learning Rate: 0.00529851
	LOSS [training: 0.5729380628114358 | validation: 0.3377196301783857]
	TIME [epoch: 8.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20628695516247278		[learning rate: 0.0052923]
		[batch 20/20] avg loss: 0.12962638255123116		[learning rate: 0.005286]
	Learning Rate: 0.00528601
	LOSS [training: 0.16795666885685195 | validation: 0.3916318048089336]
	TIME [epoch: 8.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17114182703695033		[learning rate: 0.0052798]
		[batch 20/20] avg loss: 0.13865312275424843		[learning rate: 0.0052735]
	Learning Rate: 0.00527354
	LOSS [training: 0.15489747489559944 | validation: 0.10526256389227705]
	TIME [epoch: 8.37 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19502790082467325		[learning rate: 0.0052673]
		[batch 20/20] avg loss: 0.2525097858852264		[learning rate: 0.0052611]
	Learning Rate: 0.0052611
	LOSS [training: 0.2237688433549498 | validation: 0.20678581015018524]
	TIME [epoch: 8.37 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16195766322192534		[learning rate: 0.0052549]
		[batch 20/20] avg loss: 0.19256857654348974		[learning rate: 0.0052487]
	Learning Rate: 0.00524869
	LOSS [training: 0.17726311988270754 | validation: 0.2897969654293055]
	TIME [epoch: 8.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19965047231014238		[learning rate: 0.0052425]
		[batch 20/20] avg loss: 0.2158968953813912		[learning rate: 0.0052363]
	Learning Rate: 0.00523631
	LOSS [training: 0.2077736838457668 | validation: 0.15242962558430176]
	TIME [epoch: 8.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20049798686940049		[learning rate: 0.0052301]
		[batch 20/20] avg loss: 0.21289381184985104		[learning rate: 0.005224]
	Learning Rate: 0.00522396
	LOSS [training: 0.2066958993596258 | validation: 0.12232269376141947]
	TIME [epoch: 8.38 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20335427367442635		[learning rate: 0.0052178]
		[batch 20/20] avg loss: 0.2003730459068485		[learning rate: 0.0052116]
	Learning Rate: 0.00521164
	LOSS [training: 0.20186365979063745 | validation: 0.5408697878268821]
	TIME [epoch: 8.37 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25437934967467957		[learning rate: 0.0052055]
		[batch 20/20] avg loss: 0.31074775778688374		[learning rate: 0.0051993]
	Learning Rate: 0.00519935
	LOSS [training: 0.2825635537307816 | validation: 0.16057668718012924]
	TIME [epoch: 8.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2358341141090992		[learning rate: 0.0051932]
		[batch 20/20] avg loss: 0.22904576928048198		[learning rate: 0.0051871]
	Learning Rate: 0.00518708
	LOSS [training: 0.23243994169479057 | validation: 0.115548852184456]
	TIME [epoch: 8.36 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18370517368229913		[learning rate: 0.005181]
		[batch 20/20] avg loss: 0.17253351913595277		[learning rate: 0.0051748]
	Learning Rate: 0.00517485
	LOSS [training: 0.178119346409126 | validation: 0.1407087028585089]
	TIME [epoch: 8.39 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13499782043611278		[learning rate: 0.0051687]
		[batch 20/20] avg loss: 0.23243423608768016		[learning rate: 0.0051626]
	Learning Rate: 0.00516264
	LOSS [training: 0.18371602826189645 | validation: 0.27893622158952386]
	TIME [epoch: 8.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1516129246635442		[learning rate: 0.0051565]
		[batch 20/20] avg loss: 0.1986907299863928		[learning rate: 0.0051505]
	Learning Rate: 0.00515046
	LOSS [training: 0.1751518273249685 | validation: 0.08227999216339879]
	TIME [epoch: 8.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17303605552287998		[learning rate: 0.0051444]
		[batch 20/20] avg loss: 0.2998275310628877		[learning rate: 0.0051383]
	Learning Rate: 0.00513831
	LOSS [training: 0.23643179329288383 | validation: 0.06761270028805781]
	TIME [epoch: 8.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24951980049167885		[learning rate: 0.0051322]
		[batch 20/20] avg loss: 0.2262876137838381		[learning rate: 0.0051262]
	Learning Rate: 0.00512619
	LOSS [training: 0.23790370713775846 | validation: 0.07694881119948756]
	TIME [epoch: 8.38 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16538738454245855		[learning rate: 0.0051201]
		[batch 20/20] avg loss: 0.24514824275797734		[learning rate: 0.0051141]
	Learning Rate: 0.0051141
	LOSS [training: 0.20526781365021796 | validation: 0.31558484394219877]
	TIME [epoch: 8.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20292739451866232		[learning rate: 0.0051081]
		[batch 20/20] avg loss: 0.18434301206068568		[learning rate: 0.005102]
	Learning Rate: 0.00510204
	LOSS [training: 0.19363520328967398 | validation: 0.2168038200900162]
	TIME [epoch: 8.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21228619703275897		[learning rate: 0.005096]
		[batch 20/20] avg loss: 0.18696993257159372		[learning rate: 0.00509]
	Learning Rate: 0.00509
	LOSS [training: 0.19962806480217635 | validation: 0.09873467227900538]
	TIME [epoch: 8.36 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1529093380530938		[learning rate: 0.005084]
		[batch 20/20] avg loss: 0.15440895857306894		[learning rate: 0.005078]
	Learning Rate: 0.00507799
	LOSS [training: 0.1536591483130814 | validation: 0.10386482845874595]
	TIME [epoch: 8.38 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19834409848693185		[learning rate: 0.005072]
		[batch 20/20] avg loss: 0.17675816803582084		[learning rate: 0.005066]
	Learning Rate: 0.00506602
	LOSS [training: 0.18755113326137635 | validation: 0.07734734561389106]
	TIME [epoch: 8.36 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22350706087676384		[learning rate: 0.00506]
		[batch 20/20] avg loss: 0.24768956432218134		[learning rate: 0.0050541]
	Learning Rate: 0.00505407
	LOSS [training: 0.23559831259947264 | validation: 0.21441752001351783]
	TIME [epoch: 8.35 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14706901497713162		[learning rate: 0.0050481]
		[batch 20/20] avg loss: 0.16912003891669047		[learning rate: 0.0050421]
	Learning Rate: 0.00504215
	LOSS [training: 0.15809452694691106 | validation: 0.08960462334294751]
	TIME [epoch: 8.36 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13861709134698347		[learning rate: 0.0050362]
		[batch 20/20] avg loss: 0.2174091416392563		[learning rate: 0.0050303]
	Learning Rate: 0.00503025
	LOSS [training: 0.1780131164931199 | validation: 0.40785650511941784]
	TIME [epoch: 8.38 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22337497344926596		[learning rate: 0.0050243]
		[batch 20/20] avg loss: 0.11527703323520924		[learning rate: 0.0050184]
	Learning Rate: 0.00501839
	LOSS [training: 0.1693260033422376 | validation: 0.3893590623585781]
	TIME [epoch: 8.36 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1712264016294116		[learning rate: 0.0050125]
		[batch 20/20] avg loss: 1.6323914557531303		[learning rate: 0.0050065]
	Learning Rate: 0.00500655
	LOSS [training: 0.901808928691271 | validation: 3.4354014710121032]
	TIME [epoch: 8.36 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.310239928633196		[learning rate: 0.0050006]
		[batch 20/20] avg loss: 3.6234391781740234		[learning rate: 0.0049947]
	Learning Rate: 0.00499474
	LOSS [training: 3.9668395534036107 | validation: 1.5508793552922602]
	TIME [epoch: 8.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1394213379898308		[learning rate: 0.0049888]
		[batch 20/20] avg loss: 1.1290369674667544		[learning rate: 0.004983]
	Learning Rate: 0.00498296
	LOSS [training: 1.1342291527282926 | validation: 1.255470421946605]
	TIME [epoch: 8.38 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1538302130522942		[learning rate: 0.0049771]
		[batch 20/20] avg loss: 1.1331582610106954		[learning rate: 0.0049712]
	Learning Rate: 0.0049712
	LOSS [training: 1.1434942370314953 | validation: 1.0979768115029471]
	TIME [epoch: 8.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3006068432969995		[learning rate: 0.0049653]
		[batch 20/20] avg loss: 2.447046826605922		[learning rate: 0.0049595]
	Learning Rate: 0.00495948
	LOSS [training: 1.8738268349514609 | validation: 1.557458389773802]
	TIME [epoch: 8.35 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7715542790398302		[learning rate: 0.0049536]
		[batch 20/20] avg loss: 1.999922115441444		[learning rate: 0.0049478]
	Learning Rate: 0.00494778
	LOSS [training: 1.8857381972406368 | validation: 2.074425643400306]
	TIME [epoch: 8.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9621527197031519		[learning rate: 0.0049419]
		[batch 20/20] avg loss: 2.1113145102787936		[learning rate: 0.0049361]
	Learning Rate: 0.00493611
	LOSS [training: 2.0367336149909727 | validation: 2.5237565982712775]
	TIME [epoch: 8.39 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.060632942359137		[learning rate: 0.0049303]
		[batch 20/20] avg loss: 2.056301477328989		[learning rate: 0.0049245]
	Learning Rate: 0.00492446
	LOSS [training: 2.0584672098440633 | validation: 2.0490423601508265]
	TIME [epoch: 8.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.047076731919728		[learning rate: 0.0049187]
		[batch 20/20] avg loss: 2.8539005995926816		[learning rate: 0.0049128]
	Learning Rate: 0.00491285
	LOSS [training: 2.450488665756205 | validation: 2.3985159932722864]
	TIME [epoch: 8.36 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2302827216254313		[learning rate: 0.0049071]
		[batch 20/20] avg loss: 2.6361333585793667		[learning rate: 0.0049013]
	Learning Rate: 0.00490126
	LOSS [training: 2.433208040102399 | validation: 3.262015879419409]
	TIME [epoch: 8.35 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.030695880036158		[learning rate: 0.0048955]
		[batch 20/20] avg loss: 3.1193330566177906		[learning rate: 0.0048897]
	Learning Rate: 0.0048897
	LOSS [training: 3.075014468326974 | validation: 3.329460310457481]
	TIME [epoch: 8.38 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8916672404827164		[learning rate: 0.0048839]
		[batch 20/20] avg loss: 3.0264340477503198		[learning rate: 0.0048782]
	Learning Rate: 0.00487816
	LOSS [training: 2.959050644116518 | validation: 3.9529031996315247]
	TIME [epoch: 8.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3083961740554253		[learning rate: 0.0048724]
		[batch 20/20] avg loss: 3.3058339831611945		[learning rate: 0.0048667]
	Learning Rate: 0.00486666
	LOSS [training: 3.3071150786083097 | validation: 3.444833881331265]
	TIME [epoch: 8.35 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.274720686932001		[learning rate: 0.0048609]
		[batch 20/20] avg loss: 3.44130252984926		[learning rate: 0.0048552]
	Learning Rate: 0.00485518
	LOSS [training: 3.3580116083906306 | validation: 3.5568801522039974]
	TIME [epoch: 8.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.459611216378957		[learning rate: 0.0048494]
		[batch 20/20] avg loss: 3.170949568741945		[learning rate: 0.0048437]
	Learning Rate: 0.00484372
	LOSS [training: 3.315280392560451 | validation: 2.6040000066930857]
	TIME [epoch: 8.38 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6685900236553597		[learning rate: 0.004838]
		[batch 20/20] avg loss: 1.1763256873300953		[learning rate: 0.0048323]
	Learning Rate: 0.0048323
	LOSS [training: 1.4224578554927274 | validation: 1.832817510543678]
	TIME [epoch: 8.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.289551101470635		[learning rate: 0.0048266]
		[batch 20/20] avg loss: 2.475535550601936		[learning rate: 0.0048209]
	Learning Rate: 0.0048209
	LOSS [training: 2.382543326036285 | validation: 1.723792444019604]
	TIME [epoch: 8.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3688135144604354		[learning rate: 0.0048152]
		[batch 20/20] avg loss: 3.467660876789187		[learning rate: 0.0048095]
	Learning Rate: 0.00480953
	LOSS [training: 2.918237195624811 | validation: 3.2911241208493363]
	TIME [epoch: 8.35 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3087073624435		[learning rate: 0.0048039]
		[batch 20/20] avg loss: 2.7656095705698975		[learning rate: 0.0047982]
	Learning Rate: 0.00479818
	LOSS [training: 3.0371584665066984 | validation: 3.0791142960937563]
	TIME [epoch: 8.38 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.466242616682601		[learning rate: 0.0047925]
		[batch 20/20] avg loss: 3.462819227817083		[learning rate: 0.0047869]
	Learning Rate: 0.00478687
	LOSS [training: 3.9645309222498417 | validation: 3.144476924470456]
	TIME [epoch: 8.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.354324630092697		[learning rate: 0.0047812]
		[batch 20/20] avg loss: 4.822618074307025		[learning rate: 0.0047756]
	Learning Rate: 0.00477557
	LOSS [training: 4.0884713521998615 | validation: 4.129655234170985]
	TIME [epoch: 8.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.584609834574925		[learning rate: 0.0047699]
		[batch 20/20] avg loss: 3.75836756371649		[learning rate: 0.0047643]
	Learning Rate: 0.00476431
	LOSS [training: 4.1714886991457085 | validation: 2.9992822472650964]
	TIME [epoch: 8.35 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.248668193950347		[learning rate: 0.0047587]
		[batch 20/20] avg loss: 1.9519430234288673		[learning rate: 0.0047531]
	Learning Rate: 0.00475307
	LOSS [training: 2.100305608689607 | validation: 2.6778285083465674]
	TIME [epoch: 8.39 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.395019860262802		[learning rate: 0.0047475]
		[batch 20/20] avg loss: 4.064705400514336		[learning rate: 0.0047419]
	Learning Rate: 0.00474186
	LOSS [training: 3.7298626303885696 | validation: 3.342311653176376]
	TIME [epoch: 8.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.6331715220175815		[learning rate: 0.0047363]
		[batch 20/20] avg loss: 4.081792287425921		[learning rate: 0.0047307]
	Learning Rate: 0.00473067
	LOSS [training: 3.857481904721751 | validation: 4.7187180869169145]
	TIME [epoch: 8.36 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5099236137847725		[learning rate: 0.0047251]
		[batch 20/20] avg loss: 3.630728425298488		[learning rate: 0.0047195]
	Learning Rate: 0.00471952
	LOSS [training: 3.5703260195416306 | validation: 3.8586053814977292]
	TIME [epoch: 8.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.080219424490791		[learning rate: 0.0047139]
		[batch 20/20] avg loss: 3.5289795939428905		[learning rate: 0.0047084]
	Learning Rate: 0.00470838
	LOSS [training: 3.804599509216841 | validation: 4.119979956569437]
	TIME [epoch: 8.38 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.225326775629805		[learning rate: 0.0047028]
		[batch 20/20] avg loss: 5.254289641261484		[learning rate: 0.0046973]
	Learning Rate: 0.00469728
	LOSS [training: 4.739808208445645 | validation: 6.1641704548096525]
	TIME [epoch: 8.36 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.320379559402214		[learning rate: 0.0046917]
		[batch 20/20] avg loss: 7.28176953497122		[learning rate: 0.0046862]
	Learning Rate: 0.0046862
	LOSS [training: 6.801074547186718 | validation: 7.295581243848309]
	TIME [epoch: 8.35 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.387116486743619		[learning rate: 0.0046807]
		[batch 20/20] avg loss: 7.59825080192695		[learning rate: 0.0046751]
	Learning Rate: 0.00467514
	LOSS [training: 7.492683644335285 | validation: 7.279846651892185]
	TIME [epoch: 8.35 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.1222907145775505		[learning rate: 0.0046696]
		[batch 20/20] avg loss: 4.396168534730526		[learning rate: 0.0046641]
	Learning Rate: 0.00466411
	LOSS [training: 4.759229624654037 | validation: 4.321504870802324]
	TIME [epoch: 8.38 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.490235125909405		[learning rate: 0.0046586]
		[batch 20/20] avg loss: 4.504324282863717		[learning rate: 0.0046531]
	Learning Rate: 0.00465311
	LOSS [training: 4.497279704386561 | validation: 4.790285912250266]
	TIME [epoch: 8.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.002727298145746		[learning rate: 0.0046476]
		[batch 20/20] avg loss: 4.749500162498174		[learning rate: 0.0046421]
	Learning Rate: 0.00464214
	LOSS [training: 4.8761137303219595 | validation: 4.3824129251189685]
	TIME [epoch: 8.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.880307789644462		[learning rate: 0.0046367]
		[batch 20/20] avg loss: 5.373155356754184		[learning rate: 0.0046312]
	Learning Rate: 0.00463119
	LOSS [training: 5.126731573199324 | validation: 5.20115846756509]
	TIME [epoch: 8.35 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.1612905849560144		[learning rate: 0.0046257]
		[batch 20/20] avg loss: 5.881200087028546		[learning rate: 0.0046203]
	Learning Rate: 0.00462026
	LOSS [training: 5.521245335992279 | validation: 6.758332220843905]
	TIME [epoch: 8.38 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.7836929506314725		[learning rate: 0.0046148]
		[batch 20/20] avg loss: 6.7665577879758745		[learning rate: 0.0046094]
	Learning Rate: 0.00460936
	LOSS [training: 6.275125369303675 | validation: 7.319643213492911]
	TIME [epoch: 8.36 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.420214320188096		[learning rate: 0.0046039]
		[batch 20/20] avg loss: 7.224990665511916		[learning rate: 0.0045985]
	Learning Rate: 0.00459849
	LOSS [training: 7.322602492850004 | validation: 8.215657623578384]
	TIME [epoch: 8.35 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.703136360343224		[learning rate: 0.0045931]
		[batch 20/20] avg loss: 8.921962798102998		[learning rate: 0.0045876]
	Learning Rate: 0.00458764
	LOSS [training: 8.812549579223111 | validation: 9.087105591006697]
	TIME [epoch: 8.36 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.1792791794565		[learning rate: 0.0045822]
		[batch 20/20] avg loss: 9.5790361223144		[learning rate: 0.0045768]
	Learning Rate: 0.00457682
	LOSS [training: 9.37915765088545 | validation: 9.7997679112481]
	TIME [epoch: 8.38 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.626707028520466		[learning rate: 0.0045714]
		[batch 20/20] avg loss: 9.767875610987835		[learning rate: 0.004566]
	Learning Rate: 0.00456603
	LOSS [training: 9.69729131975415 | validation: 9.949385527021798]
	TIME [epoch: 8.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.671825642126038		[learning rate: 0.0045606]
		[batch 20/20] avg loss: 8.266337231362558		[learning rate: 0.0045553]
	Learning Rate: 0.00455526
	LOSS [training: 8.469081436744299 | validation: 8.598613913198017]
	TIME [epoch: 8.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.358988134252579		[learning rate: 0.0045499]
		[batch 20/20] avg loss: 8.54541323488667		[learning rate: 0.0045445]
	Learning Rate: 0.00454451
	LOSS [training: 8.452200684569624 | validation: 9.638857841071214]
	TIME [epoch: 8.36 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.081550260194962		[learning rate: 0.0045391]
		[batch 20/20] avg loss: 8.803757550355533		[learning rate: 0.0045338]
	Learning Rate: 0.00453379
	LOSS [training: 8.94265390527525 | validation: 9.218157109411449]
	TIME [epoch: 8.39 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.606063058059373		[learning rate: 0.0045284]
		[batch 20/20] avg loss: 8.493662414604653		[learning rate: 0.0045231]
	Learning Rate: 0.0045231
	LOSS [training: 8.549862736332013 | validation: 8.673563303052484]
	TIME [epoch: 8.35 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.211708223337123		[learning rate: 0.0045178]
		[batch 20/20] avg loss: 8.486396332801608		[learning rate: 0.0045124]
	Learning Rate: 0.00451243
	LOSS [training: 8.349052278069363 | validation: 10.068153501424767]
	TIME [epoch: 8.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.608765804685175		[learning rate: 0.0045071]
		[batch 20/20] avg loss: 7.2588006081267125		[learning rate: 0.0045018]
	Learning Rate: 0.00450178
	LOSS [training: 7.933783206405943 | validation: 7.66231894590007]
	TIME [epoch: 8.36 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.7824824328529205		[learning rate: 0.0044965]
		[batch 20/20] avg loss: 6.440874798303672		[learning rate: 0.0044912]
	Learning Rate: 0.00449116
	LOSS [training: 6.611678615578297 | validation: 8.275834052707545]
	TIME [epoch: 8.39 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.67646269642492		[learning rate: 0.0044859]
		[batch 20/20] avg loss: 7.832259687749601		[learning rate: 0.0044806]
	Learning Rate: 0.00448057
	LOSS [training: 8.25436119208726 | validation: 6.961608974574649]
	TIME [epoch: 8.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.419123663578615		[learning rate: 0.0044753]
		[batch 20/20] avg loss: 5.6709029508799915		[learning rate: 0.00447]
	Learning Rate: 0.00447
	LOSS [training: 5.545013307229302 | validation: 5.963985132665091]
	TIME [epoch: 8.36 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.723317105499541		[learning rate: 0.0044647]
		[batch 20/20] avg loss: 3.774882425753458		[learning rate: 0.0044595]
	Learning Rate: 0.00445946
	LOSS [training: 4.249099765626498 | validation: 3.9670923076170794]
	TIME [epoch: 8.35 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4612102476220015		[learning rate: 0.0044542]
		[batch 20/20] avg loss: 3.278963011090009		[learning rate: 0.0044489]
	Learning Rate: 0.00444894
	LOSS [training: 3.370086629356005 | validation: 3.7147369029997357]
	TIME [epoch: 8.38 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.188068285112414		[learning rate: 0.0044437]
		[batch 20/20] avg loss: 2.7102194693072184		[learning rate: 0.0044384]
	Learning Rate: 0.00443844
	LOSS [training: 2.949143877209816 | validation: 2.7056216249624843]
	TIME [epoch: 8.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8317185608816842		[learning rate: 0.0044332]
		[batch 20/20] avg loss: 3.7042825750499078		[learning rate: 0.004428]
	Learning Rate: 0.00442797
	LOSS [training: 2.7680005679657955 | validation: 3.5793772311198637]
	TIME [epoch: 8.36 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.232981100583943		[learning rate: 0.0044227]
		[batch 20/20] avg loss: 4.123862405802847		[learning rate: 0.0044175]
	Learning Rate: 0.00441753
	LOSS [training: 4.178421753193396 | validation: 3.8033728257863846]
	TIME [epoch: 8.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.740474544716296		[learning rate: 0.0044123]
		[batch 20/20] avg loss: 5.029814541238046		[learning rate: 0.0044071]
	Learning Rate: 0.00440711
	LOSS [training: 4.385144542977171 | validation: 5.3787882599848835]
	TIME [epoch: 8.4 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.604531492388315		[learning rate: 0.0044019]
		[batch 20/20] avg loss: 5.490585241937492		[learning rate: 0.0043967]
	Learning Rate: 0.00439671
	LOSS [training: 5.547558367162902 | validation: 5.840032359553488]
	TIME [epoch: 8.35 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.738355112312869		[learning rate: 0.0043915]
		[batch 20/20] avg loss: 5.74299131414528		[learning rate: 0.0043863]
	Learning Rate: 0.00438634
	LOSS [training: 5.7406732132290745 | validation: 6.083982062444564]
	TIME [epoch: 8.37 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.228688068217183		[learning rate: 0.0043812]
		[batch 20/20] avg loss: 6.172333816956841		[learning rate: 0.004376]
	Learning Rate: 0.004376
	LOSS [training: 6.200510942587012 | validation: 6.1404846655836405]
	TIME [epoch: 8.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.574115974532312		[learning rate: 0.0043708]
		[batch 20/20] avg loss: 6.209449571370331		[learning rate: 0.0043657]
	Learning Rate: 0.00436567
	LOSS [training: 6.891782772951321 | validation: 5.556828590011784]
	TIME [epoch: 8.38 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.56351463310464		[learning rate: 0.0043605]
		[batch 20/20] avg loss: 5.380655909178981		[learning rate: 0.0043554]
	Learning Rate: 0.00435538
	LOSS [training: 5.47208527114181 | validation: 4.942592848560663]
	TIME [epoch: 8.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.083957763697225		[learning rate: 0.0043502]
		[batch 20/20] avg loss: 5.196248758041926		[learning rate: 0.0043451]
	Learning Rate: 0.0043451
	LOSS [training: 5.140103260869575 | validation: 5.05925704125366]
	TIME [epoch: 8.35 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.417728709445001		[learning rate: 0.00434]
		[batch 20/20] avg loss: 5.612461174544562		[learning rate: 0.0043349]
	Learning Rate: 0.00433485
	LOSS [training: 5.51509494199478 | validation: 5.267305083410956]
	TIME [epoch: 8.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.623826101096947		[learning rate: 0.0043297]
		[batch 20/20] avg loss: 6.068145731821703		[learning rate: 0.0043246]
	Learning Rate: 0.00432463
	LOSS [training: 5.845985916459326 | validation: 5.5890469354452765]
	TIME [epoch: 8.38 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.063835999012511		[learning rate: 0.0043195]
		[batch 20/20] avg loss: 6.152255326653885		[learning rate: 0.0043144]
	Learning Rate: 0.00431443
	LOSS [training: 6.108045662833197 | validation: 5.6629895470167195]
	TIME [epoch: 8.35 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.988667330114183		[learning rate: 0.0043093]
		[batch 20/20] avg loss: 5.557303967612176		[learning rate: 0.0043042]
	Learning Rate: 0.00430425
	LOSS [training: 5.772985648863179 | validation: 5.261033198024425]
	TIME [epoch: 8.35 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.7177902675327035		[learning rate: 0.0042992]
		[batch 20/20] avg loss: 5.445894718669938		[learning rate: 0.0042941]
	Learning Rate: 0.0042941
	LOSS [training: 5.5818424931013215 | validation: 5.033640710285828]
	TIME [epoch: 8.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.5035007207836895		[learning rate: 0.004289]
		[batch 20/20] avg loss: 6.151749072604154		[learning rate: 0.004284]
	Learning Rate: 0.00428397
	LOSS [training: 5.827624896693922 | validation: 6.2110429473047795]
	TIME [epoch: 8.38 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.943278207332426		[learning rate: 0.0042789]
		[batch 20/20] avg loss: 6.823256485466246		[learning rate: 0.0042739]
	Learning Rate: 0.00427386
	LOSS [training: 6.383267346399337 | validation: 7.400845380076715]
	TIME [epoch: 8.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.574488361565575		[learning rate: 0.0042688]
		[batch 20/20] avg loss: 6.624847427503914		[learning rate: 0.0042638]
	Learning Rate: 0.00426378
	LOSS [training: 7.099667894534744 | validation: 6.029593670442226]
	TIME [epoch: 8.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.5253638739041735		[learning rate: 0.0042587]
		[batch 20/20] avg loss: 6.373828162572268		[learning rate: 0.0042537]
	Learning Rate: 0.00425372
	LOSS [training: 6.44959601823822 | validation: 6.485488740239601]
	TIME [epoch: 8.35 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.5117886805204375		[learning rate: 0.0042487]
		[batch 20/20] avg loss: 6.856389329174839		[learning rate: 0.0042437]
	Learning Rate: 0.00424369
	LOSS [training: 6.684089004847638 | validation: 6.329455093295186]
	TIME [epoch: 8.38 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.548804166642929		[learning rate: 0.0042387]
		[batch 20/20] avg loss: 5.182424708133118		[learning rate: 0.0042337]
	Learning Rate: 0.00423368
	LOSS [training: 5.3656144373880235 | validation: 5.040422603423388]
	TIME [epoch: 8.35 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.4832655405592945		[learning rate: 0.0042287]
		[batch 20/20] avg loss: 3.9179189875157308		[learning rate: 0.0042237]
	Learning Rate: 0.00422369
	LOSS [training: 4.200592264037512 | validation: 4.039445527500485]
	TIME [epoch: 8.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.634908139287067		[learning rate: 0.0042187]
		[batch 20/20] avg loss: 5.012711303172959		[learning rate: 0.0042137]
	Learning Rate: 0.00421373
	LOSS [training: 4.823809721230012 | validation: 5.0703296699205875]
	TIME [epoch: 8.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.612481991200985		[learning rate: 0.0042088]
		[batch 20/20] avg loss: 6.077149164363597		[learning rate: 0.0042038]
	Learning Rate: 0.00420379
	LOSS [training: 5.84481557778229 | validation: 6.18066520482346]
	TIME [epoch: 8.38 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.783112271631971		[learning rate: 0.0041988]
		[batch 20/20] avg loss: 6.832226676074884		[learning rate: 0.0041939]
	Learning Rate: 0.00419387
	LOSS [training: 6.807669473853427 | validation: 7.433837109803176]
	TIME [epoch: 8.35 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.478404827058779		[learning rate: 0.0041889]
		[batch 20/20] avg loss: 6.551016487873547		[learning rate: 0.004184]
	Learning Rate: 0.00418398
	LOSS [training: 6.514710657466162 | validation: 5.979145869497741]
	TIME [epoch: 8.35 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.272571723325422		[learning rate: 0.004179]
		[batch 20/20] avg loss: 6.641634709566527		[learning rate: 0.0041741]
	Learning Rate: 0.00417411
	LOSS [training: 6.457103216445974 | validation: 6.680317200233372]
	TIME [epoch: 8.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.945134088088008		[learning rate: 0.0041692]
		[batch 20/20] avg loss: 6.994514315200756		[learning rate: 0.0041643]
	Learning Rate: 0.00416427
	LOSS [training: 6.969824201644383 | validation: 7.016559137334344]
	TIME [epoch: 8.37 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.580683728838257		[learning rate: 0.0041594]
		[batch 20/20] avg loss: 8.68842922767601		[learning rate: 0.0041544]
	Learning Rate: 0.00415444
	LOSS [training: 8.134556478257132 | validation: 8.747551120991174]
	TIME [epoch: 8.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.698700324966818		[learning rate: 0.0041495]
		[batch 20/20] avg loss: 7.65301339794248		[learning rate: 0.0041446]
	Learning Rate: 0.00414464
	LOSS [training: 8.17585686145465 | validation: 6.886518330537109]
	TIME [epoch: 8.35 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.983041694449201		[learning rate: 0.0041398]
		[batch 20/20] avg loss: 6.562347013944075		[learning rate: 0.0041349]
	Learning Rate: 0.00413487
	LOSS [training: 6.77269435419664 | validation: 6.19019526750502]
	TIME [epoch: 8.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.454633154200115		[learning rate: 0.00413]
		[batch 20/20] avg loss: 5.9568382328867076		[learning rate: 0.0041251]
	Learning Rate: 0.00412511
	LOSS [training: 6.705735693543413 | validation: 6.187676042248264]
	TIME [epoch: 8.38 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.402848879087455		[learning rate: 0.0041202]
		[batch 20/20] avg loss: 6.2943372591227815		[learning rate: 0.0041154]
	Learning Rate: 0.00411538
	LOSS [training: 6.348593069105119 | validation: 7.300755614935365]
	TIME [epoch: 8.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.938699997966387		[learning rate: 0.0041105]
		[batch 20/20] avg loss: 6.566440655884703		[learning rate: 0.0041057]
	Learning Rate: 0.00410568
	LOSS [training: 6.752570326925546 | validation: 6.081928595383028]
	TIME [epoch: 8.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.675539580145493		[learning rate: 0.0041008]
		[batch 20/20] avg loss: 5.456291501912035		[learning rate: 0.004096]
	Learning Rate: 0.00409599
	LOSS [training: 5.5659155410287635 | validation: 5.337190580947218]
	TIME [epoch: 8.37 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.434425092453895		[learning rate: 0.0040912]
		[batch 20/20] avg loss: 4.885031505295997		[learning rate: 0.0040863]
	Learning Rate: 0.00408633
	LOSS [training: 4.659728298874945 | validation: 5.483377452505257]
	TIME [epoch: 8.37 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.9544934085909045		[learning rate: 0.0040815]
		[batch 20/20] avg loss: 4.064429407707989		[learning rate: 0.0040767]
	Learning Rate: 0.00407669
	LOSS [training: 4.509461408149447 | validation: 3.094530505467091]
	TIME [epoch: 8.35 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.097160750647073		[learning rate: 0.0040719]
		[batch 20/20] avg loss: 2.6671048516341798		[learning rate: 0.0040671]
	Learning Rate: 0.00406707
	LOSS [training: 2.882132801140626 | validation: 2.458832333829127]
	TIME [epoch: 8.35 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.206034088947896		[learning rate: 0.0040623]
		[batch 20/20] avg loss: 2.7921598449989244		[learning rate: 0.0040575]
	Learning Rate: 0.00405748
	LOSS [training: 2.9990969669734104 | validation: 2.7434527156152875]
	TIME [epoch: 8.37 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3826623985212003		[learning rate: 0.0040527]
		[batch 20/20] avg loss: 2.573304105516411		[learning rate: 0.0040479]
	Learning Rate: 0.00404791
	LOSS [training: 2.477983252018806 | validation: 3.4201091237209704]
	TIME [epoch: 8.37 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1362647166056865		[learning rate: 0.0040431]
		[batch 20/20] avg loss: 2.6344990654123004		[learning rate: 0.0040384]
	Learning Rate: 0.00403836
	LOSS [training: 2.3853818910089934 | validation: 3.008294117240536]
	TIME [epoch: 8.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3204269676163345		[learning rate: 0.0040336]
		[batch 20/20] avg loss: 2.4859929263850318		[learning rate: 0.0040288]
	Learning Rate: 0.00402883
	LOSS [training: 2.4032099470006836 | validation: 2.697218603465463]
	TIME [epoch: 8.35 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2396236475488913		[learning rate: 0.0040241]
		[batch 20/20] avg loss: 2.097144309545272		[learning rate: 0.0040193]
	Learning Rate: 0.00401933
	LOSS [training: 2.1683839785470815 | validation: 0.9594754951744947]
	TIME [epoch: 8.37 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.126077450031778		[learning rate: 0.0040146]
		[batch 20/20] avg loss: 2.2658590955968743		[learning rate: 0.0040099]
	Learning Rate: 0.00400985
	LOSS [training: 2.195968272814326 | validation: 3.111026845845525]
	TIME [epoch: 8.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.420213714595424		[learning rate: 0.0040051]
		[batch 20/20] avg loss: 2.42825787029638		[learning rate: 0.0040004]
	Learning Rate: 0.00400039
	LOSS [training: 2.4242357924459017 | validation: 2.1891827736801273]
	TIME [epoch: 8.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1728111999743016		[learning rate: 0.0039957]
		[batch 20/20] avg loss: 2.6959649545607802		[learning rate: 0.003991]
	Learning Rate: 0.00399096
	LOSS [training: 1.9343880772675413 | validation: 3.633852693643576]
	TIME [epoch: 8.35 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0970875416596724		[learning rate: 0.0039862]
		[batch 20/20] avg loss: 2.6271530020206693		[learning rate: 0.0039815]
	Learning Rate: 0.00398154
	LOSS [training: 2.862120271840171 | validation: 1.1444585720153084]
	TIME [epoch: 8.37 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0307815525313526		[learning rate: 0.0039768]
		[batch 20/20] avg loss: 2.115115038408181		[learning rate: 0.0039721]
	Learning Rate: 0.00397215
	LOSS [training: 1.5729482954697667 | validation: 3.170017415873392]
	TIME [epoch: 8.37 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1570097940455755		[learning rate: 0.0039675]
		[batch 20/20] avg loss: 2.740615425596393		[learning rate: 0.0039628]
	Learning Rate: 0.00396278
	LOSS [training: 2.948812609820984 | validation: 1.537119044852781]
	TIME [epoch: 8.35 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5242589978279026		[learning rate: 0.0039581]
		[batch 20/20] avg loss: 3.7699762317632577		[learning rate: 0.0039534]
	Learning Rate: 0.00395343
	LOSS [training: 3.1471176147955804 | validation: 3.6223148814095656]
	TIME [epoch: 8.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9416003513154068		[learning rate: 0.0039488]
		[batch 20/20] avg loss: 2.3729226949242124		[learning rate: 0.0039441]
	Learning Rate: 0.00394411
	LOSS [training: 2.65726152311981 | validation: 1.100806176311363]
	TIME [epoch: 8.37 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8117821938553829		[learning rate: 0.0039395]
		[batch 20/20] avg loss: 2.166070568721694		[learning rate: 0.0039348]
	Learning Rate: 0.0039348
	LOSS [training: 1.9889263812885383 | validation: 1.7751162667386333]
	TIME [epoch: 8.37 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9263252518684686		[learning rate: 0.0039302]
		[batch 20/20] avg loss: 1.5952191130613802		[learning rate: 0.0039255]
	Learning Rate: 0.00392552
	LOSS [training: 2.2607721824649247 | validation: 1.8434695239908796]
	TIME [epoch: 8.35 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0667923462383504		[learning rate: 0.0039209]
		[batch 20/20] avg loss: 1.7402286075054731		[learning rate: 0.0039163]
	Learning Rate: 0.00391626
	LOSS [training: 1.903510476871912 | validation: 1.8150237268611358]
	TIME [epoch: 8.35 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.501504583266262		[learning rate: 0.0039116]
		[batch 20/20] avg loss: 2.246926140586462		[learning rate: 0.003907]
	Learning Rate: 0.00390702
	LOSS [training: 1.8742153619263617 | validation: 2.645986075810363]
	TIME [epoch: 8.37 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.108078189322156		[learning rate: 0.0039024]
		[batch 20/20] avg loss: 2.214533263540603		[learning rate: 0.0038978]
	Learning Rate: 0.00389781
	LOSS [training: 2.6613057264313795 | validation: 1.892872261619955]
	TIME [epoch: 8.37 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8584568438343225		[learning rate: 0.0038932]
		[batch 20/20] avg loss: 2.738765696672437		[learning rate: 0.0038886]
	Learning Rate: 0.00388861
	LOSS [training: 2.2986112702533794 | validation: 4.6180560507756665]
	TIME [epoch: 8.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.3347876669483565		[learning rate: 0.003884]
		[batch 20/20] avg loss: 3.9174078255737115		[learning rate: 0.0038794]
	Learning Rate: 0.00387944
	LOSS [training: 4.126097746261033 | validation: 6.083569336560101]
	TIME [epoch: 8.35 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.807210414011043		[learning rate: 0.0038749]
		[batch 20/20] avg loss: 4.605261498126199		[learning rate: 0.0038703]
	Learning Rate: 0.00387029
	LOSS [training: 4.70623595606862 | validation: 5.434995134706908]
	TIME [epoch: 8.37 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.384776033904745		[learning rate: 0.0038657]
		[batch 20/20] avg loss: 5.49660539134435		[learning rate: 0.0038612]
	Learning Rate: 0.00386116
	LOSS [training: 5.440690712624549 | validation: 4.327547801563971]
	TIME [epoch: 8.37 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.85961197873147		[learning rate: 0.0038566]
		[batch 20/20] avg loss: 4.887570508775786		[learning rate: 0.0038521]
	Learning Rate: 0.00385205
	LOSS [training: 4.873591243753628 | validation: 4.992868506890491]
	TIME [epoch: 8.35 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.834706567814355		[learning rate: 0.0038475]
		[batch 20/20] avg loss: 4.6130982548769		[learning rate: 0.003843]
	Learning Rate: 0.00384297
	LOSS [training: 4.723902411345626 | validation: 4.259991330119192]
	TIME [epoch: 8.35 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.449487073818268		[learning rate: 0.0038384]
		[batch 20/20] avg loss: 4.581589206114885		[learning rate: 0.0038339]
	Learning Rate: 0.0038339
	LOSS [training: 4.515538139966578 | validation: 4.355900932007008]
	TIME [epoch: 8.37 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.2011913567601535		[learning rate: 0.0038294]
		[batch 20/20] avg loss: 3.226025049424473		[learning rate: 0.0038249]
	Learning Rate: 0.00382486
	LOSS [training: 3.7136082030923143 | validation: 3.504823052266164]
	TIME [epoch: 8.37 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.6137528743782		[learning rate: 0.0038203]
		[batch 20/20] avg loss: 3.9172587021963565		[learning rate: 0.0038158]
	Learning Rate: 0.00381584
	LOSS [training: 3.765505788287278 | validation: 3.231887245553612]
	TIME [epoch: 8.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.2590226377311025		[learning rate: 0.0038113]
		[batch 20/20] avg loss: 5.618614029778163		[learning rate: 0.0038068]
	Learning Rate: 0.00380684
	LOSS [training: 4.938818333754632 | validation: 7.128117078256567]
	TIME [epoch: 8.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.516110592663513		[learning rate: 0.0038023]
		[batch 20/20] avg loss: 6.375516590240691		[learning rate: 0.0037979]
	Learning Rate: 0.00379786
	LOSS [training: 6.445813591452101 | validation: 5.686405611107359]
	TIME [epoch: 8.37 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.494220550176324		[learning rate: 0.0037934]
		[batch 20/20] avg loss: 5.613306975426899		[learning rate: 0.0037889]
	Learning Rate: 0.0037889
	LOSS [training: 5.553763762801612 | validation: 5.746198970124392]
	TIME [epoch: 8.37 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.232270890510476		[learning rate: 0.0037844]
		[batch 20/20] avg loss: 7.497009344065584		[learning rate: 0.00378]
	Learning Rate: 0.00377996
	LOSS [training: 6.8646401172880305 | validation: 8.388564116098614]
	TIME [epoch: 8.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.012941081984412		[learning rate: 0.0037755]
		[batch 20/20] avg loss: 8.192910438018913		[learning rate: 0.003771]
	Learning Rate: 0.00377104
	LOSS [training: 8.102925760001662 | validation: 9.57825573370749]
	TIME [epoch: 8.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.809746087419994		[learning rate: 0.0037666]
		[batch 20/20] avg loss: 9.805383729727708		[learning rate: 0.0037621]
	Learning Rate: 0.00376215
	LOSS [training: 9.807564908573854 | validation: 10.354232707690205]
	TIME [epoch: 8.36 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.93658513170669		[learning rate: 0.0037577]
		[batch 20/20] avg loss: 9.822101925758897		[learning rate: 0.0037533]
	Learning Rate: 0.00375327
	LOSS [training: 9.879343528732795 | validation: 10.29228075968009]
	TIME [epoch: 8.36 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.6800031203222		[learning rate: 0.0037488]
		[batch 20/20] avg loss: 8.704272210792306		[learning rate: 0.0037444]
	Learning Rate: 0.00374442
	LOSS [training: 9.192137665557256 | validation: 7.9945209491993365]
	TIME [epoch: 8.35 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.17828094342064		[learning rate: 0.00374]
		[batch 20/20] avg loss: 8.513640677129521		[learning rate: 0.0037356]
	Learning Rate: 0.00373559
	LOSS [training: 7.845960810275081 | validation: 9.989155940072013]
	TIME [epoch: 8.36 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.603892842409275		[learning rate: 0.0037312]
		[batch 20/20] avg loss: 8.509182953743359		[learning rate: 0.0037268]
	Learning Rate: 0.00372678
	LOSS [training: 9.056537898076318 | validation: 7.707971208075943]
	TIME [epoch: 8.37 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.922904406762544		[learning rate: 0.0037224]
		[batch 20/20] avg loss: 8.365968062064264		[learning rate: 0.003718]
	Learning Rate: 0.00371799
	LOSS [training: 8.144436234413403 | validation: 8.131619605350934]
	TIME [epoch: 8.35 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.132160650351619		[learning rate: 0.0037136]
		[batch 20/20] avg loss: 7.582072387476176		[learning rate: 0.0037092]
	Learning Rate: 0.00370922
	LOSS [training: 7.857116518913896 | validation: 7.751200096984074]
	TIME [epoch: 8.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.995500957526132		[learning rate: 0.0037048]
		[batch 20/20] avg loss: 4.499267571365966		[learning rate: 0.0037005]
	Learning Rate: 0.00370047
	LOSS [training: 5.247384264446048 | validation: 6.194068866474371]
	TIME [epoch: 8.35 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.577148812573708		[learning rate: 0.0036961]
		[batch 20/20] avg loss: 6.770295469303103		[learning rate: 0.0036917]
	Learning Rate: 0.00369174
	LOSS [training: 6.673722140938405 | validation: 9.27456572365275]
	TIME [epoch: 8.38 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.018882915063752		[learning rate: 0.0036874]
		[batch 20/20] avg loss: 8.782067360902582		[learning rate: 0.003683]
	Learning Rate: 0.00368303
	LOSS [training: 8.900475137983166 | validation: 9.310736208563917]
	TIME [epoch: 8.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.9439680105576		[learning rate: 0.0036787]
		[batch 20/20] avg loss: 6.786472846148248		[learning rate: 0.0036743]
	Learning Rate: 0.00367434
	LOSS [training: 7.3652204283529255 | validation: 7.173764139000012]
	TIME [epoch: 8.35 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.808028775639075		[learning rate: 0.00367]
		[batch 20/20] avg loss: 7.278366421629516		[learning rate: 0.0036657]
	Learning Rate: 0.00366567
	LOSS [training: 7.543197598634295 | validation: 7.40676610372144]
	TIME [epoch: 8.35 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.20922746366942		[learning rate: 0.0036613]
		[batch 20/20] avg loss: 7.277593124417406		[learning rate: 0.003657]
	Learning Rate: 0.00365703
	LOSS [training: 7.243410294043414 | validation: 8.885783545542752]
	TIME [epoch: 8.38 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.264615785713634		[learning rate: 0.0036527]
		[batch 20/20] avg loss: 8.153630596364165		[learning rate: 0.0036484]
	Learning Rate: 0.0036484
	LOSS [training: 8.2091231910389 | validation: 8.559827373325346]
	TIME [epoch: 8.36 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.659165355290336		[learning rate: 0.0036441]
		[batch 20/20] avg loss: 8.26363930462948		[learning rate: 0.0036398]
	Learning Rate: 0.00363979
	LOSS [training: 7.961402329959904 | validation: 8.981749518826458]
	TIME [epoch: 8.35 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.740664609885952		[learning rate: 0.0036355]
		[batch 20/20] avg loss: 8.033071054849929		[learning rate: 0.0036312]
	Learning Rate: 0.00363121
	LOSS [training: 8.386867832367944 | validation: 7.7725748587879]
	TIME [epoch: 8.35 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.359542155033198		[learning rate: 0.0036269]
		[batch 20/20] avg loss: 6.951922992720969		[learning rate: 0.0036226]
	Learning Rate: 0.00362264
	LOSS [training: 7.155732573877083 | validation: 6.830001777373433]
	TIME [epoch: 8.37 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.716010734925457		[learning rate: 0.0036184]
		[batch 20/20] avg loss: 6.747086844168216		[learning rate: 0.0036141]
	Learning Rate: 0.0036141
	LOSS [training: 6.731548789546837 | validation: 6.221276262182407]
	TIME [epoch: 8.36 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.571311474560909		[learning rate: 0.0036098]
		[batch 20/20] avg loss: 6.842230110994668		[learning rate: 0.0036056]
	Learning Rate: 0.00360557
	LOSS [training: 6.706770792777789 | validation: 6.689737803990711]
	TIME [epoch: 8.36 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.034321881352123		[learning rate: 0.0036013]
		[batch 20/20] avg loss: 7.35383987839479		[learning rate: 0.0035971]
	Learning Rate: 0.00359707
	LOSS [training: 7.194080879873458 | validation: 7.194413463589987]
	TIME [epoch: 8.35 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.447321455943168		[learning rate: 0.0035928]
		[batch 20/20] avg loss: 7.865246011480403		[learning rate: 0.0035886]
	Learning Rate: 0.00358858
	LOSS [training: 7.656283733711786 | validation: 8.710475075771084]
	TIME [epoch: 8.38 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.668639637319735		[learning rate: 0.0035843]
		[batch 20/20] avg loss: 8.8923134823812		[learning rate: 0.0035801]
	Learning Rate: 0.00358012
	LOSS [training: 8.780476559850468 | validation: 9.069354345623378]
	TIME [epoch: 8.35 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.922215337662204		[learning rate: 0.0035759]
		[batch 20/20] avg loss: 8.965181437618837		[learning rate: 0.0035717]
	Learning Rate: 0.00357167
	LOSS [training: 8.943698387640522 | validation: 8.882988164385145]
	TIME [epoch: 8.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.855451949609034		[learning rate: 0.0035675]
		[batch 20/20] avg loss: 8.851125870488445		[learning rate: 0.0035632]
	Learning Rate: 0.00356325
	LOSS [training: 8.85328891004874 | validation: 8.961688266315717]
	TIME [epoch: 8.34 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.718911250098829		[learning rate: 0.003559]
		[batch 20/20] avg loss: 7.924519040650014		[learning rate: 0.0035548]
	Learning Rate: 0.00355484
	LOSS [training: 8.321715145374421 | validation: 7.002750872760533]
	TIME [epoch: 8.38 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.752564054855439		[learning rate: 0.0035506]
		[batch 20/20] avg loss: 7.610314516686524		[learning rate: 0.0035465]
	Learning Rate: 0.00354646
	LOSS [training: 7.681439285770982 | validation: 7.340395751136344]
	TIME [epoch: 8.35 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.743595332004574		[learning rate: 0.0035423]
		[batch 20/20] avg loss: 7.791097929300539		[learning rate: 0.0035381]
	Learning Rate: 0.00353809
	LOSS [training: 7.767346630652557 | validation: 7.387319363165538]
	TIME [epoch: 8.35 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.585821582905386		[learning rate: 0.0035339]
		[batch 20/20] avg loss: 7.397459185497667		[learning rate: 0.0035297]
	Learning Rate: 0.00352975
	LOSS [training: 7.491640384201526 | validation: 6.93267425995805]
	TIME [epoch: 8.35 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.944854211859758		[learning rate: 0.0035256]
		[batch 20/20] avg loss: 6.412672906478432		[learning rate: 0.0035214]
	Learning Rate: 0.00352142
	LOSS [training: 6.678763559169096 | validation: 6.1748634896642995]
	TIME [epoch: 8.37 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.602131493303807		[learning rate: 0.0035173]
		[batch 20/20] avg loss: 7.070803220408088		[learning rate: 0.0035131]
	Learning Rate: 0.00351311
	LOSS [training: 6.836467356855948 | validation: 6.686528071818844]
	TIME [epoch: 8.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.323622198712738		[learning rate: 0.003509]
		[batch 20/20] avg loss: 7.34420949071378		[learning rate: 0.0035048]
	Learning Rate: 0.00350483
	LOSS [training: 7.333915844713259 | validation: 6.927507302348736]
	TIME [epoch: 8.35 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.403913428620781		[learning rate: 0.0035007]
		[batch 20/20] avg loss: 7.475819451744178		[learning rate: 0.0034966]
	Learning Rate: 0.00349656
	LOSS [training: 7.439866440182479 | validation: 7.526772379790371]
	TIME [epoch: 8.35 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.761976164077067		[learning rate: 0.0034924]
		[batch 20/20] avg loss: 7.961660701540095		[learning rate: 0.0034883]
	Learning Rate: 0.00348831
	LOSS [training: 7.861818432808579 | validation: 7.833881076515217]
	TIME [epoch: 8.38 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.776146140338123		[learning rate: 0.0034842]
		[batch 20/20] avg loss: 7.3322621814217115		[learning rate: 0.0034801]
	Learning Rate: 0.00348008
	LOSS [training: 7.554204160879918 | validation: 6.972810513166659]
	TIME [epoch: 8.35 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.369291999895788		[learning rate: 0.003476]
		[batch 20/20] avg loss: 7.631404868003995		[learning rate: 0.0034719]
	Learning Rate: 0.00347187
	LOSS [training: 7.500348433949893 | validation: 7.486027565782618]
	TIME [epoch: 8.35 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.459295239922089		[learning rate: 0.0034678]
		[batch 20/20] avg loss: 7.543294114764713		[learning rate: 0.0034637]
	Learning Rate: 0.00346369
	LOSS [training: 7.501294677343401 | validation: 7.187287166093416]
	TIME [epoch: 8.35 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.263896688786714		[learning rate: 0.0034596]
		[batch 20/20] avg loss: 7.8479413662732		[learning rate: 0.0034555]
	Learning Rate: 0.00345552
	LOSS [training: 7.555919027529957 | validation: 9.007036310253213]
	TIME [epoch: 8.38 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.710780978880525		[learning rate: 0.0034514]
		[batch 20/20] avg loss: 9.073982431274747		[learning rate: 0.0034474]
	Learning Rate: 0.00344736
	LOSS [training: 8.892381705077636 | validation: 9.488647666033437]
	TIME [epoch: 8.35 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.348218700593545		[learning rate: 0.0034433]
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.05
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.025
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.0125
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.00625
