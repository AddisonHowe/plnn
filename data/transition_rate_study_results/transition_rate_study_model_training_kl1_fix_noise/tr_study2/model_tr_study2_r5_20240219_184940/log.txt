Args:
Namespace(name='model_tr_study2', outdir='out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5', training_data='data/transition_rate_studies/tr_study2/tr_study2_training/r5', validation_data='data/transition_rate_studies/tr_study2/tr_study2_validation/r5', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.1, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 511104757

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.554737107671886		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.095333910838273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.825035509255078 | validation: 6.274950414237891]
	TIME [epoch: 78.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.436577542705752		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.860537136857824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.148557339781787 | validation: 4.894860334218454]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.435134713771819		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.176411490338215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.305773102055017 | validation: 3.359521821419876]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1077014383999324		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.577448176540284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.842574807470108 | validation: 2.644562738083308]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4943233026610274		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.603361278327917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5488422904944725 | validation: 1.8110756646868564]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.857998305976831		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0456461959074943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4518222509421626 | validation: 1.8737270578951848]
	TIME [epoch: 8.3 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9570550406197327		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.953460335563254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9552576880914934 | validation: 1.6746673286893183]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6206900751659004		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.582781938773989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6017360069699447 | validation: 2.0741112749364135]
	TIME [epoch: 8.28 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5162730998952907		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4159197962206072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.466096448057949 | validation: 2.131514377559359]
	TIME [epoch: 8.31 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.31215128302068		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2340476887971588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2730994859089195 | validation: 1.2739104041645302]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1142963239556791		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.374241871327221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.24426909764145 | validation: 1.0427100226649504]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2792356518152714		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0900524199260841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.184644035870678 | validation: 0.9637434755658512]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0402920551147945		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1279501677071821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0841211114109879 | validation: 0.7830529666561459]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8802865932123393		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8885045975263266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.884395595369333 | validation: 1.5874574928120646]
	TIME [epoch: 8.32 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0173405218551204		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7196696773251542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8685050995901371 | validation: 0.6236661737394364]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7811084552484446		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1401156581689933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9606120567087186 | validation: 0.7741491379853003]
	TIME [epoch: 8.29 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6477690035992103		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6850519837862805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6664104936927455 | validation: 0.5040155030793058]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7830732168512409		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5646016387606712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.673837427805956 | validation: 0.6813286633329687]
	TIME [epoch: 8.33 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8181047229036972		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5771509521853262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6976278375445119 | validation: 0.6793164636025346]
	TIME [epoch: 8.32 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6211932602398953		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7897745503541425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7054839052970188 | validation: 0.7825842999612085]
	TIME [epoch: 8.29 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8035300601254335		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7012417635511197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7523859118382765 | validation: 0.5845707364862275]
	TIME [epoch: 8.29 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7336073860263894		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6601854069412989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6968963964838442 | validation: 0.5239210756333446]
	TIME [epoch: 8.32 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.702492117586712		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7261205803035752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7143063489451437 | validation: 0.5474898570629564]
	TIME [epoch: 8.32 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6625449825959938		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7967843202039552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7296646513999744 | validation: 0.5173679101682808]
	TIME [epoch: 8.29 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6418581323348962		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6954742498242188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6686661910795576 | validation: 0.9317665194512916]
	TIME [epoch: 8.29 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6540788701721698		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6304226467452827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6422507584587264 | validation: 0.5225163109834671]
	TIME [epoch: 8.32 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7295655394326174		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7683298017042508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7489476705684341 | validation: 0.7187546070253913]
	TIME [epoch: 8.33 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6668149564010136		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7131156191485536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6899652877747836 | validation: 0.7125246084141121]
	TIME [epoch: 8.29 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7410780020213508		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7892165831198307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7651472925705908 | validation: 0.612604719969754]
	TIME [epoch: 8.29 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6043930665463488		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.75726901661719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6808310415817693 | validation: 0.8367963740352694]
	TIME [epoch: 8.29 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.653502208212187		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6197820978099704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6366421530110786 | validation: 0.8949025598032756]
	TIME [epoch: 8.36 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7387615028039651		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7117376686712091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7252495857375874 | validation: 0.9461892045775349]
	TIME [epoch: 8.29 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.741636404500967		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.701361755891826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7214990801963965 | validation: 0.6564676968981993]
	TIME [epoch: 8.3 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6842916535496595		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6711385893813933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6777151214655265 | validation: 0.5456816420026229]
	TIME [epoch: 8.3 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7195313384261619		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7439374288908679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7317343836585148 | validation: 0.599872709008444]
	TIME [epoch: 8.34 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.629992780123447		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6835767932519266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6567847866876867 | validation: 0.7656619210038782]
	TIME [epoch: 8.3 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6494401929070669		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6386558013599567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6440479971335117 | validation: 0.7537769951507809]
	TIME [epoch: 8.3 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6880923297150148		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7399602197017427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7140262747083788 | validation: 0.5807292742589942]
	TIME [epoch: 8.3 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5802822246669055		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7670930252892095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6736876249780577 | validation: 0.7061255543176357]
	TIME [epoch: 8.34 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6807316029566952		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6097487330852317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6452401680209634 | validation: 0.5105701867019419]
	TIME [epoch: 8.31 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.084092173794102		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5833144616119481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8337033177030252 | validation: 0.9587157249444693]
	TIME [epoch: 8.29 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6642528433494935		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6724639503354818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6683583968424877 | validation: 0.5869135799400038]
	TIME [epoch: 8.31 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6874109002759288		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7218470472697401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7046289737728345 | validation: 0.5861082260749554]
	TIME [epoch: 8.31 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6404015423621808		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7661029004556215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7032522214089013 | validation: 0.7210849823860973]
	TIME [epoch: 8.32 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.61388375411433		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6821824561214367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6480331051178834 | validation: 0.8404409479785679]
	TIME [epoch: 8.29 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5684377970363016		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6760050627274862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6222214298818938 | validation: 0.8359226161998472]
	TIME [epoch: 8.3 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7633195186570557		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6308595250236043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.69708952184033 | validation: 0.6961248623895389]
	TIME [epoch: 8.32 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6556559846664639		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5711227091889041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6133893469276839 | validation: 0.7128044169666299]
	TIME [epoch: 8.31 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6383301516272708		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7410951829416581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6897126672844643 | validation: 0.6440394779981151]
	TIME [epoch: 8.3 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6088232960527056		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6700939280793893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6394586120660474 | validation: 0.8264825196826112]
	TIME [epoch: 8.3 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6761797977802531		[learning rate: 0.0099894]
		[batch 20/20] avg loss: 0.6567268203121819		[learning rate: 0.0099776]
	Learning Rate: 0.00997759
	LOSS [training: 0.6664533090462174 | validation: 0.5732030216042614]
	TIME [epoch: 8.31 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6586816620973754		[learning rate: 0.0099658]
		[batch 20/20] avg loss: 0.6534583679803962		[learning rate: 0.0099541]
	Learning Rate: 0.00995405
	LOSS [training: 0.6560700150388861 | validation: 0.5761853280948745]
	TIME [epoch: 8.31 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.631260329096663		[learning rate: 0.0099423]
		[batch 20/20] avg loss: 0.5764954520576513		[learning rate: 0.0099306]
	Learning Rate: 0.00993057
	LOSS [training: 0.6038778905771571 | validation: 0.549018592796626]
	TIME [epoch: 8.3 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5456328161591882		[learning rate: 0.0099189]
		[batch 20/20] avg loss: 0.8220741131318		[learning rate: 0.0099071]
	Learning Rate: 0.00990715
	LOSS [training: 0.6838534646454941 | validation: 0.7046173019449611]
	TIME [epoch: 8.31 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5653309330375996		[learning rate: 0.0098955]
		[batch 20/20] avg loss: 0.5764333289278876		[learning rate: 0.0098838]
	Learning Rate: 0.00988378
	LOSS [training: 0.5708821309827437 | validation: 0.579944124805901]
	TIME [epoch: 8.31 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7156351078769921		[learning rate: 0.0098721]
		[batch 20/20] avg loss: 0.6704294595372817		[learning rate: 0.0098605]
	Learning Rate: 0.00986047
	LOSS [training: 0.6930322837071369 | validation: 0.4791697907047879]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6549687225198848		[learning rate: 0.0098488]
		[batch 20/20] avg loss: 0.5858498439167709		[learning rate: 0.0098372]
	Learning Rate: 0.00983721
	LOSS [training: 0.620409283218328 | validation: 0.6765039553122955]
	TIME [epoch: 8.31 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6828053770895894		[learning rate: 0.0098256]
		[batch 20/20] avg loss: 0.6365246288784814		[learning rate: 0.009814]
	Learning Rate: 0.009814
	LOSS [training: 0.6596650029840353 | validation: 0.3749119982529639]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5332702991341326		[learning rate: 0.0098024]
		[batch 20/20] avg loss: 0.6687531389420667		[learning rate: 0.0097909]
	Learning Rate: 0.00979085
	LOSS [training: 0.6010117190380997 | validation: 0.45040737042246015]
	TIME [epoch: 8.3 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5138063749840075		[learning rate: 0.0097793]
		[batch 20/20] avg loss: 0.7166755625489344		[learning rate: 0.0097678]
	Learning Rate: 0.00976776
	LOSS [training: 0.6152409687664709 | validation: 0.6080507995013702]
	TIME [epoch: 8.28 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5836717486394676		[learning rate: 0.0097562]
		[batch 20/20] avg loss: 0.8024444575338198		[learning rate: 0.0097447]
	Learning Rate: 0.00974472
	LOSS [training: 0.6930581030866436 | validation: 0.4989875270234402]
	TIME [epoch: 8.31 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7385390643006622		[learning rate: 0.0097332]
		[batch 20/20] avg loss: 0.49317263584870225		[learning rate: 0.0097217]
	Learning Rate: 0.00972173
	LOSS [training: 0.6158558500746821 | validation: 1.001686465772756]
	TIME [epoch: 8.3 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6460096683089609		[learning rate: 0.0097103]
		[batch 20/20] avg loss: 0.5300236122522044		[learning rate: 0.0096988]
	Learning Rate: 0.0096988
	LOSS [training: 0.5880166402805826 | validation: 0.4491851927673546]
	TIME [epoch: 8.31 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6022690898664719		[learning rate: 0.0096874]
		[batch 20/20] avg loss: 0.6080340666907824		[learning rate: 0.0096759]
	Learning Rate: 0.00967592
	LOSS [training: 0.6051515782786272 | validation: 0.4110730369434603]
	TIME [epoch: 8.28 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6233093775880051		[learning rate: 0.0096645]
		[batch 20/20] avg loss: 0.5703652379929681		[learning rate: 0.0096531]
	Learning Rate: 0.0096531
	LOSS [training: 0.5968373077904865 | validation: 0.6219543127860041]
	TIME [epoch: 8.3 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5464484359515266		[learning rate: 0.0096417]
		[batch 20/20] avg loss: 0.6674808481273475		[learning rate: 0.0096303]
	Learning Rate: 0.00963033
	LOSS [training: 0.6069646420394372 | validation: 0.671356196255795]
	TIME [epoch: 8.31 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5072005127682437		[learning rate: 0.009619]
		[batch 20/20] avg loss: 0.6146237455276898		[learning rate: 0.0096076]
	Learning Rate: 0.00960761
	LOSS [training: 0.5609121291479668 | validation: 0.4765948110791546]
	TIME [epoch: 8.3 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5694916978289586		[learning rate: 0.0095963]
		[batch 20/20] avg loss: 0.7575383169152131		[learning rate: 0.0095849]
	Learning Rate: 0.00958495
	LOSS [training: 0.6635150073720858 | validation: 0.6547212649208158]
	TIME [epoch: 8.28 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5553195812314162		[learning rate: 0.0095736]
		[batch 20/20] avg loss: 0.5252842601467813		[learning rate: 0.0095623]
	Learning Rate: 0.00956234
	LOSS [training: 0.5403019206890989 | validation: 0.9296232969397573]
	TIME [epoch: 8.28 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5843142275451773		[learning rate: 0.0095511]
		[batch 20/20] avg loss: 0.6792805056771412		[learning rate: 0.0095398]
	Learning Rate: 0.00953978
	LOSS [training: 0.631797366611159 | validation: 0.47897691124626784]
	TIME [epoch: 8.34 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8073488502454357		[learning rate: 0.0095285]
		[batch 20/20] avg loss: 0.6280367208921629		[learning rate: 0.0095173]
	Learning Rate: 0.00951728
	LOSS [training: 0.7176927855687991 | validation: 0.6812079850204715]
	TIME [epoch: 8.29 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5479235020157656		[learning rate: 0.009506]
		[batch 20/20] avg loss: 0.486563567877596		[learning rate: 0.0094948]
	Learning Rate: 0.00949483
	LOSS [training: 0.5172435349466806 | validation: 0.41750687090752653]
	TIME [epoch: 8.28 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5332716365081296		[learning rate: 0.0094836]
		[batch 20/20] avg loss: 0.6295992919877753		[learning rate: 0.0094724]
	Learning Rate: 0.00947243
	LOSS [training: 0.5814354642479524 | validation: 0.5960711251398014]
	TIME [epoch: 8.29 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47342015413255184		[learning rate: 0.0094613]
		[batch 20/20] avg loss: 0.5855171007605466		[learning rate: 0.0094501]
	Learning Rate: 0.00945009
	LOSS [training: 0.5294686274465492 | validation: 0.5460828620556939]
	TIME [epoch: 8.32 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5000649439585891		[learning rate: 0.0094389]
		[batch 20/20] avg loss: 0.5940277034233727		[learning rate: 0.0094278]
	Learning Rate: 0.0094278
	LOSS [training: 0.5470463236909809 | validation: 0.8100155225229625]
	TIME [epoch: 8.29 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5782155122106891		[learning rate: 0.0094167]
		[batch 20/20] avg loss: 0.5018702563210504		[learning rate: 0.0094056]
	Learning Rate: 0.00940556
	LOSS [training: 0.5400428842658699 | validation: 0.3562244473757644]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5529207505640337		[learning rate: 0.0093945]
		[batch 20/20] avg loss: 0.5920042626319957		[learning rate: 0.0093834]
	Learning Rate: 0.00938337
	LOSS [training: 0.5724625065980147 | validation: 0.3866748156889896]
	TIME [epoch: 8.28 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6053816765564315		[learning rate: 0.0093723]
		[batch 20/20] avg loss: 0.5460751441003131		[learning rate: 0.0093612]
	Learning Rate: 0.00936124
	LOSS [training: 0.5757284103283723 | validation: 0.7164488898049983]
	TIME [epoch: 8.32 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5948017822093952		[learning rate: 0.0093502]
		[batch 20/20] avg loss: 0.5739264613805092		[learning rate: 0.0093392]
	Learning Rate: 0.00933916
	LOSS [training: 0.5843641217949521 | validation: 0.6219109655728584]
	TIME [epoch: 8.3 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6303795992275681		[learning rate: 0.0093281]
		[batch 20/20] avg loss: 0.49136456990434396		[learning rate: 0.0093171]
	Learning Rate: 0.00931713
	LOSS [training: 0.5608720845659562 | validation: 0.8323968941546036]
	TIME [epoch: 8.27 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5320191065341426		[learning rate: 0.0093061]
		[batch 20/20] avg loss: 0.5895805534596362		[learning rate: 0.0092951]
	Learning Rate: 0.00929515
	LOSS [training: 0.5607998299968895 | validation: 0.736309662136289]
	TIME [epoch: 8.29 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4776479384374368		[learning rate: 0.0092842]
		[batch 20/20] avg loss: 0.5567065789990238		[learning rate: 0.0092732]
	Learning Rate: 0.00927322
	LOSS [training: 0.5171772587182303 | validation: 0.5058530763615674]
	TIME [epoch: 8.32 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48178912161214543		[learning rate: 0.0092623]
		[batch 20/20] avg loss: 0.49455889733987907		[learning rate: 0.0092514]
	Learning Rate: 0.00925135
	LOSS [training: 0.4881740094760123 | validation: 0.4627576486862061]
	TIME [epoch: 8.31 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5064747227139593		[learning rate: 0.0092404]
		[batch 20/20] avg loss: 0.5514146881543849		[learning rate: 0.0092295]
	Learning Rate: 0.00922953
	LOSS [training: 0.5289447054341722 | validation: 0.37793996036732086]
	TIME [epoch: 8.28 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5411561715817526		[learning rate: 0.0092186]
		[batch 20/20] avg loss: 0.49490413081416096		[learning rate: 0.0092078]
	Learning Rate: 0.00920776
	LOSS [training: 0.5180301511979567 | validation: 0.49817527897248237]
	TIME [epoch: 8.29 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5539682305597611		[learning rate: 0.0091969]
		[batch 20/20] avg loss: 0.4488609448412313		[learning rate: 0.009186]
	Learning Rate: 0.00918604
	LOSS [training: 0.5014145877004961 | validation: 0.487637382681286]
	TIME [epoch: 8.32 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5383915043656372		[learning rate: 0.0091752]
		[batch 20/20] avg loss: 0.4967466682920073		[learning rate: 0.0091644]
	Learning Rate: 0.00916437
	LOSS [training: 0.5175690863288223 | validation: 0.47254693926342684]
	TIME [epoch: 8.31 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46901398530814575		[learning rate: 0.0091536]
		[batch 20/20] avg loss: 0.4547116765186182		[learning rate: 0.0091428]
	Learning Rate: 0.00914275
	LOSS [training: 0.4618628309133819 | validation: 0.3043111950824257]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4660973121884863		[learning rate: 0.009132]
		[batch 20/20] avg loss: 0.5289988858237807		[learning rate: 0.0091212]
	Learning Rate: 0.00912119
	LOSS [training: 0.4975480990061336 | validation: 0.5731580032266355]
	TIME [epoch: 8.3 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5544199153349588		[learning rate: 0.0091104]
		[batch 20/20] avg loss: 0.4526049329764318		[learning rate: 0.0090997]
	Learning Rate: 0.00909967
	LOSS [training: 0.5035124241556953 | validation: 0.42428856948868365]
	TIME [epoch: 8.31 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4762060089142143		[learning rate: 0.0090889]
		[batch 20/20] avg loss: 0.48766553819603187		[learning rate: 0.0090782]
	Learning Rate: 0.0090782
	LOSS [training: 0.48193577355512307 | validation: 0.45826891369811407]
	TIME [epoch: 8.28 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4761797953814654		[learning rate: 0.0090675]
		[batch 20/20] avg loss: 0.551425761967548		[learning rate: 0.0090568]
	Learning Rate: 0.00905679
	LOSS [training: 0.5138027786745066 | validation: 0.4645703199792206]
	TIME [epoch: 8.3 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6042741078470499		[learning rate: 0.0090461]
		[batch 20/20] avg loss: 0.42376185138014366		[learning rate: 0.0090354]
	Learning Rate: 0.00903543
	LOSS [training: 0.514017979613597 | validation: 0.6393018956820198]
	TIME [epoch: 8.28 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44028962157296475		[learning rate: 0.0090248]
		[batch 20/20] avg loss: 0.41657219842077053		[learning rate: 0.0090141]
	Learning Rate: 0.00901411
	LOSS [training: 0.4284309099968676 | validation: 0.41786777543284326]
	TIME [epoch: 8.31 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49193755500758696		[learning rate: 0.0090035]
		[batch 20/20] avg loss: 0.4667959093446992		[learning rate: 0.0089929]
	Learning Rate: 0.00899285
	LOSS [training: 0.4793667321761431 | validation: 0.8533510934293764]
	TIME [epoch: 8.28 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45647230781884646		[learning rate: 0.0089822]
		[batch 20/20] avg loss: 0.49464778972673396		[learning rate: 0.0089716]
	Learning Rate: 0.00897164
	LOSS [training: 0.4755600487727901 | validation: 0.4696282625965348]
	TIME [epoch: 8.31 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4939150387611928		[learning rate: 0.0089611]
		[batch 20/20] avg loss: 0.5090641716290674		[learning rate: 0.0089505]
	Learning Rate: 0.00895048
	LOSS [training: 0.5014896051951301 | validation: 0.48538502194493904]
	TIME [epoch: 8.3 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5765818160286743		[learning rate: 0.0089399]
		[batch 20/20] avg loss: 0.5521420830626721		[learning rate: 0.0089294]
	Learning Rate: 0.00892936
	LOSS [training: 0.5643619495456733 | validation: 0.37984959666559437]
	TIME [epoch: 8.31 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4560928585469524		[learning rate: 0.0089188]
		[batch 20/20] avg loss: 0.44402114472032767		[learning rate: 0.0089083]
	Learning Rate: 0.0089083
	LOSS [training: 0.45005700163364004 | validation: 0.7054041214641715]
	TIME [epoch: 8.28 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45805031439076654		[learning rate: 0.0088978]
		[batch 20/20] avg loss: 0.5285232354824991		[learning rate: 0.0088873]
	Learning Rate: 0.00888729
	LOSS [training: 0.4932867749366328 | validation: 0.47242521439777885]
	TIME [epoch: 8.29 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41627829391770954		[learning rate: 0.0088768]
		[batch 20/20] avg loss: 0.4680876180523527		[learning rate: 0.0088663]
	Learning Rate: 0.00886632
	LOSS [training: 0.44218295598503116 | validation: 0.42386426528914006]
	TIME [epoch: 8.3 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4203580215033528		[learning rate: 0.0088559]
		[batch 20/20] avg loss: 1.0943817300762295		[learning rate: 0.0088454]
	Learning Rate: 0.00884541
	LOSS [training: 0.7573698757897912 | validation: 0.4086412497353437]
	TIME [epoch: 8.3 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37792226379798644		[learning rate: 0.008835]
		[batch 20/20] avg loss: 0.5373633588732324		[learning rate: 0.0088245]
	Learning Rate: 0.00882454
	LOSS [training: 0.4576428113356094 | validation: 0.4219975309179581]
	TIME [epoch: 8.28 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.363023182106397		[learning rate: 0.0088141]
		[batch 20/20] avg loss: 0.461010608248134		[learning rate: 0.0088037]
	Learning Rate: 0.00880373
	LOSS [training: 0.4120168951772655 | validation: 0.39750559431760313]
	TIME [epoch: 8.28 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43809544456083405		[learning rate: 0.0087933]
		[batch 20/20] avg loss: 0.4841713251283292		[learning rate: 0.008783]
	Learning Rate: 0.00878296
	LOSS [training: 0.46113338484458166 | validation: 1.2730672169335566]
	TIME [epoch: 8.32 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5588852382976309		[learning rate: 0.0087726]
		[batch 20/20] avg loss: 0.3703794164475902		[learning rate: 0.0087622]
	Learning Rate: 0.00876225
	LOSS [training: 0.46463232737261057 | validation: 0.6492969788383973]
	TIME [epoch: 8.3 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42623568744469187		[learning rate: 0.0087519]
		[batch 20/20] avg loss: 0.5204677387252342		[learning rate: 0.0087416]
	Learning Rate: 0.00874158
	LOSS [training: 0.47335171308496315 | validation: 0.3777259048411602]
	TIME [epoch: 8.28 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4013760617314811		[learning rate: 0.0087313]
		[batch 20/20] avg loss: 0.37908630088780537		[learning rate: 0.008721]
	Learning Rate: 0.00872096
	LOSS [training: 0.3902311813096432 | validation: 0.476841251255341]
	TIME [epoch: 8.29 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.524035923830694		[learning rate: 0.0087107]
		[batch 20/20] avg loss: 0.4312783405274752		[learning rate: 0.0087004]
	Learning Rate: 0.00870038
	LOSS [training: 0.4776571321790846 | validation: 0.3084248849929647]
	TIME [epoch: 8.32 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42222293455825133		[learning rate: 0.0086901]
		[batch 20/20] avg loss: 0.46250866267203306		[learning rate: 0.0086799]
	Learning Rate: 0.00867986
	LOSS [training: 0.44236579861514225 | validation: 0.34855902984466103]
	TIME [epoch: 8.3 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3870733417115396		[learning rate: 0.0086696]
		[batch 20/20] avg loss: 0.4583403698317391		[learning rate: 0.0086594]
	Learning Rate: 0.00865939
	LOSS [training: 0.42270685577163947 | validation: 0.29188195389448374]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4441816628409554		[learning rate: 0.0086492]
		[batch 20/20] avg loss: 0.41015339804842793		[learning rate: 0.008639]
	Learning Rate: 0.00863896
	LOSS [training: 0.4271675304446917 | validation: 0.45169642170402285]
	TIME [epoch: 8.27 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3879572373726579		[learning rate: 0.0086288]
		[batch 20/20] avg loss: 0.4441957919989548		[learning rate: 0.0086186]
	Learning Rate: 0.00861858
	LOSS [training: 0.4160765146858063 | validation: 0.37978767477864245]
	TIME [epoch: 8.33 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35226742106257247		[learning rate: 0.0086084]
		[batch 20/20] avg loss: 0.47496711191578617		[learning rate: 0.0085983]
	Learning Rate: 0.00859825
	LOSS [training: 0.41361726648917935 | validation: 0.3516834305449884]
	TIME [epoch: 8.3 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3581141890074641		[learning rate: 0.0085881]
		[batch 20/20] avg loss: 0.49926183678743763		[learning rate: 0.008578]
	Learning Rate: 0.00857797
	LOSS [training: 0.42868801289745095 | validation: 0.5572151919235979]
	TIME [epoch: 8.26 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37086339233458004		[learning rate: 0.0085678]
		[batch 20/20] avg loss: 0.4802400531838372		[learning rate: 0.0085577]
	Learning Rate: 0.00855774
	LOSS [training: 0.4255517227592088 | validation: 0.39373572438002413]
	TIME [epoch: 8.26 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44260409016073465		[learning rate: 0.0085476]
		[batch 20/20] avg loss: 0.40024447900486626		[learning rate: 0.0085376]
	Learning Rate: 0.00853755
	LOSS [training: 0.42142428458280035 | validation: 0.30210802437343276]
	TIME [epoch: 8.29 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43693721834949706		[learning rate: 0.0085275]
		[batch 20/20] avg loss: 0.42157319521146713		[learning rate: 0.0085174]
	Learning Rate: 0.00851741
	LOSS [training: 0.429255206780482 | validation: 0.30008234515803883]
	TIME [epoch: 8.32 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4136679743449652		[learning rate: 0.0085074]
		[batch 20/20] avg loss: 0.8043645787240598		[learning rate: 0.0084973]
	Learning Rate: 0.00849732
	LOSS [training: 0.6090162765345124 | validation: 0.4170551643229464]
	TIME [epoch: 8.27 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39090198992419734		[learning rate: 0.0084873]
		[batch 20/20] avg loss: 0.4665875310851339		[learning rate: 0.0084773]
	Learning Rate: 0.00847728
	LOSS [training: 0.4287447605046656 | validation: 0.3066218222298882]
	TIME [epoch: 8.26 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43562367749312025		[learning rate: 0.0084673]
		[batch 20/20] avg loss: 0.4291497716558174		[learning rate: 0.0084573]
	Learning Rate: 0.00845728
	LOSS [training: 0.4323867245744689 | validation: 0.7370803021416632]
	TIME [epoch: 8.29 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5264733637753942		[learning rate: 0.0084473]
		[batch 20/20] avg loss: 0.43306936162990317		[learning rate: 0.0084373]
	Learning Rate: 0.00843733
	LOSS [training: 0.47977136270264864 | validation: 0.50892554361662]
	TIME [epoch: 8.31 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49556394459952663		[learning rate: 0.0084274]
		[batch 20/20] avg loss: 0.3822841099712763		[learning rate: 0.0084174]
	Learning Rate: 0.00841743
	LOSS [training: 0.4389240272854014 | validation: 0.9379403864856447]
	TIME [epoch: 8.28 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.505719473600949		[learning rate: 0.0084075]
		[batch 20/20] avg loss: 0.5144035532146372		[learning rate: 0.0083976]
	Learning Rate: 0.00839757
	LOSS [training: 0.5100615134077932 | validation: 0.6442366494572301]
	TIME [epoch: 8.27 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4715125682689664		[learning rate: 0.0083877]
		[batch 20/20] avg loss: 0.4970925027958174		[learning rate: 0.0083778]
	Learning Rate: 0.00837777
	LOSS [training: 0.48430253553239194 | validation: 0.47453506538869433]
	TIME [epoch: 8.3 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.410851271060106		[learning rate: 0.0083679]
		[batch 20/20] avg loss: 0.46638021058881574		[learning rate: 0.008358]
	Learning Rate: 0.008358
	LOSS [training: 0.43861574082446086 | validation: 0.5489028340951114]
	TIME [epoch: 8.3 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42120746612409643		[learning rate: 0.0083481]
		[batch 20/20] avg loss: 0.4253142014522465		[learning rate: 0.0083383]
	Learning Rate: 0.00833829
	LOSS [training: 0.42326083378817153 | validation: 0.38076702030696874]
	TIME [epoch: 8.3 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5011699477325385		[learning rate: 0.0083284]
		[batch 20/20] avg loss: 0.403762638301706		[learning rate: 0.0083186]
	Learning Rate: 0.00831862
	LOSS [training: 0.45246629301712227 | validation: 0.29102765328176416]
	TIME [epoch: 8.27 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4350127329452743		[learning rate: 0.0083088]
		[batch 20/20] avg loss: 0.3993112665038064		[learning rate: 0.008299]
	Learning Rate: 0.008299
	LOSS [training: 0.4171619997245403 | validation: 0.38832243257858917]
	TIME [epoch: 8.32 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43770354214856483		[learning rate: 0.0082892]
		[batch 20/20] avg loss: 0.48743566464348975		[learning rate: 0.0082794]
	Learning Rate: 0.00827942
	LOSS [training: 0.46256960339602726 | validation: 0.46080719246542673]
	TIME [epoch: 8.3 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4584319000106204		[learning rate: 0.0082697]
		[batch 20/20] avg loss: 0.3859038001940724		[learning rate: 0.0082599]
	Learning Rate: 0.00825989
	LOSS [training: 0.4221678501023464 | validation: 0.2947775991385763]
	TIME [epoch: 8.31 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3894202306956229		[learning rate: 0.0082501]
		[batch 20/20] avg loss: 0.4456814936294746		[learning rate: 0.0082404]
	Learning Rate: 0.00824041
	LOSS [training: 0.4175508621625488 | validation: 0.5050127994909561]
	TIME [epoch: 8.28 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7168739170538223		[learning rate: 0.0082307]
		[batch 20/20] avg loss: 0.43994921295939166		[learning rate: 0.008221]
	Learning Rate: 0.00822097
	LOSS [training: 0.578411565006607 | validation: 0.3969813921087937]
	TIME [epoch: 8.31 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3893335003770029		[learning rate: 0.0082113]
		[batch 20/20] avg loss: 0.36006733544347613		[learning rate: 0.0082016]
	Learning Rate: 0.00820158
	LOSS [training: 0.37470041791023945 | validation: 0.47274557784434895]
	TIME [epoch: 8.3 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3266872690086039		[learning rate: 0.0081919]
		[batch 20/20] avg loss: 0.5961303050105633		[learning rate: 0.0081822]
	Learning Rate: 0.00818223
	LOSS [training: 0.46140878700958377 | validation: 0.28901285017097833]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3821721065673454		[learning rate: 0.0081726]
		[batch 20/20] avg loss: 0.4435755161290246		[learning rate: 0.0081629]
	Learning Rate: 0.00816293
	LOSS [training: 0.412873811348185 | validation: 0.26640331722491045]
	TIME [epoch: 8.27 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36680380069763274		[learning rate: 0.0081533]
		[batch 20/20] avg loss: 0.3729752873803388		[learning rate: 0.0081437]
	Learning Rate: 0.00814368
	LOSS [training: 0.36988954403898583 | validation: 0.25044226510328066]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3588069603438212		[learning rate: 0.0081341]
		[batch 20/20] avg loss: 0.5813328653870261		[learning rate: 0.0081245]
	Learning Rate: 0.00812447
	LOSS [training: 0.4700699128654236 | validation: 0.47513345501949844]
	TIME [epoch: 8.31 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43210867819443194		[learning rate: 0.0081149]
		[batch 20/20] avg loss: 0.4412573202634877		[learning rate: 0.0081053]
	Learning Rate: 0.0081053
	LOSS [training: 0.4366829992289598 | validation: 0.46110181642884723]
	TIME [epoch: 8.32 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34508984574117985		[learning rate: 0.0080957]
		[batch 20/20] avg loss: 0.4379050917660489		[learning rate: 0.0080862]
	Learning Rate: 0.00808618
	LOSS [training: 0.39149746875361435 | validation: 0.27867296647658674]
	TIME [epoch: 8.33 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6287559563837563		[learning rate: 0.0080766]
		[batch 20/20] avg loss: 0.4273927824902016		[learning rate: 0.0080671]
	Learning Rate: 0.00806711
	LOSS [training: 0.5280743694369787 | validation: 0.3939783113317501]
	TIME [epoch: 8.34 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45602111845831494		[learning rate: 0.0080576]
		[batch 20/20] avg loss: 0.377015577264223		[learning rate: 0.0080481]
	Learning Rate: 0.00804808
	LOSS [training: 0.41651834786126896 | validation: 0.4648367522941627]
	TIME [epoch: 8.3 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5052683631088698		[learning rate: 0.0080386]
		[batch 20/20] avg loss: 0.48364126167175925		[learning rate: 0.0080291]
	Learning Rate: 0.0080291
	LOSS [training: 0.4944548123903146 | validation: 0.2896562887487139]
	TIME [epoch: 8.3 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42311306775364643		[learning rate: 0.0080196]
		[batch 20/20] avg loss: 0.3985429804107065		[learning rate: 0.0080102]
	Learning Rate: 0.00801016
	LOSS [training: 0.4108280240821764 | validation: 0.46716528504142013]
	TIME [epoch: 8.33 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.532389921836629		[learning rate: 0.0080007]
		[batch 20/20] avg loss: 0.3608517709229389		[learning rate: 0.0079913]
	Learning Rate: 0.00799126
	LOSS [training: 0.446620846379784 | validation: 0.29201918627950874]
	TIME [epoch: 8.34 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5198227875882884		[learning rate: 0.0079818]
		[batch 20/20] avg loss: 0.5096657215440372		[learning rate: 0.0079724]
	Learning Rate: 0.00797241
	LOSS [training: 0.5147442545661628 | validation: 0.4150593194658035]
	TIME [epoch: 8.3 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3659577441268929		[learning rate: 0.007963]
		[batch 20/20] avg loss: 0.4320014042074036		[learning rate: 0.0079536]
	Learning Rate: 0.00795361
	LOSS [training: 0.3989795741671483 | validation: 1.1857731345871634]
	TIME [epoch: 8.29 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8712088447872384		[learning rate: 0.0079442]
		[batch 20/20] avg loss: 0.3411142533922048		[learning rate: 0.0079348]
	Learning Rate: 0.00793485
	LOSS [training: 0.6061615490897215 | validation: 0.32049699129019005]
	TIME [epoch: 8.32 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41019275830873553		[learning rate: 0.0079255]
		[batch 20/20] avg loss: 0.36143148625241894		[learning rate: 0.0079161]
	Learning Rate: 0.00791613
	LOSS [training: 0.38581212228057726 | validation: 0.22520181694842595]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40145810127573833		[learning rate: 0.0079068]
		[batch 20/20] avg loss: 0.395024528377104		[learning rate: 0.0078975]
	Learning Rate: 0.00789746
	LOSS [training: 0.3982413148264211 | validation: 0.2677366162768121]
	TIME [epoch: 8.29 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38399424422766704		[learning rate: 0.0078881]
		[batch 20/20] avg loss: 0.3057003616384271		[learning rate: 0.0078788]
	Learning Rate: 0.00787883
	LOSS [training: 0.344847302933047 | validation: 0.46149508401128436]
	TIME [epoch: 8.3 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.439407249811831		[learning rate: 0.0078695]
		[batch 20/20] avg loss: 0.4339181638581331		[learning rate: 0.0078602]
	Learning Rate: 0.00786024
	LOSS [training: 0.43666270683498204 | validation: 0.31469646622380276]
	TIME [epoch: 8.33 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.516460006587092		[learning rate: 0.007851]
		[batch 20/20] avg loss: 0.46217491608779165		[learning rate: 0.0078417]
	Learning Rate: 0.0078417
	LOSS [training: 0.4893174613374419 | validation: 0.3990464764870572]
	TIME [epoch: 8.34 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32814693725156946		[learning rate: 0.0078324]
		[batch 20/20] avg loss: 0.4497651366059232		[learning rate: 0.0078232]
	Learning Rate: 0.0078232
	LOSS [training: 0.3889560369287463 | validation: 0.42971260603995315]
	TIME [epoch: 8.3 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32196670041557895		[learning rate: 0.007814]
		[batch 20/20] avg loss: 0.36945356501020116		[learning rate: 0.0078047]
	Learning Rate: 0.00780475
	LOSS [training: 0.34571013271289 | validation: 0.2056757240234308]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.459669454444788		[learning rate: 0.0077955]
		[batch 20/20] avg loss: 0.36618659276011056		[learning rate: 0.0077863]
	Learning Rate: 0.00778634
	LOSS [training: 0.4129280236024491 | validation: 0.708896850727698]
	TIME [epoch: 8.31 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4764161135040793		[learning rate: 0.0077772]
		[batch 20/20] avg loss: 0.4069945841341607		[learning rate: 0.007768]
	Learning Rate: 0.00776797
	LOSS [training: 0.44170534881912005 | validation: 0.28775444860994176]
	TIME [epoch: 8.34 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30607367987522116		[learning rate: 0.0077588]
		[batch 20/20] avg loss: 0.3620337152447512		[learning rate: 0.0077496]
	Learning Rate: 0.00774965
	LOSS [training: 0.3340536975599862 | validation: 0.45203258331693763]
	TIME [epoch: 8.29 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33552313908495446		[learning rate: 0.0077405]
		[batch 20/20] avg loss: 0.4271578955471952		[learning rate: 0.0077314]
	Learning Rate: 0.00773137
	LOSS [training: 0.38134051731607493 | validation: 0.2673467141003498]
	TIME [epoch: 8.28 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34318962986251433		[learning rate: 0.0077222]
		[batch 20/20] avg loss: 0.3457390197166349		[learning rate: 0.0077131]
	Learning Rate: 0.00771313
	LOSS [training: 0.34446432478957467 | validation: 0.5432284775578712]
	TIME [epoch: 8.31 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3587472575112782		[learning rate: 0.007704]
		[batch 20/20] avg loss: 0.329494015146384		[learning rate: 0.0076949]
	Learning Rate: 0.00769494
	LOSS [training: 0.34412063632883105 | validation: 0.27118690252452327]
	TIME [epoch: 8.33 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5993294119441737		[learning rate: 0.0076859]
		[batch 20/20] avg loss: 0.3698276960367375		[learning rate: 0.0076768]
	Learning Rate: 0.00767679
	LOSS [training: 0.4845785539904556 | validation: 0.3522165671928718]
	TIME [epoch: 8.29 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38349804014319944		[learning rate: 0.0076677]
		[batch 20/20] avg loss: 0.38488092873132984		[learning rate: 0.0076587]
	Learning Rate: 0.00765868
	LOSS [training: 0.3841894844372646 | validation: 0.6856971469425046]
	TIME [epoch: 8.29 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36219373171595176		[learning rate: 0.0076496]
		[batch 20/20] avg loss: 0.3508436543169244		[learning rate: 0.0076406]
	Learning Rate: 0.00764061
	LOSS [training: 0.3565186930164382 | validation: 1.5639008212539782]
	TIME [epoch: 8.32 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4482833026594091		[learning rate: 0.0076316]
		[batch 20/20] avg loss: 0.4102134873807489		[learning rate: 0.0076226]
	Learning Rate: 0.00762259
	LOSS [training: 0.429248395020079 | validation: 0.3738926838666521]
	TIME [epoch: 8.32 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45513732111255323		[learning rate: 0.0076136]
		[batch 20/20] avg loss: 0.3579978887829417		[learning rate: 0.0076046]
	Learning Rate: 0.00760461
	LOSS [training: 0.40656760494774746 | validation: 0.5742055688459158]
	TIME [epoch: 8.3 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3846647549536503		[learning rate: 0.0075956]
		[batch 20/20] avg loss: 0.42257329033735075		[learning rate: 0.0075867]
	Learning Rate: 0.00758667
	LOSS [training: 0.40361902264550054 | validation: 0.6976592395085631]
	TIME [epoch: 8.29 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4710027038053206		[learning rate: 0.0075777]
		[batch 20/20] avg loss: 0.30557373557785533		[learning rate: 0.0075688]
	Learning Rate: 0.00756878
	LOSS [training: 0.388288219691588 | validation: 0.2949698525097548]
	TIME [epoch: 8.32 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3363661772207144		[learning rate: 0.0075598]
		[batch 20/20] avg loss: 0.3962641658598923		[learning rate: 0.0075509]
	Learning Rate: 0.00755092
	LOSS [training: 0.36631517154030335 | validation: 0.2925846014457438]
	TIME [epoch: 8.29 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33497593387986885		[learning rate: 0.007542]
		[batch 20/20] avg loss: 0.39990107170111056		[learning rate: 0.0075331]
	Learning Rate: 0.00753311
	LOSS [training: 0.36743850279048973 | validation: 0.45207542751948426]
	TIME [epoch: 8.32 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3214224408801565		[learning rate: 0.0075242]
		[batch 20/20] avg loss: 0.3918146309316907		[learning rate: 0.0075153]
	Learning Rate: 0.00751534
	LOSS [training: 0.35661853590592363 | validation: 0.34381262740543334]
	TIME [epoch: 8.28 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3756539085182361		[learning rate: 0.0075065]
		[batch 20/20] avg loss: 0.48926991209022674		[learning rate: 0.0074976]
	Learning Rate: 0.00749761
	LOSS [training: 0.4324619103042314 | validation: 0.37092965700887365]
	TIME [epoch: 8.32 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38264828349973		[learning rate: 0.0074888]
		[batch 20/20] avg loss: 0.35921367000633714		[learning rate: 0.0074799]
	Learning Rate: 0.00747993
	LOSS [training: 0.37093097675303355 | validation: 0.20841416271999938]
	TIME [epoch: 8.3 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32616624213832685		[learning rate: 0.0074711]
		[batch 20/20] avg loss: 0.390473888554744		[learning rate: 0.0074623]
	Learning Rate: 0.00746228
	LOSS [training: 0.35832006534653543 | validation: 0.6789586187239017]
	TIME [epoch: 8.31 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4076526604458035		[learning rate: 0.0074535]
		[batch 20/20] avg loss: 0.447859228899042		[learning rate: 0.0074447]
	Learning Rate: 0.00744468
	LOSS [training: 0.42775594467242284 | validation: 0.3250966942460225]
	TIME [epoch: 8.29 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36398145665396225		[learning rate: 0.0074359]
		[batch 20/20] avg loss: 0.36389888809990767		[learning rate: 0.0074271]
	Learning Rate: 0.00742712
	LOSS [training: 0.363940172376935 | validation: 0.20340947960906386]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4362728211935229		[learning rate: 0.0074184]
		[batch 20/20] avg loss: 0.3497829592061403		[learning rate: 0.0074096]
	Learning Rate: 0.0074096
	LOSS [training: 0.3930278901998316 | validation: 0.3374128154357854]
	TIME [epoch: 8.29 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3221904370781687		[learning rate: 0.0074009]
		[batch 20/20] avg loss: 0.5237530797755329		[learning rate: 0.0073921]
	Learning Rate: 0.00739212
	LOSS [training: 0.42297175842685075 | validation: 0.2549266743361148]
	TIME [epoch: 8.31 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38370163476707164		[learning rate: 0.0073834]
		[batch 20/20] avg loss: 0.33094019937453534		[learning rate: 0.0073747]
	Learning Rate: 0.00737469
	LOSS [training: 0.3573209170708035 | validation: 0.24990431339068953]
	TIME [epoch: 8.31 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4451916857558286		[learning rate: 0.007366]
		[batch 20/20] avg loss: 0.35780595020680844		[learning rate: 0.0073573]
	Learning Rate: 0.00735729
	LOSS [training: 0.40149881798131853 | validation: 0.5329347487786581]
	TIME [epoch: 8.31 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39249394595865766		[learning rate: 0.0073486]
		[batch 20/20] avg loss: 0.28907998402199075		[learning rate: 0.0073399]
	Learning Rate: 0.00733994
	LOSS [training: 0.3407869649903241 | validation: 0.32495218300253803]
	TIME [epoch: 8.29 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4737379662866094		[learning rate: 0.0073313]
		[batch 20/20] avg loss: 0.40073881449224047		[learning rate: 0.0073226]
	Learning Rate: 0.00732262
	LOSS [training: 0.4372383903894249 | validation: 0.27260835983777665]
	TIME [epoch: 8.28 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2857714943669623		[learning rate: 0.007314]
		[batch 20/20] avg loss: 0.341103837554088		[learning rate: 0.0073053]
	Learning Rate: 0.00730535
	LOSS [training: 0.31343766596052514 | validation: 0.6153218485493828]
	TIME [epoch: 8.32 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48142260381705115		[learning rate: 0.0072967]
		[batch 20/20] avg loss: 0.29043057733337074		[learning rate: 0.0072881]
	Learning Rate: 0.00728812
	LOSS [training: 0.38592659057521095 | validation: 0.26597556083802865]
	TIME [epoch: 8.3 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.579455067852946		[learning rate: 0.0072795]
		[batch 20/20] avg loss: 0.3097911883068022		[learning rate: 0.0072709]
	Learning Rate: 0.00727093
	LOSS [training: 0.44462312807987414 | validation: 0.188022543661022]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.365848544698948		[learning rate: 0.0072623]
		[batch 20/20] avg loss: 0.27867893996846044		[learning rate: 0.0072538]
	Learning Rate: 0.00725377
	LOSS [training: 0.3222637423337042 | validation: 0.2743550780482107]
	TIME [epoch: 8.28 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32657723214039674		[learning rate: 0.0072452]
		[batch 20/20] avg loss: 0.3969233392198461		[learning rate: 0.0072367]
	Learning Rate: 0.00723666
	LOSS [training: 0.36175028568012146 | validation: 0.1841452820678984]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.287175871967116		[learning rate: 0.0072281]
		[batch 20/20] avg loss: 0.43299996888472403		[learning rate: 0.0072196]
	Learning Rate: 0.00721959
	LOSS [training: 0.3600879204259201 | validation: 0.45941971032389417]
	TIME [epoch: 8.3 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3992307746082628		[learning rate: 0.0072111]
		[batch 20/20] avg loss: 0.37798574771513105		[learning rate: 0.0072026]
	Learning Rate: 0.00720256
	LOSS [training: 0.388608261161697 | validation: 2.1464932287060345]
	TIME [epoch: 8.29 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5464575151246164		[learning rate: 0.0071941]
		[batch 20/20] avg loss: 0.3751667613874224		[learning rate: 0.0071856]
	Learning Rate: 0.00718558
	LOSS [training: 0.46081213825601947 | validation: 0.27554025559264184]
	TIME [epoch: 8.28 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33496914675294687		[learning rate: 0.0071771]
		[batch 20/20] avg loss: 0.28631486776916293		[learning rate: 0.0071686]
	Learning Rate: 0.00716863
	LOSS [training: 0.31064200726105495 | validation: 0.3449577801512072]
	TIME [epoch: 8.33 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27990057284171005		[learning rate: 0.0071602]
		[batch 20/20] avg loss: 0.2874853804101215		[learning rate: 0.0071517]
	Learning Rate: 0.00715172
	LOSS [training: 0.2836929766259158 | validation: 0.14092650818192326]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3251601769341976		[learning rate: 0.0071433]
		[batch 20/20] avg loss: 0.3881089630265691		[learning rate: 0.0071348]
	Learning Rate: 0.00713485
	LOSS [training: 0.35663456998038334 | validation: 0.3019304980462753]
	TIME [epoch: 8.28 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34539865826436633		[learning rate: 0.0071264]
		[batch 20/20] avg loss: 0.3989307425037831		[learning rate: 0.007118]
	Learning Rate: 0.00711802
	LOSS [training: 0.3721647003840746 | validation: 0.21354790675278282]
	TIME [epoch: 8.28 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39507152181117144		[learning rate: 0.0071096]
		[batch 20/20] avg loss: 0.2861516755122189		[learning rate: 0.0071012]
	Learning Rate: 0.00710123
	LOSS [training: 0.3406115986616952 | validation: 0.31361666407286354]
	TIME [epoch: 8.32 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.300593909248141		[learning rate: 0.0070928]
		[batch 20/20] avg loss: 0.33867849814596396		[learning rate: 0.0070845]
	Learning Rate: 0.00708447
	LOSS [training: 0.31963620369705253 | validation: 0.5289204880106816]
	TIME [epoch: 8.31 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4177660302382922		[learning rate: 0.0070761]
		[batch 20/20] avg loss: 0.275221760856544		[learning rate: 0.0070678]
	Learning Rate: 0.00706776
	LOSS [training: 0.34649389554741805 | validation: 0.26417687505323095]
	TIME [epoch: 8.28 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3346664463556554		[learning rate: 0.0070594]
		[batch 20/20] avg loss: 0.32371482340570373		[learning rate: 0.0070511]
	Learning Rate: 0.00705109
	LOSS [training: 0.32919063488067957 | validation: 0.45034150961714753]
	TIME [epoch: 8.28 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29050607367094344		[learning rate: 0.0070428]
		[batch 20/20] avg loss: 0.3254488821924788		[learning rate: 0.0070345]
	Learning Rate: 0.00703446
	LOSS [training: 0.3079774779317111 | validation: 0.9858052527627613]
	TIME [epoch: 8.32 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3957248790548986		[learning rate: 0.0070262]
		[batch 20/20] avg loss: 0.43038290515858507		[learning rate: 0.0070179]
	Learning Rate: 0.00701787
	LOSS [training: 0.4130538921067418 | validation: 0.18712047625724595]
	TIME [epoch: 8.31 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3578607354206986		[learning rate: 0.0070096]
		[batch 20/20] avg loss: 0.3597704207848442		[learning rate: 0.0070013]
	Learning Rate: 0.00700131
	LOSS [training: 0.3588155781027714 | validation: 0.18187271057908408]
	TIME [epoch: 8.28 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3090313600394966		[learning rate: 0.006993]
		[batch 20/20] avg loss: 0.32554076528412557		[learning rate: 0.0069848]
	Learning Rate: 0.0069848
	LOSS [training: 0.31728606266181114 | validation: 0.2701549410276602]
	TIME [epoch: 8.3 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28244130233785597		[learning rate: 0.0069766]
		[batch 20/20] avg loss: 0.40555376000072707		[learning rate: 0.0069683]
	Learning Rate: 0.00696832
	LOSS [training: 0.3439975311692915 | validation: 0.4764741081786237]
	TIME [epoch: 8.31 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3752865788418511		[learning rate: 0.0069601]
		[batch 20/20] avg loss: 0.41548057482168116		[learning rate: 0.0069519]
	Learning Rate: 0.00695188
	LOSS [training: 0.3953835768317663 | validation: 0.19560237274936182]
	TIME [epoch: 8.29 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32307335135815246		[learning rate: 0.0069437]
		[batch 20/20] avg loss: 0.2369512106431105		[learning rate: 0.0069355]
	Learning Rate: 0.00693549
	LOSS [training: 0.2800122810006315 | validation: 0.3890152599057264]
	TIME [epoch: 8.3 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3626390291586372		[learning rate: 0.0069273]
		[batch 20/20] avg loss: 0.35384650892574193		[learning rate: 0.0069191]
	Learning Rate: 0.00691913
	LOSS [training: 0.35824276904218955 | validation: 0.21122590226939325]
	TIME [epoch: 8.3 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31018987575933915		[learning rate: 0.006911]
		[batch 20/20] avg loss: 0.3484508757751759		[learning rate: 0.0069028]
	Learning Rate: 0.00690281
	LOSS [training: 0.32932037576725753 | validation: 0.37388948464662897]
	TIME [epoch: 8.3 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2976054243211176		[learning rate: 0.0068947]
		[batch 20/20] avg loss: 0.5609502789536218		[learning rate: 0.0068865]
	Learning Rate: 0.00688652
	LOSS [training: 0.4292778516373697 | validation: 0.21732623782647648]
	TIME [epoch: 8.28 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29737392234985893		[learning rate: 0.0068784]
		[batch 20/20] avg loss: 0.3576083268066281		[learning rate: 0.0068703]
	Learning Rate: 0.00687028
	LOSS [training: 0.3274911245782435 | validation: 0.41529759373026265]
	TIME [epoch: 8.31 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3152738126051883		[learning rate: 0.0068622]
		[batch 20/20] avg loss: 0.32638776562195393		[learning rate: 0.0068541]
	Learning Rate: 0.00685407
	LOSS [training: 0.3208307891135711 | validation: 0.19742880494287107]
	TIME [epoch: 8.31 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24258353261832002		[learning rate: 0.006846]
		[batch 20/20] avg loss: 0.41036523256464025		[learning rate: 0.0068379]
	Learning Rate: 0.0068379
	LOSS [training: 0.32647438259148015 | validation: 0.15111297072820468]
	TIME [epoch: 8.31 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28456047504274384		[learning rate: 0.0068298]
		[batch 20/20] avg loss: 0.3996167474942951		[learning rate: 0.0068218]
	Learning Rate: 0.00682178
	LOSS [training: 0.3420886112685195 | validation: 0.3289476431496597]
	TIME [epoch: 8.29 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3555851991902609		[learning rate: 0.0068137]
		[batch 20/20] avg loss: 0.2626966331918422		[learning rate: 0.0068057]
	Learning Rate: 0.00680568
	LOSS [training: 0.3091409161910515 | validation: 0.14253714529684225]
	TIME [epoch: 8.3 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37442818431745906		[learning rate: 0.0067977]
		[batch 20/20] avg loss: 0.49748759449022295		[learning rate: 0.0067896]
	Learning Rate: 0.00678963
	LOSS [training: 0.43595788940384095 | validation: 0.5180402312781507]
	TIME [epoch: 8.32 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47511547707789276		[learning rate: 0.0067816]
		[batch 20/20] avg loss: 0.3229885955326237		[learning rate: 0.0067736]
	Learning Rate: 0.00677361
	LOSS [training: 0.3990520363052582 | validation: 0.2208309767625859]
	TIME [epoch: 8.32 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3271218462548819		[learning rate: 0.0067656]
		[batch 20/20] avg loss: 0.31708393163050674		[learning rate: 0.0067576]
	Learning Rate: 0.00675764
	LOSS [training: 0.3221028889426943 | validation: 0.2219190757727874]
	TIME [epoch: 8.29 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3126699389521781		[learning rate: 0.0067497]
		[batch 20/20] avg loss: 0.287503861128874		[learning rate: 0.0067417]
	Learning Rate: 0.0067417
	LOSS [training: 0.3000869000405261 | validation: 0.22131272124249185]
	TIME [epoch: 8.29 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3370887670047188		[learning rate: 0.0067337]
		[batch 20/20] avg loss: 0.34673232170582374		[learning rate: 0.0067258]
	Learning Rate: 0.00672579
	LOSS [training: 0.3419105443552713 | validation: 0.16380437067366485]
	TIME [epoch: 8.34 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21419669731043603		[learning rate: 0.0067179]
		[batch 20/20] avg loss: 0.47013632750642154		[learning rate: 0.0067099]
	Learning Rate: 0.00670993
	LOSS [training: 0.3421665124084288 | validation: 0.1878418556265736]
	TIME [epoch: 8.32 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2446608882496882		[learning rate: 0.006702]
		[batch 20/20] avg loss: 0.37040396226420347		[learning rate: 0.0066941]
	Learning Rate: 0.0066941
	LOSS [training: 0.30753242525694585 | validation: 0.2164286162217005]
	TIME [epoch: 8.3 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2612023729566174		[learning rate: 0.0066862]
		[batch 20/20] avg loss: 0.23594290916254113		[learning rate: 0.0066783]
	Learning Rate: 0.00667831
	LOSS [training: 0.24857264105957927 | validation: 0.42180494053797396]
	TIME [epoch: 8.3 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4497087290396829		[learning rate: 0.0066704]
		[batch 20/20] avg loss: 0.39387098393222253		[learning rate: 0.0066626]
	Learning Rate: 0.00666256
	LOSS [training: 0.4217898564859527 | validation: 0.2993356028001182]
	TIME [epoch: 8.34 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24530251553068103		[learning rate: 0.0066547]
		[batch 20/20] avg loss: 0.4608580232363891		[learning rate: 0.0066468]
	Learning Rate: 0.00664684
	LOSS [training: 0.353080269383535 | validation: 0.251897861729994]
	TIME [epoch: 8.32 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3321493391597895		[learning rate: 0.006639]
		[batch 20/20] avg loss: 0.28640537554852935		[learning rate: 0.0066312]
	Learning Rate: 0.00663116
	LOSS [training: 0.30927735735415945 | validation: 0.5172418976189455]
	TIME [epoch: 8.3 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36138249510345466		[learning rate: 0.0066233]
		[batch 20/20] avg loss: 0.32207071911773755		[learning rate: 0.0066155]
	Learning Rate: 0.00661552
	LOSS [training: 0.3417266071105961 | validation: 0.2812693952892088]
	TIME [epoch: 8.3 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3551329148898916		[learning rate: 0.0066077]
		[batch 20/20] avg loss: 0.29037401025206344		[learning rate: 0.0065999]
	Learning Rate: 0.00659992
	LOSS [training: 0.3227534625709775 | validation: 0.27388091961964245]
	TIME [epoch: 8.33 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27023269315647847		[learning rate: 0.0065921]
		[batch 20/20] avg loss: 0.2904923905246951		[learning rate: 0.0065843]
	Learning Rate: 0.00658435
	LOSS [training: 0.28036254184058673 | validation: 0.2534080432504753]
	TIME [epoch: 8.33 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33245897186995876		[learning rate: 0.0065766]
		[batch 20/20] avg loss: 0.24746232402889223		[learning rate: 0.0065688]
	Learning Rate: 0.00656882
	LOSS [training: 0.28996064794942555 | validation: 0.2934152835309341]
	TIME [epoch: 8.29 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26616565147560206		[learning rate: 0.0065611]
		[batch 20/20] avg loss: 0.327292180151037		[learning rate: 0.0065533]
	Learning Rate: 0.00655332
	LOSS [training: 0.29672891581331945 | validation: 0.5331027394165297]
	TIME [epoch: 8.29 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30897091571430724		[learning rate: 0.0065456]
		[batch 20/20] avg loss: 0.29845597802395707		[learning rate: 0.0065379]
	Learning Rate: 0.00653786
	LOSS [training: 0.30371344686913215 | validation: 0.2010152102857262]
	TIME [epoch: 8.32 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42596480089163036		[learning rate: 0.0065301]
		[batch 20/20] avg loss: 0.23524989761817539		[learning rate: 0.0065224]
	Learning Rate: 0.00652244
	LOSS [training: 0.33060734925490287 | validation: 0.7501921854726473]
	TIME [epoch: 8.34 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2848876897814004		[learning rate: 0.0065147]
		[batch 20/20] avg loss: 0.2879786342534636		[learning rate: 0.0065071]
	Learning Rate: 0.00650706
	LOSS [training: 0.28643316201743196 | validation: 0.5105569975991804]
	TIME [epoch: 8.29 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3698493645486347		[learning rate: 0.0064994]
		[batch 20/20] avg loss: 0.3201137698430143		[learning rate: 0.0064917]
	Learning Rate: 0.00649171
	LOSS [training: 0.3449815671958245 | validation: 0.21927625019937394]
	TIME [epoch: 8.29 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2966179038918623		[learning rate: 0.006484]
		[batch 20/20] avg loss: 0.35877565495897407		[learning rate: 0.0064764]
	Learning Rate: 0.00647639
	LOSS [training: 0.3276967794254183 | validation: 0.18154900858508063]
	TIME [epoch: 8.33 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30924701195951065		[learning rate: 0.0064688]
		[batch 20/20] avg loss: 0.2731374859769583		[learning rate: 0.0064611]
	Learning Rate: 0.00646112
	LOSS [training: 0.2911922489682345 | validation: 0.3119984245446485]
	TIME [epoch: 8.33 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3909966529829143		[learning rate: 0.0064535]
		[batch 20/20] avg loss: 0.3630152024749913		[learning rate: 0.0064459]
	Learning Rate: 0.00644588
	LOSS [training: 0.3770059277289528 | validation: 0.2710148300378077]
	TIME [epoch: 8.3 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36038233669304387		[learning rate: 0.0064383]
		[batch 20/20] avg loss: 0.2659244478259791		[learning rate: 0.0064307]
	Learning Rate: 0.00643067
	LOSS [training: 0.31315339225951144 | validation: 0.5550252362330924]
	TIME [epoch: 8.29 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35750553081434455		[learning rate: 0.0064231]
		[batch 20/20] avg loss: 0.3067551038354776		[learning rate: 0.0064155]
	Learning Rate: 0.0064155
	LOSS [training: 0.33213031732491116 | validation: 0.4320073787874503]
	TIME [epoch: 8.33 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3804611821283504		[learning rate: 0.0064079]
		[batch 20/20] avg loss: 0.42743518248720935		[learning rate: 0.0064004]
	Learning Rate: 0.00640037
	LOSS [training: 0.4039481823077799 | validation: 0.21141620600570232]
	TIME [epoch: 8.32 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3450190133549518		[learning rate: 0.0063928]
		[batch 20/20] avg loss: 0.31017361008631006		[learning rate: 0.0063853]
	Learning Rate: 0.00638527
	LOSS [training: 0.3275963117206309 | validation: 0.14692808359022325]
	TIME [epoch: 8.3 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.271525362278635		[learning rate: 0.0063777]
		[batch 20/20] avg loss: 0.3739630763161162		[learning rate: 0.0063702]
	Learning Rate: 0.00637021
	LOSS [training: 0.32274421929737557 | validation: 0.2406364149123622]
	TIME [epoch: 8.3 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33891863659757837		[learning rate: 0.0063627]
		[batch 20/20] avg loss: 0.2976607588019506		[learning rate: 0.0063552]
	Learning Rate: 0.00635518
	LOSS [training: 0.31828969769976456 | validation: 0.2506217777775955]
	TIME [epoch: 8.32 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39342485535052896		[learning rate: 0.0063477]
		[batch 20/20] avg loss: 0.5148268254536215		[learning rate: 0.0063402]
	Learning Rate: 0.00634019
	LOSS [training: 0.4541258404020752 | validation: 0.6370013168455305]
	TIME [epoch: 8.3 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3109925556553169		[learning rate: 0.0063327]
		[batch 20/20] avg loss: 0.43784685824417274		[learning rate: 0.0063252]
	Learning Rate: 0.00632524
	LOSS [training: 0.37441970694974486 | validation: 0.17649717901526935]
	TIME [epoch: 8.32 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24833558001930642		[learning rate: 0.0063178]
		[batch 20/20] avg loss: 0.23770952585023894		[learning rate: 0.0063103]
	Learning Rate: 0.00631032
	LOSS [training: 0.24302255293477276 | validation: 0.2385144428165307]
	TIME [epoch: 8.29 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28705902507155556		[learning rate: 0.0063029]
		[batch 20/20] avg loss: 0.31923415251641163		[learning rate: 0.0062954]
	Learning Rate: 0.00629543
	LOSS [training: 0.30314658879398365 | validation: 0.18404381627098607]
	TIME [epoch: 8.33 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37269176981120133		[learning rate: 0.006288]
		[batch 20/20] avg loss: 0.45622691004674565		[learning rate: 0.0062806]
	Learning Rate: 0.00628058
	LOSS [training: 0.4144593399289735 | validation: 0.1363867041704895]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3479889258722945		[learning rate: 0.0062732]
		[batch 20/20] avg loss: 0.26446810691308614		[learning rate: 0.0062658]
	Learning Rate: 0.00626577
	LOSS [training: 0.30622851639269033 | validation: 0.2693447459558553]
	TIME [epoch: 8.31 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26539036296372887		[learning rate: 0.0062584]
		[batch 20/20] avg loss: 0.2835732289486104		[learning rate: 0.006251]
	Learning Rate: 0.00625099
	LOSS [training: 0.27448179595616967 | validation: 0.19422728072587606]
	TIME [epoch: 8.28 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42390149773731184		[learning rate: 0.0062436]
		[batch 20/20] avg loss: 0.25126919945025394		[learning rate: 0.0062362]
	Learning Rate: 0.00623624
	LOSS [training: 0.33758534859378286 | validation: 0.30678302337801233]
	TIME [epoch: 8.31 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2812275358341891		[learning rate: 0.0062289]
		[batch 20/20] avg loss: 0.25831787965520414		[learning rate: 0.0062215]
	Learning Rate: 0.00622153
	LOSS [training: 0.26977270774469664 | validation: 0.4347008292743729]
	TIME [epoch: 8.27 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2205784613190719		[learning rate: 0.0062142]
		[batch 20/20] avg loss: 0.36412098001098026		[learning rate: 0.0062069]
	Learning Rate: 0.00620686
	LOSS [training: 0.2923497206650261 | validation: 0.15582079448392894]
	TIME [epoch: 8.28 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28732870323067694		[learning rate: 0.0061995]
		[batch 20/20] avg loss: 0.21704633052415628		[learning rate: 0.0061922]
	Learning Rate: 0.00619222
	LOSS [training: 0.2521875168774167 | validation: 0.20282426612634413]
	TIME [epoch: 8.29 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20117718597825002		[learning rate: 0.0061849]
		[batch 20/20] avg loss: 0.3341967770108457		[learning rate: 0.0061776]
	Learning Rate: 0.00617761
	LOSS [training: 0.2676869814945479 | validation: 0.1777373973118181]
	TIME [epoch: 8.31 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3850107245967666		[learning rate: 0.0061703]
		[batch 20/20] avg loss: 0.3147364916851897		[learning rate: 0.006163]
	Learning Rate: 0.00616304
	LOSS [training: 0.34987360814097823 | validation: 0.17681060877653496]
	TIME [epoch: 8.27 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29581415748171		[learning rate: 0.0061558]
		[batch 20/20] avg loss: 0.27460619611023596		[learning rate: 0.0061485]
	Learning Rate: 0.0061485
	LOSS [training: 0.28521017679597294 | validation: 0.19303661291686888]
	TIME [epoch: 8.27 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24948628805841136		[learning rate: 0.0061412]
		[batch 20/20] avg loss: 0.34332413543679347		[learning rate: 0.006134]
	Learning Rate: 0.006134
	LOSS [training: 0.2964052117476024 | validation: 0.5099638682437104]
	TIME [epoch: 8.31 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2849867918144081		[learning rate: 0.0061268]
		[batch 20/20] avg loss: 0.29237068000921596		[learning rate: 0.0061195]
	Learning Rate: 0.00611953
	LOSS [training: 0.28867873591181203 | validation: 0.4811686968207125]
	TIME [epoch: 8.31 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30182136821679706		[learning rate: 0.0061123]
		[batch 20/20] avg loss: 0.32269831139286337		[learning rate: 0.0061051]
	Learning Rate: 0.00610509
	LOSS [training: 0.3122598398048301 | validation: 0.12790153971790832]
	TIME [epoch: 8.27 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study2/model_tr_study2_r5_20240219_184940/states/model_tr_study2_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41026996274810035		[learning rate: 0.0060979]
		[batch 20/20] avg loss: 0.42740946758677045		[learning rate: 0.0060907]
	Learning Rate: 0.00609069
	LOSS [training: 0.4188397151674354 | validation: 0.24935793077320131]
	TIME [epoch: 8.26 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22729291613425612		[learning rate: 0.0060835]
		[batch 20/20] avg loss: 0.2428834707465488		[learning rate: 0.0060763]
	Learning Rate: 0.00607633
	LOSS [training: 0.23508819344040247 | validation: 0.18797253381664503]
	TIME [epoch: 8.3 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3131500347170035		[learning rate: 0.0060692]
		[batch 20/20] avg loss: 0.24243529137226377		[learning rate: 0.006062]
	Learning Rate: 0.00606199
	LOSS [training: 0.2777926630446336 | validation: 0.12810006513781613]
	TIME [epoch: 8.28 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37445004315869734		[learning rate: 0.0060548]
		[batch 20/20] avg loss: 0.3952333753757167		[learning rate: 0.0060477]
	Learning Rate: 0.00604769
	LOSS [training: 0.384841709267207 | validation: 0.30555790395200294]
	TIME [epoch: 8.26 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1958225269440666		[learning rate: 0.0060406]
		[batch 20/20] avg loss: 0.5273602381074394		[learning rate: 0.0060334]
	Learning Rate: 0.00603343
	LOSS [training: 0.361591382525753 | validation: 0.1497535312205834]
	TIME [epoch: 8.26 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34649859468451716		[learning rate: 0.0060263]
		[batch 20/20] avg loss: 0.2832752227972381		[learning rate: 0.0060192]
	Learning Rate: 0.0060192
	LOSS [training: 0.31488690874087766 | validation: 0.2637222705588502]
	TIME [epoch: 8.28 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3886313363470971		[learning rate: 0.0060121]
		[batch 20/20] avg loss: 0.40184397875882266		[learning rate: 0.006005]
	Learning Rate: 0.006005
	LOSS [training: 0.3952376575529599 | validation: 0.7295237183632227]
	TIME [epoch: 8.3 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27326961555833096		[learning rate: 0.0059979]
		[batch 20/20] avg loss: 0.5580707720752125		[learning rate: 0.0059908]
	Learning Rate: 0.00599083
	LOSS [training: 0.4156701938167718 | validation: 0.190582478035071]
	TIME [epoch: 8.25 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3732754513223595		[learning rate: 0.0059838]
		[batch 20/20] avg loss: 0.33611960492300275		[learning rate: 0.0059767]
	Learning Rate: 0.0059767
	LOSS [training: 0.3546975281226812 | validation: 0.28878464724079517]
	TIME [epoch: 8.26 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5934550701133229		[learning rate: 0.0059696]
		[batch 20/20] avg loss: 0.3619285186874755		[learning rate: 0.0059626]
	Learning Rate: 0.0059626
	LOSS [training: 0.47769179440039905 | validation: 0.5850362745433831]
	TIME [epoch: 8.27 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49337310624352526		[learning rate: 0.0059556]
		[batch 20/20] avg loss: 0.5702020348742468		[learning rate: 0.0059485]
	Learning Rate: 0.00594854
	LOSS [training: 0.531787570558886 | validation: 0.22426971577571098]
	TIME [epoch: 8.31 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1426739483951138		[learning rate: 0.0059415]
		[batch 20/20] avg loss: 1.4506190595108908		[learning rate: 0.0059345]
	Learning Rate: 0.00593451
	LOSS [training: 1.2966465039530026 | validation: 0.35192621633192267]
	TIME [epoch: 8.25 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.440004559519516		[learning rate: 0.0059275]
		[batch 20/20] avg loss: 0.6640563405241855		[learning rate: 0.0059205]
	Learning Rate: 0.00592051
	LOSS [training: 0.5520304500218508 | validation: 0.2805434815057295]
	TIME [epoch: 8.25 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0545744685644378		[learning rate: 0.0059135]
		[batch 20/20] avg loss: 1.5269904917299573		[learning rate: 0.0059065]
	Learning Rate: 0.00590654
	LOSS [training: 1.2907824801471974 | validation: 0.4717733079083318]
	TIME [epoch: 8.28 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5137362598984689		[learning rate: 0.0058996]
		[batch 20/20] avg loss: 0.4380840694029766		[learning rate: 0.0058926]
	Learning Rate: 0.00589261
	LOSS [training: 0.4759101646507228 | validation: 0.24843796152423714]
	TIME [epoch: 8.29 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42320263427251936		[learning rate: 0.0058857]
		[batch 20/20] avg loss: 0.3836079351822283		[learning rate: 0.0058787]
	Learning Rate: 0.00587871
	LOSS [training: 0.4034052847273738 | validation: 0.5250534703531929]
	TIME [epoch: 8.27 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4434705121059638		[learning rate: 0.0058718]
		[batch 20/20] avg loss: 0.5323555690307191		[learning rate: 0.0058648]
	Learning Rate: 0.00586484
	LOSS [training: 0.4879130405683415 | validation: 0.36713400763909443]
	TIME [epoch: 8.25 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4806651909600662		[learning rate: 0.0058579]
		[batch 20/20] avg loss: 0.5369722281934959		[learning rate: 0.005851]
	Learning Rate: 0.00585101
	LOSS [training: 0.5088187095767811 | validation: 0.3787868556312228]
	TIME [epoch: 8.29 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35469333481564025		[learning rate: 0.0058441]
		[batch 20/20] avg loss: 0.39445106746896		[learning rate: 0.0058372]
	Learning Rate: 0.00583721
	LOSS [training: 0.37457220114230017 | validation: 0.3410643506189268]
	TIME [epoch: 8.28 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31055215159031224		[learning rate: 0.0058303]
		[batch 20/20] avg loss: 0.8688099890033989		[learning rate: 0.0058234]
	Learning Rate: 0.00582344
	LOSS [training: 0.5896810702968557 | validation: 1.0637109457379799]
	TIME [epoch: 8.3 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7130183986645137		[learning rate: 0.0058166]
		[batch 20/20] avg loss: 0.3774447626021459		[learning rate: 0.0058097]
	Learning Rate: 0.0058097
	LOSS [training: 0.5452315806333299 | validation: 1.2430855812625392]
	TIME [epoch: 8.26 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8972378709991677		[learning rate: 0.0058028]
		[batch 20/20] avg loss: 1.0768820767246805		[learning rate: 0.005796]
	Learning Rate: 0.005796
	LOSS [training: 0.9870599738619241 | validation: 0.386380599694956]
	TIME [epoch: 8.29 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9392168305615387		[learning rate: 0.0057892]
		[batch 20/20] avg loss: 0.44911924257750496		[learning rate: 0.0057823]
	Learning Rate: 0.00578233
	LOSS [training: 0.6941680365695219 | validation: 0.21702189804099462]
	TIME [epoch: 8.27 sec]
EPOCH 283/2000:
	Training over batches...
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.05
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.025
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.0125
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.00625
